From jfox at mcmaster.ca  Fri Oct  1 02:34:53 2004
From: jfox at mcmaster.ca (John Fox)
Date: Thu, 30 Sep 2004 20:34:53 -0400
Subject: [R] polr (MASS) and lrm (Design) differences in tests of
	statistical signifcance 
In-Reply-To: <415C7D68.2020103@ku.edu>
Message-ID: <20041001003449.DLRH15612.tomts16-srv.bellnexxia.net@JohnDesktop8300>

Dear Paul,

I tried polr() and lrm() on a different problem and (except for the
difference in signs for the cut-points/intercepts) got identical results for
both coefficients and standard errors. There might be something
ill-conditioned about your problem that produces the discrepancy -- I
noticed, for example, that some of the upper categories of the response are
very sparse. Perhaps the two functions use different forms of the
information matrix. I expect that someone else will be able to supply more
details.

I believe that the t-statistics in the polr() output are actually Wald
statistics.

I hope this helps,
 John



> -----Original Message-----
> From: r-help-bounces at stat.math.ethz.ch 
> [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Paul Johnson
> Sent: Thursday, September 30, 2004 4:41 PM
> To: r help
> Subject: [R] polr (MASS) and lrm (Design) differences in 
> tests of statistical signifcance 
> 
> Greetings:
> 
> I'm running R-1.9.1 on Fedora Core 2 Linux.
> 
> I tested a proportional odds logistic regression with MASS's 
> polr and Design's lrm.  Parameter estimates between the 2 are 
> consistent, but the standard errors are quite different, and 
> the conclusions from the t and Wald tests are dramatically 
> different. I cranked the "abstol" argument up quite a bit in 
> the polr method and it did not make the differences go away.
> 
> So
> 
> 1. Can you help me see why the std. errors in the polr are so 
> much smaller, and
> 
> 2. Can I hear more opinions on the question of t vs. Wald in 
> making these signif tests. So far, I understand the t is 
> based on the asymptotic Normality of the estimate of b, and 
> for finite samples b/se is not exactly distributed as a t. 
> But I also had the impression that the Wald value was an 
> approximation as well.
> 
>  > summary(polr(as.factor(RENUCYC) ~ DOCS + PCT65PLS*RANNEY2 
> + OLDCRASH 
> +  FISCAL2 + PCTMETRO + ADMLICEN, data=elaine1))
> 
> Re-fitting to get Hessian
> 
> Call:
> polr(formula = as.factor(RENUCYC) ~ DOCS + PCT65PLS * RANNEY2 +
>      OLDCRASH + FISCAL2 + PCTMETRO + ADMLICEN, data = elaine1)
> 
> Coefficients:
>                          Value  Std. Error   t value
> DOCS              0.004942217 0.002952001  1.674192
> PCT65PLS          0.454638558 0.113504288  4.005475
> RANNEY2           0.110473483 0.010829826 10.200855
> OLDCRASH          0.139808663 0.042245692  3.309418
> FISCAL2           0.025592117 0.011465812  2.232037
> PCTMETRO          0.018184093 0.007792680  2.333484
> ADMLICEN         -0.028490387 0.011470999 -2.483688
> PCT65PLS:RANNEY2 -0.008559228 0.001456543 -5.876400
> 
> Intercepts:
>        Value   Std. Error t value
> 2|3    6.6177  0.3019    21.9216
> 3|4    7.1524  0.2773    25.7938
> 4|5   10.5856  0.2149    49.2691
> 5|6   12.2132  0.1858    65.7424
> 6|8   12.2704  0.1856    66.1063
> 8|10  13.0345  0.2184    59.6707
> 10|12 13.9801  0.3517    39.7519
> 12|18 14.6806  0.5587    26.2782
> 
> Residual Deviance: 587.0995
> AIC: 619.0995
> 
> 
>  > lrm(RENUCYC ~ DOCS + PCT65PLS*RANNEY2 + OLDCRASH +  
> FISCAL2 + PCTMETRO + ADMLICEN, data=elaine1)
> 
> Logistic Regression Model
> 
> lrm(formula = RENUCYC ~ DOCS + PCT65PLS * RANNEY2 + OLDCRASH +
>      FISCAL2 + PCTMETRO + ADMLICEN, data = elaine1)
> 
> 
> Frequencies of Responses
>    2   3   4   5   6   8  10  12  18
>   21  12 149  46   1  10   6   2   2
> 
> Frequencies of Missing Values Due to Each Variable
>   RENUCYC     DOCS PCT65PLS  RANNEY2 OLDCRASH  FISCAL2 
> PCTMETRO ADMLICEN
>         5        0        0        6        0        5        
> 0        5
> 
>         Obs  Max Deriv Model L.R.       d.f.          P          C 
>    Dxy
>         249      7e-05      56.58          8          0      0.733 
> 0.465
>       Gamma      Tau-a         R2      Brier
>        0.47      0.278       0.22      0.073
> 
>                     Coef       S.E.     Wald Z P
> y>=3                -6.617857 6.716688 -0.99  0.3245
> y>=4                -7.152561 6.716571 -1.06  0.2869
> y>=5               -10.585705 6.742222 -1.57  0.1164
> y>=6               -12.213340 6.755656 -1.81  0.0706
> y>=8               -12.270506 6.755571 -1.82  0.0693
> y>=10              -13.034584 6.756829 -1.93  0.0537
> y>=12              -13.980235 6.767724 -2.07  0.0389
> y>=18              -14.680760 6.786639 -2.16  0.0305
> DOCS                 0.004942 0.002932  1.69  0.0918
> PCT65PLS             0.454653 0.552430  0.82  0.4105
> RANNEY2              0.110475 0.076438  1.45  0.1484
> OLDCRASH             0.139805 0.042104  3.32  0.0009
> FISCAL2              0.025592 0.011374  2.25  0.0245
> PCTMETRO             0.018184 0.007823  2.32  0.0201
> ADMLICEN            -0.028490 0.011576 -2.46  0.0138
> PCT65PLS * RANNEY2  -0.008559 0.006417 -1.33  0.1822
> 
>  >
> 
> -- 
> Paul E. Johnson                       email: pauljohn at ku.edu
> Dept. of Political Science            http://lark.cc.ku.edu/~pauljohn
> 1541 Lilac Lane, Rm 504
> University of Kansas                  Office: (785) 864-9086
> Lawrence, Kansas 66044-3177           FAX: (785) 864-5700
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html



From nusbj at hotmail.com  Fri Oct  1 04:00:30 2004
From: nusbj at hotmail.com (Zhen Pang)
Date: Fri, 01 Oct 2004 10:00:30 +0800
Subject: Vectorising and loop (was Re: [R] optim "alog-likelihoodfunction")
Message-ID: <BAY22-F16h7wFI1ZG5y00006798@hotmail.com>

Thank you for your kind help. my s does not depend on the input theta. Code 
without loop is really efficient.


>From: Gabor Grothendieck <ggrothendieck at myway.com>
>To: r-help at stat.math.ethz.ch
>Subject: Re: Vectorising and loop (was Re: [R] optim 
>"alog-likelihoodfunction")
>Date: Thu, 30 Sep 2004 17:59:21 +0000 (UTC)
>
>Zhen Pang <nusbj <at> hotmail.com> writes:
>
>:
>: Mr. Grothendieck does suggest me to paste the data here. I just show a 
>small
>: one here. I must metion that I made a mistake in my former email. The 
>first
>: column should be j and is greater than the second column. I have 
>corrected
>: ll.
>:
>: z is the matrix below
>:
>: 2 1 3
>: 3 1 1
>: 3 3 1
>: 5 2 4
>:
>: k<-max(z[,1])
>: ll <- function(theta)
>:   {t<-0
>:    for (ii in 1:k)
>:       {t<-t+exp(theta[ii])}
>:    lll<-0
>:    x00<-1/(1+t)
>:    x0<-x00*exp(theta)
>: for (m in 1:length(z[,1]))
>:    {j<-z[m,1]
>:     i<-z[m,2]
>:     a<-z[m,3]
>:     l<-i:(k-j+i)
>:     s<-rep(0,k)
>:     s[l]<-choose(j,i)*choose((k-j),(l-i))/choose(k,l)
>: # we only define some of s to be non-zero, since dim(l) might be smaller
>: than dim(s)
>:     ss<-sum(s*x0)  # ss is a weighted sum of x0
>:      lll<-lll+a*log(ss)
>:     }
>: -lll
>: # the negative sign is to find the maximum of the log-likelihood 
>function.
>: It can be omitted if we #use the finscale option in optim.
>: }
>:
>: Then I need to optim(b0,ll,hessian=T),
>: where b0<-c(0.8331934, 20.8009068, -7.0893623,  1.2109221, 18.7213273).
>:
>: optim(b0,ll,hessian=T)
>: $par
>: [1]  0.8331934 20.8009068 -7.0893623  1.2109221 18.7213273
>:
>: $value
>: [1] 5.182791
>:
>: $counts
>: function gradient
>:       52       NA
>:
>: $convergence
>: [1] 0
>:
>: $message
>: NULL
>:
>: $hessian
>:               [,1]          [,2]          [,3]          [,4]          
>[,5]
>: [1,]  1.065814e-08 -9.325873e-09  0.000000e+00 -3.330669e-10 
>-2.109424e-09
>: [2,] -9.325873e-09  8.887936e-01 -3.330669e-10 -1.620926e-08 
>-8.887936e-01
>: [3,]  0.000000e+00 -3.330669e-10 -6.661338e-10  0.000000e+00  
>0.000000e+00
>: [4,] -3.330669e-10 -1.620926e-08  0.000000e+00  7.549517e-09  
>7.105427e-09
>: [5,] -2.109424e-09 -8.887936e-01  0.000000e+00  7.105427e-09  
>8.887936e-01
>:
>:
>: I have tried to use eval() and modify my function, it seems to be able to
>: remove the m loop, however, optim() can not recognize it. So my main 
>concern
>: is to avoid the loop and optim() can works for my function. Thanks.
>
>
>
>I suspect your code may be wrong but taking it at face value
>s does not depend on the input theta so precalculate it
>as a matrix whose mth column is s[,m].  Also the only
>purpose of the loop indexed by m is to calculate lll and
>the final iteration of that loop calculates an lll which
>does not depend on the prior iterations so remove the loop
>and just run the final iteration.  Similarly we only need
>the final value of x0 that is calculated.  Note that the
>value of ll(b0), your loopy function, and ll2(b0) the one
>line non-loopy function using the precalculated s, give
>the same result:
>
>R> z <- matrix(c( 2,1,3, 3,1,1, 3,3,1, 5,2,4), 4, 3, byrow = TRUE)
>R> b0<-c(0.8331934, 20.8009068, -7.0893623, 1.2109221, 18.7213273)
>R>
>R> k<-max(z[,1])
>R> s <- apply(z, 1, function(z) {
>+ j<-z[1]; i<-z[2]
>+ l<-i:(k-j+i)
>+ s<-rep(0,k)
>+ s[l]<-choose(j,i)*choose((k-j),(l-i))/choose(k,l)
>+ s
>+ })
>
>R> ll2 <- function(theta) {
>+ - sum(z[,3]*log(crossprod(s, exp(theta)* 1/(1+sum(exp(theta))))))
>+ }
>R> ll(b0) # this is the ll function from your post
>[1] 5.182791
>R> ll2(b0)
>[1] 5.182791
>
>______________________________________________
>R-help at stat.math.ethz.ch mailing list
>https://stat.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide! 
>http://www.R-project.org/posting-guide.html



From vikas at mail.jnu.ac.in  Fri Oct  1 07:20:55 2004
From: vikas at mail.jnu.ac.in (Vikas Rawal)
Date: Fri, 01 Oct 2004 10:50:55 +0530
Subject: [R] Reading multiple files into R
Message-ID: <415CE937.2010406@mail.jnu.ac.in>

I want to read data from a number of files into R.
Reading individual files one by one requires writing enormous amount of 
code that will look something like the following.

****************
maptools:::dbf.read("wb-01vc.dbf")->dist1
maptools:::dbf.read("wb-02vc.dbf")->dist2
maptools:::dbf.read("wb-03vc.dbf")->dist3
maptools:::dbf.read("wb-04vc.dbf")->dist4
maptools:::dbf.read("wb-05vc.dbf")->dist5
maptools:::dbf.read("wb-06vc.dbf")->dist6
maptools:::dbf.read("wb-07vc.dbf")->dist7
maptools:::dbf.read("wb-08vc.dbf")->dist8
maptools:::dbf.read("wb-09vc.dbf")->dist9
*****************

Is there a better way of doing this?

Vikas



From arv at ono.com  Fri Oct  1 08:31:50 2004
From: arv at ono.com (antonio rodriguez)
Date: Fri, 1 Oct 2004 08:31:50 +0200
Subject: [R] Can't load rgl library
In-Reply-To: <415C3F34.8030901@swissinfo.org>
Message-ID: <IPEFKICOHOECENGJBAGLMELNDHAA.arv@ono.com>

Hi Thomas,

Yes I have the xlibmesa-gl, libglu1-mesa, libglu1-mesa-dev, etc, libraries.
But no way. You mentioned TLS (transport Layer Security?), is this OK? I'm
working on a Toshiba laptop with graphics driver I852GM

Cheers,

Antonio

> -----Mensaje original-----
> De: r-help-bounces at stat.math.ethz.ch
> [mailto:r-help-bounces at stat.math.ethz.ch]En nombre de Thomas Sch??nhoff
> Enviado el: jueves, 30 de septiembre de 2004 19:16
> Para: R User-Liste
> Asunto: Re: [R] Can't load rgl library
>
>
>
> Hello,
>
> arv at ono.com schrieb:
> > Hi,
> >
> > I've installed rgl package through R CMD INSTALL on a
> Debian-Sarge machine
> > (PIV) without any compiling error (see attached file), but when
> trying to
> > load this package within R (and also Rcmdr library) I get:
> >
> >
> >>library(rgl)
> >
> > RGL: GLX extension missing on server
> > Error in firstlib(which.lib.loc, package) :
> > error rgl_init
> > error in library(rgl) : .First.lib failed
> > Segmentation fault
> >
> > Same failure problem if I use the apt option (apt-get install
> r-cran-gl).
> > I've searched through the R mail archives but couldn't find or
> understand
> > the answer for this problem.
>
> I just forgot to say that you also could try to rename /usr/share/tls
> (IIRC, its tls directory!) to /usr/share/tls_old and re-try loading the
> package.
> Maybe default installation doesn't recognize the working drivers,
> especially if you run NVidia driver on your box!
>
> HtH
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide!
http://www.R-project.org/posting-guide.html
---
Incoming mail is certified Virus Free.



---



From wolski at molgen.mpg.de  Fri Oct  1 09:13:16 2004
From: wolski at molgen.mpg.de (Witold Eryk Wolski)
Date: Fri, 01 Oct 2004 09:13:16 +0200
Subject: [R] Reading multiple files into R
In-Reply-To: <415CE937.2010406@mail.jnu.ac.in>
References: <415CE937.2010406@mail.jnu.ac.in>
Message-ID: <415D038C.9080106@molgen.mpg.de>

Hi!
There is a function ?dir which returns you the content of the dir_ectory.
If this is more then there is a function ?grep which allows you to 
extract relevant items.
If you need to postprocess the names you have a function ?paste for example.
And finally you have an S language construct for(){}
And there is help.search() and An Introduction to R to which tells you 
how to write functions.

/E



Vikas Rawal wrote:

> I want to read data from a number of files into R.
> Reading individual files one by one requires writing enormous amount 
> of code that will look something like the following.
>
> ****************
> maptools:::dbf.read("wb-01vc.dbf")->dist1
> maptools:::dbf.read("wb-02vc.dbf")->dist2
> maptools:::dbf.read("wb-03vc.dbf")->dist3
> maptools:::dbf.read("wb-04vc.dbf")->dist4
> maptools:::dbf.read("wb-05vc.dbf")->dist5
> maptools:::dbf.read("wb-06vc.dbf")->dist6
> maptools:::dbf.read("wb-07vc.dbf")->dist7
> maptools:::dbf.read("wb-08vc.dbf")->dist8
> maptools:::dbf.read("wb-09vc.dbf")->dist9
> *****************
>
> Is there a better way of doing this?
>
> Vikas
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
>


-- 
Dipl. bio-chem. Witold Eryk Wolski         
MPI-Moleculare Genetic
Ihnestrasse 63-73 14195 Berlin           _
tel: 0049-30-83875219                   'v'
http://www.molgen.mpg.de/~wolski       /   \
mail: witek96 at users.sourceforge.net  ---W-W----
      wolski at molgen.mpg.de



From Roger.Bivand at nhh.no  Fri Oct  1 09:17:45 2004
From: Roger.Bivand at nhh.no (Roger Bivand)
Date: Fri, 1 Oct 2004 09:17:45 +0200 (CEST)
Subject: [R] Reading multiple files into R
In-Reply-To: <415CE937.2010406@mail.jnu.ac.in>
Message-ID: <Pine.LNX.4.44.0410010905110.14758-100000@reclus.nhh.no>

On Fri, 1 Oct 2004, Vikas Rawal wrote:

> I want to read data from a number of files into R.
> Reading individual files one by one requires writing enormous amount of 
> code that will look something like the following.
> 
> ****************
> maptools:::dbf.read("wb-01vc.dbf")->dist1
> maptools:::dbf.read("wb-02vc.dbf")->dist2
> maptools:::dbf.read("wb-03vc.dbf")->dist3
> maptools:::dbf.read("wb-04vc.dbf")->dist4
> maptools:::dbf.read("wb-05vc.dbf")->dist5
> maptools:::dbf.read("wb-06vc.dbf")->dist6
> maptools:::dbf.read("wb-07vc.dbf")->dist7
> maptools:::dbf.read("wb-08vc.dbf")->dist8
> maptools:::dbf.read("wb-09vc.dbf")->dist9
> *****************
> 

In this case, you could pre-allocate a list and:

res <- vector(mode="list", length=9)
for (i in 1:length(res)) 
    res[[i]] <- maptools:::dbf.read(paste("wb-0", i, "vc.dbf", sep=""))

> res <- vector(mode="list", length=9)
> for (i in 1:length(res)) cat(paste("wb-0", i, "vc.dbf", sep=""), "\n")
wb-01vc.dbf 
wb-02vc.dbf 
wb-03vc.dbf 
...

gives a check on what file names are being used.

For 10 to 99 preserving the 01-09, use paste("wb-", formatC(i, width=2, 
flag="0"), "vc.dbf", sep="").

If the token is a character (string) that varies, you can roll out a 
character vector of tokens first and step along it.

> res <- vector(mode="list", length=length(LETTERS))
> for (i in 1:length(res)) cat(paste("wb-", LETTERS[i], "vc.dbf", sep=""), 
+ "\n")
wb-Avc.dbf 
wb-Bvc.dbf 
wb-Cvc.dbf 
...


> Is there a better way of doing this?
> 
> Vikas
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
> 

-- 
Roger Bivand
Economic Geography Section, Department of Economics, Norwegian School of
Economics and Business Administration, Breiviksveien 40, N-5045 Bergen,
Norway. voice: +47 55 95 93 55; fax +47 55 95 93 93
e-mail: Roger.Bivand at nhh.no



From josh8912 at yahoo.com  Fri Oct  1 09:21:31 2004
From: josh8912 at yahoo.com (J)
Date: Fri, 1 Oct 2004 00:21:31 -0700 (PDT)
Subject: [R] two questions on nlme:  error messages and nested variance
Message-ID: <20041001072131.46169.qmail@web51703.mail.yahoo.com>

Hello:
Im hoping the list can shed light on two items.  I am
using an nlme model to fit a logistic function and
have these questions:

1)  The model works fine using varIdent(~1|factor) and
also using
varComb(varIdent(~1|factor),varFixed(~covariate)), but
not when varFixed(~covariate) is used alone.  Using
VarFixed alone gives the message: 

Error in recalc.varFunc(object[[i]], conLin) : dim<- :
dims [product 153] do not match the length of object
[846]  In addition: Warning message: longer object
length is not a multiple of shorter object length in:
conLin$Xy * varWeights(object).

Can anyone shed light on what this message means and
how I might go about fixing the problem?  I am still
new to all this, so please explain with that in mind.

2)  In my data, the mean of the variance of the
within-group errors is dependent on a covariate, and
in addition the variance of the variance is also
dependent on a covariate.  To model this would one use
something like: 
varIdent(~varIdent(~1|covariate)|covariate)?  I get
the same error message as above when I try this or if
I try: varIdent(~varFixed(~covariate)|factor).  Is
there a better way to do such nesting?  Is it even
possible?  And again, what does the error mean?

Thanks much in advance.  John



From P.Lemmens at nici.ru.nl  Fri Oct  1 09:31:02 2004
From: P.Lemmens at nici.ru.nl (Paul Lemmens)
Date: Fri, 01 Oct 2004 09:31:02 +0200
Subject: [R] Reading multiple files into R
In-Reply-To: <415CE937.2010406@mail.jnu.ac.in>
References: <415CE937.2010406@mail.jnu.ac.in>
Message-ID: <6773C7A6317EF2DB0192AE4A@lemmens.socsci.kun.nl>

Hoi Vikas,

--On vrijdag 1 oktober 2004 10:50 +0530 Vikas Rawal <vikas at mail.jnu.ac.in> 
wrote:

> I want to read data from a number of files into R.
> Reading individual files one by one requires writing enormous amount of
> code that will look something like the following.
>
> Is there a better way of doing this?
>
These days I'm using the code below to read in each datafile I have, and 
come out with a single dataframe.

#  Concatenate the raw data files.
(datafiles <- list.files(path="../raw data/", pattern="pp.+\.dat$"))
tst <- do.call('rbind', lapply(datafiles, function(x) read.table(
  paste('../raw data/', x, sep=""), skip=1)))
rm(datafiles)




-- 
Paul Lemmens
NICI, University of Nijmegen              ASCII Ribbon Campaign /"\
Montessorilaan 3 (B.01.05)                    Against HTML Mail \ /
NL-6525 HR Nijmegen                                              X
The Netherlands                                                 / \
Phonenumber    +31-24-3612648
Fax            +31-24-3616066



From tom_woody at swissinfo.org  Fri Oct  1 09:44:45 2004
From: tom_woody at swissinfo.org (=?ISO-8859-1?Q?Thomas_Sch=F6nhoff?=)
Date: Fri, 01 Oct 2004 09:44:45 +0200
Subject: [R] Can't load rgl library
In-Reply-To: <IPEFKICOHOECENGJBAGLMELNDHAA.arv@ono.com>
References: <IPEFKICOHOECENGJBAGLMELNDHAA.arv@ono.com>
Message-ID: <415D0AED.9090702@swissinfo.org>

antonio rodriguez schrieb:
> Hi Thomas,
> 
> Yes I have the xlibmesa-gl, libglu1-mesa, libglu1-mesa-dev, etc, libraries.
> But no way. You mentioned TLS (transport Layer Security?), is this OK? I'm
> working on a Toshiba laptop with graphics driver I852GM

No, just look at /usr/share/  to find a diretory named /tls. It seems 
that if you are running NVidia chip with accelerated driver that wrong 
driver is loaded (IIRC, it was libnvidia.so1 or something similar).
Just rename the mentioned directory to tls_old (don't delete!) and try 
again to load your package.

Just an idea!

Thomas



From montpied at nancy.inra.fr  Fri Oct  1 09:48:56 2004
From: montpied at nancy.inra.fr (Pierre MONTPIED)
Date: Fri, 01 Oct 2004 09:48:56 +0200
Subject: [R] gnls or nlme : how to obtain confidence intervals of fitted
	values
Message-ID: <415D0BE8.9050104@nancy.inra.fr>

Hi

I use gnls to fit non linear models of the form y = alpha * x**beta 
(alpha and beta being linear functions of a 2nd regressor z i.e. 
alpha=a1+a2*z and beta=b1+b2*z) with variance function 
varPower(fitted(.)) which sounds correct for the data set I use.

My purpose is to use the fitted models for predictions with other sets 
of regressors x, z than those used in fitting. I therefore need to 
estimate y with (95%) confidence intervals.

Does any body knows how to do this with R ?

Thanks



From kbartz at loyaltymatrix.com  Fri Oct  1 10:04:05 2004
From: kbartz at loyaltymatrix.com (Kevin Bartz)
Date: Fri, 01 Oct 2004 01:04:05 -0700
Subject: [R] Reading multiple files into R
In-Reply-To: <Pine.LNX.4.44.0410010905110.14758-100000@reclus.nhh.no>
References: <Pine.LNX.4.44.0410010905110.14758-100000@reclus.nhh.no>
Message-ID: <415D0F75.7010409@loyaltymatrix.com>

Roger Bivand wrote:
> On Fri, 1 Oct 2004, Vikas Rawal wrote:
> 
> 
>>I want to read data from a number of files into R.
>>Reading individual files one by one requires writing enormous amount of 
>>code that will look something like the following.
>>
>>****************
>>maptools:::dbf.read("wb-01vc.dbf")->dist1
>>maptools:::dbf.read("wb-02vc.dbf")->dist2
>>maptools:::dbf.read("wb-03vc.dbf")->dist3
>>maptools:::dbf.read("wb-04vc.dbf")->dist4
>>maptools:::dbf.read("wb-05vc.dbf")->dist5
>>maptools:::dbf.read("wb-06vc.dbf")->dist6
>>maptools:::dbf.read("wb-07vc.dbf")->dist7
>>maptools:::dbf.read("wb-08vc.dbf")->dist8
>>maptools:::dbf.read("wb-09vc.dbf")->dist9
>>*****************
>>
> 
> 
> In this case, you could pre-allocate a list and:
> 
> res <- vector(mode="list", length=9)
> for (i in 1:length(res)) 
>     res[[i]] <- maptools:::dbf.read(paste("wb-0", i, "vc.dbf", sep=""))
> 
> 
>>res <- vector(mode="list", length=9)
>>for (i in 1:length(res)) cat(paste("wb-0", i, "vc.dbf", sep=""), "\n")
> 
> wb-01vc.dbf 
> wb-02vc.dbf 
> wb-03vc.dbf 
> ...
> 
> gives a check on what file names are being used.
> 
> For 10 to 99 preserving the 01-09, use paste("wb-", formatC(i, width=2, 
> flag="0"), "vc.dbf", sep="").
> 
> If the token is a character (string) that varies, you can roll out a 
> character vector of tokens first and step along it.
> 
> 
>>res <- vector(mode="list", length=length(LETTERS))
>>for (i in 1:length(res)) cat(paste("wb-", LETTERS[i], "vc.dbf", sep=""), 
> 
> + "\n")
> wb-Avc.dbf 
> wb-Bvc.dbf 
> wb-Cvc.dbf 
> ...
> 
> 
> 
>>Is there a better way of doing this?
>>
>>Vikas
>>
>>______________________________________________
>>R-help at stat.math.ethz.ch mailing list
>>https://stat.ethz.ch/mailman/listinfo/r-help
>>PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>>
> 
> 

Good call. Here's a somewhat more R-ified version:

res <- lapply(paste("wb-", formatC(1:99, width=2, flag="0"), "vc.dbf",
                     sep=""), maptools:::dbf.read)

Kevin



From arv at ono.com  Fri Oct  1 10:38:34 2004
From: arv at ono.com (antonio rodriguez)
Date: Fri, 1 Oct 2004 10:38:34 +0200
Subject: [R] Can't load rgl library
In-Reply-To: <415D0AED.9090702@swissinfo.org>
Message-ID: <IPEFKICOHOECENGJBAGLMELPDHAA.arv@ono.com>

Hi Thomas

> antonio rodriguez schrieb:
> > Hi Thomas,
> >
> > Yes I have the xlibmesa-gl, libglu1-mesa, libglu1-mesa-dev,
> etc, libraries.
> > But no way. You mentioned TLS (transport Layer Security?), is
> this OK? I'm
> > working on a Toshiba laptop with graphics driver I852GM
>
> No, just look at /usr/share/  to find a diretory named /tls. It seems
> that if you are running NVidia chip with accelerated driver that wrong
> driver is loaded (IIRC, it was libnvidia.so1 or something similar).
> Just rename the mentioned directory to tls_old (don't delete!) and try
> again to load your package.
>
> Just an idea!

No I don't have neither a tls directory under /usr/share nor a NVidia chip.
The only tls directory is under /lib and it has a lot of libraries I feel
won't do their work if I rename it to tls_old

Thanks again :-)

Antonio
---



From sam.kemp2 at ntlworld.com  Fri Oct  1 11:20:07 2004
From: sam.kemp2 at ntlworld.com (Samuel Kemp)
Date: Fri, 01 Oct 2004 10:20:07 +0100
Subject: [R] Rnewsletter article example
Message-ID: <415D2147.5030307@ntlworld.com>

Hi,

I am trying to write an article for the Rnewsletter, but keep getting 
errors. I have googled around for some decent examples that contain 
figures, maths, etc but with no joy. Would any be so kind as to send me 
an example article in latex code?

Cheers,

Sam.



From wolski at molgen.mpg.de  Fri Oct  1 11:32:29 2004
From: wolski at molgen.mpg.de (Witold Eryk Wolski)
Date: Fri, 01 Oct 2004 11:32:29 +0200
Subject: [R] Rnewsletter article example
In-Reply-To: <415D2147.5030307@ntlworld.com>
References: <415D2147.5030307@ntlworld.com>
Message-ID: <415D242D.3010802@molgen.mpg.de>

me too,

E,

Samuel Kemp wrote:

> Hi,
>
> I am trying to write an article for the Rnewsletter, but keep getting 
> errors. I have googled around for some decent examples that contain 
> figures, maths, etc but with no joy. Would any be so kind as to send 
> me an example article in latex code?
>
> Cheers,
>
> Sam.
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
>


-- 
Dipl. bio-chem. Witold Eryk Wolski         
MPI-Moleculare Genetic
Ihnestrasse 63-73 14195 Berlin           _
tel: 0049-30-83875219                   'v'
http://www.molgen.mpg.de/~wolski       /   \
mail: witek96 at users.sourceforge.net  ---W-W----
      wolski at molgen.mpg.de



From ligges at statistik.uni-dortmund.de  Fri Oct  1 11:55:53 2004
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Fri, 01 Oct 2004 11:55:53 +0200
Subject: [R] Rnewsletter article example
In-Reply-To: <415D2147.5030307@ntlworld.com>
References: <415D2147.5030307@ntlworld.com>
Message-ID: <415D29A9.5060608@statistik.uni-dortmund.de>

Samuel Kemp wrote:

> Hi,
> 
> I am trying to write an article for the Rnewsletter, but keep getting 
> errors. I have googled around for some decent examples that contain 
> figures, maths, etc but with no joy. Would any be so kind as to send me 
> an example article in latex code?
> 
> Cheers,

I'll send one to both of you in a minute.
Also, please read the article in R News 1/1 on how to writes articles.

Uwe Ligges



From rksh at soc.soton.ac.uk  Fri Oct  1 13:24:56 2004
From: rksh at soc.soton.ac.uk (Robin Hankin)
Date: Fri, 1 Oct 2004 12:24:56 +0100
Subject: [R] multiple dimensional  diag()
Message-ID: <a06002000bd82edeb8c91@[139.166.242.29]>

Hi

I have two arbitrarily dimensioned arrays, "a" and "b", with
length(dim(a))==length(dim(b)).  I want to form a sort of
"corner-to-corner" version of abind(), or a multidimensional version
of blockdiag().

In the case of matrices, the function is easy to write and if
a=matrix(1,3,4) and b=matrix(2,2,2), then adiag(a,b) would return:

      [,1] [,2] [,3] [,4] [,5] [,6]
[1,]    1    1    1    1    0    0
[2,]    1    1    1    1    0    0
[3,]    1    1    1    1    0    0
[4,]    0    0    0    0    2    2
[5,]    0    0    0    0    2    2


I am trying to generalize this to two higher dimensional arrays.
If x <- adiag(a,b) then I want all(dim(x)==dim(a)+dim(b)); and if
dim(a)=c(a_1, a_2,...a_d) then x[1:a_1,1:a_2,...,1:a_d]=a, and
x[(a_1+1):(a_1+b_1),...,(a_d+1):(a_d+b_d)]=b.  Other elements of x are
zero.

The fact that I'm having difficulty expressing this succinctly makes
me think I'm missing something basic.

If a and b have identical dimensions [ie all(dim(a)==dim(b)) ], the
following ghastly kludge (which is one of many) works:

adiag <- function(a,b) {
   if(any(dim(a) != dim(b))){stop("a and b must have identical dimensions")}
   jj <- array(0,rep(2,length(dim(a))))
   jj[1] <- 1
   jj[length(jj)] <- 1
   jj <- kronecker(jj,b)
   f <- function(i){1:i}
   do.call("[<-",c(list(jj),sapply(dim(a),f,simplify=FALSE),list(a)))
}

Then "adiag(array(1:8,rep(2,3)),array(-1,rep(2,3)))" is OK.  What is
the best way to bind arbitrarily dimensioned arrays together
corner-to-corner?



-- 
Robin Hankin
Uncertainty Analyst
Southampton Oceanography Centre
SO14 3ZH
tel +44(0)23-8059-7743
initialDOTsurname at soc.soton.ac.uk (edit in obvious way; spam precaution)



From abitbol at sent.com  Fri Oct  1 14:04:46 2004
From: abitbol at sent.com (Jean-Louis Abitbol)
Date: Fri, 01 Oct 2004 14:04:46 +0200
Subject: [R] Background color Windows device (newbie)
Message-ID: <1096632286.2883.205557389@webmail.messagingengine.com>

Dear R Gurus

Just started on R !

Using xYplot from Hmisc (R 1.9, W2K) I get a grey/blue background that I
would like to change to white (ie no background) or may be to another
color.

Tried to do that with par(bg) but only changed the color of the trellis
heading.

What's the right command to do that ?

Kind regards, JL

PS if anyone has nice default settings for win device please let me
know. I used win.slide in Splus but apparently this does not work in R.



From andy_liaw at merck.com  Fri Oct  1 14:13:14 2004
From: andy_liaw at merck.com (Liaw, Andy)
Date: Fri, 1 Oct 2004 08:13:14 -0400
Subject: [R] Background color Windows device (newbie)
Message-ID: <3A822319EB35174CA3714066D590DCD504AF849D@usrymx25.merck.com>

What you should realize is that xYplot() uses lattice, and that color theme
is the default for lattice.  trellis.device() has the `theme' argument that
you can use to change it.  The help page explains how you can change the
default:

   theme: list of components that change the settings of the device
          opened, or, a function that when called produces such a list.
          The function name can be supplied as a quoted string.

          A possible use of this argument is to change the default
          settings at session startup, for example by setting
          'options(lattice.theme = "col.whitebg")'. If 'theme' is a
          function, it will not be supplied any arguments, however, it
          is guaranteed that a device will already be open when it is
          called, so one may use '.Device' inside the function to
          ascertain what device has been opened. 

Andy

> From: Jean-Louis Abitbol
> 
> Dear R Gurus
> 
> Just started on R !
> 
> Using xYplot from Hmisc (R 1.9, W2K) I get a grey/blue 
> background that I
> would like to change to white (ie no background) or may be to another
> color.
> 
> Tried to do that with par(bg) but only changed the color of 
> the trellis
> heading.
> 
> What's the right command to do that ?
> 
> Kind regards, JL
> 
> PS if anyone has nice default settings for win device please let me
> know. I used win.slide in Splus but apparently this does not 
> work in R.
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
> 
>



From ccleland at optonline.net  Fri Oct  1 14:16:14 2004
From: ccleland at optonline.net (Chuck Cleland)
Date: Fri, 01 Oct 2004 08:16:14 -0400
Subject: [R] Background color Windows device (newbie)
In-Reply-To: <1096632286.2883.205557389@webmail.messagingengine.com>
References: <1096632286.2883.205557389@webmail.messagingengine.com>
Message-ID: <415D4A8E.80404@optonline.net>

   See the bg argument to trellis.device().  Here is an example:

library(Hmisc)
library(lattice)
trellis.device(width=7, height=5, new = TRUE, col = FALSE, bg = "white")

dfr <- expand.grid(month=1:12, continent=c('Europe','USA'),
                          sex=c('female','male'))

set.seed(1)

dfr <- upData(dfr, y=month/10 + 1*(sex=='female') +
               2*(continent=='Europe') +
               runif(48,-.15,.15),
               lower=y - runif(48,.05,.15),
               upper=y + runif(48,.05,.15))

xYplot(Cbind(y,lower,upper) ~ month,subset=sex=='male' &
        continent=='USA', data=dfr)

hope this helps,

Chuck Cleland

Jean-Louis Abitbol wrote:
> Dear R Gurus
> 
> Just started on R !
> 
> Using xYplot from Hmisc (R 1.9, W2K) I get a grey/blue background that I
> would like to change to white (ie no background) or may be to another
> color.
> 
> Tried to do that with par(bg) but only changed the color of the trellis
> heading.
> 
> What's the right command to do that ?
> 
> Kind regards, JL
> 
> PS if anyone has nice default settings for win device please let me
> know. I used win.slide in Splus but apparently this does not work in R.
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
> 

-- 
Chuck Cleland, Ph.D.
NDRI, Inc.
71 West 23rd Street, 8th floor
New York, NY 10010
tel: (212) 845-4495 (Tu, Th)
tel: (732) 452-1424 (M, W, F)
fax: (917) 438-0894



From Luisr at frs.fo  Fri Oct  1 14:44:59 2004
From: Luisr at frs.fo (Luis Rideau Cruz)
Date: Fri, 01 Oct 2004 13:44:59 +0100
Subject: [R] Can grid lines color in a plot be specified?
Message-ID: <s15d5f5e.062@ffdata.setur.fo>

R-help

Is there any way to specify the color of grid lines in a simple plot?

par(color.tick.marks=c("grey"))
plot(rnorm(10),tck=1)


Thank you



From ripley at stats.ox.ac.uk  Fri Oct  1 14:44:36 2004
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Fri, 1 Oct 2004 13:44:36 +0100 (BST)
Subject: [R] dev.print and win.print
In-Reply-To: <200409302111.i8ULBp927686@gator.dt.uh.edu>
Message-ID: <Pine.LNX.4.44.0410011337100.15997-100000@gannet.stats>

Just format your plot suitably and select on the Windows driver portrait 
or landscape orientation.

The rotation in the Windows device driver to landscape is exactly what 
postscript does for horizontal = TRUE (the default).  It's just that 
Unix print commands do not have rotation built in (in general), so
postscript() does.

I think a lot of this comes about because by default S-PLUS graphics
devices are landscape format, whereas R ones are square.

On Thu, 30 Sep 2004, Erin Hodgess wrote:

> Was there an answer to the question about
> using dev.print and win.print to print as horizontal = FALSE,
> please?
> 
> I was working on it and I didn't find the solution.
> 
> R 1.9.1 Windows
> Thanks,
> Erin Hodgess
> mailto: hodgess at gator.uhd.edu

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From maechler at stat.math.ethz.ch  Fri Oct  1 14:59:08 2004
From: maechler at stat.math.ethz.ch (Martin Maechler)
Date: Fri, 1 Oct 2004 14:59:08 +0200
Subject: [R] Rpy vs RSPython
In-Reply-To: <20040930164013.GB3606@queenbee.fhcrc.org>
References: <1096552672.8152.1.camel@blue.chem.psu.edu>
	<20040930164013.GB3606@queenbee.fhcrc.org>
Message-ID: <16733.21660.157644.507568@gargle.gargle.HOWL>

>>>>> "Seth" == Seth Falcon <sfalcon at fhcrc.org>
>>>>>     on Thu, 30 Sep 2004 09:40:13 -0700 writes:

    Seth> On Thu, Sep 30, 2004 at 09:57:53AM -0400, Rajarshi Guha wrote:
    >> Rpy seems easier to get up and running with, but does anybody have any
    >> comments regarding which would be a better system to work with in the
    >> long run?

    Seth> I've been using rpy for a number of months and have been very pleased
    Seth> with it.  On the other hand, I haven't tried RSPython so I can't
    Seth> really compare.  

The (other) big difference between the two is that

 RSPython :  Python <-> R  (BI-directional)
 RPy	  :  Python  -> R  (UNI-directional).



From V.Khamenia at biovision-discovery.de  Fri Oct  1 15:13:21 2004
From: V.Khamenia at biovision-discovery.de (Khamenia, Valery)
Date: Fri, 1 Oct 2004 15:13:21 +0200 
Subject: [R] R-2.0: roadmap? release statements? plans?
Message-ID: <D15343265276D31197BC00A024A6C110C793BC@EXS_BDC>

Hi all,

I took a look at last 2 months post in R-help maillist
and surfed through the R-project.org . Unfortunately,
I can't find some page with roadmap/statements about
major changes coming in R-2.0 in comparison to R-1.9

Could anyone point me to the right URL?

Thank you in advance.
--
Valery



From MSchwartz at MedAnalytics.com  Fri Oct  1 15:15:59 2004
From: MSchwartz at MedAnalytics.com (Marc Schwartz)
Date: Fri, 01 Oct 2004 08:15:59 -0500
Subject: [R] Can grid lines color in a plot be specified?
In-Reply-To: <s15d5f5e.062@ffdata.setur.fo>
References: <s15d5f5e.062@ffdata.setur.fo>
Message-ID: <1096636558.12255.20.camel@localhost.localdomain>

On Fri, 2004-10-01 at 07:44, Luis Rideau Cruz wrote:
> R-help
> 
> Is there any way to specify the color of grid lines in a simple plot?
> 
> par(color.tick.marks=c("grey"))
> plot(rnorm(10),tck=1)
> 
> 
> Thank you


This is one approach:

plot(rnorm(10))

# Now draw both axes
axis(1, tck = 1, col = "grey", lty = "dotted")
axis(2, tck = 1, col = "grey", lty = "dotted")

# Replace the grey plot region border lines with black
box()

In this case, the grid lines are being drawn after the data is plotted,
so it is possible that the lines may overwrite your symbols or other
important visual information. An alternative would be to create the plot
background first, including the grid lines, then add the data with
lines() or points() or other functions. For example:

x <- rnorm(10)
plot(x, type = "n")
axis(1, tck = 1, col = "grey", lty = "dotted")
axis(2, tck = 1, col = "grey", lty = "dotted")
box()
points(x, pch = 19)


HTH,

Marc Schwartz



From ripley at stats.ox.ac.uk  Fri Oct  1 15:20:23 2004
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Fri, 1 Oct 2004 14:20:23 +0100 (BST)
Subject: [R] expand.model.frame gives "object not found"
In-Reply-To: <f5d84806040930084170c2e6e4@mail.gmail.com>
Message-ID: <Pine.LNX.4.44.0410011417160.24383-100000@gannet.stats>

Why are you using expand.model.frame to update a *formula*?  That is what 
update()'s formula method does.

It is rather rare to use expand.model.frame() directly.  As ever, we 
recommend reading a good book on R as applied to whatever you are trying 
to do -- several of them have examples of using update().


On Thu, 30 Sep 2004, David Hugh-Jones wrote:

> Hello,
> 
> I am a (relatively) experienced programmer, but new to R.

In short, inexperienced with R or languages like R.

> I have a problem using R 1.9.1. I have fit some data using glm(), from
> within a function:
> 
>         formula = as.formula(paste(depvarname, "~", rhs), env=globalenv())
>         return (glm(formula, family=binomial(link=logit)))
> 
> I have now come back to the formula and want to add some more
> variables to it. So I do:
> 
> expand.model.frame(formulaname, ~ new_variable)
> 
> but I get the response
> 
> Error in eval(expr, envir, enclos) : Object "foreignaid.dummy" not found
> 
> where foreignaid.dummy is my dependent variable. However,
> foreignaid.dummy is clearly visible in the global environment:
> 
> > ls(pat="foreignaid.dummy", envir=globalenv())
>  [1] "foreignaid.dummy"
>  ...
> 
> So why is my dependent variable lost?
> 
> I have read the earlier comments on the same topic, but they seem to
> indicate that a previous bug was fixed. Am I missing the point about
> scoping?
> 
> Any help much appreciated.
> 
> David Hugh-Jones
> Essex University Govt Dept
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
> 
> 

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From MSchwartz at MedAnalytics.com  Fri Oct  1 15:22:38 2004
From: MSchwartz at MedAnalytics.com (Marc Schwartz)
Date: Fri, 01 Oct 2004 08:22:38 -0500
Subject: [R] Can grid lines color in a plot be specified?
In-Reply-To: <1096636558.12255.20.camel@localhost.localdomain>
References: <s15d5f5e.062@ffdata.setur.fo>
	<1096636558.12255.20.camel@localhost.localdomain>
Message-ID: <1096636958.12255.26.camel@localhost.localdomain>

On Fri, 2004-10-01 at 08:15, Marc Schwartz wrote:
> On Fri, 2004-10-01 at 07:44, Luis Rideau Cruz wrote:
> > R-help
> > 
> > Is there any way to specify the color of grid lines in a simple plot?
> > 
> > par(color.tick.marks=c("grey"))
> > plot(rnorm(10),tck=1)
> > 
> > 
> > Thank you


Ok....now that I have finished my second cup of coffee...

For some reason, I keep forgetting about the grid() function:

plot(rnorm(10))
grid()

The same comment applies here with respect to the grid being drawn after
your data, so you might want to do something like:

x <- rnorm(10)
plot(x, type = "n")
grid()
points(x, pch = 19)

See ?grid for more information.

Marc



From ripley at stats.ox.ac.uk  Fri Oct  1 15:24:58 2004
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Fri, 1 Oct 2004 14:24:58 +0100 (BST)
Subject: [R] R-2.0: roadmap? release statements? plans?
In-Reply-To: <D15343265276D31197BC00A024A6C110C793BC@EXS_BDC>
Message-ID: <Pine.LNX.4.44.0410011420570.24383-100000@gannet.stats>

On Fri, 1 Oct 2004, Khamenia, Valery wrote:

> I took a look at last 2 months post in R-help maillist

Well, R-devel is about developments in R, and R-help is about help for end 
users.

> and surfed through the R-project.org . Unfortunately,
> I can't find some page with roadmap/statements about
> major changes coming in R-2.0 in comparison to R-1.9

Note, neither exist: it is 2.0.0 vs 1.9.1.

> Could anyone point me to the right URL?

http://developer.r-project.org

The NEWS file in the alpha/beta releases that have been made available, 
and announced on R-devel, have details of the changes, major and minor.

Or just wait for the release on Monday October 4th.

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From murdoch at stats.uwo.ca  Fri Oct  1 15:41:43 2004
From: murdoch at stats.uwo.ca (Duncan Murdoch)
Date: Fri, 01 Oct 2004 09:41:43 -0400
Subject: [R] dev.print and win.print
In-Reply-To: <200409302111.i8ULBp927686@gator.dt.uh.edu>
References: <200409302111.i8ULBp927686@gator.dt.uh.edu>
Message-ID: <ninql0pfur71k8knhsr9qgh2c89ro3r0j9@4ax.com>

On Thu, 30 Sep 2004 16:11:51 -0500, Erin Hodgess
<hodgess at gator.uhd.edu> wrote :

>Dear R Users:
>
>Was there an answer to the question about
>using dev.print and win.print to print as horizontal = FALSE,
>please?

Check the archives:  That message is here:

<https://stat.ethz.ch/pipermail/r-help/2004-September/056781.html>

and there were two answers given.

Duncan Murdoch



From ripley at stats.ox.ac.uk  Fri Oct  1 15:49:38 2004
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Fri, 1 Oct 2004 14:49:38 +0100 (BST)
Subject: [R] biplot.princomp with loadings only
In-Reply-To: <415BB6BC.6070307@gmx.ch>
Message-ID: <Pine.LNX.4.44.0410011448010.1964-100000@gannet.stats>

Not and have a biplot, as defined by the person who named them.

Have you bothered to read the references on the help page?  Please read 
them (more carefully if necessary).

On Thu, 30 Sep 2004, Christoph Lehmann wrote:

> is there a way to plot only the loadings in a biplot (with the nice 
> arrows), and to skip the scores?

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From gregory_r_warnes at groton.pfizer.com  Fri Oct  1 15:52:05 2004
From: gregory_r_warnes at groton.pfizer.com (Warnes, Gregory R)
Date: Fri, 1 Oct 2004 09:52:05 -0400 
Subject: [R] displaying sample size in boxplots
Message-ID: <D7A3CFD7825BD6119B880002A58F06C20C521D03@groexmb02.pfizer.com>


Also note that boxplot.n in the gplots library (part of the gregmisc bundle)
automatically adds the number of observations.

-Greg


> -----Original Message-----
> From: r-help-bounces at stat.math.ethz.ch
> [mailto:r-help-bounces at stat.math.ethz.ch]On Behalf Of Patrick 
> Drechsler
> Sent: Wednesday, September 29, 2004 1:47 PM
> To: r-help at stat.math.ethz.ch
> Subject: Re: [R] displaying sample size in boxplots
> 
> 
> 
> Martin Maechler wrote on 29 Sep 2004 17:11:13 MET:
> 
> >>>>>> "Roger" == Roger Bivand <Roger.Bivand at nhh.no>
> >>>>>>     on Wed, 29 Sep 2004 15:09:17 +0200 (CEST) writes:
> [snip]
> 
> >     Roger> Perhaps use the names= argument (width can help too):
> >                                             ^^^^^^^^^^^^^^^^^^
> > Indeed!
> > And that's why -- "in the good ol' times" when the box plot 
> was invented
> > and enhanced, the inventors thought about it.
> > For that reason there's the  'varwidth = TRUE/FALSE' argument
> > in boxplot() 
> >
> > Note from help(boxplot) however that the inventors thought
> > it wiser to make the width proportional to the SQRT of the
> > sample size rather than the sample.size itself, i.e.,
> > 'varwidth = TRUE' and your proposal are not equivalent.
> >
> >     >> boxplot(expend~stature, width=sample.size/length(expend), 
> >     >>   + names=paste(levels(stature), ", N=", 
> sample.size, sep=""))
> >
> > Here are the current proposals [for cut & paste]:
> >
> > library(ISwR)
> > data(energy)
> > attach(energy)
> >
> > ## 1
> > boxplot(expend~stature)
> > sample.size <- tapply(expend, stature, length)
> > ss.ch <- paste("N=", sample.size, sep="")
> > mtext(ss.ch, at=1:length(unique(stature)), line=2, side=1)
> >
> > ## 2 (Roger)
> > boxplot(expend~stature, width=sample.size/length(expend),
> >         names=paste(levels(stature), ", N=", sample.size, sep=""))
> >
> > ## 3 (Roger + Martin):
> > boxplot(expend ~ stature, varwidth= TRUE,
> >         names=paste(levels(stature), ", N=", sample.size, sep=""))
> 
> Thanks for the explanation and the nice summary Martin! I can see
> the point you're making about varwidth. I've read that part in
> the documentation before but I have to admit that up to now I
> didn't see the purpose of this parameter. Although there are
> situations were I prefer to see the number in print somewhere on
> the plot which I can now easily accomplish with `names'.
> 
> Also thanks to Stephano for the pointer to the r-newsletter
> article and to Don for showing me how one implements user
> defined functions!
> 
> Cheers
> 
> Patrick
> -- 
> For animals, the entire universe has been neatly divided into things
> to (a) mate with, (b) eat, (c) run away from, and (d) rocks.
>         -- (Terry Pratchett, Equal Rites)
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
> 


LEGAL NOTICE\ Unless expressly stated otherwise, this messag...{{dropped}}



From andy_liaw at merck.com  Fri Oct  1 16:00:05 2004
From: andy_liaw at merck.com (Liaw, Andy)
Date: Fri, 1 Oct 2004 10:00:05 -0400
Subject: [R] R-2.0: roadmap? release statements? plans?
Message-ID: <3A822319EB35174CA3714066D590DCD504AF84A2@usrymx25.merck.com>

Just to keep Valery from thinking that that web page can not be easily
found:  Go to www.r-project.org and click on `Developer Page' under the
heading `R Project'.  

A suggestion (for CRAN masters?):  On CRAN there is a link to the NEWS file
for the released version.  Maybe it's a good idea to provide one for
alpha/beta version (when available) as well?  The Windows-specific NEWS is
provided for the alpha/beta.

Cheers,
Andy

> From: Prof Brian Ripley
> 
> On Fri, 1 Oct 2004, Khamenia, Valery wrote:
> 
> > I took a look at last 2 months post in R-help maillist
> 
> Well, R-devel is about developments in R, and R-help is about 
> help for end 
> users.
> 
> > and surfed through the R-project.org . Unfortunately,
> > I can't find some page with roadmap/statements about
> > major changes coming in R-2.0 in comparison to R-1.9
> 
> Note, neither exist: it is 2.0.0 vs 1.9.1.
> 
> > Could anyone point me to the right URL?
> 
http://developer.r-project.org

The NEWS file in the alpha/beta releases that have been made available, 
and announced on R-devel, have details of the changes, major and minor.

Or just wait for the release on Monday October 4th.

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide!
http://www.R-project.org/posting-guide.html



From bill.shipley at usherbrooke.ca  Fri Oct  1 16:16:33 2004
From: bill.shipley at usherbrooke.ca (Bill Shipley)
Date: Fri, 1 Oct 2004 10:16:33 -0400
Subject: [R] controlling colour in Trellis histogram
Message-ID: <001801c4a7c1$4101a4d0$8c1ad284@BIO041>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20041001/06bbb967/attachment.pl

From bhx2 at mevik.net  Fri Oct  1 16:17:37 2004
From: bhx2 at mevik.net (=?iso-8859-1?q?Bj=F8rn-Helge_Mevik?=)
Date: Fri, 01 Oct 2004 16:17:37 +0200
Subject: [R] R-2.0: roadmap? release statements? plans?
In-Reply-To: <D15343265276D31197BC00A024A6C110C793BC@EXS_BDC> (Valery
	Khamenia's message of "Fri, 1 Oct 2004 15:13:21 +0200")
References: <D15343265276D31197BC00A024A6C110C793BC@EXS_BDC>
Message-ID: <m0ekkizfni.fsf@bar.nemo-project.org>

Well, you could download the latest beta-release and look in the NEWS
file there.

-- 
Bj??rn-Helge Mevik



From ccleland at optonline.net  Fri Oct  1 16:25:38 2004
From: ccleland at optonline.net (Chuck Cleland)
Date: Fri, 01 Oct 2004 10:25:38 -0400
Subject: [R] controlling colour in Trellis histogram
In-Reply-To: <001801c4a7c1$4101a4d0$8c1ad284@BIO041>
References: <001801c4a7c1$4101a4d0$8c1ad284@BIO041>
Message-ID: <415D68E2.807@optonline.net>

   Does this help?

mydata <- data.frame(OPTIMISM = 
c(29,35,26,22,30,37,29,32,25,35,38,33,41,28,40),
                      PARENTS = rep(c("Both Deceased", "One Deceased", 
"Both Alive"), c(4,5,6)))

library(lattice)

trellis.device(width=7, height=5, new = FALSE, col = FALSE, bg = "white")

histogram(~ OPTIMISM | PARENTS, data=mydata)

Bill Shipley wrote:
> Hello.  I am sorry for posting a (seemingly) simple question, but I have
> just spent 2 hours trying to find the answer, without success.  I want
> to make a histogram with conditioning on a factor, using Trellis
> graphics.  However, I do not want any colours (only black and white)
> either in the histograms or in the strip.  There must be some simple
> argument but I can??t find it.  Here is my code so far:
> 
>>histogram(~GRC.SLA|as.factor(species.type),data=group.means,
> 
> + strip=function(...)
> 
> +
> strip.default(...,style=1,factor.levels=c("Conifers","Trees","Herbs","Mo
> nocots")))
> 
> As a default, this produces the bars in a pale blue and the strip in
> orange-yellow.

-- 
Chuck Cleland, Ph.D.
NDRI, Inc.
71 West 23rd Street, 8th floor
New York, NY 10010
tel: (212) 845-4495 (Tu, Th)
tel: (732) 452-1424 (M, W, F)
fax: (917) 438-0894



From davidr at rhotrading.com  Fri Oct  1 16:31:20 2004
From: davidr at rhotrading.com (davidr@rhotrading.com)
Date: Fri, 1 Oct 2004 09:31:20 -0500
Subject: [R] Can grid lines color in a plot be specified?
Message-ID: <12AE52872B5C5348BE5CF47C707FF53A32723A@rhosvr02.rhotrading.com>

I usually use something like

abline(h=seq(...),col="green")
abline(v=seq(...),col="green")

This allows you to have irregularly spaced grid lines if you want. (Say
for futures expiration dates in my case.)

Also, as Marc pointed out, you may want to draw the lines or points
after the grid lines.

HTH,
David L. Reiner
Rho Trading
440 S LaSalle Suite 620
Chicago  IL  60605
312-362-4963

-----Original Message-----
From: Marc Schwartz [mailto:MSchwartz at medanalytics.com] 
Sent: Friday, October 01, 2004 8:16 AM
To: Luis Rideau Cruz
Cc: R-Help
Subject: Re: [R] Can grid lines color in a plot be specified?

On Fri, 2004-10-01 at 07:44, Luis Rideau Cruz wrote:
> R-help
> 
> Is there any way to specify the color of grid lines in a simple plot?
> 
> par(color.tick.marks=c("grey"))
> plot(rnorm(10),tck=1)
> 
> 
> Thank you


This is one approach:

plot(rnorm(10))

# Now draw both axes
axis(1, tck = 1, col = "grey", lty = "dotted")
axis(2, tck = 1, col = "grey", lty = "dotted")

# Replace the grey plot region border lines with black
box()

In this case, the grid lines are being drawn after the data is plotted,
so it is possible that the lines may overwrite your symbols or other
important visual information. An alternative would be to create the plot
background first, including the grid lines, then add the data with
lines() or points() or other functions. For example:

x <- rnorm(10)
plot(x, type = "n")
axis(1, tck = 1, col = "grey", lty = "dotted")
axis(2, tck = 1, col = "grey", lty = "dotted")
box()
points(x, pch = 19)


HTH,

Marc Schwartz

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide!
http://www.R-project.org/posting-guide.html



From deepayan at stat.wisc.edu  Fri Oct  1 16:39:45 2004
From: deepayan at stat.wisc.edu (Deepayan Sarkar)
Date: Fri, 1 Oct 2004 09:39:45 -0500
Subject: [R] controlling colour in Trellis histogram
In-Reply-To: <001801c4a7c1$4101a4d0$8c1ad284@BIO041>
References: <001801c4a7c1$4101a4d0$8c1ad284@BIO041>
Message-ID: <200410010939.45335.deepayan@stat.wisc.edu>

On Friday 01 October 2004 09:16, Bill Shipley wrote:
> Hello.  I am sorry for posting a (seemingly) simple question, but I
> have just spent 2 hours trying to find the answer, without success. 
> I want to make a histogram with conditioning on a factor, using
> Trellis graphics.  However, I do not want any colours (only black and
> white) either in the histograms or in the strip.  There must be some
> simple

trellis.device(color = FALSE) 
## assuming you want the default device. 
## See ?trellis.device for more options

histogram(<...>)


Deepayan



From gregory_r_warnes at groton.pfizer.com  Fri Oct  1 16:19:47 2004
From: gregory_r_warnes at groton.pfizer.com (Warnes, Gregory R)
Date: Fri, 1 Oct 2004 10:19:47 -0400 
Subject: [R] [R-pkgs] gregmisc 2.0.0 release
Message-ID: <D7A3CFD7825BD6119B880002A58F06C20C521D04@groexmb02.pfizer.com>


gregmisc 2.0.0
===========

gregmisc 2.0.0 has been released and is now available on CRAN.   

Important Changes
---------------------------

- Now a package bundle 

For this release, gregmisc has been split into a bundle containing 4
separate packages: gdata, gmodels, gplots and gtools. All of your favorite
functions are still present, but they are now better organized into thematic
groups, which should make them easier to maintain.

- Namespaces

Each of the gregmisc packages now provide namespaces, and are fully
compatible with R 2.0.0.   The namespaces make it easier to avoid conflicts
with functions in other packages, and prevent local utility functions from
cluttering up the global function namespace.

Consequences for Users
------------------------------------

Instead of attaching the gregmisc package, you will need to attach the
appropriate individual packages.  IE, instead of
	> library(gregmisc)
you should use one or more of
	> library(gdata)
	> library(gmodels)
	> library(gplots)
	> library(gtools)

For more details, including bug fixes, see the ChangeLog file in the source
distribution.

-Greg 

Gregory R. Warnes
Manager, Non-Clinical Statistics
Pfizer Global Research and Development



LEGAL NOTICE\ Unless expressly stated otherwise, this messag...{{dropped}}

_______________________________________________
R-packages mailing list
R-packages at stat.math.ethz.ch
https://stat.ethz.ch/mailman/listinfo/r-packages



From gregory_r_warnes at groton.pfizer.com  Fri Oct  1 16:20:58 2004
From: gregory_r_warnes at groton.pfizer.com (Warnes, Gregory R)
Date: Fri, 1 Oct 2004 10:20:58 -0400 
Subject: [R] [R-pkgs] genetics 1.1.0 released
Message-ID: <D7A3CFD7825BD6119B880002A58F06C20C521D05@groexmb02.pfizer.com>


Version 1.1.0 of the genetics package is now available on CRAN.  This
release adds namespace support, and several minor bug fixes.   

-Greg

Gregory R. Warnes
Manager, Non-Clinical Statistics
Pfizer Global Research and Development



LEGAL NOTICE\ Unless expressly stated otherwise, this messag...{{dropped}}

_______________________________________________
R-packages mailing list
R-packages at stat.math.ethz.ch
https://stat.ethz.ch/mailman/listinfo/r-packages



From jfox at mcmaster.ca  Fri Oct  1 16:47:07 2004
From: jfox at mcmaster.ca (John Fox)
Date: Fri, 1 Oct 2004 10:47:07 -0400
Subject: [R] controlling colour in Trellis histogram
In-Reply-To: <001801c4a7c1$4101a4d0$8c1ad284@BIO041>
Message-ID: <20041001144703.IPBP4905.tomts13-srv.bellnexxia.net@JohnDesktop8300>

Dear bill,

Try opening the Trellis graphics device with trellis.device(color=FALSE).

I hope this helps,
 John

> -----Original Message-----
> From: r-help-bounces at stat.math.ethz.ch 
> [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Bill Shipley
> Sent: Friday, October 01, 2004 9:17 AM
> To: R help list
> Subject: [R] controlling colour in Trellis histogram
> 
> Hello.  I am sorry for posting a (seemingly) simple question, 
> but I have just spent 2 hours trying to find the answer, 
> without success.  I want to make a histogram with 
> conditioning on a factor, using Trellis graphics.  However, I 
> do not want any colours (only black and white) either in the 
> histograms or in the strip.  There must be some simple 
> argument but I can??t find it.  Here is my code so far:
> 
>  
> 
> > histogram(~GRC.SLA|as.factor(species.type),data=group.means,
> 
> + strip=function(...)
> 
> +
> strip.default(...,style=1,factor.levels=c("Conifers","Trees","
Herbs","Mo
> nocots")))
> 
>  
> 
> As a default, this produces the bars in a pale blue and the 
> strip in orange-yellow.
> 
>  
> 
> Thanks.
> 
>  
> 
> Bill Shipley
> 
> Subject Matter Editor, Ecology
> 
> North American Editor, Annals of Botany
> 
> D??partement de biologie, Universit?? de Sherbrooke,
> 
> Sherbrooke (Qu??bec) J1K 2R1 CANADA
> 
> Bill.Shipley at USherbrooke.ca
> 
>  <http://callisto.si.usherb.ca:8080/bshipley/>
> http://callisto.si.usherb.ca:8080/bshipley/
> 
>  
> 
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html



From deepayan at stat.wisc.edu  Fri Oct  1 16:58:56 2004
From: deepayan at stat.wisc.edu (Deepayan Sarkar)
Date: Fri, 1 Oct 2004 09:58:56 -0500
Subject: [R] controlling colour in Trellis histogram
In-Reply-To: <415D68E2.807@optonline.net>
References: <001801c4a7c1$4101a4d0$8c1ad284@BIO041>
	<415D68E2.807@optonline.net>
Message-ID: <200410010958.56096.deepayan@stat.wisc.edu>

On Friday 01 October 2004 09:25, Chuck Cleland wrote:
>    Does this help?
>
> mydata <- data.frame(OPTIMISM =
> c(29,35,26,22,30,37,29,32,25,35,38,33,41,28,40),
>                       PARENTS = rep(c("Both Deceased", "One
> Deceased", "Both Alive"), c(4,5,6)))
>
> library(lattice)
>
> trellis.device(width=7, height=5, new = FALSE, col = FALSE, bg =
> "white")

The bg="white" is redundant if color=FALSE. 

Incidentally, if color=TRUE, setting bg="white" is IMO bad practice 
(although it seemed like a good idea at the time), since it creates 
settings with a white background while the other colors are only 
suitable for a dark background. To make it more difficult for users to 
achieve this, the 'bg' argument has been dropped from trellis.device in 
2.0.0.

Deepayan



From friendly at yorku.ca  Fri Oct  1 17:09:45 2004
From: friendly at yorku.ca (Michael Friendly)
Date: Fri, 01 Oct 2004 11:09:45 -0400
Subject: [R] Lattice .ps graphic is rotated in LaTeX slides
Message-ID: <415D7339.8@yorku.ca>

I've generated a version of the classic dotplot of the barley data with

library(lattice)
data(barley)

trellis.device("postscript", color=TRUE, file="barley2x3.ps")
old.settings <- trellis.par.get()
trellis.par.set("background", list(col = "white"))
lset(list(superpose.symbol=list(pch=c(19, 1, 25, 2, 15, 22, 23),
       cex=rep(1,7),col=c("blue", "red", "darkgreen", "brown",
       "orange", "turquoise", "orchid") )))
lset(list(fontsize = list(default = 14)))

n <- length(levels(barley$year))
dotplot(variety ~ yield | site, data = barley, groups = year,
 layout = c(2, 3), aspect = .5,
 xlab = "Barley Yield (bushels/acre)",
 key = list(points = Rows(trellis.par.get("superpose.symbol"), 1:n),
   text = list(levels(barley$year)), columns = n))
dev.off()
lset(theme=old.settings)

It looks fine with gv (though I'd like to make the bounding box 
tighter), but when I embed it in a LaTeX slide
(landscape, using seminar package),

\begin{slide}
 \includegraphics[,height=.6\textheight]{fig/barley2x3.ps}
\end{slide}

the image is rotated 90 deg CCW.  I tried to adjust for this with

 \includegraphics[angle=-90,height=.6\textheight]{fig/barley2x3.ps}

but  that gives
! Package graphics Error: Division by 0.

What am I doing wrong, or how could I do it differently so it would work?

thanks

-- 
Michael Friendly     Email: friendly at yorku.ca 
Professor, Psychology Dept.
York University      Voice: 416 736-5115 x66249 Fax: 416 736-5814
4700 Keele Street    http://www.math.yorku.ca/SCS/friendly.html
Toronto, ONT  M3J 1P3 CANADA



From MSchwartz at MedAnalytics.com  Fri Oct  1 17:14:55 2004
From: MSchwartz at MedAnalytics.com (Marc Schwartz)
Date: Fri, 01 Oct 2004 10:14:55 -0500
Subject: [R] Can grid lines color in a plot be specified?
In-Reply-To: <12AE52872B5C5348BE5CF47C707FF53A32723A@rhosvr02.rhotrading.com>
References: <12AE52872B5C5348BE5CF47C707FF53A32723A@rhosvr02.rhotrading.com>
Message-ID: <1096643695.12255.94.camel@localhost.localdomain>

On Fri, 2004-10-01 at 09:31, davidr at rhotrading.com wrote:
> I usually use something like
> 
> abline(h=seq(...),col="green")
> abline(v=seq(...),col="green")
> 
> This allows you to have irregularly spaced grid lines if you want. (Say
> for futures expiration dates in my case.)
> 
> Also, as Marc pointed out, you may want to draw the lines or points
> after the grid lines.

<snip>

Also, again with the aid of further caffeine, as noted in the help for
grid(), one can use the 'panel.first' argument in plot() to enable the
creation of the grid prior to the plotting of the data. 

For example:

plot(rnorm(10), panel.first = grid(), pch = 19)

yields the same results and plotting sequence as:

x <- rnorm(10)
plot(x, type = "n")
grid()
points(x, pch = 19)

As an example of using this approach with irregularly space grid lines
and axis tick marks, as per David's point:

plot(rnorm(10), panel.first = abline(v = c(1, 2, 3, 7), col = "grey", 
     lty = "dotted"), 
     pch = 19, xaxt = "n")

axis(1, at = c(1, 2, 3, 7))


There is also a 'panel.last' argument available which, of course, is
executed after all other plotting is done. More information is available
from ?plot.default.

I have not done an exhaustive review of all plotting functions/methods,
but I suspect not all of them support the panel.first and panel.last
arguments.

HTH,

Marc



From p.dalgaard at biostat.ku.dk  Fri Oct  1 17:14:53 2004
From: p.dalgaard at biostat.ku.dk (Peter Dalgaard)
Date: 01 Oct 2004 17:14:53 +0200
Subject: [R] R-2.0: roadmap? release statements? plans?
In-Reply-To: <D15343265276D31197BC00A024A6C110C793BC@EXS_BDC>
References: <D15343265276D31197BC00A024A6C110C793BC@EXS_BDC>
Message-ID: <x2ekkipj0y.fsf@biostat.ku.dk>

"Khamenia, Valery" <V.Khamenia at biovision-discovery.de> writes:

> Hi all,
> 
> I took a look at last 2 months post in R-help maillist
> and surfed through the R-project.org . Unfortunately,
> I can't find some page with roadmap/statements about
> major changes coming in R-2.0 in comparison to R-1.9
> 
> Could anyone point me to the right URL?

You actually got one from me the first time you asked....!

https://svn.r-project.org/R/trunk/NEWS

-- 
   O__  ---- Peter Dalgaard             Blegdamsvej 3  
  c/ /'_ --- Dept. of Biostatistics     2200 Cph. N   
 (*) \(*) -- University of Copenhagen   Denmark      Ph: (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)             FAX: (+45) 35327907



From machud at intellektik.informatik.tu-darmstadt.de  Fri Oct  1 17:02:56 2004
From: machud at intellektik.informatik.tu-darmstadt.de (Marco Chiarandini)
Date: Fri, 1 Oct 2004 17:02:56 +0200 (CEST)
Subject: [R] Problems in installing Rmpi library
In-Reply-To: <200409301014.i8UA4iXc000670@hypatia.math.ethz.ch>
References: <200409301014.i8UA4iXc000670@hypatia.math.ethz.ch>
Message-ID: <Pine.LNX.4.58.0410011657200.27828@kika.intellektik.informatik.tu-darmstadt.de>

dear all,

I am trying to install the Rmpi library on a cluster but I obtain the
following error, which concerns the dynamic libraries:

        Rmpi version: 0.4-8
        Rmpi is an interface (wrapper) to MPI APIs
        with interactive R slave functionalities.
        See `library (help=Rmpi)' for details.
Error in dyn.load(x, as.logical(local), as.logical(now)) :
        unable to load shared library
        "/mypath/.R/library/Rmpi/libs/Rmpi.so":
	/mypath/.R/library/Rmpi/libs/Rmpi.so: undefined
        symbol: MPI_Finalize
Error in library(Rmpi) : .First.lib failed
Error in dyn.unload(x) : dynamic/shared library
        "/mypath/.R/library/Rmpi/libs/Rmpi.so" was
        not loaded

Is there anybody who can help me in finding a way to trun around the
problem.


Thank you for the help,


Marco


--
Marco Chiarandini, Fachgebiet Intellektik, Fachbereich Informatik,
Technische Universit??t Darmstadt, Hochschulstra??e 10,
D-64289 Darmstadt - Germany, Office: S2/02 Raum E317
Tel: +49 (0)6151 16-6802 Fax: +49 (0)6151 16-5326
email: machud at intellektik.informatik.tu-darmstadt.de
web page: http://www.intellektik.informatik.tu-darmstadt.de/~machud



From s-plus at wiwi.uni-bielefeld.de  Fri Oct  1 17:27:40 2004
From: s-plus at wiwi.uni-bielefeld.de (Peter Wolf)
Date: Fri, 01 Oct 2004 17:27:40 +0200
Subject: [R] multiple dimensional  diag()
References: <a06002000bd82edeb8c91@[139.166.242.29]>
Message-ID: <415D776C.8060905@wiwi.uni-bielefeld.de>

Here is a function that does what you want (perhaps):

<<*>>=
a.block.diag <- function(a,b) {
  # a.block.daig combines arrays a and b and builds a new array which has
  # a and b as blocks on its diagonal. pw 10/2004
  if(length(dim.a<-dim(a))!= length(dim.b<-dim(b))){
    stop("a and b must have identical number of dimensions")}
  s<-array(0,dim.a+dim.b)
  s<-do.call("[<-",c(list(s),lapply(dim.a,seq),list(a)))
  ind<-lapply(seq(dim.b),function(i)seq(dim.b[[i]])+dim.a[[i]])
  do.call("[<-",c(list(s),ind,list(b)))
}

@
Try:
<<*>>=
a=matrix(1,3,4); b=matrix(2,2,2)
a.block.diag(a,b)

@
output-start
Fri Oct  1 17:20:26 2004
     [,1] [,2] [,3] [,4] [,5] [,6]
[1,]    1    1    1    1    0    0
[2,]    1    1    1    1    0    0
[3,]    1    1    1    1    0    0
[4,]    0    0    0    0    2    2
[5,]    0    0    0    0    2    2
output-end

and an another example:

<<*>>=
xx<-array(1:8,c(2,rep(2,2))); yy<-array(-1*(1:9),c(2,rep(3,2)))
a.block.diag(xx,yy)

@
output-start
Fri Oct  1 17:21:38 2004
, , 1
     [,1] [,2] [,3] [,4] [,5]
[1,]    1    3    0    0    0
[2,]    2    4    0    0    0
[3,]    0    0    0    0    0
[4,]    0    0    0    0    0
, , 2
     [,1] [,2] [,3] [,4] [,5]
[1,]    5    7    0    0    0
[2,]    6    8    0    0    0
[3,]    0    0    0    0    0
[4,]    0    0    0    0    0
, , 3
     [,1] [,2] [,3] [,4] [,5]
[1,]    0    0    0    0    0
[2,]    0    0    0    0    0
[3,]    0    0   -1   -3   -5
[4,]    0    0   -2   -4   -6
, , 4
     [,1] [,2] [,3] [,4] [,5]
[1,]    0    0    0    0    0
[2,]    0    0    0    0    0
[3,]    0    0   -7   -9   -2
[4,]    0    0   -8   -1   -3
, , 5
     [,1] [,2] [,3] [,4] [,5]
[1,]    0    0    0    0    0
[2,]    0    0    0    0    0
[3,]    0    0   -4   -6   -8
[4,]    0    0   -5   -7   -9
output-end

a.block.diag is not always the best solution. In case of
x<-array(1:8,rep(2,3)); y<-array(-1,rep(2,3))
the function adiag will be a little bit faster.

Peter Wolf



Robin Hankin wrote:

> Hi
>
> I have two arbitrarily dimensioned arrays, "a" and "b", with
> length(dim(a))==length(dim(b)).  I want to form a sort of
> "corner-to-corner" version of abind(), or a multidimensional version
> of blockdiag().
>
> In the case of matrices, the function is easy to write and if
> a=matrix(1,3,4) and b=matrix(2,2,2), then adiag(a,b) would return:
>
>      [,1] [,2] [,3] [,4] [,5] [,6]
> [1,]    1    1    1    1    0    0
> [2,]    1    1    1    1    0    0
> [3,]    1    1    1    1    0    0
> [4,]    0    0    0    0    2    2
> [5,]    0    0    0    0    2    2
>
>
> I am trying to generalize this to two higher dimensional arrays.
> If x <- adiag(a,b) then I want all(dim(x)==dim(a)+dim(b)); and if
> dim(a)=c(a_1, a_2,...a_d) then x[1:a_1,1:a_2,...,1:a_d]=a, and
> x[(a_1+1):(a_1+b_1),...,(a_d+1):(a_d+b_d)]=b.  Other elements of x are
> zero.
>
> The fact that I'm having difficulty expressing this succinctly makes
> me think I'm missing something basic.
>
> If a and b have identical dimensions [ie all(dim(a)==dim(b)) ], the
> following ghastly kludge (which is one of many) works:
>
> adiag <- function(a,b) {
>   if(any(dim(a) != dim(b))){stop("a and b must have identical 
> dimensions")}
>   jj <- array(0,rep(2,length(dim(a))))
>   jj[1] <- 1
>   jj[length(jj)] <- 1
>   jj <- kronecker(jj,b)
>   f <- function(i){1:i}
>   do.call("[<-",c(list(jj),sapply(dim(a),f,simplify=FALSE),list(a)))
> }
>
> Then "adiag(array(1:8,rep(2,3)),array(-1,rep(2,3)))" is OK.  What is
> the best way to bind arbitrarily dimensioned arrays together
> corner-to-corner?
>
>
>



From p.dalgaard at biostat.ku.dk  Fri Oct  1 17:58:09 2004
From: p.dalgaard at biostat.ku.dk (Peter Dalgaard)
Date: 01 Oct 2004 17:58:09 +0200
Subject: [R] Lattice .ps graphic is rotated in LaTeX slides
In-Reply-To: <415D7339.8@yorku.ca>
References: <415D7339.8@yorku.ca>
Message-ID: <x2acv6ph0u.fsf@biostat.ku.dk>

Michael Friendly <friendly at yorku.ca> writes:

> ! Package graphics Error: Division by 0.
> 
> What am I doing wrong, or how could I do it differently so it would work?

You might try \usepackage{graphicx} instead. I seem to recall
(vaguely) getting better results with that sometimes.

        -p

-- 
   O__  ---- Peter Dalgaard             Blegdamsvej 3  
  c/ /'_ --- Dept. of Biostatistics     2200 Cph. N   
 (*) \(*) -- University of Copenhagen   Denmark      Ph: (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)             FAX: (+45) 35327907



From nair at sdsc.edu  Fri Oct  1 18:07:59 2004
From: nair at sdsc.edu (T. Murlidharan Nair)
Date: Fri, 01 Oct 2004 09:07:59 -0700
Subject: [R] hier.part
Message-ID: <415D80DF.5010803@sdsc.edu>

I am using the hier.part package for calculating the goodness of fit. 
 Before I started with
my data I tried to use the example that the package comes with . I have 
loaded the library
hier.part and gtools. But the package gives the following error

Loading required package: gregmisc
Loading required package: gregmisc
Warning messages:
1: There is no package called 'gregmisc' in: library(package, 
character.only = TRUE, logical = TRUE, warn.conflicts = warn.conflicts, 
2: There is no package called 'gregmisc' in: library(package, 
character.only = TRUE, logical = TRUE, warn.conflicts = warn.conflicts, 

I searched for gregmisc package but it unzipped to gtools. Is there 
something other package with this name or has it been merged
with another one ?

Thanks../ Murli



From MSchwartz at MedAnalytics.com  Fri Oct  1 18:12:38 2004
From: MSchwartz at MedAnalytics.com (Marc Schwartz)
Date: Fri, 01 Oct 2004 11:12:38 -0500
Subject: [R] Lattice .ps graphic is rotated in LaTeX slides
In-Reply-To: <415D7339.8@yorku.ca>
References: <415D7339.8@yorku.ca>
Message-ID: <1096647158.12255.109.camel@localhost.localdomain>

On Fri, 2004-10-01 at 10:09, Michael Friendly wrote:
> I've generated a version of the classic dotplot of the barley data with
> 
> library(lattice)
> data(barley)
> 
> trellis.device("postscript", color=TRUE, file="barley2x3.ps")
> old.settings <- trellis.par.get()
> trellis.par.set("background", list(col = "white"))
> lset(list(superpose.symbol=list(pch=c(19, 1, 25, 2, 15, 22, 23),
>        cex=rep(1,7),col=c("blue", "red", "darkgreen", "brown",
>        "orange", "turquoise", "orchid") )))
> lset(list(fontsize = list(default = 14)))
> 
> n <- length(levels(barley$year))
> dotplot(variety ~ yield | site, data = barley, groups = year,
>  layout = c(2, 3), aspect = .5,
>  xlab = "Barley Yield (bushels/acre)",
>  key = list(points = Rows(trellis.par.get("superpose.symbol"), 1:n),
>    text = list(levels(barley$year)), columns = n))
> dev.off()
> lset(theme=old.settings)
> 
> It looks fine with gv (though I'd like to make the bounding box 
> tighter), but when I embed it in a LaTeX slide
> (landscape, using seminar package),
> 
> \begin{slide}
>  \includegraphics[,height=.6\textheight]{fig/barley2x3.ps}
> \end{slide}
> 
> the image is rotated 90 deg CCW.  I tried to adjust for this with
> 
>  \includegraphics[angle=-90,height=.6\textheight]{fig/barley2x3.ps}
> 
> but  that gives
> ! Package graphics Error: Division by 0.
> 
> What am I doing wrong, or how could I do it differently so it would work?
> 
> thanks


Michael,

Try the following when specifying the trellis.device:

trellis.device("postscript", color = TRUE, file = "barley2x3.eps",
                onefile = FALSE, paper = "special", horizontal = FALSE,
                width = 9, height = 6)

See if that works without specifying the angle in your LaTeX for
seminar:

\begin{slide}
  \begin{center}
    \includegraphics[height=.6\textheight]{fig/barley2x3.eps}
  \end{center}
\end{slide}

Note that when including graphics in LaTeX, you should use EPS files,
which (as noted in ?postscript) require certain device settings to
create. These include:

onefile = FALSE, paper = "special", horizontal = FALSE

and the device 'width' and 'height' settings.

This will also adjust the size of the bounding box.

HTH,

Marc Schwartz



From MSchwartz at MedAnalytics.com  Fri Oct  1 18:25:07 2004
From: MSchwartz at MedAnalytics.com (Marc Schwartz)
Date: Fri, 01 Oct 2004 11:25:07 -0500
Subject: [R] Lattice .ps graphic is rotated in LaTeX slides
In-Reply-To: <x2acv6ph0u.fsf@biostat.ku.dk>
References: <415D7339.8@yorku.ca>  <x2acv6ph0u.fsf@biostat.ku.dk>
Message-ID: <1096647907.12255.120.camel@localhost.localdomain>

On Fri, 2004-10-01 at 10:58, Peter Dalgaard wrote:
> Michael Friendly <friendly at yorku.ca> writes:
> 
> > ! Package graphics Error: Division by 0.
> > 
> > What am I doing wrong, or how could I do it differently so it would work?
> 
> You might try \usepackage{graphicx} instead. I seem to recall
> (vaguely) getting better results with that sometimes.


That should be part of the preamble for using 'seminar', if it is setup
properly.

There is a decent tutorial for using seminar at:

http://astronomy.sussex.ac.uk/~eddie/soft/tutorial.html

There is also a great reference for including graphics in LaTeX:

www.ctan.org/tex-archive/info/epslatex.pdf

FWIW, though I have been using seminar for such presentations, I have
been recently looking at the Beamer package:

http://latex-beamer.sourceforge.net/

and of course, there is also the Prosper package:

http://prosper.sourceforge.net/

The one advantage of the Beamer package, for those that require it, is
that it supports pdflatex, which the others do not. Though, it can be
used with dvips/latex + ps2pdf, where needed.

HTH,

Marc



From B.Rowlingson at lancaster.ac.uk  Fri Oct  1 18:49:50 2004
From: B.Rowlingson at lancaster.ac.uk (Barry Rowlingson)
Date: Fri, 01 Oct 2004 17:49:50 +0100
Subject: [R] Lattice .ps graphic is rotated in LaTeX slides - PDF weirdness
In-Reply-To: <1096647907.12255.120.camel@localhost.localdomain>
References: <415D7339.8@yorku.ca> <x2acv6ph0u.fsf@biostat.ku.dk>
	<1096647907.12255.120.camel@localhost.localdomain>
Message-ID: <415D8AAE.306@lancaster.ac.uk>

Marc Schwartz wrote:

> The one advantage of the Beamer package, for those that require it, is
> that it supports pdflatex, which the others do not. Though, it can be
> used with dvips/latex + ps2pdf, where needed.
> 

  Has anyone else hit the problem that sometimes occurs with embedded 
PostScript graphics generated by R when viewed in full-screen mode using 
Adobe Acrobat Reader in Linux? Yes, its _that_ specific.

  You get black areas all round your graphic. It looks a mess. Does it 
in acroread4 and 5. But only Linux, and only full-screen mode.

  I took some time looking at this and found it happened only when the 
EPS graphic had a clipping path set in some way. But I also stumbled 
upon a weird solution:

"Start 'acroread' on your pdf file, then do the following. From the 
menus, go to 'Edit...' then 'Preferences...' and 'Full Screen'. Change 
the default transition to 'No Transition'. Even if it already is on 'No 
Transition', in which case change it to something else and then back 
again. Now OK that dialog and get back to the main window. Go to 
full-screen mode (hint: just hit Ctrl-L for full screen mode). All 
should be well. You may wish to do this before you commence your talk."

  I don't know why this works, but it does. If this has bitten you, and 
you find xpdf not as good as acroread, then it might help.

  Note that I dont think there's anything wrong with the EPS coming out 
of R, it just manifests itself with that EPS. R could possibly work 
around it but that would probably be a huge waste of time.

Barry



From patrick.drechsler at gmx.net  Fri Oct  1 18:52:26 2004
From: patrick.drechsler at gmx.net (Patrick Drechsler)
Date: Fri, 01 Oct 2004 18:52:26 +0200
Subject: [R] Lattice .ps graphic is rotated in LaTeX slides
References: <415D7339.8@yorku.ca>
Message-ID: <m3acv6peid.fsf@pdrechsler.fqdn.th-h.de>

Hi Michael,

Michael Friendly wrote on 01 Oct 2004 16:09:45 MET:

> I've generated a version of the classic dotplot of the barley
> data with

[sniped R-code]

> It looks fine with gv

The image is in landscape format when viewed with gv.

> \begin{slide}
> \includegraphics[,height=.6\textheight]{fig/barley2x3.ps}
                  ^^^typo or copy&paste error?
> \end{slide}
>
> the image is rotated 90 deg CCW.  I tried to adjust for this
> with
>
> \includegraphics[angle=-90,height=.6\textheight]{fig/barley2x3.ps}

Short answer: use `width' instead of `height':

  \includegraphics[angle=-90,width=.8\linewidth]{fig/barley2x3}

Long answer:

,----[ http://www.tex.ac.uk/cgi-bin/texfaq2html?label=divzero ]
| Graphics division by zero
| 
| While the error
| 
| ! Package graphics Error: Division by 0.
| 
| can actually be caused by offering the package a figure which
| claims to have a zero dimension, it's more commonly caused by
| rotation.
| 
| Objects in TeX may have both height (the height above the
| baseline) and depth (the distance the object goes below the
| baseline). If you rotate an object by 180 degrees, you convert
| its height into depth, and vice versa; if the object started
| with zero depth, you've converted it to a zero-height object.
| 
| Suppose you're including your graphic with a command like:
| 
| \includegraphics[angle=180,height=5cm]{myfig.eps}
| 
| In the case that myfig.eps has no depth to start with, the
| scaling calculations will produce the division-by-zero error.
| 
| Fortunately, the graphicx package has a keyword totalheight,
| which allows you to specify the size of the image relative to
| the sum of the object's height and depth, so
| 
| \includegraphics[angle=180,totalheight=5cm]{myfig.eps}
| 
| will resolve the error, and will behave as you might hope.
| 
| If you're using the simpler graphics package, use the * form of
| the \resizebox command to specify the use of totalheight:
| 
| \resizebox*{!}{5cm}{%
|   \rotatebox{180}{%
|     \includegraphics{myfig.eps}%
|   }%
| }
| 
`----

HTH

Patrick
-- 
Computer games don't affect kids. If Pacman would have affected us as
children, we would now run around in darkened rooms, munching pills
and listening to repetetive music.



From ligges at statistik.uni-dortmund.de  Fri Oct  1 18:54:23 2004
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Fri, 01 Oct 2004 18:54:23 +0200
Subject: [R] hier.part
In-Reply-To: <415D80DF.5010803@sdsc.edu>
References: <415D80DF.5010803@sdsc.edu>
Message-ID: <415D8BBF.1060101@statistik.uni-dortmund.de>

T. Murlidharan Nair wrote:

> I am using the hier.part package for calculating the goodness of fit. 
> Before I started with
> my data I tried to use the example that the package comes with . I have 
> loaded the library
> hier.part and gtools. But the package gives the following error
> 
> Loading required package: gregmisc
> Loading required package: gregmisc
> Warning messages:
> 1: There is no package called 'gregmisc' in: library(package, 
> character.only = TRUE, logical = TRUE, warn.conflicts = warn.conflicts, 
> 2: There is no package called 'gregmisc' in: library(package, 
> character.only = TRUE, logical = TRUE, warn.conflicts = warn.conflicts,
> I searched for gregmisc package but it unzipped to gtools. Is there 
> something other package with this name or has it been merged
> with another one ?


What about installing the most recent version of gregmisc?

install.packages("gregmisc")

Uwe Ligges





> Thanks../ Murli
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html



From m.dewey at iop.kcl.ac.uk  Fri Oct  1 18:58:04 2004
From: m.dewey at iop.kcl.ac.uk (Michael Dewey)
Date: Fri, 01 Oct 2004 17:58:04 +0100
Subject: [R] Plotting panels at arbitrary places on a map, rather than on a
 lattice
Message-ID: <6.1.0.6.0.20041001175604.02639170@pop.freeserve.net>

I think it is easiest to describe
what I want in terms of the concrete
problem I have.

I have data from a number of countries
in each of which a sample of people was
interviewed. In presenting the results
in a forthcoming collaborative publication
much emphasis will be placed on the
multi-centre nature of the study. Although
I suspect colleagues may do this with
shaded maps I would prefer to avoid
them as (a) they present one fact per
country per map (b) they are unfair to
the Netherlands and other high density
countries.

What I would like to do is to make
the background represent Europe (ideally
with a map but that is a frill) then
place simple scattergrams (or radar plots)
on it located roughly where the country
is. Another way of describing it might
be to say that I want something like
the panels produced by lattice but at
arbitrary coordinates rather than on
a rectangular grid. I suspect I have
to do this from scratch and I would
welcome hints.

Am I right that there is no off the
shelf way to do this?

Is grid the way to go? Looking at the
article in Rnews 2(2) and a brief scan
of the documentation suggests so.
If grid is the way to go then bearing
in mind I have never used grid before
(a) any hints about the overall
possible solution structure
would be welcome (b) is this realistic to
do within a week or shall I revert to
lattice and lose the geography?

Is there a simple way to draw a map
in the background? It needs to cover
as far as Sweden, Spain and Greece.
It can be crude,
as long as Italy looks roughly like
a boot that is fine. I am an epidemiologist
not a geographer.





Michael Dewey
m.dewey at iop.kcl.ac.uk



From nair at sdsc.edu  Fri Oct  1 19:09:25 2004
From: nair at sdsc.edu (T. Murlidharan Nair)
Date: Fri, 01 Oct 2004 10:09:25 -0700
Subject: [R] hier.part
References: <415D80DF.5010803@sdsc.edu>
	<415D8BBF.1060101@statistik.uni-dortmund.de>
Message-ID: <415D8F45.1000409@sdsc.edu>

It still gives me the same error !!
Murli


Uwe Ligges wrote:

> T. Murlidharan Nair wrote:
>
>> I am using the hier.part package for calculating the goodness of fit. 
>> Before I started with
>> my data I tried to use the example that the package comes with . I 
>> have loaded the library
>> hier.part and gtools. But the package gives the following error
>>
>> Loading required package: gregmisc
>> Loading required package: gregmisc
>> Warning messages:
>> 1: There is no package called 'gregmisc' in: library(package, 
>> character.only = TRUE, logical = TRUE, warn.conflicts = 
>> warn.conflicts, 2: There is no package called 'gregmisc' in: 
>> library(package, character.only = TRUE, logical = TRUE, 
>> warn.conflicts = warn.conflicts,
>> I searched for gregmisc package but it unzipped to gtools. Is there 
>> something other package with this name or has it been merged
>> with another one ?
>
>
>
> What about installing the most recent version of gregmisc?
>
> install.packages("gregmisc")
>
> Uwe Ligges
>
>
>
>
>
>> Thanks../ Murli
>>
>> ______________________________________________
>> R-help at stat.math.ethz.ch mailing list
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide! 
>> http://www.R-project.org/posting-guide.html
>



From deepayan at stat.wisc.edu  Fri Oct  1 19:45:53 2004
From: deepayan at stat.wisc.edu (Deepayan Sarkar)
Date: Fri, 1 Oct 2004 12:45:53 -0500
Subject: [R] Plotting panels at arbitrary places on a map,
	rather than on a lattice
In-Reply-To: <6.1.0.6.0.20041001175604.02639170@pop.freeserve.net>
References: <6.1.0.6.0.20041001175604.02639170@pop.freeserve.net>
Message-ID: <200410011245.53578.deepayan@stat.wisc.edu>

On Friday 01 October 2004 11:58, Michael Dewey wrote:
> I think it is easiest to describe
> what I want in terms of the concrete
> problem I have.
>
> I have data from a number of countries
> in each of which a sample of people was
> interviewed. In presenting the results
> in a forthcoming collaborative publication
> much emphasis will be placed on the
> multi-centre nature of the study. Although
> I suspect colleagues may do this with
> shaded maps I would prefer to avoid
> them as (a) they present one fact per
> country per map (b) they are unfair to
> the Netherlands and other high density
> countries.
>
> What I would like to do is to make
> the background represent Europe (ideally
> with a map but that is a frill) then
> place simple scattergrams (or radar plots)
> on it located roughly where the country
> is. Another way of describing it might
> be to say that I want something like
> the panels produced by lattice but at
> arbitrary coordinates rather than on
> a rectangular grid. I suspect I have
> to do this from scratch and I would
> welcome hints.
>
> Am I right that there is no off the
> shelf way to do this?

I'm not terribly familiar with radar plots (and not at all with 
scattergrams - what are they?), but if you want radar plots as produced 
by 'stars', then it seems to have an argument called 'location' which 
you can use to specify the locations of the stars. That sounds like 
what you might want. You can also specify 'add = TRUE', which, if you 
call 'stars' after you have already drawn the map, should superpose 
your radar plots on the map. 

I'm not sure how best to draw the map you want, but I'm sure there's a 
way (maybe using the maps package).

Deepayan



From p.dalgaard at biostat.ku.dk  Fri Oct  1 19:54:45 2004
From: p.dalgaard at biostat.ku.dk (Peter Dalgaard)
Date: 01 Oct 2004 19:54:45 +0200
Subject: [R] Lattice .ps graphic is rotated in LaTeX slides - PDF weirdness
In-Reply-To: <415D8AAE.306@lancaster.ac.uk>
References: <415D7339.8@yorku.ca> <x2acv6ph0u.fsf@biostat.ku.dk>
	<1096647907.12255.120.camel@localhost.localdomain>
	<415D8AAE.306@lancaster.ac.uk>
Message-ID: <x21xgipbmi.fsf@biostat.ku.dk>

Barry Rowlingson <B.Rowlingson at lancaster.ac.uk> writes:

> Marc Schwartz wrote:
> 
> > The one advantage of the Beamer package, for those that require it, is
> > that it supports pdflatex, which the others do not. Though, it can be
> > used with dvips/latex + ps2pdf, where needed.
> >
> 
>   Has anyone else hit the problem that sometimes occurs with embedded
> PostScript graphics generated by R when viewed in full-screen mode
> using Adobe Acrobat Reader in Linux? Yes, its _that_ specific.

>   You get black areas all round your graphic. It looks a mess. Does it
> in acroread4 and 5. But only Linux, and only full-screen mode.

Doesn't seem to want to happen to me, with acrobat 5, in full-screen
mode, on Linux, so it must be more specific than that...

-- 
   O__  ---- Peter Dalgaard             Blegdamsvej 3  
  c/ /'_ --- Dept. of Biostatistics     2200 Cph. N   
 (*) \(*) -- University of Copenhagen   Denmark      Ph: (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)             FAX: (+45) 35327907



From Roger.Bivand at nhh.no  Fri Oct  1 19:59:58 2004
From: Roger.Bivand at nhh.no (Roger Bivand)
Date: Fri, 1 Oct 2004 19:59:58 +0200 (CEST)
Subject: [R] Plotting panels at arbitrary places on a map, rather than 
	on a  lattice
In-Reply-To: <6.1.0.6.0.20041001175604.02639170@pop.freeserve.net>
Message-ID: <Pine.LNX.4.44.0410011941550.15258-100000@reclus.nhh.no>

On Fri, 1 Oct 2004, Michael Dewey wrote:

> I think it is easiest to describe what I want in terms of the concrete
> problem I have.
> 
> I have data from a number of countries in each of which a sample of
> people was interviewed. In presenting the results in a forthcoming
> collaborative publication much emphasis will be placed on the
> multi-centre nature of the study. Although I suspect colleagues may do
> this with shaded maps I would prefer to avoid them as (a) they present
> one fact per country per map (b) they are unfair to the Netherlands and
> other high density countries.
> 
> What I would like to do is to make the background represent Europe
> (ideally with a map but that is a frill) then place simple scattergrams
> (or radar plots) on it located roughly where the country is. Another way
> of describing it might be to say that I want something like the panels
> produced by lattice but at arbitrary coordinates rather than on a
> rectangular grid. I suspect I have to do this from scratch and I would
> welcome hints.
> 
> Am I right that there is no off the shelf way to do this?
> 
> Is grid the way to go? Looking at the article in Rnews 2(2) and a brief
> scan of the documentation suggests so. If grid is the way to go then
> bearing in mind I have never used grid before (a) any hints about the
> overall possible solution structure would be welcome (b) is this
> realistic to do within a week or shall I revert to lattice and lose the
> geography?
> 

Perhaps rather R-News 3/2, October 2003, "Integrating grid Graphics Output
with Base Graphics Output" by Paul Murrell. You might even consider just 
scanning a basemap, using the pixmap package to read it and display it as 
a backdrop, and using locator() to get the positions for the localised 
graphics. For a vector backdrop alternative, try the maps package, world 
map, and either set plotting limits or choose countries (or both to avoid 
the French overseas administrative divisions), then locator or the World 
Factbook to give you a label point.



> Is there a simple way to draw a map in the background? It needs to cover
> as far as Sweden, Spain and Greece. It can be crude, as long as Italy
> looks roughly like a boot that is fine. I am an epidemiologist not a
> geographer.
> 
> 
> 
> 
> 
> Michael Dewey
> m.dewey at iop.kcl.ac.uk
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
> 

-- 
Roger Bivand
Economic Geography Section, Department of Economics, Norwegian School of
Economics and Business Administration, Breiviksveien 40, N-5045 Bergen,
Norway. voice: +47 55 95 93 55; fax +47 55 95 93 93
e-mail: Roger.Bivand at nhh.no



From ligges at statistik.uni-dortmund.de  Fri Oct  1 20:01:07 2004
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Fri, 01 Oct 2004 20:01:07 +0200
Subject: [R] hier.part
In-Reply-To: <415D8F45.1000409@sdsc.edu>
References: <415D80DF.5010803@sdsc.edu>	<415D8BBF.1060101@statistik.uni-dortmund.de>
	<415D8F45.1000409@sdsc.edu>
Message-ID: <415D9B63.9090100@statistik.uni-dortmund.de>

T. Murlidharan Nair wrote:

> It still gives me the same error !!

OK, what is your version of R, which OS, which library paths have you 
set, and is gregmisc installed in more than one of the libraries?
What does "it unzipped to gtools" in your former mail mean (in 
particular: What is gtools?)?

Uwe Ligges


> Murli
> 
> 
> Uwe Ligges wrote:
> 
>> T. Murlidharan Nair wrote:
>>
>>> I am using the hier.part package for calculating the goodness of fit. 
>>> Before I started with
>>> my data I tried to use the example that the package comes with . I 
>>> have loaded the library
>>> hier.part and gtools. But the package gives the following error
>>>
>>> Loading required package: gregmisc
>>> Loading required package: gregmisc
>>> Warning messages:
>>> 1: There is no package called 'gregmisc' in: library(package, 
>>> character.only = TRUE, logical = TRUE, warn.conflicts = 
>>> warn.conflicts, 2: There is no package called 'gregmisc' in: 
>>> library(package, character.only = TRUE, logical = TRUE, 
>>> warn.conflicts = warn.conflicts,
>>> I searched for gregmisc package but it unzipped to gtools. Is there 
>>> something other package with this name or has it been merged
>>> with another one ?
>>
>>
>>
>>
>> What about installing the most recent version of gregmisc?
>>
>> install.packages("gregmisc")
>>
>> Uwe Ligges
>>
>>
>>
>>
>>
>>> Thanks../ Murli
>>>
>>> ______________________________________________
>>> R-help at stat.math.ethz.ch mailing list
>>> https://stat.ethz.ch/mailman/listinfo/r-help
>>> PLEASE do read the posting guide! 
>>> http://www.R-project.org/posting-guide.html
>>
>>
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html



From nair at sdsc.edu  Fri Oct  1 20:17:16 2004
From: nair at sdsc.edu (T. Murlidharan Nair)
Date: Fri, 01 Oct 2004 11:17:16 -0700
Subject: [R] hier.part
References: <415D80DF.5010803@sdsc.edu>	<415D8BBF.1060101@statistik.uni-dortmund.de>
	<415D8F45.1000409@sdsc.edu>
	<415D9B63.9090100@statistik.uni-dortmund.de>
Message-ID: <415D9F2C.8090702@sdsc.edu>

The R version I am using is 1.9.1 . My operating system is windows. 
 gregmisc is installed in only one of the
libraries.  
gregmisc when unzipped gets installed as gtools.  After I  looked at the 
index .html page I guess gtools is gregmisc so I am
not sure why it is not happy. I can only include it by library(gtools) 
and not library(gregmisc)  since is installed as gtools.
No library with the name gregmisc gets installed when I  install it 
using install.packages(gregmisc).
Am I missing something here ??
Thanks../Murli


Uwe Ligges wrote:

> T. Murlidharan Nair wrote:
>
>> It still gives me the same error !!
>
>
> OK, what is your version of R, which OS, which library paths have you 
> set, and is gregmisc installed in more than one of the libraries?
> What does "it unzipped to gtools" in your former mail mean (in 
> particular: What is gtools?)?
>
> Uwe Ligges
>
>
>> Murli
>>
>>
>> Uwe Ligges wrote:
>>
>>> T. Murlidharan Nair wrote:
>>>
>>>> I am using the hier.part package for calculating the goodness of 
>>>> fit. Before I started with
>>>> my data I tried to use the example that the package comes with . I 
>>>> have loaded the library
>>>> hier.part and gtools. But the package gives the following error
>>>>
>>>> Loading required package: gregmisc
>>>> Loading required package: gregmisc
>>>> Warning messages:
>>>> 1: There is no package called 'gregmisc' in: library(package, 
>>>> character.only = TRUE, logical = TRUE, warn.conflicts = 
>>>> warn.conflicts, 2: There is no package called 'gregmisc' in: 
>>>> library(package, character.only = TRUE, logical = TRUE, 
>>>> warn.conflicts = warn.conflicts,
>>>> I searched for gregmisc package but it unzipped to gtools. Is there 
>>>> something other package with this name or has it been merged
>>>> with another one ?
>>>
>>>
>>>
>>>
>>>
>>> What about installing the most recent version of gregmisc?
>>>
>>> install.packages("gregmisc")
>>>
>>> Uwe Ligges
>>>
>>>
>>>
>>>
>>>
>>>> Thanks../ Murli
>>>>
>>>> ______________________________________________
>>>> R-help at stat.math.ethz.ch mailing list
>>>> https://stat.ethz.ch/mailman/listinfo/r-help
>>>> PLEASE do read the posting guide! 
>>>> http://www.R-project.org/posting-guide.html
>>>
>>>
>>>
>>
>> ______________________________________________
>> R-help at stat.math.ethz.ch mailing list
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide! 
>> http://www.R-project.org/posting-guide.html
>



From Roger.Bivand at nhh.no  Fri Oct  1 20:25:19 2004
From: Roger.Bivand at nhh.no (Roger Bivand)
Date: Fri, 1 Oct 2004 20:25:19 +0200 (CEST)
Subject: [R] hier.part
In-Reply-To: <415D9B63.9090100@statistik.uni-dortmund.de>
Message-ID: <Pine.LNX.4.44.0410012020140.15258-100000@reclus.nhh.no>

On Fri, 1 Oct 2004, Uwe Ligges wrote:

> T. Murlidharan Nair wrote:
> 
> > It still gives me the same error !!
> 
> OK, what is your version of R, which OS, which library paths have you 
> set, and is gregmisc installed in more than one of the libraries?
> What does "it unzipped to gtools" in your former mail mean (in 
> particular: What is gtools?)?
> 

As announced earlier today, gregmisc turned into a bundle including gtools 
as a component package. I guess the hier.part package depends on the 
single gregmisc package that existed before it became a bundle. I think 
the source of earlier gregmisc package should be in:

http://cran.r-project.org/src/contrib/Archive

so it is that (latest 1.11.2) which is likely to be the right one. For 
binaries look for that version number.


> Uwe Ligges
> 
> 
> > Murli
> > 
> > 
> > Uwe Ligges wrote:
> > 
> >> T. Murlidharan Nair wrote:
> >>
> >>> I am using the hier.part package for calculating the goodness of fit. 
> >>> Before I started with
> >>> my data I tried to use the example that the package comes with . I 
> >>> have loaded the library
> >>> hier.part and gtools. But the package gives the following error
> >>>
> >>> Loading required package: gregmisc
> >>> Loading required package: gregmisc
> >>> Warning messages:
> >>> 1: There is no package called 'gregmisc' in: library(package, 
> >>> character.only = TRUE, logical = TRUE, warn.conflicts = 
> >>> warn.conflicts, 2: There is no package called 'gregmisc' in: 
> >>> library(package, character.only = TRUE, logical = TRUE, 
> >>> warn.conflicts = warn.conflicts,
> >>> I searched for gregmisc package but it unzipped to gtools. Is there 
> >>> something other package with this name or has it been merged
> >>> with another one ?
> >>
> >>
> >>
> >>
> >> What about installing the most recent version of gregmisc?
> >>
> >> install.packages("gregmisc")
> >>
> >> Uwe Ligges
> >>
> >>
> >>
> >>
> >>
> >>> Thanks../ Murli
> >>>
> >>> ______________________________________________
> >>> R-help at stat.math.ethz.ch mailing list
> >>> https://stat.ethz.ch/mailman/listinfo/r-help
> >>> PLEASE do read the posting guide! 
> >>> http://www.R-project.org/posting-guide.html
> >>
> >>
> > 
> > ______________________________________________
> > R-help at stat.math.ethz.ch mailing list
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide! 
> > http://www.R-project.org/posting-guide.html
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
> 

-- 
Roger Bivand
Economic Geography Section, Department of Economics, Norwegian School of
Economics and Business Administration, Breiviksveien 40, N-5045 Bergen,
Norway. voice: +47 55 95 93 55; fax +47 55 95 93 93
e-mail: Roger.Bivand at nhh.no



From ligges at statistik.uni-dortmund.de  Fri Oct  1 20:27:45 2004
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Fri, 01 Oct 2004 20:27:45 +0200
Subject: [R] hier.part
In-Reply-To: <Pine.LNX.4.44.0410012020140.15258-100000@reclus.nhh.no>
References: <Pine.LNX.4.44.0410012020140.15258-100000@reclus.nhh.no>
Message-ID: <415DA1A1.3040805@statistik.uni-dortmund.de>

Roger Bivand wrote:

> On Fri, 1 Oct 2004, Uwe Ligges wrote:
> 
> 
>>T. Murlidharan Nair wrote:
>>
>>
>>>It still gives me the same error !!
>>
>>OK, what is your version of R, which OS, which library paths have you 
>>set, and is gregmisc installed in more than one of the libraries?
>>What does "it unzipped to gtools" in your former mail mean (in 
>>particular: What is gtools?)?
>>
> 
> 
> As announced earlier today, gregmisc turned into a bundle including gtools 
> as a component package. I guess the hier.part package depends on the 
> single gregmisc package that existed before it became a bundle. I think 
> the source of earlier gregmisc package should be in:
> 
> http://cran.r-project.org/src/contrib/Archive
> 
> so it is that (latest 1.11.2) which is likely to be the right one. For 
> binaries look for that version number.

Thank you, Roger.
Looks like I should go home for now ...

Uwe


>>Uwe Ligges
>>
>>
>>
>>>Murli
>>>
>>>
>>>Uwe Ligges wrote:
>>>
>>>
>>>>T. Murlidharan Nair wrote:
>>>>
>>>>
>>>>>I am using the hier.part package for calculating the goodness of fit. 
>>>>>Before I started with
>>>>>my data I tried to use the example that the package comes with . I 
>>>>>have loaded the library
>>>>>hier.part and gtools. But the package gives the following error
>>>>>
>>>>>Loading required package: gregmisc
>>>>>Loading required package: gregmisc
>>>>>Warning messages:
>>>>>1: There is no package called 'gregmisc' in: library(package, 
>>>>>character.only = TRUE, logical = TRUE, warn.conflicts = 
>>>>>warn.conflicts, 2: There is no package called 'gregmisc' in: 
>>>>>library(package, character.only = TRUE, logical = TRUE, 
>>>>>warn.conflicts = warn.conflicts,
>>>>>I searched for gregmisc package but it unzipped to gtools. Is there 
>>>>>something other package with this name or has it been merged
>>>>>with another one ?
>>>>
>>>>
>>>>
>>>>
>>>>What about installing the most recent version of gregmisc?
>>>>
>>>>install.packages("gregmisc")
>>>>
>>>>Uwe Ligges
>>>>
>>>>
>>>>
>>>>
>>>>
>>>>
>>>>>Thanks../ Murli
>>>>>
>>>>>______________________________________________
>>>>>R-help at stat.math.ethz.ch mailing list
>>>>>https://stat.ethz.ch/mailman/listinfo/r-help
>>>>>PLEASE do read the posting guide! 
>>>>>http://www.R-project.org/posting-guide.html
>>>>
>>>>
>>>______________________________________________
>>>R-help at stat.math.ethz.ch mailing list
>>>https://stat.ethz.ch/mailman/listinfo/r-help
>>>PLEASE do read the posting guide! 
>>>http://www.R-project.org/posting-guide.html
>>
>>______________________________________________
>>R-help at stat.math.ethz.ch mailing list
>>https://stat.ethz.ch/mailman/listinfo/r-help
>>PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>>
> 
>



From MSchwartz at MedAnalytics.com  Fri Oct  1 20:40:17 2004
From: MSchwartz at MedAnalytics.com (Marc Schwartz)
Date: Fri, 01 Oct 2004 13:40:17 -0500
Subject: [R] Lattice .ps graphic is rotated in LaTeX slides - PDF weirdness
In-Reply-To: <x21xgipbmi.fsf@biostat.ku.dk>
References: <415D7339.8@yorku.ca> <x2acv6ph0u.fsf@biostat.ku.dk>
	<1096647907.12255.120.camel@localhost.localdomain>
	<415D8AAE.306@lancaster.ac.uk>  <x21xgipbmi.fsf@biostat.ku.dk>
Message-ID: <1096656017.12255.143.camel@localhost.localdomain>

On Fri, 2004-10-01 at 12:54, Peter Dalgaard wrote:
> Barry Rowlingson <B.Rowlingson at lancaster.ac.uk> writes:
> 
> > Marc Schwartz wrote:
> > 
> > > The one advantage of the Beamer package, for those that require it, is
> > > that it supports pdflatex, which the others do not. Though, it can be
> > > used with dvips/latex + ps2pdf, where needed.
> > >
> > 
> >   Has anyone else hit the problem that sometimes occurs with embedded
> > PostScript graphics generated by R when viewed in full-screen mode
> > using Adobe Acrobat Reader in Linux? Yes, its _that_ specific.
> 
> >   You get black areas all round your graphic. It looks a mess. Does it
> > in acroread4 and 5. But only Linux, and only full-screen mode.
> 
> Doesn't seem to want to happen to me, with acrobat 5, in full-screen
> mode, on Linux, so it must be more specific than that...


Can't say that I have ever seen that and I do use acroread 5 under FC2
for full screen slide presentations. 

Barry, do you have a specific R example that I could try to replicate
here?

Marc



From patrick.drechsler at gmx.net  Fri Oct  1 20:34:38 2004
From: patrick.drechsler at gmx.net (Patrick Drechsler)
Date: Fri, 01 Oct 2004 20:34:38 +0200
Subject: [R] displaying sample size in boxplots
References: <D7A3CFD7825BD6119B880002A58F06C20C521D03@groexmb02.pfizer.com>
Message-ID: <m33c0y2sox.fsf@pdrechsler.fqdn.th-h.de>


Gregory R. Warnes wrote on 01 Oct 2004 14:52:05 MET:

[...]
>> > Here are the current proposals [for cut & paste]:
>> >
>> > library(ISwR)
>> > data(energy)
>> > attach(energy)
>> >
>> > ## 1
>> > boxplot(expend~stature)
>> > sample.size <- tapply(expend, stature, length)
>> > ss.ch <- paste("N=", sample.size, sep="")
>> > mtext(ss.ch, at=1:length(unique(stature)), line=2, side=1)
>> >
>> > ## 2 (Roger)
>> > boxplot(expend~stature, width=sample.size/length(expend),
>> >         names=paste(levels(stature), ", N=", sample.size, sep=""))
>> >
>> > ## 3 (Roger + Martin):
>> > boxplot(expend ~ stature, varwidth= TRUE,
>> >         names=paste(levels(stature), ", N=", sample.size, sep=""))
[...]

> Also note that boxplot.n in the gplots library (part of the
> gregmisc bundle) automatically adds the number of observations.

Thanks Greg! Nice to see that you've written some code just for
this purpose. I will of course also take a closer look at the
other functions that are bundled in `gregmisc'.

Since I'm still new to R: can somebody give me a pointer to the
docs where to find instructions on a package (not a function)? I
can find the man pages to specific functions with ?<functionname>
(something similar to `texdoc <packagename>' in tetex)?

TIA,

Patrick
-- 
Look Ma, this man can twist his fingers as if they were made of
rubber, isn't that amazing? -- Not really, he's been using Emacs
for years...!



From Achim.Zeileis at wu-wien.ac.at  Fri Oct  1 21:04:35 2004
From: Achim.Zeileis at wu-wien.ac.at (Achim Zeileis)
Date: Fri, 1 Oct 2004 21:04:35 +0200
Subject: [R] Lattice .ps graphic is rotated in LaTeX slides - PDF weirdness
In-Reply-To: <x21xgipbmi.fsf@biostat.ku.dk>
References: <415D7339.8@yorku.ca> <x2acv6ph0u.fsf@biostat.ku.dk>
	<1096647907.12255.120.camel@localhost.localdomain>
	<415D8AAE.306@lancaster.ac.uk> <x21xgipbmi.fsf@biostat.ku.dk>
Message-ID: <20041001210435.138ca3ce.Achim.Zeileis@wu-wien.ac.at>

On 01 Oct 2004 19:54:45 +0200 Peter Dalgaard wrote:

> Barry Rowlingson <B.Rowlingson at lancaster.ac.uk> writes:
> 
> > Marc Schwartz wrote:
> > 
> > > The one advantage of the Beamer package, for those that require
> > > it, is that it supports pdflatex, which the others do not. Though,
> > > it can be used with dvips/latex + ps2pdf, where needed.
> > >
> > 
> >   Has anyone else hit the problem that sometimes occurs with
> >   embedded
> > PostScript graphics generated by R when viewed in full-screen mode
> > using Adobe Acrobat Reader in Linux? Yes, its _that_ specific.
> 
> >   You get black areas all round your graphic. It looks a mess. Does
> >   it
> > in acroread4 and 5. But only Linux, and only full-screen mode.
> 
> Doesn't seem to want to happen to me, with acrobat 5, in full-screen
> mode, on Linux, so it must be more specific than that...

We had that problem with severl pdf-documents at useR! 2004. My
suspicion was that it depends somehow on the way the eps-graphic takes
until it ends up in the pdf-document. I seem to recall that those slides
were usually generated via 
  latex -> dvips -> ps2pdf
or maybe also via
  latex -> dvipdf
although this tends to produce nicer pdf. But I never digged deeper into
this and just forced the presenters to use xpdf ;-)

BTW: when producing pdf-slides, especially with graphics produced by R,
I usually don't generate any eps at all but go directly from my Rnw-file
  Sweave -> pdflatex
which produces very nice pdf output.
Z

> -- 
>    O__  ---- Peter Dalgaard             Blegdamsvej 3  
>   c/ /'_ --- Dept. of Biostatistics     2200 Cph. N   
>  (*) \(*) -- University of Copenhagen   Denmark      Ph: (+45)
>  35327918
> ~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)             FAX: (+45)
> 35327907
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide!
> http://www.R-project.org/posting-guide.html
>



From friendly at yorku.ca  Fri Oct  1 21:24:57 2004
From: friendly at yorku.ca (Michael Friendly)
Date: Fri, 01 Oct 2004 15:24:57 -0400
Subject: [R] Lattice .ps graphic is rotated in LaTeX slides
In-Reply-To: <1096647158.12255.109.camel@localhost.localdomain>
References: <415D7339.8@yorku.ca>
	<1096647158.12255.109.camel@localhost.localdomain>
Message-ID: <415DAF09.6020805@yorku.ca>

That does indeed work!  I had read the arguments section of ?postscript, 
but this will teach me to read
the details.  There could/should be a trellis.device("eps", ...) that 
supplies the appropriate defaults.

For the perversely inclined I was able to use my original .ps file in 
this contorted way:

 \rotatebox{180}{\includegraphics[angle=90,height=.6\textheight]{fig/barley2x3.ps}}


thanks,
-Michael

Marc Schwartz wrote:

>On Fri, 2004-10-01 at 10:09, Michael Friendly wrote:
>  
>
>>I've generated a version of the classic dotplot of the barley data with
>>
>>library(lattice)
>>data(barley)
>>
>>trellis.device("postscript", color=TRUE, file="barley2x3.ps")
>>old.settings <- trellis.par.get()
>>    
>>
 .... snip ...

>Michael,
>
>Try the following when specifying the trellis.device:
>
>trellis.device("postscript", color = TRUE, file = "barley2x3.eps",
>                onefile = FALSE, paper = "special", horizontal = FALSE,
>                width = 9, height = 6)
>
>See if that works without specifying the angle in your LaTeX for
>seminar:
>
>\begin{slide}
>  \begin{center}
>    \includegraphics[height=.6\textheight]{fig/barley2x3.eps}
>  \end{center}
>\end{slide}
>
>Note that when including graphics in LaTeX, you should use EPS files,
>which (as noted in ?postscript) require certain device settings to
>create. These include:
>
>onefile = FALSE, paper = "special", horizontal = FALSE
>
>and the device 'width' and 'height' settings.
>
>This will also adjust the size of the bounding box.
>
>HTH,
>
>Marc Schwartz
>
>  
>


-- 
Michael Friendly     Email: friendly at yorku.ca 
Professor, Psychology Dept.
York University      Voice: 416 736-5115 x66249 Fax: 416 736-5814
4700 Keele Street    http://www.math.yorku.ca/SCS/friendly.html
Toronto, ONT  M3J 1P3 CANADA



From gene02_r at smalltime.com  Fri Oct  1 22:51:27 2004
From: gene02_r at smalltime.com (gene)
Date: Fri, 1 Oct 2004 13:51:27 -0700
Subject: [R] 3d matrix * 1d matrix question
Message-ID: <AA3B544B-13EB-11D9-A0F3-000A95984108@smalltime.com>

Apologies for the rather newbie question, but I haven't been able to 
figure this out.
I've got a 3d matrix (though presumably, the answer would be the same 
as for a 2d matrix) which I want to multiply by the values in a vector 
like so:

matrix m is:

, , 1
      [,1] [,2]
[1,]    1    6
[2,]    2    7
[3,]    3    8

, , 2
      [,1] [,2]
[1,]   21   26
[2,]   22   27
[3,]   23   28

, , 3
      [,1] [,2]
[1,]   41   46
[2,]   42   47
[3,]   43   48


vector v is c(2,10).

I want to multiply m by v along the columns to get this result:

, , 1
      [,1] [,2]
[1,]    2   60
[2,]    4   70
[3,]    6   80

, , 2
      [,1] [,2]
[1,]   42   260
[2,]   44   270
[3,]   46   280

, , 3
      [,1] [,2]
[1,]   82   460
[2,]   84   470
[3,]   86   480


So, I figured I could do this by populating a new matrix the same 
dimensions as m with the values from v repeated in the right order and 
multiplying m by the new matrix.  It feels like there should be a 
one-step way to do this, though, especially since my real data is a 
very large data set and it would be memory-inefficient to create a new 
giant temporary matrix.

Thanks.

---
Gene



From p.dalgaard at biostat.ku.dk  Sat Oct  2 00:03:24 2004
From: p.dalgaard at biostat.ku.dk (Peter Dalgaard)
Date: 02 Oct 2004 00:03:24 +0200
Subject: [R] 3d matrix * 1d matrix question
In-Reply-To: <AA3B544B-13EB-11D9-A0F3-000A95984108@smalltime.com>
References: <AA3B544B-13EB-11D9-A0F3-000A95984108@smalltime.com>
Message-ID: <x2k6uakser.fsf@biostat.ku.dk>

gene <gene02_r at smalltime.com> writes:

> Apologies for the rather newbie question, but I haven't been able to
> figure this out.
> I've got a 3d matrix (though presumably, the answer would be the same
> as for a 2d matrix) which I want to multiply by the values in a vector
> like so:
> 
> matrix m is:
> 
> , , 1
>       [,1] [,2]
> [1,]    1    6
> [2,]    2    7
> [3,]    3    8
> 
> , , 2
>       [,1] [,2]
> [1,]   21   26
> [2,]   22   27
> [3,]   23   28
> 
> , , 3
>       [,1] [,2]
> [1,]   41   46
> [2,]   42   47
> [3,]   43   48
> 
> 
> vector v is c(2,10).
> 
> I want to multiply m by v along the columns to get this result:
> 
> , , 1
>       [,1] [,2]
> [1,]    2   60
> [2,]    4   70
> [3,]    6   80
> 
> , , 2
>       [,1] [,2]
> [1,]   42   260
> [2,]   44   270
> [3,]   46   280
> 
> , , 3
>       [,1] [,2]
> [1,]   82   460
> [2,]   84   470
> [3,]   86   480
> 
> 
> So, I figured I could do this by populating a new matrix the same
> dimensions as m with the values from v repeated in the right order and
> multiplying m by the new matrix.  It feels like there should be a
> one-step way to do this, though, especially since my real data is a
> very large data set and it would be memory-inefficient to create a new
> giant temporary matrix.

You don't need the full replicate matrix, only enough to allow
recycling to do its job. In this case, m * rep(c(2,10),each=3) should
do the trick. However, there's a function designed for the purpose:

  sweep(m, 2, v, "*")

(well, maybe "almost designed for"; takes a little thought to realize
that your problem is similar to sweeping out means along array extents.)

-- 
   O__  ---- Peter Dalgaard             Blegdamsvej 3  
  c/ /'_ --- Dept. of Biostatistics     2200 Cph. N   
 (*) \(*) -- University of Copenhagen   Denmark      Ph: (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)             FAX: (+45) 35327907



From stephane.dray at umontreal.ca  Sat Oct  2 00:37:24 2004
From: stephane.dray at umontreal.ca (Stephane DRAY)
Date: Fri, 01 Oct 2004 18:37:24 -0400
Subject: [R] cumsum over a list or an array
Message-ID: <5.2.1.1.0.20041001182329.00b51fd0@magellan.umontreal.ca>

Hello list,

my question is related to svd of a matrix:

b=matrix(rnorm(50),10,5)
mysvd=svd(b)

I would like to compute each xi where xi = di* ui %*% t(vi). I do it by :

xlist=sapply(1:ncol(b), function(x1,y) 
y$d[x1]*y$u[,x1]%*%t(y$v[,x1]),y=mysvd,simplify=F) # result is a list

xarray=array(sapply(1:ncol(b), function(x1,y) 
y$d[x1]*y$u[,x1]%*%t(y$v[,x1]),y=mysvd),c(nrow(b),ncol(b),ncol(b))) # 
result is an array

Now i would like to compute cumulative sum:

y1=x1 # y[,,1]
y2=x1+x2 # y[,,2]
...

I have try to do it with apply without succes:

y=apply(xarray,c(1,2),cumsum)

The results are good but not in the format that I want. I could modify the 
results to modify the format but I am sure that it exists another faster way.

Is it possible to do the same on the list ?

Thanks in advance

Sincerely



St??phane DRAY
-------------------------------------------------------------------------------------------------- 

D??partement des Sciences Biologiques
Universit?? de Montr??al, C.P. 6128, succursale centre-ville
Montr??al, Qu??bec H3C 3J7, Canada

Tel : (514) 343-6111 poste 1233         Fax : (514) 343-2293
E-mail : stephane.dray at umontreal.ca
-------------------------------------------------------------------------------------------------- 

Web                                          http://www.steph280.freesurf.fr/



From jeaneid at chass.utoronto.ca  Sat Oct  2 01:12:03 2004
From: jeaneid at chass.utoronto.ca (Jean Eid)
Date: Fri, 1 Oct 2004 19:12:03 -0400
Subject: [R] dataload for linux
Message-ID: <Pine.SGI.4.40.0409281124470.42162780-100000@origin.chass.utoronto.ca>

Is there a dataload utility for linux. The link in genstat is down but I
managed to find the utility at:
http://gurukul.ucc.american.edu/econ/gaussres/UTILITYS/DATALOAD.HTM

but this is a dos/windows version.

Thank you

Jean



From rpeng at jhsph.edu  Sat Oct  2 05:40:44 2004
From: rpeng at jhsph.edu (Roger D. Peng)
Date: Fri, 01 Oct 2004 23:40:44 -0400
Subject: [R] Lattice .ps graphic is rotated in LaTeX slides
In-Reply-To: <415D7339.8@yorku.ca>
References: <415D7339.8@yorku.ca>
Message-ID: <415E233C.9050702@jhsph.edu>

You might be encountering the infamous auto-rotation feature.  In Unix you can 
turn it off via something like

setenv GS_OPTIONS "-dAutoRotatePages=/None"

(or the bash equivalent).

-roger

Michael Friendly wrote:

> I've generated a version of the classic dotplot of the barley data with
> 
> library(lattice)
> data(barley)
> 
> trellis.device("postscript", color=TRUE, file="barley2x3.ps")
> old.settings <- trellis.par.get()
> trellis.par.set("background", list(col = "white"))
> lset(list(superpose.symbol=list(pch=c(19, 1, 25, 2, 15, 22, 23),
>       cex=rep(1,7),col=c("blue", "red", "darkgreen", "brown",
>       "orange", "turquoise", "orchid") )))
> lset(list(fontsize = list(default = 14)))
> 
> n <- length(levels(barley$year))
> dotplot(variety ~ yield | site, data = barley, groups = year,
> layout = c(2, 3), aspect = .5,
> xlab = "Barley Yield (bushels/acre)",
> key = list(points = Rows(trellis.par.get("superpose.symbol"), 1:n),
>   text = list(levels(barley$year)), columns = n))
> dev.off()
> lset(theme=old.settings)
> 
> It looks fine with gv (though I'd like to make the bounding box 
> tighter), but when I embed it in a LaTeX slide
> (landscape, using seminar package),
> 
> \begin{slide}
> \includegraphics[,height=.6\textheight]{fig/barley2x3.ps}
> \end{slide}
> 
> the image is rotated 90 deg CCW.  I tried to adjust for this with
> 
> \includegraphics[angle=-90,height=.6\textheight]{fig/barley2x3.ps}
> 
> but  that gives
> ! Package graphics Error: Division by 0.
> 
> What am I doing wrong, or how could I do it differently so it would work?
> 
> thanks
>



From ggrothendieck at myway.com  Sat Oct  2 06:55:30 2004
From: ggrothendieck at myway.com (Gabor Grothendieck)
Date: Sat, 2 Oct 2004 04:55:30 +0000 (UTC)
Subject: [R] cumsum over a list or an array
References: <5.2.1.1.0.20041001182329.00b51fd0@magellan.umontreal.ca>
Message-ID: <loom.20041002T065208-623@post.gmane.org>

Stephane DRAY <stephane.dray <at> umontreal.ca> writes:

: 
: Hello list,
: 
: my question is related to svd of a matrix:
: 
: b=matrix(rnorm(50),10,5)
: mysvd=svd(b)
: 
: I would like to compute each xi where xi = di* ui %*% t(vi). I do it by :
: 
: xlist=sapply(1:ncol(b), function(x1,y) 
: y$d[x1]*y$u[,x1]%*%t(y$v[,x1]),y=mysvd,simplify=F) # result is a list
: 
: xarray=array(sapply(1:ncol(b), function(x1,y) 
: y$d[x1]*y$u[,x1]%*%t(y$v[,x1]),y=mysvd),c(nrow(b),ncol(b),ncol(b))) # 
: result is an array
: 
: Now i would like to compute cumulative sum:
: 
: y1=x1 # y[,,1]
: y2=x1+x2 # y[,,2]
: ...
: 
: I have try to do it with apply without succes:
: 
: y=apply(xarray,c(1,2),cumsum)
: 
: The results are good but not in the format that I want. I could modify the 
: results to modify the format but I am sure that it exists another faster way.
: 
: Is it possible to do the same on the list ?
: 
: Thanks in advance

I assume the format you want is a list in which the ith
element is the approximation to b formed from the first i
singular values and the associated spaces.

Its actually pretty straighforward and short to simply use a 
for loop:

   z1 <- xlist
   for(i in 2:length(mysvd$d)) z1[[i]] <- z1[[i-1]] + xlist[[i]]

or one can do it directly from the output of the svd
without using xlist or xarray by simply zeroing all but
the first i singular values in udv' in the ith iteration 
within lapply:

   z2 <- with(mysvd, {
      idx <- seq(along = d)
      lapply(idx, function(i) u %*% diag(d*(idx<=i)) %*% t(v))
   })

   all.equal(z1, z2) # TRUE



From ggrothendieck at myway.com  Sat Oct  2 07:04:29 2004
From: ggrothendieck at myway.com (Gabor Grothendieck)
Date: Sat, 2 Oct 2004 05:04:29 +0000 (UTC)
Subject: [R] dataload for linux
References: <Pine.SGI.4.40.0409281124470.42162780-100000@origin.chass.utoronto.ca>
Message-ID: <loom.20041002T065650-413@post.gmane.org>

Jean Eid <jeaneid <at> chass.utoronto.ca> writes:

: 
: Is there a dataload utility for linux. The link in genstat is down but I
: managed to find the utility at:
: http://gurukul.ucc.american.edu/econ/gaussres/UTILITYS/DATALOAD.HTM
: 
: but this is a dos/windows version.
: 
: Thank you
: 
: Jean

It existed at one time but the author withdrew all copies of all
versions, apparently due to factors beyond his control, and AFAIK 
its now unavailable and unsupported even if you can find it.  The 
version you have seems to be very old.  It was a nice program in its 
day but most of the things that dataload could do can now be done 
directly in R thereby gaining the benefit of not using closed source 
software.



From ozric at web.de  Sat Oct  2 09:42:43 2004
From: ozric at web.de (Christian Schulz)
Date: Sat, 2 Oct 2004 09:42:43 +0200
Subject: [R] constraints in optim?
Message-ID: <200410020942.44143.ozric@web.de>

> 
optim(c(1,1),LL,method="SANN",control=list(fnscale=-1),trans=trans,times=times)
$par
[1] 17.422635 -1.606859

How could i constraint that the parameters should be both positive in
my maximizing problem?
I check constrOptim but here i could only constraint the variables trans and 
times and not my parameters?

many thanks, regards
Christian



From ripley at stats.ox.ac.uk  Sat Oct  2 10:11:40 2004
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Sat, 2 Oct 2004 09:11:40 +0100 (BST)
Subject: [R] constraints in optim?
In-Reply-To: <200410020942.44143.ozric@web.de>
Message-ID: <Pine.LNX.4.44.0410020902100.28124-100000@gannet.stats>

Do you know what you are doing using method SANN?  It is a very strange 
choice for a 2D problem.  Method L-BFGS-B would allow non-negativity 
constraints.

optim() does not have control options trans and times ...

The standard way would be to transform your problem, for example to 
optimize over log-parameters, if you really meant `positive' and not
`non-negative'.

On Sat, 2 Oct 2004, Christian Schulz wrote:

> optim(c(1,1),LL,method="SANN",control=list(fnscale=-1),trans=trans,times=times)
> $par
> [1] 17.422635 -1.606859
> 
> How could i constraint that the parameters should be both positive in
> my maximizing problem?
> I check constrOptim but here i could only constraint the variables trans and 
> times and not my parameters?


> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html

That applies to YOU.

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From spencer.graves at pdf.com  Sat Oct  2 17:19:02 2004
From: spencer.graves at pdf.com (Spencer Graves)
Date: Sat, 02 Oct 2004 10:19:02 -0500
Subject: [R] gnls or nlme : how to obtain confidence intervals of fitted
	values
In-Reply-To: <415D0BE8.9050104@nancy.inra.fr>
References: <415D0BE8.9050104@nancy.inra.fr>
Message-ID: <415EC6E6.8070600@pdf.com>

      Pinhiero and Bates (2000) Mixed-Effects Models in S and S-Plus 
(Springer) describe the use of "intervals" for that.  In library(nlme), 
R 1.9.1 for Windows, I found documentation for intervals, intervals.gls, 
intervals.lme, intervals.lmList, and gnls.  To an example in the 
documentation for gnls, I added intervals: 

 >      data(Soybean)
 >      # variance increases with a power of the absolute fitted values
 >      fm1 <- gnls(weight ~ SSlogis(Time, Asym, xmid, scal), Soybean,
+                  weights = varPower())
 >      summary(fm1)
Generalized nonlinear least squares fit
  Model: weight ~ SSlogis(Time, Asym, xmid, scal)
  Data: Soybean
       AIC      BIC    logLik
  983.7947 1003.900 -486.8974

Variance function:
 Structure: Power of variance covariate
 Formula: ~fitted(.)
 Parameter estimates:
    power
0.8815437

Coefficients:
        Value Std.Error  t-value p-value
Asym 17.35682 0.5226885 33.20682       0
xmid 51.87232 0.5916820 87.66925       0
scal  7.62053 0.1390958 54.78617       0

 Correlation:
     Asym  xmid
xmid 0.787     
scal 0.485 0.842

Standardized residuals:
         Min           Q1          Med           Q3          Max
-2.309670367 -0.646844555 -0.004897024  0.498606088  4.986727281

Residual standard error: 0.3662752
Degrees of freedom: 412 total; 409 residual
 > intervals(fm1)
Approximate 95% confidence intervals

 Coefficients:
         lower      est.     upper
Asym 16.329331 17.356822 18.384313
xmid 50.709200 51.872317 53.035434
scal  7.347094  7.620525  7.893957
attr(,"label")
[1] "Coefficients:"

 Variance function:
          lower      est.    upper
power 0.8369085 0.8815437 0.926179
attr(,"label")
[1] "Variance function:"

 Residual standard error:
    lower      est.     upper
0.3373374 0.3649392 0.3947995

      Is this what you want? 
      hope this helps. 
      spencer graves

Pierre MONTPIED wrote:

> Hi
>
> I use gnls to fit non linear models of the form y = alpha * x**beta 
> (alpha and beta being linear functions of a 2nd regressor z i.e. 
> alpha=a1+a2*z and beta=b1+b2*z) with variance function 
> varPower(fitted(.)) which sounds correct for the data set I use.
>
> My purpose is to use the fitted models for predictions with other sets 
> of regressors x, z than those used in fitting. I therefore need to 
> estimate y with (95%) confidence intervals.
>
> Does any body knows how to do this with R ?
>
> Thanks
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html


-- 
Spencer Graves, PhD, Senior Development Engineer
O:  (408)938-4420;  mobile:  (408)655-4567



From roym at ufl.edu  Sat Oct  2 18:29:43 2004
From: roym at ufl.edu (Manojit Roy)
Date: Sat, 2 Oct 2004 12:29:43 -0400 (EDT)
Subject: [R] Inverse CWT?
Message-ID: <240943378.1096734583930.JavaMail.osg@osgjas01.cns.ufl.edu>

Hello R,

Is there a function (in Contrib packages maybe) that can do 
inverse of continuous wavelet transform? I am using cwt() from 
"Rwave" library to get the transform of a time series, and need to 
reconstruct the series within a subset of scales.

Thanks a bunch,
Manojit



From lachmann at eva.mpg.de  Sat Oct  2 20:52:32 2004
From: lachmann at eva.mpg.de (Michael Lachmann)
Date: Sat, 02 Oct 2004 20:52:32 +0200
Subject: [R] conditional assignments and calculations
Message-ID: <415EF8F0.7010108@eva.mpg.de>

Hello!

I am using the TeXmacs interface to R. (Though I encountered a similar 
problem when using Sweave)
In doing calculations I often ecounter this scenario: I'll have some 
calculations in my file:
--
A=read.lots.of.data()

B=huge.calculation.on(A)

C=another.calculation.on(B)
--
Now, if A has already been read, I don't need to re-read it. If B has 
already been calculated, I don't need to recalculate it. But I would 
like to be able to just press 'enter' on each of them.

So, I would like R to somehow figure out dependencies (a bit like in 
Makefiles)

I implemented something like this with the following functions:
----------------------
touch=function(x) {attr(x,"last.updated")=Sys.time();x}

last.updated=function(a) {
   if( length(attr(a,"last.updated")) == 0 ) {
   	Sys.time()
   } else {
       attr(a,"last.updated")
   }
}


"depends<-"=function(a,value,...) {
   args=list(...)
   if( length(attr(a,"last.updated")) == 0 ) {
     a <- value
     a <-touch(a)
   } else {
     lu=(sapply(args,function(x) last.updated(x)-last.updated(a) > 0 ))
     if( sum(lu)>0 ) {
        a <- value
        a <-touch(a)
     }
   }
   a
}
------------------------
Then I can implement what I wanted above as follows:
--
if( !exists(A) ) { A=read.lots.of.data(); A=touch(A) }

depends(B,A)=huge.calculation.on(A)
# this means the assignment 'B=huge.calculation.on(A)' is
# done only if A has been updated more recently than B.

depends(C,B)=another.calculation.on(B)
# dito for C more recent than B.
--
And now I can carelessly press 'enter' on these expression that might 
otherwise take hours to compute. Each variable has a datestamp of the 
last time it was updated, and I do each calculation conditional on 
whether certain variables have been recently changed. I can also save 
A,B,C to a file,later load them, and the calculations will not be redone.

But this solution is quite ugly, because of several problems:

1. To call 'depends(A,B)=f(B)' the first time, A has to already exist, 
otherwise I get an error (before I enter the "depends<-" function.)

2. I would also like to have a convenient way to do
"if( !exists(A) ) { A=read.lots.of.data(); A=touch(A) }"
maybe something like:
depends(A)<-read.lots.of.data()
But that doesn't work, because of 1.
or
A %set% read.lots.data()
But that doesn't work, because I haven't figured out a way for a 
function to change one of its variables.
(Maybe I could do A=A %set read.lots.of.data(), but that is really ugly...)

3. It would be nice to be able to do touch(A) instead of A=touch(A)

4. If I modify A without calling 'A=touch(A)', then B will not be 
updated next time I call 'depends(B,A)=huge.calculation.on(A)'. So it 
would be nice to have the variable's 'last updated' time updated 
automatically. (Though then it is a bit problematic to decide what the 
'last updated' time should be for variables loaded from a file...)


5. The whole thing is rather cludgy. But I haven't found a good way to 
implement it.

Suggestions?

Thanks,

    Michael



From bill.shipley at usherbrooke.ca  Sat Oct  2 21:55:06 2004
From: bill.shipley at usherbrooke.ca (Bill Shipley)
Date: Sat, 2 Oct 2004 15:55:06 -0400
Subject: [R] lme function with marginal terms for ANOVA?
Message-ID: <000001c4a8b9$ba673a80$8c1ad284@BIO041>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20041002/e48ab37f/attachment.pl

From vikas at mail.jnu.ac.in  Sun Oct  3 03:38:26 2004
From: vikas at mail.jnu.ac.in (Vikas Rawal)
Date: Sun, 03 Oct 2004 07:08:26 +0530
Subject: [R] Reading multiple files into R
In-Reply-To: <415D0F75.7010409@loyaltymatrix.com>
References: <Pine.LNX.4.44.0410010905110.14758-100000@reclus.nhh.no>
	<415D0F75.7010409@loyaltymatrix.com>
Message-ID: <415F5812.5070107@mail.jnu.ac.in>

Thanks Kevin and Roger. This gave me the clue and was a great help.
I have been trying it out. There is some problem in the code that still 
needs to be figured out.

For the first 9 files, paste("wb-0", i, "vc.dbf", sep="") works fine.
But as you rightly guessed, I have more files.
So when I use paste("wb-", formatC(i, width=2, flag="0"), "vc.dbf", 
sep=""), dbf.read does not work.
formatC works find if I use it in cat(paste.................). It 
displays the file names correctly.
But when I use it in dbf.read, it gives the following error.

***********************
res[[i]] <- maptools:::dbf.read(paste("wb-", format(i, width=2,flag=0), 
"vc.dbf", sep=""))
Error in maptools:::dbf.read(paste("wb-", format(i, width=2,flag=0), 
"vc.dbf", sep=""))
Error in maptools:::dbf.read(paste("wb-", format(i, width = 2, flag = 
0),  :
    unable to open DBF file
***********************

Of course, the data files are all right. I can read them individually.

What do you think could be the problem?

Vikas

Kevin Bartz wrote:

> Roger Bivand wrote:
>
>> On Fri, 1 Oct 2004, Vikas Rawal wrote:
>>
>>
>>> I want to read data from a number of files into R.
>>> Reading individual files one by one requires writing enormous amount 
>>> of code that will look something like the following.
>>>
>>> ****************
>>> maptools:::dbf.read("wb-01vc.dbf")->dist1
>>> maptools:::dbf.read("wb-02vc.dbf")->dist2
>>> maptools:::dbf.read("wb-03vc.dbf")->dist3
>>> maptools:::dbf.read("wb-04vc.dbf")->dist4
>>> maptools:::dbf.read("wb-05vc.dbf")->dist5
>>> maptools:::dbf.read("wb-06vc.dbf")->dist6
>>> maptools:::dbf.read("wb-07vc.dbf")->dist7
>>> maptools:::dbf.read("wb-08vc.dbf")->dist8
>>> maptools:::dbf.read("wb-09vc.dbf")->dist9
>>> *****************
>>>
>>
>>
>> In this case, you could pre-allocate a list and:
>>
>> res <- vector(mode="list", length=9)
>> for (i in 1:length(res))     res[[i]] <- 
>> maptools:::dbf.read(paste("wb-0", i, "vc.dbf", sep=""))
>>
>>
>>> res <- vector(mode="list", length=9)
>>> for (i in 1:length(res)) cat(paste("wb-0", i, "vc.dbf", sep=""), "\n")
>>
>>
>> wb-01vc.dbf wb-02vc.dbf wb-03vc.dbf ...
>>
>> gives a check on what file names are being used.
>>
>> For 10 to 99 preserving the 01-09, use paste("wb-", formatC(i, 
>> width=2, flag="0"), "vc.dbf", sep="").
>>
>> If the token is a character (string) that varies, you can roll out a 
>> character vector of tokens first and step along it.
>>
>>
>>> res <- vector(mode="list", length=length(LETTERS))
>>> for (i in 1:length(res)) cat(paste("wb-", LETTERS[i], "vc.dbf", 
>>> sep=""), 
>>
>>
>> + "\n")
>> wb-Avc.dbf wb-Bvc.dbf wb-Cvc.dbf ...
>>
>>
>>
>>> Is there a better way of doing this?
>>>
>>> Vikas
>>>
>>> ______________________________________________
>>> R-help at stat.math.ethz.ch mailing list
>>> https://stat.ethz.ch/mailman/listinfo/r-help
>>> PLEASE do read the posting guide! 
>>> http://www.R-project.org/posting-guide.html
>>>
>>
>>
>
> Good call. Here's a somewhat more R-ified version:
>
> res <- lapply(paste("wb-", formatC(1:99, width=2, flag="0"), "vc.dbf",
>                     sep=""), maptools:::dbf.read)
>
> Kevin
>
>
>



From john.maindonald at anu.edu.au  Sun Oct  3 04:03:40 2004
From: john.maindonald at anu.edu.au (John Maindonald)
Date: Sun, 3 Oct 2004 12:03:40 +1000
Subject: [R] Lattice .ps graphic is rotated in LaTeX slides
In-Reply-To: <200410021004.i92A49XK005678@hypatia.math.ethz.ch>
References: <200410021004.i92A49XK005678@hypatia.math.ethz.ch>
Message-ID: <727395DC-14E0-11D9-BC06-000A95CDA0F2@anu.edu.au>


On 2 Oct 2004, at 8:04 PM, r-help-request at stat.math.ethz.ch wrote:

> On Fri, 2004-10-01 at 10:58, Peter Dalgaard wrote:
>> Michael Friendly <friendly at yorku.ca> writes:
>>
>>> ! Package graphics Error: Division by 0.
>>>
>>> What am I doing wrong, or how could I do it differently so it would 
>>> work?
>>
>> You might try \usepackage{graphicx} instead. I seem to recall
>> (vaguely) getting better results with that sometimes.
>
>
> That should be part of the preamble for using 'seminar', if it is setup
> properly.
>
> There is a decent tutorial for using seminar at:
>
> http://astronomy.sussex.ac.uk/~eddie/soft/tutorial.html
>
> There is also a great reference for including graphics in LaTeX:
>
> www.ctan.org/tex-archive/info/epslatex.pdf
>
> FWIW, though I have been using seminar for such presentations, I have
> been recently looking at the Beamer package:
>
> http://latex-beamer.sourceforge.net/
>
> and of course, there is also the Prosper package:
>
> http://prosper.sourceforge.net/
>
> The one advantage of the Beamer package, for those that require it, is
> that it supports pdflatex, which the others do not. Though, it can be
> used with dvips/latex + ps2pdf, where needed.
>
> HTH,
>
> Marc

Note also the package pdfscreen, for use with pdflatex

www.river-valley.com/download2.shtml

This pretty much uses regular latex, with the page dimensions
changed and the font attributes redefined to make them
larger than usual inside the slide environment.  Be sure to
load the packages xspace and colortbl as well as pdfscreen.

John Maindonald             email: john.maindonald at anu.edu.au
phone : +61 2 (6125)3473    fax  : +61 2(6125)5549
Centre for Bioinformation Science, Room 1194,
John Dedman Mathematical Sciences Building (Building 27)
Australian National University, Canberra ACT 0200.



From vikas at mail.jnu.ac.in  Sun Oct  3 04:28:02 2004
From: vikas at mail.jnu.ac.in (Vikas Rawal)
Date: Sun, 03 Oct 2004 07:58:02 +0530
Subject: [R] Reading multiple files into R
In-Reply-To: <415D0F75.7010409@loyaltymatrix.com>
References: <Pine.LNX.4.44.0410010905110.14758-100000@reclus.nhh.no>
	<415D0F75.7010409@loyaltymatrix.com>
Message-ID: <415F63B2.3010604@mail.jnu.ac.in>


I progressed when I combined the Kevin-Roger method with the 
Jomes-holtman method. sprintf() in place of formatC did the trick. 
Holtman's method does not work because of some problem with the assign. 
It seems you cannot have a variable target of the assignment.

Now I have a vector of lists res. I would like to append all these 
components into one single dataframe.
I tried the following:

rbind(for (i in 1:17) res[[i]]) -> distvc

But this will not work. It works if I individually specify all the res 
components.

Vikas

***************
Holtman's method:

for (i in 1:30){
      assign(paste('dist', i, sep=''),
maptools:::dbf.read(sprintf("wb-%02dvc.dbf",i)))
}



Roger's method

>>
>> res <- vector(mode="list", length=9)
>> for (i in 1:length(res))     res[[i]] <- 
>> maptools:::dbf.read(paste("wb-0", i, "vc.dbf", sep=""))
>>
>>
>>> res <- vector(mode="list", length=9)
>>> for (i in 1:length(res)) cat(paste("wb-0", i, "vc.dbf", sep=""), "\n")
>>
>>
>> wb-01vc.dbf wb-02vc.dbf wb-03vc.dbf ...
>>
>> gives a check on what file names are being used.
>>
>> For 10 to 99 preserving the 01-09, use paste("wb-", formatC(i, 
>> width=2, flag="0"), "vc.dbf", sep="").
>>
>> If the token is a character (string) that varies, you can roll out a 
>> character vector of tokens first and step along it.
>>
>>
>>> res <- vector(mode="list", length=length(LETTERS))
>>> for (i in 1:length(res)) cat(paste("wb-", LETTERS[i], "vc.dbf", 
>>> sep=""), 
>>
>>
>> + "\n")
>> wb-Avc.dbf wb-Bvc.dbf wb-Cvc.dbf ...
>>
>>



From ggrothendieck at myway.com  Sun Oct  3 08:05:16 2004
From: ggrothendieck at myway.com (Gabor Grothendieck)
Date: Sun, 3 Oct 2004 06:05:16 +0000 (UTC)
Subject: [R] conditional assignments and calculations
References: <415EF8F0.7010108@eva.mpg.de>
Message-ID: <loom.20041003T074007-245@post.gmane.org>

Michael Lachmann <lachmann <at> eva.mpg.de> writes:

: 
: Hello!
: 
: I am using the TeXmacs interface to R. (Though I encountered a similar 
: problem when using Sweave)
: In doing calculations I often ecounter this scenario: I'll have some 
: calculations in my file:
: --
: A=read.lots.of.data()
: 
: B=huge.calculation.on(A)
: 
: C=another.calculation.on(B)
: --
: Now, if A has already been read, I don't need to re-read it. If B has 
: already been calculated, I don't need to recalculate it. But I would 
: like to be able to just press 'enter' on each of them.
: 
: So, I would like R to somehow figure out dependencies (a bit like in 
: Makefiles)
: 
: I implemented something like this with the following functions:
: ----------------------
: touch=function(x) {attr(x,"last.updated")=Sys.time();x}
: 
: last.updated=function(a) {
:    if( length(attr(a,"last.updated")) == 0 ) {
:    	Sys.time()
:    } else {
:        attr(a,"last.updated")
:    }
: }
: 
: "depends<-"=function(a,value,...) {
:    args=list(...)
:    if( length(attr(a,"last.updated")) == 0 ) {
:      a <- value
:      a <-touch(a)
:    } else {
:      lu=(sapply(args,function(x) last.updated(x)-last.updated(a) > 0 ))
:      if( sum(lu)>0 ) {
:         a <- value
:         a <-touch(a)
:      }
:    }
:    a
: }
: ------------------------
: Then I can implement what I wanted above as follows:
: --
: if( !exists(A) ) { A=read.lots.of.data(); A=touch(A) }
: 
: depends(B,A)=huge.calculation.on(A)
: # this means the assignment 'B=huge.calculation.on(A)' is
: # done only if A has been updated more recently than B.
: 
: depends(C,B)=another.calculation.on(B)
: # dito for C more recent than B.
: --
: And now I can carelessly press 'enter' on these expression that might 
: otherwise take hours to compute. Each variable has a datestamp of the 
: last time it was updated, and I do each calculation conditional on 
: whether certain variables have been recently changed. I can also save 
: A,B,C to a file,later load them, and the calculations will not be redone.
: 
: But this solution is quite ugly, because of several problems:
: 
: 1. To call 'depends(A,B)=f(B)' the first time, A has to already exist, 
: otherwise I get an error (before I enter the "depends<-" function.)

The technique used to implement mulitple return values shown in 

   http://tolstoy.newcastle.edu.au/R/help/04/06/1406.html

could be adapted to this problem.  Using that technique the code would 
look like this:

   depends[A,B] <- f(B)

and A would not have to pre-exist.

You define a structure with a class of depends, say:

   depends <- structure(NA, class = "depends")

and then define the [<-.depends action on that structure
in an analogous way to what was done there.

: 2. I would also like to have a convenient way to do
: "if( !exists(A) ) { A=read.lots.of.data(); A=touch(A) }"
: maybe something like:
: depends(A)<-read.lots.of.data()
: But that doesn't work, because of 1.
: or
: A %set% read.lots.data()
: But that doesn't work, because I haven't figured out a way for a 
: function to change one of its variables.
: (Maybe I could do A=A %set read.lots.of.data(), but that is really ugly...)


Is this what you want?

R> f <- function(x,v) assign(as.character(substitute(x)), v, parent.frame())
R> x # x does not exist
Error: Object "x" not found
R> f(x,3)
R> x # now it does
[1] 3

<<- can be used if the   eval.parent is a third way (see #3 below).

: 3. It would be nice to be able to do touch(A) instead of A=touch(A)

touch <- function(x) 
    eval.parent(substitute(attr(x,"last.updated")<-Sys.time())) 
x <- 3
touch(x)

: 
: 4. If I modify A without calling 'A=touch(A)', then B will not be 
: updated next time I call 'depends(B,A)=huge.calculation.on(A)'. So it 
: would be nice to have the variable's 'last updated' time updated 
: automatically. (Though then it is a bit problematic to decide what the 
: 'last updated' time should be for variables loaded from a file...)

If its done in a function you could use on.exit to ensure that it gets
updated when leaving the function.

: 
: 5. The whole thing is rather cludgy. But I haven't found a good way to 
: implement it.
: 
: Suggestions?
: 
: Thanks,
: 
:     Michael
:



From josh8912 at yahoo.com  Sun Oct  3 09:18:46 2004
From: josh8912 at yahoo.com (J)
Date: Sun, 3 Oct 2004 00:18:46 -0700 (PDT)
Subject: [R] creating new varFunc classes in nlme .. error: "Don't know how
	to get coefficients for .. object"
Message-ID: <20041003071846.53828.qmail@web51709.mail.yahoo.com>

Hello.  I am trying my hand at modifying the varFunc
class varExp, but I must be missing a step.  All I
want to do right now is make a working copy of varExp,
call it varExp2, and then later change it. 
coef.varExp2, coef<-.varExp2, and Initialize.varExp2
all seem to work properly after I construct them.  I
can successfully use the commands:

v2 <- varExp2(form = ~age|Sex,fixed = c(Female=0))
v2 <- Initialize(v2, Orthodont)

But, after this when I type "v2" at the prompt, I get
the message: 

Error in coef.varFunc(x, uncons = FALSE, allCoef =
TRUE) : Don't know how to get coefficients for varExp2
object

Im not sure what to do.  Im sure it is a simple fix or
statement I need to enter.  Can anyone offer
suggestions?  Do I have to use the command
varFunc(varExp2) at some point?

As background, I created VarExp2 by using:

varExp2 <- function (value = numeric(0), ... [and the
rest of the VarExp function] ...)  At the end of the
function I had to change the statement: c("varExp",...
to c("varExp2...

Then I used:

setMethod("Initialize","varExp2", function (object,
data, ...)
{
    form <- formula(object)
    ... [and the rest of the Initialize.varExp
function] ...)

I did the same with the coef and coef<- functions.  

Im not sure why coef, coef<-, and Initialize seem to
work (they produce the same output and attributes as
varExp), but still I get the error message.  Would it
have anything to do with the warning I get when I
create the coef.varExp2 and other functions:

In the method signature for function "coef<-", class
"varExp2" has no current definition in:
matchSignature(signature, fdef, where)

Any help would be greatly appreciated.  Do I somehow
need to tell nlme where to find my new functions?  John



From Roger.Bivand at nhh.no  Sun Oct  3 14:17:02 2004
From: Roger.Bivand at nhh.no (Roger Bivand)
Date: Sun, 3 Oct 2004 14:17:02 +0200 (CEST)
Subject: [R] Reading multiple files into R
In-Reply-To: <415F5812.5070107@mail.jnu.ac.in>
Message-ID: <Pine.LNX.4.44.0410031353240.27177-100000@reclus.nhh.no>

On Sun, 3 Oct 2004, Vikas Rawal wrote:

> Thanks Kevin and Roger. This gave me the clue and was a great help.
> I have been trying it out. There is some problem in the code that still 
> needs to be figured out.
> 
> For the first 9 files, paste("wb-0", i, "vc.dbf", sep="") works fine.
> But as you rightly guessed, I have more files.
> So when I use paste("wb-", formatC(i, width=2, flag="0"), "vc.dbf", 
> sep=""), dbf.read does not work.
> formatC works find if I use it in cat(paste.................). It 
> displays the file names correctly.
> But when I use it in dbf.read, it gives the following error.
> 
> ***********************
> res[[i]] <- maptools:::dbf.read(paste("wb-", format(i, width=2,flag=0), 
> "vc.dbf", sep=""))
> Error in maptools:::dbf.read(paste("wb-", format(i, width=2,flag=0), 
> "vc.dbf", sep=""))
> Error in maptools:::dbf.read(paste("wb-", format(i, width = 2, flag = 
> 0),  :
>     unable to open DBF file
> ***********************
> 
> Of course, the data files are all right. I can read them individually.
> 
> What do you think could be the problem?

Look for the difference between:

> for(i in 1:12) cat(paste("wb-", format(i, width=2,flag=0),"vc.dbf", 
+ sep=""), "\n")

and 

> for(i in 1:12) cat(paste("wb-", formatC(i, width=2,flag=0),"vc.dbf", 
+ sep=""), "\n")

You need formatC(), not format(). sprintf() is:

> for(i in 1:12) cat(paste("wb-", sprintf(fmt="%0.2d", i),"vc.dbf", 
+ sep=""), "\n")

for the same as formatC().

On the rbind question:

> Now I have a vector of lists res. I would like to append all these 
> components into one single dataframe.
> I tried the following:

> rbind(for (i in 1:17) res[[i]]) -> distvc

> But this will not work. It works if I individually specify all the res 
> components.


this works:

> xx <- list(df1=data.frame(x=rnorm(10), y=rnorm(10), f=rep("A", 10)), 
+ df2=data.frame(x=rnorm(10), y=rnorm(10), f=rep("B", 10)))
> xxx <- NULL
> for(i in 1:length(xx)) xxx <- rbind(xxx, xx[[i]])

for() loops are not a bad thing if you are not repeating the operation 
(like reading in data) very frequently, and seem to me easier to debug 
than more sophisticated constructions. This for() loop will run slower as 
xxx grows, because it needs to re-allocate memory each time round. I would 
be tempted for many and large xx[[i]] to pre-allocate the combined data 
frame and just slot in the rows for each list component, if I knew that 
the numbers and classes og the columns were identical. But rbind() is 
cleaner, even though it will be slower - again, if you only need this a 
few times, the time hit is compensated for by simplicity.

> 
> Vikas
> 
> Kevin Bartz wrote:
> 
> > Roger Bivand wrote:
> >
> >> On Fri, 1 Oct 2004, Vikas Rawal wrote:
> >>
> >>
> >>> I want to read data from a number of files into R.
> >>> Reading individual files one by one requires writing enormous amount 
> >>> of code that will look something like the following.
> >>>
> >>> ****************
> >>> maptools:::dbf.read("wb-01vc.dbf")->dist1
> >>> maptools:::dbf.read("wb-02vc.dbf")->dist2
> >>> maptools:::dbf.read("wb-03vc.dbf")->dist3
> >>> maptools:::dbf.read("wb-04vc.dbf")->dist4
> >>> maptools:::dbf.read("wb-05vc.dbf")->dist5
> >>> maptools:::dbf.read("wb-06vc.dbf")->dist6
> >>> maptools:::dbf.read("wb-07vc.dbf")->dist7
> >>> maptools:::dbf.read("wb-08vc.dbf")->dist8
> >>> maptools:::dbf.read("wb-09vc.dbf")->dist9
> >>> *****************
> >>>
> >>
> >>
> >> In this case, you could pre-allocate a list and:
> >>
> >> res <- vector(mode="list", length=9)
> >> for (i in 1:length(res))     res[[i]] <- 
> >> maptools:::dbf.read(paste("wb-0", i, "vc.dbf", sep=""))
> >>
> >>
> >>> res <- vector(mode="list", length=9)
> >>> for (i in 1:length(res)) cat(paste("wb-0", i, "vc.dbf", sep=""), "\n")
> >>
> >>
> >> wb-01vc.dbf wb-02vc.dbf wb-03vc.dbf ...
> >>
> >> gives a check on what file names are being used.
> >>
> >> For 10 to 99 preserving the 01-09, use paste("wb-", formatC(i, 
> >> width=2, flag="0"), "vc.dbf", sep="").
> >>
> >> If the token is a character (string) that varies, you can roll out a 
> >> character vector of tokens first and step along it.
> >>
> >>
> >>> res <- vector(mode="list", length=length(LETTERS))
> >>> for (i in 1:length(res)) cat(paste("wb-", LETTERS[i], "vc.dbf", 
> >>> sep=""), 
> >>
> >>
> >> + "\n")
> >> wb-Avc.dbf wb-Bvc.dbf wb-Cvc.dbf ...
> >>
> >>
> >>
> >>> Is there a better way of doing this?
> >>>
> >>> Vikas
> >>>
> >>> ______________________________________________
> >>> R-help at stat.math.ethz.ch mailing list
> >>> https://stat.ethz.ch/mailman/listinfo/r-help
> >>> PLEASE do read the posting guide! 
> >>> http://www.R-project.org/posting-guide.html
> >>>
> >>
> >>
> >
> > Good call. Here's a somewhat more R-ified version:
> >
> > res <- lapply(paste("wb-", formatC(1:99, width=2, flag="0"), "vc.dbf",
> >                     sep=""), maptools:::dbf.read)
> >
> > Kevin
> >
> >
> >
> 
> 
> 

-- 
Roger Bivand
Economic Geography Section, Department of Economics, Norwegian School of
Economics and Business Administration, Breiviksveien 40, N-5045 Bergen,
Norway. voice: +47 55 95 93 55; fax +47 55 95 93 93
e-mail: Roger.Bivand at nhh.no



From josh8912 at yahoo.com  Sun Oct  3 14:42:11 2004
From: josh8912 at yahoo.com (J)
Date: Sun, 3 Oct 2004 05:42:11 -0700 (PDT)
Subject: [R] Re: creating new varFunc classes in nlme .. error: "Don't know
	how to get coefficients for .. object" 
In-Reply-To: <200410031008.i93A68qU008389@hypatia.math.ethz.ch>
Message-ID: <20041003124211.96564.qmail@web51702.mail.yahoo.com>

Ah!! After way too many hours of work Ive answered my
own question.  To get varExp2, a copy of varExp, to
work as a new varFunc, one needs to use this sort of
syntax:

update.varExp2 <- function (object, data, ...)
{
    print ("enter update")
    val <- NextMethod()
    if (length(val) == 0) {
        aux <- coef(val, allCoef = TRUE)
        if (!is.null(grps <- getGroups(val))) {
            aux <- aux[grps]
        }
        attr(val, "logLik") <- sum(log(attr(val,
"weights") <- exp(-aux *
            getCovariate(val))))
    }
    val
}
setMethod("update", "varExp2", update.varExp2)

And then do the same for all methods used by varExp
(see methods(class="varExp")).  When writing the
setMethod for coef<-.varExp2, use
cat("coef<-.varExp2") in the statement.  Or at least
that worked for me.

But I still have an unanswered question from the other
day (see [R] two questions on nlme: error messages and
nested variance).  What is the best way in nlme to
model the variance when the variance of the variance
is not constant but is dependent on a covariate?  

Thanks.  John



From l.collins at topotarget.co.uk  Sun Oct  3 14:56:55 2004
From: l.collins at topotarget.co.uk (Laura Collins)
Date: Sun, 3 Oct 2004 13:56:55 +0100
Subject: [R] Excluding data in R
Message-ID: <063E4E59A569A8459DF69577BA65CA0C21E807@isaac.topotarget.uk>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20041003/d1798bf2/attachment.pl

From Kevin.Wang at maths.anu.edu.au  Sun Oct  3 15:09:50 2004
From: Kevin.Wang at maths.anu.edu.au (Kevin Wang)
Date: Sun, 3 Oct 2004 23:09:50 +1000 (EST)
Subject: [R] Excluding data in R
In-Reply-To: <063E4E59A569A8459DF69577BA65CA0C21E807@isaac.topotarget.uk>
References: <063E4E59A569A8459DF69577BA65CA0C21E807@isaac.topotarget.uk>
Message-ID: <Pine.GSO.4.58.0410032308520.7877@yin>

Hi,

On Sun, 3 Oct 2004, Laura Collins wrote:

> How do you exclude outliers from a set of data?

Your question is too vague.  I'm assuming you have a data frame and
already know exactly which observations are the outlier(s).  If your data
frame is called foo.df, and say observation 5 is an outlier, then
something like:
  foo.df[-5, ]
will exclude it.

Kevin

--------------------------------
Ko-Kang Kevin Wang
PhD Student
Centre for Mathematics and its Applications
Building 27, Room 1004
Mathematical Sciences Institute (MSI)
Australian National University
Canberra, ACT 0200
Australia

Homepage: http://wwwmaths.anu.edu.au/~wangk/
Ph (W): +61-2-6125-2431
Ph (H): +61-2-6125-7407
Ph (M): +61-40-451-8301



From l.collins at topotarget.co.uk  Sun Oct  3 15:31:44 2004
From: l.collins at topotarget.co.uk (Laura Collins)
Date: Sun, 3 Oct 2004 14:31:44 +0100
Subject: [R] If loops
Message-ID: <063E4E59A569A8459DF69577BA65CA0C239463@isaac.topotarget.uk>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20041003/bf2ebbc8/attachment.pl

From ligges at statistik.uni-dortmund.de  Sun Oct  3 15:45:12 2004
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Sun, 03 Oct 2004 15:45:12 +0200
Subject: [R] If loops
In-Reply-To: <063E4E59A569A8459DF69577BA65CA0C239463@isaac.topotarget.uk>
References: <063E4E59A569A8459DF69577BA65CA0C239463@isaac.topotarget.uk>
Message-ID: <41600268.4010608@statistik.uni-dortmund.de>

Laura Collins wrote:

> Hi,
> 
>  
> 
> I'm a complete beginner to all this so I was hoping someone could help
> me!
> 
>  
> 
> What I'm trying to do is to write a function that returns the
> coordinates where a vector x is equal to a.  So say I invent a vector x:
> 
> 
> x<-c(,5,8,9,8,3).
> 
> If a is a<-8.
> 
>  
> 
> I want the function to return the coordinates of x where the number 8
> appears (i.e. 2 4).
> 
> I know I need to set up an if loop but I'm really not sure how to do
> this.
> 
>  
> 
> Any advice or clues will be much appreciated.




which(a==x)

Please read "An Introduction to R", the R FAQ and learn to use the help 
system as well as the mailing list archives (as described in the Posting 
Guide cited below at the end of your).


Uwe Ligges


>  
> 
> Thanks,
> 
>  
> 
> Laura
> 
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html



From andy_liaw at merck.com  Sun Oct  3 15:46:45 2004
From: andy_liaw at merck.com (Liaw, Andy)
Date: Sun, 3 Oct 2004 09:46:45 -0400
Subject: [R] If loops
Message-ID: <3A822319EB35174CA3714066D590DCD504AF84AE@usrymx25.merck.com>

> From: Laura Collins
> 
> Hi,
> 
> I'm a complete beginner to all this so I was hoping someone could help
> me!
> 
> What I'm trying to do is to write a function that returns the
> coordinates where a vector x is equal to a.  So say I invent 
> a vector x:
> 
> x<-c(,5,8,9,8,3).
> 
> If a is a<-8.
> 
> I want the function to return the coordinates of x where the number 8
> appears (i.e. 2 4).
> 
> I know I need to set up an if loop but I'm really not sure how to do
> this.

You don't if you use R (or S-PLUS):

which(x %in% a)

or even something like

x.good <- x[! x %in% a]

HTH,
Andy

> Any advice or clues will be much appreciated.
> 
> Thanks,
> Laura



From jfox at mcmaster.ca  Sun Oct  3 16:45:51 2004
From: jfox at mcmaster.ca (John Fox)
Date: Sun, 3 Oct 2004 10:45:51 -0400
Subject: [R] Excluding data in R
In-Reply-To: <063E4E59A569A8459DF69577BA65CA0C21E807@isaac.topotarget.uk>
Message-ID: <20041003144547.YOJG4905.tomts13-srv.bellnexxia.net@JohnDesktop8300>

Dear Laura, 

> -----Original Message-----
> From: r-help-bounces at stat.math.ethz.ch 
> [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Laura Collins
> Sent: Sunday, October 03, 2004 7:57 AM
> To: R-help at stat.math.ethz.ch
> Subject: [R] Excluding data in R
> 
> I was hoping someone could help me!
> 
> How do you exclude outliers from a set of data?  
> 

It depends upon the context, but it could be as simple as using a negative
subscript; for example x[-c(5, 9)] removes the fifth and ninth element from
the vector x.

This kind of information is available in every introduction to R that I've
seen, including the introductory manual that comes with R.

I hope that this helps,
 John



From jfox at mcmaster.ca  Sun Oct  3 16:50:09 2004
From: jfox at mcmaster.ca (John Fox)
Date: Sun, 3 Oct 2004 10:50:09 -0400
Subject: [R] If loops
In-Reply-To: <063E4E59A569A8459DF69577BA65CA0C239463@isaac.topotarget.uk>
Message-ID: <20041003145004.VLFK1968.tomts20-srv.bellnexxia.net@JohnDesktop8300>

Dear Laura, 

> -----Original Message-----
> From: r-help-bounces at stat.math.ethz.ch 
> [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Laura Collins
> Sent: Sunday, October 03, 2004 8:32 AM
> To: R-help at stat.math.ethz.ch
> Subject: [R] If loops
> 
> Hi,
> 
>  
> 
> I'm a complete beginner to all this so I was hoping someone 
> could help me!
> 
>  
> 
> What I'm trying to do is to write a function that returns the 
> coordinates where a vector x is equal to a.  So say I invent 
> a vector x:
> 
> 
> x<-c(,5,8,9,8,3).
> 
> If a is a<-8.
> 
>  
> 
> I want the function to return the coordinates of x where the 
> number 8 appears (i.e. 2 4).
> 
> I know I need to set up an if loop but I'm really not sure 
> how to do this.
> 
>  
> 
> Any advice or clues will be much appreciated.
> 

> which(x == 8)
[1] 2 4

See ?which.

Note that your definition of x is in error (try it) and that <- in R means
assignment, not equality.

John



From ripley at stats.ox.ac.uk  Sun Oct  3 19:30:35 2004
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Sun, 3 Oct 2004 18:30:35 +0100 (BST)
Subject: [R] Re: creating new varFunc classes in nlme .. error: "Don't
	know how to get coefficients for .. object" 
In-Reply-To: <20041003124211.96564.qmail@web51702.mail.yahoo.com>
Message-ID: <Pine.LNX.4.44.0410031827410.13439-100000@gannet.stats>

On Sun, 3 Oct 2004, J wrote:

> Ah!! After way too many hours of work Ive answered my
> own question.  To get varExp2, a copy of varExp, to
> work as a new varFunc, one needs to use this sort of
> syntax:
> 
> update.varExp2 <- function (object, data, ...)
> {
>     print ("enter update")
>     val <- NextMethod()
>     if (length(val) == 0) {
>         aux <- coef(val, allCoef = TRUE)
>         if (!is.null(grps <- getGroups(val))) {
>             aux <- aux[grps]
>         }
>         attr(val, "logLik") <- sum(log(attr(val,
> "weights") <- exp(-aux *
>             getCovariate(val))))
>     }
>     val
> }
> setMethod("update", "varExp2", update.varExp2)

update() is an S3 generic and nlme uses S3 classes, so this is definitely 
wrong.  Have you been using S4 classes without mentioning it?

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From friedm69 at msu.edu  Mon Oct  4 00:08:57 2004
From: friedm69 at msu.edu (Steven K Friedman)
Date: Sun, 03 Oct 2004 18:08:57 -0400
Subject: [R] help working with persp plot function
Message-ID: <E1CEEXJ-00089Z-KP@sys19.mail.msu.edu>


Hi, 

I don't understand why this is not working.  Help is appreciated. 

I need to plot the following as a surface, but persp returns an error. 

tpcp_xy[1:10,]
   X_COORD  Y_COORD TPCP
1  465459.7 175924.1 0.85
2  466145.8 324017.3 2.30
3  467720.2 372143.1 1.56
4  470293.2 348064.8 2.87
5  476566.9 205501.8 0.94
6  477774.9 142561.0 1.31
7  479207.0 162919.6 3.04
8  480890.8 290641.3 2.20
9  488865.9 159201.4 2.30
10 490328.3 248049.0 2.81 


Note that the X_COORD column is sorted in an increasing order. 

> objects(2)
[1] "TPCP"    "X_COORD" "Y_COORD"
> persp(X_COORD, Y_COORD, TPCP)
Error in persp.default(X_COORD, Y_COORD, TPCP) :
       increasing x and y values expected 

Ok so why is the function returning an error message? 

Thanks for your time and insights. 

Steve Friedman           email friedm69 at msu.edu
Department of Forestry
Michigan State University



From andrewr at uidaho.edu  Mon Oct  4 00:23:02 2004
From: andrewr at uidaho.edu (Andrew Robinson)
Date: Mon, 4 Oct 2004 08:23:02 +1000
Subject: [R] help working with persp plot function
In-Reply-To: <E1CEEXJ-00089Z-KP@sys19.mail.msu.edu>
References: <E1CEEXJ-00089Z-KP@sys19.mail.msu.edu>
Message-ID: <20041003222302.GU57053@uidaho.edu>

Hi Steve,

persp() is looking for x and y to be the coordinates on the axes
rather than corresponding directly to the points. So, something like

x <- c(1:3)
y <- c(1:3)

z <- c(1:9)

Take a look at the third example in help(persp) to clarify.

For your data you might try the surface() function in the fields
package - see .e.g

http://www.cgd.ucar.edu/stats/Software/Fields/fields.demo.shtml

Good luck!

Andrew

On Sun, Oct 03, 2004 at 06:08:57PM -0400, Steven K Friedman wrote:
> 
> Hi, 
> 
> I don't understand why this is not working.  Help is appreciated. 
> 
> I need to plot the following as a surface, but persp returns an error. 
> 
> tpcp_xy[1:10,]
>   X_COORD  Y_COORD TPCP
> 1  465459.7 175924.1 0.85
> 2  466145.8 324017.3 2.30
> 3  467720.2 372143.1 1.56
> 4  470293.2 348064.8 2.87
> 5  476566.9 205501.8 0.94
> 6  477774.9 142561.0 1.31
> 7  479207.0 162919.6 3.04
> 8  480890.8 290641.3 2.20
> 9  488865.9 159201.4 2.30
> 10 490328.3 248049.0 2.81 
> 
> 
> Note that the X_COORD column is sorted in an increasing order. 
> 
> >objects(2)
> [1] "TPCP"    "X_COORD" "Y_COORD"
> >persp(X_COORD, Y_COORD, TPCP)
> Error in persp.default(X_COORD, Y_COORD, TPCP) :
>       increasing x and y values expected 
> 
> Ok so why is the function returning an error message? 
> 
> Thanks for your time and insights. 
> 
> Steve Friedman           email friedm69 at msu.edu
> Department of Forestry
> Michigan State University
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html

-- 
Andrew Robinson                      Ph: 208 885 7115
Department of Forest Resources       Fa: 208 885 6226
University of Idaho                  E : andrewr at uidaho.edu
PO Box 441133                        W : http://www.uidaho.edu/~andrewr
Moscow ID 83843                      Or: http://www.biometrics.uidaho.edu
No statement above necessarily represents my employer's opinion.



From jinss at hkusua.hku.hk  Mon Oct  4 04:25:02 2004
From: jinss at hkusua.hku.hk (Jin Shusong)
Date: Mon, 4 Oct 2004 10:25:02 +0800
Subject: [R] compile R-1.9.1 with Portland fortran
Message-ID: <20041004022502.GA23477@S77.localdomain>

Dear R-users,

  Has any one compiled R-1.9.1  successfully with Portland
pgcc pgCC and pgf77?  I can not configurate it with
./configure since it told me 
    checking for Fortran name-mangling scheme... configure:
    error: cannot compile a simple Fortran program
  Can any one give me some advice.  Thank you.

  My platform: Linux-2.4.26, pentium4, portland compiler 5.1 

                                 Jin



From fhduan at gmail.com  Mon Oct  4 04:57:47 2004
From: fhduan at gmail.com (Frank Duan)
Date: Sun, 3 Oct 2004 22:57:47 -0400
Subject: [R] Could anyone tell me how to extract pvalue from "lm" fitting?
Message-ID: <3b917231041003195728a9f7b6@mail.gmail.com>

Dear R people,

I have a naive question: after fitting "lm" to a data, I can't extract
the pvalue corresponding to a specific covariate in a direct way.
Could anyone give me a hint?

Thank you very much.

Frank



From kjetil at acelerate.com  Mon Oct  4 04:26:21 2004
From: kjetil at acelerate.com (Kjetil Brinchmann Halvorsen)
Date: Sun, 03 Oct 2004 22:26:21 -0400
Subject: [R] Off-Topic: LaTeX package listings
Message-ID: <4160B4CD.9090909@acelerate.com>

Hola!

I ask here since I learnt from this list that the LaTeX package listings 
should be good
for typesetting R code. I encountered one problem:
\begin{lstlisting}
      X %*% V
\end{lstlisting}

in the output the * in %*% disappears! same with %/%, etc, the /
disappears.

Any ideas?

Kjetil

-- 

Kjetil Halvorsen.

Peace is the most effective weapon of mass construction.
               --  Mahdi Elmandjra



From andrewr at uidaho.edu  Mon Oct  4 05:18:49 2004
From: andrewr at uidaho.edu (Andrew Robinson)
Date: Mon, 4 Oct 2004 13:18:49 +1000
Subject: [R] Could anyone tell me how to extract pvalue from "lm" fitting?
In-Reply-To: <3b917231041003195728a9f7b6@mail.gmail.com>
References: <3b917231041003195728a9f7b6@mail.gmail.com>
Message-ID: <20041004031849.GA57053@uidaho.edu>

Frank,

try the following, not all of which are directly relevant, but which
explain the approach. If your lm is called "my.lm":

names(summary(my.lm))
coefficients(summary(my.lm))
class(coefficients(summary(my.lm)))
coefficients(summary(my.lm))[,4]

I hope that this helps.

Andrew

On Sun, Oct 03, 2004 at 10:57:47PM -0400, Frank Duan wrote:
> Dear R people,
> 
> I have a naive question: after fitting "lm" to a data, I can't extract
> the pvalue corresponding to a specific covariate in a direct way.
> Could anyone give me a hint?
> 
> Thank you very much.
> 
> Frank
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html

-- 
Andrew Robinson                      Ph: 208 885 7115
Department of Forest Resources       Fa: 208 885 6226
University of Idaho                  E : andrewr at uidaho.edu
PO Box 441133                        W : http://www.uidaho.edu/~andrewr
Moscow ID 83843                      Or: http://www.biometrics.uidaho.edu
No statement above necessarily represents my employer's opinion.



From MSchwartz at MedAnalytics.com  Mon Oct  4 05:37:32 2004
From: MSchwartz at MedAnalytics.com (Marc Schwartz)
Date: Sun, 03 Oct 2004 22:37:32 -0500
Subject: [R] Off-Topic: LaTeX package listings
In-Reply-To: <4160B4CD.9090909@acelerate.com>
References: <4160B4CD.9090909@acelerate.com>
Message-ID: <1096861052.4471.19.camel@localhost.localdomain>

On Sun, 2004-10-03 at 21:26, Kjetil Brinchmann Halvorsen wrote:
> Hola!
> 
> I ask here since I learnt from this list that the LaTeX package listings 
> should be good
> for typesetting R code. I encountered one problem:
> \begin{lstlisting}
>       X %*% V
> \end{lstlisting}
> 
> in the output the * in %*% disappears! same with %/%, etc, the /
> disappears.
> 
> Any ideas?
> 
> Kjetil


That's because the "%" is a comment character in LaTeX. Thus, anything
after it will be ignored.

For program code, you generally want to use the 'verbatim' or
'smallverbatim' environment:

\begin{verbatim}
  X %*% V
\end{verbatim}

In the verbatim environment, all characters are treated literally,
rather than interpreted by any special meaning.

Outside of that, say in a regular LaTeX document, you can escape the
"%":

\%

HTH,

Marc Schwartz



From montpied at nancy.inra.fr  Mon Oct  4 09:24:50 2004
From: montpied at nancy.inra.fr (Pierre MONTPIED)
Date: Mon, 04 Oct 2004 09:24:50 +0200
Subject: [R] gnls or nlme : how to obtain confidence intervals of fitted
	values
In-Reply-To: <415EC6E6.8070600@pdf.com>
References: <415D0BE8.9050104@nancy.inra.fr> <415EC6E6.8070600@pdf.com>
Message-ID: <4160FAC2.1090509@nancy.inra.fr>

Thanks Spencer but the intervals function gives confidence intervals of 
the parameters of the model not the predicted values. In the Soybean 
example it would be the CI of predicted weight for a given time, knowing 
all the parameters (Asym, xmid, scal, variance function and residual) 
and their distributions.

And what I need to calculate is precisely this CI.

My question is therefore is there an analytical way to calculate such 
CI, whatever the model, or could I try some randomizing techniques such 
as bootstrap or other ?

Thanks.


Spencer Graves wrote:

>      Pinhiero and Bates (2000) Mixed-Effects Models in S and S-Plus 
> (Springer) describe the use of "intervals" for that.  In 
> library(nlme), R 1.9.1 for Windows, I found documentation for 
> intervals, intervals.gls, intervals.lme, intervals.lmList, and gnls.  
> To an example in the documentation for gnls, I added intervals:
> >      data(Soybean)
> >      # variance increases with a power of the absolute fitted values
> >      fm1 <- gnls(weight ~ SSlogis(Time, Asym, xmid, scal), Soybean,
> +                  weights = varPower())
> >      summary(fm1)
> Generalized nonlinear least squares fit
>  Model: weight ~ SSlogis(Time, Asym, xmid, scal)
>  Data: Soybean
>       AIC      BIC    logLik
>  983.7947 1003.900 -486.8974
>
> Variance function:
> Structure: Power of variance covariate
> Formula: ~fitted(.)
> Parameter estimates:
>    power
> 0.8815437
>
> Coefficients:
>        Value Std.Error  t-value p-value
> Asym 17.35682 0.5226885 33.20682       0
> xmid 51.87232 0.5916820 87.66925       0
> scal  7.62053 0.1390958 54.78617       0
>
> Correlation:
>     Asym  xmid
> xmid 0.787     scal 0.485 0.842
>
> Standardized residuals:
>         Min           Q1          Med           Q3          Max
> -2.309670367 -0.646844555 -0.004897024  0.498606088  4.986727281
>
> Residual standard error: 0.3662752
> Degrees of freedom: 412 total; 409 residual
> > intervals(fm1)
> Approximate 95% confidence intervals
>
> Coefficients:
>         lower      est.     upper
> Asym 16.329331 17.356822 18.384313
> xmid 50.709200 51.872317 53.035434
> scal  7.347094  7.620525  7.893957
> attr(,"label")
> [1] "Coefficients:"
>
> Variance function:
>          lower      est.    upper
> power 0.8369085 0.8815437 0.926179
> attr(,"label")
> [1] "Variance function:"
>
> Residual standard error:
>    lower      est.     upper
> 0.3373374 0.3649392 0.3947995
>
>      Is this what you want?      hope this helps.      spencer graves
>
> Pierre MONTPIED wrote:
>
>> Hi
>>
>> I use gnls to fit non linear models of the form y = alpha * x**beta 
>> (alpha and beta being linear functions of a 2nd regressor z i.e. 
>> alpha=a1+a2*z and beta=b1+b2*z) with variance function 
>> varPower(fitted(.)) which sounds correct for the data set I use.
>>
>> My purpose is to use the fitted models for predictions with other 
>> sets of regressors x, z than those used in fitting. I therefore need 
>> to estimate y with (95%) confidence intervals.
>>
>> Does any body knows how to do this with R ?
>>
>> Thanks
>>
>> ______________________________________________
>> R-help at stat.math.ethz.ch mailing list
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide! 
>> http://www.R-project.org/posting-guide.html
>
>
>



From jarioksa at sun3.oulu.fi  Mon Oct  4 10:19:30 2004
From: jarioksa at sun3.oulu.fi (Jari Oksanen)
Date: 04 Oct 2004 11:19:30 +0300
Subject: [R] biplot.princomp with loadings only
In-Reply-To: <415BB6BC.6070307@gmx.ch>
References: <415BB6BC.6070307@gmx.ch>
Message-ID: <1096877970.3728.9.camel@biol102145.oulu.fi>

On Thu, 2004-09-30 at 10:33, Christoph Lehmann wrote:
> Hi
> 
> is there a way to plot only the loadings in a biplot (with the nice 
> arrows), and to skip the scores?
> 
Christoph,

I may have overlooked some email messages, but it seems to me that you
haven't yet got an answer to your practical question. From the practical
point of view, we may skip the point that you rather ask for a
"monoplot" than "biplot" if you have only one set of points. Further, I
may forget my surprise when I see that somebody really thinks that these
arrows are "nice". OK, they may be nice if you have only a couple of
them, but anybody plotting 30 or more arrows normally asks how to get
rid off this mess. 

Of course you can plot arrows in your "monoplot", since you have got
access to everything in R and you can do anything with R (but coffee
comes somewhat bland, so I recommend something else for the task cooking
coffee). Here is an example:

# Run PCA
data(USArrests)
sol <- princomp(USArrests, cor=T)
# Extract loadings
X <- sol$loadings
# Plot the frame
plot(X, asp=1, type="n")
abline(v=0, lty=3)
abline(h=0, lty=3)
# Plot arrows: see ?arrows for the syntax
arrows(0, 0, X[,1], X[,2], len=0.1, col="red")
# Label the arrows
text(1.1*X, rownames(X), col="red", xpd=T)

Cheers, jari okanen
-- 
Jari Oksanen <jarioksa at sun3.oulu.fi>



From Achim.Zeileis at wu-wien.ac.at  Mon Oct  4 10:21:24 2004
From: Achim.Zeileis at wu-wien.ac.at (Achim Zeileis)
Date: Mon, 4 Oct 2004 10:21:24 +0200
Subject: [R] Off-Topic: LaTeX package listings
In-Reply-To: <4160B4CD.9090909@acelerate.com>
References: <4160B4CD.9090909@acelerate.com>
Message-ID: <20041004102124.7a252b84.Achim.Zeileis@wu-wien.ac.at>

On Sun, 03 Oct 2004 22:26:21 -0400 Kjetil Brinchmann Halvorsen wrote:

> Hola!
> 
> I ask here since I learnt from this list that the LaTeX package
> listings should be good
> for typesetting R code. I encountered one problem:
> \begin{lstlisting}
>       X %*% V
> \end{lstlisting}
> 
> in the output the * in %*% disappears! same with %/%, etc, the /
> disappears.
> 
> Any ideas?

The comment characters in listings can be configured via \lstset{}, but
I'm not sure wether this also includes the % character. Maybe it has to
be escaped.
Z

> Kjetil
> 
> -- 
> 
> Kjetil Halvorsen.
> 
> Peace is the most effective weapon of mass construction.
>                --  Mahdi Elmandjra
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide!
> http://www.R-project.org/posting-guide.html
>



From petr.pikal at precheza.cz  Mon Oct  4 11:32:35 2004
From: petr.pikal at precheza.cz (Petr Pikal)
Date: Mon, 04 Oct 2004 11:32:35 +0200
Subject: [R] Excluding data in R
In-Reply-To: <063E4E59A569A8459DF69577BA65CA0C21E807@isaac.topotarget.uk>
Message-ID: <416134D3.9559.1203417@localhost>



On 3 Oct 2004 at 13:56, Laura Collins wrote:

> I was hoping someone could help me!
> 
> 
> 
> How do you exclude outliers from a set of data?  

It depends strongly on your definition of outliers. You can apply 
some rules of thumb (e.g. points more than 3 sd apart) but it is 
quite often questionable. (see discussion few days ago).

If you already identified your outliers use

my.data[-outliers.points] in case of vector or
my.data[-outliers.points,] in case of data frame or matrix.

Cheers
Petr


> 
> 
> 
> Thanks,
> 
> 
> 
> Laura
> 
> 
>  [[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide!
> http://www.R-project.org/posting-guide.html

Petr Pikal
petr.pikal at precheza.cz



From lachmann at eva.mpg.de  Mon Oct  4 11:34:32 2004
From: lachmann at eva.mpg.de (Michael Lachmann)
Date: Mon, 04 Oct 2004 11:34:32 +0200
Subject: [R] conditional assignments and calculations
Message-ID: <41611928.7040806@eva.mpg.de>

Thank you!
Now the conditional assignments work almost prefectly.
The current code is at the bottom of the message.
Now I can do
depends[A,B]=B+9
and this will be executed only if B was more recently updated then A, and if the last time A was updated, the expression used was 'B+9'.

And it is also possible to do
depends[A,file="table.txt"]=read.table("table.txt")
and A will only be read again if the file "table.txt" was updated more recently than it was read last time.

I have only a few remaining questions:
1. I added attributes to variables, "last.updated", and "update.expression". If the variables are just regular vectors/lists, then when I print them, I also see these attributes. Is it somehow possible to have these attributes hidden?

2. I defined an operator %set%, which is supposed to work just like "<-" and "=", but also updates "last.updated" and "update.expression". It would be best to really replace "<-" and "=" with this operator, and then any update of a variable will be registered. This doesn't seem to work, because if I define "<-" as  
function(x,value)
{
  v=as.character(as.expression(substitute(value)))
  eval.parent(substitute(.Primitive("<-")(x,value)))
  eval.parent(substitute(attr(x,"last.updated")<-Sys.time()))
  eval.parent(substitute(attr(x,"update.expression")<-v))
  x
}
then this function calls "attr<-", which then calls "<-" leading to infinite recursion. Is there a way to set an attribute on a variable without using attr<- ?

Or maybe it is a bad idea to redefine "<-" in the first place...


Thank you very much!
Michael

Gabor Grothendieck wrote:

>Michael Lachmann <lachmann <at> eva.mpg.de> writes:
>
>: 
>: Hello!
>: 
>: I am using the TeXmacs interface to R. (Though I encountered a similar 
>: problem when using Sweave)
>: In doing calculations I often ecounter this scenario: I'll have some 
>: calculations in my file:
>: --
>: A=read.lots.of.data()
>: 
>: B=huge.calculation.on(A)
>: 
>: C=another.calculation.on(B)
>: --
>: Now, if A has already been read, I don't need to re-read it. If B has 
>: already been calculated, I don't need to recalculate it. But I would 
>: like to be able to just press 'enter' on each of them.
>: 
>: So, I would like R to somehow figure out dependencies (a bit like in 
>: Makefiles)
>: 
>: I implemented something like this with the following functions 

...

>: But this solution is quite ugly, because of several problems:
>: 
>: 1. To call 'depends(A,B)=f(B)' the first time, A has to already exist, 
>: otherwise I get an error (before I enter the "depends<-" function.)
>
>The technique used to implement mulitple return values shown in 
>
>   http://tolstoy.newcastle.edu.au/R/help/04/06/1406.html
>
>could be adapted to this problem.  Using that technique the code would 
>look like this:
>
>   depends[A,B] <- f(B)
>
>and A would not have to pre-exist.
>
>You define a structure with a class of depends, say:
>
>   depends <- structure(NA, class = "depends")
>
>and then define the [<-.depends action on that structure
>in an analogous way to what was done there.
>
>: 2. I would also like to have a convenient way to do
>: "if( !exists(A) ) { A=read.lots.of.data(); A=touch(A) }"
>: maybe something like:
>: depends(A)<-read.lots.of.data()
>: But that doesn't work, because of 1.
>: or
>: A %set% read.lots.data()
>: But that doesn't work, because I haven't figured out a way for a 
>: function to change one of its variables.
>: (Maybe I could do A=A %set read.lots.of.data(), but that is really ugly...)
>
>
>Is this what you want?
>
>R> f <- function(x,v) assign(as.character(substitute(x)), v, parent.frame())
>R> x # x does not exist
>Error: Object "x" not found
>R> f(x,3)
>R> x # now it does
>[1] 3
>
><<- can be used if the   eval.parent is a third way (see #3 below).
>
>: 3. It would be nice to be able to do touch(A) instead of A=touch(A)
>
>touch <- function(x) 
>    eval.parent(substitute(attr(x,"last.updated")<-Sys.time())) 
>x <- 3
>touch(x)
>
>: 
>: 4. If I modify A without calling 'A=touch(A)', then B will not be 
>: updated next time I call 'depends(B,A)=huge.calculation.on(A)'. So it 
>: would be nice to have the variable's 'last updated' time updated 
>: automatically. (Though then it is a bit problematic to decide what the 
>: 'last updated' time should be for variables loaded from a file...)
>
>If its done in a function you could use on.exit to ensure that it gets
>updated when leaving the function.
>

touch=function(...,l=list())
{
    args <- as.list(match.call())
    if( "l" %in% names(args) ) {
	    argsl=as.list(args[[length(args)]][-1])
	    argsl=sapply(argsl,function(x) parse(text=x)[[1]])
	    args=args[-length(args)]
  } else { argsl=c() }
    args=c(args[-1],argsl)
    for( i in 1:length(args)) {
      eval.parent(substitute(attr(x,"last.updated")<-Sys.time(),
      list(x=args[[i]]) ))
    }
}

last.updated=function(a) {   
  if( length(attr(a,"last.updated")) == 0 ) {
      Sys.time()
  } else {
      attr(a,"last.updated")
  }
}


"%set%"=function(x,value)
{
  v=as.character(as.expression(substitute(value)))
  eval.parent(substitute(.Primitive("<-")(x,value)))
  eval.parent(substitute(attr(x,"last.updated")<-Sys.time()))
  eval.parent(substitute(attr(x,"update.expression")<-v))
  x
}



depends <- structure(NA,class="depends")

"[<-.depends" <- function(x,...,file=c(),value) {
	  v=as.character(as.expression(substitute(value)))
  args <- as.list(match.call())
  if( !exists(as.character(args[[3]] ), env=sys.frame(-1)) ) {
      eval(substitute(x<-v,list(x=args[[3]],v=value)),env=sys.frame(-1))
      eval(substitute(touch(x),list(x=args[[3]])),env=sys.frame(-1))
      eval.parent(substitute(attr(x,"update.expression")<-v,list(x=args[[3]],v=v)))
  } else {
    lu=list()
		xlu=eval(substitute(attr(x,"last.updated"),list(x=args[[3]]),env=sys.frame(-1)))
    if( length(file) >0 ) {
	    lu=c(lapply(file,function(f) {
		    file.info(f)$mtime
		    }),lu) 
      args=args[-(length(args)-1)]
    }
    if( length(xlu)>0 ) {
        lu=c(lapply((1:(length(args)-1))[-c(1:3)],function(i) {
            eval(substitute(last.updated(x),list(x=args[[i]])),env=sys.frame(-1))
        }),lu)
    }
    lu=prod(sapply(lu,function(x) x-xlu<0))
	  xv=eval(substitute(attr(x,"update.expression"),list(x=args[[3]]),env=sys.frame(-1))) 
	  if( length(xv)*length(v) > 0 ) {
	    if( xv != v ) lu = 0
    } else { lu=0} 
    if( (lu==0) ) {
          eval(substitute(x<-v,list(x=args[[3]],v=value)),env=sys.frame(-1))
          eval(substitute(touch(x),list(x=args[[3]])),env=sys.frame(-1))
      eval.parent(substitute(attr(x,"update.expression")<-v,list(x=args[[3]],v=v)))
    }
  }
  x
}



From tpopenfoose at earthlink.net  Mon Oct  4 11:25:02 2004
From: tpopenfoose at earthlink.net (Toby Popenfoose)
Date: Mon, 4 Oct 2004 09:25:02 +0000 (UTC)
Subject: [R] Off-Topic: LaTeX package listings
References: <4160B4CD.9090909@acelerate.com>
Message-ID: <loom.20041004T112304-836@post.gmane.org>

Kjetil Brinchmann Halvorsen <kjetil <at> acelerate.com> writes:

> 
> Hola!
> 
> I ask here since I learnt from this list that the LaTeX package listings 
> should be good
> for typesetting R code. I encountered one problem:
> \begin{lstlisting}
>       X %*% V
> \end{lstlisting}
> 
> in the output the * in %*% disappears! same with %/%, etc, the /
> disappears.
> 
> Any ideas?
> 
> Kjetil
> 

I use the following and it displays fine

\lstset{numbers=left, stepnumber=5,breaklines=true}
\lstset{basicstyle=\footnotesize,frame=single}
\lstset{language=R}

This is a test
\begin{lstlisting}
X in %*% V 
%%
\end{lstlisting}

HTH



From V.Khamenia at biovision-discovery.de  Mon Oct  4 12:01:12 2004
From: V.Khamenia at biovision-discovery.de (Khamenia, Valery)
Date: Mon, 4 Oct 2004 12:01:12 +0200 
Subject: [R] constructing specially ordered factor
Message-ID: <D15343265276D31197BC00A024A6C110C793BF@EXS_BDC>

Hi all,

my colleagues deal with tables, where every factor is 
represented in two columns. The first column contains some 
numeric codes and the second contains the corresponding 
symbolic name. For example:

ISEX	SSEX
0	Female
1	Male
0	Female
0	Female
...

another example:

ICONC	SCONC
10	Normal
1000	ExtraHigh
10	Normal
0	Nothing
100	High
...

Colleagues require that the ordering should be done 
always by numeric column and not by the column with 
symbolic equivalents.

Here comes the question:

Is it possible to create factor with properly ordered and 
labeled values in nicer form then in the following long 
solution:

  Factor<-function(Names,Weights) {
    iunique = !duplicated(Weights)
    uniqueWeights = Weights[iunique]
    uniqueNames = Names[iunique] # corresponding unique names
    factor(Names, uniqueNames[order(uniqueWeights)])
  }

  Factor(SSEX, ISEX)

  Factor(SCONC, ICONC)

Thank you in advance for the comments,
Valery.



From michael.watson at bbsrc.ac.uk  Mon Oct  4 12:04:49 2004
From: michael.watson at bbsrc.ac.uk (michael watson (IAH-C))
Date: Mon, 4 Oct 2004 11:04:49 +0100
Subject: [R] Help with normal distributions
Message-ID: <8975119BCD0AC5419D61A9CF1A923E95E894DD@iahce2knas1.iah.bbsrc.reserved>

Hi

I have two questions, the first perhaps dumber than the second.

Firstly, I have a data set, and when I plot a histogram it looks like a
normal distribution.  So I want to overlay a bell-shaped normal
distribution on top of it, to demonstrate how similar it is to the
normal distribution.  I have read the help on dnorm(), rnorm(), pnorm()
etc but still can't figure out how to plot a normal distribution.  Any
code would be appreciated....

Secondly, and perhaps more difficult, is a second data set.  This, when
plotted as a histogram, has two clear peaks, perhaps even three, all of
which look as though they are normally distributed.  So the theory is
that my data set is actually made up of two, possibly three, underlying
sub-sets of data which are normally distributed, but with different
means and standard deviations.  So 1) how do I test for this? And 2) how
can I estimate the parameters (mean and SD) for the underlying
distributions?

Thanks in advance for your help

Mick



From dimitris.rizopoulos at med.kuleuven.ac.be  Mon Oct  4 12:38:15 2004
From: dimitris.rizopoulos at med.kuleuven.ac.be (Dimitris Rizopoulos)
Date: Mon, 4 Oct 2004 12:38:15 +0200
Subject: [R] Help with normal distributions
References: <8975119BCD0AC5419D61A9CF1A923E95E894DD@iahce2knas1.iah.bbsrc.reserved>
Message-ID: <00be01c4a9fe$4141dfc0$b2133a86@www.domain>

Hi Mick,

regarding your first question try the following,

#if `x' is your data vector, then
y <- seq(min(x), max(x), length=200)
hist(x, prob=TRUE)
lines(y, dnorm(y, mean(x), sd(x)))

regarding your second question, you'd probably want to fit a mixture 
model -> look at package `mclust'.

help(package="mclust")


I hope it helps.

Best,
Dimitris

----
Dimitris Rizopoulos
Ph.D. Student
Biostatistical Centre
School of Public Health
Catholic University of Leuven

Address: Kapucijnenvoer 35, Leuven, Belgium
Tel: +32/16/396887
Fax: +32/16/337015
Web: http://www.med.kuleuven.ac.be/biostat/
     http://www.student.kuleuven.ac.be/~m0390867/dimitris.htm


----- Original Message ----- 
From: "michael watson (IAH-C)" <michael.watson at bbsrc.ac.uk>
To: <r-help at stat.math.ethz.ch>
Sent: Monday, October 04, 2004 12:04 PM
Subject: [R] Help with normal distributions


> Hi
>
> I have two questions, the first perhaps dumber than the second.
>
> Firstly, I have a data set, and when I plot a histogram it looks 
> like a
> normal distribution.  So I want to overlay a bell-shaped normal
> distribution on top of it, to demonstrate how similar it is to the
> normal distribution.  I have read the help on dnorm(), rnorm(), 
> pnorm()
> etc but still can't figure out how to plot a normal distribution. 
> Any
> code would be appreciated....
>
> Secondly, and perhaps more difficult, is a second data set.  This, 
> when
> plotted as a histogram, has two clear peaks, perhaps even three, all 
> of
> which look as though they are normally distributed.  So the theory 
> is
> that my data set is actually made up of two, possibly three, 
> underlying
> sub-sets of data which are normally distributed, but with different
> means and standard deviations.  So 1) how do I test for this? And 2) 
> how
> can I estimate the parameters (mean and SD) for the underlying
> distributions?
>
> Thanks in advance for your help
>
> Mick
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
>



From dimitris.rizopoulos at med.kuleuven.ac.be  Mon Oct  4 12:54:16 2004
From: dimitris.rizopoulos at med.kuleuven.ac.be (Dimitris Rizopoulos)
Date: Mon, 4 Oct 2004 12:54:16 +0200
Subject: [R] constructing specially ordered factor
References: <D15343265276D31197BC00A024A6C110C793BF@EXS_BDC>
Message-ID: <00cd01c4aa00$7e0a1b50$b2133a86@www.domain>

Hi Valery,

does the following work in your data:

levs <- unique.default(Names)
factor(Names, levs[order(unique.default(Weights))])


I hope it helps.

Best,
Dimitris

----
Dimitris Rizopoulos
Ph.D. Student
Biostatistical Centre
School of Public Health
Catholic University of Leuven

Address: Kapucijnenvoer 35, Leuven, Belgium
Tel: +32/16/396887
Fax: +32/16/337015
Web: http://www.med.kuleuven.ac.be/biostat/
     http://www.student.kuleuven.ac.be/~m0390867/dimitris.htm


----- Original Message ----- 
From: "Khamenia, Valery" <V.Khamenia at biovision-discovery.de>
To: <R-help at stat.math.ethz.ch>
Sent: Monday, October 04, 2004 12:01 PM
Subject: [R] constructing specially ordered factor


> Hi all,
>
> my colleagues deal with tables, where every factor is
> represented in two columns. The first column contains some
> numeric codes and the second contains the corresponding
> symbolic name. For example:
>
> ISEX SSEX
> 0 Female
> 1 Male
> 0 Female
> 0 Female
> ...
>
> another example:
>
> ICONC SCONC
> 10 Normal
> 1000 ExtraHigh
> 10 Normal
> 0 Nothing
> 100 High
> ...
>
> Colleagues require that the ordering should be done
> always by numeric column and not by the column with
> symbolic equivalents.
>
> Here comes the question:
>
> Is it possible to create factor with properly ordered and
> labeled values in nicer form then in the following long
> solution:
>
>  Factor<-function(Names,Weights) {
>    iunique = !duplicated(Weights)
>    uniqueWeights = Weights[iunique]
>    uniqueNames = Names[iunique] # corresponding unique names
>    factor(Names, uniqueNames[order(uniqueWeights)])
>  }
>
>  Factor(SSEX, ISEX)
>
>  Factor(SCONC, ICONC)
>
> Thank you in advance for the comments,
> Valery.
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
>



From petr.pikal at precheza.cz  Mon Oct  4 13:05:04 2004
From: petr.pikal at precheza.cz (Petr Pikal)
Date: Mon, 04 Oct 2004 13:05:04 +0200
Subject: [R] constructing specially ordered factor
In-Reply-To: <D15343265276D31197BC00A024A6C110C793BF@EXS_BDC>
Message-ID: <41614A80.15472.174DD2F@localhost>



On 4 Oct 2004 at 12:01, Khamenia, Valery wrote:

> Hi all,
> 
> my colleagues deal with tables, where every factor is 
> represented in two columns. The first column contains some 
> numeric codes and the second contains the corresponding 
> symbolic name. For example:
> 
> ISEX	SSEX
> 0	Female
> 1	Male
> 0	Female
> 0	Female
> ...
> 
> another example:
> 
> ICONC	SCONC
> 10	Normal
> 1000	ExtraHigh
> 10	Normal
> 0	Nothing
> 100	High
> ...
> 
> Colleagues require that the ordering should be done 
> always by numeric column and not by the column with 
> symbolic equivalents.
> 
> Here comes the question:
> 
> Is it possible to create factor with properly ordered and 
> labeled values in nicer form then in the following long 
> solution:
> 
>   Factor<-function(Names,Weights) {
>     iunique = !duplicated(Weights)
>     uniqueWeights = Weights[iunique]
>     uniqueNames = Names[iunique] # corresponding unique names
>     factor(Names, uniqueNames[order(uniqueWeights)])
>   }
> 
>   Factor(SSEX, ISEX)
> 
>   Factor(SCONC, ICONC)

Hallo

Is that what you want?

> ooo<-order(levels(factor(pokus$ICONC)), decreasing=T)
> my.order<-levels(factor(pokus$SCONC))[ooo]
> factor(pokus$SCONC, levels=my.order)
[1] Normal    ExtraHigh Normal    Nothing   High     
Levels: Nothing Normal High ExtraHigh

If you want it in a function

Factor <- function(f,n, decreasing=TRUE, ...) {
ooo<-order(levels(factor(n)), decreasing=decreasing)
my.order<-levels(factor(f))[ooo]
factor(f, levels=my.order)
}


Cheers
Petr


> 
> Thank you in advance for the comments,
> Valery.
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide!
> http://www.R-project.org/posting-guide.html

Petr Pikal
petr.pikal at precheza.cz



From fm3a004 at math.uni-hamburg.de  Mon Oct  4 13:08:28 2004
From: fm3a004 at math.uni-hamburg.de (Christian Hennig)
Date: Mon, 4 Oct 2004 13:08:28 +0200 (MEST)
Subject: [R] Help with normal distributions
In-Reply-To: <8975119BCD0AC5419D61A9CF1A923E95E894DD@iahce2knas1.iah.bbsrc.reserved>
Message-ID: <Pine.GSO.3.95q.1041004130222.28547F-100000@sun11.math.uni-hamburg.de>

Hi Michael,

> Secondly, and perhaps more difficult, is a second data set.  This, when
> plotted as a histogram, has two clear peaks, perhaps even three, all of
> which look as though they are normally distributed.  So the theory is
> that my data set is actually made up of two, possibly three, underlying
> sub-sets of data which are normally distributed, but with different
> means and standard deviations.  So 1) how do I test for this? And 2) how
> can I estimate the parameters (mean and SD) for the underlying
> distributions?

The answer to 2, as pointed out already, is to use EMclust in package
mclust.
Testing for the presence of a mixture is difficult from a theoretical
point of view, and as far as I know, nothing is already implemented in R.
What you can do is:
a) Let EMclust estimate the number of mixture components by BIC (it can
also decide for only one component).
b) Use a standard normality test such as shapiro.test to exclude homogeneous
normality. This tells you that you have to fit something more complex than
a single normal, but it does not tell you what.

Christian

> 
> Thanks in advance for your help
> 
> Mick
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
> 

***********************************************************************
Christian Hennig
Fachbereich Mathematik-SPST/ZMS, Universitaet Hamburg
hennig at math.uni-hamburg.de, http://www.math.uni-hamburg.de/home/hennig/
#######################################################################
ich empfehle www.boag-online.de



From d.scott at auckland.ac.nz  Mon Oct  4 13:18:59 2004
From: d.scott at auckland.ac.nz (David Scott)
Date: Tue, 5 Oct 2004 00:18:59 +1300 (NZDT)
Subject: [R] Help with normal distributions
In-Reply-To: <Pine.GSO.3.95q.1041004130222.28547F-100000@sun11.math.uni-hamburg.de>
References: <Pine.GSO.3.95q.1041004130222.28547F-100000@sun11.math.uni-hamburg.de>
Message-ID: <Pine.LNX.4.61.0410050016380.23253@hydra.stat.auckland.ac.nz>

On Mon, 4 Oct 2004, Christian Hennig wrote:

> Hi Michael,
>
>> Secondly, and perhaps more difficult, is a second data set.  This, when
>> plotted as a histogram, has two clear peaks, perhaps even three, all of
>> which look as though they are normally distributed.  So the theory is
>> that my data set is actually made up of two, possibly three, underlying
>> sub-sets of data which are normally distributed, but with different
>> means and standard deviations.  So 1) how do I test for this? And 2) how
>> can I estimate the parameters (mean and SD) for the underlying
>> distributions?
>
> The answer to 2, as pointed out already, is to use EMclust in package
> mclust.
> Testing for the presence of a mixture is difficult from a theoretical
> point of view, and as far as I know, nothing is already implemented in R.
>

For testing for a mixture of two random variables there is the dip test of 
Hartigan---see diptest on CRAN

David Scott

_________________________________________________________________
David Scott	Department of Statistics, Tamaki Campus
 		The University of Auckland, PB 92019
 		Auckland	NEW ZEALAND
Phone: +64 9 373 7599 ext 86830		Fax: +64 9 373 7000
Email:	d.scott at auckland.ac.nz


Graduate Officer, Department of Statistics



From d.scott at auckland.ac.nz  Mon Oct  4 13:31:48 2004
From: d.scott at auckland.ac.nz (David Scott)
Date: Tue, 5 Oct 2004 00:31:48 +1300 (NZDT)
Subject: [R] gnls or nlme : how to obtain confidence intervals of fitted
	values
In-Reply-To: <4160FAC2.1090509@nancy.inra.fr>
References: <415D0BE8.9050104@nancy.inra.fr> <415EC6E6.8070600@pdf.com>
	<4160FAC2.1090509@nancy.inra.fr>
Message-ID: <Pine.LNX.4.61.0410050024180.23253@hydra.stat.auckland.ac.nz>

On Mon, 4 Oct 2004, Pierre MONTPIED wrote:

> Thanks Spencer but the intervals function gives confidence intervals of the 
> parameters of the model not the predicted values. In the Soybean example it 
> would be the CI of predicted weight for a given time, knowing all the 
> parameters (Asym, xmid, scal, variance function and residual) and their 
> distributions.
>
> And what I need to calculate is precisely this CI.
>
> My question is therefore is there an analytical way to calculate such CI, 
> whatever the model, or could I try some randomizing techniques such as 
> bootstrap or other ?
>

I find I have a need for this too but for the fixed effects in a mixed 
model fitted with lme. I have fitted a polynomial to some longitudinal 
data, plus some other fixed effects and some random effects. I can use 
predict to get the fitted polynomial for particular values of the other 
predictors, but I would really like to put some confidence bounds on it. I 
am not wild about bootstrapping---just fitting the model can take 30 
minutes.

Any suggestions will be gratefully received (although on second thoughts 
if someone tells me to use SAS I will be less than grateful).

David Scott



_________________________________________________________________
David Scott	Department of Statistics, Tamaki Campus
 		The University of Auckland, PB 92019
 		Auckland	NEW ZEALAND
Phone: +64 9 373 7599 ext 86830		Fax: +64 9 373 7000
Email:	d.scott at auckland.ac.nz


Graduate Officer, Department of Statistics



From andrewr at uidaho.edu  Mon Oct  4 13:42:20 2004
From: andrewr at uidaho.edu (Andrew Robinson)
Date: Mon, 4 Oct 2004 21:42:20 +1000
Subject: [R] gnls or nlme : how to obtain confidence intervals of fitted
	values
In-Reply-To: <Pine.LNX.4.61.0410050024180.23253@hydra.stat.auckland.ac.nz>
References: <415D0BE8.9050104@nancy.inra.fr> <415EC6E6.8070600@pdf.com>
	<4160FAC2.1090509@nancy.inra.fr>
	<Pine.LNX.4.61.0410050024180.23253@hydra.stat.auckland.ac.nz>
Message-ID: <20041004114220.GJ67507@uidaho.edu>

The function estimable() from the gmodels part of the gregmisc package
will do this, if applied appropriately.

It allows for the estimation of arbitrary linear combinations of the
parameter estimates of a model object, and calcualtes confidence
intervals.

I hope that this helps,

Andrew



On Tue, Oct 05, 2004 at 12:31:48AM +1300, David Scott wrote:
> On Mon, 4 Oct 2004, Pierre MONTPIED wrote:
> 
> >Thanks Spencer but the intervals function gives confidence intervals of 
> >the parameters of the model not the predicted values. In the Soybean 
> >example it would be the CI of predicted weight for a given time, knowing 
> >all the parameters (Asym, xmid, scal, variance function and residual) and 
> >their distributions.
> >
> >And what I need to calculate is precisely this CI.
> >
> >My question is therefore is there an analytical way to calculate such CI, 
> >whatever the model, or could I try some randomizing techniques such as 
> >bootstrap or other ?
> >
> 
> I find I have a need for this too but for the fixed effects in a mixed 
> model fitted with lme. I have fitted a polynomial to some longitudinal 
> data, plus some other fixed effects and some random effects. I can use 
> predict to get the fitted polynomial for particular values of the other 
> predictors, but I would really like to put some confidence bounds on it. I 
> am not wild about bootstrapping---just fitting the model can take 30 
> minutes.
> 
> Any suggestions will be gratefully received (although on second thoughts 
> if someone tells me to use SAS I will be less than grateful).
> 
> David Scott
> 
> 
> 
> _________________________________________________________________
> David Scott	Department of Statistics, Tamaki Campus
> 		The University of Auckland, PB 92019
> 		Auckland	NEW ZEALAND
> Phone: +64 9 373 7599 ext 86830		Fax: +64 9 373 7000
> Email:	d.scott at auckland.ac.nz
> 
> 
> Graduate Officer, Department of Statistics
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html

-- 
Andrew Robinson                      Ph: 208 885 7115
Department of Forest Resources       Fa: 208 885 6226
University of Idaho                  E : andrewr at uidaho.edu
PO Box 441133                        W : http://www.uidaho.edu/~andrewr
Moscow ID 83843                      Or: http://www.biometrics.uidaho.edu
No statement above necessarily represents my employer's opinion.



From bill.shipley at usherbrooke.ca  Mon Oct  4 14:18:48 2004
From: bill.shipley at usherbrooke.ca (Bill Shipley)
Date: Mon, 4 Oct 2004 08:18:48 -0400
Subject: [R] (off topic) article on advantages/disadvantages of types of SS?
Message-ID: <000401c4aa0c$50ade680$8c1ad284@BIO041>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20041004/c5ab8fed/attachment.pl

From andy_liaw at merck.com  Mon Oct  4 14:19:06 2004
From: andy_liaw at merck.com (Liaw, Andy)
Date: Mon, 4 Oct 2004 08:19:06 -0400
Subject: [R] compile R-1.9.1 with Portland fortran
Message-ID: <3A822319EB35174CA3714066D590DCD504AF84B6@usrymx25.merck.com>

I managed to compile R-1.9.1 with gcc and pgf77 on x86_64 (SUSE enterprise
server 8).  Was not successful configuring with pgcc/pgf77.

I've heard that the Pathscale compilers are even better than the Portland.
Has anyone tried?

Andy

> From: Jin Shusong
> 
> Dear R-users,
> 
>   Has any one compiled R-1.9.1  successfully with Portland
> pgcc pgCC and pgf77?  I can not configurate it with
> ./configure since it told me 
>     checking for Fortran name-mangling scheme... configure:
>     error: cannot compile a simple Fortran program
>   Can any one give me some advice.  Thank you.
> 
>   My platform: Linux-2.4.26, pentium4, portland compiler 5.1 
> 
>                                  Jin
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
> 
>



From andy_liaw at merck.com  Mon Oct  4 14:27:10 2004
From: andy_liaw at merck.com (Liaw, Andy)
Date: Mon, 4 Oct 2004 08:27:10 -0400
Subject: [R] (off topic) article on advantages/disadvantages of
	types of SS?
Message-ID: <3A822319EB35174CA3714066D590DCD504AF84B7@usrymx25.merck.com>

Bill Venables' `Exegeses on Linear Models' has very good discussions:

http://www.stats.ox.ac.uk/pub/MASS3/Exegeses.pdf

Andy

> From: Bill Shipley
> 
> Hello.  Please excuse this off-topic request, but I know that the
> question has been debated in summary form on this list a number of
> times.  I would find a paper that lays out the advantages and
> disadvantages of using different types of SS in the context of
> unbalanced data in  ANOVA, regression and ANCOVA, especially including
> the use of different types of contrasts and the meaning of the
> hypotheses that are tested in such cases.
> 
> Thanks for any leads.   
> 
> Bill Shipley
> Subject Matter Editor, Ecology
> North American Editor, Annals of Botany
> D??partement de biologie, Universit?? de Sherbrooke,
> Sherbrooke (Qu??bec) J1K 2R1 CANADA
> Bill.Shipley at USherbrooke.ca
 <http://callisto.si.usherb.ca:8080/bshipley/>
http://callisto.si.usherb.ca:8080/bshipley/
	[[alternative HTML version deleted]]
______________________________________________
R-help at stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide!
http://www.R-project.org/posting-guide.html



From kjetil at acelerate.com  Mon Oct  4 14:32:15 2004
From: kjetil at acelerate.com (Kjetil Brinchmann Halvorsen)
Date: Mon, 04 Oct 2004 08:32:15 -0400
Subject: [R] biplot.princomp with loadings only
In-Reply-To: <1096877970.3728.9.camel@biol102145.oulu.fi>
References: <415BB6BC.6070307@gmx.ch>
	<1096877970.3728.9.camel@biol102145.oulu.fi>
Message-ID: <416142CF.1090708@acelerate.com>

Jari Oksanen wrote:

>On Thu, 2004-09-30 at 10:33, Christoph Lehmann wrote:
>  
>
>>Hi
>>
>>is there a way to plot only the loadings in a biplot (with the nice 
>>arrows), and to skip the scores?
>>
>>    
>>
Just let me point out that there is a function for this in the
CRAN package ade4,    s.corcircle

library(ade4)
example(s.corcircle)

Kjetil


>Christoph,
>
>I may have overlooked some email messages, but it seems to me that you
>haven't yet got an answer to your practical question. From the practical
>point of view, we may skip the point that you rather ask for a
>"monoplot" than "biplot" if you have only one set of points. Further, I
>may forget my surprise when I see that somebody really thinks that these
>arrows are "nice". OK, they may be nice if you have only a couple of
>them, but anybody plotting 30 or more arrows normally asks how to get
>rid off this mess. 
>
>Of course you can plot arrows in your "monoplot", since you have got
>access to everything in R and you can do anything with R (but coffee
>comes somewhat bland, so I recommend something else for the task cooking
>coffee). Here is an example:
>
># Run PCA
>data(USArrests)
>sol <- princomp(USArrests, cor=T)
># Extract loadings
>X <- sol$loadings
># Plot the frame
>plot(X, asp=1, type="n")
>abline(v=0, lty=3)
>abline(h=0, lty=3)
># Plot arrows: see ?arrows for the syntax
>arrows(0, 0, X[,1], X[,2], len=0.1, col="red")
># Label the arrows
>text(1.1*X, rownames(X), col="red", xpd=T)
>
>Cheers, jari okanen
>  
>



-- 

Kjetil Halvorsen.

Peace is the most effective weapon of mass construction.
               --  Mahdi Elmandjra



From andrewr at uidaho.edu  Mon Oct  4 14:38:15 2004
From: andrewr at uidaho.edu (Andrew Robinson)
Date: Mon, 4 Oct 2004 22:38:15 +1000
Subject: [R] (off topic) article on advantages/disadvantages of types of
	SS?
In-Reply-To: <000401c4aa0c$50ade680$8c1ad284@BIO041>
References: <000401c4aa0c$50ade680$8c1ad284@BIO041>
Message-ID: <20041004123815.GO67507@uidaho.edu>

Hello Bill,

two papers that might be of interest to you are: 

1) Bill Venables's Exegesis (Google will find that for you very
quickly), and

2) Nelder, J.A. 1994.  The statistics of linaer models: back to basics.
Statistics and Computing 4:221-134.

Good luck!

Andrew

On Mon, Oct 04, 2004 at 08:18:48AM -0400, Bill Shipley wrote:
> Hello.  Please excuse this off-topic request, but I know that the
> question has been debated in summary form on this list a number of
> times.  I would find a paper that lays out the advantages and
> disadvantages of using different types of SS in the context of
> unbalanced data in  ANOVA, regression and ANCOVA, especially including
> the use of different types of contrasts and the meaning of the
> hypotheses that are tested in such cases.
> 
> Thanks for any leads.   
> 
>  
> 
> Bill Shipley
> 
> Subject Matter Editor, Ecology
> 
> North American Editor, Annals of Botany
> 
> D?partement de biologie, Universit? de Sherbrooke,
> 
> Sherbrooke (Qu?bec) J1K 2R1 CANADA
> 
> Bill.Shipley at USherbrooke.ca
> 
>  <http://callisto.si.usherb.ca:8080/bshipley/>
> http://callisto.si.usherb.ca:8080/bshipley/
> 
>  
> 
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html

-- 
Andrew Robinson                      Ph: 208 885 7115
Department of Forest Resources       Fa: 208 885 6226
University of Idaho                  E : andrewr at uidaho.edu
PO Box 441133                        W : http://www.uidaho.edu/~andrewr
Moscow ID 83843                      Or: http://www.biometrics.uidaho.edu
No statement above necessarily represents my employer's opinion.



From Luisr at frs.fo  Mon Oct  4 15:09:58 2004
From: Luisr at frs.fo (Luis Rideau Cruz)
Date: Mon, 04 Oct 2004 14:09:58 +0100
Subject: [R] Error: cannot allocate vector of size 1125522 Kb, Reached total
	allocation of 510Mb
Message-ID: <s16159c2.023@ffdata.setur.fo>

R-help

I'm trying to compute the 'dist' function of a data set consisting of
16975 observations and 5 variables(2 quantitative and 3 categorical).

If I call the function on a subset of the data frame everything works
fine but when I reach above 3000 observations R either crash or gives
the following error message.

Error: cannot allocate vector of size 1125522 Kb
In addition: Warning messages: 
1: NAs introduced by coercion 
2: Reached total allocation of 510Mb: see help(memory.size) 

I have tried 'help(memory.size)' and increase the 'memory.limit' but
without success

Can anyone help?

Thank you in advance



From john.deangelis at moorecap.com  Mon Oct  4 15:00:46 2004
From: john.deangelis at moorecap.com (John DeAngelis)
Date: Mon, 4 Oct 2004 09:00:46 -0400 
Subject: [R] Identifying time series
Message-ID: <FDAE352C6A59D311A3DF00508B0169F51940DBA0@corvus.moorecap.com>




Hello,

I am currently attempting to introduce R at my company and am trying to
import time series data from a text file into R to graph. The format of the
text file is in date, data (e.g.,  20040929 3.361). My problem is that I do
not know how to get R to recognize the first column as a business date
series. Or at the very least, I am unable to find a function that will grap
the second column (y-axis) against the date series (x-axis). If you could
tell me how to graph this type of data, I would greatly appreciate it. Thank
you.


Regards,

John DeAngelis



From V.Khamenia at biovision-discovery.de  Mon Oct  4 15:16:28 2004
From: V.Khamenia at biovision-discovery.de (Khamenia, Valery)
Date: Mon, 4 Oct 2004 15:16:28 +0200 
Subject: AW: [R] constructing specially ordered factor
Message-ID: <D15343265276D31197BC00A024A6C110C793C5@EXS_BDC>

Hi Dimitris,

thank you for your reply, 

> does the following work in your data:
> 
> levs <- unique.default(Names)
> factor(Names, levs[order(unique.default(Weights))])

your solution is really shorter, but has two issues to be 
meant here:

1. "unique.default" is applied twice, what might 
   be a bit expensive for strings.

2. your solution brings an implicit prerequisit on 
  "unique.default". Indeed, "unique.default" should 
  preserve the same order for output both working 
  with strings and with numbers. In other words, 
  correspondence must be kept by "unique.default".
  If "unique.default" implementation uses (or will use) 
  lexicographic strings sorting for acceleration
  then this approach fails.


--
Valery



From ligges at statistik.uni-dortmund.de  Mon Oct  4 15:25:49 2004
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Mon, 04 Oct 2004 15:25:49 +0200
Subject: [R] Error: cannot allocate vector of size 1125522 Kb, Reached
	total	allocation of 510Mb
In-Reply-To: <s16159c2.023@ffdata.setur.fo>
References: <s16159c2.023@ffdata.setur.fo>
Message-ID: <41614F5D.4070109@statistik.uni-dortmund.de>

Luis Rideau Cruz wrote:

> R-help
> 
> I'm trying to compute the 'dist' function of a data set consisting of
> 16975 observations and 5 variables(2 quantitative and 3 categorical).

So what you get returned would be a 16975x16975 matrix
that inherits 288150625 elements.
You need 8 bytes per element, hence 8 * 288150625 = 2305205000, to 
represent the matrix in memory (only once!, and you might need twice or 
more available memory to work with it).

So you need as a lower bound 2.5 GB of memory, and I guess 5 GB are 
required in order to get it to work. If you have that much available, 
try again. But since you are on Windows (and have not told us), you 
might not have that much....

Uwe Ligges




> If I call the function on a subset of the data frame everything works
> fine but when I reach above 3000 observations R either crash or gives
> the following error message.
> 
> Error: cannot allocate vector of size 1125522 Kb
> In addition: Warning messages: 
> 1: NAs introduced by coercion 
> 2: Reached total allocation of 510Mb: see help(memory.size) 
> 
> I have tried 'help(memory.size)' and increase the 'memory.limit' but
> without success
> 
> Can anyone help?
> 
> Thank you in advance
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html



From V.Khamenia at biovision-discovery.de  Mon Oct  4 15:27:27 2004
From: V.Khamenia at biovision-discovery.de (Khamenia, Valery)
Date: Mon, 4 Oct 2004 15:27:27 +0200 
Subject: AW: [R] constructing specially ordered factor
Message-ID: <D15343265276D31197BC00A024A6C110C793C6@EXS_BDC>

Hi Petr,

Thank you for your reply.

> Factor <- function(f,n, decreasing=TRUE, ...) {
> ooo<-order(levels(factor(n)), decreasing=decreasing)
> my.order<-levels(factor(f))[ooo]
> factor(f, levels=my.order)
> }

it works incorrectly. Indeed, let's apply with your Factor:

  unames <- c("thousands", "units", "dozens", "hundreds", "thousands",
"dozens")
  u <- c(1000, 1,10,100, 1000, 10)
  Factor(unames, u)

the above produces the following output:

  [1] thousands units     dozens    hundreds  thousands dozens   
  Levels: units thousands hundreds dozens

where "dozens" > "hundreds"

---
Valery



From jfox at mcmaster.ca  Mon Oct  4 15:30:38 2004
From: jfox at mcmaster.ca (John Fox)
Date: Mon, 4 Oct 2004 09:30:38 -0400
Subject: [R] (off topic) article on advantages/disadvantages of types of
	SS?
In-Reply-To: <000401c4aa0c$50ade680$8c1ad284@BIO041>
Message-ID: <20041004133034.GNNL4905.tomts13-srv.bellnexxia.net@JohnDesktop8300>

Dear Bill,

I discuss the matter in some detail in my text Applied Regression Analysis,
Linear Models, and Related Methods (Sage, 1997) and supply a number of
references there.

I hope this helps,
 John 

> -----Original Message-----
> From: r-help-bounces at stat.math.ethz.ch 
> [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Bill Shipley
> Sent: Monday, October 04, 2004 7:19 AM
> To: R help list
> Subject: [R] (off topic) article on advantages/disadvantages 
> of types of SS?
> 
> Hello.  Please excuse this off-topic request, but I know that 
> the question has been debated in summary form on this list a 
> number of times.  I would find a paper that lays out the 
> advantages and disadvantages of using different types of SS 
> in the context of unbalanced data in  ANOVA, regression and 
> ANCOVA, especially including the use of different types of 
> contrasts and the meaning of the hypotheses that are tested 
> in such cases.
> 
> Thanks for any leads.   
> 
>  
> 
> Bill Shipley
> 
> Subject Matter Editor, Ecology
> 
> North American Editor, Annals of Botany
> 
> D??partement de biologie, Universit?? de Sherbrooke,
> 
> Sherbrooke (Qu??bec) J1K 2R1 CANADA
> 
> Bill.Shipley at USherbrooke.ca
> 
>  <http://callisto.si.usherb.ca:8080/bshipley/>
> http://callisto.si.usherb.ca:8080/bshipley/
> 
>  
> 
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html



From mmmabos at comcast.net  Mon Oct  4 15:32:35 2004
From: mmmabos at comcast.net (Matt Hart)
Date: Mon, 04 Oct 2004 09:32:35 -0400
Subject: [R] Weighted Savitzky-Golay?
Message-ID: <416150F3.2010906@comcast.net>

Hi,

Does anyone know how to use weights and generate error bounds for 
Savitzky-Golay? I have a (smallish) set of points y equally spaced each 
with a known error and would like to smooth them using S-G but so as to 
take into account the error already have and construct new error bounds 
around them that take into account the errors they had at the beginning 
and the erros they get as a result of the smoothing.

thanks, m.



From paolo.bulla at unibocconi.it  Mon Oct  4 15:33:26 2004
From: paolo.bulla at unibocconi.it (Paolo Bulla)
Date: Mon,  4 Oct 2004 15:33:26 +0200
Subject: [R] scatter plot and marginal
Message-ID: <1096896806.41615126bb6a1@webmail.uni-bocconi.it>


Hallo,

I would like to add the marginal distributions along the X and the Y axis to a
scatter plot.
Can anybody help me, please?

Thank you,
 Paolo
-- 
Paolo Bulla

Istituto di Metodi Quantitativi
Universit?? "L. Bocconi"
viale Isonzo 25
20136 Milano
paolo.bulla at unibocconi.it



From ripley at stats.ox.ac.uk  Mon Oct  4 15:37:04 2004
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Mon, 4 Oct 2004 14:37:04 +0100 (BST)
Subject: [R] Error: cannot allocate vector of size 1125522 Kb, Reached
	total allocation of 510Mb
In-Reply-To: <41614F5D.4070109@statistik.uni-dortmund.de>
Message-ID: <Pine.LNX.4.44.0410041435110.10475-100000@gannet.stats>

On Mon, 4 Oct 2004, Uwe Ligges wrote:

> Luis Rideau Cruz wrote:
> 
> > R-help
> > 
> > I'm trying to compute the 'dist' function of a data set consisting of
> > 16975 observations and 5 variables(2 quantitative and 3 categorical).
> 
> So what you get returned would be a 16975x16975 matrix
> that inherits 288150625 elements.
> You need 8 bytes per element, hence 8 * 288150625 = 2305205000, to 
> represent the matrix in memory (only once!, and you might need twice or 
> more available memory to work with it).

Not quite, as it only stores half.  In fact you need around 1125522Kb, 
just over 1Gb.

> So you need as a lower bound 2.5 GB of memory, and I guess 5 GB are 
> required in order to get it to work. If you have that much available, 
> try again. But since you are on Windows (and have not told us), you 
> might not have that much....

It has an address space limit of 2Gb unless worked on: see the rw-FAQ.

> > If I call the function on a subset of the data frame everything works
> > fine but when I reach above 3000 observations R either crash or gives
> > the following error message.
> > 
> > Error: cannot allocate vector of size 1125522 Kb
> > In addition: Warning messages: 
> > 1: NAs introduced by coercion 
> > 2: Reached total allocation of 510Mb: see help(memory.size) 
> > 
> > I have tried 'help(memory.size)' and increase the 'memory.limit' but
> > without success

You cannot increase the limit once you have reached it, AFAIK.

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From ripley at stats.ox.ac.uk  Mon Oct  4 15:41:19 2004
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Mon, 4 Oct 2004 14:41:19 +0100 (BST)
Subject: AW: [R] constructing specially ordered factor
In-Reply-To: <D15343265276D31197BC00A024A6C110C793C5@EXS_BDC>
Message-ID: <Pine.LNX.4.44.0410041437470.10475-100000@gannet.stats>

On Mon, 4 Oct 2004, Khamenia, Valery wrote:

> Hi Dimitris,
> 
> thank you for your reply, 
> 
> > does the following work in your data:
> > 
> > levs <- unique.default(Names)
> > factor(Names, levs[order(unique.default(Weights))])
> 
> your solution is really shorter, but has two issues to be 
> meant here:
> 
> 1. "unique.default" is applied twice, what might 
>    be a bit expensive for strings.
> 
> 2. your solution brings an implicit prerequisit on 
>   "unique.default". Indeed, "unique.default" should 
>   preserve the same order for output both working 
>   with strings and with numbers. In other words, 
>   correspondence must be kept by "unique.default".
>   If "unique.default" implementation uses (or will use) 
>   lexicographic strings sorting for acceleration
>   then this approach fails.

Please follow the posting guide and do your homework before posting, in 
this case the help page for unique.default.

     'unique' returns a vector, data frame or array like 'x' but with
     duplicate  elements removed.

Value:

     An object of the same type of 'x'. but if an element is equal to
     one with a smaller index, it is removed.

so the order is preserved, by definition.

BTW it uses hashing for `acceleration', not something as slow as sorting.

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From Achim.Zeileis at wu-wien.ac.at  Mon Oct  4 15:42:15 2004
From: Achim.Zeileis at wu-wien.ac.at (Achim Zeileis)
Date: Mon, 4 Oct 2004 15:42:15 +0200
Subject: [R] Identifying time series
In-Reply-To: <FDAE352C6A59D311A3DF00508B0169F51940DBA0@corvus.moorecap.com>
References: <FDAE352C6A59D311A3DF00508B0169F51940DBA0@corvus.moorecap.com>
Message-ID: <20041004154215.37667878.Achim.Zeileis@wu-wien.ac.at>

On Mon, 4 Oct 2004 09:00:46 -0400  John DeAngelis wrote:

> 
> 
> 
> Hello,
> 
> I am currently attempting to introduce R at my company and am trying
> to import time series data from a text file into R to graph. The
> format of the text file is in date, data (e.g.,  20040929 3.361).

You can read in such a file, e.g., via
R> x <- read.table(file = "mydata.txt", colClasses = c("character",
     "numeric"))

> My problem is that I do
> not know how to get R to recognize the first column as a business date
> series.

You can do that via strptime(), e.g.,

R> strptime(x[,1], "%Y%m%d")

(and possibly convert then to POSIXct or Date).

Then you can create an irregular time series, e.g. via package its or
via package zoo:

R> y <- zoo(x[,2], strptime(x[,1], "%Y%m%d"))
R> plot(y)

hth,
Z

> Or at the very least, I am unable to find a function that will
> grap the second column (y-axis) against the date series (x-axis). If
> you could tell me how to graph this type of data, I would greatly
> appreciate it. Thank you.
> 
> 
> Regards,
> 
> John DeAngelis
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide!
> http://www.R-project.org/posting-guide.html
>



From ripley at stats.ox.ac.uk  Mon Oct  4 15:47:47 2004
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Mon, 4 Oct 2004 14:47:47 +0100 (BST)
Subject: [R] R glm
In-Reply-To: <630AA368-0D87-11D9-9988-000A958F43CC@MUOhio.edu>
Message-ID: <Pine.LNX.4.44.0410041445420.17322-100000@gannet.stats>

On Thu, 23 Sep 2004, Martin Henry H. Stevens wrote:

> Upon examining Table 7.1 on page 184 of V&R 4th edition, I can see 
> where the ambiguity would arise, however. Always best to check the 

No ambiguity.  That book is about S, and S only supports the identity 
link.  (Extensions in just one of R or S-PLUS are often not mentioned.)

> help, I guess.
> Hank
> On Sep 23, 2004, at 1:04 PM, Paul Johnson wrote:
> 
> > No!
> >
> > > ?family
> >
> >  The 'gaussian' family accepts the links '"identity"', '"log"' and
> >           '"inverse"';
> >
> > Kahra Hannu wrote:
> >> In Venables & Ripley: Modern Applied Statistics with S (MASS), (4th 
> >> edition), on page 184 there is a table "Families and link functions" 
> >> that gives you the available links with different families. The 
> >> default and the only link with the gaussian family is identity.
> >> ciao,

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From rolf.wester at ilt.fraunhofer.de  Mon Oct  4 16:01:15 2004
From: rolf.wester at ilt.fraunhofer.de (Rolf Wester)
Date: Mon, 04 Oct 2004 16:01:15 +0200
Subject: [R] Beginners problem
Message-ID: <416157AB.7000905@ilt.fraunhofer.de>

Hi,

I'm new to R and have a problem with a little test program (see below). 
Why doesn't <<- in function rk4
assign the new value to y so that it is seen in rktest. I thought that 
<<- does exactly this. But it seems that I
didn't get it right. I would be very appreciative for an explanation of 
that behaviour of <<-. I know how to
write the whole thing so that it works (return the updated y to rktest) 
but I would like to learn how variable scope
in R works.

Thanks in advance

Rolf

----------------------------------------------------------------------------------------------------------------------

rk4 <- function(x,y,f,h) {
  n  <- length(y)
  hh <- h*0.5
  k1 <- f(x,y)
  k2 <- f(x,y+k1*hh)
  k3 <- f(x,y+k2*hh)
  k4 <- f(x,y+k3*h)
  y <<- y + h/6.0*(k1 + 2.0*k2 + 2.0*k3 + k4))
}

rkf <- function(x,y) {
  -y
}

rktest <- function(){
  y <- 1.0
  x <- 0.0
  h <- 0.1
  for(i in 1:10) {
    rk4(x,y,rkf,h)
    print(y)
  }
}

rktest()



From phyde at Glue.umd.edu  Mon Oct  4 16:06:00 2004
From: phyde at Glue.umd.edu (Peter Hyde)
Date: Mon, 4 Oct 2004 10:06:00 -0400 (EDT)
Subject: [R] subsetting
Message-ID: <Pine.SOL.4.44.0410041002260.3366-100000@z.glue.umd.edu>

Hi!  I have never used this before, so please forgive my lack of proper
protocol...

Would someone please tell me how to subset a dataframe?

I have a table of data with x & y (spatial) coordinates, 2 attributes
(tree height and biomass) and a unique identification number for each
record

i have also created an array of random numbers equal to the number of
records in my data frame

i am trying to subset, in order to get the records that correspond to my
unique id numbers

basically, i am trying to figure out the equivalent to the "where" command
in IDL (which I understand is FORTRAN-like)

any insight would be greatly appreciated!!

thanks!



From JonesW at kssg.com  Mon Oct  4 16:10:58 2004
From: JonesW at kssg.com (Wayne Jones)
Date: Mon, 4 Oct 2004 15:10:58 +0100 
Subject: [R] Strange Matrix Multiplication  Behaviour
Message-ID: <6B5A9304046AD411BD0200508BDFB6CB02FEFD77@gimli.middleearth.kssg.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20041004/891a5f89/attachment.pl

From wolfram at fischer-zim.ch  Mon Oct  4 16:21:14 2004
From: wolfram at fischer-zim.ch (Wolfram Fischer)
Date: Mon, 4 Oct 2004 16:21:14 +0200
Subject: [R] inverse function of order()
Message-ID: <20041004142114.GA4158@s1x.local>

I have:

 d <- sample(10:100, 9)
 o <- order(d)
 r <- d[o]

How I can get d (in the original order), knowing only r and o?

Thanks - Wolfram



From andy_liaw at merck.com  Mon Oct  4 16:28:45 2004
From: andy_liaw at merck.com (Liaw, Andy)
Date: Mon, 4 Oct 2004 10:28:45 -0400
Subject: [R] Weighted Savitzky-Golay?
Message-ID: <3A822319EB35174CA3714066D590DCD504AF84BA@usrymx25.merck.com>

If I'm not mistaken, S-G is essentially a local (even-degree) polynomial
smoother with constant bandwidth.  You can use a constant bandwidth local
polynomial smoother that allows weights; e.g., in the locfit package.

HTH,
Andy

> From: Matt Hart
> 
> Hi,
> 
> Does anyone know how to use weights and generate error bounds for 
> Savitzky-Golay? I have a (smallish) set of points y equally 
> spaced each 
> with a known error and would like to smooth them using S-G 
> but so as to 
> take into account the error already have and construct new 
> error bounds 
> around them that take into account the errors they had at the 
> beginning 
> and the erros they get as a result of the smoothing.
> 
> thanks, m.
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
> 
>



From andy_liaw at merck.com  Mon Oct  4 16:30:15 2004
From: andy_liaw at merck.com (Liaw, Andy)
Date: Mon, 4 Oct 2004 10:30:15 -0400
Subject: [R] scatter plot and marginal
Message-ID: <3A822319EB35174CA3714066D590DCD504AF84BB@usrymx25.merck.com>

See example(layout).

Andy

> From: Paolo Bulla
> 
> Hallo,
> 
> I would like to add the marginal distributions along the X 
> and the Y axis to a
> scatter plot.
> Can anybody help me, please?
> 
> Thank you,
>  Paolo
> -- 
> Paolo Bulla
> 
> Istituto di Metodi Quantitativi
> Universit?? "L. Bocconi"
> viale Isonzo 25
> 20136 Milano
> paolo.bulla at unibocconi.it
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
> 
>



From kjetil at acelerate.com  Mon Oct  4 16:02:46 2004
From: kjetil at acelerate.com (Kjetil Brinchmann Halvorsen)
Date: Mon, 04 Oct 2004 10:02:46 -0400
Subject: [R] Off-Topic: LaTeX package listings
In-Reply-To: <loom.20041004T112304-836@post.gmane.org>
References: <4160B4CD.9090909@acelerate.com>
	<loom.20041004T112304-836@post.gmane.org>
Message-ID: <41615806.4020005@acelerate.com>

Toby Popenfoose wrote:

>Kjetil Brinchmann Halvorsen <kjetil <at> acelerate.com> writes:
>
>  
>
>>Hola!
>>
>>I ask here since I learnt from this list that the LaTeX package listings 
>>should be good
>>for typesetting R code. I encountered one problem:
>>\begin{lstlisting}
>>      X %*% V
>>\end{lstlisting}
>>
>>in the output the * in %*% disappears! same with %/%, etc, the /
>>disappears.
>>
>>Any ideas?
>>
>>Kjetil
>>
>>    
>>

Answering myself, this seems to be a problem with babel and spanish. I have

\usepackage[spanish]{babel}

replacing this with
\usepackage[english]{babel}
(or removing babel)
problem disappears. The maintainers of [spanish] are known to be 
hyperactive!

Kjetil

>
>I use the following and it displays fine
>
>\lstset{numbers=left, stepnumber=5,breaklines=true}
>\lstset{basicstyle=\footnotesize,frame=single}
>\lstset{language=R}
>
>This is a test
>\begin{lstlisting}
>X in %*% V 
>%%
>\end{lstlisting}
>
>HTH
>
>______________________________________________
>R-help at stat.math.ethz.ch mailing list
>https://stat.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>
>
>  
>


-- 

Kjetil Halvorsen.

Peace is the most effective weapon of mass construction.
               --  Mahdi Elmandjra



From kjetil at acelerate.com  Mon Oct  4 16:19:47 2004
From: kjetil at acelerate.com (Kjetil Brinchmann Halvorsen)
Date: Mon, 04 Oct 2004 10:19:47 -0400
Subject: [R] Help with normal distributions
In-Reply-To: <00be01c4a9fe$4141dfc0$b2133a86@www.domain>
References: <8975119BCD0AC5419D61A9CF1A923E95E894DD@iahce2knas1.iah.bbsrc.reserved>
	<00be01c4a9fe$4141dfc0$b2133a86@www.domain>
Message-ID: <41615C03.9010707@acelerate.com>

Dimitris Rizopoulos wrote:

> Hi Mick,
>
> regarding your first question try the following,
>
> #if `x' is your data vector, then
> y <- seq(min(x), max(x), length=200)
> hist(x, prob=TRUE)
> lines(y, dnorm(y, mean(x), sd(x)))
>
Adding to this, what I do, a little bit simpler (?) is:

hist(x, prob=TRUE)
plot( function(y) dnorm(y, mean(x), sd(x)), from=min(x), to=max(x), 
col="red", add=TRUE)

Kjetil


> regarding your second question, you'd probably want to fit a mixture 
> model -> look at package `mclust'.
>
> help(package="mclust")
>
>
> I hope it helps.
>
> Best,
> Dimitris
>
> ----
> Dimitris Rizopoulos
> Ph.D. Student
> Biostatistical Centre
> School of Public Health
> Catholic University of Leuven
>
> Address: Kapucijnenvoer 35, Leuven, Belgium
> Tel: +32/16/396887
> Fax: +32/16/337015
> Web: http://www.med.kuleuven.ac.be/biostat/
>     http://www.student.kuleuven.ac.be/~m0390867/dimitris.htm
>
>
> ----- Original Message ----- From: "michael watson (IAH-C)" 
> <michael.watson at bbsrc.ac.uk>
> To: <r-help at stat.math.ethz.ch>
> Sent: Monday, October 04, 2004 12:04 PM
> Subject: [R] Help with normal distributions
>
>
>> Hi
>>
>> I have two questions, the first perhaps dumber than the second.
>>
>> Firstly, I have a data set, and when I plot a histogram it looks like a
>> normal distribution.  So I want to overlay a bell-shaped normal
>> distribution on top of it, to demonstrate how similar it is to the
>> normal distribution.  I have read the help on dnorm(), rnorm(), pnorm()
>> etc but still can't figure out how to plot a normal distribution. Any
>> code would be appreciated....
>>
>> Secondly, and perhaps more difficult, is a second data set.  This, when
>> plotted as a histogram, has two clear peaks, perhaps even three, all of
>> which look as though they are normally distributed.  So the theory is
>> that my data set is actually made up of two, possibly three, underlying
>> sub-sets of data which are normally distributed, but with different
>> means and standard deviations.  So 1) how do I test for this? And 2) how
>> can I estimate the parameters (mean and SD) for the underlying
>> distributions?
>>
>> Thanks in advance for your help
>>
>> Mick
>>
>> ______________________________________________
>> R-help at stat.math.ethz.ch mailing list
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide! 
>> http://www.R-project.org/posting-guide.html
>>
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
>
>


-- 

Kjetil Halvorsen.

Peace is the most effective weapon of mass construction.
               --  Mahdi Elmandjra



From MSchwartz at MedAnalytics.com  Mon Oct  4 16:36:42 2004
From: MSchwartz at MedAnalytics.com (Marc Schwartz)
Date: Mon, 04 Oct 2004 09:36:42 -0500
Subject: [R] scatter plot and marginal
In-Reply-To: <1096896806.41615126bb6a1@webmail.uni-bocconi.it>
References: <1096896806.41615126bb6a1@webmail.uni-bocconi.it>
Message-ID: <1096900602.4471.111.camel@localhost.localdomain>

On Mon, 2004-10-04 at 08:33, Paolo Bulla wrote:
> Hallo,
> 
> I would like to add the marginal distributions along the X and the Y axis to a
> scatter plot.
> Can anybody help me, please?
> 
> Thank you,
>  Paolo


See the last example in ?layout

HTH,

Marc Schwartz



From petr.pikal at precheza.cz  Mon Oct  4 16:37:20 2004
From: petr.pikal at precheza.cz (Petr Pikal)
Date: Mon, 04 Oct 2004 16:37:20 +0200
Subject: AW: [R] constructing specially ordered factor
In-Reply-To: <D15343265276D31197BC00A024A6C110C793C6@EXS_BDC>
Message-ID: <41617C40.28645.2373581@localhost>



On 4 Oct 2004 at 15:27, Khamenia, Valery wrote:

> Hi Petr,
> 
> Thank you for your reply.
> 
> > Factor <- function(f,n, decreasing=TRUE, ...) {
> > ooo<-order(levels(factor(n)), decreasing=decreasing)
> > my.order<-levels(factor(f))[ooo]
> > factor(f, levels=my.order)
> > }
> 
> it works incorrectly. Indeed, let's apply with your Factor:
> 
>   unames <- c("thousands", "units", "dozens", "hundreds", "thousands",
> "dozens")
>   u <- c(1000, 1,10,100, 1000, 10)
>   Factor(unames, u)
> 
> the above produces the following output:
> 
>   [1] thousands units     dozens    hundreds  thousands dozens  
>   Levels: units thousands hundreds dozens
> 
> where "dozens" > "hundreds"

Hi

So I obviously used incorect input (or an input which worked with 
my first attempt :-). 

I am not sure if this one is any better but it works with unames and 
u correctly (I hope). 

Factor <- function(f, n, ...) {

ooo<-order(u)
new.ord.unames<-unames[ooo]
factor(f, levels=unique(new.ord.unames))

}


Cheers
Petr

> 
> ---
> Valery

Petr Pikal
petr.pikal at precheza.cz



From bates at stat.wisc.edu  Mon Oct  4 16:35:01 2004
From: bates at stat.wisc.edu (Douglas Bates)
Date: Mon, 04 Oct 2004 09:35:01 -0500
Subject: [R] subsetting
In-Reply-To: <Pine.SOL.4.44.0410041002260.3366-100000@z.glue.umd.edu>
References: <Pine.SOL.4.44.0410041002260.3366-100000@z.glue.umd.edu>
Message-ID: <41615F95.60106@stat.wisc.edu>

You are most of the way to the answer when you use the word "subset". 
There is a subset function to do exactly this.  You probably want 
something like

subframe <- subset(myDataFrame, id %in% myIdSample)

Peter Hyde wrote:
> Hi!  I have never used this before, so please forgive my lack of proper
> protocol...
> 
> Would someone please tell me how to subset a dataframe?
> 
> I have a table of data with x & y (spatial) coordinates, 2 attributes
> (tree height and biomass) and a unique identification number for each
> record
> 
> i have also created an array of random numbers equal to the number of
> records in my data frame
> 
> i am trying to subset, in order to get the records that correspond to my
> unique id numbers
> 
> basically, i am trying to figure out the equivalent to the "where" command
> in IDL (which I understand is FORTRAN-like)
> 
> any insight would be greatly appreciated!!
> 
> thanks!
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html



From andy_liaw at merck.com  Mon Oct  4 16:48:55 2004
From: andy_liaw at merck.com (Liaw, Andy)
Date: Mon, 4 Oct 2004 10:48:55 -0400
Subject: [R] Strange Matrix Multiplication  Behaviour
Message-ID: <3A822319EB35174CA3714066D590DCD504AF84BC@usrymx25.merck.com>

What you need to realize is that R does vectorized computation in `Fortran'
order.  Here's a perhaps more illuminating example:

> tr <- rep(1:2, 3)
> tr
[1] 1 2 1 2 1 2
> ex1 <- as.data.frame(matrix(1:12, 2, 6))
> ex1
  V1 V2 V3 V4 V5 V6
1  1  3  5  7  9 11
2  2  4  6  8 10 12
> tr * ex1[1,]
  V1 V2 V3 V4 V5 V6
1  1  6  5 14  9 22
> (tr * ex1)[1,]
  V1 V2 V3 V4 V5 V6
1  1  3  5  7  9 11

When you do (tr * ex1)[1,], R multiplies tr and ex1 element-by-element,
stacking _columns_ of ex1 to match tr, and recyle the elements of tr to
match the length of the `stacked' ex1.

Andy

> From: Wayne Jones
> 
> Hi there fellow R-users, 
> 
> Im seeing some strange behaviour when I multiply a vector by a matrix
> 
> Here is my script: 
> 
> > tr
>         1         2         3         4         5         6 
> 0.2217903 0.1560525 0.1487908 0.1671354 0.1590643 0.1471667 
> > 
> > ex1
>            a         b          c          d          e          f
> 1  0.2309579 -3.279045 -0.6694697 -1.1024404  0.2303928 -1.5527404
> 2 -0.2865357 -2.519789 -0.1138712 -0.3571832 -2.6727913 -0.3296945
> 
> > is.vector(tr)
> [1] TRUE
> 
> > is.data.frame(ex1)
> [1] TRUE
> 
> > tr* ex1[1,]
>            a          b           c          d          e          f
> 1 0.05122423 -0.5117031 -0.09961092 -0.1842569 0.03664727 -0.2285117
> 
> > (tr* ex1)[1,]
>            a          b          c          d          e          f
> 1 0.05122423 -0.4878917 -0.1064887 -0.2445106 0.03428033 -0.2469855
> 
> 
> Notice that the output from tr * ex[1,] is different from 
> (tr* ex1)[1,]
> Especially element number 4. 
> 
> I would naturally expect both vectors to be equivalent. 
> 
> 
> Can anyone shed any light on my predicament??
> 
> 
> 
> > version
>          _              
> platform i386-pc-mingw32
> arch     i386           
> os       mingw32        
> system   i386, mingw32  
> status                  
> major    1              
> minor    9.1            
> year     2004           
> month    06             
> day      21             
> language R              
> 
> 
> 
> Regards
> 
> Wayne Jones
> 
> 
> 
> 
> 
> 
> KSS Ltd
> Seventh Floor  St James's Buildings  79 Oxford Street  
> Manchester  M1 6SS  England
> Company Registration Number 2800886
> Tel: +44 (0) 161 228 0040	Fax: +44 (0) 161 236 6305
> mailto:kssg at kssg.com		http://www.kssg.com
> 
> 
> The information in this Internet email is confidential and 
> m...{{dropped}}
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
> 
>



From greg_butler at hc-sc.gc.ca  Mon Oct  4 16:49:15 2004
From: greg_butler at hc-sc.gc.ca (Greg Butler)
Date: Mon, 4 Oct 2004 10:49:15 -0400
Subject: [R] Working with large datafiles
Message-ID: <OF9B63F926.1F942927-ON85256F23.00512C57@hc-sc.gc.ca>

Hi,

I have been enjoying r for some time now, but was wondering about working
with larger data files.  When I try to load in big files with more than
20,000 records, the programs seems unbable to store all the records.  Is
there some way that I can increase the size of records that I work with?
Ideally I would like to work with census data which can hold a million
records.

Greg



From andy_liaw at merck.com  Mon Oct  4 16:50:53 2004
From: andy_liaw at merck.com (Liaw, Andy)
Date: Mon, 4 Oct 2004 10:50:53 -0400
Subject: [R] inverse function of order()
Message-ID: <3A822319EB35174CA3714066D590DCD504AF84BD@usrymx25.merck.com>

r[order(o)] seems to work alright...

Andy

> From: Wolfram Fischer
> 
> I have:
> 
>  d <- sample(10:100, 9)
>  o <- order(d)
>  r <- d[o]
> 
> How I can get d (in the original order), knowing only r and o?
> 
> Thanks - Wolfram
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
> 
>



From B.Rowlingson at lancaster.ac.uk  Mon Oct  4 16:51:45 2004
From: B.Rowlingson at lancaster.ac.uk (Barry Rowlingson)
Date: Mon, 04 Oct 2004 15:51:45 +0100
Subject: [R] inverse function of order()
In-Reply-To: <20041004142114.GA4158@s1x.local>
References: <20041004142114.GA4158@s1x.local>
Message-ID: <41616381.1080107@lancaster.ac.uk>

Wolfram Fischer wrote:
> I have:
> 
>  d <- sample(10:100, 9)
>  o <- order(d)
>  r <- d[o]
> 
> How I can get d (in the original order), knowing only r and o?
>

  r[order(o)]?

  > d=sample(10:100,9)
  > o=order(d)
  > r=d[o]
  > all(r[order(o)] == d)
  [1] TRUE

  tested and works for duplicates in d as well.

Barry



From petr.pikal at precheza.cz  Mon Oct  4 16:53:35 2004
From: petr.pikal at precheza.cz (Petr Pikal)
Date: Mon, 04 Oct 2004 16:53:35 +0200
Subject: [R] subsetting
In-Reply-To: <Pine.SOL.4.44.0410041002260.3366-100000@z.glue.umd.edu>
Message-ID: <4161800F.10237.24615FA@localhost>



On 4 Oct 2004 at 10:06, Peter Hyde wrote:

> Hi!  I have never used this before, so please forgive my lack of
> proper protocol...
> 
> Would someone please tell me how to subset a dataframe?
> 
> I have a table of data with x & y (spatial) coordinates, 2 attributes
> (tree height and biomass) and a unique identification number for each
> record
> 
> i have also created an array of random numbers equal to the number of
> records in my data frame
> 
> i am trying to subset, in order to get the records that correspond to
> my unique id numbers

Hi

Chapter 2.7 of introduction manual says:

Subsets of the elements of a vector may be selected by 
appending to the name of the vector
an index vector in square brackets. More generally any 
expression that evaluates to a vector
may have subsets of its elements similarly selected by 
appending an index vector in square
brackets immediately after the expression.

And it gives further examples

Maybe this is what you want?

your.df[your.id.vector==your.id.no,]

shall select rows which have identical id.no and id.vector.
I hope I understood your question as you actually did not 
explained what you tried and how it failed.

Cheers
Petr




> 
> basically, i am trying to figure out the equivalent to the "where"
> command in IDL (which I understand is FORTRAN-like)
> 
> any insight would be greatly appreciated!!
> 
> thanks!
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide!
> http://www.R-project.org/posting-guide.html

Petr Pikal
petr.pikal at precheza.cz



From dimitris.rizopoulos at med.kuleuven.ac.be  Mon Oct  4 16:55:50 2004
From: dimitris.rizopoulos at med.kuleuven.ac.be (Dimitris Rizopoulos)
Date: Mon, 4 Oct 2004 16:55:50 +0200
Subject: [R] inverse function of order()
References: <20041004142114.GA4158@s1x.local>
Message-ID: <00a001c4aa22$3d6f4990$b2133a86@www.domain>

Hi Wolfram,

just use:

r[order(o)]

I hope it helps.

Best,
Dimitris

----
Dimitris Rizopoulos
Ph.D. Student
Biostatistical Centre
School of Public Health
Catholic University of Leuven

Address: Kapucijnenvoer 35, Leuven, Belgium
Tel: +32/16/396887
Fax: +32/16/337015
Web: http://www.med.kuleuven.ac.be/biostat/
     http://www.student.kuleuven.ac.be/~m0390867/dimitris.htm

----- Original Message ----- 
From: "Wolfram Fischer" <wolfram at fischer-zim.ch>
To: <r-help at stat.math.ethz.ch>
Sent: Monday, October 04, 2004 4:21 PM
Subject: [R] inverse function of order()


>I have:
>
> d <- sample(10:100, 9)
> o <- order(d)
> r <- d[o]
>
> How I can get d (in the original order), knowing only r and o?
>
> Thanks - Wolfram
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
>



From petr.pikal at precheza.cz  Mon Oct  4 16:56:38 2004
From: petr.pikal at precheza.cz (Petr Pikal)
Date: Mon, 04 Oct 2004 16:56:38 +0200
Subject: [R] inverse function of order()
In-Reply-To: <20041004142114.GA4158@s1x.local>
Message-ID: <416180C6.184.248E0F1@localhost>



On 4 Oct 2004 at 16:21, Wolfram Fischer wrote:

> I have:
> 
>  d <- sample(10:100, 9)
>  o <- order(d)
>  r <- d[o]
> 
> How I can get d (in the original order), knowing only r and o?
Hi

try to sort it again

r[order(o)]

(not sure about ties, though)

Cheers
Petr


> 
> Thanks - Wolfram
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide!
> http://www.R-project.org/posting-guide.html

Petr Pikal
petr.pikal at precheza.cz



From Achim.Zeileis at wu-wien.ac.at  Mon Oct  4 16:57:16 2004
From: Achim.Zeileis at wu-wien.ac.at (Achim Zeileis)
Date: Mon, 4 Oct 2004 16:57:16 +0200
Subject: [R] inverse function of order()
In-Reply-To: <20041004142114.GA4158@s1x.local>
References: <20041004142114.GA4158@s1x.local>
Message-ID: <20041004165716.7fb2bedb.Achim.Zeileis@wu-wien.ac.at>

On Mon, 4 Oct 2004 16:21:14 +0200 Wolfram Fischer wrote:

> I have:
> 
>  d <- sample(10:100, 9)
>  o <- order(d)
>  r <- d[o]
> 
> How I can get d (in the original order), knowing only r and o?

r[order(o)]

hth
Z

> Thanks - Wolfram
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide!
> http://www.R-project.org/posting-guide.html
>



From ripley at stats.ox.ac.uk  Mon Oct  4 16:56:49 2004
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Mon, 4 Oct 2004 15:56:49 +0100 (BST)
Subject: [R] nlme: cannot allocate vector of size 126064 Kb
In-Reply-To: <Pine.LNX.4.61.0409301443140.22886@hydra.stat.auckland.ac.nz>
Message-ID: <Pine.LNX.4.44.0410041556030.17609-100000@gannet.stats>

On Thu, 30 Sep 2004, David Scott wrote:

> 
> I have around 4000 observations of a time series. I am trying to fit a 
> regression with ARMA error structure using gls from the package nlme.
> 
> I have encountered the error: cannot allocate vector of size 126064 Kb
> 
> I know this has come up many times before and I will check out the 
> suggestions in the mail archive. I was wondering though if there is an 
> alternative package that will fit such a model? I did a quick help.search 
> on ARIMA and ARMA but only found arma and arma0.

Both will `fit a regression with ARMA error structure', recursively rather 
then via computing a large matrix.

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From reid_huntsinger at merck.com  Mon Oct  4 16:58:47 2004
From: reid_huntsinger at merck.com (Huntsinger, Reid)
Date: Mon, 4 Oct 2004 10:58:47 -0400
Subject: [R] Strange Matrix Multiplication  Behaviour
Message-ID: <D9A95B4B7B20354992E165EEADA31999056A9115@uswpmx00.merck.com>

I think this is as intended. When you do elementwise multiplication of a
vector by a matrix R there are three important points to keep in mind: 

1) element-wise multiplication treats things as vectors
2) when you treat a matrix as a vector you get the _columns_ stacked one on
top of another. 
3) R recycles elements of the shorter vector

So tr*(ex[1,]) is the same as multiplying _row_ 1 by the elements of tr, but
(tr*ex)[1,] multiplies the first _column_ of ex by the first 2 elements of
tr, then the second column by the second two, then the third column by the
third two, then recycles for columns 4, 5 and 6. Then the [1,] takes the
first row of the result, so you (tr[1] tr[3] tr[5] tr[1] tr[3] tr[5]) times
the first row. Which looks like what you have.

Reid Huntsinger

-----Original Message-----
From: r-help-bounces at stat.math.ethz.ch
[mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Wayne Jones
Sent: Monday, October 04, 2004 10:11 AM
To: R help list
Subject: [R] Strange Matrix Multiplication Behaviour



Hi there fellow R-users, 

Im seeing some strange behaviour when I multiply a vector by a matrix

Here is my script: 

> tr
        1         2         3         4         5         6 
0.2217903 0.1560525 0.1487908 0.1671354 0.1590643 0.1471667 
> 
> ex1
           a         b          c          d          e          f
1  0.2309579 -3.279045 -0.6694697 -1.1024404  0.2303928 -1.5527404
2 -0.2865357 -2.519789 -0.1138712 -0.3571832 -2.6727913 -0.3296945

> is.vector(tr)
[1] TRUE

> is.data.frame(ex1)
[1] TRUE

> tr* ex1[1,]
           a          b           c          d          e          f
1 0.05122423 -0.5117031 -0.09961092 -0.1842569 0.03664727 -0.2285117

> (tr* ex1)[1,]
           a          b          c          d          e          f
1 0.05122423 -0.4878917 -0.1064887 -0.2445106 0.03428033 -0.2469855


Notice that the output from tr * ex[1,] is different from (tr* ex1)[1,]
Especially element number 4. 

I would naturally expect both vectors to be equivalent. 


Can anyone shed any light on my predicament??



> version
         _              
platform i386-pc-mingw32
arch     i386           
os       mingw32        
system   i386, mingw32  
status                  
major    1              
minor    9.1            
year     2004           
month    06             
day      21             
language R              



Regards

Wayne Jones






KSS Ltd
Seventh Floor  St James's Buildings  79 Oxford Street  Manchester  M1 6SS
England
Company Registration Number 2800886
Tel: +44 (0) 161 228 0040	Fax: +44 (0) 161 236 6305
mailto:kssg at kssg.com		http://www.kssg.com


The information in this Internet email is confidential and m...{{dropped}}

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide!
http://www.R-project.org/posting-guide.html



From deepayan at stat.wisc.edu  Mon Oct  4 17:01:09 2004
From: deepayan at stat.wisc.edu (Deepayan Sarkar)
Date: Mon, 4 Oct 2004 10:01:09 -0500
Subject: [R] inverse function of order()
In-Reply-To: <20041004142114.GA4158@s1x.local>
References: <20041004142114.GA4158@s1x.local>
Message-ID: <200410041001.09284.deepayan@stat.wisc.edu>

On Monday 04 October 2004 09:21, Wolfram Fischer wrote:
> I have:
>
>  d <- sample(10:100, 9)
>  o <- order(d)
>  r <- d[o]
>
> How I can get d (in the original order), knowing only r and o?

Perhaps

r[order(o)]

Deepayan



From andy_liaw at merck.com  Mon Oct  4 17:00:55 2004
From: andy_liaw at merck.com (Liaw, Andy)
Date: Mon, 4 Oct 2004 11:00:55 -0400
Subject: [R] Beginners problem
Message-ID: <3A822319EB35174CA3714066D590DCD504AF84BE@usrymx25.merck.com>

[Please try to make sure the code you posted contains no syntax error.]

Try:

> rktest <- function(){
+     rk4 <- function(x,y,f,h) {
+         n  <- length(y)
+         hh <- h*0.5
+         k1 <- f(x,y)
+         k2 <- f(x,y+k1*hh)
+         k3 <- f(x,y+k2*hh)
+         k4 <- f(x,y+k3*h)
+         y <<- y + h/6.0*(k1 + 2.0*k2 + 2.0*k3 + k4)
+     }
+     y <- 1.0
+     x <- 0.0
+     h <- 0.1
+     for(i in 1:10) {
+         rk4(x,y,rkf,h)
+         print(y)
+     }
+ }
> 
> rktest()
[1] 0.9048375
[1] 0.8187309
[1] 0.7408184
[1] 0.6703203
[1] 0.6065309
[1] 0.5488119
[1] 0.4965856
[1] 0.4493293
[1] 0.40657
[1] 0.3678798

You expected `y' in rktest() to be modified by the super-assignment, which
is incorrect.  In your version, you'll find the super-assignment creating
`y' in the global environment, rather than modifying the copy in rktest().
The reason is that the order variables are looked up in rk4() is:

1. within rk4()
2. where rk4() is defined
3. if 2 above is a function, where that function is defined
...
eventually: the global environment

BTW, I'm not sure if you really meant to do 

   y <<- y + ...

because the `y' on the RHS will be the local copy (if exists).

Andy

> From: Rolf Wester
> 
> Hi,
> 
> I'm new to R and have a problem with a little test program 
> (see below). 
> Why doesn't <<- in function rk4
> assign the new value to y so that it is seen in rktest. I 
> thought that 
> <<- does exactly this. But it seems that I
> didn't get it right. I would be very appreciative for an 
> explanation of 
> that behaviour of <<-. I know how to
> write the whole thing so that it works (return the updated y 
> to rktest) 
> but I would like to learn how variable scope
> in R works.
> 
> Thanks in advance
> 
> Rolf
> 
> --------------------------------------------------------------
> --------------------------------------------------------
> 
> rk4 <- function(x,y,f,h) {
>   n  <- length(y)
>   hh <- h*0.5
>   k1 <- f(x,y)
>   k2 <- f(x,y+k1*hh)
>   k3 <- f(x,y+k2*hh)
>   k4 <- f(x,y+k3*h)
>   y <<- y + h/6.0*(k1 + 2.0*k2 + 2.0*k3 + k4))
> }
> 
> rkf <- function(x,y) {
>   -y
> }
> 
> rktest <- function(){
>   y <- 1.0
>   x <- 0.0
>   h <- 0.1
>   for(i in 1:10) {
>     rk4(x,y,rkf,h)
>     print(y)
>   }
> }
> 
> rktest()
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
> 
>



From kjetil at acelerate.com  Mon Oct  4 17:02:39 2004
From: kjetil at acelerate.com (Kjetil Brinchmann Halvorsen)
Date: Mon, 04 Oct 2004 11:02:39 -0400
Subject: [R] scatter plot and marginal
In-Reply-To: <1096896806.41615126bb6a1@webmail.uni-bocconi.it>
References: <1096896806.41615126bb6a1@webmail.uni-bocconi.it>
Message-ID: <4161660F.2080202@acelerate.com>

Paolo Bulla wrote:

>Hallo,
>
>I would like to add the marginal distributions along the X and the Y axis to a
>scatter plot.
>Can anybody help me, please?
>
>Thank you,
> Paolo
>  
>
The simplest thing you can do is like
plot(x, y)
rug(x, side=1, col="red")
rug(y, side=2, col="red")

or if there are many coincident x (y) values
rug(jitter(x), side=1, col="red")
rug(jitter(y), side=2, col="red")

Kjetil

-- 

Kjetil Halvorsen.

Peace is the most effective weapon of mass construction.
               --  Mahdi Elmandjra



From jfox at mcmaster.ca  Mon Oct  4 17:04:00 2004
From: jfox at mcmaster.ca (John Fox)
Date: Mon, 04 Oct 2004 11:04:00 -0400
Subject: [R] scatter plot and marginal
In-Reply-To: <1096896806.41615126bb6a1@webmail.uni-bocconi.it>
Message-ID: <web-67284886@cgpsrv2.cis.mcmaster.ca>

Dear Paolo,

The scatterplot() function in the car package can show marginal
boxplots.

I hope this helps,
 John


On Mon,  4 Oct 2004 15:33:26 +0200
 Paolo Bulla <paolo.bulla at unibocconi.it> wrote:
> 
> Hallo,
> 
> I would like to add the marginal distributions along the X and the Y
> axis to a
> scatter plot.
> Can anybody help me, please?
> 
> Thank you,
>  Paolo
> -- 
> Paolo Bulla
> 
> Istituto di Metodi Quantitativi
> Universit?? "L. Bocconi"
> viale Isonzo 25
> 20136 Milano
> paolo.bulla at unibocconi.it
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide!
> http://www.R-project.org/posting-guide.html

--------------------------------
John Fox
Department of Sociology
McMaster University
Hamilton, Ontario, Canada
http://socserv.mcmaster.ca/jfox/



From ripley at stats.ox.ac.uk  Mon Oct  4 17:04:42 2004
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Mon, 4 Oct 2004 16:04:42 +0100 (BST)
Subject: [R] Strange Matrix Multiplication  Behaviour
In-Reply-To: <6B5A9304046AD411BD0200508BDFB6CB02FEFD77@gimli.middleearth.kssg.com>
Message-ID: <Pine.LNX.4.44.0410041559300.17609-100000@gannet.stats>

This is *not* matrix multiplication, which uses %*% in R.
Might that help explain your confusion?

The difference is explained in `An Introduction to R'.

On Mon, 4 Oct 2004, Wayne Jones wrote:

> 
> Hi there fellow R-users, 
> 
> Im seeing some strange behaviour when I multiply a vector by a matrix
> 
> Here is my script: 
> 
> > tr
>         1         2         3         4         5         6 
> 0.2217903 0.1560525 0.1487908 0.1671354 0.1590643 0.1471667 
> > 
> > ex1
>            a         b          c          d          e          f
> 1  0.2309579 -3.279045 -0.6694697 -1.1024404  0.2303928 -1.5527404
> 2 -0.2865357 -2.519789 -0.1138712 -0.3571832 -2.6727913 -0.3296945
> 
> > is.vector(tr)
> [1] TRUE
> 
> > is.data.frame(ex1)
> [1] TRUE
> 
> > tr* ex1[1,]
>            a          b           c          d          e          f
> 1 0.05122423 -0.5117031 -0.09961092 -0.1842569 0.03664727 -0.2285117
> 
> > (tr* ex1)[1,]
>            a          b          c          d          e          f
> 1 0.05122423 -0.4878917 -0.1064887 -0.2445106 0.03428033 -0.2469855
> 
> 
> Notice that the output from tr * ex[1,] is different from (tr* ex1)[1,]
> Especially element number 4. 
> 
> I would naturally expect both vectors to be equivalent. 

Why?  The second multiplies the elements of ex by tr, in column-major 
order, and what is `natural' about that?

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From ripley at stats.ox.ac.uk  Mon Oct  4 17:06:10 2004
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Mon, 4 Oct 2004 16:06:10 +0100 (BST)
Subject: [R] inverse function of order()
In-Reply-To: <20041004142114.GA4158@s1x.local>
Message-ID: <Pine.LNX.4.44.0410041605040.17609-100000@gannet.stats>

On Mon, 4 Oct 2004, Wolfram Fischer wrote:

> I have:
> 
>  d <- sample(10:100, 9)
>  o <- order(d)
>  r <- d[o]
> 
> How I can get d (in the original order), knowing only r and o?

d <- r
d[o] <- r

It's a simple inverse, as your subject suggests.

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From nassar at noos.fr  Mon Oct  4 17:09:36 2004
From: nassar at noos.fr (Nassar)
Date: Mon, 4 Oct 2004 17:09:36 +0200
Subject: [R] A newbye question : Creating a grey palette
In-Reply-To: <Pine.LNX.4.44.0410041437470.10475-100000@gannet.stats>
Message-ID: <001d01c4aa24$29f6c800$6501a8c0@NajiNassar>

Hi all


I'd like to create a grey palette for a filled.contour()
from blank (smallest values) to black (large values)

TiA
Naji



From ahenningsen at email.uni-kiel.de  Mon Oct  4 17:09:12 2004
From: ahenningsen at email.uni-kiel.de (Arne Henningsen)
Date: Mon, 4 Oct 2004 17:09:12 +0200
Subject: [R] Strange Matrix Multiplication  Behaviour
In-Reply-To: <6B5A9304046AD411BD0200508BDFB6CB02FEFD77@gimli.middleearth.kssg.com>
References: <6B5A9304046AD411BD0200508BDFB6CB02FEFD77@gimli.middleearth.kssg.com>
Message-ID: <200410041709.12373.ahenningsen@email.uni-kiel.de>

Hi Wayne,

are you aware that you are NOT doing matrix multiplication? A matrix 
multiplication is done by %*%, e.g.
> tr %*% t(as.matrix(ex1)

Or do you want a normal scalar multiplication with the elements of each row of 
ex1 to be multiplied with the corresponding element of tr? However, R does 
the multiplication not row-wise, but column-wise. Thus, the results are not 
as you expected. To do a row-wise multiplication you can transform the 
data.frame:

> tr <- as.vector(10*c(1:6))
> tr
[1] 10 20 30 40 50 60
> ex1 <- as.data.frame(matrix(1:12,nrow=2,byrow=TRUE))
> ex1
  V1 V2 V3 V4 V5 V6
1  1  2  3  4  5  6
2  7  8  9 10 11 12
> tr * ex1
   V1  V2  V3  V4  V5  V6
1  10  60 150  40 150 300
2 140 320 540 200 440 720
> t( tr * t(ex1) )
  V1  V2  V3  V4  V5  V6
1 10  40  90 160 250 360
2 70 160 270 400 550 720
> t( tr * t(ex1) )[1,]
 V1  V2  V3  V4  V5  V6
 10  40  90 160 250 360
> t( tr * t(ex1[1,]) )
  V1 V2 V3  V4  V5  V6
1 10 40 90 160 250 360

I hope that this makes it clear.

Best wishes,
Arne


On Monday 04 October 2004 16:10, Wayne Jones wrote:
> Hi there fellow R-users,
>
> Im seeing some strange behaviour when I multiply a vector by a matrix
>
> Here is my script:
> > tr
>
>         1         2         3         4         5         6
> 0.2217903 0.1560525 0.1487908 0.1671354 0.1590643 0.1471667
>
> > ex1
>
>            a         b          c          d          e          f
> 1  0.2309579 -3.279045 -0.6694697 -1.1024404  0.2303928 -1.5527404
> 2 -0.2865357 -2.519789 -0.1138712 -0.3571832 -2.6727913 -0.3296945
>
> > is.vector(tr)
>
> [1] TRUE
>
> > is.data.frame(ex1)
>
> [1] TRUE
>
> > tr* ex1[1,]
>
>            a          b           c          d          e          f
> 1 0.05122423 -0.5117031 -0.09961092 -0.1842569 0.03664727 -0.2285117
>
> > (tr* ex1)[1,]
>
>            a          b          c          d          e          f
> 1 0.05122423 -0.4878917 -0.1064887 -0.2445106 0.03428033 -0.2469855
>
>
> Notice that the output from tr * ex[1,] is different from (tr* ex1)[1,]
> Especially element number 4.
>
> I would naturally expect both vectors to be equivalent.
>
>
> Can anyone shed any light on my predicament??
>
> > version
>
>          _
> platform i386-pc-mingw32
> arch     i386
> os       mingw32
> system   i386, mingw32
> status
> major    1
> minor    9.1
> year     2004
> month    06
> day      21
> language R
>
>
>
> Regards
>
> Wayne Jones
>
>
>
>
>
>
> KSS Ltd
> Seventh Floor  St James's Buildings  79 Oxford Street  Manchester  M1 6SS 
> England Company Registration Number 2800886
> Tel: +44 (0) 161 228 0040	Fax: +44 (0) 161 236 6305
> mailto:kssg at kssg.com		http://www.kssg.com
>
>
> The information in this Internet email is confidential and m...{{dropped}}
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide!
> http://www.R-project.org/posting-guide.html

-- 
Arne Henningsen
Department of Agricultural Economics
University of Kiel
Olshausenstr. 40
D-24098 Kiel (Germany)
Tel: +49-431-880 4445
Fax: +49-431-880 1397
ahenningsen at agric-econ.uni-kiel.de
http://www.uni-kiel.de/agrarpol/ahenningsen/



From JonesW at kssg.com  Mon Oct  4 17:28:42 2004
From: JonesW at kssg.com (Wayne Jones)
Date: Mon, 4 Oct 2004 16:28:42 +0100 
Subject: [R] Strange Matrix Multiplication  Behaviour
Message-ID: <6B5A9304046AD411BD0200508BDFB6CB02FEFD82@gimli.middleearth.kssg.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20041004/417cff5c/attachment.pl

From ligges at statistik.uni-dortmund.de  Mon Oct  4 17:34:27 2004
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Mon, 04 Oct 2004 17:34:27 +0200
Subject: [R] inverse function of order()
In-Reply-To: <20041004142114.GA4158@s1x.local>
References: <20041004142114.GA4158@s1x.local>
Message-ID: <41616D83.3010508@statistik.uni-dortmund.de>

Wolfram Fischer wrote:

> I have:
> 
>  d <- sample(10:100, 9)
>  o <- order(d)
>  r <- d[o]
> How I can get d (in the original order), knowing only r and o?

r[order(o)]

Uwe Ligges


> Thanks - Wolfram
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html



From gavin.simpson at ucl.ac.uk  Mon Oct  4 17:34:50 2004
From: gavin.simpson at ucl.ac.uk (Gavin Simpson)
Date: Mon, 04 Oct 2004 16:34:50 +0100
Subject: [R] Strange Matrix Multiplication  Behaviour
In-Reply-To: <6B5A9304046AD411BD0200508BDFB6CB02FEFD77@gimli.middleearth.kssg.com>
References: <6B5A9304046AD411BD0200508BDFB6CB02FEFD77@gimli.middleearth.kssg.com>
Message-ID: <41616D9A.7090109@ucl.ac.uk>

Wayne Jones wrote:
> Hi there fellow R-users, 
> 
> Im seeing some strange behaviour when I multiply a vector by a matrix
> 
> Here is my script: 
> 

"*" does array or element-wise multiplication. %*% is Matrix multiplication.

In the first case, you are multiplying tr by the first row of ex1

 > tr * ex1[1, ]
            a          b           c          d          e          f
1 0.05122422 -0.5117032 -0.09961093 -0.1842568 0.03664727 -0.2285117

In the second, you are extracting the first row of the result of 
multiplying tr by ex1, which as we see below returns a 2x6 matrix:

 > tr * ex1
             a          b           c           d           e           f
1  0.05122422 -0.4878917 -0.10648873 -0.24451059  0.03428033 -0.24698556
2 -0.04471461 -0.4211459 -0.01675805 -0.05573933 -0.44671804 -0.04852005
 > (tr * ex1)[1, ]
            a          b          c          d          e          f
1 0.05122422 -0.4878917 -0.1064887 -0.2445106 0.03428033 -0.2469856

Note that tr gets recycled in the tr * ex1 and (tr * ex1)[1, ] as tr is 
not as long as ex1.

Element 1b in (tr * ex1)[1, ] is formed by multiplying element 1b of ex1 
(-3.279045) by the third element of the vector tr (0.1487908):

 > tr[3] * ex1[1,2]
[1] -0.4878917

the second value of the vector tr was used to multiply against element 
2a in matrix ex1:

but in the tr * ex1[1, ] case tr and ex1[1, ] both contain 6 elements 
and the element 1b in the results is formed by multiplying element 1b of 
ex1[1, ] (-3.279045) by the second element of vector tr (0.1560525)

 > tr[2] * ex1[1,2]
[1] -0.5117032

> 
> Notice that the output from tr * ex[1,] is different from (tr* ex1)[1,]
> Especially element number 4. 

The difference is due to recycling of tr and of where you are doing your 
sub setting

> I would naturally expect both vectors to be equivalent. 
> 

Hopefully my longwinded explanation helps.

Gav
-- 
%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%
Gavin Simpson                     [T] +44 (0)20 7679 5522
ENSIS Research Fellow             [F] +44 (0)20 7679 7565
ENSIS Ltd. & ECRC                 [E] gavin.simpson at ucl.ac.uk
UCL Department of Geography       [W] http://www.ucl.ac.uk/~ucfagls/cv/
26 Bedford Way                    [W] http://www.ucl.ac.uk/~ucfagls/
London.  WC1H 0AP.
%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%



From arnehe at gmx.de  Mon Oct  4 17:35:28 2004
From: arnehe at gmx.de (Arne Henningsen)
Date: Mon, 4 Oct 2004 17:35:28 +0200
Subject: [R] Strange Matrix Multiplication  Behaviour
In-Reply-To: <6B5A9304046AD411BD0200508BDFB6CB02FEFD77@gimli.middleearth.kssg.com>
References: <6B5A9304046AD411BD0200508BDFB6CB02FEFD77@gimli.middleearth.kssg.com>
Message-ID: <200410041735.29145.arnehe@gmx.de>

Hi Wayne,

are you aware that you are NOT doing matrix multiplication? A matrix 
multiplication is done by %*%, e.g.
> tr %*% t(as.matrix(ex1)

Or do you want a normal scalar multiplication with the elements of each row of 
ex1 to be multiplied with the corresponding element of tr? However, R does 
the multiplication not row-wise, but column-wise. Thus, the results are not 
as you expected. To do a row-wise multiplication you can transform the 
data.frame:

> tr <- as.vector(10*c(1:6))
> tr
[1] 10 20 30 40 50 60
> ex1 <- as.data.frame(matrix(1:12,nrow=2,byrow=TRUE))
> ex1
  V1 V2 V3 V4 V5 V6
1  1  2  3  4  5  6
2  7  8  9 10 11 12
> tr * ex1
   V1  V2  V3  V4  V5  V6
1  10  60 150  40 150 300
2 140 320 540 200 440 720
> t( tr * t(ex1) )
  V1  V2  V3  V4  V5  V6
1 10  40  90 160 250 360
2 70 160 270 400 550 720
> t( tr * t(ex1) )[1,]
 V1  V2  V3  V4  V5  V6
 10  40  90 160 250 360
> t( tr * t(ex1[1,]) )
  V1 V2 V3  V4  V5  V6
1 10 40 90 160 250 360

I hope that this makes it clear.

Best wishes,
Arne


On Monday 04 October 2004 16:10, Wayne Jones wrote:
> Hi there fellow R-users,
>
> Im seeing some strange behaviour when I multiply a vector by a matrix
>
> Here is my script:
> > tr
>
>         1         2         3         4         5         6
> 0.2217903 0.1560525 0.1487908 0.1671354 0.1590643 0.1471667
>
> > ex1
>
>            a         b          c          d          e          f
> 1  0.2309579 -3.279045 -0.6694697 -1.1024404  0.2303928 -1.5527404
> 2 -0.2865357 -2.519789 -0.1138712 -0.3571832 -2.6727913 -0.3296945
>
> > is.vector(tr)
>
> [1] TRUE
>
> > is.data.frame(ex1)
>
> [1] TRUE
>
> > tr* ex1[1,]
>
>            a          b           c          d          e          f
> 1 0.05122423 -0.5117031 -0.09961092 -0.1842569 0.03664727 -0.2285117
>
> > (tr* ex1)[1,]
>
>            a          b          c          d          e          f
> 1 0.05122423 -0.4878917 -0.1064887 -0.2445106 0.03428033 -0.2469855
>
>
> Notice that the output from tr * ex[1,] is different from (tr* ex1)[1,]
> Especially element number 4.
>
> I would naturally expect both vectors to be equivalent.
>
>
> Can anyone shed any light on my predicament??
>
> > version
>
>          _
> platform i386-pc-mingw32
> arch     i386
> os       mingw32
> system   i386, mingw32
> status
> major    1
> minor    9.1
> year     2004
> month    06
> day      21
> language R
>
>
>
> Regards
>
> Wayne Jones
>
>
>
>
>
>
> KSS Ltd
> Seventh Floor  St James's Buildings  79 Oxford Street  Manchester  M1 6SS 
> England Company Registration Number 2800886
> Tel: +44 (0) 161 228 0040	Fax: +44 (0) 161 236 6305
> mailto:kssg at kssg.com		http://www.kssg.com
>
>
> The information in this Internet email is confidential and m...{{dropped}}
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide!
> http://www.R-project.org/posting-guide.html

-- 
Arne Henningsen
Department of Agricultural Economics
University of Kiel
Olshausenstr. 40
D-24098 Kiel (Germany)
Tel: +49-431-880 4445
Fax: +49-431-880 1397
ahenningsen at agric-econ.uni-kiel.de
http://www.uni-kiel.de/agrarpol/ahenningsen/



From ramasamy at cancer.org.uk  Mon Oct  4 17:39:39 2004
From: ramasamy at cancer.org.uk (Adaikalavan Ramasamy)
Date: Mon, 04 Oct 2004 16:39:39 +0100
Subject: [R] inverse function of order()
In-Reply-To: <20041004142114.GA4158@s1x.local>
References: <20041004142114.GA4158@s1x.local>
Message-ID: <1096904379.3357.7.camel@ndmpc126.ihs.ox.ac.uk>

r[ order(o) ] will give you the answer. More generally,

 x <- rnorm(100)
 identical(x, x[ order(x) ][ order(order(x)) ])
[1] TRUE



On Mon, 2004-10-04 at 15:21, Wolfram Fischer wrote:
> I have:
> 
>  d <- sample(10:100, 9)
>  o <- order(d)
>  r <- d[o]
> 
> How I can get d (in the original order), knowing only r and o?
> 
> Thanks - Wolfram
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>



From ggrothendieck at myway.com  Mon Oct  4 17:39:42 2004
From: ggrothendieck at myway.com (Gabor Grothendieck)
Date: Mon,  4 Oct 2004 11:39:42 -0400 (EDT)
Subject: [R] conditional assignments and calculations
Message-ID: <20041004153942.57FBE39DD@mprdmxin.myway.com>



Answers interspersed.

Michael Lachmann <lachmann <at> eva.mpg.de> writes:
: 
: Thank you!
: Now the conditional assignments work almost prefectly.
: The current code is at the bottom of the message.
: Now I can do
: depends[A,B]=B+9
: and this will be executed only if B was more recently updated then A, and if the last time A was updated, the
: expression used was 'B+9'.
: 
: And it is also possible to do
: depends[A,file="table.txt"]=read.table("table.txt")
: and A will only be read again if the file "table.txt" was updated more recently than it was read last time.
: 
: I have only a few remaining questions:
: 1. I added attributes to variables, "last.updated", and "update.expression". If the variables are just
: regular vectors/lists, then when I print them, I also see these attributes. Is it somehow possible to have
: these attributes hidden?

If you assign them to the comment attribute they will be hidden upon
printing.

comment(x) <- "...whatever..."

They must assigned as a character string or a vector of character strings.  See ?comment.
: 
: 2. I defined an operator %set%, which is supposed to work just like "<-" and "=", but also updates
: "last.updated" and "update.expression". It would be best to really replace "<-" and "=" with this
: operator, and then any update of a variable will be registered. This doesn't seem to work, because if I
: define "<-" as  
: function(x,value)
: {
:   v=as.character(as.expression(substitute(value)))
:   eval.parent(substitute(.Primitive("<-")(x,value)))
:   eval.parent(substitute(attr(x,"last.updated")<-Sys.time()))
:   eval.parent(substitute(attr(x,"update.expression")<-v))
:   x
: }
: then this function calls "attr<-", which then calls "<-" leading to infinite recursion. Is there a way to
: set an attribute on a variable without using attr<- ?
: 
: Or maybe it is a bad idea to redefine "<-" in the first place...

Its probably not a good idea to redefine <-.

You could define your own class and then assignment within that class
could be defined but, of course, that would only work with variables
of your class.  You could also define set as a replacement function
so that if you did this:

set(x) <- ...

x would have its time updated assuming you defined your own "set<-"

: 

: Thank you very much!
: Michael
: 
: Gabor Grothendieck wrote:
: 
: >Michael Lachmann <lachmann <at> eva.mpg.de> writes:
: >
: >: 
: >: Hello!
: >: 
: >: I am using the TeXmacs interface to R. (Though I encountered a similar 
: >: problem when using Sweave)
: >: In doing calculations I often ecounter this scenario: I'll have some 
: >: calculations in my file:
: >: --
: >: A=read.lots.of.data()
: >: 
: >: B=huge.calculation.on(A)
: >: 
: >: C=another.calculation.on(B)
: >: --
: >: Now, if A has already been read, I don't need to re-read it. If B has 
: >: already been calculated, I don't need to recalculate it. But I would 
: >: like to be able to just press 'enter' on each of them.
: >: 
: >: So, I would like R to somehow figure out dependencies (a bit like in 
: >: Makefiles)
: >: 
: >: I implemented something like this with the following functions 
: 
: ...
: 
: >: But this solution is quite ugly, because of several problems:
: >: 
: >: 1. To call 'depends(A,B)=f(B)' the first time, A has to already exist, 
: >: otherwise I get an error (before I enter the "depends<-" function.)
: >
: >The technique used to implement mulitple return values shown in 
: >
: >   http://tolstoy.newcastle.edu.au/R/help/04/06/1406.html
: >
: >could be adapted to this problem.  Using that technique the code would 
: >look like this:
: >
: >   depends[A,B] <- f(B)
: >
: >and A would not have to pre-exist.
: >
: >You define a structure with a class of depends, say:
: >
: >   depends <- structure(NA, class = "depends")
: >
: >and then define the [<-.depends action on that structure
: >in an analogous way to what was done there.
: >
: >: 2. I would also like to have a convenient way to do
: >: "if( !exists(A) ) { A=read.lots.of.data(); A=touch(A) }"
: >: maybe something like:
: >: depends(A)<-read.lots.of.data()
: >: But that doesn't work, because of 1.
: >: or
: >: A %set% read.lots.data()
: >: But that doesn't work, because I haven't figured out a way for a 
: >: function to change one of its variables.
: >: (Maybe I could do A=A %set read.lots.of.data(), but that is really ugly...)
: >
: >
: >Is this what you want?
: >
: >R> f <- function(x,v) assign(as.character(substitute(x)), v, parent.frame())
: >R> x # x does not exist
: >Error: Object "x" not found
: >R> f(x,3)
: >R> x # now it does
: >[1] 3
: >
: ><<- can be used if the   eval.parent is a third way (see #3 below).
: >
: >: 3. It would be nice to be able to do touch(A) instead of A=touch(A)
: >
: >touch <- function(x) 
: >    eval.parent(substitute(attr(x,"last.updated")<-Sys.time())) 
: >x <- 3
: >touch(x)
: >
: >: 
: >: 4. If I modify A without calling 'A=touch(A)', then B will not be 
: >: updated next time I call 'depends(B,A)=huge.calculation.on(A)'. So it 
: >: would be nice to have the variable's 'last updated' time updated 
: >: automatically. (Though then it is a bit problematic to decide what the 
: >: 'last updated' time should be for variables loaded from a file...)
: >
: >If its done in a function you could use on.exit to ensure that it gets
: >updated when leaving the function.
: >
: 
: touch=function(...,l=list())
: {
:     args <- as.list(match.call())
:     if( "l" %in% names(args) ) {
: 	    argsl=as.list(args[[length(args)]][-1])
: 	    argsl=sapply(argsl,function(x) parse(text=x)[[1]])
: 	    args=args[-length(args)]
:   } else { argsl=c() }
:     args=c(args[-1],argsl)
:     for( i in 1:length(args)) {
:       eval.parent(substitute(attr(x,"last.updated")<-Sys.time(),
:       list(x=args[[i]]) ))
:     }
: }
: 
: last.updated=function(a) {   
:   if( length(attr(a,"last.updated")) == 0 ) {
:       Sys.time()
:   } else {
:       attr(a,"last.updated")
:   }
: }
: 
: "%set%"=function(x,value)
: {
:   v=as.character(as.expression(substitute(value)))
:   eval.parent(substitute(.Primitive("<-")(x,value)))
:   eval.parent(substitute(attr(x,"last.updated")<-Sys.time()))
:   eval.parent(substitute(attr(x,"update.expression")<-v))
:   x
: }
: 
: depends <- structure(NA,class="depends")
: 
: "[<-.depends" <- function(x,...,file=c(),value) {
: 	  v=as.character(as.expression(substitute(value)))
:   args <- as.list(match.call())
:   if( !exists(as.character(args[[3]] ), env=sys.frame(-1)) ) {
:       eval(substitute(x<-v,list(x=args[[3]],v=value)),env=sys.frame(-1))
:       eval(substitute(touch(x),list(x=args[[3]])),env=sys.frame(-1))
:       eval.parent(substitute(attr(x,"update.expression")<-v,list(x=args[[3]],v=v)))
:   } else {
:     lu=list()
: 		xlu=eval(substitute(attr(x,"last.updated"),list(x=args[[3]]),env=sys.frame(-1)))
:     if( length(file) >0 ) {
: 	    lu=c(lapply(file,function(f) {
: 		    file.info(f)$mtime
: 		    }),lu) 
:       args=args[-(length(args)-1)]
:     }
:     if( length(xlu)>0 ) {
:         lu=c(lapply((1:(length(args)-1))[-c(1:3)],function(i) {
:             eval(substitute(last.updated(x),list(x=args[[i]])),env=sys.frame(-1))
:         }),lu)
:     }
:     lu=prod(sapply(lu,function(x) x-xlu<0))
: 	  xv=eval(substitute(attr(x,"update.expression"),list(x=args[[3]]),env=sys.frame(-1))) 
: 	  if( length(xv)*length(v) > 0 ) {
: 	    if( xv != v ) lu = 0
:     } else { lu=0} 
:     if( (lu==0) ) {
:           eval(substitute(x<-v,list(x=args[[3]],v=value)),env=sys.frame(-1))
:           eval(substitute(touch(x),list(x=args[[3]])),env=sys.frame(-1))
:       eval.parent(substitute(attr(x,"update.expression")<-v,list(x=args[[3]],v=v)))
:     }
:   }
:   x
: }
:



From tlumley at u.washington.edu  Mon Oct  4 17:42:38 2004
From: tlumley at u.washington.edu (Thomas Lumley)
Date: Mon, 4 Oct 2004 08:42:38 -0700 (PDT)
Subject: [R] Working with large datafiles
In-Reply-To: <OF9B63F926.1F942927-ON85256F23.00512C57@hc-sc.gc.ca>
References: <OF9B63F926.1F942927-ON85256F23.00512C57@hc-sc.gc.ca>
Message-ID: <Pine.A41.4.61.0410040837201.92704@homer10.u.washington.edu>

On Mon, 4 Oct 2004, Greg Butler wrote:

> Hi,
>
> I have been enjoying r for some time now, but was wondering about working
> with larger data files.  When I try to load in big files with more than
> 20,000 records, the programs seems unbable to store all the records.  Is
> there some way that I can increase the size of records that I work with?
> Ideally I would like to work with census data which can hold a million
> records.
>

You should be able to handle 20,000 records on a reasonable computer (my 
laptop, with 256Mb memory can, very slowly, do survey analyses on a file 
with 26,000 records and about 100 variables).

A million records is likely to be infeasible. A 32bit computer can't 
even address enough memory to store that much data.  You would need to put 
the data either in a database or in a file format such as netCDF or hdf5 
that allows smaller chunks to be read and processed.

 	-thomas



From ripley at stats.ox.ac.uk  Mon Oct  4 17:46:07 2004
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Mon, 4 Oct 2004 16:46:07 +0100 (BST)
Subject: [R] Working with large datafiles
In-Reply-To: <OF9B63F926.1F942927-ON85256F23.00512C57@hc-sc.gc.ca>
Message-ID: <Pine.LNX.4.44.0410041641370.9561-100000@gannet.stats>

You will need to be more specific (and careful). You talk about

  large datafiles
  increase the size of records

so is the problem

1) The size of the files
2) The size of the records
3) Storage for the R objects you are creating, in some unstated way
4) Something else

?  Without more details, the most helpful advice we can give is

A) Use a 64-bit Linux/Unix OS (e.g. AMD64/Opteron)
B) Add as much RAM as you can afford/fit.

On Mon, 4 Oct 2004, Greg Butler wrote:

> Hi,
> 
> I have been enjoying r for some time now, but was wondering about working
> with larger data files.  When I try to load in big files with more than
> 20,000 records, the programs seems unbable to store all the records.  Is
> there some way that I can increase the size of records that I work with?
> Ideally I would like to work with census data which can hold a million
> records.

> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html

Please do, and tell us the OS you are using and resources you have (e.g. 
RAM).

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From mmmabos at comcast.net  Mon Oct  4 17:46:31 2004
From: mmmabos at comcast.net (Matt Hart)
Date: Mon, 04 Oct 2004 11:46:31 -0400
Subject: [R] Weighted Savitzky-Golay?
In-Reply-To: <3A822319EB35174CA3714066D590DCD504AF84BA@usrymx25.merck.com>
References: <3A822319EB35174CA3714066D590DCD504AF84BA@usrymx25.merck.com>
Message-ID: <41617057.5010205@comcast.net>

Thanks. Yes, I think this is right. As I understand it S-G does a local 
unweighted polynomial regression. I think other smoothers already weigh 
the data by the distance from the estimation point. I'm not sure what 
happens when you mix the two...

m.

Liaw, Andy wrote:

>If I'm not mistaken, S-G is essentially a local (even-degree) polynomial
>smoother with constant bandwidth.  You can use a constant bandwidth local
>polynomial smoother that allows weights; e.g., in the locfit package.
>
>HTH,
>Andy
>
>  
>
>>From: Matt Hart
>>
>>Hi,
>>
>>Does anyone know how to use weights and generate error bounds for 
>>Savitzky-Golay? I have a (smallish) set of points y equally 
>>spaced each 
>>with a known error and would like to smooth them using S-G 
>>but so as to 
>>take into account the error already have and construct new 
>>error bounds 
>>around them that take into account the errors they had at the 
>>beginning 
>>and the erros they get as a result of the smoothing.
>>
>>thanks, m.
>>
>>______________________________________________
>>R-help at stat.math.ethz.ch mailing list
>>https://stat.ethz.ch/mailman/listinfo/r-help
>>PLEASE do read the posting guide! 
>>http://www.R-project.org/posting-guide.html
>>
>>
>>    
>>
>
>______________________________________________
>R-help at stat.math.ethz.ch mailing list
>https://stat.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>
>  
>



From ggrothendieck at myway.com  Mon Oct  4 17:49:19 2004
From: ggrothendieck at myway.com (Gabor Grothendieck)
Date: Mon,  4 Oct 2004 11:49:19 -0400 (EDT)
Subject: [R] inverse function of order()
Message-ID: <20041004154919.DC5E93A03@mprdmxin.myway.com>


Wolfram Fischer <wolfram <at> fischer-zim.ch> writes:
> I have:
>  d <- sample(10:100, 9)
>  o <- order(d)
>  r <- d[o]
> How I can get d (in the original order), knowing only r and o?


When order acts on a permutation of 1:n it is an involution, i.e.
it is its own inverse, therefore:

r[order(o)]



From p.dalgaard at biostat.ku.dk  Mon Oct  4 17:50:31 2004
From: p.dalgaard at biostat.ku.dk (Peter Dalgaard)
Date: 04 Oct 2004 17:50:31 +0200
Subject: [R] Beginners problem
In-Reply-To: <416157AB.7000905@ilt.fraunhofer.de>
References: <416157AB.7000905@ilt.fraunhofer.de>
Message-ID: <x2k6u6pjnc.fsf@biostat.ku.dk>

Rolf Wester <rolf.wester at ilt.fraunhofer.de> writes:

> Hi,
> 
> I'm new to R and have a problem with a little test program (see
> below). Why doesn't <<- in function rk4
> assign the new value to y so that it is seen in rktest. I thought that
> <<- does exactly this. But it seems that I
> didn't get it right. I would be very appreciative for an explanation
> of that behaviour of <<-. I know how to
> write the whole thing so that it works (return the updated y to
> rktest) but I would like to learn how variable scope
> in R works.

R has lexical scope and rk4() is not lexically scoped in rktest(). I
believe that there's a rather thorough discussion of these matters in
the FAQ.

> rk4 <- function(x,y,f,h) {
....
>   y <<- y + h/6.0*(k1 + 2.0*k2 + 2.0*k3 + k4))
> }
> 
> rkf <- function(x,y) {
>   -y
> }
> 
> rktest <- function(){
>   y <- 1.0
>   x <- 0.0
>   h <- 0.1
>   for(i in 1:10) {
>     rk4(x,y,rkf,h)
>     print(y)
>   }
> }
> 
> rktest()


-- 
   O__  ---- Peter Dalgaard             Blegdamsvej 3  
  c/ /'_ --- Dept. of Biostatistics     2200 Cph. N   
 (*) \(*) -- University of Copenhagen   Denmark      Ph: (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)             FAX: (+45) 35327907



From johnc at atlasdmt.com  Mon Oct  4 17:54:40 2004
From: johnc at atlasdmt.com (John Chandler)
Date: Mon, 4 Oct 2004 08:54:40 -0700
Subject: [R] Reading Version 4 .sdd files
Message-ID: <2A03167DD08194408B35A53E75EFE07E6EC599@seaexch01.corp.avenuea.com>

Dear R-Help,

I've never had any trouble importing data into R until I had to import
an .sdd file for a class. The file can be found here:
http://www.math.umt.edu/steele/Math%20549/Farms.sdd. It begins with the
line "## Dump S Version 4 Dump ##". I first attempted read.S which
issued the message "not an S object". I then checked the Import/Export
manual which seemed to indicate that data.restore might do the trick.
Alas, it seems that function is trying to interpret the name of the data
frame as an S mode or some such thing.

Searched the archives (courtesy of Dr. Baron) and found this from Dr.
Ripley: ".sdd files from S-PLUS 6.0 are
S4 dumps, and will not work." This was written in 2002, is it still
accurate? Right now my only obvious course of action is to get on a
machine that has S+, read in this file and dump it in a more useful
format. Are their others?

Thanks so much,
John

P.S. I've been an R user for 4 years now and a subscriber to this list
for almost that long. I'd just like to say an enormous thanks to
everyone who contributes to the R project in some way. R is the most
impressive realization of the free software movement I've ever seen.
Particular thanks to the R core, those who publish the journal, and the
authors of excellent and too-numerous-to-mention books on R and S.


John Chandler-Pepelnjak 
Senior Analyst
atlas DMT(tm)
johnc at atlasdmt.com
206.816.8454 // direct 
206.816.8909 // fax
www.atlasdmt.com



From p.dalgaard at biostat.ku.dk  Mon Oct  4 18:09:37 2004
From: p.dalgaard at biostat.ku.dk (Peter Dalgaard)
Date: 04 Oct 2004 18:09:37 +0200
Subject: [R] inverse function of order()
In-Reply-To: <41616381.1080107@lancaster.ac.uk>
References: <20041004142114.GA4158@s1x.local>
	<41616381.1080107@lancaster.ac.uk>
Message-ID: <x2fz4upiri.fsf@biostat.ku.dk>

Barry Rowlingson <B.Rowlingson at lancaster.ac.uk> writes:

> Wolfram Fischer wrote:
> > I have:
> >  d <- sample(10:100, 9)
> >  o <- order(d)
> >  r <- d[o]
> > How I can get d (in the original order), knowing only r and o?
> >
> 
>   r[order(o)]?
> 
>   > d=sample(10:100,9)
>   > o=order(d)
>   > r=d[o]
>   > all(r[order(o)] == d)
>   [1] TRUE
> 
>   tested and works for duplicates in d as well.

Neat. It is O(n log n) though. 

 D <- numeric(length(r))
 D[o] <- r

should be O(n)

Notice btw that 

> rank(d)
[1] 3 4 1 8 5 6 9 7 2
> order(order(d))
[1] 3 4 1 8 5 6 9 7 2


-- 
   O__  ---- Peter Dalgaard             Blegdamsvej 3  
  c/ /'_ --- Dept. of Biostatistics     2200 Cph. N   
 (*) \(*) -- University of Copenhagen   Denmark      Ph: (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)             FAX: (+45) 35327907



From taochen at spymac.com  Mon Oct  4 18:19:14 2004
From: taochen at spymac.com (Tao Chen)
Date: Mon, 4 Oct 2004 17:19:14 +0100
Subject: [R] problems when compiling R
Message-ID: <200410041619.RAA11574@burnmoor.ncl.ac.uk>

When I was trying to compile R-1.9.1 from source, I've got problems which seemed related to compiling the LAPACK package.

The error message is as follows:

gcc -shared -L/usr/local/lib  -o libRlapack.so dlamc.lo dlapack0.lo dlapack1.lo dlapack2.lo dlapack3.lo cmplx.lo  -lblas -L/usr/local/lib -L/usr/lib/gcc-lib/ia64-redhat-linux/2.96 -L/usr/lib/gcc-lib/ia64-redhat-linux/2.96/../../.. -lg2c -lm
/usr/bin/ld: open.o: @gprel relocation against dynamic symbol f__buflen
/usr/bin/ld: open.o: @gprel relocation against dynamic symbol f__buflen
/usr/bin/ld: open.o: @gprel relocation against dynamic symbol f__buflen
/usr/bin/ld: open.o: @gprel relocation against dynamic symbol f__buflen
/usr/bin/ld: open.o: @gprel relocation against dynamic symbol f__buflen
/usr/bin/ld: open.o: @gprel relocation against dynamic symbol f__buflen
collect2: ld returned 1 exit status


I just run the "configure" and then "make". The system information is as follows:

Hardware:
SGI Altix 3700 Supercluster
28 1.3G Itanium dual processors (ia64)

OS:
Redhat Linux AS2.1 (ia64)


I re-run the compling process on my own PC (AMD XP 2000+ with redhat linux 9) and got no problem at all ...

almost hang my neck on that as I need to move all my work to that supercluster

Thanks,
Tao Chen



From reid_huntsinger at merck.com  Mon Oct  4 18:19:24 2004
From: reid_huntsinger at merck.com (Huntsinger, Reid)
Date: Mon, 4 Oct 2004 12:19:24 -0400
Subject: [R] Working with large datafiles
Message-ID: <D9A95B4B7B20354992E165EEADA31999056A9117@uswpmx00.merck.com>

Out of the box R keeps everything in memory. 1 million wide records could
easily take all your RAM. What do you want to do with all the data at once?
Some suggestions (not original by any means)

1) read the data via the "connection" functions, which would allow you to
for example keep the data gzipped (help(gzfile)) and read chunks at a time,
e.g., in order to
2) sample
3) if you really need more or less random access to records, look into the
database access packages for postgres or oracle etc, or have a look at the
RObjectTables package from Omegahat (I don't have experience with it yet).
4) I wrote some R functions to "stash" objects to disk so they're still
"there" just like any R object but don't use RAM. Each access reads the
whole object, though, and each write writes the whole object, so it's not at
all suited to random access. Let me know if it would help.

Reid Huntsinger

-----Original Message-----
From: r-help-bounces at stat.math.ethz.ch
[mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Greg Butler
Sent: Monday, October 04, 2004 10:49 AM
To: R-help at stat.math.ethz.ch
Subject: [R] Working with large datafiles


Hi,

I have been enjoying r for some time now, but was wondering about working
with larger data files.  When I try to load in big files with more than
20,000 records, the programs seems unbable to store all the records.  Is
there some way that I can increase the size of records that I work with?
Ideally I would like to work with census data which can hold a million
records.

Greg

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide!
http://www.R-project.org/posting-guide.html



From andy_liaw at merck.com  Mon Oct  4 18:48:33 2004
From: andy_liaw at merck.com (Liaw, Andy)
Date: Mon, 4 Oct 2004 12:48:33 -0400
Subject: [R] Weighted Savitzky-Golay?
Message-ID: <3A822319EB35174CA3714066D590DCD504AF84BF@usrymx25.merck.com>

> From: Matt Hart 
> 
> Thanks. Yes, I think this is right. As I understand it S-G 
> does a local 
> unweighted polynomial regression. I think other smoothers 
> already weigh 
> the data by the distance from the estimation point. I'm not sure what 
> happens when you mix the two...

Makes perfect sense to statisticians.  The two sets of weights are for
different reasons.  The distance weights (kernels) are for smoothness and
bias/variance properties of the estimates.

Andy
 
> m.
> 
> Liaw, Andy wrote:
> 
> >If I'm not mistaken, S-G is essentially a local 
> (even-degree) polynomial
> >smoother with constant bandwidth.  You can use a constant 
> bandwidth local
> >polynomial smoother that allows weights; e.g., in the locfit package.
> >
> >HTH,
> >Andy
> >
> >  
> >
> >>From: Matt Hart
> >>
> >>Hi,
> >>
> >>Does anyone know how to use weights and generate error bounds for 
> >>Savitzky-Golay? I have a (smallish) set of points y equally 
> >>spaced each 
> >>with a known error and would like to smooth them using S-G 
> >>but so as to 
> >>take into account the error already have and construct new 
> >>error bounds 
> >>around them that take into account the errors they had at the 
> >>beginning 
> >>and the erros they get as a result of the smoothing.
> >>
> >>thanks, m.
> >>
> >>______________________________________________
> >>R-help at stat.math.ethz.ch mailing list
> >>https://stat.ethz.ch/mailman/listinfo/r-help
> >>PLEASE do read the posting guide! 
> >>http://www.R-project.org/posting-guide.html
> >>
> >>
> >>    
> >>
> >
> >______________________________________________
> >R-help at stat.math.ethz.ch mailing list
> >https://stat.ethz.ch/mailman/listinfo/r-help
> >PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
> >
> >  
> >
> 
> 
>



From p.dalgaard at biostat.ku.dk  Mon Oct  4 17:29:39 2004
From: p.dalgaard at biostat.ku.dk (Peter Dalgaard)
Date: 04 Oct 2004 17:29:39 +0200
Subject: [R] R 2.0.0 is released
Message-ID: <x2oejipkm4.fsf@biostat.ku.dk>


I've rolled up R-2.0.0.tar.gz a short while ago. This is a new version
with a number of new features. See below for the details.

As was the case with R 1.0.0, this new version represents a coming of
age more than a radical change to R. We do plan to celebrate the new
major version with press releases and such.

The release will be available from 

http://cran.r-project.org/src/base/R-2/R-2.0.0.tar.gz

or you might wait for it to be mirrored at a CRAN site nearer to you.
Binaries for various platforms will appear in due course.
 
There is also a version split for floppies.


        For the R Core Team
        Peter Dalgaard




These are the md5sums for the freshly created files, in case you wish
to check that they are uncorrupted:

3900bca37cabb4b76b8d736d51cc9251  R-2.0.0.tar.gz
4a3c7595b112d879997f7455fa8c1c0d  R-2.0.0.tar.gz-split.aa
45d376c7c533c62c657ef0fafac8a784  R-2.0.0.tar.gz-split.ab
7dbdb241e1fb7263701719ea856ebe41  R-2.0.0.tar.gz-split.ac
1e1077f7593778b79e9b0d5c676af63b  R-2.0.0.tar.gz-split.ad
6f4ffaa48a54e002f586f80fd4f2461c  R-2.0.0.tar.gz-split.ae
34b62bb31f6ecf84da329d21c2a21561  R-2.0.0.tar.gz-split.af
c880f8f06ca3fe367bba6757e9cfbf32  R-2.0.0.tar.gz-split.ag
b1a2d17d3ae523d04ffc8d3c6db4b67b  R-2.0.0.tar.gz-split.ah


Here is the relevant part of the NEWS file


		CHANGES IN R VERSION 2.0.0


USER-VISIBLE CHANGES

    o	The stub packages from 1.9.x have been removed: the library()
	function selects the new home for their code.

    o	`Lazy loading' of R code has been implemented, and is used for
	the standard and recommended packages by default.  Rather than
	keep R objects in memory, they are kept in a database on disc
	and only loaded on first use.  This accelerates startup (down
	to 40% of the time for 1.9.x) and reduces memory usage -- the
	latter is probably unimportant of itself, but reduces
	commensurately the time spent in garbage collection.

	Packages are by default installed using lazy loading if they
	have more than 25Kb of R code and did not use a saved image.
	This can be overridden by INSTALL --[no-]lazy or via a field
	in the DESCRIPTION file.  Note that as with --save, any other
	packages which are required must be already installed.

	As the lazy-loading databases will be consulted often, R
	will be slower if run from a slow network-mounted disc.

    o	All the datasets formerly in packages 'base' and 'stats' have
	been moved to a new package 'datasets'.	 data() does the
	appropriate substitution, with a warning.  However, calls to
	data() are not normally needed as the data objects are visible
	in the 'datasets' package.

	Packages can be installed to make their data objects visible
	via R CMD INSTALL --lazy-data or via a field in the
	DESCRIPTION file.

    o	Package 'graphics' has been split into 'grDevices' (the graphics
	devices shared between base and grid graphics) and 'graphics'
	(base graphics).  Each of the 'graphics' and 'grid' packages
	load 'grDevices' when they are attached.  Note that
	ps.options() has been moved to grDevices and user hooks may
	need to be updated.

    o	The semantics of data() have changed (and were incorrectly
	documented in recent releases) and the function has been moved
	to package 'utils'.  Please read the help page carefully if
	you use the 'package' or 'lib.loc' arguments.

	data() now lists datasets, and not just names which data() accepts.

    o	Dataset 'phones' has been renamed to 'WorldPhones'.

    o	Datasets 'sunspot.month' and 'sunspot.year' are available
	separately but not via data(sunspot) (which was used by package
	lattice to retrieve a dataset 'sunspot').

    o	Packages must have been re-installed for this version, and
	library() will enforce this.

    o	Package names must now be given exactly in library() and
	require(), regardless of whether the underlying file system is
	case-sensitive or not.	So 'library(mass)' will not work, even
	on Windows.

    o	R no longer accepts associative use of relational operators.
	That is, 3 < 2 < 1 (which used to evalute as TRUE!) now causes
	a syntax error.	 If this breaks existing code, just add
	parentheses -- or braces in the case of plotmath.

    o	The R parser now allows multiline strings, without escaping
	the newlines with backslashes (the old method still works).
	Patch by Mark Bravington.


NEW FEATURES

    o	There is a new atomic vector type, class "raw".	 See ?raw for
	full details including the operators and utility functions provided.

    o	The default barplot() method by default uses a
	gamma-corrected grey palette (rather than the heat color
	palette) for coloring its output when given a matrix.

    o	The 'formula' method for boxplot() has a 'na.action' argument,
	defaulting to NULL.  This is mainly useful if the response
	is a matrix when the previous default of 'na.omit' would omit
	entire rows.  (Related to PR#6846.)

	boxplot() and bxp() now obey global 'par' settings and also
	allow the specification of graphical options in more detail,
	compatibly with S-PLUS (fulfilling wishlist entry PR#6832)
	thanks to contributions from Arni Magnusson.  For consistency,
	'boxwex' is not an explicit argument anymore.

    o	chull() has been moved to package graphics (as it uses xy.coords).

    o	There is now a coef() method for summaries of "nls" objects.

    o	compareVersion(), packageDescription() and read.00Index()
	have been moved to package 'utils'.

    o	convolve(), fft(), mvfft() and nextn() have been moved to
	package stats.

    o	coplot() now makes use of cex.lab and font.lab par() settings.

    o	cumsum/prod/max/min() now preserve names.

    o	data(), .path.packages() and .find.packages() now interpret
	package = NULL to mean all loaded packages.

    o	data.frame() and its replacement methods remove the names from
	vector columns.	 Using I() will ensure that names are
	preserved.

    o	data.frame(check.names = TRUE) (the default) enforces unique
	names, as S does.

    o	.Defunct() now has 'new' and 'package' arguments like those of
	.Deprecated().

    o	The plot() method for "dendrogram" objects now respects many more
	nodePar and edgePar settings and for edge labeling computes the
	extents of the diamond more correctly.

    o	deparse(), dput() and dump() have a new 'control' argument to
	control the level of detail when deparsing.  dump() defaults to
	the most detail, the others default to less.  See ?.deparseOpts
	for the details.

	They now evaluate promises by default: see ?dump for details.

    o	dir.create() now expands '~' in filenames.

    o	download.file() has a new progress meter (under Unix) if the
	length of the file is known -- it uses 50 equals signs.

    o	dyn.load() and library.dynam() return an object describing the
	DLL that was loaded.  For packages with namespaces, the DLL
	objects are stored in a list within the namespace.

    o	New function eapply() - apply for environments.	 The supplied
	function is applied to each element of the environment; the order
	of application is not specified.

    o	edit() and fix() use the object name in the window caption on
	some platforms (e.g. Windows).

    o	Function file.edit() function added: like file.show(), but
	allows editing.

    o	Function file.info() can return file sizes > 2G if the
	underlying OS supports such.

    o	fisher.test(*, conf.int=FALSE) allows the confidence interval
	computation to be skipped.

    o	formula() methods for classes "lm" and "glm" used the expanded
	formula (with '.' expanded) from the terms component.

    o	The `formula' method for ftable() now looks for variables in the
	environment of the formula before the usual search path.

    o	A new function getDLLRegisteredRoutines() returns information
	about the routines available from a DLL that were explicitly
	registered with R's dynamic loading facilities.

    o	A new function getLoadedDLLs() returns information about the
	DLLs that are currently loaded within this session.

    o	The package element returned by getNativeSymbolInfo() contains
	reference to both the internal object used to resolve symbols
	with the DLL, and the internal DllInfo structure used to
	represent the DLL within R.

    o	help() now returns information about available documentation for
	a given topic, and notifies about multiple matches.  It has a
	separate print() method.

	If the latex help files were not installed, help() will offer
	to create a latex file on-the-fly from the installed .Rd file.

    o	heatmap() has a new argument 'reorderfun'.

    o	Most versions of install.packages() have an new optional
	argument 'dependencies = TRUE' which will not only fetch the
	packages but also their uninstalled dependencies and their
	dependencies ....

	The Unix version of install.packages() attempts to install
	packages in an order that reflects their dependencies.	(This
	is not needed for binary installs as used under Windows.)

    o	interaction() has new argument 'sep'.

    o	interaction.plot() allows 'type = "b"' and doesn't give spurious
	warnings when passed a matplot()-only argument such as 'main'.

    o	is.integer() and is.numeric() always return FALSE for a
	factor.	 (Previously they were true and false respectively for
	well-formed factors, but it is possible to create factors
	with non-integer codes by underhand means.)

    o	New functions is.leaf(), dendrapply() and a labels() method for
	dendrogram objects.

    o	legend() has an argument 'pt.lwd' and setting 'density' now works
	because 'angle' now defaults to 45 (mostly contributed by Uwe Ligges).

    o	library() now checks the version dependence (if any) of
	required packages mentioned in the Depends: field of the
	DESCRIPTION file.

    o	load() now detects and gives a warning (rather than an error)
	for empty input, and tries to detect (but not correct) files
	which have had LF replaced by CR.

    o	ls.str() and lsf.str() now return an object of class "ls_str" which
	has a print method.

    o	make.names() has a new argument allow_, which if false allows
	its behaviour in R 1.8.1 to be reproduced.

    o	The 'formula' method for mosaicplot() has a 'na.action' argument
	defaulting to 'na.omit'.

    o	model.frame() now warns if it is given data = newdata and it
	creates a model frame with a different number of rows from
	that implied by the size of 'newdata'.

	Time series attributes are never copied to variables in the
	model frame unless na.action = NULL.  (This was always the
	intention, but they sometimes were as the result of an earlier
	bug fix.)

    o	There is a new 'padj' argument to mtext() and axis().
	Code patch provided by Uwe Ligges (fixes PR#1659 and PR#7188).

    o	Function package.dependencies() has been moved to package 'tools'.

    o	The 'formula' method for pairs() has a 'na.action' argument,
	defaulting to 'na.pass', rather than the value of
	getOption("na.action").

    o	There are five new par() settings:

	'family' can be used to specify a font family for graphics
	text.  This is a device-independent family specification
	which gets mapped by the graphics device to a device-specific
	font specification (see, for example, postscriptFonts()).
	Currently, only PostScript, PDF, X11, Quartz, and Windows
	respond to this setting.

	'lend', 'ljoin', and 'lmitre' control the cap style and
	join style for drawing lines (only noticeable on thick lines
	or borders).  Currently, only PostScript, PDF, X11, and Quartz
	respond to these settings.

	'lheight' is a multiplier used in determining the vertical
	spacing of multi-line text.

	All of these settings are currently only available via par()
	(i.e., not in-line as arguments to plot(), lines(), ...)

    o	PCRE (as used by grep etc) has been updated to version 5.0.

    o	A 'version' argument has been added to pdf() device.  If this is
	set to "1.4", the device will support transparent colours.

    o	plot.xy(), the workhorse function of points(), lines() and
	plot.default() now has 'lwd' as explicit argument instead of
	implicitly in '...', and now recycles lwd where it makes
	sense, i.e. for line-based plot symbols.

    o	The png() and jpeg() devices (and the bmp() device under Windows)
	now allow a nominal resolution to be recorded in the file.

    o   New functions to control mapping from device-independent
	graphics font family to device-specific family:
	postscriptFont() and postscriptFonts() (for both postscript()
	and pdf()); X11Font() and X11Fonts(); windowsFont() and
	windowsFonts(); quartzFont() and quartzFonts().

    o	power (x^y) has optimised code for y == 2.

    o	prcomp() is now generic, with a formula method (based on an
	idea of Jari Oksanen).

	prcomp() now has a simple predict() method.

    o	printCoefmat() has a new logical argument 'signif.legend'.

    o	quantile() has the option of several methods described in
	Hyndman & Fan (1996). (Contributed by Rob Hyndman.)

    o	rank() has two new 'ties.method's, "min" and "max".

    o	New function read.fortran() reads Fortran-style fixed-format
	specifications.

    o	read.fwf() reads multiline records, is faster for large files.

    o	read.table() now accepts "NULL", "factor", "Date" and
	"POSIXct" as possible values of colClasses, and colClasses can
	be a named character vector.

    o	readChar() can now read strings with embedded nuls.

    o	The "dendrogram" method for reorder() now has a 'agglo.FUN'
	argument for specification of a weights agglomeration
	function.

    o	New reorder() method for factors, slightly extending that in
	lattice.  Contributed by Deepayan Sarkar.

    o	Replaying a plot (with replayPlot() or via autoprinting) now
	automagically opens a device if none is open.

    o	replayPlot() issues a warning if an attempt is made to replay
	a plot that was recorded using a different R version (the
	format for recorded plots is not guaranteed to be stable
	across different R versions).  The Windows-menu equivalent
	(History...Get from variable) issues a similar warning.

    o	reshape() can handle multiple 'id' variables.

    o	It is now possible to specify colours with a full alpha
	transparency channel via the new 'alpha' argument to the
	rgb() and hsv() functions, or as a string of the form "#RRGGBBAA".

	NOTE: most devices draw nothing if a colour is not opaque,
	but PDF and Quartz devices will render semitransparent colours.

	A new argument 'alpha' to the function col2rgb()
	provides the ability to return the alpha component of
	colours (as well as the red, green, and blue components).

    o	save() now checks that a binary connection is used.

    o	seek() on connections now accepts and returns a double for the
	file position.	This allows >2Gb files to be handled on a
	64-bit platform (and some 32-bit platforms).

    o	source() with 'echo = TRUE' uses the function source attribute
	when displaying commands as they are parsed.

    o	setClass() and its utilities now warn if either superclasses
	or classes for slots are undefined.  (Use setOldClass to
	register S3 classes for use as slots)

    o	str(obj) now displays more reasonably the STRucture of S4 objects.

	It is also improved for language objects and lists with promise
	components.

	The method for class "dendrogram" has a new argument 'stem' and
	indicates when it's not printing all levels (as typically when
	e.g., 'max.level = 2').

	Specifying 'max.level = 0' now allows to suppress all but the top
	level for hierarchical objects such as lists. This is different
	to previous behavior which was the default behavior of giving all
	levels is unchanged.  The default behavior is unchanged but now
	specified by 'max.level = NA'.

    o	system.time() has a new argument 'gcFirst' which, when TRUE,
	forces a garbage collection before timing begins.

    o	tail() of a matrix now displays the original row numbers.

    o	The default method for text() now coerces a factor to character
	and not to its internal codes.	This is incompatible with S
	but seems what users would expect.

	It now also recycles (x,y) to the length of 'labels' if that
	is longer.  This is now compatible with grid.text() and
	S. (See also PR#7084.)

    o	TukeyHSD() now labels comparisons when applied to an
	interaction in an aov() fit.  It detects non-factor terms in
	'which' and drops them if sensible to do so.

    o	There is now a replacement method for window(), to allow a
	range of values of time series to be replaced by specifying the
	start and end times (and optionally a frequency).

    o	If writeLines() is given a connection that is not open, it now
	attempts to open it in mode = "wt" rather than the default
	mode specified when creating the connection.

    o	The screen devices x11(), windows() and quartz() have a new
	argument 'bg' to set the default background colour.


    o	Subassignments involving NAs and with a replacement value of
	length > 1 are now disallowed.	(They were handled
	inconsistently in R < 2.0.0, see PR#7210.)  For data frames
	they are disallowed altogether, even for logical matrix indices
	(the only case which used to work).

    o	The way the comparison operators handle a list argument has
	been rationalized so a few more cases will now work -- see
	?Comparison.

    o	Indexing a vector by a character vector was slow if both the
	vector and index were long (say 10,000).  Now hashing is used
	and the time should be linear in the longer of the lengths
	(but more memory is used).

    o	Printing a character string with embedded nuls now prints the
	whole string, and non-printable characters are represented by
	octal escape sequences.

    o	Objects created from a formally defined class now include the
	name of the corresponding package as an attribute in the
	object's class.	 This allows packages with namespaces to have
	private (non-exported) classes.

    o	Changes to package 'grid':

	- Calculation of number of circles to draw in circleGrob now
	  looks at length of y and r as well as length of x.

	- Calculation of number of rectangles to draw in rectGrob now
	  looks at length of y, w, and h as well as length of x.

	- All primitives (rectangles, lines, text, ...) now handle
	  non-finite values (NA, Inf, -Inf, NaN) for locations and
	  sizes.

	  Non-finite values for locations, sizes, and scales of
	  viewports result in error messages.

	  There is a new vignette ("nonfinite") which describes this
	  new behaviour.

	- Fixed (unreported) bug in drawing circles.  Now checks that
	  radius is non-negative.

	- downViewport() now reports the depth it went down to find a
	  viewport.  Handy for "going back" to where you started, e.g., ...

	    depth <- downViewport("vpname")
	    <draw stuff>
	    upViewport(depth)

	- The "alpha" gpar() is now combined with the alpha channel of
	  colours when creating a gcontext as follows: (internal C code)

	    finalAlpha = gpar("alpha")*(R_ALPHA(col)/255)

	  This means that gpar(alpha=) settings now affect internal
	  colours so grid alpha transparency settings now are sent to
	  graphics devices.

	  The alpha setting is also cumulative.	 For example, ...

	    grid.rect(width=0.5, height=0.5,
		      gp=gpar(fill="blue"))		 # alpha = 1
	    pushViewport(viewport(gp=gpar(alpha=0.5)))
	    grid.rect(height=0.25, gp=gpar(fill="red"))	 # alpha = 0.5
	    pushViewport(viewport(gp=gpar(alpha=0.5)))
	    grid.rect(width=0.25, gp=gpar(fill="red"))	 # alpha = 0.25 !

	- Editing a gp slot in a grob is now incremental.  For example ...

	    grid.lines(name="line")
	    grid.edit("line", gp=gpar(col="red")) # line turns red
	    grid.edit("line", gp=gpar(lwd=3)) # line becomes thick
					      # AND STAYS red

	- The "cex" gpar is now cumulative.  For example ...

	    grid.rect(height=unit(4, "char")) # cex = 1
	    pushViewport(viewport(gp=gpar(cex=0.5)))
	    grid.rect(height=unit(4, "char")) # cex = 0.5
	    pushViewport(viewport(gp=gpar(cex=0.5)))
	    grid.rect(height=unit(4, "char")) # cex = 0.125 !!!

	- New childNames() function to list the names of children
	  of a gTree.

	- The "grep" and "global" arguments have been implemented for
	  grid.[add|edit|get|remove]Grob() functions.

	  The "grep" argument has also been implemented for the
	  grid.set() and setGrob().

	- New function grid.grab() which creates a gTree from the
	  current display list (i.e., the current page of output can
	  be converted into a single gTree object with all grobs
	  on the current page as children of the gTree and all the
	  viewports used in drawing the current page in the childrenvp
	  slot of the gTree).

	- New "lineend", "linejoin", and "linemitre" gpar()s:

	  line end can be "round", "butt", or "square".
	  line join can be "round", "mitre", or "bevel".
	  line mitre can be any number larger than 1
	    (controls when a mitre join gets turned into a bevel join;
	     proportional to angle between lines at join;
	     very big number means that conversion only happens for lines
	     that are almost parallel at join).

	- New grid.prompt() function for controlling whether the user is
	  prompted before starting a new page of output.

	  Grid no longer responds to the par(ask) setting in the "graphics"
	  package.

    o	The tcltk package has had the tkcmd() function renamed as
	tcl() since it could be used to invoke commands that had
	nothing to do with Tk. The old name is retained, but will be
	deprecated in a future release. Similarly, we now have
	tclopen(), tclclose(), tclread(), tclputs(), tclfile.tail(),
	and tclfile.dir() replacing counterparts starting with "tk",
	with old names retained for now.


UTILITIES

    o	R CMD check now checks for file names in a directory that
	differ only by case.

    o	R CMD check now checks Rd files using R code from package tools,
	and gives refined diagnostics about "likely" Rd problems (stray
	top-level text which is silently discarded by Rdconv).

    o	R CMD INSTALL now fails for packages with incomplete/invalid
	DESCRIPTION metadata, using new code from package tools which is
	also used by R CMD check.

    o	list_files_with_exts (package tools) now handles zipped directories.

    o	Package 'tools' now provides Rd_parse(), a simple top-level
	parser/analyzer for R documentation format.

    o	tools::codoc() (and hence R CMD check) now checks any documentation
	for registered S3 methods and unexported objects in packages
	with namespaces.

    o	Package 'utils' contains several new functions:

	- Generics toBibtex() and toLatex() for converting
	  R objects to BibTeX and LaTeX (but almost no methods yet).

	- A much improved citation() function which also has a package
	  argument.  By default the citation is auto-generated from
	  the package DESCRIPTION, the file 'inst/CITATION' can be
	  used to override this, see help(citation) and
	  help(citEntry).

	- sessionInfo() can be used to include version information about
	  R and R packages in text or LaTeX documents.


DOCUMENTATION

    o	The DVI and PDF manuals are now all made on the paper specified
	by R_PAPERSIZE (default 'a4'), even the .texi manuals which
	were made on US letter paper in previous versions.

    o	The reference manual now omits 'internal' help pages.

    o	There is a new help page shown by help("Memory-limits") which
	documents the current design limitations on large objects.

    o	The format of the LaTeX version of the documentation has
	changed.  The old format is still accepted, but only the new
	resolves cross-references to object names containing _, for
	example.

    o	HTML help pages now contain a reference to the package and
	version in the footer, and HTML package index pages give their
	name and version at the top.

    o	All manuals in the 2.x series have new ISBN numbers.

    o	The 'R Data Import/Export' manual has been revised and has a
	new chapter on `Reading Excel spreadsheets'.


C-LEVEL FACILITIES

    o	The PACKAGE argument for .C/.Call/.Fortran/.External can be
	omitted if the call is within code within a package with a
	namespace.  This ensures that the native routine being called
	is found in the DLL of the correct version of the package if
	multiple versions of a package are loaded in the R session.
	Using a namespace and omitting the PACKAGE argument is
	currently the only way to ensure that the correct version is
	used.

    o	The header Rmath.h contains a definition for R_VERSION_STRING
	which can be used to track different versions of R and libRmath.

    o	The Makefile in src/nmath/standalone now has 'install' and
	'uninstall' targets -- see the README file in that directory.

    o	More of the header files, including Rinternals.h, Rdefines.h and
	Rversion.h, are now suitable for calling directly from C++.

    o   Configure looks to a suitable option for inlining C code which
	made available as macro R_INLINE: see `Writing R Extensions'
	for further details.


DEPRECATED & DEFUNCT

    o	Direct use of R INSTALL|REMOVE|BATCH|COMPILE|SHLIB has been
	removed: use R CMD instead.

    o	La.eigen(), tetragamma(), pentagamma(), package.contents() and
	package.description() are defunct.

    o	The undocumented function newestVersion() is no longer exported
	from package utils.  (Mainly because it was not completely general.)

    o	C-level entry point ptr_R_GetX11Image has been removed, as it
	was replaced by R_GetX11Image at 1.7.0.

    o	The undocumented C-level entry point R_IsNaNorNA has been
	removed.  It was used in a couple of packages, and should be
	replaced by a call to the documented macro ISNAN.

    o	The gnome/GNOME graphics device is now defunct.


INSTALLATION CHANGES

    o	Arithmetic supporting +/-Inf, NaNs and the IEC 60559 (aka
	IEEE 754) standard is now required -- the partial and often
	untested support for more limited arithmetic has been removed.

	The C99 macro isfinite is used in preference to finite if available
	(and its correct functioning is checked at configure time).

	Where isfinite or finite is available and works, it is used as
	the substitution value for R_FINITE.  On some platforms this
	leads to a performance gain.  (This applies to compiled code
	in packages only for isfinite.)

    o	The dynamic libraries libR and libRlapack are now installed in
	R_HOME/lib rather than R_HOME/bin.

    o	When --enable-R-shlib is specified, the R executable is now a
	small executable linked against libR: see the R-admin manual
	for further discussion.	 The 'extra' libraries bzip2, pcre,
	xdr and zlib are now compiled in a way that allows the code to
	be included in a shared library only if this option is
	specified, which might improve performance when it is not.

    o	The main R executable is now R_HOME/exec/R not R_HOME/R.bin, to
	ease issues on MacOS X.	 (The location is needed when debugging
	core dumps, on other platforms.)

    o	Configure now tests for 'inline' and alternatives, and the
	src/extra/bzip2 code now (potentially) uses inlining where
	available and not just under gcc.

    o	The XPG4 sed is used on Solaris for forming dependencies,
	which should now be done correctly.

    o	Makeinfo 4.5 or later is now required for building the HTML and
	Info versions of the manuals.  However, binary distributions
	need to be made with 4.7 or later to ensure some of the
	links are correct.

    o	f2c is not allowed on 64-bit platforms, as it uses longs for
	Fortran integers.

    o	There are new options on how to make the PDF version of the
	reference manual -- see the 'R Administration and Installation
	Manual' section 2.2.

    o	The concatenated Rd files in the installed 'man' directory are
	now compressed and the R CMD check routines can read the
	compressed files.

    o   There is a new configure option --enable-linux-lfs that will
	build R with support for > 2Gb files on suitably recent 32-bit
	Linux systems.


PACKAGE INSTALLATION CHANGES

    o	The DESCRIPTION file of packages may contain a 'Imports:'
	field for packages whose namespaces are used but do not need
	to be attached.	 Such packages should no longer be listed in
	'Depends:'.

    o	There are new optional fields 'SaveImage', 'LazyLoad' and
	'LazyData' in the DESCRIPTION file.  Using 'SaveImage' is
	preferred to using an empty file 'install.R'.

    o	A package can contain a file 'R/sysdata.rda' to contain
	system datasets to be lazy-loaded into the namespace/package
	environment.

    o	The packages listed in 'Depends' are now loaded before a package
	is loaded (or its image is saved or it is prepared for lazy
	loading).  This means that almost all uses of R_PROFILE.R and
	install.R are now unnecessary.

    o	If installation of any package in a bundle fails, R CMD
	INSTALL will back out the installation of all of the bundle,
	not just the failed package (on both Unix and Windows).


BUG FIXES

    o	Complex superassignments were wrong when a variable with the same
	name existed locally, and were not documented in R-lang.

    o	rbind.data.frame() dropped names/rownames from columns in all
	but the first data frame.

    o	The dimnames<- method for data.frames was not checking the
	validity of the row names.

    o	Various memory leaks reported by valgrind have been plugged.

    o	gzcon() connections would sometimes read the crc bytes from
	the wrong place, possibly uninitialized memory.

    o	Rd.sty contained a length \middle that was not needed after a
	revision in July 2000.	It caused problems with LaTeX systems
	based on e-TeX which are starting to appear.

    o	save() to a connection did not check that the connection was
	open for writing, nor that non-ascii saves cannot be made to a
	text-mode connection.

    o	phyper() uses a new algorithm based on Morten Welinder's bug
	report (PR#6772).  This leads to faster code for large arguments
	and more precise code, e.g. for phyper(59, 150,150, 60,	lower=FALSE).
	This also fixes bug (PR#7064) about fisher.test().

    o	print.default(*, gap = <n>) now in principle accepts all
	non-negative values <n>.

    o	smooth.spline(...)$pen.crit had a typo in its computation;
	note this was printed in print.smooth.spline(*) but not used in
	other "smooth.spline" methods.

    o	write.table() handles zero-row and zero-column inputs correctly.

    o	debug() works on trivial functions instead of crashing. (PR#6804)

    o	eval() could alter a data.frame/list second argument, so
	with(trees, Girth[1] <- NA) altered 'trees' (and any copy of
	'trees' too).

    o	cor() could corrupt memory when the standard deviation was
	zero. (PR#7037)

    o	inverse.gaussian() always printed 1/mu^2 as the link function.

    o	constrOptim() now passes ... arguments through optim to the
	objective function.

    o	object.size() now has a better estimate for character vectors:
	it was in general too low (but only significantly so for
	very short character strings) but over-estimated NA and
	duplicated elements.

    o	quantile() now interpolates correctly between finite and
	infinite values (giving +/-Inf rather than NaN).

    o	library() now gives more informative error messages mentioning
	the package being loaded.

    o	Building the reference manual no longer uses roman upright
	quotes in typewriter output.

    o	model.frame() no longer builds invalid data frames if the
	data contains time series and rows are omitted by na.action.

    o	write.table() did not escape quotes in column names.  (PR#7171)

    o	Range checks missing in recursive assignments using [[ ]].  (PR#7196)

    o	packageStatus() reported partially-installed bundles as
	installed.

    o	apply() failed on an array of dimension >=3 when for each
	iteration the function returns a named vector of length >=2.
	(PR#7205)

    o	The GNOME interface was in some circumstances failing if run
	from a menu -- it needed to always specify that R be interactive.

    o	depMtrxToStrings (part of pkgDepends) applied nrow() to a
	non-matrix and aborted on the result.

    o	Fix some issues with nonsyntactical names in modelling code
	(PR#7202), relating to backquoting.  There are likely more.

    o	Support for S4 classes that extend basic classes has been fixed
	in several ways.  as() methods and x at .Data should work better.

    o	hist() and pretty() accept (and ignore) infinite values.  (PR#7220)

    o	It is no longer possible to call gzcon() more than once on a
	connection.

    o	t.test() now detects nearly-constant input data.  (PR#7225)

    o	mle() had problems if ndeps or parscale was supplied in the
	control arguments for optim().  Also, the profiler is now more
	careful to reevaluate modified mle() calls in its parent
	environment.

    o	Fix to rendering of accented superscripts and subscripts e.g.,
	expression((b[dot(a)])).  (Patch from Uwe Ligges.)

    o	attach(*, pos=1) now gives a warning (and will give an error).

    o	power.*test() now gives an error when 'sig.level' is outside [0,1].
	(PR#7245)

    o	Fitting a binomial glm with a matrix response lost the names of
	the response, which should have been transferred to the
	residuals and fitted values.

    o   print.ts() could get the year wrong because rounding issue
        (PR#7255)


-- 
   O__  ---- Peter Dalgaard             Blegdamsvej 3  
  c/ /'_ --- Dept. of Biostatistics     2200 Cph. N   
 (*) \(*) -- University of Copenhagen   Denmark      Ph: (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)             FAX: (+45) 35327907

_______________________________________________
R-announce at stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-announce



From dgrove at fhcrc.org  Mon Oct  4 20:02:29 2004
From: dgrove at fhcrc.org (Douglas Grove)
Date: Mon, 4 Oct 2004 11:02:29 -0700 (PDT)
Subject: [R] inverse function of order()
In-Reply-To: <1096904379.3357.7.camel@ndmpc126.ihs.ox.ac.uk>
Message-ID: <Pine.LNX.4.44.0410041059330.16009-100000@echidna.fhcrc.org>

An alternate method that saves having to use order() again is

r[o] <- r

Doug




On Mon, 2004-10-04 at 15:21, Wolfram Fischer wrote:
> I have:
> 
>  d <- sample(10:100, 9)
>  o <- order(d)
>  r <- d[o]
> 
> How I can get d (in the original order), knowing only r and o?
> 
> Thanks - Wolfram
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>



From Mike.Prager at noaa.gov  Mon Oct  4 20:04:55 2004
From: Mike.Prager at noaa.gov (Mike Prager)
Date: Mon, 04 Oct 2004 14:04:55 -0400
Subject: [R] A newbye question : Creating a grey palette
In-Reply-To: <001d01c4aa24$29f6c800$6501a8c0@NajiNassar>
References: <Pine.LNX.4.44.0410041437470.10475-100000@gannet.stats>
	<001d01c4aa24$29f6c800$6501a8c0@NajiNassar>
Message-ID: <6.1.2.0.0.20041004140220.018ebda0@hermes.nos.noaa.gov>

At 11:09 AM 10/04/2004, you wrote:
>Hi all
>
>I'd like to create a grey palette for a filled.contour()
>from blank (smallest values) to black (large values)

I have used somethig like this with bar plots:

colvec <- gray(sqrt(seq(from=0.05, to=1.0, length=ncolor)))

That will save a color palette in colvec for use in your call.

MHP


-- 
Michael Prager, Ph.D.                <Mike.Prager at noaa.gov>
NOAA Beaufort Laboratory
Beaufort, North Carolina  28516
http://shrimp.ccfhrb.noaa.gov/~mprager/
***



From lisas at salford-systems.com  Mon Oct  4 20:25:18 2004
From: lisas at salford-systems.com (Lisa Solomon)
Date: Mon, 04 Oct 2004 11:25:18 -0700
Subject: [R] Announcement and CFP: Salford Systems Data Mining Conferences
 2005, New York and Barcelona
Message-ID: <4161958E.9060007@salford-systems.com>

Apologies for cross posting....

-----------------------------------------------------------------------------
                    Salford Systems Data Mining 2005
              Second International Data Mining Conferences
Focusing on the Contributions of Data Mining to Solving Real World 
Challenges

     Honoring the Real-World Experiences of Data Mining Visionaries
                    Leo Breiman and Jerome Friedman

                and including MORE THAN 24 PRESENTATIONS
             WITH AN INDUSTRY FOCUS AND PRACTICAL EMPHASIS

                       First Call For Submissions
------------------------------------------------------------------------------

                 US Venue:  New York, March 29-30, 2005
                 EU Venue:  Barcelona, April 4-6, 2005

         Conference home page: http://www.salforddatamining.com
Sample Conference Schedule(last year): 
http://www.salforddatamining.com/program.htm

  If you have an interest in attending or presenting at this conference
       please let us know via email at info at salforddatamining.com

       ----------------------------------------------------------
       ---                                                    ---
       ---             FIRST CALL FOR SUBMISSIONS             ---
       ---           Abstract submission deadlines:           ---
       ---            New York: November 15, 2004             ---
       ---            Barcelona: December 15, 2004            ---
       ---                                                    ---
       ----------------------------------------------------------

The conferences are intended to serve several functions:

o A venue to exchange ideas and experiences focused on the
 practice of data mining and the art and practice of the real
 world analysis of complex data.

o A place to learn about advanced data mining technology and
 anticipated future developments.

o An opportunity to obtain both basic and advanced training
 offered by practical and theoretical experts.

Contributed papers covering any application of CART(R), MARS(R),
PRIM(TM), MART(TM), TreeNet(TM) and RandomForests(TM) are encouraged.
Papers may focus on ordinary everyday challenges of applied data
analysis or on innovative and unusual applications.  We especially
welcome presentations rooted in a substantive industrial data
mining context that report on real world data.

Abstract Submission: papers at salforddatamining.com

Topics of Interest to Attendees:
http://www.salforddatamining.com/program.htm
(Last year's program schedule)

-----------------------------------------------------------------------
IMPORTANT DATES
-----------------------------------------------------------------------

November 15, 2004: New York Submission deadline for contributed abstracts
November 27, 2004: Notification of interest in topic

December 15, 2004: Barcelona Submission deadline for contributed abstracts
January 10,  2005: Notification of interest in topic

January 30,  2005: Submission of PowerPoint Presentations (New York and 
Barcelona)
February 15, 2005: Final Approval of Presentations

February 21, 2005: Early-bird Registration Deadline

February 28, 2005  Camera-ready presentations due

March 29-30, 2005  New York conference
April 4-6    2005  Barcelona conference



From ripley at stats.ox.ac.uk  Mon Oct  4 21:05:07 2004
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Mon, 4 Oct 2004 20:05:07 +0100 (BST)
Subject: [R] Reading Version 4 .sdd files
In-Reply-To: <2A03167DD08194408B35A53E75EFE07E6EC599@seaexch01.corp.avenuea.com>
Message-ID: <Pine.LNX.4.44.0410041956370.2419-100000@gannet.stats>

On Mon, 4 Oct 2004, John Chandler wrote:

> Dear R-Help,
> 
> I've never had any trouble importing data into R until I had to import
> an .sdd file for a class. The file can be found here:
> http://www.math.umt.edu/steele/Math%20549/Farms.sdd. It begins with the
> line "## Dump S Version 4 Dump ##". I first attempted read.S which
> issued the message "not an S object". I then checked the Import/Export
> manual which seemed to indicate that data.restore might do the trick.

That manual says otherwise.

> Alas, it seems that function is trying to interpret the name of the data
> frame as an S mode or some such thing.
> 
> Searched the archives (courtesy of Dr. Baron) and found this from Dr.
> Ripley: ".sdd files from S-PLUS 6.0 are
> S4 dumps, and will not work." This was written in 2002, is it still
> accurate? 

Yes.  It's right there on the help page for data.restore, twice!  It is
also in the `R Data Import/Export Manual' you cited.

> Right now my only obvious course of action is to get on a
> machine that has S+, read in this file and dump it in a more useful
> format. Are their others?

Not that I have ever heard of, at least not open to those who have no
access to S4 source code.  Whereas the S3 dump format has been disclosed, 
I don't believe that the S4 one has.

If this is an S4 class then you really want a dump of it, as R's internal 
representation is not at all like S4's, I believe.

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From sundar.dorai-raj at PDF.COM  Mon Oct  4 21:05:02 2004
From: sundar.dorai-raj at PDF.COM (Sundar Dorai-Raj)
Date: Mon, 04 Oct 2004 14:05:02 -0500
Subject: [R] Reading Version 4 .sdd files
In-Reply-To: <2A03167DD08194408B35A53E75EFE07E6EC599@seaexch01.corp.avenuea.com>
References: <2A03167DD08194408B35A53E75EFE07E6EC599@seaexch01.corp.avenuea.com>
Message-ID: <41619EDE.7080904@pdf.com>


John Chandler wrote:

> Dear R-Help,
> 
> I've never had any trouble importing data into R until I had to import
> an .sdd file for a class. The file can be found here:
> http://www.math.umt.edu/steele/Math%20549/Farms.sdd. It begins with the
> line "## Dump S Version 4 Dump ##". I first attempted read.S which
> issued the message "not an S object". I then checked the Import/Export
> manual which seemed to indicate that data.restore might do the trick.
> Alas, it seems that function is trying to interpret the name of the data
> frame as an S mode or some such thing.
> 
> Searched the archives (courtesy of Dr. Baron) and found this from Dr.
> Ripley: ".sdd files from S-PLUS 6.0 are
> S4 dumps, and will not work." This was written in 2002, is it still
> accurate? Right now my only obvious course of action is to get on a
> machine that has S+, read in this file and dump it in a more useful
> format. Are their others?
> 

Yes, it is still accurate. If you load the file in S-PLUS then do a 
data.dump with oldStyle = TRUE then you should be able to use 
data.restore in R quite easily. If you don't have access to S-PLUS I'm 
sure someone on this list (including myself) would be happy to do the 
conversion for you.

--sundar



From ripley at stats.ox.ac.uk  Mon Oct  4 21:10:31 2004
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Mon, 4 Oct 2004 20:10:31 +0100 (BST)
Subject: [R] problems when compiling R
In-Reply-To: <200410041619.RAA11574@burnmoor.ncl.ac.uk>
Message-ID: <Pine.LNX.4.44.0410042006280.2419-100000@gannet.stats>

On Mon, 4 Oct 2004, Tao Chen wrote:

> When I was trying to compile R-1.9.1 from source, I've got problems
> which seemed related to compiling the LAPACK package.

There is no `LAPACK package': I think you mean the `lapack module'.

> The error message is as follows:
> 
> gcc -shared -L/usr/local/lib -o libRlapack.so dlamc.lo dlapack0.lo
> dlapack1.lo dlapack2.lo dlapack3.lo cmplx.lo -lblas -L/usr/local/lib
> -L/usr/lib/gcc-lib/ia64-redhat-linux/2.96
> -L/usr/lib/gcc-lib/ia64-redhat-linux/2.96/../../.. -lg2c -lm
> /usr/bin/ld: open.o: @gprel relocation against dynamic symbol f__buflen
> /usr/bin/ld: open.o: @gprel relocation against dynamic symbol f__buflen
> /usr/bin/ld: open.o: @gprel relocation against dynamic symbol f__buflen
> /usr/bin/ld: open.o: @gprel relocation against dynamic symbol f__buflen
> /usr/bin/ld: open.o: @gprel relocation against dynamic symbol f__buflen
> /usr/bin/ld: open.o: @gprel relocation against dynamic symbol f__buflen
> collect2: ld returned 1 exit status

You will need to get a released version of gcc (2.96 is a RedHat fiction),
and one with a shared libg2c.  Gcc 3.4.2 is current whereas your compiler
is both ancient and known to be bug-ridden.

> I just run the "configure" and then "make". The system information is as
> follows:
> 
> Hardware:
> SGI Altix 3700 Supercluster
> 28 1.3G Itanium dual processors (ia64)
> 
> OS:
> Redhat Linux AS2.1 (ia64)
> 
> 
> I re-run the compling process on my own PC (AMD XP 2000+ with redhat linux 9) and got no problem at all ...
> 
> almost hang my neck on that as I need to move all my work to that supercluster

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From Auston_Wei at mdanderson.org  Mon Oct  4 21:27:02 2004
From: Auston_Wei at mdanderson.org (Auston_Wei@mdanderson.org)
Date: Mon, 4 Oct 2004 14:27:02 -0500
Subject: [R] how to change data type in data frame?
Message-ID: <OF86F5390D.DCB0DF58-ON86256F23.00692F2D-86256F23.006ACA75@mdacc.tmc.edu>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20041004/b3265d15/attachment.pl

From andy_liaw at merck.com  Mon Oct  4 22:16:32 2004
From: andy_liaw at merck.com (Liaw, Andy)
Date: Mon, 4 Oct 2004 16:16:32 -0400
Subject: [R] how to change data type in data frame?
Message-ID: <3A822319EB35174CA3714066D590DCD504AF84C1@usrymx25.merck.com>

Here's one way:

> temp <- as.matrix(trash)
> temp[temp=='b'] <- 1
> temp[temp=='a'] <- 0
> temp <- as.data.frame(structure(as.numeric(temp), dim=dim(trash),
dimnames=dimnames(trash)))
> temp
  age typeI typeII
1   1     0      1
2   2     0      0
3   3     1      1
4   4     0      1
5   5     1      0

HTH,
Andy

> From: Auston_Wei at mdanderson.org
> 
> Hi, list,
> 
> suppose i have such a data frame:
> 
> trash <- 
> data.frame(cbind(seq(1:5),c('a','a','b','a','b'),c('b','a','b'
> ,'b','a')))
> names(trash) <- c('age','typeI','typeII')
> 
> and I want to change all 'a's to be 0 and 'b's to be 1. 
> 
> temp <- as.matrix(trash)
> temp[temp=='a'] <- 0
> temp[temp=='b'] <- 1
> temp <- data.frame(temp)
> 
> the problem was that temp$typeI and temp$typeII were still factors, 
> whereas I want numeric type. How can I make it?
> 
> Thanks,
> Auston
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
> 
>



From weihong at ualberta.ca  Mon Oct  4 22:51:23 2004
From: weihong at ualberta.ca (weihong)
Date: Mon, 4 Oct 2004 14:51:23 -0600
Subject: [R] call step inside a function
Message-ID: <41659EB9@webmail.ualberta.ca>

I am wondering why my function works fine in R1.7.1 and R1.8.1 but not in 
R1.9.0. I thought it's an environment related problem but I can't solve it.

>data
   weta jd
1     1  4
2     2 13
3     2 13
4     6  4
5     1  3
6     1  7
7     2 10
8     3 10
9     1  8
10    1  8
11    3  6
12    1  9
13    1  5
14    1  1
15    3 13
16    1  2
17    2  2
18    7 11
19    1  3
20    5  4
21    1  6
22    4  9
23    1  6
24    4  5
25    5  5
26    2  6

> program
function(dataset)
{
        tmp<-glm(weta~1, family=poisson, data=dataset)
        tmp.f<-step(tmp,~.+jd)    
}

When I run program(data) in 1.9.0, an error message appears:

Error in model.frame.default(formula = WETA ~ jd, data = dataset, 
drop.unused.levels = TRUE) :
        Object "dataset" not found


Thanks for help in advance!

Weihong Zeng
University of Albert



From kvyas at utk.edu  Mon Oct  4 22:48:12 2004
From: kvyas at utk.edu (kvyas)
Date: Mon, 4 Oct 2004 16:48:12 -0400
Subject: [R] Help with Affymetrix data
Message-ID: <41634429@webmail.utk.edu>

I have CEL files from Affymetrix Mouse Array 430_2 and am trying to get the 
the individual PM intensities (11 per gene) for each sample. I would like to 
write out this into a tab delimited text file. Where am I stalling? This is 
what I've done:

Change dir(to where CEL files are saved)
Data <- ReadAffy()
eset <- rma(Data)
write.exprs(eset, file="mydata.txt")

With this I am only able to get the average intensities per gene not the 
intensity per probe of each gene.

Please help me out.

Kanan



From fhduan at gmail.com  Mon Oct  4 22:56:37 2004
From: fhduan at gmail.com (Frank Duan)
Date: Mon, 4 Oct 2004 16:56:37 -0400
Subject: [R] Help with Affymetrix data
In-Reply-To: <41634429@webmail.utk.edu>
References: <41634429@webmail.utk.edu>
Message-ID: <3b917231041004135654d26192@mail.gmail.com>

use PM function in Affy package.

Frank


On Mon, 4 Oct 2004 16:48:12 -0400, kvyas <kvyas at utk.edu> wrote:
> I have CEL files from Affymetrix Mouse Array 430_2 and am trying to get the
> the individual PM intensities (11 per gene) for each sample. I would like to
> write out this into a tab delimited text file. Where am I stalling? This is
> what I've done:
> 
> Change dir(to where CEL files are saved)
> Data <- ReadAffy()
> eset <- rma(Data)
> write.exprs(eset, file="mydata.txt")
> 
> With this I am only able to get the average intensities per gene not the
> intensity per probe of each gene.
> 
> Please help me out.
> 
> Kanan
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>



From wwei at mdanderson.org  Mon Oct  4 22:48:10 2004
From: wwei at mdanderson.org (Auston Wei)
Date: Mon, 4 Oct 2004 20:48:10 +0000 (UTC)
Subject: [R] how to change data type in data frame?
References: <3A822319EB35174CA3714066D590DCD504AF84C1@usrymx25.merck.com>
Message-ID: <loom.20041004T224033-541@post.gmane.org>

Liaw, Andy <andy_liaw <at> merck.com> writes:

> 
> Here's one way:
> 
> > temp <- as.matrix(trash)
> > temp[temp=='b'] <- 1
> > temp[temp=='a'] <- 0
> > temp <- as.data.frame(structure(as.numeric(temp), dim=dim(trash),
> dimnames=dimnames(trash)))
> > temp
>   age typeI typeII
> 1   1     0      1
> 2   2     0      0
> 3   3     1      1
> 4   4     0      1
> 5   5     1      0
> 
> HTH,
> Andy
> 

Thank you very much, Andy. It works. Yet another question, what if I want to 
keep age as a factor? Is it possible?

thank you,
Auston



From sundar.dorai-raj at PDF.COM  Mon Oct  4 23:05:27 2004
From: sundar.dorai-raj at PDF.COM (Sundar Dorai-Raj)
Date: Mon, 04 Oct 2004 16:05:27 -0500
Subject: [R] call step inside a function
In-Reply-To: <41659EB9@webmail.ualberta.ca>
References: <41659EB9@webmail.ualberta.ca>
Message-ID: <4161BB17.4030703@pdf.com>



weihong wrote:
> I am wondering why my function works fine in R1.7.1 and R1.8.1 but not in 
> R1.9.0. I thought it's an environment related problem but I can't solve it.
> 
> 
>>data
> 
>    weta jd
> 1     1  4
> 2     2 13
> 3     2 13
> 4     6  4
> 5     1  3
> 6     1  7
> 7     2 10
> 8     3 10
> 9     1  8
> 10    1  8
> 11    3  6
> 12    1  9
> 13    1  5
> 14    1  1
> 15    3 13
> 16    1  2
> 17    2  2
> 18    7 11
> 19    1  3
> 20    5  4
> 21    1  6
> 22    4  9
> 23    1  6
> 24    4  5
> 25    5  5
> 26    2  6
> 
> 
>>program
> 
> function(dataset)
> {
>         tmp<-glm(weta~1, family=poisson, data=dataset)
>         tmp.f<-step(tmp,~.+jd)    
> }
> 
> When I run program(data) in 1.9.0, an error message appears:
> 
> Error in model.frame.default(formula = WETA ~ jd, data = dataset, 
> drop.unused.levels = TRUE) :
>         Object "dataset" not found
> 
> 
> Thanks for help in advance!
> 
> Weihong Zeng
> University of Albert
> 

This is a scoping problem. There may be better answers but the following 
trick has worked for me in the past:

f <- function(z) {
   eval(substitute(fit <- glm(weta ~ 1, family = poisson, data = z),
                   list(z = z)))
   step(fit, ~ . + jd)
}

--sundar



From thpe at hhbio.wasser.tu-dresden.de  Mon Oct  4 23:09:31 2004
From: thpe at hhbio.wasser.tu-dresden.de (Thomas Petzoldt)
Date: Mon, 04 Oct 2004 23:09:31 +0200
Subject: [R] Beginners problem
In-Reply-To: <416157AB.7000905@ilt.fraunhofer.de>
References: <416157AB.7000905@ilt.fraunhofer.de>
Message-ID: <4161BC0B.3030909@hhbio.wasser.tu-dresden.de>

Rolf Wester wrote:
> Hi,
> 
> I'm new to R and have a problem with a little test program (see below). 
> Why doesn't <<- in function rk4

The reason is lexical scoping (see FAQ), and I suggest to implement rk4 
as function with a return value (see # !!!). BTW there are already a rk4 
(and lsoda) functions in the odesolve package.

Thomas P.

Your example formally corrected:

rk4 <- function(x,y,f,h) {
  n  <- length(y)
  hh <- h*0.5
  k1 <- f(x,y)
  k2 <- f(x,y+k1*hh)
  k3 <- f(x,y+k2*hh)
  k4 <- f(x,y+k3*h)
  y + h/6.0*(k1 + 2.0*k2 + 2.0*k3 + k4) # !!!
}

rkf <- function(x,y) {
  -y
}

rktest <- function(){
  y <- 1.0
  x <- 0.0
  h <- 0.1
  for(i in 1:10) {
    y <- rk4(x,y,rkf,h) # !!!
    print(y)
  }
}

rktest()



From MSchwartz at MedAnalytics.com  Mon Oct  4 23:09:29 2004
From: MSchwartz at MedAnalytics.com (Marc Schwartz)
Date: Mon, 04 Oct 2004 16:09:29 -0500
Subject: [R] how to change data type in data frame?
In-Reply-To: <OF86F5390D.DCB0DF58-ON86256F23.00692F2D-86256F23.006ACA75@mdacc.tmc.edu>
References: <OF86F5390D.DCB0DF58-ON86256F23.00692F2D-86256F23.006ACA75@mdacc.tmc.edu>
Message-ID: <1096924168.4471.189.camel@localhost.localdomain>

On Mon, 2004-10-04 at 14:27, Auston_Wei at mdanderson.org wrote:
> Hi, list,
> 
> suppose i have such a data frame:
> 
> trash <- 
> data.frame(cbind(seq(1:5),c('a','a','b','a','b'),c('b','a','b','b','a')))
> names(trash) <- c('age','typeI','typeII')
> 
> and I want to change all 'a's to be 0 and 'b's to be 1. 
> 
> temp <- as.matrix(trash)
> temp[temp=='a'] <- 0
> temp[temp=='b'] <- 1
> temp <- data.frame(temp)
> 
> the problem was that temp$typeI and temp$typeII were still factors, 
> whereas I want numeric type. How can I make it?
> 
> Thanks,
> Auston

First, you need to be careful relative to the way in which you are
creating the data frame.

'trash', as you have created it, is a data frame of all factors:

> str(trash)
`data.frame':	5 obs. of  3 variables:
 $ age   : Factor w/ 5 levels "1","2","3","4",..: 1 2 3 4 5
 $ typeI : Factor w/ 2 levels "a","b": 1 1 2 1 2
 $ typeII: Factor w/ 2 levels "a","b": 2 1 2 2 1


This is because you used cbind(), which will first result in a matrix of
characters:

> cbind(seq(1:5), c('a','a','b','a','b'), c('b','a','b','b','a'))
     [,1] [,2] [,3]
[1,] "1"  "a"  "b" 
[2,] "2"  "a"  "a" 
[3,] "3"  "b"  "b" 
[4,] "4"  "a"  "b" 
[5,] "5"  "b"  "a" 

and then this matrix is converted into a data frame. In the process of
converting the character matrix into a data frame, the characters are
converted into factors.

Thus, if you want to preserve the multiple data types, for which a data
frame is used, you can do the following, noting that you can name the
columns here in the same step:

trash <- data.frame(age = 1:5,
                    typeI = I(c('a','a','b','a','b')),
                    typeII = I(c('b','a','b','b','a')))

In the above, note the use of "I(...)", which preserves the character
nature of typeI and typeII:

> str(trash)
`data.frame':	5 obs. of  3 variables:
 $ age   : int  1 2 3 4 5
 $ typeI :Class 'AsIs'  chr [1:5] "a" "a" "b" "a" ...
 $ typeII:Class 'AsIs'  chr [1:5] "b" "a" "b" "b" ...

Once you have the data frame in this format, you can then do your
replacements. You could either do the conversion one column at a time,
as you have done above, or you can do them in one step:

trash[, 2:3] <- ifelse(trash[, 2:3] == 'a', 0, 1)

> trash
  age typeI typeII
1   1     0      1
2   2     0      0
3   3     1      1
4   4     0      1
5   5     1      0

> str(trash)
`data.frame':	5 obs. of  3 variables:
 $ age   : int  1 2 3 4 5
 $ typeI : num  0 0 1 0 1
 $ typeII: num  1 0 1 1 0

You should note however, that depending upon what you intend to do with
the data in typeI and typeII, you may want to keep them as factors,
since many functions (ie. modeling functions) utilize the factor data
type specifically.

See ?data.frame and ?I for more information.

HTH,

Marc Schwartz



From p.dalgaard at biostat.ku.dk  Mon Oct  4 23:29:24 2004
From: p.dalgaard at biostat.ku.dk (Peter Dalgaard)
Date: 04 Oct 2004 23:29:24 +0200
Subject: [R] Help with Affymetrix data
In-Reply-To: <3b917231041004135654d26192@mail.gmail.com>
References: <41634429@webmail.utk.edu>
	<3b917231041004135654d26192@mail.gmail.com>
Message-ID: <x2zn32yxxn.fsf@biostat.ku.dk>

Frank Duan <fhduan at gmail.com> writes:

> use PM function in Affy package.

 - and the Bioconductor mailing list, not the general R one....

-- 
   O__  ---- Peter Dalgaard             Blegdamsvej 3  
  c/ /'_ --- Dept. of Biostatistics     2200 Cph. N   
 (*) \(*) -- University of Copenhagen   Denmark      Ph: (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)             FAX: (+45) 35327907



From tlumley at u.washington.edu  Mon Oct  4 23:32:15 2004
From: tlumley at u.washington.edu (Thomas Lumley)
Date: Mon, 4 Oct 2004 14:32:15 -0700 (PDT)
Subject: [R] call step inside a function
In-Reply-To: <41659EB9@webmail.ualberta.ca>
References: <41659EB9@webmail.ualberta.ca>
Message-ID: <Pine.A41.4.61.0410041426120.92704@homer10.u.washington.edu>

On Mon, 4 Oct 2004, weihong wrote:
>> program
> function(dataset)
> {
>        tmp<-glm(weta~1, family=poisson, data=dataset)
>        tmp.f<-step(tmp,~.+jd)
> }
>
> When I run program(data) in 1.9.0, an error message appears:
>
> Error in model.frame.default(formula = WETA ~ jd, data = dataset,
> drop.unused.levels = TRUE) :
>        Object "dataset" not found

Yes, it looks as though add1.glm is not putting the environment 
information in the same place that model.frame.glm is looking for it.

add1.glm sets up an empty shell of a glm object and calls model.frame.glm, 
but model.frame.glm uses the $terms component to specify the environment, 
and add1.glm doesn't give a $terms component.  Presumably at some time in 
the past model.frame.glm used the $call$formula  component instead.


 	-thomas



From ripley at stats.ox.ac.uk  Mon Oct  4 23:57:08 2004
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Mon, 4 Oct 2004 22:57:08 +0100 (BST)
Subject: [R] call step inside a function
In-Reply-To: <Pine.A41.4.61.0410041426120.92704@homer10.u.washington.edu>
Message-ID: <Pine.LNX.4.44.0410042247540.18043-100000@gannet.stats>

On Mon, 4 Oct 2004, Thomas Lumley wrote:

> On Mon, 4 Oct 2004, weihong wrote:
> >> program
> > function(dataset)
> > {
> >        tmp<-glm(weta~1, family=poisson, data=dataset)
> >        tmp.f<-step(tmp,~.+jd)
> > }
> >
> > When I run program(data) in 1.9.0, an error message appears:
> >
> > Error in model.frame.default(formula = WETA ~ jd, data = dataset,
> > drop.unused.levels = TRUE) :
> >        Object "dataset" not found
> 
> Yes, it looks as though add1.glm is not putting the environment 
> information in the same place that model.frame.glm is looking for it.
> 
> add1.glm sets up an empty shell of a glm object and calls model.frame.glm, 
> but model.frame.glm uses the $terms component to specify the environment, 
> and add1.glm doesn't give a $terms component.  Presumably at some time in 
> the past model.frame.glm used the $call$formula  component instead.

Previously it just ignored the environment of the formula, which was not
part of the search when it and add1.glm were originally written.  
($call$formula has no environment.)  It needs updating (and probably
drop1.glm too).

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From zhuw at mail.smu.edu  Mon Oct  4 23:58:01 2004
From: zhuw at mail.smu.edu (Wang, Zhu)
Date: Mon, 4 Oct 2004 16:58:01 -0500
Subject: [R] simulate bivariate life time distributions
Message-ID: <4FB6E9BD986F194EA932E1F135CDF6363029FA@s31xs3>

Dear helpers,
 
I was wondering if there are some bivariate distribution simulation functions in any R packages. Specifically, I want to simulate bivariate log logistic, bivariate Weibull, or other common bivariate life time functions, and I can specify the correlations. Any comments about this will be appreciated.
 
Thanks,
 
Zhu Wang

From ggrothendieck at myway.com  Tue Oct  5 00:09:58 2004
From: ggrothendieck at myway.com (Gabor Grothendieck)
Date: Mon, 4 Oct 2004 22:09:58 +0000 (UTC)
Subject: [R] R 2.0.0 is released
References: <x2oejipkm4.fsf@biostat.ku.dk>
Message-ID: <loom.20041005T000134-803@post.gmane.org>

Peter Dalgaard <p.dalgaard <at> biostat.ku.dk> writes:

: 
: I've rolled up R-2.0.0.tar.gz a short while ago. This is a new version
: with a number of new features. See below for the details.
: 
: As was the case with R 1.0.0, this new version represents a coming of
: age more than a radical change to R. We do plan to celebrate the new
: major version with press releases and such.
: 
: The release will be available from 
: 
: http://cran.r-project.org/src/base/R-2/R-2.0.0.tar.gz
: 
: or you might wait for it to be mirrored at a CRAN site nearer to you.
: Binaries for various platforms will appear in due course.
:  
: There is also a version split for floppies.
: 
: 
:         For the R Core Team
:         Peter Dalgaard

Congratulations to the R team and all involved for reaching the 2.0.0
milestone.  The progress of R is truly astounding.



From kvyas at utk.edu  Tue Oct  5 00:30:11 2004
From: kvyas at utk.edu (kvyas)
Date: Mon, 4 Oct 2004 18:30:11 -0400
Subject: [R] Help with Affymetrix data
Message-ID: <41648975@webmail.utk.edu>

I have made some progress.

Change dir(to where CEL files are saved)
Data <- ReadAffy()
eset <- rma(Data)
my.pm.data <- pm(Data)
write.table (my.pm.data, file = "pmprobes3.csv", sep = ",", col.names = TRUE, 
row.names = probeNames(Data))

This results in the following:

Probe	Sam1	Sam2	Sam3	Sam4
1415670_at01	340	383	376	320
1415670_at02	794	932	734	697
1415670_at03	1117	1366	1013	932
1415670_at04	97	97	148	94
1415670_at05	387	405	367	402
1415670_at06	485	553	391	343
1415670_at07	458	509	410	385
1415670_at08	507	684	616	545
1415670_at09	311	392	576	463
1415670_at10	691	822	688	588
1415670_at11	512	726	3201	2317
1415671_at01	1230	1212	873	770
1415671_at02	886	877	794	729
1415671_at03	1472	1682	1263	1152
1415671_at04	1171	1214	948	914
1415671_at05	390	479	435	396
1415671_at06	462	513	423	389
1415671_at07	987	1151	1102	815
1415671_at08	992	1066	1203	1013
1415671_at09	489	534	501	385
1415671_at10	401	509	379	293
1415671_at11	1443	1686	1410	1215


How can I get the p-value for each probe?



>===== Original Message From Frank Duan <fhduan at gmail.com> =====
>use PM function in Affy package.
>
>Frank
>
>
>On Mon, 4 Oct 2004 16:48:12 -0400, kvyas <kvyas at utk.edu> wrote:
>> I have CEL files from Affymetrix Mouse Array 430_2 and am trying to get the
>> the individual PM intensities (11 per gene) for each sample. I would like 
to
>> write out this into a tab delimited text file. Where am I stalling? This is
>> what I've done:
>>
>> Change dir(to where CEL files are saved)
>> Data <- ReadAffy()
>> eset <- rma(Data)
>> write.exprs(eset, file="mydata.txt")
>>
>> With this I am only able to get the average intensities per gene not the
>> intensity per probe of each gene.
>>
>> Please help me out.
>>
>> Kanan
>>
>> ______________________________________________
>> R-help at stat.math.ethz.ch mailing list
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide! 
http://www.R-project.org/posting-guide.html
>>



From ggrothendieck at myway.com  Tue Oct  5 00:52:18 2004
From: ggrothendieck at myway.com (Gabor Grothendieck)
Date: Mon, 4 Oct 2004 22:52:18 +0000 (UTC)
Subject: [R] Identifying time series
References: <FDAE352C6A59D311A3DF00508B0169F51940DBA0@corvus.moorecap.com>
Message-ID: <loom.20041005T001259-259@post.gmane.org>

John DeAngelis <john.deangelis <at> moorecap.com> writes:

: 
: Hello,
: 
: I am currently attempting to introduce R at my company and am trying to
: import time series data from a text file into R to graph. The format of the
: text file is in date, data (e.g.,  20040929 3.361). My problem is that I do
: not know how to get R to recognize the first column as a business date
: series. Or at the very least, I am unable to find a function that will grap
: the second column (y-axis) against the date series (x-axis). If you could
: tell me how to graph this type of data, I would greatly appreciate it. Thank
: you.


The most straight forward way is just to read it into a data frame,
convert the first column to Date class and then plot:

	x <- read.table("myseries.dat")
	colnames(x) = c("date", "value")   # label the columns
	x$date <- as.Date(as.character(x$date), "%Y%m%d")
	plot(value ~ date, x)

However, you might wish to use one of the irregular time series packages
in R.  These include zoo (in package zoo), its (in package its) and
timeSeries (in package fBasics).  My preference here would be zoo since
it can use the Date class which is particularly suitable for daily
data:

	require(zoo)
	z <- read.table("myseries.dat") 
	colnames(z) = c("date", "value")  
	z <- zoo(z$value, as.Date(as.character(z$date), "%Y%m%d"))
	plot(z)



From savano at superig.com.br  Tue Oct  5 01:42:06 2004
From: savano at superig.com.br (Savano)
Date: Mon, 04 Oct 2004 20:42:06 -0300
Subject: [R] BDS.TEST
Message-ID: <5.1.0.14.2.20041004203714.00b9bda0@pop.superig.com.br>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20041004/d328e00c/attachment.pl

From yshao at wadsworth.org  Tue Oct  5 01:39:33 2004
From: yshao at wadsworth.org (Yu Shao)
Date: Mon, 4 Oct 2004 19:39:33 -0400 (EDT)
Subject: [R] Using model operator in stepwise function's upper scope formula
Message-ID: <200410042339.i94NdXj25088@csserv.wadsworth.org>

Hello:

I am doing forward stepwise analysis on the glm model. I am trying to use model 
operator in the "upper" scope formula, for example,          

                       scope=list(lower=~1,upper=~ .^2)

but the upper bound of the scope seems to be ignored and add1 is not performed 
at all, while if the terms are explicitly listed in the formula, the step 
function seems to work properly. Example:

> version 
platform sparc-sun-solaris2.9
arch     sparc               
os       solaris2.9          
system   sparc, solaris2.9   
status                       
major    1                   
minor    9.0                 
year     2004                
month    04                  
day      12                  
language R                   

> testdata
      effect  fa1 fa2    fa3
1  0.5054526 -1.4   1 -15.42
2  0.1366526 -2.5   3 -14.72
3  0.4798526 -1.1   3 -16.43
4  0.5566526 -4.6   2 -17.62
5  0.6568526 -3.5   7  -5.41
6  0.6653526  0.0   5 -11.88
7  0.6376526  0.0   5 -11.72
8  0.5203526 -2.9  -1 -14.72
9  0.4905526 -2.1   2 -17.68
10 0.6376526 -1.1   1 -13.39

> step (glm(effect ~ 1 , data=testdata), scope=list(lower=~1,upper=~.), 
direction='forward')
Start:  AIC= -5.89 
 effect ~ 1 


Call:  glm(formula = effect ~ 1, data = testdata) 

Coefficients:
(Intercept)  
     0.5287  

Degrees of Freedom: 9 Total (i.e. Null);  9 Residual
Null Deviance:      0.2178 
Residual Deviance: 0.2178       AIC: -5.89 

================================================================

But if I listed the term explicitly in the formula, then step function seemed to 
work correctly:

> step (glm(effect ~ 1 , data=testdata), 
scope=list(lower=~1,upper=~fa1+fa2+fa3), direction='forward')
Start:  AIC= -5.89 
 effect ~ 1 

       Df Deviance     AIC
+ fa3   1   0.1778 -5.9188
<none>      0.2178 -5.8901
+ fa2   1   0.2023 -4.6271
+ fa1   1   0.2065 -4.4194

Step:  AIC= -5.92 
 effect ~ fa3 

       Df Deviance     AIC
<none>      0.1778 -5.9188
+ fa1   1   0.1698 -4.3783
+ fa2   1   0.1770 -3.9605

Call:  glm(formula = effect ~ fa3, data = testdata) 

Coefficients:
(Intercept)          fa3  
    0.78310      0.01830  

Degrees of Freedom: 9 Total (i.e. Null);  8 Residual
Null Deviance:      0.2178 
Residual Deviance: 0.1778       AIC: -5.919 
================================================================

I've lookup the online manual of step function but couldn't find a solution. 
Thanks in advance,

Yu Shao
Wadsworth Center, New York State Department of Health,
Albany, NY 12208



From ok at cs.otago.ac.nz  Tue Oct  5 02:56:14 2004
From: ok at cs.otago.ac.nz (Richard A. O'Keefe)
Date: Tue, 5 Oct 2004 13:56:14 +1300 (NZDT)
Subject: [R] R for education research tutoral?
Message-ID: <200410050056.i950uEfe293099@atlas.otago.ac.nz>

I have a couple of colleagues doing research in Computer Science
Education.  I've talked one of them into using R (he is also interested
in data mining) and am about half-way to persuading the other (who agrees
that he ought to know more about analysing his data).  Neither of them
has a strong statistic background, but both are fairly able and willing
to learn new tools and techniques.

If anyone has a tutorial or set of course notes on how to use R to
analyse the results of education studies, or knows where I might find
such a thing, could you please tell me?

I have downloaded and read all the English-language tutorials on the
"Contributed Documentation" page; Baron & Li is probably closest to
what I'm after but not quite there.

This is *not* a request for general "Introduction to Statistics for
Education Research" tutorials, although I wouldn't turn down any such
references that people sent my way.  I specifically want something to
give my colleagues that explains how to use *R* to analyse their kinds
of data.



From unung at enciety.com  Tue Oct  5 03:34:13 2004
From: unung at enciety.com (Unung Istopo Hartanto)
Date: Tue, 05 Oct 2004 08:34:13 +0700
Subject: [R] R 2.0.0 is released
In-Reply-To: <x2oejipkm4.fsf@biostat.ku.dk>
References: <x2oejipkm4.fsf@biostat.ku.dk>
Message-ID: <1096940053.3286.4.camel@IT05>

Congratulation Team and Thanks R for your innovation in Statistical
Analysis.

Regards,


On Mon, 2004-10-04 at 22:29, Peter Dalgaard wrote:
> I've rolled up R-2.0.0.tar.gz a short while ago. This is a new version
> with a number of new features. See below for the details.
> 
> As was the case with R 1.0.0, this new version represents a coming of
> age more than a radical change to R. We do plan to celebrate the new
> major version with press releases and such.
> 
> The release will be available from 
> 
> http://cran.r-project.org/src/base/R-2/R-2.0.0.tar.gz
> 
> or you might wait for it to be mirrored at a CRAN site nearer to you.
> Binaries for various platforms will appear in due course.
>  
> There is also a version split for floppies.
> 
> 
>         For the R Core Team
>         Peter Dalgaard
> 
> 
> 
> 
> These are the md5sums for the freshly created files, in case you wish
> to check that they are uncorrupted:
> 
> 3900bca37cabb4b76b8d736d51cc9251  R-2.0.0.tar.gz
> 4a3c7595b112d879997f7455fa8c1c0d  R-2.0.0.tar.gz-split.aa
> 45d376c7c533c62c657ef0fafac8a784  R-2.0.0.tar.gz-split.ab
> 7dbdb241e1fb7263701719ea856ebe41  R-2.0.0.tar.gz-split.ac
> 1e1077f7593778b79e9b0d5c676af63b  R-2.0.0.tar.gz-split.ad
> 6f4ffaa48a54e002f586f80fd4f2461c  R-2.0.0.tar.gz-split.ae
> 34b62bb31f6ecf84da329d21c2a21561  R-2.0.0.tar.gz-split.af
> c880f8f06ca3fe367bba6757e9cfbf32  R-2.0.0.tar.gz-split.ag
> b1a2d17d3ae523d04ffc8d3c6db4b67b  R-2.0.0.tar.gz-split.ah
> 
> 
> Here is the relevant part of the NEWS file
> 
> 
> 		CHANGES IN R VERSION 2.0.0
> 
> 
> USER-VISIBLE CHANGES
> 
>     o	The stub packages from 1.9.x have been removed: the library()
> 	function selects the new home for their code.
> 
>     o	`Lazy loading' of R code has been implemented, and is used for
> 	the standard and recommended packages by default.  Rather than
> 	keep R objects in memory, they are kept in a database on disc
> 	and only loaded on first use.  This accelerates startup (down
> 	to 40% of the time for 1.9.x) and reduces memory usage -- the
> 	latter is probably unimportant of itself, but reduces
> 	commensurately the time spent in garbage collection.
> 
> 	Packages are by default installed using lazy loading if they
> 	have more than 25Kb of R code and did not use a saved image.
> 	This can be overridden by INSTALL --[no-]lazy or via a field
> 	in the DESCRIPTION file.  Note that as with --save, any other
> 	packages which are required must be already installed.
> 
> 	As the lazy-loading databases will be consulted often, R
> 	will be slower if run from a slow network-mounted disc.
> 
>     o	All the datasets formerly in packages 'base' and 'stats' have
> 	been moved to a new package 'datasets'.	 data() does the
> 	appropriate substitution, with a warning.  However, calls to
> 	data() are not normally needed as the data objects are visible
> 	in the 'datasets' package.
> 
> 	Packages can be installed to make their data objects visible
> 	via R CMD INSTALL --lazy-data or via a field in the
> 	DESCRIPTION file.
> 
>     o	Package 'graphics' has been split into 'grDevices' (the graphics
> 	devices shared between base and grid graphics) and 'graphics'
> 	(base graphics).  Each of the 'graphics' and 'grid' packages
> 	load 'grDevices' when they are attached.  Note that
> 	ps.options() has been moved to grDevices and user hooks may
> 	need to be updated.
> 
>     o	The semantics of data() have changed (and were incorrectly
> 	documented in recent releases) and the function has been moved
> 	to package 'utils'.  Please read the help page carefully if
> 	you use the 'package' or 'lib.loc' arguments.
> 
> 	data() now lists datasets, and not just names which data() accepts.
> 
>     o	Dataset 'phones' has been renamed to 'WorldPhones'.
> 
>     o	Datasets 'sunspot.month' and 'sunspot.year' are available
> 	separately but not via data(sunspot) (which was used by package
> 	lattice to retrieve a dataset 'sunspot').
> 
>     o	Packages must have been re-installed for this version, and
> 	library() will enforce this.
> 
>     o	Package names must now be given exactly in library() and
> 	require(), regardless of whether the underlying file system is
> 	case-sensitive or not.	So 'library(mass)' will not work, even
> 	on Windows.
> 
>     o	R no longer accepts associative use of relational operators.
> 	That is, 3 < 2 < 1 (which used to evalute as TRUE!) now causes
> 	a syntax error.	 If this breaks existing code, just add
> 	parentheses -- or braces in the case of plotmath.
> 
>     o	The R parser now allows multiline strings, without escaping
> 	the newlines with backslashes (the old method still works).
> 	Patch by Mark Bravington.
> 
> 
> NEW FEATURES
> 
>     o	There is a new atomic vector type, class "raw".	 See ?raw for
> 	full details including the operators and utility functions provided.
> 
>     o	The default barplot() method by default uses a
> 	gamma-corrected grey palette (rather than the heat color
> 	palette) for coloring its output when given a matrix.
> 
>     o	The 'formula' method for boxplot() has a 'na.action' argument,
> 	defaulting to NULL.  This is mainly useful if the response
> 	is a matrix when the previous default of 'na.omit' would omit
> 	entire rows.  (Related to PR#6846.)
> 
> 	boxplot() and bxp() now obey global 'par' settings and also
> 	allow the specification of graphical options in more detail,
> 	compatibly with S-PLUS (fulfilling wishlist entry PR#6832)
> 	thanks to contributions from Arni Magnusson.  For consistency,
> 	'boxwex' is not an explicit argument anymore.
> 
>     o	chull() has been moved to package graphics (as it uses xy.coords).
> 
>     o	There is now a coef() method for summaries of "nls" objects.
> 
>     o	compareVersion(), packageDescription() and read.00Index()
> 	have been moved to package 'utils'.
> 
>     o	convolve(), fft(), mvfft() and nextn() have been moved to
> 	package stats.
> 
>     o	coplot() now makes use of cex.lab and font.lab par() settings.
> 
>     o	cumsum/prod/max/min() now preserve names.
> 
>     o	data(), .path.packages() and .find.packages() now interpret
> 	package = NULL to mean all loaded packages.
> 
>     o	data.frame() and its replacement methods remove the names from
> 	vector columns.	 Using I() will ensure that names are
> 	preserved.
> 
>     o	data.frame(check.names = TRUE) (the default) enforces unique
> 	names, as S does.
> 
>     o	.Defunct() now has 'new' and 'package' arguments like those of
> 	.Deprecated().
> 
>     o	The plot() method for "dendrogram" objects now respects many more
> 	nodePar and edgePar settings and for edge labeling computes the
> 	extents of the diamond more correctly.
> 
>     o	deparse(), dput() and dump() have a new 'control' argument to
> 	control the level of detail when deparsing.  dump() defaults to
> 	the most detail, the others default to less.  See ?.deparseOpts
> 	for the details.
> 
> 	They now evaluate promises by default: see ?dump for details.
> 
>     o	dir.create() now expands '~' in filenames.
> 
>     o	download.file() has a new progress meter (under Unix) if the
> 	length of the file is known -- it uses 50 equals signs.
> 
>     o	dyn.load() and library.dynam() return an object describing the
> 	DLL that was loaded.  For packages with namespaces, the DLL
> 	objects are stored in a list within the namespace.
> 
>     o	New function eapply() - apply for environments.	 The supplied
> 	function is applied to each element of the environment; the order
> 	of application is not specified.
> 
>     o	edit() and fix() use the object name in the window caption on
> 	some platforms (e.g. Windows).
> 
>     o	Function file.edit() function added: like file.show(), but
> 	allows editing.
> 
>     o	Function file.info() can return file sizes > 2G if the
> 	underlying OS supports such.
> 
>     o	fisher.test(*, conf.int=FALSE) allows the confidence interval
> 	computation to be skipped.
> 
>     o	formula() methods for classes "lm" and "glm" used the expanded
> 	formula (with '.' expanded) from the terms component.
> 
>     o	The `formula' method for ftable() now looks for variables in the
> 	environment of the formula before the usual search path.
> 
>     o	A new function getDLLRegisteredRoutines() returns information
> 	about the routines available from a DLL that were explicitly
> 	registered with R's dynamic loading facilities.
> 
>     o	A new function getLoadedDLLs() returns information about the
> 	DLLs that are currently loaded within this session.
> 
>     o	The package element returned by getNativeSymbolInfo() contains
> 	reference to both the internal object used to resolve symbols
> 	with the DLL, and the internal DllInfo structure used to
> 	represent the DLL within R.
> 
>     o	help() now returns information about available documentation for
> 	a given topic, and notifies about multiple matches.  It has a
> 	separate print() method.
> 
> 	If the latex help files were not installed, help() will offer
> 	to create a latex file on-the-fly from the installed .Rd file.
> 
>     o	heatmap() has a new argument 'reorderfun'.
> 
>     o	Most versions of install.packages() have an new optional
> 	argument 'dependencies = TRUE' which will not only fetch the
> 	packages but also their uninstalled dependencies and their
> 	dependencies ....
> 
> 	The Unix version of install.packages() attempts to install
> 	packages in an order that reflects their dependencies.	(This
> 	is not needed for binary installs as used under Windows.)
> 
>     o	interaction() has new argument 'sep'.
> 
>     o	interaction.plot() allows 'type = "b"' and doesn't give spurious
> 	warnings when passed a matplot()-only argument such as 'main'.
> 
>     o	is.integer() and is.numeric() always return FALSE for a
> 	factor.	 (Previously they were true and false respectively for
> 	well-formed factors, but it is possible to create factors
> 	with non-integer codes by underhand means.)
> 
>     o	New functions is.leaf(), dendrapply() and a labels() method for
> 	dendrogram objects.
> 
>     o	legend() has an argument 'pt.lwd' and setting 'density' now works
> 	because 'angle' now defaults to 45 (mostly contributed by Uwe Ligges).
> 
>     o	library() now checks the version dependence (if any) of
> 	required packages mentioned in the Depends: field of the
> 	DESCRIPTION file.
> 
>     o	load() now detects and gives a warning (rather than an error)
> 	for empty input, and tries to detect (but not correct) files
> 	which have had LF replaced by CR.
> 
>     o	ls.str() and lsf.str() now return an object of class "ls_str" which
> 	has a print method.
> 
>     o	make.names() has a new argument allow_, which if false allows
> 	its behaviour in R 1.8.1 to be reproduced.
> 
>     o	The 'formula' method for mosaicplot() has a 'na.action' argument
> 	defaulting to 'na.omit'.
> 
>     o	model.frame() now warns if it is given data = newdata and it
> 	creates a model frame with a different number of rows from
> 	that implied by the size of 'newdata'.
> 
> 	Time series attributes are never copied to variables in the
> 	model frame unless na.action = NULL.  (This was always the
> 	intention, but they sometimes were as the result of an earlier
> 	bug fix.)
> 
>     o	There is a new 'padj' argument to mtext() and axis().
> 	Code patch provided by Uwe Ligges (fixes PR#1659 and PR#7188).
> 
>     o	Function package.dependencies() has been moved to package 'tools'.
> 
>     o	The 'formula' method for pairs() has a 'na.action' argument,
> 	defaulting to 'na.pass', rather than the value of
> 	getOption("na.action").
> 
>     o	There are five new par() settings:
> 
> 	'family' can be used to specify a font family for graphics
> 	text.  This is a device-independent family specification
> 	which gets mapped by the graphics device to a device-specific
> 	font specification (see, for example, postscriptFonts()).
> 	Currently, only PostScript, PDF, X11, Quartz, and Windows
> 	respond to this setting.
> 
> 	'lend', 'ljoin', and 'lmitre' control the cap style and
> 	join style for drawing lines (only noticeable on thick lines
> 	or borders).  Currently, only PostScript, PDF, X11, and Quartz
> 	respond to these settings.
> 
> 	'lheight' is a multiplier used in determining the vertical
> 	spacing of multi-line text.
> 
> 	All of these settings are currently only available via par()
> 	(i.e., not in-line as arguments to plot(), lines(), ...)
> 
>     o	PCRE (as used by grep etc) has been updated to version 5.0.
> 
>     o	A 'version' argument has been added to pdf() device.  If this is
> 	set to "1.4", the device will support transparent colours.
> 
>     o	plot.xy(), the workhorse function of points(), lines() and
> 	plot.default() now has 'lwd' as explicit argument instead of
> 	implicitly in '...', and now recycles lwd where it makes
> 	sense, i.e. for line-based plot symbols.
> 
>     o	The png() and jpeg() devices (and the bmp() device under Windows)
> 	now allow a nominal resolution to be recorded in the file.
> 
>     o   New functions to control mapping from device-independent
> 	graphics font family to device-specific family:
> 	postscriptFont() and postscriptFonts() (for both postscript()
> 	and pdf()); X11Font() and X11Fonts(); windowsFont() and
> 	windowsFonts(); quartzFont() and quartzFonts().
> 
>     o	power (x^y) has optimised code for y == 2.
> 
>     o	prcomp() is now generic, with a formula method (based on an
> 	idea of Jari Oksanen).
> 
> 	prcomp() now has a simple predict() method.
> 
>     o	printCoefmat() has a new logical argument 'signif.legend'.
> 
>     o	quantile() has the option of several methods described in
> 	Hyndman & Fan (1996). (Contributed by Rob Hyndman.)
> 
>     o	rank() has two new 'ties.method's, "min" and "max".
> 
>     o	New function read.fortran() reads Fortran-style fixed-format
> 	specifications.
> 
>     o	read.fwf() reads multiline records, is faster for large files.
> 
>     o	read.table() now accepts "NULL", "factor", "Date" and
> 	"POSIXct" as possible values of colClasses, and colClasses can
> 	be a named character vector.
> 
>     o	readChar() can now read strings with embedded nuls.
> 
>     o	The "dendrogram" method for reorder() now has a 'agglo.FUN'
> 	argument for specification of a weights agglomeration
> 	function.
> 
>     o	New reorder() method for factors, slightly extending that in
> 	lattice.  Contributed by Deepayan Sarkar.
> 
>     o	Replaying a plot (with replayPlot() or via autoprinting) now
> 	automagically opens a device if none is open.
> 
>     o	replayPlot() issues a warning if an attempt is made to replay
> 	a plot that was recorded using a different R version (the
> 	format for recorded plots is not guaranteed to be stable
> 	across different R versions).  The Windows-menu equivalent
> 	(History...Get from variable) issues a similar warning.
> 
>     o	reshape() can handle multiple 'id' variables.
> 
>     o	It is now possible to specify colours with a full alpha
> 	transparency channel via the new 'alpha' argument to the
> 	rgb() and hsv() functions, or as a string of the form "#RRGGBBAA".
> 
> 	NOTE: most devices draw nothing if a colour is not opaque,
> 	but PDF and Quartz devices will render semitransparent colours.
> 
> 	A new argument 'alpha' to the function col2rgb()
> 	provides the ability to return the alpha component of
> 	colours (as well as the red, green, and blue components).
> 
>     o	save() now checks that a binary connection is used.
> 
>     o	seek() on connections now accepts and returns a double for the
> 	file position.	This allows >2Gb files to be handled on a
> 	64-bit platform (and some 32-bit platforms).
> 
>     o	source() with 'echo = TRUE' uses the function source attribute
> 	when displaying commands as they are parsed.
> 
>     o	setClass() and its utilities now warn if either superclasses
> 	or classes for slots are undefined.  (Use setOldClass to
> 	register S3 classes for use as slots)
> 
>     o	str(obj) now displays more reasonably the STRucture of S4 objects.
> 
> 	It is also improved for language objects and lists with promise
> 	components.
> 
> 	The method for class "dendrogram" has a new argument 'stem' and
> 	indicates when it's not printing all levels (as typically when
> 	e.g., 'max.level = 2').
> 
> 	Specifying 'max.level = 0' now allows to suppress all but the top
> 	level for hierarchical objects such as lists. This is different
> 	to previous behavior which was the default behavior of giving all
> 	levels is unchanged.  The default behavior is unchanged but now
> 	specified by 'max.level = NA'.
> 
>     o	system.time() has a new argument 'gcFirst' which, when TRUE,
> 	forces a garbage collection before timing begins.
> 
>     o	tail() of a matrix now displays the original row numbers.
> 
>     o	The default method for text() now coerces a factor to character
> 	and not to its internal codes.	This is incompatible with S
> 	but seems what users would expect.
> 
> 	It now also recycles (x,y) to the length of 'labels' if that
> 	is longer.  This is now compatible with grid.text() and
> 	S. (See also PR#7084.)
> 
>     o	TukeyHSD() now labels comparisons when applied to an
> 	interaction in an aov() fit.  It detects non-factor terms in
> 	'which' and drops them if sensible to do so.
> 
>     o	There is now a replacement method for window(), to allow a
> 	range of values of time series to be replaced by specifying the
> 	start and end times (and optionally a frequency).
> 
>     o	If writeLines() is given a connection that is not open, it now
> 	attempts to open it in mode = "wt" rather than the default
> 	mode specified when creating the connection.
> 
>     o	The screen devices x11(), windows() and quartz() have a new
> 	argument 'bg' to set the default background colour.
> 
> 
>     o	Subassignments involving NAs and with a replacement value of
> 	length > 1 are now disallowed.	(They were handled
> 	inconsistently in R < 2.0.0, see PR#7210.)  For data frames
> 	they are disallowed altogether, even for logical matrix indices
> 	(the only case which used to work).
> 
>     o	The way the comparison operators handle a list argument has
> 	been rationalized so a few more cases will now work -- see
> 	?Comparison.
> 
>     o	Indexing a vector by a character vector was slow if both the
> 	vector and index were long (say 10,000).  Now hashing is used
> 	and the time should be linear in the longer of the lengths
> 	(but more memory is used).
> 
>     o	Printing a character string with embedded nuls now prints the
> 	whole string, and non-printable characters are represented by
> 	octal escape sequences.
> 
>     o	Objects created from a formally defined class now include the
> 	name of the corresponding package as an attribute in the
> 	object's class.	 This allows packages with namespaces to have
> 	private (non-exported) classes.
> 
>     o	Changes to package 'grid':
> 
> 	- Calculation of number of circles to draw in circleGrob now
> 	  looks at length of y and r as well as length of x.
> 
> 	- Calculation of number of rectangles to draw in rectGrob now
> 	  looks at length of y, w, and h as well as length of x.
> 
> 	- All primitives (rectangles, lines, text, ...) now handle
> 	  non-finite values (NA, Inf, -Inf, NaN) for locations and
> 	  sizes.
> 
> 	  Non-finite values for locations, sizes, and scales of
> 	  viewports result in error messages.
> 
> 	  There is a new vignette ("nonfinite") which describes this
> 	  new behaviour.
> 
> 	- Fixed (unreported) bug in drawing circles.  Now checks that
> 	  radius is non-negative.
> 
> 	- downViewport() now reports the depth it went down to find a
> 	  viewport.  Handy for "going back" to where you started, e.g., ...
> 
> 	    depth <- downViewport("vpname")
> 	    <draw stuff>
> 	    upViewport(depth)
> 
> 	- The "alpha" gpar() is now combined with the alpha channel of
> 	  colours when creating a gcontext as follows: (internal C code)
> 
> 	    finalAlpha = gpar("alpha")*(R_ALPHA(col)/255)
> 
> 	  This means that gpar(alpha=) settings now affect internal
> 	  colours so grid alpha transparency settings now are sent to
> 	  graphics devices.
> 
> 	  The alpha setting is also cumulative.	 For example, ...
> 
> 	    grid.rect(width=0.5, height=0.5,
> 		      gp=gpar(fill="blue"))		 # alpha = 1
> 	    pushViewport(viewport(gp=gpar(alpha=0.5)))
> 	    grid.rect(height=0.25, gp=gpar(fill="red"))	 # alpha = 0.5
> 	    pushViewport(viewport(gp=gpar(alpha=0.5)))
> 	    grid.rect(width=0.25, gp=gpar(fill="red"))	 # alpha = 0.25 !
> 
> 	- Editing a gp slot in a grob is now incremental.  For example ...
> 
> 	    grid.lines(name="line")
> 	    grid.edit("line", gp=gpar(col="red")) # line turns red
> 	    grid.edit("line", gp=gpar(lwd=3)) # line becomes thick
> 					      # AND STAYS red
> 
> 	- The "cex" gpar is now cumulative.  For example ...
> 
> 	    grid.rect(height=unit(4, "char")) # cex = 1
> 	    pushViewport(viewport(gp=gpar(cex=0.5)))
> 	    grid.rect(height=unit(4, "char")) # cex = 0.5
> 	    pushViewport(viewport(gp=gpar(cex=0.5)))
> 	    grid.rect(height=unit(4, "char")) # cex = 0.125 !!!
> 
> 	- New childNames() function to list the names of children
> 	  of a gTree.
> 
> 	- The "grep" and "global" arguments have been implemented for
> 	  grid.[add|edit|get|remove]Grob() functions.
> 
> 	  The "grep" argument has also been implemented for the
> 	  grid.set() and setGrob().
> 
> 	- New function grid.grab() which creates a gTree from the
> 	  current display list (i.e., the current page of output can
> 	  be converted into a single gTree object with all grobs
> 	  on the current page as children of the gTree and all the
> 	  viewports used in drawing the current page in the childrenvp
> 	  slot of the gTree).
> 
> 	- New "lineend", "linejoin", and "linemitre" gpar()s:
> 
> 	  line end can be "round", "butt", or "square".
> 	  line join can be "round", "mitre", or "bevel".
> 	  line mitre can be any number larger than 1
> 	    (controls when a mitre join gets turned into a bevel join;
> 	     proportional to angle between lines at join;
> 	     very big number means that conversion only happens for lines
> 	     that are almost parallel at join).
> 
> 	- New grid.prompt() function for controlling whether the user is
> 	  prompted before starting a new page of output.
> 
> 	  Grid no longer responds to the par(ask) setting in the "graphics"
> 	  package.
> 
>     o	The tcltk package has had the tkcmd() function renamed as
> 	tcl() since it could be used to invoke commands that had
> 	nothing to do with Tk. The old name is retained, but will be
> 	deprecated in a future release. Similarly, we now have
> 	tclopen(), tclclose(), tclread(), tclputs(), tclfile.tail(),
> 	and tclfile.dir() replacing counterparts starting with "tk",
> 	with old names retained for now.
> 
> 
> UTILITIES
> 
>     o	R CMD check now checks for file names in a directory that
> 	differ only by case.
> 
>     o	R CMD check now checks Rd files using R code from package tools,
> 	and gives refined diagnostics about "likely" Rd problems (stray
> 	top-level text which is silently discarded by Rdconv).
> 
>     o	R CMD INSTALL now fails for packages with incomplete/invalid
> 	DESCRIPTION metadata, using new code from package tools which is
> 	also used by R CMD check.
> 
>     o	list_files_with_exts (package tools) now handles zipped directories.
> 
>     o	Package 'tools' now provides Rd_parse(), a simple top-level
> 	parser/analyzer for R documentation format.
> 
>     o	tools::codoc() (and hence R CMD check) now checks any documentation
> 	for registered S3 methods and unexported objects in packages
> 	with namespaces.
> 
>     o	Package 'utils' contains several new functions:
> 
> 	- Generics toBibtex() and toLatex() for converting
> 	  R objects to BibTeX and LaTeX (but almost no methods yet).
> 
> 	- A much improved citation() function which also has a package
> 	  argument.  By default the citation is auto-generated from
> 	  the package DESCRIPTION, the file 'inst/CITATION' can be
> 	  used to override this, see help(citation) and
> 	  help(citEntry).
> 
> 	- sessionInfo() can be used to include version information about
> 	  R and R packages in text or LaTeX documents.
> 
> 
> DOCUMENTATION
> 
>     o	The DVI and PDF manuals are now all made on the paper specified
> 	by R_PAPERSIZE (default 'a4'), even the .texi manuals which
> 	were made on US letter paper in previous versions.
> 
>     o	The reference manual now omits 'internal' help pages.
> 
>     o	There is a new help page shown by help("Memory-limits") which
> 	documents the current design limitations on large objects.
> 
>     o	The format of the LaTeX version of the documentation has
> 	changed.  The old format is still accepted, but only the new
> 	resolves cross-references to object names containing _, for
> 	example.
> 
>     o	HTML help pages now contain a reference to the package and
> 	version in the footer, and HTML package index pages give their
> 	name and version at the top.
> 
>     o	All manuals in the 2.x series have new ISBN numbers.
> 
>     o	The 'R Data Import/Export' manual has been revised and has a
> 	new chapter on `Reading Excel spreadsheets'.
> 
> 
> C-LEVEL FACILITIES
> 
>     o	The PACKAGE argument for .C/.Call/.Fortran/.External can be
> 	omitted if the call is within code within a package with a
> 	namespace.  This ensures that the native routine being called
> 	is found in the DLL of the correct version of the package if
> 	multiple versions of a package are loaded in the R session.
> 	Using a namespace and omitting the PACKAGE argument is
> 	currently the only way to ensure that the correct version is
> 	used.
> 
>     o	The header Rmath.h contains a definition for R_VERSION_STRING
> 	which can be used to track different versions of R and libRmath.
> 
>     o	The Makefile in src/nmath/standalone now has 'install' and
> 	'uninstall' targets -- see the README file in that directory.
> 
>     o	More of the header files, including Rinternals.h, Rdefines.h and
> 	Rversion.h, are now suitable for calling directly from C++.
> 
>     o   Configure looks to a suitable option for inlining C code which
> 	made available as macro R_INLINE: see `Writing R Extensions'
> 	for further details.
> 
> 
> DEPRECATED & DEFUNCT
> 
>     o	Direct use of R INSTALL|REMOVE|BATCH|COMPILE|SHLIB has been
> 	removed: use R CMD instead.
> 
>     o	La.eigen(), tetragamma(), pentagamma(), package.contents() and
> 	package.description() are defunct.
> 
>     o	The undocumented function newestVersion() is no longer exported
> 	from package utils.  (Mainly because it was not completely general.)
> 
>     o	C-level entry point ptr_R_GetX11Image has been removed, as it
> 	was replaced by R_GetX11Image at 1.7.0.
> 
>     o	The undocumented C-level entry point R_IsNaNorNA has been
> 	removed.  It was used in a couple of packages, and should be
> 	replaced by a call to the documented macro ISNAN.
> 
>     o	The gnome/GNOME graphics device is now defunct.
> 
> 
> INSTALLATION CHANGES
> 
>     o	Arithmetic supporting +/-Inf, NaNs and the IEC 60559 (aka
> 	IEEE 754) standard is now required -- the partial and often
> 	untested support for more limited arithmetic has been removed.
> 
> 	The C99 macro isfinite is used in preference to finite if available
> 	(and its correct functioning is checked at configure time).
> 
> 	Where isfinite or finite is available and works, it is used as
> 	the substitution value for R_FINITE.  On some platforms this
> 	leads to a performance gain.  (This applies to compiled code
> 	in packages only for isfinite.)
> 
>     o	The dynamic libraries libR and libRlapack are now installed in
> 	R_HOME/lib rather than R_HOME/bin.
> 
>     o	When --enable-R-shlib is specified, the R executable is now a
> 	small executable linked against libR: see the R-admin manual
> 	for further discussion.	 The 'extra' libraries bzip2, pcre,
> 	xdr and zlib are now compiled in a way that allows the code to
> 	be included in a shared library only if this option is
> 	specified, which might improve performance when it is not.
> 
>     o	The main R executable is now R_HOME/exec/R not R_HOME/R.bin, to
> 	ease issues on MacOS X.	 (The location is needed when debugging
> 	core dumps, on other platforms.)
> 
>     o	Configure now tests for 'inline' and alternatives, and the
> 	src/extra/bzip2 code now (potentially) uses inlining where
> 	available and not just under gcc.
> 
>     o	The XPG4 sed is used on Solaris for forming dependencies,
> 	which should now be done correctly.
> 
>     o	Makeinfo 4.5 or later is now required for building the HTML and
> 	Info versions of the manuals.  However, binary distributions
> 	need to be made with 4.7 or later to ensure some of the
> 	links are correct.
> 
>     o	f2c is not allowed on 64-bit platforms, as it uses longs for
> 	Fortran integers.
> 
>     o	There are new options on how to make the PDF version of the
> 	reference manual -- see the 'R Administration and Installation
> 	Manual' section 2.2.
> 
>     o	The concatenated Rd files in the installed 'man' directory are
> 	now compressed and the R CMD check routines can read the
> 	compressed files.
> 
>     o   There is a new configure option --enable-linux-lfs that will
> 	build R with support for > 2Gb files on suitably recent 32-bit
> 	Linux systems.
> 
> 
> PACKAGE INSTALLATION CHANGES
> 
>     o	The DESCRIPTION file of packages may contain a 'Imports:'
> 	field for packages whose namespaces are used but do not need
> 	to be attached.	 Such packages should no longer be listed in
> 	'Depends:'.
> 
>     o	There are new optional fields 'SaveImage', 'LazyLoad' and
> 	'LazyData' in the DESCRIPTION file.  Using 'SaveImage' is
> 	preferred to using an empty file 'install.R'.
> 
>     o	A package can contain a file 'R/sysdata.rda' to contain
> 	system datasets to be lazy-loaded into the namespace/package
> 	environment.
> 
>     o	The packages listed in 'Depends' are now loaded before a package
> 	is loaded (or its image is saved or it is prepared for lazy
> 	loading).  This means that almost all uses of R_PROFILE.R and
> 	install.R are now unnecessary.
> 
>     o	If installation of any package in a bundle fails, R CMD
> 	INSTALL will back out the installation of all of the bundle,
> 	not just the failed package (on both Unix and Windows).
> 
> 
> BUG FIXES
> 
>     o	Complex superassignments were wrong when a variable with the same
> 	name existed locally, and were not documented in R-lang.
> 
>     o	rbind.data.frame() dropped names/rownames from columns in all
> 	but the first data frame.
> 
>     o	The dimnames<- method for data.frames was not checking the
> 	validity of the row names.
> 
>     o	Various memory leaks reported by valgrind have been plugged.
> 
>     o	gzcon() connections would sometimes read the crc bytes from
> 	the wrong place, possibly uninitialized memory.
> 
>     o	Rd.sty contained a length \middle that was not needed after a
> 	revision in July 2000.	It caused problems with LaTeX systems
> 	based on e-TeX which are starting to appear.
> 
>     o	save() to a connection did not check that the connection was
> 	open for writing, nor that non-ascii saves cannot be made to a
> 	text-mode connection.
> 
>     o	phyper() uses a new algorithm based on Morten Welinder's bug
> 	report (PR#6772).  This leads to faster code for large arguments
> 	and more precise code, e.g. for phyper(59, 150,150, 60,	lower=FALSE).
> 	This also fixes bug (PR#7064) about fisher.test().
> 
>     o	print.default(*, gap = <n>) now in principle accepts all
> 	non-negative values <n>.
> 
>     o	smooth.spline(...)$pen.crit had a typo in its computation;
> 	note this was printed in print.smooth.spline(*) but not used in
> 	other "smooth.spline" methods.
> 
>     o	write.table() handles zero-row and zero-column inputs correctly.
> 
>     o	debug() works on trivial functions instead of crashing. (PR#6804)
> 
>     o	eval() could alter a data.frame/list second argument, so
> 	with(trees, Girth[1] <- NA) altered 'trees' (and any copy of
> 	'trees' too).
> 
>     o	cor() could corrupt memory when the standard deviation was
> 	zero. (PR#7037)
> 
>     o	inverse.gaussian() always printed 1/mu^2 as the link function.
> 
>     o	constrOptim() now passes ... arguments through optim to the
> 	objective function.
> 
>     o	object.size() now has a better estimate for character vectors:
> 	it was in general too low (but only significantly so for
> 	very short character strings) but over-estimated NA and
> 	duplicated elements.
> 
>     o	quantile() now interpolates correctly between finite and
> 	infinite values (giving +/-Inf rather than NaN).
> 
>     o	library() now gives more informative error messages mentioning
> 	the package being loaded.
> 
>     o	Building the reference manual no longer uses roman upright
> 	quotes in typewriter output.
> 
>     o	model.frame() no longer builds invalid data frames if the
> 	data contains time series and rows are omitted by na.action.
> 
>     o	write.table() did not escape quotes in column names.  (PR#7171)
> 
>     o	Range checks missing in recursive assignments using [[ ]].  (PR#7196)
> 
>     o	packageStatus() reported partially-installed bundles as
> 	installed.
> 
>     o	apply() failed on an array of dimension >=3 when for each
> 	iteration the function returns a named vector of length >=2.
> 	(PR#7205)
> 
>     o	The GNOME interface was in some circumstances failing if run
> 	from a menu -- it needed to always specify that R be interactive.
> 
>     o	depMtrxToStrings (part of pkgDepends) applied nrow() to a
> 	non-matrix and aborted on the result.
> 
>     o	Fix some issues with nonsyntactical names in modelling code
> 	(PR#7202), relating to backquoting.  There are likely more.
> 
>     o	Support for S4 classes that extend basic classes has been fixed
> 	in several ways.  as() methods and x at .Data should work better.
> 
>     o	hist() and pretty() accept (and ignore) infinite values.  (PR#7220)
> 
>     o	It is no longer possible to call gzcon() more than once on a
> 	connection.
> 
>     o	t.test() now detects nearly-constant input data.  (PR#7225)
> 
>     o	mle() had problems if ndeps or parscale was supplied in the
> 	control arguments for optim().  Also, the profiler is now more
> 	careful to reevaluate modified mle() calls in its parent
> 	environment.
> 
>     o	Fix to rendering of accented superscripts and subscripts e.g.,
> 	expression((b[dot(a)])).  (Patch from Uwe Ligges.)
> 
>     o	attach(*, pos=1) now gives a warning (and will give an error).
> 
>     o	power.*test() now gives an error when 'sig.level' is outside [0,1].
> 	(PR#7245)
> 
>     o	Fitting a binomial glm with a matrix response lost the names of
> 	the response, which should have been transferred to the
> 	residuals and fitted values.
> 
>     o   print.ts() could get the year wrong because rounding issue
>         (PR#7255)
-- 
Unung Istopo Hartanto
------------------------------------------------
ENCIETY Business Consult
Jl. Manyar Tirtoyoso Utara V/7
Telp. +62-31-5992340, Fax. +62-31-5994230
www.enciety.com/data/isp_gallery.php



From sorn.norng at dpi.vic.gov.au  Tue Oct  5 03:11:01 2004
From: sorn.norng at dpi.vic.gov.au (sorn.norng@dpi.vic.gov.au)
Date: Tue, 5 Oct 2004 11:11:01 +1000
Subject: [R] R for education research tutoral?
Message-ID: <OFDF08C34C.E1DE7DBA-ONCA256F24.000639E2-CA256F24.00066D7C@nre.vic.gov.au>


I am in the process of learning R myself (when I have the time). I came
across these references that I think will be useful for your colleagues.
'Introductory Statistics with R' by Peter Dalgaard and 'Using R for Data
Analysis and Graphics' by Ian Maindonald.


                                                                                                                                           
                      ok at cs.otago.ac.nz                                                                                                    
                      Sent by:                     To:       R-help at stat.math.ethz.ch                                                      
                      r-help-bounces at stat.m        cc:                                                                                     
                      ath.ethz.ch                  Subject:  [R] R for education research tutoral?                                         
                                                                                                                                           
                                                                                                                                           
                      05/10/2004 10:56 AM                                                                                                  
                                                                                                                                           
                                                                                                                                           




I have a couple of colleagues doing research in Computer Science
Education.  I've talked one of them into using R (he is also interested
in data mining) and am about half-way to persuading the other (who agrees
that he ought to know more about analysing his data).  Neither of them
has a strong statistic background, but both are fairly able and willing
to learn new tools and techniques.

If anyone has a tutorial or set of course notes on how to use R to
analyse the results of education studies, or knows where I might find
such a thing, could you please tell me?

I have downloaded and read all the English-language tutorials on the
"Contributed Documentation" page; Baron & Li is probably closest to
what I'm after but not quite there.

This is *not* a request for general "Introduction to Statistics for
Education Research" tutorials, although I wouldn't turn down any such
references that people sent my way.  I specifically want something to
give my colleagues that explains how to use *R* to analyse their kinds
of data.

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide!
http://www.R-project.org/posting-guide.html



From nini at npgcable.com  Tue Oct  5 04:57:53 2004
From: nini at npgcable.com (Nancy Hornewer)
Date: Mon, 4 Oct 2004 19:57:53 -0700
Subject: [R] installation help for mac os x
Message-ID: <59F15714-167A-11D9-B6E0-000A95C43AAC@npgcable.com>

Hi,

I'm  trying to install R for Mac OS X. I printed out the Mac OS X FAQ 
(version 1.9-1 2004-03-22) and have been following those instructions 
step-by-step. I'm a _complete_ novice when it comes to non-mac type 
software installations and am not very unix literate, but I'm hoping I 
can get this to work. I'm running a dual 1.8 GHz mac with the latest 
version of panther.

Here's my progress to date:
(1) Installed the  C/C++ compiler from the Apple Developer Connection 
(v. 1.5) - no problems
(2) Installed the Fortran compiler g77 for gcc 3.3 (v. 3.4) - no 
problems
(3) Installed the libreadline  library (v. 4.3) [note faq said 
optional, but I saw that it was recommended in one of the r-help 
archives] - no problems
(4) Didn't install X11 SDK package because it was optional and I don't 
think I need that (just want to run this on my machine)
(5) Installed tcl8.4.7 - no problems
(6) Tried installing tk8.4.7 but received many many error messages when 
I typed "make". Here is the last portion of those messages:

/tk8.4.7/generic/tk3d.c: In function `Tk_Get3DBorderFromObj':
/tk8.4.7/generic/tk3d.c:1262: error: `borderPtr' undeclared (first use 
in this function)
/tk8.4.7/generic/tk3d.c:1264: error: invalid operands to binary *
/tk8.4.7/generic/tk3d.c:1264: error: parse error before ')' token
/tk8.4.7/generic/tk3d.c:1277: error: parse error before ')' token
/tk8.4.7/generic/tk3d.c:1280: error: parse error before ')' token
/tk8.4.7/generic/tk3d.c:1280: error: parse error before ')' token
/tk8.4.7/generic/tk3d.c:1281: error: parse error before ')' token
/tk8.4.7/generic/tk3d.c:1301: error: request for member `borderTable' 
in something not a structure or union
/tk8.4.7/generic/tk3d.c:1301: error: request for member `borderTable' 
in something not a structure or union
/tk8.4.7/generic/tk3d.c:1305: error: parse error before ')' token
/tk8.4.7/generic/tk3d.c:1307: error: parse error before ')' token
/tk8.4.7/generic/tk3d.c:1307: error: parse error before ')' token
/tk8.4.7/generic/tk3d.c:1308: error: parse error before ')' token
/tk8.4.7/generic/tk3d.c: In function `TkDebugBorder':
/tk8.4.7/generic/tk3d.c:1388: error: `borderPtr' undeclared (first use 
in this function)
/tk8.4.7/generic/tk3d.c:1391: error: invalid operands to binary *
/tk8.4.7/generic/tk3d.c:1391: error: parse error before ')' token
/tk8.4.7/generic/tk3d.c:1394: error: request for member `borderTable' 
in something not a structure or union
/tk8.4.7/generic/tk3d.c:1394: error: request for member `borderTable' 
in something not a structure or union
/tk8.4.7/generic/tk3d.c:1396: error: parse error before ')' token
/usr/include/ctype.h: At top level:
/tk8.4.7/generic/tk3d.c:1134: error: storage size of `shiftTable' isn't 
known
/tk8.4.7/generic/tk3d.c:31: warning: `BorderInit' declared `static' but 
never defined
/tk8.4.7/generic/tk3d.c:36: warning: `Intersect' declared `static' but 
never defined
/tk8.4.7/generic/tk3d.c:39: warning: `ShiftLine' declared `static' but 
never defined
{standard input}:1940:Ignoring attempt to re-define symbol.
{standard input}:1943:Ignoring attempt to re-define symbol.
{standard input}:1979:Ignoring attempt to re-define symbol.
{standard input}:1982:Ignoring attempt to re-define symbol.
{standard input}:1985:Ignoring attempt to re-define symbol.
{standard input}:1988:Ignoring attempt to re-define symbol.
make: *** [tk3d.o] Error 1
cm-24-121-11-63:/tk8.4.7/unix nancy$

I'm not sure why this didn't work and tried the "make" command twice (I 
hope I didn't mess anything up by running that command twice). I can 
email the entire list of errors if necessary, but it is really long.

After the above failed, I did install the next item on the faq list:  
TeX and that seemed to work fine.

I haven't yet tried installing the R sources yet because I was afraid 
that it might have to be installed after tk.

Any assistance that you can provide would be greatly appreciated.

Thank you,
Nancy



From ajayshah at mayin.org  Sun Oct  3 13:35:42 2004
From: ajayshah at mayin.org (Ajay Shah)
Date: Sun, 3 Oct 2004 17:05:42 +0530
Subject: [R] How might one write this better?
Message-ID: <20041003113542.GJ23573@igidr.ac.in>

I am trying to simulate the trajectory of the pension assets of one
person. In C-like syntax, it looks like this:

daily.wage.growth = 1.001                 # deterministic
contribution.rate = 0.08                  # deterministic 8%
Wage = 10                                 # initial
Asset = 0                                 # initial
for (10,000 days) {
    Asset += contribution.rate * Wage           # accreting contributions
    Wage *= daily.wage.growth * Wage            # wage growth
    Asset *= draw from a normal distribution    # Asset returns
}
cat("Terminal asset = ", Asset, "\n")

How can one do this well in R? What I tried so far is to notice that
the wage trajectory is deterministic, it does not change from one run
to the next, and it can be done in one line. The asset returns
trajectory can be obtained using a single call to rnorm(). Both these
can be done nicely using R functions (if you're curious, I can give
you my code). Using these, I efficiently get a vector of contributions
c[] and a vector of returns r[]. But that still leaves the loop:

  Asset <- 0
  for (t in 1:T) {
    Asset <- c[t] + r[t]*Asset
  }

How might one do this better?

I find that using this code, it takes roughly 0.3 seconds per
computation of Asset (on my dinky 500 MHz Celeron). I need to do
50,000 of these every now and then, and it's a pain to have to wait 3
hours. It'll be great if there is some neat R way to rewrite the
little loop above.

-- 
Ajay Shah                                                   Consultant
ajayshah at mayin.org                      Department of Economic Affairs
http://www.mayin.org/ajayshah           Ministry of Finance, New Delhi



From ajayshah at mayin.org  Sun Oct  3 11:10:04 2004
From: ajayshah at mayin.org (Ajay Shah)
Date: Sun, 3 Oct 2004 14:40:04 +0530
Subject: [R] Making a 'joint distribution'?
Message-ID: <20041003091004.GH23573@igidr.ac.in>

Suppose I make two discrete variables --
> D <- data.frame(f1=sample(1:5,100,replace=T), f2=sample(1:5,100,replace=T))

I know I can do:

> table(D$f1, D$f2)
    0 1 2 3 4
  0 5 5 5 5 4
  1 4 2 6 7 3
  2 5 3 5 3 6
  3 3 1 3 1 2
  4 6 4 3 3 6
> table(D$f1)
 0  1  2  3  4 
24 22 22 10 22 
> table(D$f2)
 0  1  2  3  4 
23 15 22 19 21 

which is all great. But how do I produce the typical presentation of
the "joint distribution" where we put the marginal distributions in
the margins? E.g. I'd like to get some object "joint" where one would
get :

> joint
     0  1  2  3  4  f1
  0  5  5  5  5  4  24
  1  4  2  6  7  3  22
  2  5  3  5  3  6  22
  3  3  1  3  1  2  10
  4  6  4  3  3  6  22
 f2 23 15 22 19 21 100

So that one could then say "joint/nrow(D)" and get nice probabilities
out of it. It would great to be able to say "xtable(joint/nrow(D))" :-)

I'm sure R has a lovely way to do this, but I'm not able to figure it out.

-- 
Ajay Shah                                                   Consultant
ajayshah at mayin.org                      Department of Economic Affairs
http://www.mayin.org/ajayshah           Ministry of Finance, New Delhi



From ajayshah at mayin.org  Sun Oct  3 20:41:46 2004
From: ajayshah at mayin.org (Ajay Shah)
Date: Mon, 4 Oct 2004 00:11:46 +0530
Subject: [R] Computing and testing transition probabilities?
Message-ID: <20041003184146.GA28159@igidr.ac.in>

Folks, I have a situation with many firms, observed for many years
(but it's not panel data, so every now and then data for a firm just
goes missing).

Let me show you an example. There are 3 firms "a", "b" and "c". There
are 3 years: 1981, 1982 and 1983. There's a factor f which takes
values 1, 2 or 3.

set.seed(5)
D = data.frame(
  names=c("a", "a", "a", "b", "b", "c", "c", "c", "d", "d"),
  year= c( 81,  82,  83,  81,  83,  81,  82,  83,  82,  83),
  f=    sample(1:3, 10, replace=T)
  )
print(D)

What I'd like to do is locate situations where a firm is observed for
two consecutive years, and put together conditional probabilities of
state transition.

Expressed as counts, putting time $t$ as the rows and time $t+1$ as
the columms, I'd like to get the table:

    1   2   3
1           1
2       1
3   1   1   1

For example, this says that we only observe one situation (in this
data) where f=1 and then we got to see the next year, and in that year
f was 3. So we have a 100% probability of going from 1 -> 3. In the
case of f=3, we have 3 situations where we observe two consecutive
years where the 1st is f=3, and it turns out that the outcomes are
1,2,3 each happening once.

Expressed as conditional probabilities it is:

    1   2   3
1           1
2       1
3 .33  .33 .33

How might one do this? And then, how might one go about getting
confidence intervals for the transition probabilities that are seen in
this transition matrix? I'd like to test the null that the state
variable does not change. I.e., if a firm is in f=2, under the null,
it's just stay at f=2 in the following year. That'd be an identity
matrix for the transition probabilities:

    1   2   3
1   1
2       1
3           1

How does one talk about the distribution of the transition
probabilities under this null, and thus get a test?

-- 
Ajay Shah                                                   Consultant
ajayshah at mayin.org                      Department of Economic Affairs
http://www.mayin.org/ajayshah           Ministry of Finance, New Delhi



From ripley at stats.ox.ac.uk  Tue Oct  5 08:45:48 2004
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Tue, 5 Oct 2004 07:45:48 +0100 (BST)
Subject: [R] Using model operator in stepwise function's upper scope
	formula
In-Reply-To: <200410042339.i94NdXj25088@csserv.wadsworth.org>
Message-ID: <Pine.LNX.4.44.0410050743580.18807-100000@gannet.stats>

`.' means `what is already there'.  In your case, .^2 is 1^2 which is 1, 
so it behaves correctly.

Only in a *fit* involving a *data* argument does `.' mean `the rest of the 
variables'.

On Mon, 4 Oct 2004, Yu Shao wrote:

> Hello:
> 
> I am doing forward stepwise analysis on the glm model. I am trying to use model 
> operator in the "upper" scope formula, for example,          
> 
>                        scope=list(lower=~1,upper=~ .^2)
> 
> but the upper bound of the scope seems to be ignored and add1 is not performed 
> at all, while if the terms are explicitly listed in the formula, the step 
> function seems to work properly. Example:
> 
> > version 
> platform sparc-sun-solaris2.9
> arch     sparc               
> os       solaris2.9          
> system   sparc, solaris2.9   
> status                       
> major    1                   
> minor    9.0                 
> year     2004                
> month    04                  
> day      12                  
> language R                   
> 
> > testdata
>       effect  fa1 fa2    fa3
> 1  0.5054526 -1.4   1 -15.42
> 2  0.1366526 -2.5   3 -14.72
> 3  0.4798526 -1.1   3 -16.43
> 4  0.5566526 -4.6   2 -17.62
> 5  0.6568526 -3.5   7  -5.41
> 6  0.6653526  0.0   5 -11.88
> 7  0.6376526  0.0   5 -11.72
> 8  0.5203526 -2.9  -1 -14.72
> 9  0.4905526 -2.1   2 -17.68
> 10 0.6376526 -1.1   1 -13.39
> 
> > step (glm(effect ~ 1 , data=testdata), scope=list(lower=~1,upper=~.), 
> direction='forward')
> Start:  AIC= -5.89 
>  effect ~ 1 
> 
> 
> Call:  glm(formula = effect ~ 1, data = testdata) 
> 
> Coefficients:
> (Intercept)  
>      0.5287  
> 
> Degrees of Freedom: 9 Total (i.e. Null);  9 Residual
> Null Deviance:      0.2178 
> Residual Deviance: 0.2178       AIC: -5.89 
> 
> ================================================================
> 
> But if I listed the term explicitly in the formula, then step function seemed to 
> work correctly:
> 
> > step (glm(effect ~ 1 , data=testdata), 
> scope=list(lower=~1,upper=~fa1+fa2+fa3), direction='forward')
> Start:  AIC= -5.89 
>  effect ~ 1 
> 
>        Df Deviance     AIC
> + fa3   1   0.1778 -5.9188
> <none>      0.2178 -5.8901
> + fa2   1   0.2023 -4.6271
> + fa1   1   0.2065 -4.4194
> 
> Step:  AIC= -5.92 
>  effect ~ fa3 
> 
>        Df Deviance     AIC
> <none>      0.1778 -5.9188
> + fa1   1   0.1698 -4.3783
> + fa2   1   0.1770 -3.9605
> 
> Call:  glm(formula = effect ~ fa3, data = testdata) 
> 
> Coefficients:
> (Intercept)          fa3  
>     0.78310      0.01830  
> 
> Degrees of Freedom: 9 Total (i.e. Null);  8 Residual
> Null Deviance:      0.2178 
> Residual Deviance: 0.1778       AIC: -5.919 
> ================================================================
> 
> I've lookup the online manual of step function but couldn't find a solution. 
> Thanks in advance,
> 
> Yu Shao
> Wadsworth Center, New York State Department of Health,
> Albany, NY 12208
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
> 
> 

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From V.Khamenia at biovision-discovery.de  Tue Oct  5 09:02:44 2004
From: V.Khamenia at biovision-discovery.de (Khamenia, Valery)
Date: Tue, 5 Oct 2004 09:02:44 +0200 
Subject: AW: AW: [R] constructing specially ordered factor
Message-ID: <D15343265276D31197BC00A024A6C110C793C7@EXS_BDC>

> Please follow the posting guide and do your homework before 
> posting, 

1. my last homework in university was done a lot of years ago.

2. I always try to follow posting guide.


>      An object of the same type of 'x'. but if an element is equal to
>      one with a smaller index, it is removed.
> 
> so the order is preserved, by definition.

Here stated when element is removed. There is no explicit statement,
that the order is preserved. If one writes its own implementation
with reodered output, it still matches the docs. Or?
 

> BTW it uses hashing for `acceleration', not something as slow 
> as sorting.

BTW, do you mean that current hash-based implementation brings *clearly* 
better performance than any O(n*log(n)) sort based algorithm? 
If I have correctly understood src/main/unique.c then current 
hash function is niether minimal perfect hash function nor even 
minimal hash function. In addition, as might be expected, 
current hash function uses full pass through the string to get 
a hash key. 

So, in particular, can anyone clearly show that the current 
hash-based algorithm will be quicker then sort-based 
algorithm if the input has:

1. a lot of strings;
2. strings are very long; 
3. strings are quite unsimilar

?

Hm, I don't believe you are ready to prove smth. like that. 

P.S. Sorry for broken email-reference. I am bound to Outlook 
Express now.

--
Valery



From ripley at stats.ox.ac.uk  Tue Oct  5 09:30:25 2004
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Tue, 5 Oct 2004 08:30:25 +0100 (BST)
Subject: [R] installation help for mac os x
In-Reply-To: <59F15714-167A-11D9-B6E0-000A95C43AAC@npgcable.com>
Message-ID: <Pine.LNX.4.44.0410050823200.19087-100000@gannet.stats>

There is a binary installation available at

	http://cran.r-project.org/bin/macosx/

so do you actually need to be building from the sources?  If you do, your
immediate problem is installing Tk on MacOS X, and that's not something
relevant to R-help.  However, Tk on Unix needs X11, whose installation you 
omitted.

On Mon, 4 Oct 2004, Nancy Hornewer wrote:

> Hi,
> 
> I'm  trying to install R for Mac OS X. I printed out the Mac OS X FAQ 
> (version 1.9-1 2004-03-22) and have been following those instructions 
> step-by-step. I'm a _complete_ novice when it comes to non-mac type 
> software installations and am not very unix literate, but I'm hoping I 
> can get this to work. I'm running a dual 1.8 GHz mac with the latest 
> version of panther.
> 
> Here's my progress to date:
> (1) Installed the  C/C++ compiler from the Apple Developer Connection 
> (v. 1.5) - no problems
> (2) Installed the Fortran compiler g77 for gcc 3.3 (v. 3.4) - no 
> problems
> (3) Installed the libreadline  library (v. 4.3) [note faq said 
> optional, but I saw that it was recommended in one of the r-help 
> archives] - no problems
> (4) Didn't install X11 SDK package because it was optional and I don't 
> think I need that (just want to run this on my machine)
> (5) Installed tcl8.4.7 - no problems
> (6) Tried installing tk8.4.7 but received many many error messages when 
> I typed "make". Here is the last portion of those messages:
> 
> /tk8.4.7/generic/tk3d.c: In function `Tk_Get3DBorderFromObj':
> /tk8.4.7/generic/tk3d.c:1262: error: `borderPtr' undeclared (first use 
> in this function)
> /tk8.4.7/generic/tk3d.c:1264: error: invalid operands to binary *
> /tk8.4.7/generic/tk3d.c:1264: error: parse error before ')' token
> /tk8.4.7/generic/tk3d.c:1277: error: parse error before ')' token
> /tk8.4.7/generic/tk3d.c:1280: error: parse error before ')' token
> /tk8.4.7/generic/tk3d.c:1280: error: parse error before ')' token
> /tk8.4.7/generic/tk3d.c:1281: error: parse error before ')' token
> /tk8.4.7/generic/tk3d.c:1301: error: request for member `borderTable' 
> in something not a structure or union
> /tk8.4.7/generic/tk3d.c:1301: error: request for member `borderTable' 
> in something not a structure or union
> /tk8.4.7/generic/tk3d.c:1305: error: parse error before ')' token
> /tk8.4.7/generic/tk3d.c:1307: error: parse error before ')' token
> /tk8.4.7/generic/tk3d.c:1307: error: parse error before ')' token
> /tk8.4.7/generic/tk3d.c:1308: error: parse error before ')' token
> /tk8.4.7/generic/tk3d.c: In function `TkDebugBorder':
> /tk8.4.7/generic/tk3d.c:1388: error: `borderPtr' undeclared (first use 
> in this function)
> /tk8.4.7/generic/tk3d.c:1391: error: invalid operands to binary *
> /tk8.4.7/generic/tk3d.c:1391: error: parse error before ')' token
> /tk8.4.7/generic/tk3d.c:1394: error: request for member `borderTable' 
> in something not a structure or union
> /tk8.4.7/generic/tk3d.c:1394: error: request for member `borderTable' 
> in something not a structure or union
> /tk8.4.7/generic/tk3d.c:1396: error: parse error before ')' token
> /usr/include/ctype.h: At top level:
> /tk8.4.7/generic/tk3d.c:1134: error: storage size of `shiftTable' isn't 
> known
> /tk8.4.7/generic/tk3d.c:31: warning: `BorderInit' declared `static' but 
> never defined
> /tk8.4.7/generic/tk3d.c:36: warning: `Intersect' declared `static' but 
> never defined
> /tk8.4.7/generic/tk3d.c:39: warning: `ShiftLine' declared `static' but 
> never defined
> {standard input}:1940:Ignoring attempt to re-define symbol.
> {standard input}:1943:Ignoring attempt to re-define symbol.
> {standard input}:1979:Ignoring attempt to re-define symbol.
> {standard input}:1982:Ignoring attempt to re-define symbol.
> {standard input}:1985:Ignoring attempt to re-define symbol.
> {standard input}:1988:Ignoring attempt to re-define symbol.
> make: *** [tk3d.o] Error 1
> cm-24-121-11-63:/tk8.4.7/unix nancy$
> 
> I'm not sure why this didn't work and tried the "make" command twice (I 
> hope I didn't mess anything up by running that command twice). I can 
> email the entire list of errors if necessary, but it is really long.
> 
> After the above failed, I did install the next item on the faq list:  
> TeX and that seemed to work fine.
> 
> I haven't yet tried installing the R sources yet because I was afraid 
> that it might have to be installed after tk.
> 
> Any assistance that you can provide would be greatly appreciated.
> 
> Thank you,
> Nancy
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
> 
> 

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From em_alissa at yahoo.com  Tue Oct  5 10:05:12 2004
From: em_alissa at yahoo.com (Eman Alissa)
Date: Tue, 5 Oct 2004 01:05:12 -0700 (PDT)
Subject: [R] conditional logistic regression
Message-ID: <20041005080512.86072.qmail@web11401.mail.yahoo.com>

Hi all

I've got a case-control study, matched for age 1:1. I
need to run a conditional logistic regression using
SPSS version (11.5). I was told that it can be
performed using cox regression model. However, I can
not figure where the independent variables (numeric
and categoric) should go? what do strata variable and
the model time mean?!
Can someone please clarify how to fill the options of
the dialogue box?
Thanks a million



From em_alissa at yahoo.com  Tue Oct  5 10:08:19 2004
From: em_alissa at yahoo.com (Eman Alissa)
Date: Tue, 5 Oct 2004 01:08:19 -0700 (PDT)
Subject: [R] conditional logistic regression
Message-ID: <20041005080819.16157.qmail@web11413.mail.yahoo.com>

Hi all

I've got a case-control study, matched for age 1:1. I
need to run a conditional logistic regression using
SPSS version (11.5). I was told that it can be
performed using cox regression model. However, I can
not figure where the independent variables (numeric
and categoric) should go? what do strata variable and
the model time mean?!
Can someone please clarify how to fill the options of
the dialogue box?
Thanks a million


		
_______________________________

Declare Yourself - Register online to vote today!



From ligges at statistik.uni-dortmund.de  Tue Oct  5 10:20:23 2004
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Tue, 05 Oct 2004 10:20:23 +0200
Subject: [R] BDS.TEST
In-Reply-To: <5.1.0.14.2.20041004203714.00b9bda0@pop.superig.com.br>
References: <5.1.0.14.2.20041004203714.00b9bda0@pop.superig.com.br>
Message-ID: <41625947.50801@statistik.uni-dortmund.de>

Savano wrote:

> UseRs,
> 
> I want to do a bds.test but the function return this :
> 
>  > bds.test(erro.exp,m=15);
> Error in as.vector(x, mode = "double") : (list) object cannot be coerced to 
> double
> 
> How can I fix this problem?

By specifying an object that is expected by bds.test() (in package 
tseries). Let me guess that the object erro.exp is neither a numeric 
vector nor an appropriate time series.

Uwe Ligges



> Thanks
> 
> savano
> 
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html



From ahenningsen at email.uni-kiel.de  Tue Oct  5 10:24:16 2004
From: ahenningsen at email.uni-kiel.de (Arne Henningsen)
Date: Tue, 5 Oct 2004 10:24:16 +0200
Subject: [R] A newbye question : Creating a grey palette
In-Reply-To: <6.1.2.0.0.20041004140220.018ebda0@hermes.nos.noaa.gov>
References: <Pine.LNX.4.44.0410041437470.10475-100000@gannet.stats>
	<001d01c4aa24$29f6c800$6501a8c0@NajiNassar>
	<6.1.2.0.0.20041004140220.018ebda0@hermes.nos.noaa.gov>
Message-ID: <200410051024.16859.ahenningsen@email.uni-kiel.de>

On Monday 04 October 2004 20:04, Mike Prager wrote:
> At 11:09 AM 10/04/2004, you wrote:
> >Hi all
> >
> >I'd like to create a grey palette for a filled.contour()
> >from blank (smallest values) to black (large values)
>
> I have used somethig like this with bar plots:
>
> colvec <- gray(sqrt(seq(from=0.05, to=1.0, length=ncolor)))
>
> That will save a color palette in colvec for use in your call.
>
> MHP

Another option is the package "RColorBrewer":
> library(RColorBrewer)
> colvec <- brewer.pal( 5, "Greys" )

BTW: If you start a new thread, please do not reply to an another email with a 
different subject/thread. This confuses email clients and web archives (see  
https://stat.ethz.ch/pipermail/r-help/2004-October/thread.html#57024)

Best wishes,
Arne

-- 
Arne Henningsen
Department of Agricultural Economics
University of Kiel
Olshausenstr. 40
D-24098 Kiel (Germany)
Tel: +49-431-880 4445
Fax: +49-431-880 1397
ahenningsen at agric-econ.uni-kiel.de
http://www.uni-kiel.de/agrarpol/ahenningsen/



From p.dalgaard at biostat.ku.dk  Tue Oct  5 10:40:51 2004
From: p.dalgaard at biostat.ku.dk (Peter Dalgaard)
Date: 05 Oct 2004 10:40:51 +0200
Subject: [R] conditional logistic regression
In-Reply-To: <20041005080819.16157.qmail@web11413.mail.yahoo.com>
References: <20041005080819.16157.qmail@web11413.mail.yahoo.com>
Message-ID: <x2d5zxr20c.fsf@biostat.ku.dk>

Eman Alissa <em_alissa at yahoo.com> writes:

> Hi all
> 
> I've got a case-control study, matched for age 1:1. I
> need to run a conditional logistic regression using
> SPSS version (11.5). I was told that it can be
> performed using cox regression model. However, I can
> not figure where the independent variables (numeric
> and categoric) should go? what do strata variable and
> the model time mean?!
> Can someone please clarify how to fill the options of
> the dialogue box?

We can guess, but perhaps your query would be better directed at an
SPSS mailing list? In R, use clogit() from the survival package.

> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
> 

-- 
   O__  ---- Peter Dalgaard             Blegdamsvej 3  
  c/ /'_ --- Dept. of Biostatistics     2200 Cph. N   
 (*) \(*) -- University of Copenhagen   Denmark      Ph: (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)             FAX: (+45) 35327907



From B.Rowlingson at lancaster.ac.uk  Tue Oct  5 11:09:15 2004
From: B.Rowlingson at lancaster.ac.uk (Barry Rowlingson)
Date: Tue, 05 Oct 2004 10:09:15 +0100
Subject: [R] R 2.0.0 is released
In-Reply-To: <loom.20041005T000134-803@post.gmane.org>
References: <x2oejipkm4.fsf@biostat.ku.dk>
	<loom.20041005T000134-803@post.gmane.org>
Message-ID: <416264BB.6040806@lancaster.ac.uk>

Gabor Grothendieck wrote:
>
> Congratulations to the R team and all involved for reaching the 2.0.0
> milestone.  The progress of R is truly astounding.
> 

  A milestone is something that tells you how far it is to where you are 
going. With R-2.0.0, have we arrived?


Barry



From info at rhkoning.com  Tue Oct  5 11:55:22 2004
From: info at rhkoning.com (Ruud H. Koning)
Date: Tue, 05 Oct 2004 11:55:22 +0200
Subject: [R] save print survfit object to data frame
Message-ID: <200410051155220525.00E18E8F@localhost>

Hello, I have estimated a survival model with six strata:

>model.b <-
survfit(Surv(time=start.tijd,time2=eind.tijd2,event=va)~strata(product.code)
,
 data=wu.wide)

I would like to save the output of 

>print(model.b,print.n="records",show.rmean=FALSE)

in a dataframe so that I can export it later. How do I do this? Note that
summary(model.b) gives an error:
Error in as.matrix(x) : (subscript) logical subscript too long

(R 1.9.1, windows xp, sp 2)

Thanks for any help, Ruud



From p.dalgaard at biostat.ku.dk  Tue Oct  5 12:43:53 2004
From: p.dalgaard at biostat.ku.dk (Peter Dalgaard)
Date: 05 Oct 2004 12:43:53 +0200
Subject: [R] R 2.0.0 is released
In-Reply-To: <416264BB.6040806@lancaster.ac.uk>
References: <x2oejipkm4.fsf@biostat.ku.dk>
	<loom.20041005T000134-803@post.gmane.org>
	<416264BB.6040806@lancaster.ac.uk>
Message-ID: <x2vfdp8mxi.fsf@biostat.ku.dk>

Barry Rowlingson <B.Rowlingson at lancaster.ac.uk> writes:

> Gabor Grothendieck wrote:
> >
> > Congratulations to the R team and all involved for reaching the 2.0.0
> > milestone.  The progress of R is truly astounding.
> >
> 
>   A milestone is something that tells you how far it is to where you
> are going. With R-2.0.0, have we arrived?

Depending on direction, milestones might only be telling you how far
you've gone. And you may even be walking in circles around the
target/origin...

No, I don't think we have arrived (would we want to?). We still have
unresolved issues in byte compiling and event loops, and the formal
methods techniques are only just there now. There's the whole Bayesian
front, too - important even for us hardcore frequentists. 

Besides, how do you arrive at a moving target?

-- 
   O__  ---- Peter Dalgaard             Blegdamsvej 3  
  c/ /'_ --- Dept. of Biostatistics     2200 Cph. N   
 (*) \(*) -- University of Copenhagen   Denmark      Ph: (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)             FAX: (+45) 35327907



From MichaelWeber at web.de  Tue Oct  5 12:54:09 2004
From: MichaelWeber at web.de (Michael Weber)
Date: Tue, 5 Oct 2004 12:54:09 +0200
Subject: [R] =?iso-8859-1?q?Installation_Package_=22gregmisc=22_nicht_m?=
	=?iso-8859-1?q?=F6glich=3F!=3F?=
Message-ID: <000801c4aac9$a4ed9ac0$14b2a8c0@nx01>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20041005/a8498f65/attachment.pl

From ligges at statistik.uni-dortmund.de  Tue Oct  5 13:02:07 2004
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Tue, 05 Oct 2004 13:02:07 +0200
Subject: =?ISO-8859-1?Q?Re=3A_=5BR=5D_Installation_Package_=22gre?=
	=?ISO-8859-1?Q?gmisc=22_nicht_m=F6glich=3F!=3F?=
In-Reply-To: <000801c4aac9$a4ed9ac0$14b2a8c0@nx01>
References: <000801c4aac9$a4ed9ac0$14b2a8c0@nx01>
Message-ID: <41627F2F.7060205@statistik.uni-dortmund.de>

Michael Weber wrote:

> Hallo zusammen.

Bonjour!

> Da ich R zur Erstellung meiner wissenschaftlichen Arbeit brauche versuchte ich dies heute zu installieren. Zun??chst in der Vrsion 1.9.1 (rw1091.exe). Ich ben??tige das Package "gregmisc" und installierte dies von CRAN. Bei der Installation tritt folgende Meldung auf:
> 
> 
> trying URL `http://cran.r-project.org/bin/windows/contrib/2.0/gregmisc_2.0.0.zip'
> Content type `application/zip' length 687941 bytes
> opened URL
> downloaded 671Kb
> 
> bundle 'gregmisc' successfully unpacked and MD5 sums checked
> 
> Delete downloaded files (y/N)? n
> The packages are in C:\WINDOWS\TEMP\Rtmp18502\Rinstdir14176
> updating HTML package descriptions
> Warning message: 
> DLL attempted to change FPU control word from 8001f to 9001f 

S'il vous plait, write in English. And everything worked perfectly.

gregmisc is a bundle these days, hence cannot call library(gregmisc) but 
you have to call
  library(oneOfThePackagesContainedInGregmisc)

Uwe Ligges


> Das Package ist anschlie??end nicht installiert...Hab das auch mit der neuen Version 2.0.0 (rw2000.exe) nicht hinbekommen, gleiche Fehlermeldung. Kann mir irgendjemand helfen?
> Mir w??rde auch eine ??ltere Version von R gen??gen (mit den Packages foreign, x-table und gregmisc). Falls jemand einen Link auf eine ??ltere funktionsf??hige Version hat oder die Dateien direkt per Mail schicken k??nnte w??re ich dankbar.
> 
> Mfg
> 
> Michael Weber
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html



From ripley at stats.ox.ac.uk  Tue Oct  5 13:07:35 2004
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Tue, 5 Oct 2004 12:07:35 +0100 (BST)
Subject: [R] =?iso-8859-1?q?Installation_Package_=22gregmisc=22_nicht_m?=
	=?iso-8859-1?q?=F6glich=3F!=3F?=
In-Reply-To: <000801c4aac9$a4ed9ac0$14b2a8c0@nx01>
Message-ID: <Pine.LNX.4.44.0410051202590.26480-100000@gannet.stats>

gregmisc is no longer a package.  Note carefully

> bundle 'gregmisc' successfully unpacked and MD5 sums checked
  ^^^^^^
As for the rest, please write in English, the language of the list.


On Tue, 5 Oct 2004, Michael Weber wrote:

> Hallo zusammen.
> 
> Da ich R zur Erstellung meiner wissenschaftlichen Arbeit brauche versuchte ich dies heute zu installieren. Zun??chst in der Vrsion 1.9.1 (rw1091.exe). Ich ben??tige das Package "gregmisc" und installierte dies von CRAN. Bei der Installation tritt folgende Meldung auf:
> 
> 
> trying URL `http://cran.r-project.org/bin/windows/contrib/2.0/gregmisc_2.0.0.zip'
> Content type `application/zip' length 687941 bytes
> opened URL
> downloaded 671Kb
> 
> bundle 'gregmisc' successfully unpacked and MD5 sums checked
> 
> Delete downloaded files (y/N)? n
> The packages are in C:\WINDOWS\TEMP\Rtmp18502\Rinstdir14176
> updating HTML package descriptions
> Warning message: 
> DLL attempted to change FPU control word from 8001f to 9001f 
> 
> Das Package ist anschlie??end nicht installiert...Hab das auch mit der neuen Version 2.0.0 (rw2000.exe) nicht hinbekommen, gleiche Fehlermeldung. Kann mir irgendjemand helfen?
> Mir w??rde auch eine ??ltere Version von R gen??gen (mit den Packages foreign, x-table und gregmisc). Falls jemand einen Link auf eine ??ltere funktionsf??hige Version hat oder die Dateien direkt per Mail schicken k??nnte w??re ich dankbar.
> 
> Mfg
> 
> Michael Weber
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
> 
> 

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From skoien at hydro.tuwien.ac.at  Tue Oct  5 14:03:15 2004
From: skoien at hydro.tuwien.ac.at (Jon Olav Skoien)
Date: Tue, 05 Oct 2004 14:03:15 +0200
Subject: [R] Bug in optim - way to solve problem?
Message-ID: <5.2.1.1.2.20041004132328.00a96bb0@traunsee.hydro.tuwien.ac.at>


Hi,

I want to automatically fit variograms to a large number of different 
sample data sets, and call the funtion "likfit" (in package geoR) from 
within a for-loop. "likfit" does again call "optim". After ssuccessfully 
fitting variograms to some of the data sets, the procedure crashes and I 
get the error message:

Error in optim(par = ini, fn = negloglik.GRF, method = "L-BFGS-B", lower = 
lower.optim,  :
         non-finite value supplied by optim

When I restart the procedure with the data set that first caused the error, 
it is usually possible to fit the variogram successfully for this data set, 
but the procedure will crash when fitting to another data set instead. I do 
therefore assume that the problem is not related to my data sets.

I think my problem is the same as one reported earlier, and it seems like 
there is a way around it:
Dr. Hans A. Kestler: [Rd] optim-Bug (PR#6720)
http://r-bugs.biostat.ku.dk/cgi-bin/R/Analyses-fixed?id=6720

Unfortunately, as I have just started using R, and am not very familiar 
with all of the syntax, I did not understand from these postings how to 
solve the problem. Will I need a fix from somewhere, or should I just do 
some changes in the source code on my computer? And what should I 
eventually change in the source code?

Thanks in advance,
Jon



From bitwrit at ozemail.com.au  Tue Oct  5 14:29:55 2004
From: bitwrit at ozemail.com.au (Jim Lemon)
Date: Tue, 5 Oct 2004 22:29:55 +1000
Subject: [R] R 2.0.0 is released
Message-ID: <20041005121815.RAXT25795.smta07.mail.ozemail.net@there>

Peter Dalgaard wrote:
> ...
> Besides, how do you arrive at a moving target?

Zeno sez:

Time of flight = 
(Location(target)-Location(archer))/(Velocity(arrow)-Velocity(target))

where Velocity(arrow)>Velocity(target)

conveniently avoiding all those difficult angling shots, windage and
the thousand insults to which slings and arrows are heir.

Jim



From andrewr at uidaho.edu  Tue Oct  5 14:31:28 2004
From: andrewr at uidaho.edu (Andrew Robinson)
Date: Tue, 5 Oct 2004 22:31:28 +1000
Subject: [R] R 2.0.0 is released
In-Reply-To: <20041005121815.RAXT25795.smta07.mail.ozemail.net@there>
References: <20041005121815.RAXT25795.smta07.mail.ozemail.net@there>
Message-ID: <20041005123128.GM76624@uidaho.edu>

Not to mention relativity.  R is developing pretty quickly.

Andrew

> conveniently avoiding all those difficult angling shots, windage and
> the thousand insults to which slings and arrows are heir.
> 
> Jim
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html

-- 
Andrew Robinson                      Ph: 208 885 7115
Department of Forest Resources       Fa: 208 885 6226
University of Idaho                  E : andrewr at uidaho.edu
PO Box 441133                        W : http://www.uidaho.edu/~andrewr
Moscow ID 83843                      Or: http://www.biometrics.uidaho.edu
No statement above necessarily represents my employer's opinion.



From p.dalgaard at biostat.ku.dk  Tue Oct  5 14:44:30 2004
From: p.dalgaard at biostat.ku.dk (Peter Dalgaard)
Date: 05 Oct 2004 14:44:30 +0200
Subject: [R] Bug in optim - way to solve problem?
In-Reply-To: <5.2.1.1.2.20041004132328.00a96bb0@traunsee.hydro.tuwien.ac.at>
References: <5.2.1.1.2.20041004132328.00a96bb0@traunsee.hydro.tuwien.ac.at>
Message-ID: <x2ekkd8hch.fsf@biostat.ku.dk>

Jon Olav Skoien <skoien at hydro.tuwien.ac.at> writes:

> Hi,
> 
> I want to automatically fit variograms to a large number of different
> sample data sets, and call the funtion "likfit" (in package geoR) from
> within a for-loop. "likfit" does again call "optim". After
> ssuccessfully fitting variograms to some of the data sets, the
> procedure crashes and I get the error message:
> 
> Error in optim(par = ini, fn = negloglik.GRF, method = "L-BFGS-B",
> lower = lower.optim,  :
>          non-finite value supplied by optim
> 
> When I restart the procedure with the data set that first caused the
> error, it is usually possible to fit the variogram successfully for
> this data set, but the procedure will crash when fitting to another
> data set instead. I do therefore assume that the problem is not
> related to my data sets.
> 
> I think my problem is the same as one reported earlier, and it seems
> like there is a way around it:
> Dr. Hans A. Kestler: [Rd] optim-Bug (PR#6720)
> http://r-bugs.biostat.ku.dk/cgi-bin/R/Analyses-fixed?id=6720
> 
> Unfortunately, as I have just started using R, and am not very
> familiar with all of the syntax, I did not understand from these
> postings how to solve the problem. Will I need a fix from somewhere,
> or should I just do some changes in the source code on my computer?
> And what should I eventually change in the source code?

If you find a bug report sitting in Whatever-fixed, it usually
means that the bug was fixed... The question is then whether your
version is older or newer than the fix (you're not telling us) since
we can't fix bugs retroactively.

(There is an interim between releases where fixes are available only
as source patches, but give that we had a major release yesterday,
that hardly applies.)

This sort of error is generally related to uninitialized data, and
(IIRC) that was indeed the issue in PR#6720. However, there's no
guarantee that this was the only bug, nor that there wouldn't be one
of the same sort in geoR, or indeed your own code.

Anyways, the first thing to try would be to upgrade R and geoR. If
that doesn't help, try putting together a reproducible example
(w/data), and let people here (or better: on the r-devel list) see if
they can spot the problem.

-- 
   O__  ---- Peter Dalgaard             Blegdamsvej 3  
  c/ /'_ --- Dept. of Biostatistics     2200 Cph. N   
 (*) \(*) -- University of Copenhagen   Denmark      Ph: (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)             FAX: (+45) 35327907



From MSchwartz at MedAnalytics.com  Tue Oct  5 15:15:39 2004
From: MSchwartz at MedAnalytics.com (Marc Schwartz)
Date: Tue, 05 Oct 2004 08:15:39 -0500
Subject: [R] R 2.0.0 is released
In-Reply-To: <x2vfdp8mxi.fsf@biostat.ku.dk>
References: <x2oejipkm4.fsf@biostat.ku.dk>
	<loom.20041005T000134-803@post.gmane.org>
	<416264BB.6040806@lancaster.ac.uk>  <x2vfdp8mxi.fsf@biostat.ku.dk>
Message-ID: <1096980420.12012.1.camel@localhost.localdomain>

On Tue, 2004-10-05 at 05:43, Peter Dalgaard wrote: 
> Barry Rowlingson <B.Rowlingson at lancaster.ac.uk> writes:
> 
> > Gabor Grothendieck wrote:
> > >
> > > Congratulations to the R team and all involved for reaching the 2.0.0
> > > milestone.  The progress of R is truly astounding.
> > >
> > 
> >   A milestone is something that tells you how far it is to where you
> > are going. With R-2.0.0, have we arrived?
> 
> Depending on direction, milestones might only be telling you how far
> you've gone. And you may even be walking in circles around the
> target/origin...
> 
> No, I don't think we have arrived (would we want to?). We still have
> unresolved issues in byte compiling and event loops, and the formal
> methods techniques are only just there now. There's the whole Bayesian
> front, too - important even for us hardcore frequentists. 
> 
> Besides, how do you arrive at a moving target?


Well, as some would say, it's not the destination, it's the journey. :-)

Nevertheless, a hearty congratulations to R Core!

Marc



From jfox at mcmaster.ca  Tue Oct  5 15:30:26 2004
From: jfox at mcmaster.ca (John Fox)
Date: Tue, 5 Oct 2004 09:30:26 -0400
Subject: [R] Making a 'joint distribution'?
In-Reply-To: <20041003091004.GH23573@igidr.ac.in>
Message-ID: <20041005133021.QHDB2048.tomts10-srv.bellnexxia.net@JohnDesktop8300>

Dear Ajay,

There may be a function that does this already, but if not, it not hard to
do, at least for two-way tables:

Table <- function(x, y) {
      tab <- table(x, y)
      tab <- cbind(tab, rowSums(tab))
      tab <- rbind(tab, colSums(tab))
      rownames(tab)[nrow(tab)] <- deparse(substitute(y))
      colnames(tab)[ncol(tab)] <- deparse(substitute(x))
      tab
      }

I hope this helps,
 John 

> -----Original Message-----
> From: r-help-bounces at stat.math.ethz.ch 
> [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Ajay Shah
> Sent: Sunday, October 03, 2004 4:10 AM
> To: r-help
> Subject: [R] Making a 'joint distribution'?
> 
> Suppose I make two discrete variables --
> > D <- data.frame(f1=sample(1:5,100,replace=T), 
> > f2=sample(1:5,100,replace=T))
> 
> I know I can do:
> 
> > table(D$f1, D$f2)
>     0 1 2 3 4
>   0 5 5 5 5 4
>   1 4 2 6 7 3
>   2 5 3 5 3 6
>   3 3 1 3 1 2
>   4 6 4 3 3 6
> > table(D$f1)
>  0  1  2  3  4
> 24 22 22 10 22 
> > table(D$f2)
>  0  1  2  3  4
> 23 15 22 19 21 
> 
> which is all great. But how do I produce the typical 
> presentation of the "joint distribution" where we put the 
> marginal distributions in the margins? E.g. I'd like to get 
> some object "joint" where one would get :
> 
> > joint
>      0  1  2  3  4  f1
>   0  5  5  5  5  4  24
>   1  4  2  6  7  3  22
>   2  5  3  5  3  6  22
>   3  3  1  3  1  2  10
>   4  6  4  3  3  6  22
>  f2 23 15 22 19 21 100
> 
> So that one could then say "joint/nrow(D)" and get nice 
> probabilities out of it. It would great to be able to say 
> "xtable(joint/nrow(D))" :-)
> 
> I'm sure R has a lovely way to do this, but I'm not able to 
> figure it out.
> 
> -- 
> Ajay Shah                                                   Consultant
> ajayshah at mayin.org                      Department of Economic Affairs
> http://www.mayin.org/ajayshah           Ministry of Finance, New Delhi
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html



From James_A_Rogers at groton.pfizer.com  Tue Oct  5 16:18:52 2004
From: James_A_Rogers at groton.pfizer.com (Rogers, James A [PGRD Groton])
Date: Tue, 5 Oct 2004 10:18:52 -0400 
Subject: [R] gnls or nlme : how to obtain confidence intervals of 
	fitted values
Message-ID: <C735670CCC69D61193DA0002A58EE99013223865@groexmb07.pfizer.com>


I believe what Pierre means to be asking for are "prediction intervals", or
possibly "tolerance intervals", which are different from "confidence
intervals". A great reference is:

Hahn G and Meeker WQ: Statistical Intervals. John Wiley and Sons, New York,
1991

For example, in a one-sample Normal problem, \bar{X} \pm 2
\hat{sigma}/\sqrt{n} is an approximate 95% confidence interval, while
\bar{X} \pm 2 \hat{sigma} is an approximate 95% prediction interval
(assuming \bar{X} and \hat{sigma} are high precision estimates; if they are
not you probably want to consider tolerance intervals, which are discussed
in the above reference). 

Both intervals.lme{nlme} and estimable{gmodels} will give you confidence
intervals, but neither will give you prediction intervals. I am not aware of
any published R function that gives you prediction intervals or tolerance
intervals for lme models. It is not easy to write such a function for the
general case, but it may be relatively easy to write your own for special
cases of lme models.  

Jim 

> Message: 9
> Date: Mon, 4 Oct 2004 21:42:20 +1000
> From: Andrew Robinson <andrewr at uidaho.edu>
> Subject: Re: [R] gnls or nlme : how to obtain confidence intervals of
> 	fitted	values
> To: David Scott <d.scott at auckland.ac.nz>
> Cc: r-help at stat.math.ethz.ch, Spencer Graves <spencer.graves at pdf.com>
> Message-ID: <20041004114220.GJ67507 at uidaho.edu>
> Content-Type: text/plain; charset=us-ascii
> 
> The function estimable() from the gmodels part of the gregmisc package
> will do this, if applied appropriately.
> 
> It allows for the estimation of arbitrary linear combinations of the
> parameter estimates of a model object, and calcualtes confidence
> intervals.
> 
> I hope that this helps,
> 
> Andrew
> 
> 
> 
> On Tue, Oct 05, 2004 at 12:31:48AM +1300, David Scott wrote:
> > On Mon, 4 Oct 2004, Pierre MONTPIED wrote:
> > 
> > >Thanks Spencer but the intervals function gives confidence intervals of

> > >the parameters of the model not the predicted values. In the Soybean 
> > >example it would be the CI of predicted weight for a given time,
knowing 
> > >all the parameters (Asym, xmid, scal, variance function and residual)
and 
> > >their distributions.
> > >
> > >And what I need to calculate is precisely this CI.
> > >
> > >My question is therefore is there an analytical way to calculate such
CI, 
> > >whatever the model, or could I try some randomizing techniques such as 
> > >bootstrap or other ?
> > >


LEGAL NOTICE\ Unless expressly stated otherwise, this messag...{{dropped}}



From tlumley at u.washington.edu  Tue Oct  5 16:24:53 2004
From: tlumley at u.washington.edu (Thomas Lumley)
Date: Tue, 5 Oct 2004 07:24:53 -0700 (PDT)
Subject: [R] installation help for mac os x
In-Reply-To: <59F15714-167A-11D9-B6E0-000A95C43AAC@npgcable.com>
References: <59F15714-167A-11D9-B6E0-000A95C43AAC@npgcable.com>
Message-ID: <Pine.A41.4.61.0410050723510.52942@homer12.u.washington.edu>

On Mon, 4 Oct 2004, Nancy Hornewer wrote:
> (4) Didn't install X11 SDK package because it was optional and I don't think 
> I need that (just want to run this on my machine)
> (5) Installed tcl8.4.7 - no problems
> (6) Tried installing tk8.4.7 but received many many error messages when I 
> typed "make". Here is the last portion of those messages:

I think you may need the X11 SDK to compile Tk, since it is an X11-based 
version.

 	-thomas



From tlumley at u.washington.edu  Tue Oct  5 16:26:49 2004
From: tlumley at u.washington.edu (Thomas Lumley)
Date: Tue, 5 Oct 2004 07:26:49 -0700 (PDT)
Subject: [R] Making a 'joint distribution'?
In-Reply-To: <20041003091004.GH23573@igidr.ac.in>
References: <20041003091004.GH23573@igidr.ac.in>
Message-ID: <Pine.A41.4.61.0410050726070.52942@homer12.u.washington.edu>

On Sun, 3 Oct 2004, Ajay Shah wrote:

> Suppose I make two discrete variables --
>> D <- data.frame(f1=sample(1:5,100,replace=T), f2=sample(1:5,100,replace=T))
>
<sniP>
> which is all great. But how do I produce the typical presentation of
> the "joint distribution" where we put the marginal distributions in
> the margins? E.g. I'd like to get some object "joint" where one would
> get :

?addmargins.

 	-thomas



From tlumley at u.washington.edu  Tue Oct  5 16:38:09 2004
From: tlumley at u.washington.edu (Thomas Lumley)
Date: Tue, 5 Oct 2004 07:38:09 -0700 (PDT)
Subject: AW: AW: [R] constructing specially ordered factor
In-Reply-To: <D15343265276D31197BC00A024A6C110C793C7@EXS_BDC>
References: <D15343265276D31197BC00A024A6C110C793C7@EXS_BDC>
Message-ID: <Pine.A41.4.61.0410050728210.52942@homer12.u.washington.edu>

On Tue, 5 Oct 2004, Khamenia, Valery wrote:
> BTW, do you mean that current hash-based implementation brings *clearly*
> better performance than any O(n*log(n)) sort based algorithm?
> If I have correctly understood src/main/unique.c then current
> hash function is niether minimal perfect hash function nor even
> minimal hash function. In addition, as might be expected,
> current hash function uses full pass through the string to get
> a hash key.
>
> So, in particular, can anyone clearly show that the current
> hash-based algorithm will be quicker then sort-based
> algorithm if the input has:
>
> 1. a lot of strings;
> 2. strings are very long;
> 3. strings are quite unsimilar
>
> ?
>
> Hm, I don't believe you are ready to prove smth. like that.

Clearly the algorithm is not optimal for all possible sets of 
inputs (eg if the inputs are already known to be unique then an even 
faster implementation is just to do nothing).

As R's string comparison function use C's strcmp, for which the C standard 
makes no performance guarantees whatsoever, it is not possible to prove 
*anything* about the relative performance of algorithms without making 
some additional assumptions.

However, if this was a serious question, it is known that
a) in some versions of R on some byte-orderings the hash was broken and 
the performance was far inferior, suggesting that it is ordinarily quite 
effective.
b) Switching the rowsum function from a sort-based implementation to 
hashing produced a substantial speed increase.

 	-thomas



From tlumley at u.washington.edu  Tue Oct  5 16:48:23 2004
From: tlumley at u.washington.edu (Thomas Lumley)
Date: Tue, 5 Oct 2004 07:48:23 -0700 (PDT)
Subject: [R] save print survfit object to data frame
In-Reply-To: <200410051155220525.00E18E8F@localhost>
References: <200410051155220525.00E18E8F@localhost>
Message-ID: <Pine.A41.4.61.0410050739110.52942@homer12.u.washington.edu>

On Tue, 5 Oct 2004, Ruud H. Koning wrote:

> Hello, I have estimated a survival model with six strata:
>
>> model.b <-
> survfit(Surv(time=start.tijd,time2=eind.tijd2,event=va)~strata(product.code)
> ,
> data=wu.wide)
>
> I would like to save the output of
>
>> print(model.b,print.n="records",show.rmean=FALSE)
>
> in a dataframe so that I can export it later. How do I do this? Note that
> summary(model.b) gives an error:
> Error in as.matrix(x) : (subscript) logical subscript too long
>

I believe this bug has been fixed now, but summary() doesn't give the same 
information as print().

It's a bit embarassing that the best solution seems to be

tmp<-capture.output(print(model.b,print.n="records",show.rmean=FALSE))
read.table(textConnection(tmp), skip=3, header=TRUE)


 	-thomas



From Tatsuki.Koyama at Vanderbilt.edu  Tue Oct  5 16:48:38 2004
From: Tatsuki.Koyama at Vanderbilt.edu (Tatsuki Koyama)
Date: Tue, 05 Oct 2004 09:48:38 -0500
Subject: [R] cat function within a loop
Message-ID: <4162B446.2020205@Vanderbilt.edu>

I have a 'cat' function within a for loop.
I would like it to print out the result everytime it goes through the
for loop so that I can monitor the progress, but it only prints out
(execute the cat function) once at the very end of the for loop.
A simple example:

for(i in 1:100000){
	cat(i, '\n')
}

What should I do in order for the cat function to be executed
everytime it goes thorough the for loop?
Thanks!



From tplate at blackmesacapital.com  Tue Oct  5 16:59:30 2004
From: tplate at blackmesacapital.com (Tony Plate)
Date: Tue, 05 Oct 2004 08:59:30 -0600
Subject: [R] How might one write this better?
In-Reply-To: <20041003113542.GJ23573@igidr.ac.in>
References: <20041003113542.GJ23573@igidr.ac.in>
Message-ID: <6.1.0.6.2.20041005084527.06259020@mailhost.blackmesacapital.com>

The trick to vectorizing

 > asset <- numeric(T+1)
 > for (t in 1:T) asset[t+1] <- cont[t] + ret[t]*asset[t]

is to expand it algebraically into a sum of terms like:

asset[4] = cont[3] + ret[3] * cont[2] + ret[3] * ret[2] * cont[1]

(where the general case should be reasonably obvious, but is more work to 
write down)

Then recognize that this a sum of the elementwise product of a pair of 
vectors, one of which can be constructed with careful use of rev() and 
cumprod():

 > set.seed(1)
 > ret <- (rnorm(5)+1)/10
 > cont <- seq(along=ret)+100
 > asset <- numeric(length(ret)+1)
 > # loop way of computing assets -- final asset value is in the last 
element of asset[]
 > for (i in seq(along=ret)) asset[i+1] <- cont[i] + (1+ret[i]) * asset[i]
 > asset
[1]   0.0000 101.0000 214.9548 321.4880 508.9232 681.5849
 > # vectorized way of computing final asset value
 > sum(cumprod(rev(c(1+ret[-1],1))) * rev(cont))
[1] 681.585
 > # compare the two
 > sum(cumprod(rev(c(1+ret[-1],1))) * rev(cont)) - asset[length(ret)+1]
[1] 0
 >


At Sunday 05:35 AM 10/3/2004, you wrote:
>I am trying to simulate the trajectory of the pension assets of one
>person. In C-like syntax, it looks like this:
>
>daily.wage.growth = 1.001                 # deterministic
>contribution.rate = 0.08                  # deterministic 8%
>Wage = 10                                 # initial
>Asset = 0                                 # initial
>for (10,000 days) {
>     Asset += contribution.rate * Wage           # accreting contributions
>     Wage *= daily.wage.growth * Wage            # wage growth
>     Asset *= draw from a normal distribution    # Asset returns
>}
>cat("Terminal asset = ", Asset, "\n")
>
>How can one do this well in R? What I tried so far is to notice that
>the wage trajectory is deterministic, it does not change from one run
>to the next, and it can be done in one line. The asset returns
>trajectory can be obtained using a single call to rnorm(). Both these
>can be done nicely using R functions (if you're curious, I can give
>you my code). Using these, I efficiently get a vector of contributions
>c[] and a vector of returns r[]. But that still leaves the loop:
>
>   Asset <- 0
>   for (t in 1:T) {
>     Asset <- c[t] + r[t]*Asset
>   }
>
>How might one do this better?
>
>I find that using this code, it takes roughly 0.3 seconds per
>computation of Asset (on my dinky 500 MHz Celeron). I need to do
>50,000 of these every now and then, and it's a pain to have to wait 3
>hours. It'll be great if there is some neat R way to rewrite the
>little loop above.
>
>--
>Ajay Shah                                                   Consultant
>ajayshah at mayin.org                      Department of Economic Affairs
>http://www.mayin.org/ajayshah           Ministry of Finance, New Delhi
>
>______________________________________________
>R-help at stat.math.ethz.ch mailing list
>https://stat.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html



From sundar.dorai-raj at PDF.COM  Tue Oct  5 17:01:40 2004
From: sundar.dorai-raj at PDF.COM (Sundar Dorai-Raj)
Date: Tue, 05 Oct 2004 10:01:40 -0500
Subject: [R] cat function within a loop
In-Reply-To: <4162B446.2020205@Vanderbilt.edu>
References: <4162B446.2020205@Vanderbilt.edu>
Message-ID: <4162B754.5090007@pdf.com>



Tatsuki Koyama wrote:

> I have a 'cat' function within a for loop.
> I would like it to print out the result everytime it goes through the
> for loop so that I can monitor the progress, but it only prints out
> (execute the cat function) once at the very end of the for loop.
> A simple example:
> 
> for(i in 1:100000){
> 	cat(i, '\n')
> }
> 
> What should I do in order for the cat function to be executed
> everytime it goes thorough the for loop?
> Thanks!
> 

You haven't told us what operating system you're using. But I bet it's 
Windows and what you're seeing is the buffered output. If that's the 
case see the Windows FAQ 6.3.

http://cran.r-project.org/bin/windows/rw-FAQ.html

--sundar



From ligges at statistik.uni-dortmund.de  Tue Oct  5 17:03:40 2004
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Tue, 05 Oct 2004 17:03:40 +0200
Subject: [R] cat function within a loop
In-Reply-To: <4162B446.2020205@Vanderbilt.edu>
References: <4162B446.2020205@Vanderbilt.edu>
Message-ID: <4162B7CC.6020509@statistik.uni-dortmund.de>

Tatsuki Koyama wrote:

> I have a 'cat' function within a for loop.
> I would like it to print out the result everytime it goes through the
> for loop so that I can monitor the progress, but it only prints out
> (execute the cat function) once at the very end of the for loop.
> A simple example:
> 
> for(i in 1:100000){
> 	cat(i, '\n')
> }
> 
> What should I do in order for the cat function to be executed
> everytime it goes thorough the for loop?

This is a FAQ mentioned in the R for Windows FAQ!
Please read it before posting (and do not forget to mention on which OS 
you are working with which version of R).

Uwe Ligges


> Thanks!
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html



From dimitris.rizopoulos at med.kuleuven.ac.be  Tue Oct  5 17:10:32 2004
From: dimitris.rizopoulos at med.kuleuven.ac.be (Dimitris Rizopoulos)
Date: Tue, 5 Oct 2004 17:10:32 +0200
Subject: [R] cat function within a loop
References: <4162B446.2020205@Vanderbilt.edu>
Message-ID: <008f01c4aaed$7534eb70$b2133a86@www.domain>

Just press Ctrl+W (it controls buffered output) if you are using the 
RGui.

I hope it helps.

Best,
Dimitris

----
Dimitris Rizopoulos
Ph.D. Student
Biostatistical Centre
School of Public Health
Catholic University of Leuven

Address: Kapucijnenvoer 35, Leuven, Belgium
Tel: +32/16/396887
Fax: +32/16/337015
Web: http://www.med.kuleuven.ac.be/biostat/
     http://www.student.kuleuven.ac.be/~m0390867/dimitris.htm


----- Original Message ----- 
From: "Tatsuki Koyama" <Tatsuki.Koyama at Vanderbilt.edu>
To: <r-help at stat.math.ethz.ch>
Sent: Tuesday, October 05, 2004 4:48 PM
Subject: [R] cat function within a loop


>I have a 'cat' function within a for loop.
> I would like it to print out the result everytime it goes through 
> the
> for loop so that I can monitor the progress, but it only prints out
> (execute the cat function) once at the very end of the for loop.
> A simple example:
>
> for(i in 1:100000){
> cat(i, '\n')
> }
>
> What should I do in order for the cat function to be executed
> everytime it goes thorough the for loop?
> Thanks!
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
>



From ripley at stats.ox.ac.uk  Tue Oct  5 17:15:12 2004
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Tue, 5 Oct 2004 16:15:12 +0100 (BST)
Subject: [R] cat function within a loop
In-Reply-To: <4162B446.2020205@Vanderbilt.edu>
Message-ID: <Pine.LNX.4.44.0410051611010.13107-100000@gannet.stats>

If perchance you are using Windows (and specifically Rgui) and not telling 
us (as the posting guide asks you to), do read the rw-FAQ Q6.3 (as the 
posting guide also asks you to).

On Tue, 5 Oct 2004, Tatsuki Koyama wrote:

> I have a 'cat' function within a for loop.
> I would like it to print out the result everytime it goes through the
> for loop so that I can monitor the progress, but it only prints out
> (execute the cat function) once at the very end of the for loop.

Have you evidence for that or are you speculating?  When I run your 
example I get a line for every i.  If you do not, something is wrong with 
your (unspecified) installation of R.

> A simple example:
> 
> for(i in 1:100000){
> 	cat(i, '\n')
> }
> 
> What should I do in order for the cat function to be executed
> everytime it goes thorough the for loop?

> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From shuangge at biostat.wisc.edu  Tue Oct  5 17:30:35 2004
From: shuangge at biostat.wisc.edu (Shuangge Ma)
Date: Tue, 5 Oct 2004 10:30:35 -0500 (CDT)
Subject: [R] constrOptim convergence
Message-ID: <Pine.GSO.4.58.0410051027340.5237@naos.biostat.wisc.edu>

Hello, I got a question with the R function constrOptim.

>From the R help, it says that the return values of "constrOptim" are the
same as "optim". For the return value "convergence" of the function
"optim", the values should be 0, 1, 10, 51 and 52. See
http://www.maths.lth.se/help/R/.R/library/stats/html/optim.html

When I use constrOptim, I get "convergence" values 7 and 11. What do they
mean exactly?

Thanks again,

Shuangge Ma, Ph.D.



From MichaelWeber at web.de  Tue Oct  5 17:39:35 2004
From: MichaelWeber at web.de (Michael Weber)
Date: Tue, 5 Oct 2004 17:39:35 +0200
Subject: [R] Need R Version 1.8.0 or earlier
Message-ID: <000e01c4aaf1$848d27a0$14b2a8c0@nx01>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20041005/411b6ea3/attachment.pl

From vito_ricci at yahoo.com  Tue Oct  5 17:48:44 2004
From: vito_ricci at yahoo.com (Vito Ricci)
Date: Tue, 5 Oct 2004 17:48:44 +0200 (CEST)
Subject: [R] Need R Version 1.8.0 or earlier
Message-ID: <20041005154844.8883.qmail@web41208.mail.yahoo.com>

Hi,
you didn't specify on which OS (Linux, Win, Mac) would
run R 1.8. 
For R souces see:
http://cran.r-project.org/src/base/R-1/
Best
Vito




You wrote:

Hello.

I need R in Version 1.8.0 or earlier. I also need the
packages foreign, x-table and gregmisc for this
version. Does anyone know where I can get it?

Greetings

Michael Weber

=====
Diventare costruttori di soluzioni

"The business of the statistician is to catalyze 
the scientific learning process."  
George E. P. Box


Visitate il portale http://www.modugno.it/
e in particolare la sezione su Palese http://www.modugno.it/archivio/cat_palese.shtml



From ligges at statistik.uni-dortmund.de  Tue Oct  5 17:54:02 2004
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Tue, 05 Oct 2004 17:54:02 +0200
Subject: [R] Need R Version 1.8.0 or earlier
In-Reply-To: <000e01c4aaf1$848d27a0$14b2a8c0@nx01>
References: <000e01c4aaf1$848d27a0$14b2a8c0@nx01>
Message-ID: <4162C39A.30304@statistik.uni-dortmund.de>

Michael Weber wrote:

> Hello.
> 
> I need R in Version 1.8.0 or earlier. I also need the packages foreign, x-table and gregmisc for this version. Does anyone know where I can get it?

REALLY?


The R-1.8.x sources are available at:
  Your-CRAN-Mirror/src/base/R-1/

Old packages sources are available at:
  Your-CRAN-Mirror/src/contrib/Archive/



BTW: If you need a Windows version (you forgot to tell it again!), I can 
put up R-1.8.x somewhere temporarily, packages are available at :
  Your-CRAN-Mirror/bin/windows/contrib/1.8/


Uwe Ligges



> Greetings
> 
> Michael Weber
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html



From B.Rowlingson at lancaster.ac.uk  Tue Oct  5 17:58:13 2004
From: B.Rowlingson at lancaster.ac.uk (Barry Rowlingson)
Date: Tue, 05 Oct 2004 16:58:13 +0100
Subject: [R] Need R Version 1.8.0 or earlier
In-Reply-To: <000e01c4aaf1$848d27a0$14b2a8c0@nx01>
References: <000e01c4aaf1$848d27a0$14b2a8c0@nx01>
Message-ID: <4162C495.7040608@lancaster.ac.uk>

Michael Weber wrote:
> Hello.
> 
> I need R in Version 1.8.0 or earlier. I also need the packages foreign, x-table and gregmisc for this version. Does anyone know where I can get it?
> 

Start here:
  http://www.r-project.org/

Click CRAN. Choose a mirror.

Follow the "Source code of  older versions of R is available here." link 
from the CRAN page.

You'll have to compile this, I cant find an archive of binaries. Easy on 
Linux, poss fiddly on Windows.

If you then go to the 'Contributed Extension Packages' link, you'll find 
an 'Archive' link at the bottom. Go there. Lots of old and crusty code.

But now you really want to get the most recent version that still works 
with R-1.8.0 (or whichever). Tricky. You really need to download all the 
versions for the packages you want, look at the DESCRIPTION files, and 
check the Depends: fields.

I did start work on a utility to do this, to make it easy to find the 
most recent version of a package for a given R version, but stopped half 
way through. Some other day...

Baz



From spencer.graves at pdf.com  Tue Oct  5 18:32:26 2004
From: spencer.graves at pdf.com (Spencer Graves)
Date: Tue, 05 Oct 2004 09:32:26 -0700
Subject: [R] Computing and testing transition probabilities?
In-Reply-To: <20041003184146.GA28159@igidr.ac.in>
References: <20041003184146.GA28159@igidr.ac.in>
Message-ID: <4162CC9A.3040207@pdf.com>

      Regarding how to compile the table, have you considered something 
like the following: 

 > Trans <- array(0, dim=c(3,3))
 > for(i in 1:(n-1))
+   if(D$names[i]==D$names[i+1])
+     (Trans[D$f[i], D$f[i+1]] <- Trans[D$f[i], D$f[i+1]]+1)
 > Trans> Trans <- array(0, dim=c(3,3))
 > for(i in 1:(n-1))
+   if(D$names[i]==D$names[i+1])
+     (Trans[D$f[i], D$f[i+1]] <- Trans[D$f[i], D$f[i+1]]+1)
 > Trans
 > Trans <- array(0, dim=c(3,3))
 > for(i in 1:(n-1))
+   if(D$names[i]==D$names[i+1])
+     (Trans[D$f[i], D$f[i+1]] <- Trans[D$f[i], D$f[i+1]]+1)
 > Trans
     [,1] [,2] [,3]
[1,]    1    0    1
[2,]    0    0    1
[3,]    1    1    1

      This is not the answers you got;  I got this in R 1.9.1 under 
Windows 2000.  I'm sure there are ways to do this without a for loop, 
but I couldn't think of one immediately. 

      Regarding how to analyze, have you searched for "discrete markov 
estimation" and "hidden markov estimate" from www.r-project.org -> 
search -> "R site search"?  There are many possibilities.

      hope this helps.  spencer graves

Ajay Shah wrote:

>Folks, I have a situation with many firms, observed for many years
>(but it's not panel data, so every now and then data for a firm just
>goes missing).
>
>Let me show you an example. There are 3 firms "a", "b" and "c". There
>are 3 years: 1981, 1982 and 1983. There's a factor f which takes
>values 1, 2 or 3.
>
>set.seed(5)
>D = data.frame(
>  names=c("a", "a", "a", "b", "b", "c", "c", "c", "d", "d"),
>  year= c( 81,  82,  83,  81,  83,  81,  82,  83,  82,  83),
>  f=    sample(1:3, 10, replace=T)
>  )
>print(D)
>
>What I'd like to do is locate situations where a firm is observed for
>two consecutive years, and put together conditional probabilities of
>state transition.
>
>Expressed as counts, putting time $t$ as the rows and time $t+1$ as
>the columms, I'd like to get the table:
>
>    1   2   3
>1           1
>2       1
>3   1   1   1
>
>For example, this says that we only observe one situation (in this
>data) where f=1 and then we got to see the next year, and in that year
>f was 3. So we have a 100% probability of going from 1 -> 3. In the
>case of f=3, we have 3 situations where we observe two consecutive
>years where the 1st is f=3, and it turns out that the outcomes are
>1,2,3 each happening once.
>
>Expressed as conditional probabilities it is:
>
>    1   2   3
>1           1
>2       1
>3 .33  .33 .33
>
>How might one do this? And then, how might one go about getting
>confidence intervals for the transition probabilities that are seen in
>this transition matrix? I'd like to test the null that the state
>variable does not change. I.e., if a firm is in f=2, under the null,
>it's just stay at f=2 in the following year. That'd be an identity
>matrix for the transition probabilities:
>
>    1   2   3
>1   1
>2       1
>3           1
>
>How does one talk about the distribution of the transition
>probabilities under this null, and thus get a test?
>
>  
>

-- 
Spencer Graves, PhD, Senior Development Engineer
O:  (408)938-4420;  mobile:  (408)655-4567



From s-plus at wiwi.uni-bielefeld.de  Tue Oct  5 19:14:06 2004
From: s-plus at wiwi.uni-bielefeld.de (Peter Wolf)
Date: Tue, 05 Oct 2004 19:14:06 +0200
Subject: [R] multiple dimensional  diag()
References: <a06002000bd82edeb8c91@[139.166.242.29]>
	<415D776C.8060905@wiwi.uni-bielefeld.de>
	<a06002000bd86ae189ae7@[139.166.242.29]>
	<416184D4.50903@wiwi.uni-bielefeld.de>
	<a06002000bd87f6a29c96@[139.166.242.29]>
Message-ID: <4162D65E.3010306@wiwi.uni-bielefeld.de>

Robin Hankin wrote:

> hi again Peter
>
> thanks for this.  Now we've got the legal stuff sorted,  on to business!
>
> One of the reasons I wanted the function was to to multidimensional 
> moving-
> window averaging, and for this I needed to be able to call 
> a.block.diag(a,b) when a and b might
> have one or more dimensions of zero extent.
>
> I also figure that a scalar argument should be interpreted as having 
> dimensions
> rep(1,d) where d=length(dim(<other matrix>)), and that a.block.diag(n,m)
> should return plain old diag(n,m) if both n and m are scalars.
>
> I've modified your function do to this, and added a padding value 
> argument (function
> and a couple of calls pasted  below)
>
> what do you think? 



perfect!

... and if you want to combine more than two arrays you can add a 
function like   mbd

mbd<-function(...,pad=0){
  result<-(args<-list(...))[[1]]
  for(li in args[-1]) result<-a.block.diag(result,li,pad=pad)
  return(result)
}
a <- matrix(1:4,2,2)
mbd(a,a,a,a,pad=10)
     [,1] [,2] [,3] [,4] [,5] [,6] [,7] [,8]
[1,]    1    3   10   10   10   10   10   10
[2,]    2    4   10   10   10   10   10   10
[3,]   10   10    1    3   10   10   10   10
[4,]   10   10    2    4   10   10   10   10
[5,]   10   10   10   10    1    3   10   10
[6,]   10   10   10   10    2    4   10   10
[7,]   10   10   10   10   10   10    1    3
[8,]   10   10   10   10   10   10    2    4

... or integrate the loop into a.block.diag

Peter Wolf



>
>
>
> best wishes
>
> Robin
>
>
>
> a.block.diag" <- function(a,b,pad=0) {
>   ## a.block.daig combines arrays a and b and builds a new array which 
> has
>   ## a and b as blocks on its diagonal. pw 10/2004
>
>   if( (length(a)==1) & (length(b)==1) ){return(diag(c(a,b)))}
>   if(length(a)==1){dim(a) <- rep(1,length(dim(b)))}
>   if(length(b)==1){dim(b) <- rep(1,length(dim(a)))}
>
>   if(length(dim.a <- dim(a)) != length(dim.b <- dim(b))){
>     stop("a and b must have identical number of dimensions")
>   }
>
>   seq.new <- function(i){if(i==0){return(NULL)} else {return(1:i)}}
>   s <- array(pad, dim.a+dim.b)
>   s <- do.call("[<-",c(list(s),lapply(dim.a,seq.new),list(a)))
>   ind <- lapply(seq(dim.b),function(i)seq.new(dim.b[[i]])+dim.a[[i]])
>   do.call("[<-",c(list(s),ind,list(b)))
> }
>
>
>  a <- matrix(1:4,2,2)
>
>>  a
>
>      [,1] [,2]
> [1,]    1    3
> [2,]    2    4
>
>>  b <- array(1e44,c(0,3))
>>  b
>
>      [,1] [,2] [,3]
>
>>  a.block.diag(a,b)
>
>      [,1] [,2] [,3] [,4] [,5]
> [1,]    1    3    0    0    0
> [2,]    2    4    0    0    0
>
>>
>>
>
>>>
>>>
>>> Before I go any further, I need to check that it's OK with you for 
>>> me to put this
>>> function (or modifications of it) into my package, which is GPL. 
>>> Full authorship credit given.
>>> Is this OK?
>>
>>
>> Hallo Robin,
>>
>> you are welcome to put  a.block.diag()  in your package.
>> The function is free software; you can redistribute it and/or modify 
>> it under the terms of the GNU...
>>
>> Peter Wolf
>>
>>>>
>>>>
>



From gotrout at gmail.com  Tue Oct  5 19:27:17 2004
From: gotrout at gmail.com (gotrout@gmail.com)
Date: Tue, 5 Oct 2004 10:27:17 -0700
Subject: [R] correct my method of estimating mean of two POSIXlt data frames
Message-ID: <ae9f881904100510272616e350@mail.gmail.com>

Hello,  I searched the archives but could not come to a solution.  I
have to two columns of information

t_start_cdt looks like:
> t_start_cdt[1:4]
[1] "2003-07-09 11:02:25" "2003-07-09 11:10:25" "2003-07-09 11:30:25"
[4] "2003-07-09 12:00:25"
> class(t_start_cdt)
[1] "POSIXt"  "POSIXlt"

t_end_cdt looks like:
> t_end_cdt[1:4]
[1] "2003-07-09 11:02:35" "2003-07-09 11:10:35" "2003-07-09 11:30:35"
[4] "2003-07-09 12:00:35"
> class(t_end_cdt)
[1] "POSIXt"  "POSIXlt"


I'd like to estimate the mean of each "pair".  For example, the mean
of (t_start_cdt[1] and t_end_cdt[1]).  The only way I could do this
is:
hi <- cbind(as.matrix(as.POSIXct(t_start_cdt)),
as.matrix(as.POSIXct(t_end_cdt)))
hi <- apply(hi, MARGIN=1, FUN=mean)
class(hi) <- c("POSIXt", "POSIXct")
t_mean_cdt <- as.POSIXlt(hi)
rm(hi)

What am I missing conceptually about POSIXlt, and what is the better method?
thanks,
Mike



From jmc at research.bell-labs.com  Tue Oct  5 19:31:26 2004
From: jmc at research.bell-labs.com (John Chambers)
Date: Tue, 05 Oct 2004 13:31:26 -0400
Subject: [R] Reading Version 4 .sdd files
References: <2A03167DD08194408B35A53E75EFE07E6EC599@seaexch01.corp.avenuea.com>
Message-ID: <4162DA6E.A7D1822A@research.bell-labs.com>


John Chandler wrote:

> Dear R-Help,
> 
> I've never had any trouble importing data into R until I had to import
> an .sdd file for a class. The file can be found here:
> http://www.math.umt.edu/steele/Math%20549/Farms.sdd. It begins with the
> line "## Dump S Version 4 Dump ##". I first attempted read.S which
> issued the message "not an S object". I then checked the Import/Export
> manual which seemed to indicate that data.restore might do the trick.
> Alas, it seems that function is trying to interpret the name of the data
> frame as an S mode or some such thing.
> 
> Searched the archives (courtesy of Dr. Baron) and found this from Dr.
> Ripley: ".sdd files from S-PLUS 6.0 are
> S4 dumps, and will not work." This was written in 2002, is it still
> accurate? Right now my only obvious course of action is to get on a
> machine that has S+, read in this file and dump it in a more useful
> format. Are their others?
> 

The S Version 4 dump format is described, somewhat briefly, in my
"Programming with Data", pp 234-241.  It would not be particularly hard
to write a "reader" for the format, but as far as I know, this has not
been done.

Whether it's a good idea to read these dumps into R would depend on what
data is stored there.  For reasonably simple objects, the format is
pretty obvious & the results would likely be fine.

As the objects get more complicated, it's unlikely that the format will
contain enough infomation, without the definition of the classes of the
objects, to recreate them in R.

The purpose of the data dump was to generate a portable & fully general
dump for objects--"portable" however meaning between machines running
the same system (with the same class definitions).

In particular, if the objects were generated from a formally defined
("S4") class, the object as dumped will not give you much of a clue
without the definition of the class.  Nor, for that matter, will the
ordinary dump() output, but that at least may be re-executable, once you
have the correct class definition.

In either case, you would need to get the S-language definition of the
class.

John Chambers



From biocperi at yahoo.com  Tue Oct  5 19:53:31 2004
From: biocperi at yahoo.com (S Peri)
Date: Tue, 5 Oct 2004 10:53:31 -0700 (PDT)
Subject: [R] Deleting a particular row 
Message-ID: <20041005175331.66787.qmail@web50009.mail.yahoo.com>

Hi,
  I have a data frame. 
E.g:

             Fertility Agriculture
Courtelary        80.2        17.0
Delemont          83.1        45.1
Franches-Mnt      92.5        39.7


How can I delete :
Courtelary        80.2        17.0


Thanks.

P


		
_______________________________

Declare Yourself - Register online to vote today!



From sundar.dorai-raj at PDF.COM  Tue Oct  5 20:03:00 2004
From: sundar.dorai-raj at PDF.COM (Sundar Dorai-Raj)
Date: Tue, 05 Oct 2004 13:03:00 -0500
Subject: [R] Deleting a particular row
In-Reply-To: <20041005175331.66787.qmail@web50009.mail.yahoo.com>
References: <20041005175331.66787.qmail@web50009.mail.yahoo.com>
Message-ID: <4162E1D4.5000106@pdf.com>



S Peri wrote:

> Hi,
>   I have a data frame. 
> E.g:
> 
>              Fertility Agriculture
> Courtelary        80.2        17.0
> Delemont          83.1        45.1
> Franches-Mnt      92.5        39.7
> 
> 
> How can I delete :
> Courtelary        80.2        17.0
> 
> 
> Thanks.
> 
> P
> 

See ?subset. Assuming your data is called `mydata':

subset(mydata, Fertility != 80.2 | Agriculture != 17.0)

Note the single `|' for OR. See ?Logic for explanation.

--sundar



From tlumley at u.washington.edu  Tue Oct  5 20:04:42 2004
From: tlumley at u.washington.edu (Thomas Lumley)
Date: Tue, 5 Oct 2004 11:04:42 -0700 (PDT)
Subject: [R] constrOptim convergence
In-Reply-To: <Pine.GSO.4.58.0410051027340.5237@naos.biostat.wisc.edu>
References: <Pine.GSO.4.58.0410051027340.5237@naos.biostat.wisc.edu>
Message-ID: <Pine.A41.4.61.0410051059500.226686@homer08.u.washington.edu>

On Tue, 5 Oct 2004, Shuangge Ma wrote:

> Hello, I got a question with the R function constrOptim.
>
>> From the R help, it says that the return values of "constrOptim" are the
> same as "optim". For the return value "convergence" of the function
> "optim", the values should be 0, 1, 10, 51 and 52. See
> http://www.maths.lth.se/help/R/.R/library/stats/html/optim.html
>
> When I use constrOptim, I get "convergence" values 7 and 11. What do they
> mean exactly?
>

You should also get a convergence message with these (in the $message 
component)

7 is "Barrier algorithm ran out of iterations and did not converge"
so the inner iteration converged but the outer iteration with the log 
barrier didn't

11 is "Objective function increased at outer iteration i" (or "decreased" 
if you are doing maximisation). This probably means that you haven't 
found the optimum.

 	-thomas

Thomas Lumley			Assoc. Professor, Biostatistics
tlumley at u.washington.edu	University of Washington, Seattle



From p.dalgaard at biostat.ku.dk  Tue Oct  5 20:15:41 2004
From: p.dalgaard at biostat.ku.dk (Peter Dalgaard)
Date: 05 Oct 2004 20:15:41 +0200
Subject: [R] Deleting a particular row
In-Reply-To: <4162E1D4.5000106@pdf.com>
References: <20041005175331.66787.qmail@web50009.mail.yahoo.com>
	<4162E1D4.5000106@pdf.com>
Message-ID: <x2brfhhvzm.fsf@biostat.ku.dk>

Sundar Dorai-Raj <sundar.dorai-raj at pdf.com> writes:

> S Peri wrote:
> 
> > Hi,
> >   I have a data frame. E.g:
> >              Fertility Agriculture
> > Courtelary        80.2        17.0
> > Delemont          83.1        45.1
> > Franches-Mnt      92.5        39.7
> > How can I delete :
> > Courtelary        80.2        17.0
> > Thanks.
> > P
> >
> 
> See ?subset. Assuming your data is called `mydata':
> 
> subset(mydata, Fertility != 80.2 | Agriculture != 17.0)
> 
> Note the single `|' for OR. See ?Logic for explanation.

subset(mydata, -1)  might be more to the point, or
subset(mydata, rownames(mydata) != "Courtelary") , or just
mydata[-1,] , or
mydata[-match("Courtelary",rownames(mydata))] ,
or....


-- 
   O__  ---- Peter Dalgaard             Blegdamsvej 3  
  c/ /'_ --- Dept. of Biostatistics     2200 Cph. N   
 (*) \(*) -- University of Copenhagen   Denmark      Ph: (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)             FAX: (+45) 35327907



From ggrothendieck at myway.com  Tue Oct  5 20:57:23 2004
From: ggrothendieck at myway.com (Gabor Grothendieck)
Date: Tue,  5 Oct 2004 14:57:23 -0400 (EDT)
Subject: [R] correct my method of estimating mean of two POSIXlt data
	frames
Message-ID: <20041005185723.E56FE12D14@mprdmxin.myway.com>


<gotrout <at> gmail.com> writes:

: 
: Hello,  I searched the archives but could not come to a solution.  I
: have to two columns of information
: 
: t_start_cdt looks like:
: > t_start_cdt[1:4]
: [1] "2003-07-09 11:02:25" "2003-07-09 11:10:25" "2003-07-09 11:30:25"
: [4] "2003-07-09 12:00:25"
: > class(t_start_cdt)
: [1] "POSIXt"  "POSIXlt"
: 
: t_end_cdt looks like:
: > t_end_cdt[1:4]
: [1] "2003-07-09 11:02:35" "2003-07-09 11:10:35" "2003-07-09 11:30:35"
: [4] "2003-07-09 12:00:35"
: > class(t_end_cdt)
: [1] "POSIXt"  "POSIXlt"
: 
: I'd like to estimate the mean of each "pair".  For example, the mean
: of (t_start_cdt[1] and t_end_cdt[1]).  The only way I could do this
: is:
: hi <- cbind(as.matrix(as.POSIXct(t_start_cdt)),
: as.matrix(as.POSIXct(t_end_cdt)))
: hi <- apply(hi, MARGIN=1, FUN=mean)
: class(hi) <- c("POSIXt", "POSIXct")
: t_mean_cdt <- as.POSIXlt(hi)
: rm(hi)
: 
: What am I missing conceptually about POSIXlt, and what is the better method?
: thanks,


If a.lt and b.lt are the two vectors of POSIXlt dates then try 
converting each to POSIXct and unclassing to make each numeric.
Take the mean of the two numeric vectors and convert them back to
POSIXct and finally to POSIXlt:

a <- unclass(as.POSIXct(a.lt))
b <- unclass(as.POSIXct(b.lt))
as.POSIXlt(structure((a+b)/2, class = c("POSIXt", "POSIXct")))



From gotrout at gmail.com  Tue Oct  5 21:49:20 2004
From: gotrout at gmail.com (gotrout@gmail.com)
Date: Tue, 5 Oct 2004 12:49:20 -0700
Subject: [R] correct my method of estimating mean of two POSIXlt data
	frames
In-Reply-To: <20041005185723.E56FE12D14@mprdmxin.myway.com>
References: <20041005185723.E56FE12D14@mprdmxin.myway.com>
Message-ID: <ae9f88190410051249685db1ae@mail.gmail.com>

> If a.lt and b.lt are the two vectors of POSIXlt dates then try
> converting each to POSIXct and unclassing to make each numeric.
> Take the mean of the two numeric vectors and convert them back to

I see.  I'm a little confused with the use of class/unclass versus
as.XX.  For example, instead of using unclass, why wouldn't I use
as.numeric?  Could someone explain the difference?
thanks,
Mike



From biocperi at yahoo.com  Tue Oct  5 22:03:04 2004
From: biocperi at yahoo.com (S Peri)
Date: Tue, 5 Oct 2004 13:03:04 -0700 (PDT)
Subject: [R] row.name in data.frame
In-Reply-To: <x2brfhhvzm.fsf@biostat.ku.dk>
Message-ID: <20041005200304.93268.qmail@web50002.mail.yahoo.com>

Hi Group, 
 
 I have a table with column names and row names. 

Species  A  B  C  D  E   F  G 
Human    1  2  3  4  1   0  3
Rat      0  2  3  3  2   1  2


I read this tab delim. text file into R like the
following:
> mydata <-read.table('mydata.txt',header=TRUE)
> mydata
  Species A B C D
1   Human 1 2 3 2
2     Rat 0 2 5 2
3     Cat 9 2 4 1


Why am I getting 1,2,3 row names even after declaring
heder = TRUE.  I tried declaring row.names = NULL,
however it is not accepting it and expects a vector
with some names. I wanted Human, Rat and Cat as my row
names. I wanted my data frame look like:

Species A B C D
Human 1 2 3 2
Rat 0 2 5 2
Cat 9 2 4 1

I apologise for asking a lame question and this
question might have been posted several times. I could
not find an answer. 

Thanks
P



From murdoch at stats.uwo.ca  Tue Oct  5 21:52:22 2004
From: murdoch at stats.uwo.ca (Duncan Murdoch)
Date: Tue, 05 Oct 2004 15:52:22 -0400
Subject: [R] R 2.0.0 is released
In-Reply-To: <x2oejipkm4.fsf@biostat.ku.dk>
References: <x2oejipkm4.fsf@biostat.ku.dk>
Message-ID: <jeu5m0536ni4a2b4p31kn451j8j4tvm4d7@4ax.com>

On 04 Oct 2004 17:29:39 +0200, Peter Dalgaard
<p.dalgaard at biostat.ku.dk> wrote :

>
>I've rolled up R-2.0.0.tar.gz a short while ago. This is a new version
>with a number of new features. See below for the details.
>
>As was the case with R 1.0.0, this new version represents a coming of
>age more than a radical change to R. We do plan to celebrate the new
>major version with press releases and such.
>
>The release will be available from 
>
>http://cran.r-project.org/src/base/R-2/R-2.0.0.tar.gz
>
>or you might wait for it to be mirrored at a CRAN site nearer to you.
>Binaries for various platforms will appear in due course.

The Windows binary build is now on CRAN at 

  http://cran.r-project.org/bin/windows/base

and the mirrors, e.g. the U.S. mirror

  http://cran.us.r-project.org/bin/windows/base

A few of the links on the page are broken today, but should be fixed
tomorrow.

Duncan Murdoch

_______________________________________________
R-announce at stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-announce



From ripley at stats.ox.ac.uk  Tue Oct  5 22:13:05 2004
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Tue, 5 Oct 2004 21:13:05 +0100 (BST)
Subject: [R] correct my method of estimating mean of two POSIXlt data
	frames
In-Reply-To: <ae9f88190410051249685db1ae@mail.gmail.com>
Message-ID: <Pine.LNX.4.44.0410052109020.25047-100000@gannet.stats>

On Tue, 5 Oct 2004 "gotrout at gmail.com" (but with no name nor signature)
wrote:

Quoting someone without credit (and therefore in breach of their 
copyright)

> > If a.lt and b.lt are the two vectors of POSIXlt dates then try
> > converting each to POSIXct and unclassing to make each numeric.
> > Take the mean of the two numeric vectors and convert them back to
> 
> I see.  I'm a little confused with the use of class/unclass versus
> as.XX.  For example, instead of using unclass, why wouldn't I use
> as.numeric?  Could someone explain the difference?

That advice is wrong: you should not be unclassing before forming the
mean as mean() has a method for POSIXct.

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From TTWu at mednet.ucla.edu  Tue Oct  5 22:22:04 2004
From: TTWu at mednet.ucla.edu (Wu, Tongtong)
Date: Tue, 5 Oct 2004 13:22:04 -0700 
Subject: [R] Nelson-Aalen estimator in R
Message-ID: <3F4024494055CA41BC8B3FE78B837422011E3005@medmail5.mednet.ucla.edu>

Hi,

I am taking a survival class.  Recently I need to do the Nelson-Aalen
estimtor in R.  I searched through the R help manual and internet, but could
not find such a R function.  I tried another way by calculating the
Kaplan-Meier estimator and take -log(S).  However, the function only
provides the summary of KM estimator but no estimated values.  Could you
please help me with this?  I would highly appreciate your great help!

Thanks,
Tongtong

----------------------------------------------------------
IMPORTANT WARNING:  This email (and any attachments) is only intended for the use of the person or entity to which it is addressed, and may contain information that is privileged and confidential.  You, the recipient, are obligated to maintain it in a safe, secure and confidential manner.  Unauthorized redisclosure or failure to maintain confidentiality may subject you to federal and state penalties. If you are not the intended recipient, please immediately notify us by return email, and delete this message from your computer.



From ripley at stats.ox.ac.uk  Tue Oct  5 22:25:02 2004
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Tue, 5 Oct 2004 21:25:02 +0100 (BST)
Subject: [R] row.name in data.frame
In-Reply-To: <20041005200304.93268.qmail@web50002.mail.yahoo.com>
Message-ID: <Pine.LNX.4.44.0410052122040.25170-100000@gannet.stats>

?read.table and the R Data Import/Export manual (and An Introduction to R)
do all explain this.

To use header=TRUE the header should not name the row names column, *OR* 
you need to add row.names=1 to the call.  So just omit `Species'.

On Tue, 5 Oct 2004, S Peri wrote:

> Hi Group, 
>  
>  I have a table with column names and row names. 
> 
> Species  A  B  C  D  E   F  G 
> Human    1  2  3  4  1   0  3
> Rat      0  2  3  3  2   1  2
> 
> 
> I read this tab delim. text file into R like the
> following:
> > mydata <-read.table('mydata.txt',header=TRUE)
> > mydata
>   Species A B C D
> 1   Human 1 2 3 2
> 2     Rat 0 2 5 2
> 3     Cat 9 2 4 1
> 
> 
> Why am I getting 1,2,3 row names even after declaring
> heder = TRUE.  I tried declaring row.names = NULL,
> however it is not accepting it and expects a vector
> with some names. I wanted Human, Rat and Cat as my row
> names. I wanted my data frame look like:
> 
> Species A B C D
> Human 1 2 3 2
> Rat 0 2 5 2
> Cat 9 2 4 1
> 
> I apologise for asking a lame question and this
> question might have been posted several times. I could
> not find an answer. 

Really?  Have you looked at the examples in `An Introduction to R'?

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From NordlDJ at dshs.wa.gov  Tue Oct  5 22:39:24 2004
From: NordlDJ at dshs.wa.gov (Nordlund, Dan)
Date: Tue, 5 Oct 2004 13:39:24 -0700
Subject: [R] row.name in data.frame
Message-ID: <592E8923DB6EA348BE8E33FCAADEFFFC0A646F@dshs-exch2>

Does this do what you want:

mydata <-read.table('mydata.txt',header=TRUE, row.names=1)

see ?read.table

Daniel Nordlund
Research and Data Analysis
Washington State Department of Social and Health Services
P.O. Box 45204
Olympia, WA  98504-5204


-----Original Message-----
From: S Peri [mailto:biocperi at yahoo.com] 
Sent: Tuesday, October 05, 2004 1:03 PM
To: r-help at stat.math.ethz.ch
Subject: [R] row.name in data.frame

Hi Group, 
 
 I have a table with column names and row names. 

Species  A  B  C  D  E   F  G 
Human    1  2  3  4  1   0  3
Rat      0  2  3  3  2   1  2


I read this tab delim. text file into R like the
following:
> mydata <-read.table('mydata.txt',header=TRUE)
> mydata
  Species A B C D
1   Human 1 2 3 2
2     Rat 0 2 5 2
3     Cat 9 2 4 1


Why am I getting 1,2,3 row names even after declaring
heder = TRUE.  I tried declaring row.names = NULL,
however it is not accepting it and expects a vector
with some names. I wanted Human, Rat and Cat as my row
names. I wanted my data frame look like:

Species A B C D
Human 1 2 3 2
Rat 0 2 5 2
Cat 9 2 4 1

I apologise for asking a lame question and this
question might have been posted several times. I could
not find an answer. 

Thanks
P

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide!
http://www.R-project.org/posting-guide.html



From saran.2.vardhanabhuti at gsk.com  Tue Oct  5 22:42:15 2004
From: saran.2.vardhanabhuti at gsk.com (saran.2.vardhanabhuti@gsk.com)
Date: Tue, 5 Oct 2004 16:42:15 -0400
Subject: [R] How to install affy package in R?
Message-ID: <OF7DFF3E86.43B6CBA7-ON85256F24.0070702B-85256F24.0072386C@sb.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20041005/0b9b3cf4/attachment.pl

From ripley at stats.ox.ac.uk  Tue Oct  5 23:05:10 2004
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Tue, 5 Oct 2004 22:05:10 +0100 (BST)
Subject: [R] How to install affy package in R?
In-Reply-To: <OF7DFF3E86.43B6CBA7-ON85256F24.0070702B-85256F24.0072386C@sb.com>
Message-ID: <Pine.LNX.4.44.0410052203060.28516-100000@gannet.stats>

You are installing to `lib'.  You will only see packages you install there 
if that is in your R library path (set by R_LIBS), and neither will
R CMD INSTALL.

This is a question about BioConductor, so please use their mailing list, 
not the R ones.

On Tue, 5 Oct 2004 saran.2.vardhanabhuti at gsk.com wrote:

> Hello,
>         I am trying to install affy package in R as follow:
>         >R CMD INSTALL -l lib ~/rstuffs/affy_1.4.32.tar.gz
> 
>         Then I get an error at the end:
>         Warning message:
>         There is no package called 'Biobase' in: library(package, 
> character.only = TRUE, logical = TRUE, warn.conflicts = warn.conflicts,
>         [1] "ProgressBarText"
>         [1] "initialize"
>         [1] "open"
>         [1] "open"
>         [1] "update"
>         [1] "update"
>         [1] "close"
>         [1] "close"
>         Error in getClass(Class, where = topenv(parent.frame())) :
>                 "MIAME" is not a defined class
>         Execution halted
>         /bioinfo/apps/lib/R/bin/INSTALL: line 1:   761 Broken pipe     cat 
> "/home/sxv6413/rstuffs/lib/affy/R/affy"
>         ERROR: execution of package source for 'affy' failed
>         ** Removing '/home/sxv6413/rstuffs/lib/affy'
> 
>         I have also installed Biobase using the same command successfully 
> but when i go into R and type library() to see all the available packages, 
> I dont see Biobase package there.
> 
>             Am i missing something here?
>         Since I am using Bioconductor, I also try to install packages 
> using getBioC() but I still get errors like:
> 
>         Error in file(file, "r") unable to connect to 
> 'www.bioconductor.org' on port 80.
>  
>         If anyone has any advice on this, I will greatly appreciate.
> 
> Thanks,
> Saran
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
> 
> 

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From gotrout at gmail.com  Tue Oct  5 23:56:41 2004
From: gotrout at gmail.com (gotrout@gmail.com)
Date: Tue, 5 Oct 2004 14:56:41 -0700
Subject: [R] correct my method of estimating mean of two POSIXlt data
	frames
In-Reply-To: <Pine.LNX.4.44.0410052109020.25047-100000@gannet.stats>
References: <ae9f88190410051249685db1ae@mail.gmail.com>
	<Pine.LNX.4.44.0410052109020.25047-100000@gannet.stats>
Message-ID: <ae9f8819041005145648fa6943@mail.gmail.com>

My apology for not properly quoting someone.

Let me amend my original email.  In R version 1.8.1, if I have two
objects that look like:

> t.start.cdt[1:4]
[1] "2003-07-09 11:02:25" "2003-07-09 11:10:25" "2003-07-09 11:30:25"
[4] "2003-07-09 12:00:25"
> class(t.start.cdt)
[1] "POSIXt"  "POSIXlt"

> t.end.cdt[1:4]
[1] "2003-07-09 11:02:35" "2003-07-09 11:10:35" "2003-07-09 11:30:35"
[4] "2003-07-09 12:00:35"
> class(t.end.cdt)
[1] "POSIXt"  "POSIXlt"

I could estimate the mean of "pairs" of times (such as t.start.cdt[1]
and t.end.cdt[1]) using these commands:
tmp <- data.frame(t.start.cdt, t.end.cdt)
tmp <- apply(tmp, MARGIN=1, FUN=mean)
class(tmp) <- c("POSIXt", "POSIXct")
t.mean.cdt <- as.POSIXlt(tmp)

In version 1.9.1, I get these warnings
Warning messages:
1: longer object length
        is not a multiple of shorter object length in: cl == c("Date",
"POSIXct", "POSIXlt")
2: longer object length
        is not a multiple of shorter object length in: cl == c("Date",
"POSIXct", "POSIXlt")
3: argument is not numeric or logical: returning NA in:
mean.default(newX[, i], ...)
4: argument is not numeric or logical: returning NA in:
mean.default(newX[, i], ...)

I think I wrote a rather convoluted way around this problem, which I
stated in my original email, but I don't "understand" why the method
stated here doesn't work.  Can you explain what I'm missing.

Also, please note that in my original message, I used underscore
instead of period in the variable names.

Mike
Berkeley, California, USA



On Tue, 5 Oct 2004 21:13:05 +0100 (BST), Prof Brian Ripley
<ripley at stats.ox.ac.uk> wrote:
> On Tue, 5 Oct 2004 "gotrout at gmail.com" (but with no name nor signature)
> wrote:
> 
> Quoting someone without credit (and therefore in breach of their
> copyright)
> 
> 
> 
> > > If a.lt and b.lt are the two vectors of POSIXlt dates then try
> > > converting each to POSIXct and unclassing to make each numeric.
> > > Take the mean of the two numeric vectors and convert them back to
> >
> > I see.  I'm a little confused with the use of class/unclass versus
> > as.XX.  For example, instead of using unclass, why wouldn't I use
> > as.numeric?  Could someone explain the difference?
> 
> That advice is wrong: you should not be unclassing before forming the
> mean as mean() has a method for POSIXct.
> 
> --
> Brian D. Ripley,                  ripley at stats.ox.ac.uk
> Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
> University of Oxford,             Tel:  +44 1865 272861 (self)
> 1 South Parks Road,                     +44 1865 272866 (PA)
> Oxford OX1 3TG, UK                Fax:  +44 1865 272595
> 
>



From andrew.j.booker at boeing.com  Wed Oct  6 00:10:41 2004
From: andrew.j.booker at boeing.com (Booker, Andrew J)
Date: Tue, 5 Oct 2004 15:10:41 -0700
Subject: [R] Confounded data frame column names
Message-ID: <04088F154165F84E89F96113157E4B6108619DEF@xch-nw-11.nw.nos.boeing.com>

This is probably a know problem (problem for me anyway) in R but I
don't quite know what to search for in help archives. When I name a
column "x11" in a data frame R thinks a column named "x1" exists. In
my application I am trying to test for the existence of a column, then
add it if it's not there. Here is a simple example:

> temd <- data.frame(x11=c(0:10))
> is.null(temd[["x1"]])
[1] FALSE
> temd[["x1"]]
 [1]  0  1  2  3  4  5  6  7  8  9 10
> temd$x1
 [1]  0  1  2  3  4  5  6  7  8  9 10
> temd[,"x1"]
Error in "[.data.frame"(temd, , "x1") : undefined columns selected
>

This is in R 2.0.0.

Anyone know how to run (safely) through a list of names like "x1"-"x100" and check for data frame columns with the names?



----------------------------------------------------------- 
* Opinions herein are mine only *
----------------------------------------------------------- 

			Andrew Booker 	
			The Boeing Company
			P.O. Box 3707 MC 7L-22
			Seattle, WA  98124-2207
			425-865-3573
			FAX# 425-865-2966
			andrew.j.booker at boeing.com



From tlumley at u.washington.edu  Wed Oct  6 00:14:59 2004
From: tlumley at u.washington.edu (Thomas Lumley)
Date: Tue, 5 Oct 2004 15:14:59 -0700 (PDT)
Subject: [R] Nelson-Aalen estimator in R
In-Reply-To: <3F4024494055CA41BC8B3FE78B837422011E3005@medmail5.mednet.ucla.edu>
References: <3F4024494055CA41BC8B3FE78B837422011E3005@medmail5.mednet.ucla.edu>
Message-ID: <Pine.A41.4.61.0410051510230.226686@homer08.u.washington.edu>

On Tue, 5 Oct 2004, Wu, Tongtong wrote:

> Hi,
>
> I am taking a survival class.  Recently I need to do the Nelson-Aalen
> estimtor in R.  I searched through the R help manual and internet, but could
> not find such a R function.  I tried another way by calculating the
> Kaplan-Meier estimator and take -log(S).  However, the function only
> provides the summary of KM estimator but no estimated values.  Could you
> please help me with this?  I would highly appreciate your great help!
>

Taking -log(KM) does not give the Nelson-Aalen estimator, though it does 
give another consistent estimator of the cumulative hazard function.

The easiest way to get the Nelson-Aalen estimator is
   basehaz(coxph(Surv(time,status)~1,data=aml))
because the (Breslow) hazard estimator for a Cox model reduces to the 
Nelson-Aalen estimator when there are no covariates.

You can also compute it from information returned by survfit().

> fit <- survfit(Surv(time, status) ~ 1, data = aml)
> str(fit)
List of 12
  $ n        : int 23
  $ time     : num [1:18] 5 8 9 12 13 16 18 23 27 28 ...
  $ n.risk   : num [1:18] 23 21 19 18 17 15 14 13 11 10 ...
  $ n.event  : num [1:18] 2 2 1 1 1 0 1 2 1 0 ...
  $ surv     : num [1:18] 0.913 0.826 0.783 0.739 0.696 ...
  $ type     : chr "right"
  $ std.err  : num [1:18] 0.0643 0.0957 0.1099 0.1239 0.1379 ...
  $ upper    : num [1:18] 1.000 0.996 0.971 0.942 0.912 ...
  $ lower    : num [1:18] 0.805 0.685 0.631 0.580 0.531 ...
  $ conf.type: chr "log"
  $ conf.int : num 0.95
  $ call     : language survfit(formula = Surv(time, status) ~ 1, data = aml)
  - attr(*, "class")= chr "survfit"

so cumsum(fit$n.event/fit$n.risk) is the Nelson-Aalen estimator for the 
times given by fit$times.

If you want -log(S), then -log(fit$surv) will give it to you.


 	-thomas



From tlumley at u.washington.edu  Wed Oct  6 00:17:14 2004
From: tlumley at u.washington.edu (Thomas Lumley)
Date: Tue, 5 Oct 2004 15:17:14 -0700 (PDT)
Subject: [R] Confounded data frame column names
In-Reply-To: <04088F154165F84E89F96113157E4B6108619DEF@xch-nw-11.nw.nos.boeing.com>
References: <04088F154165F84E89F96113157E4B6108619DEF@xch-nw-11.nw.nos.boeing.com>
Message-ID: <Pine.A41.4.61.0410051516040.226686@homer08.u.washington.edu>

On Tue, 5 Oct 2004, Booker, Andrew J wrote:

> This is probably a know problem (problem for me anyway) in R but I
> don't quite know what to search for in help archives. When I name a
> column "x11" in a data frame R thinks a column named "x1" exists. In
> my application I am trying to test for the existence of a column, then
> add it if it's not there. Here is a simple example:
>
>> temd <- data.frame(x11=c(0:10))
>> is.null(temd[["x1"]])
> [1] FALSE

Yes. R allows partial matching in this and many other contexts.  You could 
use
   "x1" %in% names(temd)
to test for the name without partial matching (probably more efficiently 
if the data frame is large)

 	-thomas



From ggrothendieck at myway.com  Wed Oct  6 00:17:27 2004
From: ggrothendieck at myway.com (Gabor Grothendieck)
Date: Tue,  5 Oct 2004 18:17:27 -0400 (EDT)
Subject: [R] correct my method of estimating mean of two POSIXlt data
	frames
Message-ID: <20041005221727.C18BD12CFC@mprdmxin.myway.com>


 page took 0.31 seconds   home | my page | my email 
  .   
 
 
 
 

 email     
 
 
 

Mail Addresses Calendar Notepad ggrothendieck at myway.com sign out 
<< Hide Folders Check Messages Compose Message POP Accounts | Mail Preferences | Help 
 
 
Folders 
Inbox 
Drafts 
Sent 
Trash (Empty) 
Bulk Mail (Empty) 
 
 
 
My Folders edit 
R 
Rcom 
Rtmp 
Saved 
this 
Tx 
zoo 
 
 
 
Spam Tools info 
   Spam Filter Level:
 OffLowMediumMedium-highHigh 
My Block List 
Image Filter 
Custom Filters 
 
 
 
 
 
 
Now Live: 

 125MB Free Storage Upgrade  
 
 
 

 Send/Receive 10MB emails!  
 
 
 

            < Prev Next >   Back to Inbox Print View  Full Header 
 
    As AttachmentAs Inline Text Move to Folder----- Folders ------InboxDraftsSentTrashBulk Mail---- My Folders ----RRcomRtmpSavedthisTxzoo
 
 

 Message is not flagged. [ Flag for Follow Up ] 



If instead of a data frame of POSIXlt objects you use a matrix
of POSIXct objects then that should work.   That is, replace

   tmp <- data.frame(t.start.cdt, t.end.cdt)

with

   tmp <- cbind(t.start.cdt+0, t.end.cdt+0)

in your code.

Date:   Tue, 5 Oct 2004 14:56:41 -0700 
From:   <gotrout at gmail.com>
To:   <r-help at stat.math.ethz.ch> 
Subject:   Re: [R] correct my method of estimating mean of two POSIXlt data frames 

 
My apology for not properly quoting someone.

Let me amend my original email. In R version 1.8.1, if I have two
objects that look like:

> t.start.cdt[1:4]
[1] "2003-07-09 11:02:25" "2003-07-09 11:10:25" "2003-07-09 11:30:25"
[4] "2003-07-09 12:00:25"
> class(t.start.cdt)
[1] "POSIXt"  "POSIXlt"

> t.end.cdt[1:4]
[1] "2003-07-09 11:02:35" "2003-07-09 11:10:35" "2003-07-09 11:30:35"
[4] "2003-07-09 12:00:35"
> class(t.end.cdt)
[1] "POSIXt"  "POSIXlt"

I could estimate the mean of "pairs" of times (such as t.start.cdt[1]
and t.end.cdt[1]) using these commands:
tmp <- data.frame(t.start.cdt, t.end.cdt)
tmp <- apply(tmp, MARGIN=1, FUN=mean)
class(tmp) <- c("POSIXt", "POSIXct")
t.mean.cdt <- as.POSIXlt(tmp)

In version 1.9.1, I get these warnings
Warning messages:
1: longer object length
is not a multiple of shorter object length in: cl == c("Date",
"POSIXct", "POSIXlt")
2: longer object length
is not a multiple of shorter object length in: cl == c("Date",
"POSIXct", "POSIXlt")
3: argument is not numeric or logical: returning NA in:
mean.default(newX[, i], ...)
4: argument is not numeric or logical: returning NA in:
mean.default(newX[, i], ...)

I think I wrote a rather convoluted way around this problem, which I
stated in my original email, but I don't "understand" why the method
stated here doesn't work. Can you explain what I'm missing.

Also, please note that in my original message, I used underscore
instead of period in the variable names.

Mike
Berkeley, California, USA



On Tue, 5 Oct 2004 21:13:05 +0100 (BST), Prof Brian Ripley
<ripley at stats.ox.ac.uk> wrote:
> On Tue, 5 Oct 2004 "gotrout at gmail.com" (but with no name nor signature)
> wrote:
> 
> Quoting someone without credit (and therefore in breach of their
> copyright)
> 
> 
> 
> > > If a.lt and b.lt are the two vectors of POSIXlt dates then try
> > > converting each to POSIXct and unclassing to make each numeric.
> > > Take the mean of the two numeric vectors and convert them back to
> >
> > I see. I'm a little confused with the use of class/unclass versus
> > as.XX. For example, instead of using unclass, why wouldn't I use
> > as.numeric? Could someone explain the difference?
> 
> That advice is wrong: you should not be unclassing before forming the
> mean as mean() has a method for POSIXct.



From p.dalgaard at biostat.ku.dk  Wed Oct  6 00:21:42 2004
From: p.dalgaard at biostat.ku.dk (Peter Dalgaard)
Date: 06 Oct 2004 00:21:42 +0200
Subject: [R] Nelson-Aalen estimator in R
In-Reply-To: <3F4024494055CA41BC8B3FE78B837422011E3005@medmail5.mednet.ucla.edu>
References: <3F4024494055CA41BC8B3FE78B837422011E3005@medmail5.mednet.ucla.edu>
Message-ID: <x2mzz0u7pl.fsf@biostat.ku.dk>

"Wu, Tongtong" <TTWu at mednet.ucla.edu> writes:

> I am taking a survival class.  Recently I need to do the Nelson-Aalen
> estimtor in R.  I searched through the R help manual and internet, but could
> not find such a R function.  I tried another way by calculating the
> Kaplan-Meier estimator and take -log(S).  However, the function only
> provides the summary of KM estimator but no estimated values.  Could you
> please help me with this?  I would highly appreciate your great help!

It's the log of the Fleming-Harrington estimator that equals
Nelson-Aalen. However, the main thing is that the survival package is
peculiar in that it does such transformations as part of
plot.survfit(). 

If you want the actual values of the N-A estimator, I don't think
there's any other way besides extracting the survival curve values from
the survfit object and taking logs. 

So: you should study the help pages for plot.survfit and
survfit.object.

-- 
   O__  ---- Peter Dalgaard             Blegdamsvej 3  
  c/ /'_ --- Dept. of Biostatistics     2200 Cph. N   
 (*) \(*) -- University of Copenhagen   Denmark      Ph: (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)             FAX: (+45) 35327907



From spencer.graves at pdf.com  Wed Oct  6 00:31:31 2004
From: spencer.graves at pdf.com (Spencer Graves)
Date: Tue, 05 Oct 2004 15:31:31 -0700
Subject: [R] Confounded data frame column names
In-Reply-To: <04088F154165F84E89F96113157E4B6108619DEF@xch-nw-11.nw.nos.boeing.com>
References: <04088F154165F84E89F96113157E4B6108619DEF@xch-nw-11.nw.nos.boeing.com>
Message-ID: <416320C3.9020401@pdf.com>

How about: 

 > DF <- data.frame(x11=1)
 > c("x1", "x11") %in% names(DF)
[1] FALSE  TRUE

hope this helps.  spencer graves

Booker, Andrew J wrote:

>This is probably a know problem (problem for me anyway) in R but I
>don't quite know what to search for in help archives. When I name a
>column "x11" in a data frame R thinks a column named "x1" exists. In
>my application I am trying to test for the existence of a column, then
>add it if it's not there. Here is a simple example:
>
>  
>
>>temd <- data.frame(x11=c(0:10))
>>is.null(temd[["x1"]])
>>    
>>
>[1] FALSE
>  
>
>>temd[["x1"]]
>>    
>>
> [1]  0  1  2  3  4  5  6  7  8  9 10
>  
>
>>temd$x1
>>    
>>
> [1]  0  1  2  3  4  5  6  7  8  9 10
>  
>
>>temd[,"x1"]
>>    
>>
>Error in "[.data.frame"(temd, , "x1") : undefined columns selected
>  
>
>
>This is in R 2.0.0.
>
>Anyone know how to run (safely) through a list of names like "x1"-"x100" and check for data frame columns with the names?
>
>
>
>----------------------------------------------------------- 
>* Opinions herein are mine only *
>----------------------------------------------------------- 
>
>			Andrew Booker 	
>			The Boeing Company
>			P.O. Box 3707 MC 7L-22
>			Seattle, WA  98124-2207
>			425-865-3573
>			FAX# 425-865-2966
>			andrew.j.booker at boeing.com
>
>______________________________________________
>R-help at stat.math.ethz.ch mailing list
>https://stat.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>  
>

-- 
Spencer Graves, PhD, Senior Development Engineer
O:  (408)938-4420;  mobile:  (408)655-4567



From pioclaudio at yahoo.com  Wed Oct  6 04:42:39 2004
From: pioclaudio at yahoo.com (pio claudio)
Date: Tue, 5 Oct 2004 19:42:39 -0700 (PDT)
Subject: [R] How to get F Distribution values in R
Message-ID: <20041006024239.78831.qmail@web61107.mail.yahoo.com>

i tried to use df(x,df1,df2) but the values arent the
same when i looked it up in a F Distribution table..

How to get the same F Distribution values in R as in
the f table?



From kjetil at acelerate.com  Wed Oct  6 04:49:55 2004
From: kjetil at acelerate.com (Kjetil Brinchmann Halvorsen)
Date: Tue, 05 Oct 2004 22:49:55 -0400
Subject: [R] How to get F Distribution values in R
In-Reply-To: <20041006024239.78831.qmail@web61107.mail.yahoo.com>
References: <20041006024239.78831.qmail@web61107.mail.yahoo.com>
Message-ID: <41635D53.7030904@acelerate.com>

pio claudio wrote:

>i tried to use df(x,df1,df2) but the values arent the
>same when i looked it up in a F Distribution table..
>
>How to get the same F Distribution values in R as in
>the f table?
>
>______________________________________________
>R-help at stat.math.ethz.ch mailing list
>https://stat.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>
>
>  
>
You looked up the F density (d in df is density) Try pf, which gives 
cumulative probabilities

-- 

Kjetil Halvorsen.

Peace is the most effective weapon of mass construction.
               --  Mahdi Elmandjra



From andy_liaw at merck.com  Wed Oct  6 04:50:19 2004
From: andy_liaw at merck.com (Liaw, Andy)
Date: Tue, 5 Oct 2004 22:50:19 -0400
Subject: [R] How to get F Distribution values in R
Message-ID: <3A822319EB35174CA3714066D590DCD504AF84D4@usrymx25.merck.com>

Have you looked up what df() computes?  What did you think the `d' in df()
means?

Andy

> From: pio claudio
> 
> i tried to use df(x,df1,df2) but the values arent the
> same when i looked it up in a F Distribution table..
> 
> How to get the same F Distribution values in R as in
> the f table?
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
> 
>



From pioclaudio at yahoo.com  Wed Oct  6 05:12:26 2004
From: pioclaudio at yahoo.com (pio claudio)
Date: Tue, 5 Oct 2004 20:12:26 -0700 (PDT)
Subject: [R] How to get F Distribution values in R 
Message-ID: <20041006031226.28091.qmail@web61108.mail.yahoo.com>

is x in df(x,df1,df2) the significance level? the
alpha?


alpha .05
    \v1    1       2       3
   v2
  1      161.448 199.500 215.707
  2       18.513  19.000  19.164
  3       10.128   9.552   9.277
this is a portion of the F distribution table..

sorry, but i still cant figure out how to use df()(or
pf())...

thanks


pio claudio wrote:

>i tried to use df(x,df1,df2) but the values arent the
>same when i looked it up in a F Distribution table..
>
>How to get the same F Distribution values in R as in
>the f table?
>
>______________________________________________
>R-help at stat.math.ethz.ch mailing list
>https://stat.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide! 
http://www.R-project.org/posting-guide.html
>
>
>  
>
You looked up the F density (d in df is density) Try
pf, which gives 
cumulative probabilities

-- 

Kjetil Halvorsen.




		
_______________________________

Declare Yourself - Register online to vote today!



From andy_liaw at merck.com  Wed Oct  6 05:28:58 2004
From: andy_liaw at merck.com (Liaw, Andy)
Date: Tue, 5 Oct 2004 23:28:58 -0400
Subject: [R] How to get F Distribution values in R
Message-ID: <3A822319EB35174CA3714066D590DCD504AF84D5@usrymx25.merck.com>

> outer(1:3, 1:3, function(df1, df2) qf(0.95, df1, df2))
         [,1]     [,2]      [,3]
[1,] 161.4476 18.51282 10.127964
[2,] 199.5000 19.00000  9.552094
[3,] 215.7073 19.16429  9.276628

Andy

> From: pio claudio
> 
> is x in df(x,df1,df2) the significance level? the
> alpha?
> 
> 
> alpha .05
>     \v1    1       2       3
>    v2
>   1      161.448 199.500 215.707
>   2       18.513  19.000  19.164
>   3       10.128   9.552   9.277
> this is a portion of the F distribution table..
> 
> sorry, but i still cant figure out how to use df()(or
> pf())...
> 
> thanks
> 
> 
> pio claudio wrote:
> 
> >i tried to use df(x,df1,df2) but the values arent the
> >same when i looked it up in a F Distribution table..
> >
> >How to get the same F Distribution values in R as in
> >the f table?
> >
> >______________________________________________
> >R-help at stat.math.ethz.ch mailing list
> >https://stat.ethz.ch/mailman/listinfo/r-help
> >PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
> >
> >
> >  
> >
> You looked up the F density (d in df is density) Try
> pf, which gives 
> cumulative probabilities
> 
> -- 
> 
> Kjetil Halvorsen.
> 
> 
> 
> 
> 		
> _______________________________
> 
> Declare Yourself - Register online to vote today!
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
> 
>



From ggrothendieck at myway.com  Wed Oct  6 06:51:32 2004
From: ggrothendieck at myway.com (Gabor Grothendieck)
Date: Wed, 6 Oct 2004 04:51:32 +0000 (UTC)
Subject: [R] correct my method of estimating mean of two POSIXlt
	=?utf-8?b?ZGF0YQlmcmFtZXM=?=
References: <ae9f88190410051249685db1ae@mail.gmail.com>
	<Pine.LNX.4.44.0410052109020.25047-100000@gannet.stats>
Message-ID: <loom.20041006T064536-480@post.gmane.org>


Prof Brian Ripley <ripley at stats.ox.ac.uk> writes:

: > > If a.lt and b.lt are the two vectors of POSIXlt dates then try
: > > converting each to POSIXct and unclassing to make each numeric.
: > > Take the mean of the two numeric vectors and convert them back to
: >
: > I see. I'm a little confused with the use of class/unclass versus
: > as.XX. For example, instead of using unclass, why wouldn't I use
: > as.numeric? Could someone explain the difference?
: 
: That advice is wrong: you should not be unclassing before forming the
: mean as mean() has a method for POSIXct.

You did not quote the code I posted but, actually, it gives
the correct answer.  For example, with the poster's data:

R> a.lt <- as.POSIXlt(c("2003-07-09 11:02:25", "2003-07-09 11:10:25", 
+                   "2003-07-09 11:30:25", "2003-07-09 12:00:25"))
R> b.lt <- as.POSIXlt(c("2003-07-09 11:02:35", "2003-07-09 11:10:35", 
+                   "2003-07-09 11:30:35", "2003-07-09 12:00:35"))

R> # code and the answer it gives:

R> a <- unclass(as.POSIXct(a.lt))
R> b <- unclass(as.POSIXct(b.lt))
R> as.POSIXlt(structure((a+b)/2, class = c("POSIXt", "POSIXct")))
[1] "2003-07-09 11:02:30 Eastern Daylight Time"
[2] "2003-07-09 11:10:30 Eastern Daylight Time"
[3] "2003-07-09 11:30:30 Eastern Daylight Time"
[4] "2003-07-09 12:00:30 Eastern Daylight Time"

Also, mean.POSIXlt calls mean.POSIXct which in turn unclasses its
argument so the data ultimately gets unclassed along the way one
way or another even if one uses mean.

Perhaps you are referring to the fact that unclass violates
encapsulation in which case I agree that as.numeric is 
better conceptually though I would add the caveat that if one
is really going to be sticky about encapsulation then R's
object model is probably not the one for you.

Anyways, it did occur to me, as an afterthought, that one could
avoid the structure part of the above solution, at the expense
of a slight obsfucation, by using the fact that for any two
real numbers (a+b)/2 = (a-b)/2 + b so:

R> a <- as.numeric(a.lt+0)
R> b <- as.numeric(b.lt+0)
R> (a-b)/2 + b.lt
[1] "2003-07-09 11:02:30 Eastern Daylight Time"
[2] "2003-07-09 11:10:30 Eastern Daylight Time"
[3] "2003-07-09 11:30:30 Eastern Daylight Time"
[4] "2003-07-09 12:00:30 Eastern Daylight Time"



From vikas at mail.jnu.ac.in  Wed Oct  6 07:14:05 2004
From: vikas at mail.jnu.ac.in (Vikas Rawal)
Date: Wed, 06 Oct 2004 10:44:05 +0530
Subject: [R] Problems with merge
Message-ID: <41637F1D.20506@mail.jnu.ac.in>

This issue has been discussed on this list before but the solutions 
offerred are not satisfactory. So I thought I shall raise it again.

I want to merge two datasets which have three common variables. These 
variables DO NOT have the same names in both the files. In addition, 
there are two variables with same name which do not necessarily have 
exactly same data. That is, there could be some discrepancy between the 
two datasets when it comes to these variables. I do not want them to be 
used when I merge the datasets.

The problem is that R allows you to use by.x and by.y variables to 
specify only one variable in x dataset and one variable in y dataset to 
merge. Otherwise, if you do not specify anything, it matches all the 
variables that have common names to merge. This is very problemmatic. In 
my case, the variables I want to use to match do not have same names in 
two datasets and the ones that have same names must not be used to match.

One approach will be to change names of variables and then merge. But 
that is not elegant, to say the least.

If nothing else works, that is what I shall have to do. There again we 
have some problem. How do I change the name of a particular column. One 
solution suggested somewhere in the archives of the list is to use

names(data.frame)=c(list of column names)

But this requires you to list all the variable names. That can obviously 
be cumbersome when you have large number of variables. What would be the 
syntax if I want to change just one column name.

Vikas



From petr.pikal at precheza.cz  Wed Oct  6 08:18:21 2004
From: petr.pikal at precheza.cz (Petr Pikal)
Date: Wed, 06 Oct 2004 08:18:21 +0200
Subject: [R] R 2.0.0 is released
In-Reply-To: <1096980420.12012.1.camel@localhost.localdomain>
References: <x2vfdp8mxi.fsf@biostat.ku.dk>
Message-ID: <4163AA4D.17069.1F5D6B@localhost>



On 5 Oct 2004 at 8:15, Marc Schwartz wrote:

> On Tue, 2004-10-05 at 05:43, Peter Dalgaard wrote: 
> > Barry Rowlingson <B.Rowlingson at lancaster.ac.uk> writes:
> > 
> > > Gabor Grothendieck wrote:
> > > >
> > > > Congratulations to the R team and all involved for reaching the
> > > > 2.0.0 milestone.  The progress of R is truly astounding.
> > > >
> > > 
> > >   A milestone is something that tells you how far it is to where
> > >   you
> > > are going. With R-2.0.0, have we arrived?
> > 
> > Depending on direction, milestones might only be telling you how far
> > you've gone. And you may even be walking in circles around the
> > target/origin...
> > 
> > No, I don't think we have arrived (would we want to?). We still have
> > unresolved issues in byte compiling and event loops, and the formal
> > methods techniques are only just there now. There's the whole
> > Bayesian front, too - important even for us hardcore frequentists. 
> > 
> > Besides, how do you arrive at a moving target?
> 
> 
> Well, as some would say, it's not the destination, it's the journey.
> :-)
> 
> Nevertheless, a hearty congratulations to R Core!
> 
> Marc

And a very interesting and inspiring one (journey :-).

My best wishes to continuing prosperous and good fortune for R 
and R Core, too.

Petr


> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide!
> http://www.R-project.org/posting-guide.html

Petr Pikal
petr.pikal at precheza.cz



From Matthias.Templ at statistik.gv.at  Wed Oct  6 08:31:09 2004
From: Matthias.Templ at statistik.gv.at (TEMPL Matthias)
Date: Wed, 6 Oct 2004 08:31:09 +0200
Subject: [R] Problems with merge
Message-ID: <83536658864BC243BE3C06D7E936ABD50153A68B@xchg1.statistik.gv.at>

Hello,

You can change e.g. the second column name in the following way:

data(iris)
colnames(iris)
[1] "Sepal.Length" "Sepal.Width"  "Petal.Length" "Petal.Width"
"Species"   

To change the second column name:

colnames(iris)[2] <- "name"
colnames(iris)
[1] "Sepal.Length" "name"         "Petal.Length" "Petal.Width"
"Species"  


Best,
Matthias

> 
> 
> This issue has been discussed on this list before but the solutions 
> offerred are not satisfactory. So I thought I shall raise it again.
> 
> I want to merge two datasets which have three common variables. These 
> variables DO NOT have the same names in both the files. In addition, 
> there are two variables with same name which do not necessarily have 
> exactly same data. That is, there could be some discrepancy 
> between the 
> two datasets when it comes to these variables. I do not want 
> them to be 
> used when I merge the datasets.
> 
> The problem is that R allows you to use by.x and by.y variables to 
> specify only one variable in x dataset and one variable in y 
> dataset to 
> merge. Otherwise, if you do not specify anything, it matches all the 
> variables that have common names to merge. This is very 
> problemmatic. In 
> my case, the variables I want to use to match do not have 
> same names in 
> two datasets and the ones that have same names must not be 
> used to match.
> 
> One approach will be to change names of variables and then merge. But 
> that is not elegant, to say the least.
> 
> If nothing else works, that is what I shall have to do. There 
> again we 
> have some problem. How do I change the name of a particular 
> column. One 
> solution suggested somewhere in the archives of the list is to use
> 
> names(data.frame)=c(list of column names)
> 
> But this requires you to list all the variable names. That 
> can obviously 
> be cumbersome when you have large number of variables. What 
> would be the 
> syntax if I want to change just one column name.
> 
> Vikas
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list 
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read 
> the posting guide! http://www.R-project.org/posting-guide.html
>



From ligges at statistik.uni-dortmund.de  Wed Oct  6 08:59:23 2004
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Wed, 06 Oct 2004 08:59:23 +0200
Subject: [R] Confounded data frame column names
In-Reply-To: <04088F154165F84E89F96113157E4B6108619DEF@xch-nw-11.nw.nos.boeing.com>
References: <04088F154165F84E89F96113157E4B6108619DEF@xch-nw-11.nw.nos.boeing.com>
Message-ID: <416397CB.5090003@statistik.uni-dortmund.de>

Booker, Andrew J wrote:

> This is probably a know problem (problem for me anyway) in R but I
> don't quite know what to search for in help archives. When I name a
> column "x11" in a data frame R thinks a column named "x1" exists. 

Not exactly. R uses partial matching here. Iff "x11" and "x1" exists, 
"x11" will be selected. Pretty much like matching for arguments in 
functions.

Probably you are looking for names(temd)?

Uwe Ligges


 > In
> my application I am trying to test for the existence of a column, then
> add it if it's not there. Here is a simple example:
> 
> 
>>temd <- data.frame(x11=c(0:10))
>>is.null(temd[["x1"]])
> 
> [1] FALSE
> 
>>temd[["x1"]]
> 
>  [1]  0  1  2  3  4  5  6  7  8  9 10
> 
>>temd$x1
> 
>  [1]  0  1  2  3  4  5  6  7  8  9 10
> 
>>temd[,"x1"]
> 
> Error in "[.data.frame"(temd, , "x1") : undefined columns selected
> 
> 
> This is in R 2.0.0.
> 
> Anyone know how to run (safely) through a list of names like "x1"-"x100" and check for data frame columns with the names?
> 
> 
> 
> ----------------------------------------------------------- 
> * Opinions herein are mine only *
> ----------------------------------------------------------- 
> 
> 			Andrew Booker 	
> 			The Boeing Company
> 			P.O. Box 3707 MC 7L-22
> 			Seattle, WA  98124-2207
> 			425-865-3573
> 			FAX# 425-865-2966
> 			andrew.j.booker at boeing.com
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html



From rksh at soc.soton.ac.uk  Wed Oct  6 10:09:04 2004
From: rksh at soc.soton.ac.uk (Robin Hankin)
Date: Wed, 6 Oct 2004 09:09:04 +0100
Subject: [R] crossprod vs %*% timing
Message-ID: <a06002001bd895845800d@[139.166.242.29]>

Hi

the manpage says that crossprod(x,y) is formally equivalent to, but
faster than, the call 't(x) %*% y'.

I have a vector 'a' and a matrix 'A', and need to evaluate 't(a) %*% A
%*% a' many many times, and performance is becoming crucial.  With

f1 <- function(a,X){ ignore <- t(a) %*% X %*% a               }
f2 <- function(a,X){ ignore <- crossprod(t(crossprod(a,X)),a) }
f3 <- function(a,X){ ignore <- crossprod(a,X) %*% a           }

a <- rnorm(100)
X <- matrix(rnorm(10000),100,100)

print(system.time( for(i in 1:10000){ f1(a,X)}))
print(system.time( for(i in 1:10000){ f2(a,X)}))
print(system.time( for(i in 1:10000){ f3(a,X)}))


I get something like:

[1] 2.68 0.05 2.66 0.00 0.00
[1] 0.48 0.00 0.49 0.00 0.00
[1] 0.29 0.00 0.31 0.00 0.00

with quite low variability from run to run.  What surprises me is the
third figure: about 40% faster than the second one, the extra time
possibly related to the call to t() (and Rprof shows about 35% of
total time in t() for my application).

So it looks like f3() is the winner hands down, at least for this
task.  What is a good way of thinking about such issues?  Anyone got
any performance tips?

I quite often need things like 'a %*% X %*% t(Y) %*% Z %*% t(b)' which
would be something like
crossprod(t(crossprod(t(crossprod(t(crossprod(a,X)),t(Y))),Z)),t(b))
(I think).

(R-1.9.1, 2GHz G5 PowerPC, MacOSX10.3.5)

-- 
Robin Hankin
Uncertainty Analyst
Southampton Oceanography Centre
SO14 3ZH
tel +44(0)23-8059-7743
initialDOTsurname at soc.soton.ac.uk (edit in obvious way; spam precaution)



From dimitris.rizopoulos at med.kuleuven.ac.be  Wed Oct  6 10:52:26 2004
From: dimitris.rizopoulos at med.kuleuven.ac.be (Dimitris Rizopoulos)
Date: Wed, 6 Oct 2004 10:52:26 +0200
Subject: [R] crossprod vs %*% timing
References: <a06002001bd895845800d@[139.166.242.29]>
Message-ID: <000c01c4ab81$ce6ae7a0$b2133a86@www.domain>

Hi Robin,

I some cases you could benefit from special features of the matrices 
at hand (I don't know if this is applicable in your case). For 
instance, in Bayesian computations I often face the quadratic form 
t(y)%*%solve(Sigma)%*%y, where Sigma is a covariance matrix. In this 
case you could gain by using the positive definiteness of Sigma i.e.,

library(MASS)
y <- rnorm(100)
Sigma <- var(mvrnorm(1000, rep(0,100), diag(100)))
###
system.time( for(i in 1:1000) c(t(y)%*%solve(Sigma)%*%y) )
system.time( for(i in 1:1000) c(crossprod(y, solve(Sigma))%*%y) )
system.time( for(i in 1:1000) c(t(y)%*%solve(Sigma, y)) )
system.time( for(i in 1:1000) quadform(y, Sigma) )

where

quadform <- function(y, Sigma){
    # Warning: The code does not check for symmetry of Sigma
    stopifnot(is.vector(y), length(y)==nrow(Sigma))
    chol.Sigma <- chol(Sigma)
    x <- forwardsolve(t(chol.Sigma), y)
    sum(x*x)
}

I hope this helps.

Best,
Dimitris

----
Dimitris Rizopoulos
Ph.D. Student
Biostatistical Centre
School of Public Health
Catholic University of Leuven

Address: Kapucijnenvoer 35, Leuven, Belgium
Tel: +32/16/396887
Fax: +32/16/337015
Web: http://www.med.kuleuven.ac.be/biostat/
     http://www.student.kuleuven.ac.be/~m0390867/dimitris.htm



----- Original Message ----- 
From: "Robin Hankin" <rksh at soc.soton.ac.uk>
To: <R-help at stat.math.ethz.ch>
Sent: Wednesday, October 06, 2004 10:09 AM
Subject: [R] crossprod vs %*% timing


> Hi
>
> the manpage says that crossprod(x,y) is formally equivalent to, but
> faster than, the call 't(x) %*% y'.
>
> I have a vector 'a' and a matrix 'A', and need to evaluate 't(a) %*% 
> A
> %*% a' many many times, and performance is becoming crucial.  With
>
> f1 <- function(a,X){ ignore <- t(a) %*% X %*% a               }
> f2 <- function(a,X){ ignore <- crossprod(t(crossprod(a,X)),a) }
> f3 <- function(a,X){ ignore <- crossprod(a,X) %*% a           }
>
> a <- rnorm(100)
> X <- matrix(rnorm(10000),100,100)
>
> print(system.time( for(i in 1:10000){ f1(a,X)}))
> print(system.time( for(i in 1:10000){ f2(a,X)}))
> print(system.time( for(i in 1:10000){ f3(a,X)}))
>
>
> I get something like:
>
> [1] 2.68 0.05 2.66 0.00 0.00
> [1] 0.48 0.00 0.49 0.00 0.00
> [1] 0.29 0.00 0.31 0.00 0.00
>
> with quite low variability from run to run.  What surprises me is 
> the
> third figure: about 40% faster than the second one, the extra time
> possibly related to the call to t() (and Rprof shows about 35% of
> total time in t() for my application).
>
> So it looks like f3() is the winner hands down, at least for this
> task.  What is a good way of thinking about such issues?  Anyone got
> any performance tips?
>
> I quite often need things like 'a %*% X %*% t(Y) %*% Z %*% t(b)' 
> which
> would be something like
> crossprod(t(crossprod(t(crossprod(t(crossprod(a,X)),t(Y))),Z)),t(b))
> (I think).
>
> (R-1.9.1, 2GHz G5 PowerPC, MacOSX10.3.5)
>
> -- 
> Robin Hankin
> Uncertainty Analyst
> Southampton Oceanography Centre
> SO14 3ZH
> tel +44(0)23-8059-7743
> initialDOTsurname at soc.soton.ac.uk (edit in obvious way; spam 
> precaution)
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
>
>



From ripley at stats.ox.ac.uk  Wed Oct  6 11:01:34 2004
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Wed, 6 Oct 2004 10:01:34 +0100 (BST)
Subject: [R] crossprod vs %*% timing
In-Reply-To: <a06002001bd895845800d@[139.166.242.29]>
Message-ID: <Pine.LNX.4.44.0410060955110.2689-100000@gannet.stats>

t(a) %*% A %*% a is a quadratic form.  What varies `many many times'? If A
does not vary (often), you want to find B with B'B = A (e.g. via chol,
possibly after symmetrizing A) and the squared length of Ba.

Doing the calculations with compiled code calling LAPACK (and making use
of a decent BLAS) would save a lot of overhead.

On Wed, 6 Oct 2004, Robin Hankin wrote:

> Hi
> 
> the manpage says that crossprod(x,y) is formally equivalent to, but
> faster than, the call 't(x) %*% y'.
> 
> I have a vector 'a' and a matrix 'A', and need to evaluate 't(a) %*% A
> %*% a' many many times, and performance is becoming crucial.  With
> 
> f1 <- function(a,X){ ignore <- t(a) %*% X %*% a               }
> f2 <- function(a,X){ ignore <- crossprod(t(crossprod(a,X)),a) }
> f3 <- function(a,X){ ignore <- crossprod(a,X) %*% a           }
> 
> a <- rnorm(100)
> X <- matrix(rnorm(10000),100,100)
> 
> print(system.time( for(i in 1:10000){ f1(a,X)}))
> print(system.time( for(i in 1:10000){ f2(a,X)}))
> print(system.time( for(i in 1:10000){ f3(a,X)}))
> 
> 
> I get something like:
> 
> [1] 2.68 0.05 2.66 0.00 0.00
> [1] 0.48 0.00 0.49 0.00 0.00
> [1] 0.29 0.00 0.31 0.00 0.00
> 
> with quite low variability from run to run.  What surprises me is the
> third figure: about 40% faster than the second one, the extra time
> possibly related to the call to t() (and Rprof shows about 35% of
> total time in t() for my application).
> 
> So it looks like f3() is the winner hands down, at least for this
> task.  What is a good way of thinking about such issues?  Anyone got
> any performance tips?
> 
> I quite often need things like 'a %*% X %*% t(Y) %*% Z %*% t(b)' which
> would be something like
> crossprod(t(crossprod(t(crossprod(t(crossprod(a,X)),t(Y))),Z)),t(b))
> (I think).
> 
> (R-1.9.1, 2GHz G5 PowerPC, MacOSX10.3.5)
> 
> 

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From karlknoblich at yahoo.de  Wed Oct  6 11:12:56 2004
From: karlknoblich at yahoo.de (Karl Knoblick)
Date: Wed, 6 Oct 2004 11:12:56 +0200 (CEST)
Subject: [R] Problems with merge
Message-ID: <20041006091256.70064.qmail@web52509.mail.yahoo.com>

Hello!

merge(TablePatient, TableSpecial, by.x="ID",
by.y="PATIENTID")

works fine for me. (There is also a variable ID in
TableSpecial).

One problem - or what has to be known - is that merge
is using the levels, not the labels, if the merged
variables are factors.

Karl



From s-plus at wiwi.uni-bielefeld.de  Wed Oct  6 11:37:38 2004
From: s-plus at wiwi.uni-bielefeld.de (Peter Wolf)
Date: Wed, 06 Oct 2004 11:37:38 +0200
Subject: [R] crossprod vs %*% timing
References: <a06002001bd895845800d@[139.166.242.29]>
Message-ID: <4163BCE2.8050903@wiwi.uni-bielefeld.de>

You can study that the order of the operation has an effect on
the times of the computations.

<<*>>=
f1 <- function(a,X){ ignore <- t(a) %*% X %*% a               }
f2 <- function(a,X){ ignore <- crossprod(t(crossprod(a,X)),a) }
f3 <- function(a,X){ ignore <- crossprod(a,X) %*% a           }
f4 <- function(a,X){ ignore <- (t(a) %*% X) %*% a               }
f5 <- function(a,X){ ignore <- t(a) %*% (X %*% a)               }
f6 <- function(a,X){ ignore <- crossprod(a,crossprod(X,a)) }

a <- rnorm(100); X <- matrix(rnorm(10000),100,100)

print(system.time( for(i in 1:10000){ a1<-f1(a,X)}))
print(system.time( for(i in 1:10000){ a2<-f2(a,X)}))
print(system.time( for(i in 1:10000){ a3<-f3(a,X)}))
print(system.time( for(i in 1:10000){ a4<-f4(a,X)}))
print(system.time( for(i in 1:10000){ a5<-f5(a,X)}))
print(system.time( for(i in 1:10000){ a6<-f6(a,X)}))
c(a1,a2,a3,a4,a5,a6)


@
output-start
[1] 4.06 0.04 4.11 0.00 0.00
[1] 1.48 0.00 1.53 0.00 0.00
[1] 1.17 0.00 1.22 0.00 0.00
[1] 4.10 0.01 4.39 0.00 0.00
[1] 2.58 0.01 3.24 0.00 0.00
[1] 1.10 0.00 1.29 0.00 0.00
Wed Oct  6 11:26:38 2004
[1] -79.34809 -79.34809 -79.34809 -79.34809 -79.34809 -79.34809
output-end

Peter Wolf



Robin Hankin wrote:

> Hi
>
> the manpage says that crossprod(x,y) is formally equivalent to, but
> faster than, the call 't(x) %*% y'.
>
> I have a vector 'a' and a matrix 'A', and need to evaluate 't(a) %*% A
> %*% a' many many times, and performance is becoming crucial.  With
>
> f1 <- function(a,X){ ignore <- t(a) %*% X %*% a               }
> f2 <- function(a,X){ ignore <- crossprod(t(crossprod(a,X)),a) }
> f3 <- function(a,X){ ignore <- crossprod(a,X) %*% a           }
>
> a <- rnorm(100)
> X <- matrix(rnorm(10000),100,100)
>
> print(system.time( for(i in 1:10000){ f1(a,X)}))
> print(system.time( for(i in 1:10000){ f2(a,X)}))
> print(system.time( for(i in 1:10000){ f3(a,X)}))
>
>
> I get something like:
>
> [1] 2.68 0.05 2.66 0.00 0.00
> [1] 0.48 0.00 0.49 0.00 0.00
> [1] 0.29 0.00 0.31 0.00 0.00
>
> with quite low variability from run to run.  What surprises me is the
> third figure: about 40% faster than the second one, the extra time
> possibly related to the call to t() (and Rprof shows about 35% of
> total time in t() for my application).
>
> So it looks like f3() is the winner hands down, at least for this
> task.  What is a good way of thinking about such issues?  Anyone got
> any performance tips?
>
> I quite often need things like 'a %*% X %*% t(Y) %*% Z %*% t(b)' which
> would be something like
> crossprod(t(crossprod(t(crossprod(t(crossprod(a,X)),t(Y))),Z)),t(b))
> (I think).
>
> (R-1.9.1, 2GHz G5 PowerPC, MacOSX10.3.5)
>



From mdowle at concordiafunds.com  Wed Oct  6 12:10:29 2004
From: mdowle at concordiafunds.com (Matthew Dowle)
Date: Wed, 6 Oct 2004 11:10:29 +0100 
Subject: [R] RExcel : problem with error handler?
Message-ID: <78166BFC5165D811AA0400065BF0324B314969@wisconsin.concordia>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20041006/cf9ce1dd/attachment.pl

From ernesto at ipimar.pt  Wed Oct  6 11:49:55 2004
From: ernesto at ipimar.pt (Ernesto Jardim)
Date: Wed, 06 Oct 2004 10:49:55 +0100
Subject: [R] R 2.0.0 is released
In-Reply-To: <x2oejipkm4.fsf@biostat.ku.dk>
References: <x2oejipkm4.fsf@biostat.ku.dk>
Message-ID: <1097056195.3580.24.camel@mordor.ipimar.pt>

On Mon, 2004-10-04 at 16:29, Peter Dalgaard wrote:
> I've rolled up R-2.0.0.tar.gz a short while ago. This is a new version
> with a number of new features. See below for the details.
> 
> As was the case with R 1.0.0, this new version represents a coming of
> age more than a radical change to R. We do plan to celebrate the new
> major version with press releases and such.
> 
> The release will be available from 
> 
> http://cran.r-project.org/src/base/R-2/R-2.0.0.tar.gz
> 
> or you might wait for it to be mirrored at a CRAN site nearer to you.
> Binaries for various platforms will appear in due course.
>  
> There is also a version split for floppies.
> 
> 
>         For the R Core Team
>         Peter Dalgaard
> 

Congratulations,

Very good work, R is "alive and kicking" ;-)

EJ



From K.M.Csillery at sms.ed.ac.uk  Wed Oct  6 12:11:04 2004
From: K.M.Csillery at sms.ed.ac.uk (K.M.Csillery@sms.ed.ac.uk)
Date: Wed, 6 Oct 2004 11:11:04 +0100 (BST)
Subject: [R] R-1.9.1 on OS 10.2
In-Reply-To: <3F09C2F3.5030702@stat.ucla.edu>
References: <Pine.OSF.4.21.0307071052560.26988-100000@selway.umt.edu>
	<3F09C2F3.5030702@stat.ucla.edu>
Message-ID: <Pine.GSO.4.58.0410061051230.28082@holyrood.ed.ac.uk>


Hi,

I've been using R 1.7.1 so far and now need to update to 1.9.1.
Since the new version is not yet on the Fink tree I tried to install R
from source to my sw library. The configuration runs fine but make doesn't.
Here is the error message:
/sw/lib/libg2c.a(err.o) definition of common f(short, void, int, char)
(size 4)
/usr/bin/libtool: internal link edit command failed
make[4]: *** [libRlapack.dylib] Error 1
make[3]: *** [R] Error 2
make[2]: *** [R] Error 1
make[1]: *** [R] Error 1
make: *** [R] Error 1

Ok, than I went to the binaries, but I get software installation error.

So, again my machine is OS 10.2.8, I use emacs, ESS, R 1.7.1 under X11 and
it works fine.

Any suggestions are welcome!

Katalin
----
Institute of Evolutionary Biology
The University of Edinburgh
K.M.Csillery at sms.ed.ac.uk



From K.M.Csillery at sms.ed.ac.uk  Wed Oct  6 12:23:50 2004
From: K.M.Csillery at sms.ed.ac.uk (K.M.Csillery@sms.ed.ac.uk)
Date: Wed, 6 Oct 2004 11:23:50 +0100 (BST)
Subject: [R] R-1.9.1 on OS 10.2
Message-ID: <Pine.GSO.4.58.0410061114430.10028@holyrood.ed.ac.uk>


Hi,

I've been using R 1.7.1 but now I need to upgrade to R-1.9.1.
Since it is not on the Fink tree yet I tried to install it from source to
my sw library. The configuration runs fine, but make doesn't.
Here is the error message I get:
/sw/lib/libg2c.a(err.o) definition of common f(short, void, int, char)
(size 4)
/usr/bin/libtool: internal link edit command failed
make[4]: *** [libRlapack.dylib] Error 1
make[3]: *** [R] Error 2
make[2]: *** [R] Error 1
make[1]: *** [R] Error 1
make: *** [R] Error 1

Ok, than I tried to use the binaries for OS, but I get an installation
error.

So, again, my machine is OS 10.2.8, and I use emacs and ESS under X11 and
R-1.7.1 just workes fine (installed from Fink tree after release).

Any suggestions are welcome!

Katalin

PS Sorry, but I might have double posted the message.

---
Institute of Evolutionary Biology
The University of Edinburgh
K.M.Csillery at sms.ed.ac.uk



From ripley at stats.ox.ac.uk  Wed Oct  6 12:40:10 2004
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Wed, 6 Oct 2004 11:40:10 +0100 (BST)
Subject: [R] RExcel : problem with error handler?
In-Reply-To: <78166BFC5165D811AA0400065BF0324B314969@wisconsin.concordia>
Message-ID: <Pine.LNX.4.44.0410061136560.5751-100000@gannet.stats>

Please note, this is R-help.  The mailing list for help with RExcel (which
is not part of R) is given on
http://cran.r-project.org/contrib/extra/dcom/RSrv135.html as
http://mailman.csd.univie.ac.at/mailman/listinfo/rcom-l

On Wed, 6 Oct 2004, Matthew Dowle wrote:

> 
> Dear R-help,
> 
> Call RInterface.StartRServer      ' Works fine
> Call RInterface.RRun("objects()")		' Works fine
> Call Rinterface.RRun( other R commands which do not generate errors, the
> demos etc ) 	  ' Works fine
> But ...
> Call RInterface.RRun("doesnotexist")	' Sends Excel into endless loop it
> appears
> 
> After about 90 seconds a dialog box appears  "Microsoft Excel is waiting for
> another application to complete an OLE action".  Click OK then it hangs for
> another minute or so, and displays the dialogue again. Task manager is
> required to kill Excel.  I rebooted to ensure only Excel is running, so
> there should be no conflicts with any other app, but the same error is
> repeatable. In this case I was expecting the R error "Object "doesnotexist"
> not found" be passed back into Excel, either as a dialogue box in Excel, or
> available to VBA using a GetRError function or something?
> 
> I hope someone can help, or point me in the right direction.
> 
> Info from "About RExcel" :
> 
> RExcel version 1.35
> RDCOM server 1.35
> R system software: Version 2.0.0
> Microsoft Excel 10.0
> Windows NT 5.01
> 
> > version
>          _              
> platform i386-pc-mingw32
> arch     i386           
> os       mingw32        
> system   i386, mingw32  
> status   beta           
> major    2              
> minor    0.0            
> year     2004           
> month    09             
> day      28             
> language R              
> > 
> 
> Many thanks,
> Matthew
> 
> 
> 
> 
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
> 
> 

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From ripley at stats.ox.ac.uk  Wed Oct  6 12:50:57 2004
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Wed, 6 Oct 2004 11:50:57 +0100 (BST)
Subject: [R] R-1.9.1 on OS 10.2
In-Reply-To: <Pine.GSO.4.58.0410061114430.10028@holyrood.ed.ac.uk>
Message-ID: <Pine.LNX.4.44.0410061144480.5882-100000@gannet.stats>

On Wed, 6 Oct 2004 K.M.Csillery at sms.ed.ac.uk wrote:

> I've been using R 1.7.1 but now I need to upgrade to R-1.9.1.
> Since it is not on the Fink tree yet 

What is `it'?   The current version of R is 2.0.0, BTW.

> I tried to install it from source to
> my sw library. The configuration runs fine, but make doesn't.
> Here is the error message I get:
> /sw/lib/libg2c.a(err.o) definition of common f(short, void, int, char)
> (size 4)
> /usr/bin/libtool: internal link edit command failed
> make[4]: *** [libRlapack.dylib] Error 1
> make[3]: *** [R] Error 2
> make[2]: *** [R] Error 1
> make[1]: *** [R] Error 1
> make: *** [R] Error 1

Do read what R-admin.html says (as INSTALL asks you to):

tar zxvf R-1.9.1.tgz
cd R-1.9.1
./configure --with-blas='-framework vecLib' --with-lapack --with-aqua 
make
...
The first two options are strongly recommended. 

Now you know why!  Your compiler has a libg2c that is not shared and so 
cannot be used in a dynamic library.  Please do follow the strong 
recommendation.

> Ok, than I tried to use the binaries for OS, but I get an installation
> error.

PLEASE do read the posting guide and give useful information: this is 
almost maximally uniformative.

> So, again, my machine is OS 10.2.8, and I use emacs and ESS under X11 and
> R-1.7.1 just workes fine (installed from Fink tree after release).
> 
> Any suggestions are welcome!
> 
> Katalin
> 
> PS Sorry, but I might have double posted the message.

You did.  I suspect you have not read the MacOS FAQ either, which probably 
will resolve your issues with the binary distribution.

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From rolf.wester at ilt.fraunhofer.de  Wed Oct  6 13:00:44 2004
From: rolf.wester at ilt.fraunhofer.de (Rolf Wester)
Date: Wed, 06 Oct 2004 13:00:44 +0200
Subject: [R] Foreign code problem
Message-ID: <4163D05C.6020005@ilt.fraunhofer.de>

Hello,

I wanted to test the odesolve package and tried to use compiled C-code.
But when I do:

erg <- lsoda(y, times, "mond", parms, rtol, atol, tcrit=NULL, jacfunc=NULL,
               verbose=FALSE, dllname="mond", hmin=0, hmax=Inf)

I get the error message:

Error in lsoda(y, times, "mond", parms, rtol, atol, tcrit = NULL,             
               jacfunc = NULL,  : 
    Unable to find mond in mond

The C code is:

#include <math.h>

void mond(long int *neq, double *t, double *y, double *ydot)
{
	double a = sqrt(y[2]*y[2] + y[4]*y[4]);
	a = -1.0/(a*a*a);
	ydot[0] = a*y[1];
	ydot[1] = y[0]; 
	ydot[2] = a*y[3];
	ydot[3] = y[2];
}

and mond.so is build by:

gcc -O3 -shared mond.c -lm -o mond.so

I have set LD_LIBRARY_PATH to the correct directory and also tried to use the full path of mond.so in the call to lsoda but got the same error message. I would be very appreciative for help in order to understand how compiled code can be used from within R.

Besides this I have a further question. Am I right that every function argument is copyied when calling a function, even if the argument is a 
huge matrix? Wouldn't that be very inefficient?

Thank you very much in advance

Regards

Rolf Wester

P.S.: I'm new to R and find it is a great tool and programming environment.



From ripley at stats.ox.ac.uk  Wed Oct  6 13:22:13 2004
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Wed, 6 Oct 2004 12:22:13 +0100 (BST)
Subject: [R] Foreign code problem
In-Reply-To: <4163D05C.6020005@ilt.fraunhofer.de>
Message-ID: <Pine.LNX.4.44.0410061213470.27397-100000@gannet.stats>

On Wed, 6 Oct 2004, Rolf Wester wrote:

> Hello,
> 
> I wanted to test the odesolve package and tried to use compiled C-code.
> But when I do:
> 
> erg <- lsoda(y, times, "mond", parms, rtol, atol, tcrit=NULL, jacfunc=NULL,
>                verbose=FALSE, dllname="mond", hmin=0, hmax=Inf)
> 
> I get the error message:
> 
> Error in lsoda(y, times, "mond", parms, rtol, atol, tcrit = NULL,             
>                jacfunc = NULL,  : 
>     Unable to find mond in mond

Well, ?lsoda says

          If 'func' is a string, then 'dllname' must give the name of
          the shared library (without extension) which must be loaded
          before 'lsoda()' is called.

Have you loaded it (with dyn.load)?  The example in 
dynload/c/testdynload.R may help you.

> The C code is:
> 
> #include <math.h>
> 
> void mond(long int *neq, double *t, double *y, double *ydot)
> {
> 	double a = sqrt(y[2]*y[2] + y[4]*y[4]);
> 	a = -1.0/(a*a*a);
> 	ydot[0] = a*y[1];
> 	ydot[1] = y[0]; 
> 	ydot[2] = a*y[3];
> 	ydot[3] = y[2];
> }
> 
> and mond.so is build by:
> 
> gcc -O3 -shared mond.c -lm -o mond.so
> 
> I have set LD_LIBRARY_PATH to the correct directory and also tried to
> use the full path of mond.so in the call to lsoda but got the same error
> message. I would be very appreciative for help in order to understand
> how compiled code can be used from within R.


> Besides this I have a further question. Am I right that every function
> argument is copyied when calling a function, even if the argument is a
> huge matrix? Wouldn't that be very inefficient?

It would be.  It is marked for potential copying, but the copy only occurs 
when the potential copy is changed

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From 0034058 at fudan.edu.cn  Wed Oct  6 13:44:34 2004
From: 0034058 at fudan.edu.cn (rongguiwong)
Date: Wed, 06 Oct 2004 19:44:34 +0800
Subject: [R] R 2.0.0 is released
In-Reply-To: <x2oejipkm4.fsf@biostat.ku.dk>
References: <x2oejipkm4.fsf@biostat.ku.dk>
Message-ID: <200410061944.34324.0034058@fudan.edu.cn>

congratulations!

i download the software and install it.
it is pretty good.it start much more quickly than 1.9.1.
but i find a problem.
i have a data frame,the varible"PROVICE' is in Chinese,but it appears like 
this:
[1] \261\261\276\251               \314\354\275\362
 [3] \272\323\261\261               \311\275\316\367
 [5] \304\332\303\311\271\305       \301\311\304\376
 [7] \274\252\301\326               \272\332\301\372\275\255
 [9] \311\317\272\243               \275\255\313\325
[11] \325\343\275\255               \260\262\273\325
[13] \270\243\275\250               \275\255\316\367
[15] \311\275\266\253               \272\323\304\317
[17] \272\376\261\261               \272\376\304\317
[19] \271\343\266\253               \271\343\316\367
[21] \272\243\304\317               \326\330\307\354
[23] \313\304\264\250               \271\363\326\335
[25] \324\306\304\317               \316\367\262\330
[27] \311\302\316\367               \270\312\313\340
[29] \307\340\272\243               \304\376\317\304
[31] \320\302\275\256
31 Levels:                       ... 

it seems can not display chinese correctly.but in R1.9.1,it display 
correctly .in R1.9.1,when i use >data$PROVICE,it display chinese directly,but 
in R2.0.0,the display numbers like "\261\261\276\251 ".
is it a bug or something else?

 2004104  23:29Peter Dalgaard 
> I've rolled up R-2.0.0.tar.gz a short while ago. This is a new version
> with a number of new features. See below for the details.
>
> As was the case with R 1.0.0, this new version represents a coming of
> age more than a radical change to R. We do plan to celebrate the new
> major version with press releases and such.
>
> The release will be available from
>
> http://cran.r-project.org/src/base/R-2/R-2.0.0.tar.gz
>
> or you might wait for it to be mirrored at a CRAN site nearer to you.
> Binaries for various platforms will appear in due course.
>
> There is also a version split for floppies.
>
>
>         For the R Core Team
>         Peter Dalgaard
>
>
>
>
> These are the md5sums for the freshly created files, in case you wish
> to check that they are uncorrupted:
>
> 3900bca37cabb4b76b8d736d51cc9251  R-2.0.0.tar.gz
> 4a3c7595b112d879997f7455fa8c1c0d  R-2.0.0.tar.gz-split.aa
> 45d376c7c533c62c657ef0fafac8a784  R-2.0.0.tar.gz-split.ab
> 7dbdb241e1fb7263701719ea856ebe41  R-2.0.0.tar.gz-split.ac
> 1e1077f7593778b79e9b0d5c676af63b  R-2.0.0.tar.gz-split.ad
> 6f4ffaa48a54e002f586f80fd4f2461c  R-2.0.0.tar.gz-split.ae
> 34b62bb31f6ecf84da329d21c2a21561  R-2.0.0.tar.gz-split.af
> c880f8f06ca3fe367bba6757e9cfbf32  R-2.0.0.tar.gz-split.ag
> b1a2d17d3ae523d04ffc8d3c6db4b67b  R-2.0.0.tar.gz-split.ah
>
>
> Here is the relevant part of the NEWS file
>
>
> 		CHANGES IN R VERSION 2.0.0
>
>
> USER-VISIBLE CHANGES
>
>     o	The stub packages from 1.9.x have been removed: the library()
> 	function selects the new home for their code.
>
>     o	`Lazy loading' of R code has been implemented, and is used for
> 	the standard and recommended packages by default.  Rather than
> 	keep R objects in memory, they are kept in a database on disc
> 	and only loaded on first use.  This accelerates startup (down
> 	to 40% of the time for 1.9.x) and reduces memory usage -- the
> 	latter is probably unimportant of itself, but reduces
> 	commensurately the time spent in garbage collection.
>
> 	Packages are by default installed using lazy loading if they
> 	have more than 25Kb of R code and did not use a saved image.
> 	This can be overridden by INSTALL --[no-]lazy or via a field
> 	in the DESCRIPTION file.  Note that as with --save, any other
> 	packages which are required must be already installed.
>
> 	As the lazy-loading databases will be consulted often, R
> 	will be slower if run from a slow network-mounted disc.
>
>     o	All the datasets formerly in packages 'base' and 'stats' have
> 	been moved to a new package 'datasets'.	 data() does the
> 	appropriate substitution, with a warning.  However, calls to
> 	data() are not normally needed as the data objects are visible
> 	in the 'datasets' package.
>
> 	Packages can be installed to make their data objects visible
> 	via R CMD INSTALL --lazy-data or via a field in the
> 	DESCRIPTION file.
>
>     o	Package 'graphics' has been split into 'grDevices' (the graphics
> 	devices shared between base and grid graphics) and 'graphics'
> 	(base graphics).  Each of the 'graphics' and 'grid' packages
> 	load 'grDevices' when they are attached.  Note that
> 	ps.options() has been moved to grDevices and user hooks may
> 	need to be updated.
>
>     o	The semantics of data() have changed (and were incorrectly
> 	documented in recent releases) and the function has been moved
> 	to package 'utils'.  Please read the help page carefully if
> 	you use the 'package' or 'lib.loc' arguments.
>
> 	data() now lists datasets, and not just names which data() accepts.
>
>     o	Dataset 'phones' has been renamed to 'WorldPhones'.
>
>     o	Datasets 'sunspot.month' and 'sunspot.year' are available
> 	separately but not via data(sunspot) (which was used by package
> 	lattice to retrieve a dataset 'sunspot').
>
>     o	Packages must have been re-installed for this version, and
> 	library() will enforce this.
>
>     o	Package names must now be given exactly in library() and
> 	require(), regardless of whether the underlying file system is
> 	case-sensitive or not.	So 'library(mass)' will not work, even
> 	on Windows.
>
>     o	R no longer accepts associative use of relational operators.
> 	That is, 3 < 2 < 1 (which used to evalute as TRUE!) now causes
> 	a syntax error.	 If this breaks existing code, just add
> 	parentheses -- or braces in the case of plotmath.
>
>     o	The R parser now allows multiline strings, without escaping
> 	the newlines with backslashes (the old method still works).
> 	Patch by Mark Bravington.
>
>
> NEW FEATURES
>
>     o	There is a new atomic vector type, class "raw".	 See ?raw for
> 	full details including the operators and utility functions provided.
>
>     o	The default barplot() method by default uses a
> 	gamma-corrected grey palette (rather than the heat color
> 	palette) for coloring its output when given a matrix.
>
>     o	The 'formula' method for boxplot() has a 'na.action' argument,
> 	defaulting to NULL.  This is mainly useful if the response
> 	is a matrix when the previous default of 'na.omit' would omit
> 	entire rows.  (Related to PR#6846.)
>
> 	boxplot() and bxp() now obey global 'par' settings and also
> 	allow the specification of graphical options in more detail,
> 	compatibly with S-PLUS (fulfilling wishlist entry PR#6832)
> 	thanks to contributions from Arni Magnusson.  For consistency,
> 	'boxwex' is not an explicit argument anymore.
>
>     o	chull() has been moved to package graphics (as it uses xy.coords).
>
>     o	There is now a coef() method for summaries of "nls" objects.
>
>     o	compareVersion(), packageDescription() and read.00Index()
> 	have been moved to package 'utils'.
>
>     o	convolve(), fft(), mvfft() and nextn() have been moved to
> 	package stats.
>
>     o	coplot() now makes use of cex.lab and font.lab par() settings.
>
>     o	cumsum/prod/max/min() now preserve names.
>
>     o	data(), .path.packages() and .find.packages() now interpret
> 	package = NULL to mean all loaded packages.
>
>     o	data.frame() and its replacement methods remove the names from
> 	vector columns.	 Using I() will ensure that names are
> 	preserved.
>
>     o	data.frame(check.names = TRUE) (the default) enforces unique
> 	names, as S does.
>
>     o	.Defunct() now has 'new' and 'package' arguments like those of
> 	.Deprecated().
>
>     o	The plot() method for "dendrogram" objects now respects many more
> 	nodePar and edgePar settings and for edge labeling computes the
> 	extents of the diamond more correctly.
>
>     o	deparse(), dput() and dump() have a new 'control' argument to
> 	control the level of detail when deparsing.  dump() defaults to
> 	the most detail, the others default to less.  See ?.deparseOpts
> 	for the details.
>
> 	They now evaluate promises by default: see ?dump for details.
>
>     o	dir.create() now expands '~' in filenames.
>
>     o	download.file() has a new progress meter (under Unix) if the
> 	length of the file is known -- it uses 50 equals signs.
>
>     o	dyn.load() and library.dynam() return an object describing the
> 	DLL that was loaded.  For packages with namespaces, the DLL
> 	objects are stored in a list within the namespace.
>
>     o	New function eapply() - apply for environments.	 The supplied
> 	function is applied to each element of the environment; the order
> 	of application is not specified.
>
>     o	edit() and fix() use the object name in the window caption on
> 	some platforms (e.g. Windows).
>
>     o	Function file.edit() function added: like file.show(), but
> 	allows editing.
>
>     o	Function file.info() can return file sizes > 2G if the
> 	underlying OS supports such.
>
>     o	fisher.test(*, conf.int=FALSE) allows the confidence interval
> 	computation to be skipped.
>
>     o	formula() methods for classes "lm" and "glm" used the expanded
> 	formula (with '.' expanded) from the terms component.
>
>     o	The `formula' method for ftable() now looks for variables in the
> 	environment of the formula before the usual search path.
>
>     o	A new function getDLLRegisteredRoutines() returns information
> 	about the routines available from a DLL that were explicitly
> 	registered with R's dynamic loading facilities.
>
>     o	A new function getLoadedDLLs() returns information about the
> 	DLLs that are currently loaded within this session.
>
>     o	The package element returned by getNativeSymbolInfo() contains
> 	reference to both the internal object used to resolve symbols
> 	with the DLL, and the internal DllInfo structure used to
> 	represent the DLL within R.
>
>     o	help() now returns information about available documentation for
> 	a given topic, and notifies about multiple matches.  It has a
> 	separate print() method.
>
> 	If the latex help files were not installed, help() will offer
> 	to create a latex file on-the-fly from the installed .Rd file.
>
>     o	heatmap() has a new argument 'reorderfun'.
>
>     o	Most versions of install.packages() have an new optional
> 	argument 'dependencies = TRUE' which will not only fetch the
> 	packages but also their uninstalled dependencies and their
> 	dependencies ....
>
> 	The Unix version of install.packages() attempts to install
> 	packages in an order that reflects their dependencies.	(This
> 	is not needed for binary installs as used under Windows.)
>
>     o	interaction() has new argument 'sep'.
>
>     o	interaction.plot() allows 'type = "b"' and doesn't give spurious
> 	warnings when passed a matplot()-only argument such as 'main'.
>
>     o	is.integer() and is.numeric() always return FALSE for a
> 	factor.	 (Previously they were true and false respectively for
> 	well-formed factors, but it is possible to create factors
> 	with non-integer codes by underhand means.)
>
>     o	New functions is.leaf(), dendrapply() and a labels() method for
> 	dendrogram objects.
>
>     o	legend() has an argument 'pt.lwd' and setting 'density' now works
> 	because 'angle' now defaults to 45 (mostly contributed by Uwe Ligges).
>
>     o	library() now checks the version dependence (if any) of
> 	required packages mentioned in the Depends: field of the
> 	DESCRIPTION file.
>
>     o	load() now detects and gives a warning (rather than an error)
> 	for empty input, and tries to detect (but not correct) files
> 	which have had LF replaced by CR.
>
>     o	ls.str() and lsf.str() now return an object of class "ls_str" which
> 	has a print method.
>
>     o	make.names() has a new argument allow_, which if false allows
> 	its behaviour in R 1.8.1 to be reproduced.
>
>     o	The 'formula' method for mosaicplot() has a 'na.action' argument
> 	defaulting to 'na.omit'.
>
>     o	model.frame() now warns if it is given data = newdata and it
> 	creates a model frame with a different number of rows from
> 	that implied by the size of 'newdata'.
>
> 	Time series attributes are never copied to variables in the
> 	model frame unless na.action = NULL.  (This was always the
> 	intention, but they sometimes were as the result of an earlier
> 	bug fix.)
>
>     o	There is a new 'padj' argument to mtext() and axis().
> 	Code patch provided by Uwe Ligges (fixes PR#1659 and PR#7188).
>
>     o	Function package.dependencies() has been moved to package 'tools'.
>
>     o	The 'formula' method for pairs() has a 'na.action' argument,
> 	defaulting to 'na.pass', rather than the value of
> 	getOption("na.action").
>
>     o	There are five new par() settings:
>
> 	'family' can be used to specify a font family for graphics
> 	text.  This is a device-independent family specification
> 	which gets mapped by the graphics device to a device-specific
> 	font specification (see, for example, postscriptFonts()).
> 	Currently, only PostScript, PDF, X11, Quartz, and Windows
> 	respond to this setting.
>
> 	'lend', 'ljoin', and 'lmitre' control the cap style and
> 	join style for drawing lines (only noticeable on thick lines
> 	or borders).  Currently, only PostScript, PDF, X11, and Quartz
> 	respond to these settings.
>
> 	'lheight' is a multiplier used in determining the vertical
> 	spacing of multi-line text.
>
> 	All of these settings are currently only available via par()
> 	(i.e., not in-line as arguments to plot(), lines(), ...)
>
>     o	PCRE (as used by grep etc) has been updated to version 5.0.
>
>     o	A 'version' argument has been added to pdf() device.  If this is
> 	set to "1.4", the device will support transparent colours.
>
>     o	plot.xy(), the workhorse function of points(), lines() and
> 	plot.default() now has 'lwd' as explicit argument instead of
> 	implicitly in '...', and now recycles lwd where it makes
> 	sense, i.e. for line-based plot symbols.
>
>     o	The png() and jpeg() devices (and the bmp() device under Windows)
> 	now allow a nominal resolution to be recorded in the file.
>
>     o   New functions to control mapping from device-independent
> 	graphics font family to device-specific family:
> 	postscriptFont() and postscriptFonts() (for both postscript()
> 	and pdf()); X11Font() and X11Fonts(); windowsFont() and
> 	windowsFonts(); quartzFont() and quartzFonts().
>
>     o	power (x^y) has optimised code for y == 2.
>
>     o	prcomp() is now generic, with a formula method (based on an
> 	idea of Jari Oksanen).
>
> 	prcomp() now has a simple predict() method.
>
>     o	printCoefmat() has a new logical argument 'signif.legend'.
>
>     o	quantile() has the option of several methods described in
> 	Hyndman & Fan (1996). (Contributed by Rob Hyndman.)
>
>     o	rank() has two new 'ties.method's, "min" and "max".
>
>     o	New function read.fortran() reads Fortran-style fixed-format
> 	specifications.
>
>     o	read.fwf() reads multiline records, is faster for large files.
>
>     o	read.table() now accepts "NULL", "factor", "Date" and
> 	"POSIXct" as possible values of colClasses, and colClasses can
> 	be a named character vector.
>
>     o	readChar() can now read strings with embedded nuls.
>
>     o	The "dendrogram" method for reorder() now has a 'agglo.FUN'
> 	argument for specification of a weights agglomeration
> 	function.
>
>     o	New reorder() method for factors, slightly extending that in
> 	lattice.  Contributed by Deepayan Sarkar.
>
>     o	Replaying a plot (with replayPlot() or via autoprinting) now
> 	automagically opens a device if none is open.
>
>     o	replayPlot() issues a warning if an attempt is made to replay
> 	a plot that was recorded using a different R version (the
> 	format for recorded plots is not guaranteed to be stable
> 	across different R versions).  The Windows-menu equivalent
> 	(History...Get from variable) issues a similar warning.
>
>     o	reshape() can handle multiple 'id' variables.
>
>     o	It is now possible to specify colours with a full alpha
> 	transparency channel via the new 'alpha' argument to the
> 	rgb() and hsv() functions, or as a string of the form "#RRGGBBAA".
>
> 	NOTE: most devices draw nothing if a colour is not opaque,
> 	but PDF and Quartz devices will render semitransparent colours.
>
> 	A new argument 'alpha' to the function col2rgb()
> 	provides the ability to return the alpha component of
> 	colours (as well as the red, green, and blue components).
>
>     o	save() now checks that a binary connection is used.
>
>     o	seek() on connections now accepts and returns a double for the
> 	file position.	This allows >2Gb files to be handled on a
> 	64-bit platform (and some 32-bit platforms).
>
>     o	source() with 'echo = TRUE' uses the function source attribute
> 	when displaying commands as they are parsed.
>
>     o	setClass() and its utilities now warn if either superclasses
> 	or classes for slots are undefined.  (Use setOldClass to
> 	register S3 classes for use as slots)
>
>     o	str(obj) now displays more reasonably the STRucture of S4 objects.
>
> 	It is also improved for language objects and lists with promise
> 	components.
>
> 	The method for class "dendrogram" has a new argument 'stem' and
> 	indicates when it's not printing all levels (as typically when
> 	e.g., 'max.level = 2').
>
> 	Specifying 'max.level = 0' now allows to suppress all but the top
> 	level for hierarchical objects such as lists. This is different
> 	to previous behavior which was the default behavior of giving all
> 	levels is unchanged.  The default behavior is unchanged but now
> 	specified by 'max.level = NA'.
>
>     o	system.time() has a new argument 'gcFirst' which, when TRUE,
> 	forces a garbage collection before timing begins.
>
>     o	tail() of a matrix now displays the original row numbers.
>
>     o	The default method for text() now coerces a factor to character
> 	and not to its internal codes.	This is incompatible with S
> 	but seems what users would expect.
>
> 	It now also recycles (x,y) to the length of 'labels' if that
> 	is longer.  This is now compatible with grid.text() and
> 	S. (See also PR#7084.)
>
>     o	TukeyHSD() now labels comparisons when applied to an
> 	interaction in an aov() fit.  It detects non-factor terms in
> 	'which' and drops them if sensible to do so.
>
>     o	There is now a replacement method for window(), to allow a
> 	range of values of time series to be replaced by specifying the
> 	start and end times (and optionally a frequency).
>
>     o	If writeLines() is given a connection that is not open, it now
> 	attempts to open it in mode = "wt" rather than the default
> 	mode specified when creating the connection.
>
>     o	The screen devices x11(), windows() and quartz() have a new
> 	argument 'bg' to set the default background colour.
>
>
>     o	Subassignments involving NAs and with a replacement value of
> 	length > 1 are now disallowed.	(They were handled
> 	inconsistently in R < 2.0.0, see PR#7210.)  For data frames
> 	they are disallowed altogether, even for logical matrix indices
> 	(the only case which used to work).
>
>     o	The way the comparison operators handle a list argument has
> 	been rationalized so a few more cases will now work -- see
> 	?Comparison.
>
>     o	Indexing a vector by a character vector was slow if both the
> 	vector and index were long (say 10,000).  Now hashing is used
> 	and the time should be linear in the longer of the lengths
> 	(but more memory is used).
>
>     o	Printing a character string with embedded nuls now prints the
> 	whole string, and non-printable characters are represented by
> 	octal escape sequences.
>
>     o	Objects created from a formally defined class now include the
> 	name of the corresponding package as an attribute in the
> 	object's class.	 This allows packages with namespaces to have
> 	private (non-exported) classes.
>
>     o	Changes to package 'grid':
>
> 	- Calculation of number of circles to draw in circleGrob now
> 	  looks at length of y and r as well as length of x.
>
> 	- Calculation of number of rectangles to draw in rectGrob now
> 	  looks at length of y, w, and h as well as length of x.
>
> 	- All primitives (rectangles, lines, text, ...) now handle
> 	  non-finite values (NA, Inf, -Inf, NaN) for locations and
> 	  sizes.
>
> 	  Non-finite values for locations, sizes, and scales of
> 	  viewports result in error messages.
>
> 	  There is a new vignette ("nonfinite") which describes this
> 	  new behaviour.
>
> 	- Fixed (unreported) bug in drawing circles.  Now checks that
> 	  radius is non-negative.
>
> 	- downViewport() now reports the depth it went down to find a
> 	  viewport.  Handy for "going back" to where you started, e.g., ...
>
> 	    depth <- downViewport("vpname")
> 	    <draw stuff>
> 	    upViewport(depth)
>
> 	- The "alpha" gpar() is now combined with the alpha channel of
> 	  colours when creating a gcontext as follows: (internal C code)
>
> 	    finalAlpha = gpar("alpha")*(R_ALPHA(col)/255)
>
> 	  This means that gpar(alpha=) settings now affect internal
> 	  colours so grid alpha transparency settings now are sent to
> 	  graphics devices.
>
> 	  The alpha setting is also cumulative.	 For example, ...
>
> 	    grid.rect(width=0.5, height=0.5,
> 		      gp=gpar(fill="blue"))		 # alpha = 1
> 	    pushViewport(viewport(gp=gpar(alpha=0.5)))
> 	    grid.rect(height=0.25, gp=gpar(fill="red"))	 # alpha = 0.5
> 	    pushViewport(viewport(gp=gpar(alpha=0.5)))
> 	    grid.rect(width=0.25, gp=gpar(fill="red"))	 # alpha = 0.25 !
>
> 	- Editing a gp slot in a grob is now incremental.  For example ...
>
> 	    grid.lines(name="line")
> 	    grid.edit("line", gp=gpar(col="red")) # line turns red
> 	    grid.edit("line", gp=gpar(lwd=3)) # line becomes thick
> 					      # AND STAYS red
>
> 	- The "cex" gpar is now cumulative.  For example ...
>
> 	    grid.rect(height=unit(4, "char")) # cex = 1
> 	    pushViewport(viewport(gp=gpar(cex=0.5)))
> 	    grid.rect(height=unit(4, "char")) # cex = 0.5
> 	    pushViewport(viewport(gp=gpar(cex=0.5)))
> 	    grid.rect(height=unit(4, "char")) # cex = 0.125 !!!
>
> 	- New childNames() function to list the names of children
> 	  of a gTree.
>
> 	- The "grep" and "global" arguments have been implemented for
> 	  grid.[add|edit|get|remove]Grob() functions.
>
> 	  The "grep" argument has also been implemented for the
> 	  grid.set() and setGrob().
>
> 	- New function grid.grab() which creates a gTree from the
> 	  current display list (i.e., the current page of output can
> 	  be converted into a single gTree object with all grobs
> 	  on the current page as children of the gTree and all the
> 	  viewports used in drawing the current page in the childrenvp
> 	  slot of the gTree).
>
> 	- New "lineend", "linejoin", and "linemitre" gpar()s:
>
> 	  line end can be "round", "butt", or "square".
> 	  line join can be "round", "mitre", or "bevel".
> 	  line mitre can be any number larger than 1
> 	    (controls when a mitre join gets turned into a bevel join;
> 	     proportional to angle between lines at join;
> 	     very big number means that conversion only happens for lines
> 	     that are almost parallel at join).
>
> 	- New grid.prompt() function for controlling whether the user is
> 	  prompted before starting a new page of output.
>
> 	  Grid no longer responds to the par(ask) setting in the "graphics"
> 	  package.
>
>     o	The tcltk package has had the tkcmd() function renamed as
> 	tcl() since it could be used to invoke commands that had
> 	nothing to do with Tk. The old name is retained, but will be
> 	deprecated in a future release. Similarly, we now have
> 	tclopen(), tclclose(), tclread(), tclputs(), tclfile.tail(),
> 	and tclfile.dir() replacing counterparts starting with "tk",
> 	with old names retained for now.
>
>
> UTILITIES
>
>     o	R CMD check now checks for file names in a directory that
> 	differ only by case.
>
>     o	R CMD check now checks Rd files using R code from package tools,
> 	and gives refined diagnostics about "likely" Rd problems (stray
> 	top-level text which is silently discarded by Rdconv).
>
>     o	R CMD INSTALL now fails for packages with incomplete/invalid
> 	DESCRIPTION metadata, using new code from package tools which is
> 	also used by R CMD check.
>
>     o	list_files_with_exts (package tools) now handles zipped directories.
>
>     o	Package 'tools' now provides Rd_parse(), a simple top-level
> 	parser/analyzer for R documentation format.
>
>     o	tools::codoc() (and hence R CMD check) now checks any documentation
> 	for registered S3 methods and unexported objects in packages
> 	with namespaces.
>
>     o	Package 'utils' contains several new functions:
>
> 	- Generics toBibtex() and toLatex() for converting
> 	  R objects to BibTeX and LaTeX (but almost no methods yet).
>
> 	- A much improved citation() function which also has a package
> 	  argument.  By default the citation is auto-generated from
> 	  the package DESCRIPTION, the file 'inst/CITATION' can be
> 	  used to override this, see help(citation) and
> 	  help(citEntry).
>
> 	- sessionInfo() can be used to include version information about
> 	  R and R packages in text or LaTeX documents.
>
>
> DOCUMENTATION
>
>     o	The DVI and PDF manuals are now all made on the paper specified
> 	by R_PAPERSIZE (default 'a4'), even the .texi manuals which
> 	were made on US letter paper in previous versions.
>
>     o	The reference manual now omits 'internal' help pages.
>
>     o	There is a new help page shown by help("Memory-limits") which
> 	documents the current design limitations on large objects.
>
>     o	The format of the LaTeX version of the documentation has
> 	changed.  The old format is still accepted, but only the new
> 	resolves cross-references to object names containing _, for
> 	example.
>
>     o	HTML help pages now contain a reference to the package and
> 	version in the footer, and HTML package index pages give their
> 	name and version at the top.
>
>     o	All manuals in the 2.x series have new ISBN numbers.
>
>     o	The 'R Data Import/Export' manual has been revised and has a
> 	new chapter on `Reading Excel spreadsheets'.
>
>
> C-LEVEL FACILITIES
>
>     o	The PACKAGE argument for .C/.Call/.Fortran/.External can be
> 	omitted if the call is within code within a package with a
> 	namespace.  This ensures that the native routine being called
> 	is found in the DLL of the correct version of the package if
> 	multiple versions of a package are loaded in the R session.
> 	Using a namespace and omitting the PACKAGE argument is
> 	currently the only way to ensure that the correct version is
> 	used.
>
>     o	The header Rmath.h contains a definition for R_VERSION_STRING
> 	which can be used to track different versions of R and libRmath.
>
>     o	The Makefile in src/nmath/standalone now has 'install' and
> 	'uninstall' targets -- see the README file in that directory.
>
>     o	More of the header files, including Rinternals.h, Rdefines.h and
> 	Rversion.h, are now suitable for calling directly from C++.
>
>     o   Configure looks to a suitable option for inlining C code which
> 	made available as macro R_INLINE: see `Writing R Extensions'
> 	for further details.
>
>
> DEPRECATED & DEFUNCT
>
>     o	Direct use of R INSTALL|REMOVE|BATCH|COMPILE|SHLIB has been
> 	removed: use R CMD instead.
>
>     o	La.eigen(), tetragamma(), pentagamma(), package.contents() and
> 	package.description() are defunct.
>
>     o	The undocumented function newestVersion() is no longer exported
> 	from package utils.  (Mainly because it was not completely general.)
>
>     o	C-level entry point ptr_R_GetX11Image has been removed, as it
> 	was replaced by R_GetX11Image at 1.7.0.
>
>     o	The undocumented C-level entry point R_IsNaNorNA has been
> 	removed.  It was used in a couple of packages, and should be
> 	replaced by a call to the documented macro ISNAN.
>
>     o	The gnome/GNOME graphics device is now defunct.
>
>
> INSTALLATION CHANGES
>
>     o	Arithmetic supporting +/-Inf, NaNs and the IEC 60559 (aka
> 	IEEE 754) standard is now required -- the partial and often
> 	untested support for more limited arithmetic has been removed.
>
> 	The C99 macro isfinite is used in preference to finite if available
> 	(and its correct functioning is checked at configure time).
>
> 	Where isfinite or finite is available and works, it is used as
> 	the substitution value for R_FINITE.  On some platforms this
> 	leads to a performance gain.  (This applies to compiled code
> 	in packages only for isfinite.)
>
>     o	The dynamic libraries libR and libRlapack are now installed in
> 	R_HOME/lib rather than R_HOME/bin.
>
>     o	When --enable-R-shlib is specified, the R executable is now a
> 	small executable linked against libR: see the R-admin manual
> 	for further discussion.	 The 'extra' libraries bzip2, pcre,
> 	xdr and zlib are now compiled in a way that allows the code to
> 	be included in a shared library only if this option is
> 	specified, which might improve performance when it is not.
>
>     o	The main R executable is now R_HOME/exec/R not R_HOME/R.bin, to
> 	ease issues on MacOS X.	 (The location is needed when debugging
> 	core dumps, on other platforms.)
>
>     o	Configure now tests for 'inline' and alternatives, and the
> 	src/extra/bzip2 code now (potentially) uses inlining where
> 	available and not just under gcc.
>
>     o	The XPG4 sed is used on Solaris for forming dependencies,
> 	which should now be done correctly.
>
>     o	Makeinfo 4.5 or later is now required for building the HTML and
> 	Info versions of the manuals.  However, binary distributions
> 	need to be made with 4.7 or later to ensure some of the
> 	links are correct.
>
>     o	f2c is not allowed on 64-bit platforms, as it uses longs for
> 	Fortran integers.
>
>     o	There are new options on how to make the PDF version of the
> 	reference manual -- see the 'R Administration and Installation
> 	Manual' section 2.2.
>
>     o	The concatenated Rd files in the installed 'man' directory are
> 	now compressed and the R CMD check routines can read the
> 	compressed files.
>
>     o   There is a new configure option --enable-linux-lfs that will
> 	build R with support for > 2Gb files on suitably recent 32-bit
> 	Linux systems.
>
>
> PACKAGE INSTALLATION CHANGES
>
>     o	The DESCRIPTION file of packages may contain a 'Imports:'
> 	field for packages whose namespaces are used but do not need
> 	to be attached.	 Such packages should no longer be listed in
> 	'Depends:'.
>
>     o	There are new optional fields 'SaveImage', 'LazyLoad' and
> 	'LazyData' in the DESCRIPTION file.  Using 'SaveImage' is
> 	preferred to using an empty file 'install.R'.
>
>     o	A package can contain a file 'R/sysdata.rda' to contain
> 	system datasets to be lazy-loaded into the namespace/package
> 	environment.
>
>     o	The packages listed in 'Depends' are now loaded before a package
> 	is loaded (or its image is saved or it is prepared for lazy
> 	loading).  This means that almost all uses of R_PROFILE.R and
> 	install.R are now unnecessary.
>
>     o	If installation of any package in a bundle fails, R CMD
> 	INSTALL will back out the installation of all of the bundle,
> 	not just the failed package (on both Unix and Windows).
>
>
> BUG FIXES
>
>     o	Complex superassignments were wrong when a variable with the same
> 	name existed locally, and were not documented in R-lang.
>
>     o	rbind.data.frame() dropped names/rownames from columns in all
> 	but the first data frame.
>
>     o	The dimnames<- method for data.frames was not checking the
> 	validity of the row names.
>
>     o	Various memory leaks reported by valgrind have been plugged.
>
>     o	gzcon() connections would sometimes read the crc bytes from
> 	the wrong place, possibly uninitialized memory.
>
>     o	Rd.sty contained a length \middle that was not needed after a
> 	revision in July 2000.	It caused problems with LaTeX systems
> 	based on e-TeX which are starting to appear.
>
>     o	save() to a connection did not check that the connection was
> 	open for writing, nor that non-ascii saves cannot be made to a
> 	text-mode connection.
>
>     o	phyper() uses a new algorithm based on Morten Welinder's bug
> 	report (PR#6772).  This leads to faster code for large arguments
> 	and more precise code, e.g. for phyper(59, 150,150, 60,	lower=FALSE).
> 	This also fixes bug (PR#7064) about fisher.test().
>
>     o	print.default(*, gap = <n>) now in principle accepts all
> 	non-negative values <n>.
>
>     o	smooth.spline(...)$pen.crit had a typo in its computation;
> 	note this was printed in print.smooth.spline(*) but not used in
> 	other "smooth.spline" methods.
>
>     o	write.table() handles zero-row and zero-column inputs correctly.
>
>     o	debug() works on trivial functions instead of crashing. (PR#6804)
>
>     o	eval() could alter a data.frame/list second argument, so
> 	with(trees, Girth[1] <- NA) altered 'trees' (and any copy of
> 	'trees' too).
>
>     o	cor() could corrupt memory when the standard deviation was
> 	zero. (PR#7037)
>
>     o	inverse.gaussian() always printed 1/mu^2 as the link function.
>
>     o	constrOptim() now passes ... arguments through optim to the
> 	objective function.
>
>     o	object.size() now has a better estimate for character vectors:
> 	it was in general too low (but only significantly so for
> 	very short character strings) but over-estimated NA and
> 	duplicated elements.
>
>     o	quantile() now interpolates correctly between finite and
> 	infinite values (giving +/-Inf rather than NaN).
>
>     o	library() now gives more informative error messages mentioning
> 	the package being loaded.
>
>     o	Building the reference manual no longer uses roman upright
> 	quotes in typewriter output.
>
>     o	model.frame() no longer builds invalid data frames if the
> 	data contains time series and rows are omitted by na.action.
>
>     o	write.table() did not escape quotes in column names.  (PR#7171)
>
>     o	Range checks missing in recursive assignments using [[ ]].  (PR#7196)
>
>     o	packageStatus() reported partially-installed bundles as
> 	installed.
>
>     o	apply() failed on an array of dimension >=3 when for each
> 	iteration the function returns a named vector of length >=2.
> 	(PR#7205)
>
>     o	The GNOME interface was in some circumstances failing if run
> 	from a menu -- it needed to always specify that R be interactive.
>
>     o	depMtrxToStrings (part of pkgDepends) applied nrow() to a
> 	non-matrix and aborted on the result.
>
>     o	Fix some issues with nonsyntactical names in modelling code
> 	(PR#7202), relating to backquoting.  There are likely more.
>
>     o	Support for S4 classes that extend basic classes has been fixed
> 	in several ways.  as() methods and x at .Data should work better.
>
>     o	hist() and pretty() accept (and ignore) infinite values.  (PR#7220)
>
>     o	It is no longer possible to call gzcon() more than once on a
> 	connection.
>
>     o	t.test() now detects nearly-constant input data.  (PR#7225)
>
>     o	mle() had problems if ndeps or parscale was supplied in the
> 	control arguments for optim().  Also, the profiler is now more
> 	careful to reevaluate modified mle() calls in its parent
> 	environment.
>
>     o	Fix to rendering of accented superscripts and subscripts e.g.,
> 	expression((b[dot(a)])).  (Patch from Uwe Ligges.)
>
>     o	attach(*, pos=1) now gives a warning (and will give an error).
>
>     o	power.*test() now gives an error when 'sig.level' is outside [0,1].
> 	(PR#7245)
>
>     o	Fitting a binomial glm with a matrix response lost the names of
> 	the response, which should have been transferred to the
> 	residuals and fitted values.
>
>     o   print.ts() could get the year wrong because rounding issue
>         (PR#7255)



From stefan.albrecht at allianz.com  Wed Oct  6 13:59:32 2004
From: stefan.albrecht at allianz.com (stefan.albrecht@allianz.com)
Date: Wed, 6 Oct 2004 13:59:32 +0200
Subject: [R] lattice package for R 2.0.0
Message-ID: <OFA51BC07B.FD5C9CB3-ONC1256F25.00402554-C1256F25.0041E01A@inside.allianz.de>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20041006/8fb25484/attachment.pl

From sdavis2 at mail.nih.gov  Wed Oct  6 14:07:38 2004
From: sdavis2 at mail.nih.gov (Sean Davis)
Date: Wed, 6 Oct 2004 08:07:38 -0400
Subject: [R] Repeated measures
Message-ID: <5125203F-1790-11D9-97DA-000A95D7BA10@mail.nih.gov>

I have a data set in which I have 5000 repeated measures on 6 subjects 
over time (varying intervals, but measurements for all individuals are 
at the same times).  There are two states, a "resting" state (the 
majority of the time), and a perturbed state.  I have a continuous 
measurement at each time point for each of the individuals.  I would 
like to determine the "state" for each individual at each time point.  
It looks to me like I should be able to do this with the "hidden" 
command from the "repeated" package 
(http://popgen0146uns50.unimaas.nl/~jlindsey/rcode.html), but I have 
found it a bit confusing to get started.  The distributions in the two 
states are approximately normal with differences in centrality and 
possibly variance (but I can start by assuming similar variances).

Thanks,
Sean



From ripley at stats.ox.ac.uk  Wed Oct  6 14:09:13 2004
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Wed, 6 Oct 2004 13:09:13 +0100 (BST)
Subject: [R] R 2.0.0 is released
In-Reply-To: <200410061944.34324.0034058@fudan.edu.cn>
Message-ID: <Pine.LNX.4.44.0410061258340.24627-100000@gannet.stats>

R does not claim, in 1.9.1 or 2.0.0, to support Chinese characters, so
this is certainly not a bug.  PLEASE read the posting guide and FAQ about 
the definition of a `bug', as well as the need to give useful information 
such as your platform, locale settings (for language-related questions) 
....

It was an extravagant waste of bandwidth to include the whole original
posting, none of which was relevant to your point.

On Wed, 6 Oct 2004, rongguiwong wrote:

> congratulations!
> 
> i download the software and install it.
> it is pretty good.it start much more quickly than 1.9.1.
> but i find a problem.
> i have a data frame,the varible"PROVICE' is in Chinese,but it appears like 
> this:
> [1] \261\261\276\251               \314\354\275\362
>  [3] \272\323\261\261               \311\275\316\367
>  [5] \304\332\303\311\271\305       \301\311\304\376
>  [7] \274\252\301\326               \272\332\301\372\275\255
>  [9] \311\317\272\243               \275\255\313\325
> [11] \325\343\275\255               \260\262\273\325
> [13] \270\243\275\250               \275\255\316\367
> [15] \311\275\266\253               \272\323\304\317
> [17] \272\376\261\261               \272\376\304\317
> [19] \271\343\266\253               \271\343\316\367
> [21] \272\243\304\317               \326\330\307\354
> [23] \313\304\264\250               \271\363\326\335
> [25] \324\306\304\317               \316\367\262\330
> [27] \311\302\316\367               \270\312\313\340
> [29] \307\340\272\243               \304\376\317\304
> [31] \320\302\275\256
> 31 Levels: ????????   ????????   ???????????
 ????????   ????????   ????????   ????????   ????????   ... ????????????
> 
> it seems can not display chinese correctly.but in R1.9.1,it display 
> correctly .in R1.9.1,when i use >data$PROVICE,it display chinese directly,but 
> in R2.0.0,the display numbers like "\261\261\276\251 ".
> is it a bug or something else?

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From ligges at statistik.uni-dortmund.de  Wed Oct  6 14:10:55 2004
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Wed, 06 Oct 2004 14:10:55 +0200
Subject: [R] lattice package for R 2.0.0
In-Reply-To: <OFA51BC07B.FD5C9CB3-ONC1256F25.00402554-C1256F25.0041E01A@inside.allianz.de>
References: <OFA51BC07B.FD5C9CB3-ONC1256F25.00402554-C1256F25.0041E01A@inside.allianz.de>
Message-ID: <4163E0CF.3070400@statistik.uni-dortmund.de>

stefan.albrecht at allianz.com wrote:

> 
> 
> 
> Dear all,
> 
> I am trying to install packages with the new R 2.0.0. However for
> several packages, like MASS, lattice or R2HTML, I get an error like:
> 
> 
>>library(lattice)
> 
> Error in library(lattice) : 'lattice' is not a valid package --
> installed < 2.0.0?


Eh? "lattice" and "MASS" (the latter from bundle "VR") are both 
recommended packages that are both shipped with the released version of 
R. You don't need to download them separately.


> 
> However, I have just downloaded the latest versions from CRAN.
> 
> As far as lattice is concerned, it seems that the Windows binary is
> not the latest version of lattice (0.9-16 vs. 0.10-11).

It is:
http://cran.r-project.org/bin/windows/contrib/2.0/lattice_0.10-11.zip

Where do you look, maybe a mirror that is not completely up to date?

See also http://cran.r-project.org/bin/windows/contrib/checkSummaryWin.html

Uwe Ligges




> Is there any solution to this?
> 
> Many thanks and best regards,
> 
> Stefan
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html



From thpe at hhbio.wasser.tu-dresden.de  Wed Oct  6 14:11:10 2004
From: thpe at hhbio.wasser.tu-dresden.de (Thomas Petzoldt)
Date: Wed, 06 Oct 2004 14:11:10 +0200
Subject: [R] R 2.0.0 (Windows): slow startup over the network
Message-ID: <4163E0DE.5020300@hhbio.wasser.tu-dresden.de>

Hello,

I installed R 2.0.0/Windows on our network server (Win NT 4.0) with all 
CRAN packages and observed a quick startup on this machine (3s).

However, when startet from different client machines (e.g. Win NT 4.0 or 
Win XP Pro SP2, Athlon XP 1700, 100 Mbit Network) the R Gui Window 
appears immediately but then it takes 90 seconds to get the startup 
message and the command prompt.

The R base configuration (without additional packages) takes only 4 
seconds on both, the server and the client.

I disabled the virus scanner, read ?Startup, startet R with the 
--vanilla option, with no effect. Does anyone an have idea, what I can 
do next?

Thank you in advance

Thomas Petzoldt



From ripley at stats.ox.ac.uk  Wed Oct  6 14:16:29 2004
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Wed, 6 Oct 2004 13:16:29 +0100 (BST)
Subject: [R] lattice package for R 2.0.0
In-Reply-To: <OFA51BC07B.FD5C9CB3-ONC1256F25.00402554-C1256F25.0041E01A@inside.allianz.de>
Message-ID: <Pine.LNX.4.44.0410061309290.24627-100000@gannet.stats>

On Wed, 6 Oct 2004 stefan.albrecht at allianz.com wrote:

> I am trying to install packages with the new R 2.0.0. However for

On Windows, not mentioned!  Probably a pre-compiled binary, not mentioned.

> several packages, like MASS, lattice or R2HTML, I get an error like:
> 
> > library(lattice)
> Error in library(lattice) : 'lattice' is not a valid package --
> installed < 2.0.0?
> >
> 
> However, I have just downloaded the latest versions from CRAN.
>
> As far as lattice is concerned, it seems that the Windows binary is
> not the latest version of lattice (0.9-16 vs. 0.10-11).

You must be looking in the wrong place:  
http://cran.r-project.org/bin/windows/contrib/2.0/ has 0.10-11.

Something you have done has overwritten the versions of packages for 2.0.0 
by those for 1.9.x.

There is currently no Windows binary version of R2HTML for 2.0.0, as a
valid source version was only released this morning.

> Is there any solution to this?

Undo whatever you did to cause this.  Preferably start with a clean 
directory and reinstall from rw2000.exe.

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From murdoch at stats.uwo.ca  Wed Oct  6 14:20:29 2004
From: murdoch at stats.uwo.ca (Duncan Murdoch)
Date: Wed, 06 Oct 2004 08:20:29 -0400
Subject: [R] lattice package for R 2.0.0
In-Reply-To: <OFA51BC07B.FD5C9CB3-ONC1256F25.00402554-C1256F25.0041E01A@inside.allianz.de>
References: <OFA51BC07B.FD5C9CB3-ONC1256F25.00402554-C1256F25.0041E01A@inside.allianz.de>
Message-ID: <3ho7m011b9abcrn4seh51g8vvf6tkn6dl1@4ax.com>

On Wed, 6 Oct 2004 13:59:32 +0200, stefan.albrecht at allianz.com wrote :

>
>
>
>
>Dear all,
>
>I am trying to install packages with the new R 2.0.0. However for
>several packages, like MASS, lattice or R2HTML, I get an error like:
>
>> library(lattice)
>Error in library(lattice) : 'lattice' is not a valid package --
>installed < 2.0.0?
>>
>
>However, I have just downloaded the latest versions from CRAN.
>
>As far as lattice is concerned, it seems that the Windows binary is
>not the latest version of lattice (0.9-16 vs. 0.10-11).

Sounds as though you have a bad installation, because lattice 0.10-11
*is* the version in the Windows binary.  Do you have write permission
on the directory where you were installing R?  Do you have a .First
function or some other way of specifying a non-standard location for
the libraries?

Duncan Murdoch



From p.dalgaard at biostat.ku.dk  Wed Oct  6 14:23:16 2004
From: p.dalgaard at biostat.ku.dk (Peter Dalgaard)
Date: 06 Oct 2004 14:23:16 +0200
Subject: [R] lattice package for R 2.0.0
In-Reply-To: <OFA51BC07B.FD5C9CB3-ONC1256F25.00402554-C1256F25.0041E01A@inside.allianz.de>
References: <OFA51BC07B.FD5C9CB3-ONC1256F25.00402554-C1256F25.0041E01A@inside.allianz.de>
Message-ID: <x2655o828b.fsf@biostat.ku.dk>

stefan.albrecht at allianz.com writes:

> Dear all,
> 
> I am trying to install packages with the new R 2.0.0. However for
> several packages, like MASS, lattice or R2HTML, I get an error like:
> 
> > library(lattice)
> Error in library(lattice) : 'lattice' is not a valid package --
> installed < 2.0.0?
> >
> 
> However, I have just downloaded the latest versions from CRAN.
> 
> As far as lattice is concerned, it seems that the Windows binary is
> not the latest version of lattice (0.9-16 vs. 0.10-11).
> 
> Is there any solution to this?

0.10-11 is the version that SHIPS WITH R!

-- 
   O__  ---- Peter Dalgaard             Blegdamsvej 3  
  c/ /'_ --- Dept. of Biostatistics     2200 Cph. N   
 (*) \(*) -- University of Copenhagen   Denmark      Ph: (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)             FAX: (+45) 35327907



From ripley at stats.ox.ac.uk  Wed Oct  6 14:36:04 2004
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Wed, 6 Oct 2004 13:36:04 +0100 (BST)
Subject: [R] R 2.0.0 (Windows): slow startup over the network
In-Reply-To: <4163E0DE.5020300@hhbio.wasser.tu-dresden.de>
Message-ID: <Pine.LNX.4.44.0410061328060.24668-100000@gannet.stats>

On Wed, 6 Oct 2004, Thomas Petzoldt wrote:

> I installed R 2.0.0/Windows on our network server (Win NT 4.0) with all 
> CRAN packages and observed a quick startup on this machine (3s).

That's actually very slow: decent machine are well under 1s.

> However, when startet from different client machines (e.g. Win NT 4.0 or 
> Win XP Pro SP2, Athlon XP 1700, 100 Mbit Network) the R Gui Window 
> appears immediately but then it takes 90 seconds to get the startup 
> message and the command prompt.
> 
> The R base configuration (without additional packages) takes only 4 
> seconds on both, the server and the client.

So it's slower without the additional packages!

> I disabled the virus scanner, read ?Startup, startet R with the 
> --vanilla option, with no effect. Does anyone an have idea, what I can 
> do next?

Read the NEWS file, which says

	As the lazy-loading databases will be consulted often, R
	will be slower if run from a slow network-mounted disc.

and I have only seen this with Windows network-mounted discs. With our
7-year-old Sun Sparc 1 server running Samba it takes about 25s, and that
is a pretty extreme situation (and that server is about to be retired),
which were are solving by installing R 2.0.0 locally.

Your network looks very slow, and maybe needs checking out.  No one 
checking alpha or beta versions has reported any problems, including you.

I do plan to address this in a future version, and did not do for 2.0.0
precisely because no end-user reported any problem during testing.

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From stefan.albrecht at allianz.com  Wed Oct  6 14:38:09 2004
From: stefan.albrecht at allianz.com (stefan.albrecht@allianz.com)
Date: Wed, 6 Oct 2004 14:38:09 +0200
Subject: [R] lattice package for R 2.0.0
Message-ID: <OFBA2DE0D6.D752D181-ONC1256F25.0044D24B-C1256F25.00456953@inside.allianz.de>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20041006/12cdc674/attachment.pl

From murdoch at stats.uwo.ca  Wed Oct  6 14:56:08 2004
From: murdoch at stats.uwo.ca (Duncan Murdoch)
Date: Wed, 06 Oct 2004 08:56:08 -0400
Subject: [R] lattice package for R 2.0.0
In-Reply-To: <OFBA2DE0D6.D752D181-ONC1256F25.0044D24B-C1256F25.00456953@inside.allianz.de>
References: <OFBA2DE0D6.D752D181-ONC1256F25.0044D24B-C1256F25.00456953@inside.allianz.de>
Message-ID: <6oq7m0djfdvv86hb7q96uauuguf6lqvejf@4ax.com>

On Wed, 6 Oct 2004 14:38:09 +0200, stefan.albrecht at allianz.com wrote :

>
>
>
>
>Dear all,
>
>thanks a lot for your help.
>
>Indeed, I have overloaded the MASS and lattice packages already
>shipped with R. However, I have followed the web-site at
>http://cran.at.r-project.org/
>and, as it seems, have been directed to an old version of the
>packages.

Sounds like a cache somewhere is messing you up.  CRAN is listing the
correct version of lattice right now, at 

http://cran.at.r-project.org/src/contrib/Descriptions/lattice.html

Duncan Murdoch



From friendly at yorku.ca  Wed Oct  6 15:18:10 2004
From: friendly at yorku.ca (Michael Friendly)
Date: Wed, 06 Oct 2004 09:18:10 -0400
Subject: [R] read.delim problem with trailing spaces
Message-ID: <4163F092.6070501@yorku.ca>

I'm trying to read a comma delimited dataset that uses '.' for NA.  I 
found that if the last field on a line was a missing '.'
it was not read as NA, but just a '.', and the life variable was made a 
factor.  The data looks like this,

income,imr,region,oilexprt,imr80,gnp80,life
Afghanistan,75,400.0,4,0,185.0,.,37.5
Algeria,400,86.3,2,1,20.5,1920,50.7
Argentina,1191,59.6,1,0,40.8,2390,67.1
Australia,3426,26.7,4,0,12.5,9820,71.0
Austria,3350,23.7,3,0,14.8,10230,70.4
Bangladesh,100,124.3,4,0,139.0,120,.
Belgium,3346,17.0,3,0,11.2,12180,70.6
Benin,81,109.6,2,0,109.6,300,.
Bolivia,200,60.4,1,0,77.3,570,49.7
Brazil,425,170.0,1,0,84.0,2020,60.7
Britain,2503,17.5,3,0,12.6,7920,72.0
Burma,73,200.0,4,0,195.0,180,42.3
  ...

and I used
 > nations <- 
read.delim("~/sasuser/data/nations2.dat",na.strings=".",row.name=1,sep=",",header=TRUE)

> nations[1:10,]
            income   imr region oilexprt imr80 gnp80 life
Afghanistan     75 400.0      4        0 185.0    NA 37.5
Algeria        400  86.3      2        1  20.5  1920 50.7
Argentina     1191  59.6      1        0  40.8  2390 67.1
Australia     3426  26.7      4        0  12.5  9820 71.0
Austria       3350  23.7      3        0  14.8 10230 70.4
Bangladesh     100 124.3      4        0 139.0   120   .
Belgium       3346  17.0      3        0  11.2 12180 70.6
Benin           81 109.6      2        0 109.6   300   .
Bolivia        200  60.4      1        0  77.3   570 49.7
Brazil         425 170.0      1        0  84.0  2020 60.7
> summary(nations$life)
  .  27.0 31.6 32.0 32.6 34.5 35.0 36.0 36.7 36.9 37.1 37.2 37.5 38.5 38.8 40.5
   2    1    1    1    1    1    2    1    1    1    1    1    1    3    1    1
40.6 41.0 41.2 42.3 43.5 43.7 44.9 45.1 46.8 47.5 47.6 49.0 49.7 49.9 50.0 50.5
   1    6    1    4    1    1    1    1    1    3    1    3    1    1    2    1


After much hair-pulling, I discovered that the data lines for Bangladesh and Benin contained a trailing space after the '.'.  Removing those made the problem go away, but that shouldn't happen and I wonder if this is
still a potential problem for others.   I'm using R 1.8.1.

-Michael

-- 
Michael Friendly     Email: friendly at yorku.ca 
Professor, Psychology Dept.
York University      Voice: 416 736-5115 x66249 Fax: 416 736-5814
4700 Keele Street    http://www.math.yorku.ca/SCS/friendly.html
Toronto, ONT  M3J 1P3 CANADA



From thpe at hhbio.wasser.tu-dresden.de  Wed Oct  6 15:18:30 2004
From: thpe at hhbio.wasser.tu-dresden.de (Thomas Petzoldt)
Date: Wed, 06 Oct 2004 15:18:30 +0200
Subject: [R] R 2.0.0 (Windows): slow startup over the network
In-Reply-To: <Pine.LNX.4.44.0410061328060.24668-100000@gannet.stats>
References: <Pine.LNX.4.44.0410061328060.24668-100000@gannet.stats>
Message-ID: <4163F0A6.8030808@hhbio.wasser.tu-dresden.de>

Prof Brian Ripley wrote:

 > On Wed, 6 Oct 2004, Thomas Petzoldt wrote:
 >
 >
>> I installed R 2.0.0/Windows on our network server (Win NT 4.0) with
>> all CRAN packages and observed a quick startup on this machine
>> (3s).
 >
 > That's actually very slow: decent machine are well under 1s.

The NT server is optimized for network traffic, not for running local 
applications and it's not brand new.

>> However, when started from different client machines (e.g. Win NT
>> 4.0 or Win XP Pro SP2, Athlon XP 1700, 100 Mbit Network) the R Gui
>> Window appears immediately but then it takes 90 seconds to get the
>> startup message and the command prompt.
>> 
>> The R base configuration (without additional packages) takes only 4
>>  seconds on both, the server and the client.
 >
 > So it's slower without the additional packages!

Yes, but the difference lies within the confidence interval (median of 
two experiments with sample size n=3). The load of the server during the 
test was very low, but not zero and there might be some autocorrelation 
between the trials.

> Read the NEWS file, which says
> 
> As the lazy-loading databases will be consulted often, R will be
> slower if run from a slow network-mounted disc.

I see, but I don't yet understand why additional packages which are 
*not* loaded do require startup time.

> and I have only seen this with Windows network-mounted discs. With
> our 7-year-old Sun Sparc 1 server running Samba it takes about 25s,
> and that is a pretty extreme situation (and that server is about to
> be retired), which were are solving by installing R 2.0.0 locally.

This is not a good solution, but I may try a mixed approach. Small 
version on the network and complete versions on dedicated clients.

> Your network looks very slow, and maybe needs checking out. No one 
> checking alpha or beta versions has reported any problems, including
> you.

During the testing phase with the development version of R 2.0 I 
installed only the necessary packages, not all, but from time to time 
it's nice to look into some more contributed packages.

I can do some additional tests, e.g. on a Samba share and see, if I can 
isolate the problem.

> I do plan to address this in a future version, and did not do for
> 2.0.0 precisely because no end-user reported any problem during
> testing.

Thank you very much!

Thomas P.


-- 
Thomas Petzoldt
Dresden University of Technology
Institute of Hydrobiology          petzoldt at rcs.urz.tu-dresden.de
01062 Dresden                      http://www.tu-dresden.de/fghhihb/



From ripley at stats.ox.ac.uk  Wed Oct  6 15:35:46 2004
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Wed, 6 Oct 2004 14:35:46 +0100 (BST)
Subject: [R] R 2.0.0 (Windows): slow startup over the network
In-Reply-To: <4163F0A6.8030808@hhbio.wasser.tu-dresden.de>
Message-ID: <Pine.LNX.4.44.0410061429520.24910-100000@gannet.stats>

On Wed, 6 Oct 2004, Thomas Petzoldt wrote:

> Prof Brian Ripley wrote:
> 
>  > On Wed, 6 Oct 2004, Thomas Petzoldt wrote:
>  >
>  >
> >> I installed R 2.0.0/Windows on our network server (Win NT 4.0) with
> >> all CRAN packages and observed a quick startup on this machine
> >> (3s).
>  >
>  > That's actually very slow: decent machine are well under 1s.
> 
> The NT server is optimized for network traffic, not for running local 
> applications and it's not brand new.
> 
> >> However, when started from different client machines (e.g. Win NT
> >> 4.0 or Win XP Pro SP2, Athlon XP 1700, 100 Mbit Network) the R Gui
> >> Window appears immediately but then it takes 90 seconds to get the
> >> startup message and the command prompt.
> >> 
> >> The R base configuration (without additional packages) takes only 4
> >>  seconds on both, the server and the client.
>  >
>  > So it's slower without the additional packages!
> 
> Yes, but the difference lies within the confidence interval (median of 
> two experiments with sample size n=3). The load of the server during the 
> test was very low, but not zero and there might be some autocorrelation 
> between the trials.
> 
> > Read the NEWS file, which says
> > 
> > As the lazy-loading databases will be consulted often, R will be
> > slower if run from a slow network-mounted disc.
> 
> I see, but I don't yet understand why additional packages which are 
> *not* loaded do require startup time.

The base configuration means package base only, surely?  Or did you mean 
`default' or `standard'?  If the latter, it does look like a problem with 
remote access to a file system that has a large directory.

I've just checked and during startup R accesses lazy-load databases about 
630 times, which is enough to matter on some systems.  With
R_DEFAULT_PACKAGES=NULL, that is just base, it is about 60 times.

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From ripley at stats.ox.ac.uk  Wed Oct  6 15:41:42 2004
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Wed, 6 Oct 2004 14:41:42 +0100 (BST)
Subject: [R] read.delim problem with trailing spaces
In-Reply-To: <4163F092.6070501@yorku.ca>
Message-ID: <Pine.LNX.4.44.0410061438170.24910-100000@gannet.stats>

On Wed, 6 Oct 2004, Michael Friendly wrote:

> I'm trying to read a comma delimited dataset that uses '.' for NA.  I 
> found that if the last field on a line was a missing '.'
> it was not read as NA, but just a '.', and the life variable was made a 
> factor.  The data looks like this,
> 
> income,imr,region,oilexprt,imr80,gnp80,life
> Afghanistan,75,400.0,4,0,185.0,.,37.5
> Algeria,400,86.3,2,1,20.5,1920,50.7
> Argentina,1191,59.6,1,0,40.8,2390,67.1
> Australia,3426,26.7,4,0,12.5,9820,71.0
> Austria,3350,23.7,3,0,14.8,10230,70.4
> Bangladesh,100,124.3,4,0,139.0,120,.
> Belgium,3346,17.0,3,0,11.2,12180,70.6
> Benin,81,109.6,2,0,109.6,300,.
> Bolivia,200,60.4,1,0,77.3,570,49.7
> Brazil,425,170.0,1,0,84.0,2020,60.7
> Britain,2503,17.5,3,0,12.6,7920,72.0
> Burma,73,200.0,4,0,195.0,180,42.3
>   ...
> 
> and I used
>  > nations <- 
> read.delim("~/sasuser/data/nations2.dat",na.strings=".",row.name=1,sep=",",header=TRUE)
> 
> > nations[1:10,]
>             income   imr region oilexprt imr80 gnp80 life
> Afghanistan     75 400.0      4        0 185.0    NA 37.5
> Algeria        400  86.3      2        1  20.5  1920 50.7
> Argentina     1191  59.6      1        0  40.8  2390 67.1
> Australia     3426  26.7      4        0  12.5  9820 71.0
> Austria       3350  23.7      3        0  14.8 10230 70.4
> Bangladesh     100 124.3      4        0 139.0   120   .
> Belgium       3346  17.0      3        0  11.2 12180 70.6
> Benin           81 109.6      2        0 109.6   300   .
> Bolivia        200  60.4      1        0  77.3   570 49.7
> Brazil         425 170.0      1        0  84.0  2020 60.7
> > summary(nations$life)
>   .  27.0 31.6 32.0 32.6 34.5 35.0 36.0 36.7 36.9 37.1 37.2 37.5 38.5 38.8 40.5
>    2    1    1    1    1    1    2    1    1    1    1    1    1    3    1    1
> 40.6 41.0 41.2 42.3 43.5 43.7 44.9 45.1 46.8 47.5 47.6 49.0 49.7 49.9 50.0 50.5
>    1    6    1    4    1    1    1    1    1    3    1    3    1    1    2    1
> 
> 
> After much hair-pulling, I discovered that the data lines for Bangladesh
> and Benin contained a trailing space after the '.'.  Removing those made
> the problem go away, but that shouldn't happen and I wonder if this is
> still a potential problem for others.  I'm using R 1.8.1.

It should happen.  The entry there is ". " and that is not an NA string.
If you use a non-whitespace delimiter, all whitespace is significant.

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From ligges at statistik.uni-dortmund.de  Wed Oct  6 15:50:34 2004
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Wed, 06 Oct 2004 15:50:34 +0200
Subject: [R] R 2.0.0 (Windows): slow startup over the network
In-Reply-To: <Pine.LNX.4.44.0410061328060.24668-100000@gannet.stats>
References: <Pine.LNX.4.44.0410061328060.24668-100000@gannet.stats>
Message-ID: <4163F82A.6030806@statistik.uni-dortmund.de>

Prof Brian Ripley wrote:

> On Wed, 6 Oct 2004, Thomas Petzoldt wrote:
> 
> 
>>I installed R 2.0.0/Windows on our network server (Win NT 4.0) with all 
>>CRAN packages and observed a quick startup on this machine (3s).
> 
> 
> That's actually very slow: decent machine are well under 1s.
> 
> 
>>However, when startet from different client machines (e.g. Win NT 4.0 or 
>>Win XP Pro SP2, Athlon XP 1700, 100 Mbit Network) the R Gui Window 
>>appears immediately but then it takes 90 seconds to get the startup 
>>message and the command prompt.
>>
>>The R base configuration (without additional packages) takes only 4 
>>seconds on both, the server and the client.
> 

Just for those who are interested:

R-2.0.0 with the complete CRAN collection, Bioconductor and some other 
stuff installed on a capable Windows 2003 Server using a client with 
WinNT4.0:
The RGui window appears at once, the prompt within 5 seconds.


As Brian Ripley already pointed out, there must be a bottleneck at your 
side, either the server performance or network traffic.
I know from own experiences that WinNT 4.0 Server were not as good as 
the 2003 Server in file delivery, but I don't think it can be that dramatic.

Uwe Ligges



> So it's slower without the additional packages!
> 
> 
>>I disabled the virus scanner, read ?Startup, startet R with the 
>>--vanilla option, with no effect. Does anyone an have idea, what I can 
>>do next?
> 
> 
> Read the NEWS file, which says
> 
> 	As the lazy-loading databases will be consulted often, R
> 	will be slower if run from a slow network-mounted disc.
> 
> and I have only seen this with Windows network-mounted discs. With our
> 7-year-old Sun Sparc 1 server running Samba it takes about 25s, and that
> is a pretty extreme situation (and that server is about to be retired),
> which were are solving by installing R 2.0.0 locally.
> 
> Your network looks very slow, and maybe needs checking out.  No one 
> checking alpha or beta versions has reported any problems, including you.
> 
> I do plan to address this in a future version, and did not do for 2.0.0
> precisely because no end-user reported any problem during testing.
>



From jfox at mcmaster.ca  Wed Oct  6 15:59:19 2004
From: jfox at mcmaster.ca (John Fox)
Date: Wed, 6 Oct 2004 09:59:19 -0400
Subject: [R] read.delim problem with trailing spaces
In-Reply-To: <4163F092.6070501@yorku.ca>
Message-ID: <20041006135914.HNPN4905.tomts13-srv.bellnexxia.net@JohnDesktop8300>

Dear Mike,

This is a trap, but it's not a bug, and to "correct" it wouldn't be
appropriate, I think. That is, the string ". " wasn't declared as NA. One
could do the following to avoid the problem:

> read.csv("c:/temp/test.txt", na.strings=".", strip.white=TRUE)
            income   imr region oilexprt imr80 gnp80 life
Afghanistan     75 400.0      4        0 185.0    NA 37.5
Algeria        400  86.3      2        1  20.5  1920 50.7
Argentina     1191  59.6      1        0  40.8  2390 67.1
Australia     3426  26.7      4        0  12.5  9820 71.0
Austria       3350  23.7      3        0  14.8 10230 70.4
Bangladesh     100 124.3      4        0 139.0   120   NA
Belgium       3346  17.0      3        0  11.2 12180 70.6
Benin           81 109.6      2        0 109.6   300   NA
Bolivia        200  60.4      1        0  77.3   570 49.7
Brazil         425 170.0      1        0  84.0  2020 60.7
Britain       2503  17.5      3        0  12.6  7920 72.0
Burma           73 200.0      4        0 195.0   180 42.3

Regards,
 John



> -----Original Message-----
> From: r-help-bounces at stat.math.ethz.ch 
> [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of 
> Michael Friendly
> Sent: Wednesday, October 06, 2004 8:18 AM
> To: R-help
> Subject: [R] read.delim problem with trailing spaces
> 
> I'm trying to read a comma delimited dataset that uses '.' 
> for NA.  I found that if the last field on a line was a missing '.'
> it was not read as NA, but just a '.', and the life variable 
> was made a factor.  The data looks like this,
> 
> income,imr,region,oilexprt,imr80,gnp80,life
> Afghanistan,75,400.0,4,0,185.0,.,37.5
> Algeria,400,86.3,2,1,20.5,1920,50.7
> Argentina,1191,59.6,1,0,40.8,2390,67.1
> Australia,3426,26.7,4,0,12.5,9820,71.0
> Austria,3350,23.7,3,0,14.8,10230,70.4
> Bangladesh,100,124.3,4,0,139.0,120,.
> Belgium,3346,17.0,3,0,11.2,12180,70.6
> Benin,81,109.6,2,0,109.6,300,.
> Bolivia,200,60.4,1,0,77.3,570,49.7
> Brazil,425,170.0,1,0,84.0,2020,60.7
> Britain,2503,17.5,3,0,12.6,7920,72.0
> Burma,73,200.0,4,0,195.0,180,42.3
>   ...
> 
> and I used
>  > nations <-
> read.delim("~/sasuser/data/nations2.dat",na.strings=".",row.na
> me=1,sep=",",header=TRUE)
> 
> > nations[1:10,]
>             income   imr region oilexprt imr80 gnp80 life
> Afghanistan     75 400.0      4        0 185.0    NA 37.5
> Algeria        400  86.3      2        1  20.5  1920 50.7
> Argentina     1191  59.6      1        0  40.8  2390 67.1
> Australia     3426  26.7      4        0  12.5  9820 71.0
> Austria       3350  23.7      3        0  14.8 10230 70.4
> Bangladesh     100 124.3      4        0 139.0   120   .
> Belgium       3346  17.0      3        0  11.2 12180 70.6
> Benin           81 109.6      2        0 109.6   300   .
> Bolivia        200  60.4      1        0  77.3   570 49.7
> Brazil         425 170.0      1        0  84.0  2020 60.7
> > summary(nations$life)
>   .  27.0 31.6 32.0 32.6 34.5 35.0 36.0 36.7 36.9 37.1 37.2 
> 37.5 38.5 38.8 40.5
>    2    1    1    1    1    1    2    1    1    1    1    1   
>  1    3    1    1
> 40.6 41.0 41.2 42.3 43.5 43.7 44.9 45.1 46.8 47.5 47.6 49.0 
> 49.7 49.9 50.0 50.5
>    1    6    1    4    1    1    1    1    1    3    1    3   
>  1    1    2    1
> 
> 
> After much hair-pulling, I discovered that the data lines for 
> Bangladesh and Benin contained a trailing space after the 
> '.'.  Removing those made the problem go away, but that 
> shouldn't happen and I wonder if this is
> still a potential problem for others.   I'm using R 1.8.1.
> 
> -Michael
> 
> -- 
> Michael Friendly     Email: friendly at yorku.ca 
> Professor, Psychology Dept.
> York University      Voice: 416 736-5115 x66249 Fax: 416 736-5814
> 4700 Keele Street    http://www.math.yorku.ca/SCS/friendly.html
> Toronto, ONT  M3J 1P3 CANADA
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html



From nini at npgcable.com  Wed Oct  6 16:10:41 2004
From: nini at npgcable.com (Nancy Hornewer)
Date: Wed, 6 Oct 2004 07:10:41 -0700
Subject: [R] installation help for mac os x
In-Reply-To: <Pine.LNX.4.44.0410050823200.19087-100000@gannet.stats>
References: <Pine.LNX.4.44.0410050823200.19087-100000@gannet.stats>
Message-ID: <81B686D5-17A1-11D9-A0EF-000A95C43AAC@npgcable.com>

Hi,

Thanks to everyone that replied! Sorry I didn't respond sooner, but I 
was out of town. I did install the X11 SDK package and that took care 
of the subsequent problems with Tk. However, as suggested by Prof. 
RIpley, I could have just installed the binary from the link below 
(just looked again at the website now and there will be a newer version 
available on 10-/15/04). I do need to learn more about unix... Anyway, 
it's working great and I'm excited to get started. I'll download all 
the manuals and start using R.

I do have one more question regarding the contributed packages for base 
distribution. I will mostly be using this software for basic summary 
stats, plots (histograms, boxplots, scatterplots), regression, 
non-parametric tests, and time series. Will I need to install any of 
these additional packages?

Thanks again to everyone for their assistance.

Nancy



On Oct 5, 2004, at 12:30 AM, Prof Brian Ripley wrote:

> There is a binary installation available at
>
> 	http://cran.r-project.org/bin/macosx/
>
> so do you actually need to be building from the sources?  If you do, 
> your
> immediate problem is installing Tk on MacOS X, and that's not something
> relevant to R-help.  However, Tk on Unix needs X11, whose installation 
> you
> omitted.
>
> On Mon, 4 Oct 2004, Nancy Hornewer wrote:
>
>> Hi,
>>
>> I'm  trying to install R for Mac OS X. I printed out the Mac OS X FAQ
>> (version 1.9-1 2004-03-22) and have been following those instructions
>> step-by-step. I'm a _complete_ novice when it comes to non-mac type
>> software installations and am not very unix literate, but I'm hoping I
>> can get this to work. I'm running a dual 1.8 GHz mac with the latest
>> version of panther.
>>
>> Here's my progress to date:
>> (1) Installed the  C/C++ compiler from the Apple Developer Connection
>> (v. 1.5) - no problems
>> (2) Installed the Fortran compiler g77 for gcc 3.3 (v. 3.4) - no
>> problems
>> (3) Installed the libreadline  library (v. 4.3) [note faq said
>> optional, but I saw that it was recommended in one of the r-help
>> archives] - no problems
>> (4) Didn't install X11 SDK package because it was optional and I don't
>> think I need that (just want to run this on my machine)
>> (5) Installed tcl8.4.7 - no problems
>> (6) Tried installing tk8.4.7 but received many many error messages 
>> when
>> I typed "make". Here is the last portion of those messages:
>>
>> /tk8.4.7/generic/tk3d.c: In function `Tk_Get3DBorderFromObj':
>> /tk8.4.7/generic/tk3d.c:1262: error: `borderPtr' undeclared (first use
>> in this function)
>> /tk8.4.7/generic/tk3d.c:1264: error: invalid operands to binary *
>> /tk8.4.7/generic/tk3d.c:1264: error: parse error before ')' token
>> /tk8.4.7/generic/tk3d.c:1277: error: parse error before ')' token
>> /tk8.4.7/generic/tk3d.c:1280: error: parse error before ')' token
>> /tk8.4.7/generic/tk3d.c:1280: error: parse error before ')' token
>> /tk8.4.7/generic/tk3d.c:1281: error: parse error before ')' token
>> /tk8.4.7/generic/tk3d.c:1301: error: request for member `borderTable'
>> in something not a structure or union
>> /tk8.4.7/generic/tk3d.c:1301: error: request for member `borderTable'
>> in something not a structure or union
>> /tk8.4.7/generic/tk3d.c:1305: error: parse error before ')' token
>> /tk8.4.7/generic/tk3d.c:1307: error: parse error before ')' token
>> /tk8.4.7/generic/tk3d.c:1307: error: parse error before ')' token
>> /tk8.4.7/generic/tk3d.c:1308: error: parse error before ')' token
>> /tk8.4.7/generic/tk3d.c: In function `TkDebugBorder':
>> /tk8.4.7/generic/tk3d.c:1388: error: `borderPtr' undeclared (first use
>> in this function)
>> /tk8.4.7/generic/tk3d.c:1391: error: invalid operands to binary *
>> /tk8.4.7/generic/tk3d.c:1391: error: parse error before ')' token
>> /tk8.4.7/generic/tk3d.c:1394: error: request for member `borderTable'
>> in something not a structure or union
>> /tk8.4.7/generic/tk3d.c:1394: error: request for member `borderTable'
>> in something not a structure or union
>> /tk8.4.7/generic/tk3d.c:1396: error: parse error before ')' token
>> /usr/include/ctype.h: At top level:
>> /tk8.4.7/generic/tk3d.c:1134: error: storage size of `shiftTable' 
>> isn't
>> known
>> /tk8.4.7/generic/tk3d.c:31: warning: `BorderInit' declared `static' 
>> but
>> never defined
>> /tk8.4.7/generic/tk3d.c:36: warning: `Intersect' declared `static' but
>> never defined
>> /tk8.4.7/generic/tk3d.c:39: warning: `ShiftLine' declared `static' but
>> never defined
>> {standard input}:1940:Ignoring attempt to re-define symbol.
>> {standard input}:1943:Ignoring attempt to re-define symbol.
>> {standard input}:1979:Ignoring attempt to re-define symbol.
>> {standard input}:1982:Ignoring attempt to re-define symbol.
>> {standard input}:1985:Ignoring attempt to re-define symbol.
>> {standard input}:1988:Ignoring attempt to re-define symbol.
>> make: *** [tk3d.o] Error 1
>> cm-24-121-11-63:/tk8.4.7/unix nancy$
>>
>> I'm not sure why this didn't work and tried the "make" command twice 
>> (I
>> hope I didn't mess anything up by running that command twice). I can
>> email the entire list of errors if necessary, but it is really long.
>>
>> After the above failed, I did install the next item on the faq list:
>> TeX and that seemed to work fine.
>>
>> I haven't yet tried installing the R sources yet because I was afraid
>> that it might have to be installed after tk.
>>
>> Any assistance that you can provide would be greatly appreciated.
>>
>> Thank you,
>> Nancy
>>
>> ______________________________________________
>> R-help at stat.math.ethz.ch mailing list
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide! 
>> http://www.R-project.org/posting-guide.html
>>
>>
>
> -- 
> Brian D. Ripley,                  ripley at stats.ox.ac.uk
> Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
> University of Oxford,             Tel:  +44 1865 272861 (self)
> 1 South Parks Road,                     +44 1865 272866 (PA)
> Oxford OX1 3TG, UK                Fax:  +44 1865 272595
>
>



From sdavis2 at mail.nih.gov  Wed Oct  6 16:20:38 2004
From: sdavis2 at mail.nih.gov (Sean Davis)
Date: Wed, 6 Oct 2004 10:20:38 -0400
Subject: [R] installation help for mac os x
In-Reply-To: <81B686D5-17A1-11D9-A0EF-000A95C43AAC@npgcable.com>
References: <Pine.LNX.4.44.0410050823200.19087-100000@gannet.stats>
	<81B686D5-17A1-11D9-A0EF-000A95C43AAC@npgcable.com>
Message-ID: <E54EF209-17A2-11D9-97DA-000A95D7BA10@mail.nih.gov>

The authors have done if all for us--you don't need to download all the 
manuals.  They are included.  Just type help.start() in the window.  A 
web browser will magically open with all the R-help.

Sean

On Oct 6, 2004, at 10:10 AM, Nancy Hornewer wrote:

> Hi,
>
> Thanks to everyone that replied! Sorry I didn't respond sooner, but I 
> was out of town. I did install the X11 SDK package and that took care 
> of the subsequent problems with Tk. However, as suggested by Prof. 
> RIpley, I could have just installed the binary from the link below 
> (just looked again at the website now and there will be a newer 
> version available on 10-/15/04). I do need to learn more about unix... 
> Anyway, it's working great and I'm excited to get started. I'll 
> download all the manuals and start using R.
>
> I do have one more question regarding the contributed packages for 
> base distribution. I will mostly be using this software for basic 
> summary stats, plots (histograms, boxplots, scatterplots), regression, 
> non-parametric tests, and time series. Will I need to install any of 
> these additional packages?
>
> Thanks again to everyone for their assistance.
>
> Nancy
>
>
>
> On Oct 5, 2004, at 12:30 AM, Prof Brian Ripley wrote:
>
>> There is a binary installation available at
>>
>> 	http://cran.r-project.org/bin/macosx/
>>
>> so do you actually need to be building from the sources?  If you do, 
>> your
>> immediate problem is installing Tk on MacOS X, and that's not 
>> something
>> relevant to R-help.  However, Tk on Unix needs X11, whose 
>> installation you
>> omitted.
>>
>> On Mon, 4 Oct 2004, Nancy Hornewer wrote:
>>
>>> Hi,
>>>
>>> I'm  trying to install R for Mac OS X. I printed out the Mac OS X FAQ
>>> (version 1.9-1 2004-03-22) and have been following those instructions
>>> step-by-step. I'm a _complete_ novice when it comes to non-mac type
>>> software installations and am not very unix literate, but I'm hoping 
>>> I
>>> can get this to work. I'm running a dual 1.8 GHz mac with the latest
>>> version of panther.
>>>
>>> Here's my progress to date:
>>> (1) Installed the  C/C++ compiler from the Apple Developer Connection
>>> (v. 1.5) - no problems
>>> (2) Installed the Fortran compiler g77 for gcc 3.3 (v. 3.4) - no
>>> problems
>>> (3) Installed the libreadline  library (v. 4.3) [note faq said
>>> optional, but I saw that it was recommended in one of the r-help
>>> archives] - no problems
>>> (4) Didn't install X11 SDK package because it was optional and I 
>>> don't
>>> think I need that (just want to run this on my machine)
>>> (5) Installed tcl8.4.7 - no problems
>>> (6) Tried installing tk8.4.7 but received many many error messages 
>>> when
>>> I typed "make". Here is the last portion of those messages:
>>>
>>> /tk8.4.7/generic/tk3d.c: In function `Tk_Get3DBorderFromObj':
>>> /tk8.4.7/generic/tk3d.c:1262: error: `borderPtr' undeclared (first 
>>> use
>>> in this function)
>>> /tk8.4.7/generic/tk3d.c:1264: error: invalid operands to binary *
>>> /tk8.4.7/generic/tk3d.c:1264: error: parse error before ')' token
>>> /tk8.4.7/generic/tk3d.c:1277: error: parse error before ')' token
>>> /tk8.4.7/generic/tk3d.c:1280: error: parse error before ')' token
>>> /tk8.4.7/generic/tk3d.c:1280: error: parse error before ')' token
>>> /tk8.4.7/generic/tk3d.c:1281: error: parse error before ')' token
>>> /tk8.4.7/generic/tk3d.c:1301: error: request for member `borderTable'
>>> in something not a structure or union
>>> /tk8.4.7/generic/tk3d.c:1301: error: request for member `borderTable'
>>> in something not a structure or union
>>> /tk8.4.7/generic/tk3d.c:1305: error: parse error before ')' token
>>> /tk8.4.7/generic/tk3d.c:1307: error: parse error before ')' token
>>> /tk8.4.7/generic/tk3d.c:1307: error: parse error before ')' token
>>> /tk8.4.7/generic/tk3d.c:1308: error: parse error before ')' token
>>> /tk8.4.7/generic/tk3d.c: In function `TkDebugBorder':
>>> /tk8.4.7/generic/tk3d.c:1388: error: `borderPtr' undeclared (first 
>>> use
>>> in this function)
>>> /tk8.4.7/generic/tk3d.c:1391: error: invalid operands to binary *
>>> /tk8.4.7/generic/tk3d.c:1391: error: parse error before ')' token
>>> /tk8.4.7/generic/tk3d.c:1394: error: request for member `borderTable'
>>> in something not a structure or union
>>> /tk8.4.7/generic/tk3d.c:1394: error: request for member `borderTable'
>>> in something not a structure or union
>>> /tk8.4.7/generic/tk3d.c:1396: error: parse error before ')' token
>>> /usr/include/ctype.h: At top level:
>>> /tk8.4.7/generic/tk3d.c:1134: error: storage size of `shiftTable' 
>>> isn't
>>> known
>>> /tk8.4.7/generic/tk3d.c:31: warning: `BorderInit' declared `static' 
>>> but
>>> never defined
>>> /tk8.4.7/generic/tk3d.c:36: warning: `Intersect' declared `static' 
>>> but
>>> never defined
>>> /tk8.4.7/generic/tk3d.c:39: warning: `ShiftLine' declared `static' 
>>> but
>>> never defined
>>> {standard input}:1940:Ignoring attempt to re-define symbol.
>>> {standard input}:1943:Ignoring attempt to re-define symbol.
>>> {standard input}:1979:Ignoring attempt to re-define symbol.
>>> {standard input}:1982:Ignoring attempt to re-define symbol.
>>> {standard input}:1985:Ignoring attempt to re-define symbol.
>>> {standard input}:1988:Ignoring attempt to re-define symbol.
>>> make: *** [tk3d.o] Error 1
>>> cm-24-121-11-63:/tk8.4.7/unix nancy$
>>>
>>> I'm not sure why this didn't work and tried the "make" command twice 
>>> (I
>>> hope I didn't mess anything up by running that command twice). I can
>>> email the entire list of errors if necessary, but it is really long.
>>>
>>> After the above failed, I did install the next item on the faq list:
>>> TeX and that seemed to work fine.
>>>
>>> I haven't yet tried installing the R sources yet because I was afraid
>>> that it might have to be installed after tk.
>>>
>>> Any assistance that you can provide would be greatly appreciated.
>>>
>>> Thank you,
>>> Nancy
>>>
>>> ______________________________________________
>>> R-help at stat.math.ethz.ch mailing list
>>> https://stat.ethz.ch/mailman/listinfo/r-help
>>> PLEASE do read the posting guide! 
>>> http://www.R-project.org/posting-guide.html
>>>
>>>
>>
>> -- 
>> Brian D. Ripley,                  ripley at stats.ox.ac.uk
>> Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
>> University of Oxford,             Tel:  +44 1865 272861 (self)
>> 1 South Parks Road,                     +44 1865 272866 (PA)
>> Oxford OX1 3TG, UK                Fax:  +44 1865 272595
>>
>>
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html



From murdoch at stats.uwo.ca  Wed Oct  6 16:32:54 2004
From: murdoch at stats.uwo.ca (Duncan Murdoch)
Date: Wed, 06 Oct 2004 10:32:54 -0400
Subject: [R] R 2.0.0 (Windows): slow startup over the network
In-Reply-To: <4163F82A.6030806@statistik.uni-dortmund.de>
References: <Pine.LNX.4.44.0410061328060.24668-100000@gannet.stats>
	<4163F82A.6030806@statistik.uni-dortmund.de>
Message-ID: <sd08m0d4mk4rrff98rcmq5uaroqg76epe3@4ax.com>

On Wed, 06 Oct 2004 15:50:34 +0200, Uwe Ligges
<ligges at statistik.uni-dortmund.de> wrote :

>R-2.0.0 with the complete CRAN collection, Bioconductor and some other 
>stuff installed on a capable Windows 2003 Server using a client with 
>WinNT4.0:
>The RGui window appears at once, the prompt within 5 seconds.
>
>
>As Brian Ripley already pointed out, there must be a bottleneck at your 
>side, either the server performance or network traffic.
>I know from own experiences that WinNT 4.0 Server were not as good as 
>the 2003 Server in file delivery, but I don't think it can be that dramatic.

I know almost nothing about Windows network administration.  Is there
a way to log activity from a particular client, so you can see what
it's requesting, and how long it takes to service the requests?

Duncan Murdoch



From ripley at stats.ox.ac.uk  Wed Oct  6 16:50:04 2004
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Wed, 6 Oct 2004 15:50:04 +0100 (BST)
Subject: [R] R 2.0.0 (Windows): slow startup over the network
In-Reply-To: <sd08m0d4mk4rrff98rcmq5uaroqg76epe3@4ax.com>
Message-ID: <Pine.LNX.4.44.0410061539140.12290-100000@gannet.stats>

On Wed, 6 Oct 2004, Duncan Murdoch wrote:

> On Wed, 06 Oct 2004 15:50:34 +0200, Uwe Ligges
> <ligges at statistik.uni-dortmund.de> wrote :
> 
> >R-2.0.0 with the complete CRAN collection, Bioconductor and some other 
> >stuff installed on a capable Windows 2003 Server using a client with 
> >WinNT4.0:
> >The RGui window appears at once, the prompt within 5 seconds.
> >
> >
> >As Brian Ripley already pointed out, there must be a bottleneck at your 
> >side, either the server performance or network traffic.
> >I know from own experiences that WinNT 4.0 Server were not as good as 
> >the 2003 Server in file delivery, but I don't think it can be that dramatic.
> 
> I know almost nothing about Windows network administration.  Is there
> a way to log activity from a particular client, so you can see what
> it's requesting, and how long it takes to service the requests?

Given that we run a Samba server on Solaris (and on Linux) and only use a 
Windows 2000/2003 server for authentication and profiles, I did this 
from the client.  FileMon is one of a great set of utilities from
www.sysinternals.com which reports all file accesses, time-stamped.

Since we now have established that Thomas meant the default configuration
(not just base), 4 secs is tolerable and it looks like a server problem
(rather than a network problem). I would defragment the volume in case the
R\library directory has got badly fragmented.

For the record, R 2.0.0 took 2s to start on a Linux 500MHz Celeron box 
over our network, and takes 0.62s to start on a 2.6GHz P4 under Windows XP 
with a fast local disc (and about 0.38s on the same box under FC2 Linux).

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From tlumley at u.washington.edu  Wed Oct  6 17:13:10 2004
From: tlumley at u.washington.edu (Thomas Lumley)
Date: Wed, 6 Oct 2004 08:13:10 -0700 (PDT)
Subject: [R] Problems with merge
In-Reply-To: <41637F1D.20506@mail.jnu.ac.in>
References: <41637F1D.20506@mail.jnu.ac.in>
Message-ID: <Pine.A41.4.61.0410060810470.339672@homer08.u.washington.edu>

On Wed, 6 Oct 2004, Vikas Rawal wrote:
>
> The problem is that R allows you to use by.x and by.y variables to specify 
> only one variable in x dataset and one variable in y dataset to merge.

This turns out not to be the case.

> names(df)
[1] "x" "y" "z"
> names(df2)
[1] "a" "b" "c"
> merge(df,df2,by.x=c("x","y"),by.y=c("a","b"))
      x   y   z   c
1  101 111 121 221
2  102 112 120 220
3  103 113 121 221
4  104 114 120 220
5  105 115 121 221
6  106 116 120 220
7  107 117 121 221
8  108 118 120 220
9  109 119 121 221
10 110 120 120 220


> If nothing else works, that is what I shall have to do. There again we have 
> some problem. How do I change the name of a particular column. One solution 
> suggested somewhere in the archives of the list is to use
>

names(df)[names(df) == "oldname"] <- "newname"

is one possibility that doesn't even require working out which variable 
number it is.

 	-thomas



From dmb at mrc-dunn.cam.ac.uk  Wed Oct  6 17:30:32 2004
From: dmb at mrc-dunn.cam.ac.uk (Dan Bolser)
Date: Wed, 6 Oct 2004 16:30:32 +0100 (BST)
Subject: [R] 2x2 test: total confusion.
Message-ID: <Pine.LNX.4.21.0410061603320.9022-100000@mail.mrc-dunn.cam.ac.uk>


I wan't a test for the 'association' between two events, lets say the
color of balls picked and the pickers (this is quite a good analogy to my
data).

I have       200 different pickers P
I have     1,000 colors of balls   C

I have 1,000,000 picks in total


I am totally confused about what test to apply and when and why. 


This is what I *think*

I know how many balls each picker picked      - so that marginal is fixed.
I know how many balls of each color there are - so that marginal is fixed.
I know the total picks.

I can test the 'association' between Picker p and color c by doing the
following...

prob_of_pick(p)  = picks made by p  / total picks
prob_of_color(c) = balls of color c / total picks 

prob_of_sucess = prob_of_pick_of_color(pc) = 
  picks made by p  / total picks *
  balls of color c / total picks


USE BINOMIAL DISTRIBUTION

  n = total picks
  k = number of balls of color c picked by picker p
  p = prob_of_pick_of_color(pc)


Significance of this particular observation = 

if( k < n*p ){
  for (x in 0:k){
    sig += dbinom(x,n,p)
  }
}
else{
  for (x in k:n){
    sig += dbinom(x,n,p)
  }
}

In the case that np and npq > 10, I use the normal approximation to the
binomial distribution with mean np and variance np(1-p), and correction
for continuity (+-0.5 depending on the direction of the test).

Should I use Fishers exact test? What do I do when the numbers are very
large?

Here is a sample of my data...

COLOR	PICKER	PICKED	C_TOTAL P_TOTAL GRAND_TOTAL
46458   rs      2       706     3285    878702
46548   rs      6       725     3285    878702
46557   rs      2       180     3285    878702
46561   rs      1       243     3285    878702
46565   rs      2       1864    3285    878702
46579   rs      1       1263    3285    878702
46589   rs      3       1168    3285    878702
46600   rs      2       301     3285    878702
46604   rs      1       105     3285    878702
46609   rs      1       302     3285    878702
46626   rs      32      1532    3285    878702
...
89095   ho      1       265     1369    878702
89124   ho      1       176     1369    878702
89360   ho      2       290     1369    878702
89392   ho      1       146     1369    878702
89447   ho      1       114     1369    878702
89550   ho      1       413     1369    878702
89919   ho      1       174     1369    878702
90002   ho      2       183     1369    878702
90096   ho      1       154     1369    878702
90123   ho      4       2130    1369    878702


How can I simply add an extra column to this data that gives me a measure
of the significance of 'association' (positive or negative) between Picker
and color?

I am totally confused!

Sorry for the lenght of the email.... Dan.



From dimitris.rizopoulos at med.kuleuven.ac.be  Wed Oct  6 17:30:05 2004
From: dimitris.rizopoulos at med.kuleuven.ac.be (Dimitris Rizopoulos)
Date: Wed, 6 Oct 2004 17:30:05 +0200
Subject: [R] dlogis for large negative numbers
Message-ID: <007c01c4abb9$5b148f30$b2133a86@www.domain>

Hi to all,

> dlogis(-2000)
[1] NaN
Warning message: 
NaNs produced in: dlogis(x, location, scale, log) 
> dnorm(-2000)
[1] 0


Is this an expected behaviour of `dlogis()'?

Thanks in advance for any comments,
Dimitris

platform i386-pc-mingw32
arch     i386           
os       mingw32        
system   i386, mingw32  
status                  
major    1              
minor    9.1            
year     2004           
month    06             
day      21             
language R      

----
Dimitris Rizopoulos
Ph.D. Student
Biostatistical Centre
School of Public Health
Catholic University of Leuven

Address: Kapucijnenvoer 35, Leuven, Belgium
Tel: +32/16/396887
Fax: +32/16/337015
Web: http://www.med.kuleuven.ac.be/biostat/
     http://www.student.kuleuven.ac.be/~m0390867/dimitris.htm



From par at wiklund.net  Wed Oct  6 17:31:51 2004
From: par at wiklund.net (Per Wiklund)
Date: Wed, 6 Oct 2004 08:31:51 -0700
Subject: [R] plotOHLC unequally spaced time.
Message-ID: <1182d01c4abb9$9a1e6840$66cb010a@mail2world.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20041006/10c757e4/attachment.pl

From ripley at stats.ox.ac.uk  Wed Oct  6 17:45:35 2004
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Wed, 6 Oct 2004 16:45:35 +0100 (BST)
Subject: [R] dlogis for large negative numbers
In-Reply-To: <007c01c4abb9$5b148f30$b2133a86@www.domain>
Message-ID: <Pine.LNX.4.44.0410061640400.15667-100000@gannet.stats>

On Wed, 6 Oct 2004, Dimitris Rizopoulos wrote:

> Hi to all,
> 
> > dlogis(-2000)
> [1] NaN
> Warning message: 
> NaNs produced in: dlogis(x, location, scale, log) 
> > dnorm(-2000)
> [1] 0
> 
> 
> Is this an expected behaviour of `dlogis()'?

No, it's a poor algorithm.  See dlogis.c

    x = (x - location) / scale;
    e = exp(-x);
    f = 1.0 + e;
    return give_log ? -(x + log(scale * f * f)) : e / (scale * f * f);

That needs to be 

    x = fabs((x - location) / scale);

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From zhou at uiuc.edu  Wed Oct  6 18:06:22 2004
From: zhou at uiuc.edu (Jianhui Zhou)
Date: Wed, 06 Oct 2004 11:06:22 -0500
Subject: [R] quadratically constrained quadratic programming
Message-ID: <416417FE.5030906@uiuc.edu>

Hi,

Does anybody have experience to solve an quadratic programming problem 
with quadratic constraints in R?
It seems that the package "quadprog" only handles the quadratic 
programming with linear constraint.  My probelm is to maximze 
x^T\Sigma_{xy} y,
subject to x^Tx=1, y^T\Sigma_{yy} y=1, and sum(y)<t, or sum(y)=t, where 
x and y are the variable, and the Sigma's and t are know.
Can R slove this problem, or do you know any other Fortran or C  
subroutine I can load into R to solve this problem?

Thanks in advance,
Jianhui



From john.gavin at ubs.com  Wed Oct  6 18:18:25 2004
From: john.gavin at ubs.com (john.gavin@ubs.com)
Date: Wed, 6 Oct 2004 17:18:25 +0100
Subject: [R] lapply with argument "X"
Message-ID: <012821F286ED1D4ABDC72F9E1DD63D0C041EA9F4@NLDNC003PEX1.ubsgs.ubsgroup.net>

Hi,

I am probably making a simple mistake but I can't see it

> X
Error: Object "X" not found
> exists("X")
[1] FALSE
> lapply("X", exists)
[[1]]
[1] TRUE

Why is lapply producing true?
Is it something to do with the first
argument of lapply also being called 'X'?

> version
         _              
platform i386-pc-mingw32
arch     i386           
os       mingw32        
system   i386, mingw32  
status                  
major    2              
minor    0.0            
year     2004           
month    10             
day      04             
language R                  

Regards,

John.

John Gavin <john.gavin at ubs.com>,
Quantitative Risk Models and Statistics,
UBS Investment Bank, 6th floor, 
100 Liverpool St., London EC2M 2RH, UK.
Phone +44 (0) 207 567 4289
Fax   +44 (0) 207 568 5352

Visit our website at http://www.ubs.com

This message contains confidential information and is intend...{{dropped}}



From dominik.bach at directbox.com  Wed Oct  6 18:23:07 2004
From: dominik.bach at directbox.com (Dominik Bach)
Date: Wed, 06 Oct 2004 18:23:07 +0200
Subject: [R] Checking if an element is part of a vector
Message-ID: <200410061623.i96FiK7X018281@relay01.directbox.com>

Hi!

I want to know if a given element <- ('c64') is part of a vector<- ('c64',  'amiga', 'atari'). 

Does a function exist for this which gives back a logical value?

thankx
db



From p.dalgaard at biostat.ku.dk  Wed Oct  6 18:31:14 2004
From: p.dalgaard at biostat.ku.dk (Peter Dalgaard)
Date: 06 Oct 2004 18:31:14 +0200
Subject: [R] lapply with argument "X"
In-Reply-To: <012821F286ED1D4ABDC72F9E1DD63D0C041EA9F4@NLDNC003PEX1.ubsgs.ubsgroup.net>
References: <012821F286ED1D4ABDC72F9E1DD63D0C041EA9F4@NLDNC003PEX1.ubsgs.ubsgroup.net>
Message-ID: <x2sm8r7qr1.fsf@biostat.ku.dk>

<john.gavin at ubs.com> writes:

> Hi,
> 
> I am probably making a simple mistake but I can't see it
> 
> > X
> Error: Object "X" not found
> > exists("X")
> [1] FALSE
> > lapply("X", exists)
> [[1]]
> [1] TRUE
> 
> Why is lapply producing true?
> Is it something to do with the first
> argument of lapply also being called 'X'?

Internal variable capture. I think this is a bug.

Also, try  lapply("FUN",get)


-- 
   O__  ---- Peter Dalgaard             Blegdamsvej 3  
  c/ /'_ --- Dept. of Biostatistics     2200 Cph. N   
 (*) \(*) -- University of Copenhagen   Denmark      Ph: (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)             FAX: (+45) 35327907



From ripley at stats.ox.ac.uk  Wed Oct  6 18:33:32 2004
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Wed, 6 Oct 2004 17:33:32 +0100 (BST)
Subject: [R] lapply with argument "X"
In-Reply-To: <012821F286ED1D4ABDC72F9E1DD63D0C041EA9F4@NLDNC003PEX1.ubsgs.ubsgroup.net>
Message-ID: <Pine.LNX.4.44.0410061729370.564-100000@gannet.stats>

On Wed, 6 Oct 2004 john.gavin at ubs.com wrote:

> Hi,
> 
> I am probably making a simple mistake but I can't see it
> 
> > X
> Error: Object "X" not found
> > exists("X")
> [1] FALSE
> > lapply("X", exists)
> [[1]]
> [1] TRUE
> 
> Why is lapply producing true?
> Is it something to do with the first
> argument of lapply also being called 'X'?

Yes.  Note that the first argument of lapply is supposed to be a list,
so what you are really using is

lapply(as.list("X"), exists)

and indeed an object named "X" exists in the environment within which 
exists() is run.

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From ripley at stats.ox.ac.uk  Wed Oct  6 18:34:50 2004
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Wed, 6 Oct 2004 17:34:50 +0100 (BST)
Subject: [R] Checking if an element is part of a vector
In-Reply-To: <200410061623.i96FiK7X018281@relay01.directbox.com>
Message-ID: <Pine.LNX.4.44.0410061733450.564-100000@gannet.stats>

?"%in%"
?is.element

On Wed, 6 Oct 2004, Dominik Bach wrote:

> I want to know if a given element <- ('c64') is part of a vector<-
> ('c64', 'amiga', 'atari').
> 
> Does a function exist for this which gives back a logical value?

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From p.dalgaard at biostat.ku.dk  Wed Oct  6 18:33:24 2004
From: p.dalgaard at biostat.ku.dk (Peter Dalgaard)
Date: 06 Oct 2004 18:33:24 +0200
Subject: [R] Checking if an element is part of a vector
In-Reply-To: <200410061623.i96FiK7X018281@relay01.directbox.com>
References: <200410061623.i96FiK7X018281@relay01.directbox.com>
Message-ID: <x2oejf7qnf.fsf@biostat.ku.dk>

"Dominik Bach" <dominik.bach at directbox.com> writes:

> Hi!
> 
> I want to know if a given element <- ('c64') is part of a vector<- ('c64',  'amiga', 'atari'). 
> 
> Does a function exist for this which gives back a logical value?

> 'c64' %in% c('c64',  'amiga', 'atari')
[1] TRUE

-- 
   O__  ---- Peter Dalgaard             Blegdamsvej 3  
  c/ /'_ --- Dept. of Biostatistics     2200 Cph. N   
 (*) \(*) -- University of Copenhagen   Denmark      Ph: (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)             FAX: (+45) 35327907



From MSchwartz at MedAnalytics.com  Wed Oct  6 18:38:02 2004
From: MSchwartz at MedAnalytics.com (Marc Schwartz)
Date: Wed, 06 Oct 2004 11:38:02 -0500
Subject: [R] Checking if an element is part of a vector
In-Reply-To: <200410061623.i96FiK7X018281@relay01.directbox.com>
References: <200410061623.i96FiK7X018281@relay01.directbox.com>
Message-ID: <1097080681.13656.14.camel@simba.localdomain>

On Wed, 2004-10-06 at 11:23, Dominik Bach wrote:
> Hi!
> 
> I want to know if a given element <- ('c64') is part of a vector<-
> ('c64',  'amiga', 'atari'). 
> 
> Does a function exist for this which gives back a logical value?
> 
> thankx
> db

> v <- c("c64",  "amiga", "atari", "MITS Altair", "pong")

> "c64" %in% v
[1] TRUE

Also, for the position in the vector, you can use:

> which(v == "c64")
[1] 1


See ?"%in%" and ?which for more information.

HTH,

Marc Schwartz



From rpeng at jhsph.edu  Wed Oct  6 18:48:21 2004
From: rpeng at jhsph.edu (Roger D. Peng)
Date: Wed, 06 Oct 2004 12:48:21 -0400
Subject: [R] Checking if an element is part of a vector
In-Reply-To: <200410061623.i96FiK7X018281@relay01.directbox.com>
References: <200410061623.i96FiK7X018281@relay01.directbox.com>
Message-ID: <416421D5.5030809@jhsph.edu>

Try

element %in% vector

or look at match()

-roger

Dominik Bach wrote:
> Hi!
> 
> I want to know if a given element <- ('c64') is part of a vector<- ('c64',  'amiga', 'atari'). 
> 
> Does a function exist for this which gives back a logical value?
> 
> thankx
> db
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>



From john.gavin at ubs.com  Wed Oct  6 18:54:12 2004
From: john.gavin at ubs.com (john.gavin@ubs.com)
Date: Wed, 6 Oct 2004 17:54:12 +0100
Subject: [R] lapply with argument "X"
Message-ID: <012821F286ED1D4ABDC72F9E1DD63D0C041EA9F8@NLDNC003PEX1.ubsgs.ubsgroup.net>

Hi,

> From: Peter Dalgaard [mailto:p.dalgaard at biostat.ku.dk]
> <john.gavin at ubs.com> writes:
> 
> > Hi,
> > 
> > I am probably making a simple mistake but I can't see it
> > 
> > > X
> > Error: Object "X" not found
> > > exists("X")
> > [1] FALSE
> > > lapply("X", exists)
> > [[1]]
> > [1] TRUE
> > 
> > Why is lapply producing true?
> > Is it something to do with the first
> > argument of lapply also being called 'X'?
> 
> Internal variable capture. I think this is a bug.

Fyi, I wanted to show that users have to use single
character variables with caution because of the existance
of objects like 'c', 'q', 'T' and 'F'.
So I tried to say

> xx <- c(letters, LETTERS) ; xx[sapply(xx, exists)]
 [1] "c" "q" "t" "C" "D" "F" "I" "Q" "T" "X"

but the appearance of 'X' undermines my code.
In that sense, it seems like a bug to me.
(I have a local definition for 'Q' - quit without asking.)

> Also, try  lapply("FUN",get)

Agreed.

Regards,

John.

John Gavin <john.gavin at ubs.com>,
Quantitative Risk Models and Statistics,
UBS Investment Bank, 6th floor, 
100 Liverpool St., London EC2M 2RH, UK.
Phone +44 (0) 207 567 4289
Fax   +44 (0) 207 568 5352


> -----Original Message-----
> From: Peter Dalgaard [mailto:p.dalgaard at biostat.ku.dk]
> Sent: 06 October 2004 17:31
> To: Gavin, John
> Cc: r-help at stat.math.ethz.ch
> Subject: Re: [R] lapply with argument "X"
> 
> 
> <john.gavin at ubs.com> writes:
> 
> > Hi,
> > 
> > I am probably making a simple mistake but I can't see it
> > 
> > > X
> > Error: Object "X" not found
> > > exists("X")
> > [1] FALSE
> > > lapply("X", exists)
> > [[1]]
> > [1] TRUE
> > 
> > Why is lapply producing true?
> > Is it something to do with the first
> > argument of lapply also being called 'X'?
> 
> Internal variable capture. I think this is a bug.
> 
> Also, try  lapply("FUN",get)
> 

Visit our website at http://www.ubs.com

This message contains confidential information and is intend...{{dropped}}



From elvis at xlsolutions-corp.com  Wed Oct  6 19:22:14 2004
From: elvis at xlsolutions-corp.com (elvis@xlsolutions-corp.com)
Date: Wed,  6 Oct 2004 10:22:14 -0700
Subject: [R] Course***R/S-plus Fundamentals and Programming Techniques @ 3
	locations, November 2004
Message-ID: <20041006172214.4155.qmail@webmail07.mesa1.secureserver.net>

XLSolutions Corporation (www.xlsolutions-corp.com) is proud to 
announce  2-day "R/S-plus Fundamentals and Programming 
Techniques".

****Washington, DC ----------------------- November 22nd-23rd
****San Francisco, CA ------------------- November 25th-26th
****Boston, MA   --------------------------- November 25th-26th


Reserve your seat now at the early bird rates! Payment due AFTER 
the class.


Course Description:
This two-day beginner to intermediate R/S-plus course focuses on a 
broad spectrum of topics, from reading raw data to a comparison of R 
and S. We will learn the essentials of data manipulation, graphical 
visualization and R/S-plus programming. We will explore statistical 
data analysis tools,including graphics with data sets. How to enhance 
your plots. We will perform basic statistics and fit linear regression
models. Participants are encouraged to bring data for interactive 
sessions


With the following outline:
- An Overview of R and S
- Data Manipulation and Graphics
- Using Lattice Graphics
- A Comparison of R and S-Plus
- How can R Complement SAS?
- Writing Functions
- Avoiding Loops
- Vectorization
- Statistical Modeling
- Project Management
- Techniques for Effective use of R and S
- Enhancing Plots
- Using High-level Plotting Functions
- Building and Distributing Packages (libraries)


Email us for group discounts.
Email Sue Turner: sue at xlsolutions-corp.com
Phone: 206-686-1578
Visit us: www.xlsolutions-corp.com/training.htm
Please let us know if you and your colleagues are interested in this 
classto take advantage of group discount. Register now to secure your 
seat! Interested in R/Splus Advanced course? email us.


Cheers,
Elvis Miller, PhD
Manager Training.
XLSolutions Corporation
206 686 1578
www.xlsolutions-corp.com
elvis at xlsolutions-corp.com



From ggrothendieck at myway.com  Wed Oct  6 19:23:45 2004
From: ggrothendieck at myway.com (Gabor Grothendieck)
Date: Wed, 6 Oct 2004 17:23:45 +0000 (UTC)
Subject: [R] lapply with argument "X"
References: <012821F286ED1D4ABDC72F9E1DD63D0C041EA9F8@NLDNC003PEX1.ubsgs.ubsgroup.net>
Message-ID: <loom.20041006T192202-532@post.gmane.org>

 <john.gavin <at> ubs.com> writes:

: 
: Hi,
: 
: > From: Peter Dalgaard [mailto:p.dalgaard <at> biostat.ku.dk]
: > <john.gavin <at> ubs.com> writes:
: > 
: > > Hi,
: > > 
: > > I am probably making a simple mistake but I can't see it
: > > 
: > > > X
: > > Error: Object "X" not found
: > > > exists("X")
: > > [1] FALSE
: > > > lapply("X", exists)
: > > [[1]]
: > > [1] TRUE
: > > 
: > > Why is lapply producing true?
: > > Is it something to do with the first
: > > argument of lapply also being called 'X'?
: > 
: > Internal variable capture. I think this is a bug.
: 
: Fyi, I wanted to show that users have to use single
: character variables with caution because of the existance
: of objects like 'c', 'q', 'T' and 'F'.
: So I tried to say
: 
: > xx <- c(letters, LETTERS) ; xx[sapply(xx, exists)]
:  [1] "c" "q" "t" "C" "D" "F" "I" "Q" "T" "X"
: 
: but the appearance of 'X' undermines my code.
: In that sense, it seems like a bug to me.
: (I have a local definition for 'Q' - quit without asking.)
: 
: > Also, try  lapply("FUN",get)
: 
: Agreed.
: 
: Regards,
: 
: John.


Try this:

R> xx <- c(letters, LETTERS) ; xx[sapply(xx, exists, envir = .GlobalEnv)]
 [1] "c" "n" "q" "t" "u" "v" "x" "C" "D" "F" "I" "T"



From m.dewey at iop.kcl.ac.uk  Wed Oct  6 18:34:28 2004
From: m.dewey at iop.kcl.ac.uk (Michael Dewey)
Date: Wed, 06 Oct 2004 17:34:28 +0100
Subject: Summary, was [R] Plotting panels at arbitrary places on a map,
	rather than on a lattice
In-Reply-To: <6.1.0.6.0.20041001175604.02639170@pop.freeserve.net>
References: <6.1.0.6.0.20041001175604.02639170@pop.freeserve.net>
Message-ID: <6.1.0.6.0.20041006172206.026529f0@pop.freeserve.net>

At 17:58 01/10/04, Michael Dewey wrote:
>I think it is easiest to describe
>what I want in terms of the concrete
>problem I have.
>
>I have data from a number of countries
>in each of which a sample of people was
>interviewed. In presenting the results
>in a forthcoming collaborative publication
>much emphasis will be placed on the
>multi-centre nature of the study. Although
>I suspect colleagues may do this with
>shaded maps I would prefer to avoid
>them as (a) they present one fact per
>country per map (b) they are unfair to
>the Netherlands and other high density
>countries.
>
>What I would like to do is to make
>the background represent Europe (ideally
>with a map but that is a frill) then
>place simple scattergrams (or radar plots)
>on it located roughly where the country
>is. Another way of describing it might
>be to say that I want something like
>the panels produced by lattice but at
>arbitrary coordinates rather than on
>a rectangular grid. I suspect I have
>to do this from scratch and I would
>welcome hints.
>
>Am I right that there is no off the
>shelf way to do this?
>
>Is grid the way to go? Looking at the
>article in Rnews 2(2) and a brief scan
>of the documentation suggests so.
>If grid is the way to go then bearing
>in mind I have never used grid before
>(a) any hints about the overall
>possible solution structure
>would be welcome (b) is this realistic to
>do within a week or shall I revert to
>lattice and lose the geography?
>
>Is there a simple way to draw a map
>in the background? It needs to cover
>as far as Sweden, Spain and Greece.
>It can be crude,
>as long as Italy looks roughly like
>a boot that is fine. I am an epidemiologist
>not a geographer.
>

I received some very helpful hints and was able to get a satisfactory solution.
Roger Bivand  pointed me in the right way with map. After loading maps
and mapproj I go

map("world", region = c("France", "Belgium", "Greece", "Spain", "Italy",
    "Switzerland", "Sweden", "Germany", "Netherlands", "Austria",
    "Denmark", "Sicily", "Sardinia"),
    xlim = c(-10, 30), ylim = c(30, 60),
    projection = "albers", parameters = c(30,60), col = "lightgreen")

then bearing in mind that my data is in a file called merg
which contains the coordinates and the data to plot

merg$x <- mapproject(merg$long,merg$lat)$x
merg$y <- mapproject(merg$long,merg$lat)$y

gets the coordinates in the new system.
Deepayan Sarkar reminded me about stars which I should
have remembered myself from reading MASS.
I now go

stars(2*merg[,ord+4], scale = FALSE, len = 0.07,
    locations = cbind(merg$x,merg$y), labels = NULL,
    cex = 0.3,
    key.loc = c(mapproject(-10,60)$x,mapproject(-10,60)$y),
    add = TRUE
)

and get a plot which does what I wanted.

Roger had pointed out that gridbase was probably the way to put 
scatterplots on the map
but I decided after looking at the stars that scatterplots would end up too 
small to use
so I stayed with lattice for them.
(When I said scattergrams I meant what nearly everyone else calls 
scatterplots.)

Thanks to both of them, and to the people who made maps and stars so easy
when you know what to look for.



Michael Dewey
m.dewey at iop.kcl.ac.uk



From B.Rowlingson at lancaster.ac.uk  Wed Oct  6 18:54:02 2004
From: B.Rowlingson at lancaster.ac.uk (Barry Rowlingson)
Date: Wed, 06 Oct 2004 17:54:02 +0100
Subject: [R] lapply with argument "X"
In-Reply-To: <x2sm8r7qr1.fsf@biostat.ku.dk>
References: <012821F286ED1D4ABDC72F9E1DD63D0C041EA9F4@NLDNC003PEX1.ubsgs.ubsgroup.net>
	<x2sm8r7qr1.fsf@biostat.ku.dk>
Message-ID: <4164232A.8040109@lancaster.ac.uk>

Peter Dalgaard wrote:

> 
> Internal variable capture. I think this is a bug.
> 
> Also, try  lapply("FUN",get)
> 
> 

If 'exists' was vectorised in its first argument then there wouldn't be 
a need to use lapply with it, would there? I'm guessing the original 
problem stems from something like:

  got = exists(c("X","Y","Z"))

  not working and the user thinking lapply would do nicely...

  How to vectorise it without introducing another existing variable is 
another problem...

Baz



From laura at env.leeds.ac.uk  Wed Oct  6 19:54:32 2004
From: laura at env.leeds.ac.uk (Laura Quinn)
Date: Wed, 6 Oct 2004 18:54:32 +0100 (BST)
Subject: [R] Performing Analysis on Subset of External data
Message-ID: <Pine.LNX.4.44.0410061847290.16851-100000@env-pc-phd13>

Hi,

I want to perform some analysis on subsets of huge data files. There are
20 of the files and I want to select the same subsets of each one (each
subset is a chunk of 1500 or so consecutive rows from several million). To
save time and processing power is there a method to tell R to *only* read
in these rows, rather than reading in the entire dataset then selecting
subsets and deleting the extraneous data? This method takes a rather silly
amount of time and results in memory problems.

I am using R 1.9.0 on SuSe 9.0

Thanks in advance!


Laura Quinn
Institute of Atmospheric Science
School of Earth and Environment
University of Leeds
Leeds
LS2 9JT

tel: +44 113 343 1596
fax: +44 113 343 6716
mail: laura at env.leeds.ac.uk



From ggrothendieck at myway.com  Wed Oct  6 19:56:02 2004
From: ggrothendieck at myway.com (Gabor Grothendieck)
Date: Wed, 6 Oct 2004 17:56:02 +0000 (UTC)
Subject: [R] plotOHLC unequally spaced time.
References: <1182d01c4abb9$9a1e6840$66cb010a@mail2world.com>
Message-ID: <loom.20041006T194934-78@post.gmane.org>

Per Wiklund <par <at> wiklund.net> writes:

 
: Hi I have som financial tick data that i have converted in to 45 min
: bars. These are unequally spaced in time. I wonder if it is possible to
: plot these like bars as the plotOHLC does. As I have understood plotOHLC
: uses class(mts) that must be equally spaced?

plotOHLC is only a couple screenfuls of code so you could adapt it to your
needs.  Another possibility is to fill in the missing spots with NAs and
turn the resulting regular series into an mts.

Here is an example of the latter.

Not sure how you have it represented right now but suppose its in 
matrix mat with the times in tt:

   mat <- cbind(Open = 11:20, High = 21:30, Low = 1:10, Close = 12:21)
   tt <- c(1:2, 4:11)

Then you could:

- convert it to zoo
- create a regular sequence of times and use merge.zoo to merge that 
  with your series thereby filling in the missing times with NAs
- convert result back to ts/mts and then 
- use plotOHLC on the result

   require(zoo)
   z <- zoo(mat, tt)
   fill <- zoo(,seq(min(tt), max(tt)))
   zf <- merge(fill, z)[,-1]  # fill in missing times with NAs
   z.mt <- ts(unclass(zf), start = index(zf)[1], end = index(zf)[nrow(zf)])
   plotOHLC(z.mt)

You will likely have to adapt the above to your specific 
data and time representation.



From tlumley at u.washington.edu  Wed Oct  6 20:08:13 2004
From: tlumley at u.washington.edu (Thomas Lumley)
Date: Wed, 6 Oct 2004 11:08:13 -0700 (PDT)
Subject: [R] lapply with argument "X"
In-Reply-To: <012821F286ED1D4ABDC72F9E1DD63D0C041EA9F8@NLDNC003PEX1.ubsgs.ubsgroup.net>
References: <012821F286ED1D4ABDC72F9E1DD63D0C041EA9F8@NLDNC003PEX1.ubsgs.ubsgroup.net>
Message-ID: <Pine.A41.4.61.0410061106500.346816@homer06.u.washington.edu>

On Wed, 6 Oct 2004 john.gavin at ubs.com wrote:
>
> Fyi, I wanted to show that users have to use single
> character variables with caution because of the existance
> of objects like 'c', 'q', 'T' and 'F'.
> So I tried to say
>
>> xx <- c(letters, LETTERS) ; xx[sapply(xx, exists)]
> [1] "c" "q" "t" "C" "D" "F" "I" "Q" "T" "X"
>
> but the appearance of 'X' undermines my code.

I would have said it actually reinforces the point you are making -- 
inadvertent masking or variable capture is a danger with single letter 
names.

 	-thomas



From gavin.simpson at ucl.ac.uk  Wed Oct  6 20:11:05 2004
From: gavin.simpson at ucl.ac.uk (Gavin Simpson)
Date: Wed, 06 Oct 2004 19:11:05 +0100
Subject: [R] Performing Analysis on Subset of External data
In-Reply-To: <Pine.LNX.4.44.0410061847290.16851-100000@env-pc-phd13>
References: <Pine.LNX.4.44.0410061847290.16851-100000@env-pc-phd13>
Message-ID: <41643539.10301@ucl.ac.uk>

Laura Quinn wrote:
> Hi,
> 
> I want to perform some analysis on subsets of huge data files. There are
> 20 of the files and I want to select the same subsets of each one (each
> subset is a chunk of 1500 or so consecutive rows from several million). To
> save time and processing power is there a method to tell R to *only* read
> in these rows, rather than reading in the entire dataset then selecting
> subsets and deleting the extraneous data? This method takes a rather silly
> amount of time and results in memory problems.
> 
> I am using R 1.9.0 on SuSe 9.0
> 
> Thanks in advance!
> 

Hi Laura,

I guess if you knew which row of the file your subset started from and 
you knew how many lines you wanted to read in you could use scan with 
arguments skip and nlines (see ?scan)

A better way that gets recommended a lot on the list is to store your 
data in a database and use the various R packages and/or tools available 
that can connect to your database and only extract the rows you need.

See the R Data Import/Export manual for more on scan and using 
relational databases with R.

Hope this helps,

Gav
-- 
%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%
Gavin Simpson                     [T] +44 (0)20 7679 5522
ENSIS Research Fellow             [F] +44 (0)20 7679 7565
ENSIS Ltd. & ECRC                 [E] gavin.simpson at ucl.ac.uk
UCL Department of Geography       [W] http://www.ucl.ac.uk/~ucfagls/cv/
26 Bedford Way                    [W] http://www.ucl.ac.uk/~ucfagls/
London.  WC1H 0AP.
%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%



From tlumley at u.washington.edu  Wed Oct  6 20:00:08 2004
From: tlumley at u.washington.edu (Thomas Lumley)
Date: Wed, 6 Oct 2004 11:00:08 -0700 (PDT)
Subject: [R] R Newsletter
Message-ID: <Pine.A41.4.61.0410061057090.346816@homer06.u.washington.edu>


Issue 4/2 of the R Newsletter is up on www.r-project.org.

This issue is especially commended to your attention since it has an 
article by Brian Ripley describing the largest change in R 2.0.0, lazy 
loading of packages.

 	-thomas

Thomas Lumley			Assoc. Professor, Biostatistics
tlumley at u.washington.edu	University of Washington, Seattle

_______________________________________________
R-announce at stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-announce



From tlumley at u.washington.edu  Wed Oct  6 20:13:17 2004
From: tlumley at u.washington.edu (Thomas Lumley)
Date: Wed, 6 Oct 2004 11:13:17 -0700 (PDT)
Subject: [R] Performing Analysis on Subset of External data
In-Reply-To: <Pine.LNX.4.44.0410061847290.16851-100000@env-pc-phd13>
References: <Pine.LNX.4.44.0410061847290.16851-100000@env-pc-phd13>
Message-ID: <Pine.A41.4.61.0410061109350.346816@homer06.u.washington.edu>

On Wed, 6 Oct 2004, Laura Quinn wrote:

> Hi,
>
> I want to perform some analysis on subsets of huge data files. There are
> 20 of the files and I want to select the same subsets of each one (each
> subset is a chunk of 1500 or so consecutive rows from several million). To
> save time and processing power is there a method to tell R to *only* read
> in these rows, rather than reading in the entire dataset then selecting
> subsets and deleting the extraneous data? This method takes a rather silly
> amount of time and results in memory problems.

It depends on the data format.  If, for example, you have free-format text 
files it isn't possible to locate a specific chunk without reading all the 
earlier entries.  You can still save time and space by having some other 
program (?Perl) read the file and spit out a file with just the 1500 rows 
you want.

A better strategy would be for the data to be either in a database or in a 
format such as netCDF designed for random access.

 	-thomas



From ripley at stats.ox.ac.uk  Wed Oct  6 20:15:17 2004
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Wed, 6 Oct 2004 19:15:17 +0100 (BST)
Subject: [R] Performing Analysis on Subset of External data
In-Reply-To: <Pine.LNX.4.44.0410061847290.16851-100000@env-pc-phd13>
Message-ID: <Pine.LNX.4.44.0410061912130.4432-100000@gannet.stats>

1) Use the skip= and nrows= arguments to read.table.

2) Open a connection, read and discard rows, read the block you want then 
close the connection. (Which is how 1 works, essentially.)

3) Use perl, awk or some such to extract the rows you want -- this is 
probably rather faster.

On Wed, 6 Oct 2004, Laura Quinn wrote:

> I want to perform some analysis on subsets of huge data files. There are
> 20 of the files and I want to select the same subsets of each one (each
> subset is a chunk of 1500 or so consecutive rows from several million). To
> save time and processing power is there a method to tell R to *only* read
> in these rows, rather than reading in the entire dataset then selecting
> subsets and deleting the extraneous data? This method takes a rather silly
> amount of time and results in memory problems.
> 
> I am using R 1.9.0 on SuSe 9.0


-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From lockwood at rand.org  Wed Oct  6 20:27:00 2004
From: lockwood at rand.org (J.R. Lockwood)
Date: Wed, 6 Oct 2004 14:27:00 -0400 (EDT)
Subject: [R] odd behavior of summary()$r.squared
Message-ID: <Pine.LNX.4.58.0410061422170.459@penguin.rand.org>

I may be missing something obvious here, but consider the following simple
dataset simulating repeated measures on 5 individuals with pretty strong
between-individual variance.

set.seed(1003)
n<-5
v<-rep(1:n,each=2)
d<-data.frame(factor(v),v+rnorm(2*n))
names(d)<-c("id","y")

Now consider the following two linear models that provide identical fitted
values, residuals, and estimated residual variance:
  
m1<-lm(y~id,data=d)
m2<-lm(y~id-1,data=d)
print(max(abs(fitted(m1)-fitted(m2))))

The r-squared reported by summary(m1) appears to be correct in that it is
equal to the squared correlation between the fitted and observed values:

print(summary(m1)$r.squared - cor(fitted(m1),d$y)^2)

However, the same is not true of m2.

print(summary(m2)$r.squared - cor(fitted(m2),d$y)^2)

> R.version
         _
platform i686-pc-linux-gnu
arch     i686
os       linux-gnu
system   i686, linux-gnu
status
major    1
minor    9.0
year     2004
month    04
day      12
language R


J.R. Lockwood
412-683-2300 x4941
lockwood at rand.org
http://www.rand.org/methodology/stat/members/lockwood/

--------------------

This email message is for the sole use of the intended recipient(s) and
may contain privileged information. Any unauthorized review, use,
disclosure or distribution is prohibited. If you are not the intended
recipient, please contact the sender by reply email and destroy all copies
of the original message.



From Ted.Harding at nessie.mcc.ac.uk  Wed Oct  6 20:58:32 2004
From: Ted.Harding at nessie.mcc.ac.uk ( (Ted Harding))
Date: Wed, 06 Oct 2004 19:58:32 +0100 (BST)
Subject: [R] Performing Analysis on Subset of External data
In-Reply-To: <Pine.LNX.4.44.0410061847290.16851-100000@env-pc-phd13>
Message-ID: <XFMail.041006195832.Ted.Harding@nessie.mcc.ac.uk>

On 06-Oct-04 Laura Quinn wrote:
> I want to perform some analysis on subsets of huge data files.
> There are 20 of the files and I want to select the same subsets
> of each one (each subset is a chunk of 1500 or so consecutive
> rows from several million).
> To save time and processing power is there a method to tell R
> to *only* read in these rows, rather than reading in the entire
> dataset then selecting subsets and deleting the extraneous data?
> This method takes a rather silly amount of time and results in
> memory problems.
> 
> I am using R 1.9.0 on SuSe 9.0

Hi Laura,
If there is a neat time&memory-efficient R solution then I'm sure
someone will tell you! But since you're using Linux, I can suggest
an alternative, which is to use some combination of the Unix file
utilities which you will already have in your SuSE, entering them
as a command-line at the system prompt, or executing a shell script
file which contains the command.

For example, to read just lines (say) 500001-501500 you could use

  cat bigdata | head -501500 | tail -1500 > smalldata

which reads the first 501500 lines of bigdata and then the last
1500 lines of these, and directs the result of this into the file
smalldata.

That's OK for a single chunk of 1500, but suppose (as seems might
be the case) you want (say) the first line of the file (for names)
and 5 chunks of 1500 starting at lines 100001, 200001, 300001,
400001, 500001 respectively. Then awk will do what you want, on
the lines of

  cat bigdata | awk '
    {nr=NR;
      if(
          (nr==1) ||
          ((nr>=100001)&&(nr<=101500)) ||
          ((nr>=200001)&&(nr<=201500)) ||
          ((nr>=300001)&&(nr<=301500)) ||
          ((nr>=400001)&&(nr<=401500)) ||
          ((nr>=500001)&&(nr<=501500))
        ) {print $0}
      else {next}
    }' > smalldata

(The above can be typed in as shown, and will be a single command).

Having done this, you can then use smalldata as the dataset to
read into R, instead of bigdata.

These are just examples of what can be done externally using such
utilities.

(Now, whether or not there's a simple R-workround, I shall undoubtedly
be trumped by some perl freak).

Best wishes to all,
Ted.


--------------------------------------------------------------------
E-Mail: (Ted Harding) <Ted.Harding at nessie.mcc.ac.uk>
Fax-to-email: +44 (0)870 094 0861  [NB: New number!]
Date: 06-Oct-04                                       Time: 19:58:32
------------------------------ XFMail ------------------------------



From sundar.dorai-raj at PDF.COM  Wed Oct  6 21:21:04 2004
From: sundar.dorai-raj at PDF.COM (Sundar Dorai-Raj)
Date: Wed, 06 Oct 2004 14:21:04 -0500
Subject: [R] odd behavior of summary()$r.squared
In-Reply-To: <Pine.LNX.4.58.0410061422170.459@penguin.rand.org>
References: <Pine.LNX.4.58.0410061422170.459@penguin.rand.org>
Message-ID: <416445A0.9080400@pdf.com>



J.R. Lockwood wrote:

> I may be missing something obvious here, but consider the following simple
> dataset simulating repeated measures on 5 individuals with pretty strong
> between-individual variance.
> 
> set.seed(1003)
> n<-5
> v<-rep(1:n,each=2)
> d<-data.frame(factor(v),v+rnorm(2*n))
> names(d)<-c("id","y")
> 
> Now consider the following two linear models that provide identical fitted
> values, residuals, and estimated residual variance:
>   
> m1<-lm(y~id,data=d)
> m2<-lm(y~id-1,data=d)
> print(max(abs(fitted(m1)-fitted(m2))))
> 
> The r-squared reported by summary(m1) appears to be correct in that it is
> equal to the squared correlation between the fitted and observed values:
> 
> print(summary(m1)$r.squared - cor(fitted(m1),d$y)^2)
> 
> However, the same is not true of m2.
> 
> print(summary(m2)$r.squared - cor(fitted(m2),d$y)^2)
> 
> 
>>R.version
> 
>          _
> platform i686-pc-linux-gnu
> arch     i686
> os       linux-gnu
> system   i686, linux-gnu
> status
> major    1
> minor    9.0
> year     2004
> month    04
> day      12
> language R

I think what you're trying to do is better accomplished by looking at 
the anova table of the two results

a1 <- anova(m1)
a2 <- anova(m2)
r2.1 <- a1[1, 2]/sum(a1[, 2])
r2.2 <- a2[1, 2]/sum(a2[, 2])

summary(m1)$r.squared - r2.1
summary(m2)$r.squared - r2.2

The result you used above using "cor" still adjusts your data for the 
grand mean, which m2 doesn't fit.

HTH,

--sundar



From wolski at molgen.mpg.de  Wed Oct  6 21:22:20 2004
From: wolski at molgen.mpg.de (Witold Eryk Wolski)
Date: Wed, 06 Oct 2004 21:22:20 +0200
Subject: [R] setClass - equvalent declarations?
Message-ID: <416445EC.20500@molgen.mpg.de>

Hi,

Why the third declation is not equivalent to the first 2 and gives a Warning

#1
setClass("MVE",representation("list",names="character"))

#2

setClass(
         "MVE"
         ,contains="list"
         ,representation(
                        names="character"
                        )
         )
#3
setClass("MVE","matrix",representation(names="character"))

[1] "MVE"
Warning message: 
prototype is a list with named elements (could be ambiguous):  better to use function prototype() to avoid trouble. in: reconcilePropertiesAndPrototype(name, slots, prototype, superClasses,  


/E.



-- 
Dipl. bio-chem. Witold Eryk Wolski         
MPI-Moleculare Genetic
Ihnestrasse 63-73 14195 Berlin                 _
tel: 0049-30-83875219                 __("<   'v'
http://www.molgen.mpg.de/~wolski      \__/   /   \
mail: witek96 at users.sourceforge.net    ^^     w w
      wolski at molgen.mpg.de



From gblevins at mn.rr.com  Wed Oct  6 22:26:34 2004
From: gblevins at mn.rr.com (Greg Blevins)
Date: Wed, 6 Oct 2004 15:26:34 -0500
Subject: [R] Dataframe manipulation question
Message-ID: <04a401c4abe2$c699e6f0$aaca5e18@glblpyirxqz5lp>

Hello,

I have a data frame that has three fields.

Resp#     ActCode     ProdUsed
100          3                  2
100          3                  2
100          4                  3
100          4                  3
101          3                  6
102          2                  1
102          3                  1
103          5                  1
103          5                  1
103          3                  2
103          3                  2
104          3                  1

What I seek to do.

If a row following a row is identical for the fields Resp3 and ActCode, I
then want to delete one of the two matching rows. Based on this logic, the
resulting df would look like that shown below.

Resp# ActCode    ProdUsed
100      3           2
100      4           3
101      3           6
102      2           1
102      3           1
103      5           1
103      3           2
104      3           1

I have tried match, tried to write something that if the current row minus
the previous row equal 0 for both Resp# and ActCode, then delete the row,
but to no avail.  Not knowing what to search on for this problem, I turn to
the R experts for help.

(Windows 2000, R 2.0, 384 meg memory)
Greg Blevins
The Market Solutions Group



From sundar.dorai-raj at PDF.COM  Wed Oct  6 22:38:02 2004
From: sundar.dorai-raj at PDF.COM (Sundar Dorai-Raj)
Date: Wed, 06 Oct 2004 15:38:02 -0500
Subject: [R] Dataframe manipulation question
In-Reply-To: <04a401c4abe2$c699e6f0$aaca5e18@glblpyirxqz5lp>
References: <04a401c4abe2$c699e6f0$aaca5e18@glblpyirxqz5lp>
Message-ID: <416457AA.9080708@pdf.com>



Greg Blevins wrote:

> Hello,
> 
> I have a data frame that has three fields.
> 
> Resp#     ActCode     ProdUsed
> 100          3                  2
> 100          3                  2
> 100          4                  3
> 100          4                  3
> 101          3                  6
> 102          2                  1
> 102          3                  1
> 103          5                  1
> 103          5                  1
> 103          3                  2
> 103          3                  2
> 104          3                  1
> 
> What I seek to do.
> 
> If a row following a row is identical for the fields Resp3 and ActCode, I
> then want to delete one of the two matching rows. Based on this logic, the
> resulting df would look like that shown below.
> 
> Resp# ActCode    ProdUsed
> 100      3           2
> 100      4           3
> 101      3           6
> 102      2           1
> 102      3           1
> 103      5           1
> 103      3           2
> 104      3           1
> 
> I have tried match, tried to write something that if the current row minus
> the previous row equal 0 for both Resp# and ActCode, then delete the row,
> but to no avail.  Not knowing what to search on for this problem, I turn to
> the R experts for help.
> 
> (Windows 2000, R 2.0, 384 meg memory)
> Greg Blevins
> The Market Solutions Group

Would ?unique work for you?

 > x
    Resp ActCode ProdUsed
1   100       3        2
2   100       3        2
3   100       4        3
4   100       4        3
5   101       3        6
6   102       2        1
7   102       3        1
8   103       5        1
9   103       5        1
10  103       3        2
11  103       3        2
12  104       3        1
 > unique(x)
    Resp ActCode ProdUsed
1   100       3        2
3   100       4        3
5   101       3        6
6   102       2        1
7   102       3        1
8   103       5        1
10  103       3        2
12  104       3        1

Or if you only want to keep the unique rows of the first two columns then:

 > x[!duplicated(x[, 1:2]), ]

which for this example is identical to using unique directly.

--sundar



From gblevins at mn.rr.com  Wed Oct  6 22:45:24 2004
From: gblevins at mn.rr.com (Greg Blevins)
Date: Wed, 6 Oct 2004 15:45:24 -0500
Subject: [R] Dataframe Manipulation Question
Message-ID: <04e401c4abe5$67bb11b0$aaca5e18@glblpyirxqz5lp>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20041006/557026d7/attachment.pl

From rajarshi at presidency.com  Wed Oct  6 22:39:53 2004
From: rajarshi at presidency.com (Rajarshi Guha)
Date: Wed, 06 Oct 2004 16:39:53 -0400
Subject: [R] Dataframe manipulation question
In-Reply-To: <04a401c4abe2$c699e6f0$aaca5e18@glblpyirxqz5lp>
References: <04a401c4abe2$c699e6f0$aaca5e18@glblpyirxqz5lp>
Message-ID: <1097095193.25991.4.camel@blue.chem.psu.edu>

On Wed, 2004-10-06 at 16:26, Greg Blevins wrote:
> Hello,
> 
> I have a data frame that has three fields.
> 
> Resp#     ActCode     ProdUsed
> 100          3                  2
> 100          3                  2
> 100          4                  3
> 100          4                  3
> 101          3                  6
> 102          2                  1
> 102          3                  1
> 103          5                  1
> 103          5                  1
> 103          3                  2
> 103          3                  2
> 104          3                  1
> 
> What I seek to do.
> 
> If a row following a row is identical for the fields Resp3 and ActCode, I
> then want to delete one of the two matching rows. Based on this logic, the
> resulting df would look like that shown below.

I'm sure that the Rexperts will provide more elegant solutions but
heres a way which works assuming columns 2 & 3 are numeric so that
diff() works.

If d is the data.frame

> d
   Resp ActCode ProdUsed
1   100       3        2
2   100       3        2
3   100       4        3
4   100       4        3
5   101       3        6
6   102       2        1
7   102       3        1
8   103       5        1
9   103       5        1
10  103       3        2
11  103       3        2
12  104       3        1
>
> y <- which( diff(d[,2]) == 0 & diff(d[,3]) == 0 )
> d[-y,]
   Resp ActCode ProdUsed
2   100       3        2
4   100       4        3
5   101       3        6
6   102       2        1
7   102       3        1
9   103       5        1
11  103       3        2
12  104       3        1

> Resp# ActCode    ProdUsed
> 100      3           2
> 100      4           3
> 101      3           6
> 102      2           1
> 102      3           1
> 103      5           1
> 103      3           2
> 104      3           1

HTH

-------------------------------------------------------------------
Rajarshi Guha <rxg218 at psu.edu> <http://jijo.cjb.net>
GPG Fingerprint: 0CCA 8EE2 2EEB 25E2 AB04 06F7 1BB9 E634 9B87 56EE
-------------------------------------------------------------------
After a number of decimal places, nobody gives a damn.



From jfox at mcmaster.ca  Wed Oct  6 22:57:23 2004
From: jfox at mcmaster.ca (John Fox)
Date: Wed, 06 Oct 2004 16:57:23 -0400
Subject: [R] Dataframe manipulation question
In-Reply-To: <04a401c4abe2$c699e6f0$aaca5e18@glblpyirxqz5lp>
Message-ID: <web-67673933@cgpsrv2.cis.mcmaster.ca>

Dear Greg,

How about this?

> n <- nrow(Data)
> index <- 1:n
> sel <- c(TRUE, !apply(Data[index[-1], c(1,2)] ==
+     Data[index[-n], c(1,2)], 1, all))
> Data[sel,]
   Resp ActCode ProdUsed
1   100       3        2
3   100       4        3
5   101       3        6
6   102       2        1
7   102       3        1
8   103       5        1
10  103       3        2
12  104       3        1

I hope this helps,
 John

On Wed, 6 Oct 2004 15:26:34 -0500
 "Greg Blevins" <gblevins at mn.rr.com> wrote:
> Hello,
> 
> I have a data frame that has three fields.
> 
> Resp#     ActCode     ProdUsed
> 100          3                  2
> 100          3                  2
> 100          4                  3
> 100          4                  3
> 101          3                  6
> 102          2                  1
> 102          3                  1
> 103          5                  1
> 103          5                  1
> 103          3                  2
> 103          3                  2
> 104          3                  1
> 
> What I seek to do.
> 
> If a row following a row is identical for the fields Resp3 and
> ActCode, I
> then want to delete one of the two matching rows. Based on this
> logic, the
> resulting df would look like that shown below.
> 
> Resp# ActCode    ProdUsed
> 100      3           2
> 100      4           3
> 101      3           6
> 102      2           1
> 102      3           1
> 103      5           1
> 103      3           2
> 104      3           1
> 
> I have tried match, tried to write something that if the current row
> minus
> the previous row equal 0 for both Resp# and ActCode, then delete the
> row,
> but to no avail.  Not knowing what to search on for this problem, I
> turn to
> the R experts for help.
> 
> (Windows 2000, R 2.0, 384 meg memory)
> Greg Blevins
> The Market Solutions Group
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide!
> http://www.R-project.org/posting-guide.html

--------------------------------
John Fox
Department of Sociology
McMaster University
Hamilton, Ontario, Canada
http://socserv.mcmaster.ca/jfox/



From gerifalte28 at hotmail.com  Wed Oct  6 23:02:08 2004
From: gerifalte28 at hotmail.com (F Z)
Date: Wed, 06 Oct 2004 21:02:08 +0000
Subject: [R] Repeated measures
Message-ID: <BAY2-F39eWgUSp3sJWG00004977@hotmail.com>

5,000 repeated measures! You might have problems fitting a "old school" 
univariate or multivariate ANOVA RM model.  Can you group some of the 
measurements or you actually want to make inferences at each time point?  
Anyway, try using a newer method, like lme from the library lme4.  The book 
by Pinheiro and Bates "Mixed-Effects Models in S and S-Plus" describes 
several examples of RM analysis using their package.

Good luck!

Francisco


>From: Sean Davis <sdavis2 at mail.nih.gov>
>To: r-help <r-help at stat.math.ethz.ch>
>Subject: [R] Repeated measures
>Date: Wed, 6 Oct 2004 08:07:38 -0400
>
>I have a data set in which I have 5000 repeated measures on 6 subjects over 
>time (varying intervals, but measurements for all individuals are at the 
>same times).  There are two states, a "resting" state (the majority of the 
>time), and a perturbed state.  I have a continuous measurement at each time 
>point for each of the individuals.  I would like to determine the "state" 
>for each individual at each time point.  It looks to me like I should be 
>able to do this with the "hidden" command from the "repeated" package 
>(http://popgen0146uns50.unimaas.nl/~jlindsey/rcode.html), but I have found 
>it a bit confusing to get started.  The distributions in the two states are 
>approximately normal with differences in centrality and possibly variance 
>(but I can start by assuming similar variances).
>
>Thanks,
>Sean
>
>______________________________________________
>R-help at stat.math.ethz.ch mailing list
>https://stat.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide! 
>http://www.R-project.org/posting-guide.html



From erich.neuwirth at univie.ac.at  Thu Oct  7 00:29:53 2004
From: erich.neuwirth at univie.ac.at (Erich Neuwirth)
Date: Thu, 07 Oct 2004 00:29:53 +0200
Subject: [R] as.complex
In-Reply-To: <web-67673933@cgpsrv2.cis.mcmaster.ca>
References: <web-67673933@cgpsrv2.cis.mcmaster.ca>
Message-ID: <416471E1.8070001@univie.ac.at>

as.complex("2+1i") -> 2+1i
as.complex("2+i") -> NA

Does somebody have a modified version of as.complex
which does the coercion in a less strict manner and
produces a complex number also for strings like the
second example?

Perhaps it would even make sense to change the behavior
of as.complex to handle the second case.
After all, this is an established way of writing
complex numbers in some programming tools
able to deal with complex numbers.



-- 
Erich Neuwirth, Computer Supported Didactics Working Group
Visit our SunSITE at http://sunsite.univie.ac.at
Phone: +43-1-4277-38624 Fax: +43-1-4277-9386



From dmb at mrc-dunn.cam.ac.uk  Thu Oct  7 01:20:05 2004
From: dmb at mrc-dunn.cam.ac.uk (Dan Bolser)
Date: Thu, 7 Oct 2004 00:20:05 +0100 (BST)
Subject: [R] R-(wiki)-pedia?
Message-ID: <Pine.LNX.4.21.0410070012330.12890-100000@mail.mrc-dunn.cam.ac.uk>


Is there an R wiki?

Looking at the huge amount of traffic on this list, I think wiki could be
an exelet outlet for all the constructive enthusiasm here.

I don't think it would be too hard to port the existing R documentation
(the stuff you get with the ?) onto a wiki system, then users could add
their own examples and comments. 

Having distinct web page style organization would be easier to navigate
than the mailing list archives. I like the online version of the R-docs,
but I miss 'user comments' and I miss being able to fix trivial things.

The user contributed links could supply the best statistical online
resources to supplement the R-documentation, with links to the relevant
FAQ's and Tutorials supplementing the whole thing.

Now I have said it this sounds too good to not exist already... Anyone
dumped the R-documentation onto a wiki system?

Given the capability of R to be integrated into web pages, this could be
really really great. 

Cheers,
Dan.



From ggrothendieck at myway.com  Thu Oct  7 01:49:44 2004
From: ggrothendieck at myway.com (Gabor Grothendieck)
Date: Wed, 6 Oct 2004 23:49:44 +0000 (UTC)
Subject: [R] R-(wiki)-pedia?
References: <Pine.LNX.4.21.0410070012330.12890-100000@mail.mrc-dunn.cam.ac.uk>
Message-ID: <loom.20041007T014852-860@post.gmane.org>

Dan Bolser <dmb <at> mrc-dunn.cam.ac.uk> writes:

: 
: Is there an R wiki?
: 
: Looking at the huge amount of traffic on this list, I think wiki could be
: an exelet outlet for all the constructive enthusiasm here.
: 
: I don't think it would be too hard to port the existing R documentation
: (the stuff you get with the ?) onto a wiki system, then users could add
: their own examples and comments. 
: 
: Having distinct web page style organization would be easier to navigate
: than the mailing list archives. I like the online version of the R-docs,
: but I miss 'user comments' and I miss being able to fix trivial things.
: 
: The user contributed links could supply the best statistical online
: resources to supplement the R-documentation, with links to the relevant
: FAQ's and Tutorials supplementing the whole thing.
: 
: Now I have said it this sounds too good to not exist already... Anyone
: dumped the R-documentation onto a wiki system?
: 
: Given the capability of R to be integrated into web pages, this could be
: really really great. 
: 
: Cheers,
: Dan.

There is one at

http://fawn.unibw-hamburg.de/cgi-bin/Rwiki.pl?RwikiHome

Unfortunately, no one seems to use it.



From Ted.Harding at nessie.mcc.ac.uk  Thu Oct  7 01:46:42 2004
From: Ted.Harding at nessie.mcc.ac.uk ( (Ted Harding))
Date: Thu, 07 Oct 2004 00:46:42 +0100 (BST)
Subject: [R] Dataframe Manipulation Question
In-Reply-To: <04e401c4abe5$67bb11b0$aaca5e18@glblpyirxqz5lp>
Message-ID: <XFMail.041006234217.Ted.Harding@nessie.mcc.ac.uk>

On 06-Oct-04 Greg Blevins wrote:
> Thanks to all that responded to my problem.  I am always amazed by how
> helpful the R community is!

Because the R community, via the R list, like any good mailing
list community, is like a global coffee-room where anyone can
start talking about what concerns them at the time, and other
people looking over their shoulder, or over-hearing, can come
in with their view of the matter.

Ted.


--------------------------------------------------------------------
E-Mail: (Ted Harding) <Ted.Harding at nessie.mcc.ac.uk>
Fax-to-email: +44 (0)870 094 0861  [NB: New number!]
Date: 06-Oct-04                                       Time: 23:42:17
------------------------------ XFMail ------------------------------



From rjvbertin at hotmail.com  Thu Oct  7 02:09:01 2004
From: rjvbertin at hotmail.com (=?ISO-8859-1?Q?=22Ren=E9_J=2EV=2E_Bertin=22?=)
Date: Thu, 07 Oct 2004 02:09:01 +0200
Subject: [R] Mac: importing saved PDF figures into Illustrator CS
Message-ID: <4164891D.9080402@hotmail.com>

Hello,

This is a usage question for others with experience of R under the Aqua Mac OS X interface.

Basically, I don't succeed in importing PDF files (created with the 'Save As' menu to Quartz device windows) into Illustrator. Versions up to (and including?) 10 loose paths (lines/polygons) and or fill them in black. Version CS (11) on the Mac imports almost correctly, but, "to preserve appearance, some text has been outlined". In other words: *all* text in the figure is converted to polygons. Somewhat annoying if one plans to edit the text, and in general change the appearance. I've asked on both the Apple and Adobe Illustrator forums (http://www.adobeforums.com/cgi-bin/webx?13 at 114.dQNuchE8byf.1@.3bb6383d/0), but have not yet received any reply. Maybe someone here has found a way to tell Illustrator not to outline ('preserve editability', in their jargon), or some other workaround?

Thanks in advance,
Ren?? Bertin



From joseph.gazaille at videotron.ca  Tue Oct  5 00:21:36 2004
From: joseph.gazaille at videotron.ca (Joseph J. Gazaille)
Date: Mon, 04 Oct 2004 18:21:36 -0400
Subject: [R] update.packages() with R 2.0.0
Message-ID: <001a01c4aa60$8aca1ba0$b7cdfdcf@cyber>

Good day to all of you and thank you for reading this.

I certainly must have done something awfully wrong
when I downloaded and installed R 2.0.0 on a PC
with Windows 98.

You will find below what happens when  I try to 'update.packages()'.

I know what the first 'warning message'  means.
About the others, I ain't got no clue.

Thank you very handsomely for your wisdom.
And thank you also for all that tremendously
wonderful work.

 - joseph




> update.packages()
trying URL `http://cran.r-project.org/bin/windows/contrib/2.0/PACKAGES'
Content type `text/plain; charset=iso-8859-1' length 21411 bytes
opened URL
downloaded 20Kb

Error in "colnames<-"(`*tmp*`, value = c("Package", "LibPath", "Version",  :
        length of dimnames [2] not equal to array extent
In addition: There were 50 or more warnings (use warnings() to see the first
50)
>

> warnings()
Warning messages:
1: DESCRIPTION file of package  'aaa'  missing or broken
 in: packageDescription(p, lib = lib, fields = pkgFlds)
2: number of columns of result
        not a multiple of vector length (arg 2) in: rbind(retval, c(p, lib,
desc))
3: number of columns of result
        not a multiple of vector length (arg 2) in: rbind(retval, c(p, lib,
desc))
4: number of columns of result
        not a multiple of vector length (arg 2) in: rbind(retval, c(p, lib,
desc))
5: number of columns of result
        not a multiple of vector length (arg 2) in: rbind(retval, c(p, lib,
desc))
6: number of columns of result
        not a multiple of vector length (arg 2) in: rbind(retval, c(p, lib,
desc))
7: number of columns of result
        not a multiple of vector length (arg 2) in: rbind(retval, c(p, lib,
desc))
 .
 .
 .
 .
 .
 .... etc.  all the same ...



From facS93 at hampshire.edu  Thu Oct  7 02:49:24 2004
From: facS93 at hampshire.edu (facS93@hampshire.edu)
Date: Wed,  6 Oct 2004 20:49:24 -0400
Subject: [R] control enviromnet
Message-ID: <1097110164.41649294c3664@webmail.hampshire.edu>

Hi all:

I would like to implement an option in my function so that it warns me of any 
variables that are not defined in the current environment - if it needs to 
look up variables in the parent frame, it tells me so. 

The following is an example and it does what I want, and I'd rather have the 
environment control option inside this function instead of outside. Any help 
would be greatly appreciated.

x = 1
test = function(y) {
 ans = y + x;
 return(ans);
}
environment(test)=NULL
test(y = 1)

Thanks much!

Fang



From rpeng at jhsph.edu  Thu Oct  7 03:34:09 2004
From: rpeng at jhsph.edu (Roger D. Peng)
Date: Wed, 06 Oct 2004 21:34:09 -0400
Subject: [R] control enviromnet
In-Reply-To: <1097110164.41649294c3664@webmail.hampshire.edu>
References: <1097110164.41649294c3664@webmail.hampshire.edu>
Message-ID: <41649D11.2010100@jhsph.edu>

I think you want something like

exists("x", where = environment(), inherits = FALSE)

-roger

facS93 at hampshire.edu wrote:
> Hi all:
> 
> I would like to implement an option in my function so that it warns me of any 
> variables that are not defined in the current environment - if it needs to 
> look up variables in the parent frame, it tells me so. 
> 
> The following is an example and it does what I want, and I'd rather have the 
> environment control option inside this function instead of outside. Any help 
> would be greatly appreciated.
> 
> x = 1
> test = function(y) {
>  ans = y + x;
>  return(ans);
> }
> environment(test)=NULL
> test(y = 1)
> 
> Thanks much!
> 
> Fang
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>



From ggrothendieck at myway.com  Thu Oct  7 03:41:49 2004
From: ggrothendieck at myway.com (Gabor Grothendieck)
Date: Thu, 7 Oct 2004 01:41:49 +0000 (UTC)
Subject: [R] control enviromnet
References: <1097110164.41649294c3664@webmail.hampshire.edu>
Message-ID: <loom.20041007T033317-234@post.gmane.org>

 <facS93 <at> hampshire.edu> writes:

: 
: Hi all:
: 
: I would like to implement an option in my function so that it warns me of 
any 
: variables that are not defined in the current environment - if it needs to 
: look up variables in the parent frame, it tells me so. 
: 
: The following is an example and it does what I want, and I'd rather have the 
: environment control option inside this function instead of outside. Any help 
: would be greatly appreciated.
: 
: x = 1
: test = function(y) {
:  ans = y + x;
:  return(ans);
: }
: environment(test)=NULL
: test(y = 1)
: 

Here is one way.  

x <- 1
test <- function(y) {
	test <- function(y) x+y
	environment(test) <- NULL
	test(y)
}
test(9) # gives required error



From facS93 at hampshire.edu  Thu Oct  7 03:52:13 2004
From: facS93 at hampshire.edu (facS93@hampshire.edu)
Date: Wed,  6 Oct 2004 21:52:13 -0400
Subject: [R] control enviromnet
In-Reply-To: <41649D11.2010100@jhsph.edu>
References: <1097110164.41649294c3664@webmail.hampshire.edu>
	<41649D11.2010100@jhsph.edu>
Message-ID: <1097113933.4164a14de320d@webmail.hampshire.edu>

thanks Roger, 
this doesn't do exactly what I want, which I didn't explain clearly enough in 
the original post. I want to have a check mechanism to find whether there are 
variables in the function that are not "locally" defined, in which case I 
wouldn't know in advance what they are - well, I probably should because I am 
writing the function. It's just that when the function gets kind of large, and 
I am testing it as I am building it, it's easy to lose track of all these 
variables.

thanks,

Fang

Quoting "Roger D. Peng" <rpeng at jhsph.edu>:

> I think you want something like
> 
> exists("x", where = environment(), inherits = FALSE)
> 
> -roger
> 
> facS93 at hampshire.edu wrote:
> > Hi all:
> > 
> > I would like to implement an option in my function so that it warns me of
> any 
> > variables that are not defined in the current environment - if it needs to
> 
> > look up variables in the parent frame, it tells me so. 
> > 
> > The following is an example and it does what I want, and I'd rather have
> the 
> > environment control option inside this function instead of outside. Any
> help 
> > would be greatly appreciated.
> > 
> > x = 1
> > test = function(y) {
> >  ans = y + x;
> >  return(ans);
> > }
> > environment(test)=NULL
> > test(y = 1)
> > 
> > Thanks much!
> > 
> > Fang
> > 
> > ______________________________________________
> > R-help at stat.math.ethz.ch mailing list
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide!
> http://www.R-project.org/posting-guide.html
> > 
>



From rpeng at jhsph.edu  Thu Oct  7 05:01:50 2004
From: rpeng at jhsph.edu (Roger D. Peng)
Date: Wed, 06 Oct 2004 23:01:50 -0400
Subject: [R] control enviromnet
In-Reply-To: <1097113933.4164a14de320d@webmail.hampshire.edu>
References: <1097110164.41649294c3664@webmail.hampshire.edu>
	<41649D11.2010100@jhsph.edu>
	<1097113933.4164a14de320d@webmail.hampshire.edu>
Message-ID: <4164B19E.3040107@jhsph.edu>

I think you can use exists() in combination with ls(envir = environment()).

-roger

facS93 at hampshire.edu wrote:

> thanks Roger, 
> this doesn't do exactly what I want, which I didn't explain clearly enough in 
> the original post. I want to have a check mechanism to find whether there are 
> variables in the function that are not "locally" defined, in which case I 
> wouldn't know in advance what they are - well, I probably should because I am 
> writing the function. It's just that when the function gets kind of large, and 
> I am testing it as I am building it, it's easy to lose track of all these 
> variables.
> 
> thanks,
> 
> Fang
> 
> Quoting "Roger D. Peng" <rpeng at jhsph.edu>:
> 
> 
>>I think you want something like
>>
>>exists("x", where = environment(), inherits = FALSE)
>>
>>-roger
>>
>>facS93 at hampshire.edu wrote:
>>
>>>Hi all:
>>>
>>>I would like to implement an option in my function so that it warns me of
>>
>>any 
>>
>>>variables that are not defined in the current environment - if it needs to
>>
>>>look up variables in the parent frame, it tells me so. 
>>>
>>>The following is an example and it does what I want, and I'd rather have
>>
>>the 
>>
>>>environment control option inside this function instead of outside. Any
>>
>>help 
>>
>>>would be greatly appreciated.
>>>
>>>x = 1
>>>test = function(y) {
>>> ans = y + x;
>>> return(ans);
>>>}
>>>environment(test)=NULL
>>>test(y = 1)
>>>
>>>Thanks much!
>>>
>>>Fang
>>>
>>>______________________________________________
>>>R-help at stat.math.ethz.ch mailing list
>>>https://stat.ethz.ch/mailman/listinfo/r-help
>>>PLEASE do read the posting guide!
>>
>>http://www.R-project.org/posting-guide.html
>>
> 
> 
>



From joseph.gazaille at videotron.ca  Tue Oct  5 03:27:47 2004
From: joseph.gazaille at videotron.ca (Joseph J. Gazaille)
Date: Mon, 04 Oct 2004 21:27:47 -0400
Subject: [R] update.packages() with R 2.0.0
Message-ID: <001a01c4aa7a$a17751a0$306ffdcf@cyber>

Good day to all of you and thank you for reading this.

I certainly must have done something awfully wrong
when I downloaded and installed R 2.0.0 on a PC
with Windows 98.

You will find below what happens when  I try to 'update.packages()'.

I know what the first 'warning message'  means.
About the others, I ain't got no clue.

Thank you very handsomely for your wisdom.
And thank you also for all that tremendously
wonderful work.

 - joseph




> update.packages()
trying URL `http://cran.r-project.org/bin/windows/contrib/2.0/PACKAGES'
Content type `text/plain; charset=iso-8859-1' length 21411 bytes
opened URL
downloaded 20Kb

Error in "colnames<-"(`*tmp*`, value = c("Package", "LibPath", "Version",  :
        length of dimnames [2] not equal to array extent
In addition: There were 50 or more warnings (use warnings() to see the first
50)
>

> warnings()
Warning messages:
1: DESCRIPTION file of package  'aaa'  missing or broken
 in: packageDescription(p, lib = lib, fields = pkgFlds)
2: number of columns of result
        not a multiple of vector length (arg 2) in: rbind(retval, c(p, lib,
desc))
3: number of columns of result
        not a multiple of vector length (arg 2) in: rbind(retval, c(p, lib,
desc))
4: number of columns of result
        not a multiple of vector length (arg 2) in: rbind(retval, c(p, lib,
desc))
5: number of columns of result
        not a multiple of vector length (arg 2) in: rbind(retval, c(p, lib,
desc))
6: number of columns of result
        not a multiple of vector length (arg 2) in: rbind(retval, c(p, lib,
desc))
7: number of columns of result
        not a multiple of vector length (arg 2) in: rbind(retval, c(p, lib,
desc))
 .
 .
 .
 .
 .
 .... etc.  all the same ...



From murdoch at stats.uwo.ca  Thu Oct  7 05:56:32 2004
From: murdoch at stats.uwo.ca (Duncan Murdoch)
Date: Wed, 06 Oct 2004 23:56:32 -0400
Subject: [R] update.packages() with R 2.0.0
In-Reply-To: <001a01c4aa7a$a17751a0$306ffdcf@cyber>
References: <001a01c4aa7a$a17751a0$306ffdcf@cyber>
Message-ID: <8if9m0dl6dak8m2gmuo7o6untgpochc4a4@4ax.com>

On Mon, 04 Oct 2004 21:27:47 -0400, "Joseph J. Gazaille"
<joseph.gazaille at videotron.ca> wrote:

>Good day to all of you and thank you for reading this.
>
>I certainly must have done something awfully wrong
>when I downloaded and installed R 2.0.0 on a PC
>with Windows 98.
>
>You will find below what happens when  I try to 'update.packages()'.
>
>I know what the first 'warning message'  means.
>About the others, I ain't got no clue.
>
>Thank you very handsomely for your wisdom.
>And thank you also for all that tremendously
>wonderful work.

I don't know what you did wrong, but update.packages() doesn't do that
for me:  it appears to work.

Duncan Murdoch



From ajayshah at mayin.org  Wed Oct  6 18:50:55 2004
From: ajayshah at mayin.org (Ajay Shah)
Date: Wed, 06 Oct 2004 22:20:55 +0530
Subject: [R] Making a 'joint distribution'?
Message-ID: <E1CFF0B-0005P9-00@sanna.igidr.ac.in>


Thanks to everyone who helped me solve this question. My cleanest
solution is:

joint.and.marginals <- function(x,y) {
  t <- addmargins(table(x, y))
  rownames(t)[nrow(t)] <- deparse(substitute(y))
  colnames(t)[ncol(t)] <- deparse(substitute(x))
  return(t)
}

There are many other valid solutions, but this one struck me as being
the simplest.

As a demo of it's use:

> D <- data.frame(f1=sample(1:5,10000,replace=T), f2=sample(1:5,10000,replace=T)
> system.time(print(joint.and.marginals(D$f1, D$f2)))
      y
x         1    2    3    4    5  D$f1
  1     420  427  385  376  423  2031
  2     425  432  429  375  347  2008
  3     405  419  434  401  352  2011
  4     374  374  370  417  403  1938
  5     403  381  409  388  431  2012
  D$f2 2027 2033 2027 1957 1956 10000
[1] 0.05 0.00 0.07 0.00 0.00

Hmm, how would one get rid of the 'x' and 'y' that are occuring in the
table? :-)

	-ans.



From ggrothendieck at myway.com  Thu Oct  7 07:17:55 2004
From: ggrothendieck at myway.com (Gabor Grothendieck)
Date: Thu, 7 Oct 2004 05:17:55 +0000 (UTC)
Subject: [R] Making a 'joint distribution'?
References: <E1CFF0B-0005P9-00@sanna.igidr.ac.in>
Message-ID: <loom.20041007T070605-582@post.gmane.org>

Ajay Shah <ajayshah <at> mayin.org> writes:

: 
: Thanks to everyone who helped me solve this question. My cleanest
: solution is:
: 
: joint.and.marginals <- function(x,y) {
:   t <- addmargins(table(x, y))
:   rownames(t)[nrow(t)] <- deparse(substitute(y))
:   colnames(t)[ncol(t)] <- deparse(substitute(x))
:   return(t)
: }
: 
: There are many other valid solutions, but this one struck me as being
: the simplest.
: 
: As a demo of it's use:
: 
: > D <- data.frame(f1=sample(1:5,10000,replace=T), f2=sample
(1:5,10000,replace=T)
: > system.time(print(joint.and.marginals(D$f1, D$f2)))
:       y
: x         1    2    3    4    5  D$f1
:   1     420  427  385  376  423  2031
:   2     425  432  429  375  347  2008
:   3     405  419  434  401  352  2011
:   4     374  374  370  417  403  1938
:   5     403  381  409  388  431  2012
:   D$f2 2027 2033 2027 1957 1956 10000
: [1] 0.05 0.00 0.07 0.00 0.00
: 
: Hmm, how would one get rid of the 'x' and 'y' that are occuring in the
: table? 


Just add this line to your function:

	names(dimnames(t)) <- NULL

or you might consider the following which replaces x and y
with D$f1 and D$f2 and leaves the added rows and columns
with the Sum heading:

jm2 <- function(x,y) {
  t <- addmargins(table(x,y))
  names(dimnames(t)) <- list(deparse(substitute(x)), deparse(substitute(y)))
  tab
}
jm2(D$f1, D$f2)



From ggrothendieck at myway.com  Thu Oct  7 07:38:48 2004
From: ggrothendieck at myway.com (Gabor Grothendieck)
Date: Thu, 7 Oct 2004 05:38:48 +0000 (UTC)
Subject: [R] Making a 'joint distribution'?
References: <E1CFF0B-0005P9-00@sanna.igidr.ac.in>
	<loom.20041007T070605-582@post.gmane.org>
Message-ID: <loom.20041007T073733-836@post.gmane.org>

Gabor Grothendieck <ggrothendieck <at> myway.com> writes:

: 
: Ajay Shah <ajayshah <at> mayin.org> writes:
: 
: : 
: : Thanks to everyone who helped me solve this question. My cleanest
: : solution is:
: : 
: : joint.and.marginals <- function(x,y) {
: :   t <- addmargins(table(x, y))
: :   rownames(t)[nrow(t)] <- deparse(substitute(y))
: :   colnames(t)[ncol(t)] <- deparse(substitute(x))
: :   return(t)
: : }
: : 
: : There are many other valid solutions, but this one struck me as being
: : the simplest.
: : 
: : As a demo of it's use:
: : 
: : > D <- data.frame(f1=sample(1:5,10000,replace=T), f2=sample
: (1:5,10000,replace=T)
: : > system.time(print(joint.and.marginals(D$f1, D$f2)))
: :       y
: : x         1    2    3    4    5  D$f1
: :   1     420  427  385  376  423  2031
: :   2     425  432  429  375  347  2008
: :   3     405  419  434  401  352  2011
: :   4     374  374  370  417  403  1938
: :   5     403  381  409  388  431  2012
: :   D$f2 2027 2033 2027 1957 1956 10000
: : [1] 0.05 0.00 0.07 0.00 0.00
: : 
: : Hmm, how would one get rid of the 'x' and 'y' that are occuring in the
: : table? 
: 
: Just add this line to your function:
: 
: 	names(dimnames(t)) <- NULL
: 
: or you might consider the following which replaces x and y
: with D$f1 and D$f2 and leaves the added rows and columns
: with the Sum heading:
: 
: jm2 <- function(x,y) {
:   t <- addmargins(table(x,y))
:   names(dimnames(t)) <- list(deparse(substitute(x)), deparse(substitute(y)))
:   tab
: }
: jm2(D$f1, D$f2)


Sorry.  The line that says tab should be t:

 jm2 <- function(x,y) {
   t <- addmargins(table(x,y))
   names(dimnames(t)) <- list(deparse(substitute(x)), deparse(substitute(y)))
   t
 }
 jm2(D$f1, D$f2)



From ripley at stats.ox.ac.uk  Thu Oct  7 08:58:50 2004
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Thu, 7 Oct 2004 07:58:50 +0100 (BST)
Subject: [R] as.complex
In-Reply-To: <416471E1.8070001@univie.ac.at>
Message-ID: <Pine.LNX.4.44.0410070755040.5471-100000@gannet.stats>

On Thu, 7 Oct 2004, Erich Neuwirth wrote:

> as.complex("2+1i") -> 2+1i
> as.complex("2+i") -> NA
> 
> Does somebody have a modified version of as.complex
> which does the coercion in a less strict manner and
> produces a complex number also for strings like the
> second example?
> 
> Perhaps it would even make sense to change the behavior
> of as.complex to handle the second case.
> After all, this is an established way of writing
> complex numbers in some programming tools
> able to deal with complex numbers.

R does document that 1i is required, and as.complex uses the same rules 
for entering complex numbers as e.g. using them as part of an arithmetic 
expression.  Consistency is very important in a programming language.

It is easy to change 2+i to 2+1i using sub(), for example.

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From petr.pikal at precheza.cz  Thu Oct  7 09:02:01 2004
From: petr.pikal at precheza.cz (Petr Pikal)
Date: Thu, 07 Oct 2004 09:02:01 +0200
Subject: [R] library in R2.0.0
Message-ID: <41650609.8990.391A49@localhost>

Hi all

I upgraded to 2.0.0 version and did everything as I used to do 
before.

I installed windows binary, copy/paste other than bundled 
packages. 

I got e.g.
> library(chron)
Error in library(chron) : 'chron' is not a valid package -- installed < 
2.0.0?

so I loaded it from CRAN and everything worked OK except my 
own personal functions (they are not on CRAN). So I went 
through NEWS file which says:

o	Packages must have been re-installed for this version, and
	library() will enforce this.

I have never done it, my simple set of functions was never actually 
installed. I used copy/paste the directory from old R version to 
new one and I used library(fun) in personal .Rprofile to load my 
function set. Doing that again I got:

Error in library(fun) : There is no package called 'fun'

I know I can use source
source("D:/programy/R/rw2000/library/fun/R/fun")
but I feel library way is better way and I think I could try it.

Before I start to try transferring my function set to a real package I 
would like to ask simple ***question***.

Is the error message result of not installing package fun correctly 
(at all) or I shall do something more than follow instructions in 
README.packages and other documentation?
-----------------------------------------------------------------------------

Thank you.

Petr Pikal
petr.pikal at precheza.cz



From ligges at statistik.uni-dortmund.de  Thu Oct  7 09:20:21 2004
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Thu, 07 Oct 2004 09:20:21 +0200
Subject: [R] update.packages() with R 2.0.0
In-Reply-To: <001a01c4aa7a$a17751a0$306ffdcf@cyber>
References: <001a01c4aa7a$a17751a0$306ffdcf@cyber>
Message-ID: <4164EE35.6010201@statistik.uni-dortmund.de>

Joseph J. Gazaille wrote:

> Good day to all of you and thank you for reading this.
> 
> I certainly must have done something awfully wrong
> when I downloaded and installed R 2.0.0 on a PC
> with Windows 98.
> 
> You will find below what happens when  I try to 'update.packages()'.


Beside other things that might have gone wrong:
update.packages() is not sufficient after upgrading to R-2.0.0.
You have to reinstall into a clean library, because packages some 
packages have still the same version, but needed to be recompiled for 
R-2.0.0. Please install all the stuff from scratch.

Uwe Ligges



> I know what the first 'warning message'  means.
> About the others, I ain't got no clue.
> 
> Thank you very handsomely for your wisdom.
> And thank you also for all that tremendously
> wonderful work.
> 
>  - joseph
> 
> 
> 
> 
> 
>>update.packages()
> 
> trying URL `http://cran.r-project.org/bin/windows/contrib/2.0/PACKAGES'
> Content type `text/plain; charset=iso-8859-1' length 21411 bytes
> opened URL
> downloaded 20Kb
> 
> Error in "colnames<-"(`*tmp*`, value = c("Package", "LibPath", "Version",  :
>         length of dimnames [2] not equal to array extent
> In addition: There were 50 or more warnings (use warnings() to see the first
> 50)
> 
> 
>>warnings()
> 
> Warning messages:
> 1: DESCRIPTION file of package  'aaa'  missing or broken
>  in: packageDescription(p, lib = lib, fields = pkgFlds)
> 2: number of columns of result
>         not a multiple of vector length (arg 2) in: rbind(retval, c(p, lib,
> desc))
> 3: number of columns of result
>         not a multiple of vector length (arg 2) in: rbind(retval, c(p, lib,
> desc))
> 4: number of columns of result
>         not a multiple of vector length (arg 2) in: rbind(retval, c(p, lib,
> desc))
> 5: number of columns of result
>         not a multiple of vector length (arg 2) in: rbind(retval, c(p, lib,
> desc))
> 6: number of columns of result
>         not a multiple of vector length (arg 2) in: rbind(retval, c(p, lib,
> desc))
> 7: number of columns of result
>         not a multiple of vector length (arg 2) in: rbind(retval, c(p, lib,
> desc))
>  .
>  .
>  .
>  .
>  .
>  .... etc.  all the same ...
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html



From ripley at stats.ox.ac.uk  Thu Oct  7 09:28:40 2004
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Thu, 7 Oct 2004 08:28:40 +0100 (BST)
Subject: [R] library in R2.0.0
In-Reply-To: <41650609.8990.391A49@localhost>
Message-ID: <Pine.LNX.4.44.0410070818590.5471-100000@gannet.stats>

On Thu, 7 Oct 2004, Petr Pikal wrote:

> Hi all
> 
> I upgraded to 2.0.0 version and did everything as I used to do 
> before.
> 
> I installed windows binary, copy/paste other than bundled 
> packages. 
> 
> I got e.g.
> > library(chron)
> Error in library(chron) : 'chron' is not a valid package -- installed < 
> 2.0.0?
> 
> so I loaded it from CRAN and everything worked OK except my 
> own personal functions (they are not on CRAN). So I went 
> through NEWS file which says:
> 
> o	Packages must have been re-installed for this version, and
> 	library() will enforce this.
> 
> I have never done it, my simple set of functions was never actually 
> installed. I used copy/paste the directory from old R version to 
> new one and I used library(fun) in personal .Rprofile to load my 
> function set. Doing that again I got:
> 
> Error in library(fun) : There is no package called 'fun'
> 
> I know I can use source
> source("D:/programy/R/rw2000/library/fun/R/fun")
> but I feel library way is better way and I think I could try it.
> 
> Before I start to try transferring my function set to a real package I 
> would like to ask simple ***question***.
> 
> Is the error message result of not installing package fun correctly 
> (at all) or I shall do something more than follow instructions in 
> README.packages and other documentation?

I believe that message comes from your self-built package not having a 
DESCRIPTION file.  

Installing a package checks it has the features needed (such as a 
DESCRIPTION file) and then creates a lot of metadata (indices, parsed 
NAMESPACE and DESCRIPTION files and so on) that R assumes will be present 
when a package is loaded.  So as of 2.0.0 you do need to use R CMD INSTALL 
to create a usable package.

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From josh8912 at yahoo.com  Thu Oct  7 10:07:07 2004
From: josh8912 at yahoo.com (JJ)
Date: Thu, 7 Oct 2004 01:07:07 -0700 (PDT)
Subject: [R] Re: creating new varFunc classes in nlme .. error: "Don't
	know how to get coefficients for .. object" 
In-Reply-To: <Pine.LNX.4.44.0410031827410.13439-100000@gannet.stats>
Message-ID: <20041007080707.93415.qmail@web51706.mail.yahoo.com>

Thanks much Dr. Ripley.  Looks like the setMethod
statment was unneeded.  If anyone can help, Im still
looking for some insight into the error message I
receive using nlme:

Warning in conLin$Xy * varWeights(object) : longer
object length is not a multiple of shorter object
length Error in recalc.varFunc(object[[i]], conLin):
dim<- : dims [product 95] do not match the length of
object [600]

I receive this when I use a simple varFix statment,
but not a more complicated varCombo statement.  What
does it mean, in general, and is there a way to avoid
it?

Thanks.  John



--- Prof Brian Ripley <ripley at stats.ox.ac.uk> wrote:

> On Sun, 3 Oct 2004, J wrote:
> 
> > Ah!! After way too many hours of work Ive answered
> my
> > own question.  To get varExp2, a copy of varExp,
> to
> > work as a new varFunc, one needs to use this sort
> of
> > syntax:
> > 
> > update.varExp2 <- function (object, data, ...)
> > {
> >     print ("enter update")
> >     val <- NextMethod()
> >     if (length(val) == 0) {
> >         aux <- coef(val, allCoef = TRUE)
> >         if (!is.null(grps <- getGroups(val))) {
> >             aux <- aux[grps]
> >         }
> >         attr(val, "logLik") <- sum(log(attr(val,
> > "weights") <- exp(-aux *
> >             getCovariate(val))))
> >     }
> >     val
> > }
> > setMethod("update", "varExp2", update.varExp2)
> 
> update() is an S3 generic and nlme uses S3 classes,
> so this is definitely 
> wrong.  Have you been using S4 classes without
> mentioning it?
> 
> -- 
> Brian D. Ripley,                 
> ripley at stats.ox.ac.uk
> Professor of Applied Statistics, 
> http://www.stats.ox.ac.uk/~ripley/
> University of Oxford,             Tel:  +44 1865
> 272861 (self)
> 1 South Parks Road,                     +44 1865
> 272866 (PA)
> Oxford OX1 3TG, UK                Fax:  +44 1865
> 272595
> 
> 



		
_______________________________

Declare Yourself - Register online to vote today!



From pburns at pburns.seanet.com  Thu Oct  7 11:55:47 2004
From: pburns at pburns.seanet.com (Patrick Burns)
Date: Thu, 07 Oct 2004 10:55:47 +0100
Subject: [R] as.complex
In-Reply-To: <Pine.LNX.4.44.0410070755040.5471-100000@gannet.stats>
References: <Pine.LNX.4.44.0410070755040.5471-100000@gannet.stats>
Message-ID: <416512A3.6060305@pburns.seanet.com>

Perhaps there could be an extra argument to as.complex that
controls whether parsing is standard or loose.

Patrick Burns

Burns Statistics
patrick at burns-stat.com
+44 (0)20 8525 0696
http://www.burns-stat.com
(home of S Poetry and "A Guide for the Unwilling S User")

Prof Brian Ripley wrote:

>On Thu, 7 Oct 2004, Erich Neuwirth wrote:
>
>  
>
>>as.complex("2+1i") -> 2+1i
>>as.complex("2+i") -> NA
>>
>>Does somebody have a modified version of as.complex
>>which does the coercion in a less strict manner and
>>produces a complex number also for strings like the
>>second example?
>>
>>Perhaps it would even make sense to change the behavior
>>of as.complex to handle the second case.
>>After all, this is an established way of writing
>>complex numbers in some programming tools
>>able to deal with complex numbers.
>>    
>>
>
>R does document that 1i is required, and as.complex uses the same rules 
>for entering complex numbers as e.g. using them as part of an arithmetic 
>expression.  Consistency is very important in a programming language.
>
>It is easy to change 2+i to 2+1i using sub(), for example.
>
>  
>



From Luisr at frs.fo  Thu Oct  7 12:24:58 2004
From: Luisr at frs.fo (Luis Rideau Cruz)
Date: Thu, 07 Oct 2004 11:24:58 +0100
Subject: [R] library/utils/man/utils.RD.gz : Nosuch file or directory
Message-ID: <s1652791.000@ffdata.setur.fo>

R-help

I have installed R 2.0.0 ( Windows) on my machine and had several
'crashes' while working on it.

I have reinstalled a couple of times and still got strange behaviour
from time to time(screen goes white and no prompt is printed out,,,)

Last time I have reinstalled I checked the md5sum.txt (md5sum output
for the setup program and went O.K) and also Md5 from which I paste a
piece of output :

.
.
md5sum: library/utils/man/utils.Rd.gz: No such file or directory
library/utils/man/utils.Rd.gz: FAILED open or read
.
.
.
md5sum: library/tools/man/tools.Rd.gz: No such file or directory
library/tools/man/tools.Rd.gz: FAILED open or read

.
.
md5sum: WARNING: 25 of 3229 listed files could not be read

The "FAILED open or read" message appears for every library provided
with the R program

I don't know if there is any connection between this and the problems I
encounter.

Any explanation/solution for this?

========================
System specifications:

Microsoft Windows XP professional (Version 2002)
Pentium 4
M CPU 1.80 GHz
512 MB RAM
========================

Thank you in advance



From ripley at stats.ox.ac.uk  Thu Oct  7 13:17:41 2004
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Thu, 7 Oct 2004 12:17:41 +0100 (BST)
Subject: [R] library/utils/man/utils.RD.gz : Nosuch file or directory
In-Reply-To: <s1652791.000@ffdata.setur.fo>
Message-ID: <Pine.LNX.4.44.0410071213290.13965-100000@gannet.stats>

On Thu, 7 Oct 2004, Luis Rideau Cruz wrote:

> R-help
> 
> I have installed R 2.0.0 ( Windows) on my machine and had several
> 'crashes' while working on it.
> 
> I have reinstalled a couple of times and still got strange behaviour
> from time to time(screen goes white and no prompt is printed out,,,)
> 
> Last time I have reinstalled I checked the md5sum.txt (md5sum output
> for the setup program and went O.K) and also Md5 from which I paste a
> piece of output :
> 
> .
> .
> md5sum: library/utils/man/utils.Rd.gz: No such file or directory
> library/utils/man/utils.Rd.gz: FAILED open or read
> .
> .
> .
> md5sum: library/tools/man/tools.Rd.gz: No such file or directory
> library/tools/man/tools.Rd.gz: FAILED open or read
> 
> .
> .
> md5sum: WARNING: 25 of 3229 listed files could not be read

That's fine as you probably did not install those files (they are 
optional).

*However*, no one ever suggested that you run md5sum over the enclosed
md5sum.txt, but rather that you run the enclosed md5check.exe program
which knows how to find out which files you installed. See the CHANGES
file for how to use this.

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From danbebber at forestecology.co.uk  Thu Oct  7 13:35:00 2004
From: danbebber at forestecology.co.uk (Dan Bebber)
Date: Thu, 7 Oct 2004 12:35:00 +0100
Subject: [R] Repeated measures
In-Reply-To: <200410071014.i97A83b2007326@hypatia.math.ethz.ch>
Message-ID: <001201c4ac61$ae047770$7d2501a3@plants.ox.ac.uk>

Hi Sean,

I'm not sure I quite understand your question. Am I right in thinking that:
state = a binomial dependent variable
measure = a continuous predictor

If so, perhaps you could try using glmmPQL (Generalized Linear Mixed Models
fitted by Penalized Quasi-Likelihood) in library MASS.
The model would include random intercepts for each individual, have binomial
errors, and some kind of continuous autoregressive error structure (I
expect), and would look something like

results<-glmmPQL(fixed=state~measure,random=~1|individual, family=binomial,
correlation=corCar1(args...),data=your.data)

If I've got the wrong end of the stick, my apologies.

Dan Bebber

Department of Plant Sciences
University of Oxford
South Parks Road
Oxford OX1 3RB
UK
Tel. 01865 275000


------------------------------

Message: 11
Date: Wed, 6 Oct 2004 08:07:38 -0400
From: Sean Davis <sdavis2 at mail.nih.gov>
Subject: [R] Repeated measures
To: r-help <r-help at stat.math.ethz.ch>
Message-ID: <5125203F-1790-11D9-97DA-000A95D7BA10 at mail.nih.gov>
Content-Type: text/plain; charset=US-ASCII; format=flowed

I have a data set in which I have 5000 repeated measures on 6 subjects 
over time (varying intervals, but measurements for all individuals are 
at the same times).  There are two states, a "resting" state (the 
majority of the time), and a perturbed state.  I have a continuous 
measurement at each time point for each of the individuals.  I would 
like to determine the "state" for each individual at each time point.  
It looks to me like I should be able to do this with the "hidden" 
command from the "repeated" package 
(http://popgen0146uns50.unimaas.nl/~jlindsey/rcode.html), but I have 
found it a bit confusing to get started.  The distributions in the two 
states are approximately normal with differences in centrality and 
possibly variance (but I can start by assuming similar variances).

Thanks,
Sean



From thpe at hhbio.wasser.tu-dresden.de  Thu Oct  7 13:46:34 2004
From: thpe at hhbio.wasser.tu-dresden.de (Thomas Petzoldt)
Date: Thu, 07 Oct 2004 13:46:34 +0200
Subject: [R] R 2.0.0. startup fast! (was ... slow)
In-Reply-To: <4163E0DE.5020300@hhbio.wasser.tu-dresden.de>
References: <4163E0DE.5020300@hhbio.wasser.tu-dresden.de>
Message-ID: <41652C9A.7010501@hhbio.wasser.tu-dresden.de>

Hello and many thanks for all who replied!

I was wondering, why R 2.0.0/Windows with the complete CRAN set starts 
so slow from an NT 4.0 Server. Then I run a lot of systematic tests and 
found that only the NT4 server had this behavior but other servers 
(Windows 2003 and Linux/Samba) responded quickly.

Moreover I found, that this was due to a large number of directories in 
R_HOME/library and that there is a performance difference of local vs. 
remote directory listings on windows shares. After finding good keywords 
to describe this problem, it is solved now:

It is not an R problem but a (known) networking issue, see article 
number 177266 in the Microsoft Knowledge Base.

According to this I created a new DWORD value with name "SizeReqBuf" to

HKEY_LOCAL_MACHINE\SYSTEM\CurrentControlSet\Services\LanmanServer\Parameters

and set its value to 14596 (other articles suggest 65535, which requires 
more memory).

After ...

  net stop server
  net start server

... the problem disappeared immediately and now R starts and initializes 
within 3 seconds, that is absolutely acceptable and much faster than 
other, less powerful programs.

Many thanks to the R people for their great software!

Thomas P.



From h.andersson at nioo.knaw.nl  Thu Oct  7 13:58:10 2004
From: h.andersson at nioo.knaw.nl (Henrik Andersson)
Date: Thu, 07 Oct 2004 13:58:10 +0200
Subject: [R] confidence interval for nls
Message-ID: <ck3b10$pji$1@sea.gmane.org>

Do I have the right impression that it's currently not possible to 
produce confidence intervals for the nls predictions using R?

I had a course were we used SAS PROC nlin and there you could get 
intervals for the parameters and the prediction but I do not have access 
to SAS.

Would it be difficult to implement, I tried to dig into the help pages 
of nls, vcov and nlsModel but I could not really make sense out of this?

---------------------------------------------
Henrik Andersson
Netherlands Institute of Ecology -
Centre for Estuarine and Marine Ecology
P.O. Box 140
4400 AC Yerseke
Phone: +31 113 577473
h.andersson at nioo.knaw.nl
http://www.nioo.knaw.nl/ppages/handersson



From sdavis2 at mail.nih.gov  Thu Oct  7 14:19:31 2004
From: sdavis2 at mail.nih.gov (Sean Davis)
Date: Thu, 7 Oct 2004 08:19:31 -0400
Subject: [R] Repeated measures
In-Reply-To: <001201c4ac61$ae047770$7d2501a3@plants.ox.ac.uk>
References: <001201c4ac61$ae047770$7d2501a3@plants.ox.ac.uk>
Message-ID: <2492A72A-185B-11D9-959B-000A95D7BA10@mail.nih.gov>

Dan,

Thanks for the reply.  To be a bit more specific, the "time" variable 
continuous and the measurement is a real value at random intervals.  I 
would, ultimately, like to define a state for each measured point 
through time.  In my specific case right now, the state could be 
described as a binomial random variable with unknown p and measure is a 
continuous predictor.  I don't have a function for 
autocorrelation--this would have to be estimated from the data.  In 
R-speak, the data would be something like:

x <- runif(1000,max=50000000)
y <- c(rnorm(150,0),rnorm(350,b),rnorm(10,a),rnorm(490,0)) # just as an 
example--region lengths and positions vary

For my current specific case, I would like to find the region where the 
mean is a=b!=0 (the perturbed state) as compared to the other region 
where mean is 0 (the unperturbed state).

In a more general case, I would like to find all regions and states 
where a!=b!=0 and have state(a) be distinct from state(b), if it is 
"justifiably" different.

Thanks,

Sean

On Oct 7, 2004, at 7:35 AM, Dan Bebber wrote:

> Hi Sean,
>
> I'm not sure I quite understand your question. Am I right in thinking 
> that:
> state = a binomial dependent variable
> measure = a continuous predictor
>
> If so, perhaps you could try using glmmPQL (Generalized Linear Mixed 
> Models
> fitted by Penalized Quasi-Likelihood) in library MASS.
> The model would include random intercepts for each individual, have 
> binomial
> errors, and some kind of continuous autoregressive error structure (I
> expect), and would look something like
>
> results<-glmmPQL(fixed=state~measure,random=~1|individual, 
> family=binomial,
> correlation=corCar1(args...),data=your.data)
>
> If I've got the wrong end of the stick, my apologies.
>
> Dan Bebber
>
> Department of Plant Sciences
> University of Oxford
> South Parks Road
> Oxford OX1 3RB
> UK
> Tel. 01865 275000
>
>
> ------------------------------
>
> Message: 11
> Date: Wed, 6 Oct 2004 08:07:38 -0400
> From: Sean Davis <sdavis2 at mail.nih.gov>
> Subject: [R] Repeated measures
> To: r-help <r-help at stat.math.ethz.ch>
> Message-ID: <5125203F-1790-11D9-97DA-000A95D7BA10 at mail.nih.gov>
> Content-Type: text/plain; charset=US-ASCII; format=flowed
>
> I have a data set in which I have 5000 repeated measures on 6 subjects
> over time (varying intervals, but measurements for all individuals are
> at the same times).  There are two states, a "resting" state (the
> majority of the time), and a perturbed state.  I have a continuous
> measurement at each time point for each of the individuals.  I would
> like to determine the "state" for each individual at each time point.
> It looks to me like I should be able to do this with the "hidden"
> command from the "repeated" package
> (http://popgen0146uns50.unimaas.nl/~jlindsey/rcode.html), but I have
> found it a bit confusing to get started.  The distributions in the two
> states are approximately normal with differences in centrality and
> possibly variance (but I can start by assuming similar variances).
>
> Thanks,
> Sean



From rbelenov at yandex.ru  Thu Oct  7 14:48:48 2004
From: rbelenov at yandex.ru (Roman Belenov)
Date: Thu, 07 Oct 2004 16:48:48 +0400
Subject: [R] Problem compiling R 2.0.0 on windows
Message-ID: <ud5zu7ky7.fsf@intel.com>

I'm trying to compile R 2.0.0 on windows using cygwin. The program itself
compiled fine (I added -mno-cygwin to compilation/linkage rules so that mingw
runtime is used), but the build process fails to make packages. Here are the
error messages:

--------------------------------------------------------------------------------

  making bootstrap versions of packages ...
  ... done

writing help indices for package: base tools utils grDevices graphics stats data
sets methods grid splines stats4 tcltk

---------- Making package base ------------
  adding build stamp to DESCRIPTION
Error in library.dynam(lib, package, package.lib) :
        shared library 'tools' not found
Execution halted
make[4]: *** [frontmatter] Error 1
make[3]: *** [all] Error 2
make[2]: *** [pkg-base] Error 2
make[1]: *** [rpackage] Error 2
make: *** [all] Error 2

--------------------------------------------------------------------------------

Can anybody help ? Any ideas on what can go wrong are welcome.

-- 
 							With regards, Roman.



From p.dalgaard at biostat.ku.dk  Thu Oct  7 15:02:25 2004
From: p.dalgaard at biostat.ku.dk (Peter Dalgaard)
Date: 07 Oct 2004 15:02:25 +0200
Subject: [R] confidence interval for nls
In-Reply-To: <ck3b10$pji$1@sea.gmane.org>
References: <ck3b10$pji$1@sea.gmane.org>
Message-ID: <x2hdp6of4u.fsf@biostat.ku.dk>

Henrik Andersson <h.andersson at nioo.knaw.nl> writes:

> Do I have the right impression that it's currently not possible to
> produce confidence intervals for the nls predictions using R?
> 
> I had a course were we used SAS PROC nlin and there you could get
> intervals for the parameters and the prediction but I do not have
> access to SAS.
> 
> Would it be difficult to implement, I tried to dig into the help pages
> of nls, vcov and nlsModel but I could not really make sense out of
> this?

It's pretty trivial (if you stick with the linear approximation), once
you realize that predict.nls actually returns the gradient in an
attribute:

example(predict.nls)
se.fit <- sqrt(apply(attr(predict(fm,list(Time = tt)),"gradient"),1, 
                  function(x) sum(vcov(fm)*outer(x,x))))
matplot(tt, predict(fm,list(Time = tt))+
               outer(se.fit,qnorm(c(.5, .025,.975))),type="l")

points(demand ~ Time, data = BOD)

One slight issue is that it doesn't work if "newdata" is omitted, but
then you can easily get the gradient from fm$m$gradient()

-- 
   O__  ---- Peter Dalgaard             Blegdamsvej 3  
  c/ /'_ --- Dept. of Biostatistics     2200 Cph. N   
 (*) \(*) -- University of Copenhagen   Denmark      Ph: (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)             FAX: (+45) 35327907



From ripley at stats.ox.ac.uk  Thu Oct  7 15:18:35 2004
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Thu, 7 Oct 2004 14:18:35 +0100 (BST)
Subject: [R] Problem compiling R 2.0.0 on windows
In-Reply-To: <ud5zu7ky7.fsf@intel.com>
Message-ID: <Pine.LNX.4.44.0410071416250.22036-100000@gannet.stats>

On Thu, 7 Oct 2004, Roman Belenov wrote:

> I'm trying to compile R 2.0.0 on windows using cygwin. The program itself
> compiled fine (I added -mno-cygwin to compilation/linkage rules so that mingw
> runtime is used), but the build process fails to make packages. Here are the
> error messages:

We do not support cygwin (too many bugs over the years) even with 
-mno-cygwin, so you are on your own.  Suggestion: build under the 
supported tools first so you know what to expect.

> 
> --------------------------------------------------------------------------------
> 
>   making bootstrap versions of packages ...

There should be lines here, so if this were a fresh build something is 
badly wrong.

>   ... done
> 
> writing help indices for package: base tools utils grDevices graphics stats data
> sets methods grid splines stats4 tcltk
> 
> ---------- Making package base ------------
>   adding build stamp to DESCRIPTION
> Error in library.dynam(lib, package, package.lib) :
>         shared library 'tools' not found
> Execution halted
> make[4]: *** [frontmatter] Error 1
> make[3]: *** [all] Error 2
> make[2]: *** [pkg-base] Error 2
> make[1]: *** [rpackage] Error 2
> make: *** [all] Error 2
> 
> --------------------------------------------------------------------------------
> 
> Can anybody help ? Any ideas on what can go wrong are welcome.
> 
> 

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From ggrothendieck at myway.com  Thu Oct  7 15:39:02 2004
From: ggrothendieck at myway.com (Gabor Grothendieck)
Date: Thu, 7 Oct 2004 13:39:02 +0000 (UTC)
Subject: [R] Computing and testing transition probabilities?
References: <20041003184146.GA28159@igidr.ac.in>
Message-ID: <loom.20041007T153448-852@post.gmane.org>

Ajay Shah <ajayshah <at> mayin.org> writes:

: 
: Folks, I have a situation with many firms, observed for many years
: (but it's not panel data, so every now and then data for a firm just
: goes missing).
: 
: Let me show you an example. There are 3 firms "a", "b" and "c". There
: are 3 years: 1981, 1982 and 1983. There's a factor f which takes
: values 1, 2 or 3.
: 
: set.seed(5)
: D = data.frame(
:   names=c("a", "a", "a", "b", "b", "c", "c", "c", "d", "d"),
:   year= c( 81,  82,  83,  81,  83,  81,  82,  83,  82,  83),
:   f=    sample(1:3, 10, replace=T)
:   )
: print(D)
: 
: What I'd like to do is locate situations where a firm is observed for
: two consecutive years, and put together conditional probabilities of
: state transition.
: 
: Expressed as counts, putting time $t$ as the rows and time $t+1$ as
: the columms, I'd like to get the table:
: 
:     1   2   3
: 1           1
: 2       1
: 3   1   1   1

I do not get the same answer as you but my understanding from
your description is that this is what you are looking for:

R> set.seed(5)
R> D <- data.frame(
+  names =c("a", "a", "a", "b", "b", "c", "c", "c", "d", "d"),
+  year = c( 81, 82, 83, 81, 83, 81, 82, 83, 82, 83),
+  f= sample(1:3, 10, replace=T)
+ )
R> print(D)
   names year f
1      a   81 1
2      a   82 3
3      a   83 3
4      b   81 1
5      b   83 1
6      c   81 3
7      c   82 2
8      c   83 3
9      d   82 3
10     d   83 1
 
R> MM <- merge(D, cbind(year = D$year + 1, D), by.x = 2:1, by.y = 1:2)
R> tab <- with(MM, table(f.x, f.y))
R> tab
   f.y
f.x 1 2 3
  1 0 0 1
  2 0 0 1
  3 1 1 1
R> prop.table(tab, margin = 1)
   f.y
f.x 1         2         3        
  1 0.0000000 0.0000000 1.0000000
  2 0.0000000 0.0000000 1.0000000
  3 0.3333333 0.3333333 0.3333333



From bcutayar at lfdj.com  Thu Oct  7 15:39:40 2004
From: bcutayar at lfdj.com (Bruno Cutayar)
Date: Thu, 07 Oct 2004 15:39:40 +0200
Subject: [R] title in bold - simple question in R 1.9.0
Message-ID: <200410071340.PAA02187@bombardier2.lfdj.com>



Hi,
how can i write this simple sentence : "Hello world" with "Hello" only 
in bold ?
I try
 > plot(1:5)
 > title(main=paste(expression(bold("Hello")),"world",sep=" "))
but the result is wrong.

thanks,
Bruno


Si vous n'etes pas destinataires de ce message, merci d'avertir l'expediteur de l'erreur de distribution et de le detruire immediatement.
Ce message contient des informations confidentielles ou appartenant a La Francaise des Jeux. Il est etabli a l'intention exclusive de ses destinataires. Toute divulgation, utilisation, diffusion ou reproduction (totale ou partielle) de ce message ou des informations qu'il contient, doit etre prealablement autorisee.
Tout message electronique est susceptible d'alteration et son integrite ne peut etre assuree. La Francaise des Jeux decline toute responsabilite au titre de ce message s'il a ete modifie ou falsifie.

If you are not the intended recipient of this e-mail, please notify the sender of the wrong delivery and delete it immediately from your system.
This e-mail contains confidential information or information belonging to La Francaise des Jeux and is intended solely for the addressees. The unauthorised disclosure, use, dissemination or copying (either whole or partial) of this e-mail, or any information it contains, is prohibited.
E-mails are susceptible to alteration and their integrity cannot be guaranteed. La Francaise des Jeux shall not be liable for this e-mail if modified or falsified.



From h.andersson at nioo.knaw.nl  Thu Oct  7 16:00:31 2004
From: h.andersson at nioo.knaw.nl (Henrik Andersson)
Date: Thu, 07 Oct 2004 16:00:31 +0200
Subject: [R] confidence interval for nls
In-Reply-To: <x2hdp6of4u.fsf@biostat.ku.dk>
References: <ck3b10$pji$1@sea.gmane.org> <x2hdp6of4u.fsf@biostat.ku.dk>
Message-ID: <ck3i6c$fqd$1@sea.gmane.org>

I tried the example and it works fine,

but why o why, do I not get any gradient from another prediction?


############################################
#example code
############################################
yran <-
   c(0.0118821538311157, 0.323340819569374, 0.525108551769669, 
0.648173528359086,
     0.674941464557777, 0.745619897023202, 0.675263947547285, 
0.782342006150897,
     0.804472377613314, 0.813517628865957, 0.83178876929294, 
0.8353033444236,
     0.782341888336996, 0.782751195243735, 0.951449319233294, 
0.753939045638585,
     0.866298051948752, 0.90725708524503, 1.04254520185355, 
0.941419545223617,
     0.900965057020577)

x <- seq(0,10,length=21)

##plot(yran~x)
mich <- function(x,K,rmax) rmax*x/(x+K)
mm.nls <- nls(yran~mich(x,K,rmax),start=list(K=5,rmax=3))

xx=seq(0,10,length=200)
predict(mm.nls,newdata=list(x = xx))  #produces no gradient

mm.nls$m$gradient()                    #this does, but only 
at the 						       #observed data points

Peter Dalgaard wrote:

> Henrik Andersson <h.andersson at nioo.knaw.nl> writes:
> 
> 
>>Do I have the right impression that it's currently not possible to
>>produce confidence intervals for the nls predictions using R?
>>
>>I had a course were we used SAS PROC nlin and there you could get
>>intervals for the parameters and the prediction but I do not have
>>access to SAS.
>>
>>Would it be difficult to implement, I tried to dig into the help pages
>>of nls, vcov and nlsModel but I could not really make sense out of
>>this?
> 
> 
> It's pretty trivial (if you stick with the linear approximation), once
> you realize that predict.nls actually returns the gradient in an
> attribute:
> 
> example(predict.nls)
> se.fit <- sqrt(apply(attr(predict(fm,list(Time = tt)),"gradient"),1, 
>                   function(x) sum(vcov(fm)*outer(x,x))))
> matplot(tt, predict(fm,list(Time = tt))+
>                outer(se.fit,qnorm(c(.5, .025,.975))),type="l")
> 
> points(demand ~ Time, data = BOD)
> 
> One slight issue is that it doesn't work if "newdata" is omitted, but
> then you can easily get the gradient from fm$m$gradient()
> 


-- 
---------------------------------------------
Henrik Andersson
Netherlands Institute of Ecology -
Centre for Estuarine and Marine Ecology
P.O. Box 140
4400 AC Yerseke
Phone: +31 113 577473
h.andersson at nioo.knaw.nl
http://www.nioo.knaw.nl/ppages/handersson



From deepayan at stat.wisc.edu  Thu Oct  7 16:01:22 2004
From: deepayan at stat.wisc.edu (Deepayan Sarkar)
Date: Thu, 7 Oct 2004 09:01:22 -0500
Subject: [R] title in bold - simple question in R 1.9.0
In-Reply-To: <200410071340.PAA02187@bombardier2.lfdj.com>
References: <200410071340.PAA02187@bombardier2.lfdj.com>
Message-ID: <200410070901.22936.deepayan@stat.wisc.edu>

On Thursday 07 October 2004 08:39, Bruno Cutayar wrote:
> Hi,
> how can i write this simple sentence : "Hello world" with "Hello"
> only in bold ?
> I try
>
>  > plot(1:5)
>  > title(main=paste(expression(bold("Hello")),"world",sep=" "))
>
> but the result is wrong.

This seems to work:

title(main = expression(bold("Hello") ~ "world"))

Deepayan



From dimitris.rizopoulos at med.kuleuven.ac.be  Thu Oct  7 16:07:00 2004
From: dimitris.rizopoulos at med.kuleuven.ac.be (Dimitris Rizopoulos)
Date: Thu, 7 Oct 2004 16:07:00 +0200
Subject: [R] title in bold - simple question in R 1.9.0
References: <200410071340.PAA02187@bombardier2.lfdj.com>
Message-ID: <009101c4ac76$e9fce540$b2133a86@www.domain>

Hi Bruno,

you'd probably need the following:

plot(0)
title(main=expression(paste(bold("Hello"), " world")), font.main=1)

I hope it helps.

Best,
Dimitris

----
Dimitris Rizopoulos
Ph.D. Student
Biostatistical Centre
School of Public Health
Catholic University of Leuven

Address: Kapucijnenvoer 35, Leuven, Belgium
Tel: +32/16/396887
Fax: +32/16/337015
Web: http://www.med.kuleuven.ac.be/biostat/
     http://www.student.kuleuven.ac.be/~m0390867/dimitris.htm



----- Original Message ----- 
From: "Bruno Cutayar" <bcutayar at lfdj.com>
To: <r-help at stat.math.ethz.ch>
Sent: Thursday, October 07, 2004 3:39 PM
Subject: [R] title in bold - simple question in R 1.9.0


>
>
> Hi,
> how can i write this simple sentence : "Hello world" with "Hello" 
> only in bold ?
> I try
> > plot(1:5)
> > title(main=paste(expression(bold("Hello")),"world",sep=" "))
> but the result is wrong.
>
> thanks,
> Bruno
>
>
> Si vous n'etes pas destinataires de ce message, merci d'avertir 
> l'expediteur de l'erreur de distribution et de le detruire 
> immediatement.
> Ce message contient des informations confidentielles ou appartenant 
> a La Francaise des Jeux. Il est etabli a l'intention exclusive de 
> ses destinataires. Toute divulgation, utilisation, diffusion ou 
> reproduction (totale ou partielle) de ce message ou des informations 
> qu'il contient, doit etre prealablement autorisee.
> Tout message electronique est susceptible d'alteration et son 
> integrite ne peut etre assuree. La Francaise des Jeux decline toute 
> responsabilite au titre de ce message s'il a ete modifie ou 
> falsifie.
>
> If you are not the intended recipient of this e-mail, please notify 
> the sender of the wrong delivery and delete it immediately from your 
> system.
> This e-mail contains confidential information or information 
> belonging to La Francaise des Jeux and is intended solely for the 
> addressees. The unauthorised disclosure, use, dissemination or 
> copying (either whole or partial) of this e-mail, or any information 
> it contains, is prohibited.
> E-mails are susceptible to alteration and their integrity cannot be 
> guaranteed. La Francaise des Jeux shall not be liable for this 
> e-mail if modified or falsified.
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
>



From stonefly at mail.utexas.edu  Thu Oct  7 16:10:45 2004
From: stonefly at mail.utexas.edu (Bryan L. Brown)
Date: Thu, 7 Oct 2004 09:10:45 -0500
Subject: [R] Equivalents of Matlab's 'find' and 'end'
Message-ID: <003701c4ac77$700f8ed0$3a885380@D78DSM51>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20041007/cdb84895/attachment.pl

From nleonard at tartarus.uwa.edu.au  Thu Oct  7 16:13:58 2004
From: nleonard at tartarus.uwa.edu.au (Neil Leonard)
Date: Thu, 7 Oct 2004 22:13:58 +0800
Subject: [R] Remove Indeterminate Level
Message-ID: <215D6907-186B-11D9-91CC-003065D5B8EC@tartarus.uwa.edu.au>

Hi,

I have imported some data to R from stata and my factor variables have 
an Indeterminate level which I don't really want. For example the 
variable sex has the levels Male, Female and Indeterminate. There are 
no 'Indeterminate' values in the data. Can somebody tell me how to get 
rid of this level as it restricting my cox ph model.

Thanks
Neil



From rbelenov at yandex.ru  Thu Oct  7 16:29:32 2004
From: rbelenov at yandex.ru (Roman Belenov)
Date: Thu, 07 Oct 2004 18:29:32 +0400
Subject: [R] Problem compiling R 2.0.0 on windows
In-Reply-To: <Pine.LNX.4.44.0410071416250.22036-100000@gannet.stats> (Brian
	Ripley's message of "Thu, 7 Oct 2004 14:18:35 +0100 (BST)")
References: <Pine.LNX.4.44.0410071416250.22036-100000@gannet.stats>
Message-ID: <u3c0q7gab.fsf@intel.com>

Prof Brian Ripley <ripley at stats.ox.ac.uk> writes:

> We do not support cygwin (too many bugs over the years) even with 
> -mno-cygwin, so you are on your own.

That's OK, I don't expect any "formal" support in any sence.

> Suggestion: build under the supported tools first so you know what to
> expect.

That means downloading standalone Mingw tools that breaks initial point of
compiling R from sources - saving the bandwidth (I just wanted to compile the
sources under Linux and Windows). I'll consider it if I won't be able to do a
cygwin build after some time though.

>>   making bootstrap versions of packages ...
>
> There should be lines here, so if this were a fresh build something is 
> badly wrong.

No, it was just another 'make' run on partly built system. $(RHOME)/library
directory is already populated with DESCRIPTIONs, help files and R sources.

-- 
 							With regards, Roman.



From dimitris.rizopoulos at med.kuleuven.ac.be  Thu Oct  7 16:35:30 2004
From: dimitris.rizopoulos at med.kuleuven.ac.be (Dimitris Rizopoulos)
Date: Thu, 7 Oct 2004 16:35:30 +0200
Subject: [R] Equivalents of Matlab's 'find' and 'end'
References: <003701c4ac77$700f8ed0$3a885380@D78DSM51>
Message-ID: <00d601c4ac7a$e500f7d0$b2133a86@www.domain>

Hi Bryan,

1. which(x==1)
2. X[2:nrow(X),] or X[,2:ncol(X)]

The "An Introduction to R" document is very usdeful for this kind of 
things.

I hope it helps.

Best,
Dimitris

----
Dimitris Rizopoulos
Ph.D. Student
Biostatistical Centre
School of Public Health
Catholic University of Leuven

Address: Kapucijnenvoer 35, Leuven, Belgium
Tel: +32/16/396887
Fax: +32/16/337015
Web: http://www.med.kuleuven.ac.be/biostat/
     http://www.student.kuleuven.ac.be/~m0390867/dimitris.htm


----- Original Message ----- 
From: "Bryan L. Brown" <stonefly at mail.utexas.edu>
To: <R-help at stat.math.ethz.ch>
Sent: Thursday, October 07, 2004 4:10 PM
Subject: [R] Equivalents of Matlab's 'find' and 'end'


> Sorry if these questions have been asked recently--I'm new to this 
> list.
>
> I'm primarily a Matlab user who is attempting to learn R and I'm 
> searching for possible equivalents of commands that I found very 
> handy in Matlab.  So that I don't seem ungrateful to those who may 
> answer, I HAVE determined ways to carry out these processes in 
> 'brute force' sorts of ways in R code, but they lack the elegance 
> and simplicity of the Matlab commands.  Also, if you know that no 
> such commands exist, that bit of knowledge would be helpful to know 
> so that I don't continue fruitless searches.
>
> The first is Matlab's 'find' command.
> This is one of the most useful commands in Matab.  Basically, if X 
> is the vector
>
> X=[3, 2, 1, 1, 2, 3]
>
> the command
>
> 'find(X==1)'
>
> would return the vector [3, 4] which would indicate that the vector 
> X had the value of 1 at the 3 and 4 positions.  This was an 
> extremely useful command for subsetting in Matlab.  The closest 
> thing I've found in R has been 'match' but match only returns the 
> first value as opposed to the position of all matching values.
>
> The second Matlab command that I'd like to find an R equivalent for 
> is 'end'.  'end' is just a simple little command that indicates the 
> end of a row/column.  It is incredibly handy when used to subset 
> matrices like
>
> Y = X(2:end)
>
> and produces Y=[2, 1, 1, 2, 3] if the X is the same as in the 
> previous example.  This cutsie little command was extremely useful 
> for composing programs that were flexible and could use input 
> matrices of any size without modifying the code.  I realize that you 
> can accomplish the same by Y <- X[2:length(X)] in R, but this method 
> is ungainly, particularly when subsetting matrices rather than 
> vectors.
>
> If anyone has advice, I'd be grateful,
>
> Bryan L. Brown
> Integrative Biology
> University of Texas at Austin
> Austin, TX 78712
> 512-965-0678
> stonefly at mail.utexas.edu
> [[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
>



From tplate at blackmesacapital.com  Thu Oct  7 16:37:17 2004
From: tplate at blackmesacapital.com (Tony Plate)
Date: Thu, 07 Oct 2004 08:37:17 -0600
Subject: [R] Equivalents of Matlab's 'find' and 'end'
In-Reply-To: <003701c4ac77$700f8ed0$3a885380@D78DSM51>
References: <003701c4ac77$700f8ed0$3a885380@D78DSM51>
Message-ID: <6.1.0.6.2.20041007082917.067f6370@mailhost.blackmesacapital.com>

At Thursday 08:10 AM 10/7/2004, Bryan L. Brown wrote:
>Sorry if these questions have been asked recently--I'm new to this list.
>
>I'm primarily a Matlab user who is attempting to learn R and I'm searching 
>for possible equivalents of commands that I found very handy in 
>Matlab.  So that I don't seem ungrateful to those who may answer, I HAVE 
>determined ways to carry out these processes in 'brute force' sorts of 
>ways in R code, but they lack the elegance and simplicity of the Matlab 
>commands.  Also, if you know that no such commands exist, that bit of 
>knowledge would be helpful to know so that I don't continue fruitless 
>searches.
>
>The first is Matlab's 'find' command.
>This is one of the most useful commands in Matab.  Basically, if X is the 
>vector
>
>X=[3, 2, 1, 1, 2, 3]
>
>the command
>
>'find(X==1)'
>
>would return the vector [3, 4] which would indicate that the vector X had 
>the value of 1 at the 3 and 4 positions.  This was an extremely useful 
>command for subsetting in Matlab.  The closest thing I've found in R has 
>been 'match' but match only returns the first value as opposed to the 
>position of all matching values.

For this specific case, you can use which().  Also note that sometimes it 
can be useful to use match() with the arguments swapped, which can return 
you the positions of all matching values.  Also, the operator %in% can be 
useful:

 > X <- c(3, 2, 1, 1, 2, 3)
 > which(X==1)
[1] 3 4
 > match(1, X)
[1] 3
 > match(X, 1)
[1] NA NA  1  1 NA NA
 > which(!is.na(match(X, 1)))
[1] 3 4
 > which(X %in% 1)
[1] 3 4
 >


>The second Matlab command that I'd like to find an R equivalent for is 
>'end'.  'end' is just a simple little command that indicates the end of a 
>row/column.  It is incredibly handy when used to subset matrices like
>
>Y = X(2:end)
>
>and produces Y=[2, 1, 1, 2, 3] if the X is the same as in the previous 
>example.  This cutsie little command was extremely useful for composing 
>programs that were flexible and could use input matrices of any size 
>without modifying the code.  I realize that you can accomplish the same by 
>Y <- X[2:length(X)] in R, but this method is ungainly, particularly when 
>subsetting matrices rather than vectors.

Yep, that is a handy feature, and I often wish for something like it, but 
in my 10 years of using R/S-PLUS I've not come across anything better than 
using length(X) (or nrow(X)/ncol(X)) for the general case.  (But I do 
sometimes still discover useful things that I didn't know about.)

For your specific case of
 > Y = X(2:end)
in R/S-PLUS you can do:
 > Y = X[-1]

>If anyone has advice, I'd be grateful,
>
>Bryan L. Brown
>Integrative Biology
>University of Texas at Austin
>Austin, TX 78712
>512-965-0678
>stonefly at mail.utexas.edu
>         [[alternative HTML version deleted]]
>
>______________________________________________
>R-help at stat.math.ethz.ch mailing list
>https://stat.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html



From andy_liaw at merck.com  Thu Oct  7 16:37:05 2004
From: andy_liaw at merck.com (Liaw, Andy)
Date: Thu, 7 Oct 2004 10:37:05 -0400
Subject: [R] Remove Indeterminate Level
Message-ID: <3A822319EB35174CA3714066D590DCD504AF84E9@usrymx25.merck.com>

Try:

> sex <- factor(sample(c("M", "F"), 10, replace=TRUE), levels=c("M", "F",
"I"))
> sex
 [1] F F M F F F M M M F
Levels: M F I
> sex2 <- sex[, drop=TRUE]
> sex2
 [1] F F M F F F M M M F
Levels: M F

HTH,
Andy

> From: Neil Leonard
> 
> Hi,
> 
> I have imported some data to R from stata and my factor 
> variables have 
> an Indeterminate level which I don't really want. For example the 
> variable sex has the levels Male, Female and Indeterminate. There are 
> no 'Indeterminate' values in the data. Can somebody tell me 
> how to get 
> rid of this level as it restricting my cox ph model.
> 
> Thanks
> Neil
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
> 
>



From deepayan at stat.wisc.edu  Thu Oct  7 16:38:12 2004
From: deepayan at stat.wisc.edu (Deepayan Sarkar)
Date: Thu, 7 Oct 2004 09:38:12 -0500
Subject: [R] Equivalents of Matlab's 'find' and 'end'
In-Reply-To: <003701c4ac77$700f8ed0$3a885380@D78DSM51>
References: <003701c4ac77$700f8ed0$3a885380@D78DSM51>
Message-ID: <200410070938.12825.deepayan@stat.wisc.edu>

On Thursday 07 October 2004 09:10, Bryan L. Brown wrote:
> Sorry if these questions have been asked recently--I'm new to this
> list.
>
> I'm primarily a Matlab user who is attempting to learn R and I'm
> searching for possible equivalents of commands that I found very
> handy in Matlab.  So that I don't seem ungrateful to those who may
> answer, I HAVE determined ways to carry out these processes in 'brute
> force' sorts of ways in R code, but they lack the elegance and
> simplicity of the Matlab commands.  Also, if you know that no such
> commands exist, that bit of knowledge would be helpful to know so
> that I don't continue fruitless searches.
>
> The first is Matlab's 'find' command.
> This is one of the most useful commands in Matab.  Basically, if X is
> the vector
>
> X=[3, 2, 1, 1, 2, 3]
>
> the command
>
> 'find(X==1)'

which(X==1)

> would return the vector [3, 4] which would indicate that the vector X
> had the value of 1 at the 3 and 4 positions.  This was an extremely
> useful command for subsetting in Matlab.  The closest thing I've
> found in R has been 'match' but match only returns the first value as
> opposed to the position of all matching values.
>
> The second Matlab command that I'd like to find an R equivalent for
> is 'end'.  'end' is just a simple little command that indicates the
> end of a row/column.  It is incredibly handy when used to subset
> matrices like
>
> Y = X(2:end)
>
> and produces Y=[2, 1, 1, 2, 3] if the X is the same as in the
> previous example.  This cutsie little command was extremely useful
> for composing programs that were flexible and could use input
> matrices of any size without modifying the code.  I realize that you
> can accomplish the same by Y <- X[2:length(X)] in R, but this method
> is ungainly, particularly when subsetting matrices rather than
> vectors.

I don't know of anything better than length(X) or nrow/ncol(X), but you 
could do X[-1], which is at least as elegant for your example, and 
would work for matrices as well.

Hth,

Deepayan



From gavin.simpson at ucl.ac.uk  Thu Oct  7 16:38:24 2004
From: gavin.simpson at ucl.ac.uk (Gavin Simpson)
Date: Thu, 07 Oct 2004 15:38:24 +0100
Subject: [R] Equivalents of Matlab's 'find' and 'end'
In-Reply-To: <003701c4ac77$700f8ed0$3a885380@D78DSM51>
References: <003701c4ac77$700f8ed0$3a885380@D78DSM51>
Message-ID: <416554E0.7000004@ucl.ac.uk>

Bryan L. Brown wrote:
> Sorry if these questions have been asked recently--I'm new to this
> list.

Hi Bryan,

A useful resource is Robin Hankin's contributed document "R and Octave":

http://cran.r-project.org/doc/contrib/R-and-octave-2.txt


> The first is Matlab's 'find' command. This is one of the most useful
> commands in Matab.  Basically, if X is the vector
> 
> X=[3, 2, 1, 1, 2, 3]
> 
> the command
> 
> 'find(X==1)'
> 
> would return the vector [3, 4] 

 > x <- c(3, 2, 1, 1, 2, 3)
 > which(x == 1)
[1] 3 4


> 
> The second Matlab command that I'd like to find an R equivalent for
> is 'end'.  'end' is just a simple little command that indicates the
> end of a row/column.  It is incredibly handy when used to subset
> matrices like
> 
> Y = X(2:end)
> 
> and produces Y=[2, 1, 1, 2, 3] if the X is the same as in the
> previous example. 

 > Y <- x[-1]
 > Y
[1] 2 1 1 2 3

Not quite the same I'll grant you, but results in the same thing. If you 
wanted Y = X(4:end) you could use:

 > Y <- x[-c(1:3)]
 > Y
[1] 1 2 3

HTH

Gav
-- 
%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%
Gavin Simpson                     [T] +44 (0)20 7679 5522
ENSIS Research Fellow             [F] +44 (0)20 7679 7565
ENSIS Ltd. & ECRC                 [E] gavin.simpson at ucl.ac.uk
UCL Department of Geography       [W] http://www.ucl.ac.uk/~ucfagls/cv/
26 Bedford Way                    [W] http://www.ucl.ac.uk/~ucfagls/
London.  WC1H 0AP.
%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%



From ripley at stats.ox.ac.uk  Thu Oct  7 16:43:00 2004
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Thu, 7 Oct 2004 15:43:00 +0100 (BST)
Subject: [R] Equivalents of Matlab's 'find' and 'end'
In-Reply-To: <003701c4ac77$700f8ed0$3a885380@D78DSM51>
Message-ID: <Pine.LNX.4.44.0410071539180.5080-100000@gannet.stats>

On Thu, 7 Oct 2004, Bryan L. Brown wrote:

> Sorry if these questions have been asked recently--I'm new to this list.  
> 
> I'm primarily a Matlab user who is attempting to learn R and I'm
> searching for possible equivalents of commands that I found very handy
> in Matlab.  So that I don't seem ungrateful to those who may answer, I
> HAVE determined ways to carry out these processes in 'brute force' sorts
> of ways in R code, but they lack the elegance and simplicity of the
> Matlab commands.  Also, if you know that no such commands exist, that
> bit of knowledge would be helpful to know so that I don't continue
> fruitless searches.
> 
> The first is Matlab's 'find' command.  

> This is one of the most useful commands in Matab.  Basically, if X is
> the vector
> 
> X=[3, 2, 1, 1, 2, 3]
> 
> the command 
> 
> 'find(X==1)'
> 
> would return the vector [3, 4] which would indicate that the vector X
> had the value of 1 at the 3 and 4 positions.  This was an extremely
> useful command for subsetting in Matlab.  The closest thing I've found
> in R has been 'match' but match only returns the first value as opposed
> to the position of all matching values.

which(X==1)

> The second Matlab command that I'd like to find an R equivalent for is
> 'end'.  'end' is just a simple little command that indicates the end of
> a row/column.  It is incredibly handy when used to subset matrices like
> 
> Y = X(2:end)
> 
> and produces Y=[2, 1, 1, 2, 3] if the X is the same as in the previous example.  This cutsie little command was extremely useful for composing programs that were flexible and could use input matrices of any size without modifying the code.  I realize that you can accomplish the same by Y <- X[2:length(X)] in R, but this method is ungainly, particularly when subsetting matrices rather than vectors.  

X[2:length(X)]

but X[-1] would be easier in R.

> If anyone has advice, I'd be grateful,

Advice: read a good account of indexing in R:  `An Introduction to R' is a 
good start.

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From dimitris.rizopoulos at med.kuleuven.ac.be  Thu Oct  7 16:43:52 2004
From: dimitris.rizopoulos at med.kuleuven.ac.be (Dimitris Rizopoulos)
Date: Thu, 7 Oct 2004 16:43:52 +0200
Subject: [R] Remove Indeterminate Level
References: <215D6907-186B-11D9-91CC-003065D5B8EC@tartarus.uwa.edu.au>
Message-ID: <00dd01c4ac7c$10af42a0$b2133a86@www.domain>

Hi Neil,

you could try

sex <- sex[,drop=TRUE]

I hope it helps.

Best,
Dimitris

----
Dimitris Rizopoulos
Ph.D. Student
Biostatistical Centre
School of Public Health
Catholic University of Leuven

Address: Kapucijnenvoer 35, Leuven, Belgium
Tel: +32/16/396887
Fax: +32/16/337015
Web: http://www.med.kuleuven.ac.be/biostat/
     http://www.student.kuleuven.ac.be/~m0390867/dimitris.htm


----- Original Message ----- 
From: "Neil Leonard" <nleonard at tartarus.uwa.edu.au>
To: "r help" <r-help at stat.math.ethz.ch>
Sent: Thursday, October 07, 2004 4:13 PM
Subject: [R] Remove Indeterminate Level


> Hi,
>
> I have imported some data to R from stata and my factor variables 
> have an Indeterminate level which I don't really want. For example 
> the variable sex has the levels Male, Female and Indeterminate. 
> There are no 'Indeterminate' values in the data. Can somebody tell 
> me how to get rid of this level as it restricting my cox ph model.
>
> Thanks
> Neil
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
>



From macq at llnl.gov  Thu Oct  7 16:46:29 2004
From: macq at llnl.gov (Don MacQueen)
Date: Thu, 7 Oct 2004 07:46:29 -0700
Subject: [R] Problems with merge
In-Reply-To: <41637F1D.20506@mail.jnu.ac.in>
References: <41637F1D.20506@mail.jnu.ac.in>
Message-ID: <p06002002bd8b059af6be@[128.115.153.6]>

At 10:44 AM +0530 10/6/04, Vikas Rawal wrote:
>This issue has been discussed on this list before but the solutions 
>offerred are not satisfactory. So I thought I shall raise it again.
>
>I want to merge two datasets which have three common variables. 
>These variables DO NOT have the same names in both the files. In 
>addition, there are two variables with same name which do not 
>necessarily have exactly same data. That is, there could be some 
>discrepancy between the two datasets when it comes to these 
>variables. I do not want them to be used when I merge the datasets.
>
>The problem is that R allows you to use by.x and by.y variables to 
>specify only one variable in x dataset and one variable in y dataset 
>to merge. Otherwise, if you do not specify anything, it matches all 
>the variables that have common names to merge. This is very 
>problemmatic. In my case, the variables I want to use to match do 
>not have same names in two datasets and the ones that have same 
>names must not be used to match.
>
>One approach will be to change names of variables and then merge. 
>But that is not elegant, to say the least.
>
>If nothing else works, that is what I shall have to do. There again 
>we have some problem. How do I change the name of a particular 
>column. One solution suggested somewhere in the archives of the list 
>is to use
>
>names(data.frame)=c(list of column names)
>
>But this requires you to list all the variable names. That can 
>obviously be cumbersome when you have large number of variables. 
>What would be the syntax if I want to change just one column name.

It's not that hard to figure out the syntax, using functions like 
match(), intersect(), setdiff() and friends. Here is a suggestion:

mydf <- rename(mydf,from='oldvarname',to='newvarname')

where the rename function is this:

  rename <- function (data, from = "", to = "", info = T)
{
     dsn <- deparse(substitute(data))
     dfn <- names(data)
     if (length(from) != length(to)) {
         cat("--------- from and to not same length ---------\n")
         stop()
     }
     if (length(dfn) < length(to)) {
         cat("--------- too many new names ---------\n")
         stop()
     }
     chng <- match(from, dfn)
     frm.in <- from %in% dfn
     if (!all(frm.in)) {
         cat("---------- some of the from names not found in",
             dsn, "\n")
         stop()
     }
     if (length(to) != length(unique(to))) {
         cat("---------- New names not unique\n")
         stop()
     }
     dfn.new <- dfn
     dfn.new[chng] <- to
     if (info)
         cat("\nChanging in", dsn)
     tmp <- rbind(from, to)
     dimnames(tmp)[[1]] <- c("From:", "To:")
     dimnames(tmp)[[2]] <- rep("", length(from))
     if (info)
         print(tmp, quote = F)
     names(data) <- dfn.new
     invisible(data)
}

'from' and 'to' can be character vectors, and they must be of the same length.

It wouldn't be hard to modify it to *not* receive and return the 
entire dataframe, but I found it more convenient to use this way.

Also, I wrote that function a long time ago, when I had a lot less 
experience than I do now (just in case anyone notices some obvious 
room for improvement!)

>
>Vikas
>
>______________________________________________
>R-help at stat.math.ethz.ch mailing list
>https://stat.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html


-- 
--------------------------------------
Don MacQueen
Environmental Protection Department
Lawrence Livermore National Laboratory
Livermore, CA, USA



From andy_liaw at merck.com  Thu Oct  7 16:46:37 2004
From: andy_liaw at merck.com (Liaw, Andy)
Date: Thu, 7 Oct 2004 10:46:37 -0400
Subject: [R] Equivalents of Matlab's 'find' and 'end'
Message-ID: <3A822319EB35174CA3714066D590DCD504AF84EA@usrymx25.merck.com>

There's also the `cheat sheet'
http://cran.r-project.org/doc/contrib/R-and-octave-2.txt that might be
useful for Matlab speakers.

Andy

> From: Dimitris Rizopoulos
> 
> Hi Bryan,
> 
> 1. which(x==1)
> 2. X[2:nrow(X),] or X[,2:ncol(X)]
> 
> The "An Introduction to R" document is very usdeful for this kind of 
> things.
> 
> I hope it helps.
> 
> Best,
> Dimitris
> 
> ----
> Dimitris Rizopoulos
> Ph.D. Student
> Biostatistical Centre
> School of Public Health
> Catholic University of Leuven
> 
> Address: Kapucijnenvoer 35, Leuven, Belgium
> Tel: +32/16/396887
> Fax: +32/16/337015
> Web: http://www.med.kuleuven.ac.be/biostat/
>      http://www.student.kuleuven.ac.be/~m0390867/dimitris.htm
> 
> 
> ----- Original Message ----- 
> From: "Bryan L. Brown" <stonefly at mail.utexas.edu>
> To: <R-help at stat.math.ethz.ch>
> Sent: Thursday, October 07, 2004 4:10 PM
> Subject: [R] Equivalents of Matlab's 'find' and 'end'
> 
> 
> > Sorry if these questions have been asked recently--I'm new to this 
> > list.
> >
> > I'm primarily a Matlab user who is attempting to learn R and I'm 
> > searching for possible equivalents of commands that I found very 
> > handy in Matlab.  So that I don't seem ungrateful to those who may 
> > answer, I HAVE determined ways to carry out these processes in 
> > 'brute force' sorts of ways in R code, but they lack the elegance 
> > and simplicity of the Matlab commands.  Also, if you know that no 
> > such commands exist, that bit of knowledge would be helpful to know 
> > so that I don't continue fruitless searches.
> >
> > The first is Matlab's 'find' command.
> > This is one of the most useful commands in Matab.  Basically, if X 
> > is the vector
> >
> > X=[3, 2, 1, 1, 2, 3]
> >
> > the command
> >
> > 'find(X==1)'
> >
> > would return the vector [3, 4] which would indicate that the vector 
> > X had the value of 1 at the 3 and 4 positions.  This was an 
> > extremely useful command for subsetting in Matlab.  The closest 
> > thing I've found in R has been 'match' but match only returns the 
> > first value as opposed to the position of all matching values.
> >
> > The second Matlab command that I'd like to find an R equivalent for 
> > is 'end'.  'end' is just a simple little command that indicates the 
> > end of a row/column.  It is incredibly handy when used to subset 
> > matrices like
> >
> > Y = X(2:end)
> >
> > and produces Y=[2, 1, 1, 2, 3] if the X is the same as in the 
> > previous example.  This cutsie little command was extremely useful 
> > for composing programs that were flexible and could use input 
> > matrices of any size without modifying the code.  I realize 
> that you 
> > can accomplish the same by Y <- X[2:length(X)] in R, but 
> this method 
> > is ungainly, particularly when subsetting matrices rather than 
> > vectors.
> >
> > If anyone has advice, I'd be grateful,
> >
> > Bryan L. Brown
> > Integrative Biology
> > University of Texas at Austin
> > Austin, TX 78712
> > 512-965-0678
> > stonefly at mail.utexas.edu
> > [[alternative HTML version deleted]]
> >
> > ______________________________________________
> > R-help at stat.math.ethz.ch mailing list
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide! 
> > http://www.R-project.org/posting-guide.html
> >
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
> 
>



From kwright at eskimo.com  Thu Oct  7 17:01:16 2004
From: kwright at eskimo.com (kwright@eskimo.com)
Date: Thu, 7 Oct 2004 08:01:16 -0700 (PDT)
Subject: [R] How to use alpha transparency channel for colors?
Message-ID: <58413.170.54.59.167.1097161276.squirrel@170.54.59.167>


The release notes for R 2.0.0 states:

  It is now possible to specify colours with a full alpha
  transparency channel via the new 'alpha' argument to the
  rgb() and hsv() functions, or as a string of the form "#RRGGBBAA".

  NOTE: most devices draw nothing if a colour is not opaque,
  but PDF and Quartz devices will render semitransparent colours.

  A new argument 'alpha' to the function col2rgb()
  provides the ability to return the alpha component of
  colours (as well as the red, green, and blue components).

I'm using R 2.0.0 on Windows 2000 and wanted to try this feature.  The
following simple test works fine:

pdf("c:/alpha.pdf")
plot(rnorm(1:100),rnorm(1:100),col="#000055ff",pch=16)
dev.off()

But as soon as I change alpha value from "ff" to "fe", the points are no
longer visible for me.  I've tried viewing the pdf with Acrobat Reader
5.1.0 and gsview4.5.

Do I need a more recent pdf viewer?  Is this feature not working on
Windows?  Am I doing something wrong?  Any tips would be appreciated.

Thanks,

Kevin Wright



From ggrothendieck at myway.com  Thu Oct  7 17:01:45 2004
From: ggrothendieck at myway.com (Gabor Grothendieck)
Date: Thu, 7 Oct 2004 15:01:45 +0000 (UTC)
Subject: [R] Equivalents of Matlab's 'find' and 'end'
References: <003701c4ac77$700f8ed0$3a885380@D78DSM51>
Message-ID: <loom.20041007T165544-298@post.gmane.org>

Bryan L. Brown <stonefly <at> mail.utexas.edu> writes:

: 
: Sorry if these questions have been asked recently--I'm new to this list.  
: 
: I'm primarily a Matlab user who is attempting to learn R and I'm searching 
for possible equivalents of
: commands that I found very handy in Matlab.  So that I don't seem ungrateful 
to those who may answer, I HAVE
: determined ways to carry out these processes in 'brute force' sorts of ways 
in R code, but they lack the
: elegance and simplicity of the Matlab commands.  Also, if you know that no 
such commands exist, that bit of
: knowledge would be helpful to know so that I don't continue fruitless 
searches.  
: 
: The first is Matlab's 'find' command.  
: This is one of the most useful commands in Matab.  Basically, if X is the 
vector
: 
: X=[3, 2, 1, 1, 2, 3]
: 
: the command 
: 
: 'find(X==1)'
: 
: would return the vector [3, 4] which would indicate that the vector X had 
the value of 1 at the 3 and 4
: positions.  This was an extremely useful command for subsetting in Matlab.  
The closest thing I've found in
: R has been 'match' but match only returns the first value as opposed to the 
position of all matching values.  
: 
: The second Matlab command that I'd like to find an R equivalent for 
is 'end'.  'end' is just a simple little
: command that indicates the end of a row/column.  It is incredibly handy when 
used to subset matrices like
: 
: Y = X(2:end)
: 
: and produces Y=[2, 1, 1, 2, 3] if the X is the same as in the previous 
example.  This cutsie little command was
: extremely useful for composing programs that were flexible and could use 
input matrices of any size
: without modifying the code.  I realize that you can accomplish the same by Y 
<- X[2:length(X)] in R, but this
: method is ungainly, particularly when subsetting matrices rather than 
vectors.  
: 
: If anyone has advice, I'd be grateful,


In addition to the answers you already got there are:

X[-1]  # all but first element of a vector
mat[,-1] # all but first column
mat[-1,] # all but first row
tail(X, 8)  # last 8 elements of vector or last 8 rows of data frame


Also be sure to check out Robin Hankin's "R and Octave" referenced at:

http://cran.r-project.org/other-docs.html



From rksh at soc.soton.ac.uk  Thu Oct  7 17:12:01 2004
From: rksh at soc.soton.ac.uk (Robin Hankin)
Date: Thu, 7 Oct 2004 16:12:01 +0100
Subject: [R] crossprod vs %*% timing
In-Reply-To: <4163BCE2.8050903@wiwi.uni-bielefeld.de>
References: <a06002001bd895845800d@[139.166.242.29]>
	<4163BCE2.8050903@wiwi.uni-bielefeld.de>
Message-ID: <a06002005bd8b07df3e1b@[139.166.242.29]>

  thanks everyone who replied.


Brian Ripley wrote:

>  t(a) %*% A %*% a is a quadratic form.  What varies `many many times'? If A
>  does not vary (often), you want to find B with B'B = A (e.g. via chol,

Yes indeed!   For me, "A" varies only very rarely, so it makes sense 
to use this fact.  However, keeping track of when it _does_ change is 
not straightforward, so I am now facing
a trade-off between clear understandable code, and fast code.

The time saving  appears to be negative for matrices under about 
50x50, but increases with the order
of the matrix.  With



  f1 <- function(a,X){ t(a) %*% X %*% a               }
  f2 <- function(a,X){ crossprod(t(crossprod(a,X)),a) }
  f3 <- function(a,X){  crossprod(a,X) %*% a           }
  f4 <- function(a,X.lower){jj <- crossprod(X.lower,a )
                             crossprod(jj,jj)}

timer <- function(n,r){
   a <- rnorm(n)
   X <- matrix(rnorm(n*n),n,n)
   X <- crossprod(X,X)
   X.lower <- t(chol(X))
   print(system.time( for(i in 1:r){ f1(a,X)}))
   print(system.time( for(i in 1:r){ f2(a,X)}))
   print(system.time( for(i in 1:r){ f3(a,X)}))
   print(system.time( for(i in 1:r){ f4(a,X)}))
}
print("n=50")
timer(50,100000)
print("n=500")
timer(500,1000)

I get


[1] "n=50"
[1] 8.62 0.07 8.49 0.00 0.00
[1] 3.40 0.01 3.59 0.00 0.00
[1] 1.90 0.02 1.79 0.00 0.00
[1] 2.15 0.01 2.09 0.00 0.00
[1] "n=500"
[1] 7.44 0.50 6.90 0.00 0.00
[1] 2.12 0.39 1.58 0.00 0.00
[1] 2.16 0.33 1.53 0.00 0.00
[1] 1.56 0.43 1.28 0.00 0.00


so it's not entirely clear that any single method dominates.



-- 
Robin Hankin
Uncertainty Analyst
Southampton Oceanography Centre
SO14 3ZH
tel +44(0)23-8059-7743
initialDOTsurname at soc.soton.ac.uk (edit in obvious way; spam precaution)



From rksh at soc.soton.ac.uk  Thu Oct  7 17:14:47 2004
From: rksh at soc.soton.ac.uk (Robin Hankin)
Date: Thu, 7 Oct 2004 16:14:47 +0100
Subject: [R] Equivalents of Matlab's 'find' and 'end'
In-Reply-To: <003701c4ac77$700f8ed0$3a885380@D78DSM51>
References: <003701c4ac77$700f8ed0$3a885380@D78DSM51>
Message-ID: <a06002006bd8b0d6b8b0c@[139.166.242.29]>

Bryan

you may find R-and-octave.txt useful here.  This is a list of 
Matlab/Octave commands, and
their R equivalents.  It is in the "contributed docs" section of CRAN.

HTH

rksh


>Sorry if these questions have been asked recently--I'm new to this list. 
>
>I'm primarily a Matlab user who is attempting to learn R and I'm 
>searching for possible equivalents of commands that I found very 
>handy in Matlab.  So that I don't seem ungrateful to those who may 
>answer, I HAVE determined ways to carry out these processes in 
>'brute force' sorts of ways in R code, but they lack the elegance 
>and simplicity of the Matlab commands.  Also, if you know that no 
>such commands exist, that bit of knowledge would be helpful to know 
>so that I don't continue fruitless searches. 
>
>The f

-- 
Robin Hankin
Uncertainty Analyst
Southampton Oceanography Centre
SO14 3ZH
tel +44(0)23-8059-7743
initialDOTsurname at soc.soton.ac.uk (edit in obvious way; spam precaution)



From bill.shipley at usherbrooke.ca  Thu Oct  7 17:17:57 2004
From: bill.shipley at usherbrooke.ca (Bill Shipley)
Date: Thu, 7 Oct 2004 11:17:57 -0400
Subject: [R] question about df in linear mixed model function lme
Message-ID: <001401c4ac80$d6a51170$8c1ad284@BIO041>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20041007/2d010191/attachment.pl

From kshe4 at student.monash.edu  Thu Oct  7 17:14:57 2004
From: kshe4 at student.monash.edu (Kunal Shetty)
Date: Thu, 07 Oct 2004 15:14:57 +0000
Subject: [R] Re:How to create a R -application
Message-ID: <220.253.9.89.1097161619.74034@my.monash.edu.au>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20041007/51d2ce21/attachment.pl

From chenmh at mcu.edu.tw  Thu Oct  7 17:20:23 2004
From: chenmh at mcu.edu.tw (=?big5?B?s6+p+r33?=)
Date: Thu, 7 Oct 2004 23:20:23 +0800
Subject: [R] how to use chinese font in R/tcltk
Message-ID: <000001c4ac81$2ed4e780$f892fea9@yoyop800>

I can use Chinese fonts in R.

I can also use Chinese fonts in tcl/tk.

But I can not write Chinese using tkgrid in R.

The attached file is an example trying to write Chinese.

The Chinese font can not be displayed correctly.

Can anyone help me ?

Sincerely

-------------- next part --------------
An embedded and charset-unspecified text was scrubbed...
Name: tkttest.txt
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20041007/49b6793e/tkttest.txt

From tlumley at u.washington.edu  Thu Oct  7 17:37:10 2004
From: tlumley at u.washington.edu (Thomas Lumley)
Date: Thu, 7 Oct 2004 08:37:10 -0700 (PDT)
Subject: [R] Remove Indeterminate Level
In-Reply-To: <215D6907-186B-11D9-91CC-003065D5B8EC@tartarus.uwa.edu.au>
References: <215D6907-186B-11D9-91CC-003065D5B8EC@tartarus.uwa.edu.au>
Message-ID: <Pine.A41.4.61.0410070836540.25238@homer03.u.washington.edu>

On Thu, 7 Oct 2004, Neil Leonard wrote:

> Hi,
>
> I have imported some data to R from stata and my factor variables have an 
> Indeterminate level which I don't really want. For example the variable sex 
> has the levels Male, Female and Indeterminate. There are no 'Indeterminate' 
> values in the data. Can somebody tell me how to get rid of this level as it 
> restricting my cox ph model.
>

Two ways to do this are in the Examples section of help(factor)

 	-thomas



From deepayan at stat.wisc.edu  Thu Oct  7 17:52:33 2004
From: deepayan at stat.wisc.edu (Deepayan Sarkar)
Date: Thu, 7 Oct 2004 10:52:33 -0500
Subject: [R] How to use alpha transparency channel for colors?
In-Reply-To: <58413.170.54.59.167.1097161276.squirrel@170.54.59.167>
References: <58413.170.54.59.167.1097161276.squirrel@170.54.59.167>
Message-ID: <200410071052.33847.deepayan@stat.wisc.edu>

On Thursday 07 October 2004 10:01, kwright at eskimo.com wrote:
> The release notes for R 2.0.0 states:
>
>   It is now possible to specify colours with a full alpha
>   transparency channel via the new 'alpha' argument to the
>   rgb() and hsv() functions, or as a string of the form "#RRGGBBAA".
>
>   NOTE: most devices draw nothing if a colour is not opaque,
>   but PDF and Quartz devices will render semitransparent colours.
>
>   A new argument 'alpha' to the function col2rgb()
>   provides the ability to return the alpha component of
>   colours (as well as the red, green, and blue components).
>
> I'm using R 2.0.0 on Windows 2000 and wanted to try this feature. 
> The following simple test works fine:
>
> pdf("c:/alpha.pdf")

You probably just need (see ?pdf)

pdf("c:/alpha.pdf", version = "1.4")

> plot(rnorm(1:100),rnorm(1:100),col="#000055ff",pch=16)
> dev.off()
>
> But as soon as I change alpha value from "ff" to "fe", the points are
> no longer visible for me.  I've tried viewing the pdf with Acrobat
> Reader 5.1.0 and gsview4.5.
>
> Do I need a more recent pdf viewer?  Is this feature not working on
> Windows?  Am I doing something wrong?  Any tips would be appreciated.
>
> Thanks,
>
> Kevin Wright



From rbelenov at yandex.ru  Thu Oct  7 17:53:53 2004
From: rbelenov at yandex.ru (Roman Belenov)
Date: Thu, 07 Oct 2004 19:53:53 +0400
Subject: [R] Problem compiling R 2.0.0 on windows
In-Reply-To: <u3c0q7gab.fsf@intel.com> (Roman Belenov's message of "Thu, 07
	Oct 2004 18:29:32 +0400")
References: <Pine.LNX.4.44.0410071416250.22036-100000@gannet.stats>
	<u3c0q7gab.fsf@intel.com>
Message-ID: <usm8q5xta.fsf@intel.com>

Roman Belenov <rbelenov at yandex.ru> writes:

>>>   making bootstrap versions of packages ...
>>
>> There should be lines here, so if this were a fresh build something is 
>> badly wrong.
>
> No, it was just another 'make' run on partly built system. $(RHOME)/library
> directory is already populated with DESCRIPTIONs, help files and R sources.

Actually, that was really a manifestation of the problem - one of the 'make'
runs created DESCRIPTIONs but failed to compile DLLs; later make didn't even
try to build the shared libraries (it 'test -f's DESCRIPTION file to see
whether package is already built) and R failed to load the DLL later. Thanks for
your help!

-- 
 							With regards, Roman.



From ripley at stats.ox.ac.uk  Thu Oct  7 18:08:32 2004
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Thu, 7 Oct 2004 17:08:32 +0100 (BST)
Subject: [R] How to use alpha transparency channel for colors?
In-Reply-To: <58413.170.54.59.167.1097161276.squirrel@170.54.59.167>
Message-ID: <Pine.LNX.4.44.0410071658230.25108-100000@gannet.stats>

On Thu, 7 Oct 2004 kwright at eskimo.com wrote:

> The release notes for R 2.0.0 states:

Would that be the NEWS file?  R does not have `release notes' per se.

>   It is now possible to specify colours with a full alpha
>   transparency channel via the new 'alpha' argument to the
>   rgb() and hsv() functions, or as a string of the form "#RRGGBBAA".
> 
>   NOTE: most devices draw nothing if a colour is not opaque,
>   but PDF and Quartz devices will render semitransparent colours.
> 
>   A new argument 'alpha' to the function col2rgb()
>   provides the ability to return the alpha component of
>   colours (as well as the red, green, and blue components).
> 
> I'm using R 2.0.0 on Windows 2000 and wanted to try this feature.  The
> following simple test works fine:
> 
> pdf("c:/alpha.pdf")
> plot(rnorm(1:100),rnorm(1:100),col="#000055ff",pch=16)
> dev.off()
> 
> But as soon as I change alpha value from "ff" to "fe", the points are no
> longer visible for me.  I've tried viewing the pdf with Acrobat Reader
> 5.1.0 and gsview4.5.
> 
> Do I need a more recent pdf viewer?  Is this feature not working on
> Windows?  Am I doing something wrong?  Any tips would be appreciated.

As so often happens, the `something wrong' is not reading the help file.
As the NEWS file _also_ says

    o	A 'version' argument has been added to pdf() device.  If this is
	set to "1.4", the device will support transparent colours.

pdf("alpha.pdf", version="1.4")  should work for you: it does for me.


-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From ramasamy at cancer.org.uk  Thu Oct  7 18:49:30 2004
From: ramasamy at cancer.org.uk (Adaikalavan Ramasamy)
Date: Thu, 07 Oct 2004 17:49:30 +0100
Subject: [R] Re:How to create a R -application
In-Reply-To: <220.253.9.89.1097161619.74034@my.monash.edu.au>
References: <220.253.9.89.1097161619.74034@my.monash.edu.au>
Message-ID: <1097167770.3081.72.camel@ramasamy.stats>

This is not answering your question directly.

I usually use the BATCH command for running R non-interactively. You can
also use commandArgs() to get any arguments from the command line. For
more information, see help(BATCH) or help(commandArgs).


On Thu, 2004-10-07 at 16:14, Kunal Shetty wrote:
> Dear R- users and Helpers
> 
>        I am a beginner for R. I am using R to implement EM algorithm for treating Missing values. I would like to know how can save or compile my logical R commands to an application; so that the next time I could just execute the R- file.
> 
> Example for calculating the mean of  a data set
> 
> x <- c(8,11,16,18,6,4,20,25,9,13)
> u <- mean(x)
> u
> 
> Now I would like to save these commands as batch or an application
> 
> Anybody could please help or direct me in this problem
> Thank you
> Regards
> Kunal
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>



From ripley at stats.ox.ac.uk  Thu Oct  7 19:02:26 2004
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Thu, 7 Oct 2004 18:02:26 +0100 (BST)
Subject: [R] 'with' usage question
In-Reply-To: <20041007174341.243c2c1e@portia.local>
Message-ID: <Pine.LNX.4.44.0410071759340.25330-100000@gannet.stats>

Default arguments are evaluated in the function frame, not in the calling 
environment (nor in the same place as explicit arguments).

> Which to me reads that a with statement as above is equivalent to
>
> > attach(data) ; aov.SS1(y=Obs) ; detach(data)
>
> Or is that just wishful thinking??

The latter.


On Thu, 7 Oct 2004, RenE J.V. Bertin wrote:

> Hello,
> 
> I'm having a little argument with the 'with' function. I have 
> 
> 
> aov.SS1 <- function( y, indep=speed, fSnr=Subject, ... )
> {
> 	indep <- factor(speed)
> 	fSnr <- factor(fSnr)
>   ## ....
> }
> 
> and a dataframe containing speed, Subject, and a bunch of other columns. If I now do
> 
> > with( data, aov.SS1( y=Obs ) )
> 
> I get a message
> 
> Error in factor(indep) : Object "speed" not found
> 
> It seems that automatic argument initialisation doesn't work, as the only effectively valid call seems to be
> 
> > with( data, aov.SS1( y=Obs, indep=speed, fSnr=Subject ) )
> 
> whereas
> 
> > with( data, speed )
> 
> prints what one would expect (i.e. data$speed).
> 
> How come? From ?with, one learns that 
>      "... assignments within 'expr' take place in the constructed
>      environment and not in the user's workspace."
> 
> Which to me reads that a with statement as above is equivalent to
> 
> > attach(data) ; aov.SS1(y=Obs) ; detach(data)
> 
> Or is that just wishful thinking??
> 
> RenE
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
> 
> 

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From rpeng at jhsph.edu  Thu Oct  7 19:17:02 2004
From: rpeng at jhsph.edu (Roger D. Peng)
Date: Thu, 07 Oct 2004 13:17:02 -0400
Subject: [R] 'with' usage question
In-Reply-To: <20041007174341.243c2c1e@portia.local>
References: <20041007174341.243c2c1e@portia.local>
Message-ID: <41657A0E.6090204@jhsph.edu>

I think what's happening is that the function aov.SS1 will look up 
variables in the environment in which it was *defined*, which in your 
case (I'm guessing) is the global environment/workspace.  Therefore, 
if `speed' and `Subject' are not defined in the global environment, 
they will not be found.

-roger

RenE J.V. Bertin wrote:
> Hello,
> 
> I'm having a little argument with the 'with' function. I have 
> 
> 
> aov.SS1 <- function( y, indep=speed, fSnr=Subject, ... )
> {
> 	indep <- factor(speed)
> 	fSnr <- factor(fSnr)
>   ## ....
> }
> 
> and a dataframe containing speed, Subject, and a bunch of other columns. If I now do
> 
> 
>>with( data, aov.SS1( y=Obs ) )
> 
> 
> I get a message
> 
> Error in factor(indep) : Object "speed" not found
> 
> It seems that automatic argument initialisation doesn't work, as the only effectively valid call seems to be
> 
> 
>>with( data, aov.SS1( y=Obs, indep=speed, fSnr=Subject ) )
> 
> 
> whereas
> 
> 
>>with( data, speed )
> 
> 
> prints what one would expect (i.e. data$speed).
> 
> How come? From ?with, one learns that 
>      "... assignments within 'expr' take place in the constructed
>      environment and not in the user's workspace."
> 
> Which to me reads that a with statement as above is equivalent to
> 
> 
>>attach(data) ; aov.SS1(y=Obs) ; detach(data)
> 
> 
> Or is that just wishful thinking??
> 
> RenE
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>



From dmb at mrc-dunn.cam.ac.uk  Thu Oct  7 19:29:35 2004
From: dmb at mrc-dunn.cam.ac.uk (Dan Bolser)
Date: Thu, 7 Oct 2004 18:29:35 +0100 (BST)
Subject: [R] R-(wiki)-pedia?
In-Reply-To: <loom.20041007T014852-860@post.gmane.org>
Message-ID: <Pine.LNX.4.21.0410071823110.21400-100000@mail.mrc-dunn.cam.ac.uk>

On Wed, 6 Oct 2004, Gabor Grothendieck wrote:

>Dan Bolser <dmb <at> mrc-dunn.cam.ac.uk> writes:
>
>: 
>: Is there an R wiki?
>: 
>: Looking at the huge amount of traffic on this list, I think wiki could be
>: an exelet outlet for all the constructive enthusiasm here.
>: 
>: I don't think it would be too hard to port the existing R documentation
>: (the stuff you get with the ?) onto a wiki system, then users could add
>: their own examples and comments. 
>: 
>: Having distinct web page style organization would be easier to navigate
>: than the mailing list archives. I like the online version of the R-docs,
>: but I miss 'user comments' and I miss being able to fix trivial things.
>: 
>: The user contributed links could supply the best statistical online
>: resources to supplement the R-documentation, with links to the relevant
>: FAQ's and Tutorials supplementing the whole thing.
>: 
>: Now I have said it this sounds too good to not exist already... Anyone
>: dumped the R-documentation onto a wiki system?
>: 
>: Given the capability of R to be integrated into web pages, this could be
>: really really great. 
>: 
>: Cheers,
>: Dan.
>
>There is one at
>
>http://fawn.unibw-hamburg.de/cgi-bin/Rwiki.pl?RwikiHome
>
>Unfortunately, no one seems to use it.

I just added some pages... I think it would be great if people could get
motivated to contribute to something like this. Its one of those cases of
just getting the ball rolling...

Do you think you can dump the existing R-docs into this wiki as a
framework to get things going?

So often I read the docks and think, now if only this linked here, and if
only this example were more clear etc...

After I solve my problem I want to fix the docs. Wiki would let me do
that. 

How about you host a 'best diff in 30 days' competition each month, and
winning contributors could be announced on the list?

Can we integrate R-embedded web pages into the wiki?



>
>______________________________________________
>R-help at stat.math.ethz.ch mailing list
>https://stat.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>



From h.wickham at gmail.com  Thu Oct  7 19:27:29 2004
From: h.wickham at gmail.com (hadley wickham)
Date: Thu, 7 Oct 2004 12:27:29 -0500
Subject: [R] Subset doesn't drop unused factor levels
Message-ID: <f8e6ff0504100710271cfca472@mail.gmail.com>

a <- data.frame(b = rep(1:5, each=2), c=factor(rep("a",10), levels=c("a","b")))
levels(subset(a, b=1, drop=T)$c)
# [1] "a" "b"

Is this a bug?

Thanks,,

Hadley



From ggrothendieck at myway.com  Thu Oct  7 19:59:19 2004
From: ggrothendieck at myway.com (Gabor Grothendieck)
Date: Thu, 7 Oct 2004 17:59:19 +0000 (UTC)
Subject: [R] R-(wiki)-pedia?
References: <loom.20041007T014852-860@post.gmane.org>
	<Pine.LNX.4.21.0410071823110.21400-100000@mail.mrc-dunn.cam.ac.uk>
Message-ID: <loom.20041007T195617-232@post.gmane.org>

Dan Bolser <dmb <at> mrc-dunn.cam.ac.uk> writes:

: 
: On Wed, 6 Oct 2004, Gabor Grothendieck wrote:
: 
: >Dan Bolser <dmb <at> mrc-dunn.cam.ac.uk> writes:
: >
: >: 
: >: Is there an R wiki?
: >: 
: >: Looking at the huge amount of traffic on this list, I think wiki could be
: >: an exelet outlet for all the constructive enthusiasm here.
: >: 
: >: I don't think it would be too hard to port the existing R documentation
: >: (the stuff you get with the ?) onto a wiki system, then users could add
: >: their own examples and comments. 
: >: 
: >: Having distinct web page style organization would be easier to navigate
: >: than the mailing list archives. I like the online version of the R-docs,
: >: but I miss 'user comments' and I miss being able to fix trivial things.
: >: 
: >: The user contributed links could supply the best statistical online
: >: resources to supplement the R-documentation, with links to the relevant
: >: FAQ's and Tutorials supplementing the whole thing.
: >: 
: >: Now I have said it this sounds too good to not exist already... Anyone
: >: dumped the R-documentation onto a wiki system?
: >: 
: >: Given the capability of R to be integrated into web pages, this could be
: >: really really great. 
: >: 
: >: Cheers,
: >: Dan.
: >
: >There is one at
: >
: >http://fawn.unibw-hamburg.de/cgi-bin/Rwiki.pl?RwikiHome
: >
: >Unfortunately, no one seems to use it.
: 
: I just added some pages... I think it would be great if people could get
: motivated to contribute to something like this. Its one of those cases of
: just getting the ball rolling...
: 
: Do you think you can dump the existing R-docs into this wiki as a
: framework to get things going?
: 
: So often I read the docks and think, now if only this linked here, and if
: only this example were more clear etc...
: 
: After I solve my problem I want to fix the docs. Wiki would let me do
: that. 
: 
: How about you host a 'best diff in 30 days' competition each month, and
: winning contributors could be announced on the list?
: 
: Can we integrate R-embedded web pages into the wiki?

I certainly would be nice.  It has been mentioned before on the list before
but so far nothing has happened on that front.  BTW, the PHP language
documentation is good example of another language where this was done:

   http://www.php.net/manual/en/



From ligges at statistik.uni-dortmund.de  Thu Oct  7 20:00:37 2004
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Thu, 07 Oct 2004 20:00:37 +0200
Subject: [R] read.delim problem with trailing spaces
In-Reply-To: <4163F092.6070501@yorku.ca>
References: <4163F092.6070501@yorku.ca>
Message-ID: <41658445.70104@statistik.uni-dortmund.de>

Michael Friendly wrote:
> I'm trying to read a comma delimited dataset that uses '.' for NA.  I 
> found that if the last field on a line was a missing '.'
> it was not read as NA, but just a '.', and the life variable was made a 
> factor.  The data looks like this,
> 
> income,imr,region,oilexprt,imr80,gnp80,life
> Afghanistan,75,400.0,4,0,185.0,.,37.5
> Algeria,400,86.3,2,1,20.5,1920,50.7
> Argentina,1191,59.6,1,0,40.8,2390,67.1
> Australia,3426,26.7,4,0,12.5,9820,71.0
> Austria,3350,23.7,3,0,14.8,10230,70.4
> Bangladesh,100,124.3,4,0,139.0,120,.
> Belgium,3346,17.0,3,0,11.2,12180,70.6
> Benin,81,109.6,2,0,109.6,300,.
> Bolivia,200,60.4,1,0,77.3,570,49.7
> Brazil,425,170.0,1,0,84.0,2020,60.7
> Britain,2503,17.5,3,0,12.6,7920,72.0
> Burma,73,200.0,4,0,195.0,180,42.3
>  ...
> 
> and I used
>  > nations <- 
> read.delim("~/sasuser/data/nations2.dat",na.strings=".",row.name=1,sep=",",header=TRUE) 

You need to specify the argument
   strip.white = TRUE

BTW: Do you know that R-2.0.0 has been release?

Uwe


> 
>> nations[1:10,]
> 
>            income   imr region oilexprt imr80 gnp80 life
> Afghanistan     75 400.0      4        0 185.0    NA 37.5
> Algeria        400  86.3      2        1  20.5  1920 50.7
> Argentina     1191  59.6      1        0  40.8  2390 67.1
> Australia     3426  26.7      4        0  12.5  9820 71.0
> Austria       3350  23.7      3        0  14.8 10230 70.4
> Bangladesh     100 124.3      4        0 139.0   120   .
> Belgium       3346  17.0      3        0  11.2 12180 70.6
> Benin           81 109.6      2        0 109.6   300   .
> Bolivia        200  60.4      1        0  77.3   570 49.7
> Brazil         425 170.0      1        0  84.0  2020 60.7
> 
>> summary(nations$life)
> 
>  .  27.0 31.6 32.0 32.6 34.5 35.0 36.0 36.7 36.9 37.1 37.2 37.5 38.5 
> 38.8 40.5
>   2    1    1    1    1    1    2    1    1    1    1    1    1    3    
> 1    1
> 40.6 41.0 41.2 42.3 43.5 43.7 44.9 45.1 46.8 47.5 47.6 49.0 49.7 49.9 
> 50.0 50.5
>   1    6    1    4    1    1    1    1    1    3    1    3    1    1    
> 2    1
> 
> 
> After much hair-pulling, I discovered that the data lines for Bangladesh 
> and Benin contained a trailing space after the '.'.  Removing those made 
> the problem go away, but that shouldn't happen and I wonder if this is
> still a potential problem for others.   I'm using R 1.8.1.
> 
> -Michael
>



From sundar.dorai-raj at PDF.COM  Thu Oct  7 20:00:59 2004
From: sundar.dorai-raj at PDF.COM (Sundar Dorai-Raj)
Date: Thu, 07 Oct 2004 13:00:59 -0500
Subject: [R] Subset doesn't drop unused factor levels
In-Reply-To: <f8e6ff0504100710271cfca472@mail.gmail.com>
References: <f8e6ff0504100710271cfca472@mail.gmail.com>
Message-ID: <4165845B.40408@pdf.com>



hadley wickham wrote:
> a <- data.frame(b = rep(1:5, each=2), c=factor(rep("a",10), levels=c("a","b")))
> levels(subset(a, b=1, drop=T)$c)
> # [1] "a" "b"
> 
> Is this a bug?
> 

No, drop = TRUE is not doing what you're thinking it's doing. From 
?subset (R-1.9.1):

    The 'drop' argument is passed on to the indexing method for data
      frames.

I think you're hoping it is passed on to the indexing method for 
factors, which it isn't.

--sundar



From sundar.dorai-raj at PDF.COM  Thu Oct  7 20:02:55 2004
From: sundar.dorai-raj at PDF.COM (Sundar Dorai-Raj)
Date: Thu, 07 Oct 2004 13:02:55 -0500
Subject: [R] Subset doesn't drop unused factor levels
In-Reply-To: <f8e6ff0504100710271cfca472@mail.gmail.com>
References: <f8e6ff0504100710271cfca472@mail.gmail.com>
Message-ID: <416584CF.7070106@pdf.com>



hadley wickham wrote:

> a <- data.frame(b = rep(1:5, each=2), c=factor(rep("a",10), levels=c("a","b")))
> levels(subset(a, b=1, drop=T)$c)
> # [1] "a" "b"
> 
> Is this a bug?
> 

Also, I think you meant:

levels(subset(a, b==1, drop=T)$c)

(Note the double-equals for logical equality)

--sundar



From tplate at blackmesacapital.com  Thu Oct  7 20:04:08 2004
From: tplate at blackmesacapital.com (Tony Plate)
Date: Thu, 07 Oct 2004 12:04:08 -0600
Subject: [R] R-(wiki)-pedia?
In-Reply-To: <Pine.LNX.4.21.0410071823110.21400-100000@mail.mrc-dunn.cam
	.ac.uk>
References: <loom.20041007T014852-860@post.gmane.org>
	<Pine.LNX.4.21.0410071823110.21400-100000@mail.mrc-dunn.cam.ac.uk>
Message-ID: <6.1.0.6.2.20041007120219.066dc758@mailhost.blackmesacapital.com>

At Thursday 11:29 AM 10/7/2004, Dan Bolser wrote:
>[snip]
>I just added some pages... I think it would be great if people could get
>motivated to contribute to something like this. Its one of those cases of
>just getting the ball rolling...
>
>Do you think you can dump the existing R-docs into this wiki as a
>framework to get things going?

If the existing R-docs are "dumped" into a wiki, won't the copy in the Wiki 
quickly get out of date?  How does one get around this problem?

-- Tony Plate



From p.dalgaard at biostat.ku.dk  Thu Oct  7 20:05:31 2004
From: p.dalgaard at biostat.ku.dk (Peter Dalgaard)
Date: 07 Oct 2004 20:05:31 +0200
Subject: [R] Subset doesn't drop unused factor levels
In-Reply-To: <f8e6ff0504100710271cfca472@mail.gmail.com>
References: <f8e6ff0504100710271cfca472@mail.gmail.com>
Message-ID: <x28yaibdzo.fsf@biostat.ku.dk>

hadley wickham <h.wickham at gmail.com> writes:

> a <- data.frame(b = rep(1:5, each=2), c=factor(rep("a",10), levels=c("a","b")))
> levels(subset(a, b=1, drop=T)$c)
> # [1] "a" "b"
> 
> Is this a bug?

In some older versions of R (at least older than 1.9.0), there was a
documentation bug in that the help page said that it would drop unused
levels. Nowadays, it says 

    drop: passed on to '[' indexing operator.

which is correct and the docs for [.data.frame

    drop: logical.  If 'TRUE' the result is coerced to the lowest
          possible dimension: however, see the Warning below.

(and that Warning seems to have a typo, but leave that for now...). So
no, it is not a bug, that's not what that argument is for.

-- 
   O__  ---- Peter Dalgaard             Blegdamsvej 3  
  c/ /'_ --- Dept. of Biostatistics     2200 Cph. N   
 (*) \(*) -- University of Copenhagen   Denmark      Ph: (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)             FAX: (+45) 35327907



From andy_liaw at merck.com  Thu Oct  7 20:10:41 2004
From: andy_liaw at merck.com (Liaw, Andy)
Date: Thu, 7 Oct 2004 14:10:41 -0400
Subject: [R] Subset doesn't drop unused factor levels
Message-ID: <3A822319EB35174CA3714066D590DCD504AF84EC@usrymx25.merck.com>

> From: hadley wickham
> 
> a <- data.frame(b = rep(1:5, each=2), c=factor(rep("a",10), 
> levels=c("a","b")))
> levels(subset(a, b=1, drop=T)$c)
> # [1] "a" "b"
> 
> Is this a bug?

Don't think so:

> args("[.data.frame")
function (x, i, j, drop = if (missing(i)) TRUE else length(cols) == 
    1) 
NULL

So the `drop' argument is passed to the "[" method for data.frame (as
documented in ?subset), and not the "[" method for factor, as that's never
called.

Andy
 
> Thanks,,
> 
> Hadley
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
> 
>



From r.ghezzo at staff.mcgill.ca  Thu Oct  7 20:19:41 2004
From: r.ghezzo at staff.mcgill.ca (r.ghezzo@staff.mcgill.ca)
Date: Thu,  7 Oct 2004 14:19:41 -0400
Subject: [R] Re: problems installing package in R 2.0.0
In-Reply-To: <20041007193915.7785b3d0@portia.local>
References: <20041007174341.243c2c1e@portia.local>
	<Pine.LNX.4.44.0410071759340.25330-100000@gannet.stats>
	<20041007193915.7785b3d0@portia.local>
Message-ID: <1097173181.416588bde8caf@webmail.mcgill.ca>

Hello,
I just installed R 2.0.0 in a Win XP machine. As old programs do not wor I tried
to re-install them by:

>C:\R\RW2000\bin>Rcmd INSTALL c:\r\r_src\src\autologi
>
>----------Making package autologi ------------------
>  adding build stamp to DESCRIPTION
>  installing R files
>  installing data files
>  installing man source files
>  installing indices
>  not zipping data
>  installing help
> >>>Building/Updating help pages for package 'autologi'
>  Formats: text html latex example chm
>  autologi                 text  html   latex   example
>wc: C:/R/rw2000/library/autologi/R/autologi: No such file or directory
>  adding MD5 sums
>
>* DONE <autologit>

then in R

>library()
>Packages in library 'C:/R/rw2000/library':
>
>autologi                ** No title available (pre-2.0.0 install?) **
>base                    The R Base Package
> ...

my directory for autologi has the following structure:

c:\r\r_src\src\autologi\DESCRIPTION
                        TITLE
                        \R\autologi.r
                        \man\autologi.RD
                        \data\ex.dat

I could not find anything relevant in the last version of "Writing R Extensions"
that came with R 2.0.0.

Another question. I did a full "install packages from CRAN" but then comparing
the list of packages downloaded and installed with those in
CRAN/windows/contrib/2.0/ i found packages like moc, multidim, multiv, netCDF,
serialize, yags, xgobi. Can these packages be downloaded and installed or there
is something broken in them?
Thanks for any help and thanks to the R-Team.
Heberto Ghezzo - McGill University



From matthew_wiener at merck.com  Thu Oct  7 20:23:32 2004
From: matthew_wiener at merck.com (Wiener, Matthew)
Date: Thu, 7 Oct 2004 14:23:32 -0400
Subject: [R] R-(wiki)-pedia?
Message-ID: <45AAE6FD142DCB43A38C00A11FF5DF3E02226175@uswsmx03.merck.com>

When you think there should be new links, or can offer an example you think
would be clearer, it might be worth submitting a documentation modification
proposal (through the bug tracking link on the R home page, for example).
That would improve the basic documentation, which is what most people will
see first.

Regards,
Matt Wiener

-----Original Message-----
From: r-help-bounces at stat.math.ethz.ch
[mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Dan Bolser
Sent: Thursday, October 07, 2004 1:30 PM
To: Gabor Grothendieck
Cc: r-help at stat.math.ethz.ch
Subject: Re: [R] R-(wiki)-pedia?


On Wed, 6 Oct 2004, Gabor Grothendieck wrote:

>Dan Bolser <dmb <at> mrc-dunn.cam.ac.uk> writes:
>
>: 
>: Is there an R wiki?
>: 
>: Looking at the huge amount of traffic on this list, I think wiki could be
>: an exelet outlet for all the constructive enthusiasm here.
>: 
>: I don't think it would be too hard to port the existing R documentation
>: (the stuff you get with the ?) onto a wiki system, then users could add
>: their own examples and comments. 
>: 
>: Having distinct web page style organization would be easier to navigate
>: than the mailing list archives. I like the online version of the R-docs,
>: but I miss 'user comments' and I miss being able to fix trivial things.
>: 
>: The user contributed links could supply the best statistical online
>: resources to supplement the R-documentation, with links to the relevant
>: FAQ's and Tutorials supplementing the whole thing.
>: 
>: Now I have said it this sounds too good to not exist already... Anyone
>: dumped the R-documentation onto a wiki system?
>: 
>: Given the capability of R to be integrated into web pages, this could be
>: really really great. 
>: 
>: Cheers,
>: Dan.
>
>There is one at
>
>http://fawn.unibw-hamburg.de/cgi-bin/Rwiki.pl?RwikiHome
>
>Unfortunately, no one seems to use it.

I just added some pages... I think it would be great if people could get
motivated to contribute to something like this. Its one of those cases of
just getting the ball rolling...

Do you think you can dump the existing R-docs into this wiki as a
framework to get things going?

So often I read the docks and think, now if only this linked here, and if
only this example were more clear etc...

After I solve my problem I want to fix the docs. Wiki would let me do
that. 

How about you host a 'best diff in 30 days' competition each month, and
winning contributors could be announced on the list?

Can we integrate R-embedded web pages into the wiki?



>
>______________________________________________
>R-help at stat.math.ethz.ch mailing list
>https://stat.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide!
http://www.R-project.org/posting-guide.html
>

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide!
http://www.R-project.org/posting-guide.html



From f.harrell at vanderbilt.edu  Thu Oct  7 20:38:45 2004
From: f.harrell at vanderbilt.edu (Frank E Harrell Jr)
Date: Thu, 07 Oct 2004 13:38:45 -0500
Subject: [R] Subset doesn't drop unused factor levels
In-Reply-To: <f8e6ff0504100710271cfca472@mail.gmail.com>
References: <f8e6ff0504100710271cfca472@mail.gmail.com>
Message-ID: <41658D35.5030602@vanderbilt.edu>

hadley wickham wrote:
> a <- data.frame(b = rep(1:5, each=2), c=factor(rep("a",10), levels=c("a","b")))
> levels(subset(a, b=1, drop=T)$c)
> # [1] "a" "b"
> 
> Is this a bug?
> 
> Thanks,,
> 
> Hadley
> 

This is always controversial.  I am apparently in the small minority in 
believing that the default behavior is what you are wishing for.  That's 
why the Hmisc package by default drops unused levels (but allows you to 
override that with options(drop.unused.levels=FALSE).  It is distasteful 
to have to override system behavior but I felt I had to in this case. 
No one in R-core wanted to add a non-default option to R e.g. 
options(drop.unused.levels=TRUE).

Frank

-- 
Frank E Harrell Jr   Professor and Chair           School of Medicine
                      Department of Biostatistics   Vanderbilt University



From Ted.Harding at nessie.mcc.ac.uk  Thu Oct  7 20:32:22 2004
From: Ted.Harding at nessie.mcc.ac.uk ( (Ted Harding))
Date: Thu, 07 Oct 2004 19:32:22 +0100 (BST)
Subject: [R] Re:How to create a R -application
In-Reply-To: <220.253.9.89.1097161619.74034@my.monash.edu.au>
Message-ID: <XFMail.041007193222.Ted.Harding@nessie.mcc.ac.uk>

On 07-Oct-04 Kunal Shetty wrote:
> Dear R- users and Helpers
> 
>        I am a beginner for R. I am using R to implement EM algorithm
> for treating Missing values. I would like to know how can save or
> compile my logical R commands to an application; so that the next time
> I could just execute the R- file.
> 
> Example for calculating the mean of  a data set
> 
> x <- c(8,11,16,18,6,4,20,25,9,13)
> u <- mean(x)
> u
> 
> Now I would like to save these commands as batch or an application
> 
> Anybody could please help or direct me in this problem

There are several approaches possible, depending on what you find
convenient at the time.

1. Have a look at the history-related commands:

  ?history

  savehistory(file="mycommands")

will save the entire history of your session in "mycommands" which
you can later edit.

2. In developing an application, I often experiment with different
forms of a command or different combinations of commands. When I'm
satisfied with a group of commands, I can copy-and-paste that part
of the R window into a file which I have open in a separate editing
window on the side (using X windows in Linux here, which makes this
very easy).

It can be convenient to use the 'history' command setting the
"max.show" parameter (default=25), e.g.

  history(max.show=15)

will give you only the last 15 commands you used. You can then
copy over a selected few of these.

3. A converse version of (2) is to initially enter the commands
into your editing window, and then copy-and-paste these into
the R window to test them. Edit the commands file until you're
happy.

While I'm at it, are you aware of the R library packages 'cat',
'norm', 'mix' and 'pan' which implement Shafer's EM methods
for imputing missing data? See CRAN.

Good luck, and welcome to R!
Ted.


--------------------------------------------------------------------
E-Mail: (Ted Harding) <Ted.Harding at nessie.mcc.ac.uk>
Fax-to-email: +44 (0)870 094 0861  [NB: New number!]
Date: 07-Oct-04                                       Time: 19:32:22
------------------------------ XFMail ------------------------------



From thpe at hhbio.wasser.tu-dresden.de  Thu Oct  7 21:18:37 2004
From: thpe at hhbio.wasser.tu-dresden.de (Thomas Petzoldt)
Date: Thu, 07 Oct 2004 21:18:37 +0200
Subject: [R] R-(wiki)-pedia?
In-Reply-To: <6.1.0.6.2.20041007120219.066dc758@mailhost.blackmesacapital.com>
References: <loom.20041007T014852-860@post.gmane.org>	<Pine.LNX.4.21.0410071823110.21400-100000@mail.mrc-dunn.cam.ac.uk>
	<6.1.0.6.2.20041007120219.066dc758@mailhost.blackmesacapital.com>
Message-ID: <4165968D.6040201@hhbio.wasser.tu-dresden.de>

Tony Plate wrote:
 > At Thursday 11:29 AM 10/7/2004, Dan Bolser wrote:
> 
>> [snip] I just added some pages... I think it would be great if 
>> people could get motivated to contribute to something like this. 
>> Its one of those cases of just getting the ball rolling...
>> 
>> Do you think you can dump the existing R-docs into this wiki as a 
>> framework to get things going?
> 
> 
> If the existing R-docs are "dumped" into a wiki, won't the copy in 
> the Wiki quickly get out of date? How does one get around this 
> problem?

And another problem is, how to reverse from something like HTML or XML 
back to .Rd ? I think that a generic format is absolutely necessary. The 
generic format we have and not yet another source of confusion.

If wiki means editing .Rd sources or better: inverse transformation in 
both sides (without to many loss), it would be nice. Is there really a 
chance?

Thomas P.



From ggrothendieck at myway.com  Thu Oct  7 21:20:18 2004
From: ggrothendieck at myway.com (Gabor Grothendieck)
Date: Thu, 7 Oct 2004 19:20:18 +0000 (UTC)
Subject: [R] R-(wiki)-pedia?
References: <loom.20041007T014852-860@post.gmane.org>
	<Pine.LNX.4.21.0410071823110.21400-100000@mail.mrc-dunn.cam.ac.uk>
	<Pine.LNX.4.21.0410071823110.21400-100000@mail.mrc-dunn.cam .ac.uk>
	<6.1.0.6.2.20041007120219.066dc758@mailhost.blackmesacapital.com>
Message-ID: <loom.20041007T211651-640@post.gmane.org>

Tony Plate <tplate <at> blackmesacapital.com> writes:

: 
: At Thursday 11:29 AM 10/7/2004, Dan Bolser wrote:
: >[snip]
: >I just added some pages... I think it would be great if people could get
: >motivated to contribute to something like this. Its one of those cases of
: >just getting the ball rolling...
: >
: >Do you think you can dump the existing R-docs into this wiki as a
: >framework to get things going?
: 
: If the existing R-docs are "dumped" into a wiki, won't the copy in the Wiki 
: quickly get out of date?  How does one get around this problem?

The way the PHP manual works is that all comments are placed at the end
of each manual page, blog-style.



From kshe4 at student.monash.edu  Thu Oct  7 21:27:01 2004
From: kshe4 at student.monash.edu (Kunal Shetty)
Date: Thu, 07 Oct 2004 19:27:01 +0000
Subject: [R] Re:How to create a R -application
References: <XFMail.041007193222.Ted.Harding@nessie.mcc.ac.uk>
Message-ID: <220.253.9.89.1097176805.21874@my.monash.edu.au>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20041007/7e708c53/attachment.pl

From rpeng at jhsph.edu  Thu Oct  7 21:28:59 2004
From: rpeng at jhsph.edu (Roger D. Peng)
Date: Thu, 07 Oct 2004 15:28:59 -0400
Subject: [R] 'with' usage question
In-Reply-To: <20041007195836.40cd474b@portia.local>
References: <20041007174341.243c2c1e@portia.local> <41657A0E.6090204@jhsph.edu>
	<20041007195836.40cd474b@portia.local>
Message-ID: <416598FB.8000001@jhsph.edu>

T/F are not defined in the global environment, rather they are in the
`base' package.  That's why you can override them by redefining T/F in
the global environment (or somewhere farther up the search list than 
`base').

attach() is different because it places variables on the search list 
*after* the global environment.

-roger

RenE J.V. Bertin wrote:
> On Thu, 07 Oct 2004 13:17:02 -0400, "Roger D. Peng"
> <rpeng at jhsph.edu> wrote regarding "Re: [R] 'with' usage question"
> 
> 8-) I think what's happening is that the function aov.SS1 will look
> up 8-) variables in the environment in which it was *defined*,
> which in your 8-) case (I'm guessing) is the global
> environment/workspace.  Therefore, 8-) if `speed' and `Subject' are
> not defined in the global environment, 8-) they will not be found.
> 
> 
> If you take that to the letter, doesn't it mean that attach() and
> detach() couldn't have the effect they have? Which is, in fact and
> if memory serves me well, much like the Pascal 'with' operator. A
> (possible) counter argument: aov.SS1() used T and F as the usual
> abbreviations for TRUE and FALSE: these are defined in the global
> environment. Now my dataframes also contain a variable T: it took
> me quite a while to realise that this variable (which I in fact
> never use directly) would override the T constant/operator and
> cause hard-to-trace error messages.
> 
> I'm trying to find some analogy. with() extends the current
> environment with the variables in the given dataframe, but does not
> create a C-like scope. Rather, it functions more or less like a
> Bourne shell script: the new variables are not 'exported' to be
> available to code evoked from within that new environment, unless
> they were specifically passed as arguments. Which still doesn't
> explain why ls() sees everything when called within a with()
> statement...
> 
> Anyway, I'd better get back to work before my gray matter
> shortcircuits :)
> 
> ______________________________________________ 
> R-help at stat.math.ethz.ch mailing list 
> https://stat.ethz.ch/mailman/listinfo/r-help PLEASE do read the
> posting guide! http://www.R-project.org/posting-guide.html
>



From wolski at molgen.mpg.de  Thu Oct  7 21:55:49 2004
From: wolski at molgen.mpg.de (Witold Eryk Wolski)
Date: Thu, 07 Oct 2004 21:55:49 +0200
Subject: [R] R-(wiki)-pedia?
In-Reply-To: <4165968D.6040201@hhbio.wasser.tu-dresden.de>
References: <loom.20041007T014852-860@post.gmane.org>	<Pine.LNX.4.21.0410071823110.21400-100000@mail.mrc-dunn.cam.ac.uk>	<6.1.0.6.2.20041007120219.066dc758@mailhost.blackmesacapital.com>
	<4165968D.6040201@hhbio.wasser.tu-dresden.de>
Message-ID: <41659F45.4090206@molgen.mpg.de>

Thomas Petzoldt wrote:

> Tony Plate wrote:
> > At Thursday 11:29 AM 10/7/2004, Dan Bolser wrote:
>
>>
>>> [snip] I just added some pages... I think it would be great if 
>>> people could get motivated to contribute to something like this. Its 
>>> one of those cases of just getting the ball rolling...
>>>
>>> Do you think you can dump the existing R-docs into this wiki as a 
>>> framework to get things going?
>>
>>
>>
>> If the existing R-docs are "dumped" into a wiki, won't the copy in 
>> the Wiki quickly get out of date? How does one get around this problem?
>
>
> And another problem is, how to reverse from something like HTML or XML 
> back to .Rd ? I think that a generic format is absolutely necessary. 
> The generic format we have and not yet another source of confusion.
>
> If wiki means editing .Rd sources or better: inverse transformation in 
> both sides (without to many loss), it would be nice. Is there really a 
> chance?
>
Shurely their is. Define an XML schema. Write functions Rd2XML, XML2Rd,  
twiki2XML,  .....
 I am looking since ages for something like R2XML and XML2Rd.
If we can agree on an XML format for the documentation it will be 
something to start with.

Yours



> Thomas P.
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
>


-- 
Dipl. bio-chem. Witold Eryk Wolski         
MPI-Moleculare Genetic
Ihnestrasse 63-73 14195 Berlin                 _
tel: 0049-30-83875219                 __("<   'v'
http://www.molgen.mpg.de/~wolski      \__/   /   \
mail: witek96 at users.sourceforge.net    ^^     w w
      wolski at molgen.mpg.de



From kwright at eskimo.com  Thu Oct  7 22:00:09 2004
From: kwright at eskimo.com (kwright@eskimo.com)
Date: Thu, 7 Oct 2004 13:00:09 -0700 (PDT)
Subject: [R] How to use alpha transparency channel for colors?
In-Reply-To: <Pine.LNX.4.44.0410071658230.25108-100000@gannet.stats>
References: <58413.170.54.59.167.1097161276.squirrel@170.54.59.167>
	<Pine.LNX.4.44.0410071658230.25108-100000@gannet.stats>
Message-ID: <20654.170.54.59.167.1097179209.squirrel@170.54.59.167>

Brian Ripley wrote:

> As so often happens, the `something wrong' is not reading the help file.
> As the NEWS file _also_ says
>
>     o	A 'version' argument has been added to pdf() device.  If this is
> 	set to "1.4", the device will support transparent colours.
>
> pdf("alpha.pdf", version="1.4")  should work for you: it does for me.

Thanks.  I did actually spend quite a bit of time searching around before
posting to R-help (always do as a matter of courtesy) but was unlucky in
that I grep'ed for 'alpha' and not 'transparent'.  Obvious mistake in
retrospect.

Kevin Wright



From tlumley at u.washington.edu  Thu Oct  7 22:06:39 2004
From: tlumley at u.washington.edu (Thomas Lumley)
Date: Thu, 7 Oct 2004 13:06:39 -0700 (PDT)
Subject: [R] 'with' usage question
In-Reply-To: <20041007195836.40cd474b@portia.local>
References: <20041007174341.243c2c1e@portia.local> <41657A0E.6090204@jhsph.edu>
	<20041007195836.40cd474b@portia.local>
Message-ID: <Pine.A41.4.61.0410071250280.25238@homer03.u.washington.edu>

On Thu, 7 Oct 2004, RenE J.V. Bertin wrote:

> On Thu, 07 Oct 2004 13:17:02 -0400, "Roger D. Peng" <rpeng at jhsph.edu> wrote regarding "Re: [R]
> 'with' usage question"
>
> 8-) I think what's happening is that the function aov.SS1 will look up
> 8-) variables in the environment in which it was *defined*, which in your
> 8-) case (I'm guessing) is the global environment/workspace.  Therefore,
> 8-) if `speed' and `Subject' are not defined in the global environment,
> 8-) they will not be found.
>
>
> If you take that to the letter, doesn't it mean that attach() and 
> detach() couldn't have the effect they have? Which is, in fact and if 
> memory serves me well, much like the Pascal 'with' operator.

No. attach() attaches the data frame to the search path after the global 
environment. So variables in an attached data frame are always available 
(but may be overridden by variables in the global environment).  attach() 
is superficially like with(), but not exactly the same. If you define

    with_attach <-function(data, expr){
 			attach(data)
 			on.exit(detach(data))
 			eval(substitute(expr), parent.frame())
 		}

it would behave more as you expected in this case.  Inside a function this 
is not as good as with() because variables in the data frame can be 
overriden by local variables.

>								A 
> (possible) counter argument: aov.SS1() used T and F as the usual 
> abbreviations for TRUE and FALSE: these are defined in the global 
> environment. Now my dataframes also contain a variable T: it took me 
> quite a while to realise that this variable (which I in fact never use 
> directly) would override the T constant/operator and cause hard-to-trace 
> error messages.

and that's why R CMD check tells you not to use T/F for TRUE/FALSE


> I'm trying to find some analogy. with() extends the current environment 
> with the variables in the given dataframe, but does not create a C-like 
> scope. Rather, it functions more or less like a Bourne shell script: the 
> new variables are not 'exported' to be available to code evoked from 
> within that new environment, unless they were specifically passed as 
> arguments.

It does create a new scope, as in C. The variables are not available 
inside functions called from that scope, because variables are never 
exported to functions called from a given scope. The same is true in C: 
variables will not be exported to functions *called from* a given block 
unless they are explicitly passed as arguments.

The reason many people expect variables to be available in functions 
called from a given scope ("dynamic scoping") is that variables in the 
global workspace are available in functions called from the global 
workspace. But this is just a coincidence: the variables are available 
because the function is *defined* in the global workspace (or in an 
environment that includes the global workspace).  Where it is called from 
is irrelevant.

It is also initially confusing that having a default argument value is 
different from giving exactly the same expression as the actual argument 
value.  This is also for a good reason: it is important for default 
arguments to be able to depend on other argument values. This means they 
have to be evaluated in an environment where the other formal arguments 
are available -- inside the function

So in your example
   with( data, aov.SS1( y=Obs, indep=speed, fSnr=Subject ) )
works, because actual arguments are evaluated in the calling environment, 
which is inside with(), where speed and Subject are available.
    with( data, aov.SS1( y=Obs))
doesn't work because indep=speed and fSnr=Subject are evaluated inside 
aov.SS1, where speed and Subject aren't available.


>		Which still doesn't explain why ls() sees everything when 
> called within a with() statement...
>

Yes, it does.  If ls() is used inside with() it sees the scope defined by 
with(), but if it is called inside a function called from with() it sees 
the environment inside that function: eg

> with(trees, ls())
[1] "Girth"  "Height" "Volume"
> f<-function() ls()
> with(trees, f())
character(0)



 	-thomas



From biocperi at yahoo.com  Thu Oct  7 22:10:31 2004
From: biocperi at yahoo.com (S Peri)
Date: Thu, 7 Oct 2004 13:10:31 -0700 (PDT)
Subject: [R] DEAL and Graphviz
Message-ID: <20041007201031.37234.qmail@web50004.mail.yahoo.com>


 Hi all, 
 some vague questions about deal package and
 graphs.

 
 1. How is DEAL package showing the output network
 graph? Is that graphviz installed along with R is
displaying the graph or it is some inherent module
 of
 DEAL package draws the graph. 
 
 2. can I take the output from deal in someway and
 redraw the final network graph in graphviz using DOT
 (directed graphs). 
 
 Any suggestions?
 Thank you. 
 
 Peri.
 
 		
 


		
_______________________________

Declare Yourself - Register online to vote today!



From kshe4 at student.monash.edu  Thu Oct  7 22:18:24 2004
From: kshe4 at student.monash.edu (Kunal Shetty)
Date: Thu, 07 Oct 2004 20:18:24 +0000
Subject: [R] Read.Table  Reading a Text file
Message-ID: <220.253.9.89.1097178929@my.monash.edu.au>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20041007/719c88fe/attachment.pl

From rpeng at jhsph.edu  Thu Oct  7 22:35:20 2004
From: rpeng at jhsph.edu (Roger D. Peng)
Date: Thu, 07 Oct 2004 16:35:20 -0400
Subject: [R] How to use alpha transparency channel for colors?
In-Reply-To: <58413.170.54.59.167.1097161276.squirrel@170.54.59.167>
References: <58413.170.54.59.167.1097161276.squirrel@170.54.59.167>
Message-ID: <4165A888.7020201@jhsph.edu>

There happens to be a nice article about this by Paul Murrell in the 
most recent R Newsletter at http://cran.r-project.org/doc/Rnews/.

-roger

kwright at eskimo.com wrote:
> The release notes for R 2.0.0 states:
> 
>   It is now possible to specify colours with a full alpha
>   transparency channel via the new 'alpha' argument to the
>   rgb() and hsv() functions, or as a string of the form "#RRGGBBAA".
> 
>   NOTE: most devices draw nothing if a colour is not opaque,
>   but PDF and Quartz devices will render semitransparent colours.
> 
>   A new argument 'alpha' to the function col2rgb()
>   provides the ability to return the alpha component of
>   colours (as well as the red, green, and blue components).
> 
> I'm using R 2.0.0 on Windows 2000 and wanted to try this feature.  The
> following simple test works fine:
> 
> pdf("c:/alpha.pdf")
> plot(rnorm(1:100),rnorm(1:100),col="#000055ff",pch=16)
> dev.off()
> 
> But as soon as I change alpha value from "ff" to "fe", the points are no
> longer visible for me.  I've tried viewing the pdf with Acrobat Reader
> 5.1.0 and gsview4.5.
> 
> Do I need a more recent pdf viewer?  Is this feature not working on
> Windows?  Am I doing something wrong?  Any tips would be appreciated.
> 
> Thanks,
> 
> Kevin Wright
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>



From dmb at mrc-dunn.cam.ac.uk  Thu Oct  7 23:21:29 2004
From: dmb at mrc-dunn.cam.ac.uk (Dan Bolser)
Date: Thu, 7 Oct 2004 22:21:29 +0100 (BST)
Subject: [R] R-(wiki)-pedia?
In-Reply-To: <loom.20041007T211651-640@post.gmane.org>
Message-ID: <Pine.LNX.4.21.0410072219520.21400-100000@mail.mrc-dunn.cam.ac.uk>

On Thu, 7 Oct 2004, Gabor Grothendieck wrote:

>Tony Plate <tplate <at> blackmesacapital.com> writes:
>
>: 
>: At Thursday 11:29 AM 10/7/2004, Dan Bolser wrote:
>: >[snip]
>: >I just added some pages... I think it would be great if people could get
>: >motivated to contribute to something like this. Its one of those cases of
>: >just getting the ball rolling...
>: >
>: >Do you think you can dump the existing R-docs into this wiki as a
>: >framework to get things going?
>: 
>: If the existing R-docs are "dumped" into a wiki, won't the copy in the Wiki 
>: quickly get out of date?  How does one get around this problem?
>
>The way the PHP manual works is that all comments are placed at the end
>of each manual page, blog-style.

I guess that is the best answer.

>
>______________________________________________
>R-help at stat.math.ethz.ch mailing list
>https://stat.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>



From pwlepp at pmgm2.stanford.edu  Thu Oct  7 23:13:48 2004
From: pwlepp at pmgm2.stanford.edu (Paul Lepp)
Date: Thu, 7 Oct 2004 14:13:48 -0700
Subject: [R] replace a value in a vector
Message-ID: <DEEEIJAKFKHHOHEFAAEIKEIJCKAA.pwlepp@cmgm.stanford.edu>

OK, so this is a really stupid question and should be incredibly simple to
do but I can't figure it out. So maybe someone would be so kind as to tell
me.
I have a vector of zeros and ones.  I want to replace all of the zeros with
"black" and all of the ones with "gray".  That's it.  Any help would be
appreciated.

Thanks,
Paul

 `-:-.   ,-;"`-:-.   ,-;"`-:-.   ,-;"`-:-.   ,-;"`-:-.   ,-;"`-:-.
   `=`,'=/     `=`,'=/     `=`,'=/     `=`,'=/     `=`,'=/     `=`
     >==/        >==/        >==/        >==/        >==/
   ,=,-<=`.    ,=,-<=`.    ,=,-<=`.    ,=,-<=`.    ,=,-<=`.    ,=,
,-'-'   `-=_,-'-'   `-=_,-'-'   `-=_,-'-'   `-=_,-'-'   `-=_,-'-'
Paul Lepp, Ph.D.                       Stanford School of Medicine

VAPAHCS, 154T                   Dept. of Microbiology & Immunology
3801 Miranda Ave                               Stanford University
Palo Alto, CA 94304                                   Stanford, CA
(650) 493-5000 x66762		               fax: (650) 852-3291
http://cmgm.stanford.edu/~pwlepp          pwlepp at cmgm.stanford.edu



From dmb at mrc-dunn.cam.ac.uk  Thu Oct  7 23:29:31 2004
From: dmb at mrc-dunn.cam.ac.uk (Dan Bolser)
Date: Thu, 7 Oct 2004 22:29:31 +0100 (BST)
Subject: [R] R-(wiki)-pedia?
In-Reply-To: <45AAE6FD142DCB43A38C00A11FF5DF3E02226175@uswsmx03.merck.com>
Message-ID: <Pine.LNX.4.21.0410072228180.21400-100000@mail.mrc-dunn.cam.ac.uk>

On Thu, 7 Oct 2004, Wiener, Matthew wrote:

>When you think there should be new links, or can offer an example you think
>would be clearer, it might be worth submitting a documentation modification
>proposal (through the bug tracking link on the R home page, for example).
>That would improve the basic documentation, which is what most people will
>see first.

Yup, 

http://r-bugs.biostat.ku.dk/cgi-bin/R/Documentation?user=guest

It would be good if each page (?function page) had a link to an 'update
request' page. That was the idea of a wiki, to prevent overload of someone
monitering the pages. 

>
>Regards,
>Matt Wiener
>
>-----Original Message-----
>From: r-help-bounces at stat.math.ethz.ch
>[mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Dan Bolser
>Sent: Thursday, October 07, 2004 1:30 PM
>To: Gabor Grothendieck
>Cc: r-help at stat.math.ethz.ch
>Subject: Re: [R] R-(wiki)-pedia?
>
>
>On Wed, 6 Oct 2004, Gabor Grothendieck wrote:
>
>>Dan Bolser <dmb <at> mrc-dunn.cam.ac.uk> writes:
>>
>>: 
>>: Is there an R wiki?
>>: 
>>: Looking at the huge amount of traffic on this list, I think wiki could be
>>: an exelet outlet for all the constructive enthusiasm here.
>>: 
>>: I don't think it would be too hard to port the existing R documentation
>>: (the stuff you get with the ?) onto a wiki system, then users could add
>>: their own examples and comments. 
>>: 
>>: Having distinct web page style organization would be easier to navigate
>>: than the mailing list archives. I like the online version of the R-docs,
>>: but I miss 'user comments' and I miss being able to fix trivial things.
>>: 
>>: The user contributed links could supply the best statistical online
>>: resources to supplement the R-documentation, with links to the relevant
>>: FAQ's and Tutorials supplementing the whole thing.
>>: 
>>: Now I have said it this sounds too good to not exist already... Anyone
>>: dumped the R-documentation onto a wiki system?
>>: 
>>: Given the capability of R to be integrated into web pages, this could be
>>: really really great. 
>>: 
>>: Cheers,
>>: Dan.
>>
>>There is one at
>>
>>http://fawn.unibw-hamburg.de/cgi-bin/Rwiki.pl?RwikiHome
>>
>>Unfortunately, no one seems to use it.
>
>I just added some pages... I think it would be great if people could get
>motivated to contribute to something like this. Its one of those cases of
>just getting the ball rolling...
>
>Do you think you can dump the existing R-docs into this wiki as a
>framework to get things going?
>
>So often I read the docks and think, now if only this linked here, and if
>only this example were more clear etc...
>
>After I solve my problem I want to fix the docs. Wiki would let me do
>that. 
>
>How about you host a 'best diff in 30 days' competition each month, and
>winning contributors could be announced on the list?
>
>Can we integrate R-embedded web pages into the wiki?
>
>
>
>>
>>______________________________________________
>>R-help at stat.math.ethz.ch mailing list
>>https://stat.ethz.ch/mailman/listinfo/r-help
>>PLEASE do read the posting guide!
>http://www.R-project.org/posting-guide.html
>>
>
>______________________________________________
>R-help at stat.math.ethz.ch mailing list
>https://stat.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide!
>http://www.R-project.org/posting-guide.html
>
>______________________________________________
>R-help at stat.math.ethz.ch mailing list
>https://stat.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>



From sundar.dorai-raj at PDF.COM  Thu Oct  7 23:24:13 2004
From: sundar.dorai-raj at PDF.COM (Sundar Dorai-Raj)
Date: Thu, 07 Oct 2004 16:24:13 -0500
Subject: [R] replace a value in a vector
In-Reply-To: <DEEEIJAKFKHHOHEFAAEIKEIJCKAA.pwlepp@cmgm.stanford.edu>
References: <DEEEIJAKFKHHOHEFAAEIKEIJCKAA.pwlepp@cmgm.stanford.edu>
Message-ID: <4165B3FD.8070200@pdf.com>

ifelse(x == 0, "black", "gray")

--sundar

Paul Lepp wrote:

> OK, so this is a really stupid question and should be incredibly simple to
> do but I can't figure it out. So maybe someone would be so kind as to tell
> me.
> I have a vector of zeros and ones.  I want to replace all of the zeros with
> "black" and all of the ones with "gray".  That's it.  Any help would be
> appreciated.
> 
> Thanks,
> Paul
> 
>  `-:-.   ,-;"`-:-.   ,-;"`-:-.   ,-;"`-:-.   ,-;"`-:-.   ,-;"`-:-.
>    `=`,'=/     `=`,'=/     `=`,'=/     `=`,'=/     `=`,'=/     `=`
>      >==/        >==/        >==/        >==/        >==/
>    ,=,-<=`.    ,=,-<=`.    ,=,-<=`.    ,=,-<=`.    ,=,-<=`.    ,=,
> ,-'-'   `-=_,-'-'   `-=_,-'-'   `-=_,-'-'   `-=_,-'-'   `-=_,-'-'
> Paul Lepp, Ph.D.                       Stanford School of Medicine
> 
> VAPAHCS, 154T                   Dept. of Microbiology & Immunology
> 3801 Miranda Ave                               Stanford University
> Palo Alto, CA 94304                                   Stanford, CA
> (650) 493-5000 x66762		               fax: (650) 852-3291
> http://cmgm.stanford.edu/~pwlepp          pwlepp at cmgm.stanford.edu
> 
> 
> 
> 
> ------------------------------------------------------------------------
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html



From ripley at stats.ox.ac.uk  Thu Oct  7 23:26:50 2004
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Thu, 7 Oct 2004 22:26:50 +0100 (BST)
Subject: [R] Re: problems installing package in R 2.0.0
In-Reply-To: <1097173181.416588bde8caf@webmail.mcgill.ca>
Message-ID: <Pine.LNX.4.44.0410072224020.25657-100000@gannet.stats>

On Thu, 7 Oct 2004 r.ghezzo at staff.mcgill.ca wrote:

> Hello, I just installed R 2.0.0 in a Win XP machine. As old programs do
> not wor I tried to re-install them by:

> Another question. I did a full "install packages from CRAN" but then
> comparing the list of packages downloaded and installed with those in
> CRAN/windows/contrib/2.0/ i found packages like moc, multidim, multiv,
> netCDF, serialize, yags, xgobi. Can these packages be downloaded and
> installed or there

serialize is incorporated into base R.  netCDF, yags, xgobi are available
from my site, as the ReadMe on CRAN says.  moc is broken under 2.0.0, and
multidim and multiv I believe are too.  The ReadMe tells you such things 
-- please consult it.

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From tlumley at u.washington.edu  Thu Oct  7 23:52:42 2004
From: tlumley at u.washington.edu (Thomas Lumley)
Date: Thu, 7 Oct 2004 14:52:42 -0700 (PDT)
Subject: [R] replace a value in a vector
In-Reply-To: <DEEEIJAKFKHHOHEFAAEIKEIJCKAA.pwlepp@cmgm.stanford.edu>
References: <DEEEIJAKFKHHOHEFAAEIKEIJCKAA.pwlepp@cmgm.stanford.edu>
Message-ID: <Pine.A41.4.61.0410071451300.25238@homer03.u.washington.edu>

On Thu, 7 Oct 2004, Paul Lepp wrote:

> OK, so this is a really stupid question and should be incredibly simple to
> do but I can't figure it out. So maybe someone would be so kind as to tell
> me.
> I have a vector of zeros and ones.  I want to replace all of the zeros with
> "black" and all of the ones with "gray".  That's it.  Any help would be
> appreciated.
>

You can do this with

yournewvector <-   ifelse(yourvector == 1, "black", "gray")

but you may actually want to create a factor

yournewvector <- factor(youroldvector,  labels=c("gray", "black"))


 	-thomas



From p.dalgaard at biostat.ku.dk  Thu Oct  7 23:53:51 2004
From: p.dalgaard at biostat.ku.dk (Peter Dalgaard)
Date: 07 Oct 2004 23:53:51 +0200
Subject: [R] replace a value in a vector
In-Reply-To: <DEEEIJAKFKHHOHEFAAEIKEIJCKAA.pwlepp@cmgm.stanford.edu>
References: <DEEEIJAKFKHHOHEFAAEIKEIJCKAA.pwlepp@cmgm.stanford.edu>
Message-ID: <x2vfdmgpow.fsf@biostat.ku.dk>

"Paul Lepp" <pwlepp at pmgm2.stanford.edu> writes:

> OK, so this is a really stupid question and should be incredibly simple to
> do but I can't figure it out. So maybe someone would be so kind as to tell
> me.
> I have a vector of zeros and ones.  I want to replace all of the zeros with
> "black" and all of the ones with "gray".  That's it.  Any help would be
> appreciated.

v2 <- ifelse(v==0,"black","gray")

or

v2 <- c("black","gray")[v+1]

-- 
   O__  ---- Peter Dalgaard             Blegdamsvej 3  
  c/ /'_ --- Dept. of Biostatistics     2200 Cph. N   
 (*) \(*) -- University of Copenhagen   Denmark      Ph: (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)             FAX: (+45) 35327907



From Kevin.Wang at maths.anu.edu.au  Thu Oct  7 23:58:54 2004
From: Kevin.Wang at maths.anu.edu.au (Kevin Wang)
Date: Fri, 8 Oct 2004 07:58:54 +1000 (EST)
Subject: [R] Re:How to create a R -application
In-Reply-To: <220.253.9.89.1097161619.74034@my.monash.edu.au>
References: <220.253.9.89.1097161619.74034@my.monash.edu.au>
Message-ID: <Pine.GSO.4.58.0410080756030.19966@yin>

Hi,

On Thu, 7 Oct 2004, Kunal Shetty wrote:

> Example for calculating the mean of  a data set
>
> x <- c(8,11,16,18,6,4,20,25,9,13)
> u <- mean(x)
> u

R is an interpreted language, not a compiled one like C++ or Java.
Therefore you don't compile it.  And for such a simple example, there
isn't much need in doing so.

However, compilation may come in handy when you are running simulations
and/or with some long loops, as interpreted languages aren't really good
at handling loops -- although you can get around with many loops by
vectorisation.  I think Luke Tierney is working on an R Compiler, which
will allow you to compile such simulation and will decrease the running
time a lot...but I'm not sure when it will be released.

Cheers,

Kevin

--------------------------------
Ko-Kang Kevin Wang
PhD Student
Centre for Mathematics and its Applications
Building 27, Room 1004
Mathematical Sciences Institute (MSI)
Australian National University
Canberra, ACT 0200
Australia

Homepage: http://wwwmaths.anu.edu.au/~wangk/
Ph (W): +61-2-6125-2431
Ph (H): +61-2-6125-7407
Ph (M): +61-40-451-8301



From h.wickham at gmail.com  Fri Oct  8 02:30:49 2004
From: h.wickham at gmail.com (hadley wickham)
Date: Thu, 7 Oct 2004 19:30:49 -0500
Subject: [R] Subset doesn't drop unused factor levels
In-Reply-To: <41658D35.5030602@vanderbilt.edu>
References: <f8e6ff0504100710271cfca472@mail.gmail.com>
	<41658D35.5030602@vanderbilt.edu>
Message-ID: <f8e6ff0504100717307ba9d7cc@mail.gmail.com>

Ok.  That makes sense.  Thanks to you all for your help.
Hadley



From bfbraum at fas.harvard.edu  Fri Oct  8 02:42:51 2004
From: bfbraum at fas.harvard.edu (Bear F. Braumoeller)
Date: Thu, 7 Oct 2004 20:42:51 -0400
Subject: [R] [R-pkgs] updated package boolean 1.5
Message-ID: <FBBB9805-18C2-11D9-A4F9-000A95A672D6@fas.harvard.edu>


I have just uploaded an updated version of my boolean package (v1.05), 
which should be propagating through CRAN's mirrors soon.  Boolean 
permits estimation of Boolean logit and probit models (see Braumoeller 
(2003), full reference in help file, for derivation).  Boolean logit 
and probit are a family of partial-observability n-variate models 
designed to permit researchers to model causal complexity, or multiple 
causal "paths" to a given outcome.  The various "paths" are modeled as 
latent dependent variables that are multiplied together in a manner 
determined by the logic of their (Boolean) interaction.  Any 
combination of ANDs and ORs can be posited, and the interaction of any 
number of latent dependent variables can be modeled, although the 
procedure becomes exponentially more data-intensive as the number of 
latent dependent variables increases.

The update to version 1.04 was not announced; only minor modifications 
were made to ensure compatibility with R 1.9.

The update to version 1.05 permits the use of Sekhon and Mebane's 
GENOUD optimizer for likelihood surfaces that are convoluted and 
therefore contain multiple local maxima.  Because this procedure can 
produce such surfaces quite easily, the use of GENOUD is recommended.


Bear F. Braumoeller
Associate Professor
Department of Government
Harvard University
http://www.people.fas.harvard.edu/~bfbraum

_______________________________________________
R-packages mailing list
R-packages at stat.math.ethz.ch
https://stat.ethz.ch/mailman/listinfo/r-packages



From kshe4 at student.monash.edu  Fri Oct  8 03:44:20 2004
From: kshe4 at student.monash.edu (Kunal Shetty)
Date: Fri, 08 Oct 2004 01:44:20 +0000
Subject: [R] Read.Table  Reading a Text file
References: <OF6137526B.36B72A2F-ON85256F26.0073D7A0@nd.convergys.com>
Message-ID: <220.253.9.89.1097198925.81334@my.monash.edu.au>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20041008/a8cc0e49/attachment.pl

From kshe4 at student.monash.edu  Fri Oct  8 05:15:53 2004
From: kshe4 at student.monash.edu (Kunal Shetty)
Date: Fri, 08 Oct 2004 03:15:53 +0000
Subject: [R] Read.Table  Reading a Text file
References: <E7D5AB4811D20B489622AABA9C53859101F113F5@teal-exch.amgen.com>
Message-ID: <220.253.9.89.1097205260.74987@my.monash.edu.au>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20041008/ab877c32/attachment.pl

From petr.pikal at precheza.cz  Fri Oct  8 09:04:46 2004
From: petr.pikal at precheza.cz (Petr Pikal)
Date: Fri, 08 Oct 2004 09:04:46 +0200
Subject: [R] Re:How to create a R -application
In-Reply-To: <1097167770.3081.72.camel@ramasamy.stats>
References: <220.253.9.89.1097161619.74034@my.monash.edu.au>
Message-ID: <4166582E.27329.423353@localhost>



On 7 Oct 2004 at 17:49, Adaikalavan Ramasamy wrote:

> This is not answering your question directly.
> 
> I usually use the BATCH command for running R non-interactively. You
> can also use commandArgs() to get any arguments from the command line.
> For more information, see help(BATCH) or help(commandArgs).
> 

Or you can make a function from your commands and use it to 
process your data. See ?function and "Writing your own functions" 
in intro docs.

Or you can copy/paste to some suitable text editor, save it as plain 
text and copy/paste it back to R later (with any modifications).

Cheers
Petr


> 
> On Thu, 2004-10-07 at 16:14, Kunal Shetty wrote:
> > Dear R- users and Helpers
> > 
> >        I am a beginner for R. I am using R to implement EM algorithm
> >        for treating Missing values. I would like to know how can
> >        save or compile my logical R commands to an application; so
> >        that the next time I could just execute the R- file.
> > 
> > Example for calculating the mean of  a data set
> > 
> > x <- c(8,11,16,18,6,4,20,25,9,13)
> > u <- mean(x)
> > u
> > 
> > Now I would like to save these commands as batch or an application
> > 
> > Anybody could please help or direct me in this problem
> > Thank you
> > Regards
> > Kunal
> > 
> > ______________________________________________
> > R-help at stat.math.ethz.ch mailing list
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide!
> > http://www.R-project.org/posting-guide.html
> >
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide!
> http://www.R-project.org/posting-guide.html

Petr Pikal
petr.pikal at precheza.cz



From M.Mamin at intershop.de  Fri Oct  8 09:38:33 2004
From: M.Mamin at intershop.de (Marc Mamin)
Date: Fri, 8 Oct 2004 09:38:33 +0200
Subject: [R] reading partial file content
Message-ID: <A03188C6623C0D46A703CB5AA59907F201C11C03@JENMAIL01.ad.intershop.net>

hi,


I'd like to extract data from very large files (ca 1-2 Mio lines),
and I already know which lines of these files I need.

Is there a way to do it without filling the memory with the whole file content?

The lines I need are randomly distributed within the files.

Thanks,

Marc






-----Original Message-----
From: r-help-bounces at stat.math.ethz.ch
[mailto:r-help-bounces at stat.math.ethz.ch]On Behalf Of Kunal Shetty
Sent: Friday, October 08, 2004 5:16 AM
To: Austin, Matt
Cc: R-help
Subject: RE: [R] Read.Table Reading a Text file




  thanks austin, it worked..it was exactly what I was looking for

regards
Kunal


"Austin, Matt" <maustin at amgen.com> wrote:
> x.1$V1
> or
> x.1[,1]
> or
> x.1['V1']
> 
> and you shouldn't need to call print.default() directly, just call
> print().
> 
> --Matt
> -----Original Message-----
> From: r-help-bounces at stat.math.ethz.ch
> [mailto:r-help-bounces at stat.math.ethz.ch]On Behalf Of Kunal Shetty
> Sent: Thursday, October 07, 2004 18:44 PM
> To: james.holtman at convergys.com
> Cc: R-help
> Subject: Re: [R] Read.Table Reading a Text file
> 
> 
> 
> James
>            Thank you for response. I am working on treatment for missing
> data for both bivariate and multivariate normal data. Coming back to
> example. My problem was that once we do execute this command 
> x.1 <- read.table('/tempxx.txt', fill=T)
> 
> 
> How can access the particular column say X8 and all it's values so that I
> could assign some other operations on them.
> Because if I say 
> print.default(x.1)
> 
> the result..
> 
>    V1  V2
> 1  X    y
> 2  8    10
> 3 11   1 4
> 4 16   16
> 5 18   15
> 6  6    20
> 7  4     4
> 8 20   18
> 9 25    22
> 
> 
> and I want to access V1  values.....
> 
> 
> thank you
> regards
> Kunal
> 
> 
> james.holtman at convergys.com wrote:
> >
> >
> >
> >
> > If you have an unequal number of columns, then use 'fill=T' onread.table
> >
> > It putsNAs.
> >
> > Is this what youwant?
> >
> > Here is what happens on the input file which is your data and someextra
> >columns:
> >
> > 8           10
> > 11          14
> > 16          16
> > 18           15  12
> > 6           20
> > 4            4  12
> > 20          18
> >
> >
> > > x.1 <- read.table('/tempxx.txt',fill=T)
> > >x.1
> >    X8X10
> > 11 14 NA
> > 16 16 NA
> > 18 15 12
> > 6  20 NA
> > 4   4 12
> > 20 18 NA
> >__________________________________________________________
> > James Holtman        "What is the problem you are trying tosolve?"
> > Executive Technical Consultant  --  Office of Technology,Convergys
> >james.holtman at convergys.com
> > +1 (513)723-2929
> >
> >
> >
> > KunalShetty
> > <kshe4 at student.monash To:r-help at stat.math.ethz.ch
> > .edu>cc:
> > Sent by: Subject: [R] Read.Table Reading a Textfile
> >r-help-bounces at stat.m
> >ath.ethz.ch
> >
> >
> > 10/07/200416:18
> >
> >
> >
> >
> >
> >
> > Dear R users andHelpers
> >
> > I am beginner with using R and interested in carrying out certaintask
> >for
> > my statisticalresearch.
> > I am reading data for a text file, which could contain data infollowing
> >pattern
> >
> > x           y
> > 8           10
> > 11          14
> > 16          16
> > 18          15
> > 6           20
> > 4           4
> > 20          18
> >
> > As per the example I have two columns and 7 rows of data ineach.
> > However is real life data situation I may not know how many columnsare
> > present and how rows are present  and also with the certain datais
> > missing. Yes I am assuming the data is delimited myTab.
> >
> >
> > My question or rather problem is I want read data from each colum saycol
> >x
> > (8,11,16,18??EUR??.20) and store it into a variable so that I couldperform
> >some
> > operations onthem.
> >
> > I have also looked into certain R-help for Read.table and data.framebut
> > still struggling on my requirement. Theyare
> >
> >http://tolstoy.newcastle.edu.au/R/help/04/07/2040.html
> >http://tolstoy.newcastle.edu.au/R/help/04/07/3152.html
> >
> >
> >Regards
> >Kunal
> >
> >______________________________________________
> > R-help at stat.math.ethz.ch mailinglist
> >https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the postingguide!
> >http://www.R-project.org/posting-guide.html
> >
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide!
> http://www.R-project.org/posting-guide.html

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html



From ripley at stats.ox.ac.uk  Fri Oct  8 09:56:52 2004
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Fri, 8 Oct 2004 08:56:52 +0100 (BST)
Subject: [R] reading partial file content
In-Reply-To: <A03188C6623C0D46A703CB5AA59907F201C11C03@JENMAIL01.ad.intershop.net>
Message-ID: <Pine.LNX.4.44.0410080848400.12624-100000@gannet.stats>

Please do not tack your question on to a different one (as the posting 
guide does ask you not to).

On Fri, 8 Oct 2004, Marc Mamin wrote:

> hi,
> 
> 
> I'd like to extract data from very large files (ca 1-2 Mio lines),
> and I already know which lines of these files I need.
> 
> Is there a way to do it without filling the memory with the whole file
> content?

Yes, several, and it has been discussed on this list on Wednesday, in a 
thread entitled

`Performing Analysis on Subset of External data'

Please look in the archives.

> The lines I need are randomly distributed within the files.

[irrelevant other message discarded here.]

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From petr.pikal at precheza.cz  Fri Oct  8 10:23:56 2004
From: petr.pikal at precheza.cz (Petr Pikal)
Date: Fri, 08 Oct 2004 10:23:56 +0200
Subject: [R] Read.Table  Reading a Text file
In-Reply-To: <220.253.9.89.1097178929@my.monash.edu.au>
Message-ID: <41666ABC.13917.8AAC7D@localhost>



On 7 Oct 2004 at 20:18, Kunal Shetty wrote:

> Dear R users and Helpers
> 
> I am beginner with using R and interested in carrying out certain task
> for my statistical research. I am reading data for a text file, which
> could contain data in following pattern
> 
> x	y
> 8	10
> 11	14
> 16	16
> 18	15
> 6	20
> 4	4
> 20	18
> 
> As per the example I have two columns and 7 rows of data in each.
> However is real life data situation I may not know how many columns
> are present and how rows are present  and also with the certain data
> is missing. Yes I am assuming the data is delimited my Tab.
> 
> 
> My question or rather problem is I want read data from each colum say
> col x (8,11,16,18.....20) and store it into a variable so that I could
> perform some operations on them.

Hi

Better to read by appropriate read.whatever() function to data 
frame, see eg. ?read.table 

I hope you have already read some intro documents in which there 
is stated how you could read your data and organize them. I also 
recommend to go through Paul Johnsons Rtips. You can find it by 
e.g. Google.

Cheers
Petr



> 
> I have also looked into certain R-help for Read.table and data.frame
> but still struggling on my requirement. They are
> 
> http://tolstoy.newcastle.edu.au/R/help/04/07/2040.html
> http://tolstoy.newcastle.edu.au/R/help/04/07/3152.html
> 
> 
> Regards
> Kunal
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide!
> http://www.R-project.org/posting-guide.html

Petr Pikal
petr.pikal at precheza.cz



From petr.pikal at precheza.cz  Fri Oct  8 10:38:08 2004
From: petr.pikal at precheza.cz (Petr Pikal)
Date: Fri, 08 Oct 2004 10:38:08 +0200
Subject: [R] Read.Table  Reading a Text file
In-Reply-To: <220.253.9.89.1097205260.74987@my.monash.edu.au>
Message-ID: <41666E10.10123.97AC6E@localhost>

Hi

you really should spend some time to go through introductory 
documentation and do some examples provided.

<snip>

On 8 Oct 2004 at 3:15, Kunal Shetty wrote:

you probably read it by read.table without saying/specifying 
header=TRUE so your X and y is included to your data not as 
names but as values. Therefore V1 and V2 are factors.

Please consult especially R-intro manual in doc directory. It helps 
you to save a lot of frustration and misunderstanding, especially at 
the begging of your work with R. 

And consider help list as a last source of help as what you have 
learned yourself you remember better and you understand better.

Cheers
Petr

 
> > 
> >    V1  V2
> > 1  X    y
> > 2  8    10
> > 3 11   1 4
> > 4 16   16
> > 5 18   15
> > 6  6    20
> > 7  4     4
> > 8 20   18
> > 9 25    22
> > 
> > 
> > and I want to access V1  values.....
> > 
> > 
> > thank you
> > regards
> > Kunal
> > 
> > 
> > james.holtman at convergys.com wrote:
> > >
> > >
> > >
> > >
> > > If you have an unequal number of columns, then use 'fill=T'
> > > onread.table
> > >
> > > It putsNAs.
> > >
> > > Is this what youwant?
> > >
> > > Here is what happens on the input file which is your data and
> > > someextra
> > >columns:
> > >
> > > 8           10
> > > 11          14
> > > 16          16
> > > 18           15  12
> > > 6           20
> > > 4            4  12
> > > 20          18
> > >
> > >
> > > > x.1 <- read.table('/tempxx.txt',fill=T)
> > > >x.1
> > >    X8X10
> > > 11 14 NA
> > > 16 16 NA
> > > 18 15 12
> > > 6  20 NA
> > > 4   4 12
> > > 20 18 NA
> > >__________________________________________________________
> > > James Holtman        "What is the problem you are trying tosolve?"
> > > Executive Technical Consultant  --  Office of Technology,Convergys
> > >james.holtman at convergys.com
> > > +1 (513)723-2929
> > >
> > >
> > >
> > > KunalShetty
> > > <kshe4 at student.monash To:r-help at stat.math.ethz.ch
> > > .edu>cc:
> > > Sent by: Subject: [R] Read.Table Reading a Textfile
> > >r-help-bounces at stat.m
> > >ath.ethz.ch
> > >
> > >
> > > 10/07/200416:18
> > >
> > >
> > >
> > >
> > >
> > >
> > > Dear R users andHelpers
> > >
> > > I am beginner with using R and interested in carrying out
> > > certaintask
> > >for
> > > my statisticalresearch.
> > > I am reading data for a text file, which could contain data
> > > infollowing
> > >pattern
> > >
> > > x           y
> > > 8           10
> > > 11          14
> > > 16          16
> > > 18          15
> > > 6           20
> > > 4           4
> > > 20          18
> > >
> > > As per the example I have two columns and 7 rows of data ineach.
> > > However is real life data situation I may not know how many
> > > columnsare present and how rows are present  and also with the
> > > certain datais missing. Yes I am assuming the data is delimited
> > > myTab.
> > >
> > >
> > > My question or rather problem is I want read data from each colum
> > > saycol
> > >x
> > > (8,11,16,18??EUR??.20) and store it into a variable so that I
> > > couldperform
> > >some
> > > operations onthem.
> > >
> > > I have also looked into certain R-help for Read.table and
> > > data.framebut still struggling on my requirement. Theyare
> > >
> > >http://tolstoy.newcastle.edu.au/R/help/04/07/2040.html
> > >http://tolstoy.newcastle.edu.au/R/help/04/07/3152.html
> > >
> > >
> > >Regards
> > >Kunal
> > >
> > >______________________________________________
> > > R-help at stat.math.ethz.ch mailinglist
> > >https://stat.ethz.ch/mailman/listinfo/r-help
> > > PLEASE do read the postingguide!
> > >http://www.R-project.org/posting-guide.html
> > >
> > 
> > ______________________________________________
> > R-help at stat.math.ethz.ch mailing list
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide!
> > http://www.R-project.org/posting-guide.html
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide!
> http://www.R-project.org/posting-guide.html

Petr Pikal
petr.pikal at precheza.cz



From petr.pikal at precheza.cz  Fri Oct  8 10:58:23 2004
From: petr.pikal at precheza.cz (Petr Pikal)
Date: Fri, 08 Oct 2004 10:58:23 +0200
Subject: [R] library in R2.0.0 - summary
In-Reply-To: <3A822319EB35174CA3714066D590DCD504AF84E4@usrymx25.merck.com>
Message-ID: <416672CF.31170.AA42BB@localhost>

Thanks to Andy Liaw and prof.Ripley for their comments.

I finally managed my function set work without need for proper 
installation. However I must try it sometimes.

I got tools from 
http://www.murdoch-sutherland.com/Rtools/tools.zip

but I did not install Pearl yet. I tried

make pkg-fun

which resulted in sequence of errors which I corrected according 
what I was told by screen and documentation (best thanks to all 
who provided it)

and finally I end with

1. modified DESCRIPTION file
2. functional set of my functions
3. error message perl: not found

So next step will be trying to install perl (if I am allowed by 
suspicious network administrator) and completing the whole 
installation.

Thanks again.

Petr


> > From: Petr Pikal
> > 
> > Hi all
> > 
> > I upgraded to 2.0.0 version and did everything as I used to do
> > before.
> > 
> > I installed windows binary, copy/paste other than bundled 
> > packages. 
> > 
> > I got e.g.
> > > library(chron)
> > Error in library(chron) : 'chron' is not a valid package -- 
> > installed < 
> > 2.0.0?
> > 
> > so I loaded it from CRAN and everything worked OK except my 
> > own personal functions (they are not on CRAN). So I went 
> > through NEWS file which says:
> > 
> > o	Packages must have been re-installed for this version, and
> > 	library() will enforce this.
> > 
> > I have never done it, my simple set of functions was never actually
> > installed. I used copy/paste the directory from old R version to new
> > one and I used library(fun) in personal .Rprofile to load my
> > function set. Doing that again I got:
> > 
> > Error in library(fun) : There is no package called 'fun'
> > 
> > I know I can use source
> > source("D:/programy/R/rw2000/library/fun/R/fun")
> > but I feel library way is better way and I think I could try it.
> > 
> > Before I start to try transferring my function set to a real 
> > package I 
> > would like to ask simple ***question***.
> > 
> > Is the error message result of not installing package fun correctly
> > (at all) or I shall do something more than follow instructions in
> > README.packages and other documentation?
> > --------------------------------------------------------------
> > ---------------
> > 
> > Thank you.
> > 
> > Petr Pikal
> > petr.pikal at precheza.cz
> > 
Petr Pikal
petr.pikal at precheza.cz



From Soren.Hojsgaard at agrsci.dk  Fri Oct  8 11:18:49 2004
From: Soren.Hojsgaard at agrsci.dk (=?iso-8859-1?Q?S=F8ren_H=F8jsgaard?=)
Date: Fri, 8 Oct 2004 11:18:49 +0200
Subject: [R] Bug in nlme under version 2.0.0
Message-ID: <C83C5E3DEEE97E498B74729A33F6EAEC01AD97CA@DJFPOST01.djf.agrsci.dk>

Dear all,
Under version 2.0.0, I get the error below when calling summary() on a lme-object, whereas it works under version 1.9.1 (well, it did last week, before I upgraded). Any help on this?
Thx in advance
S??ren

> library(nlme)
> mf <- formula(Weight~Cu*(Time+I(Time^2)+I(Time^3)))
> lme1 <- lme(mf, data = dietox, random=~1|Pig)
> summary(lme1)
Linear mixed-effects model fit by REML
 Data: dietox 
       AIC      BIC    logLik
  4748.664 4815.081 -2360.332

Random effects:
 Formula: ~1 | Pig
        (Intercept) Residual
StdDev:    6.360083 3.116751

Fixed effects: Error in as.vector(x, "list") : cannot coerce to vector



From friedemann at gehrt.info  Fri Oct  8 11:36:59 2004
From: friedemann at gehrt.info (friedemann)
Date: Fri, 8 Oct 2004 11:36:59 +0200
Subject: [R] Evaluating Assignment-Operator R 2.0.0
Message-ID: <20041008093554.M61075@gehrt.info>

Dear Helpers,

in which way I have to use the assignment-operator evaluating left to right 
with R 2.0.0?

With R 2.0.0 (w2k-installation) the "superassignment" operator '<<-' seems to 
have a different behaviour compared with R 1.9.1 :

  > x<<-4
  > x
  [1] 4
  > attr(x,'y')<<-5
  Error: Object "x" not found
  >

Using earlier versions of R I never got this error.

In the NEWS file and the docs coming along with R 2.0.0 I did not found 
anything concerning this problem.
AFAIK '=' instead of '<<-' works - but is allowed only for toplevel-
assignments. 

Thanks in advance, Friedemann



From p.dalgaard at biostat.ku.dk  Fri Oct  8 11:59:57 2004
From: p.dalgaard at biostat.ku.dk (Peter Dalgaard)
Date: 08 Oct 2004 11:59:57 +0200
Subject: [R] confidence interval for nls
In-Reply-To: <ck3i6c$fqd$1@sea.gmane.org>
References: <ck3b10$pji$1@sea.gmane.org> <x2hdp6of4u.fsf@biostat.ku.dk>
	<ck3i6c$fqd$1@sea.gmane.org>
Message-ID: <x2zn2xjzs2.fsf@biostat.ku.dk>

Henrik Andersson <h.andersson at nioo.knaw.nl> writes:

> I tried the example and it works fine,
> 
> but why o why, do I not get any gradient from another prediction?


Apparently this depends on whether you supply an RHS which returns the
gradient. The built-in SSxxxx all do that.
 
> ##plot(yran~x)
> mich <- function(x,K,rmax) rmax*x/(x+K)
> mm.nls <- nls(yran~mich(x,K,rmax),start=list(K=5,rmax=3))

So: Here you need to add a gradient computation to mich(). One simple
way (and pretty much what nls does internally) is

mich <- function(x,K,rmax)
           numericDeriv(quote(rmax*x/(x+K)),c("K","rmax"),
                       parent.frame())

(well, figuring out the parent.frame() bit wasn't all *that* trivial!)

Or, nice when dealing with simple functions:

mmDeriv <- deriv(quote(rmax*x/(x+K)),c("K","rmax"))
mich <- function(x,K,rmax) eval.parent(mmDeriv)

-- 
   O__  ---- Peter Dalgaard             Blegdamsvej 3  
  c/ /'_ --- Dept. of Biostatistics     2200 Cph. N   
 (*) \(*) -- University of Copenhagen   Denmark      Ph: (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)             FAX: (+45) 35327907



From ripley at stats.ox.ac.uk  Fri Oct  8 12:06:46 2004
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Fri, 8 Oct 2004 11:06:46 +0100 (BST)
Subject: [R] Evaluating Assignment-Operator R 2.0.0
In-Reply-To: <20041008093554.M61075@gehrt.info>
Message-ID: <Pine.LNX.4.44.0410081056100.10446-100000@gannet.stats>

On Fri, 8 Oct 2004, friedemann wrote:

> Dear Helpers,
> 
> in which way I have to use the assignment-operator evaluating left to right 
> with R 2.0.0?

<- is the operator you should normally use.

> With R 2.0.0 (w2k-installation) the "superassignment" operator '<<-' seems to 
> have a different behaviour compared with R 1.9.1 :
> 
>   > x<<-4
>   > x
>   [1] 4
>   > attr(x,'y')<<-5
>   Error: Object "x" not found
>   >
> 
> Using earlier versions of R I never got this error.
> 
> In the NEWS file and the docs coming along with R 2.0.0 I did not found 
> anything concerning this problem.

Did you not notice NEWS contains

   o   Complex superassignments were wrong when a variable with the same
        name existed locally, and were not documented in R-lang.

> AFAIK '=' instead of '<<-' works - but is allowed only for toplevel-
> assignments. 

= is allowed almost everywhere <- is, and not just at top-level.
I don't know what you are really trying to do, but

x <- 4
testit <- function() attr(x,'y') <<- 5
testit()
x

works as documented, and that is the normal type of usage of
superassignments.  As the NEWS entry says, R-lang (`The R Language 
Definition') gives fuller details (and I guess the help page for <<- 
should point there).

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From p.dalgaard at biostat.ku.dk  Fri Oct  8 12:21:51 2004
From: p.dalgaard at biostat.ku.dk (Peter Dalgaard)
Date: 08 Oct 2004 12:21:51 +0200
Subject: [R] Evaluating Assignment-Operator R 2.0.0
In-Reply-To: <20041008093554.M61075@gehrt.info>
References: <20041008093554.M61075@gehrt.info>
Message-ID: <x2vfdljyrk.fsf@biostat.ku.dk>

"friedemann" <friedemann at gehrt.info> writes:

> With R 2.0.0 (w2k-installation) the "superassignment" operator '<<-' seems to 
> have a different behaviour compared with R 1.9.1 :
> 
>   > x<<-4
>   > x
>   [1] 4
>   > attr(x,'y')<<-5
>   Error: Object "x" not found
>   >
> 
> Using earlier versions of R I never got this error.
> 
> In the NEWS file and the docs coming along with R 2.0.0 I did not found 
> anything concerning this problem.

It's there. Look for "complex superassignment".

> AFAIK '=' instead of '<<-' works - but is allowed only for toplevel-
> assignments. 

Eh? You mean "assignments in the current environment"? 

Why do you want to use <<- at top level (the command-line) anyway? 

-- 
   O__  ---- Peter Dalgaard             Blegdamsvej 3  
  c/ /'_ --- Dept. of Biostatistics     2200 Cph. N   
 (*) \(*) -- University of Copenhagen   Denmark      Ph: (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)             FAX: (+45) 35327907



From B.Rowlingson at lancaster.ac.uk  Fri Oct  8 13:03:04 2004
From: B.Rowlingson at lancaster.ac.uk (Barry Rowlingson)
Date: Fri, 08 Oct 2004 12:03:04 +0100
Subject: [R] R-(wiki)-pedia?
In-Reply-To: <loom.20041007T195617-232@post.gmane.org>
References: <loom.20041007T014852-860@post.gmane.org>	<Pine.LNX.4.21.0410071823110.21400-100000@mail.mrc-dunn.cam.ac.uk>
	<loom.20041007T195617-232@post.gmane.org>
Message-ID: <416673E8.2010101@lancaster.ac.uk>

Here's a function for searching the Rwiki from R:

rwiki.search <- function(string){

   RwikiURL="http://fawn.unibw-hamburg.de/cgi-bin/Rwiki.pl"
   RwikiSearchURL=paste(RwikiURL(),"?search=",string,sep='')
   browseURL(RwikiSearchURL)

   return(invisible(0))
}


Then you can do rwiki.search("gabor") and the results pop up in your 
browser.

Perhaps we need some more documentation-getting functions in R, such as 
this, something to search the mailing list archives, and maybe a 
readPostingGuide() function...

Or is this spoonfeeding too much, and people should be able to go to a 
web page and stick something in the search box?

Baz



From lizg at cs.llu.lv  Fri Oct  8 13:21:04 2004
From: lizg at cs.llu.lv (Ziedonis Grislis)
Date: Fri, 8 Oct 2004 14:21:04 +0300
Subject: [R] submitting to R
Message-ID: <01C4AD42.0C836140.lizg@cs.llu.lv>

Dear Colleagues,
would you like to explain in which way we can get possibility to use some version of R for Window.
Sincerely yours
Ziedonis Greislis

	*******************************************
	      Dr. Ziedonis Grislis
	Head of Laboratory of Quantitative Genetics
	Department of Animal Science
	LatviaUniversity of Agriculture
	2, Liela Str, Jelgava, LV-3001
	LATVIA
	Tel.:  +371-30-05661, +371-30- 05663
	Fax: +371- 30-27238
	E-mail: lizg at cs.llu.lv
	**********************************************



From vito_ricci at yahoo.com  Fri Oct  8 13:29:35 2004
From: vito_ricci at yahoo.com (Vito Ricci)
Date: Fri, 8 Oct 2004 13:29:35 +0200 (CEST)
Subject: [R] Correlation Matrix
Message-ID: <20041008112935.87911.qmail@web41206.mail.yahoo.com>

Hi,

I'm dealing with a datamining analysis: I've a lot of
categories of product sold per week (n. week =26, n.
categories about 50.
my dataframe is like this:

  Settimana ALIMENTI..ALTRI. ALIMENTI.APROTEICI
1          1                3                 19
2          2                2                  0
3          3                1                 22
4          4                2                  6

I computed correlation coefficents among categories
having a correlation matrix (53X53). Now I will
extract from this matrix only significative
correlations, or, in alternative correlations >0.5 and
<-0.5, excluding  the other, and put this coefficients
in a dataframe.

I'm looking for significative correlations among
categories.

Is someone could help me? Many thanks in advance.
Is also someone using R for dataminig analysis like
me?

Vito


=====
Diventare costruttori di soluzioni

"The business of the statistician is to catalyze 
the scientific learning process."  
George E. P. Box


Visitate il portale http://www.modugno.it/
e in particolare la sezione su Palese http://www.modugno.it/archivio/cat_palese.shtml



From fm3a004 at math.uni-hamburg.de  Fri Oct  8 13:33:11 2004
From: fm3a004 at math.uni-hamburg.de (Christian Hennig)
Date: Fri, 8 Oct 2004 13:33:11 +0200 (MEST)
Subject: [R] submitting to R
In-Reply-To: <01C4AD42.0C836140.lizg@cs.llu.lv>
Message-ID: <Pine.GSO.3.95q.1041008133208.173G-100000@sun11.math.uni-hamburg.de>

Read the (Windows-)FAQ.

Next time, please read the FAQ and the posting guide before posting to
r-help...

Christian

On Fri, 8 Oct 2004, Ziedonis Grislis wrote:

> Dear Colleagues,
> would you like to explain in which way we can get possibility to use some version of R for Window.
> Sincerely yours
> Ziedonis Greislis
> 
> 	*******************************************
> 	      Dr. Ziedonis Grislis
> 	Head of Laboratory of Quantitative Genetics
> 	Department of Animal Science
> 	LatviaUniversity of Agriculture
> 	2, Liela Str, Jelgava, LV-3001
> 	LATVIA
> 	Tel.:  +371-30-05661, +371-30- 05663
> 	Fax: +371- 30-27238
> 	E-mail: lizg at cs.llu.lv
> 	**********************************************
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
> 

***********************************************************************
Christian Hennig
Fachbereich Mathematik-SPST/ZMS, Universitaet Hamburg
hennig at math.uni-hamburg.de, http://www.math.uni-hamburg.de/home/hennig/
#######################################################################
ich empfehle www.boag-online.de



From dimitris.rizopoulos at med.kuleuven.ac.be  Fri Oct  8 13:34:42 2004
From: dimitris.rizopoulos at med.kuleuven.ac.be (Dimitris Rizopoulos)
Date: Fri, 8 Oct 2004 13:34:42 +0200
Subject: [R] submitting to R
References: <01C4AD42.0C836140.lizg@cs.llu.lv>
Message-ID: <001801c4ad2a$cde7aac0$b2133a86@www.domain>

try these urls:

http://www.cran.mirrors.pair.com/bin/windows/
http://www.cran.mirrors.pair.com/bin/windows/base/

Best,
Dimitris

----
Dimitris Rizopoulos
Ph.D. Student
Biostatistical Centre
School of Public Health
Catholic University of Leuven

Address: Kapucijnenvoer 35, Leuven, Belgium
Tel: +32/16/396887
Fax: +32/16/337015
Web: http://www.med.kuleuven.ac.be/biostat/
     http://www.student.kuleuven.ac.be/~m0390867/dimitris.htm


----- Original Message ----- 
From: "Ziedonis Grislis" <lizg at cs.llu.lv>
To: "'r-help at lists.r-project.org'" <r-help at stat.math.ethz.ch>
Sent: Friday, October 08, 2004 1:21 PM
Subject: [R] submitting to R


> Dear Colleagues,
> would you like to explain in which way we can get possibility to use 
> some version of R for Window.
> Sincerely yours
> Ziedonis Greislis
>
> *******************************************
>       Dr. Ziedonis Grislis
> Head of Laboratory of Quantitative Genetics
> Department of Animal Science
> LatviaUniversity of Agriculture
> 2, Liela Str, Jelgava, LV-3001
> LATVIA
> Tel.:  +371-30-05661, +371-30- 05663
> Fax: +371- 30-27238
> E-mail: lizg at cs.llu.lv
> **********************************************
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
>



From sundar.dorai-raj at PDF.COM  Fri Oct  8 13:59:49 2004
From: sundar.dorai-raj at PDF.COM (Sundar Dorai-Raj)
Date: Fri, 08 Oct 2004 06:59:49 -0500
Subject: [R] Correlation Matrix
In-Reply-To: <20041008112935.87911.qmail@web41206.mail.yahoo.com>
References: <20041008112935.87911.qmail@web41206.mail.yahoo.com>
Message-ID: <41668135.8050908@pdf.com>



Vito Ricci wrote:

> Hi,
> 
> I'm dealing with a datamining analysis: I've a lot of
> categories of product sold per week (n. week =26, n.
> categories about 50.
> my dataframe is like this:
> 
>   Settimana ALIMENTI..ALTRI. ALIMENTI.APROTEICI
> 1          1                3                 19
> 2          2                2                  0
> 3          3                1                 22
> 4          4                2                  6
> 
> I computed correlation coefficents among categories
> having a correlation matrix (53X53). Now I will
> extract from this matrix only significative
> correlations, or, in alternative correlations >0.5 and
> <-0.5, excluding  the other, and put this coefficients
> in a dataframe.
> 
> I'm looking for significative correlations among
> categories.
> 
> Is someone could help me? Many thanks in advance.
> Is also someone using R for dataminig analysis like
> me?
> 
> Vito
> 

Assuming `x' is your data above, how about the following:

r <- cor(x)
y <- which(lower.tri(r), TRUE)
z <- data.frame(row = rownames(r)[y[, 1]],
                 col = colnames(r)[y[, 2]],
                 cor = r[y])
subset(z, abs(cor) > 0.5)

#                row       col        cor
# 1 ALIMENTI..ALTRI. Settimana -0.6324555

--sundar



From vito_ricci at yahoo.com  Fri Oct  8 14:09:13 2004
From: vito_ricci at yahoo.com (Vito Ricci)
Date: Fri, 8 Oct 2004 14:09:13 +0200 (CEST)
Subject: [R] Correlation Matrix
In-Reply-To: <41668135.8050908@pdf.com>
Message-ID: <20041008120913.77768.qmail@web41212.mail.yahoo.com>

Hi Sundar,
many thanks for your suggestion: it's just I wished!
Best
Vito



 --- Sundar Dorai-Raj <sundar.dorai-raj at PDF.COM> ha
scritto: 
> 
> 
> Vito Ricci wrote:
> 
> > Hi,
> > 
> > I'm dealing with a datamining analysis: I've a lot
> of
> > categories of product sold per week (n. week =26,
> n.
> > categories about 50.
> > my dataframe is like this:
> > 
> >   Settimana ALIMENTI..ALTRI. ALIMENTI.APROTEICI
> > 1          1                3                 19
> > 2          2                2                  0
> > 3          3                1                 22
> > 4          4                2                  6
> > 
> > I computed correlation coefficents among
> categories
> > having a correlation matrix (53X53). Now I will
> > extract from this matrix only significative
> > correlations, or, in alternative correlations >0.5
> and
> > <-0.5, excluding  the other, and put this
> coefficients
> > in a dataframe.
> > 
> > I'm looking for significative correlations among
> > categories.
> > 
> > Is someone could help me? Many thanks in advance.
> > Is also someone using R for dataminig analysis
> like
> > me?
> > 
> > Vito
> > 
> 
> Assuming `x' is your data above, how about the
> following:
> 
> r <- cor(x)
> y <- which(lower.tri(r), TRUE)
> z <- data.frame(row = rownames(r)[y[, 1]],
>                  col = colnames(r)[y[, 2]],
>                  cor = r[y])
> subset(z, abs(cor) > 0.5)
> 
> #                row       col        cor
> # 1 ALIMENTI..ALTRI. Settimana -0.6324555
> 
> --sundar
> 
> 
>  

=====
Diventare costruttori di soluzioni

"The business of the statistician is to catalyze 
the scientific learning process."  
George E. P. Box


Visitate il portale http://www.modugno.it/
e in particolare la sezione su Palese http://www.modugno.it/archivio/cat_palese.shtml



From henna001 at students.uni-mainz.de  Fri Oct  8 14:14:07 2004
From: henna001 at students.uni-mainz.de (henna001@students.uni-mainz.de)
Date: Fri,  8 Oct 2004 14:14:07 +0200
Subject: [R] RWinEdt
Message-ID: <1097237647.4166848fda9ed@mail.students.uni-mainz.de>

Hi,

I have troubles getting RWinEdt with the R2.0.0 Version startet.

Do I have to install a different version of RWinEdt or WinEdt?
I have RWinEdt 1.6.1 and WinEdt 5.3.

Thanks
Anna Hennig



From abunn at whrc.org  Fri Oct  8 14:42:33 2004
From: abunn at whrc.org (Andy Bunn)
Date: Fri, 8 Oct 2004 08:42:33 -0400
Subject: [R] RWinEdt
In-Reply-To: <1097237647.4166848fda9ed@mail.students.uni-mainz.de>
Message-ID: <NEBBIPHDAMMOKDKPOFFIOEEECKAA.abunn@whrc.org>

Anna:

That is the most current version of RWinEdt. Uwe Ligges is working on a
version for 2.0.0 so check back soon for a new release.

http://cran.r-project.org/contrib/extra/winedt/

Also, when inquiring about a specific package it is often helpful to contact
the maintainer directly and not R-help.

HTH,
Andy

> -----Original Message-----
> From: r-help-bounces at stat.math.ethz.ch
> [mailto:r-help-bounces at stat.math.ethz.ch]On Behalf Of
> henna001 at students.uni-mainz.de
> Sent: Friday, October 08, 2004 8:14 AM
> To: R-help at lists.R-project.org
> Subject: [R] RWinEdt
>
>
> Hi,
>
> I have troubles getting RWinEdt with the R2.0.0 Version startet.
>
> Do I have to install a different version of RWinEdt or WinEdt?
> I have RWinEdt 1.6.1 and WinEdt 5.3.
>
> Thanks
> Anna Hennig
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide!
> http://www.R-project.org/posting-guide.html
>



From ligges at statistik.uni-dortmund.de  Fri Oct  8 14:53:44 2004
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Fri, 08 Oct 2004 14:53:44 +0200
Subject: [R] RWinEdt
In-Reply-To: <1097237647.4166848fda9ed@mail.students.uni-mainz.de>
References: <1097237647.4166848fda9ed@mail.students.uni-mainz.de>
Message-ID: <41668DD8.40408@statistik.uni-dortmund.de>

henna001 at students.uni-mainz.de wrote:
> Hi,
> 
> I have troubles getting RWinEdt with the R2.0.0 Version startet.
> 
> Do I have to install a different version of RWinEdt or WinEdt?
> I have RWinEdt 1.6.1 and WinEdt 5.3.

You have either to wait for RWinEdt_1.6-2 (which already has been 
uploaded to CRAN's incoming area and waits for the CRAN maintainers to 
move it) or to follow the alternative way for installation described in 
the ReadMe, or to get it right now (but only temporarily available) from 
the following link:

http://www.statistik.uni-dortmund.de/~ligges/RWinEdt_1.6-2.zip


Maybe of interest to all the other R-WinEdt users (some of them already 
wrote private messages, which is the appropriate way in this case!):
The new version of R-WinEdt is updated for use with R-2.0.0. 
Installation has been simplified, because Omegahat's SWinRegistry 
package is no longer required. The complete code from SWinRegistry has 
been included in the RWinEdt package, therefore many thanks to Duncan 
Temple Lang.

Uwe Ligges


> Thanks
> Anna Hennig
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html



From murdoch at stats.uwo.ca  Fri Oct  8 14:56:16 2004
From: murdoch at stats.uwo.ca (Duncan Murdoch)
Date: Fri, 08 Oct 2004 08:56:16 -0400
Subject: [R] submitting to R
In-Reply-To: <001801c4ad2a$cde7aac0$b2133a86@www.domain>
References: <01C4AD42.0C836140.lizg@cs.llu.lv>
	<001801c4ad2a$cde7aac0$b2133a86@www.domain>
Message-ID: <jf3dm0d3kdt6a8la7bopm6dakc67jg63ug@4ax.com>

On Fri, 8 Oct 2004 13:34:42 +0200, "Dimitris Rizopoulos"
<dimitris.rizopoulos at med.kuleuven.ac.be> wrote :

>try these urls:
>
>http://www.cran.mirrors.pair.com/bin/windows/
>http://www.cran.mirrors.pair.com/bin/windows/base/

The pair.com mirror can certainly handle the traffic, but you might
find one of the European mirrors (see the list at
http://cran.r-project.org/mirrors.html) is faster from Leuven or
Jelgava.

Duncan Murdoch
>
>Best,
>Dimitris
>
>----
>Dimitris Rizopoulos
>Ph.D. Student
>Biostatistical Centre
>School of Public Health
>Catholic University of Leuven
>
>Address: Kapucijnenvoer 35, Leuven, Belgium
>Tel: +32/16/396887
>Fax: +32/16/337015
>Web: http://www.med.kuleuven.ac.be/biostat/
>     http://www.student.kuleuven.ac.be/~m0390867/dimitris.htm
>
>
>----- Original Message ----- 
>From: "Ziedonis Grislis" <lizg at cs.llu.lv>
>To: "'r-help at lists.r-project.org'" <r-help at stat.math.ethz.ch>
>Sent: Friday, October 08, 2004 1:21 PM
>Subject: [R] submitting to R
>
>
>> Dear Colleagues,
>> would you like to explain in which way we can get possibility to use 
>> some version of R for Window.
>> Sincerely yours
>> Ziedonis Greislis
>>
>> *******************************************
>>       Dr. Ziedonis Grislis
>> Head of Laboratory of Quantitative Genetics
>> Department of Animal Science
>> LatviaUniversity of Agriculture
>> 2, Liela Str, Jelgava, LV-3001
>> LATVIA
>> Tel.:  +371-30-05661, +371-30- 05663
>> Fax: +371- 30-27238
>> E-mail: lizg at cs.llu.lv
>> **********************************************
>>
>> ______________________________________________
>> R-help at stat.math.ethz.ch mailing list
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide! 
>> http://www.R-project.org/posting-guide.html
>>
>
>______________________________________________
>R-help at stat.math.ethz.ch mailing list
>https://stat.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html



From vicgrave at hotmail.com  Fri Oct  8 15:08:02 2004
From: vicgrave at hotmail.com (Victor Gravenholt)
Date: Fri, 08 Oct 2004 13:08:02 +0000
Subject: [R] testing for differences in parameter estimates from parametric
	bootstrapping
Message-ID: <BAY18-F33XL7kb6uS0W0001221d@hotmail.com>

Based on a rather complicated model I do parametric bootstrapping to obtain 
bootstrap distributions of the parameters of interest. What is the proper 
way to test whether the parameter estimates are significantly different (the 
differences in parameter estimates are expected from including different 
covariates in the model)?



From bates at wisc.edu  Fri Oct  8 15:11:30 2004
From: bates at wisc.edu (Douglas Bates)
Date: Fri, 08 Oct 2004 08:11:30 -0500
Subject: [R] Bug in nlme under version 2.0.0
In-Reply-To: <C83C5E3DEEE97E498B74729A33F6EAEC01AD97CA@DJFPOST01.djf.agrsci.dk>
References: <C83C5E3DEEE97E498B74729A33F6EAEC01AD97CA@DJFPOST01.djf.agrsci.dk>
Message-ID: <41669202.50003@wisc.edu>

S??ren H??jsgaard wrote:
> Dear all,
> Under version 2.0.0, I get the error below when calling summary() on a lme-object, whereas it works under version 1.9.1 (well, it did last week, before I upgraded). Any help on this?
> Thx in advance
> S??ren
> 
> 
>>library(nlme)
>>mf <- formula(Weight~Cu*(Time+I(Time^2)+I(Time^3)))
>>lme1 <- lme(mf, data = dietox, random=~1|Pig)
>>summary(lme1)
> 
> Linear mixed-effects model fit by REML
>  Data: dietox 
>        AIC      BIC    logLik
>   4748.664 4815.081 -2360.332
> 
> Random effects:
>  Formula: ~1 | Pig
>         (Intercept) Residual
> StdDev:    6.360083 3.116751
> 
> Fixed effects: Error in as.vector(x, "list") : cannot coerce to vector

I can't reproduce this problem on a simple example like

library(nlme)
example(lme)
summary(fm1)

Can you give us more information on your platform, the version of nlme, 
and perhaps make the data set available on the Internet so we can try to 
reproduce the problem?

Just as a guess, do you happen to have the lme4 package loaded before 
loading nlme?  The two packages cannot be loaded simultaneously.  This 
is detected if you load them in the order nlme, lme4 but not if you load 
them in the other order.



From V.Khamenia at biovision-discovery.de  Fri Oct  8 15:33:57 2004
From: V.Khamenia at biovision-discovery.de (Khamenia, Valery)
Date: Fri, 8 Oct 2004 15:33:57 +0200 
Subject: [R] provide extra variables to environment in call
Message-ID: <D15343265276D31197BC00A024A6C110C793D2@EXS_BDC>

Hi all,

Situation: 

 there is a function `f' already defined by someone and 
 provided in package. `f' looks like that:

  f <- function() {
    x+1
  }

 i.e. `f' is not closed i.r.t. term `x'

 now I have my own function `g', where I'd like
 to override variable `x' while calling `f':

  x <- "dummy gloabal value"

  g <- function() {
    x <- 42
    eval(f(), environment()) # how to make loacl `x' visible in `f'?
  }

  g() # => Error in x + 1 : non-numeric argument to binary operator

Here comes the question:

 What is the right way to call `f' in order to override 
 global value of `x' with the local value defined within 
 of function `g' ?

I see that i've missed something in docs for eval/environments and related.

Thank you in advance.
--
Valery.



From andy_liaw at merck.com  Fri Oct  8 15:54:08 2004
From: andy_liaw at merck.com (Liaw, Andy)
Date: Fri, 8 Oct 2004 09:54:08 -0400
Subject: [R] provide extra variables to environment in call
Message-ID: <3A822319EB35174CA3714066D590DCD504AF84F7@usrymx25.merck.com>

Not an direct answer to your question, but why not do something like:

> f <- function() x + 1
> x <- 10
> f()
[1] 11
> g <- function(x, ...) {
+     my.f <- f
+     formals(my.f) <- c(list(x=NULL), formals(f))
+     my.f(x, ...)
+ }
> g(5)
[1] 6

Andy

> From: Khamenia, Valery
> 
> Hi all,
> 
> Situation: 
> 
>  there is a function `f' already defined by someone and 
>  provided in package. `f' looks like that:
> 
>   f <- function() {
>     x+1
>   }
> 
>  i.e. `f' is not closed i.r.t. term `x'
> 
>  now I have my own function `g', where I'd like
>  to override variable `x' while calling `f':
> 
>   x <- "dummy gloabal value"
> 
>   g <- function() {
>     x <- 42
>     eval(f(), environment()) # how to make loacl `x' visible in `f'?
>   }
> 
>   g() # => Error in x + 1 : non-numeric argument to binary operator
> 
> Here comes the question:
> 
>  What is the right way to call `f' in order to override 
>  global value of `x' with the local value defined within 
>  of function `g' ?
> 
> I see that i've missed something in docs for 
> eval/environments and related.
> 
> Thank you in advance.
> --
> Valery.
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
> 
>



From ggrothendieck at myway.com  Fri Oct  8 16:16:07 2004
From: ggrothendieck at myway.com (Gabor Grothendieck)
Date: Fri, 8 Oct 2004 14:16:07 +0000 (UTC)
Subject: [R] provide extra variables to environment in call
References: <D15343265276D31197BC00A024A6C110C793D2@EXS_BDC>
Message-ID: <loom.20041008T161348-942@post.gmane.org>

Khamenia, Valery <V.Khamenia <at> biovision-discovery.de> writes:

: 
: Hi all,
: 
: Situation: 
: 
:  there is a function `f' already defined by someone and 
:  provided in package. `f' looks like that:
: 
:   f <- function() {
:     x+1
:   }
: 
:  i.e. `f' is not closed i.r.t. term `x'
: 
:  now I have my own function `g', where I'd like
:  to override variable `x' while calling `f':
: 
:   x <- "dummy gloabal value"
: 
:   g <- function() {
:     x <- 42
:     eval(f(), environment()) # how to make loacl `x' visible in `f'?
:   }
: 
:   g() # => Error in x + 1 : non-numeric argument to binary operator
: 
: Here comes the question:
: 
:  What is the right way to call `f' in order to override 
:  global value of `x' with the local value defined within 
:  of function `g' ?
: 
: I see that i've missed something in docs for eval/environments and related.

f <- function() x+1
g <- function() {
   x <- 42
   environment(f) <- environment()
   f()
}



From flom at ndri.org  Fri Oct  8 16:18:16 2004
From: flom at ndri.org (Peter Flom)
Date: Fri, 08 Oct 2004 10:18:16 -0400
Subject: [R] polr and optim question
Message-ID: <s1666990.092@MAIL.NDRI.ORG>

Hello again

I am trying to fit an ordinal logistic model using the polr function
from MASS.  When I run

model.loan.ordinal <- polr(loancat~age + sex + racgp + yrseduc +
 needlchg + gallery  + sniffball + smokeball + sniffher +
 smokeher + nicocaine + inject + poly(year.of.int,3)  + druginj +
inj.years)


I get an error 

Error in optim(start, fmin, gmin, method = "BFGS", hessian = Hess, ...)
: 
        non-finite value supplied by optim


I checked in the MASS book, and in John Fox's book An R and S-Plus
Companion to Applied Regression,  I also checked in R-help, where a
similar problem was solved by using as.ordered, but that did not help
here.

Any help appreciated

Thanks



Peter L. Flom, PhD
Assistant Director, Statistics and Data Analysis Core
Center for Drug Use and HIV Research
National Development and Research Institutes
71 W. 23rd St
www.peterflom.com
New York, NY 10010
(212) 845-4485 (voice)
(917) 438-0894 (fax)



From tlumley at u.washington.edu  Fri Oct  8 16:54:21 2004
From: tlumley at u.washington.edu (Thomas Lumley)
Date: Fri, 8 Oct 2004 07:54:21 -0700 (PDT)
Subject: [R] provide extra variables to environment in call
In-Reply-To: <D15343265276D31197BC00A024A6C110C793D2@EXS_BDC>
References: <D15343265276D31197BC00A024A6C110C793D2@EXS_BDC>
Message-ID: <Pine.A41.4.61.0410080750370.278798@homer04.u.washington.edu>

On Fri, 8 Oct 2004, Khamenia, Valery wrote:

> Hi all,
>
> Situation:
>
> there is a function `f' already defined by someone and
> provided in package. `f' looks like that:
>
>  f <- function() {
>    x+1
>  }
>
> i.e. `f' is not closed i.r.t. term `x'
>
> now I have my own function `g', where I'd like
> to override variable `x' while calling `f':
>
>  x <- "dummy gloabal value"
>
>  g <- function() {
>    x <- 42
>    eval(f(), environment()) # how to make loacl `x' visible in `f'?
>  }
>
>  g() # => Error in x + 1 : non-numeric argument to binary operator
>
> Here comes the question:
>
> What is the right way to call `f' in order to override
> global value of `x' with the local value defined within
> of function `g' ?
>

This is messy because f() should not have been defined that way.

You can't do it without modifying (a copy of) f().

If the package has a namespace, so that you can't override the definition 
of f() it may be impossible.

If you can override the definition of f() you can do something like
       environment(f)<-environment()
inside g().  On the other hand, you could then just replace the definition 
of f() with one that takes x as an argument.

 	-thomas



From jeff.hamann at forestinformatics.com  Fri Oct  8 17:14:51 2004
From: jeff.hamann at forestinformatics.com (Jeff D. Hamann)
Date: Fri, 8 Oct 2004 08:14:51 -0700 (PDT)
Subject: [R] constrained opt with lagrange multiplier example?
Message-ID: <60546.128.193.0.6.1097248491.squirrel@www.forestinformatics.com>

I'm curious to find out if there is an example of R code for optimization
of two variable function, with contraints, using lagrange multiplier
(using optim/nlm?). I have a problem that contains one discrete variable,
but need a simple problem/example to start with.

I haven't been able to find any examples and thought I should ask here
before I plunged into writing a few miles of R code.

Thanks,
Jeff.


-- 
Jeff D. Hamann
Forest Informatics, Inc.
PO Box 1421
Corvallis, Oregon 97339-1421
office 541-753-4218
fax 541-752-0288
home 541-754-1428
jeff.hamann at forestinformatics.com
http://www.forestinformatics.com



From jfox at mcmaster.ca  Fri Oct  8 18:11:50 2004
From: jfox at mcmaster.ca (John Fox)
Date: Fri, 8 Oct 2004 12:11:50 -0400
Subject: [R] polr and optim question
In-Reply-To: <s1666990.092@MAIL.NDRI.ORG>
Message-ID: <20041008161144.FWTP6973.tomts5-srv.bellnexxia.net@JohnDesktop8300>

Dear Peter,

It's hard to know from your message what the source of the problem might be,
but a good guess is that data are ill-conditioned in some way. Some things
to check: What's the distribution of the response variable? (Are there many
categories, some with very few observations?) How ill-conditioned is the
model matrix?

I hope this helps,
 John 
 

> -----Original Message-----
> From: r-help-bounces at stat.math.ethz.ch 
> [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Peter Flom
> Sent: Friday, October 08, 2004 9:18 AM
> To: r-help at stat.math.ethz.ch
> Subject: [R] polr and optim question
> 
> Hello again
> 
> I am trying to fit an ordinal logistic model using the polr 
> function from MASS.  When I run
> 
> model.loan.ordinal <- polr(loancat~age + sex + racgp + 
> yrseduc +  needlchg + gallery  + sniffball + smokeball + 
> sniffher +  smokeher + nicocaine + inject + 
> poly(year.of.int,3)  + druginj +
> inj.years)
> 
> 
> I get an error 
> 
> Error in optim(start, fmin, gmin, method = "BFGS", hessian = 
> Hess, ...)
> : 
>         non-finite value supplied by optim
> 
> 
> I checked in the MASS book, and in John Fox's book An R and 
> S-Plus Companion to Applied Regression,  I also checked in 
> R-help, where a similar problem was solved by using 
> as.ordered, but that did not help here.
> 
> Any help appreciated
> 
> Thanks
> 
> 
> 
> Peter L. Flom, PhD
> Assistant Director, Statistics and Data Analysis Core Center 
> for Drug Use and HIV Research National Development and 
> Research Institutes
> 71 W. 23rd St
> www.peterflom.com
> New York, NY 10010
> (212) 845-4485 (voice)
> (917) 438-0894 (fax)
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html



From RichardsTJ2 at UPMC.EDU  Fri Oct  8 18:15:42 2004
From: RichardsTJ2 at UPMC.EDU (Richards, Thomas)
Date: Fri, 8 Oct 2004 12:15:42 -0400
Subject: [R] problem Installing R 2.0.0 on SuSe 9.1
Message-ID: <2554B4CA518D504A81E6908E39478A6A326F5F@1upmc-msx10.isdip.upmc.edu>

Hello:

	I first tried it by using ./configure with the source files, but found that there is a huge problem with the gcc-g77 package, then I tried using the RPM obtained from CRAN, getting the following, the first of which appears to be another manifestation of the g77 problem:

linux:/tmp # rpm -i R-base-2.0.0-1.i586.rpm
warning: R-base-2.0.0-1.i586.rpm: V3 DSA signature: NOKEY, key ID a3278da3
error: Failed dependencies:
        libg2c.so.0 is needed by R-base-2.0.0-1
        libglade-gnome.so.0 is needed by R-base-2.0.0-1
        libglade.so.0 is needed by R-base-2.0.0-1

Do you have a suggestion about what I (a suse beginner) should do in order to get R up and running under suse 9.1?  I have done some looking around and I do not see any package on suse for version 3.3.3 of the gcc-g77 package that corresponds to the gcc-g++ version 3.3.3, and I don't know if I am up to changing the versions of everything, so I hope I don't have to do that.  Thanks in advance.



From ripley at stats.ox.ac.uk  Fri Oct  8 18:19:16 2004
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Fri, 8 Oct 2004 17:19:16 +0100 (BST)
Subject: [R] Java and help.start() search engine
Message-ID: <Pine.LNX.4.44.0410081706200.10077-100000@gannet.stats>

The latest Sun Java release is jre-1_5_0, and unlike the recent versions
of j2re-1_4_2-0[2345], this one does work with Firefox 1.0PR, Mozilla
1.7.3 and Netscape 7.1 (and hopefully other Mozilla-based browsers).

>From http://plugindoc.mozdev.org/faqs/java.html:

  Java Runtime Environment 5.0 has been released, and fixes many problems 
  users are having with Java. Go get it!. 

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From wolfram at fischer-zim.ch  Fri Oct  8 18:31:34 2004
From: wolfram at fischer-zim.ch (Wolfram Fischer)
Date: Fri, 8 Oct 2004 18:31:34 +0200
Subject: [R] Chernoff faces
Message-ID: <20041008163134.GA4575@s1x.local>

> >>>>> "Kenneth" == Kenneth Cabrera <krcabrer at perseus.unalmed.edu.co> writes:
> 
> Kenneth> Hello everybody: Does any one has a function to build Chernoff
> Kenneth> faces?
> 
> Many of us don't think it's worth them.
> But we know that opinions differ and gladly incorporate
> (good quality) submissions of source code.
> 
> R *is* an open source project and to some extent a community effort.
> Please don't hesitate to contribute and enter the hall of fame of R
> contributors :-)
> 
> Martin 

An example code from H.P. Wolf (2003) can be found at:
	http://www.wiwi.uni-bielefeld.de/~wolf/ : S/R - functions : faces

Wolfram



From tofesi at web.de  Fri Oct  8 18:36:20 2004
From: tofesi at web.de (tofesi@web.de)
Date: Fri, 08 Oct 2004 18:36:20 +0200
Subject: [R] Number of characters per line
Message-ID: <704649632@web.de>

Content-Type: text/plain; charset="iso-8859-1"
Content-Transfer-Encoding: 7bit
Received-SPF: none (hypatia: domain of tofesi at web.de does not designate permitted sender hosts)
X-Virus-Scanned: by amavisd-new at stat.math.ethz.ch
X-Spam-Checker-Version: SpamAssassin 2.63 (2004-01-11) on hypatia.math.ethz.ch
X-Spam-Level: *
X-Spam-Status: No, hits=1.9 required=5.0 tests=AWL,NO_REAL_NAME,RCVD_IN_BL_SPAMCOP_NET autolearn=no version=2.63

Hi all,

how can I tell R after how many characters it should do a line break in its output?
For example, instead of this output

[1] 0.692927557 0.016564402 0.053789695 0.457274207 0.320261162 0.343467947
[7] 0.289319881 0.820783841 0.737314682 0.305060765 0.009107208 0.271747209

I'd like to have that output:

[1] 0.692927557 0.016564402 0.053789695 0.457274207
[5] 0.320261162 0.343467947 0.289319881 0.820783841
[9] 0.737314682 0.305060765 0.009107208 0.271747209

Running R 1.9.1 (2004-06-21),
ESS 5.1.21 from within XEmacs 21.4.15 (April 2004),
Suse 9.1

I've tried to find out in FAQ and R-help archive, without success, so thanks a lot in advance for any help.
  Tobi

__________________________________________________________
Mit WEB.DE FreePhone mit hoechster Qualitaet ab 0 Ct./Min.



From bshapiro-lists-R at getdown.org  Fri Oct  8 18:46:19 2004
From: bshapiro-lists-R at getdown.org (Ben Shapiro)
Date: Fri, 8 Oct 2004 11:46:19 -0500 (CDT)
Subject: [R] creating named elements of lists on the fly 
Message-ID: <200410081646.i98GkJTw018205@hecky.it.northwestern.edu>

HI Folks,

I'm trying to create a list with named elements. Only, I don't know the names of the elements a priori (they come from the data being calculated). Currently, my approach is to create an environment, then assign things to the environement, then as.list the environment to get a list. 

Running the code gives, for example:
> e2 <- new.env(FALSE, NULL)
> assign("dude", 123, env=e2)
> assign("chick", 456, env=e2)
> as.list(e2)
Error in as.vector(x, "list") : cannot coerce to vector

Is this the best way to make a list like this? 

Thanks,
Ben



From jfox at mcmaster.ca  Fri Oct  8 18:57:27 2004
From: jfox at mcmaster.ca (John Fox)
Date: Fri, 8 Oct 2004 12:57:27 -0400
Subject: [R] Number of characters per line
In-Reply-To: <704649632@web.de>
Message-ID: <20041008165721.EUPH15612.tomts16-srv.bellnexxia.net@JohnDesktop8300>

Dear Tobi,

You can use options(width=n.of.characters).

I hope this helps,
 John

> -----Original Message-----
> From: r-help-bounces at stat.math.ethz.ch 
> [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of tofesi at web.de
> Sent: Friday, October 08, 2004 11:36 AM
> To: r-help at stat.math.ethz.ch
> Subject: [R] Number of characters per line
> 
> Content-Type: text/plain; charset="iso-8859-1"
> Content-Transfer-Encoding: 7bit
> Received-SPF: none (hypatia: domain of tofesi at web.de does not 
> designate permitted sender hosts)
> X-Virus-Scanned: by amavisd-new at stat.math.ethz.ch
> X-Spam-Checker-Version: SpamAssassin 2.63 (2004-01-11) on 
> hypatia.math.ethz.ch
> X-Spam-Level: *
> X-Spam-Status: No, hits=1.9 required=5.0 
> tests=AWL,NO_REAL_NAME,RCVD_IN_BL_SPAMCOP_NET autolearn=no 
> version=2.63
> 
> Hi all,
> 
> how can I tell R after how many characters it should do a 
> line break in its output?
> For example, instead of this output
> 
> [1] 0.692927557 0.016564402 0.053789695 0.457274207 
> 0.320261162 0.343467947 [7] 0.289319881 0.820783841 
> 0.737314682 0.305060765 0.009107208 0.271747209
> 
> I'd like to have that output:
> 
> [1] 0.692927557 0.016564402 0.053789695 0.457274207 [5] 
> 0.320261162 0.343467947 0.289319881 0.820783841 [9] 
> 0.737314682 0.305060765 0.009107208 0.271747209
> 
> Running R 1.9.1 (2004-06-21),
> ESS 5.1.21 from within XEmacs 21.4.15 (April 2004), Suse 9.1
> 
> I've tried to find out in FAQ and R-help archive, without 
> success, so thanks a lot in advance for any help.
>   Tobi
> 
> __________________________________________________________
> Mit WEB.DE FreePhone mit hoechster Qualitaet ab 0 Ct./Min.
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html



From jfox at mcmaster.ca  Fri Oct  8 19:01:43 2004
From: jfox at mcmaster.ca (John Fox)
Date: Fri, 8 Oct 2004 13:01:43 -0400
Subject: [R] creating named elements of lists on the fly 
In-Reply-To: <200410081646.i98GkJTw018205@hecky.it.northwestern.edu>
Message-ID: <20041008170137.CJHM2048.tomts10-srv.bellnexxia.net@JohnDesktop8300>

Dear Ben,

Is this the kind of thing you had in mind?

> lst <- list()
> element <- "a"
> lst[[element]] <- 1:5
> element <- "b"
> lst[[element]] <- letters[1:5]
> lst
$a
[1] 1 2 3 4 5

$b
[1] "a" "b" "c" "d" "e"


I hope this helps,
 John 

> -----Original Message-----
> From: r-help-bounces at stat.math.ethz.ch 
> [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Ben Shapiro
> Sent: Friday, October 08, 2004 11:46 AM
> To: r-help at stat.math.ethz.ch
> Subject: [R] creating named elements of lists on the fly 
> 
> HI Folks,
> 
> I'm trying to create a list with named elements. Only, I 
> don't know the names of the elements a priori (they come from 
> the data being calculated). Currently, my approach is to 
> create an environment, then assign things to the 
> environement, then as.list the environment to get a list. 
> 
> Running the code gives, for example:
> > e2 <- new.env(FALSE, NULL)
> > assign("dude", 123, env=e2)
> > assign("chick", 456, env=e2)
> > as.list(e2)
> Error in as.vector(x, "list") : cannot coerce to vector
> 
> Is this the best way to make a list like this? 
> 
> Thanks,
> Ben



From gavin.simpson at ucl.ac.uk  Fri Oct  8 19:05:04 2004
From: gavin.simpson at ucl.ac.uk (Gavin Simpson)
Date: Fri, 08 Oct 2004 18:05:04 +0100
Subject: [R] Number of characters per line
In-Reply-To: <704649632@web.de>
References: <704649632@web.de>
Message-ID: <4166C8C0.6080204@ucl.ac.uk>

tofesi at web.de wrote:
> Content-Type: text/plain; charset="iso-8859-1"
> Content-Transfer-Encoding: 7bit
> Received-SPF: none (hypatia: domain of tofesi at web.de does not designate permitted sender hosts)
> X-Virus-Scanned: by amavisd-new at stat.math.ethz.ch
> X-Spam-Checker-Version: SpamAssassin 2.63 (2004-01-11) on hypatia.math.ethz.ch
> X-Spam-Level: *
> X-Spam-Status: No, hits=1.9 required=5.0 tests=AWL,NO_REAL_NAME,RCVD_IN_BL_SPAMCOP_NET autolearn=no version=2.63
> 
> Hi all,
> 
> how can I tell R after how many characters it should do a line break in its output?
> For example, instead of this output
> 
> [1] 0.692927557 0.016564402 0.053789695 0.457274207 0.320261162 0.343467947
> [7] 0.289319881 0.820783841 0.737314682 0.305060765 0.009107208 0.271747209
> 
> I'd like to have that output:
> 
> [1] 0.692927557 0.016564402 0.053789695 0.457274207
> [5] 0.320261162 0.343467947 0.289319881 0.820783841
> [9] 0.737314682 0.305060765 0.009107208 0.271747209
> 

?options

which states...

      'width': controls the number of characters on a line. You may want
           to change this if you re-size the window that R is running
           in.  Valid values are 10...10000 with default normally 80.
           (The valid values are in file 'Print.h' and can be changed by
           re-compiling R.)

Gav
-- 
%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%
Gavin Simpson                     [T] +44 (0)20 7679 5522
ENSIS Research Fellow             [F] +44 (0)20 7679 7565
ENSIS Ltd. & ECRC                 [E] gavin.simpson at ucl.ac.uk
UCL Department of Geography       [W] http://www.ucl.ac.uk/~ucfagls/cv/
26 Bedford Way                    [W] http://www.ucl.ac.uk/~ucfagls/
London.  WC1H 0AP.
%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%



From sundar.dorai-raj at PDF.COM  Fri Oct  8 19:09:24 2004
From: sundar.dorai-raj at PDF.COM (Sundar Dorai-Raj)
Date: Fri, 08 Oct 2004 12:09:24 -0500
Subject: [R] creating named elements of lists on the fly
In-Reply-To: <200410081646.i98GkJTw018205@hecky.it.northwestern.edu>
References: <200410081646.i98GkJTw018205@hecky.it.northwestern.edu>
Message-ID: <4166C9C4.6050503@pdf.com>



Ben Shapiro wrote:

> HI Folks,
> 
> I'm trying to create a list with named elements. Only, I don't know the names of the elements a priori (they come from the data being calculated). Currently, my approach is to create an environment, then assign things to the environement, then as.list the environment to get a list. 
> 
> Running the code gives, for example:
> 
>>e2 <- new.env(FALSE, NULL)
>>assign("dude", 123, env=e2)
>>assign("chick", 456, env=e2)
>>as.list(e2)
> 
> Error in as.vector(x, "list") : cannot coerce to vector
> 
> Is this the best way to make a list like this? 
> 
> Thanks,
> Ben
> 

You can use the "[[" operator for list objects:

nm <- c("dude", "chick")
e2[[nm[1]]] <- 123
e2[[nm[2]]] <- 456

HTH,

--sundar



From ggrothendieck at myway.com  Fri Oct  8 19:17:52 2004
From: ggrothendieck at myway.com (Gabor Grothendieck)
Date: Fri, 8 Oct 2004 17:17:52 +0000 (UTC)
Subject: [R] creating named elements of lists on the fly
References: <200410081646.i98GkJTw018205@hecky.it.northwestern.edu>
Message-ID: <loom.20041008T191631-109@post.gmane.org>

Ben Shapiro <bshapiro-lists-R <at> getdown.org> writes:

: 
: HI Folks,
: 
: I'm trying to create a list with named elements. Only, I don't know the 
names of the elements a priori (they
: come from the data being calculated). Currently, my approach is to create an 
environment, then assign
: things to the environement, then as.list the environment to get a list. 
: 
: Running the code gives, for example:
: > e2 <- new.env(FALSE, NULL)
: > assign("dude", 123, env=e2)
: > assign("chick", 456, env=e2)
: > as.list(e2)
: Error in as.vector(x, "list") : cannot coerce to vector
: 
: Is this the best way to make a list like this? 
: 
: Thanks,
: Ben


If you need to add them one at a time then John Fox has
already provided an answer.  If you want to create it
all at once and you have the contents in a list and
the names in a vector like this:

nams <- letters[1:3]
contents <- list(1:3, 4:5, 6:9)

# then here are two ways:

# 1
mapply("{", nams, contents, SIMPLIFY = FALSE)

# 2
names(contents) <-  nams
contents

#  Actually in your example the elements are all homogeneous so
#  you could alternately use a vector to hold the results:

# 1a
contents <- c(1, 2, 3)
mapply("{", nams, contents)

# 2a
names(contents) <- nams
contents



From HDoran at air.org  Fri Oct  8 19:27:18 2004
From: HDoran at air.org (Doran, Harold)
Date: Fri, 8 Oct 2004 13:27:18 -0400
Subject: [R] nlme vs gls
Message-ID: <88EAF3512A55DF46B06B1954AEF73F7405E45BFB@dc1ex2.air.org>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20041008/4694634b/attachment.pl

From jawegelin at ucdavis.edu  Fri Oct  8 21:02:43 2004
From: jawegelin at ucdavis.edu (Jacob Wegelin)
Date: Fri, 8 Oct 2004 12:02:43 -0700 (PDT)
Subject: [R] reading Systat into R
In-Reply-To: <Pine.OSX.4.53.0407101928480.2644@biostat5.ucdavis.edu>
References: <Pine.OSX.4.53.0407101928480.2644@biostat5.ucdavis.edu>
Message-ID: <Pine.OSX.4.53.0410081157110.700@biostat5.ucdavis.edu>


How do I read a Systat file into R?  The  following email by Marc Schwartz
http://tolstoy.newcastle.edu.au/R/help/04/06/1005.html deals with reading Systat on
a Linux machine.  I'm running R on Windows (precise version info below).  A colleague
sent me data in systat, a *.SYD file.  Do I have to ask the colleague to re-save the
data as Excel or text?

(I tried opening it in SPSS, and SPSS also does not seem to recognize an SYD file.)

Thanks for any info.

Jake Wegelin

> version
         _
platform i386-pc-mingw32
arch     i386
os       mingw32
system   i386, mingw32
status
major    1
minor    9.1
year     2004
month    06
day      21
language R



From baron at psych.upenn.edu  Fri Oct  8 21:13:00 2004
From: baron at psych.upenn.edu (Jonathan Baron)
Date: Fri, 8 Oct 2004 15:13:00 -0400
Subject: [R] reading Systat into R
In-Reply-To: <Pine.OSX.4.53.0410081157110.700@biostat5.ucdavis.edu>
References: <Pine.OSX.4.53.0407101928480.2644@biostat5.ucdavis.edu>
	<Pine.OSX.4.53.0410081157110.700@biostat5.ucdavis.edu>
Message-ID: <20041008191300.GA24778@psych>

On 10/08/04 12:02, Jacob Wegelin wrote:
>
>How do I read a Systat file into R?  The  following email by Marc Schwartz
>http://tolstoy.newcastle.edu.au/R/help/04/06/1005.html deals with reading Systat on
>a Linux machine.  I'm running R on Windows (precise version info below).  A colleague
>sent me data in systat, a *.SYD file.  Do I have to ask the colleague to re-save the
>data as Excel or text?

Unfortunately, yes, to my knowledge.  Text, of course, could be
SYC, or one of the various outputs that Systat produces.

Because I used to use Systat, and many of my colleagues still do,
I spent some time investigating this problem.  Here are some
excerpts from correspondence.  Because one was just to me and
not the list, I'm removing the From.  I have the code, but I
could not get it to work.  I put it aside.  I think it needs
someone who is more familiar with c than I am (which doesn't take
much).

Jon
---

>>Contact David.Baird at AgResearch.CO.NZ because his dataload program will
>>import from SYSTAT and output to an .rda file.  But I fear that dataload
>>is no longer available to new users, only those grandfathered in before
>>a license issue came up.  -Frank
>
> Thanks.  DataLoad does have a free version, but the whole thing
> is Windows only.  (Just search for dataload on Google.)  If I had
> Windows, I'd have Systat.

No, David gave me the ready-to-run linux executable.  Never used it on 
windows.

--

I've put three header files and two pre-ANSI C files in a tarball. I note
that sys_errlist causes an error under compilation now, but for code last
looked at 12 years ago, that's not bad. The reading code was written by me
alone based on reverse engineering, but when I had a question - as you'll
see from the code - someone I think called P. Fleury from Systat sent me
some C that they'd been trying out, which I saw after writing my own. I
think mine was more thorough. 

The endian-ness was because I needed to read and write on non-Intel
machines (you may need the functions I used then - please ask if so, or
substitute more modern ones if required). Please treat this as GPL - I
think there was not difficulty with Systat at the time, and they knew that
I had this on my ftp server then (a 1992 paper in Computers and
Geosciences).  I still have *.sys files that I could try it on if it works
- please let me know if there are unresolved calls (walert() wrote an
error message, for example).

Best wishes,

Roger [Bivand]

>From r-help-bounces at stat.math.ethz.ch  Wed Jun 16 11:33:01 2004

The commercial package dbmscopy has a Linux version. 
I have used dbmscopy for several years and have been happy 
with it as it converts data files among many spreadsheets and 
statistics programs.

http://www.conceptual.com/dbmscopt.htm

However, somewhat recently they were purchased by SAS, so 
I'm not sure of current state of the program. There are 
probably other commercial packages as well.

Anne

Hi Jon and Anne!

One other commercial product to check out is Stat/Transfer.  More
information on supported formats is at:
http://www.stattransfer.com/html/formats.html

They do support Windows, MacOS and Unix/Linux. Demo downloads are
available from: http://www.stattransfer.com/html/download.html

Unix/Linux pricing is available at:
http://www.stattransfer.com/html/prices_-_unix.html.

HTH,

Marc Schwartz


-- 
Jonathan Baron, Professor of Psychology, University of Pennsylvania
Home page: http://www.sas.upenn.edu/~baron



From drf5n at maplepark.com  Fri Oct  8 21:15:05 2004
From: drf5n at maplepark.com (David Forrest)
Date: Fri, 8 Oct 2004 14:15:05 -0500 (CDT)
Subject: [R] R-(wiki)-pedia?
In-Reply-To: <416673E8.2010101@lancaster.ac.uk>
References: <loom.20041007T014852-860@post.gmane.org>
	<Pine.LNX.4.21.0410071823110.21400-100000@mail.mrc-dunn.cam.ac.uk>
	<loom.20041007T195617-232@post.gmane.org>
	<416673E8.2010101@lancaster.ac.uk>
Message-ID: <Pine.LNX.4.58.0410081347090.27890@maplepark.com>

On Fri, 8 Oct 2004, Barry Rowlingson wrote:

> Here's a function for searching the Rwiki from R:
>
> rwiki.search <- function(string){
>
>    RwikiURL="http://fawn.unibw-hamburg.de/cgi-bin/Rwiki.pl"
>    RwikiSearchURL=paste(RwikiURL(),"?search=",string,sep='')
>    browseURL(RwikiSearchURL)
>
>    return(invisible(0))
> }
>

Hmm, that's nice.

>
> Then you can do rwiki.search("gabor") and the results pop up in your
> browser.
>
> Perhaps we need some more documentation-getting functions in R, such as
> this, something to search the mailing list archives, and maybe a
> readPostingGuide() function...
>
> Or is this spoonfeeding too much, and people should be able to go to a
> web page and stick something in the search box?

Some people need spoonfeeding, and sometimes its nice to be able to do
things more than one way.


rwiki.search('SearchFunctions')

help.search.archive<-function(string){
   RURL="http://www.google.com/u/newcastlemaths"
   RSearchURL=paste(RURL,"?q=",string,sep='')
   browseURL(RSearchURL)
   return(invisible(0))
 }

help.search.archive('wiki')

Dave
-- 
 Dave Forrest
 drf at vims.edu                                    (804)684-7900w
 drf5n at maplepark.com                             (804)642-0662h
                                   http://maplepark.com/~drf5n/



From MSchwartz at MedAnalytics.com  Fri Oct  8 21:19:12 2004
From: MSchwartz at MedAnalytics.com (Marc Schwartz)
Date: Fri, 08 Oct 2004 14:19:12 -0500
Subject: [R] reading Systat into R
In-Reply-To: <Pine.OSX.4.53.0410081157110.700@biostat5.ucdavis.edu>
References: <Pine.OSX.4.53.0407101928480.2644@biostat5.ucdavis.edu>
	<Pine.OSX.4.53.0410081157110.700@biostat5.ucdavis.edu>
Message-ID: <1097263152.17307.118.camel@localhost.localdomain>

On Fri, 2004-10-08 at 14:02, Jacob Wegelin wrote:
> How do I read a Systat file into R?  The  following email by Marc Schwartz
> http://tolstoy.newcastle.edu.au/R/help/04/06/1005.html deals with reading Systat on
> a Linux machine.  I'm running R on Windows (precise version info below).  A colleague
> sent me data in systat, a *.SYD file.  Do I have to ask the colleague to re-save the
> data as Excel or text?
> 
> (I tried opening it in SPSS, and SPSS also does not seem to recognize an SYD file.)
> 
> Thanks for any info.
> 
> Jake Wegelin

Jacob,

Stat/Transfer does support reading SYSTAT files on Windows. The "W"
logos on the formats page
(http://www.stattransfer.com/html/formats.html) indicate that those
other particular formats are available on Windows only. SYSTAT file
support is available on each supported OS.

There are inexpensive academic/student prices available for
Stat/Transfer if this is something that makes sense to invest in. If
this is going to be a one-time issue, I would have your colleague
re-save the data file to an ASCII CSV file, if possible.

HTH,

Marc



From jawegelin at ucdavis.edu  Fri Oct  8 21:28:52 2004
From: jawegelin at ucdavis.edu (Jacob Wegelin)
Date: Fri, 8 Oct 2004 12:28:52 -0700 (PDT)
Subject: [R] reading Systat into R
In-Reply-To: <1097263152.17307.118.camel@localhost.localdomain>
References: <Pine.OSX.4.53.0407101928480.2644@biostat5.ucdavis.edu> 
	<Pine.OSX.4.53.0410081157110.700@biostat5.ucdavis.edu>
	<1097263152.17307.118.camel@localhost.localdomain>
Message-ID: <Pine.OSX.4.53.0410081222270.802@biostat5.ucdavis.edu>


Thanks to Jonathan and Marc for your swift replies.  Here is how I solved
the problem:

I downloaded a 30 day trial version of systat from
http://www.clecom.co.uk/science/systat/systat_download.html, installed it
on my PC, opened the file in Systat, then used File, Save As and saved it
in every imaginable format: Excel, ascii, SPSS, and SAS.

The SPSS *.SAV file, when I opened it, proved not to have any data in it.
It only contained the "variable view" not the "data view."  But the Excel
file appears to contain the data.

Jake Wegelin

On Fri, 8 Oct 2004, Marc Schwartz wrote:

> On Fri, 2004-10-08 at 14:02, Jacob Wegelin wrote:
> > How do I read a Systat file into R?  The  following email by Marc Schwartz
> > http://tolstoy.newcastle.edu.au/R/help/04/06/1005.html deals with reading Systat on
> > a Linux machine.  I'm running R on Windows (precise version info below).  A colleague
> > sent me data in systat, a *.SYD file.  Do I have to ask the colleague to re-save the
> > data as Excel or text?
> >
> > (I tried opening it in SPSS, and SPSS also does not seem to recognize an SYD file.)
> >
> > Thanks for any info.
> >
> > Jake Wegelin
>
> Jacob,
>
> Stat/Transfer does support reading SYSTAT files on Windows. The "W"
> logos on the formats page
> (http://www.stattransfer.com/html/formats.html) indicate that those
> other particular formats are available on Windows only. SYSTAT file
> support is available on each supported OS.
>
> There are inexpensive academic/student prices available for
> Stat/Transfer if this is something that makes sense to invest in. If
> this is going to be a one-time issue, I would have your colleague
> re-save the data file to an ASCII CSV file, if possible.
>
> HTH,
>
> Marc
>
>
>
>



From rbaer at atsu.edu  Fri Oct  8 21:49:41 2004
From: rbaer at atsu.edu (Robert W. Baer, Ph.D.)
Date: Fri, 8 Oct 2004 14:49:41 -0500
Subject: [R] reading Systat into R
References: <Pine.OSX.4.53.0407101928480.2644@biostat5.ucdavis.edu>
	<Pine.OSX.4.53.0410081157110.700@biostat5.ucdavis.edu><1097263152.17307.118.camel@localhost.localdomain>
	<Pine.OSX.4.53.0410081222270.802@biostat5.ucdavis.edu>
Message-ID: <005701c4ad6f$f390ba60$3e80010a@BigBaer>

> The SPSS *.SAV file, when I opened it, proved not to have any data in it.
> It only contained the "variable view" not the "data view."  But the Excel
> file appears to contain the data.
>

My experience with getting spss files suggests that you might actually be
seeing the file as a list.  Look at help for
?read.spss.  The most usable format to get a data frame is:

dframe=read.spss("MyData.sav",to.data.frame=T)

See if this doesn't do what you expect.



From richard_raubertas at merck.com  Fri Oct  8 22:02:34 2004
From: richard_raubertas at merck.com (Raubertas, Richard)
Date: Fri, 8 Oct 2004 16:02:34 -0400
Subject: [R] nlme vs gls
Message-ID: <B88F4BCF37DD0847937C1C98255291FB02A8C69C@uswsmx05.merck.com>

One thing to be aware of (as Pinheiro and Bates point out on
the same page) is that the general random effects and gls
models are not nested.  This means that the general covariance
matrix you estimate with gls may not correspond to *any* 
random effects model.  In that case there are no subject-
specific coefficients (e.g. slopes), in the random effects sense.

Rich Raubertas

> -----Original Message-----
> From: r-help-bounces at stat.math.ethz.ch 
> [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Doran, Harold
> Sent: Friday, October 08, 2004 1:27 PM
> To: r-help at stat.math.ethz.ch
> Subject: [R] nlme vs gls
> 
> 
> Dear List:
> 
> My question is more statistical than R oriented (although it 
> originates
> from my work with nlme). I know statistical questions are occasionally
> posted, so I hope my question is relevant to the list as I cannot turn
> up a solution anywhere else. I will frame it in the context of an R
> related issue.
> 
> To illustrate the problem, consider student achievement test 
> score data
> with multiple observations available for each student. One way of
> modeling these data might be
> 
> Y_{ti} = (\mu + \mu_{i} ) + (\beta_0 + \beta_{i} )*(time) +
> \epsilon_{ti} ; t indexes time and i indexes student
> 
> The nlme code is 
> 
> tt<-lme(reponse~time, data, random=~time|ID)
> 
> With this, I can extract the growth rate for each individual 
> in the data
> set. Conceptually this is the sum of the main effect for time plus the
> empirical bayes estimate for each individual:
> 
> \beta_0 + \beta_{i}
> 
> I can use the coef(tt, ...) to extract these coefficients. 
> 
> Now, assume that I do not want to include random effects 
> associated with
> the slope and intercept, but instead use a gls to account for the
> variances and covariances through an unstructured covariance matrix.
> 
> For example, assume the following model fit to the same data
> 
> Y_{ti} = \mu  + \beta_0 * (time) + \epsilon_{ti}; where 
> e~N(0, \Sigma) 
> 
> With Sigma forming a more complex covariance matrix. We can 
> use the gls
> option as follows for example, 
> 
> tt1<-gls(response~time, data, correlation=corSymm(form=~1|ID),
> weights=varIdent(form=~1|time))
> 
> On p. 254 of P&B, they note that the mixed model "gives as a 
> by-product,
> estimates for the random effects, which may be of interest in
> themselves". And in my situation they are. Specifically, I want to
> estimate the growth rate for each individual student.
> 
> My questions boils down to:
> 
> 1) Is there any way possible to extract or to compute (estimate) the
> growth rate of individual i when the data have been modeled 
> using gls? 
> 
> 2) Can anyone point me to an example or reference where this has been
> done? I have searched but have really turned up empty handed.
> 
> It seems that there must be a methodology for doing so as we are
> accomplishing a similar task. Would there be information in the new
> covariance matrix, Sigma, that would help play this role?
> 
> These only illustrate the issue, the actual model I am dealing with is
> more complex, but the issue generalizes. Fitting random effects in the
> current model I am dealing with is not a particularly attractive
> solution.  I actually have the issue layed out in more detail 
> in a paper
> I am working on and would be happy to share if requested.
> 
> I would appreciate any thoughts you might have on this problem.
> 
> Harold
> 
> 
> 
>  
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
> 
>



From brian_cade at usgs.gov  Fri Oct  8 22:18:32 2004
From: brian_cade at usgs.gov (Brian S Cade)
Date: Fri, 8 Oct 2004 14:18:32 -0600
Subject: [R] reading Systat into R
Message-ID: <OFB15A89A9.6C995260-ON87256F27.006F7172@cr.usgs.gov>


Jacob:  One solution I use is to import Systat into S-Plus (has very good
data import capability), save the S-Plus data frame, and then read it into
R with read.S().

Brian

Brian S. Cade

U. S. Geological Survey
Fort Collins Science Center
2150 Centre Ave., Bldg. C
Fort Collins, CO  80526-8818

email:  brian_cade at usgs.gov
tel:  970 226-9326



From JAROSLAW.W.TUSZYNSKI at saic.com  Fri Oct  8 22:30:32 2004
From: JAROSLAW.W.TUSZYNSKI at saic.com (Tuszynski, Jaroslaw W.)
Date: Fri, 8 Oct 2004 16:30:32 -0400 
Subject: [R] Survey of "moving window" statistical functions - still looking
	f or fast mad function
Message-ID: <9CC1B717EF3BD511AD98000103D63FC53FA6C8@us-arl-asg.mail.saic.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20041008/29aedf57/attachment.pl

From tortosa at uji.es  Sat Oct  9 02:50:52 2004
From: tortosa at uji.es (Emili Tortosa-Ausina)
Date: Fri, 08 Oct 2004 17:50:52 -0700
Subject: [R] RWinEdt
In-Reply-To: <1097237647.4166848fda9ed@mail.students.uni-mainz.de>
References: <1097237647.4166848fda9ed@mail.students.uni-mainz.de>
Message-ID: <6.0.0.22.2.20041008175008.01ca4660@mail.uji.es>

Hi,

All do also have troubles. In my case, I get the following message:

 > install.packages(choose.files('',filters=Filters[c('zip','All'),]), 
.libPaths()[1], CRAN = NULL)
package 'RWinEdt' successfully unpacked and MD5 sums checked
updating HTML package descriptions
 > local({pkg <- select.list(sort(.packages(all.available = TRUE)))
+ if(nchar(pkg)) library(pkg, character.only=TRUE)})
Error in firstlib(which.lib.loc, package) :
         couldn't find function "lazyLoad"
In addition: Warning message:
package RWinEdt was built under R version 2.0.0
Error in library(pkg, character.only = TRUE) :
         .First.lib failed
 >

Any help would be appreciated!!!

Emili

At 05:14 AM 10/8/2004, henna001 at students.uni-mainz.de wrote:
>Hi,
>
>I have troubles getting RWinEdt with the R2.0.0 Version startet.
>
>Do I have to install a different version of RWinEdt or WinEdt?
>I have RWinEdt 1.6.1 and WinEdt 5.3.
>
>Thanks
>Anna Hennig
>
>______________________________________________
>R-help at stat.math.ethz.ch mailing list
>https://stat.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html



From nleonard at tartarus.uwa.edu.au  Sat Oct  9 03:03:23 2004
From: nleonard at tartarus.uwa.edu.au (Neil Leonard)
Date: Sat, 9 Oct 2004 09:03:23 +0800
Subject: [R] Adding factor variable to a CoxPH
Message-ID: <04E35D6C-198F-11D9-8B4B-003065D5B8EC@tartarus.uwa.edu.au>

I think this is a pretty basic statistic question:

If I have a variable which has 4 factors how can I add it to a coxph 
model? I have other variables added which have digits 1-4 instead of 
four factors. If I recode the variable will that work?


Thanks
Neil



From flom at ndri.org  Sat Oct  9 04:11:09 2004
From: flom at ndri.org (Peter Flom)
Date: Fri, 08 Oct 2004 22:11:09 -0400
Subject: [R] polr problem solved
Message-ID: <s1671089.069@MAIL.NDRI.ORG>

I'd like to thank John Fox and Chuck Cleland for their help in resovling
this issue.  It turned out to be something simple, but perhaps others
have had similar problems

In my original data frame, I had 4 categories of race/ethnicity.  One of
the categories (other) was very small, and not similar to any of the
other three categories, so I created a new data frame deleting those
people.

However, the level "other" was still there, with no one in it.
This didn't cause a problem for glm or lm, but it did for polr.  When I
eliminated that level, the problem disappeared.

Thanks again for the help

Peter

Peter L. Flom, PhD
Assistant Director, Statistics and Data Analysis Core
Center for Drug Use and HIV Research
National Development and Research Institutes
71 W. 23rd St
www.peterflom.com
New York, NY 10010
(212) 845-4485 (voice)
(917) 438-0894 (fax)



From tortosa at uji.es  Sat Oct  9 04:37:02 2004
From: tortosa at uji.es (Emili Tortosa-Ausina)
Date: Fri, 08 Oct 2004 19:37:02 -0700
Subject: [R] RWinEdt
In-Reply-To: <6.0.0.22.2.20041008175008.01ca4660@mail.uji.es>
References: <1097237647.4166848fda9ed@mail.students.uni-mainz.de>
	<6.0.0.22.2.20041008175008.01ca4660@mail.uji.es>
Message-ID: <6.0.0.22.2.20041008193649.01bce530@mail.uji.es>

OK, now it's fine.

Thanks,

Emili

At 05:50 PM 10/8/2004, Emili Tortosa-Ausina wrote:
>Hi,
>
>All do also have troubles. In my case, I get the following message:
>
> > install.packages(choose.files('',filters=Filters[c('zip','All'),]), 
> .libPaths()[1], CRAN = NULL)
>package 'RWinEdt' successfully unpacked and MD5 sums checked
>updating HTML package descriptions
> > local({pkg <- select.list(sort(.packages(all.available = TRUE)))
>+ if(nchar(pkg)) library(pkg, character.only=TRUE)})
>Error in firstlib(which.lib.loc, package) :
>         couldn't find function "lazyLoad"
>In addition: Warning message:
>package RWinEdt was built under R version 2.0.0
>Error in library(pkg, character.only = TRUE) :
>         .First.lib failed
> >
>
>Any help would be appreciated!!!
>
>Emili
>
>At 05:14 AM 10/8/2004, henna001 at students.uni-mainz.de wrote:
>>Hi,
>>
>>I have troubles getting RWinEdt with the R2.0.0 Version startet.
>>
>>Do I have to install a different version of RWinEdt or WinEdt?
>>I have RWinEdt 1.6.1 and WinEdt 5.3.
>>
>>Thanks
>>Anna Hennig
>>
>>______________________________________________
>>R-help at stat.math.ethz.ch mailing list
>>https://stat.ethz.ch/mailman/listinfo/r-help
>>PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>
>______________________________________________
>R-help at stat.math.ethz.ch mailing list
>https://stat.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html



From ripley at stats.ox.ac.uk  Sat Oct  9 09:18:20 2004
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Sat, 9 Oct 2004 08:18:20 +0100 (BST)
Subject: [R] polr problem solved
In-Reply-To: <s1671089.069@MAIL.NDRI.ORG>
Message-ID: <Pine.LNX.4.44.0410090815010.24696-100000@gannet.stats>

On Fri, 8 Oct 2004, Peter Flom wrote:

> I'd like to thank John Fox and Chuck Cleland for their help in resovling
> this issue.  It turned out to be something simple, but perhaps others
> have had similar problems
> 
> In my original data frame, I had 4 categories of race/ethnicity.  One of
> the categories (other) was very small, and not similar to any of the
> other three categories, so I created a new data frame deleting those
> people.
> 
> However, the level "other" was still there, with no one in it.
> This didn't cause a problem for glm or lm, but it did for polr.  When I
> eliminated that level, the problem disappeared.

How did you use `glm or lm' for an order factor response?  An empty factor 
level will certainly cause glm problems, depending which one it is.

An empty level will always cause polr problems, as there is no MLE under 
those circumstances.  I will add a sanity check in due course.

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From ripley at stats.ox.ac.uk  Sat Oct  9 09:22:52 2004
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Sat, 9 Oct 2004 08:22:52 +0100 (BST)
Subject: [R] Survey of "moving window" statistical functions - still
	looking f or fast mad function
In-Reply-To: <9CC1B717EF3BD511AD98000103D63FC53FA6C8@us-arl-asg.mail.saic.com>
Message-ID: <Pine.LNX.4.44.0410090820240.24696-100000@gannet.stats>

On Fri, 8 Oct 2004, Tuszynski, Jaroslaw W. wrote:

[...]

> 	Finally a question: I still need to get moving windows mad function
> faster my "runmad" function is not that much faster than apply/embed combo,
> and that I used before, and this is where my code spends most of its time. I
> need something like "runmed" but for a mad function. Any suggestions?

Write your own C-level implementation, as runmed and most of the other
fast functions you cite are.

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From gb at stat.umu.se  Sat Oct  9 10:34:12 2004
From: gb at stat.umu.se (Gran Brostrm)
Date: Sat, 9 Oct 2004 10:34:12 +0200
Subject: [R] Adding factor variable to a CoxPH
In-Reply-To: <04E35D6C-198F-11D9-8B4B-003065D5B8EC@tartarus.uwa.edu.au>
References: <04E35D6C-198F-11D9-8B4B-003065D5B8EC@tartarus.uwa.edu.au>
Message-ID: <20041009083412.GA16502@stat.umu.se>

On Sat, Oct 09, 2004 at 09:03:23AM +0800, Neil Leonard wrote:
> I think this is a pretty basic statistic question:
> 
> If I have a variable which has 4 factors how can I add it to a coxph 
> model? I have other variables added which have digits 1-4 instead of 
> four factors. If I recode the variable will that work?

?factor
?as.factor

> 
> 
> Thanks
> Neil
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html

-- 
 G??ran Brostr??m                    tel: +46 90 786 5223
 Department of Statistics          fax: +46 90 786 6614
 Ume?? University                   http://www.stat.umu.se/egna/gb/
 SE-90187 Ume??, Sweden             e-mail: gb at stat.umu.se



From patrick.giraudoux at univ-fcomte.fr  Sat Oct  9 12:28:49 2004
From: patrick.giraudoux at univ-fcomte.fr (Patrick Giraudoux)
Date: Sat, 9 Oct 2004 12:28:49 +0200
Subject: [R] which() and value replacement in a matrix
Message-ID: <000701c4adea$c93283e0$77970e50@PC728329681112>

Hi,

I cannot go through the archives with which() as key-word... so common. Though I am sure to have seen something about this subject
in the past could somebody put me on the track. I have a matrix (actually a data.frame) in which I would replace the non-null values
by 1.

I tried the following:

indices<-which(myforetbin > 0,arr.ind=T)
myforetbin[indices[,1],indices[,2]]<-1

and get the message:

> myforetbin[indices[,1],indices[,2]]<-1
Error in "[<-.data.frame"(`*tmp*`, indices[, 1], indices[, 2], value = 1) :
        duplicate subscripts for columns

I get the same with

myforetbin[indices]<-1

However, with:

myforetbin[indices]

I well get a vector with the corresponding non-null values.

Can somebody put me on the track?

All the best,

Patrick



From flom at ndri.org  Sat Oct  9 13:45:01 2004
From: flom at ndri.org (Peter Flom)
Date: Sat, 09 Oct 2004 07:45:01 -0400
Subject: [R] polr problem solved
Message-ID: <s1679728.086@MAIL.NDRI.ORG>

Prof Brian Ripley <ripley at stats.ox.ac.uk> 10/09/04 3:18 AM asked

<<<
How did you use `glm or lm' for an order factor response?  An empty
factor 
level will certainly cause glm problems, depending which one it is.

An empty level will always cause polr problems, as there is no MLE under

those circumstances.  I will add a sanity check in due course.
>>>


The analyses were part of a paper I am writing, illustrating that, when
the DV is oddly
distributed (the DV in question was a count, with many 0's, and a long
right tail) that
the 'usual' methods not only are wrong for statisically reasaons (such
as grossly violating
model assumptions) but also give bad results.

While this is widely known to statisticians, in the fields in which I
work, people sometimes
analyze such variables using either OLS regression (hence lm), or by
categorizing the DV into 
something like 0, 1, 2, more than 2 (hence the need for polr). I also
tried Poisson regression and
negative binomial regression (hence glm).  

The empty level of the IV only caused a problem for polr

Thanks 

Peter


Peter

Peter L. Flom, PhD
Assistant Director, Statistics and Data Analysis Core
Center for Drug Use and HIV Research
National Development and Research Institutes
71 W. 23rd St
www.peterflom.com
New York, NY 10010
(212) 845-4485 (voice)
(917) 438-0894 (fax)



From lobry at biomserv.univ-lyon1.fr  Sat Oct  9 14:20:10 2004
From: lobry at biomserv.univ-lyon1.fr (Jean lobry)
Date: Sat, 9 Oct 2004 14:20:10 +0200
Subject: [R] Shangai ranking of world Universities
In-Reply-To: <200410091014.i99A5nIw022655@hypatia.math.ethz.ch>
References: <200410091014.i99A5nIw022655@hypatia.math.ethz.ch>
Message-ID: <p05200f02bd8d8186e56e@[80.14.71.158]>

Dear all,

this is somewhat off-topic, but given the audience of this
mailling list this might interest some people here.

I have imported under R the data used in 2004 by Liu et al.
from Shanghai Jiao Tong University to build their "Academic
Ranking of World Universities", see:
http://ed.sjtu.edu.cn/rank/2004/2004Main.htm

The following R code:
loadURL("http://pbil.univ-lyon1.fr/R/donnees/shangai.RData", mode = "wb")
should import a dataframe called "shangai" in your workspace.

If you are interested I have also a draft Sweave-generated document
based on this dataframe, but all comments are in french.

I would be interested by references of scientific articles
about this ranking.

Best,

Jean Lobry
-- 
Jean R. Lobry            (lobry at biomserv.univ-lyon1.fr)
Laboratoire BBE-CNRS-UMR-5558, Univ. C. Bernard - LYON I,
43 Bd 11/11/1918, F-69622 VILLEURBANNE CEDEX, FRANCE
allo  : +33 472 43 12 87     fax    : +33 472 43 13 88
http://pbil.univ-lyon1.fr/members/lobry/



From andy_liaw at merck.com  Sat Oct  9 15:00:22 2004
From: andy_liaw at merck.com (Liaw, Andy)
Date: Sat, 9 Oct 2004 09:00:22 -0400
Subject: [R] which() and value replacement in a matrix
Message-ID: <3A822319EB35174CA3714066D590DCD504AF8500@usrymx25.merck.com>

Use the index vector directly, rather than breaking it up:

> x <- matrix(sample(30), 10, 3)
> idx <- which(x > 25, arr.ind=TRUE)
> idx
     row col
[1,]   6   1
[2,]   9   1
[3,]   4   2
[4,]   6   2
[5,]   4   3
> x[idx] <- 999
> x
      [,1] [,2] [,3]
 [1,]    7   14   16
 [2,]   20   24    8
 [3,]   17   18   11
 [4,]   19  999  999
 [5,]   23    4   15
 [6,]  999  999    5
 [7,]   21    9   12
 [8,]   22    2    3
 [9,]  999   13    1
[10,]    6   10   25

HTH,
Andy

> From: Patrick Giraudoux
> 
> Hi,
> 
> I cannot go through the archives with which() as key-word... 
> so common. Though I am sure to have seen something about this subject
> in the past could somebody put me on the track. I have a 
> matrix (actually a data.frame) in which I would replace the 
> non-null values
> by 1.
> 
> I tried the following:
> 
> indices<-which(myforetbin > 0,arr.ind=T)
> myforetbin[indices[,1],indices[,2]]<-1
> 
> and get the message:
> 
> > myforetbin[indices[,1],indices[,2]]<-1
> Error in "[<-.data.frame"(`*tmp*`, indices[, 1], indices[, 
> 2], value = 1) :
>         duplicate subscripts for columns
> 
> I get the same with
> 
> myforetbin[indices]<-1
> 
> However, with:
> 
> myforetbin[indices]
> 
> I well get a vector with the corresponding non-null values.
> 
> Can somebody put me on the track?
> 
> All the best,
> 
> Patrick
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
> 
>



From chris.albert at mcgill.ca  Sat Oct  9 15:16:40 2004
From: chris.albert at mcgill.ca (Christopher Albert)
Date: Sat, 9 Oct 2004 09:16:40 -0400
Subject: [R] problem Installing R 2.0.0 on SuSe 9.1
Message-ID: <CE5144E6CD150848B5526EBEAEC4FFC00369BE5C@EXCHANGEVS3.campus.mcgill.ca>

Hello:

	I first tried it by using ./configure with the source files, but
found that there is a huge problem with the gcc-g77 package, then I
tried using the RPM obtained from CRAN, getting the following, the first
of which appears to be another manifestation of the g77 problem:

linux:/tmp # rpm -i R-base-2.0.0-1.i586.rpm
warning: R-base-2.0.0-1.i586.rpm: V3 DSA signature: NOKEY, key ID
a3278da3
error: Failed dependencies:
        libg2c.so.0 is needed by R-base-2.0.0-1
        libglade-gnome.so.0 is needed by R-base-2.0.0-1
        libglade.so.0 is needed by R-base-2.0.0-1

Do you have a suggestion about what I (a suse beginner) should do in
order to get R up and running under suse 9.1?  I have done some looking
around and I do not see any package on suse for version 3.3.3 of the
gcc-g77 package that corresponds to the gcc-g++ version 3.3.3, and I
don't know if I am up to changing the versions of everything, so I hope
I don't have to do that.  Thanks in advance.


I'm not a Suse user, but this probably means you are missing a couple
packages that a required for the install. 
On a Fedora 2 box :
[albert at pentaquark albert]$ rpm -qf /usr/lib/libg2c.so.0
libf2c-3.3.3-7
[albert at pentaquark albert]$ rpm -qf /usr/lib/libglade-gnome.so.0
libglade-0.17-13.2.1
[albert at pentaquark albert]$ rpm -qf /usr/lib/libglade.so.0
libglade-0.17-13.2.1
[albert at pentaquark albert]$

Do you have those files? If not, you might just need to install a couple
of rpms, libf2c and libglade. They seem to be available for 9.1:

http://www.suse.com/us/private/products/suse_linux/prof/packages_profess
ional/libglade.html
http://www.suse.de/en/private/products/suse_linux/prof/packages_professi
onal/f2c.html

What does "rpm -q f2c libglade" tell you?

Chris



From jfox at mcmaster.ca  Sat Oct  9 16:01:20 2004
From: jfox at mcmaster.ca (John Fox)
Date: Sat, 9 Oct 2004 10:01:20 -0400
Subject: [R] RE: zero-inflated count models (was polr problem solved)
In-Reply-To: <s1679728.086@MAIL.NDRI.ORG>
Message-ID: <20041009140114.VKQU1890.tomts36-srv.bellnexxia.net@JohnDesktop8300>

Dear Peter,

> -----Original Message-----

. . .
 
> The analyses were part of a paper I am writing, illustrating 
> that, when the DV is oddly distributed (the DV in question 
> was a count, with many 0's, and a long right tail) that the 
> 'usual' methods not only are wrong for statisically reasaons 
> (such as grossly violating model assumptions) but also give 
> bad results.
> 
> While this is widely known to statisticians, in the fields in 
> which I work, people sometimes analyze such variables using 
> either OLS regression (hence lm), or by categorizing the DV 
> into something like 0, 1, 2, more than 2 (hence the need for 
> polr). I also tried Poisson regression and negative binomial 
> regression (hence glm).  
> 

>From your description, it seems possible that there are too many zeros for a
Poisson or negative-binomial model. Since the focus of your paper is the
methodology, you might want to try a zero-inflated Poisson or
negative-binomial model. Though I haven't tried them, I'm aware of two
sources of R functions for zero-inflated count models -- zeroinfl(), from
Simon Jackman's web site <http://pscl.stanford.edu/content.html>, and gnlr()
in Jim Lindsey's gnlm package, which is not available on CRAN but at
<www.luc.ac.be/~jlindsey/rcode.html>.

I hope this helps,
 John

--------------------------------
John Fox
Department of Sociology
McMaster University
Hamilton, Ontario
Canada L8S 4M4
905-525-9140x23604
http://socserv.mcmaster.ca/jfox



From patrick.giraudoux at univ-fcomte.fr  Sat Oct  9 16:14:09 2004
From: patrick.giraudoux at univ-fcomte.fr (Patrick Giraudoux)
Date: Sat, 9 Oct 2004 16:14:09 +0200
Subject: [R] which() and value replacement in a matrix
References: <3A822319EB35174CA3714066D590DCD504AF8500@usrymx25.merck.com>
Message-ID: <005f01c4ae0a$44331180$d5f00e50@PC728329681112>

Thanks for the clue.

Actually the trouble comes when refering to a data.frame. If I use the matrix from the data.frame (matrix(mydataframe)), everything
goes smoothly...

So I wrote:

indices<-which(myforetbin > 0,arr.ind=T)
myforetbin<-as.matrix(myforetbin)
myforetbin[indices]<-1
myforetbin<-as.data.frame(myforetbin)

It works but I wonder if there are no more simple ways...

All the best,

Patrick


----- Original Message ----- 
From: "Liaw, Andy" <andy_liaw at merck.com>
To: "'Patrick Giraudoux'" <patrick.giraudoux at univ-fcomte.fr>; "r-help" <r-help at stat.math.ethz.ch>
Sent: Saturday, October 09, 2004 3:00 PM
Subject: RE: [R] which() and value replacement in a matrix


> Use the index vector directly, rather than breaking it up:
>
> > x <- matrix(sample(30), 10, 3)
> > idx <- which(x > 25, arr.ind=TRUE)
> > idx
>      row col
> [1,]   6   1
> [2,]   9   1
> [3,]   4   2
> [4,]   6   2
> [5,]   4   3
> > x[idx] <- 999
> > x
>       [,1] [,2] [,3]
>  [1,]    7   14   16
>  [2,]   20   24    8
>  [3,]   17   18   11
>  [4,]   19  999  999
>  [5,]   23    4   15
>  [6,]  999  999    5
>  [7,]   21    9   12
>  [8,]   22    2    3
>  [9,]  999   13    1
> [10,]    6   10   25
>
> HTH,
> Andy
>
> > From: Patrick Giraudoux
> >
> > Hi,
> >
> > I cannot go through the archives with which() as key-word...
> > so common. Though I am sure to have seen something about this subject
> > in the past could somebody put me on the track. I have a
> > matrix (actually a data.frame) in which I would replace the
> > non-null values
> > by 1.
> >
> > I tried the following:
> >
> > indices<-which(myforetbin > 0,arr.ind=T)
> > myforetbin[indices[,1],indices[,2]]<-1
> >
> > and get the message:
> >
> > > myforetbin[indices[,1],indices[,2]]<-1
> > Error in "[<-.data.frame"(`*tmp*`, indices[, 1], indices[,
> > 2], value = 1) :
> >         duplicate subscripts for columns
> >
> > I get the same with
> >
> > myforetbin[indices]<-1
> >
> > However, with:
> >
> > myforetbin[indices]
> >
> > I well get a vector with the corresponding non-null values.
> >
> > Can somebody put me on the track?
> >
> > All the best,
> >
> > Patrick
> >
> > ______________________________________________
> > R-help at stat.math.ethz.ch mailing list
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide!
> > http://www.R-project.org/posting-guide.html
> >
> >
>
>
> ------------------------------------------------------------------------------
> Notice:  This e-mail message, together with any attachments, contains information of Merck & Co., Inc. (One Merck Drive,
Whitehouse Station, New Jersey, USA 08889), and/or its affiliates (which may be known outside the United States as Merck Frosst,
Merck Sharp & Dohme or MSD and in Japan, as Banyu) that may be confidential, proprietary copyrighted and/or legally privileged. It
is intended solely for the use of the individual or entity named on this message.  If you are not the intended recipient, and have
received this message in error, please notify us immediately by reply e-mail and then delete it from your system.
> ------------------------------------------------------------------------------



From patrick.giraudoux at univ-fcomte.fr  Sat Oct  9 16:20:11 2004
From: patrick.giraudoux at univ-fcomte.fr (Patrick Giraudoux)
Date: Sat, 9 Oct 2004 16:20:11 +0200
Subject: [R] conversion of a data.frame of numerics to a data.frame of
	factors
Message-ID: <008101c4ae0b$2eb16ef0$d5f00e50@PC728329681112>

Hi,

I am trying to convert a data.frame of numerics (this could be a matrix as well in this case) into a data.frame of factors.

I did it in a way that is less than direct...

myforet2<-t(myforet)
for (i in 1:length(myforet2[1,])) {
    if (i == 1)myforetfact<-list(as.factor(myforet2[,i]))
    else myforetfact<-c(myforetfact,list(as.factor(myforet2[,i])))
}
myforetfact<-data.frame(myforetfact)
names(myforetfact)<-row.names(myforet)

Here again, I wonder if there are no easier way to go through.... (the loop is not "R" style, for the least). However, I cannot do
it otherway so far...

Cheers,

Patrick



From bwheeler at echip.com  Sat Oct  9 16:28:22 2004
From: bwheeler at echip.com (Bob Wheeler)
Date: Sat, 09 Oct 2004 10:28:22 -0400
Subject: [R] inst directory
Message-ID: <4167F586.2030501@echip.com>

R CMD check now balks at my inst directory. It contains a single folder 
"doc," but apparently any folder causes the problem. If inst is empty, 
the project checks OK. This was not a problem before 1.9. I've checked 
the documentation, but don't see a change. What am I missing.

-- 
Bob Wheeler --- http://www.bobwheeler.com/
         ECHIP, Inc. ---
Randomness comes in bunches.



From ripley at stats.ox.ac.uk  Sat Oct  9 16:32:50 2004
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Sat, 9 Oct 2004 15:32:50 +0100 (BST)
Subject: [R] conversion of a data.frame of numerics to a data.frame of
	factors
In-Reply-To: <008101c4ae0b$2eb16ef0$d5f00e50@PC728329681112>
Message-ID: <Pine.LNX.4.44.0410091528140.8632-100000@gannet.stats>

On Sat, 9 Oct 2004, Patrick Giraudoux wrote:

mydf[] <- lapply(mydf, as.factor)

would appear to be what you want.

For a matrix, I presume you want a factor matrix as the result.  Something 
like

my <- matrix(1:12, 3, 4)
dmy <- dim(my)
my <- as.factor(my)
dim(my) <- dmy
my

which does not print as a matrix but is one.  If you want a data frame 
result, convert to a data frame first.

> Hi,
> 
> I am trying to convert a data.frame of numerics (this could be a matrix
> as well in this case) into a data.frame of factors.
> 
> I did it in a way that is less than direct...
> 
> myforet2<-t(myforet)
> for (i in 1:length(myforet2[1,])) {
>     if (i == 1)myforetfact<-list(as.factor(myforet2[,i]))
>     else myforetfact<-c(myforetfact,list(as.factor(myforet2[,i])))
> }
> myforetfact<-data.frame(myforetfact)
> names(myforetfact)<-row.names(myforet)
> 
> Here again, I wonder if there are no easier way to go through.... (the loop is not "R" style, for the least). However, I cannot do
> it otherway so far...
> 
> Cheers,
> 
> Patrick
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
> 
> 

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From deleeuw at stat.ucla.edu  Sat Oct  2 20:55:16 2004
From: deleeuw at stat.ucla.edu (Jan de Leeuw)
Date: Sat, 2 Oct 2004 11:55:16 -0700
Subject: [R] [R-pkgs] Publishing R package descriptions in JSS
Message-ID: <99A4F49A-14A4-11D9-9D7C-000D932C7F48@stat.ucla.edu>

More and more R packages come with a corresponding article in the  
Journal
of Statistical Software (www.jstatsoft.org). Achim Zeileis, our
TeXnical Editor, has recently contributed jss style files for issues,
bookreviews, software reviews, and code snippets. They can be
downloaded from

http://www.jstatsoft.org/JSSstyle.zip

Package authors who want to transform their package into a
published article are encouraged to use these style files.

===
Jan de Leeuw; Professor and Chair, UCLA Department of Statistics;
Editor: Journal of Multivariate Analysis, Journal of Statistical  
Software
US mail: 8125 Math Sciences Bldg, Box 951554, Los Angeles, CA 90095-1554
phone (310)-825-9550;  fax (310)-206-5658;  email: deleeuw at stat.ucla.edu
homepage: http://gifi.stat.ucla.edu
   
------------------------------------------------------------------------ 
-------------------------
           No matter where you go, there you are. --- Buckaroo Banzai
                    http://gifi.stat.ucla.edu/sounds/nomatter.au

_______________________________________________
R-packages mailing list
R-packages at stat.math.ethz.ch
https://stat.ethz.ch/mailman/listinfo/r-packages



From pocernic at rap.ucar.edu  Mon Oct  4 18:02:33 2004
From: pocernic at rap.ucar.edu (Matt Pocernich)
Date: Mon, 4 Oct 2004 10:02:33 -0600 (MDT)
Subject: [R] [R-pkgs] verfication package announcement 
Message-ID: <Pine.LNX.4.44.0410041000490.13615-100000@albedo.rap.ucar.edu>

The verification package has recently been posted to CRAN.  This package
was initially developed by people in the Verification Group at the
National Center for Atmospheric Research to verfiy and study weather
models and forecasts.  It has been written in general terms to be
applicable to other fields of study.  Functions include

receiver operating characteristic curves
attribute diagrams
reliability plots
spatial scale and frequency plots
conditional quantile plots and more.

Suggestions and comments on the package can be directed to me at
pocernic at ucar.edu.  More information on verification (with a somewhat
meteorolgical perspective) can be found at
http://www.bom.gov.au/bmrc/wefor/staff/eee/verif/verif_web_page.html.

Thanks,

Matt

#####################
Matt Pocernich
NCAR - Research Applications Program
303-497-8312

_______________________________________________
R-packages mailing list
R-packages at stat.math.ethz.ch
https://stat.ethz.ch/mailman/listinfo/r-packages



From Timur.Elzhov at jinr.ru  Fri Oct  8 09:20:22 2004
From: Timur.Elzhov at jinr.ru (Timur Elzhov)
Date: Fri, 8 Oct 2004 11:20:22 +0400
Subject: [R] [R-pkgs] R interface for MINPACK least squares optimization
	library
Message-ID: <20041008072022.GB16210@nf034.jinr.ru>

Hello guys.

I've built and uploaded to CRAN an R interface to MINPACK Fortran library,
which  solves non-linear  least squares problem  by  modification of the
Levenberg-Marquardt algorithm. The package includes one R function, which
passes  all  the necessary control parameters to the appropriate Fortran
functions.

The package location is
http://cran.r-project.org/src/contrib/Descriptions/minpack.lm.html


Best wishes,
Timur.

_______________________________________________
R-packages mailing list
R-packages at stat.math.ethz.ch
https://stat.ethz.ch/mailman/listinfo/r-packages



From ripley at stats.ox.ac.uk  Sat Oct  9 16:57:13 2004
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Sat, 9 Oct 2004 15:57:13 +0100 (BST)
Subject: [R] inst directory
In-Reply-To: <4167F586.2030501@echip.com>
Message-ID: <Pine.LNX.4.44.0410091549380.8690-100000@gannet.stats>

On Sat, 9 Oct 2004, Bob Wheeler wrote:

> R CMD check now balks at my inst directory. It contains a single folder 
> "doc," but apparently any folder causes the problem. If inst is empty, 
> the project checks OK. This was not a problem before 1.9. I've checked 
> the documentation, but don't see a change. What am I missing.

Hard to tell.  (What does `balk at' really mean?  What OS?  What is meant
by `now'?: there is no `1.9' and 1.9.0 is six months ago.  And so on.)

There are many packages on CRAN with sub-directories of inst (I presume
that is what you mean by `folder'), for example MASS, sm, spatstat, and
several (e.g. strucchange) have an inst directory just containing a doc 
subdirectory.

As I hope is clear we would need a lot more information.  Also, I think 
R-devel is a more appropriate place to ask such questions.

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From patrick.giraudoux at univ-fcomte.fr  Sat Oct  9 17:02:20 2004
From: patrick.giraudoux at univ-fcomte.fr (Patrick Giraudoux)
Date: Sat, 9 Oct 2004 17:02:20 +0200
Subject: [R] conversion of a data.frame of numerics to a data.frame of
	factors
Message-ID: <001201c4ae11$0038b550$791e0e50@PC728329681112>

This work perfectly with a data.frame. Actually my so-called "data.frame" was an output of t(), and thus not a data.frame (what I
realise after trying Prof. Ripley's example). If applied to an output of t() (eg t(df)), the result is quite unxpected (to me): a
strange list of 989 elements, each of them being a factor of one digit and one level corresponding to each cell of the matrix 23
rows x 43 columns...

So in this particular case, advised to write:

mydf<-as.data.frame(t(mydf0))
mydf[] <- lapply(mydf, as.factor)



> ----- Original Message ----- 
> From: "Prof Brian Ripley" <ripley at stats.ox.ac.uk>
> To: "Patrick Giraudoux" <patrick.giraudoux at univ-fcomte.fr>
> Cc: "r-help" <r-help at stat.math.ethz.ch>
> Sent: Saturday, October 09, 2004 4:32 PM
> Subject: Re: [R] conversion of a data.frame of numerics to a data.frame of factors
>
>
> > On Sat, 9 Oct 2004, Patrick Giraudoux wrote:
> >
> > mydf[] <- lapply(mydf, as.factor)
> >
> > would appear to be what you want.
> >
> > For a matrix, I presume you want a factor matrix as the result.  Something
> > like
> >
> > my <- matrix(1:12, 3, 4)
> > dmy <- dim(my)
> > my <- as.factor(my)
> > dim(my) <- dmy
> > my
> >
> > which does not print as a matrix but is one.  If you want a data frame
> > result, convert to a data frame first.
> >
> > > Hi,
> > >
> > > I am trying to convert a data.frame of numerics (this could be a matrix
> > > as well in this case) into a data.frame of factors.
> > >
> > > I did it in a way that is less than direct...
> > >
> > > myforet2<-t(myforet)
> > > for (i in 1:length(myforet2[1,])) {
> > >     if (i == 1)myforetfact<-list(as.factor(myforet2[,i]))
> > >     else myforetfact<-c(myforetfact,list(as.factor(myforet2[,i])))
> > > }
> > > myforetfact<-data.frame(myforetfact)
> > > names(myforetfact)<-row.names(myforet)
> > >
> > > Here again, I wonder if there are no easier way to go through.... (the loop is not "R" style, for the least). However, I
cannot
> do
> > > it otherway so far...
> > >
> > > Cheers,
> > >
> > > Patrick
> > >
> > > ______________________________________________
> > > R-help at stat.math.ethz.ch mailing list
> > > https://stat.ethz.ch/mailman/listinfo/r-help
> > > PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
> > >
> > >
> >
> > -- 
> > Brian D. Ripley,                  ripley at stats.ox.ac.uk
> > Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
> > University of Oxford,             Tel:  +44 1865 272861 (self)
> > 1 South Parks Road,                     +44 1865 272866 (PA)
> > Oxford OX1 3TG, UK                Fax:  +44 1865 272595
>



From clarapg at oninet.pt  Sat Oct  9 17:17:03 2004
From: clarapg at oninet.pt (=?Windows-1252?B?Y2xhcmEgZ29u52FsdmVz?=)
Date: Sat, 09 Oct 2004 16:17:03 +0100
Subject: [R] Clustering of variable
Message-ID: <231af77c323443218ef3c0caec18d09b@oninet.pt>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20041009/12210b08/attachment.pl

From mase at is.titech.ac.jp  Sat Oct  9 17:35:49 2004
From: mase at is.titech.ac.jp (Shigeru Mase)
Date: Sun, 10 Oct 2004 00:35:49 +0900
Subject: [R] Chernoff faces
In-Reply-To: <20041008163134.GA4575@s1x.local>
References: <20041008163134.GA4575@s1x.local>
Message-ID: <41680555.1040105@is.titech.ac.jp>

There is also an code which is more faithful (?) to the original
one by S. Aoki although it is in a Japanese HP

http://aoki2.si.gunma-u.ac.jp/R/face.html

Quality? Judge yourself!

Wolfram Fischer wrote:
>>>>>>>"Kenneth" == Kenneth Cabrera <krcabrer at perseus.unalmed.edu.co> writes:
>>
>>Kenneth> Hello everybody: Does any one has a function to build Chernoff
>>Kenneth> faces?
>>
>>Many of us don't think it's worth them.
>>But we know that opinions differ and gladly incorporate
>>(good quality) submissions of source code.
>>
>>R *is* an open source project and to some extent a community effort.
>>Please don't hesitate to contribute and enter the hall of fame of R
>>contributors :-)
>>
>>Martin 
> 
> 
> An example code from H.P. Wolf (2003) can be found at:
> 	http://www.wiwi.uni-bielefeld.de/~wolf/ : S/R - functions : faces
> 
> Wolfram
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>



From flom at ndri.org  Sat Oct  9 18:00:22 2004
From: flom at ndri.org (Peter Flom)
Date: Sat, 09 Oct 2004 12:00:22 -0400
Subject: [R] RE: zero-inflated count models (was polr problem solved)
Message-ID: <s167d2f0.040@MAIL.NDRI.ORG>

John Fox wrote
<<<
>From your description, it seems possible that there are too many zeros
for a Poisson or negative-binomial model. Since the focus of your paper
is the methodology, you might want to try a zero-inflated Poisson or
negative-binomial model. Though I haven't tried them, I'm aware of two
sources of R functions for zero-inflated count models -- zeroinfl(),
from Simon Jackman's web site <http://pscl.stanford.edu/content.html>,
and gnlr()
in Jim Lindsey's gnlm package, which is not available on CRAN but at
<www.luc.ac.be/~jlindsey/rcode.html>.
>>>

Indeed. I am going to use these, as well, and attempt to demonstrate
that fitting the correct model not only is correct, statistically, but
also give results that are substantively different than the 'usual'
models.  I hope that this will increase the use of appropriate models in
the field (which, in case anyone is interested, is substance abuse
research, and, in my particular case, the syringe-sharing behaviors of
drug injectors).

Peter

Peter L. Flom, PhD
Assistant Director, Statistics and Data Analysis Core
Center for Drug Use and HIV Research
National Development and Research Institutes
71 W. 23rd St
www.peterflom.com
New York, NY 10010
(212) 845-4485 (voice)
(917) 438-0894 (fax)



From jeaneid at chass.utoronto.ca  Sat Oct  9 18:36:51 2004
From: jeaneid at chass.utoronto.ca (Jean Eid)
Date: Sat, 9 Oct 2004 12:36:51 -0400
Subject: [R] R-2.0.0 and tcltk package
Message-ID: <Pine.SGI.4.40.0410020724300.42749011-100000@origin.chass.utoronto.ca>

there does not seem to be a package "tcltk" on CRAN for 2.0.0.
I have successfully installed the same package for 1.9.1.

In essence I require the package for a GUI interface to setwd.
All work fine with 1.9.1.

This is on a Linux Debian unstable kernel 2.4.20

The build version of R 2.0.0 (issuing version whithin R) has the following

platform i686-pc-linux-gnu
arch     i686
os       linux-gnu
system   i686, linux-gnu
status
major    2
minor    0.0
year     2004
month    10
day      04
language R



However, the build version of 1.9.1. has the paltform as i386

platform i386-pc-linux-gnu
arch     i386
os       linux-gnu
system   i386, linux-gnu
status
major    1
minor    9.1
year     2004
month    06
day      21
language R



Thank you for all the help,


Jean,



From dmb at mrc-dunn.cam.ac.uk  Sat Oct  9 19:18:09 2004
From: dmb at mrc-dunn.cam.ac.uk (Dan Bolser)
Date: Sat, 9 Oct 2004 18:18:09 +0100 (BST)
Subject: [R] Is it safe? Cochran etc
Message-ID: <Pine.LNX.4.21.0410091746370.9221-100000@mail.mrc-dunn.cam.ac.uk>


I have the following contingency table

dat <- matrix(c(1,506,13714,878702),nr=2)

And I want to test if their is an association between events 

A:{a,not(a)} and B:{b,not(b)}

        | b   | not(b) |
--------+-----+--------+
 a      |   1 |  13714 |
--------+-----+--------+
 not(a) | 506 | 878702 |
--------+-----+--------+

I am worried that prop.test and chisq.test are not valid given the low
counts and low probabilites associated with 'sucess' in each category.

Is it safe to use them, and what is the alternative? (given that
fisher.test can't handle this data... hold the phone...

I just found fisher.test can handle this data if the test is one-tailed
and not two-tailed.

I don't understand the difference between chisq.test, prop.test and
fisher.test when the hybrid=1 option is used for the fisher.test.

I was using the binomial distribution to test the 'extremity' of the
observed data, but now I think I know why that is inapropriate, however,
with the binomial (and its approximation) at least I know what I am
doing. And I can do it in perl easily...

Generally, how should I calculate fisher.test in perl (i.e. what are its
principles). When is it safe to approximate fisher to chisq?

I cannot get insight into this problem...

How come if I do...

dat <- matrix(c(50,60,100,100),nr=2)

prop.test(dat)$p.value
chisq.test(dat)$p.value
fisher.test(dat)$p.value

I get 

[1] 0.5173269
[1] 0.5173269
[1] 0.4771358

When I looked at the binomial distribution and the normal approximation
thereof with similar counts I never had a p-value difference > 0.004

I am so fed up with this problem :(



From dmb at mrc-dunn.cam.ac.uk  Sat Oct  9 20:06:01 2004
From: dmb at mrc-dunn.cam.ac.uk (Dan Bolser)
Date: Sat, 9 Oct 2004 19:06:01 +0100 (BST)
Subject: [R] Is it safe? Cochran etc
In-Reply-To: <Pine.LNX.4.21.0410091746370.9221-100000@mail.mrc-dunn.cam.ac.uk>
Message-ID: <Pine.LNX.4.21.0410091901400.9221-100000@mail.mrc-dunn.cam.ac.uk>


Why can't I just use Log odds? Does the standard error of the logs score
depend on a similar chisq assumption?



On Sat, 9 Oct 2004, Dan Bolser wrote:

>
>I have the following contingency table
>
>dat <- matrix(c(1,506,13714,878702),nr=2)
>
>And I want to test if their is an association between events 
>
>A:{a,not(a)} and B:{b,not(b)}
>
>        | b   | not(b) |
>--------+-----+--------+
> a      |   1 |  13714 |
>--------+-----+--------+
> not(a) | 506 | 878702 |
>--------+-----+--------+
>
>I am worried that prop.test and chisq.test are not valid given the low
>counts and low probabilites associated with 'sucess' in each category.
>
>Is it safe to use them, and what is the alternative? (given that
>fisher.test can't handle this data... hold the phone...
>
>I just found fisher.test can handle this data if the test is one-tailed
>and not two-tailed.
>
>I don't understand the difference between chisq.test, prop.test and
>fisher.test when the hybrid=1 option is used for the fisher.test.
>
>I was using the binomial distribution to test the 'extremity' of the
>observed data, but now I think I know why that is inapropriate, however,
>with the binomial (and its approximation) at least I know what I am
>doing. And I can do it in perl easily...
>
>Generally, how should I calculate fisher.test in perl (i.e. what are its
>principles). When is it safe to approximate fisher to chisq?
>
>I cannot get insight into this problem...
>
>How come if I do...
>
>dat <- matrix(c(50,60,100,100),nr=2)
>
>prop.test(dat)$p.value
>chisq.test(dat)$p.value
>fisher.test(dat)$p.value
>
>I get 
>
>[1] 0.5173269
>[1] 0.5173269
>[1] 0.4771358
>
>When I looked at the binomial distribution and the normal approximation
>thereof with similar counts I never had a p-value difference > 0.004
>
>I am so fed up with this problem :(
>
>______________________________________________
>R-help at stat.math.ethz.ch mailing list
>https://stat.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>



From mi2kelgrum at yahoo.com  Sat Oct  9 20:13:59 2004
From: mi2kelgrum at yahoo.com (Mikkel Grum)
Date: Sat, 9 Oct 2004 11:13:59 -0700 (PDT)
Subject: [R] RSQLite query error
In-Reply-To: <E31D0558-FBFE-11D8-838A-000A95D7BA10@mail.nih.gov>
Message-ID: <20041009181359.20492.qmail@web60206.mail.yahoo.com>

Dear R-helpers,

I ran the following little test on RSQLite and got the
data below from the query.  Unless I've made some
mistake, the results of both the where and order by
statements have problems:

library(RSQLite)
con<-dbConnect(dbDriver("SQLite"),dbname="test")
data(USArrests)
dbWriteTable(con,"arrests",USArrests,overwrite=TRUE)
dbListTables(con)
dbReadTable(con,"arrests")
dbGetQuery(con,paste("SELECT row_names,Murder,Rape
FROM arrests",
"WHERE Rape>30 ORDER BY Murder"))

       row_names Murder Rape
1         Alaska   10.0 44.5
2     New Mexico   11.4 32.1
3       Michigan   12.1 35.1
4         Nevada   12.2 46.0
5        Florida   15.4 31.9
6   North Dakota    0.8  7.3
7  New Hampshire    2.1  9.5
8          Maine    2.1  7.8
9   Rhode Island    3.4  8.3
10 West Virginia    5.7  9.3
11      Colorado    7.9 38.7
12       Arizona    8.1 31.0
13    California    9.0 40.6

I'm running R 2.0.0 on Windows XP.  Should I make a
bug report or can someone point to an error that I've
made?

cheers
Mikkel



From fred-l at poleto.com  Sat Oct  9 20:56:00 2004
From: fred-l at poleto.com (Frederico Zanqueta Poleto)
Date: Sat, 09 Oct 2004 15:56:00 -0300
Subject: [R] Is it safe? Cochran etc
In-Reply-To: <Pine.LNX.4.21.0410091901400.9221-100000@mail.mrc-dunn.cam.ac.uk>
References: <Pine.LNX.4.21.0410091901400.9221-100000@mail.mrc-dunn.cam.ac.uk>
Message-ID: <41683440.4050309@poleto.com>

Dan,

I don't know what is the theory behind this "hybrid" option and what 
consists the Cochran conditions.

However, I think even if you suppose the asymptotic distribution is not 
too accurate, because your sampled 1, there is a too strong association 
of A and B, as this can be noticed by conservative methods such as using 
the Yates continuity correction or Wald/Neyman tests (that usually does 
not reject the null hypothesis of no interaction much more than the 
Pearson/score test and likelihood ratio test, in this order) of the log 
odds.
Both procedures inflate the pvalues, but not sufficiently to change your 
conclusion as you can notice by:

> chisq.test(dat,correct=FALSE)

        Pearson's Chi-squared test

data:  dat 

X-squared = 6.0115, df = 1, p-value = 0.01421

> chisq.test(dat)

        Pearson's Chi-squared test with Yates' continuity correction

data:  dat 

X-squared = 5.1584, df = 1, p-value = 0.02313

> 1-pchisq( (log(878702/(13714*506))^2)/(1+1/878702+1/13714+1/506) ,1)
# Wald test of null log odds

[1] 0.03898049

The book "Categorical data analysis" from Agresti (2002) has an ample 
discussion about tests like this on chapters 1 (basics and one sample) 
and 3 (two variables). You may look there if you still have doubts about 
this tests.

Sincerely,

-- 
Frederico Zanqueta Poleto
fred at poleto.com
--
"An approximate answer to the right problem is worth a good deal more than an exact answer to an approximate problem." J. W. Tukey



Dan Bolser wrote:

>Why can't I just use Log odds? Does the standard error of the logs score
>depend on a similar chisq assumption?
>
>
>
>On Sat, 9 Oct 2004, Dan Bolser wrote:
>
>  
>
>>I have the following contingency table
>>
>>dat <- matrix(c(1,506,13714,878702),nr=2)
>>
>>And I want to test if their is an association between events 
>>
>>A:{a,not(a)} and B:{b,not(b)}
>>
>>       | b   | not(b) |
>>--------+-----+--------+
>>a      |   1 |  13714 |
>>--------+-----+--------+
>>not(a) | 506 | 878702 |
>>--------+-----+--------+
>>
>>I am worried that prop.test and chisq.test are not valid given the low
>>counts and low probabilites associated with 'sucess' in each category.
>>
>>Is it safe to use them, and what is the alternative? (given that
>>fisher.test can't handle this data... hold the phone...
>>
>>I just found fisher.test can handle this data if the test is one-tailed
>>and not two-tailed.
>>
>>I don't understand the difference between chisq.test, prop.test and
>>fisher.test when the hybrid=1 option is used for the fisher.test.
>>
>>I was using the binomial distribution to test the 'extremity' of the
>>observed data, but now I think I know why that is inapropriate, however,
>>with the binomial (and its approximation) at least I know what I am
>>doing. And I can do it in perl easily...
>>
>>Generally, how should I calculate fisher.test in perl (i.e. what are its
>>principles). When is it safe to approximate fisher to chisq?
>>
>>I cannot get insight into this problem...
>>
>>How come if I do...
>>
>>dat <- matrix(c(50,60,100,100),nr=2)
>>
>>prop.test(dat)$p.value
>>chisq.test(dat)$p.value
>>fisher.test(dat)$p.value
>>
>>I get 
>>
>>[1] 0.5173269
>>[1] 0.5173269
>>[1] 0.4771358
>>
>>When I looked at the binomial distribution and the normal approximation
>>thereof with similar counts I never had a p-value difference > 0.004
>>
>>I am so fed up with this problem :(
>>
>>    
>>
>  
>



From dj at research.bell-labs.com  Sat Oct  9 21:00:15 2004
From: dj at research.bell-labs.com (David James)
Date: Sat, 9 Oct 2004 15:00:15 -0400
Subject: [R] RSQLite query error
In-Reply-To: <20041009181359.20492.qmail@web60206.mail.yahoo.com>;
	from mi2kelgrum@yahoo.com on Sat, Oct 09, 2004 at 11:13:59AM -0700
References: <E31D0558-FBFE-11D8-838A-000A95D7BA10@mail.nih.gov>
	<20041009181359.20492.qmail@web60206.mail.yahoo.com>
Message-ID: <20041009150015.A20818@jessie.research.bell-labs.com>

Mikkel Grum wrote:
> Dear R-helpers,
> 
> I ran the following little test on RSQLite and got the
> data below from the query.  Unless I've made some
> mistake, the results of both the where and order by
> statements have problems:

This is due to the fact that SQLite as of version 2.8 is typeless
and values are stored as ASCII strings. Most expressions are evaluated 
as string expressions, as in your query.  For details see
http://www.sqlite.org/datatypes.html (SQLite Version 2.8) and
http://www.sqlite.org/datatype3.html (Version 3.0).

You can use a trivial arithmetic expression to force numeric
comparisons (awk users may recognize this trick):

> dbGetQuery(con, 
     paste("select * from arrests where Rape+0.0 > 30",
           "order by Murder+0.0"))

       row_names Murder Assault UrbanPop Rape
    1   Colorado    7.9     204       78 38.7
    2    Arizona    8.1     294       80 31.0
    3 California    9.0     276       91 40.6
    4     Alaska   10.0     263       48 44.5
    5 New Mexico   11.4     285       70 32.1
    6   Michigan   12.1     255       74 35.1
    7     Nevada   12.2     252       81 46.0
    8    Florida   15.4     335       80 31.9

Regards,

--
David
 
> 
> library(RSQLite)
> con<-dbConnect(dbDriver("SQLite"),dbname="test")
> data(USArrests)
> dbWriteTable(con,"arrests",USArrests,overwrite=TRUE)
> dbListTables(con)
> dbReadTable(con,"arrests")
> dbGetQuery(con,paste("SELECT row_names,Murder,Rape
> FROM arrests",
> "WHERE Rape>30 ORDER BY Murder"))
> 
>        row_names Murder Rape
> 1         Alaska   10.0 44.5
> 2     New Mexico   11.4 32.1
> 3       Michigan   12.1 35.1
> 4         Nevada   12.2 46.0
> 5        Florida   15.4 31.9
> 6   North Dakota    0.8  7.3
> 7  New Hampshire    2.1  9.5
> 8          Maine    2.1  7.8
> 9   Rhode Island    3.4  8.3
> 10 West Virginia    5.7  9.3
> 11      Colorado    7.9 38.7
> 12       Arizona    8.1 31.0
> 13    California    9.0 40.6
> 
> I'm running R 2.0.0 on Windows XP.  Should I make a
> bug report or can someone point to an error that I've
> made?
> 
> cheers
> Mikkel
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html



From ripley at stats.ox.ac.uk  Sat Oct  9 21:06:38 2004
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Sat, 9 Oct 2004 20:06:38 +0100 (BST)
Subject: [R] R-2.0.0 and tcltk package
In-Reply-To: <Pine.SGI.4.40.0410020724300.42749011-100000@origin.chass.utoronto.ca>
Message-ID: <Pine.LNX.4.44.0410092004250.18532-100000@gannet.stats>

On Sat, 9 Oct 2004, Jean Eid wrote:

> there does not seem to be a package "tcltk" on CRAN for 2.0.0.

It is part of the R tarball, so library(tcltk) should do something.
It has never been on CRAN, to my knowledge.  See the R FAQ 5.1.1
(as the posting guide suggests).

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From kjetil at acelerate.com  Sat Oct  9 21:16:58 2004
From: kjetil at acelerate.com (Kjetil Brinchmann Halvorsen)
Date: Sat, 09 Oct 2004 15:16:58 -0400
Subject: [R] Is it safe? Cochran etc
In-Reply-To: <Pine.LNX.4.21.0410091746370.9221-100000@mail.mrc-dunn.cam.ac.uk>
References: <Pine.LNX.4.21.0410091746370.9221-100000@mail.mrc-dunn.cam.ac.uk>
Message-ID: <4168392A.7050603@acelerate.com>

Dan Bolser wrote:

>I have the following contingency table
>
>dat <- matrix(c(1,506,13714,878702),nr=2)
>
>And I want to test if their is an association between events 
>
>A:{a,not(a)} and B:{b,not(b)}
>
>        | b   | not(b) |
>--------+-----+--------+
> a      |   1 |  13714 |
>--------+-----+--------+
> not(a) | 506 | 878702 |
>--------+-----+--------+
>
>I am worried that prop.test and chisq.test are not valid given the low
>counts and low probabilites associated with 'sucess' in each category.
>  
>

 > test <- matrix( c(1,506, 13714, 878702), 2,2)
 > test
     [,1]   [,2]
[1,]    1  13714
[2,]  506 878702
 > chisq.test(test)

        Pearson's Chi-squared test with Yates' continuity correction

data:  test
X-squared = 5.1584, df = 1, p-value = 0.02313

 > chisq.test(test,sim=TRUE)

        Pearson's Chi-squared test with simulated p-value (based on 2000
        replicates)

data:  test
X-squared = 6.0115, df = NA, p-value = 0.02099

 > chisq.test(test,sim=TRUE, B=200000)
     # To rule out simulation uncertainty
        Pearson's Chi-squared test with simulated p-value (based on 200000
        replicates)

data:  test
X-squared = 6.0115, df = NA, p-value = 0.01634

 > N <- sum(test)
 > rows <- rowSums(test)
 > cols <- colSums(test)
 > E <- rows %o% cols/N
 > E
           [,1]      [,2]
[1,]   7.787351  13707.21
[2,] 499.212649 878708.79
 >

None of the expected'eds are lesser than 5, an often  used rule of thumb 
(which might even be
to conservative. Just to check on the distribution:

rows <- round(rows)
cols <- round(cols)

 > pvals <- sapply(r2dtable(100000, rows, cols), function(x) 
chisq.test(x)$p.value)
 > hist(pvals)
     # not very good approximation to uniform histogram, but:
 > sum(pvals < 0.05)/100000
[1] 0.03068
 > sum(pvals < 0.01)/100000
[1] 0.00669

So the true levels are not very far from the calculated by te chisq 
approximation, and it seems safe to use it.

All of this to show that with R you are not anymore dependent on old 
"rules os thumb",
you can investigate for yourself.

Kjetil

>Is it safe to use them, and what is the alternative? (given that
>fisher.test can't handle this data... hold the phone...
>
>I just found fisher.test can handle this data if the test is one-tailed
>and not two-tailed.
>
>I don't understand the difference between chisq.test, prop.test and
>fisher.test when the hybrid=1 option is used for the fisher.test.
>
>I was using the binomial distribution to test the 'extremity' of the
>observed data, but now I think I know why that is inapropriate, however,
>with the binomial (and its approximation) at least I know what I am
>doing. And I can do it in perl easily...
>
>Generally, how should I calculate fisher.test in perl (i.e. what are its
>principles). When is it safe to approximate fisher to chisq?
>
>I cannot get insight into this problem...
>
>How come if I do...
>
>dat <- matrix(c(50,60,100,100),nr=2)
>
>prop.test(dat)$p.value
>chisq.test(dat)$p.value
>fisher.test(dat)$p.value
>
>I get 
>
>[1] 0.5173269
>[1] 0.5173269
>[1] 0.4771358
>
>When I looked at the binomial distribution and the normal approximation
>thereof with similar counts I never had a p-value difference > 0.004
>
>I am so fed up with this problem :(
>
>______________________________________________
>R-help at stat.math.ethz.ch mailing list
>https://stat.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>
>
>  
>


-- 

Kjetil Halvorsen.

Peace is the most effective weapon of mass construction.
               --  Mahdi Elmandjra



From kjetil at acelerate.com  Sat Oct  9 20:35:45 2004
From: kjetil at acelerate.com (Kjetil Brinchmann Halvorsen)
Date: Sat, 09 Oct 2004 14:35:45 -0400
Subject: [R] Clustering of variable
In-Reply-To: <231af77c323443218ef3c0caec18d09b@oninet.pt>
References: <231af77c323443218ef3c0caec18d09b@oninet.pt>
Message-ID: <41682F81.7000507@acelerate.com>

clara gon??alves wrote:

>Hi!
>Does anybody know if there is any way to do clustering of variables?
>Thank you in advance,
>  
>
varclust() in Hmisc

Kjetil

>Clara.
>	[[alternative HTML version deleted]]
>
>______________________________________________
>R-help at stat.math.ethz.ch mailing list
>https://stat.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>
>
>  
>


-- 

Kjetil Halvorsen.

Peace is the most effective weapon of mass construction.
               --  Mahdi Elmandjra



From andy_liaw at merck.com  Sat Oct  9 23:26:20 2004
From: andy_liaw at merck.com (Liaw, Andy)
Date: Sat, 9 Oct 2004 17:26:20 -0400
Subject: [R] which() and value replacement in a matrix
Message-ID: <3A822319EB35174CA3714066D590DCD504AF8501@usrymx25.merck.com>

Use matrix indexing, instead of generating the index vector:

> x <- as.data.frame(x)
> x[x > 25] <- -999
> x
     V1   V2   V3
1     7   14   16
2    20   24    8
3    17   18   11
4    19 -999 -999
5    23    4   15
6  -999 -999    5
7    21    9   12
8    22    2    3
9  -999   13    1
10    6   10   25

Andy

> From: Patrick Giraudoux
> 
> Thanks for the clue.
> 
> Actually the trouble comes when refering to a data.frame. If 
> I use the matrix from the data.frame (matrix(mydataframe)), everything
> goes smoothly...
> 
> So I wrote:
> 
> indices<-which(myforetbin > 0,arr.ind=T)
> myforetbin<-as.matrix(myforetbin)
> myforetbin[indices]<-1
> myforetbin<-as.data.frame(myforetbin)
> 
> It works but I wonder if there are no more simple ways...
> 
> All the best,
> 
> Patrick
> 
> 
> ----- Original Message ----- 
> From: "Liaw, Andy" <andy_liaw at merck.com>
> To: "'Patrick Giraudoux'" <patrick.giraudoux at univ-fcomte.fr>; 
> "r-help" <r-help at stat.math.ethz.ch>
> Sent: Saturday, October 09, 2004 3:00 PM
> Subject: RE: [R] which() and value replacement in a matrix
> 
> 
> > Use the index vector directly, rather than breaking it up:
> >
> > > x <- matrix(sample(30), 10, 3)
> > > idx <- which(x > 25, arr.ind=TRUE)
> > > idx
> >      row col
> > [1,]   6   1
> > [2,]   9   1
> > [3,]   4   2
> > [4,]   6   2
> > [5,]   4   3
> > > x[idx] <- 999
> > > x
> >       [,1] [,2] [,3]
> >  [1,]    7   14   16
> >  [2,]   20   24    8
> >  [3,]   17   18   11
> >  [4,]   19  999  999
> >  [5,]   23    4   15
> >  [6,]  999  999    5
> >  [7,]   21    9   12
> >  [8,]   22    2    3
> >  [9,]  999   13    1
> > [10,]    6   10   25
> >
> > HTH,
> > Andy
> >
> > > From: Patrick Giraudoux
> > >
> > > Hi,
> > >
> > > I cannot go through the archives with which() as key-word...
> > > so common. Though I am sure to have seen something about 
> this subject
> > > in the past could somebody put me on the track. I have a
> > > matrix (actually a data.frame) in which I would replace the
> > > non-null values
> > > by 1.
> > >
> > > I tried the following:
> > >
> > > indices<-which(myforetbin > 0,arr.ind=T)
> > > myforetbin[indices[,1],indices[,2]]<-1
> > >
> > > and get the message:
> > >
> > > > myforetbin[indices[,1],indices[,2]]<-1
> > > Error in "[<-.data.frame"(`*tmp*`, indices[, 1], indices[,
> > > 2], value = 1) :
> > >         duplicate subscripts for columns
> > >
> > > I get the same with
> > >
> > > myforetbin[indices]<-1
> > >
> > > However, with:
> > >
> > > myforetbin[indices]
> > >
> > > I well get a vector with the corresponding non-null values.
> > >
> > > Can somebody put me on the track?
> > >
> > > All the best,
> > >
> > > Patrick
> > >
> > > ______________________________________________
> > > R-help at stat.math.ethz.ch mailing list
> > > https://stat.ethz.ch/mailman/listinfo/r-help
> > > PLEASE do read the posting guide!
> > > http://www.R-project.org/posting-guide.html
> > >
> > >
> >
> >
> > 
> --------------------------------------------------------------
> ----------------
> > Notice:  This e-mail message, together with any 
> attachments, contains information of Merck & Co., Inc. (One 
> Merck Drive,
> Whitehouse Station, New Jersey, USA 08889), and/or its 
> affiliates (which may be known outside the United States as 
> Merck Frosst,
> Merck Sharp & Dohme or MSD and in Japan, as Banyu) that may 
> be confidential, proprietary copyrighted and/or legally privileged. It
> is intended solely for the use of the individual or entity 
> named on this message.  If you are not the intended 
> recipient, and have
> received this message in error, please notify us immediately 
> by reply e-mail and then delete it from your system.
> > 
> --------------------------------------------------------------
> ----------------
> 
> 
>



From tilo.blenk at charite.de  Sun Oct 10 00:03:28 2004
From: tilo.blenk at charite.de (Tilo Blenk)
Date: Sun, 10 Oct 2004 00:03:28 +0200
Subject: [R] Mac: importing saved PDF figures into Illustrator CS
Message-ID: <0CF217B2-1A3F-11D9-8947-000D93AD656A@charite.de>

Rene Bertin wrote:

 > Hello,
 >
 > This is a usage question for others with experience of R under the  
Aqua Mac OS
 > X interface.
 >
 > Basically, I don't succeed in importing PDF files (created with the  
'Save As'
 > menu to Quartz device windows) into Illustrator. Versions up to (and
 > including?) 10 loose paths (lines/polygons) and or fill them in  
black. Version
 > CS (11) on the Mac imports almost correctly, but, "to preserve  
appearance,
 > some text has been outlined". In other words: *all* text in the  
figure is
 > converted to polygons. Somewhat annoying if one plans to edit the  
text, and in
 > general change the appearance. I've asked on both the Apple and Adobe
 > Illustrator forums
 >  
(http://www.adobeforums.com/cgi-bin/webx? 
13 at 114.dQNuchE8byf.1@.3bb6383d/0),
 > but have not yet received any reply. Maybe someone here has found a  
way to
 > tell Illustrator not to outline ('preserve editability', in their  
jargon), or
 > some other workaround?
 >
 > Thanks in advance, Ren?? Bertin


No real solution but try the pdf command instead of saving with the menu

pdf(file='~/Desktop/figure.pdf')
curve(x^2)
dev.off()

the resulting pdf file can be read into Illustrator CS without  
outlining the text (Mac OS X 10.3.5 with R 1.9.1) but each character is  
in a separate text frame.

As far as I remember the paths were not lost in Illustrator 10 but just  
were invisible. Selecting everything inside the plot region and setting  
the opacity to 100% (can be done in the transparency window) solved the  
problem.

Tilo



From Benjamin.Osborne at uvm.edu  Sun Oct 10 00:12:20 2004
From: Benjamin.Osborne at uvm.edu (Benjamin M. Osborne)
Date: Sat,  9 Oct 2004 18:12:20 -0400
Subject: [R] functions
Message-ID: <1097359940.416862444ef15@webmail.uvm.edu>

Does anyone know of a list of functions that R already "knows?"  I can't seem to
find this anywhere in the help documentation.  For example, I want to count the
number of occurences of a certain value in a column of a data frame: What do I
have to do to tell R to "Count?"
Thanks,
Ben Osborne

--
Botany Department
University of Vermont
109 Carrigan Drive
Burlington, VT 05405

benjamin.osborne at uvm.edu
phone: 802-656-0297
fax: 802-656-0440



From murdoch at stats.uwo.ca  Sun Oct 10 00:56:29 2004
From: murdoch at stats.uwo.ca (Duncan Murdoch)
Date: Sat, 09 Oct 2004 18:56:29 -0400
Subject: [R] functions
In-Reply-To: <1097359940.416862444ef15@webmail.uvm.edu>
References: <1097359940.416862444ef15@webmail.uvm.edu>
Message-ID: <ftqgm0pol549oaqrb9pqg6peuafseh5tkh@4ax.com>

On Sat,  9 Oct 2004 18:12:20 -0400, "Benjamin M. Osborne"
<Benjamin.Osborne at uvm.edu> wrote:

>Does anyone know of a list of functions that R already "knows?"  I can't seem to
>find this anywhere in the help documentation.  For example, I want to count the
>number of occurences of a certain value in a column of a data frame: What do I
>have to do to tell R to "Count?"

There are several such lists.  The one you want is probably the one
the help system uses.  For example,

help.search('count')

lists a number of functions that count various things.  I don't think
any of them is exactly what you want, which could be accomplished in a
couple of ways:

table(x)

gives counts of all the different values in x;

length(x[x==3])

gives the count of occurences of 3 in x.

Duncan Murdoch



From sty2102 at columbia.edu  Sun Oct 10 04:30:46 2004
From: sty2102 at columbia.edu (Susan Yeh)
Date: Sat,  9 Oct 2004 22:30:46 -0400
Subject: [R] Fatal error: unable to restore saved data in .Rdata
Message-ID: <1097375446.41689ed63cfa3@cubmail.cc.columbia.edu>

Dear Sir/Madam:
I'm currently running R 1.9.1 on a Mac OS 10.3. I have been able to
use it fine for a while now. But for some reason, it now crashes
each time I open it with the error: Fatal error: unable to restore
saved data in .Rdata
I read on the online help forum that I can just delete the bad
.Rdata file. But I cannot find it. I even tried to delete
everything related to R and reinstalling but the problem has not
gone away.
Please HELP!
Thank you,
Susan Yeh



From aldi at wubios.wustl.edu  Sun Oct 10 05:07:12 2004
From: aldi at wubios.wustl.edu (Aldi Kraja)
Date: Sat, 09 Oct 2004 22:07:12 -0500
Subject: [R] How to install a package that needs to see oher pkg
	dependencies:
Message-ID: <4168A760.1060701@wubios.wustl.edu>

Hi,
I am trying to install the genetics package on a server with Linux, 
Fedora. I installed it in a PC and worked fine.

In a server since I am not used with R, I am not sure what do I need to 
change so genetics pkg can see some package dependencies:
Any suggestion is appreciated, Aldi

Note:
genetics expects gregmisc and mvtnorm to be installed already. gregmisc 
creates gdata etc dependencies.
So here is what I have done:
Retrieved the base R and installed and compiled it with make under :
/users/genetics/aldi/r/R/R-2.0.0

Under it directories:
/mvtnorm
/gregmisc
/genetics
are created.
Installed mvtnorm and gregmisc with no problem, but genetics is not 
recognizing the location of /gregmisc/gdata

The command I am using to install genetics package is the following:
../bin/R CMD ../bin/INSTALL genetics_1.1.0.tar.gz

Here is the problem:

genetics at genxeon1:/users/genetics/aldi/r/R-2.0.0/genetics% ../bin/R CMD 
../bin/INSTALL -l . genetics_1.1.0.tar.gz

* Installing *source* package 'genetics' ...

** R

** data

** inst

** preparing package for lazy loading

Error in loadNamespace(i[[1]], c(lib.loc, .libPaths()), keep.source) :

        There is no package called 'gdata'

Execution halted

ERROR: lazy loading failed for package 'genetics'

genetics at genxeon1:/users/genetics/aldi/r/R-2.0.0/genetics%



From edd at debian.org  Sun Oct 10 05:21:49 2004
From: edd at debian.org (Dirk Eddelbuettel)
Date: Sat, 9 Oct 2004 22:21:49 -0500
Subject: [R] R-2.0.0 and tcltk package
In-Reply-To: <Pine.SGI.4.40.0410020724300.42749011-100000@origin.chass.utoronto.ca>
References: <Pine.SGI.4.40.0410020724300.42749011-100000@origin.chass.utoronto.ca>
Message-ID: <20041010032149.GA1108@sonny.eddelbuettel.com>

On Sat, Oct 09, 2004 at 12:36:51PM -0400, Jean Eid wrote:
> there does not seem to be a package "tcltk" on CRAN for 2.0.0.
> I have successfully installed the same package for 1.9.1.
> 
> In essence I require the package for a GUI interface to setwd.
> All work fine with 1.9.1.
> 
> This is on a Linux Debian unstable kernel 2.4.20

If you were using the pre-built Debian package, you'd have the tcltk package:

edd at homebud:~> dpkg -L r-base-core | grep tcltk/ | wc -l
48
edd at homebud:~> dpkg -L r-base-core | grep tcltk/ | head
/usr/lib/R/library/tcltk/R
/usr/lib/R/library/tcltk/R/tcltk
/usr/lib/R/library/tcltk/R/tcltk.rdb
/usr/lib/R/library/tcltk/R/tcltk.rdx
/usr/lib/R/library/tcltk/NAMESPACE
/usr/lib/R/library/tcltk/DESCRIPTION
/usr/lib/R/library/tcltk/Meta
/usr/lib/R/library/tcltk/Meta/package.rds
/usr/lib/R/library/tcltk/Meta/nsInfo.rds
/usr/lib/R/library/tcltk/Meta/Rd.rds

As you seem to prefer to roll your own (which you could also base on the
Debian package sources which enforce build-time dependencies so that these
things work), I highly recommend to fetch the output of the build process
(e.g. via script(1)) so that you can go back and check what configure told
you -- for it will have almost surely have told you if and when it missed
headers or libraries for tcl/tk. 

Hope this helps, regards, Dirk

-- 
Those are my principles, and if you don't like them... well, I have others.
                                                -- Groucho Marx



From zhliur at yahoo.com  Sun Oct 10 07:59:19 2004
From: zhliur at yahoo.com (yyan liu)
Date: Sat, 9 Oct 2004 22:59:19 -0700 (PDT)
Subject: [R] Modified Bessel function (third kind)
Message-ID: <20041010055919.72227.qmail@web90102.mail.scd.yahoo.com>

Hi:
   There is a Modified Bessel function (third kind,
real order) besselK in R. Is there a C version of this
function?
I can only find a Fortran function rkbesel on
http://www.netlib.org/specfun/. However, this rkbesel
fortran function is the second kind.
    Thank you!
liu


		
_______________________________

Declare Yourself - Register online to vote today!



From ripley at stats.ox.ac.uk  Sun Oct 10 08:35:03 2004
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Sun, 10 Oct 2004 07:35:03 +0100 (BST)
Subject: [R] How to install a package that needs to see oher pkg
	dependencies:
In-Reply-To: <4168A760.1060701@wubios.wustl.edu>
Message-ID: <Pine.LNX.4.44.0410100731060.27173-100000@gannet.stats>

On Sat, 9 Oct 2004, Aldi Kraja wrote:

> Hi,
> I am trying to install the genetics package on a server with Linux, 
> Fedora. I installed it in a PC and worked fine.
> 
> In a server since I am not used with R, I am not sure what do I need to 
> change so genetics pkg can see some package dependencies:
> Any suggestion is appreciated, Aldi
> 
> Note:
> genetics expects gregmisc and mvtnorm to be installed already. gregmisc 
> creates gdata etc dependencies.
> So here is what I have done:
> Retrieved the base R and installed and compiled it with make under :
> /users/genetics/aldi/r/R/R-2.0.0
> 
> Under it directories:
> /mvtnorm
> /gregmisc
> /genetics
> are created.
> Installed mvtnorm and gregmisc with no problem, but genetics is not 
> recognizing the location of /gregmisc/gdata
> 
> The command I am using to install genetics package is the following:
> ../bin/R CMD ../bin/INSTALL genetics_1.1.0.tar.gz
> 
> Here is the problem:
> 
> genetics at genxeon1:/users/genetics/aldi/r/R-2.0.0/genetics% ../bin/R CMD 
> ../bin/INSTALL -l . genetics_1.1.0.tar.gz

The correct commands are

R CMD INSTALL gregmisc_2.0.0.tar.gz
R CMD INSTALL genetics_1.1.0.tar.gz

Wherever did you get `../bin/INSTALL -l .' from?

You need to install into a library that is in the library path for the 
package to be usable, and the system library (the default) is the best 
choice.  (This is no different from S-PLUS, on which you have been asking
questions on s-news for many years.)

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From ripley at stats.ox.ac.uk  Sun Oct 10 08:37:11 2004
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Sun, 10 Oct 2004 07:37:11 +0100 (BST)
Subject: [R] Modified Bessel function (third kind)
In-Reply-To: <20041010055919.72227.qmail@web90102.mail.scd.yahoo.com>
Message-ID: <Pine.LNX.4.44.0410100735160.27173-100000@gannet.stats>

On Sat, 9 Oct 2004, yyan liu wrote:

>    There is a Modified Bessel function (third kind,
> real order) besselK in R. Is there a C version of this
> function?

Yes, and it is documented in `Writing R Extensions' as part of the R API.


-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From WeiQiang.Li at seagate.com  Sun Oct 10 09:59:40 2004
From: WeiQiang.Li at seagate.com (WeiQiang.Li@seagate.com)
Date: Sun, 10 Oct 2004 15:59:40 +0800
Subject: [R] How to estimate Variance Components
Message-ID: <OFD869F1B0.B2540C7A-ON48256F29.0029E01C-48256F29.002BD859@seagate.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20041010/abb0d9e1/attachment.pl

From ripley at stats.ox.ac.uk  Sun Oct 10 12:00:25 2004
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Sun, 10 Oct 2004 11:00:25 +0100 (BST)
Subject: [R] How to estimate Variance Components
In-Reply-To: <OFD869F1B0.B2540C7A-ON48256F29.0029E01C-48256F29.002BD859@seagate.com>
Message-ID: <Pine.LNX.4.44.0410101056550.28309-100000@gannet.stats>

On Sun, 10 Oct 2004 WeiQiang.Li at seagate.com wrote:

>         I want to know how to calculate estimated Variance Components for 
> unbalance ANOVA. Can anybody please provide me the formula? Thanks in 
> advance!

Please read the posting guide: this is not an R question.

It has no simple answer, as there is no `the formula'.  There are a number 
of different methods based on different principles, most of which are
iterative.

If your email address (you give no signature) accurately reflects your 
position, I think the best answer is `consult your employer's statistical 
consultants'.

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From dmb at mrc-dunn.cam.ac.uk  Sun Oct 10 12:29:09 2004
From: dmb at mrc-dunn.cam.ac.uk (Dan Bolser)
Date: Sun, 10 Oct 2004 11:29:09 +0100 (BST)
Subject: [R] Is it safe? Cochran etc
In-Reply-To: <4168392A.7050603@acelerate.com>
Message-ID: <Pine.LNX.4.21.0410101106040.17550-100000@mail.mrc-dunn.cam.ac.uk>

On Sat, 9 Oct 2004, Kjetil Brinchmann Halvorsen wrote:

>Dan Bolser wrote:
>
>>I have the following contingency table
>>
>>dat <- matrix(c(1,506,13714,878702),nr=2)
>>
>>And I want to test if their is an association between events 
>>
>>A:{a,not(a)} and B:{b,not(b)}
>>
>>        | b   | not(b) |
>>--------+-----+--------+
>> a      |   1 |  13714 |
>>--------+-----+--------+
>> not(a) | 506 | 878702 |
>>--------+-----+--------+
>>
>>I am worried that prop.test and chisq.test are not valid given the low
>>counts and low probabilites associated with 'sucess' in each category.
>>  
>>
>
> > test <- matrix( c(1,506, 13714, 878702), 2,2)
> > test
>     [,1]   [,2]
>[1,]    1  13714
>[2,]  506 878702
> > chisq.test(test)
>
>        Pearson's Chi-squared test with Yates' continuity correction
>
>data:  test
>X-squared = 5.1584, df = 1, p-value = 0.02313
>
> > chisq.test(test,sim=TRUE)
>
>        Pearson's Chi-squared test with simulated p-value (based on 2000
>        replicates)
>
>data:  test
>X-squared = 6.0115, df = NA, p-value = 0.02099
>
> > chisq.test(test,sim=TRUE, B=200000)
>     # To rule out simulation uncertainty
>        Pearson's Chi-squared test with simulated p-value (based on 200000
>        replicates)

What is being simulated? Sorry for my ignorance. Is it the same as below?


>data:  test
>X-squared = 6.0115, df = NA, p-value = 0.01634
>
> > N <- sum(test)
> > rows <- rowSums(test)
> > cols <- colSums(test)
> > E <- rows %o% cols/N
> > E
>           [,1]      [,2]
>[1,]   7.787351  13707.21
>[2,] 499.212649 878708.79
> >
>
>None of the expected'eds are lesser than 5, an often  used rule of thumb 
>(which might even be
>to conservative. Just to check on the distribution:

OK, we are 'allowed' to use an approximation, but what am I approximating,
and how does this relate to fishers exact test for 2x2 table?


>rows <- round(rows)
>cols <- round(cols)

The above dosn't do anything in this example, did you mean to do 

rows <- rowSums(E)
cols <- colSums(E)

first?

>
> > pvals <- sapply(r2dtable(100000, rows, cols), function(x) 
>chisq.test(x)$p.value)
> > hist(pvals)
>     # not very good approximation to uniform histogram, but:
> > sum(pvals < 0.05)/100000
>[1] 0.03068
> > sum(pvals < 0.01)/100000
>[1] 0.00669

Above you simulate fishers exact test by making permutations on the given
table and measuring the probability of the resulting table with the same
marginals. You then count how much of this probability is more extreem
than standard critical values to see how similar to the chisq distribution
this simultion data is? Why should the probabilites above be 'uniform' as
you state?

>So the true levels are not very far from the calculated by te chisq 
>approximation, and it seems safe to use it.
>
>All of this to show that with R you are not anymore dependent on old 
>"rules os thumb",
>you can investigate for yourself.

Thanks very much for your explainations and guedeance, but it is my
exploration that is driving me mad :( I feel like I am running in circles.

Can you explain this for example...

dat <- matrix(c(500,560,1000,1000),nr=2)


prop.test(dat)$p.value
[1] 0.1464539
> chisq.test(dat)$p.value
[1] 0.1464539
> fisher.test(dat)$p.value
[1] 0.1385046

Is the difference of 0.0079493 OK

Now...

> dat <- matrix(c(5,7,10,10),nr=2)
> prop.test(dat)$p.value
[1] 0.9271224
> chisq.test(dat)$p.value
[1] 0.9271224
> fisher.test(dat)$p.value
[1] 0.7256694

The expecteds for the above are all above 5, but we get a difference of
0.201453

You may say (just use fisher) above, but then i am back where I started,
when should I use / not use fisher.

Why can't I use an exact multinomial distribution? (how?)

sorry for my total confusion,
thanks again,
Dan.


>Kjetil
>
>>Is it safe to use them, and what is the alternative? (given that
>>fisher.test can't handle this data... hold the phone...
>>
>>I just found fisher.test can handle this data if the test is one-tailed
>>and not two-tailed.
>>
>>I don't understand the difference between chisq.test, prop.test and
>>fisher.test when the hybrid=1 option is used for the fisher.test.
>>
>>I was using the binomial distribution to test the 'extremity' of the
>>observed data, but now I think I know why that is inapropriate, however,
>>with the binomial (and its approximation) at least I know what I am
>>doing. And I can do it in perl easily...
>>
>>Generally, how should I calculate fisher.test in perl (i.e. what are its
>>principles). When is it safe to approximate fisher to chisq?
>>
>>I cannot get insight into this problem...
>>
>>How come if I do...
>>
>>dat <- matrix(c(50,60,100,100),nr=2)
>>
>>prop.test(dat)$p.value
>>chisq.test(dat)$p.value
>>fisher.test(dat)$p.value
>>
>>I get 
>>
>>[1] 0.5173269
>>[1] 0.5173269
>>[1] 0.4771358
>>
>>When I looked at the binomial distribution and the normal approximation
>>thereof with similar counts I never had a p-value difference > 0.004
>>
>>I am so fed up with this problem :(
>>
>>______________________________________________
>>R-help at stat.math.ethz.ch mailing list
>>https://stat.ethz.ch/mailman/listinfo/r-help
>>PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>>
>>
>>  
>>
>
>
>



From dmb at mrc-dunn.cam.ac.uk  Sun Oct 10 13:03:13 2004
From: dmb at mrc-dunn.cam.ac.uk (Dan Bolser)
Date: Sun, 10 Oct 2004 12:03:13 +0100 (BST)
Subject: [R] Is it safe? Cochran etc
In-Reply-To: <41683440.4050309@poleto.com>
Message-ID: <Pine.LNX.4.21.0410101130041.17550-100000@mail.mrc-dunn.cam.ac.uk>


Hey

On Sat, 9 Oct 2004, Frederico Zanqueta Poleto wrote:

>Dan,
>
>I don't know what is the theory behind this "hybrid" option and what 
>consists the Cochran conditions.
>
>However, I think even if you suppose the asymptotic distribution is not 
>too accurate, because your sampled 1, there is a too strong association 
>of A and B, as this can be noticed by conservative methods such as using 
>the Yates continuity correction or Wald/Neyman tests (that usually does 
>not reject the null hypothesis of no interaction much more than the 
>Pearson/score test and likelihood ratio test, in this order) of the log 
>odds.

So I read from this that Wald/Neyman tests of the log odds is
conservative, but not by much?


>Both procedures inflate the pvalues, but not sufficiently to change your 
>conclusion as you can notice by:
>
>> chisq.test(dat,correct=FALSE)
>
>        Pearson's Chi-squared test
>
>data:  dat 
>
>X-squared = 6.0115, df = 1, p-value = 0.01421
>
>> chisq.test(dat)
>
>        Pearson's Chi-squared test with Yates' continuity correction
>
>data:  dat 
>
>X-squared = 5.1584, df = 1, p-value = 0.02313

OK, I see the p-value was inflated, making Yates more conservative. 


>> 1-pchisq( (log(878702/(13714*506))^2)/(1+1/878702+1/13714+1/506) ,1)
># Wald test of null log odds
>
>[1] 0.03898049

OK... 

logOD <- log((1*878702)/(13714*506))

same as

logOD <- log( (    1/   506) /
              (13714/878702)
            )

The odds ratio has an approximatly normal distribution 

Then you divide the logOD^2 by the variance (same as
logOD/StDev?)... Nope...

Is the above just the nature of the test?


Using data from the paper on the description of the log odds I have...


dat <- matrix(c(141,928,420,13525),nr=2)

dat

1-pchisq((log((141*13525)/(928*420))^2)/(1/141+1/13525+1/928+1/420),1)

[1] 0


a very cautious ... ok....

The data does not have a logOD of 1, very strongly, so the p-value is 0
for that test. 

How does the above differ from just saying ...

chisq.test(dat)

Other than the latter appears to never go below 2.2e-16 and the former
hapily says 0.


>The book "Categorical data analysis" from Agresti (2002) has an ample 
>discussion about tests like this on chapters 1 (basics and one sample) 
>and 3 (two variables). You may look there if you still have doubts about 
>this tests.

I should have got that book last week when I was still enthusiastic about
this problem.

Thanks very much for your help,
Dan.

>Sincerely,
>
>



From aldi at wubios.wustl.edu  Sun Oct 10 14:51:28 2004
From: aldi at wubios.wustl.edu (Aldi Kraja)
Date: Sun, 10 Oct 2004 07:51:28 -0500
Subject: [R] How to install a package that needs to see oher pkg
	dependencies:
In-Reply-To: <Pine.LNX.4.44.0410100731060.27173-100000@gannet.stats>
References: <Pine.LNX.4.44.0410100731060.27173-100000@gannet.stats>
Message-ID: <41693050.3080906@wubios.wustl.edu>

Prof Brian Ripley wrote:

>The correct commands are
>
>R CMD INSTALL gregmisc_2.0.0.tar.gz
>R CMD INSTALL genetics_1.1.0.tar.gz
>
>Wherever did you get `../bin/INSTALL -l .' from?
>
>You need to install into a library that is in the library path for the 
>package to be usable, and the system library (the default) is the best 
>choice.  (This is no different from S-PLUS, on which you have been asking
>questions on s-news for many years.)
>  
>
Thank you Brian for your response. In the online manual for the R 
installation and administration (2004)
it says:
"Note that you need to specify implicitly or explicitly the library to 
which the package is
to be installed. This is only an issue if you have more than one 
library, of course.
To install packages from source on Unix use
R CMD INSTALL -l /path/to/library pkg1 pkg2 ..."

I am working in a case where in the general path is already version 1.9, 
and I am installing locally version 2.0.0
My local copy will be removed in the moment that my Net. Adm. will 
install version 2.0.0 in the general path.

I am executing INSTALL command under ..../aldi/r/R-2.0.0/gregmisc%
I am executing INSTALL command under ..../aldi/r/R-2.0.0/genetics%
So ../bin/ is the local place where I have INSTALL for v. 2,
-l  . is the path where INSTALL has to look for the package, gregmisc etc.

The gregmisc works fine, it creates gdata.
Genetics package is searching for gdata, but does not see it under my 
local ...../gregmisc/gdata/
So my question was: what do I need to provide to genetics to see gdata, 
or what do I need to change to genetics to see the gdata?
TIA,
Aldi



From ligges at statistik.uni-dortmund.de  Sun Oct 10 15:07:39 2004
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Sun, 10 Oct 2004 15:07:39 +0200
Subject: [R] How to install a package that needs to see oher
	pkg	dependencies:
In-Reply-To: <41693050.3080906@wubios.wustl.edu>
References: <Pine.LNX.4.44.0410100731060.27173-100000@gannet.stats>
	<41693050.3080906@wubios.wustl.edu>
Message-ID: <4169341B.5070905@statistik.uni-dortmund.de>

Aldi Kraja wrote:

> Prof Brian Ripley wrote:
> 
>> The correct commands are
>>
>> R CMD INSTALL gregmisc_2.0.0.tar.gz
>> R CMD INSTALL genetics_1.1.0.tar.gz
>>
>> Wherever did you get `../bin/INSTALL -l .' from?
>>
>> You need to install into a library that is in the library path for the 
>> package to be usable, and the system library (the default) is the best 
>> choice.  (This is no different from S-PLUS, on which you have been asking
>> questions on s-news for many years.)
>>  
>>
> Thank you Brian for your response. In the online manual for the R 
> installation and administration (2004)
> it says:
> "Note that you need to specify implicitly or explicitly the library to 
> which the package is
> to be installed. This is only an issue if you have more than one 
> library, of course.
> To install packages from source on Unix use
> R CMD INSTALL -l /path/to/library pkg1 pkg2 ..."
> 
> I am working in a case where in the general path is already version 1.9, 
> and I am installing locally version 2.0.0
> My local copy will be removed in the moment that my Net. Adm. will 
> install version 2.0.0 in the general path.
> 
> I am executing INSTALL command under ..../aldi/r/R-2.0.0/gregmisc%
> I am executing INSTALL command under ..../aldi/r/R-2.0.0/genetics%
> So ../bin/ is the local place where I have INSTALL for v. 2,

The correct INSTALL script is found for each R version automatically (in 
its bin directory). You need to call the correct R binary, though.

> -l  . is the path where INSTALL has to look for the package, gregmisc etc.

No. since you are already in a folder called gregmisc.
If you want your private library, do it as mentioned in the doc you have 
cited yourself:
Create your own library tree and and install into that one. Note that 
you have to tell R to look into that library, e.g. by specifying an 
environment variable called R_LIBS.

If you are still confused, you might want to read the Help Desk article 
in R News 3(3).

Uwe Ligges



> The gregmisc works fine, it creates gdata.
> Genetics package is searching for gdata, but does not see it under my 
> local ...../gregmisc/gdata/
> So my question was: what do I need to provide to genetics to see gdata, 
> or what do I need to change to genetics to see the gdata?
> TIA,
> Aldi
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html



From aldi at wubios.wustl.edu  Sun Oct 10 17:18:52 2004
From: aldi at wubios.wustl.edu (Aldi Kraja)
Date: Sun, 10 Oct 2004 10:18:52 -0500
Subject: [R] How to install a package that needs to see other pkg
	dependencies:
In-Reply-To: <4169341B.5070905@statistik.uni-dortmund.de>
References: <Pine.LNX.4.44.0410100731060.27173-100000@gannet.stats>
	<41693050.3080906@wubios.wustl.edu>
	<4169341B.5070905@statistik.uni-dortmund.de>
Message-ID: <416952DC.6080004@wubios.wustl.edu>

Uwe Ligges wrote:

>
> If you are still confused, you might want to read the Help Desk 
> article in R News 3(3).
>
> Uwe Ligges

Thanks Uwe for the comments. I think R News 3(3) was a clear description 
of how to install other packages.
I would suggest to the maintainer of the manual of R Installation and 
Administration to include that description in it, or to create in the 
manual a hyperlink to the R News 3(3).
Aldi



From xiao.gang.fan1 at libertysurf.fr  Sun Oct 10 18:11:56 2004
From: xiao.gang.fan1 at libertysurf.fr (Fan)
Date: Sun, 10 Oct 2004 18:11:56 +0200
Subject: [R] R 2.0.0 not suffisantly reliable to be be used
Message-ID: <41695F4C.4090203@libertysurf.fr>

After wasting one whole day, I've finally decided to stay with 1.9.1,
some problems have been reported to R-Bugs.

For occasional users, I would say, there's no worst thing than that:
you installed the new release, and soem of your existing codes no
longer work !
--
Fan



From ripley at stats.ox.ac.uk  Sun Oct 10 19:04:23 2004
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Sun, 10 Oct 2004 18:04:23 +0100 (BST)
Subject: [R] some help interpreting ANOVA results, please?
In-Reply-To: <20041010183749.2f535f68@portia.local>
Message-ID: <Pine.LNX.4.44.0410101753210.29146-100000@gannet.stats>

On Sun, 10 Oct 2004, RenE J.V. Bertin wrote:

> Could I ask some hints/help in interpreting the following ANOVA results,
> please? This concerns an experiment where I study the incidence and
> severity of motion sickness. I have Sickness.norm, a subjective
> discomfort/sickness estimate, normalised to 0..1, the session time T
> (normalised to 0..1 and binned in 0.2 wide bins) and a qualitive
> indicator if a given Subject was sick or not. For instance, to see if
> there is an effect of time, I do
> 
> 
> > summary( aov.SS1( Sickness.norm~WasSick*T.norm.Class + Error(Subject/(WasSick*T.norm.Class)), na.action=na.exclude ) )
> Factor "WasSick", levels: 0 1 
> Factor "T.norm.Class", levels: 0 0.2 0.4 0.6 0.8 1 
> Sun Oct 10 13:45:45 2004 
> WasSick data: 377
> Warning in aov(Sickness.norm ~ WasSick * T.norm.Class + Error(Subject/(WasSick*T.norm.Class)), na.action=naa,  :
> 	 Error model is singular
> 
> Error: Subject
>                       Df Sum Sq Mean Sq F value Pr(>F)    
> WasSick               1 11.170  11.170  21.037 0.0001 ***
> T.norm.Class          5  5.058   1.012   1.905 0.1278    
> WasSick:T.norm.Class  5  1.680   0.336   0.633 0.6765    
> Residuals 26 13.805   0.531                   
> ---
> Signif. codes:  0 `***' 0.001 `**' 0.01 `*' 0.05 `.' 0.1 ` ' 1 
> 
> Error: Subject:T.norm.Class
>                        Df Sum Sq Mean Sq F value   Pr(>F)    
> T.norm.Class           5 10.916   2.183  13.673 3.29e-11 ***
> WasSick:T.norm.Class   5  2.318   0.464   2.903   0.0153 *  
> Residuals 169 26.984   0.160                     
> 
> 
> 
> There is a very nice effect of time (the line T.norm.Class under
> Subject:T.norm.Class). There is no interaction between the two factors
> (that would have been given on a line WasSick:T.norm.Class under the
> Subject:WasSick:T.norm.Class caption, right?!). 

Not right.

> That means that the
> effect of time is not different for the two groups, which is true:
> sickness increases for both, only less so for the non-sick group. The
> time effect is significant only for the sick group, when tested
> seperately. But what I do not manage to grasp is the interpretation of
> the other significant effects:


> *1 What does the very significant WasSick effect (under the Error:
> Subject caption) mean? That there is a sign. difference between the
> subjects who were sick and who were not, which is indeed very true?

See next point.

> *2 What is the meaning of the interaction WasSick:T.norm.Class
> (p=0.0153)? Can I interpret this as meaning that sickness increases
> significantly less in the non-sick group?

Here you have a problem, as that term appears in two strata, so presumably 
the nesting was unbalanced.  You either need to use Yates' `recovery of 
inter-block information' or, simpler, use lme.

You have evidence that the effect of WasSick differs by T.norm.Class.
If that is substantiated by a more refined analysis, it does not make 
sense to say there is a `very significant WasSick effect', as it is likely 
there is an effect for some session times and not others, or at least that 
the size of the effect differs by session time.

Trying to interpret main effects in the presence of interactions depends 
on the (unstated) coding of factors used.  If this is treatment coding,
WasSick is the effect at T=0 (significant) and the effect appears to 
change with time.

> *3 What importance should I give to the warning that the error model is
> singular? Does this stem from the fact that Subject and WasSick are not
> independent? R 1.8 generally refused to calculate this sort of test: can
> I rely on the results R 1.9.1 gives me??

I think that means the correct error model is Error(Subject/T.norm.Class):
my guess is that WasSick is a subject-level observation and so each 
subject only has one level of it.  Certainly that is the model which was 
fitted.

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From p.dalgaard at biostat.ku.dk  Sun Oct 10 20:19:44 2004
From: p.dalgaard at biostat.ku.dk (Peter Dalgaard)
Date: 10 Oct 2004 20:19:44 +0200
Subject: [R] Is it safe? Cochran etc
In-Reply-To: <Pine.LNX.4.21.0410101106040.17550-100000@mail.mrc-dunn.cam.ac.uk>
References: <Pine.LNX.4.21.0410101106040.17550-100000@mail.mrc-dunn.cam.ac.uk>
Message-ID: <x2brfa3073.fsf@biostat.ku.dk>

Dan Bolser <dmb at mrc-dunn.cam.ac.uk> writes:

> OK, we are 'allowed' to use an approximation, but what am I approximating,
> and how does this relate to fishers exact test for 2x2 table?

The odd thing is that FEXACT is giving up way before it needs to on
this example. According to my calculations, an array of length 507
should suffice! You can get the exact p value as 

> phyper(1, 13725, 879208, 507)+ phyper(16,13725,879208,507,low=F)
[1] 0.006098377

(This happens to be the same whether you use tail-balancing or
likelihood criterion to define the two-sided test)

And of course with huge numbers in one column, you have a very good
approximation in treating the ratio as known:

> binom.test(1,507, 13714/(13714+878702))

        Exact binomial test

data:  1 and 507
number of successes = 1, number of trials = 507, p-value = 0.006112
alternative hypothesis: true probability of success is not equal to
0.01536727
95 percent confidence interval:
 4.993526e-05 1.094002e-02
sample estimates:
probability of success
           0.001972387


-- 
   O__  ---- Peter Dalgaard             Blegdamsvej 3  
  c/ /'_ --- Dept. of Biostatistics     2200 Cph. N   
 (*) \(*) -- University of Copenhagen   Denmark      Ph: (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)             FAX: (+45) 35327907



From ggrothendieck at myway.com  Sun Oct 10 20:46:15 2004
From: ggrothendieck at myway.com (Gabor Grothendieck)
Date: Sun, 10 Oct 2004 18:46:15 +0000 (UTC)
Subject: [R] R 2.0.0 not suffisantly reliable to be be used
References: <41695F4C.4090203@libertysurf.fr>
Message-ID: <loom.20041010T203750-658@post.gmane.org>

Fan <xiao.gang.fan1 <at> libertysurf.fr> writes:

: 
: After wasting one whole day, I've finally decided to stay with 1.9.1,
: some problems have been reported to R-Bugs.
: 
: For occasional users, I would say, there's no worst thing than that:
: you installed the new release, and soem of your existing codes no
: longer work !

Based on your bug postings it may be that your problems stem
from how you are using dates and times.  I suggest you 
read the R Help Desk article in R News 4/1 which you can find at 
www.r-project.org by clicking on Newsletter in left pane and choosing
the indicated issue.



From ripley at stats.ox.ac.uk  Sun Oct 10 21:00:53 2004
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Sun, 10 Oct 2004 20:00:53 +0100 (BST)
Subject: [R] some help interpreting ANOVA results, please?
In-Reply-To: <20041010195541.5f05817f@portia.local>
Message-ID: <Pine.LNX.4.44.0410101958490.29444-100000@gannet.stats>

On Sun, 10 Oct 2004, RenE J.V. Bertin wrote:

> On Sun, 10 Oct 2004 18:04:23 +0100 (BST), Prof Brian Ripley <ripley at stats.ox.ac.uk> wrote regarding
> "Re: [R] some help interpreting ANOVA results, please?"
> 
> Thank you for your answers.
> 
> 8-) Here you have a problem, as that term appears in two strata, so presumably 
> 8-) the nesting was unbalanced.  You either need to use Yates' `recovery of 
> 8-) inter-block information' or, simpler, use lme.
> 
> 	Would you have any suggestions for an accessible text explaining lme, apart from MASS-4?
> 
> 8-) You have evidence that the effect of WasSick differs by T.norm.Class.
> 8-) If that is substantiated by a more refined analysis, it does not make 
> 8-) sense to say there is a `very significant WasSick effect', as it is likely 
> 8-) there is an effect for some session times and not others, or at least that 
> 8-) the size of the effect differs by session time.
> 
> 	WasSick is an overall assessment made at the end of the experimental sessions. That is why I tend to use it as an independent variable. Of course I did not decide whether or not a given subject would be sick (WasSick==1) or not, nor was it designated by a random process. It is in a way intrinsically linked to the individual subjects, not unlike gender. Maybe that is something to take into account for choosing the right test?
> 
> 8-) Trying to interpret main effects in the presence of interactions depends 
> 8-) on the (unstated) coding of factors used.  If this is treatment coding,
> 8-) WasSick is the effect at T=0 (significant) and the effect appears to 
> 8-) change with time.
> 
> 	I am not sure what you mean with (treatment) coding: a

options(contrasts=c("contr.treatment", something))

uses treatment coding for unordered factors.  That's the default in R, 
but not usually what is used in ANOVA.

> between-subjects design where some subjects receive a particular
> treatment and other don't? For WasSick, this comes down to the same
> thing as my design, and also for T.norm.Class, in a certain way. I have
> a number of subjects who all did a nauseogenic task (driving a car
> simulator), and indicated their level of discomfort while doing so.
> There was no strict timing protocol, and besides, each subject will have
> his/her own time scale on which the sickness eveolves and/or is
> indicated, hence my use of normalised time. WasSick at T=0 is not of
> interest, as it is 0, by definition.
> 
> 
> 8-) I think that means the correct error model is Error(Subject/T.norm.Class):
> 8-) my guess is that WasSick is a subject-level observation and so each 
> 8-) subject only has one level of it.  Certainly that is the model which was 
> 8-) fitted.
> 
> 	Yes, that is true (see above), and there are not as many sick as non-sick subjects (although the difference is not huge). When I use the Error term as you suggest, I do indeed get the same results, but also still the warning message.
> 
> Fortunately, when I run the test on only one group, the results are much clearer (N.S. in the non-sick):
> 
> > with2( SelectCases(ss600.3, "WasSick==1"), summary( aov( Sickness.norm~T.norm.Class + Error(Subject/(T.norm.Class)) ) ) )
> Warning in aov(Sickness.norm ~ T.norm.Class + Error(Subject/(T.norm.Class))) : 
> 	 Error model is singular
> 
> Error: Subject
>              Df Sum Sq Mean Sq F value Pr(>F)
> T.norm.Class  5  3.716   0.743   2.191  0.110
> Residuals    15  5.087   0.339               
> 
> Error: Subject:T.norm.Class
>              Df Sum Sq Mean Sq F value   Pr(>F)    
> T.norm.Class  5 11.561   2.312   16.15 2.25e-11 ***
> Residuals    91 13.027   0.143                     
> 
> RenE Bertin
> 
> 

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From kjetil at acelerate.com  Sun Oct 10 19:49:43 2004
From: kjetil at acelerate.com (Kjetil Brinchmann Halvorsen)
Date: Sun, 10 Oct 2004 13:49:43 -0400
Subject: [R] How to estimate Variance Components
In-Reply-To: <OFD869F1B0.B2540C7A-ON48256F29.0029E01C-48256F29.002BD859@seagate.com>
References: <OFD869F1B0.B2540C7A-ON48256F29.0029E01C-48256F29.002BD859@seagate.com>
Message-ID: <41697637.8040202@acelerate.com>

WeiQiang.Li at seagate.com wrote:

>Hi ALL
>
>        I want to know how to calculate estimated Variance Components for 
>unbalance ANOVA. Can anybody please provide me the formula? Thanks in 
>advance!
>
>WeiQiang
>
>	[[alternative HTML version deleted]]
>
>______________________________________________
>R-help at stat.math.ethz.ch mailing list
>https://stat.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>
>
>  
>
have a look at package nlme or at package lme4.

-- 

Kjetil Halvorsen.

Peace is the most effective weapon of mass construction.
               --  Mahdi Elmandjra



From kjetil at acelerate.com  Sun Oct 10 20:00:10 2004
From: kjetil at acelerate.com (Kjetil Brinchmann Halvorsen)
Date: Sun, 10 Oct 2004 14:00:10 -0400
Subject: [R] Fatal error: unable to restore saved data in .Rdata
In-Reply-To: <1097375446.41689ed63cfa3@cubmail.cc.columbia.edu>
References: <1097375446.41689ed63cfa3@cubmail.cc.columbia.edu>
Message-ID: <416978AA.6080504@acelerate.com>

Susan Yeh wrote:

>Dear Sir/Madam:
>I'm currently running R 1.9.1 on a Mac OS 10.3. I have been able to
>use it fine for a while now. But for some reason, it now crashes
>each time I open it with the error: Fatal error: unable to restore
>saved data in .Rdata
>I read on the online help forum that I can just delete the bad
>.Rdata file. But I cannot find it. I even tried to delete
>everything related to R and reinstalling but the problem has not
>gone away.
>Please HELP!
>Thank you,
>Susan Yeh
>
>______________________________________________
>R-help at stat.math.ethz.ch mailing list
>https://stat.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>
>
>  
>
start R with a flag --no-restore-data
How you specify that on a Mac I don't know, but should be easy, on 
windows it can be defined in
then shortcut)
then type getwd()
to get your working directory.
Then list.files(pattern="*.Rdata")
Now you know where is the bad file an d can continue to remove it.

Kjetil

-- 

Kjetil Halvorsen.

Peace is the most effective weapon of mass construction.
               --  Mahdi Elmandjra



From kjetil at acelerate.com  Sun Oct 10 21:34:10 2004
From: kjetil at acelerate.com (Kjetil Brinchmann Halvorsen)
Date: Sun, 10 Oct 2004 15:34:10 -0400
Subject: [R] Is it safe? Cochran etc
In-Reply-To: <Pine.LNX.4.21.0410101106040.17550-100000@mail.mrc-dunn.cam.ac.uk>
References: <Pine.LNX.4.21.0410101106040.17550-100000@mail.mrc-dunn.cam.ac.uk>
Message-ID: <41698EB2.1000300@acelerate.com>

Dan Bolser wrote:

>On Sat, 9 Oct 2004, Kjetil Brinchmann Halvorsen wrote:
>
>  
>
>>Dan Bolser wrote:
>>
>>    
>>
>>>I have the following contingency table
>>>
>>>dat <- matrix(c(1,506,13714,878702),nr=2)
>>>
>>>And I want to test if their is an association between events 
>>>
>>>A:{a,not(a)} and B:{b,not(b)}
>>>
>>>       | b   | not(b) |
>>>--------+-----+--------+
>>>a      |   1 |  13714 |
>>>--------+-----+--------+
>>>not(a) | 506 | 878702 |
>>>--------+-----+--------+
>>>
>>>I am worried that prop.test and chisq.test are not valid given the low
>>>counts and low probabilites associated with 'sucess' in each category.
>>> 
>>>
>>>      
>>>
>>>test <- matrix( c(1,506, 13714, 878702), 2,2)
>>>
>>>      
>>>
>>    
>>
>>>chisq.test(test,sim=TRUE, B=200000)
>>>      
>>>
>>    # To rule out simulation uncertainty
>>       Pearson's Chi-squared test with simulated p-value (based on 200000
>>       replicates)
>>    
>>
>
>What is being simulated? Sorry for my ignorance. Is it the same as below?
>  
>
It is the same as below, see the reference in ?chisq.test
Under the null hypothesis of   no association, tables are sampled 
conditional on the marginals. So it can be seen as a simulated version 
of Fisher's exact test.
But even if fisher.test fail, we can calculate it by hand:
 > test <- matrix( c(1,506, 13714, 878702), 2,2)
 > fisher.test(test)
Error in fisher.test(test) : FEXACT error 40.

r-core: fisher.test maybe should special-case the 2 x 2 case?

Out of workspace.
 > test
     [,1]   [,2]
[1,]    1  13714
[2,]  506 878702
 > # we can calculate the exact fisher test "by hand":
 > sum(dhyper(0:1, 507, 892416, 13715))
[1] 0.003473946
# for a one-sided p-value

>
>  
>
>>data:  test
>>X-squared = 6.0115, df = NA, p-value = 0.01634
>>
>>    
>>
>>>N <- sum(test)
>>>rows <- rowSums(test)
>>>cols <- colSums(test)
>>>E <- rows %o% cols/N
>>>E
>>>      
>>>
>>          [,1]      [,2]
>>[1,]   7.787351  13707.21
>>[2,] 499.212649 878708.79
>>    
>>
>>None of the expected'eds are lesser than 5, an often  used rule of thumb 
>>(which might even be
>>to conservative. Just to check on the distribution:
>>    
>>
>
>OK, we are 'allowed' to use an approximation, but what am I approximating,
>and how does this relate to fishers exact test for 2x2 table?
>  
>

You are approximating a p-value. relation to Fisher's exact test, se 
above, but note that there is a small difference:
chisq.test measures distances fdrom null hypothesis by chisquared 
distance, while Fisher exact are ordering the
tables directly by the (1,1) value. Below we do a simulation to study 
the difference, which shuold'nt be large in
the 2*2 case.

>
>  
>
>>rows <- round(rows)
>>cols <- round(cols)
>>    
>>
>
>The above dosn't do anything in this example, did you mean to do 
>
>rows <- rowSums(E)
>cols <- colSums(E)
>
>first?
>
>  
>
yes.

>>>pvals <- sapply(r2dtable(100000, rows, cols), function(x) 
>>>      
>>>
>>chisq.test(x)$p.value)
>>    
>>
>>>hist(pvals)
>>>      
>>>
>>    # not very good approximation to uniform histogram, but:
>>    
>>
>>>sum(pvals < 0.05)/100000
>>>      
>>>
>>[1] 0.03068
>>    
>>
>>>sum(pvals < 0.01)/100000
>>>      
>>>
>>[1] 0.00669
>>    
>>
>
>Above you simulate fishers exact test by making permutations on the given
>table and measuring the probability of the resulting table with the same
>marginals. You then count how much of this probability is more extreem
>than standard critical values to see how similar to the chisq distribution
>this simultion data is? Why should the probabilites above be 'uniform' as
>you state?
>  
>

Because p-values under the null hypothesis should be distributed 
uniformly on (0,1). So testing the uniformity
is a way of testing quality of approximation.

Redoing the simulation under the null to get some more information:

 > res <- sapply(r2dtable(100000, rows, cols), function(x) {
           chi <- chisq.test(x)
           OR <- x[1,1]*x[2,2]/(x[2,1]*x[1,2])
           T <- x[1,1]
           c(chi$statistic, chi$p.value, OR, T) } )
 > test[1,1]*test[2,2]/(test[1,2]*test[2,1])
[1] 0.1266272
 > dim(res)
[1]      4 100000
 > hist(res[1,])
 > # compare this with chisq dist
 > hist(res[2,])
 > # a bit discrete, but not far from uniform
 > hist(res[3,])
 > sum(res[3,]<0.1266272)/100000
[1] 0.00353
 > #very close to the hypergeometric calculation
 > hist(res[4,])



>>So the true levels are not very far from the calculated by te chisq 
>>approximation, and it seems safe to use it.
>>
>>All of this to show that with R you are not anymore dependent on old 
>>"rules os thumb",
>>you can investigate for yourself.
>>    
>>
>
>Thanks very much for your explainations and guedeance, but it is my
>exploration that is driving me mad :( I feel like I am running in circles.
>
>Can you explain this for example...
>
>dat <- matrix(c(500,560,1000,1000),nr=2)
>
>
>prop.test(dat)$p.value
>[1] 0.1464539
>  
>
>>chisq.test(dat)$p.value
>>    
>>
>[1] 0.1464539
>  
>
>>fisher.test(dat)$p.value
>>    
>>
>[1] 0.1385046
>
>Is the difference of 0.0079493 OK
>
>Now...
>
>  
>

Well, the dstribution theory used are different so you should not expect 
exactly the same answer.

>>dat <- matrix(c(5,7,10,10),nr=2)
>>prop.test(dat)$p.value
>>    
>>
>[1] 0.9271224
>  
>
>>chisq.test(dat)$p.value
>>    
>>
>[1] 0.9271224
>  
>
>>fisher.test(dat)$p.value
>>    
>>
>[1] 0.7256694
>
>The expecteds for the above are all above 5, but we get a difference of
>0.201453
>  
>
I guess the base for the rule of five is to get the same decision at a 
significance level of 0.05, and you do get that.

>You may say (just use fisher) above, but then i am back where I started,
>when should I use / not use fisher.
>
>Why can't I use an exact multinomial distribution? (how?)
>  
>

See above.

>sorry for my total confusion,
>thanks again,
>Dan.
>
>  
>
You can also use glm to fit logistic regression  to estimate the lo odds 
ratio, and use likelihood profiling
to get a confidence interval:

 > confint(glm(test ~ 1, family=binomial))
Waiting for profiling to be done...
    2.5 %    97.5 %
-7.561529 -7.387353


so the null value of one is rather fare from the confint....

If your only interest is in testing the null of odds ratio = 1, the 
conclusion 'reject'
seems to be robust enough!

>  
>
>>Kjetil
>>    
>>

-- 

Kjetil Halvorsen.

Peace is the most effective weapon of mass construction.
               --  Mahdi Elmandjra



From andy_liaw at merck.com  Sun Oct 10 23:57:24 2004
From: andy_liaw at merck.com (Liaw, Andy)
Date: Sun, 10 Oct 2004 17:57:24 -0400
Subject: [R] R 2.0.0 not suffisantly reliable to be be used
Message-ID: <3A822319EB35174CA3714066D590DCD504AF8504@usrymx25.merck.com>

> From: Fan
> 
> After wasting one whole day, I've finally decided to stay with 1.9.1,
> some problems have been reported to R-Bugs.
> 
> For occasional users, I would say, there's no worst thing than that:
> you installed the new release, and soem of your existing codes no
> longer work !

Given the attitude that you've taken, I'd guess you do not deserve the
improvements introduced in R-2.0.0.  If you are at all serious about having
your code working with R-2.0.0, you would have tested it in the alpha/beta
cycle, and try to resolve it before the official release, rather than
ranting about it after the fact.  Most people know better, and will not
appreciate how you denigrate R-core's effort.

Andy

ps: If it's too hard for you to learn to spell, at least learn to use a
spell checker.

> --
> Fan
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
> 
>



From xiao.gang.fan1 at libertysurf.fr  Mon Oct 11 01:16:55 2004
From: xiao.gang.fan1 at libertysurf.fr (Fan)
Date: Mon, 11 Oct 2004 01:16:55 +0200
Subject: [R] R 2.0.0 is not suffisantly reliable to be used
References: <3A822319EB35174CA3714066D590DCD504AF8504@usrymx25.merck.com>
Message-ID: <4169C2E7.90803@libertysurf.fr>

WHAT CAN I DO FOR YOU, SIR ?

Perhaps you're expecting some sort of congratulations for that new
release ?  I've taken time to download it, test it, and
I've said what I've got to say.

You're helping nobody with that sort of "process of intention".
--
Fan

Liaw, Andy wrote:
>>From: Fan
>>
>>After wasting one whole day, I've finally decided to stay with 1.9.1,
>>some problems have been reported to R-Bugs.
>>
>>For occasional users, I would say, there's no worst thing than that:
>>you installed the new release, and soem of your existing codes no
>>longer work !
> 
> 
> Given the attitude that you've taken, I'd guess you do not deserve the
> improvements introduced in R-2.0.0.  If you are at all serious about having
> your code working with R-2.0.0, you would have tested it in the alpha/beta
> cycle, and try to resolve it before the official release, rather than
> ranting about it after the fact.  Most people know better, and will not
> appreciate how you denigrate R-core's effort.
> 
> Andy
> 
> ps: If it's too hard for you to learn to spell, at least learn to use a
> spell checker.
> 
> 
>>--
>>Fan
>>
>>______________________________________________
>>R-help at stat.math.ethz.ch mailing list
>>https://stat.ethz.ch/mailman/listinfo/r-help
>>PLEASE do read the posting guide! 
>>http://www.R-project.org/posting-guide.html
>>
>>
> 
> 
> 
> ------------------------------------------------------------------------------
> Notice:  This e-mail message, together with any attachments, contains information of Merck & Co., Inc. (One Merck Drive, Whitehouse Station, New Jersey, USA 08889), and/or its affiliates (which may be known outside the United States as Merck Frosst, Merck Sharp & Dohme or MSD and in Japan, as Banyu) that may be confidential, proprietary copyrighted and/or legally privileged. It is intended solely for the use of the individual or entity named on this message.  If you are not the intended recipient, and have received this message in error, please notify us immediately by reply e-mail and then delete it from your system.
> ------------------------------------------------------------------------------
>



From kbartz at loyaltymatrix.com  Mon Oct 11 01:27:06 2004
From: kbartz at loyaltymatrix.com (Kevin Bartz)
Date: Sun, 10 Oct 2004 16:27:06 -0700
Subject: [R] R 2.0.0 is not suffisantly reliable to be used
In-Reply-To: <4169C2E7.90803@libertysurf.fr>
References: <3A822319EB35174CA3714066D590DCD504AF8504@usrymx25.merck.com>
	<4169C2E7.90803@libertysurf.fr>
Message-ID: <4169C54A.4080609@loyaltymatrix.com>

Fan wrote:
> WHAT CAN I DO FOR YOU, SIR ?
> 
> Perhaps you're expecting some sort of congratulations for that new
> release ?  I've taken time to download it, test it, and
> I've said what I've got to say.
> 
> You're helping nobody with that sort of "process of intention".
> -- 
> Fan
> 
> Liaw, Andy wrote:
> 
>>> From: Fan
>>>
>>> After wasting one whole day, I've finally decided to stay with 1.9.1,
>>> some problems have been reported to R-Bugs.
>>>
>>> For occasional users, I would say, there's no worst thing than that:
>>> you installed the new release, and soem of your existing codes no
>>> longer work !
>>
>>
>>
>> Given the attitude that you've taken, I'd guess you do not deserve the
>> improvements introduced in R-2.0.0.  If you are at all serious about 
>> having
>> your code working with R-2.0.0, you would have tested it in the 
>> alpha/beta
>> cycle, and try to resolve it before the official release, rather than
>> ranting about it after the fact.  Most people know better, and will not
>> appreciate how you denigrate R-core's effort.
>>
>> Andy
>>
>> ps: If it's too hard for you to learn to spell, at least learn to use a
>> spell checker.
>>
>>
>>> -- 
>>> Fan
>>>
>>> ______________________________________________
>>> R-help at stat.math.ethz.ch mailing list
>>> https://stat.ethz.ch/mailman/listinfo/r-help
>>> PLEASE do read the posting guide! 
>>> http://www.R-project.org/posting-guide.html
>>>
>>>
>>
>>
>>
>> ------------------------------------------------------------------------------ 
>>
>> Notice:  This e-mail message, together with any attachments, contains 
>> information of Merck & Co., Inc. (One Merck Drive, Whitehouse Station, 
>> New Jersey, USA 08889), and/or its affiliates (which may be known 
>> outside the United States as Merck Frosst, Merck Sharp & Dohme or MSD 
>> and in Japan, as Banyu) that may be confidential, proprietary 
>> copyrighted and/or legally privileged. It is intended solely for the 
>> use of the individual or entity named on this message.  If you are not 
>> the intended recipient, and have received this message in error, 
>> please notify us immediately by reply e-mail and then delete it from 
>> your system.
>> ------------------------------------------------------------------------------ 
>>
>>
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
> 

Hey Fan, sorry to hear that the latest version isn't working for you. 
You know, maybe I can help... what specifically were the problems that 
emerged when you installed the new version? I'd be happy to try to 
diagnose the lines or commands that are now giving you errors. I have a 
vast amount of existing code that ported with very few problems from 
1.9.1 to 2.0.0. I think it would be very informative for everyone--and 
you may get some quality answers--if you posted the lines that once 
worked but now bomb out.

Kevin



From spencer.graves at pdf.com  Mon Oct 11 05:06:42 2004
From: spencer.graves at pdf.com (Spencer Graves)
Date: Sun, 10 Oct 2004 20:06:42 -0700
Subject: [R] some help interpreting ANOVA results, please?
In-Reply-To: <20041010195541.5f05817f@portia.local>
References: <20041010183749.2f535f68@portia.local>	<Pine.LNX.4.44.0410101753210.29146-100000@gannet.stats>
	<20041010195541.5f05817f@portia.local>
Message-ID: <4169F8C2.60001@pdf.com>



RenE J.V. Bertin wrote:

>...
>
>	Would you have any suggestions for an accessible text explaining lme, apart from MASS-4?
>
      The standard reference on lme is Pinheiro and Bates (2000) 
Mixed-Effects Models in S and S-Plus (Springer).  I have found this book 
very valuable (as I have Bates' other book on nonlinear regression with 
Don Watts). 

      hope this helps.  spencer graves



From lauraholt_983 at hotmail.com  Mon Oct 11 08:06:22 2004
From: lauraholt_983 at hotmail.com (Laura Holt)
Date: Mon, 11 Oct 2004 01:06:22 -0500
Subject: [R] Puzzled on nlm
Message-ID: <BAY12-F32IkyPzGNsok0004498c@hotmail.com>

Dear R People:

Here is a function to minimized:
>mfun1
function(x,a) {
  x[1] <- a[1]*x[2] + a[3] - a[2]*(a[1]-a[2])*a[3]
  x[2] <- a[1]*x[1] - a[2]*a[3]
  return(x)
}

Here is my first try:
>nlm(mfun1,c(1,1))
Error in f(x, ...) : Argument "a" is missing, with no default
>

>nlm(mfun1,c(1,1),a=c(0.8,0.5,1))
Error in nlm(mfun1, c(1, 1), a = c(0.8, 0.5, 1)) :
        invalid function value in 'nlm' optimizer
>

I am stumped.  I'm sure that it's something really small that I'm 
overlooking.

Thanks in advance for any help.

Sincerely,
Laura Holt
mailto: lauraholt_983 at hotmail.com



From lauraholt_983 at hotmail.com  Mon Oct 11 08:15:36 2004
From: lauraholt_983 at hotmail.com (Laura Holt)
Date: Mon, 11 Oct 2004 01:15:36 -0500
Subject: [R] please ignore question about nlm
Message-ID: <BAY12-F1nAKYoo9tjxq0001a9cd@hotmail.com>

sorry for the previous message

an attack of the stupids.

Thanks,
Laura



From ligges at statistik.uni-dortmund.de  Mon Oct 11 08:23:30 2004
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Mon, 11 Oct 2004 08:23:30 +0200
Subject: [R] Puzzled on nlm
In-Reply-To: <BAY12-F32IkyPzGNsok0004498c@hotmail.com>
References: <BAY12-F32IkyPzGNsok0004498c@hotmail.com>
Message-ID: <416A26E2.4050403@statistik.uni-dortmund.de>

Laura Holt wrote:
> Dear R People:
> 
> Here is a function to minimized:
> 
>> mfun1
> 
> function(x,a) {
>  x[1] <- a[1]*x[2] + a[3] - a[2]*(a[1]-a[2])*a[3]
>  x[2] <- a[1]*x[1] - a[2]*a[3]
>  return(x)
> }
> 
> Here is my first try:
> 
>> nlm(mfun1,c(1,1))
> 
> Error in f(x, ...) : Argument "a" is missing, with no default
> 
>>
> 
>> nlm(mfun1,c(1,1),a=c(0.8,0.5,1))
> 
> Error in nlm(mfun1, c(1, 1), a = c(0.8, 0.5, 1)) :
>        invalid function value in 'nlm' optimizer

You function returns a vector of length two, but nlm() expects length 
one (what is the minimum, otherwise?).

Uwe Ligges


>>
> 
> I am stumped.  I'm sure that it's something really small that I'm 
> overlooking.
> 
> Thanks in advance for any help.
> 
> Sincerely,
> Laura Holt
> mailto: lauraholt_983 at hotmail.com
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html



From ripley at stats.ox.ac.uk  Mon Oct 11 08:48:13 2004
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Mon, 11 Oct 2004 07:48:13 +0100 (BST)
Subject: [R] R 2.0.0 not suffisantly reliable to be be used
In-Reply-To: <41695F4C.4090203@libertysurf.fr>
Message-ID: <Pine.LNX.4.44.0410101727370.29146-100000@gannet.stats>

[I wrote this yesterday, but decided not to send it.  In the light of 
subsequent correspondence I do now.]

On Sun, 10 Oct 2004, Fan wrote:

> After wasting one whole day, I've finally decided to stay with 1.9.1,
> some problems have been reported to R-Bugs.

Four reports have been received from Fan, one of which was a duplicate of
another.  Two had already been replied to, and another reply has crossed
this message.  The details can all be seen on R-bugs or in the R-devel 
archive.

PR#7274 is a question answered in the first section of the NEWS file,
`USER-VISIBLE CHANGES', and also in the rw-FAQ.  PR#7275 is incorrect code
that also fails in 1.9.1 (and even 1.8.0) despite Fan's claim to the
contrary.  PR#7272/3 contains one small point (not involving code) that as
far as we can tell is already in the archive (and if so was described
incorrectly) and is already fixed in R-patched, and another that has no
code to reproduce it but appears to be a previously untested problem in
his datasets (as also mentioned in the top section of the NEWS file).

> For occasional users, I would say, there's no worst thing than that:
> you installed the new release, and soem of your existing codes no
> longer work !

This is 2.0.0, a major release, so one might expect some incompatibility
(and there are a few points documented early in the NEWS file).  In
particular we do document that packages must be reinstalled, and that we
do check more stringently that packages work at both install and load
times.  But large amounts of code and several hundred packages have been
tested under 2.0.0, and some of the performance benefits come from not 
checking if packages are correctly installed at run time.

There have so far been very few verifiable reports of bugs introduced in
2.0.0 (indeed fewer than I can recall in the first week for any 1.x.0
release) and all of those have already been fixed in R-patched.  (The only
ones I could find are R CMD INSTALL with versioned installs on packages
which save images, setting "bg" on Windows devices and failing to find
aliases in Windows CHM help.)  Compare that with the list of bugs that
have been fixed since 1.9.1 (and even of earlier bugs fixed in R-patched).
It would be even better if more users (especially Windows users) would 
help with alpha/beta testing new releases -- bugs tend to more prevalent 
in things which can only be tested interactively.

(Unfortunately due to repeated SVN problems, R-patched is not as easy to
get hold of as it is intended to be.  We do have up
ftp://ftp.stat.math.ethz.ch/Software/R/R-patched.tar.bz2, a source tarball
from early this morning, and
http://cran.r-project.org/bin/windows/base/rpatched.html has a Windows
build from Saturday.)

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From ripley at stats.ox.ac.uk  Fri Oct  1 17:28:59 2004
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Fri, 1 Oct 2004 16:28:59 +0100 (BST)
Subject: [R] [R-pkgs] gregmisc 2.0.0 release
In-Reply-To: <D7A3CFD7825BD6119B880002A58F06C20C521D04@groexmb02.pfizer.com>
Message-ID: <Pine.LNX.4.44.0410011624410.23508-100000@gannet.stats>

A small practical point: you need to remove.packages("gregmisc") manually
if you update.packages to this version, or you will be continually asked
to update the gremisc package which no longer exists.

No one has previously ever changed a package into a bundle not containing
a package of the same name, so the update software is not smart enough to
remove the old gregmisc *package* when installing the bundle.  (It will
probably continue to be not smart enough in future.)

On Fri, 1 Oct 2004, Warnes, Gregory R wrote:

> 
> gregmisc 2.0.0
> ===========
> 
> gregmisc 2.0.0 has been released and is now available on CRAN.   
> 
> Important Changes
> ---------------------------
> 
> - Now a package bundle 
> 
> For this release, gregmisc has been split into a bundle containing 4
> separate packages: gdata, gmodels, gplots and gtools. All of your favorite
> functions are still present, but they are now better organized into thematic
> groups, which should make them easier to maintain.

...

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From petr.pikal at precheza.cz  Mon Oct 11 09:01:44 2004
From: petr.pikal at precheza.cz (Petr Pikal)
Date: Mon, 11 Oct 2004 09:01:44 +0200
Subject: [R] which() and value replacement in a matrix
In-Reply-To: <005f01c4ae0a$44331180$d5f00e50@PC728329681112>
Message-ID: <416A4BF8.2459.18D694@localhost>



On 9 Oct 2004 at 16:14, Patrick Giraudoux wrote:

> Thanks for the clue.
> 
> Actually the trouble comes when refering to a data.frame. If I use the
> matrix from the data.frame (matrix(mydataframe)), everything goes
> smoothly...
> 
> So I wrote:
> 
> indices<-which(myforetbin > 0,arr.ind=T)
> myforetbin<-as.matrix(myforetbin)
> myforetbin[indices]<-1
> myforetbin<-as.data.frame(myforetbin)
> 
> It works but I wonder if there are no more simple ways...

Hi.

Well, did you try directly:

indices<-myforetbin > 0
myforetbin[indices]<-1

It works for me with artificial data frame.

Cheers
Petr






> 
> All the best,
> 
> Patrick
> 
> 
> ----- Original Message ----- 
> From: "Liaw, Andy" <andy_liaw at merck.com>
> To: "'Patrick Giraudoux'" <patrick.giraudoux at univ-fcomte.fr>; "r-help"
> <r-help at stat.math.ethz.ch> Sent: Saturday, October 09, 2004 3:00 PM
> Subject: RE: [R] which() and value replacement in a matrix
> 
> 
> > Use the index vector directly, rather than breaking it up:
> >
> > > x <- matrix(sample(30), 10, 3)
> > > idx <- which(x > 25, arr.ind=TRUE)
> > > idx
> >      row col
> > [1,]   6   1
> > [2,]   9   1
> > [3,]   4   2
> > [4,]   6   2
> > [5,]   4   3
> > > x[idx] <- 999
> > > x
> >       [,1] [,2] [,3]
> >  [1,]    7   14   16
> >  [2,]   20   24    8
> >  [3,]   17   18   11
> >  [4,]   19  999  999
> >  [5,]   23    4   15
> >  [6,]  999  999    5
> >  [7,]   21    9   12
> >  [8,]   22    2    3
> >  [9,]  999   13    1
> > [10,]    6   10   25
> >
> > HTH,
> > Andy
> >
> > > From: Patrick Giraudoux
> > >
> > > Hi,
> > >
> > > I cannot go through the archives with which() as key-word...
> > > so common. Though I am sure to have seen something about this
> > > subject in the past could somebody put me on the track. I have a
> > > matrix (actually a data.frame) in which I would replace the
> > > non-null values by 1.
> > >
> > > I tried the following:
> > >
> > > indices<-which(myforetbin > 0,arr.ind=T)
> > > myforetbin[indices[,1],indices[,2]]<-1
> > >
> > > and get the message:
> > >
> > > > myforetbin[indices[,1],indices[,2]]<-1
> > > Error in "[<-.data.frame"(`*tmp*`, indices[, 1], indices[,
> > > 2], value = 1) :
> > >         duplicate subscripts for columns
> > >
> > > I get the same with
> > >
> > > myforetbin[indices]<-1
> > >
> > > However, with:
> > >
> > > myforetbin[indices]
> > >
> > > I well get a vector with the corresponding non-null values.
> > >
> > > Can somebody put me on the track?
> > >
> > > All the best,
> > >
> > > Patrick
> > >
> > > ______________________________________________
> > > R-help at stat.math.ethz.ch mailing list
> > > https://stat.ethz.ch/mailman/listinfo/r-help
> > > PLEASE do read the posting guide!
> > > http://www.R-project.org/posting-guide.html
> > >
> > >
> >
> >
> > --------------------------------------------------------------------
> > ---------- Notice:  This e-mail message, together with any
> > attachments, contains information of Merck & Co., Inc. (One Merck
> > Drive,
> Whitehouse Station, New Jersey, USA 08889), and/or its affiliates
> (which may be known outside the United States as Merck Frosst, Merck
> Sharp & Dohme or MSD and in Japan, as Banyu) that may be confidential,
> proprietary copyrighted and/or legally privileged. It is intended
> solely for the use of the individual or entity named on this message. 
> If you are not the intended recipient, and have received this message
> in error, please notify us immediately by reply e-mail and then delete
> it from your system. >
> ----------------------------------------------------------------------
> --------
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide!
> http://www.R-project.org/posting-guide.html

Petr Pikal
petr.pikal at precheza.cz



From Ted.Harding at nessie.mcc.ac.uk  Mon Oct 11 09:48:43 2004
From: Ted.Harding at nessie.mcc.ac.uk ( (Ted Harding))
Date: Mon, 11 Oct 2004 08:48:43 +0100 (BST)
Subject: [R] R 2.0.0 not suffisantly reliable to be be used
In-Reply-To: <3A822319EB35174CA3714066D590DCD504AF8504@usrymx25.merck.com>
Message-ID: <XFMail.041011084843.Ted.Harding@nessie.mcc.ac.uk>

On 10-Oct-04 Liaw, Andy wrote:
>> From: Fan
>> 
>> After wasting one whole day, I've finally decided to stay with 1.9.1,
>> some problems have been reported to R-Bugs.
>> 
>> For occasional users, I would say, there's no worst thing than that:
>> you installed the new release, and soem of your existing codes no
>> longer work !
> 
> [...]
> ps: If it's too hard for you to learn to spell, at least learn to use a
> spell checker.

Dear Andy,
I think the signs are that Fan knows how to spell, really; but perhaps
(as in my case) while the spitir is willing the felsh is weak ...
Ted.


--------------------------------------------------------------------
E-Mail: (Ted Harding) <Ted.Harding at nessie.mcc.ac.uk>
Fax-to-email: +44 (0)870 094 0861  [NB: New number!]
Date: 11-Oct-04                                       Time: 08:48:43
------------------------------ XFMail ------------------------------



From e.leuven at uva.nl  Mon Oct 11 13:20:28 2004
From: e.leuven at uva.nl (Edwin Leuven)
Date: Mon, 11 Oct 2004 13:20:28 +0200 (CEST)
Subject: [R] question on function argument
Message-ID: <Pine.LNX.4.61.0410111205480.13907@e182104.fee.uva.nl>

dear all,

i've looked at the r-intro (chapter 10, writing your own functions) 
and searched the r-help archives but am still stuck at the following.

i have a simple function, something like:

myhist<-function(yvar) {
  y<-subset(myframe,yvar>1 & yvar<=150000,select=yvar)
  attach(y)
  hist(yvar)
}


calling it as follows:

myhist(x1)

gives the following error:

Error in attach(y) : attempt to set an attribute on NULL



what am i doing wrong here?

thanks in advance, edwin



From dmb at mrc-dunn.cam.ac.uk  Mon Oct 11 13:40:44 2004
From: dmb at mrc-dunn.cam.ac.uk (Dan Bolser)
Date: Mon, 11 Oct 2004 12:40:44 +0100 (BST)
Subject: [R] hclust title and paste - messed up
Message-ID: <Pine.LNX.4.21.0410111234170.25948-100000@mail.mrc-dunn.cam.ac.uk>


I use the following code to scan a (limited) parameter space of clustering
strategies ...

data <- read.table(... 

dataTranspose <- t(data)

distMeth <- c("euclidean",
              "maximum",
              "manhattan",
              "canberra",
              "binary"
              )

clustMeth <- c("ward",
             "single",
             "complete",
             "average",
             "mcquitty",
             "median",
             "centroid"
             )

par(ask=TRUE)

for (d in distMeth){
  print(d)
  dt <- dist(dataTranspose,method=d)
  for (m in clustMeth){
    print(m)
    hc <- hclust(dt,method=m)
    plot(hc,
         main=paste(c(
           "Distance Measure", d,
           "Cluster Method", m),sep=" "),
         xlab='',
         sub=''
         )
  }
}


However, my plot title (main) has 4 lines, when I think it should only
have one line.

What am I doing wrong?



From p.dalgaard at biostat.ku.dk  Mon Oct 11 14:03:20 2004
From: p.dalgaard at biostat.ku.dk (Peter Dalgaard)
Date: 11 Oct 2004 14:03:20 +0200
Subject: [R] question on function argument
In-Reply-To: <Pine.LNX.4.61.0410111205480.13907@e182104.fee.uva.nl>
References: <Pine.LNX.4.61.0410111205480.13907@e182104.fee.uva.nl>
Message-ID: <x2lled7987.fsf@biostat.ku.dk>

Edwin Leuven <e.leuven at uva.nl> writes:

> dear all,
> 
> i've looked at the r-intro (chapter 10, writing your own functions)
> and searched the r-help archives but am still stuck at the following.
> 
> i have a simple function, something like:
> 
> myhist<-function(yvar) {
>   y<-subset(myframe,yvar>1 & yvar<=150000,select=yvar)
>   attach(y)
>   hist(yvar)
> }
> 
> 
> calling it as follows:
> 
> myhist(x1)
> 
> gives the following error:
> 
> Error in attach(y) : attempt to set an attribute on NULL
> 
> 
> 
> what am i doing wrong here?

(1) Insufficient information in problem report (what is "myframe"?)

Assuming that myframe is a data.frame containing a variable called
"x1":

(2) Believing that function calls are macros and
(3) Not noticing that subset() has nonstandard argument handling

The construct y<-subset(myframe,yvar>1 & yvar<=150000,select=yvar)
looks inside "myframe" for a variable called "yvar". If there is no
"yvar" in the data frame, then it will search the calling environment,
which might be OK for the "subset" argument, but will almost surely go
wrong for "select". (Read the source for subset.data.frame for the
full story.)

I think that what you thought you were doing is more like

eval(substitute({
   y<-subset(myframe,yvar>1 & yvar<=150000,select=yvar)
   attach(y)
   hist(yvar)
}), list(yvar=quote(x1)))

or maybe

 myhist<-function(yvar) eval.parent(substitute({
   y<-subset(myframe,yvar>1 & yvar<=150000,select=yvar)
   attach(y)
   hist(yvar)
 }))

In either case, you'd probably want a detach() in there as well.

-- 
   O__  ---- Peter Dalgaard             Blegdamsvej 3  
  c/ /'_ --- Dept. of Biostatistics     2200 Cph. N   
 (*) \(*) -- University of Copenhagen   Denmark      Ph: (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)             FAX: (+45) 35327907



From gregoire.thomas at ugent.be  Mon Oct 11 14:12:47 2004
From: gregoire.thomas at ugent.be (Gregoire Thomas)
Date: Mon, 11 Oct 2004 14:12:47 +0200
Subject: [R] hclust title and paste - messed up
In-Reply-To: <Pine.LNX.4.21.0410111234170.25948-100000@mail.mrc-dunn.cam.ac.uk>
References: <Pine.LNX.4.21.0410111234170.25948-100000@mail.mrc-dunn.cam.ac.uk>
Message-ID: <1097496767.2518.27.camel@localhost.localdomain>

paste(c("a","b","c"),collapse="")
paste("a","b","c",sep="")

On Mon, 2004-10-11 at 13:40, Dan Bolser wrote:
> I use the following code to scan a (limited) parameter space of clustering
> strategies ...
> 
> data <- read.table(... 
> 
> dataTranspose <- t(data)
> 
> distMeth <- c("euclidean",
>               "maximum",
>               "manhattan",
>               "canberra",
>               "binary"
>               )
> 
> clustMeth <- c("ward",
>              "single",
>              "complete",
>              "average",
>              "mcquitty",
>              "median",
>              "centroid"
>              )
> 
> par(ask=TRUE)
> 
> for (d in distMeth){
>   print(d)
>   dt <- dist(dataTranspose,method=d)
>   for (m in clustMeth){
>     print(m)
>     hc <- hclust(dt,method=m)
>     plot(hc,
>          main=paste(c(
>            "Distance Measure", d,
>            "Cluster Method", m),sep=" "),
>          xlab='',
>          sub=''
>          )
>   }
> }
> 
> 
> However, my plot title (main) has 4 lines, when I think it should only
> have one line.
> 
> What am I doing wrong?
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html



From p.dalgaard at biostat.ku.dk  Mon Oct 11 14:13:08 2004
From: p.dalgaard at biostat.ku.dk (Peter Dalgaard)
Date: 11 Oct 2004 14:13:08 +0200
Subject: [R] hclust title and paste - messed up
In-Reply-To: <Pine.LNX.4.21.0410111234170.25948-100000@mail.mrc-dunn.cam.ac.uk>
References: <Pine.LNX.4.21.0410111234170.25948-100000@mail.mrc-dunn.cam.ac.uk>
Message-ID: <x2d5zp78rv.fsf@biostat.ku.dk>

Dan Bolser <dmb at mrc-dunn.cam.ac.uk> writes:

>          main=paste(c(
>            "Distance Measure", d,
>            "Cluster Method", m),sep=" "),
>          xlab='',
>          sub=''
>          )
....
> However, my plot title (main) has 4 lines, when I think it should only
> have one line.
> 
> What am I doing wrong?

Oops. Ignore previous post and just get rid of the c() construct.

-- 
   O__  ---- Peter Dalgaard             Blegdamsvej 3  
  c/ /'_ --- Dept. of Biostatistics     2200 Cph. N   
 (*) \(*) -- University of Copenhagen   Denmark      Ph: (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)             FAX: (+45) 35327907



From e.leuven at uva.nl  Mon Oct 11 14:33:19 2004
From: e.leuven at uva.nl (Edwin Leuven)
Date: Mon, 11 Oct 2004 14:33:19 +0200 (CEST)
Subject: [R] question on function argument
In-Reply-To: <x2lled7987.fsf@biostat.ku.dk>
References: <Pine.LNX.4.61.0410111205480.13907@e182104.fee.uva.nl>
	<x2lled7987.fsf@biostat.ku.dk>
Message-ID: <Pine.LNX.4.61.0410111422140.13907@e182104.fee.uva.nl>

> (1) Insufficient information in problem report (what is "myframe"?)
>
> Assuming that myframe is a data.frame containing a variable called
> "x1":

correct

> (2) Believing that function calls are macros and

you must be right. from the examples in the r-intro i was under the 
impression that R substitutes the value of the arguments in the 
expression in the function body.

but apparently this is not happening (although i only noticed now).

is there a document which explains more in detail what does happen 
when functions are evaluated? because...

> (3) Not noticing that subset() has nonstandard argument handling

...otherwise there might be unexpected behavior...


> I think that what you thought you were doing is more like
>
> myhist<-function(yvar) eval.parent(substitute({
>   y<-subset(myframe,yvar>1 & yvar<=150000,select=yvar)
>   attach(y)
>   hist(yvar)
> }))

this is indeed the one

> In either case, you'd probably want a detach() in there as well.

yes, thanks for your help!

regards, edwin



From andy_liaw at merck.com  Mon Oct 11 14:38:29 2004
From: andy_liaw at merck.com (Liaw, Andy)
Date: Mon, 11 Oct 2004 08:38:29 -0400
Subject: [R] question on function argument
Message-ID: <3A822319EB35174CA3714066D590DCD504AF8509@usrymx25.merck.com>

> From: Edwin Leuven
> 
> is there a document which explains more in detail what does happen 
> when functions are evaluated? 

The `R Language Definition' manual has a chapter `Functions' with a section
`Evaluation' (Sec. 4.3 in the PDF version).  That might be helpful for you.
Also, Thomas Lumley wrote a `Programmer's Niche' column in the R News on how
to get the effect of macros in R.  You might be interested in that, too.

Cheers,
Andy



From Mike.Prager at noaa.gov  Mon Oct 11 15:44:24 2004
From: Mike.Prager at noaa.gov (Mike Prager)
Date: Mon, 11 Oct 2004 09:44:24 -0400
Subject: [R] R 2.0.0 not suffisantly reliable to be be used
In-Reply-To: <Pine.LNX.4.44.0410101727370.29146-100000@gannet.stats>
References: <41695F4C.4090203@libertysurf.fr>
	<Pine.LNX.4.44.0410101727370.29146-100000@gannet.stats>
Message-ID: <6.1.2.0.0.20041011092436.018fae48@hermes.nos.noaa.gov>


>On Sun, 10 Oct 2004, Fan wrote:
>
> > For occasional users, I would say, there's no worst thing than that:
> > you installed the new release, and soem of your existing codes no
> > longer work !

There is a close parallel here.  On comp.lang.fortran, a frequent problem 
is that existing code doesn't work with a new compiler. Such complaints are 
often from occasional Fortran users, those who have inherited code from 
others, or those who have been using a single compiler for 20 years and 
have finally updated.

In the vast majority of cases, the code in question is not standard 
conforming.  Either (1) the old compiler ignored various syntax errors, (2) 
it gave the right answer by luck (i.e., the programmer made assumptions 
that the compiler met, but that Fortran in general does not meet), or (3) 
the old release gave the wrong answers, which were never checked.

So, in response to Fan's assertion, I say:  NO!  It's often a good thing 
when existing codes no longer work.  That provides an opportunity to fix 
them. The worst thing is to have bad code run and give the wrong answer for 
the question (you think) you're asking.


-- 
Michael Prager, Ph.D.                <Mike.Prager at noaa.gov>
NOAA Beaufort Laboratory
Beaufort, North Carolina  28516
http://shrimp.ccfhrb.noaa.gov/~mprager/
***



From luk111111 at yahoo.com  Mon Oct 11 15:45:10 2004
From: luk111111 at yahoo.com (lu kan)
Date: Mon, 11 Oct 2004 06:45:10 -0700 (PDT)
Subject: [R] R-1.9.1 compiling problem
In-Reply-To: <3A822319EB35174CA3714066D590DCD504AF8509@usrymx25.merck.com>
Message-ID: <20041011134510.91636.qmail@web61301.mail.yahoo.com>

I downloaded R-1.9.1.tgz, and tried to install it on
debian linux machine according to the installation
commands
./configure
make

but I got the following error message. Could you
please advise me what is the reason for this. 


_____________

make[1]: Entering directory
`/home1/lu/usr/tmp1/R-1.9.1/m4'
make[1]: Nothing to be done for `R'.
make[1]: Leaving directory
`/home1/lu/usr/tmp1/R-1.9.1/m4'
make[1]: Entering directory
`/home1/lu/usr/tmp1/R-1.9.1/tools'
make[1]: Nothing to be done for `R'.
make[1]: Leaving directory
`/home1/lu/usr/tmp1/R-1.9.1/tools'
make[1]: Entering directory
`/home1/lu/usr/tmp1/R-1.9.1/afm'
make[1]: Leaving directory
`/home1/lu/usr/tmp1/R-1.9.1/afm'
make[1]: Entering directory
`/home1/lu/usr/tmp1/R-1.9.1/doc'
make[2]: Entering directory
`/home1/lu/usr/tmp1/R-1.9.1/doc/html'
make[3]: Entering directory
`/home1/lu/usr/tmp1/R-1.9.1/doc/html/search'
make[3]: Leaving directory
`/home1/lu/usr/tmp1/R-1.9.1/doc/html/search'
make[2]: Leaving directory
`/home1/lu/usr/tmp1/R-1.9.1/doc/html'
make[2]: Entering directory
`/home1/lu/usr/tmp1/R-1.9.1/doc/manual'
make[2]: Nothing to be done for `R'.
make[2]: Leaving directory
`/home1/lu/usr/tmp1/R-1.9.1/doc/manual'
make[1]: Leaving directory
`/home1/lu/usr/tmp1/R-1.9.1/doc'
make[1]: Entering directory
`/home1/lu/usr/tmp1/R-1.9.1/etc'
make[1]: Nothing to be done for `R'.
make[1]: Leaving directory
`/home1/lu/usr/tmp1/R-1.9.1/etc'
make[1]: Entering directory
`/home1/lu/usr/tmp1/R-1.9.1/share'
make[1]: Leaving directory
`/home1/lu/usr/tmp1/R-1.9.1/share'
make[1]: Entering directory
`/home1/lu/usr/tmp1/R-1.9.1/src'
make[2]: Entering directory
`/home1/lu/usr/tmp1/R-1.9.1/src/scripts'
make[3]: Entering directory
`/home1/lu/usr/tmp1/R-1.9.1/src/scripts'

    The current directory must be set to the RSI
directory.
    Change the default to the RSI directory and re-run
    this script.
        

    The current directory must be set to the RSI
directory.
    Change the default to the RSI directory and re-run
    this script.
        

    The current directory must be set to the RSI
directory.
    Change the default to the RSI directory and re-run
    this script.
        

    The current directory must be set to the RSI
directory.
    Change the default to the RSI directory and re-run
    this script.
        

    The current directory must be set to the RSI
directory.
    Change the default to the RSI directory and re-run
    this script.
        

    The current directory must be set to the RSI
directory.
    Change the default to the RSI directory and re-run
    this script.
        

    The current directory must be set to the RSI
directory.
    Change the default to the RSI directory and re-run
    this script.
        

    The current directory must be set to the RSI
directory.
    Change the default to the RSI directory and re-run
    this script.
        

    The current directory must be set to the RSI
directory.
    Change the default to the RSI directory and re-run
    this script.
        

    The current directory must be set to the RSI
directory.
    Change the default to the RSI directory and re-run
    this script.
        
make[3]: Leaving directory
`/home1/lu/usr/tmp1/R-1.9.1/src/scripts'
make[2]: Leaving directory
`/home1/lu/usr/tmp1/R-1.9.1/src/scripts'
make[1]: Leaving directory
`/home1/lu/usr/tmp1/R-1.9.1/src'




		
_______________________________

Declare Yourself - Register online to vote today!



From rxg218 at psu.edu  Mon Oct 11 15:51:01 2004
From: rxg218 at psu.edu (Rajarshi Guha)
Date: Mon, 11 Oct 2004 09:51:01 -0400
Subject: [R] R 2.0.0 not suffisantly reliable to be be used
In-Reply-To: <6.1.2.0.0.20041011092436.018fae48@hermes.nos.noaa.gov>
References: <41695F4C.4090203@libertysurf.fr>
	<Pine.LNX.4.44.0410101727370.29146-100000@gannet.stats>
	<6.1.2.0.0.20041011092436.018fae48@hermes.nos.noaa.gov>
Message-ID: <1097502661.31379.7.camel@blue.chem.psu.edu>

(forgot to post to the list)

On Mon, 2004-10-11 at 09:44, Mike Prager wrote:
> >On Sun, 10 Oct 2004, Fan wrote:
> >
> > > For occasional users, I would say, there's no worst thing than that:
> > > you installed the new release, and soem of your existing codes no
> > > longer work !
> 
> There is a close parallel here.  On comp.lang.fortran, a frequent problem 
> is that existing code doesn't work with a new compiler. Such complaints are 
> often from occasional Fortran users, those who have inherited code from 
> others, or those who have been using a single compiler for 20 years and 
> have finally updated.
> 
> In the vast majority of cases, the code in question is not standard 
> conforming.  Either (1) the old compiler ignored various syntax errors, (2) 
> it gave the right answer by luck (i.e., the programmer made assumptions 
> that the compiler met, but that Fortran in general does not meet), or (3) 
> the old release gave the wrong answers, which were never checked.

I'll second this. I inherited a ton of old fortran code from the 70's
and 80's and each time I recompile with different compilers (or newer
versions even) I get more bugs - which as mentioned above, were based on
asumptions the compiler met but were'nt really standard.

I'm all for my code breaking with new releases if it shows me where the
bugs are :)

-------------------------------------------------------------------
Rajarshi Guha <rxg218 at psu.edu> <http://jijo.cjb.net>
GPG Fingerprint: 0CCA 8EE2 2EEB 25E2 AB04 06F7 1BB9 E634 9B87 56EE
-------------------------------------------------------------------
Nothing spoils fun like finding out it builds character"
-Calvin



From cristian at biometria.univr.it  Mon Oct 11 15:56:57 2004
From: cristian at biometria.univr.it (Cristian Pattaro)
Date: Mon, 11 Oct 2004 15:56:57 +0200
Subject: [R] File23525
Message-ID: <416A9129.3080304@biometria.univr.it>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20041011/2a919c02/attachment.pl

From fietz at dionea.ch  Mon Oct 11 16:07:25 2004
From: fietz at dionea.ch (fietz@dionea.ch)
Date: Mon, 11 Oct 2004 16:07:25 +0200
Subject: [R] Multiple regression trees
Message-ID: <000701c4af9b$a97ed5f0$2401a8c0@GIS>

Dear list,

I'm quite new in R, but I'm working with multiple regression trees and I
would like to know a couple of things:
1. how can I label the regression tree with the number of node (each node
should be labled with its number)
2. I like to know for each end node (leaves) the number of occurency for
each response variable (species) that falls in that group.The tree-plot
gives me only the number of sites that falls in each leave.

Thank you in advance
Annette



From ripley at stats.ox.ac.uk  Mon Oct 11 16:21:09 2004
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Mon, 11 Oct 2004 15:21:09 +0100 (BST)
Subject: [R] File23525
In-Reply-To: <416A9129.3080304@biometria.univr.it>
Message-ID: <Pine.LNX.4.44.0410111519150.2338-100000@gannet.stats>

On Mon, 11 Oct 2004, Cristian Pattaro wrote:

> After installing R 2.0.0, I have problem with the "help.search" as below:
> 
>  > help.search("table")
> Error in help.search("table") : could not find package 'file23525'
> 
>  > help.search("mean")
> Error in help.search("mean") : could not find package 'file23525'
> 
> I have had the same problem with other request...
> What is "file23525"? How can bypass the problem?

Might your unstated OS be Windows?

I think you may have an incorrectly installed package in your library 
directory.  If you have R_HOME/library/file23525, delete it.  If not, 
search for "file23525" on your system.

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From s-luppescu at uchicago.edu  Mon Oct 11 16:27:32 2004
From: s-luppescu at uchicago.edu (Stuart Luppescu)
Date: Mon, 11 Oct 2004 09:27:32 -0500
Subject: [R] split and rlm
Message-ID: <1097504852.16842.7.camel@musuko.uchicago.edu>

Hello, I'm trying to do a little rlm of some data that looks like this:

UNIT    COHORT     perdo     adjodds
 1010      96      0.39890    1.06894
 1010      97      0.48113    1.57500
 1010      98      0.36328    1.21498
 1010      99      0.44391    1.38608

It works fine like this: rlm(perdo ~ COHORT, psi=psisquare)
But the problem is that I have about 100 UNITs, and I want to do a
separate rlm for each one. I tried to use split and lapply but it didn't
work at all. Is this possible?

In addition, I'm trying to extract the t statistic for the slope
coefficient and the degrees of freedom so I can put them into dt() to
get the p-value. I can get the t from coef(summary(u))[2,3] (where u is
my rlm object), but u$df.residual gives me NULL. Also, the help for
summary.lm says it returns coefficients, which contains a 4x4 matrix
including the p-values, but when I do summary(u)$coefficients I get:

 summary(u)$coefficients
                                       Value Std. Error    t value
(Intercept)                      0.151756859 3.00972988 0.05042209
drops$COHORT[drops$UNIT == unit] 0.002769108 0.03086700 0.08971097

Any help with this, and on getting the degrees of freedom or the p-value
would be much appreciated. 

-- 
Stuart Luppescu -=- s-luppescu .at. uchicago.edu        
University of Chicago -=- CCSR 
$B:MJ8$HCRF`H~$NIc(B -=-    Kernel 2.6.8-gentoo-r3                
"I don't remember debates.  I don't think we spent 
 a lot of time debating it. Maybe we did, but I
 don't remember."  George W. Bush July 27, 1999
 Referring to whether he had discussions about the 
 Vietnam War while an undergraduate at Yale



From heikz at gmx.de  Mon Oct 11 16:32:51 2004
From: heikz at gmx.de (Heike Zimmermann)
Date: Mon, 11 Oct 2004 16:32:51 +0200 (MEST)
Subject: [R] logistic regression
Message-ID: <6680.1097505171@www25.gmx.net>

Hello,

I have a problem concerning logistic regressions.  When I add a quadratic
term to my linear model, I cannot draw the line through my scatterplot
anymore, which is no problem without the quadratic term.
In this example my binary response variable is "incidence", the explanatory
variable is "sun":
> model0<-glm(incidence~1,binomial)
> model1<-glm(incidence~sun,binomial)
> anova(model0,model1,test="Chi")
Analysis of Deviance Table

Model 1: incidence ~ 1
Model 2: incidence ~ sun
  Resid. Df Resid. Dev  Df Deviance P(>|Chi|)
1       299     332.94                       
2       298     287.19   1    45.74 1.349e-11
> qsun<-sun^2
> model2<-glm(incidence~sun+qsun,binomial)
> anova(model1,model2,test="Chi")
Analysis of Deviance Table

Model 1: incidence ~ sun
Model 2: incidence ~ sun + qsun
  Resid. Df Resid. Dev  Df Deviance P(>|Chi|)
1       298    287.194                       
2       297    280.623   1    6.571     0.010

So the second, non-linear, model explains more than the first. 
Now to create a graph I write:

> plot(sun,incidence)
> min(sun)
[1] 0
> max(sun)
[1] 90
> xsun<-seq(0,90,1)

>lines(xsun,predict(model2,type="response",data.frame(sun=xsun)))

Error in model.frame(formula, rownames, variables, varnames, extras,
extranames,  : 
        variable lengths differ
> 

So this is the message I receive everytime I try to draw the fitted values
of my model. I know for sure that exactly the same command works in S-Plus
(with the same data). How is ist possible to do this in R?

Thank you in advance, Heike

-- 
+++ GMX DSL Premiumtarife 3 Monate gratis* + WLAN-Router 0,- EUR* +++
Clevere DSL-Nutzer wechseln jetzt zu GMX: http://www.gmx.net/de/go/dsl



From ripley at stats.ox.ac.uk  Mon Oct 11 16:33:18 2004
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Mon, 11 Oct 2004 15:33:18 +0100 (BST)
Subject: [R] Multiple regression trees
In-Reply-To: <000701c4af9b$a97ed5f0$2401a8c0@GIS>
Message-ID: <Pine.LNX.4.44.0410111528470.2406-100000@gannet.stats>

You haven't even told us which add-on package you are using to get
`multiple regression trees'.  Is that multiple 'regression trees' or
'multiple regression' trees or what?

On Mon, 11 Oct 2004 fietz at dionea.ch wrote:

> I'm quite new in R, but I'm working with multiple regression trees and I
> would like to know a couple of things:
> 1. how can I label the regression tree with the number of node (each node
> should be labled with its number)

In both tree and rpart nodes are labelled by number in the printout.

> 2. I like to know for each end node (leaves) the number of occurency for
> each response variable (species) that falls in that group.The tree-plot
> gives me only the number of sites that falls in each leave.

Again, for both tree and rpart the printout gives you this, but they have 
one response variable.

I suggest you ask the author of whatever package you are using (but (s)he 
might be upset by your posting here and giving no credit).

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From sundar.dorai-raj at PDF.COM  Mon Oct 11 16:52:31 2004
From: sundar.dorai-raj at PDF.COM (Sundar Dorai-Raj)
Date: Mon, 11 Oct 2004 09:52:31 -0500
Subject: [R] logistic regression
In-Reply-To: <6680.1097505171@www25.gmx.net>
References: <6680.1097505171@www25.gmx.net>
Message-ID: <416A9E2F.2090601@pdf.com>



Heike Zimmermann wrote:

> Hello,
> 
> I have a problem concerning logistic regressions.  When I add a quadratic
> term to my linear model, I cannot draw the line through my scatterplot
> anymore, which is no problem without the quadratic term.
> In this example my binary response variable is "incidence", the explanatory
> variable is "sun":
> 
>>model0<-glm(incidence~1,binomial)
>>model1<-glm(incidence~sun,binomial)
>>anova(model0,model1,test="Chi")
> 
> Analysis of Deviance Table
> 
> Model 1: incidence ~ 1
> Model 2: incidence ~ sun
>   Resid. Df Resid. Dev  Df Deviance P(>|Chi|)
> 1       299     332.94                       
> 2       298     287.19   1    45.74 1.349e-11
> 
>>qsun<-sun^2
>>model2<-glm(incidence~sun+qsun,binomial)
>>anova(model1,model2,test="Chi")
> 
> Analysis of Deviance Table
> 
> Model 1: incidence ~ sun
> Model 2: incidence ~ sun + qsun
>   Resid. Df Resid. Dev  Df Deviance P(>|Chi|)
> 1       298    287.194                       
> 2       297    280.623   1    6.571     0.010
> 
> So the second, non-linear, model explains more than the first. 
> Now to create a graph I write:
> 
> 
>>plot(sun,incidence)
>>min(sun)
> 
> [1] 0
> 
>>max(sun)
> 
> [1] 90
> 
>>xsun<-seq(0,90,1)
> 
> 
>>lines(xsun,predict(model2,type="response",data.frame(sun=xsun)))
> 
> 
> Error in model.frame(formula, rownames, variables, varnames, extras,
> extranames,  : 
>         variable lengths differ
> 
> 
> So this is the message I receive everytime I try to draw the fitted values
> of my model. I know for sure that exactly the same command works in S-Plus
> (with the same data). How is ist possible to do this in R?
> 
> Thank you in advance, Heike
> 

This is because your newdata=data.frame(sun=xsun) does not contain a 
variable called "qsun". Try the following instead:

model2 <- glm(incidence ~ sun + I(sun^2), binomial)
...
lines(xsun, predict(model2, type = "response", data.frame(sun = xsun)))


--sundar



From stefan.albrecht at allianz.com  Mon Oct 11 16:52:57 2004
From: stefan.albrecht at allianz.com (stefan.albrecht@allianz.com)
Date: Mon, 11 Oct 2004 16:52:57 +0200
Subject: [R] Quality of png Plot
Message-ID: <OFF276CC67.DC3A0297-ONC1256F2A.004FBCF3-C1256F2A.0051C068@inside.allianz.de>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20041011/9ec73108/attachment.pl

From ripley at stats.ox.ac.uk  Mon Oct 11 16:54:18 2004
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Mon, 11 Oct 2004 15:54:18 +0100 (BST)
Subject: [R] split and rlm
In-Reply-To: <1097504852.16842.7.camel@musuko.uchicago.edu>
Message-ID: <Pine.LNX.4.44.0410111547540.2465-100000@gannet.stats>

On Mon, 11 Oct 2004, Stuart Luppescu wrote:

> Hello, I'm trying to do a little rlm of some data that looks like this:
> 
> UNIT    COHORT     perdo     adjodds
>  1010      96      0.39890    1.06894
>  1010      97      0.48113    1.57500
>  1010      98      0.36328    1.21498
>  1010      99      0.44391    1.38608
> 
> It works fine like this: rlm(perdo ~ COHORT, psi=psisquare)
> But the problem is that I have about 100 UNITs, and I want to do a
> separate rlm for each one. I tried to use split and lapply but it didn't
> work at all. Is this possible?
> 
> In addition, I'm trying to extract the t statistic for the slope
> coefficient and the degrees of freedom so I can put them into dt() to
> get the p-value. I can get the t from coef(summary(u))[2,3] (where u is
> my rlm object), but u$df.residual gives me NULL. Also, the help for
> summary.lm says it returns coefficients, which contains a 4x4 matrix
> including the p-values, but when I do summary(u)$coefficients I get:
> 
>  summary(u)$coefficients
>                                        Value Std. Error    t value
> (Intercept)                      0.151756859 3.00972988 0.05042209
> drops$COHORT[drops$UNIT == unit] 0.002769108 0.03086700 0.08971097
> 
> Any help with this, and on getting the degrees of freedom or the p-value
> would be much appreciated. 

0) Do use extractor function like coef() and df.residual().

1) for the 100 fits try using the default interface, not the formula one.

2) df.residual is not a relevant concept, and the coefficients are not 
t-distributed.  Where did you read that it had?

You can try a normal approximation, but beware it may be rough.  If you
look up MASS4 (the book this software supports) you will see better ideas
illustrated.

3) I get in ?summary.rlm

coefficients: A matrix with three columns, containing the coefficients,
          their standard errors and the corresponding t statistic.

What has summary.lm got to do with rlm fits?

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From p.dalgaard at biostat.ku.dk  Mon Oct 11 16:51:43 2004
From: p.dalgaard at biostat.ku.dk (Peter Dalgaard)
Date: 11 Oct 2004 16:51:43 +0200
Subject: [R] R-1.9.1 compiling problem
In-Reply-To: <20041011134510.91636.qmail@web61301.mail.yahoo.com>
References: <20041011134510.91636.qmail@web61301.mail.yahoo.com>
Message-ID: <x28yad71fk.fsf@biostat.ku.dk>

lu kan <luk111111 at yahoo.com> writes:

> I downloaded R-1.9.1.tgz, and tried to install it on
> debian linux machine according to the installation
> commands
> ./configure
> make
> 
> but I got the following error message. Could you
> please advise me what is the reason for this. 
 
> `/home1/lu/usr/tmp1/R-1.9.1/src/scripts'
> 
>     The current directory must be set to the RSI
> directory.
>     Change the default to the RSI directory and re-run
>     this script.

I think it means that you're picking up the wrong version of the
"install" script. That exact phrasing occurs in (thank Google!)


#!/bin/sh 
#
#       $Id: install,v 1.67 2003/01/28 19:54:23 beth Exp $
#
# install
#
# Research Systems, Inc                         March 1, 1988
# This shell script carries out the steps required to install IDL
# or an IDL based product once the main directory has been unpacked.
# It expects to be run from the RSI directory.
#
...


Does that ring a bell with you?

-- 
   O__  ---- Peter Dalgaard             Blegdamsvej 3  
  c/ /'_ --- Dept. of Biostatistics     2200 Cph. N   
 (*) \(*) -- University of Copenhagen   Denmark      Ph: (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)             FAX: (+45) 35327907



From andy_liaw at merck.com  Mon Oct 11 16:56:24 2004
From: andy_liaw at merck.com (Liaw, Andy)
Date: Mon, 11 Oct 2004 10:56:24 -0400
Subject: [R] split and rlm
Message-ID: <3A822319EB35174CA3714066D590DCD504AF850D@usrymx25.merck.com>

Here's one example:

[BTW, please tell us when you're using functions in contributed packages.]

> library(MASS)
> dat <- data.frame(unit=rep(1:5, each=10), y = rnorm(50), x1=rnorm(50),
+                   x2 = rnorm(50))
> fit <- by(dat, dat$unit, function(dat) rlm(y ~ x1 + x2, dat,
+                                            psi=psi.bisquare))
> lapply(fit, function(m) coefficients(summary(m)))
$"1"
                 Value Std. Error   t value
(Intercept) -0.4353587  0.3657624 -1.190277
x1           0.7605763  0.2717837  2.798462
x2           0.2196208  0.3236791  0.678514

$"2"
                  Value Std. Error    t value
(Intercept) -0.26731627 0.04553002 -5.8712088
x1          -0.12206044 0.05581878 -2.1867272
x2           0.01277407 0.04808985  0.2656293

$"3"
                  Value Std. Error    t value
(Intercept)  0.07899741  0.4420049  0.1787252
x1          -0.14290441  0.4790551 -0.2983048
x2          -0.19731260  0.4246512 -0.4646463

$"4"
                 Value Std. Error   t value
(Intercept)  0.4925298  0.3332106  1.478134
x1           0.4938363  0.3866812  1.277115
x2          -0.3475754  0.3281346 -1.059246

$"5"
                  Value Std. Error    t value
(Intercept) -0.10928768  0.2582368 -0.4232073
x1           0.33719681  0.3028123  1.1135506
x2           0.04587871  0.2680164  0.1711788

For residual df, you need to look at the object returned by the summary()
method for rlm.  You should not be reading the help page for summary.lm when
you're dealing with rlm objects.

> names(summary(fit[[1]]))
 [1] "call"         "residuals"    "coefficients" "sigma"        "stddev"

 [6] "df"           "r.squared"    "cov.unscaled" "correlation"  "terms"


HTH,
Andy

> From: Stuart Luppescu
> 
> Hello, I'm trying to do a little rlm of some data that looks 
> like this:
> 
> UNIT    COHORT     perdo     adjodds
>  1010      96      0.39890    1.06894
>  1010      97      0.48113    1.57500
>  1010      98      0.36328    1.21498
>  1010      99      0.44391    1.38608
> 
> It works fine like this: rlm(perdo ~ COHORT, psi=psisquare)
> But the problem is that I have about 100 UNITs, and I want to do a
> separate rlm for each one. I tried to use split and lapply 
> but it didn't
> work at all. Is this possible?
> 
> In addition, I'm trying to extract the t statistic for the slope
> coefficient and the degrees of freedom so I can put them into dt() to
> get the p-value. I can get the t from coef(summary(u))[2,3] 
> (where u is
> my rlm object), but u$df.residual gives me NULL. Also, the help for
> summary.lm says it returns coefficients, which contains a 4x4 matrix
> including the p-values, but when I do summary(u)$coefficients I get:
> 
>  summary(u)$coefficients
>                                        Value Std. Error    t value
> (Intercept)                      0.151756859 3.00972988 0.05042209
> drops$COHORT[drops$UNIT == unit] 0.002769108 0.03086700 0.08971097
> 
> Any help with this, and on getting the degrees of freedom or 
> the p-value
> would be much appreciated. 
> 
> -- 
> Stuart Luppescu -=- s-luppescu .at. uchicago.edu        
> University of Chicago -=- CCSR 
> $B:MJ8$HCRF`H~$NIc(B -=-    Kernel 2.6.8-gentoo-r3                
> "I don't remember debates.  I don't think we spent 
>  a lot of time debating it. Maybe we did, but I
>  don't remember."  George W. Bush July 27, 1999
>  Referring to whether he had discussions about the 
>  Vietnam War while an undergraduate at Yale
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
> 
>



From ripley at stats.ox.ac.uk  Mon Oct 11 17:01:00 2004
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Mon, 11 Oct 2004 16:01:00 +0100 (BST)
Subject: [R] logistic regression
In-Reply-To: <6680.1097505171@www25.gmx.net>
Message-ID: <Pine.LNX.4.44.0410111555200.2465-100000@gannet.stats>

On Mon, 11 Oct 2004, Heike Zimmermann wrote:

> Hello,
> 
> I have a problem concerning logistic regressions.  When I add a quadratic
> term to my linear model, I cannot draw the line through my scatterplot
> anymore, which is no problem without the quadratic term.
> In this example my binary response variable is "incidence", the explanatory
> variable is "sun":
> > model0<-glm(incidence~1,binomial)
> > model1<-glm(incidence~sun,binomial)
> > anova(model0,model1,test="Chi")
> Analysis of Deviance Table
> 
> Model 1: incidence ~ 1
> Model 2: incidence ~ sun
>   Resid. Df Resid. Dev  Df Deviance P(>|Chi|)
> 1       299     332.94                       
> 2       298     287.19   1    45.74 1.349e-11
> > qsun<-sun^2
> > model2<-glm(incidence~sun+qsun,binomial)
> > anova(model1,model2,test="Chi")
> Analysis of Deviance Table
> 
> Model 1: incidence ~ sun
> Model 2: incidence ~ sun + qsun
>   Resid. Df Resid. Dev  Df Deviance P(>|Chi|)
> 1       298    287.194                       
> 2       297    280.623   1    6.571     0.010
> 
> So the second, non-linear, model explains more than the first. 
> Now to create a graph I write:
> 
> > plot(sun,incidence)
> > min(sun)
> [1] 0
> > max(sun)
> [1] 90
> > xsun<-seq(0,90,1)
> 
> >lines(xsun,predict(model2,type="response",data.frame(sun=xsun)))

Why are you predicting using a new data frame containing just one of the 
two predictors?

> 
> Error in model.frame(formula, rownames, variables, varnames, extras,
> extranames,  : 
>         variable lengths differ
> > 
> 
> So this is the message I receive everytime I try to draw the fitted values
> of my model. I know for sure that exactly the same command works in S-Plus
> (with the same data). 

If so, you have been trapped by a bug in S-PLUS, for you have at no point
supplied the quadratic term needed for prediction.  I suspect more likely
that the command is accepted but gives incorrect answers.  See the
warnings in MASS, for example.

> How is ist possible to do this in R?

NB: note the use of spaces for readability.

mydata <- data.frame(sun)
model2 <- glm(incidence ~ sun + I(sun^2), binomial, data=mydata)
plot(sun, incidence)
lines(xsun, predict(model2, type="response", data.frame(sun=0:90)))

PLEASE use a meaningful subject line.

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From dimitris.rizopoulos at med.kuleuven.ac.be  Mon Oct 11 17:02:26 2004
From: dimitris.rizopoulos at med.kuleuven.ac.be (Dimitris Rizopoulos)
Date: Mon, 11 Oct 2004 17:02:26 +0200
Subject: [R] logistic regression
References: <6680.1097505171@www25.gmx.net>
Message-ID: <008601c4afa3$51f5cd90$b2133a86@www.domain>

Hi Heike,

you'd  probably want

lines(xsun, predict(model2, type="response", 
newdata=data.frame(sun=xsun, qsun=xsun*xsun)))

I hope it helps.

Best,
Dimitris

----
Dimitris Rizopoulos
Ph.D. Student
Biostatistical Centre
School of Public Health
Catholic University of Leuven

Address: Kapucijnenvoer 35, Leuven, Belgium
Tel: +32/16/396887
Fax: +32/16/337015
Web: http://www.med.kuleuven.ac.be/biostat/
     http://www.student.kuleuven.ac.be/~m0390867/dimitris.htm


----- Original Message ----- 
From: "Heike Zimmermann" <heikz at gmx.de>
To: <R-help at stat.math.ethz.ch>
Sent: Monday, October 11, 2004 4:32 PM
Subject: [R] logistic regression


> Hello,
>
> I have a problem concerning logistic regressions.  When I add a 
> quadratic
> term to my linear model, I cannot draw the line through my 
> scatterplot
> anymore, which is no problem without the quadratic term.
> In this example my binary response variable is "incidence", the 
> explanatory
> variable is "sun":
>> model0<-glm(incidence~1,binomial)
>> model1<-glm(incidence~sun,binomial)
>> anova(model0,model1,test="Chi")
> Analysis of Deviance Table
>
> Model 1: incidence ~ 1
> Model 2: incidence ~ sun
>  Resid. Df Resid. Dev  Df Deviance P(>|Chi|)
> 1       299     332.94
> 2       298     287.19   1    45.74 1.349e-11
>> qsun<-sun^2
>> model2<-glm(incidence~sun+qsun,binomial)
>> anova(model1,model2,test="Chi")
> Analysis of Deviance Table
>
> Model 1: incidence ~ sun
> Model 2: incidence ~ sun + qsun
>  Resid. Df Resid. Dev  Df Deviance P(>|Chi|)
> 1       298    287.194
> 2       297    280.623   1    6.571     0.010
>
> So the second, non-linear, model explains more than the first.
> Now to create a graph I write:
>
>> plot(sun,incidence)
>> min(sun)
> [1] 0
>> max(sun)
> [1] 90
>> xsun<-seq(0,90,1)
>
>>lines(xsun,predict(model2,type="response",data.frame(sun=xsun)))
>
> Error in model.frame(formula, rownames, variables, varnames, extras,
> extranames,  :
>        variable lengths differ
>>
>
> So this is the message I receive everytime I try to draw the fitted 
> values
> of my model. I know for sure that exactly the same command works in 
> S-Plus
> (with the same data). How is ist possible to do this in R?
>
> Thank you in advance, Heike
>
> -- 
> +++ GMX DSL Premiumtarife 3 Monate gratis* + WLAN-Router 0,- EUR* 
> +++
> Clevere DSL-Nutzer wechseln jetzt zu GMX: 
> http://www.gmx.net/de/go/dsl
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
>



From br44114 at yahoo.com  Mon Oct 11 17:04:07 2004
From: br44114 at yahoo.com (bogdan romocea)
Date: Mon, 11 Oct 2004 08:04:07 -0700 (PDT)
Subject: [R] read "4-jan-02" as date
Message-ID: <20041011150408.52253.qmail@web50301.mail.yahoo.com>

Dear R users,

I have a column with dates (character) in a data frame:
12-Jan-01 11-Jan-01 10-Jan-01 9-Jan-01  8-Jan-01  5-Jan-01
and I need to convert them to (Julian) dates so that I can
sort the whole data frame by date. I thought it would be
very simple, but after checking the documentation and the
list I still don't have something that works.

1. as.Date returns the error below. What am I doing wrong?
As far as I can see the character strings are in standard
format.
d$Date <- as.Date(d$Date, format="%d-%b-%y")
Error in fromchar(x) : character string is not in a
standard unambiguous format

2. as.date {Survival} produces this error,
d$Date <- as.date(d$Date, order = "dmy")
Error in as.date(d$Date, order = "dmy") : Cannot coerce to
date format

3. Assuming all else fails, is there a text function
similar to SCAN in SAS? Given a string like "9-Jan-01" and
"-" as separator, I'd like a function that can read the
first, second and third values (9, Jan, 01), so that I can
get Julian dates with mdy.date {survival}.

Thanks in advance,
b.



From luk111111 at yahoo.com  Mon Oct 11 17:10:32 2004
From: luk111111 at yahoo.com (lu kan)
Date: Mon, 11 Oct 2004 08:10:32 -0700 (PDT)
Subject: [R] R-1.9.1 compiling problem
In-Reply-To: <x28yad71fk.fsf@biostat.ku.dk>
Message-ID: <20041011151032.41507.qmail@web61307.mail.yahoo.com>


Hi Peter,

Thanks for your help. 

But the installation file "INSTALL" comes with
R-1.9.1.
The following is a copy of it.

Lu
-------------


			INSTALLING R UNDER UNIX


This document concerns building and installing R from
sources.  Pre-made
binaries are made available for some systems with
varying regularity and
can be obtained from CRAN (see the RESOURCES file).

For building on Windows 9x/ME/NT/2000 see the file
`src/gnuwin32/INSTALL'.

The main source of information on installation is the
`R Installation
and Administration Manual', an HTML copy of which is
available as file
`doc/html/R-admin.html'.  Please read that before
installing R.  But
if you are impatient, read on but please refer to the
manual to
resolve any problems.


SIMPLE COMPILATION

As you are reading this file, you have unpacked the R
sources and are
presumably in the top directory.  Issue the following
commands:

	./configure
	make

(If your make is not called `make', set the
environment variable MAKE to
its name, and use that name throughout these
instructions.)
This will take a while, giving you time to read
`R-admin.html'.

Then check the built system worked correctly, by

	make check

and make the manuals by (as many options as preferred
from)

	make dvi	to create DVI versions
	make pdf	to create PDF versions
	make info	to create info files


INSTALLATION

You do not need to install R to run it: you can run R
by the script
`bin/R' which you can link or copy to any convenient
place in your path.

For a site-wide installation, use

	make install
	make install-dvi
	make install-info
	make install-pdf

choosing to install the manuals that you made.

This installs to the default location (typically
`/usr/local') for your
platform: see the Installation and Administration
Manual for other options.




--- Peter Dalgaard <p.dalgaard at biostat.ku.dk> wrote:

> lu kan <luk111111 at yahoo.com> writes:
> 
> > I downloaded R-1.9.1.tgz, and tried to install it
> on
> > debian linux machine according to the installation
> > commands
> > ./configure
> > make
> > 
> > but I got the following error message. Could you
> > please advise me what is the reason for this. 
>  
> > `/home1/lu/usr/tmp1/R-1.9.1/src/scripts'
> > 
> >     The current directory must be set to the RSI
> > directory.
> >     Change the default to the RSI directory and
> re-run
> >     this script.
> 
> I think it means that you're picking up the wrong
> version of the
> "install" script. That exact phrasing occurs in
> (thank Google!)
> 
> 
> #!/bin/sh 
> #
> #       $Id: install,v 1.67 2003/01/28 19:54:23 beth
> Exp $
> #
> # install
> #
> # Research Systems, Inc                        
> March 1, 1988
> # This shell script carries out the steps required
> to install IDL
> # or an IDL based product once the main directory
> has been unpacked.
> # It expects to be run from the RSI directory.
> #
> ...
> 
> 
> Does that ring a bell with you?
> 
> -- 
>    O__  ---- Peter Dalgaard             Blegdamsvej
> 3  
>   c/ /'_ --- Dept. of Biostatistics     2200 Cph. N 
>  
>  (*) \(*) -- University of Copenhagen   Denmark     
> Ph: (+45) 35327918
> ~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)            
> FAX: (+45) 35327907
> 



		
_______________________________

Declare Yourself - Register online to vote today!



From dmb at mrc-dunn.cam.ac.uk  Mon Oct 11 17:21:53 2004
From: dmb at mrc-dunn.cam.ac.uk (Dan Bolser)
Date: Mon, 11 Oct 2004 16:21:53 +0100 (BST)
Subject: [R] plot hclust - canberra dist + median linkage 
Message-ID: <Pine.LNX.4.21.0410111618390.25948-100000@mail.mrc-dunn.cam.ac.uk>


Gives strange results.

I get 'weird' dendrograms with canberra / binary distance metric and
median / centroid cluster methods.

Is this just my data?

Dan



From p.dalgaard at biostat.ku.dk  Mon Oct 11 17:17:41 2004
From: p.dalgaard at biostat.ku.dk (Peter Dalgaard)
Date: 11 Oct 2004 17:17:41 +0200
Subject: [R] split and rlm
In-Reply-To: <1097504852.16842.7.camel@musuko.uchicago.edu>
References: <1097504852.16842.7.camel@musuko.uchicago.edu>
Message-ID: <x24ql1708a.fsf@biostat.ku.dk>

Stuart Luppescu <s-luppescu at uchicago.edu> writes:

> Hello, I'm trying to do a little rlm of some data that looks like this:
> 
> UNIT    COHORT     perdo     adjodds
>  1010      96      0.39890    1.06894
>  1010      97      0.48113    1.57500
>  1010      98      0.36328    1.21498
>  1010      99      0.44391    1.38608
> 
> It works fine like this: rlm(perdo ~ COHORT, psi=psisquare)
> But the problem is that I have about 100 UNITs, and I want to do a
> separate rlm for each one. I tried to use split and lapply but it didn't
> work at all. Is this possible?

How about by()?

 
> In addition, I'm trying to extract the t statistic for the slope
> coefficient and the degrees of freedom so I can put them into dt() to

pt(), I would hope....

> get the p-value. I can get the t from coef(summary(u))[2,3] (where u is
> my rlm object), but u$df.residual gives me NULL. Also, the help for
> summary.lm says it returns coefficients, which contains a 4x4 matrix
> including the p-values, but when I do summary(u)$coefficients I get:
> 
>  summary(u)$coefficients
>                                        Value Std. Error    t value
> (Intercept)                      0.151756859 3.00972988 0.05042209
> drops$COHORT[drops$UNIT == unit] 0.002769108 0.03086700 0.08971097
> 
> Any help with this, and on getting the degrees of freedom or the p-value
> would be much appreciated. 


As Brian usually puts it: This is support software for a book. You
might want to read it...

Looking at the code, it seems to explicitly set the residual DF to NA,
which - along with the absense of p values - usually suggests that the
author(s) do not trust them and I think I can see why. Apparently the
lm() methods which get invoked through inheritance have ways of
sticking the residual DF back in, but that doesn't make them any more
sensible. If you must, have a look inside print.summary.lm and see
where "rdf" comes from, but I'd suspect that you're just as well off
by using the normal approximation.

-- 
   O__  ---- Peter Dalgaard             Blegdamsvej 3  
  c/ /'_ --- Dept. of Biostatistics     2200 Cph. N   
 (*) \(*) -- University of Copenhagen   Denmark      Ph: (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)             FAX: (+45) 35327907



From p.dalgaard at biostat.ku.dk  Mon Oct 11 17:19:59 2004
From: p.dalgaard at biostat.ku.dk (Peter Dalgaard)
Date: 11 Oct 2004 17:19:59 +0200
Subject: [R] R-1.9.1 compiling problem
In-Reply-To: <20041011151032.41507.qmail@web61307.mail.yahoo.com>
References: <20041011151032.41507.qmail@web61307.mail.yahoo.com>
Message-ID: <x2zn2t5lk0.fsf@biostat.ku.dk>

lu kan <luk111111 at yahoo.com> writes:

> Hi Peter,
> 
> Thanks for your help. 
> 
> But the installation file "INSTALL" comes with
> R-1.9.1.
> The following is a copy of it.

That's irrelevant. Try typing "which install" and have a look at your
PATH settings.

-- 
   O__  ---- Peter Dalgaard             Blegdamsvej 3  
  c/ /'_ --- Dept. of Biostatistics     2200 Cph. N   
 (*) \(*) -- University of Copenhagen   Denmark      Ph: (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)             FAX: (+45) 35327907



From fietz at dionea.ch  Mon Oct 11 17:27:19 2004
From: fietz at dionea.ch (fietz@dionea.ch)
Date: Mon, 11 Oct 2004 17:27:19 +0200
Subject: [R] Multiple regression trees-more info
References: <Pine.LNX.4.44.0410111528470.2406-100000@gannet.stats>
Message-ID: <000801c4afa6$ccbdf6d0$2401a8c0@GIS>

Sorry, as I was mentioned already I'm new in this field an thus I forgot to
tell more details:

I'm using R software 1.9.1 with the 'mvpart' package.

In the meantime I came closer to the solution of my problem, but I get an
error (see the script below). But I still don't have an idea how to label
each node with its corresponding number.
Annette


#require(mvpart)
eco.mvp <- mvpart(data.matrix(eco) ~ ., env, xv="p")
eco.mvp$where
eco.member <-eco.mvp$where
#find out the levels (in this case the end node number) of a factor
eco.memberf <- factor(eco.member)#define eco.member as factor
levels(eco.memberf) #numbers(labels) of end nodes

#find meanvalue for each response variable within each group (end node)
all.value <- c(eco$all) #extract column of response variable"all" with
values
allmeans.member <-(tapply(all.value,eco.memberf,mean))
allmeans.member

#number of occurency for each response variable within each end node
#require(vegan)
all0 <- decostand(all.value,"pa")
allcount.member <-(tapply(all0,eco.memberf,sum))

Error in tapply(all0c, eco.memberf, sum) :
        arguments must have same length



From i.visser at uva.nl  Mon Oct 11 17:45:20 2004
From: i.visser at uva.nl (Ingmar Visser)
Date: Mon, 11 Oct 2004 17:45:20 +0200
Subject: [R] install failure Ruuid package on OS X
Message-ID: <BD907730.8997%i.visser@uva.nl>

Dear All,

When installing the Ruuid package (from Bioconductor) from sources on my
MAC (OS X 10.3.5, R version 2.0.0) I get the following errors:

m00245:~ ivisser$ R CMD INSTALL  -l /Users/ivisser/Library/R/library/
/Users/ivisser/Desktop/Ruuid
* Installing *source* package 'Ruuid' ...
loading cache ./config.cache
checking for glib-config... no
checking how to run the C preprocessor... (cached) cc -E
checking for /usr/include/unistd.h... (cached) yes
creating ./config.status
creating src/Makevars
** libs
gcc -no-cpp-precomp -I/Library/Frameworks/R.framework/Resources/include
-I/usr/local/include/glib12 -DHAVE_UNISTD_H=1 -I/sw/include
-I/usr/local/include   -fno-common  -g -O2 -c Rinit.c -o Rinit.o
In file included from Ruuid.h:5,
                 from Rinit.c:1:
uuidP.h:24:18: glib.h: No such file or directory
In file included from Ruuid.h:5,
                 from Rinit.c:1:
uuidP.h:36: error: parse error before "guint32"
uuidP.h:36: warning: no semicolon at end of struct or union
uuidP.h:37: warning: data definition has no type or storage class
uuidP.h:38: error: parse error before "time_hi_and_version"
uuidP.h:38: warning: data definition has no type or storage class
uuidP.h:39: error: parse error before "clock_seq"
uuidP.h:39: warning: data definition has no type or storage class
uuidP.h:40: error: parse error before "node"
uuidP.h:40: warning: data definition has no type or storage class
uuidP.h:41: error: parse error before '}' token
make: *** [Rinit.o] Error 1
ERROR: compilation failed for package 'Ruuid'
** Removing '/Users/ivisser/Library/R/library/Ruuid'
** Restoring previous '/Users/ivisser/Library/R/library/Ruuid'
m00245:~ ivisser$ 


Any help is appreciated,

best, ingmar



From wfang at pythagoras.math.uwaterloo.ca  Mon Oct 11 17:45:49 2004
From: wfang at pythagoras.math.uwaterloo.ca (wfang)
Date: Mon, 11 Oct 2004 11:45:49 -0400
Subject: [R] How to read unbalanced data from file?
Message-ID: <6.1.2.0.0.20041011114320.02233820@mail.math.uwaterloo.ca>

Hi,
   I tried to read some unbalance data (with different number if rows) 
using x<- read.table("filename", header = true) command, but the program 
refuses to read the table. Could you tell me why this is happening and how 
to fix it? I just want to create a ANOVA table.
   Thanks.
   Wenyi



From sundar.dorai-raj at PDF.COM  Mon Oct 11 17:52:05 2004
From: sundar.dorai-raj at PDF.COM (Sundar Dorai-Raj)
Date: Mon, 11 Oct 2004 10:52:05 -0500
Subject: [R] read "4-jan-02" as date
In-Reply-To: <20041011150408.52253.qmail@web50301.mail.yahoo.com>
References: <20041011150408.52253.qmail@web50301.mail.yahoo.com>
Message-ID: <416AAC25.1070707@pdf.com>



bogdan romocea wrote:

> Dear R users,
> 
> I have a column with dates (character) in a data frame:
> 12-Jan-01 11-Jan-01 10-Jan-01 9-Jan-01  8-Jan-01  5-Jan-01
> and I need to convert them to (Julian) dates so that I can
> sort the whole data frame by date. I thought it would be
> very simple, but after checking the documentation and the
> list I still don't have something that works.
> 
> 1. as.Date returns the error below. What am I doing wrong?
> As far as I can see the character strings are in standard
> format.
> d$Date <- as.Date(d$Date, format="%d-%b-%y")
> Error in fromchar(x) : character string is not in a
> standard unambiguous format
> 
> 2. as.date {Survival} produces this error,
> d$Date <- as.date(d$Date, order = "dmy")
> Error in as.date(d$Date, order = "dmy") : Cannot coerce to
> date format
> 
> 3. Assuming all else fails, is there a text function
> similar to SCAN in SAS? Given a string like "9-Jan-01" and
> "-" as separator, I'd like a function that can read the
> first, second and third values (9, Jan, 01), so that I can
> get Julian dates with mdy.date {survival}.
> 
> Thanks in advance,
> b.
> 

If you're reading this from a file (via read.table, for example), then 
your date column is probably a factor. Convert to character first.

 > x
[1] 12-Jan-01 11-Jan-01 10-Jan-01 9-Jan-01  8-Jan-01  5-Jan-01
Levels: 10-Jan-01 11-Jan-01 12-Jan-01 5-Jan-01 8-Jan-01 9-Jan-01
 >
 > Date(x, format="%d-%b-%y")
Error in fromchar(x) : character string is not in a standard unambiguous 
format
 >
 > sort(as.Date(as.character(x), format="%d-%b-%y"))
[1] "2001-01-05" "2001-01-08" "2001-01-09" "2001-01-10" "2001-01-11"
[6] "2001-01-12"


--sundar



From tplate at blackmesacapital.com  Mon Oct 11 18:06:40 2004
From: tplate at blackmesacapital.com (Tony Plate)
Date: Mon, 11 Oct 2004 10:06:40 -0600
Subject: [R] read "4-jan-02" as date
In-Reply-To: <20041011150408.52253.qmail@web50301.mail.yahoo.com>
References: <20041011150408.52253.qmail@web50301.mail.yahoo.com>
Message-ID: <6.1.0.6.2.20041011100138.0b4c21b0@mailhost.blackmesacapital.com>

Works fine when you give as.Date() a character vector.  I suspect the Date 
column in your data frame is a factor.

 > d <- c("12-Jan-01", "11-Jan-01", "10-Jan-01", "9-Jan-01", "8-Jan-01", 
"5-Jan-01")
 > d
[1] "12-Jan-01" "11-Jan-01" "10-Jan-01" "9-Jan-01"  "8-Jan-01"  "5-Jan-01"
 > as.Date(d, format="%d-%b-%y")
[1] "2001-01-12" "2001-01-11" "2001-01-10" "2001-01-09" "2001-01-08"
[6] "2001-01-05"
 > as.Date(factor(d), format="%d-%b-%y")
Error in fromchar(x) : character string is not in a standard unambiguous format
 >

Hope this helps,

Tony Plate

At Monday 09:04 AM 10/11/2004, bogdan romocea wrote:
>Dear R users,
>
>I have a column with dates (character) in a data frame:
>12-Jan-01 11-Jan-01 10-Jan-01 9-Jan-01  8-Jan-01  5-Jan-01
>and I need to convert them to (Julian) dates so that I can
>sort the whole data frame by date. I thought it would be
>very simple, but after checking the documentation and the
>list I still don't have something that works.
>
>1. as.Date returns the error below. What am I doing wrong?
>As far as I can see the character strings are in standard
>format.
>d$Date <- as.Date(d$Date, format="%d-%b-%y")
>Error in fromchar(x) : character string is not in a
>standard unambiguous format
>
>2. as.date {Survival} produces this error,
>d$Date <- as.date(d$Date, order = "dmy")
>Error in as.date(d$Date, order = "dmy") : Cannot coerce to
>date format
>
>3. Assuming all else fails, is there a text function
>similar to SCAN in SAS? Given a string like "9-Jan-01" and
>"-" as separator, I'd like a function that can read the
>first, second and third values (9, Jan, 01), so that I can
>get Julian dates with mdy.date {survival}.
>
>Thanks in advance,
>b.
>
>______________________________________________
>R-help at stat.math.ethz.ch mailing list
>https://stat.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html



From ripley at stats.ox.ac.uk  Mon Oct 11 18:07:26 2004
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Mon, 11 Oct 2004 17:07:26 +0100 (BST)
Subject: [R] read "4-jan-02" as date
In-Reply-To: <20041011150408.52253.qmail@web50301.mail.yahoo.com>
Message-ID: <Pine.LNX.4.44.0410111705001.2563-100000@gannet.stats>

On Mon, 11 Oct 2004, bogdan romocea wrote:

> Dear R users,
> 
> I have a column with dates (character) in a data frame:
> 12-Jan-01 11-Jan-01 10-Jan-01 9-Jan-01  8-Jan-01  5-Jan-01
> and I need to convert them to (Julian) dates so that I can
> sort the whole data frame by date. I thought it would be
> very simple, but after checking the documentation and the
> list I still don't have something that works.
> 
> 1. as.Date returns the error below. What am I doing wrong?
> As far as I can see the character strings are in standard
> format.
> d$Date <- as.Date(d$Date, format="%d-%b-%y")
> Error in fromchar(x) : character string is not in a
> standard unambiguous format

I get:

> x <- scan(what="")
1: 12-Jan-01 11-Jan-01 10-Jan-01 9-Jan-01  8-Jan-01  5-Jan-01
7:
Read 6 items
> as.Date(x, format="%d-%b-%y")
[1] "2001-01-12" "2001-01-11" "2001-01-10" "2001-01-09" "2001-01-08"
[6] "2001-01-05"

Are you in an English-speaking domain?  Try running in the C locale
(?Sys.setlocale).

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From ggrothendieck at myway.com  Mon Oct 11 18:21:46 2004
From: ggrothendieck at myway.com (Gabor Grothendieck)
Date: Mon, 11 Oct 2004 16:21:46 +0000 (UTC)
Subject: [R] read "4-jan-02" as date
References: <20041011150408.52253.qmail@web50301.mail.yahoo.com>
Message-ID: <loom.20041011T182113-746@post.gmane.org>

> 
: From:   bogdan romocea <br44114 at yahoo.com>
:  
: Dear R users,
: 
: I have a column with dates (character) in a data frame:
: 12-Jan-01 11-Jan-01 10-Jan-01 9-Jan-01 8-Jan-01 5-Jan-01
: and I need to convert them to (Julian) dates so that I can
: sort the whole data frame by date. I thought it would be
: very simple, but after checking the documentation and the
: list I still don't have something that works.
: 
: 1. as.Date returns the error below. What am I doing wrong?
: As far as I can see the character strings are in standard
: format.
: d$Date <- as.Date(d$Date, format="%d-%b-%y")
: Error in fromchar(x) : character string is not in a
: standard unambiguous format

On Windows I ran it and it seems to work:
R> x <- c("12-Jan-01", "11-Jan-01", "10-Jan-01", "9-Jan-01", "8-Jan-01", "5-
Jan-01")
R> y <- as.Date(x, format="%d-%b-%y")
R> y
[1] "2001-01-12" "2001-01-11" "2001-01-10" "2001-01-09" "2001-01-08"
[6] "2001-01-05"
R> R.version.string
[1] "R version 2.0.0, 2004-10-04"

Perhaps you have read in your data as a factor (which is the default
for read.table reading in data frames)?  If that is the case, try using 
as.is = TRUE as an argument to read.table or try this:

	y <- as.Date(as.character(x), "%d-%b-%y") 

: 2. as.date {Survival} produces this error,
: d$Date <- as.date(d$Date, order = "dmy")
: Error in as.date(d$Date, order = "dmy") : Cannot coerce to
: date format

as.date requires a character argument.  Using x and y from above,
either of these should work:

	as.date(as.character(x)) # or just as.date(x) if x is already character
	as.date(as.character(y), "ymd")

: 
: 3. Assuming all else fails, is there a text function
: similar to SCAN in SAS? Given a string like "9-Jan-01" and
: "-" as separator, I'd like a function that can read the
: first, second and third values (9, Jan, 01), so that I can
: get Julian dates with mdy.date {survival}.

	x1 <- sapply(strsplit(as.character(x), split = "-"), 
		function(x) paste(x[2],x[1],x[3],sep="/"))
	as.date(x1)

	# convert from dd-mmm-yy to mmm/dd/yy
	x2 <- sub("(..)-(...)-(..)", "\\2/\\1/\\3", as.character(x))
	as.date(x2)

: 
: Thanks in advance,
: b.
: 

Check out the article in R News Help Desk in issue 4/1 to learn more
about dates.  (Click on Newsletter on left side of www.r-project.org.)



From vha at comcast.net  Mon Oct 11 18:27:27 2004
From: vha at comcast.net (vha@comcast.net)
Date: Mon, 11 Oct 2004 16:27:27 +0000
Subject: [R] Problem with RMySQL-0.5-3.zip/R 2.0.0 on Windows XP 
Message-ID: <101120041627.1404.416AB46F000D8F0F0000057C22007589420E0890@comcast.net>

I have been installing and using the RMySQL package, version 0.5-3 with R versions 1.9.0 and 1.9.1 on Windows XP without  any problem. I installed the package using the .zip binary package available at http://stat.bell-labs.com/RS-DBI/download/index.html.

However,  I ran into a problem using this package with the latest version 2.0.0 of R. The installation went ok, but loading the library using

library(RMySQL)

caused the following error:

Error in library(RMySQL) : 'RMySQL' is not a valid package -- installed < 2.0.0?

Any comment/suggestion/advice on how to resolve this issue is greatly appreciated. 

Vu Ha, PhD
Information and Decision Technologies
Honeywell International
3660 Technology Dr. 
Minneapolis, MN 55418 
612-951-7114



From p.dalgaard at biostat.ku.dk  Mon Oct 11 18:50:06 2004
From: p.dalgaard at biostat.ku.dk (Peter Dalgaard)
Date: 11 Oct 2004 18:50:06 +0200
Subject: [R] read "4-jan-02" as date
In-Reply-To: <20041011150408.52253.qmail@web50301.mail.yahoo.com>
References: <20041011150408.52253.qmail@web50301.mail.yahoo.com>
Message-ID: <x2vfdhnqrl.fsf@biostat.ku.dk>

bogdan romocea <br44114 at yahoo.com> writes:

> Dear R users,
> 
> I have a column with dates (character) in a data frame:
> 12-Jan-01 11-Jan-01 10-Jan-01 9-Jan-01  8-Jan-01  5-Jan-01
> and I need to convert them to (Julian) dates so that I can
> sort the whole data frame by date. I thought it would be
> very simple, but after checking the documentation and the
> list I still don't have something that works.
> 
> 1. as.Date returns the error below. What am I doing wrong?
> As far as I can see the character strings are in standard
> format.
> d$Date <- as.Date(d$Date, format="%d-%b-%y")
> Error in fromchar(x) : character string is not in a
> standard unambiguous format

Odd. Works for me:

> as.Date("4-Jan-02", format="%d-%b-%y")
[1] "2002-01-04"
 
Which is your R version (and OS, this stuff sometimes relies on system
routines)? 

You do have to watch out for locale dependencies though:

> as.Date("4-Okt-02", format="%d-%b-%y")
[1] NA

and vice versa if you try to read "Oct" in a Danish locale. However,
shouldn't give the error message that you see.  
 
> 2. as.date {Survival} produces this error,
> d$Date <- as.date(d$Date, order = "dmy")
> Error in as.date(d$Date, order = "dmy") : Cannot coerce to
> date format
> 
> 3. Assuming all else fails, is there a text function
> similar to SCAN in SAS? Given a string like "9-Jan-01" and
> "-" as separator, I'd like a function that can read the
> first, second and third values (9, Jan, 01), so that I can
> get Julian dates with mdy.date {survival}.

gsub() should do the trick, e.g.:

> x
[1] "4-jan-02"
> gsub("([^-]*)-([^-]*)-([^-]*)", "\\1", x)
[1] "4"
> gsub("([^-]*)-([^-]*)-([^-]*)", "\\2", x)
[1] "jan"
> gsub("([^-]*)-([^-]*)-([^-]*)", "\\3", x)
[1] "02"


-- 
   O__  ---- Peter Dalgaard             Blegdamsvej 3  
  c/ /'_ --- Dept. of Biostatistics     2200 Cph. N   
 (*) \(*) -- University of Copenhagen   Denmark      Ph: (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)             FAX: (+45) 35327907



From lauraholt_983 at hotmail.com  Mon Oct 11 19:51:25 2004
From: lauraholt_983 at hotmail.com (Laura Holt)
Date: Mon, 11 Oct 2004 12:51:25 -0500
Subject: [R] nlm question
Message-ID: <BAY12-F414FR0rmTIVv00003cbc@hotmail.com>

Dear R People:

I am trying to duplicate the example from Dennis and Schnabel's "Numerical 
Methods for Unconstrained Optimization and Nonlinear Equations", which 
starts on page 149.

My reason for doing so:  to try to understand the "nlm" function.

Here is the function:
>mfun1
function(x) {
        z <- matrix(0,nrow=2,ncol=1)
        z[1,1] <- x[1]^2 + x[2]^2 - 2
        z[2,1] <- exp(x[1]-1) + x[2]^3 - 2
        res <- 0.5*t(z)%*%z
        res
}

This function has a root at c(1,1).
When I use the following:
>nlm(mfun1,c(2,0.5))
$minimum
[1] 0.09168083

$estimate
[1]  1.485078e+00 -4.973395e-07

$gradient
[1] -8.120649e-09  1.096345e-09

$code
[1] 1

$iterations
[1] 19

I get the solution of 1.485078 and zero.

Now when I use:
>nlm(mfun1,c(1.2,0.85))
$minimum
[1] 2.025965e-12

$estimate
[1] 1.000001 0.999999

$gradient
[1] 6.799786e-08 6.520232e-08

$code
[1] 1

$iterations
[1] 8

>
I get the appropriate answer.

What can I put into my code to improve the estimates, please?

Thanks
Sincerely,
Laura Holt
R Version 2.0.0
Windows
mailto: lauraholt_938 at hotmail.com



From pauljohn at ku.edu  Mon Oct 11 20:08:39 2004
From: pauljohn at ku.edu (Paul Johnson)
Date: Mon, 11 Oct 2004 13:08:39 -0500
Subject: [R] Diagnosing trouble with R-2.0, Fedora Core 2, and Rcmdf
Message-ID: <416ACC27.6030806@ku.edu>

Greetings, R-help!

On 2 Fedora Core 2 Linux systems, i've completely erased the previous R 
and all packages and then installed R-2.0 and installed fresh packages.

In using Rcmdr, I see some trouble and I wonder if other people see this 
and if it is due to the tcl/tk, or R, or Rcmdr.  (If readers have not 
yet tried Rcmdr, I recommend it not just because of the GUI it provides, 
but also because Prof. Fox has created several very handy commands, like 
scatter3d, which make it possible to use advanced packages like rgl. 
Rcmdr provides several very handy wrapper commands that "just work" and 
users don't have to fuss over so many details! And, if you are new at R, 
you can use pull down menus and then Rcmdr displays the commands that 
correspond with those menu options. Very educational!)

In no particular order, here are the issues I see with R-2.0 and Rcmdr

1. In the panel to select a dataset from a package (nenu: Data/Data in 
packages), I can type in the name of a dataset, but the GUI will not let 
me choose the name of a package from which to select a dataset. Nothing 
happens when I pick a package name.  (With previous R/Rcmdr, I did not 
have this trouble).

2. When using Rcmdr, some of the output from commands shows in the Rcmdr 
bottom window, but some is sent to the xterm in which R was started. For 
example, with a scatter3d() command, if I add the option 
model.summary=T, the model summary shows in the xterm.  But some other 
models show results in the bottom pane of Rcmdr.  And, it seems to me, 
there are some commands in which I've noticed some of the output going 
to the Rcmdr bottom buffer and  some to the xterm.

3. When I want to exit Rcmdr and choose the pull down "exit from 
Commander and R", I get a Segmentation Fault that closes Rcmdr and R.

4. 3d plots do not display on the screen until I click in the rgl window 
that displays the graph.  This might be a video card/AGP driver problem, 
I suppose.  These systems have the NVidia FX5200 cards and the NVidia 
proprietary X driver.  If other users don't see the same on other kinds 
of systems, I guess that will tell me where the problem lies.

5. At random times, I get an error tcl/tk window popping up with a 
message about MASS and categorical variables.  It says

Error in data(package=parse(text=packageName));
'MASS' must be character string or NULL

It says that over and over, sometimes it has other package names, as in:

Error in data(package=parse(text=packageName));
'relimp' must be character string or NULL

These seem to be harmless?
-- 
Paul E. Johnson                       email: pauljohn at ku.edu
Dept. of Political Science            http://lark.cc.ku.edu/~pauljohn
1541 Lilac Lane, Rm 504
University of Kansas                  Office: (785) 864-9086
Lawrence, Kansas 66044-3177           FAX: (785) 864-5700



From ripley at stats.ox.ac.uk  Mon Oct 11 20:12:00 2004
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Mon, 11 Oct 2004 19:12:00 +0100 (BST)
Subject: [R] Problem with RMySQL-0.5-3.zip/R 2.0.0 on Windows XP 
In-Reply-To: <101120041627.1404.416AB46F000D8F0F0000057C22007589420E0890@comcast.net>
Message-ID: <Pine.LNX.4.44.0410111910580.2715-100000@gannet.stats>

On Mon, 11 Oct 2004 vha at comcast.net wrote:

> I have been installing and using the RMySQL package, version 0.5-3 with
> R versions 1.9.0 and 1.9.1 on Windows XP without any problem. I
> installed the package using the .zip binary package available at
> http://stat.bell-labs.com/RS-DBI/download/index.html.
> 
> However, I ran into a problem using this package with the latest version
> 2.0.0 of R. The installation went ok, but loading the library using
> 
> library(RMySQL)
> 
> caused the following error:
> 
> Error in library(RMySQL) : 'RMySQL' is not a valid package -- installed < 2.0.0?
> 
> Any comment/suggestion/advice on how to resolve this issue is greatly appreciated. 

You need to build it from the sources under R 2.0.0 unless/until someone 
provides a binary for 2.0.0.


-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From tlumley at u.washington.edu  Mon Oct 11 20:15:44 2004
From: tlumley at u.washington.edu (Thomas Lumley)
Date: Mon, 11 Oct 2004 11:15:44 -0700 (PDT)
Subject: [R] install failure Ruuid package on OS X
In-Reply-To: <BD907730.8997%i.visser@uva.nl>
References: <BD907730.8997%i.visser@uva.nl>
Message-ID: <Pine.A41.4.61.0410111113060.80970@homer08.u.washington.edu>

On Mon, 11 Oct 2004, Ingmar Visser wrote:

> Dear All,
>
> When installing the Ruuid package (from Bioconductor) from sources on my
> MAC (OS X 10.3.5, R version 2.0.0) I get the following errors:

Yes.  You need the "GLib" library (eg 
http://developer.gnome.org/doc/API/2.0/glib/glib-building.html). It needs 
pkg-config, and IIRC, pkg-config needs something else.

It's frustrating, but the process does terminate eventually.

The Bioconductor list may be more helpful

 	-thomas


>
> m00245:~ ivisser$ R CMD INSTALL  -l /Users/ivisser/Library/R/library/
> /Users/ivisser/Desktop/Ruuid
> * Installing *source* package 'Ruuid' ...
> loading cache ./config.cache
> checking for glib-config... no
> checking how to run the C preprocessor... (cached) cc -E
> checking for /usr/include/unistd.h... (cached) yes
> creating ./config.status
> creating src/Makevars
> ** libs
> gcc -no-cpp-precomp -I/Library/Frameworks/R.framework/Resources/include
> -I/usr/local/include/glib12 -DHAVE_UNISTD_H=1 -I/sw/include
> -I/usr/local/include   -fno-common  -g -O2 -c Rinit.c -o Rinit.o
> In file included from Ruuid.h:5,
>                 from Rinit.c:1:
> uuidP.h:24:18: glib.h: No such file or directory
> In file included from Ruuid.h:5,
>                 from Rinit.c:1:
> uuidP.h:36: error: parse error before "guint32"
> uuidP.h:36: warning: no semicolon at end of struct or union
> uuidP.h:37: warning: data definition has no type or storage class
> uuidP.h:38: error: parse error before "time_hi_and_version"
> uuidP.h:38: warning: data definition has no type or storage class
> uuidP.h:39: error: parse error before "clock_seq"
> uuidP.h:39: warning: data definition has no type or storage class
> uuidP.h:40: error: parse error before "node"
> uuidP.h:40: warning: data definition has no type or storage class
> uuidP.h:41: error: parse error before '}' token
> make: *** [Rinit.o] Error 1
> ERROR: compilation failed for package 'Ruuid'
> ** Removing '/Users/ivisser/Library/R/library/Ruuid'
> ** Restoring previous '/Users/ivisser/Library/R/library/Ruuid'
> m00245:~ ivisser$
>
>
> Any help is appreciated,
>
> best, ingmar
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>

Thomas Lumley			Assoc. Professor, Biostatistics
tlumley at u.washington.edu	University of Washington, Seattle



From vha at comcast.net  Mon Oct 11 20:24:40 2004
From: vha at comcast.net (vha@comcast.net)
Date: Mon, 11 Oct 2004 18:24:40 +0000
Subject: [R] Problem with RMySQL-0.5-3.zip/R 2.0.0 on Windows XP
Message-ID: <101120041824.22009.416ACFE800011718000055F922007354460E0890@comcast.net>

Thank you very much, David. I installed the compiled RMySQL zip package and it works fine now. 

Witold Eryk Wolski also suggested the following way to get around having to recompile. 

<quote>
In the package directory is directory called meta. There are some *.rds 
files generated I during the build, preparing the package for lazy eval. 
(see last Rnew BDRipleys article)
copy package.rds nad Rd.rds from any of the packages build for R2.0.0 to 
the meta directory. This should solve your problem.
</quote>

This however did not work for me. I can load the library ok, but when I make a call to dbDriver, I got a complain about dbiDriver. 

Vu~


> Dear Vu Ha,
> 
> I just re-complied the RMySQL package on Windows for R 2.0.0.  Please
> see attached file zip (you need to rename it to RMySQL_0.5-5.zip).
> Could you please tell me if that works for you?
> 
> Regards,
> 
> --
> David
> 
> vha at comcast.net wrote:
> > I have been installing and using the RMySQL package, version 0.5-3 with R 
> versions 1.9.0 and 1.9.1 on Windows XP without  any problem. I installed the 
> package using the .zip binary package available at 
> http://stat.bell-labs.com/RS-DBI/download/index.html.
> > 
> > However,  I ran into a problem using this package with the latest version 
> 2.0.0 of R. The installation went ok, but loading the library using
> > 
> > library(RMySQL)
> > 
> > caused the following error:
> > 
> > Error in library(RMySQL) : 'RMySQL' is not a valid package -- installed < 
> 2.0.0?
> > 
> > Any comment/suggestion/advice on how to resolve this issue is greatly 
> appreciated. 
> > 
> > Vu Ha, PhD
> > Information and Decision Technologies
> > Honeywell International
> > 3660 Technology Dr. 
> > Minneapolis, MN 55418 
> > 612-951-7114
> > 
> > ______________________________________________
> > R-help at stat.math.ethz.ch mailing list
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html



From tlumley at u.washington.edu  Mon Oct 11 20:28:58 2004
From: tlumley at u.washington.edu (Thomas Lumley)
Date: Mon, 11 Oct 2004 11:28:58 -0700 (PDT)
Subject: [R] nlm question
In-Reply-To: <BAY12-F414FR0rmTIVv00003cbc@hotmail.com>
References: <BAY12-F414FR0rmTIVv00003cbc@hotmail.com>
Message-ID: <Pine.A41.4.61.0410111119450.80970@homer08.u.washington.edu>

On Mon, 11 Oct 2004, Laura Holt wrote:

> Dear R People:
>
> I am trying to duplicate the example from Dennis and Schnabel's "Numerical 
> Methods for Unconstrained Optimization and Nonlinear Equations", which starts 
> on page 149.
>
> My reason for doing so:  to try to understand the "nlm" function.
>
> Here is the function:
>> mfun1
> function(x) {
>       z <- matrix(0,nrow=2,ncol=1)
>       z[1,1] <- x[1]^2 + x[2]^2 - 2
>       z[2,1] <- exp(x[1]-1) + x[2]^3 - 2
>       res <- 0.5*t(z)%*%z
>       res
> }
>
> This function has a root at c(1,1).
> When I use the following:
>> nlm(mfun1,c(2,0.5))
> $minimum
> [1] 0.09168083
>
> $estimate
> [1]  1.485078e+00 -4.973395e-07
>
> $gradient
> [1] -8.120649e-09  1.096345e-09
>
> $code
> [1] 1
>
> $iterations
> [1] 19
>
> I get the solution of 1.485078 and zero.
>

You might want to look at the function surface, eg:

   dd<-expand.grid(x=seq(0,2,length=20),y=seq(0,2,length=20))
   yy<-apply(dd,1,mfun1)
   contour(seq(0,2,length=20),seq(0,2,length=20),matrix(yy,20),nlevels=40)

The function is nearly flat on a fairly wide band where either x or y is 1 
or slightly larger, and so is far from quadratic in that region.  You need 
better starting values, or some rescaling of the objective function.

Another optimisation algorithm might give different results, and in fact 
if you try optim() you end up at (1,1) with method="Nelder-Mead" and 
method="BFGS", but at (1.48,0) with "CG" or "L-BFGS-B".


 	-thomas



From zhliur at yahoo.com  Mon Oct 11 20:55:01 2004
From: zhliur at yahoo.com (yyan liu)
Date: Mon, 11 Oct 2004 11:55:01 -0700 (PDT)
Subject: [R] memory in R
Message-ID: <20041011185501.22040.qmail@web90106.mail.scd.yahoo.com>

Hi:
  I am doing a MCMC algorithm which is well known to
consume much computer memory. And I have a problem
everytime I run my R program. It stopped at certain
iteration and says "can not allocate a vector of 19
kb".
It seems that the computer's memory has been
exhausted. However, it is said that after each
iteration the objects (such as a huge matrix) can be
set to NULL. And the memory will be released so the
program will consume as much memory as before. I
wonder how to do that, that is, set the object to be
NULL. Say, A is a matrix in each iteration. I just
need to write A<-NULL ?? And this approach really
works?
  Thank you very much!

liu 


		
_______________________________

Declare Yourself - Register online to vote today!



From vograno at evafunds.com  Mon Oct 11 21:05:21 2004
From: vograno at evafunds.com (Vadim Ogranovich)
Date: Mon, 11 Oct 2004 12:05:21 -0700
Subject: [R] memory in R
Message-ID: <C698D707214E6F4AB39AB7096C3DE5A56D8B30@phost015.EVAFUNDS.intermedia.net>

Setting A <- NULL doesn't immediately release the memory, the memory is
actually released in gc(), which R calls for you at some "random" time.
In situations like this I explicitely call gc() and do not wait for R to
do this, e.g
A <- NULL; gc()

Hope this helps,
Vadim

> -----Original Message-----
> From: r-help-bounces at stat.math.ethz.ch 
> [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of yyan liu
> Sent: Monday, October 11, 2004 11:55 AM
> To: r-help at stat.math.ethz.ch
> Subject: [R] memory in R
> 
> Hi:
>   I am doing a MCMC algorithm which is well known to consume 
> much computer memory. And I have a problem everytime I run my 
> R program. It stopped at certain iteration and says "can not 
> allocate a vector of 19 kb".
> It seems that the computer's memory has been exhausted. 
> However, it is said that after each iteration the objects 
> (such as a huge matrix) can be set to NULL. And the memory 
> will be released so the program will consume as much memory 
> as before. I wonder how to do that, that is, set the object 
> to be NULL. Say, A is a matrix in each iteration. I just need 
> to write A<-NULL ?? And this approach really works?
>   Thank you very much!
> 
> liu 
> 
> 
> 		
> _______________________________
> 
> Declare Yourself - Register online to vote today!
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
>



From sgilpin at gmail.com  Mon Oct 11 21:10:46 2004
From: sgilpin at gmail.com (Scott Gilpin)
Date: Mon, 11 Oct 2004 13:10:46 -0600
Subject: [R] scoping problem when calling step inside a function
Message-ID: <5739cc2f0410111210642eb47d@mail.gmail.com>

Hi everyone -

I'm trying to do a forward stepwise regression (I've tried both step
and stepAIC) inside of a function.  I can do it outside the function
with no problems (first example in code below).  I can also do a
backward stepwise regression inside a function (second example), but
forward stepwise regression ( third example ) fails with the error:

"Error in model.frame.default(formula = y ~ X1 + X2 + X3 + X4 + X5 + X6 +  : 
        Object "lsdata" not found"

I looked through the help archives and found several threads where the
problem appears to be the environment which is used in eval.  When I
looked at the source for step, it appears OK, as it's using
eval.parent.  I also read section 7.12 of the FAQ about enclosing and
parent environments, but this leads me to belive that step is doing
the right thing.  I've tried this code with R version 1.9.0, 1.9.1 and
2.0.0 on Windows XP.  The error message is slightly different for
2.0.0, but it seems to be the same problem.

Thanks in advance,
Scott Gilpin


## Example code
rm(list=ls())

## Forward stepwise regression - works fine
set.seed(2863)
x<-matrix(runif(1000),ncol=10)
colnames(x)<-1:10
beta<-matrix(c(1,2,3,4,5,0,0,0,0,0),ncol=1)
y<-drop(x %*% beta + rnorm(100))

lsdata<-data.frame(cbind(x=x,y=y))

xnam <- names(lsdata)[names(lsdata) != "y"]
(fmla <- as.formula(paste("y ~ ", paste(xnam, collapse= "+"))))
fit<-lm(y ~ 1,data=lsdata)
step.fit<-step(fit,scope = fmla, direction="forward",data=lsdata)

## start from scratch
rm(list=ls())
set.seed(2863)
x<-matrix(runif(1000),ncol=10)
colnames(x)<-1:10
beta<-matrix(c(1,2,3,4,5,0,0,0,0,0),ncol=1)
y<-drop(x %*% beta + rnorm(100))

## backward stepwise in a function - works fine
backward.step<-function(x,y,...) {

   lsdata<-data.frame(cbind(x=x,y=y))

   xnam <- names(lsdata)[names(lsdata) != "y"]
   (fmla <- as.formula(paste("y ~ ", paste(xnam, collapse= "+"))))
   fit<-lm(fmla,data=lsdata)
   step.fit<-step(fit, direction="backward",data=lsdata)
   step.fit

}

backward.fit<-backward.step(x,y)

## forward stepwise in a function - error

forward.step<-function(x,y,...) {

   lsdata<-data.frame(cbind(x=x,y=y))

   xnam <- names(lsdata)[names(lsdata) != "y"]
   (fmla <- as.formula(paste("y ~ ", paste(xnam, collapse= "+"))))
   fit<-lm(y ~ 1,data=lsdata)
   step.fit<-step(fit,scope = fmla, direction="forward",data=lsdata)
   step.fit

}

forward.fit<-forward.step(x,y)



From ripley at stats.ox.ac.uk  Mon Oct 11 21:15:18 2004
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Mon, 11 Oct 2004 20:15:18 +0100 (BST)
Subject: [R] memory in R
In-Reply-To: <C698D707214E6F4AB39AB7096C3DE5A56D8B30@phost015.EVAFUNDS.intermedia.net>
Message-ID: <Pine.LNX.4.44.0410112013050.2825-100000@gannet.stats>

On Mon, 11 Oct 2004, Vadim Ogranovich wrote:

> Setting A <- NULL doesn't immediately release the memory, the memory is
> actually released in gc(), which R calls for you at some "random" time.
> In situations like this I explicitely call gc() and do not wait for R to
> do this, e.g
> A <- NULL; gc()

`Some random time' always includes immediately before giving an 
out-of-memory error message.

What's wrong with rm(A), as used in the first session in `An Introduction 
to R'?

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From p.dalgaard at biostat.ku.dk  Mon Oct 11 21:31:41 2004
From: p.dalgaard at biostat.ku.dk (Peter Dalgaard)
Date: 11 Oct 2004 21:31:41 +0200
Subject: [R] nlm question
In-Reply-To: <Pine.A41.4.61.0410111119450.80970@homer08.u.washington.edu>
References: <BAY12-F414FR0rmTIVv00003cbc@hotmail.com>
	<Pine.A41.4.61.0410111119450.80970@homer08.u.washington.edu>
Message-ID: <x2r7o5njaa.fsf@biostat.ku.dk>

Thomas Lumley <tlumley at u.washington.edu> writes:

> You might want to look at the function surface, eg:
> 
>    dd<-expand.grid(x=seq(0,2,length=20),y=seq(0,2,length=20))
>    yy<-apply(dd,1,mfun1)
>    contour(seq(0,2,length=20),seq(0,2,length=20),matrix(yy,20),nlevels=40)
> 
> The function is nearly flat on a fairly wide band where either x or y
> is 1 or slightly larger, and so is far from quadratic in that region.
> You need better starting values, or some rescaling of the objective
> function.

A slight modification is quite revealing:

 dd<-expand.grid(x=seq(-1,2,length=100),y=seq(-1,2,length=100))
 yy<-apply(dd,1,mfun1)
 persp(seq(-1,2,length=100),seq(-1,2,length=100),matrix(yy,100)^.1)

 contour(seq(-1,2,length=100),seq(-1,2,length=100),matrix(yy,100)^.1)
 abline(v=1.485,h=0)

> Another optimisation algorithm might give different results, and in
> fact if you try optim() you end up at (1,1) with method="Nelder-Mead"
> and method="BFGS", but at (1.48,0) with "CG" or "L-BFGS-B".

And SANN found (1,1) two times out of three for me, the third one
getting stuck in the 3rd local minimum at (-0.72,1.22)
-- 
   O__  ---- Peter Dalgaard             Blegdamsvej 3  
  c/ /'_ --- Dept. of Biostatistics     2200 Cph. N   
 (*) \(*) -- University of Copenhagen   Denmark      Ph: (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)             FAX: (+45) 35327907



From ggrothendieck at myway.com  Mon Oct 11 21:37:51 2004
From: ggrothendieck at myway.com (Gabor Grothendieck)
Date: Mon, 11 Oct 2004 19:37:51 +0000 (UTC)
Subject: [R] memory in R
References: <20041011185501.22040.qmail@web90106.mail.scd.yahoo.com>
Message-ID: <loom.20041011T213424-475@post.gmane.org>

yyan liu <zhliur <at> yahoo.com> writes:

: 
: Hi:
:   I am doing a MCMC algorithm which is well known to
: consume much computer memory. And I have a problem
: everytime I run my R program. It stopped at certain
: iteration and says "can not allocate a vector of 19
: kb".
: It seems that the computer's memory has been
: exhausted. However, it is said that after each
: iteration the objects (such as a huge matrix) can be
: set to NULL. And the memory will be released so the
: program will consume as much memory as before. I
: wonder how to do that, that is, set the object to be
: NULL. Say, A is a matrix in each iteration. I just
: need to write A<-NULL ?? And this approach really
: works?
:   Thank you very much!
: 
: liu 


Instead of, or in addition to, fighting with garbage collection, 
you might be able to find some combo of the command line options 
that allow you to run straight way.  See ?Memory and play with the 
various options to R such as:

  --min-vsize=400M --min-nsize=6M



From jeroschh at ohsu.edu  Mon Oct 11 21:41:35 2004
From: jeroschh at ohsu.edu (Michael Jerosch-Herold)
Date: Mon, 11 Oct 2004 12:41:35 -0700
Subject: [R] lmList - strange problem
Message-ID: <s16a7f8b.063@ohsu.edu>


I am using lmList, which takes 'Data' partitioned according to the
levels of a grouping factor (gf) and individual 'lm' fits are obtained
for each 'data'partition, using a model defined as in "lm". So my call
to lmList looks something like:

> mg.lis <- lmList(strmbf ~ age1c + gender1 +  sysbp.clinic +
+           diabp.clinic + ldl1 + mets.total + bmi1c | gf,
+           data = Data, subset= avblock=="normal");

but I get a strange error message: 

"Error in model.frame.default(formula = form, data = dat, na.action =
na.action,  : object is not a matrix"

If I remove either mets.total or bmi1c from my model, then the routine
works fine, i.e. it works when either of the two variables are in the
model, but not when I add one both variables to the model. Similarly I
can remove any other variable and put both mets.total and bmi1c back
into the model, and it works. It seems like the number of model
variables can not exceed 7 for some reason I fail to understand. 

I have 1809 cases and the gf factor subdivides this into 3 about
equally sized groups. Therefore the model could have much more than 8
degrees of freedom. Also I removed all cases from the dataframe with
"NA", so NA's can not explain the problem either.
Any suggestions of what could have gone wrong?

As further information I also add that I used "lme" in the same "nlme"
package to perform multi-level regression modeling (linear mixed
effects) and do not have any similar problems. Also I can "manually"
perform the analysis with "lm" for the data subsets corresponding to
each of the 3 levels of the grouping factor (gf is the grouping
variable), without any problem.

thanks in advance for help!

Michael Jerosch-Herold



From KalayliogluZ at imsweb.com  Mon Oct 11 22:21:31 2004
From: KalayliogluZ at imsweb.com (Kalaylioglu, Zeynep (IMS))
Date: Mon, 11 Oct 2004 16:21:31 -0400
Subject: [R] colClasses
Message-ID: <782D2A81EC812642B857B03B506E0B4404B73F@granite.omni.imsweb.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20041011/27039cea/attachment.pl

From mseewald at gmx.de  Mon Oct 11 22:04:07 2004
From: mseewald at gmx.de (Michael Seewald)
Date: Mon, 11 Oct 2004 22:04:07 +0200 (CEST)
Subject: [R] 64-bit R on Intel Xeon EM64T running fine
Message-ID: <Pine.LNX.4.53.0410112148540.3983@lakeforest.homelinux.com>


Dear mailing-list members,

In the days of cheap RAM and microarray applications feasting on memory,
64-bit computers become more and more useful - to actually make use of memory
beyond the magic 4GB border. I would like to report the success of running
64-bit R on an Intel Xeon EM64T machine under Linux. Just like on an AMD
Opteron, R v2.0.0 compiles fine (and out of the box) and is happily allocating
memory until RAM and swap reach their limit.

Hardware:
- HP xw6200 workstation
- dual Intel Xeon 3.4GHz with hyper-threading enabled
- 4GB RAM, 4GB swap

System: either
- Fedora Core 2 x86_64 bit Linux
or
- Red Hat Enterprise Linux Workstation 3.0 x86_64 bit

R:
- v2.0.0

Really, no problems at all during setup, a big thank you to the R developers
making this possible!

Best wishes,
Michael



From murdoch at stats.uwo.ca  Mon Oct 11 22:26:25 2004
From: murdoch at stats.uwo.ca (Duncan Murdoch)
Date: Mon, 11 Oct 2004 16:26:25 -0400
Subject: [R] colClasses
In-Reply-To: <782D2A81EC812642B857B03B506E0B4404B73F@granite.omni.imsweb.com>
References: <782D2A81EC812642B857B03B506E0B4404B73F@granite.omni.imsweb.com>
Message-ID: <e1rlm0173fgo0g86vaksivq1l7k1s4cm1r@4ax.com>

On Mon, 11 Oct 2004 16:21:31 -0400, "Kalaylioglu, Zeynep (IMS)"
<KalayliogluZ at imsweb.com> wrote:

>Hi
>I am trying to read a data frame from a text editor in to R. I want some
>of the columns to be read in as "character" not numeric.
>I figured that I can do that by using "colClasses" in  "read.table"
>command. However, I couldn't find out how to use 
>"colClasses". e.g. say I have 5 column in the data file. I want 1st and
>3rd column to be read in as character. How can I define
>this using colClasses?  (or is there a better way to do what I want?)

The help page for read.table looks reasonably clear to me.  Wouldn't
colClasses = c('character', 'numeric', 'character', 'numeric',
'numeric')

work?

Duncan Murdoch



From rpeng at jhsph.edu  Mon Oct 11 22:28:21 2004
From: rpeng at jhsph.edu (Roger D. Peng)
Date: Mon, 11 Oct 2004 16:28:21 -0400
Subject: [R] colClasses
In-Reply-To: <782D2A81EC812642B857B03B506E0B4404B73F@granite.omni.imsweb.com>
References: <782D2A81EC812642B857B03B506E0B4404B73F@granite.omni.imsweb.com>
Message-ID: <416AECE5.4000201@jhsph.edu>

When using colClasses you need to specify the types of all the 
columns.  So assuming the other columns are numeric data, you could set

colClasses = c("character", "numeric", "character", "numeric", "numeric")

-roger

Kalaylioglu, Zeynep (IMS) wrote:
> Hi
> I am trying to read a data frame from a text editor in to R. I want some
> of the columns to be read in as "character" not numeric.
> I figured that I can do that by using "colClasses" in  "read.table"
> command. However, I couldn't find out how to use 
> "colClasses". e.g. say I have 5 column in the data file. I want 1st and
> 3rd column to be read in as character. How can I define
> this using colClasses?  (or is there a better way to do what I want?)
>  
> Thanks.
> Zeynep
>  
>  
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>



From wolski at molgen.mpg.de  Mon Oct 11 22:33:20 2004
From: wolski at molgen.mpg.de (Witold Eryk Wolski)
Date: Mon, 11 Oct 2004 22:33:20 +0200
Subject: [R] colClasses
In-Reply-To: <782D2A81EC812642B857B03B506E0B4404B73F@granite.omni.imsweb.com>
References: <782D2A81EC812642B857B03B506E0B4404B73F@granite.omni.imsweb.com>
Message-ID: <416AEE10.1080401@molgen.mpg.de>

I would try.
colClasses=c("character","numeric","character","numeric","numeric")

/E

Kalaylioglu, Zeynep (IMS) wrote:

>Hi
>I am trying to read a data frame from a text editor in to R. I want some
>of the columns to be read in as "character" not numeric.
>I figured that I can do that by using "colClasses" in  "read.table"
>command. However, I couldn't find out how to use 
>"colClasses". e.g. say I have 5 column in the data file. I want 1st and
>3rd column to be read in as character. How can I define
>this using colClasses?  (or is there a better way to do what I want?)
> 
>Thanks.
>Zeynep
> 
> 
>
>	[[alternative HTML version deleted]]
>
>______________________________________________
>R-help at stat.math.ethz.ch mailing list
>https://stat.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>
>  
>


-- 
Dipl. bio-chem. Witold Eryk Wolski         
MPI-Moleculare Genetic
Ihnestrasse 63-73 14195 Berlin                 _
tel: 0049-30-83875219                 __("<   'v'
http://www.molgen.mpg.de/~wolski      \__/   /   \
mail: witek96 at users.sourceforge.net    ^^     w w
      wolski at molgen.mpg.de



From p.dalgaard at biostat.ku.dk  Mon Oct 11 22:42:27 2004
From: p.dalgaard at biostat.ku.dk (Peter Dalgaard)
Date: 11 Oct 2004 22:42:27 +0200
Subject: [R] colClasses
In-Reply-To: <782D2A81EC812642B857B03B506E0B4404B73F@granite.omni.imsweb.com>
References: <782D2A81EC812642B857B03B506E0B4404B73F@granite.omni.imsweb.com>
Message-ID: <x2mzytng0c.fsf@biostat.ku.dk>

"Kalaylioglu, Zeynep (IMS)" <KalayliogluZ at imsweb.com> writes:

> Hi
> I am trying to read a data frame from a text editor in to R. I want some
> of the columns to be read in as "character" not numeric.
> I figured that I can do that by using "colClasses" in  "read.table"
> command. However, I couldn't find out how to use 
> "colClasses". e.g. say I have 5 column in the data file. I want 1st and
> 3rd column to be read in as character. How can I define
> this using colClasses?  (or is there a better way to do what I want?)

The easiest is probably

  read.table(....as.is=c(1,3)....)

but you could also do 

  cC <- rep(NA,5)
  cC[c(1,3)] <- "character"
  read.table(....colClasses=cC...)

which would be more readily generalized to "stranger" data types.

-- 
   O__  ---- Peter Dalgaard             Blegdamsvej 3  
  c/ /'_ --- Dept. of Biostatistics     2200 Cph. N   
 (*) \(*) -- University of Copenhagen   Denmark      Ph: (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)             FAX: (+45) 35327907



From s-luppescu at uchicago.edu  Mon Oct 11 23:14:25 2004
From: s-luppescu at uchicago.edu (Stuart Luppescu)
Date: Mon, 11 Oct 2004 16:14:25 -0500
Subject: [R] split and rlm
In-Reply-To: <Pine.LNX.4.44.0410111547540.2465-100000@gannet.stats>
References: <Pine.LNX.4.44.0410111547540.2465-100000@gannet.stats>
Message-ID: <1097529265.16842.23.camel@musuko.uchicago.edu>

On Mon, 2004-10-11 at 15:54 +0100, Prof Brian Ripley wrote:
> On Mon, 11 Oct 2004, Stuart Luppescu wrote:
> 0) Do use extractor function like coef() and df.residual().
> 
> 1) for the 100 fits try using the default interface, not the formula one.
> 
> 2) df.residual is not a relevant concept, and the coefficients are not 
> t-distributed.  Where did you read that it had?
> 
> You can try a normal approximation, but beware it may be rough.  If you
> look up MASS4 (the book this software supports) you will see better ideas
> illustrated.

Thanks to Brian Ripley, Andy Liaw and Peter Dalgaard for their help on
this. One problem I still have is with the distribution of the
coefficients. I don't have MASS4 (only MASS1, MASS2 and MASS3), but I
couldn't find anything in MASS3. 

-- 
Stuart Luppescu -=- s-luppescu .at. uchicago.edu        
University of Chicago -=- CCSR 
$B:MJ8$HCRF`H~$NIc(B -=-    Kernel 2.6.8-gentoo-r3                
"But the true threats to stability and peace are
 those nations that are not very transparent, that 
 hide behind the --- that don't let people in to
 take a look and see what they're up to.  They're
 very kind of authoritarian regimes. The true



From nair at sdsc.edu  Tue Oct 12 00:16:14 2004
From: nair at sdsc.edu (T. Murlidharan Nair)
Date: Mon, 11 Oct 2004 15:16:14 -0700
Subject: [R] plotting with multiple files
Message-ID: <416B062E.90605@sdsc.edu>

Hi !!
I need to generate plots from several multiple file and I am doing this 
by reading it as a list using c(file1, file2.....).
Since I need to either print the plots or save it as an plot image while 
generating  the plots in a loop, has anyone
done this before. Also is there a getchar() equivalent in R, I mean just 
to wait for an input from the console so that
I can print before it plots the next plot.
Thanks ../Murli



From nair at sdsc.edu  Tue Oct 12 02:02:06 2004
From: nair at sdsc.edu (T. Murlidharan Nair)
Date: Mon, 11 Oct 2004 17:02:06 -0700
Subject: [R] plotting directly to the printer 
Message-ID: <416B1EFE.5040906@sdsc.edu>

How do I send the plots directly to the printer ?
Thanks ../Murli



From mathtester at yahoo.com  Tue Oct 12 04:42:21 2004
From: mathtester at yahoo.com (Heng Sun)
Date: Mon, 11 Oct 2004 19:42:21 -0700 (PDT)
Subject: [R] KalmanLike: missing exogenous factor?
Message-ID: <20041012024221.24459.qmail@web13308.mail.yahoo.com>

>From the help document on KalmanLike, KalmanRun, etc.,
I see the linear Gaussian state space model is 

a <- T a + R e
y = Z' a + eta

following the book of Durbin and Koopman.

In practice, it is useful to run Kalman
filtering/smoothing/forecasting with exogenous factor:

a <- T a + L b + R e
y = Z' a + M b + eta

where b is some known vector (a function of time).

Some other software like S-plus and Mathematica
include the above exogenous factor. SsfPack by
Koopman, etal. also has the factor built in the model
to accommodate practical uses.

So what is the rationale for R to leave off the
exogenous factor? Is there a feasible way to convert
the general model to the simple model in R?

Thanks,

Heng Sun


		
_______________________________

Declare Yourself - Register online to vote today!



From threehounds at operamail.com  Tue Oct 12 05:08:05 2004
From: threehounds at operamail.com (Matt Austin)
Date: Tue, 12 Oct 2004 03:08:05 +0000 (UTC)
Subject: [R] plotting with multiple files
References: <416B062E.90605@sdsc.edu>
Message-ID: <loom.20041012T050510-466@post.gmane.org>

T. Murlidharan Nair <nair <at> sdsc.edu> writes:

> 
> Hi !!
> I need to generate plots from several multiple file and I am doing this 
> by reading it as a list using c(file1, file2.....).
> Since I need to either print the plots or save it as an plot image while 
> generating  the plots in a loop, has anyone
> done this before. Also is there a getchar() equivalent in R, I mean just 
> to wait for an input from the console so that
> I can print before it plots the next plot.
> Thanks ../Murli
> 
> ______________________________________________
> R-help <at> stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
> 
> 


The first example directs output to tmp.pdf while the second waits for 
input "y".  The fun() function throws an error if anything but "y" is input and 
ends execution (not pretty).

tempDat <- lapply( 1:10, function(x) list( x= rnorm(10), y = rnorm(10)))

pdf( "tmp.pdf" )
  lapply( tempDat , function( inDat ){

    plot( inDat$x, inDat$y )

  } )
dev.off()


fun <- function() {
  ANSWER <- readline("Next Plot (y=continue else break)")
  if (substr(ANSWER, 1, 1) != "y")
    stop("Ending routine\n")
}
lapply( tempDat , function( inDat ){

    plot( inDat$x, inDat$y )
    fun()

  } )



From threehounds at operamail.com  Tue Oct 12 05:16:07 2004
From: threehounds at operamail.com (Matt Austin)
Date: Tue, 12 Oct 2004 03:16:07 +0000 (UTC)
Subject: [R] plotting directly to the printer
References: <416B1EFE.5040906@sdsc.edu>
Message-ID: <loom.20041012T051532-428@post.gmane.org>

?postscript provides an example



From petr.pikal at precheza.cz  Tue Oct 12 07:01:41 2004
From: petr.pikal at precheza.cz (Petr Pikal)
Date: Tue, 12 Oct 2004 07:01:41 +0200
Subject: [R] How to read unbalanced data from file?
In-Reply-To: <6.1.2.0.0.20041011114320.02233820@mail.math.uwaterloo.ca>
Message-ID: <416B8155.18922.32AF47@localhost>

On 11 Oct 2004 at 11:45, wfang wrote:

> Hi,
>    I tried to read some unbalance data (with different number if rows)
>    
> using x<- read.table("filename", header = true) command, but the
> program refuses to read the table. Could you tell me why this is

Hallo

Your OS, your R version is missing, but having this:

a	b	d	e
1	1	1	1
2	2	2	2
3		3	3
4		4	
5		5	
		6	

read.delim("clipboard")

produces

> mydata<-read.delim("clipboard")
   a  b d  e
1  1  1 1  1
2  2  2 2  2
3  3 NA 3  3
4  4 NA 4 NA
5  5 NA 5 NA
6 NA NA 6 NA

so it ***do not refuse*** to read the data. I do not expect some 
dwarf climbs out from your computer and says he will not read 
your data. Or is he? 

?read.table 

gives you more details about how to read some data.

Cheers
Petr


> happening and how to fix it? I just want to create a ANOVA table.
>    Thanks.
>    Wenyi
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide!
> http://www.R-project.org/posting-guide.html

Petr Pikal
petr.pikal at precheza.cz



From ripley at stats.ox.ac.uk  Tue Oct 12 08:51:02 2004
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Tue, 12 Oct 2004 07:51:02 +0100 (BST)
Subject: [R] KalmanLike: missing exogenous factor?
In-Reply-To: <20041012024221.24459.qmail@web13308.mail.yahoo.com>
Message-ID: <Pine.LNX.4.44.0410120744010.5609-100000@gannet.stats>

On Mon, 11 Oct 2004, Heng Sun wrote:

> From the help document on KalmanLike, KalmanRun, etc.,
> I see the linear Gaussian state space model is 
> 
> a <- T a + R e
> y = Z' a + eta
> 
> following the book of Durbin and Koopman.
> 
> In practice, it is useful to run Kalman
> filtering/smoothing/forecasting with exogenous factor:
> 
> a <- T a + L b + R e
> y = Z' a + M b + eta
> 
> where b is some known vector (a function of time).
> 
> Some other software like S-plus and Mathematica
> include the above exogenous factor. SsfPack by
> Koopman, etal. also has the factor built in the model
> to accommodate practical uses.
> 
> So what is the rationale for R to leave off the
> exogenous factor? Is there a feasible way to convert
> the general model to the simple model in R?

What is the rationale for your raising this?

KalmanLike, KalmanRun, etc were written for R 1.5.0 as part of the ts 
package (see my article in R-news), and the ts applications (see the See 
Also section) do not need a so-called `exogenous factor' (which is not a 
`factor').   R does not pretend to have facilities for whatever subject 
area you mean (but do not say) by `in practice'.  That's what addon
packages are for (and some do touch on this area).

We have no idea who `mathtester at yahoo.com' is: it is courteous to use a 
signature and give your affiliation.

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From ripley at stats.ox.ac.uk  Tue Oct 12 09:07:50 2004
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Tue, 12 Oct 2004 08:07:50 +0100 (BST)
Subject: [R] plotting directly to the printer 
In-Reply-To: <416B1EFE.5040906@sdsc.edu>
Message-ID: <Pine.LNX.4.44.0410120805070.5609-100000@gannet.stats>

On Mon, 11 Oct 2004, T. Murlidharan Nair wrote:

> How do I send the plots directly to the printer ?

On what operating system?

On Windows, see the rw-FAQ Q2.9.

On Linux/Unix, see ?dev.print and the print.it argument in ?postscript.

Please do read the posting guide and supply enough information for us to 
answer your questions.

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From msvika at mscc.huji.ac.il  Tue Oct 12 09:14:16 2004
From: msvika at mscc.huji.ac.il (Victoria Landsman)
Date: Tue, 12 Oct 2004 09:14:16 +0200
Subject: [R] Statistical analysis of a large database  
Message-ID: <005b01c4b02b$15b0b100$8600a8c0@home2>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20041012/debcfb14/attachment.pl

From ripley at stats.ox.ac.uk  Tue Oct 12 09:15:20 2004
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Tue, 12 Oct 2004 08:15:20 +0100 (BST)
Subject: [R] plotting with multiple files
In-Reply-To: <416B062E.90605@sdsc.edu>
Message-ID: <Pine.LNX.4.44.0410120809560.5609-100000@gannet.stats>

On Mon, 11 Oct 2004, T. Murlidharan Nair wrote:

> I need to generate plots from several multiple file and I am doing this 

I presume you are making plots from *data* in those files rather than R 
scripts, but you did not say.

> by reading it as a list using c(file1, file2.....).
> Since I need to either print the plots or save it as an plot image while 
> generating  the plots in a loop, has anyone
> done this before. 

Yes.  (I have no idea what you really want to know here, so am answering 
the question you actually asked.  Please read the posting guide and its 
references and try to ask smarter questions.)

> Also is there a getchar() equivalent in R, I mean just 
> to wait for an input from the console so that
> I can print before it plots the next plot.

?readline, par(ask=TRUE).

However, whilst the console is waiting for input, *you* can print, but you 
cannot run R commands to print.

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From schnitzlerj at lyon.who.int  Tue Oct 12 09:43:05 2004
From: schnitzlerj at lyon.who.int (Johannes SCHNITZLER)
Date: Tue, 12 Oct 2004 09:43:05 +0200
Subject: [R] dot density maps
Message-ID: <AD1C47A0C9C12C46987E2D31EE1E63B4361567@lps001.lyon.who.int>

Dear Roger, 

Thank you very much for the reply.

And now I have to a apologize for my late response.
I was quite a while out of the office. 

This is exactly what I was looking for. I think it will be of great help
to have this implemented in a package.

Johannes 

-----Original Message-----
From: Roger Bivand [mailto:Roger.Bivand at nhh.no] 
Sent: Thursday, September 30, 2004 8:42 PM
To: Johannes SCHNITZLER
Cc: r-help at stat.math.ethz.ch
Subject: Re: [R] dot density maps

On Wed, 22 Sep 2004, Johannes SCHNITZLER wrote:

...
> I'm looking for the possibility to draw points (randomly positioned or
> positioned according to a grid) into the polygons instead of filling
the
> polygons with colors.
> 

I apologize for not replying earlier. I think that you can combine the 
polylist class from the maptools package with the csr function in the 
splancs package to position points randomly (the gridpts() finction can 
be used instead of csr() to give grid points):

.....
.....

Could you indicate whether this function is any use, and if you would
like 
it added to some suitable package?

Roger Bivand 
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide!
http://www.R-project.org/posting-guide.html
> 

-- 
Roger Bivand
Economic Geography Section, Department of Economics, Norwegian School of
Economics and Business Administration, Breiviksveien 40, N-5045 Bergen,
Norway. voice: +47 55 95 93 55; fax +47 55 95 93 93
e-mail: Roger.Bivand at nhh.no



From vito_ricci at yahoo.com  Tue Oct 12 10:11:48 2004
From: vito_ricci at yahoo.com (Vito Ricci)
Date: Tue, 12 Oct 2004 10:11:48 +0200 (CEST)
Subject: [R] Statistical analysis of a large database
Message-ID: <20041012081148.11000.qmail@web41210.mail.yahoo.com>

Hi,

for your analysis use the package:

ROracle	Oracle database interface for R

http://microarrays.unife.it/CRAN/src/contrib/Descriptions/ROracle.html

see also:

Diego Kuonen, Introduction au data mining avec R :
vers la reconqu??te du `knowledge discovery in
databases' par les statisticiens. Bulletin of the
Swiss Statistical Society, 40:3-7, 2001.
http://www.statoo.com/en/publications/2001.R.SSS.40/

Diego Kuonen and Reinhard Furrer, Data mining avec R
dans un monde libre. Flash Informatique Sp??cial ??t??,
pages 45-50, sep 2001.
http://sawww.epfl.ch/SIC/SA/publications/FI01/fi-sp-1/sp-1-page45.html


R Development Core Team, R Data Import/Export,
versione 1.9.0, aprile 2004, pagg. 11-18
http://cran.r-project.org/doc/manuals/R-data.pdf

Brian D. Ripley, Datamining: Large Databases and
Methods, in Proceedings  of "useR! 2004 - The R User
Conference", maggio 2004
http://www.ci.tuwien.ac.at/Conferences/useR-2004/Keynotes/Ripley.pdf

Brian D. Ripley, Using Databases with R, R News,
Gennaio 2001, pagg. 18-20
http://cran.r-project.org/doc/Rnews/Rnews_2001-1.pdf

B. D. Ripley, R. M. Ripley,  Applications of R Clients
and Servers in Proceedings of the Distributed
Statistical Computing 2001 Workshop, 2001, Vienna
University of Technology.
http://www.ci.tuwien.ac.at/Conferences/DSC-2001/Proceedings/Ripley.pdf


Torsten Hothorn, David A. James, Brian D. Ripley,  R/S
Interfaces to Databases  in Proceedings of the
Distributed Statistical Computing 2001 Workshop,
2001,Vienna University of Technology.
http://www.ci.tuwien.ac.at/Conferences/DSC-2001/Proceedings/HothornJamesRipley.pdf

Lu??s Torgo, Data Mining with R. Learning by case
studies, Maggio 2003
http://www.liacc.up.pt/~ltorgo/DataMiningWithR/

I hope I give you a little help.
Best
Vito




You wrote:

Deall all, 
We need to perform a statistical analysis of a large
database (40,000 entries with approximately 500 fields
in each entry) currently handled in Oracle. The data
contains categorical variables only. 
At the current stage we suggest classification and
clustering analysis. 
We are planning to perform the analysis in R  and
would be very grateful for any
recommendations/suggestions/references regarding the
packages/tools appropriate for this task. 
Thank you in advance for your attention, 
Vicky Landsman


=====
Diventare costruttori di soluzioni

"The business of the statistician is to catalyze 
the scientific learning process."  
George E. P. Box


Visitate il portale http://www.modugno.it/
e in particolare la sezione su Palese http://www.modugno.it/archivio/cat_palese.shtml



From ripley at stats.ox.ac.uk  Tue Oct 12 10:36:44 2004
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Tue, 12 Oct 2004 09:36:44 +0100 (BST)
Subject: [R] Statistical analysis of a large database
In-Reply-To: <20041012081148.11000.qmail@web41210.mail.yahoo.com>
Message-ID: <Pine.LNX.4.44.0410120927450.6004-100000@gannet.stats>

I believe this is a reply to a posting -- since this is by no means the 
first time this has happened, please

- use the Reply function of your mailer, or at least use Re: in the 
subject line and include the relevant part of the original posting, and
- send the reply to the questioner, as well as possibly to the list.

On Tue, 12 Oct 2004, Vito Ricci wrote:

[...]

> Lu??s Torgo, Data Mining with R. Learning by case
> studies, Maggio 2003
> http://www.liacc.up.pt/~ltorgo/DataMiningWithR/

Please note that that reference is not about large datasets, nor about 
`data mining' in the generally used sense.  It has two studies, one 
incomplete, on linear regression (with 200 samples) and on time series.

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From christian.ritter at shell.com  Tue Oct 12 13:40:11 2004
From: christian.ritter at shell.com (Ritter, Christian C MCIL-CTANL/S)
Date: Tue, 12 Oct 2004 13:40:11 +0200
Subject: [R] lm#contrasts#one level in factor: bug or feature
Message-ID: <156CDC8CCFD1894295D2907F16337A48B2ADA2@bru-s-006.europe.shell.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20041012/08f22296/attachment.pl

From rpeng at jhsph.edu  Tue Oct 12 14:45:43 2004
From: rpeng at jhsph.edu (Roger D. Peng)
Date: Tue, 12 Oct 2004 08:45:43 -0400
Subject: [R] 64-bit R on Intel Xeon EM64T running fine
In-Reply-To: <Pine.LNX.4.53.0410112148540.3983@lakeforest.homelinux.com>
References: <Pine.LNX.4.53.0410112148540.3983@lakeforest.homelinux.com>
Message-ID: <416BD1F7.6040006@jhsph.edu>

This is good news.  As far as I know R has built for quite some time 
now on a number of 64 bit platforms (Linux on AMD Opteron/Athlon64, 
Solaris/Sparc) but I can't recall seeing a build on Intel with the 64 
bit extensions.  By the way, did you happen to run `make check' just 
for kicks?

-roger

Michael Seewald wrote:
> Dear mailing-list members,
> 
> In the days of cheap RAM and microarray applications feasting on memory,
> 64-bit computers become more and more useful - to actually make use of memory
> beyond the magic 4GB border. I would like to report the success of running
> 64-bit R on an Intel Xeon EM64T machine under Linux. Just like on an AMD
> Opteron, R v2.0.0 compiles fine (and out of the box) and is happily allocating
> memory until RAM and swap reach their limit.
> 
> Hardware:
> - HP xw6200 workstation
> - dual Intel Xeon 3.4GHz with hyper-threading enabled
> - 4GB RAM, 4GB swap
> 
> System: either
> - Fedora Core 2 x86_64 bit Linux
> or
> - Red Hat Enterprise Linux Workstation 3.0 x86_64 bit
> 
> R:
> - v2.0.0
> 
> Really, no problems at all during setup, a big thank you to the R developers
> making this possible!
> 
> Best wishes,
> Michael
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>



From jfox at mcmaster.ca  Tue Oct 12 15:20:58 2004
From: jfox at mcmaster.ca (John Fox)
Date: Tue, 12 Oct 2004 09:20:58 -0400
Subject: [R] Diagnosing trouble with R-2.0, Fedora Core 2, and Rcmdf
In-Reply-To: <416ACC27.6030806@ku.edu>
Message-ID: <web-68275375@cgpsrv2.cis.mcmaster.ca>

Dear Paul,

I'm currently out of town, limiting a bit my ability to answer these
questions definitively, but I'll give it a shot (hoping that my memory
isn't faulty):

On Mon, 11 Oct 2004 13:08:39 -0500
 Paul Johnson <pauljohn at ku.edu> wrote:
> Greetings, R-help!
> 
> On 2 Fedora Core 2 Linux systems, i've completely erased the previous
> R and all packages and then installed R-2.0 and installed fresh
> packages.
> 
> In using Rcmdr, I see some trouble and I wonder if other people see
> this and if it is due to the tcl/tk, or R, or Rcmdr.  (If readers
> have not yet tried Rcmdr, I recommend it not just because of the GUI
> it provides, but also because Prof. Fox has created several very
> handy commands, like scatter3d, which make it possible to use
> advanced packages like rgl. Rcmdr provides several very handy wrapper
> commands that "just work" and users don't have to fuss over so many
> details! And, if you are new at R, you can use pull down menus and
> then Rcmdr displays the commands that correspond with those menu
> options. Very educational!)
> 
> In no particular order, here are the issues I see with R-2.0 and
> Rcmdr
> 
> 1. In the panel to select a dataset from a package (nenu: Data/Data
> in packages), I can type in the name of a dataset, but the GUI will
> not let me choose the name of a package from which to select a
> dataset. Nothing happens when I pick a package name.  (With previous
> R/Rcmdr, I did not have this trouble).
> 

The problem is that the Rcmdr code doesn't quote the name of the
package. I believe that I've fixed this problem, but haven't posted the
fixed version either to my web site or to CRAN.

> 2. When using Rcmdr, some of the output from commands shows in the
> Rcmdr bottom window, but some is sent to the xterm in which R was
> started. For example, with a scatter3d() command, if I add the option
> model.summary=T, the model summary shows in the xterm.  But some
> other models show results in the bottom pane of Rcmdr.  And, it seems
> to me, there are some commands in which I've noticed some of the
> output going to the Rcmdr bottom buffer and  some to the xterm.
> 

As I recall, I've fixed this problem as well, but perhaps not in a
completely general way. Again, I haven't yet released the fixed
version.

> 3. When I want to exit Rcmdr and choose the pull down "exit from
> Commander and R", I get a Segmentation Fault that closes Rcmdr and R.
> 

This I haven't seen before.

> 4. 3d plots do not display on the screen until I click in the rgl
> window that displays the graph.  This might be a video card/AGP
> driver problem, I suppose.  These systems have the NVidia FX5200
> cards and the NVidia proprietary X driver.  If other users don't see
> the same on other kinds of systems, I guess that will tell me where
> the problem lies.
> 

I believe that I've noticed this problem on a Quantian system.

> 5. At random times, I get an error tcl/tk window popping up with a
> message about MASS and categorical variables.  It says
> 

This reflects the first problem, and is produced by the way that the
Rcmdr initially suppresses and then reports warnings.

I'll check further when I return home.

Sorry for the problems.

John

> Error in data(package=parse(text=packageName));
> 'MASS' must be character string or NULL
> 
> It says that over and over, sometimes it has other package names, as
> in:
> 
> Error in data(package=parse(text=packageName));
> 'relimp' must be character string or NULL
> 
> These seem to be harmless?
> -- 
> Paul E. Johnson                       email: pauljohn at ku.edu
> Dept. of Political Science            http://lark.cc.ku.edu/~pauljohn
> 1541 Lilac Lane, Rm 504
> University of Kansas                  Office: (785) 864-9086
> Lawrence, Kansas 66044-3177           FAX: (785) 864-5700
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide!
> http://www.R-project.org/posting-guide.html

--------------------------------
John Fox
Department of Sociology
McMaster University
Hamilton, Ontario, Canada
http://socserv.mcmaster.ca/jfox/



From ripley at stats.ox.ac.uk  Tue Oct 12 16:09:39 2004
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Tue, 12 Oct 2004 15:09:39 +0100 (BST)
Subject: [R] 64-bit R on Intel Xeon EM64T running fine
In-Reply-To: <416BD1F7.6040006@jhsph.edu>
Message-ID: <Pine.LNX.4.44.0410121435040.1337-100000@gannet.stats>

On Tue, 12 Oct 2004, Roger D. Peng wrote:

> This is good news.  As far as I know R has built for quite some time 
> now on a number of 64 bit platforms (Linux on AMD Opteron/Athlon64, 
> Solaris/Sparc

and Alpha and Irix and HP-UX and AIX as far as I understand.

) but I can't recall seeing a build on Intel with the 64 
> bit extensions.  

As I understand it, this is an Intel `clone' of AMD64, so the only
news would be if there were any problems: in almost all cases the
executable code is identical to that compiled for AMD64 and in the GNU
classification it is also "x86_64-unknown-linux-gnu".

I would even expect a Goto BLAS to work, if not as well as on the specific 
Opteron it is tuned for.  (Fast BLASes are an essential part of making the 
most of 64-bit processors on large problems.)

> Michael Seewald wrote:
> > Dear mailing-list members,
> > 
> > In the days of cheap RAM and microarray applications feasting on memory,
> > 64-bit computers become more and more useful - to actually make use of memory
> > beyond the magic 4GB border. I would like to report the success of running
> > 64-bit R on an Intel Xeon EM64T machine under Linux. Just like on an AMD
> > Opteron, R v2.0.0 compiles fine (and out of the box) and is happily allocating
> > memory until RAM and swap reach their limit.
> > 
> > Hardware:
> > - HP xw6200 workstation
> > - dual Intel Xeon 3.4GHz with hyper-threading enabled
> > - 4GB RAM, 4GB swap
> > 
> > System: either
> > - Fedora Core 2 x86_64 bit Linux
> > or
> > - Red Hat Enterprise Linux Workstation 3.0 x86_64 bit
> > 
> > R:
> > - v2.0.0
> > 
> > Really, no problems at all during setup, a big thank you to the R developers
> > making this possible!

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From jeaneid at chass.utoronto.ca  Tue Oct 12 16:24:56 2004
From: jeaneid at chass.utoronto.ca (Jean Eid)
Date: Tue, 12 Oct 2004 10:24:56 -0400
Subject: [R] tclk, tcltk
Message-ID: <Pine.SGI.4.40.0410121012050.40883670-100000@origin.chass.utoronto.ca>

I have been having problems with these two 'libraries' since I installed
2.0.0.
I have built a package with couple of functions so that I can load it at
startup every time R is booted. The problem is that I have the following
error every time I call the library

Loading required package: tclk
Error: package 'tclk' could not be loaded
In addition: Warning message:
There is no package called 'tclk' in: library(pkg, character.only = TRUE, logical = TRUE, lib.loc = lib.loc)


However I can load tcltk library now (thanks to Dirk for helping me with
this). I can load the functions I have in the library and everything works
fine.

I cannot find any information on this except the following thread which
was not resolved.
http://tolstoy.newcastle.edu.au/R/help/04/01/0410.html

Any help or ideas are greatly appreciated.

P.S. what is the difference between tclk and tcltk? and why calling the
installed library is requiring tclk? I just made up a dummy package with a
function

fun<-function(x) x^2

just to test and the same thing happens (Loading required package: tclk
...).

This is on a Linux debian unstable with the kernel  2.4.20-bf2.4-xfs

Thank you for all the help.

jean



From HDoran at air.org  Tue Oct 12 16:22:16 2004
From: HDoran at air.org (Doran, Harold)
Date: Tue, 12 Oct 2004 10:22:16 -0400
Subject: [R] Sweave and xtable
Message-ID: <88EAF3512A55DF46B06B1954AEF73F7405E46308@dc1ex2.air.org>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20041012/96943a89/attachment.pl

From ripley at stats.ox.ac.uk  Tue Oct 12 16:39:43 2004
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Tue, 12 Oct 2004 15:39:43 +0100 (BST)
Subject: [R] tclk, tcltk
In-Reply-To: <Pine.SGI.4.40.0410121012050.40883670-100000@origin.chass.utoronto.ca>
Message-ID: <Pine.LNX.4.44.0410121535190.1534-100000@gannet.stats>

What is package `tclk'?  I think you have a typo somewhere in your code.
At a guess you have

Depends: tclk

in a DESCRIPTION file for one of your packages.


On Tue, 12 Oct 2004, Jean Eid wrote:

> I have been having problems with these two 'libraries' since I installed
> 2.0.0.
> I have built a package with couple of functions so that I can load it at
> startup every time R is booted. The problem is that I have the following
> error every time I call the library
> 
> Loading required package: tclk
> Error: package 'tclk' could not be loaded
> In addition: Warning message:
> There is no package called 'tclk' in: library(pkg, character.only = TRUE, logical = TRUE, lib.loc = lib.loc)
> 
> 
> However I can load tcltk library now (thanks to Dirk for helping me with
> this). I can load the functions I have in the library and everything works
> fine.
> 
> I cannot find any information on this except the following thread which
> was not resolved.
> http://tolstoy.newcastle.edu.au/R/help/04/01/0410.html

It was resolved, and in any case it was about installing 64-bit Tcl/Tk on 
Solaris, something that was (and is) covered in the R Installation manual.

> Any help or ideas are greatly appreciated.
> 
> P.S. what is the difference between tclk and tcltk? and why calling the
> installed library is requiring tclk? I just made up a dummy package with a
> function
> 
> fun<-function(x) x^2
> 
> just to test and the same thing happens (Loading required package: tclk
> ...).
> 
> This is on a Linux debian unstable with the kernel  2.4.20-bf2.4-xfs

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From jeaneid at chass.utoronto.ca  Tue Oct 12 16:58:43 2004
From: jeaneid at chass.utoronto.ca (Jean Eid)
Date: Tue, 12 Oct 2004 10:58:43 -0400
Subject: [R] tclk, tcltk
In-Reply-To: <Pine.LNX.4.44.0410121535190.1534-100000@gannet.stats>
Message-ID: <Pine.SGI.4.40.0410121057440.45280158-100000@origin.chass.utoronto.ca>

Thanks to Brian. I was looking everywhere except the typo. I guess the
previous build (1.9.1) ignored the dependece error.

Thanks again,

jean

On Tue, 12 Oct 2004, Prof Brian Ripley wrote:

> What is package `tclk'?  I think you have a typo somewhere in your code.
> At a guess you have
>
> Depends: tclk
>
> in a DESCRIPTION file for one of your packages.
>
>
> On Tue, 12 Oct 2004, Jean Eid wrote:
>
> > I have been having problems with these two 'libraries' since I installed
> > 2.0.0.
> > I have built a package with couple of functions so that I can load it at
> > startup every time R is booted. The problem is that I have the following
> > error every time I call the library
> >
> > Loading required package: tclk
> > Error: package 'tclk' could not be loaded
> > In addition: Warning message:
> > There is no package called 'tclk' in: library(pkg, character.only = TRUE, logical = TRUE, lib.loc = lib.loc)
> >
> >
> > However I can load tcltk library now (thanks to Dirk for helping me with
> > this). I can load the functions I have in the library and everything works
> > fine.
> >
> > I cannot find any information on this except the following thread which
> > was not resolved.
> > http://tolstoy.newcastle.edu.au/R/help/04/01/0410.html
>
> It was resolved, and in any case it was about installing 64-bit Tcl/Tk on
> Solaris, something that was (and is) covered in the R Installation manual.
>
> > Any help or ideas are greatly appreciated.
> >
> > P.S. what is the difference between tclk and tcltk? and why calling the
> > installed library is requiring tclk? I just made up a dummy package with a
> > function
> >
> > fun<-function(x) x^2
> >
> > just to test and the same thing happens (Loading required package: tclk
> > ...).
> >
> > This is on a Linux debian unstable with the kernel  2.4.20-bf2.4-xfs
>
> --
> Brian D. Ripley,                  ripley at stats.ox.ac.uk
> Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
> University of Oxford,             Tel:  +44 1865 272861 (self)
> 1 South Parks Road,                     +44 1865 272866 (PA)
> Oxford OX1 3TG, UK                Fax:  +44 1865 272595
>
>



From f.harrell at vanderbilt.edu  Tue Oct 12 17:32:12 2004
From: f.harrell at vanderbilt.edu (Frank E Harrell Jr)
Date: Tue, 12 Oct 2004 10:32:12 -0500
Subject: [R] Sweave and xtable
In-Reply-To: <88EAF3512A55DF46B06B1954AEF73F7405E46308@dc1ex2.air.org>
References: <88EAF3512A55DF46B06B1954AEF73F7405E46308@dc1ex2.air.org>
Message-ID: <416BF8FC.5000601@vanderbilt.edu>

Doran, Harold wrote:
> Dear List:
> 
> I have some coded embedded within a LaTeX document where I subset a
> dataframe and use xtable to place it in my appendix. However, one of the
> tables is rather large and seems to extend beyond the page length.
> 
> Is there a nice way to use Sweave to continue this table onto the next
> page? I could easily break this up in tex into two tables, but I imagine
> there is a better way of doing so.
> 
> Thanks,
> 
> Harold
If xtable will not do that, you can use latex( ) in the Hmisc package 
with its longtable=TRUE argument.

-- 
Frank E Harrell Jr   Professor and Chair           School of Medicine
                      Department of Biostatistics   Vanderbilt University



From maechler at stat.math.ethz.ch  Tue Oct 12 17:38:38 2004
From: maechler at stat.math.ethz.ch (Martin Maechler)
Date: Tue, 12 Oct 2004 17:38:38 +0200
Subject: [R] plot hclust - canberra dist + median linkage 
In-Reply-To: <Pine.LNX.4.21.0410111618390.25948-100000@mail.mrc-dunn.cam.ac.uk>
References: <Pine.LNX.4.21.0410111618390.25948-100000@mail.mrc-dunn.cam.ac.uk>
Message-ID: <16747.64126.266598.397639@gargle.gargle.HOWL>

>>>>> "Dan" == Dan Bolser <dmb at mrc-dunn.cam.ac.uk>
>>>>>     on Mon, 11 Oct 2004 16:21:53 +0100 (BST) writes:

    Dan> Gives strange results.

    Dan> I get 'weird' dendrograms with canberra / binary distance metric and
    Dan> median / centroid cluster methods.

it doesn't depend on the metric: 
Both 'median' and 'centroid' methods are known to *not*
guarantee ``monotone distance measures'', or equivalently to
possibly lead to dendrograms with so called ``inversions''.
We should add this to help page for hclust().

Probably for this reason, agnes() from the cluster package
doesn't have these two methods [explicitly] -- though it now
allows general parameter Lance-William formula methods which can
also lead to inversions.

    Dan> Is this just my data?

evidently not.  Though the problem does not appear for all data
sets...

Regards,
Martin Maechler



From harel_yair at yahoo.com  Tue Oct 12 17:49:34 2004
From: harel_yair at yahoo.com (Yayira har)
Date: Tue, 12 Oct 2004 08:49:34 -0700 (PDT)
Subject: [R] A question in R
Message-ID: <20041012154934.91929.qmail@web50902.mail.yahoo.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20041012/ff92e9a4/attachment.pl

From kbartz at loyaltymatrix.com  Tue Oct 12 17:56:45 2004
From: kbartz at loyaltymatrix.com (Kevin Bartz)
Date: Tue, 12 Oct 2004 08:56:45 -0700
Subject: [R] A question in R
In-Reply-To: <20041012154934.91929.qmail@web50902.mail.yahoo.com>
References: <20041012154934.91929.qmail@web50902.mail.yahoo.com>
Message-ID: <416BFEBD.2060300@loyaltymatrix.com>

Yayira har wrote:
> I started to learn the R language, but I didn't suceed to use an external file.
>  
> Let say that I have an excel file called "test1.xls" in the directory 
> "C:/program files/R/rw2000/external_files" that looks like that:
>  
> name  mark
>   yair      80
>  yosi      70  ...
>  
> In the appropriate directory I wrote this:
>  
> x<-read.delim("test1.xls")
>  
> or this:
>  
> x<-read.csv("test1.xls")
>  
> but I got:
>  
> [1] X......
> <0 rows> (or 0-length row.names)
>  
> way cannot I read the file? what is the appropriate command for reading an excel file?
> 
> I looked at the site of R-project but I didn't find a suitable comand.
> 
> __________________________________________________
> 
> 
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
> 

Hi Yayiya,

R looks less than fondly on Excel files. The easiest solution for you 
will be to export your Excel file to a tab-delimited text format (Save 
-> (.txt) Tab-delimited Text), and then use read.delim as you did. Does 
that make sense?

Kevin



From sundar.dorai-raj at PDF.COM  Tue Oct 12 17:56:58 2004
From: sundar.dorai-raj at PDF.COM (Sundar Dorai-Raj)
Date: Tue, 12 Oct 2004 10:56:58 -0500
Subject: [R] A question in R
In-Reply-To: <20041012154934.91929.qmail@web50902.mail.yahoo.com>
References: <20041012154934.91929.qmail@web50902.mail.yahoo.com>
Message-ID: <416BFECA.2090001@pdf.com>



Yayira har wrote:

> I started to learn the R language, but I didn't suceed to use an external file.
>  
> Let say that I have an excel file called "test1.xls" in the directory 
> "C:/program files/R/rw2000/external_files" that looks like that:
>  
> name  mark
>   yair      80
>  yosi      70  ...
>  
> In the appropriate directory I wrote this:
>  
> x<-read.delim("test1.xls")
>  
> or this:
>  
> x<-read.csv("test1.xls")
>  
> but I got:
>  
> [1] X......
> <0 rows> (or 0-length row.names)
>  
> way cannot I read the file? what is the appropriate command for reading an excel file?
> 
> I looked at the site of R-project but I didn't find a suitable comand.
> 

Is the file an Excel file (probably from the extension) or can you open 
the file in notepad and read the text? If not, you need to save the file 
as csv or txt from within Excel and import the file into R as you tried 
above. You may also want to read the "R Data Import/Export" manual which 
is available from the r-project.org website.

--sundar



From ripley at stats.ox.ac.uk  Tue Oct 12 18:04:16 2004
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Tue, 12 Oct 2004 17:04:16 +0100 (BST)
Subject: [R] A question in R
In-Reply-To: <20041012154934.91929.qmail@web50902.mail.yahoo.com>
Message-ID: <Pine.LNX.4.44.0410121701270.31482-100000@gannet.stats>

Take a look at the `R Data Import/Export' manual.

Hint: if test1.xls is an Excel worksheet, it is not a tab-delimited file.

On Tue, 12 Oct 2004, Yayira har wrote:

> I started to learn the R language, but I didn't suceed to use an external file.
>  
> Let say that I have an excel file called "test1.xls" in the directory 
> "C:/program files/R/rw2000/external_files" that looks like that:
>  
> name  mark
>   yair      80
>  yosi      70  ...
>  
> In the appropriate directory I wrote this:
>  
> x<-read.delim("test1.xls")
>  
> or this:
>  
> x<-read.csv("test1.xls")
>  
> but I got:
>  
> [1] X......
> <0 rows> (or 0-length row.names)
>  
> way cannot I read the file? what is the appropriate command for reading an excel file?
> 
> I looked at the site of R-project but I didn't find a suitable comand.
> 
> __________________________________________________
> 
> 
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
> 
> 

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From ym at climpact.com  Tue Oct 12 18:14:13 2004
From: ym at climpact.com (Yves Magliulo)
Date: 12 Oct 2004 18:14:13 +0200
Subject: [R] A question in R
In-Reply-To: <416BFEBD.2060300@loyaltymatrix.com>
References: <20041012154934.91929.qmail@web50902.mail.yahoo.com>
	<416BFEBD.2060300@loyaltymatrix.com>
Message-ID: <1097597653.22020.271.camel@new-york.climpact.net>

hi,

check R-data import/export document at this link

cran.r-project.org/doc/manuals/R-data.pdf

let me give you an advice for further questions : it's easiest to find
answer to your question searching directly with google
 
-- 
------
Yves Magliulo <ym at climpact.com>
R&D Engineer, CLIMPACT

Tel.   : +33 (0) 1 44 27 34 31
Fax.   : +33 (0) 1 44 27 49 96 
Universite Pierre et Marie Curie
Boite 101 - Tour 45 - 5eme etage - Couloir 45/46
4 place Jussieu, 75252 Paris CEDEX 05, France
Le mar 12/10/2004 ?? 17:56, Kevin Bartz a ??crit :
> Yayira har wrote:
> > I started to learn the R language, but I didn't suceed to use an external file.
> >  
> > Let say that I have an excel file called "test1.xls" in the directory 
> > "C:/program files/R/rw2000/external_files" that looks like that:
> >  
> > name  mark
> >   yair      80
> >  yosi      70  ...
> >  
> > In the appropriate directory I wrote this:
> >  
> > x<-read.delim("test1.xls")
> >  
> > or this:
> >  
> > x<-read.csv("test1.xls")
> >  
> > but I got:
> >  
> > [1] X......
> > <0 rows> (or 0-length row.names)
> >  
> > way cannot I read the file? what is the appropriate command for reading an excel file?
> > 
> > I looked at the site of R-project but I didn't find a suitable comand.
> > 
> > __________________________________________________
> > 
> > 
> > 
> > 	[[alternative HTML version deleted]]
> > 
> > ______________________________________________
> > R-help at stat.math.ethz.ch mailing list
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
> > 
> 
> Hi Yayiya,
> 
> R looks less than fondly on Excel files. The easiest solution for you 
> will be to export your Excel file to a tab-delimited text format (Save 
> -> (.txt) Tab-delimited Text), and then use read.delim as you did. Does 
> that make sense?
> 
> Kevin
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html



From liusong at stat.umn.edu  Tue Oct 12 18:24:45 2004
From: liusong at stat.umn.edu (Liu Song)
Date: Tue, 12 Oct 2004 11:24:45 -0500 (CDT)
Subject: [R] need help on GAM
Message-ID: <Pine.LNX.4.58.0410111558320.8037@owasso.stat.umn.edu>


Get some question about the function "gam".
Suppose I have a semiparametric model,
Y~x1+x2+s(z1).
Using "gam", how could I get the estimates for the parametric part and 
nonparametric part respectively?

And another question: we could find the coefficients for both 
parametric term and nonparametric term, what do these coefficients
for the nonparametric term stand for, the coefficients for the base 
functions?
 
Thank you!

Song
********************************************************************
Song Liu
School of Statistics
313 FordH
224 Church St. SE
Minneapolis,MN 55455



From ripley at stats.ox.ac.uk  Tue Oct 12 18:43:39 2004
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Tue, 12 Oct 2004 17:43:39 +0100 (BST)
Subject: [R] need help on GAM
In-Reply-To: <Pine.LNX.4.58.0410111558320.8037@owasso.stat.umn.edu>
Message-ID: <Pine.LNX.4.44.0410121735010.2122-100000@gannet.stats>

Please read the posting guide and do give sufficient details for your 
questions to be answered.

On Tue, 12 Oct 2004, Liu Song wrote:

> Get some question about the function "gam".

There are at least two functions gam() available for R, but none in
standard R.  Which package are you talking about?  This is covered in the
R FAQ, BTW.

> Suppose I have a semiparametric model,
> Y~x1+x2+s(z1).
> Using "gam", how could I get the estimates for the parametric part and 
> nonparametric part respectively?

What do you mean by those terms?  In the gam() in package gam, s(z1) is 
split into a linear and non-linear part, but all the terms *are* 
parametric (a smoothing spline is a parametric function of its argument).

> And another question: we could find the coefficients for both 
> parametric term and nonparametric term, what do these coefficients
> for the nonparametric term stand for, the coefficients for the base 
> functions?

By definition, non-parametric terms do not have parameters aka 
coefficients.

I suggest you ask for local advice on the statistical background to your 
questions, which you do need to understand before getting to using them in 
R.

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From ggrothendieck at myway.com  Tue Oct 12 18:57:35 2004
From: ggrothendieck at myway.com (Gabor Grothendieck)
Date: Tue, 12 Oct 2004 16:57:35 +0000 (UTC)
Subject: [R] need help on GAM
References: <Pine.LNX.4.58.0410111558320.8037@owasso.stat.umn.edu>
	<Pine.LNX.4.44.0410121735010.2122-100000@gannet.stats>
Message-ID: <loom.20041012T185604-109@post.gmane.org>

Prof Brian Ripley <ripley <at> stats.ox.ac.uk> writes:
: > And another question: we could find the coefficients for both 
: > parametric term and nonparametric term, what do these coefficients
: > for the nonparametric term stand for, the coefficients for the base 
: > functions?
: 
: By definition, non-parametric terms do not have parameters aka 
: coefficients.

This is definitely a candidate for the fortunes package.



From suresh at brahms.cpmc.columbia.edu  Tue Oct 12 19:01:55 2004
From: suresh at brahms.cpmc.columbia.edu (Suresh Krishna)
Date: Tue, 12 Oct 2004 13:01:55 -0400
Subject: [R] locfit confidence intervals
Message-ID: <6.0.3.0.0.20041012125219.01bce8a8@brahms.cpmc.columbia.edu>


hi,

after

m=locfit(y~x,..........., family=binomial)
plot(m,band="local") gives me a plot of locfit's result with a confidence 
interval around it. i would like to get the actual values that are being 
used to plot the lines in this figure.

i tried using predict, but the standard error it returns when i supply the 
se=T argument, appears not to be the same as the CI produced by plot (it is 
much larger...in my one case).  i have tried all the different type=".." 
options, but none of them matches the CI produced by plot.

i only recently started using both R and local regression methods and am 
still feeling my way around, so even though I have gone through the FAQs 
and the online manuals and the archives of this group and the Loader book, 
I have been unable to make much progress. any help would be very appreciated !!

thanks, suresh



From simon at stats.gla.ac.uk  Tue Oct 12 19:08:03 2004
From: simon at stats.gla.ac.uk (Simon Wood)
Date: Tue, 12 Oct 2004 18:08:03 +0100 (BST)
Subject: [R] need help on GAM
In-Reply-To: <Pine.LNX.4.58.0410111558320.8037@owasso.stat.umn.edu>
References: <Pine.LNX.4.58.0410111558320.8037@owasso.stat.umn.edu>
Message-ID: <Pine.LNX.4.58.0410121800240.10195@moon.stats.gla.ac.uk>


> Get some question about the function "gam".
> Suppose I have a semiparametric model,
> Y~x1+x2+s(z1).
> Using "gam", how could I get the estimates for the parametric part and 
> nonparametric part respectively?
For a gam object (called, b,say) fitted using the mgcv `gam' then coef(b) 
will extract the coefficients for both the strictly parametric and smooth 
components of the model, labelled appropriately. summary(b) is also 
helpful. Often it is most useful to extract estimates of the smooths 
evaluated at a specified set of covariate values by using the gam `predict' 
method with type="terms". 
 
> And another question: we could find the coefficients for both 
> parametric term and nonparametric term, what do these coefficients
> for the nonparametric term stand for, the coefficients for the base 
> functions?
For mgcv gam objects the coefficients of the smooths are indeed the 
coefficients of the basis functions of the smooths.

best,
Simon

_____________________________________________________________________
> Simon Wood simon at stats.gla.ac.uk        www.stats.gla.ac.uk/~simon/
>>  Department of Statistics, University of Glasgow, Glasgow, G12 8QQ
>>>   Direct telephone: (0)141 330 4530          Fax: (0)141 330 4814



From jeff.hamann at forestinformatics.com  Tue Oct 12 19:15:44 2004
From: jeff.hamann at forestinformatics.com (Jeff D. Hamann)
Date: Tue, 12 Oct 2004 10:15:44 -0700 (PDT)
Subject: [R] constrained optimization using nlm/optim?
Message-ID: <46037.128.193.0.6.1097601344.squirrel@www.forestinformatics.com>

I'm looking for an example of a simple R script that impliments a
contrained nonlinear function using nlm or optim. I'm not exactly sure how
to impliment the constraints within the objective function that is passed
to nlm/optim.

obj.func <- function( p ) {

   x(p) <- unconstrained obj function value

   if( constraint1 > something ) {
      obj.func <- x(p)
   } else {
      obj.func <- some super huge number
  }

}

p <- c(0.1,2.4, 5)

nlm( obj.func, p, and a bunch of other arguments )

Any suggestions would be very helpful...

Jeff.



-- 
Jeff D. Hamann
Forest Informatics, Inc.
PO Box 1421
Corvallis, Oregon 97339-1421
phone 541-754-1428
fax 541-752-0288
jeff.hamann at forestinformatics.com
http://www.forestinformatics.com



From ripley at stats.ox.ac.uk  Tue Oct 12 19:30:36 2004
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Tue, 12 Oct 2004 18:30:36 +0100 (BST)
Subject: [R] locfit confidence intervals
In-Reply-To: <6.0.3.0.0.20041012125219.01bce8a8@brahms.cpmc.columbia.edu>
Message-ID: <Pine.LNX.4.44.0410121828060.2375-100000@gannet.stats>

Why not use predict(se=TRUE, band="local")?  That's what plot.locfit does,
via preplot.locfit.

I'm afraid the only real way to answer questions like this is to read the 
sources.

On Tue, 12 Oct 2004, Suresh Krishna wrote:

> 
> hi,
> 
> after
> 
> m=locfit(y~x,..........., family=binomial)
> plot(m,band="local") gives me a plot of locfit's result with a confidence 
> interval around it. i would like to get the actual values that are being 
> used to plot the lines in this figure.
> 
> i tried using predict, but the standard error it returns when i supply the 
> se=T argument, appears not to be the same as the CI produced by plot (it is 
> much larger...in my one case).  i have tried all the different type=".." 
> options, but none of them matches the CI produced by plot.
> 
> i only recently started using both R and local regression methods and am 
> still feeling my way around, so even though I have gone through the FAQs 
> and the online manuals and the archives of this group and the Loader book, 
> I have been unable to make much progress. any help would be very appreciated !!
> 
> thanks, suresh
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
> 
> 

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From fiebach at socrates.Berkeley.EDU  Tue Oct 12 19:46:09 2004
From: fiebach at socrates.Berkeley.EDU (Christian Fiebach)
Date: Tue, 12 Oct 2004 10:46:09 -0700
Subject: [R] problem with lapack.so under Debian Sid
Message-ID: <416C1861.5020704@socrates.berkeley.edu>

Dear list,

I am sorry to bother you with this.

I just upgraded yesterday to R 2.0.0 (using apt-get under Debian Sid), and
now have problems running e.g., summary(lm(...))

the lm is calculated, but the summary statement gives me the following 
error:

Error in La.chol2inv(x, size) : lapack routines cannot be loaded
In addition: Warning message:
unable to load shared library "/usr/lob/R/modules/lapack.so":
   /usr/lib/Pentium4_SSE2_512KB/liblapack.so.3: undefined symbol: ieeeck_

I searched through the archives, but unfortunately I am just not 
LINUX-knowledgeable
enough to understand the discussions concerning the algebra libraries.

Following an older mail to this list, I did
ldd /usr/lib/R/modules/lapack.so
everything looks fine except probably for the first line, which reads
libR.so => not found

I would appreciate any hints on how to fix this problem!

Thanks a lot.

Christian


-- 
Christian Fiebach, PhD

Department of Psychology &
Helen Wills Institute of Neuroscience
132 Barker Hall, MC# 3190
University of California, Berkeley
CA 94720-3190 Berkeley, USA
fon:  510-642-2839
fax:  510-642-3192
web:  http://fiebach.org
email: mailto:christian at fiebach.org



From dsonneborn at ucdavis.edu  Tue Oct 12 20:01:54 2004
From: dsonneborn at ucdavis.edu (Dean Sonneborn)
Date: Tue, 12 Oct 2004 11:01:54 -0700
Subject: [R] graph question
Message-ID: <6.1.0.6.2.20041012105809.01b5ad90@yellow.ucdavis.edu>

I would like to produce a graph which plots a log scale variable on the 
y-axis but have the tick marks on the y-axis  be the non log transformed 
values that are round like .5, 1, 2, 3, 4 etc. Has anyone done something 
like this in the past? How did you implement it in the code?

Thanks,
Dean



From elvis at xlsolutions-corp.com  Tue Oct 12 20:08:29 2004
From: elvis at xlsolutions-corp.com (elvis@xlsolutions-corp.com)
Date: Tue, 12 Oct 2004 11:08:29 -0700
Subject: [R] Course***R/S-plus Fundamentals and Programming Techniques In
	New York and Chicago
Message-ID: <20041012180829.26145.qmail@webmail01.mesa1.secureserver.net>

XLSolutions Corporation (www.xlsolutions-corp.com) is proud to 
announce  2-day "R/S-plus Fundamentals and Programming 
Techniques".

****New York, NY ----------------------- November 18th-19th
****Chicago, IL -------------------------- November 11th-12th
 


Reserve your seat now at the early bird rates! Payment due AFTER 
the class.


Course Description:
This two-day beginner to intermediate R/S-plus course focuses on a 
broad spectrum of topics, from reading raw data to a comparison of R 
and S. We will learn the essentials of data manipulation, graphical 
visualization and R/S-plus programming. We will explore statistical 
data analysis tools,including graphics with data sets. How to enhance 
your plots. We will perform basic statistics and fit linear regression
models. Participants are encouraged to bring data for interactive 
sessions


With the following outline:
- An Overview of R and S
- Data Manipulation and Graphics
- Using Lattice Graphics
- A Comparison of R and S-Plus
- How can R Complement SAS?
- Writing Functions
- Avoiding Loops
- Vectorization
- Statistical Modeling
- Project Management
- Techniques for Effective use of R and S
- Enhancing Plots
- Using High-level Plotting Functions
- Building and Distributing Packages (libraries)


Email us for group discounts.
Email Sue Turner: sue at xlsolutions-corp.com
Phone: 206-686-1578
Visit us: www.xlsolutions-corp.com/training.htm
Please let us know if you and your colleagues are interested in this 
classto take advantage of group discount. Register now to secure your 
seat! Interested in R/Splus Advanced course? email us.


Cheers,
Elvis Miller, PhD
Manager Training.
XLSolutions Corporation
206 686 1578
www.xlsolutions-corp.com
elvis at xlsolutions-corp.com



From pburns at pburns.seanet.com  Tue Oct 12 20:08:32 2004
From: pburns at pburns.seanet.com (Patrick Burns)
Date: Tue, 12 Oct 2004 19:08:32 +0100
Subject: [R] constrained optimization using nlm/optim?
In-Reply-To: <46037.128.193.0.6.1097601344.squirrel@www.forestinformatics.com>
References: <46037.128.193.0.6.1097601344.squirrel@www.forestinformatics.com>
Message-ID: <416C1DA0.3080407@pburns.seanet.com>

You want the penalty for the constraint to depend on how
much the constraint is broken -- unlike your sample code.
Page 336 of S Poetry has an example.

Patrick Burns

Burns Statistics
patrick at burns-stat.com
+44 (0)20 8525 0696
http://www.burns-stat.com
(home of S Poetry and "A Guide for the Unwilling S User")

Jeff D. Hamann wrote:

>I'm looking for an example of a simple R script that impliments a
>contrained nonlinear function using nlm or optim. I'm not exactly sure how
>to impliment the constraints within the objective function that is passed
>to nlm/optim.
>
>obj.func <- function( p ) {
>
>   x(p) <- unconstrained obj function value
>
>   if( constraint1 > something ) {
>      obj.func <- x(p)
>   } else {
>      obj.func <- some super huge number
>  }
>
>}
>
>p <- c(0.1,2.4, 5)
>
>nlm( obj.func, p, and a bunch of other arguments )
>
>Any suggestions would be very helpful...
>
>Jeff.
>
>
>
>  
>



From ripley at stats.ox.ac.uk  Tue Oct 12 20:12:20 2004
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Tue, 12 Oct 2004 19:12:20 +0100 (BST)
Subject: [R] problem with lapack.so under Debian Sid
In-Reply-To: <416C1861.5020704@socrates.berkeley.edu>
Message-ID: <Pine.LNX.4.44.0410121906170.2553-100000@gannet.stats>

On Tue, 12 Oct 2004, Christian Fiebach wrote:

> Dear list,
> 
> I am sorry to bother you with this.
> 
> I just upgraded yesterday to R 2.0.0 (using apt-get under Debian Sid), and
> now have problems running e.g., summary(lm(...))
> 
> the lm is calculated, but the summary statement gives me the following 
> error:
> 
> Error in La.chol2inv(x, size) : lapack routines cannot be loaded
> In addition: Warning message:
> unable to load shared library "/usr/lob/R/modules/lapack.so":
>    /usr/lib/Pentium4_SSE2_512KB/liblapack.so.3: undefined symbol: ieeeck_
> 
> I searched through the archives, but unfortunately I am just not 
> LINUX-knowledgeable
> enough to understand the discussions concerning the algebra libraries.
> 
> Following an older mail to this list, I did
> ldd /usr/lib/R/modules/lapack.so
> everything looks fine except probably for the first line, which reads
> libR.so => not found

Use R CMD ldd /usr/lib/R/modules/lapack.so to get an accurate picture.

I think you need to ask this on a Debian list.  The R developers do not 
recommend the use of an external Lapack library (and the Debian ones have 
had `patches' that were incorrect before now).  (See the discussion in the 
R-admin manual.)  In this case I guess that it has not been linked against 
the right set of external libraries for your particular machine.

> I would appreciate any hints on how to fix this problem!

Should work if you build R from the sources with the default options.

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From rpeng at jhsph.edu  Tue Oct 12 20:15:34 2004
From: rpeng at jhsph.edu (Roger D. Peng)
Date: Tue, 12 Oct 2004 14:15:34 -0400
Subject: [R] constrained optimization using nlm/optim?
In-Reply-To: <46037.128.193.0.6.1097601344.squirrel@www.forestinformatics.com>
References: <46037.128.193.0.6.1097601344.squirrel@www.forestinformatics.com>
Message-ID: <416C1F46.80603@jhsph.edu>

My guess is that approach will not work particularly well with nlm(), 
which expects the objective function to be smooth.  You might have 
better luck using the Nelder-Mead method in optim().

-roger

Jeff D. Hamann wrote:
> I'm looking for an example of a simple R script that impliments a
> contrained nonlinear function using nlm or optim. I'm not exactly sure how
> to impliment the constraints within the objective function that is passed
> to nlm/optim.
> 
> obj.func <- function( p ) {
> 
>    x(p) <- unconstrained obj function value
> 
>    if( constraint1 > something ) {
>       obj.func <- x(p)
>    } else {
>       obj.func <- some super huge number
>   }
> 
> }
> 
> p <- c(0.1,2.4, 5)
> 
> nlm( obj.func, p, and a bunch of other arguments )
> 
> Any suggestions would be very helpful...
> 
> Jeff.
> 
> 
>



From ripley at stats.ox.ac.uk  Tue Oct 12 20:15:53 2004
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Tue, 12 Oct 2004 19:15:53 +0100 (BST)
Subject: [R] graph question
In-Reply-To: <6.1.0.6.2.20041012105809.01b5ad90@yellow.ucdavis.edu>
Message-ID: <Pine.LNX.4.44.0410121912470.2553-100000@gannet.stats>

On Tue, 12 Oct 2004, Dean Sonneborn wrote:

> I would like to produce a graph which plots a log scale variable on the 
> y-axis but have the tick marks on the y-axis  be the non log transformed 
> values that are round like .5, 1, 2, 3, 4 etc. Has anyone done something 
> like this in the past? How did you implement it in the code?

plot(c(0.5, 5), 1:2, log="x", xaxt="n")
axis(1, at=c(0.5, 1:4))

Take a look at `An Introduction to R' and ?axis.

BTW, please try to use a more precise subject line. Something like
`Adding a custom axis to log-scale plots'?

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From mn216 at columbia.edu  Tue Oct 12 20:10:19 2004
From: mn216 at columbia.edu (Murad Nayal)
Date: Tue, 12 Oct 2004 14:10:19 -0400
Subject: [R] 64-bit R on Intel Xeon EM64T running fine
References: <Pine.LNX.4.53.0410112148540.3983@lakeforest.homelinux.com>
	<416BD1F7.6040006@jhsph.edu>
Message-ID: <416C1E0B.47EBC724@columbia.edu>



"Roger D. Peng" wrote:
> 
> This is good news.  As far as I know R has built for quite some time
> now on a number of 64 bit platforms (Linux on AMD Opteron/Athlon64,
> Solaris/Sparc) 


I"ll add SGI/IRIX 64 bit platform to the list. I've been running a 64
bit-compiled R on an SGI octane 2 for over a year now without a problem.
R sessions often allocate around 8 GB of memory.


>but I can't recall seeing a build on Intel with the 64
> bit extensions.  By the way, did you happen to run `make check' just
> for kicks?
> 
> -roger
> 
> Michael Seewald wrote:
> > Dear mailing-list members,
> >
> > In the days of cheap RAM and microarray applications feasting on memory,
> > 64-bit computers become more and more useful - to actually make use of memory
> > beyond the magic 4GB border. I would like to report the success of running
> > 64-bit R on an Intel Xeon EM64T machine under Linux. Just like on an AMD
> > Opteron, R v2.0.0 compiles fine (and out of the box) and is happily allocating
> > memory until RAM and swap reach their limit.
> >
> > Hardware:
> > - HP xw6200 workstation
> > - dual Intel Xeon 3.4GHz with hyper-threading enabled
> > - 4GB RAM, 4GB swap
> >
> > System: either
> > - Fedora Core 2 x86_64 bit Linux
> > or
> > - Red Hat Enterprise Linux Workstation 3.0 x86_64 bit
> >
> > R:
> > - v2.0.0
> >
> > Really, no problems at all during setup, a big thank you to the R developers
> > making this possible!
> >
> > Best wishes,
> > Michael
> >
> > ______________________________________________
> > R-help at stat.math.ethz.ch mailing list
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
> >
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html

-- 
Murad Nayal M.D. Ph.D.
Department of Biochemistry and Molecular Biophysics
College of Physicians and Surgeons of Columbia University
630 West 168th Street. New York, NY 10032
Tel: 212-305-6884	Fax: 212-305-6926



From tortosa at uji.es  Tue Oct 12 20:33:45 2004
From: tortosa at uji.es (Emili Tortosa-Ausina)
Date: Tue, 12 Oct 2004 11:33:45 -0700
Subject: [R] bandwidths for bivariate density estimation
Message-ID: <6.0.0.22.2.20041012112709.01c30e60@mail.uji.es>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20041012/975070d4/attachment.pl

From tortosa at uji.es  Tue Oct 12 20:51:11 2004
From: tortosa at uji.es (Emili Tortosa-Ausina)
Date: Tue, 12 Oct 2004 11:51:11 -0700
Subject: [R] bandwidths for bivariate density estimation
Message-ID: <6.0.0.22.2.20041012115014.01c0e148@mail.uji.es>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20041012/bda424ff/attachment.pl

From dmb at mrc-dunn.cam.ac.uk  Tue Oct 12 21:04:04 2004
From: dmb at mrc-dunn.cam.ac.uk (Dan Bolser)
Date: Tue, 12 Oct 2004 20:04:04 +0100 (BST)
Subject: [R] plot hclust - canberra dist + median linkage 
In-Reply-To: <16747.64126.266598.397639@gargle.gargle.HOWL>
Message-ID: <Pine.LNX.4.21.0410122002510.6300-100000@mail.mrc-dunn.cam.ac.uk>

On Tue, 12 Oct 2004, Martin Maechler wrote:

>>>>>> "Dan" == Dan Bolser <dmb at mrc-dunn.cam.ac.uk>
>>>>>>     on Mon, 11 Oct 2004 16:21:53 +0100 (BST) writes:
>
>    Dan> Gives strange results.
>
>    Dan> I get 'weird' dendrograms with canberra / binary distance metric and
>    Dan> median / centroid cluster methods.
>
>it doesn't depend on the metric: 
>Both 'median' and 'centroid' methods are known to *not*
>guarantee ``monotone distance measures'', or equivalently to
>possibly lead to dendrograms with so called ``inversions''.
>We should add this to help page for hclust().

Cool, I could do with more to read about these methods. It might be worth
noteing that no bootstrap exists for the method (implemented in R that
is).

How can I quickly run a test on a sub-set of x? (i.e. quick bootstrap by
hand)?

Cheers,
Dan.

>Probably for this reason, agnes() from the cluster package
>doesn't have these two methods [explicitly] -- though it now
>allows general parameter Lance-William formula methods which can
>also lead to inversions.
>
>    Dan> Is this just my data?
>
>evidently not.  Though the problem does not appear for all data
>sets...
>
>Regards,
>Martin Maechler
>



From edd at debian.org  Tue Oct 12 21:22:26 2004
From: edd at debian.org (Dirk Eddelbuettel)
Date: Tue, 12 Oct 2004 14:22:26 -0500
Subject: [R] problem with lapack.so under Debian Sid
In-Reply-To: <416C1861.5020704@socrates.berkeley.edu>
References: <416C1861.5020704@socrates.berkeley.edu>
Message-ID: <20041012192226.GA2970@sonny.eddelbuettel.com>

On Tue, Oct 12, 2004 at 10:46:09AM -0700, Christian Fiebach wrote:
> Dear list,
> 
> I am sorry to bother you with this.
> 
> I just upgraded yesterday to R 2.0.0 (using apt-get under Debian Sid), and
> now have problems running e.g., summary(lm(...))
> 
> the lm is calculated, but the summary statement gives me the following 
> error:
> 
> Error in La.chol2inv(x, size) : lapack routines cannot be loaded
> In addition: Warning message:
> unable to load shared library "/usr/lob/R/modules/lapack.so":
>   /usr/lib/Pentium4_SSE2_512KB/liblapack.so.3: undefined symbol: ieeeck_

Is that the liblapack, i.e. do you actually have a Pentium IV ?  It could be
that this isn;t R specific, but related to the choice Debian gives with
respect to lapack and blas libraries.  You could try to downgrade to the
refblas3 and lapack3 packages.  

We should probably continue this off-list.

Dirk


> I searched through the archives, but unfortunately I am just not 
> LINUX-knowledgeable
> enough to understand the discussions concerning the algebra libraries.
> 
> Following an older mail to this list, I did
> ldd /usr/lib/R/modules/lapack.so
> everything looks fine except probably for the first line, which reads
> libR.so => not found
> 
> I would appreciate any hints on how to fix this problem!
> 
> Thanks a lot.
> 
> Christian
> 
> 
> -- 
> Christian Fiebach, PhD
> 
> Department of Psychology &
> Helen Wills Institute of Neuroscience
> 132 Barker Hall, MC# 3190
> University of California, Berkeley
> CA 94720-3190 Berkeley, USA
> fon:  510-642-2839
> fax:  510-642-3192
> web:  http://fiebach.org
> email: mailto:christian at fiebach.org
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html

-- 
Those are my principles, and if you don't like them... well, I have others.
                                                -- Groucho Marx



From tlumley at u.washington.edu  Tue Oct 12 22:00:02 2004
From: tlumley at u.washington.edu (Thomas Lumley)
Date: Tue, 12 Oct 2004 13:00:02 -0700 (PDT)
Subject: [R] lm#contrasts#one level in factor: bug or feature
In-Reply-To: <156CDC8CCFD1894295D2907F16337A48B2ADA2@bru-s-006.europe.shell.com>
References: <156CDC8CCFD1894295D2907F16337A48B2ADA2@bru-s-006.europe.shell.com>
Message-ID: <Pine.A41.4.61a.0410121253150.295394@homer07.u.washington.edu>

On Tue, 12 Oct 2004, Ritter, Christian C MCIL-CTANL/S wrote:

> (R.1.9.1; win2000)
>
> Since it's about the tenth time I had to write an "if" around this to catch the error ...
> Let's look at the line
>
> 	myfit<-lm(res~groupvar,data=Data)
>
> Here res is of numeric type and groupvar is a factor. On first sight, it 
> would be logical that if groupvar had only one (single) level we would 
> get:
>
> 	Error in "contrasts<-"(`*tmp*`, value = "contr.treatment") : 
> contrasts can be applied only to factors with 2 or more levels
>
> But then again, it's also inconsistent: Normally redundant variables on 
> the right of the ~ (variables which are constant or linearly dependent 
> on previous variables) don't lead to "Error". They just get a "NA" as a 
> coefficient estimate. A factor with a single level is of this type, it 
> is just a constant.  Obviously (to me) lm (or model.matrix.default) 
> should not try to calculate contrasts on it and it's coefficient should 
> be NA. Shouldn't it?

No, a factor with k levels isn't just a set of k linearly dependent 
variables (otherwise a factor would always give an NA coefficient for one 
level).  A factor specifies a set of contrasts (usually k-1 of them) that 
go into the model matrix.  In that sense a factor with 1 level should 
specify zero columns of the model matrix, rather than giving an error.

>
> Why is this not unimportant? Imagine you make a model of the above type 
> for a data set with more than one level in groupvar. Then you select 
> subsets and refit. It shouldn't die with error if you select a subset 
> with only one level of groupvar.
>

Possibly, but your results are likely to be pretty meaningless.  If the 
subset does not have *all* the levels of the factor then the contrasts 
will change and the variable will be coded differently, possibly giving 
coefficients with different meanings.

In the case of contr.treatment, for example, if the lowest level of the 
factor is missing then all the other levels are recoded as contrasts to 
the next lowest level.


 	-thomas



From ifiske at ufl.edu  Tue Oct 12 23:07:46 2004
From: ifiske at ufl.edu (Ian Fiske)
Date: Tue, 12 Oct 2004 17:07:46 -0400
Subject: [R] covariate selection?
Message-ID: <416C47A2.6000403@ufl.edu>

Hello,

I am hoping someone can help me with the following multivariate issue:  
I have a model consisting of about 50 covariates.  I would like to 
reduce this to about 5 covariate for the reduced model by combining 
cofactors that are strongly correlated.  Is there a package or function 
that would help me with this in R?  I appreciate any suggestions.

Thanks,
Ian



From szhan at uoguelph.ca  Tue Oct 12 23:09:22 2004
From: szhan at uoguelph.ca (szhan@uoguelph.ca)
Date: Tue, 12 Oct 2004 17:09:22 -0400
Subject: [R] Why I can't retrieve GO identifier correctly?
Message-ID: <1097615362.416c4802951a7@webmail.uoguelph.ca>

Hello, R experts,
I tried to retrieve all biological process GO terms at level 3 starting
"biological process" as level 1 using the code as bellows:

1 library(GO)
2 library(GOstats)
3 level2<-getGOChildren("GO:0008150")$"GO:0008150"$Children
4 for ( i in 1:length(level2)) {
5    level3 <- getGOChildren(level2[i])$level2[i]$Children
6    for ( j in 1:length(level3)){
7       level3term <- getGOTerm(as.character(level3[j]))$BP$level3[j]
8       level3term
9    }
10 }
What is the difference between line 3 and line5? In the line 3 I retrieved the
GO identifiers at level 2 successfullly but in the line 5 I got nothing. How to
correct the line 5 to retrieve the GO terms at level 3 correctly?
Thank you in advance!
Josh



From jcmartinez at banxico.org.mx  Tue Oct 12 23:25:02 2004
From: jcmartinez at banxico.org.mx (=?iso-8859-1?Q?Mart=EDnez_Ovando_Juan_Carlos?=)
Date: Tue, 12 Oct 2004 16:25:02 -0500
Subject: [R] covariate selection?
Message-ID: <F74A1EABDCCFFB4893FD93C96408F58A63BC3B@BMCORREO.banxico.org.mx>

Hello Ian,

?princomp

If your covariates are scalars, and the following documents:

http://www.jstatsoft.org/v07/i01/drdoc.pdf

http://www.bioconductor.org/workshops/Milan/PDF/Lab12.pdf


Best wishes. 

Saludos,
 
Juan Carlos Mart??nez Ovando
Banco de M??xico
Av. 5 de Mayo No. 18
Piso 5 Secci??n D
Col. Centro
06059  M??xico, D. F.
Tel. +52 55 52.37.20.00 ext. 3594
Fax. +52 55 52.37.27.03
e-mail: jcmartinez at banxico.org.mx
 

-----Mensaje original-----
De: Ian Fiske [mailto:ifiske at ufl.edu] 
Enviado el: Martes, 12 de Octubre de 2004 04:08 PM
Para: r-help at stat.math.ethz.ch
Asunto: [R] covariate selection?

Hello,

I am hoping someone can help me with the following multivariate issue:  
I have a model consisting of about 50 covariates.  I would like to 
reduce this to about 5 covariate for the reduced model by combining 
cofactors that are strongly correlated.  Is there a package or function 
that would help me with this in R?  I appreciate any suggestions.

Thanks,
Ian

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html



From Kevin.Wang at maths.anu.edu.au  Tue Oct 12 23:31:42 2004
From: Kevin.Wang at maths.anu.edu.au (Kevin Wang)
Date: Wed, 13 Oct 2004 07:31:42 +1000 (EST)
Subject: [R] A question in R
In-Reply-To: <416BFEBD.2060300@loyaltymatrix.com>
References: <20041012154934.91929.qmail@web50902.mail.yahoo.com>
	<416BFEBD.2060300@loyaltymatrix.com>
Message-ID: <Pine.GSO.4.58.0410130730450.11781@yin>

Hi,

On Tue, 12 Oct 2004, Kevin Bartz wrote:

> R looks less than fondly on Excel files. The easiest solution for you
> will be to export your Excel file to a tab-delimited text format (Save
> -> (.txt) Tab-delimited Text), and then use read.delim as you did. Does
> that make sense?

The alternative (tricky but more convinient in the long run) is to use the
read.xls() function from gregmisc bundle.

Cheers,

Kevin

--------------------------------
Ko-Kang Kevin Wang
PhD Student
Centre for Mathematics and its Applications
Building 27, Room 1004
Mathematical Sciences Institute (MSI)
Australian National University
Canberra, ACT 0200
Australia

Homepage: http://wwwmaths.anu.edu.au/~wangk/
Ph (W): +61-2-6125-2431
Ph (H): +61-2-6125-7407
Ph (M): +61-40-451-8301



From p.dalgaard at biostat.ku.dk  Wed Oct 13 00:07:17 2004
From: p.dalgaard at biostat.ku.dk (Peter Dalgaard)
Date: 13 Oct 2004 00:07:17 +0200
Subject: [R] Why I can't retrieve GO identifier correctly?
In-Reply-To: <1097615362.416c4802951a7@webmail.uoguelph.ca>
References: <1097615362.416c4802951a7@webmail.uoguelph.ca>
Message-ID: <x2wtxvh9pm.fsf@biostat.ku.dk>

szhan at uoguelph.ca writes:

> Hello, R experts,
> I tried to retrieve all biological process GO terms at level 3 starting
> "biological process" as level 1 using the code as bellows:
> 
> 1 library(GO)
> 2 library(GOstats)
> 3 level2<-getGOChildren("GO:0008150")$"GO:0008150"$Children
> 4 for ( i in 1:length(level2)) {
> 5    level3 <- getGOChildren(level2[i])$level2[i]$Children
> 6    for ( j in 1:length(level3)){
> 7       level3term <- getGOTerm(as.character(level3[j]))$BP$level3[j]
> 8       level3term
> 9    }
> 10 }
> What is the difference between line 3 and line5? In the line 3 I retrieved the
> GO identifiers at level 2 successfullly but in the line 5 I got nothing. How to
> correct the line 5 to retrieve the GO terms at level 3 correctly?
> Thank you in advance!
> Josh

I think you'll have better luck with that question if you take it to
the Bioconductor list.


-- 
   O__  ---- Peter Dalgaard             Blegdamsvej 3  
  c/ /'_ --- Dept. of Biostatistics     2200 Cph. N   
 (*) \(*) -- University of Copenhagen   Denmark      Ph: (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)             FAX: (+45) 35327907



From ifiske at ufl.edu  Wed Oct 13 00:17:02 2004
From: ifiske at ufl.edu (Ian Fiske)
Date: Tue, 12 Oct 2004 18:17:02 -0400
Subject: [R] covariate selection?
In-Reply-To: <F74A1EABDCCFFB4893FD93C96408F58A63BC3B@BMCORREO.banxico.org.mx>
References: <F74A1EABDCCFFB4893FD93C96408F58A63BC3B@BMCORREO.banxico.org.mx>
Message-ID: <416C57DE.2060103@ufl.edu>

Thanks Juan.  I thought that was what I was looking for, but really, I 
want to know which of the original covariates could best be used to take 
advantage of their colinearity without creating new variables.  I think 
PCA creates new variables.  SAS and SPSS can do what I'm talking about, 
but I would like to use R for this.

Thanks,
Ian



Mart??nez Ovando Juan Carlos wrote:

>Hello Ian,
>
>?princomp
>
>If your covariates are scalars, and the following documents:
>
>http://www.jstatsoft.org/v07/i01/drdoc.pdf
>
>http://www.bioconductor.org/workshops/Milan/PDF/Lab12.pdf
>
>
>Best wishes. 
>
>Saludos,
> 
>Juan Carlos Mart??nez Ovando
>Banco de M??xico
>Av. 5 de Mayo No. 18
>Piso 5 Secci??n D
>Col. Centro
>06059  M??xico, D. F.
>Tel. +52 55 52.37.20.00 ext. 3594
>Fax. +52 55 52.37.27.03
>e-mail: jcmartinez at banxico.org.mx
> 
>
>-----Mensaje original-----
>De: Ian Fiske [mailto:ifiske at ufl.edu] 
>Enviado el: Martes, 12 de Octubre de 2004 04:08 PM
>Para: r-help at stat.math.ethz.ch
>Asunto: [R] covariate selection?
>
>Hello,
>
>I am hoping someone can help me with the following multivariate issue:  
>I have a model consisting of about 50 covariates.  I would like to 
>reduce this to about 5 covariate for the reduced model by combining 
>cofactors that are strongly correlated.  Is there a package or function 
>that would help me with this in R?  I appreciate any suggestions.
>
>Thanks,
>Ian
>
>______________________________________________
>R-help at stat.math.ethz.ch mailing list
>https://stat.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>
>
>  
>



From bates at wisc.edu  Wed Oct 13 00:25:31 2004
From: bates at wisc.edu (Douglas Bates)
Date: Wed, 13 Oct 2004 00:25:31 +0200
Subject: [R] problem with lapack.so under Debian Sid
In-Reply-To: <Pine.LNX.4.44.0410121906170.2553-100000@gannet.stats>
References: <Pine.LNX.4.44.0410121906170.2553-100000@gannet.stats>
Message-ID: <416C59DB.5020503@wisc.edu>

On Tue, 12 Oct 2004, Christian Fiebach wrote:

>  
>
>>Dear list,
>>
>>I am sorry to bother you with this.
>>
>>I just upgraded yesterday to R 2.0.0 (using apt-get under Debian Sid), and
>>now have problems running e.g., summary(lm(...))
>>
>>the lm is calculated, but the summary statement gives me the following 
>>error:
>>
>>Error in La.chol2inv(x, size) : lapack routines cannot be loaded
>>In addition: Warning message:
>>unable to load shared library "/usr/lob/R/modules/lapack.so":
>>   /usr/lib/Pentium4_SSE2_512KB/liblapack.so.3: undefined symbol: ieeeck_
>>
>>I searched through the archives, but unfortunately I am just not 
>>LINUX-knowledgeable
>>enough to understand the discussions concerning the algebra libraries.
>>
>>Following an older mail to this list, I did
>>ldd /usr/lib/R/modules/lapack.so
>>everything looks fine except probably for the first line, which reads
>>libR.so => not found
>>    
>>
So does that mean that you have one of the Debian packages atlas3-base 
or atlas3-3dnow or atlas3-sse or atlas3-sse2 or lapack3 installed?  You 
should have had at least one of those installed to be able to install 
the r-base-core Debian package.

In the output from ldd /usr/lib/R/modules/lapack.so, does liblapack.so.3 
get resolved to /usr/lib/liblapack.so.3 and, if so, what is that file?  
It is probably a link to /usr/lib/liblapack-3.so which, in turn, is a 
link to /etc/alternatives/liblapack-3.so.  Check that the file linked as 
/etc/alternatives/liblapack-3.so actually exists.



From spencer.graves at pdf.com  Wed Oct 13 00:26:11 2004
From: spencer.graves at pdf.com (Spencer Graves)
Date: Tue, 12 Oct 2004 15:26:11 -0700
Subject: [R] covariate selection?
In-Reply-To: <416C47A2.6000403@ufl.edu>
References: <416C47A2.6000403@ufl.edu>
Message-ID: <416C5A03.7040709@pdf.com>

      Have you considered stepwise regression, e.g., "step" or "stepAIC" 
in library(MASS)?  The documentation for both contain examples. 

      hope this helps. 
      spencer graves

Ian Fiske wrote:

> Hello,
>
> I am hoping someone can help me with the following multivariate 
> issue:  I have a model consisting of about 50 covariates.  I would 
> like to reduce this to about 5 covariate for the reduced model by 
> combining cofactors that are strongly correlated.  Is there a package 
> or function that would help me with this in R?  I appreciate any 
> suggestions.
>
> Thanks,
> Ian
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html


-- 
Spencer Graves, PhD, Senior Development Engineer
O:  (408)938-4420;  mobile:  (408)655-4567



From jcmartinez at banxico.org.mx  Wed Oct 13 00:55:58 2004
From: jcmartinez at banxico.org.mx (=?iso-8859-1?Q?Mart=EDnez_Ovando_Juan_Carlos?=)
Date: Tue, 12 Oct 2004 17:55:58 -0500
Subject: [R] covariate selection?
Message-ID: <F74A1EABDCCFFB4893FD93C96408F58A652CCF@BMCORREO.banxico.org.mx>

Hello Ian,

Sorry. I don't really understand your problem, which is of model selection. That's right? 

You could use some criteria based in likelihood. For instante Akaike (AIC) or Schwarz criteria (BIC), see: 

?AIC

?mle.aic

(The best model is determined minimizing AIC or BIC).

I hope this help you.

Greetings,
 
		Juan Carlos 

-----Mensaje original-----
De: Ian Fiske [mailto:ifiske at ufl.edu] 
Enviado el: Martes, 12 de Octubre de 2004 05:17 PM
Para: Mart??nez Ovando Juan Carlos
CC: r-help at stat.math.ethz.ch
Asunto: Re: [R] covariate selection?

Thanks Juan.  I thought that was what I was looking for, but really, I 
want to know which of the original covariates could best be used to take 
advantage of their colinearity without creating new variables.  I think 
PCA creates new variables.  SAS and SPSS can do what I'm talking about, 
but I would like to use R for this.

Thanks,
Ian



Mart??nez Ovando Juan Carlos wrote:

>Hello Ian,
>
>?princomp
>
>If your covariates are scalars, and the following documents:
>
>http://www.jstatsoft.org/v07/i01/drdoc.pdf
>
>http://www.bioconductor.org/workshops/Milan/PDF/Lab12.pdf
>
>
>Best wishes. 
>
>Saludos,
> 
>Juan Carlos Mart??nez Ovando
>Banco de M??xico
>Av. 5 de Mayo No. 18
>Piso 5 Secci??n D
>Col. Centro
>06059  M??xico, D. F.
>Tel. +52 55 52.37.20.00 ext. 3594
>Fax. +52 55 52.37.27.03
>e-mail: jcmartinez at banxico.org.mx
> 
>
>-----Mensaje original-----
>De: Ian Fiske [mailto:ifiske at ufl.edu] 
>Enviado el: Martes, 12 de Octubre de 2004 04:08 PM
>Para: r-help at stat.math.ethz.ch
>Asunto: [R] covariate selection?
>
>Hello,
>
>I am hoping someone can help me with the following multivariate issue:  
>I have a model consisting of about 50 covariates.  I would like to 
>reduce this to about 5 covariate for the reduced model by combining 
>cofactors that are strongly correlated.  Is there a package or function 
>that would help me with this in R?  I appreciate any suggestions.
>
>Thanks,
>Ian
>
>______________________________________________
>R-help at stat.math.ethz.ch mailing list
>https://stat.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>
>
>  
>



From christian_mora at vtr.net  Wed Oct 13 01:07:05 2004
From: christian_mora at vtr.net (Christian Mora)
Date: Tue, 12 Oct 2004 20:07:05 -0300
Subject: [R] covariate selection?
In-Reply-To: <F74A1EABDCCFFB4893FD93C96408F58A652CCF@BMCORREO.banxico.org.mx>
Message-ID: <000401c4b0b0$31048910$fd5a68c8@CPQ26719243321>

Hi Ian
Have you tried help.search("pca")?
Christian

-----Original Message-----
From: r-help-bounces at stat.math.ethz.ch
[mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Mart??nez Ovando
Juan Carlos
Sent: Tuesday, October 12, 2004 7:56 PM
To: Ian Fiske
Cc: r-help at stat.math.ethz.ch
Subject: RE: [R] covariate selection?


Hello Ian,

Sorry. I don't really understand your problem, which is of model
selection. That's right? 

You could use some criteria based in likelihood. For instante Akaike
(AIC) or Schwarz criteria (BIC), see: 

?AIC

?mle.aic

(The best model is determined minimizing AIC or BIC).

I hope this help you.

Greetings,
 
		Juan Carlos 

-----Mensaje original-----
De: Ian Fiske [mailto:ifiske at ufl.edu] 
Enviado el: Martes, 12 de Octubre de 2004 05:17 PM
Para: Mart??nez Ovando Juan Carlos
CC: r-help at stat.math.ethz.ch
Asunto: Re: [R] covariate selection?

Thanks Juan.  I thought that was what I was looking for, but really, I 
want to know which of the original covariates could best be used to take

advantage of their colinearity without creating new variables.  I think 
PCA creates new variables.  SAS and SPSS can do what I'm talking about, 
but I would like to use R for this.

Thanks,
Ian



Mart??nez Ovando Juan Carlos wrote:

>Hello Ian,
>
>?princomp
>
>If your covariates are scalars, and the following documents:
>
>http://www.jstatsoft.org/v07/i01/drdoc.pdf
>
>http://www.bioconductor.org/workshops/Milan/PDF/Lab12.pdf
>
>
>Best wishes.
>
>Saludos,
> 
>Juan Carlos Mart??nez Ovando
>Banco de M??xico
>Av. 5 de Mayo No. 18
>Piso 5 Secci??n D
>Col. Centro
>06059  M??xico, D. F.
>Tel. +52 55 52.37.20.00 ext. 3594
>Fax. +52 55 52.37.27.03
>e-mail: jcmartinez at banxico.org.mx
> 
>
>-----Mensaje original-----
>De: Ian Fiske [mailto:ifiske at ufl.edu]
>Enviado el: Martes, 12 de Octubre de 2004 04:08 PM
>Para: r-help at stat.math.ethz.ch
>Asunto: [R] covariate selection?
>
>Hello,
>
>I am hoping someone can help me with the following multivariate issue:
>I have a model consisting of about 50 covariates.  I would like to 
>reduce this to about 5 covariate for the reduced model by combining 
>cofactors that are strongly correlated.  Is there a package or function

>that would help me with this in R?  I appreciate any suggestions.
>
>Thanks,
>Ian
>
>______________________________________________
>R-help at stat.math.ethz.ch mailing list 
>https://stat.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide! 
>http://www.R-project.org/posting-guide.html
>
>
>  
>

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide!
http://www.R-project.org/posting-guide.html



From br44114 at yahoo.com  Wed Oct 13 01:56:21 2004
From: br44114 at yahoo.com (bogdan romocea)
Date: Tue, 12 Oct 2004 16:56:21 -0700 (PDT)
Subject: [R] read "4-jan-02" as date
In-Reply-To: <416AAC25.1070707@pdf.com>
Message-ID: <20041012235621.32149.qmail@web50310.mail.yahoo.com>

Thank you everyone. Indeed, I had read the data via
read.csv and the date column was a factor. Everything works
fine if I convert to character first.

Regards,
b.


--- Sundar Dorai-Raj <sundar.dorai-raj at PDF.COM> wrote:

> 
> 
> bogdan romocea wrote:
> 
> > Dear R users,
> > 
> > I have a column with dates (character) in a data frame:
> > 12-Jan-01 11-Jan-01 10-Jan-01 9-Jan-01  8-Jan-01 
> 5-Jan-01
> > and I need to convert them to (Julian) dates so that I
> can
> > sort the whole data frame by date. I thought it would
> be
> > very simple, but after checking the documentation and
> the
> > list I still don't have something that works.
> > 
> > 1. as.Date returns the error below. What am I doing
> wrong?
> > As far as I can see the character strings are in
> standard
> > format.
> > d$Date <- as.Date(d$Date, format="%d-%b-%y")
> > Error in fromchar(x) : character string is not in a
> > standard unambiguous format
> > 
> > 2. as.date {Survival} produces this error,
> > d$Date <- as.date(d$Date, order = "dmy")
> > Error in as.date(d$Date, order = "dmy") : Cannot coerce
> to
> > date format
> > 
> > 3. Assuming all else fails, is there a text function
> > similar to SCAN in SAS? Given a string like "9-Jan-01"
> and
> > "-" as separator, I'd like a function that can read the
> > first, second and third values (9, Jan, 01), so that I
> can
> > get Julian dates with mdy.date {survival}.
> > 
> > Thanks in advance,
> > b.
> > 
> 
> If you're reading this from a file (via read.table, for
> example), then 
> your date column is probably a factor. Convert to
> character first.
> 
>  > x
> [1] 12-Jan-01 11-Jan-01 10-Jan-01 9-Jan-01  8-Jan-01 
> 5-Jan-01
> Levels: 10-Jan-01 11-Jan-01 12-Jan-01 5-Jan-01 8-Jan-01
> 9-Jan-01
>  >
>  > Date(x, format="%d-%b-%y")
> Error in fromchar(x) : character string is not in a
> standard unambiguous 
> format
>  >
>  > sort(as.Date(as.character(x), format="%d-%b-%y"))
> [1] "2001-01-05" "2001-01-08" "2001-01-09" "2001-01-10"
> "2001-01-11"
> [6] "2001-01-12"
> 
> 
> --sundar
> 
>



From andy_liaw at merck.com  Wed Oct 13 02:10:12 2004
From: andy_liaw at merck.com (Liaw, Andy)
Date: Tue, 12 Oct 2004 20:10:12 -0400
Subject: [R] 64-bit R on Intel Xeon EM64T running fine
Message-ID: <3A822319EB35174CA3714066D590DCD504AF8526@usrymx25.merck.com>

This article might be of interest to some:
http://www.linux-mag.com/2004-07/athlon_01.html

Regarding 64-bit build of R, I can confirm Irix, Alpha and AIX, although
some w/o readline/jpeg/png/X support, partially because of difficulties
linking against 64-bit version of those libraries.

Best,
Andy

> From: Prof Brian Ripley
> 
> On Tue, 12 Oct 2004, Roger D. Peng wrote:
> 
> > This is good news.  As far as I know R has built for quite 
> some time 
> > now on a number of 64 bit platforms (Linux on AMD Opteron/Athlon64, 
> > Solaris/Sparc
> 
> and Alpha and Irix and HP-UX and AIX as far as I understand.
> 
> ) but I can't recall seeing a build on Intel with the 64 
> > bit extensions.  
> 
> As I understand it, this is an Intel `clone' of AMD64, so the only
> news would be if there were any problems: in almost all cases the
> executable code is identical to that compiled for AMD64 and in the GNU
> classification it is also "x86_64-unknown-linux-gnu".
> 
> I would even expect a Goto BLAS to work, if not as well as on 
> the specific 
> Opteron it is tuned for.  (Fast BLASes are an essential part 
> of making the 
> most of 64-bit processors on large problems.)
> 
> > Michael Seewald wrote:
> > > Dear mailing-list members,
> > > 
> > > In the days of cheap RAM and microarray applications 
> feasting on memory,
> > > 64-bit computers become more and more useful - to 
> actually make use of memory
> > > beyond the magic 4GB border. I would like to report the 
> success of running
> > > 64-bit R on an Intel Xeon EM64T machine under Linux. Just 
> like on an AMD
> > > Opteron, R v2.0.0 compiles fine (and out of the box) and 
> is happily allocating
> > > memory until RAM and swap reach their limit.
> > > 
> > > Hardware:
> > > - HP xw6200 workstation
> > > - dual Intel Xeon 3.4GHz with hyper-threading enabled
> > > - 4GB RAM, 4GB swap
> > > 
> > > System: either
> > > - Fedora Core 2 x86_64 bit Linux
> > > or
> > > - Red Hat Enterprise Linux Workstation 3.0 x86_64 bit
> > > 
> > > R:
> > > - v2.0.0
> > > 
> > > Really, no problems at all during setup, a big thank you 
> to the R developers
> > > making this possible!
> 
> -- 
> Brian D. Ripley,                  ripley at stats.ox.ac.uk
> Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
> University of Oxford,             Tel:  +44 1865 272861 (self)
> 1 South Parks Road,                     +44 1865 272866 (PA)
> Oxford OX1 3TG, UK                Fax:  +44 1865 272595
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
> 
>



From jfox at mcmaster.ca  Wed Oct 13 02:23:08 2004
From: jfox at mcmaster.ca (John Fox)
Date: Tue, 12 Oct 2004 20:23:08 -0400
Subject: [R] Diagnosing trouble with R-2.0, Fedora Core 2, and Rcmdf
In-Reply-To: <416ACC27.6030806@ku.edu>
Message-ID: <20041013002309.PFFY15612.tomts16-srv.bellnexxia.net@JohnDesktop8300>

Dear Paul,

I've now had a chance to check into these problems further:

> -----Original Message-----
> From: r-help-bounces at stat.math.ethz.ch 
> [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Paul Johnson
> Sent: Monday, October 11, 2004 1:09 PM
> To: r help
> Subject: [R] Diagnosing trouble with R-2.0, Fedora Core 2, and Rcmdf
> 
> Greetings, R-help!
> 
> On 2 Fedora Core 2 Linux systems, i've completely erased the 
> previous R and all packages and then installed R-2.0 and 
> installed fresh packages.
> 
> In using Rcmdr, I see some trouble and I wonder if other 
> people see this and if it is due to the tcl/tk, or R, or 
> Rcmdr.  (If readers have not yet tried Rcmdr, I recommend it 
> not just because of the GUI it provides, but also because 
> Prof. Fox has created several very handy commands, like 
> scatter3d, which make it possible to use advanced packages like rgl. 
> Rcmdr provides several very handy wrapper commands that "just 
> work" and users don't have to fuss over so many details! And, 
> if you are new at R, you can use pull down menus and then 
> Rcmdr displays the commands that correspond with those menu 
> options. Very educational!)
> 
> In no particular order, here are the issues I see with R-2.0 and Rcmdr
> 
> 1. In the panel to select a dataset from a package (nenu: 
> Data/Data in packages), I can type in the name of a dataset, 
> but the GUI will not let me choose the name of a package from 
> which to select a dataset. Nothing happens when I pick a 
> package name.  (With previous R/Rcmdr, I did not have this trouble).
> 

I had in fact fixed this in the development version of the package (Rcmdr
0.9-12). I've now posted that to my web site, at
<http://socserv.socsci.mcmaster.ca/jfox/Misc/Rcmdr/index.html>. I'd like to
do some more testing before sending this version to CRAN.

> 2. When using Rcmdr, some of the output from commands shows 
> in the Rcmdr bottom window, but some is sent to the xterm in 
> which R was started. For example, with a scatter3d() command, 
> if I add the option model.summary=T, the model summary shows 
> in the xterm.  But some other models show results in the 
> bottom pane of Rcmdr.  And, it seems to me, there are some 
> commands in which I've noticed some of the output going to 
> the Rcmdr bottom buffer and  some to the xterm.
> 

I did in fact try to deal with this problem. The solution that I employed
didn't work for scatter3d() with model.summary=TRUE, since the printed
summaries were produced as side effects by scatter3d(). That's a bad
practice, and I've fixed it; now scatter3d() returns a list of summaries,
and the Rcmdr can cope with that. (As you know, it's not my intention that
model.summary=TRUE be set from the Rcmdr GUI, but it's better to have it
work if you enter this from the script editor.)

You imply, however, that there are other cases of this problem. It would
help to know what they are -- assuming that they're not fixed in the
development version now on my web site.

> 3. When I want to exit Rcmdr and choose the pull down "exit 
> from Commander and R", I get a Segmentation Fault that closes 
> Rcmdr and R.
> 

I haven't been able to duplicate this problem, to this point testing only
under Windows XP. Does this occur only when an rgl graphics device is open?
(The Rcmdr should close the rgl device before it exits.)

> 4. 3d plots do not display on the screen until I click in the 
> rgl window that displays the graph.  This might be a video 
> card/AGP driver problem, I suppose.  These systems have the 
> NVidia FX5200 cards and the NVidia proprietary X driver.  If 
> other users don't see the same on other kinds of systems, I 
> guess that will tell me where the problem lies.
> 

As I said, I believe that I've seen this problem before under Quantian. It
would help to know whether it's peculiar to the Rcmdr, scatter3d(), or rgl.

> 5. At random times, I get an error tcl/tk window popping up 
> with a message about MASS and categorical variables.  It says
> 
> Error in data(package=parse(text=packageName));
> 'MASS' must be character string or NULL
> 
> It says that over and over, sometimes it has other package 
> names, as in:
> 
> Error in data(package=parse(text=packageName));
> 'relimp' must be character string or NULL
> 
> These seem to be harmless?

This should go away now that problem 1 is fixed.

Regards,
 John

> -- 
> Paul E. Johnson                       email: pauljohn at ku.edu
> Dept. of Political Science            http://lark.cc.ku.edu/~pauljohn
> 1541 Lilac Lane, Rm 504
> University of Kansas                  Office: (785) 864-9086
> Lawrence, Kansas 66044-3177           FAX: (785) 864-5700
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html



From kjetil at acelerate.com  Wed Oct 13 02:16:25 2004
From: kjetil at acelerate.com (Kjetil Brinchmann Halvorsen)
Date: Tue, 12 Oct 2004 20:16:25 -0400
Subject: [R] covariate selection?
In-Reply-To: <416C47A2.6000403@ufl.edu>
References: <416C47A2.6000403@ufl.edu>
Message-ID: <416C73D9.8050204@acelerate.com>

Ian Fiske wrote:

> Hello,
>
> I am hoping someone can help me with the following multivariate 
> issue:  I have a model consisting of about 50 covariates.  I would 
> like to reduce this to about 5 covariate for the reduced model by 
> combining cofactors that are strongly correlated.  Is there a package 
> or function that would help me with this in R?  I appreciate any 
> suggestions.
>
> Thanks,
> Ian
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
>
>
have a look at package leaps, and also consider ridge regression.

-- 

Kjetil Halvorsen.

Peace is the most effective weapon of mass construction.
               --  Mahdi Elmandjra



From maustin at amgen.com  Wed Oct 13 03:18:32 2004
From: maustin at amgen.com (Austin, Matt)
Date: Tue, 12 Oct 2004 18:18:32 -0700
Subject: [R] covariate selection?
Message-ID: <E7D5AB4811D20B489622AABA9C53859101F1143D@teal-exch.amgen.com>

I like Kjetil's suggestion of a shrinkage estimator.  Perhaps this would be
a good time to experiment with Trevor Hastie's 'lars' package.

If you have a lot of correlated inputs I might suggest using Andy Liaw's
randomforest package.  I have found this technique to be very valuable in
this setting.  The partial dependency plots are a good way to explore the
functional relationships of the variables.

--Matt

-----Original Message-----
From: r-help-bounces at stat.math.ethz.ch
[mailto:r-help-bounces at stat.math.ethz.ch]On Behalf Of Kjetil Brinchmann
Halvorsen
Sent: Tuesday, October 12, 2004 17:16 PM
To: Ian Fiske
Cc: r-help at stat.math.ethz.ch
Subject: Re: [R] covariate selection?


Ian Fiske wrote:

> Hello,
>
> I am hoping someone can help me with the following multivariate 
> issue:  I have a model consisting of about 50 covariates.  I would 
> like to reduce this to about 5 covariate for the reduced model by 
> combining cofactors that are strongly correlated.  Is there a package 
> or function that would help me with this in R?  I appreciate any 
> suggestions.
>
> Thanks,
> Ian
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
>
>
have a look at package leaps, and also consider ridge regression.

-- 

Kjetil Halvorsen.

Peace is the most effective weapon of mass construction.
               --  Mahdi Elmandjra

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide!
http://www.R-project.org/posting-guide.html



From andy_liaw at merck.com  Wed Oct 13 02:58:14 2004
From: andy_liaw at merck.com (Liaw, Andy)
Date: Tue, 12 Oct 2004 20:58:14 -0400
Subject: [R] tclk, tcltk
Message-ID: <3A822319EB35174CA3714066D590DCD504AF8527@usrymx25.merck.com>

>From NEWS of R-2.0.0:

    o   library() now checks the version dependence (if any) of
        required packages mentioned in the Depends: field of the
        DESCRIPTION file.

This is not done in R-1.9.1 and prior, I believe (or it wouldn't be in
NEWS).

Andy

> From: Jean Eid
> 
> Thanks to Brian. I was looking everywhere except the typo. I guess the
> previous build (1.9.1) ignored the dependece error.
> 
> Thanks again,
> 
> jean
> 
> On Tue, 12 Oct 2004, Prof Brian Ripley wrote:
> 
> > What is package `tclk'?  I think you have a typo somewhere 
> in your code.
> > At a guess you have
> >
> > Depends: tclk
> >
> > in a DESCRIPTION file for one of your packages.
> >
> >
> > On Tue, 12 Oct 2004, Jean Eid wrote:
> >
> > > I have been having problems with these two 'libraries' 
> since I installed
> > > 2.0.0.
> > > I have built a package with couple of functions so that I 
> can load it at
> > > startup every time R is booted. The problem is that I 
> have the following
> > > error every time I call the library
> > >
> > > Loading required package: tclk
> > > Error: package 'tclk' could not be loaded
> > > In addition: Warning message:
> > > There is no package called 'tclk' in: library(pkg, 
> character.only = TRUE, logical = TRUE, lib.loc = lib.loc)
> > >
> > >
> > > However I can load tcltk library now (thanks to Dirk for 
> helping me with
> > > this). I can load the functions I have in the library and 
> everything works
> > > fine.
> > >
> > > I cannot find any information on this except the 
> following thread which
> > > was not resolved.
> > > http://tolstoy.newcastle.edu.au/R/help/04/01/0410.html
> >
> > It was resolved, and in any case it was about installing 
> 64-bit Tcl/Tk on
> > Solaris, something that was (and is) covered in the R 
> Installation manual.
> >
> > > Any help or ideas are greatly appreciated.
> > >
> > > P.S. what is the difference between tclk and tcltk? and 
> why calling the
> > > installed library is requiring tclk? I just made up a 
> dummy package with a
> > > function
> > >
> > > fun<-function(x) x^2
> > >
> > > just to test and the same thing happens (Loading required 
> package: tclk
> > > ...).
> > >
> > > This is on a Linux debian unstable with the kernel  
> 2.4.20-bf2.4-xfs
> >
> > --
> > Brian D. Ripley,                  ripley at stats.ox.ac.uk
> > Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
> > University of Oxford,             Tel:  +44 1865 272861 (self)
> > 1 South Parks Road,                     +44 1865 272866 (PA)
> > Oxford OX1 3TG, UK                Fax:  +44 1865 272595
> >
> >
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
> 
>



From mathtester at yahoo.com  Wed Oct 13 03:25:48 2004
From: mathtester at yahoo.com (Heng Sun)
Date: Tue, 12 Oct 2004 18:25:48 -0700 (PDT)
Subject: [R] KalmanLike: missing exogenous factor?
In-Reply-To: <Pine.LNX.4.44.0410120744010.5609-100000@gannet.stats>
Message-ID: <20041013012548.11734.qmail@web13301.mail.yahoo.com>

Prof Ripley,

Thanks for explanation. I now understand where
KalmanLike fits. 

I should not use "exogenous factor". It should be
called "exogenous variable" or "inputs" or "known
effects". My study on how trading sizes impact on
stock prices has trading sizes as this exogenous
variable. As you said, this should belong to some
package. I did internet searches and found something
similar but not covering my case.

The restrictive access to web makes subscription from
my work place not convenient. Sorry.

Heng Sun
Senior Quantitative Analyst
Depository Trust and Clearing Corporation
New York, USA 10041
Tel: 212-755-5754


--- Prof Brian Ripley <ripley at stats.ox.ac.uk> wrote:

> On Mon, 11 Oct 2004, Heng Sun wrote:
> 
> > From the help document on KalmanLike, KalmanRun,
> etc.,
> > I see the linear Gaussian state space model is 
> > 
> > a <- T a + R e
> > y = Z' a + eta
> > 
> > following the book of Durbin and Koopman.
> > 
> > In practice, it is useful to run Kalman
> > filtering/smoothing/forecasting with exogenous
> factor:
> > 
> > a <- T a + L b + R e
> > y = Z' a + M b + eta
> > 
> > where b is some known vector (a function of time).
> > 
> > Some other software like S-plus and Mathematica
> > include the above exogenous factor. SsfPack by
> > Koopman, etal. also has the factor built in the
> model
> > to accommodate practical uses.
> > 
> > So what is the rationale for R to leave off the
> > exogenous factor? Is there a feasible way to
> convert
> > the general model to the simple model in R?
> 
> What is the rationale for your raising this?
> 
> KalmanLike, KalmanRun, etc were written for R 1.5.0
> as part of the ts 
> package (see my article in R-news), and the ts
> applications (see the See 
> Also section) do not need a so-called `exogenous
> factor' (which is not a 
> `factor').   R does not pretend to have facilities
> for whatever subject 
> area you mean (but do not say) by `in practice'. 
> That's what addon
> packages are for (and some do touch on this area).
> 
> We have no idea who `mathtester at yahoo.com' is: it is
> courteous to use a 
> signature and give your affiliation.
> 
> -- 
> Brian D. Ripley,                 
> ripley at stats.ox.ac.uk
> Professor of Applied Statistics, 
> http://www.stats.ox.ac.uk/~ripley/
> University of Oxford,             Tel:  +44 1865
> 272861 (self)
> 1 South Parks Road,                     +44 1865
> 272866 (PA)
> Oxford OX1 3TG, UK                Fax:  +44 1865
> 272595
> 
>



From matthew_wiener at merck.com  Wed Oct 13 04:18:33 2004
From: matthew_wiener at merck.com (Wiener, Matthew)
Date: Tue, 12 Oct 2004 22:18:33 -0400
Subject: [R] graph question
Message-ID: <45AAE6FD142DCB43A38C00A11FF5DF3E022261E1@uswsmx03.merck.com>

Dean --

I believe just setting log = "y" in your plot command should do this.

For example:

> plot(runif(100, 1, 100), runif(100, 1, 100), log = "xy")
gives me tick marks at 2, 5, 10, 20, 50, and 100.  (YMMV because of the
random numbers.)

Hope this helps,

Matt Wiener

-----Original Message-----
From: r-help-bounces at stat.math.ethz.ch
[mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Dean Sonneborn
Sent: Tuesday, October 12, 2004 2:02 PM
To: R-help at stat.math.ethz.ch
Subject: [R] graph question


I would like to produce a graph which plots a log scale variable on the 
y-axis but have the tick marks on the y-axis  be the non log transformed 
values that are round like .5, 1, 2, 3, 4 etc. Has anyone done something 
like this in the past? How did you implement it in the code?

Thanks,
Dean

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide!
http://www.R-project.org/posting-guide.html



From chrisryan at wvdnr.gov  Wed Oct 13 05:08:56 2004
From: chrisryan at wvdnr.gov (Chris Ryan)
Date: Tue, 12 Oct 2004 23:08:56 -0400
Subject: [R] diagonal matrix construction
Message-ID: <9264E8787B90F5489A957EBA4B2F81CF01941E@DNRMAIL01.wvdnr.net>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20041012/aba6f1e9/attachment.pl

From dmb at mrc-dunn.cam.ac.uk  Wed Oct 13 05:21:03 2004
From: dmb at mrc-dunn.cam.ac.uk (Dan Bolser)
Date: Wed, 13 Oct 2004 04:21:03 +0100 (BST)
Subject: [R] data(eurodist) and PCA ??
Message-ID: <Pine.LNX.4.21.0410130416540.6300-100000@mail.mrc-dunn.cam.ac.uk>



If I perform PCA on the 'eurodist' data, should I get an accurate
geographic layout of the cities with biplot?

(barring inversions, i.e. their is no way to define north.. but you get
the idea...)

I have a complex distance matrix, and I am thinking about how to cluster
it and how to visualize the quality of the resulting clusters. 

If I could 'see' the clusters in space I could understand how / what the
cluster algorithms were doing. 

Can I use PCA over the distance matrix to to do that?

Sorry for the dumb questions.

Dan.



From ggrothendieck at myway.com  Wed Oct 13 05:22:31 2004
From: ggrothendieck at myway.com (Gabor Grothendieck)
Date: Wed, 13 Oct 2004 03:22:31 +0000 (UTC)
Subject: [R] diagonal matrix construction
References: <9264E8787B90F5489A957EBA4B2F81CF01941E@DNRMAIL01.wvdnr.net>
Message-ID: <loom.20041013T051405-775@post.gmane.org>

Chris Ryan <chrisryan <at> wvdnr.gov> writes:

: 
: Hi, 
: 
: I have worked long and hard and looked in the manuals and am having a hard 
time constructing a diagonal
: matrix.  I can get the diagonals out of a matrix but can't construct the 
matrix with just the diagonals.  I
: have been on the web site and manuals and I think that it says to use:
: 
: dsj <- diag (three = 1, nrow, ncol = 7)                           three is 
the name of my matrix and dsj is where I'm trying to put it but it
: comes back:  
: 
: unused argument(s) (three ...)
: every time that I try it.  
: 
: Am I doing something wrong in trying to make this matrix?
: 
: Thank you very much for your time, Chris Ryan
:

Looking at ?diag we see that diag takes 3 arguments: 'x', 'nrow' and 'ncol'.
'three' is not one of them so it is correctly telling you that you specified
an unused argument.

diag(nrow = 7) # will create a 7x7 identity matrix.
diag(1, 7)  # same using positional rather than named arguments
diag(, 7)  # same since x defaults to 1

Look at the examples at the bottom of the ?diag page and try them out 
by running

example(diag)

at the R prompt or by just typing them in to get more insight.  Also
you might consider rereading the Introduction to R manual where it 
is covered on page 22.



From spencer.graves at pdf.com  Wed Oct 13 05:30:02 2004
From: spencer.graves at pdf.com (Spencer Graves)
Date: Tue, 12 Oct 2004 20:30:02 -0700
Subject: [R] diagonal matrix construction
In-Reply-To: <9264E8787B90F5489A957EBA4B2F81CF01941E@DNRMAIL01.wvdnr.net>
References: <9264E8787B90F5489A957EBA4B2F81CF01941E@DNRMAIL01.wvdnr.net>
Message-ID: <416CA13A.4030901@pdf.com>

      Did you try working the examples in the "diag" documentation, one 
of which is as follows: 

 >      diag(10,3,4) # guess what?
     [,1] [,2] [,3] [,4]
[1,]   10    0    0    0
[2,]    0   10    0    0
[3,]    0    0   10    0

(R 1.9.1 for Windows). 

      The R syntax expects "three" in that context to be an argument of 
the function "diag".  If you want a 7x7 identity matrix, try diag(7).  
If you want something else, please study the examples in the 
documentation for "diag".  If that is not adequate, let us know. 

      hope this helps.  spencer graves

Chris Ryan wrote:

>Hi, 
> 
>I have worked long and hard and looked in the manuals and am having a hard time constructing a diagonal matrix.  I can get the diagonals out of a matrix but can't construct the matrix with just the diagonals.  I have been on the web site and manuals and I think that it says to use:
> 
>dsj <- diag (three = 1, nrow, ncol = 7)                           three is the name of my matrix and dsj is where I'm trying to put it but it comes back:  
> 
>unused argument(s) (three ...)
>every time that I try it.  
> 
>Am I doing something wrong in trying to make this matrix?
> 
>Thank you very much for your time, Chris Ryan
> 
> 
>Christopher W. Ryan
>Black Bear Project Leader
>West Virginia Division of Natural Resources
>Capitol Complex, Bldg 3, Rm 824
>Charleston, WV 25305
>
>	[[alternative HTML version deleted]]
>
>______________________________________________
>R-help at stat.math.ethz.ch mailing list
>https://stat.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>  
>

-- 
Spencer Graves, PhD, Senior Development Engineer
O:  (408)938-4420;  mobile:  (408)655-4567



From gregory.r.warnes at pfizer.com  Wed Oct 13 07:01:39 2004
From: gregory.r.warnes at pfizer.com (Warnes, Gregory R)
Date: Wed, 13 Oct 2004 01:01:39 -0400
Subject: [R] displaying sample size in boxplots
Message-ID: <915D2D65A9986440A277AC5C98AA466F0A1A39@groamrexm02.amer.pfizer.com>



> -----Original Message-----
> From: r-help-bounces at stat.math.ethz.ch
> [mailto:r-help-bounces at stat.math.ethz.ch]On Behalf Of Patrick 
> Drechsler
> 
> Gregory R. Warnes wrote on 01 Oct 2004 14:52:05 MET:
> 
> [...]
> 
> > Also note that boxplot.n in the gplots library (part of the
> > gregmisc bundle) automatically adds the number of observations.
> 
> Thanks Greg! Nice to see that you've written some code just for
> this purpose. I will of course also take a closer look at the
> other functions that are bundled in `gregmisc'.
> 

:^)

> Since I'm still new to R: can somebody give me a pointer to the
> docs where to find instructions on a package (not a function)? I
> can find the man pages to specific functions with ?<functionname>
> (something similar to `texdoc <packagename>' in tetex)?

library(help=<packagename>)

you can also get the PDF package documentation available on cran, e.g.
http://cran.r-project.org/doc/packages/gregmisc.pdf

-Greg


> 
> TIA,
> 
> Patrick
> -- 
> Look Ma, this man can twist his fingers as if they were made of
> rubber, isn't that amazing? -- Not really, he's been using Emacs
> for years...!
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
> 


LEGAL NOTICE\ Unless expressly stated otherwise, this messag...{{dropped}}



From mn216 at columbia.edu  Wed Oct 13 07:17:48 2004
From: mn216 at columbia.edu (Murad Nayal)
Date: Wed, 13 Oct 2004 01:17:48 -0400
Subject: [R] debugging non-visible functions
Message-ID: <416CBA7C.CD459F37@columbia.edu>



Hi,

I would like to step-through a non-visible function. but apparently I
don't know enough about namespaces to get that to work:

> methods(predict) 
 ... deleted lines ... 
[27] predict.rpart*             predict.smooth.spline*    
[31] predict.survreg.penal*    

    Non-visible functions are asterisked


> debug(predict.rpart)
Error: Object "predict.rpart" not found


> getAnywhere("predict.rpart")
A single object matching 'predict.rpart' was found
It was found in the following places
  registered S3 method for predict from namespace rpart
  namespace:rpart
with value

function (object, newdata = list(), type = c("vector", "prob", 
    "class", "matrix"), ...) 
{
... deleted code ...
}
<environment: namespace:rpart>


> debug(predict.rpart,pos="package:rpart")
Error: Object "predict.rpart" not found


how can I 'debug' non-visible functions, like predict.rpart?

many thanks
Murad Nayal



From ggrothendieck at myway.com  Wed Oct 13 07:52:57 2004
From: ggrothendieck at myway.com (Gabor Grothendieck)
Date: Wed, 13 Oct 2004 05:52:57 +0000 (UTC)
Subject: [R] debugging non-visible functions
References: <416CBA7C.CD459F37@columbia.edu>
Message-ID: <loom.20041013T074654-414@post.gmane.org>

Murad Nayal <mn216 <at> columbia.edu> writes:

: 
: Hi,
: 
: I would like to step-through a non-visible function. but apparently I
: don't know enough about namespaces to get that to work:
: 
: > methods(predict) 
:  ... deleted lines ... 
: [27] predict.rpart*             predict.smooth.spline*    
: [31] predict.survreg.penal*    
: 
:     Non-visible functions are asterisked
: 
: 
: > debug(predict.rpart)
: Error: Object "predict.rpart" not found
: 
: 
: > getAnywhere("predict.rpart")
: A single object matching 'predict.rpart' was found
: It was found in the following places
:   registered S3 method for predict from namespace rpart
:   namespace:rpart
: with value
: 
: function (object, newdata = list(), type = c("vector", "prob", 
:     "class", "matrix"), ...) 
: {
: ... deleted code ...
: }
: <environment: namespace:rpart>
: 
: 
: > debug(predict.rpart,pos="package:rpart")
: Error: Object "predict.rpart" not found
: 
: 
: how can I 'debug' non-visible functions, like predict.rpart?
: 

>From the ?debug page we see that that only one argument can
be given to debug, the function name, so there is no two
argument form as per your example above.

What you can do is that you can debug it like this:

   require(rpart)
   debug(rpart:::predict.rpart)

Also note that even if its not visible it may still have a help
page, which in this case it does:

   ?predict.rpart



From ggrothendieck at myway.com  Wed Oct 13 08:16:07 2004
From: ggrothendieck at myway.com (Gabor Grothendieck)
Date: Wed, 13 Oct 2004 06:16:07 +0000 (UTC)
Subject: [R] displaying sample size in boxplots
References: <915D2D65A9986440A277AC5C98AA466F0A1A39@groamrexm02.amer.pfizer.com>
Message-ID: <loom.20041013T075351-152@post.gmane.org>

Warnes, Gregory R <gregory.r.warnes <at> pfizer.com> writes:

: 
: > -----Original Message-----
: > From: r-help-bounces <at> stat.math.ethz.ch
: > [mailto:r-help-bounces <at> stat.math.ethz.ch]On Behalf Of Patrick 
: > Drechsler
: > 
: > Gregory R. Warnes wrote on 01 Oct 2004 14:52:05 MET:
: > 
: > [...]
: > 
: > > Also note that boxplot.n in the gplots library (part of the
: > > gregmisc bundle) automatically adds the number of observations.
: > 
: > Thanks Greg! Nice to see that you've written some code just for
: > this purpose. I will of course also take a closer look at the
: > other functions that are bundled in `gregmisc'.
: > 
: 
: :^)
: 
: > Since I'm still new to R: can somebody give me a pointer to the
: > docs where to find instructions on a package (not a function)? I
: > can find the man pages to specific functions with ?<functionname>
: > (something similar to `texdoc <packagename>' in tetex)?
: 
: library(help=<packagename>)
: 
: you can also get the PDF package documentation available on cran, e.g.
: http://cran.r-project.org/doc/packages/gregmisc.pdf
: 
: -Greg

Also some packages, though not many, have vignettes which are documents
that describe the package as a whole as opposed to single functions.
Just issue the command:

   vignette()

to get a list.  Assuming you have the package strucchange 
installed, the above command will have let you know that
strucchange has a vignette called strucchange-intro which
you can view like this:

   vignette("strucchange-intro")

Check out the R News article on vignettes in R News 3/2 for 
more info.

In addition, its always a good idea to check if there is an 
R News article on the package.  In the case of strucchange, 
a google search for 

   rnews strucchange 

would have found a reference telling you what issue its in.
R News is found via the Newsletter link on the www.r-project.org
home page.



From ligges at statistik.uni-dortmund.de  Wed Oct 13 08:20:05 2004
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Wed, 13 Oct 2004 08:20:05 +0200
Subject: [R] Why I can't retrieve GO identifier correctly?
In-Reply-To: <1097615362.416c4802951a7@webmail.uoguelph.ca>
References: <1097615362.416c4802951a7@webmail.uoguelph.ca>
Message-ID: <416CC915.2010107@statistik.uni-dortmund.de>

szhan at uoguelph.ca wrote:
> Hello, R experts,
> I tried to retrieve all biological process GO terms at level 3 starting
> "biological process" as level 1 using the code as bellows:
> 
> 1 library(GO)
> 2 library(GOstats)
> 3 level2<-getGOChildren("GO:0008150")$"GO:0008150"$Children
> 4 for ( i in 1:length(level2)) {
> 5    level3 <- getGOChildren(level2[i])$level2[i]$Children

If you want to index by an object that contains a character (or string), 
you have to use "[[]]" rather than "$".
The question is completely independent of GO, so don't confuse potential 
responders who expect you to send messages re. Bioconductor packages to 
the corresponding list.

Uwe Ligges


> 6    for ( j in 1:length(level3)){
> 7       level3term <- getGOTerm(as.character(level3[j]))$BP$level3[j]
> 8       level3term
> 9    }
> 10 }
> What is the difference between line 3 and line5? In the line 3 I retrieved the
> GO identifiers at level 2 successfullly but in the line 5 I got nothing. How to
> correct the line 5 to retrieve the GO terms at level 3 correctly?
> Thank you in advance!
> Josh
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html



From allan at stats.uct.ac.za  Wed Oct 13 08:33:02 2004
From: allan at stats.uct.ac.za (allan clark)
Date: Wed, 13 Oct 2004 08:33:02 +0200
Subject: [R] R: r course
Message-ID: <416CCC1E.454D06AF@stats.uct.ac.za>

hi all

i need some advice. i am a university lecturer and will be teaching a R
programming course next year. the course will be taught to second year
statistics students. the aim is to introduce them to programming. the
emphasis will be on solving real life consulting projects by using R. i
must still develop the course but if anyone has any suggestions on
possible content and interesting data sets to explore, please email. if
any lecturers are offering similar courses could you please send me a
course outline- some notes if at all possible. references will also be a
big help. i will appreciate any comments.

thanking you in advance
*****
allan



From ripley at stats.ox.ac.uk  Wed Oct 13 08:51:40 2004
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Wed, 13 Oct 2004 07:51:40 +0100 (BST)
Subject: [R] data(eurodist) and PCA ??
In-Reply-To: <Pine.LNX.4.21.0410130416540.6300-100000@mail.mrc-dunn.cam.ac.uk>
Message-ID: <Pine.LNX.4.44.0410130740140.3778-100000@gannet.stats>

On Wed, 13 Oct 2004, Dan Bolser wrote:

> If I perform PCA on the 'eurodist' data, should I get an accurate
> geographic layout of the cities with biplot?

No, but a good approximation.

> (barring inversions, i.e. their is no way to define north.. but you get
> the idea...)
> 
> I have a complex distance matrix, and I am thinking about how to cluster
> it and how to visualize the quality of the resulting clusters. 

Using PCA and plotting the first two components is classical
multi-dimensional scaling, as implemented by cmdscale().  Look up MDS
somewhere (e.g. in MASS).  It is exact if the distances are Euclidean in
2D.  However, eurodist gives road distances on the surface of sphere.

Classic examples for the illustration of MDS are departements of France 
based on proximity data and cities in the UK based on road distances.

There is a minor point as to what you mean `with biplot', covered in 
MASS4: it depends on the exact definition of biplot (and biplot.princomp 
has a parameter -- this is not by default done in S-PLUS in a way that 
makes your statement correct).

> If I could 'see' the clusters in space I could understand how / what the
> cluster algorithms were doing. 

A standard topic for MDS: see e.g. two of my books (MASS and my Pattern 
Recognition and Neural Networks) for extensive examples.

> Can I use PCA over the distance matrix to to do that?
> 
> Sorry for the dumb questions.

Please do some homework: suggestions above and in the posting guide.

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From ripley at stats.ox.ac.uk  Wed Oct 13 09:00:38 2004
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Wed, 13 Oct 2004 08:00:38 +0100 (BST)
Subject: [R] debugging non-visible functions
In-Reply-To: <416CBA7C.CD459F37@columbia.edu>
Message-ID: <Pine.LNX.4.44.0410130756140.3778-100000@gannet.stats>

On Wed, 13 Oct 2004, Murad Nayal wrote:

> I would like to step-through a non-visible function. but apparently I
> don't know enough about namespaces to get that to work:
> 
> > methods(predict) 
>  ... deleted lines ... 
> [27] predict.rpart*             predict.smooth.spline*    
> [31] predict.survreg.penal*    
> 
>     Non-visible functions are asterisked
> 
> 
> > debug(predict.rpart)
> Error: Object "predict.rpart" not found
> 
> 
> > getAnywhere("predict.rpart")
> A single object matching 'predict.rpart' was found
> It was found in the following places
>   registered S3 method for predict from namespace rpart
>   namespace:rpart
> with value
> 
> function (object, newdata = list(), type = c("vector", "prob", 
>     "class", "matrix"), ...) 
> {
> ... deleted code ...
> }
> <environment: namespace:rpart>
> 
> 
> > debug(predict.rpart,pos="package:rpart")
> Error: Object "predict.rpart" not found
> 
> 
> how can I 'debug' non-visible functions, like predict.rpart?

The issue here is a non-visible S3 method.  In that case, just make a 
local copy:

predict.rpart <- rpart:::predict.rpart
debug(predict.rpart)

In general (not an S3 method), you cannot do this since the local copy may 
not be called, and the only way I know is to use fixInNamespace and insert 
browser() at the top.  Even that may not always work.

Luke Tierney recommends removing the NAMESPACE file during development of 
a package if you need frequent access to debug/change its functions.

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From ripley at stats.ox.ac.uk  Wed Oct 13 09:12:15 2004
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Wed, 13 Oct 2004 08:12:15 +0100 (BST)
Subject: [R] R: r course
In-Reply-To: <416CCC1E.454D06AF@stats.uct.ac.za>
Message-ID: <Pine.LNX.4.44.0410130801500.3778-100000@gannet.stats>

`second year statistics students' is rather imprecise, but I will guess
these are undergraduate statistics majors.  However, the book by Nolan &
Speed listed in the R FAQ would seem to be at about that level and is
rather close to your description.

@Book{Nolan.Speed.00,
  author =       {Deborah Nolan and Terry Speed},
  title =        {Stat Labs. Mathematical Statistics Through Applications},
  publisher =    {Springer},
  year =         2000,
  address =      {New York},
  ISBN =         "0-387-98974-9",
}

I've also used examples from 

@Book{Ramsey.Schafer.02,
  author =       {Fred L. Ramsey and Daniel W. Schafer},
  title =        {The Statistical Sleuth. A Course in Methods of Data Analysis},
  publisher =    {Duxbury Press},
  year =         2002,
  edition =      "Second",
  address =      {Belmont, CA},
  ISBN         = "0-534-38670-9",
}

and 

@Book{Fox.02,
  author =       {John Fox},
  title =        {A {R} and {S-PLUS} Companion to Applied Regression},
  publisher =    {Sage Publications},
  year =         2002,
  address =      {Thousand Oaks, CA},
  ISBN =         "0-7619-2280-6",
}

in courses to graduates based on case studies.

It depends what you mean by `R programming', too.  Almost all the 
statistics our third-year students (let alone second-year) know can be 
done by direct calls to existing R code.  (The third years use R as from 
last year, and this allows topics such as robust estimation and smoothing 
to be covered.)


On Wed, 13 Oct 2004, allan clark wrote:

> hi all
> 
> i need some advice. i am a university lecturer and will be teaching a R
> programming course next year. the course will be taught to second year
> statistics students. the aim is to introduce them to programming. the
> emphasis will be on solving real life consulting projects by using R. i
> must still develop the course but if anyone has any suggestions on
> possible content and interesting data sets to explore, please email. if
> any lecturers are offering similar courses could you please send me a
> course outline- some notes if at all possible. references will also be a
> big help. i will appreciate any comments.

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From patrick.drechsler at gmx.net  Wed Oct 13 09:53:32 2004
From: patrick.drechsler at gmx.net (Patrick Drechsler)
Date: Wed, 13 Oct 2004 09:53:32 +0200
Subject: [R] displaying sample size in boxplots
References: <915D2D65A9986440A277AC5C98AA466F0A1A39@groamrexm02.amer.pfizer.com>
	<loom.20041013T075351-152@post.gmane.org>
Message-ID: <m3r7o3m4ub.fsf@pdrechsler.fqdn.th-h.de>


Gabor Grothendieck wrote on 13 Oct 2004 07:16:07 MET:

> Warnes, Gregory R <gregory.r.warnes <at> pfizer.com> writes:

> : > Since I'm still new to R: can somebody give me a pointer to the
> : > docs where to find instructions on a package (not a function)? I
> : > can find the man pages to specific functions with ?<functionname>
> : > (something similar to `texdoc <packagename>' in tetex)?
> : 
> : library(help=<packagename>)
> : 
> : you can also get the PDF package documentation available on cran, e.g.
> : http://cran.r-project.org/doc/packages/gregmisc.pdf

[snip]

> Also some packages, though not many, have vignettes which are
> documents that describe the package as a whole as opposed to
> single functions.  Just issue the command:
>
>    vignette()
[snip]

Greg and Gabor: Thank you for the pointers!

Cheers

Patrick
-- 
"I really should talk to him. He's had a near-death experience!"
"We all have. It's called living." Terry Pratchett, Hogfather



From tom_woody at swissinfo.org  Wed Oct 13 09:59:46 2004
From: tom_woody at swissinfo.org (=?ISO-8859-1?Q?Thomas_Sch=F6nhoff?=)
Date: Wed, 13 Oct 2004 09:59:46 +0200
Subject: [R] Cleaning up R-2.0.0 installation on GNU/Linux ?
Message-ID: <416CE072.2060209@swissinfo.org>

Hello,

first, big thank you to the developers of R, the release of recent 
version is very nice.!

Well, it seems that I've messed up my current installation by using two 
update mechanism at the same time on my Debian Sid box.

That is :

1)Debian Way per apt-get freshing up my R installation

2) irregularily "update packages()" from within R.


I guess this obviousely resulted in messing up my installation looking 
like this:

-------------------------------------------------------------
 > library()
(..)
Packages in library '/usr/local/lib/R/site-library':

accuracy                ** No title available (pre-2.0.0 install?) **
ade4                    ** No title available (pre-2.0.0 install?) **
debug                   ** No title available (pre-2.0.0 install?) **
distr                   ** No title available (pre-2.0.0 install?) **
eha                     Event History Analysis.
ellipse                 ** No title available (pre-2.0.0 install?) **
faraway                 ** No title available (pre-2.0.0 install?) **
fortunes                ** No title available (pre-2.0.0 install?) **
leaps                   ** No title available (pre-2.0.0 install?) **
mvbutils                ** No title available (pre-2.0.0 install?) **
mvtnorm                 ** No title available (pre-2.0.0 install?) **
nlme                    Linear and nonlinear mixed effects models
R2WinBUGS               ** No title available (pre-2.0.0 install?) **
RColorBrewer            ** No title available (pre-2.0.0 install?) **
sca                     ** No title available (pre-2.0.0 install?) **
scatterplot3d           ** No title available (pre-2.0.0 install?) **
sn                      ** No title available (pre-2.0.0 install?) **
sna                     ** No title available (pre-2.0.0 install?) **
tree                    ** No title available (pre-2.0.0 install?) **
xgobi                   ** No title available (pre-2.0.0 install?) **
xtable                  ** No title available (pre-2.0.0 install?) **
--------------------------------------------------------------------------

It seems that my insane update method has entirely messed up the 
attached package section! All other packages still seem to be or run 
fine! Loading a package from above package section

  > library(mvtnorm)

gives me:
--------------------------------------------------------------------------
Error in library(mvtnorm) : 'mvtnorm' is not a valid package -- 
installed < 2.0.0?
---------------------------------------------------------------------------

Since this is the first time I encounter a problem like this in R I 
wonder where to go from here:

1) Wipe out the existent installation and re-install GNU R ?

or

2) Is there possibly any other method to cope with this situation from 
within GNU R-2.0.0?


regards

Thomas




platform i386-pc-linux-gnu
arch     i386
os       linux-gnu
system   i386, linux-gnu
status
major    2
minor    0.0
year     2004
month    10
day      04
language R



From holck at hawaii.edu  Wed Oct 13 10:12:47 2004
From: holck at hawaii.edu (Peter Holck)
Date: Tue, 12 Oct 2004 22:12:47 -1000
Subject: [R] "Centered" dummy variables; non zero/one coding
Message-ID: <000001c4b0fc$6e0ed710$0202a8c0@psh>

I'm uncertain if this is perhaps a stupid question:

I want to create "centered" dummy variables to use in a call to glm(), and
wondering if there's some slick method in R to do so.  That is, rather than
have a factor, which results in a glm() fit returning coefficients
specifying either absence or presence of the factor, I'd like to fit a glm()
without intercept such that the estimated coefficients (standard errors)
represent the "average" value in my data set for that variable.  

An example: a data set has Race specified with 4 levels.  I can manually
specify 4 dummy variables for a no-intercept model with each variable rather
than having a value of zero or one, has a centered value based on its
frequency of occurrence in the data set.  Thus if 30% of the records in the
data set have Race of Hispanic, I can define a variable HISP that has a
value of either -.3 or .7, resulting in my coefficient estimate for HISP
representing the effect of an "average" person in the database (and a
corresponding valid standard error).   

One way to create these "centered dummy variables" from the original factor
is:
		"B"=scale(RACE=="B",scale=F),
		"W"=scale(RACE=="W",scale=F),
		"H"=scale(RACE=="H",scale=F),
		"OTHRACE"=scale(RACE=="OTHER",scale=F)

However I wonder if there is some method in R to avoid having to manually
define a large number of these dummy variables for a more complicated
dataset.

Thanks in advance,
Peter Holck



From ripley at stats.ox.ac.uk  Wed Oct 13 10:34:37 2004
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Wed, 13 Oct 2004 09:34:37 +0100 (BST)
Subject: [R] Cleaning up R-2.0.0 installation on GNU/Linux ?
In-Reply-To: <416CE072.2060209@swissinfo.org>
Message-ID: <Pine.LNX.4.44.0410130930170.4082-100000@gannet.stats>

On Wed, 13 Oct 2004, Thomas Sch??nhoff wrote:

> Hello,
> 
> first, big thank you to the developers of R, the release of recent 
> version is very nice.!
> 
> Well, it seems that I've messed up my current installation by using two 
> update mechanism at the same time on my Debian Sid box.
> 
> That is :
> 
> 1)Debian Way per apt-get freshing up my R installation
> 
> 2) irregularily "update packages()" from within R.
> 
> 
> I guess this obviousely resulted in messing up my installation looking 
> like this:
> 
> -------------------------------------------------------------
>  > library()
> (..)
> Packages in library '/usr/local/lib/R/site-library':
> 
> accuracy                ** No title available (pre-2.0.0 install?) **
> ade4                    ** No title available (pre-2.0.0 install?) **
> debug                   ** No title available (pre-2.0.0 install?) **
> distr                   ** No title available (pre-2.0.0 install?) **
> eha                     Event History Analysis.
> ellipse                 ** No title available (pre-2.0.0 install?) **
> faraway                 ** No title available (pre-2.0.0 install?) **
> fortunes                ** No title available (pre-2.0.0 install?) **
> leaps                   ** No title available (pre-2.0.0 install?) **
> mvbutils                ** No title available (pre-2.0.0 install?) **
> mvtnorm                 ** No title available (pre-2.0.0 install?) **
> nlme                    Linear and nonlinear mixed effects models
> R2WinBUGS               ** No title available (pre-2.0.0 install?) **
> RColorBrewer            ** No title available (pre-2.0.0 install?) **
> sca                     ** No title available (pre-2.0.0 install?) **
> scatterplot3d           ** No title available (pre-2.0.0 install?) **
> sn                      ** No title available (pre-2.0.0 install?) **
> sna                     ** No title available (pre-2.0.0 install?) **
> tree                    ** No title available (pre-2.0.0 install?) **
> xgobi                   ** No title available (pre-2.0.0 install?) **
> xtable                  ** No title available (pre-2.0.0 install?) **
> --------------------------------------------------------------------------
> 
> It seems that my insane update method has entirely messed up the 
> attached package section! All other packages still seem to be or run 
> fine! Loading a package from above package section
> 
>   > library(mvtnorm)
> 
> gives me:
> --------------------------------------------------------------------------
> Error in library(mvtnorm) : 'mvtnorm' is not a valid package -- 
> installed < 2.0.0?
> ---------------------------------------------------------------------------
> 
> Since this is the first time I encounter a problem like this in R I 
> wonder where to go from here:
> 
> 1) Wipe out the existent installation and re-install GNU R ?
> 
> or
> 
> 2) Is there possibly any other method to cope with this situation from 
> within GNU R-2.0.0?

remove.packages("nlme", "/usr/local/lib/R/site-library") # a duplicate
foo <- installed.packages(lib="/usr/local/lib/R/site-library")[, 1]
install.packages(foo, lib="/usr/local/lib/R/site-library")

will reinstall all the packages in that library.  I could work out a way
to only get those that were installed in an earlier version, but it would 
be cumbersome.


-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From tom_woody at swissinfo.org  Wed Oct 13 11:19:52 2004
From: tom_woody at swissinfo.org (=?ISO-8859-1?Q?Thomas_Sch=F6nhoff?=)
Date: Wed, 13 Oct 2004 11:19:52 +0200
Subject: [R] Cleaning up R-2.0.0 installation on GNU/Linux ?[SOLVED]
In-Reply-To: <Pine.LNX.4.44.0410130930170.4082-100000@gannet.stats>
References: <Pine.LNX.4.44.0410130930170.4082-100000@gannet.stats>
Message-ID: <416CF338.9000102@swissinfo.org>

Hello,


Prof Brian Ripley said the following on 13.10.2004 10:34:
> On Wed, 13 Oct 2004, Thomas Sch??nhoff wrote:

> remove.packages("nlme", "/usr/local/lib/R/site-library") # a duplicate
> foo <- installed.packages(lib="/usr/local/lib/R/site-library")[, 1]
> install.packages(foo, lib="/usr/local/lib/R/site-library")

Besides one minor error regarding to distr package:

--------------------------------------------------------------
Warning message:
Installation of package distr had non-zero exit status in: 
install.packages(foo, lib = "/usr/local/lib/R/site-library")
---------------------------------------------------------------



your recommendations worked out fine, thanks!



> will reinstall all the packages in that library.  I could work out a way
> to only get those that were installed in an earlier version, but it would 
> be cumbersome.

No need for doing this at the moment!


If understand correctly the previous situation is only arising when 
there are major version upgrades, i.e. from 1.9.1 too 2.0.0.
But when using "update packages() within a major release everything will 
be fine, isn't it?
I would be interested to hear what upgrade methods GNU/Linux (Debian) 
users prefer to avoid a situation like this!

Many thanks for help!

Thomas



From andreasbetz at earthlink.net  Wed Oct 13 20:44:07 2004
From: andreasbetz at earthlink.net (Andreas Betz)
Date: Wed, 13 Oct 2004 11:44:07 -0700
Subject: [R] importing data
Message-ID: <410-220041031318447590@earthlink.net>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20041013/96c9a380/attachment.pl

From falaise at web.de  Wed Oct 13 12:08:24 2004
From: falaise at web.de (Ute)
Date: Wed, 13 Oct 2004 12:08:24 +0200
Subject: [R] random forest -optimising mtry
Message-ID: <416CFE98.7090009@web.de>

Dear R-helpers,

I'm working on mass spectra in randomForest/R, and following the 
recommendations for the case of noisy variables, I don't want to use the 
default mtry (sqrt of nvariables), but I'm not sure up to which 
proportion mtry/nvariables it makes sense to increase mtry without 
"overtuning" RF.
Let me tell my example: I have 106 spectra belonging to 4 classes, the 
number of variables is 172. I'm interested in finding information about 
variables (importance, split points etc.) and proximities.
First I  ran a forest with mtry =30 and ntree=2500. The result was an 
oob-estimate of overall error rate of zero, perfect classification.  In 
order to explore my results, I calculated the average proximity between 
the classes. I got:
 > res
           op12          op13           op14           op23          
op24          op34
[1,] 0.06145473 0.1369406 0.08036264 0.06171053 0.1113126 0.06732087
For me, the important meaning of these values is that from comparision 
of class 1 and 3, as well as class 2 and 4 result more common features 
than from other comparisions. I have worked yet a lot about these data, 
I have looked a lot on my spectra, and I believe these proximities to be 
realistic.

Then I ran the tune RF function(step factor 1.5), I got out an mtry=63. 
A new forest having this mtry and 2500 trees gave me perfect 
classification as well, but the relation between proximitiy values 
changed a lot:
res
          op12         op13       op14           op23               
op24       op34
[1,] 0.1092702 0.117489 0.09696328 0.08725208 0.08495621 0.06506148

This is what makes me think that I have overtuned my second forest...So 
how should I choose mtry?

Best regards,
Ute



From andy_liaw at merck.com  Wed Oct 13 12:10:39 2004
From: andy_liaw at merck.com (Liaw, Andy)
Date: Wed, 13 Oct 2004 06:10:39 -0400
Subject: [R] random forest -optimising mtry
Message-ID: <3A822319EB35174CA3714066D590DCD504AF8529@usrymx25.merck.com>

What I would try is repeat the RF with both mtry a couple of times and see
how variable those inter-class proximities are.  Even though the differences
seem large, they could be within the noise.  If so, your only option would
be to increase the number of trees.

If you are getting 0 OOB error, (and the inter-class proximities are so low)
the classes are probably rather well-separated.  I wouldn't worry too much
about pinning down an `optimal' mtry.  The OOB error curve is usually fairly
flat over a wide range of mtry, so you're likely to get different optima on
repeated runs, with similar OOB errors.

HTH,
Andy

> From: Ute
> 
> Dear R-helpers,
> 
> I'm working on mass spectra in randomForest/R, and following the 
> recommendations for the case of noisy variables, I don't want 
> to use the 
> default mtry (sqrt of nvariables), but I'm not sure up to which 
> proportion mtry/nvariables it makes sense to increase mtry without 
> "overtuning" RF.
> Let me tell my example: I have 106 spectra belonging to 4 
> classes, the 
> number of variables is 172. I'm interested in finding 
> information about 
> variables (importance, split points etc.) and proximities.
> First I  ran a forest with mtry =30 and ntree=2500. The result was an 
> oob-estimate of overall error rate of zero, perfect 
> classification.  In 
> order to explore my results, I calculated the average 
> proximity between 
> the classes. I got:
>  > res
>            op12          op13           op14           op23          
> op24          op34
> [1,] 0.06145473 0.1369406 0.08036264 0.06171053 0.1113126 0.06732087
> For me, the important meaning of these values is that from 
> comparision 
> of class 1 and 3, as well as class 2 and 4 result more common 
> features 
> than from other comparisions. I have worked yet a lot about 
> these data, 
> I have looked a lot on my spectra, and I believe these 
> proximities to be 
> realistic.
> 
> Then I ran the tune RF function(step factor 1.5), I got out 
> an mtry=63. 
> A new forest having this mtry and 2500 trees gave me perfect 
> classification as well, but the relation between proximitiy values 
> changed a lot:
> res
>           op12         op13       op14           op23               
> op24       op34
> [1,] 0.1092702 0.117489 0.09696328 0.08725208 0.08495621 0.06506148
> 
> This is what makes me think that I have overtuned my second 
> forest...So 
> how should I choose mtry?
> 
> Best regards,
> Ute
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
> 
>



From ripley at stats.ox.ac.uk  Wed Oct 13 12:48:50 2004
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Wed, 13 Oct 2004 11:48:50 +0100 (BST)
Subject: [R] "Centered" dummy variables; non zero/one coding
In-Reply-To: <000001c4b0fc$6e0ed710$0202a8c0@psh>
Message-ID: <Pine.LNX.4.44.0410131138280.4310-100000@gannet.stats>

This can done by setting a contrast function or matrix on a variable.
Look in e.g. chapter 6 of MASS (the only comprehensive tutorial on coding 
factors in R, it seems).

On Tue, 12 Oct 2004, Peter Holck wrote:

> I'm uncertain if this is perhaps a stupid question:
> 
> I want to create "centered" dummy variables to use in a call to glm(), and
> wondering if there's some slick method in R to do so.  That is, rather than
> have a factor, which results in a glm() fit returning coefficients
> specifying either absence or presence of the factor, I'd like to fit a glm()
> without intercept such that the estimated coefficients (standard errors)
> represent the "average" value in my data set for that variable.  

Is that really what you want?  An `average' person having linear predictor 
0, or more precisely, the linear predictor have average zero over the 
dataset?  What family of glm is this?

> An example: a data set has Race specified with 4 levels.  I can manually
> specify 4 dummy variables for a no-intercept model with each variable rather
> than having a value of zero or one, has a centered value based on its
> frequency of occurrence in the data set.  Thus if 30% of the records in the
> data set have Race of Hispanic, I can define a variable HISP that has a
> value of either -.3 or .7, resulting in my coefficient estimate for HISP
> representing the effect of an "average" person in the database (and a
> corresponding valid standard error).   

Nope.  A person can only have one race, so the coefficient estimates can 
only represent jointly the effect of picking one of the possible races.

I think what you are striving for is that the average of the term `race' 
be zero over the whole dataset.  That's easy -- just compute the average 
and subtract it via an offset term.

Once you have two or more factor predictors you will get aliasing your 
way.

> One way to create these "centered dummy variables" from the original factor
> is:
> 		"B"=scale(RACE=="B",scale=F),
> 		"W"=scale(RACE=="W",scale=F),
> 		"H"=scale(RACE=="H",scale=F),
> 		"OTHRACE"=scale(RACE=="OTHER",scale=F)
> 
> However I wonder if there is some method in R to avoid having to manually
> define a large number of these dummy variables for a more complicated
> dataset.

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From ripley at stats.ox.ac.uk  Wed Oct 13 12:54:49 2004
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Wed, 13 Oct 2004 11:54:49 +0100 (BST)
Subject: [R] Cleaning up R-2.0.0 installation on GNU/Linux ?[SOLVED]
In-Reply-To: <416CF338.9000102@swissinfo.org>
Message-ID: <Pine.LNX.4.44.0410131149420.4310-100000@gannet.stats>

On Wed, 13 Oct 2004, Thomas Sch??nhoff wrote:

> Hello,
> 
> 
> Prof Brian Ripley said the following on 13.10.2004 10:34:
> > On Wed, 13 Oct 2004, Thomas Sch??nhoff wrote:
> 
> > remove.packages("nlme", "/usr/local/lib/R/site-library") # a duplicate
> > foo <- installed.packages(lib="/usr/local/lib/R/site-library")[, 1]
> > install.packages(foo, lib="/usr/local/lib/R/site-library")
> 
> Besides one minor error regarding to distr package:
> 
> --------------------------------------------------------------
> Warning message:
> Installation of package distr had non-zero exit status in: 
> install.packages(foo, lib = "/usr/local/lib/R/site-library")
> ---------------------------------------------------------------

You seem to be missing its dependency setRNG, I should have suggested

install.packages(foo, lib = "/usr/local/lib/R/site-library", 
                 dependencies = TRUE)

but thought about it and thought that you must have all the dependencies 
if things are working in 1.9.1 (so presumably they were not).

> your recommendations worked out fine, thanks!
> 
> 
> 
> > will reinstall all the packages in that library.  I could work out a way
> > to only get those that were installed in an earlier version, but it would 
> > be cumbersome.
> 
> No need for doing this at the moment!
> 
> 
> If understand correctly the previous situation is only arising when 
> there are major version upgrades, i.e. from 1.9.1 too 2.0.0.

It has only happened once in R's history.

> But when using "update packages() within a major release everything will 
> be fine, isn't it?

Yes, but even when upgrading a minor version is may be a good idea to 
re-install as, e.g. the processing of help files may get improved or more 
metadata may be installed.

Maybe we should add an option to update.packages to reinstall old 
installations too.

> I would be interested to hear what upgrade methods GNU/Linux (Debian) 
> users prefer to avoid a situation like this!
> 
> Many thanks for help!
> 
> Thomas
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
> 
> 

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From tom_woody at swissinfo.org  Wed Oct 13 13:33:32 2004
From: tom_woody at swissinfo.org (=?ISO-8859-1?Q?Thomas_Sch=F6nhoff?=)
Date: Wed, 13 Oct 2004 13:33:32 +0200
Subject: [R] Cleaning up R-2.0.0 installation on GNU/Linux ?[SOLVED]
In-Reply-To: <Pine.LNX.4.44.0410131149420.4310-100000@gannet.stats>
References: <Pine.LNX.4.44.0410131149420.4310-100000@gannet.stats>
Message-ID: <416D128C.9040004@swissinfo.org>

Hello,

Prof Brian Ripley said the following on 13.10.2004 12:54:
> On Wed, 13 Oct 2004, Thomas Sch??nhoff wrote:

> 
> Maybe we should add an option to update.packages to reinstall old 
> installations too.

This might clear up GNU R mysteries for newbies like me ;-)

cheers

Thomas



From Luisr at frs.fo  Wed Oct 13 13:45:31 2004
From: Luisr at frs.fo (Luis Rideau Cruz)
Date: Wed, 13 Oct 2004 12:45:31 +0100
Subject: [R] 'Replace' in internal editor causes R to crash
Message-ID: <s16d2374.044@ffdata.setur.fo>

R-help,

While using the internal editor in R (R 2.0.0) I have encountered the
following problem :

When I execute the facility 'Replace' (Ctrl H) R crashes (exactly when
I paste to field 'look for')

This is not one-off.I have faced it several days ago (and today as
well).

System specifications:

Microsoft Windows XP professional (Version 2002,Danish)
Pentium 4
M CPU 1.80 GHz
512 MB RAM

Thank you in advance



From ripley at stats.ox.ac.uk  Wed Oct 13 14:13:52 2004
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Wed, 13 Oct 2004 13:13:52 +0100 (BST)
Subject: [R] importing data
In-Reply-To: <410-220041031318447590@earthlink.net>
Message-ID: <Pine.LNX.4.44.0410131310090.4514-100000@gannet.stats>

On Wed, 13 Oct 2004, Andreas Betz wrote:

>  I am newcomer to R and I am loborating on this problem:
> 
> How do I import data from a CD into R for further evaluation. 
> Using code "newgotcha <- read.table("e:\\asciiwin\\gotcha.dat") "
> returmns a list containing an additional column with indices. The
> command "plot (gotcha)" gives the expected plot. However, the line
> hist(gotcha) returns the message "Error in hist.default(newgotcha) : `x'
> must be numeric".

Um, you read `newgotcha' so what did you expect "plot (gotcha)" to do?

> How can I get the data in an accessible from  into R.

Please read `An Introduction to R' and note the difference between
read.table() and read.table(header=TRUE).

If that is not sufficient, please read the R Data Import/Export manual.

> Any suggestions are appreciated.
> 
> Andreas
> 
> Andreas Betz
> andreasbetz at earthlink.net
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html

Please do, and don't send HTML mail and do read the introductory
documentation.

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From dataanalytics at rediffmail.com  Wed Oct 13 15:18:30 2004
From: dataanalytics at rediffmail.com (data Analytics)
Date: 13 Oct 2004 13:18:30 -0000
Subject: [R] Seeking advice on "introducing R"
Message-ID: <20041013131830.27794.qmail@webmail28.rediffmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20041013/447dfa76/attachment.pl

From jfox at mcmaster.ca  Wed Oct 13 15:46:32 2004
From: jfox at mcmaster.ca (John Fox)
Date: Wed, 13 Oct 2004 09:46:32 -0400
Subject: [R] Seeking advice on "introducing R"
In-Reply-To: <20041013131830.27794.qmail@webmail28.rediffmail.com>
Message-ID: <20041013134632.UTGL1536.tomts25-srv.bellnexxia.net@JohnDesktop8300>

Dear Arin,

I have a variety of materials at
<http://socserv.socsci.mcmaster.ca/jfox/Courses/S-course/index.html> that
may be helpful.

Regards,
 John

--------------------------------
John Fox
Department of Sociology
McMaster University
Hamilton, Ontario
Canada L8S 4M4
905-525-9140x23604
http://socserv.mcmaster.ca/jfox 
-------------------------------- 

> -----Original Message-----
> From: r-help-bounces at stat.math.ethz.ch 
> [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of data Analytics
> Sent: Wednesday, October 13, 2004 8:19 AM
> To: r-help at stat.math.ethz.ch
> Subject: [R] Seeking advice on "introducing R"
> 
> Dear List:
> 
> This is in search of advices/opinions for a lecture 
> presentation on R. I have this presentation scheduled on 
> Saturday (16-October-2004); the topic is "introduction to R". 
> The audience is a group of researchers and scholars who have 
> either never used R/are beginning to use R, but otherwise 
> have used a                   
> packagecalledlimdeb,andSPSS.ThereisalsoarequesttocoverRgnumeri
> c,andexcelbindingstoR.
> 
> I have no knowledge of limdeb.
> 
> Would greatly appreciate your advices, opinions, and look 
> forward to learn from your experiences in introducing R to 
> similar audiences. Also, pointers to web resources would be 
> greatly appreciated.
> 
> TIA,
> Arin Basu
> Kolkata, India
> 
>  
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html



From jarioksa at sun3.oulu.fi  Wed Oct 13 16:18:40 2004
From: jarioksa at sun3.oulu.fi (Jari Oksanen)
Date: 13 Oct 2004 17:18:40 +0300
Subject: [R] data(eurodist) and PCA ??
In-Reply-To: <Pine.LNX.4.44.0410130740140.3778-100000@gannet.stats>
References: <Pine.LNX.4.44.0410130740140.3778-100000@gannet.stats>
Message-ID: <1097677119.3717.26.camel@biol102145.oulu.fi>

On Wed, 2004-10-13 at 09:51, Prof Brian Ripley wrote:
> On Wed, 13 Oct 2004, Dan Bolser wrote:

> > I have a complex distance matrix, and I am thinking about how to cluster
> > it and how to visualize the quality of the resulting clusters. 
> 
> Using PCA and plotting the first two components is classical
> multi-dimensional scaling, as implemented by cmdscale().  Look up MDS
> somewhere (e.g. in MASS).  It is exact if the distances are Euclidean in
> 2D.  However, eurodist gives road distances on the surface of sphere.
> 
> Classic examples for the illustration of MDS are departements of France 
> based on proximity data and cities in the UK based on road distances.
> 
These road distances seem to be very non-Euclidean indeed (even
non-metric). It seems to be 2282km from Athens to Milan if you go
directly, but if you go via Rome it is only 1403km:

> trip <- c("Athens", "Rome", "Milan")
> as.matrix(eurodist)[trip, trip]
       Athens Rome Milan
Athens      0  817  2282
Rome      817    0   586
Milan    2282  586     0
> 817 + 586
[1] 1403

I thought that World is non-Euclidean, but not that obviously.

cheers, jari oksanen


-- 
Jari Oksanen <jarioksa at sun3.oulu.fi>



From adi at sas.unibuc.ro  Wed Oct 13 13:35:15 2004
From: adi at sas.unibuc.ro (Adrian Dusa)
Date: Wed, 13 Oct 2004 14:35:15 +0300
Subject: [R] one more Rcmdr problem
In-Reply-To: <200410131014.i9DA8Lo1024675@hypatia.math.ethz.ch>
Message-ID: <200410131829.i9DITvZ9024159@swork.sas.unibuc.ro>

Hello,

I'm using R 2.0.0 with the latest Rcmdr package installed from CRAN, on
Windows XP Professional.

When trying to copy some commands or results, either from the upper or lower
text window, this causes Rcmdr to crash:

"R for Windows GUI front-end has encountered a problem and needs to close"

Did anyone have the same problem? I don't think it's my system, as it
happened to reinstall my Windows just a few days ago, and the same problem
occurred in the former one.

Regards,
Adrian

~~~~~~~~~~~~~~~~~~~~~~~~~~~~
Adrian Dusa
Romanian Social Data Archive
1 Schitu Magureanu Bd.
050025 Bucharest sector 5
Tel. +40 21 3126618\
     +40 21 3153122/ int.101



-- 
This message was scanned for spam and viruses by BitDefender
For more information please visit http://linux.bitdefender.com/



From dmb at mrc-dunn.cam.ac.uk  Wed Oct 13 16:47:19 2004
From: dmb at mrc-dunn.cam.ac.uk (Dan Bolser)
Date: Wed, 13 Oct 2004 15:47:19 +0100 (BST)
Subject: [R] data(eurodist) and PCA ??
In-Reply-To: <1097677119.3717.26.camel@biol102145.oulu.fi>
Message-ID: <Pine.LNX.4.21.0410131545450.21888-100000@mail.mrc-dunn.cam.ac.uk>

On 13 Oct 2004, Jari Oksanen wrote:

>On Wed, 2004-10-13 at 09:51, Prof Brian Ripley wrote:
>> On Wed, 13 Oct 2004, Dan Bolser wrote:
>
>> > I have a complex distance matrix, and I am thinking about how to cluster
>> > it and how to visualize the quality of the resulting clusters. 
>> 
>> Using PCA and plotting the first two components is classical
>> multi-dimensional scaling, as implemented by cmdscale().  Look up MDS
>> somewhere (e.g. in MASS).  It is exact if the distances are Euclidean in
>> 2D.  However, eurodist gives road distances on the surface of sphere.
>> 
>> Classic examples for the illustration of MDS are departements of France 
>> based on proximity data and cities in the UK based on road distances.
>> 
>These road distances seem to be very non-Euclidean indeed (even
>non-metric). It seems to be 2282km from Athens to Milan if you go
>directly, but if you go via Rome it is only 1403km:

All roads lead to rome? Aparently that is true if you ever try to get out
of the place in rush hour.

>> trip <- c("Athens", "Rome", "Milan")
>> as.matrix(eurodist)[trip, trip]
>       Athens Rome Milan
>Athens      0  817  2282
>Rome      817    0   586
>Milan    2282  586     0
>> 817 + 586
>[1] 1403
>
>I thought that World is non-Euclidean, but not that obviously.

yes, especially not europe on its own. My geography is worse than my
statistics, but it looked a bit mangled up even to me.

Thanks very much both again,
Dan.

>
>cheers, jari oksanen
>
>
>



From jfox at mcmaster.ca  Wed Oct 13 17:00:39 2004
From: jfox at mcmaster.ca (John Fox)
Date: Wed, 13 Oct 2004 11:00:39 -0400
Subject: [R] one more Rcmdr problem
In-Reply-To: <200410131829.i9DITvZ9024159@swork.sas.unibuc.ro>
Message-ID: <20041013150040.VYVM15612.tomts16-srv.bellnexxia.net@JohnDesktop8300>

Dear Adrian,

I've just been able to verify this problem. It occurs for me when I try to
copy via Ctrl-C, but not when I use the Rcmdr Edit -> Copy menu, nor when I
use the right-click context menu. As near as I can tell, the problem is
general to all Ctrl-key combinations. I don't have time at the moment to
investigate further, but I suspect a problem between tcltk and Rgui. I'm
copying this response to the r-devel list, since that might be a better
place to pursue the problem.

Sorry for the trouble.
 John

--------------------------------
John Fox
Department of Sociology
McMaster University
Hamilton, Ontario
Canada L8S 4M4
905-525-9140x23604
http://socserv.mcmaster.ca/jfox 
-------------------------------- 

> -----Original Message-----
> From: r-help-bounces at stat.math.ethz.ch 
> [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Adrian Dusa
> Sent: Wednesday, October 13, 2004 6:35 AM
> To: r-help at stat.math.ethz.ch
> Subject: [R] one more Rcmdr problem
> 
> Hello,
> 
> I'm using R 2.0.0 with the latest Rcmdr package installed 
> from CRAN, on Windows XP Professional.
> 
> When trying to copy some commands or results, either from the 
> upper or lower text window, this causes Rcmdr to crash:
> 
> "R for Windows GUI front-end has encountered a problem and 
> needs to close"
> 
> Did anyone have the same problem? I don't think it's my 
> system, as it happened to reinstall my Windows just a few 
> days ago, and the same problem occurred in the former one.
> 
> Regards,
> Adrian
> 
> ~~~~~~~~~~~~~~~~~~~~~~~~~~~~
> Adrian Dusa
> Romanian Social Data Archive
> 1 Schitu Magureanu Bd.
> 050025 Bucharest sector 5
> Tel. +40 21 3126618\
>      +40 21 3153122/ int.101
> 
> 
> 
> --
> This message was scanned for spam and viruses by BitDefender 
> For more information please visit http://linux.bitdefender.com/
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html



From kshe4 at student.monash.edu  Wed Oct 13 17:20:39 2004
From: kshe4 at student.monash.edu (Kunal Shetty)
Date: Wed, 13 Oct 2004 15:20:39 +0000
Subject: [R] Maximum Likelihood :- Log likehoood function
Message-ID: <220.253.10.88.1097679702.24219@my.monash.edu.au>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20041013/66a21304/attachment.pl

From Ted.Harding at nessie.mcc.ac.uk  Wed Oct 13 17:44:49 2004
From: Ted.Harding at nessie.mcc.ac.uk ( (Ted Harding))
Date: Wed, 13 Oct 2004 16:44:49 +0100 (BST)
Subject: [R] Maximum Likelihood :- Log likehoood function
In-Reply-To: <220.253.10.88.1097679702.24219@my.monash.edu.au>
Message-ID: <XFMail.041013164449.Ted.Harding@nessie.mcc.ac.uk>

On 13-Oct-04 Kunal Shetty wrote:
> Dear R - users/Helpers
> 
>  I am dealing with bivariate Normal data with missing values. Further I
> am trying to implement Expectation-Maximization (EM) algorithm to 
> resolve the missing data problem.
>  Now one of the requirements is use the Log likehood function i.e -2Log
> so as to find a reliable convergence....
> 
>        My question is there any R  built function for the same or do i
> have to use the packages contributed by other R Developers.

Dear Kunal,
It looks as though you should be able to do what you want with the
library 'norm', available on CRAN as an extra package.

Hoping this helps,
Best wishes,
Ted.


--------------------------------------------------------------------
E-Mail: (Ted Harding) <Ted.Harding at nessie.mcc.ac.uk>
Fax-to-email: +44 (0)870 094 0861  [NB: New number!]
Date: 13-Oct-04                                       Time: 16:44:49
------------------------------ XFMail ------------------------------



From kshe4 at student.monash.edu  Wed Oct 13 18:23:54 2004
From: kshe4 at student.monash.edu (Kunal Shetty)
Date: Wed, 13 Oct 2004 16:23:54 +0000
Subject: [R] Re:How to create a R -application
References: <XFMail.041007193222.Ted.Harding@nessie.mcc.ac.uk>
Message-ID: <220.253.10.88.1097684233@my.monash.edu.au>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20041013/5b49ac3d/attachment.pl

From kshe4 at student.monash.edu  Wed Oct 13 18:32:42 2004
From: kshe4 at student.monash.edu (Kunal Shetty)
Date: Wed, 13 Oct 2004 16:32:42 +0000
Subject: [R] Re:How to create a R -application
References: <220.253.10.88.1097684233@my.monash.edu.au>
Message-ID: <220.253.10.88.1097684936.64787@my.monash.edu.au>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20041013/1764c5c5/attachment.pl

From szhan at uoguelph.ca  Wed Oct 13 18:35:29 2004
From: szhan at uoguelph.ca (szhan@uoguelph.ca)
Date: Wed, 13 Oct 2004 12:35:29 -0400
Subject: [R] Why I can't retrieve GO identifier correctly?
In-Reply-To: <416CC915.2010107@statistik.uni-dortmund.de>
References: <1097615362.416c4802951a7@webmail.uoguelph.ca>
	<416CC915.2010107@statistik.uni-dortmund.de>
Message-ID: <1097685329.416d5951d289c@webmail.uoguelph.ca>

Hello, Uwe and Robert,
Thank you for your help! I fixed the problem as your suggestion. I run into a
new issue: I can't sink the output to a file in the script within loops as
bellows, but I can sink it in the command line( I run R1.9.1 on WinXP).
The scrip:
sink("C:/level3BPterm.txt")
level2<-getGOChildren("GO:0008150")$"GO:0008150"$Children
for ( i in 1:length(level2)) {

    level3 <- c(getGOChildren(level2[i])[[1]]$Children)
    for ( j in 1:length(level3)){

       level3term <- getGOTerm(as.character(level3[j]))$BP

       paste(level3term)

    }
}
sink()

Run in the command line:
> sink("C:/level3BPterm.txt")
>  paste(level3term)
> sink()
So what is wrong with my script?
Thanks again
Josh

Quoting Uwe Ligges <ligges at statistik.uni-dortmund.de>:

> szhan at uoguelph.ca wrote:
> > Hello, R experts,
> > I tried to retrieve all biological process GO terms at level 3 starting
> > "biological process" as level 1 using the code as bellows:
> >
> > 1 library(GO)
> > 2 library(GOstats)
> > 3 level2<-getGOChildren("GO:0008150")$"GO:0008150"$Children
> > 4 for ( i in 1:length(level2)) {
> > 5    level3 <- getGOChildren(level2[i])$level2[i]$Children
>
> If you want to index by an object that contains a character (or string),
> you have to use "[[]]" rather than "$".
> The question is completely independent of GO, so don't confuse potential
> responders who expect you to send messages re. Bioconductor packages to
> the corresponding list.
>
> Uwe Ligges
>
>
> > 6    for ( j in 1:length(level3)){
> > 7       level3term <- getGOTerm(as.character(level3[j]))$BP$level3[j]
> > 8       level3term
> > 9    }
> > 10 }
> > What is the difference between line 3 and line5? In the line 3 I retrieved
> the
> > GO identifiers at level 2 successfullly but in the line 5 I got nothing.
> How to
> > correct the line 5 to retrieve the GO terms at level 3 correctly?
> > Thank you in advance!
> > Josh
> >
> > ______________________________________________
> > R-help at stat.math.ethz.ch mailing list
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide!
> http://www.R-project.org/posting-guide.html
>



From kjetil at acelerate.com  Wed Oct 13 18:41:13 2004
From: kjetil at acelerate.com (Kjetil Brinchmann Halvorsen)
Date: Wed, 13 Oct 2004 12:41:13 -0400
Subject: [R] Problems understanding qr.  family of functions
Message-ID: <416D5AA9.7070502@acelerate.com>

Hola!

By my understanding of reading   ?qr
the following shold result a 3 x 3 matrix:  (rw2000)

 x <- matrix( rnorm(12), 4,3)
 y <- matrix( rnorm(12), 4,3)
 xqr <- qr(x)
 yqr <- qr(y)

 qr.qty( xqr, qr.Q(yqr) ) # dim (3,3)
            [,1]       [,2]      [,3]
[1,]  0.16815466 -0.1970936 0.2351670
[2,]  0.15667444  0.4939800 0.8443340
[3,] -0.97096971  0.1029652 0.1456355
[4,] -0.06629443 -0.8405570 0.4588976

 # The following should be equal:
 t( qr.Q(xqr) ) %*% qr.Q(yqr)
           [,1]       [,2]      [,3]
[1,]  0.1681547 -0.1970936 0.2351670
[2,]  0.1566744  0.4939800 0.8443340
[3,] -0.9709697  0.1029652 0.1456355

but evidently is not.

There is at least one bug:
1) in R
2) in the help page
3) in my reading of the help page

Kjetil



-- 

Kjetil Halvorsen.

Peace is the most effective weapon of mass construction.
               --  Mahdi Elmandjra



From ligges at statistik.uni-dortmund.de  Wed Oct 13 18:41:28 2004
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Wed, 13 Oct 2004 18:41:28 +0200
Subject: [R] Why I can't retrieve GO identifier correctly?
In-Reply-To: <1097685329.416d5951d289c@webmail.uoguelph.ca>
References: <1097615362.416c4802951a7@webmail.uoguelph.ca>
	<416CC915.2010107@statistik.uni-dortmund.de>
	<1097685329.416d5951d289c@webmail.uoguelph.ca>
Message-ID: <416D5AB8.7060909@statistik.uni-dortmund.de>

szhan at uoguelph.ca wrote:

> Hello, Uwe and Robert,

Well, you are sending your message to Uwe and R-help rather than "Uwe 
and Robert" ......


> Thank you for your help! I fixed the problem as your suggestion. I run into a
> new issue: I can't sink the output to a file in the script within loops as
> bellows, but I can sink it in the command line( I run R1.9.1 on WinXP).
> The scrip:
> sink("C:/level3BPterm.txt")
> level2<-getGOChildren("GO:0008150")$"GO:0008150"$Children
> for ( i in 1:length(level2)) {
> 
>     level3 <- c(getGOChildren(level2[i])[[1]]$Children)
>     for ( j in 1:length(level3)){
> 
>        level3term <- getGOTerm(as.character(level3[j]))$BP
> 
>        paste(level3term)

You need to print() it! But you really want to return() it from a 
function ...

Uwe

>     }
> }
> sink()
> 
> Run in the command line:
> 
>>sink("C:/level3BPterm.txt")
>> paste(level3term)
>>sink()
> 
> So what is wrong with my script?
> Thanks again
> Josh
> 
> Quoting Uwe Ligges <ligges at statistik.uni-dortmund.de>:
> 
> 
>>szhan at uoguelph.ca wrote:
>>
>>>Hello, R experts,
>>>I tried to retrieve all biological process GO terms at level 3 starting
>>>"biological process" as level 1 using the code as bellows:
>>>
>>>1 library(GO)
>>>2 library(GOstats)
>>>3 level2<-getGOChildren("GO:0008150")$"GO:0008150"$Children
>>>4 for ( i in 1:length(level2)) {
>>>5    level3 <- getGOChildren(level2[i])$level2[i]$Children
>>
>>If you want to index by an object that contains a character (or string),
>>you have to use "[[]]" rather than "$".
>>The question is completely independent of GO, so don't confuse potential
>>responders who expect you to send messages re. Bioconductor packages to
>>the corresponding list.
>>
>>Uwe Ligges
>>
>>
>>
>>>6    for ( j in 1:length(level3)){
>>>7       level3term <- getGOTerm(as.character(level3[j]))$BP$level3[j]
>>>8       level3term
>>>9    }
>>>10 }
>>>What is the difference between line 3 and line5? In the line 3 I retrieved
>>
>>the
>>
>>>GO identifiers at level 2 successfullly but in the line 5 I got nothing.
>>
>>How to
>>
>>>correct the line 5 to retrieve the GO terms at level 3 correctly?
>>>Thank you in advance!
>>>Josh
>>>
>>>______________________________________________
>>>R-help at stat.math.ethz.ch mailing list
>>>https://stat.ethz.ch/mailman/listinfo/r-help
>>>PLEASE do read the posting guide!
>>
>>http://www.R-project.org/posting-guide.html
>>
> 
>



From br44114 at yahoo.com  Wed Oct 13 19:19:56 2004
From: br44114 at yahoo.com (bogdan romocea)
Date: Wed, 13 Oct 2004 10:19:56 -0700 (PDT)
Subject: [R] incomplete function output
Message-ID: <20041013171956.85632.qmail@web50308.mail.yahoo.com>

Dear R users,

I have a function (below) which encompasses several tests.
However, when I run it, only the output of the last test is
displayed. How can I ensure that the function root(var)
will run and display the output from all tests, and not
just the last one?

Thank you,
b.

root <- function(var)
{
#---Phillips-Perron
PP.test(var, lshort = TRUE) 
PP.test(var, lshort = FALSE) 

#---Augmented Dickey-Fuller 
adf.test(var, alternative = "stationary", k =
trunc((length(var)-1)^(1/3)))

#---KPSS
kpss.test(var, null = "Level", lshort = TRUE)
kpss.test(var, null = "Trend", lshort = FALSE)
}



From jmoreira at fe.up.pt  Wed Oct 13 19:21:22 2004
From: jmoreira at fe.up.pt (jmoreira@fe.up.pt)
Date: Wed, 13 Oct 2004 18:21:22 +0100
Subject: [R] Problems with randomForest for regression
Message-ID: <1097688082.416d641250ed6@webmail.fe.up.pt>

Dear list,

I am trying to do a benchmark study for my case study. It is a regression 
problem. Among other models I use randomForest.

Using the following code the result is around 0.628, and this make sense 
comparing with other methods. The Theil function implements Theil's U 
statistic. I do not present the definition of some variables because it is not 
important to understand my problem. I use sliding window trategy.

library("randomForest")

rf.theil <- vector()
learner='randomForest'

for (i in 1:6)
  {
   eval.sum <- 0
   test.pos=test.pos.ini

   while (test.pos <= n)
     {
      naive.pred <- c(orig.data[test.pos-1,7])
      model <- randomForest(Duracao ~ ., data=orig.data[1:(test.pos-1),], 
		     na.action=na.omit, ntree=5000, mtry=i)
      preds <- predict(model,orig.data[test.pos:min(n,test.pos+relearn.step-
1),])
      test.pos <- test.pos+relearn.step

      a<-theil(preds, naive.pred, orig.data[test.pos:min
(n,test.pos+relearn.step-1),7])
      if (is.na(a)==FALSE) {eval.sum <- eval.sum + a}
     }
   rf.theil <- c(rf.theil, eval.sum/(trunc((n-test.pos.ini)/relearn.step)+1))
  }

rf.min <- min(rf.theil, na.rm=TRUE)
rf.indices <- seq(along=rf.theil)[rf.theil == rf.min]


But running 5 times randomForest for each value of i, and choosing the best 
result according U statistic, I got a value around 0.178... And this value 
does not make sense. I use the some strategie with nnet and it gives good 
results. The code is:

library("randomForest")

rf.theil <- vector()

for (i in 1:6)
  {
   eval <- 100000
   eval.sum <- 0
   test.pos=test.pos.ini

   while (test.pos <= n)
     {
      naive.pred <- c(orig.data[test.pos-1,7])
      for (j in 1:5)
        {
         model <- randomForest(Duracao ~ ., data=orig.data[1:(test.pos-1),], 
		     na.action=na.omit, ntree=5000, mtry=i)
         preds <- predict(model,
			  orig.data[test.pos:min(n,test.pos+relearn.step-1),])
         eval.temp <- theil(preds, naive.pred, 
		       orig.data[test.pos:min(n,test.pos+relearn.step-1),7])
         if (eval.temp < eval)
           eval <- eval.temp
        }
      if (is.na(eval)==FALSE) 
	eval.sum <- eval.sum + eval
      test.pos <- test.pos+relearn.step
     }
   rf.theil <- c(rf.theil, eval.sum/(trunc((n-test.pos.ini)/relearn.step)+1))
  }

rf.min <- min(rf.theil, na.rm=TRUE)

Thanks for any help

Joao Moreira



From maustin at amgen.com  Wed Oct 13 19:28:12 2004
From: maustin at amgen.com (Austin, Matt)
Date: Wed, 13 Oct 2004 10:28:12 -0700
Subject: [R] incomplete function output
Message-ID: <E7D5AB4811D20B489622AABA9C53859104E0D99B@teal-exch.amgen.com>

I would return the values from the various tests in a list. If you only want
them to print and not for use in other parts of your program you could
explicitly print each test using print().

root <- function(var)
{
#---Phillips-Perron
 test1 <- PP.test(var, lshort = TRUE) 
 test2 <- PP.test(var, lshort = FALSE) 

#---Augmented Dickey-Fuller 
 test3 <- adf.test(var, alternative = "stationary", k =
 test4 <- trunc((length(var)-1)^(1/3)))

#---KPSS
 test5 <- kpss.test(var, null = "Level", lshort = TRUE)
 test6 <- kpss.test(var, null = "Trend", lshort = FALSE)
 list(test1, test2, test3, test4, test5, test6)
}

--Matt
-----Original Message-----
From: r-help-bounces at stat.math.ethz.ch
[mailto:r-help-bounces at stat.math.ethz.ch]On Behalf Of bogdan romocea
Sent: Wednesday, October 13, 2004 10:20 AM
To: r-help at stat.math.ethz.ch
Subject: [R] incomplete function output


Dear R users,

I have a function (below) which encompasses several tests.
However, when I run it, only the output of the last test is
displayed. How can I ensure that the function root(var)
will run and display the output from all tests, and not
just the last one?

Thank you,
b.

root <- function(var)
{
#---Phillips-Perron
PP.test(var, lshort = TRUE) 
PP.test(var, lshort = FALSE) 

#---Augmented Dickey-Fuller 
adf.test(var, alternative = "stationary", k =
trunc((length(var)-1)^(1/3)))

#---KPSS
kpss.test(var, null = "Level", lshort = TRUE)
kpss.test(var, null = "Trend", lshort = FALSE)
}

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide!
http://www.R-project.org/posting-guide.html



From spencer.graves at pdf.com  Wed Oct 13 19:28:26 2004
From: spencer.graves at pdf.com (Spencer Graves)
Date: Wed, 13 Oct 2004 10:28:26 -0700
Subject: [R] incomplete function output
In-Reply-To: <20041013171956.85632.qmail@web50308.mail.yahoo.com>
References: <20041013171956.85632.qmail@web50308.mail.yahoo.com>
Message-ID: <416D65BA.2000608@pdf.com>

      Only the last appears, because the function returns only the last, 
which then gets printed.  The answer to your question depends on whether 
you want to see the results or store them.  To see them, try the 
following:      

root.print <- function(var)
{
#---Phillips-Perron
print(PP.test(var, lshort = TRUE) )
print(PP.test(var, lshort = FALSE) )

#---Augmented Dickey-Fuller 
print(adf.test(var, alternative = "stationary", k =
trunc((length(var)-1)^(1/3))))

#---KPSS
print(kpss.test(var, null = "Level", lshort = TRUE))
print(kpss.test(var, null = "Trend", lshort = FALSE))
}

	  To store the results, try:  

root.store <- function(var)
{
#---Phillips-Perron
t1 <- PP.test(var, lshort = TRUE) 
t2 <- PP.test(var, lshort = FALSE) 

#---Augmented Dickey-Fuller 
t3 <- adf.test(var, alternative = "stationary", k =
trunc((length(var)-1)^(1/3)))

#---KPSS
t4 <- kpss.test(var, null = "Level", lshort = TRUE)
t5 <- kpss.test(var, null = "Trend", lshort = FALSE)
list(PP.testT=t1, PP.testF=t2, adf.test=t3, kpss.testT=t4, kpss.testF=t5)
}

	  hope this helps.  
	  spencer graves

bogdan romocea wrote:

>Dear R users,
>
>I have a function (below) which encompasses several tests.
>However, when I run it, only the output of the last test is
>displayed. How can I ensure that the function root(var)
>will run and display the output from all tests, and not
>just the last one?
>
>Thank you,
>b.
>
>root <- function(var)
>{
>#---Phillips-Perron
>PP.test(var, lshort = TRUE) 
>PP.test(var, lshort = FALSE) 
>
>#---Augmented Dickey-Fuller 
>adf.test(var, alternative = "stationary", k =
>trunc((length(var)-1)^(1/3)))
>
>#---KPSS
>kpss.test(var, null = "Level", lshort = TRUE)
>kpss.test(var, null = "Trend", lshort = FALSE)
>}
>
>______________________________________________
>R-help at stat.math.ethz.ch mailing list
>https://stat.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>  
>

-- 
Spencer Graves, PhD, Senior Development Engineer
O:  (408)938-4420;  mobile:  (408)655-4567



From sundar.dorai-raj at PDF.COM  Wed Oct 13 19:30:48 2004
From: sundar.dorai-raj at PDF.COM (Sundar Dorai-Raj)
Date: Wed, 13 Oct 2004 12:30:48 -0500
Subject: [R] incomplete function output
In-Reply-To: <20041013171956.85632.qmail@web50308.mail.yahoo.com>
References: <20041013171956.85632.qmail@web50308.mail.yahoo.com>
Message-ID: <416D6648.9040308@pdf.com>



bogdan romocea wrote:
> Dear R users,
> 
> I have a function (below) which encompasses several tests.
> However, when I run it, only the output of the last test is
> displayed. How can I ensure that the function root(var)
> will run and display the output from all tests, and not
> just the last one?
> 
> Thank you,
> b.
> 
> root <- function(var)
> {
> #---Phillips-Perron
> PP.test(var, lshort = TRUE) 
> PP.test(var, lshort = FALSE) 
> 
> #---Augmented Dickey-Fuller 
> adf.test(var, alternative = "stationary", k =
> trunc((length(var)-1)^(1/3)))
> 
> #---KPSS
> kpss.test(var, null = "Level", lshort = TRUE)
> kpss.test(var, null = "Trend", lshort = FALSE)
> }
> 


You should store all your results in a list and return the list:

root <- function(var) {
   # create empty list
   ret <- list()

   #---Phillips-Perron
   ret[[1]] <- PP.test(var, lshort = TRUE)
   ret[[2]] <- PP.test(var, lshort = FALSE)

   #---Augmented Dickey-Fuller
   ret[[3]] <- adf.test(var, alternative = "stationary",
                        k = trunc((length(var)-1)^(1/3)))

   #---KPSS
   ret[[4]] <- kpss.test(var, null = "Level", lshort = TRUE)
   ret[[5]] <- kpss.test(var, null = "Trend", lshort = FALSE)

   # give `ret' some meaningful names
   names(ret) <- c("PP1", "PP2", "ADF", "KPSS1", "KPSS2")

   # return list
   ret
}

results <- root(somevar)

HTH,

--sundar

P.S. Also note my indenting which makes code more readable, especially 
if you expect other to try to read it.



From mjw at celos.net  Wed Oct 13 19:32:20 2004
From: mjw at celos.net (Mark White)
Date: Wed, 13 Oct 2004 18:32:20 +0100
Subject: [R] Large arrays and reference passing
Message-ID: <20041013173220.GG7395@celos.net>

I'm one of the handful of people using R on very large
numerical arrays; in my case, magnetic resonance images.

The speed/memory issues of pass-by-value when writing
functions (both R or .Call/.External) that update regions of
large arrays have been discussed before, and there are
several possible workarounds ('references' implemented with
environments, <<-, R_MakeExternalPointer for opaque
wrapping, monolithic functions).  I've tried some of them
and am trying to settle on an approach that works well.

Does anybody have experience relating to what works fastest,
or suggestions for how to write R code in a way that avoids
excessive copying of large arrays?  In my case most (though
not all) of the number crunching is done in C.

Thanks,
Mark <><



From rjohnson at ncifcrf.gov  Wed Oct 13 19:41:09 2004
From: rjohnson at ncifcrf.gov (Randy Johnson [Contr])
Date: Wed, 13 Oct 2004 13:41:09 -0400
Subject: [R] incomplete function output
In-Reply-To: <20041013171956.85632.qmail@web50308.mail.yahoo.com>
Message-ID: <000601c4b14b$d2fdf9e0$940d2b81@s056631NB>

You can return a list of them all

root <- function(var)
{
    return(list(test1 = PP.test(var, lshort = T), 
                test2 = ...))
}

output <- root(var)

Then you can print them, save them, or whatever.
Or you could just print them out in the function

root <- function(var)
{
#---Phillips-Perron
print(PP.test(var, lshort = TRUE))
print(PP.test(var, lshort = FALSE))
.
.
.
}

Randy

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
Randy Johnson [Contr.]
Laboratory of Genomic Diversity   |\
NCI Frederick   ___lll__/|   |\  ()
(301)846-1304    (_|||_)\|  ()
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

-----Original Message-----
From: r-help-bounces at stat.math.ethz.ch
[mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of bogdan romocea
Sent: Wednesday, October 13, 2004 1:20 PM
To: r-help at stat.math.ethz.ch
Subject: [R] incomplete function output

Dear R users,

I have a function (below) which encompasses several tests.
However, when I run it, only the output of the last test is
displayed. How can I ensure that the function root(var)
will run and display the output from all tests, and not
just the last one?

Thank you,
b.

root <- function(var)
{
#---Phillips-Perron
PP.test(var, lshort = TRUE) 
PP.test(var, lshort = FALSE) 

#---Augmented Dickey-Fuller 
adf.test(var, alternative = "stationary", k =
trunc((length(var)-1)^(1/3)))

#---KPSS
kpss.test(var, null = "Level", lshort = TRUE)
kpss.test(var, null = "Trend", lshort = FALSE)
}

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide!
http://www.R-project.org/posting-guide.html



From bates at cs.wisc.edu  Wed Oct 13 19:40:35 2004
From: bates at cs.wisc.edu (Douglas Bates)
Date: Wed, 13 Oct 2004 12:40:35 -0500
Subject: [R] Problems understanding qr.  family of functions
In-Reply-To: <416D5AA9.7070502@acelerate.com>
References: <416D5AA9.7070502@acelerate.com>
Message-ID: <416D6893.9080606@cs.wisc.edu>

Kjetil Brinchmann Halvorsen wrote:
> Hola!
> 
> By my understanding of reading   ?qr
> the following shold result a 3 x 3 matrix:  (rw2000)
> 
> x <- matrix( rnorm(12), 4,3)
> y <- matrix( rnorm(12), 4,3)
> xqr <- qr(x)
> yqr <- qr(y)
> 
> qr.qty( xqr, qr.Q(yqr) ) # dim (3,3)
>            [,1]       [,2]      [,3]
> [1,]  0.16815466 -0.1970936 0.2351670
> [2,]  0.15667444  0.4939800 0.8443340
> [3,] -0.97096971  0.1029652 0.1456355
> [4,] -0.06629443 -0.8405570 0.4588976
> 
> # The following should be equal:
> t( qr.Q(xqr) ) %*% qr.Q(yqr)
>           [,1]       [,2]      [,3]
> [1,]  0.1681547 -0.1970936 0.2351670
> [2,]  0.1566744  0.4939800 0.8443340
> [3,] -0.9709697  0.1029652 0.1456355
> 
> but evidently is not.
> 
> There is at least one bug:
> 1) in R
> 2) in the help page
> 3) in my reading of the help page
> 
> Kjetil

You are assuming that qr.Q produces the complete Q matrix.  By default 
it produces only the initial columns of the Q matrix sufficient to 
generate the same size matrix as was decomposed.  Use qr.Q(qrx, complete 
= TRUE) to generate the complete Q matrix.


 > qrx <- qr(matrix(rnorm(12), 4, 3))
 > qr.Q(qrx)
            [,1]       [,2]        [,3]
[1,] -0.6477697 -0.2925012 -0.68404548
[2,]  0.6645340 -0.3817050 -0.33452846
[3,] -0.1567635  0.7073773  0.01126533
[4,] -0.3379560 -0.5180364  0.64810924
 > qr.qy(qrx, diag(4))  ## multiply Q %*% I
            [,1]       [,2]        [,3]      [,4]
[1,] -0.6477697 -0.2925012 -0.68404548 0.1640710
[2,]  0.6645340 -0.3817050 -0.33452846 0.5484401
[3,] -0.1567635  0.7073773  0.01126533 0.6891413
[4,] -0.3379560 -0.5180364  0.64810924 0.4442730
 > qr.Q(qrx, complete = TRUE)
            [,1]       [,2]        [,3]      [,4]
[1,] -0.6477697 -0.2925012 -0.68404548 0.1640710
[2,]  0.6645340 -0.3817050 -0.33452846 0.5484401
[3,] -0.1567635  0.7073773  0.01126533 0.6891413
[4,] -0.3379560 -0.5180364  0.64810924 0.4442730



From sway at tanox.com  Wed Oct 13 19:52:34 2004
From: sway at tanox.com (Shawn Way)
Date: Wed, 13 Oct 2004 12:52:34 -0500
Subject: [R] Maps and plotting
Message-ID: <2DBF8A8E1A1AEE4AB3618AC4D6BF30888D0DFA@houston.tanox.net>

 At our facility we have multiple sample points that are sampled on any
given day.  What I would like to do is create a map of the facility with
the sample points (and point labels) and when we have out of
specification results, place a transparent dot over the area on the map.
As the number of OOS results builds up, I envision the dot getting
darker.

Are there any packages out there that can aid me in doing this?

Thanks,
Shawn Way, PE
Engineering Manager
 

________________________________

"Policies are many, Principles are few, Policies will change, Principles
never do." 
-John C. Maxwell



From ltorgo at liacc.up.pt  Wed Oct 13 21:03:03 2004
From: ltorgo at liacc.up.pt (Luis Torgo)
Date: Wed, 13 Oct 2004 19:03:03 +0000
Subject: [R] Statistical analysis of a large database
In-Reply-To: <Pine.LNX.4.44.0410120927450.6004-100000@gannet.stats>
References: <Pine.LNX.4.44.0410120927450.6004-100000@gannet.stats>
Message-ID: <1097694183.5740.96.camel@nassa.niaad.liacc.up.pt>

On Tue, 2004-10-12 at 08:36, Prof Brian Ripley wrote:
> > Lu??s Torgo, Data Mining with R. Learning by case
> > studies, Maggio 2003
> > http://www.liacc.up.pt/~ltorgo/DataMiningWithR/
> 
> Please note that that reference is not about large datasets, nor about 
> `data mining' in the generally used sense.  It has two studies, one 
> incomplete, on linear regression (with 200 samples) and on time series.

I would like to add a few information on these incomplete comments on
the book I'm writing. The book is unfinished as mentioned on its Web
page. It has currently two reasonably finished chapters: an introduction
to R and MySQL and a case study. As mentioned in the book, the first
case study is small by data mining standards (200 observations) and has
the goal of illustrating techniques that are shared by data mining and
other disciplines as well as smoothly introducing the reader to R and
its power. It addresses data pre-processing techniques, data
visualization, model construction (yes, linear regression but also
regression trees), and model evaluation, selection and combination, so I
think it is a bit incorrect to say that it is about linear regression
that corresponds to 5 of the 50 pages of that chapter.
 
The third (unfinished) chapter (2nd case study) is about financial
trading. It includes topics like connections to data bases as well as
many other components of a knowledge discovery process. Among those
components it includes model construction that involves obviously time
series models given the nature of the data. The chapter will include
other steps like issues concerning moving from predictions into actions,
creation of variables from the original time series, etc.. It is
currently being re-written and I expect to upload soon a new revised
version of this chapter.

The book will include at least two further cases studies that will be
larger. Still, I would note that the financial trading case study is
potentially very large, as it is a problem where data is constantly
growing. The final version of that chapter addresses this issue of
having a system that is online in the sense that it is receiving new
data in real time (also known as mining data streams in the data mining
field).

I'm sorry for being so long, but I think it is dangerous to try to
resume around 200 pages of an unfinished work in two lines of text.

Still, all comments on this on going project are very well welcome and I
would like to take this opportunity to thank all people that have been
sending me encouraging comments/emails.

Luis Torgo

-- 
Luis Torgo
  FEP/LIACC, University of Porto   Phone : (+351) 22 607 88 30
  Machine Learning Group           Fax   : (+351) 22 600 36 54
  R. Campo Alegre, 823             email : ltorgo at liacc.up.pt
  4150 PORTO   -  PORTUGAL         WWW   : http://www.liacc.up.pt/~ltorgo



From ripley at stats.ox.ac.uk  Wed Oct 13 20:26:57 2004
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Wed, 13 Oct 2004 19:26:57 +0100 (BST)
Subject: [R] Problems understanding qr.  family of functions
In-Reply-To: <416D5AA9.7070502@acelerate.com>
Message-ID: <Pine.LNX.4.44.0410131918440.14472-100000@gannet.stats>

On Wed, 13 Oct 2004, Kjetil Brinchmann Halvorsen wrote:

> Hola!
> 
> By my understanding of reading   ?qr
> the following shold result a 3 x 3 matrix:  (rw2000)
> 
>  x <- matrix( rnorm(12), 4,3)
>  y <- matrix( rnorm(12), 4,3)
>  xqr <- qr(x)
>  yqr <- qr(y)
> 
>  qr.qty( xqr, qr.Q(yqr) ) # dim (3,3)
>             [,1]       [,2]      [,3]
> [1,]  0.16815466 -0.1970936 0.2351670
> [2,]  0.15667444  0.4939800 0.8443340
> [3,] -0.97096971  0.1029652 0.1456355
> [4,] -0.06629443 -0.8405570 0.4588976
> 
>  # The following should be equal:
>  t( qr.Q(xqr) ) %*% qr.Q(yqr)
>            [,1]       [,2]      [,3]
> [1,]  0.1681547 -0.1970936 0.2351670
> [2,]  0.1566744  0.4939800 0.8443340
> [3,] -0.9709697  0.1029652 0.1456355
> 
> but evidently is not.
> 
> There is at least one bug:
> 1) in R
> 2) in the help page
> 3) in my reading of the help page

Probably 3).  The Q matrix for a 4x3 matrix is 4x4, but qr.Q only returns 
3 cols of it.  That is stated on ?qr.Q, a separate help page, and you 
needed complete=TRUE.


-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From Roger.Bivand at nhh.no  Wed Oct 13 20:40:48 2004
From: Roger.Bivand at nhh.no (Roger Bivand)
Date: Wed, 13 Oct 2004 20:40:48 +0200 (CEST)
Subject: [R] Maps and plotting
In-Reply-To: <2DBF8A8E1A1AEE4AB3618AC4D6BF30888D0DFA@houston.tanox.net>
Message-ID: <Pine.LNX.4.44.0410132034200.16386-100000@reclus.nhh.no>

On Wed, 13 Oct 2004, Shawn Way wrote:

>  At our facility we have multiple sample points that are sampled on any
> given day.  What I would like to do is create a map of the facility with
> the sample points (and point labels) and when we have out of
> specification results, place a transparent dot over the area on the map.
> As the number of OOS results builds up, I envision the dot getting
> darker.
> 
> Are there any packages out there that can aid me in doing this?

See R 2.0.0 NEWS: 

    o	It is now possible to specify colours with a full alpha
	transparency channel via the new 'alpha' argument to the
	rgb() and hsv() functions, or as a string of the form "#RRGGBBAA".

	NOTE: most devices draw nothing if a colour is not opaque,
	but PDF and Quartz devices will render semitransparent colours.

	A new argument 'alpha' to the function col2rgb()
	provides the ability to return the alpha component of
	colours (as well as the red, green, and blue components).

and output on the PDF device - works very nicely with the standard 
points() function and other graphics functions. For on-screen rendering, 
it looks as though only the Quartz device will do this, but PDF is 
cross-platform.

> 
> Thanks,
> Shawn Way, PE
> Engineering Manager
>  
> 
> ________________________________
> 
> "Policies are many, Principles are few, Policies will change, Principles
> never do." 
> -John C. Maxwell
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
> 

-- 
Roger Bivand
Economic Geography Section, Department of Economics, Norwegian School of
Economics and Business Administration, Breiviksveien 40, N-5045 Bergen,
Norway. voice: +47 55 95 93 55; fax +47 55 95 93 93
e-mail: Roger.Bivand at nhh.no



From maechler at stat.math.ethz.ch  Thu Oct 14 11:34:53 2004
From: maechler at stat.math.ethz.ch (Martin Maechler)
Date: Thu, 14 Oct 2004 11:34:53 +0200
Subject: [R] R-help problems?
In-Reply-To: <16750.16628.739377.564366@gargle.gargle.HOWL>
References: <47E7266B7FEB2E48B8F756760D9C6CC2188756@ex115.smic-sh.com>
	<16750.16628.739377.564366@gargle.gargle.HOWL>
Message-ID: <16750.18493.677976.854145@gargle.gargle.HOWL>

ETH had several small electric power outages last
evening and even though most big network problems have
been resolved by about midnight, we still have seen services not
working properly.

All mailing lists -- but R-help -- have seemingly worked fine
all the time.  This is also a test mail trying to confirm / investigate
the problem.

Martin Maechler
ETH Zurich



From B.Rowlingson at lancaster.ac.uk  Thu Oct 14 12:42:13 2004
From: B.Rowlingson at lancaster.ac.uk (Barry Rowlingson)
Date: Thu, 14 Oct 2004 11:42:13 +0100
Subject: [R] Maps and plotting
In-Reply-To: <2DBF8A8E1A1AEE4AB3618AC4D6BF30888D0DFA@houston.tanox.net>
References: <2DBF8A8E1A1AEE4AB3618AC4D6BF30888D0DFA@houston.tanox.net>
Message-ID: <416E5805.7020509@lancaster.ac.uk>

Shawn Way wrote:
>  At our facility we have multiple sample points that are sampled on any
> given day.  What I would like to do is create a map of the facility with
> the sample points (and point labels) and when we have out of
> specification results, place a transparent dot over the area on the map.
> As the number of OOS results builds up, I envision the dot getting
> darker.
> 

  Over what timescale? This sounds like it could be an interactive, 
real-time on-line monitoring thing. Is it?

  In which case R's graphics devices might not be good enough, and you'd 
be better off using a TclTk graphics canvas.

  library(tcltk) and read the docs!

  Another idea, if all you are doing is updating a daily image, would be 
to use a language like Python, and the Python Imaging Library (PIL) to 
draw pretty graphs.

  I've done something similar that produces daily maps of disease 
incidence, but I used different size and colour circles and not 
transparency, so I just used base R graphics and produced a PNG file. If 
I wanted transparency I'd probably use Python/PIL, which can handle 
alpha channels.


Baz



From ripley at stats.ox.ac.uk  Thu Oct 14 13:04:59 2004
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Thu, 14 Oct 2004 12:04:59 +0100 (BST)
Subject: [R] Maps and plotting
In-Reply-To: <416E5805.7020509@lancaster.ac.uk>
Message-ID: <Pine.LNX.4.44.0410141200160.13081-100000@gannet.stats>

On Thu, 14 Oct 2004, Barry Rowlingson wrote:

> Shawn Way wrote:
> >  At our facility we have multiple sample points that are sampled on any
> > given day.  What I would like to do is create a map of the facility with
> > the sample points (and point labels) and when we have out of
> > specification results, place a transparent dot over the area on the map.
> > As the number of OOS results builds up, I envision the dot getting
> > darker.
> > 
> 
>   Over what timescale? This sounds like it could be an interactive, 
> real-time on-line monitoring thing. Is it?
> 
>   In which case R's graphics devices might not be good enough, and you'd 
> be better off using a TclTk graphics canvas.
> 
>   library(tcltk) and read the docs!
> 
>   Another idea, if all you are doing is updating a daily image, would be 
> to use a language like Python, and the Python Imaging Library (PIL) to 
> draw pretty graphs.
> 
>   I've done something similar that produces daily maps of disease 
> incidence, but I used different size and colour circles and not 
> transparency, so I just used base R graphics and produced a PNG file. If 
> I wanted transparency I'd probably use Python/PIL, which can handle 
> alpha channels.

I think this may mean *translucent* dots.  R has been able to do 
transparent dots for a very long time, but PNG cannot handle 
translucency.  On devices (e.g. pdf) which can, you can do this as of R 
2.0.0.

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From sway at tanox.com  Thu Oct 14 14:13:10 2004
From: sway at tanox.com (Shawn Way)
Date: Thu, 14 Oct 2004 07:13:10 -0500
Subject: [R] Maps and plotting
Message-ID: <2DBF8A8E1A1AEE4AB3618AC4D6BF30888D0DFD@houston.tanox.net>

Thank you very much..  It seems that the points problem is fairly easy
to solve, I just need to work on the mapping..

Thanks again... 


Shawn Way, PE
Engineering Manager
sway at tanox.com

-----Original Message-----
From: Barry Rowlingson [mailto:B.Rowlingson at lancaster.ac.uk] 
Sent: Thursday, October 14, 2004 5:42 AM
To: Shawn Way
Cc: R-help at stat.math.ethz.ch
Subject: Re: [R] Maps and plotting

Shawn Way wrote:
>  At our facility we have multiple sample points that are sampled on 
> any given day.  What I would like to do is create a map of the 
> facility with the sample points (and point labels) and when we have 
> out of specification results, place a transparent dot over the area on
the map.
> As the number of OOS results builds up, I envision the dot getting 
> darker.
> 

  Over what timescale? This sounds like it could be an interactive,
real-time on-line monitoring thing. Is it?

  In which case R's graphics devices might not be good enough, and you'd
be better off using a TclTk graphics canvas.

  library(tcltk) and read the docs!

  Another idea, if all you are doing is updating a daily image, would be
to use a language like Python, and the Python Imaging Library (PIL) to
draw pretty graphs.

  I've done something similar that produces daily maps of disease
incidence, but I used different size and colour circles and not
transparency, so I just used base R graphics and produced a PNG file. If
I wanted transparency I'd probably use Python/PIL, which can handle
alpha channels.


Baz



From sway at tanox.com  Thu Oct 14 14:13:33 2004
From: sway at tanox.com (Shawn Way)
Date: Thu, 14 Oct 2004 07:13:33 -0500
Subject: FW: [R] Maps and plotting
Message-ID: <2DBF8A8E1A1AEE4AB3618AC4D6BF30888D0DFE@houston.tanox.net>


Thanks for the help on the translucent dots.  What would be the best
method for creating a map of the facility?  I looked into map* in the
libraries and didn't find anything on creating the maps, just using
them.

Thanks again... 


Shawn Way, PE
Engineering Manager
sway at tanox.com

-----Original Message-----
From: Prof Brian Ripley [mailto:ripley at stats.ox.ac.uk]
Sent: Thursday, October 14, 2004 6:05 AM
To: Barry Rowlingson
Cc: Shawn Way; R-help at stat.math.ethz.ch
Subject: Re: [R] Maps and plotting

On Thu, 14 Oct 2004, Barry Rowlingson wrote:

> Shawn Way wrote:
> >  At our facility we have multiple sample points that are sampled on 
> > any given day.  What I would like to do is create a map of the 
> > facility with the sample points (and point labels) and when we have 
> > out of specification results, place a transparent dot over the area
on the map.
> > As the number of OOS results builds up, I envision the dot getting 
> > darker.
> > 
> 
>   Over what timescale? This sounds like it could be an interactive, 
> real-time on-line monitoring thing. Is it?
> 
>   In which case R's graphics devices might not be good enough, and 
> you'd be better off using a TclTk graphics canvas.
> 
>   library(tcltk) and read the docs!
> 
>   Another idea, if all you are doing is updating a daily image, would 
> be to use a language like Python, and the Python Imaging Library (PIL)

> to draw pretty graphs.
> 
>   I've done something similar that produces daily maps of disease 
> incidence, but I used different size and colour circles and not 
> transparency, so I just used base R graphics and produced a PNG file.
> If I wanted transparency I'd probably use Python/PIL, which can handle

> alpha channels.

I think this may mean *translucent* dots.  R has been able to do
transparent dots for a very long time, but PNG cannot handle
translucency.  On devices (e.g. pdf) which can, you can do this as of R
2.0.0.

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From Stegmann at imbg.ku.dk  Thu Oct 14 14:37:01 2004
From: Stegmann at imbg.ku.dk (Anders Stegmann)
Date: Thu, 14 Oct 2004 14:37:01 +0200
Subject: [R] correlating between two vectors of numbers
Message-ID: <D65AE6BB3C6B534991F65E6DD4F56E4F13C991@biokemi.imbg.ku.dk>

Hi, R!



Question1:

I am trying to correlate two vectors of numbers (two columns of microarray
signal values) by using the non-parametric Spearman's rank correlation
coefficient rho:

> cor.test(V2.Signal,V3.Signal,method="spearman")

but I get the error message:

Error in if (q > (n^3 - n)/6) pspearman(q - 1, n, lower.tail = FALSE) else
pspearman(q,  : 
        missing value where logical needed
In addition: Warning message: 
NAs introduced by coercion 

I have tried to use the parametric Pearson correlation and the
non-parametric Kendall's tau correlation and had no problem with that!!

> cor(V2.Signal,V3.Signal,use="complete.obs")                        (the
Pearson correlation)

> cor.test(V2.Signal,V3.Signal,method="kendall")                     (the
Kendall's correlation)


what's wrong?



Question2: 

Does anyone accidently know which correlation method would be the most
correct to use when the microarray signal values (the values to be
correlated) are not normal distributed (the Kendall's method seem to fit
better to my other tests than the Pearson method).



From sdavis2 at mail.nih.gov  Thu Oct 14 14:53:24 2004
From: sdavis2 at mail.nih.gov (Sean Davis)
Date: Thu, 14 Oct 2004 08:53:24 -0400
Subject: [R] correlating between two vectors of numbers
In-Reply-To: <D65AE6BB3C6B534991F65E6DD4F56E4F13C991@biokemi.imbg.ku.dk>
References: <D65AE6BB3C6B534991F65E6DD4F56E4F13C991@biokemi.imbg.ku.dk>
Message-ID: <08CE5448-1DE0-11D9-B94A-000A95D7BA10@mail.nih.gov>

Anders,

Does your data have missing values?  It looks like they might.   Look 
at the 'use' parameter in cor.  Also, is there a reason to use cor.test 
instead of cor.  Finally, if the expression values are not normal, 
could you transform them first to make them more so--log2, for example? 
  And, no, no one has figured out the "best" way to define 
distances/correlations for microarray, at least to the best of my 
knowledge.

Sean

On Oct 14, 2004, at 8:37 AM, Anders Stegmann wrote:

> Hi, R!
>
>
>
> Question1:
>
> I am trying to correlate two vectors of numbers (two columns of 
> microarray
> signal values) by using the non-parametric Spearman's rank correlation
> coefficient rho:
>
>> cor.test(V2.Signal,V3.Signal,method="spearman")
>
> but I get the error message:
>
> Error in if (q > (n^3 - n)/6) pspearman(q - 1, n, lower.tail = FALSE) 
> else
> pspearman(q,  :
>         missing value where logical needed
> In addition: Warning message:
> NAs introduced by coercion
>
> I have tried to use the parametric Pearson correlation and the
> non-parametric Kendall's tau correlation and had no problem with that!!
>
>> cor(V2.Signal,V3.Signal,use="complete.obs")                        
>> (the
> Pearson correlation)
>
>> cor.test(V2.Signal,V3.Signal,method="kendall")                     
>> (the
> Kendall's correlation)
>
>
> what's wrong?
>
>
>
> Question2:
>
> Does anyone accidently know which correlation method would be the most
> correct to use when the microarray signal values (the values to be
> correlated) are not normal distributed (the Kendall's method seem to 
> fit
> better to my other tests than the Pearson method).
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html



From Roger.Bivand at nhh.no  Thu Oct 14 14:57:21 2004
From: Roger.Bivand at nhh.no (Roger Bivand)
Date: Thu, 14 Oct 2004 14:57:21 +0200 (CEST)
Subject: FW: [R] Maps and plotting
In-Reply-To: <2DBF8A8E1A1AEE4AB3618AC4D6BF30888D0DFE@houston.tanox.net>
Message-ID: <Pine.LNX.4.44.0410141453580.17091-100000@reclus.nhh.no>

On Thu, 14 Oct 2004, Shawn Way wrote:

> 
> Thanks for the help on the translucent dots.  What would be the best
> method for creating a map of the facility?  I looked into map* in the
> libraries and didn't find anything on creating the maps, just using
> them.
> 

Depends what you mean by "creating"? If you mean accessing map data, then 
you most likely will have to define for where (and maybe when), what 
scale, etc., and which format the map data files should be in. Like Prof. 
Ripley, I also thought your question was about translucency, that is using 
the alpha channel to darken overlapping symbols, which works very nicely 
for the PDF device in R 2.0.0.

> Thanks again... 
> 
> 
> Shawn Way, PE
> Engineering Manager
> sway at tanox.com
> 
> -----Original Message-----
> From: Prof Brian Ripley [mailto:ripley at stats.ox.ac.uk]
> Sent: Thursday, October 14, 2004 6:05 AM
> To: Barry Rowlingson
> Cc: Shawn Way; R-help at stat.math.ethz.ch
> Subject: Re: [R] Maps and plotting
> 
> On Thu, 14 Oct 2004, Barry Rowlingson wrote:
> 
> > Shawn Way wrote:
> > >  At our facility we have multiple sample points that are sampled on 
> > > any given day.  What I would like to do is create a map of the 
> > > facility with the sample points (and point labels) and when we have 
> > > out of specification results, place a transparent dot over the area
> on the map.
> > > As the number of OOS results builds up, I envision the dot getting 
> > > darker.
> > > 
> > 
> >   Over what timescale? This sounds like it could be an interactive, 
> > real-time on-line monitoring thing. Is it?
> > 
> >   In which case R's graphics devices might not be good enough, and 
> > you'd be better off using a TclTk graphics canvas.
> > 
> >   library(tcltk) and read the docs!
> > 
> >   Another idea, if all you are doing is updating a daily image, would 
> > be to use a language like Python, and the Python Imaging Library (PIL)
> 
> > to draw pretty graphs.
> > 
> >   I've done something similar that produces daily maps of disease 
> > incidence, but I used different size and colour circles and not 
> > transparency, so I just used base R graphics and produced a PNG file.
> > If I wanted transparency I'd probably use Python/PIL, which can handle
> 
> > alpha channels.
> 
> I think this may mean *translucent* dots.  R has been able to do
> transparent dots for a very long time, but PNG cannot handle
> translucency.  On devices (e.g. pdf) which can, you can do this as of R
> 2.0.0.
> 
> 

-- 
Roger Bivand
Economic Geography Section, Department of Economics, Norwegian School of
Economics and Business Administration, Breiviksveien 40, N-5045 Bergen,
Norway. voice: +47 55 95 93 55; fax +47 55 95 93 93
e-mail: Roger.Bivand at nhh.no



From Michael.Wolf at bezreg-muenster.nrw.de  Thu Oct 14 15:16:30 2004
From: Michael.Wolf at bezreg-muenster.nrw.de (Wolf, Michael)
Date: Thu, 14 Oct 2004 15:16:30 +0200
Subject: [R] Filling polygons with points
Message-ID: <9E00F1C36CEF614CA4AA6CC2587B594D3611E9@EXCHANGE2.harz.bezreg-muenster.nrw.de>

Dear list,

are there any possibilities to fill a polygon with a point pattern or with a symbol pattern like '+' oder '-' instead of shading lines?

Thanks in advance

Dr. Michael Wolf
Bezirksregierung M??nster
Dezernat 61
Domplatz 1-3    48161 M??nster
Tel.:   ++ 49 (02 51) / 4 11 - 17 95
Fax.:   ++ 49 (02 51) / 4 11 - 8 17 95
E-Mail: michael.wolf at bezreg-muenster.nrw.de



From Matthias.Kohl at uni-bayreuth.de  Thu Oct 14 15:19:49 2004
From: Matthias.Kohl at uni-bayreuth.de (Matthias.Kohl@uni-bayreuth.de)
Date: Thu, 14 Oct 2004 15:19:49 +0200 (MEST)
Subject: [R] setClassUnion
Message-ID: <1101.132.180.246.63.1097759989.squirrel@mail.uni-bayreuth.de>

Hello,

I have a question concerning "setClassUnion".
I'm working with R 2.0.0 Patched (2004-10-06) on windows 2000.

I tried to use "setClassUnion" in a package I am currently working on. The
situation is similar to the following example:

The DESCRIPTION file has entries:
  Depends: R (>= 2.0.0), methods
  Imports: methods
  LazyLoad: yes

The NAMESPACE file has entries:
  importClassesFrom("methods", "NULL", "numeric")
  exportClass("OptionalNumeric", "class1", "class2")

The example R code is:
.onLoad <- function(lib, pkg){
    require("methods", character = TRUE, quietly = TRUE)
}

setClassUnion("OptionalNumeric", c("numeric", "NULL"))

setClass("class1",
  representation(test1 = "OptionalNumeric"),
  prototype(test1 = numeric(1)))

# why does this not work?
# The error I get is:
# Error in makePrototypeFromClassDef(properties, ClassDef, immediate,
# where) :In making the prototype for class "class1" elements of the
# prototype failed to match the corresponding slot class: test1
# (class "OptionalNumeric ")
# Sourcing this into R gives no error for me

# but instead using
  prototype(test1 = NULL)
# works

# Moreover, using the second version (with test1 = NULL)
# the following works, too
setClass("class2",
  representation(test2 = "class1"),
  prototype(test2 = new("class1", test1 = numeric(1))))

What am I doing wrong?
Can someone please explain this to me?

Thanks for your help,
Matthias



From Matthias.Kohl at uni-bayreuth.de  Thu Oct 14 15:34:17 2004
From: Matthias.Kohl at uni-bayreuth.de (Matthias.Kohl@uni-bayreuth.de)
Date: Thu, 14 Oct 2004 15:34:17 +0200 (MEST)
Subject: [R] setClassUnion
In-Reply-To: <1101.132.180.246.63.1097759989.squirrel@mail.uni-bayreuth.de>
References: <1101.132.180.246.63.1097759989.squirrel@mail.uni-bayreuth.de>
Message-ID: <1130.132.180.246.66.1097760857.squirrel@mail.uni-bayreuth.de>

sorry, please replace exportClass by exportClasses

> Hello,
>
> I have a question concerning "setClassUnion".
> I'm working with R 2.0.0 Patched (2004-10-06) on windows 2000.
>
> I tried to use "setClassUnion" in a package I am currently working on.
> The situation is similar to the following example:
>
> The DESCRIPTION file has entries:
>   Depends: R (>= 2.0.0), methods
>   Imports: methods
>   LazyLoad: yes
>
> The NAMESPACE file has entries:
>   importClassesFrom("methods", "NULL", "numeric")
>   exportClass("OptionalNumeric", "class1", "class2")

    exportClasses("OptionalNumeric", "class1", "class2")

>
> The example R code is:
> .onLoad <- function(lib, pkg){
>     require("methods", character = TRUE, quietly = TRUE)
> }
>
> setClassUnion("OptionalNumeric", c("numeric", "NULL"))
>
> setClass("class1",
>   representation(test1 = "OptionalNumeric"),
>   prototype(test1 = numeric(1)))
>
> # why does this not work?
> # The error I get is:
> # Error in makePrototypeFromClassDef(properties, ClassDef, immediate, #
> where) :In making the prototype for class "class1" elements of the #
> prototype failed to match the corresponding slot class: test1
> # (class "OptionalNumeric ")
> # Sourcing this into R gives no error for me
>
> # but instead using
>   prototype(test1 = NULL)
> # works
>
> # Moreover, using the second version (with test1 = NULL)
> # the following works, too
> setClass("class2",
>   representation(test2 = "class1"),
>   prototype(test2 = new("class1", test1 = numeric(1))))
>
> What am I doing wrong?
> Can someone please explain this to me?
>
> Thanks for your help,
> Matthias
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide!
> http://www.R-project.org/posting-guide.html



From terry_mu at yahoo.com  Thu Oct 14 16:00:01 2004
From: terry_mu at yahoo.com (Terry Mu)
Date: Thu, 14 Oct 2004 07:00:01 -0700 (PDT)
Subject: [R] beginner questions: objects and scripts
Message-ID: <20041014140001.47950.qmail@web54304.mail.yahoo.com>

hi,

I got some questions on using R,

1. How do I check and edit definition of an object?

for example,

>x <- 1:100

then after a while I want to check how x is defined.

list(x) or whatever functions I know only list its
content, but I want to see its definition, without
scrolling up and down, and edit it like "fixing" a
function.

2. How to save my work in current session as a nice
script?

again, I want to save objects as they are defined, not
numbers, other than copy / paste. I tried dump(), etc.
In another word, how do you work with R?

Thanks,
Terry



From murdoch at stats.uwo.ca  Thu Oct 14 16:00:26 2004
From: murdoch at stats.uwo.ca (Duncan Murdoch)
Date: Thu, 14 Oct 2004 10:00:26 -0400
Subject: [R] Filling polygons with points
In-Reply-To: <9E00F1C36CEF614CA4AA6CC2587B594D3611E9@EXCHANGE2.harz.bezreg-muenster.nrw.de>
References: <9E00F1C36CEF614CA4AA6CC2587B594D3611E9@EXCHANGE2.harz.bezreg-muenster.nrw.de>
Message-ID: <n91tm05s35rlhb56k5dknken7j3kvvju6v@4ax.com>

On Thu, 14 Oct 2004 15:16:30 +0200, "Wolf, Michael"
<Michael.Wolf at bezreg-muenster.nrw.de> wrote :

>Dear list,
>
>are there any possibilities to fill a polygon with a point pattern or with a symbol pattern like '+' oder '-' instead of shading lines?

I don't think the graphics devices know how to do that, but you could
program it yourself:

Supposed interior(x,y) is a function that determines whether the
points (x,y) are in the polygon, then just create a grid of locations,
subset when interior(x,y) is true, and plot using pch='+'.

I made a quick search and was unable to find a general implementation
of the "interior" function for an arbitrary polygon; I'm a bit
surprised about that.  Hopefully someone else can point to one,
otherwise please write one, and document it and contribute it to R.
It's a relatively standard algorithm, and would be useful.

Duncan Murdoch



From bates at wisc.edu  Thu Oct 14 16:06:41 2004
From: bates at wisc.edu (Douglas Bates)
Date: Thu, 14 Oct 2004 16:06:41 +0200
Subject: [R] beginner questions: objects and scripts
In-Reply-To: <20041014140001.47950.qmail@web54304.mail.yahoo.com>
References: <20041014140001.47950.qmail@web54304.mail.yahoo.com>
Message-ID: <416E87F1.1050906@wisc.edu>

Terry Mu wrote:

>hi,
>
>I got some questions on using R,
>
>1. How do I check and edit definition of an object?
>
>for example,
>
>  
>
>>x <- 1:100
>>    
>>
>
>then after a while I want to check how x is defined.
>
>list(x) or whatever functions I know only list its
>content, but I want to see its definition, without
>scrolling up and down, and edit it like "fixing" a
>function.
>
>  
>
str(x)

gives a concise description of the structure of x

>2. How to save my work in current session as a nice
>script?
>
>again, I want to save objects as they are defined, not
>numbers, other than copy / paste. I tried dump(), etc.
>In another word, how do you work with R?
>  
>
Many people use the ESS (Emacs Speaks Statistics) package for Emacs or 
XEmacs for this.  Although starting with Emacs or XEmacs is not simple, 
John Fox and others have written introductory guides for Windows users.



From abunn at whrc.org  Thu Oct 14 16:24:23 2004
From: abunn at whrc.org (Andy Bunn)
Date: Thu, 14 Oct 2004 10:24:23 -0400
Subject: [R] Filling polygons with points
In-Reply-To: <n91tm05s35rlhb56k5dknken7j3kvvju6v@4ax.com>
Message-ID: <NEBBIPHDAMMOKDKPOFFIMEKECKAA.abunn@whrc.org>

> I made a quick search and was unable to find a general implementation
> of the "interior" function for an arbitrary polygon; I'm a bit
> surprised about that.  Hopefully someone else can point to one,
> otherwise please write one, and document it and contribute it to R.
> It's a relatively standard algorithm, and would be useful.

A good place to start might be with the pip function in splancs. That should
do it with a little fussing, I think.

require(splancs)
data(bodmin)
pts <- gen(bodmin$poly,5000)
pts.inside <- pip(pts, bodmin$poly)
plot(bodmin$poly, asp=1, type="n")
pointmap(pts.inside, add=TRUE)
polymap(bodmin$poly, add=TRUE)

-Andy



From petr.pikal at precheza.cz  Thu Oct 14 16:37:28 2004
From: petr.pikal at precheza.cz (Petr Pikal)
Date: Thu, 14 Oct 2004 16:37:28 +0200
Subject: [R] Filling polygons with points
In-Reply-To: <n91tm05s35rlhb56k5dknken7j3kvvju6v@4ax.com>
References: <9E00F1C36CEF614CA4AA6CC2587B594D3611E9@EXCHANGE2.harz.bezreg-muenster.nrw.de>
Message-ID: <416EAB48.29841.1335A8E@localhost>



On 14 Oct 2004 at 10:00, Duncan Murdoch wrote:

> On Thu, 14 Oct 2004 15:16:30 +0200, "Wolf, Michael"
> <Michael.Wolf at bezreg-muenster.nrw.de> wrote :
> 
> >Dear list,
> >
> >are there any possibilities to fill a polygon with a point pattern or
> >with a symbol pattern like '+' oder '-' instead of shading lines?
> 
> I don't think the graphics devices know how to do that, but you could
> program it yourself:
> 
> Supposed interior(x,y) is a function that determines whether the
> points (x,y) are in the polygon, then just create a grid of locations,
> subset when interior(x,y) is true, and plot using pch='+'.
> 
> I made a quick search and was unable to find a general implementation
> of the "interior" function for an arbitrary polygon; I'm a bit
> surprised about that.  Hopefully someone else can point to one,

Hi

I think that you can find such functionity in splancs package 
probably function pip() find if points are inside or outside of the 
polygon.

Cheers
Petr



> otherwise please write one, and document it and contribute it to R.
> It's a relatively standard algorithm, and would be useful.
> 
> Duncan Murdoch
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide!
> http://www.R-project.org/posting-guide.html

Petr Pikal
petr.pikal at precheza.cz



From murdoch at stats.uwo.ca  Thu Oct 14 16:44:49 2004
From: murdoch at stats.uwo.ca (Duncan Murdoch)
Date: Thu, 14 Oct 2004 10:44:49 -0400
Subject: [R] beginner questions: objects and scripts
In-Reply-To: <20041014140001.47950.qmail@web54304.mail.yahoo.com>
References: <20041014140001.47950.qmail@web54304.mail.yahoo.com>
Message-ID: <5t2tm09tm1cvjlh5befa8gamqku822a6vt@4ax.com>

On Thu, 14 Oct 2004 07:00:01 -0700 (PDT), Terry Mu
<terry_mu at yahoo.com> wrote :

>hi,
>
>I got some questions on using R,
>
>1. How do I check and edit definition of an object?
>
>for example,
>
>>x <- 1:100
>
>then after a while I want to check how x is defined.
>
>list(x) or whatever functions I know only list its
>content, but I want to see its definition, without
>scrolling up and down, and edit it like "fixing" a
>function.

You can use history() to see the past history of commands, but there's
no automatic way to find the command that produced x.

>2. How to save my work in current session as a nice
>script?
>
>again, I want to save objects as they are defined, not
>numbers, other than copy / paste. I tried dump(), etc.
>In another word, how do you work with R?

history() is very good for that.  

Duncan Murdoch



From Mike.Prager at noaa.gov  Thu Oct 14 16:45:38 2004
From: Mike.Prager at noaa.gov (Mike Prager)
Date: Thu, 14 Oct 2004 10:45:38 -0400
Subject: [R] beginner questions: objects and scripts
In-Reply-To: <416E87F1.1050906@wisc.edu>
References: <20041014140001.47950.qmail@web54304.mail.yahoo.com>
	<416E87F1.1050906@wisc.edu>
Message-ID: <6.1.2.0.2.20041014103602.01f71dc8@hermes.nos.noaa.gov>


>>[...]How to save my work in current session as a nice
>>script?
>>
>>again, I want to save objects as they are defined, not
>>numbers, other than copy / paste. I tried dump(), etc.
>>In another word, how do you work with R?[...]
>
>Many people use the ESS (Emacs Speaks Statistics) package for Emacs or 
>XEmacs for this.[...]


I would add that (under Windows, and I would assume under other OS) you can 
use any programming editor to develop the R commands, and then copy and 
paste them into the R session.  Under Windows, the "Tinn-R" editor is much 
simpler than Emacs, and has some useful features for R users: line 
numbering, syntax coloring, and the ability to submit all or part of the 
edited text to R easily. http://tinn.solarvoid.com/



-- 
Michael Prager
NOAA Center for Coastal Fisheries and Habitat Research
Beaufort, North Carolina  28516  USA
http://shrimp.ccfhrb.noaa.gov/~mprager/

NOTE: Opinions expressed are personal, not official. No government
endorsement of any product is made or implied.



From Roger.Bivand at nhh.no  Thu Oct 14 16:55:33 2004
From: Roger.Bivand at nhh.no (Roger Bivand)
Date: Thu, 14 Oct 2004 16:55:33 +0200 (CEST)
Subject: [R] Filling polygons with points
In-Reply-To: <416EAB48.29841.1335A8E@localhost>
Message-ID: <Pine.LNX.4.44.0410141647480.17262-100000@reclus.nhh.no>

On Thu, 14 Oct 2004, Petr Pikal wrote:

> 
> 
> On 14 Oct 2004 at 10:00, Duncan Murdoch wrote:
> 
> > On Thu, 14 Oct 2004 15:16:30 +0200, "Wolf, Michael"
> > <Michael.Wolf at bezreg-muenster.nrw.de> wrote :
> > 
> > >Dear list,
> > >
> > >are there any possibilities to fill a polygon with a point pattern or
> > >with a symbol pattern like '+' oder '-' instead of shading lines?
> > 
> > I don't think the graphics devices know how to do that, but you could
> > program it yourself:
> > 
> > Supposed interior(x,y) is a function that determines whether the
> > points (x,y) are in the polygon, then just create a grid of locations,
> > subset when interior(x,y) is true, and plot using pch='+'.
> > 
> > I made a quick search and was unable to find a general implementation
> > of the "interior" function for an arbitrary polygon; I'm a bit
> > surprised about that.  Hopefully someone else can point to one,
> 
> Hi
> 
> I think that you can find such functionity in splancs package 
> probably function pip() find if points are inside or outside of the 
> polygon.
> 
> Cheers
> Petr
> 

Going a bit further, you could use areapl() and gridpts() in splancs to 
return plotting points for the symbols that would give an even symbol per 
unit area coverage. This is a very similar question to:

https://stat.ethz.ch/pipermail/r-help/2004-September/056351.html

which was about dot density maps. A function to do that will be in the 
next maptools, but I think this is a bit different and is more like a 
hatching but with arbitrary symbols and chosen spacing - it can be done, 
but do you need this kind of symbology?

Roger

> 
> 
> > otherwise please write one, and document it and contribute it to R.
> > It's a relatively standard algorithm, and would be useful.
> > 
> > Duncan Murdoch
> > 
> > ______________________________________________
> > R-help at stat.math.ethz.ch mailing list
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide!
> > http://www.R-project.org/posting-guide.html
> 
> Petr Pikal
> petr.pikal at precheza.cz
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
> 

-- 
Roger Bivand
Economic Geography Section, Department of Economics, Norwegian School of
Economics and Business Administration, Breiviksveien 40, N-5045 Bergen,
Norway. voice: +47 55 95 93 55; fax +47 55 95 93 93
e-mail: Roger.Bivand at nhh.no



From laura.dow at enscitech.com  Thu Oct 14 17:01:41 2004
From: laura.dow at enscitech.com (laura.dow@enscitech.com)
Date: Thu, 14 Oct 2004 16:01:41 +0100
Subject: [R] R and Java
Message-ID: <E1CI775-0007i7-Uf@hydrus.concept2100.co.uk>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20041014/132731dc/attachment.pl

From harris41 at msu.edu  Thu Oct 14 17:13:16 2004
From: harris41 at msu.edu (Scott Harrison)
Date: Thu, 14 Oct 2004 11:13:16 -0400
Subject: [R] fidelity of generated raster images (R and perl)
Message-ID: <416E978C.6090501@msu.edu>

Hi:

Goal: use R to turn a matrix of 1's and 0's
          into a corresponding image (e.g. png)
          of black and white pixels.

Why R: Yes, I can do this more efficiently and precisely
            with a perl module like Image::PBM.  Been there,
            done that many times, etc.  (Just humor me.
            I'm trying to do this with R for a number of reasons.)

Problem: Difficult to get a perfect rasterization.  There can
                be appended or removed pixel columns or pixel rows
                depending on plot region dimensions.  I witness this
                with both R version 1.8.1 and R version 2.0.

print($out "bitmap('/usr/local/mycrow/tmp/out.png', type = 'png256', 
height = ".(int($height*68/64)/64).", width = 
".(int($width*69/64)/64).", res = 64, pointsize=0)\n");
print($out 'par(mar=c(0,0,0,0))'."\n");
my $width1 = $width-1;
my $height1 = $height-1;
print($out <<END);
plot.new()
plot.window(c(0,$width1),c(0,$height1))
rect(m[,1], m[,2], m[,1], m[,2], col="black", border="black")

There are alternatives to rect (plot with type="p", pch=".", etc)
and I have also tried png() instead of bitmap().  (I do prefer
bitmap so this can run without x11.)

I am guessing that R's internal region calculations are
vector based, which generally makes sense for most statistical
plots.  However, I do have some ideas for R and the presentation
of cellular automata results.

Any tips out there?  (Is it just a matter of height=50px to
overcome the inches default, etc?).....

Regards,
Scott



From jlall5 at hotmail.com  Thu Oct 14 17:34:00 2004
From: jlall5 at hotmail.com (Jean-Luc aLLARD)
Date: Thu, 14 Oct 2004 15:34:00 +0000
Subject: [R] (no subject)
Message-ID: <BAY13-F283SU0hXCRQc00044e49@hotmail.com>

Hi,


Why this code is good in S-plus, and don't good in R ?

         getMethod("[","matrix")

thanks



From henric.nilsson at statisticon.se  Thu Oct 14 17:40:00 2004
From: henric.nilsson at statisticon.se (Henric Nilsson)
Date: Thu, 14 Oct 2004 17:40:00 +0200
Subject: [R] xyplot gets overplotted by plot
Message-ID: <6.1.2.0.0.20041014170501.0aeac118@10.0.10.66>

Dear all,

After having set par(mfrow = c(2, 2)) and switching between plot() and 
xyplot() using R 2.0.0 Patched (2004-10-13) under Windows 2000, the lattice 
plot gets overplotted. I also tried this under 1.9.1 Patched (2004-09-22), 
since this is the only older version I've got installed, and it did the 
same thing apart from that the overplotting now starts in the opposite 
lower corner.

I'm having a hard time believing that this is the intended behaviour, but 
I've been proved wrong before... Try

library(lattice)
y <- x <- 1:10
par(mfrow = c(2, 2))
# plots on a new page
plot(y ~ x)
# plots on a new page
xyplot(y ~ x)
# overplotting occurs
plot(y ~ x)

for a reproducible example.

Best wishes,
Henric



From Roger.Bivand at nhh.no  Thu Oct 14 17:56:41 2004
From: Roger.Bivand at nhh.no (Roger Bivand)
Date: Thu, 14 Oct 2004 17:56:41 +0200 (CEST)
Subject: [R] fidelity of generated raster images (R and perl)
In-Reply-To: <416E978C.6090501@msu.edu>
Message-ID: <Pine.LNX.4.44.0410141744240.17262-100000@reclus.nhh.no>

On Thu, 14 Oct 2004, Scott Harrison wrote:

> Hi:
> 
> Goal: use R to turn a matrix of 1's and 0's
>           into a corresponding image (e.g. png)
>           of black and white pixels.

Maybe use the write.pnm() function in the pixmap package:

> try1 <- matrix(0, nrow=100, ncol=100)
> try1[upper.tri(try1)] <- 1
> write.pnm(pixmapGrey(try1), file="~/tmp/try1.pbm", type="pbm")

should do it (but as an ASCII file, bits are not so easy to do one by 
one). You need to watch which row order is used if the matrix and image 
row orders are to "look" the same. This is assuming that you don't 
actually need to use a graphics device, but have an R matrix object you 
need to represent. 

Another package you could explore is rimage, which looks relevant.

Roger

> 
> Why R: Yes, I can do this more efficiently and precisely
>             with a perl module like Image::PBM.  Been there,
>             done that many times, etc.  (Just humor me.
>             I'm trying to do this with R for a number of reasons.)
> 
> Problem: Difficult to get a perfect rasterization.  There can
>                 be appended or removed pixel columns or pixel rows
>                 depending on plot region dimensions.  I witness this
>                 with both R version 1.8.1 and R version 2.0.
> 
> print($out "bitmap('/usr/local/mycrow/tmp/out.png', type = 'png256', 
> height = ".(int($height*68/64)/64).", width = 
> ".(int($width*69/64)/64).", res = 64, pointsize=0)\n");
> print($out 'par(mar=c(0,0,0,0))'."\n");
> my $width1 = $width-1;
> my $height1 = $height-1;
> print($out <<END);
> plot.new()
> plot.window(c(0,$width1),c(0,$height1))
> rect(m[,1], m[,2], m[,1], m[,2], col="black", border="black")
> 
> There are alternatives to rect (plot with type="p", pch=".", etc)
> and I have also tried png() instead of bitmap().  (I do prefer
> bitmap so this can run without x11.)
> 
> I am guessing that R's internal region calculations are
> vector based, which generally makes sense for most statistical
> plots.  However, I do have some ideas for R and the presentation
> of cellular automata results.
> 
> Any tips out there?  (Is it just a matter of height=50px to
> overcome the inches default, etc?).....
> 
> Regards,
> Scott
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
> 

-- 
Roger Bivand
Economic Geography Section, Department of Economics, Norwegian School of
Economics and Business Administration, Breiviksveien 40, N-5045 Bergen,
Norway. voice: +47 55 95 93 55; fax +47 55 95 93 93
e-mail: Roger.Bivand at nhh.no



From maechler at stat.math.ethz.ch  Thu Oct 14 18:08:44 2004
From: maechler at stat.math.ethz.ch (Martin Maechler)
Date: Thu, 14 Oct 2004 18:08:44 +0200
Subject: [R] fidelity of generated raster images (R and perl)
In-Reply-To: <416E978C.6090501@msu.edu>
References: <416E978C.6090501@msu.edu>
Message-ID: <16750.42124.439056.199902@gargle.gargle.HOWL>

>>>>> "Scott" == Scott Harrison <harris41 at msu.edu>
>>>>>     on Thu, 14 Oct 2004 11:13:16 -0400 writes:

    Scott> Hi: Goal: use R to turn a matrix of 1's and 0's into
    Scott> a corresponding image (e.g. png) of black and white
    Scott> pixels.

    Scott> Why R: Yes, I can do this more efficiently and
    Scott> precisely with a perl module like Image::PBM.  Been
    Scott> there, done that many times, etc.  (Just humor me.
    Scott> I'm trying to do this with R for a number of
    Scott> reasons.)

    Scott> Problem: Difficult to get a perfect rasterization.
    Scott> There can be appended or removed pixel columns or
    Scott> pixel rows depending on plot region dimensions.  I
    Scott> witness this with both R version 1.8.1 and R version
    Scott> 2.0.

 <....>

    Scott> There are alternatives to rect (plot with type="p",
    Scott> pch=".", etc) and I have also tried png() instead of
    Scott> bitmap().  (I do prefer bitmap so this can run
    Scott> without x11.)

I think the "most typical" alternative is to use  image()
which is probably the most efficient currently --- it is used by
the plot() method of "pixmap" objects in package 'pixmap' {which
you probably should really look at!}.

Note however that image() is still very inefficient for
high-resolution (already 1000x1000) images since it really fills
a rectangular polygon for each pixel.  This e.g. leads to
horribly large postscript files when plotting such pixmaps.

In some way, this is a known "missing feature" in R's graphics
engines.  However a nice implementation would probably use
properties of the graphics device:

A smartImage() function would ``see'' that for a given output
device, it should only draw pixels instead of rects(); even
smarter for even larger pixmaps (where there are less "pixels"
on the output device than you have in your image) it would even average
("color-smooth") neighboring pixmap pixels into device pixels.


    Scott> I am guessing that R's internal region calculations
    Scott> are vector based, which generally makes sense for
    Scott> most statistical plots.  However, I do have some
    Scott> ideas for R and the presentation of cellular automata
    Scott> results.

    Scott> Any tips out there?  (Is it just a matter of
    Scott> height=50px to overcome the inches default,
    Scott> etc?).....

    Scott> Regards, Scott

Regards,
Martin Maechler



From ripley at stats.ox.ac.uk  Thu Oct 14 18:09:01 2004
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Thu, 14 Oct 2004 17:09:01 +0100 (BST)
Subject: [R] xyplot gets overplotted by plot
In-Reply-To: <6.1.2.0.0.20041014170501.0aeac118@10.0.10.66>
Message-ID: <Pine.LNX.4.44.0410141659230.30536-100000@gannet.stats>

On Thu, 14 Oct 2004, Henric Nilsson wrote:

> After having set par(mfrow = c(2, 2)) and switching between plot() and 
> xyplot() using R 2.0.0 Patched (2004-10-13) under Windows 2000, the lattice 
> plot gets overplotted. I also tried this under 1.9.1 Patched (2004-09-22), 
> since this is the only older version I've got installed, and it did the 
> same thing apart from that the overplotting now starts in the opposite 
> lower corner.

I think that was a bug in 1.9.1.

> I'm having a hard time believing that this is the intended behaviour, but 
> I've been proved wrong before... Try

I think you are wrong.  You can't expect base graphics to know that you
have used a grid plot (via lattice) after the last base graphics plot and
did not intend to write on top of the latter.  Base graphics has been told
to write on the existing page of the device, in the upper right figure
region of an already divided device region.  So it does.

> library(lattice)
> y <- x <- 1:10
> par(mfrow = c(2, 2))
> # plots on a new page
> plot(y ~ x)
> # plots on a new page
> xyplot(y ~ x)
> # overplotting occurs
> plot(y ~ x)
> 
> for a reproducible example.

Why do you want to mix base and lattice graphics on the same device?
Think of it as a bonus that it works at all (it has not always, and base 
and Trellis interwork less well in S-PLUS).

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From Mike.Prager at noaa.gov  Thu Oct 14 18:18:46 2004
From: Mike.Prager at noaa.gov (Mike Prager)
Date: Thu, 14 Oct 2004 12:18:46 -0400
Subject: [R] beginner questions: objects and scripts
In-Reply-To: <6.1.2.0.2.20041014103602.01f71dc8@hermes.nos.noaa.gov>
References: <20041014140001.47950.qmail@web54304.mail.yahoo.com>
	<416E87F1.1050906@wisc.edu>
	<6.1.2.0.2.20041014103602.01f71dc8@hermes.nos.noaa.gov>
Message-ID: <6.1.2.0.2.20041014121756.0433c878@hermes.nos.noaa.gov>

At 10/14/2004 10:45 AM Thursday, you wrote:

>>>[...]How to save my work in current session as a nice
>>>script?
>>>
>>>again, I want to save objects as they are defined, not
>>>numbers, other than copy / paste. I tried dump(), etc.
>>>In another word, how do you work with R?[...]
>>
>>Many people use the ESS (Emacs Speaks Statistics) package for Emacs or 
>>XEmacs for this.[...]
>
>
>I would add that (under Windows, and I would assume under other OS) you 
>can use any programming editor to develop the R commands, and then copy 
>and paste them into the R session.  Under Windows, the "Tinn-R" editor is 
>much simpler than Emacs, and has some useful features for R users: line 
>numbering, syntax coloring, and the ability to submit all or part of the 
>edited text to R easily. http://tinn.solarvoid.com/

Oops!  Correct URL is  http://www.sciviews.org/Tinn-R/

MHP



NOTE: Opinions expressed are personal, not official. No government
endorsement of any product is made or implied.



From rpeng at jhsph.edu  Thu Oct 14 18:54:03 2004
From: rpeng at jhsph.edu (Roger D. Peng)
Date: Thu, 14 Oct 2004 12:54:03 -0400
Subject: [R] xyplot gets overplotted by plot
In-Reply-To: <6.1.2.0.0.20041014170501.0aeac118@10.0.10.66>
References: <6.1.2.0.0.20041014170501.0aeac118@10.0.10.66>
Message-ID: <416EAF2B.40802@jhsph.edu>

Mixing base and lattice graphics, in general, does not work.  If you 
really want to do something along these lines, look at the `gridBase' 
package on CRAN.

-roger

Henric Nilsson wrote:
> Dear all,
> 
> After having set par(mfrow = c(2, 2)) and switching between plot() and 
> xyplot() using R 2.0.0 Patched (2004-10-13) under Windows 2000, the 
> lattice plot gets overplotted. I also tried this under 1.9.1 Patched 
> (2004-09-22), since this is the only older version I've got installed, 
> and it did the same thing apart from that the overplotting now starts in 
> the opposite lower corner.
> 
> I'm having a hard time believing that this is the intended behaviour, 
> but I've been proved wrong before... Try
> 
> library(lattice)
> y <- x <- 1:10
> par(mfrow = c(2, 2))
> # plots on a new page
> plot(y ~ x)
> # plots on a new page
> xyplot(y ~ x)
> # overplotting occurs
> plot(y ~ x)
> 
> for a reproducible example.
> 
> Best wishes,
> Henric
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
>



From hb at maths.lth.se  Thu Oct 14 19:20:25 2004
From: hb at maths.lth.se (Henrik Bengtsson)
Date: Thu, 14 Oct 2004 19:20:25 +0200
Subject: [R] fidelity of generated raster images (R and perl)
In-Reply-To: <16750.42124.439056.199902@gargle.gargle.HOWL>
Message-ID: <000301c4b212$188bff40$1bf1ba51@hblaptop>

Hi. To add to Roger's pixmap format and Martin's image() replies, I played
with such problems a while ago and implemented a few metods and classes for
this. Install my R.classes bundle
(http://www.maths.lth.se/help/R/R.classes/) and try the MonochromeImage
class in the R.graphics package. R.classes is still not compatible with
2.0.0 (minor modification to DESCRIPTION etc are needed) so you have use R
v1.9.1 to try the below. Example:

library(R.graphics)
bits <- matrix(c(
 0,0,0,0,0,0,0,
 0,1,1,1,1,0,0,
 0,1,0,0,0,1,0,
 0,1,0,0,0,1,0,
 0,1,1,1,1,0,0,
 0,1,0,0,1,0,0,
 0,1,0,0,0,1,0,
 0,0,0,0,0,0,0
), nrow=8, byrow=TRUE)
img <- MonochromeImage(1-bits) # 1=white, 0=black
image(img)
plot(img)
persp(img, phi=60, theta=150)
write(img, "foo.pbm")  # Write directly
write(img, "foo.png")  # Write via png() and image()
write(img, "foo.jpg")
...

The method for PBM (monochrome pixmap) will write to file directly without
using graphical devices. You can either write to binary or ASCII format.
Here is the code I use to write to PBM:

'this' is basically a list structure with the matrix element 'gray' (equals
'bits' in the above example).

writePBM.MonochromeImage <- function (this, filename, ascii = FALSE) {
    open <- if (ascii == TRUE) 
      "w"
    else "wb"
    fout <- file(filename, open = open)
    on.exit(close(fout))
    magicNumber <- if (ascii == TRUE) 
        "P1"
    else "P4"
    cat(file = fout, magicNumber, "\n", sep = "")
    cat(file = fout, "# Creator: Class MonochromeImage in [R] package
R.graphics by Henrik Bengtsson. http://www.braju.com/R/\n", 
        sep = "")
    width <- ncol(this$gray)
    height <- nrow(this$gray)
    cat(file = fout, width, " ", height, "\n", sep = "")
    if (ascii == TRUE) {
        for (row in seq(length = height)) cat(file = fout, 
            1 - this$gray[row, ], sep = "\n")
    }
    else {
        if (width%%8 == 0) 
            w <- width
        else w <- width + (8 - (width%%8))
        x <- matrix(0, nrow = height, ncol = w)
        x[1:height, 1:width] <- this$gray
        x <- as.vector(t(x))
        x <- as.integer((x == 0))
        x <- matrix(x, nrow = 8, byrow = FALSE)
        x <- (2^(7:0)) * x
        x <- colSums(x)
        x <- as.integer(x)
        writeBin(x, size = 1, con = fout)
    }
}

The manipulation of x, e.g. t(x), can most likely be optimized by for
instance writing in blocks and so on. The bits-to-(0,255) convertion is
probably not optimal either.

The other write method utilizing png() and image() seems to be broken for
gray and monochrome images right now, but works for color images. I'll fix
this for the R v2.0.0 release. Workaround for now: write(as.RGBImage(img),
"foo.png"). However, I agree with Martin that image() might be too
inefficient for this so I would suggest to write to PBM and then use an
external converted, say, ImageMagick. For more info to use image() directly
see example at http://www.maths.lth.se/help/R/image/.

Hope this helps.

Cheers

Henrik Bengtsson



> -----Original Message-----
> From: r-help-bounces at stat.math.ethz.ch 
> [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Martin Maechler
> Sent: Thursday, October 14, 2004 6:09 PM
> To: Scott Harrison
> Cc: r-help at stat.math.ethz.ch
> Subject: Re: [R] fidelity of generated raster images (R and perl)
> 
> 
> >>>>> "Scott" == Scott Harrison <harris41 at msu.edu>
> >>>>>     on Thu, 14 Oct 2004 11:13:16 -0400 writes:
> 
>     Scott> Hi: Goal: use R to turn a matrix of 1's and 0's into
>     Scott> a corresponding image (e.g. png) of black and white
>     Scott> pixels.
> 
>     Scott> Why R: Yes, I can do this more efficiently and
>     Scott> precisely with a perl module like Image::PBM.  Been
>     Scott> there, done that many times, etc.  (Just humor me.
>     Scott> I'm trying to do this with R for a number of
>     Scott> reasons.)
> 
>     Scott> Problem: Difficult to get a perfect rasterization.
>     Scott> There can be appended or removed pixel columns or
>     Scott> pixel rows depending on plot region dimensions.  I
>     Scott> witness this with both R version 1.8.1 and R version
>     Scott> 2.0.
> 
>  <....>
> 
>     Scott> There are alternatives to rect (plot with type="p",
>     Scott> pch=".", etc) and I have also tried png() instead of
>     Scott> bitmap().  (I do prefer bitmap so this can run
>     Scott> without x11.)
> 
> I think the "most typical" alternative is to use  image()
> which is probably the most efficient currently --- it is used 
> by the plot() method of "pixmap" objects in package 'pixmap' 
> {which you probably should really look at!}.
> 
> Note however that image() is still very inefficient for 
> high-resolution (already 1000x1000) images since it really 
> fills a rectangular polygon for each pixel.  This e.g. leads 
> to horribly large postscript files when plotting such pixmaps.
> 
> In some way, this is a known "missing feature" in R's 
> graphics engines.  However a nice implementation would 
> probably use properties of the graphics device:
> 
> A smartImage() function would ``see'' that for a given output 
> device, it should only draw pixels instead of rects(); even 
> smarter for even larger pixmaps (where there are less 
> "pixels" on the output device than you have in your image) it 
> would even average
> ("color-smooth") neighboring pixmap pixels into device pixels.
> 
> 
>     Scott> I am guessing that R's internal region calculations
>     Scott> are vector based, which generally makes sense for
>     Scott> most statistical plots.  However, I do have some
>     Scott> ideas for R and the presentation of cellular automata
>     Scott> results.
> 
>     Scott> Any tips out there?  (Is it just a matter of
>     Scott> height=50px to overcome the inches default,
>     Scott> etc?).....
> 
>     Scott> Regards, Scott
> 
> Regards,
> Martin Maechler
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list 
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
>



From henric.nilsson at statisticon.se  Thu Oct 14 19:25:40 2004
From: henric.nilsson at statisticon.se (Henric Nilsson)
Date: Thu, 14 Oct 2004 19:25:40 +0200
Subject: [R] xyplot gets overplotted by plot
In-Reply-To: <Pine.LNX.4.44.0410141659230.30536-100000@gannet.stats>
References: <6.1.2.0.0.20041014170501.0aeac118@10.0.10.66>
	<Pine.LNX.4.44.0410141659230.30536-100000@gannet.stats>
Message-ID: <6.1.2.0.0.20041014183335.0b097088@10.0.10.66>

At 17:09 2004-10-14 +0100, Prof Brian Ripley wrote:

>You can't expect base graphics to know that you
>have used a grid plot (via lattice) after the last base graphics plot and
>did not intend to write on top of the latter.  Base graphics has been told
>to write on the existing page of the device, in the upper right figure
>region of an already divided device region.  So it does.

OK. I got fooled by that it seems to 'work' if we repeatedly just switch 
between xyplot and plot, i.e. without altering the par setting. But is that 
just due to everything on the device getting overwritten? I assumed that 
every call to plot erased the device before plotting.

>Why do you want to mix base and lattice graphics on the same device?

Convenience (if you define mixing as being able to direct all output from 
both graphic systems to a single window without risking one getting 
overlayed by the other).

>Think of it as a bonus that it works at all (it has not always, and base
>and Trellis interwork less well in S-PLUS).

I think of whole of R as a bonus. Thanks for your reply!

Henric



From ripley at stats.ox.ac.uk  Thu Oct 14 19:33:49 2004
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Thu, 14 Oct 2004 18:33:49 +0100 (BST)
Subject: [R] xyplot gets overplotted by plot
In-Reply-To: <6.1.2.0.0.20041014183335.0b097088@10.0.10.66>
Message-ID: <Pine.LNX.4.44.0410141831280.1876-100000@gannet.stats>

On Thu, 14 Oct 2004, Henric Nilsson wrote:

> At 17:09 2004-10-14 +0100, Prof Brian Ripley wrote:
> 
> >You can't expect base graphics to know that you
> >have used a grid plot (via lattice) after the last base graphics plot and
> >did not intend to write on top of the latter.  Base graphics has been told
> >to write on the existing page of the device, in the upper right figure
> >region of an already divided device region.  So it does.
> 
> OK. I got fooled by that it seems to 'work' if we repeatedly just switch 
> between xyplot and plot, i.e. without altering the par setting. But is that 
> just due to everything on the device getting overwritten? I assumed that 
> every call to plot erased the device before plotting.

Not if you partition the device (by par(mfrow/mfcol), layout, 
split.screen, etc).

> >Why do you want to mix base and lattice graphics on the same device?
> 
> Convenience (if you define mixing as being able to direct all output from 
> both graphic systems to a single window without risking one getting 
> overlayed by the other).

Yes.  I use separate devices for base and lattice graphics: it is easy to 
switch between them using dev.*.  That's habit from when you had too.

> >Think of it as a bonus that it works at all (it has not always, and base
> >and Trellis interwork less well in S-PLUS).
> 
> I think of whole of R as a bonus. Thanks for your reply!
> 
> Henric 
> 
> 

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From r.ghezzo at staff.mcgill.ca  Thu Oct 14 19:34:02 2004
From: r.ghezzo at staff.mcgill.ca (r.ghezzo@staff.mcgill.ca)
Date: Thu, 14 Oct 2004 13:34:02 -0400
Subject: [R] problems compiling packages in R 2.0.0
Message-ID: <1097775242.416eb88ab6fb7@webmail.mcgill.ca>

Hello, I am trying to get my old packages to work in R 2.0.0
in Windows XP. Here is what I did
Etc is a package of pure R functions

Rcmd INSTALL -l c:/R/R_Src/library C:/R/R_Src/src/Etc

---------Making package Etc -----------------
  adding build stamp to DESCRIPTION
  installing R files
  installing man source files
  installing indices
cat: c:/r/rw2000/library/*/CONTENTS: No such file or directory
make[2]: ***[indices] Error 1
make[1]: ***[all] Error 2
make: *** [pkg-Etc] Error 2
*** Instalation of Etc failes  ***

Removing 'c:/R/R_Src/library/Etc'

Dunnett is a package that computes the p value from Dunnett t-test
has source code in Fortran

Rcmd INSTALL -l c:/R/R_Src/library C:/R/R_Src/src/Dunnett

---------Making package Dunnett -----------------
  adding build stamp to DESCRIPTION
  making DLL ...
  ... DLL made
  installing R files
  installing man source files
  installing indices
cat: c:/r/rw2000/library/*/CONTENTS: No such file or directory
make[2]: ***[indices] Error 1
make[1]: ***[all] Error 2
make: *** [pkg-Etc] Error 2
*** Instalation of Etc failes  ***

Removing 'c:/R/R_Src/library/Etc'

Can somebody help me with that 'CONTENTS' file that does not exist?
thanks for any help.
Heberto Ghezzo
McGill University
Canada



From henric.nilsson at statisticon.se  Thu Oct 14 19:40:50 2004
From: henric.nilsson at statisticon.se (Henric Nilsson)
Date: Thu, 14 Oct 2004 19:40:50 +0200
Subject: [R] xyplot gets overplotted by plot
In-Reply-To: <6.1.2.0.0.20041014183335.0b097088@10.0.10.66>
References: <6.1.2.0.0.20041014170501.0aeac118@10.0.10.66>
	<Pine.LNX.4.44.0410141659230.30536-100000@gannet.stats>
	<6.1.2.0.0.20041014183335.0b097088@10.0.10.66>
Message-ID: <6.1.2.0.0.20041014193742.0b089e68@10.0.10.66>

At 19:25 2004-10-14 +0200, Henric Nilsson wrote:

>[...] I assumed that every call to plot erased the device before plotting.

Please disregard! If it worked that way, the par(mfrow = c(2, 2)) would 
have been quite useless in the first place...

Henric



From vograno at evafunds.com  Thu Oct 14 19:50:08 2004
From: vograno at evafunds.com (Vadim Ogranovich)
Date: Thu, 14 Oct 2004 10:50:08 -0700
Subject: [R] Statistical analysis of a large database
Message-ID: <C698D707214E6F4AB39AB7096C3DE5A56D8CC4@phost015.EVAFUNDS.intermedia.net>

I thought that maybe authors of books on R should be allowed (encouraged ?) to announce availability/revisions of their books via the R-packages list?
For example I'd be very interested to have another look at Dr. Torgo's book when it becomes more complete and I'd appreciate a revision notice via the list.

Just a suggestion. Thanks, Vadim


> -----Original Message-----
> From: r-help-bounces at stat.math.ethz.ch 
> [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Luis Torgo
> Sent: Wednesday, October 13, 2004 12:03 PM
> To: Prof Brian Ripley
> Cc: Vito Ricci; r-help at stat.math.ethz.ch
> Subject: Re: [R] Statistical analysis of a large database
> 
> On Tue, 2004-10-12 at 08:36, Prof Brian Ripley wrote:
> > > Lu??s Torgo, Data Mining with R. Learning by case studies, Maggio 
> > > 2003 http://www.liacc.up.pt/~ltorgo/DataMiningWithR/
> > 
> > Please note that that reference is not about large 
> datasets, nor about 
> > `data mining' in the generally used sense.  It has two studies, one 
> > incomplete, on linear regression (with 200 samples) and on 
> time series.
> 
> I would like to add a few information on these incomplete 
> comments on the book I'm writing. The book is unfinished as 
> mentioned on its Web page. It has currently two reasonably 
> finished chapters: an introduction to R and MySQL and a case 
> study. As mentioned in the book, the first case study is 
> small by data mining standards (200 observations) and has the 
> goal of illustrating techniques that are shared by data 
> mining and other disciplines as well as smoothly introducing 
> the reader to R and its power. It addresses data 
> pre-processing techniques, data visualization, model 
> construction (yes, linear regression but also regression 
> trees), and model evaluation, selection and combination, so I 
> think it is a bit incorrect to say that it is about linear 
> regression that corresponds to 5 of the 50 pages of that chapter.
>  
> The third (unfinished) chapter (2nd case study) is about 
> financial trading. It includes topics like connections to 
> data bases as well as many other components of a knowledge 
> discovery process. Among those components it includes model 
> construction that involves obviously time series models given 
> the nature of the data. The chapter will include other steps 
> like issues concerning moving from predictions into actions, 
> creation of variables from the original time series, etc.. It 
> is currently being re-written and I expect to upload soon a 
> new revised version of this chapter.
> 
> The book will include at least two further cases studies that 
> will be larger. Still, I would note that the financial 
> trading case study is potentially very large, as it is a 
> problem where data is constantly growing. The final version 
> of that chapter addresses this issue of having a system that 
> is online in the sense that it is receiving new data in real 
> time (also known as mining data streams in the data mining field).
> 
> I'm sorry for being so long, but I think it is dangerous to 
> try to resume around 200 pages of an unfinished work in two 
> lines of text.
> 
> Still, all comments on this on going project are very well 
> welcome and I would like to take this opportunity to thank 
> all people that have been sending me encouraging comments/emails.
> 
> Luis Torgo
> 
> --
> Luis Torgo
>   FEP/LIACC, University of Porto   Phone : (+351) 22 607 88 30
>   Machine Learning Group           Fax   : (+351) 22 600 36 54
>   R. Campo Alegre, 823             email : ltorgo at liacc.up.pt
>   4150 PORTO   -  PORTUGAL         WWW   : 
> http://www.liacc.up.pt/~ltorgo
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
>



From scott.waichler at pnl.gov  Thu Oct 14 20:31:33 2004
From: scott.waichler at pnl.gov (Scott Waichler)
Date: Thu, 14 Oct 2004 11:31:33 -0700
Subject: [R] Problem with number characters
Message-ID: <200410141831.i9EIVX322036@snow.pnl.gov>

I am trying to process text fields scanned in from a csv file that is
output from the Windows database program FileMakerPro.  The characters
onscreen look like regular text, but R does not like their underlying binary form.
For example, one of text fields contains a name and a number, but
R recognizes the number as something other than what it appears
to be in plain text.  The character string "Draszt  03" after being
read into R using scan and ="" becomes "Draszt 03" where the 3 is 
displayed in my R session as a superscript.  Here is the result pasted
into this email I'm composing in emacs:  "Draszt 0%/1???iso8859-15??"
Another clue for the knowledgable:  when I try to display the vector element
causing trouble, I get
  <CHARSXP: "Draszt 0%/1???iso8859-15??">
where again the superscipt part is just "3" in my R session.  I'm working in
Linux, R version 1.9.1, 2004-06-21.  Your help will be much appreciated.

Scott Waichler
Pacific Northwest National Laboratory
scott.waichler at pnl.gov



From ggrothendieck at myway.com  Thu Oct 14 21:30:54 2004
From: ggrothendieck at myway.com (Gabor Grothendieck)
Date: Thu, 14 Oct 2004 15:30:54 -0400 (EDT)
Subject: [R] Problem with number characters
Message-ID: <20041014193054.CDDF439FD@mprdmxin.myway.com>



Assuming that the problem is that your input file has 
additional embedded characters added by the data base
program you could try extracting just the text using
the UNIX strings program:

   strings myfile.csv > myfile.txt

and see if myfile.txt works with R and if not check out
what the differences are between it and the .csv file.

Date:   Thu, 14 Oct 2004 11:31:33 -0700 
From:   Scott Waichler <scott.waichler at pnl.gov>
To:   <r-help at stat.math.ethz.ch> 
Subject:   [R] Problem with number characters 

 
I am trying to process text fields scanned in from a csv file that is
output from the Windows database program FileMakerPro. The characters
onscreen look like regular text, but R does not like their underlying binary form.
For example, one of text fields contains a name and a number, but
R recognizes the number as something other than what it appears
to be in plain text. The character string "Draszt 03" after being
read into R using scan and ="" becomes "Draszt 03" where the 3 is 
displayed in my R session as a superscript. Here is the result pasted
into this email I'm composing in emacs: "Draszt 0%/1?iso8859-15"
Another clue for the knowledgable: when I try to display the vector element
causing trouble, I get
<CHARSXP: "Draszt 0%/1?iso8859-15">
where again the superscipt part is just "3" in my R session. I'm working in
Linux, R version 1.9.1, 2004-06-21. Your help will be much appreciated.

Scott Waichler
Pacific Northwest National Laboratory
scott.waichler at pnl.gov



From sgilpin at gmail.com  Thu Oct 14 21:40:46 2004
From: sgilpin at gmail.com (Scott Gilpin)
Date: Thu, 14 Oct 2004 13:40:46 -0600
Subject: [R] random forest problem when calculating variable importance
Message-ID: <5739cc2f04101412403739027d@mail.gmail.com>

Hi - 

When using the randomForest function for regression, I get different
results for mean-squared error of the predictions depending on whether
or not I specify to calculate variable importance.  There is an
example below.  I looked briefly at the source code, but couldn't find
anything that would indicate why calculating variable importance would
(or should) change predictions.

I'm using randomForest version 4.3-3 (the latest from CRAN), and tried
 R 1.9.0, 1.9.1 and 2.0.0 on Windows XP, and R 1.9.1 on solaris 8.

Thanks,
Scott Gilpin

library(randomForest)
set.seed(2863)
x<-matrix(runif(1000),ncol=10)
colnames(x)<-1:10
beta<-matrix(c(1,2,3,4,5,0,0,0,0,0),ncol=1)
y<-drop(x %*% beta + rnorm(100))
newx<-matrix(runif(1000),ncol=10)
newy<-drop(newx %*% beta + rnorm(100))

set.seed(2863)
rf.fit <- randomForest(x=x,y=y,xtest=newx,ytest=newy,importance=F)
print(rf.fit$test$mse[500])

set.seed(2863)
rf.fit <- randomForest(x=x,y=y,xtest=newx,ytest=newy,importance=T)
print(rf.fit$test$mse[500])



From pmrdealmeida at mac.com  Thu Oct 14 22:24:40 2004
From: pmrdealmeida at mac.com (Pedro Rodrigues de Almeida)
Date: Thu, 14 Oct 2004 23:24:40 +0300
Subject: [R] defining a function by branches
Message-ID: <1360C3F3-1E1F-11D9-95E6-000D9366B47C@mac.com>

Hi there,

How can I plot a function in R which is defined by conditional 
branches?, e.g.:

u(x) = 1, Ed<x<2Ed (where Ed is a positive constant, say, 25);
u(x)=0.5*x, x>2Ed

curve(u(x), ....)

So far I have had to use curve(..., add=TRUE) with two function define 
by segments, i.e., two branches or intervals for the variable x, say, 
25,50 and then from 50,... and I would appreciate a more elegant 
solution if possible.

Thanks a lot,

Pedro Rodrigues de Almeida
Helsinki - Finland



From andy_liaw at merck.com  Thu Oct 14 22:28:02 2004
From: andy_liaw at merck.com (Liaw, Andy)
Date: Thu, 14 Oct 2004 16:28:02 -0400
Subject: [R] random forest problem when calculating variable
 importanc e
Message-ID: <3A822319EB35174CA3714066D590DCD504AF853D@usrymx25.merck.com>

Are the results dramatically different?

The result would be expected to be somewhat different, as setting
importance=TRUE would make many calls to the random number generator (for
permuting OOB data in each variable), making all but the first tree in the
forest different than if importance=FALSE.

Cheers,
Andy

> From: Scott Gilpin
> 
> Hi - 
> 
> When using the randomForest function for regression, I get different
> results for mean-squared error of the predictions depending on whether
> or not I specify to calculate variable importance.  There is an
> example below.  I looked briefly at the source code, but couldn't find
> anything that would indicate why calculating variable importance would
> (or should) change predictions.
> 
> I'm using randomForest version 4.3-3 (the latest from CRAN), and tried
>  R 1.9.0, 1.9.1 and 2.0.0 on Windows XP, and R 1.9.1 on solaris 8.
> 
> Thanks,
> Scott Gilpin
> 
> library(randomForest)
> set.seed(2863)
> x<-matrix(runif(1000),ncol=10)
> colnames(x)<-1:10
> beta<-matrix(c(1,2,3,4,5,0,0,0,0,0),ncol=1)
> y<-drop(x %*% beta + rnorm(100))
> newx<-matrix(runif(1000),ncol=10)
> newy<-drop(newx %*% beta + rnorm(100))
> 
> set.seed(2863)
> rf.fit <- randomForest(x=x,y=y,xtest=newx,ytest=newy,importance=F)
> print(rf.fit$test$mse[500])
> 
> set.seed(2863)
> rf.fit <- randomForest(x=x,y=y,xtest=newx,ytest=newy,importance=T)
> print(rf.fit$test$mse[500])
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
> 
>



From murdoch at stats.uwo.ca  Thu Oct 14 22:33:38 2004
From: murdoch at stats.uwo.ca (Duncan Murdoch)
Date: Thu, 14 Oct 2004 16:33:38 -0400
Subject: [R] defining a function by branches
In-Reply-To: <1360C3F3-1E1F-11D9-95E6-000D9366B47C@mac.com>
References: <1360C3F3-1E1F-11D9-95E6-000D9366B47C@mac.com>
Message-ID: <shotm05evg5t1dbuj0quh6d3uarmce278r@4ax.com>

On Thu, 14 Oct 2004 23:24:40 +0300, Pedro Rodrigues de Almeida
<pmrdealmeida at mac.com> wrote :

>Hi there,
>
>How can I plot a function in R which is defined by conditional 
>branches?, e.g.:
>
>u(x) = 1, Ed<x<2Ed (where Ed is a positive constant, say, 25);
>u(x)=0.5*x, x>2Ed

ifelse(x < 2*Ed, 1, 0.5*x)

is close to what you want (but I'm not sure what you want for x < Ed).

Duncan Murdoch



From vincent.goulet at act.ulaval.ca  Thu Oct 14 22:39:32 2004
From: vincent.goulet at act.ulaval.ca (Vincent Goulet)
Date: Thu, 14 Oct 2004 16:39:32 -0400
Subject: [R] defining a function by branches
In-Reply-To: <1360C3F3-1E1F-11D9-95E6-000D9366B47C@mac.com>
References: <1360C3F3-1E1F-11D9-95E6-000D9366B47C@mac.com>
Message-ID: <200410141639.32603.vincent.goulet@act.ulaval.ca>

Le 14 Octobre 2004 16:24, Pedro Rodrigues de Almeida a ??crit :
> Hi there,
>
> How can I plot a function in R which is defined by conditional
> branches?, e.g.:
>
> u(x) = 1, Ed<x<2Ed (where Ed is a positive constant, say, 25);
> u(x)=0.5*x, x>2Ed
>
> curve(u(x), ....)
>
> So far I have had to use curve(..., add=TRUE) with two function define
> by segments, i.e., two branches or intervals for the variable x, say,
> 25,50 and then from 50,... and I would appreciate a more elegant
> solution if possible.
>
> Thanks a lot,
>
> Pedro Rodrigues de Almeida
> Helsinki - Finland

Have you tried something along the lines of:

u <- function(x, Ed)  ifelse(x > 2 * Ed, 0.5 * x, 1)
curve(u(x), ...)

(Note that this does take the case x <= Ed into account, which is not covered 
in your definition.)

HTH

-- 
  Vincent Goulet, Associate Professor
  ??cole d'actuariat
  Universit?? Laval, Qu??bec 
  Vincent.Goulet at act.ulaval.ca   http://vgoulet.act.ulaval.ca



From jean.vidal at freesurf.fr  Thu Oct 14 22:38:34 2004
From: jean.vidal at freesurf.fr (Jean Vidal)
Date: Thu, 14 Oct 2004 22:38:34 +0200
Subject: [R] Problem with R version 2.0.0 (and patched)
Message-ID: <6.0.0.22.0.20041014220015.01d5c760@pop.freesurf.fr>

What I am doing : create a simple correlation matrix on 41 variables, then 
plot an heatmap with this program :

library(gplots)
mat<-cor(temp.alln,use="pairwise.complete.obs")
hm<-heatmap.2(mat,symm=T)
HM<-format(round(mat[hm[[1]],hm[[2]]],2))
library(RColorBrewer)
brewer.pal(10,"Spectral")->mp
heatmap.2(mat,  symm = TRUE, col = mp,
             trace='none',breaks=seq(-1,1,.2),rowsep=5,
             cellnote=HM,notecex=.6,notecol='black' )

If I repeat 5 times the heatmap command, I get what can be called a "crash" 
of the Rgui. A little Microsoft dialog box appears telling that a problem 
has occured and that I should send the error report (AppName: 
rgui.exe     AppVer: 2.0.41004.0     ModName: r.dll
ModVer: 2.0.41004.0      Offset: 000c4c18) to Mr Gates. The Rgui seems to 
be frozen with the little sandbox, and vanishes when I close or the Windows 
error dialog or the graphic window.
This happens with this version on my home PC (Windows XP)
 > version
          _
platform i386-pc-mingw32
arch     i386
os       mingw32
system   i386, mingw32
status
major    2
minor    0.0
year     2004
month    10
day      04
language R

But I have the same problem on my work computer (NT2000) .
I installed the patched version, which still crashes in the same way.
 > version
          _
platform i386-pc-mingw32
arch     i386
os       mingw32
system   i386, mingw32
status   Patched
major    2
minor    0.0
year     2004
month    10
day      09
language R

It crashes in the same way with the following message :
"the instruction at "0x6b4c4c18" uses the memory address "0xa00130bb". The 
memory cannot be "written".
Click on "OK" to terminate the program."
(This is a  translation from the french message )

Hope this can help  you to  track the problem.



From spencer.graves at pdf.com  Thu Oct 14 22:41:24 2004
From: spencer.graves at pdf.com (Spencer Graves)
Date: Thu, 14 Oct 2004 13:41:24 -0700
Subject: [R] Problem with number characters
In-Reply-To: <20041014193054.CDDF439FD@mprdmxin.myway.com>
References: <20041014193054.CDDF439FD@mprdmxin.myway.com>
Message-ID: <416EE474.6030302@pdf.com>

  It looks like you have several non-printing characters.

"nchar" will give you the total number of characters in each character 
string.

"strsplit" can break character strings into single characters, and 
"%in%" can be used to classify them.

Consider the following:
 > x <- "Draszt 0%/1?????iso8859-15????"
 > nx <- nchar(x)
 > x. <- strsplit(x, "")
 > length(x.[[1]])
[1] 29
 >
 > namechars <- c(letters, LETTERS,
+ as.character(0:9), ".")
 > punctuation <- c(",", "!", "+", "*", "&", "|")
 > legalchars <- c(namechars, punctuation)
 >
 > legalx <- lapply(x., function(y)(y %in% legalchars))
 > x.[[1]][!legalx[[1]]]
[1] " " "" "%" "/" "??" "" "??" "?" "-" "" "??" "??"
 >
 > sapply(legalx, sum)
[1] 17

Will this give you ideas about what to do what you want?
hope this helps. spencer graves

Gabor Grothendieck wrote:

>Assuming that the problem is that your input file has 
>additional embedded characters added by the data base
>program you could try extracting just the text using
>the UNIX strings program:
>
>   strings myfile.csv > myfile.txt
>
>and see if myfile.txt works with R and if not check out
>what the differences are between it and the .csv file.
>
>Date:   Thu, 14 Oct 2004 11:31:33 -0700 
>From:   Scott Waichler <scott.waichler at pnl.gov>
>To:   <r-help at stat.math.ethz.ch> 
>Subject:   [R] Problem with number characters 
>
> 
>I am trying to process text fields scanned in from a csv file that is
>output from the Windows database program FileMakerPro. The characters
>onscreen look like regular text, but R does not like their underlying binary form.
>For example, one of text fields contains a name and a number, but
>R recognizes the number as something other than what it appears
>to be in plain text. The character string "Draszt 03" after being
>read into R using scan and ="" becomes "Draszt 03" where the 3 is 
>displayed in my R session as a superscript. Here is the result pasted
>into this email I'm composing in emacs: "Draszt 0%/1?????iso8859-15????"
>Another clue for the knowledgable: when I try to display the vector element
>causing trouble, I get
><CHARSXP: "Draszt 0%/1?????iso8859-15????">
>where again the superscipt part is just "3" in my R session. I'm working in
>Linux, R version 1.9.1, 2004-06-21. Your help will be much appreciated.
>
>Scott Waichler
>Pacific Northwest National Laboratory
>scott.waichler at pnl.gov
>
>______________________________________________
>R-help at stat.math.ethz.ch mailing list
>https://stat.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>  
>

-- 
Spencer Graves, PhD, Senior Development Engineer
O:  (408)938-4420;  mobile:  (408)655-4567



From nair at sdsc.edu  Thu Oct 14 22:44:15 2004
From: nair at sdsc.edu (T. Murlidharan Nair)
Date: Thu, 14 Oct 2004 13:44:15 -0700
Subject: [R] maximum likelihood estimation
Message-ID: <416EE51F.6090106@sdsc.edu>

Since mle is defunct is there anyother function I can use for maximum 
likelihood
estimation ?
Thanks ../Murli



From murdoch at stats.uwo.ca  Thu Oct 14 23:00:47 2004
From: murdoch at stats.uwo.ca (Duncan Murdoch)
Date: Thu, 14 Oct 2004 17:00:47 -0400
Subject: [R] Problem with R version 2.0.0 (and patched)
In-Reply-To: <6.0.0.22.0.20041014220015.01d5c760@pop.freesurf.fr>
References: <6.0.0.22.0.20041014220015.01d5c760@pop.freesurf.fr>
Message-ID: <apptm0982r16fvtgjaob46tjtickpglvpn@4ax.com>

On Thu, 14 Oct 2004 22:38:34 +0200, Jean Vidal
<jean.vidal at freesurf.fr> wrote :

>What I am doing : create a simple correlation matrix on 41 variables, then 
>plot an heatmap with this program :
>
>library(gplots)

gplots is a package in the gregmisc bundle.

>mat<-cor(temp.alln,use="pairwise.complete.obs")

I've got no idea what temp.alln is.

 ... deletions ...

>Hope this can help  you to  track the problem.

Can't track it if I can't reproduce it.  Please try to simplify it to
the point where you can post a self-contained example that only uses
base packages.  If you can't do that, then send a self-contained
example to the package maintainer of the package containing the code
that crashes.  In this case that looks like Greg Warnes
<Gregory_R_Warnes at groton.pfizer.com>.  It might be an R bug or it
might be a bug in his package; he's in the best position to determine
that.

Duncan Murdoch



From rpeng at jhsph.edu  Thu Oct 14 23:06:25 2004
From: rpeng at jhsph.edu (Roger D. Peng)
Date: Thu, 14 Oct 2004 17:06:25 -0400
Subject: [R] maximum likelihood estimation
In-Reply-To: <416EE51F.6090106@sdsc.edu>
References: <416EE51F.6090106@sdsc.edu>
Message-ID: <416EEA51.8040804@jhsph.edu>

What lead you to believe that mle() is defunct?  It's still in the 
`stats4' package in my installation of R.

-roger

T. Murlidharan Nair wrote:
> Since mle is defunct is there anyother function I can use for maximum 
> likelihood
> estimation ?
> Thanks ../Murli
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
>



From Scott.Waichler at pnl.gov  Fri Oct 15 01:09:25 2004
From: Scott.Waichler at pnl.gov (Waichler, Scott R)
Date: Thu, 14 Oct 2004 16:09:25 -0700
Subject: [R] Problem with number characters
Message-ID: <7E4C06F49D6FEB49BE4B60E5FC92ED7AB76296@pnlmse35.pnl.gov>

Gabor wrote:
>Assuming that the problem is that your input file has 
>additional embedded characters added by the data base
>program you could try extracting just the text using
>the UNIX strings program:
>
>   strings myfile.csv > myfile.txt

Spencer wrote:
>"strsplit" can break character strings into single 
>characters, and "%in%" can be used to classify them.

The first suggestion helped me identify and remove
some of the embedded characters, namely "^K".  Many more remained
hidden.

The second suggestion gave me the idea of
splitting the string on whitespace first, and seeing if the
embedded character problem would go way along with the "blank"
spaces.  It did.  In the snippet below, x is the character variable
I am trying to process:

      str.vec <- strsplit(x, "\\s+", perl=T)[[1]]
      if(length(str.vec) > 0) {
        x <- paste(str.vec, collapse=" ")
        x <- gsub("^\\s+", "", x, perl=T)
        x <- gsub("\\s+$", "", x, perl=T)
      }

There were no problems in processing x thereafter.

Thank you, gentlemen.

Scott Waichler



From doktora at gmail.com  Fri Oct 15 02:31:13 2004
From: doktora at gmail.com (doktora v)
Date: Thu, 14 Oct 2004 20:31:13 -0400
Subject: [R] C/C++
Message-ID: <3398909b041014173177b45d01@mail.gmail.com>

Hey everyone,

I have been looking for a while for ways to integrate R's wonderful
functions into my C++ software, but I have not found anything
concrete.

So finally, i post to this list to see if anyouse else knows about
this, or has done it!? Is it possible? Are there C++ or C R libraries?
Or is it sufficiently easy to build them?

your help is much appreciated!
thanks
doktora



From gregory.r.warnes at pfizer.com  Fri Oct 15 02:59:51 2004
From: gregory.r.warnes at pfizer.com (Warnes, Gregory R)
Date: Thu, 14 Oct 2004 20:59:51 -0400
Subject: [R] Problem with R version 2.0.0 (and patched)
Message-ID: <915D2D65A9986440A277AC5C98AA466F0A1A8C@groamrexm02.amer.pfizer.com>


Yes, please provide an example of the data that can lead to the crash.
FWIW, the heatmap.2 function uses only standrd R calls and doesn't use any
external C code. (There isn't any C code in the entire library).  So, the
bug is likely to stem from something in base R.

-G

> -----Original Message-----
> From: r-help-bounces at stat.math.ethz.ch
> [mailto:r-help-bounces at stat.math.ethz.ch]On Behalf Of Duncan Murdoch
> Sent: Thursday, October 14, 2004 5:01 PM
> To: Jean Vidal
> Cc: r-help at stat.math.ethz.ch
> Subject: Re: [R] Problem with R version 2.0.0 (and patched)
> 
> 
> On Thu, 14 Oct 2004 22:38:34 +0200, Jean Vidal
> <jean.vidal at freesurf.fr> wrote :
> 
> >What I am doing : create a simple correlation matrix on 41 
> variables, then 
> >plot an heatmap with this program :
> >
> >library(gplots)
> 
> gplots is a package in the gregmisc bundle.
> 
> >mat<-cor(temp.alln,use="pairwise.complete.obs")
> 
> I've got no idea what temp.alln is.
> 
>  ... deletions ...
> 
> >Hope this can help  you to  track the problem.
> 
> Can't track it if I can't reproduce it.  Please try to simplify it to
> the point where you can post a self-contained example that only uses
> base packages.  If you can't do that, then send a self-contained
> example to the package maintainer of the package containing the code
> that crashes.  In this case that looks like Greg Warnes
> <Gregory_R_Warnes at groton.pfizer.com>.  It might be an R bug or it
> might be a bug in his package; he's in the best position to determine
> that.
> 
> Duncan Murdoch
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
> 


LEGAL NOTICE\ Unless expressly stated otherwise, this messag...{{dropped}}



From vograno at evafunds.com  Fri Oct 15 03:09:29 2004
From: vograno at evafunds.com (Vadim Ogranovich)
Date: Thu, 14 Oct 2004 18:09:29 -0700
Subject: [R] 2d approx
Message-ID: <C698D707214E6F4AB39AB7096C3DE5A56D8D23@phost015.EVAFUNDS.intermedia.net>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20041014/c8097200/attachment.pl

From roger at ysidro.econ.uiuc.edu  Fri Oct 15 03:34:08 2004
From: roger at ysidro.econ.uiuc.edu (roger koenker)
Date: Thu, 14 Oct 2004 20:34:08 -0500
Subject: [R] 2d approx
In-Reply-To: <C698D707214E6F4AB39AB7096C3DE5A56D8D23@phost015.EVAFUNDS.intermedia.net>
References: <C698D707214E6F4AB39AB7096C3DE5A56D8D23@phost015.EVAFUNDS.intermedia.net>
Message-ID: <4EF550D7-1E4A-11D9-9608-000393A361A2@ysidro.econ.uiuc.edu>

?interp in akima for f: R^2 -> R.

url:	www.econ.uiuc.edu/~roger        	Roger Koenker
email	rkoenker at uiuc.edu			Department of Economics
vox: 	217-333-4558				University of Illinois
fax:   	217-244-6678				Champaign, IL 61820

On Oct 14, 2004, at 8:09 PM, Vadim Ogranovich wrote:

> Hi,
>
> I am looking for a function that generalizes 'approx' to two (or more)
> dimensions. The references on the approx help page point toward 
> splines,
> but a) splines is what I am trying to avoid in the first place and b)
> splines (except for mgcv splines) seem to be one dimensional.
>
> Here is a more detailed account. Using mgcv:gam I fit an additive model
> xy.gam according to the formula y ~ s(x), which is a spline under the
> hood. If I now wish to compute model prediction for new data I could 
> use
> predict.gam(xy.gam, newdata). However newdata will first be expanded
> into a large matrix of coefficients with respect to the spline basis
> functions. For example if the length of newdata is 1e6 and the size of
> the basis is 100 than the matrix of coefficients is 100*1e6, i.e. huge.
> The predict.gam recognizes the problem and works around it by doing a
> piece-meal prediction, but this turns out to be too slow for my needs.
>
> One way around is to tabulate s(x) on a fine enough grid and use approx
> for prediction. Something like this (pseudo-code)
>
> x.grid <- seq(min(newdata), max(newdata), length=1000)
> y.grid <- predict.gam(xy.gam, x.grid)
>
> y.newdata <- approx(x.grid, y.grid, newdata)$y
>
>
> I didn't test this, but I expect it to be dramatically faster than
> predict.gam.
>
> Unfortunately I don't know how to extend it into 2D. Your suggestions
> are very welcome!
>
>
> Thanks,
> Vadim
>
> 	[[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html



From kjetil at acelerate.com  Fri Oct 15 02:40:21 2004
From: kjetil at acelerate.com (Kjetil Brinchmann Halvorsen)
Date: Thu, 14 Oct 2004 20:40:21 -0400
Subject: [R] beginner questions: objects and scripts
In-Reply-To: <20041014140001.47950.qmail@web54304.mail.yahoo.com>
References: <20041014140001.47950.qmail@web54304.mail.yahoo.com>
Message-ID: <416F1C75.8090502@acelerate.com>

Terry Mu wrote:

>hi,
>
>I got some questions on using R,
>
>1. How do I check and edit definition of an object?
>
>for example,
>
>  
>
>>x <- 1:100
>>    
>>

You should'nt really redefine assignment, but something like

"<<<-" <- function(xyz4, value) {
      v <- deparse(substitute(value))
      xname <- deparse(substitute(xyz4))
      res <- value
      comment(res) <- v
      assign(xname, value=res, inherits=TRUE)
}

and then

 > "<<<-"(x, abs(sin(3*pi/2)))
 > x
[1] 1
 > comment(x)
[1] "abs(sin(3 * pi/2))"

You could even redefine <- to be <<<-, but don't

Probably better to use the history mechanism.

Kjetil

>
>then after a while I want to check how x is defined.
>
>list(x) or whatever functions I know only list its
>content, but I want to see its definition, without
>scrolling up and down, and edit it like "fixing" a
>function.
>
>2. How to save my work in current session as a nice
>script?
>
>again, I want to save objects as they are defined, not
>numbers, other than copy / paste. I tried dump(), etc.
>In another word, how do you work with R?
>
>Thanks,
>Terry
>
>______________________________________________
>R-help at stat.math.ethz.ch mailing list
>https://stat.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>
>
>  
>


-- 

Kjetil Halvorsen.

Peace is the most effective weapon of mass construction.
               --  Mahdi Elmandjra



From lindsayv at unbc.ca  Fri Oct 15 08:21:26 2004
From: lindsayv at unbc.ca (lindsayv@unbc.ca)
Date: Thu, 14 Oct 2004 23:21:26 -0700 (PDT)
Subject: [R] rev matrix
Message-ID: <48975.142.27.74.62.1097821286.squirrel@webmail.unbc.ca>

Hello

I am looking for a function which performs the same thing as rev but on a
matrix rather then a vector, based on one column.
I'd be happy to read the man page if such a function exists.

Thanks,
Vera Lindsay



From Anne.Olga.Piotet at omsv.vd.ch  Fri Oct 15 08:39:14 2004
From: Anne.Olga.Piotet at omsv.vd.ch (Anne Piotet)
Date: Fri, 15 Oct 2004 08:39:14 +0200
Subject: [R] Gumbel distribution
Message-ID: <006e01c4b281$aff06870$83dad10a@prod.omsv.ch>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20041015/b19128b2/attachment.pl

From ripley at stats.ox.ac.uk  Fri Oct 15 08:42:05 2004
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Fri, 15 Oct 2004 07:42:05 +0100 (BST)
Subject: [R] rev matrix
In-Reply-To: <48975.142.27.74.62.1097821286.squirrel@webmail.unbc.ca>
Message-ID: <Pine.LNX.4.44.0410150737160.5487-100000@gannet.stats>

On Thu, 14 Oct 2004 lindsayv at unbc.ca wrote:

> Hello
> 
> I am looking for a function which performs the same thing as rev but on a
> matrix rather then a vector, based on one column.
> I'd be happy to read the man page if such a function exists.

If I understand you, it is easy to do by indexing, but `based on >one< 
column' puzzles me and I take it you want each column reversed.

A <- matrix(1:20, 4, 5)
A[4:1, ]

or more generally

A[rev(seq(len=nrow(A))), ]

If that is not what you mean, please give us an example to look at.

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From Matthias.Kohl at uni-bayreuth.de  Fri Oct 15 08:49:24 2004
From: Matthias.Kohl at uni-bayreuth.de (Matthias.Kohl@uni-bayreuth.de)
Date: Fri, 15 Oct 2004 08:49:24 +0200 (MEST)
Subject: [R] Gumbel distribution
In-Reply-To: <006e01c4b281$aff06870$83dad10a@prod.omsv.ch>
References: <006e01c4b281$aff06870$83dad10a@prod.omsv.ch>
Message-ID: <1047.132.180.246.49.1097822964.squirrel@mail.uni-bayreuth.de>

please, take a look at package "evd"

Matthias

> Does R have  built in Gumbel distribution (pdf, ecdf, hazard, parameters
> estimation) for the minimum case? Thanks
>
> Anne
> 	[[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide!
> http://www.R-project.org/posting-guide.html



From maechler at stat.math.ethz.ch  Fri Oct 15 09:04:38 2004
From: maechler at stat.math.ethz.ch (Martin Maechler)
Date: Fri, 15 Oct 2004 09:04:38 +0200
Subject: [R] maximum likelihood estimation
In-Reply-To: <416EEA51.8040804@jhsph.edu>
References: <416EE51F.6090106@sdsc.edu>
	<416EEA51.8040804@jhsph.edu>
Message-ID: <16751.30342.584989.445907@gargle.gargle.HOWL>

>>>>> "Roger" == Roger D Peng <rpeng at jhsph.edu>
>>>>>     on Thu, 14 Oct 2004 17:06:25 -0400 writes:

    Roger> What lead you to believe that mle() is defunct?  It's
    Roger> still in the `stats4' package in my installation of
    Roger> R.

yes indeed.  Just to clarify possible confusions:

The 'mle' *package* has been merged into the new 'stats4'
package {for 1.9.0 already}.  The mle() function has always been
available and there were even discussions on extension /
generalization of its functioning and the methods working on
"mle"-class objects (produced by mle(...).

Martin

    Roger> -roger

    Roger> T. Murlidharan Nair wrote:

    >> Since mle is defunct is there anyother function I can use
    >> for maximum likelihood estimation ?  Thanks ../Murli



From maechler at stat.math.ethz.ch  Fri Oct 15 09:11:12 2004
From: maechler at stat.math.ethz.ch (Martin Maechler)
Date: Fri, 15 Oct 2004 09:11:12 +0200
Subject: [R] Problem with number characters
In-Reply-To: <416EE474.6030302@pdf.com>
References: <20041014193054.CDDF439FD@mprdmxin.myway.com>
	<416EE474.6030302@pdf.com>
Message-ID: <16751.30736.477306.49343@gargle.gargle.HOWL>

>>>>> "Spencer" == Spencer Graves <spencer.graves at pdf.com>
>>>>>     on Thu, 14 Oct 2004 13:41:24 -0700 writes:

    Spencer>   It looks like you have several non-printing
    Spencer> characters.  "nchar" will give you the total number
    Spencer> of characters in each character string.

    Spencer> "strsplit" can break character strings into single
    Spencer> characters, and "%in%" can be used to classify
    Spencer> them.

and you give nice coding examples:

    Spencer> Consider the following:
    >> x <- "Draszt 0%/1?????iso8859-15????"
    >> nx <- nchar(x)
    >> x. <- strsplit(x, "")
    >> length(x.[[1]])
    Spencer> [1] 29
    >> 
    >> namechars <- c(letters, LETTERS, as.character(0:9), ".")

just to be precise:  If 'namechars' is supposed to mean
``characters valid in R object names'', then you should have
added "_" as well:

namechars <- c(letters, LETTERS, as.character(0:9), ".", "_")

    >> punctuation <- c(",", "!", "+", "*", "&", "|")
    >> legalchars <- c(namechars, punctuation)

and 'legalchars' would have to contain quite a bit more I
presume, e.g. "$", "@", ....
(but that wouldn't have been a reason to write this e-mail..)

    >> legalx <- lapply(x., function(y)(y %in% legalchars))
    >> x.[[1]][!legalx[[1]]]
    Spencer> [1] " " "" "%" "/" "??" "" "??" "?" "-" "" "??" "??"
    >> 
    >> sapply(legalx, sum)
    Spencer> [1] 17

    Spencer> Will this give you ideas about what to do what you want?
    Spencer> hope this helps. spencer graves

(and this too)

Martin Maechler, ETH Zurich


    Spencer> Gabor Grothendieck wrote:

    >> Assuming that the problem is that your input file has 
    >> additional embedded characters added by the data base
    >> program you could try extracting just the text using
    >> the UNIX strings program:
    >> 
    >> strings myfile.csv > myfile.txt
    >> 
    >> and see if myfile.txt works with R and if not check out
    >> what the differences are between it and the .csv file.
    >> 
    >> Date:   Thu, 14 Oct 2004 11:31:33 -0700 
    >> From:   Scott Waichler <scott.waichler at pnl.gov>
    >> To:   <r-help at stat.math.ethz.ch> 
    >> Subject:   [R] Problem with number characters 
    >> 
    >> 
    >> I am trying to process text fields scanned in from a csv file that is
    >> output from the Windows database program FileMakerPro. The characters
    >> onscreen look like regular text, but R does not like their underlying binary form.
    >> For example, one of text fields contains a name and a number, but
    >> R recognizes the number as something other than what it appears
    >> to be in plain text. The character string "Draszt 03" after being
    >> read into R using scan and ="" becomes "Draszt 03" where the 3 is 
    >> displayed in my R session as a superscript. Here is the result pasted
    >> into this email I'm composing in emacs: "Draszt 0%/1?????iso8859-15????"
    >> Another clue for the knowledgable: when I try to display the vector element
    >> causing trouble, I get
    >> <CHARSXP: "Draszt 0%/1?????iso8859-15????">
    >> where again the superscipt part is just "3" in my R session. I'm working in
    >> Linux, R version 1.9.1, 2004-06-21. Your help will be much appreciated.
    >> 
    >> Scott Waichler
    >> Pacific Northwest National Laboratory
    >> scott.waichler at pnl.gov



From wolski at molgen.mpg.de  Fri Oct 15 10:15:34 2004
From: wolski at molgen.mpg.de (Witold Eryk Wolski)
Date: Fri, 15 Oct 2004 10:15:34 +0200
Subject: [R] C/C++
In-Reply-To: <3398909b041014173177b45d01@mail.gmail.com>
References: <3398909b041014173177b45d01@mail.gmail.com>
Message-ID: <416F8726.8030709@molgen.mpg.de>

Hi,

Unfortunately I have only some beginners hints to this very interesting 
topic.

Writing R extensions provides examples how to execute r functions from 
within C code.
There is also somewhere a doc how to use R as a shared library (?). But 
I dont remember in which doc exactly.

Furthermore Rserv:
rserv (http://www.rosuda.org/Rserve/index.shtml) has a C++ API for 
calling r functions.


Eryk.



doktora v wrote:

>Hey everyone,
>
>I have been looking for a while for ways to integrate R's wonderful
>functions into my C++ software, but I have not found anything
>concrete.
>
>So finally, i post to this list to see if anyouse else knows about
>this, or has done it!? Is it possible? Are there C++ or C R libraries?
>Or is it sufficiently easy to build them?
>
>your help is much appreciated!
>thanks
>doktora
>
>______________________________________________
>R-help at stat.math.ethz.ch mailing list
>https://stat.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>
>  
>


-- 
Dipl. bio-chem. Witold Eryk Wolski
MPI-Moleculare Genetic
Ihnestrasse 63-73 14195 Berlin
tel: 0049-30-83875219                 __("<    _
http://www.molgen.mpg.de/~wolski      \__/    'v'
http://r4proteomics.sourceforge.net    ||    /   \
mail: witek96 at users.sourceforge.net    ^^     m m
      wolski at molgen.mpg.de



From Ivy_Li at smics.com  Fri Oct 15 10:17:04 2004
From: Ivy_Li at smics.com (Ivy_Li)
Date: Fri, 15 Oct 2004 16:17:04 +0800
Subject: [R] R plot problems
Message-ID: <8A910F1818425847A6D18C7832D207E502A9F8@ex115.smic-sh.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20041015/f7e215b7/attachment.pl

From foadi at ysbl.york.ac.uk  Fri Oct 15 11:19:12 2004
From: foadi at ysbl.york.ac.uk (James Foadi)
Date: Fri, 15 Oct 2004 10:19:12 +0100
Subject: [R] cluster analysis
Message-ID: <200410151019.12406.foadi@ysbl.york.ac.uk>

Hello. I wonder if anyone can help me with this.

I'm performing cluster analysis by using hclust in stats package.
My data are contained in a data frame with 10 columns, named "drops".

Firs I create a distance matrix using dist:
		
		distanxe <- dist(drops)

Then I perform cluster analysis via hclust:

		clusters <- hclust(distanze)

At this point I want to view the tree plot, and use plot:

		plot(clusters)

Then, once decided which clusters to select, I start identify:

		classi <- identify(clusters)

and click on all clusters to be selected; I then finish by right-clicking.

My understanding is that "classi" is now a list containing all individual 
data, grouped in clusters. In my case "classi" contained 10 objects,
simply named [1], [2], etc.

To obtain all individual data belonging to one object I thought that
would have sufficed to type for instance:

		classe_01 <- classi[[1]]

Unfortunately, rather than obtaining a vector, I obtain a "numeric" where
each value is typed twice.

Can anyone explain why, or what I've done wrong?

Many thanks,

james
-- 
Dr James Foadi
Structural Biology Laboratory
Department of Chemistry
University of York
YORK YO10 5YW
UK



From laura.dow at enscitech.com  Fri Oct 15 11:22:25 2004
From: laura.dow at enscitech.com (laura.dow@enscitech.com)
Date: Fri, 15 Oct 2004 10:22:25 +0100
Subject: [R] R and Java
Message-ID: <E1CIOJ3-00019n-8J@hydrus.concept2100.co.uk>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20041015/f1665535/attachment.pl

From henric.nilsson at statisticon.se  Fri Oct 15 11:38:30 2004
From: henric.nilsson at statisticon.se (Henric Nilsson)
Date: Fri, 15 Oct 2004 11:38:30 +0200
Subject: [R] R plot problems
In-Reply-To: <8A910F1818425847A6D18C7832D207E502A9F8@ex115.smic-sh.com>
References: <8A910F1818425847A6D18C7832D207E502A9F8@ex115.smic-sh.com>
Message-ID: <6.1.2.0.0.20041015112714.0ab6bea0@10.0.10.66>

At 16:17 2004-10-15 +0800, you wrote:

>[...] I want to rotate the direction of  x-coordinates' letter so that it 
>can show all. But I don't know how to write this option or function .

I'm not sure if I've understood your question correctly. But if you use 
base graphics and want e.g. rotated labels, you'll achive this rather 
easily using functions in the gridBase package. (Take a look at the package 
vignette for an  example that'll get you started.)

Henric



From fm3a004 at math.uni-hamburg.de  Fri Oct 15 11:43:53 2004
From: fm3a004 at math.uni-hamburg.de (Christian Hennig)
Date: Fri, 15 Oct 2004 11:43:53 +0200 (MEST)
Subject: [R] cluster analysis
In-Reply-To: <200410151019.12406.foadi@ysbl.york.ac.uk>
Message-ID: <Pine.GSO.3.95q.1041015113520.1894E-100000@sun11.math.uni-hamburg.de>

Dear James,

sorry, this is not really an answer.
I use cutree to obtain clusters from an hclust object.
I do not get from the identify help page that identify should do anything
like what you expect it to do... I tried it out and to my surprise it
behaved as you said, i.e., it indeed does something at least similar to what
you want it to do, and that might be useful also for me. However, I wonder
where you got the information that identify could be suitable to obtain the
hclust clusters.

Puzzled,
Christian

PS: It seems that each value is typed twice because classi is named, and
each value is also a name. Try as.vector(classi). (Perhaps a little useful
help in the end?)

On Fri, 15 Oct 2004, James Foadi wrote:

> Hello. I wonder if anyone can help me with this.
> 
> I'm performing cluster analysis by using hclust in stats package.
> My data are contained in a data frame with 10 columns, named "drops".
> 
> Firs I create a distance matrix using dist:
> 		
> 		distanxe <- dist(drops)
> 
> Then I perform cluster analysis via hclust:
> 
> 		clusters <- hclust(distanze)
> 
> At this point I want to view the tree plot, and use plot:
> 
> 		plot(clusters)
> 
> Then, once decided which clusters to select, I start identify:
> 
> 		classi <- identify(clusters)
> 
> and click on all clusters to be selected; I then finish by right-clicking.
> 
> My understanding is that "classi" is now a list containing all individual 
> data, grouped in clusters. In my case "classi" contained 10 objects,
> simply named [1], [2], etc.
> 
> To obtain all individual data belonging to one object I thought that
> would have sufficed to type for instance:
> 
> 		classe_01 <- classi[[1]]
> 
> Unfortunately, rather than obtaining a vector, I obtain a "numeric" where
> each value is typed twice.
> 
> Can anyone explain why, or what I've done wrong?
> 
> Many thanks,
> 
> james
> -- 
> Dr James Foadi
> Structural Biology Laboratory
> Department of Chemistry
> University of York
> YORK YO10 5YW
> UK
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
> 

***********************************************************************
Christian Hennig
Fachbereich Mathematik-SPST/ZMS, Universitaet Hamburg
hennig at math.uni-hamburg.de, http://www.math.uni-hamburg.de/home/hennig/
#######################################################################
ich empfehle www.boag-online.de



From mrufino at ipimar.ualg.pt  Fri Oct 15 11:51:43 2004
From: mrufino at ipimar.ualg.pt (Marta Rufino)
Date: Fri, 15 Oct 2004 10:51:43 +0100
Subject: [R] R plot problems
References: <8A910F1818425847A6D18C7832D207E502A9F8@ex115.smic-sh.com>
	<6.1.2.0.0.20041015112714.0ab6bea0@10.0.10.66>
Message-ID: <007001c4b29c$93f5fa70$0b1a0e0a@PORTATILMARTA>

Dear Li,

If you are using base grafics, you use the argument srt (see help(par)),

if you are using latice, you use rot argument.
For example:

scala=list(y=list(alternating=F, draw=T, rot=0,tick.number=6),
x=list(alternating=F, draw=T, rot=0, tick.number=8))
xyplot(xx ~ yy, data=b, scale=scala)

hope this helps,
All the best
Marta


----- Original Message ----- 
From: "Henric Nilsson" <henric.nilsson at statisticon.se>
To: "Ivy_Li" <Ivy_Li at smics.com>
Cc: <r-help at stat.math.ethz.ch>
Sent: Friday, October 15, 2004 10:38 AM
Subject: Re: [R] R plot problems


> At 16:17 2004-10-15 +0800, you wrote:
>
> >[...] I want to rotate the direction of  x-coordinates' letter so that it
> >can show all. But I don't know how to write this option or function .
>
> I'm not sure if I've understood your question correctly. But if you use
> base graphics and want e.g. rotated labels, you'll achive this rather
> easily using functions in the gridBase package. (Take a look at the
package
> vignette for an  example that'll get you started.)
>
> Henric
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide!
http://www.R-project.org/posting-guide.html



From maechler at stat.math.ethz.ch  Fri Oct 15 12:02:17 2004
From: maechler at stat.math.ethz.ch (Martin Maechler)
Date: Fri, 15 Oct 2004 12:02:17 +0200
Subject: [R] cluster analysis
In-Reply-To: <Pine.GSO.3.95q.1041015113520.1894E-100000@sun11.math.uni-hamburg.de>
References: <200410151019.12406.foadi@ysbl.york.ac.uk>
	<Pine.GSO.3.95q.1041015113520.1894E-100000@sun11.math.uni-hamburg.de>
Message-ID: <16751.41001.605346.333873@gargle.gargle.HOWL>

>>>>> "ChrisH" == Christian Hennig <fm3a004 at math.uni-hamburg.de>
>>>>>     on Fri, 15 Oct 2004 11:43:53 +0200 (MEST) writes:

    ChrisH> Dear James,
    ChrisH> sorry, this is not really an answer.

nor this.  I'm answering Christian...

    ChrisH> I use cutree to obtain clusters from an hclust
    ChrisH> object.  I do not get from the identify help page
    ChrisH> that identify should do anything like what you
    ChrisH> expect it to do... I tried it out and to my surprise
well,
the reason is simple:  
There's been a nice  identify.hclust() method for a long  time 
and this is mentioned (including a link to the page) on the 
?hclust page.

    ChrisH> it behaved as you said, i.e., it indeed does
    ChrisH> something at least similar to what you want it to
    ChrisH> do, and that might be useful also for me. However, I
    ChrisH> wonder where you got the information that identify
    ChrisH> could be suitable to obtain the hclust clusters.

(see above) --- 
     you see: It *does* pay to read documentation carefully

    ChrisH> Puzzled,
    ChrisH> Christian

    ChrisH> PS: It seems that each value is typed twice because
    ChrisH> classi is named, and each value is also a name. Try
    ChrisH> as.vector(classi). (Perhaps a little useful help in
    ChrisH> the end?)

or unname(classi) -- which is slightly more expressive in this
case and possibly more desirable in other situations.

Martin Maechler, ETH Zurich


    ChrisH> On Fri, 15 Oct 2004, James Foadi wrote:

    >> Hello. I wonder if anyone can help me with this.
    >> 
    >> I'm performing cluster analysis by using hclust in stats package.
    >> My data are contained in a data frame with 10 columns, named "drops".
    >> 
    >> Firs I create a distance matrix using dist:
    >> 
    >> distanxe <- dist(drops)
    >> 
    >> Then I perform cluster analysis via hclust:
    >> 
    >> clusters <- hclust(distanze)
    >> 
    >> At this point I want to view the tree plot, and use plot:
    >> 
    >> plot(clusters)
    >> 
    >> Then, once decided which clusters to select, I start identify:
    >> 
    >> classi <- identify(clusters)
    >> 
    >> and click on all clusters to be selected; I then finish by right-clicking.
    >> 
    >> My understanding is that "classi" is now a list containing all individual 
    >> data, grouped in clusters. In my case "classi" contained 10 objects,
    >> simply named [1], [2], etc.
    >> 
    >> To obtain all individual data belonging to one object I thought that
    >> would have sufficed to type for instance:
    >> 
    >> classe_01 <- classi[[1]]
    >> 
    >> Unfortunately, rather than obtaining a vector, I obtain a "numeric" where
    >> each value is typed twice.
    >> 
    >> Can anyone explain why, or what I've done wrong?
    >> 
    >> Many thanks,
    >> 
    >> james
    >> -- 
    >> Dr James Foadi
    >> Structural Biology Laboratory
    >> Department of Chemistry
    >> University of York
    >> YORK YO10 5YW
    >> UK



From foadi at ysbl.york.ac.uk  Fri Oct 15 12:02:38 2004
From: foadi at ysbl.york.ac.uk (James Foadi)
Date: Fri, 15 Oct 2004 11:02:38 +0100
Subject: [R] cluster analysis
In-Reply-To: <Pine.GSO.3.95q.1041015113520.1894E-100000@sun11.math.uni-hamburg.de>
References: <Pine.GSO.3.95q.1041015113520.1894E-100000@sun11.math.uni-hamburg.de>
Message-ID: <200410151102.38347.foadi@ysbl.york.ac.uk>

On Friday 15 Oct 2004 10:43 am, you wrote:

> PS: It seems that each value is typed twice because classi is named, and
> each value is also a name. Try as.vector(classi). (Perhaps a little useful
> help in the end?)

Indeed. I have tried, for example:

		as.vector(classi[[1]])

and obtained only one set of values. For some strange reason
each object of list "classi" is a named vector where the name of each
component is the component itself.

By the way, the cutree function you suggested is even more useful
for what I want to do.

The info on identify() can easily be obtained using help(hclust); you'll
find it at the end of the help page.


Many thanks, Christian !

J

-- 
Dr James Foadi
Structural Biology Laboratory
Department of Chemistry
University of York
YORK YO10 5YW
UK



From maechler at stat.math.ethz.ch  Fri Oct 15 12:31:42 2004
From: maechler at stat.math.ethz.ch (Martin Maechler)
Date: Fri, 15 Oct 2004 12:31:42 +0200
Subject: [R] Announcing books on R - policy {was "Statistical analys..."}
In-Reply-To: <C698D707214E6F4AB39AB7096C3DE5A56D8CC4@phost015.EVAFUNDS.intermedia.net>
References: <C698D707214E6F4AB39AB7096C3DE5A56D8CC4@phost015.EVAFUNDS.intermedia.net>
Message-ID: <16751.42766.688271.753358@gargle.gargle.HOWL>

>>>>> "Vadim" == Vadim Ogranovich <vograno at evafunds.com>
>>>>>     on Thu, 14 Oct 2004 10:50:08 -0700 writes:

    Vadim> I thought that maybe authors of books on R should be
    Vadim> allowed (encouraged ?) to announce
    Vadim> availability/revisions of their books via the
    Vadim> R-packages list?

Well, most good *) books on R nowadays have a (usually small) R
package going along, for datasets and some own functions.

*) One notable current exception: Uwe Ligges book
   __in German__  >> http://www.statistik.uni-dortmund.de/~ligges/PmitR/
   But maybe he'll change that too.

    Vadim> For example I'd be very interested to have another
    Vadim> look at Dr. Torgo's book when it becomes more
    Vadim> complete and I'd appreciate a revision notice via the list.

Hmm, I'm really reluctant to change the 'R-packages' policy,
particularly because it won't be necessary in general (see above).
Further note that we have the 'R Newsletter' with book reviews,
too.  
Further comments are welcome.

Martin Maechler


    Vadim> Thanks, Vadim


    >> -----Original Message-----
    >> From: r-help-bounces at stat.math.ethz.ch 
    >> [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Luis Torgo
    >> Sent: Wednesday, October 13, 2004 12:03 PM
    >> To: Prof Brian Ripley
    >> Cc: Vito Ricci; r-help at stat.math.ethz.ch
    >> Subject: Re: [R] Statistical analysis of a large database
    >> 
    >> On Tue, 2004-10-12 at 08:36, Prof Brian Ripley wrote:
    >> > > Lu??s Torgo, Data Mining with R. Learning by case studies, Maggio 
    >> > > 2003 http://www.liacc.up.pt/~ltorgo/DataMiningWithR/
    >> > 
    >> > Please note that that reference is not about large 
    >> datasets, nor about 
    >> > `data mining' in the generally used sense.  It has two studies, one 
    >> > incomplete, on linear regression (with 200 samples) and on 
    >> time series.
    >> 
    >> I would like to add a few information on these incomplete 
    >> comments on the book I'm writing. The book is unfinished as 
    >> mentioned on its Web page. It has currently two reasonably 
    >> finished chapters: an introduction to R and MySQL and a case 
    >> study. As mentioned in the book, the first case study is 
    >> small by data mining standards (200 observations) and has the 
    >> goal of illustrating techniques that are shared by data 
    >> mining and other disciplines as well as smoothly introducing 
    >> the reader to R and its power. It addresses data 
    >> pre-processing techniques, data visualization, model 
    >> construction (yes, linear regression but also regression 
    >> trees), and model evaluation, selection and combination, so I 
    >> think it is a bit incorrect to say that it is about linear 
    >> regression that corresponds to 5 of the 50 pages of that chapter.
    >> 
    >> The third (unfinished) chapter (2nd case study) is about 
    >> financial trading. It includes topics like connections to 
    >> data bases as well as many other components of a knowledge 
    >> discovery process. Among those components it includes model 
    >> construction that involves obviously time series models given 
    >> the nature of the data. The chapter will include other steps 
    >> like issues concerning moving from predictions into actions, 
    >> creation of variables from the original time series, etc.. It 
    >> is currently being re-written and I expect to upload soon a 
    >> new revised version of this chapter.
    >> 
    >> The book will include at least two further cases studies that 
    >> will be larger. Still, I would note that the financial 
    >> trading case study is potentially very large, as it is a 
    >> problem where data is constantly growing. The final version 
    >> of that chapter addresses this issue of having a system that 
    >> is online in the sense that it is receiving new data in real 
    >> time (also known as mining data streams in the data mining field).
    >> 
    >> I'm sorry for being so long, but I think it is dangerous to 
    >> try to resume around 200 pages of an unfinished work in two 
    >> lines of text.
    >> 
    >> Still, all comments on this on going project are very well 
    >> welcome and I would like to take this opportunity to thank 
    >> all people that have been sending me encouraging comments/emails.
    >> 
    >> Luis Torgo
    >> 
    >> --
    >> Luis Torgo
    >> FEP/LIACC, University of Porto   Phone : (+351) 22 607 88 30
    >> Machine Learning Group           Fax   : (+351) 22 600 36 54
    >> R. Campo Alegre, 823             email : ltorgo at liacc.up.pt
    >> 4150 PORTO   -  PORTUGAL         WWW   : 
    >> http://www.liacc.up.pt/~ltorgo
    >> 
    >> ______________________________________________
    >> R-help at stat.math.ethz.ch mailing list
    >> https://stat.ethz.ch/mailman/listinfo/r-help
    >> PLEASE do read the posting guide! 
    >> http://www.R-project.org/posting-guide.html
    >> 

    Vadim> ______________________________________________
    Vadim> R-help at stat.math.ethz.ch mailing list
    Vadim> https://stat.ethz.ch/mailman/listinfo/r-help
    Vadim> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html



From f.gherardini at pigrecodata.net  Fri Oct 15 14:44:18 2004
From: f.gherardini at pigrecodata.net (Federico Gherardini)
Date: Fri, 15 Oct 2004 14:44:18 +0200
Subject: [R] Testing for normality of residuals in a regression model
Message-ID: <416FC622.9060601@pigrecodata.net>

Hi all,

Is it possible to have a test value for assessing the normality of 
residuals from a linear regression model, instead of simply relying on 
qqplots?
I've tried to use fitdistr to try and fit the residuals with a normal 
distribution, but fitdsitr only returns the parameters of the 
distribution and the standard errors, not the p-value. Am I missing 
something?

Cheers,

Federico



From vito_ricci at yahoo.com  Fri Oct 15 12:32:29 2004
From: vito_ricci at yahoo.com (Vito Ricci)
Date: Fri, 15 Oct 2004 12:32:29 +0200 (CEST)
Subject: [R] R & Neural Networks
Message-ID: <20041015103229.21235.qmail@web41212.mail.yahoo.com>

Hi,

are other R-packages using neural networks tecniques?

I've already found nnet.

Thanks

Best
Vito

=====
Diventare costruttori di soluzioni

"The business of the statistician is to catalyze 
the scientific learning process."  
George E. P. Box


Visitate il portale http://www.modugno.it/
e in particolare la sezione su Palese http://www.modugno.it/archivio/cat_palese.shtml



From foadi at ysbl.york.ac.uk  Fri Oct 15 12:36:14 2004
From: foadi at ysbl.york.ac.uk (James Foadi)
Date: Fri, 15 Oct 2004 11:36:14 +0100
Subject: [R] cluster analysis
In-Reply-To: <16751.41001.605346.333873@gargle.gargle.HOWL>
References: <200410151019.12406.foadi@ysbl.york.ac.uk>
	<Pine.GSO.3.95q.1041015113520.1894E-100000@sun11.math.uni-hamburg.de>
	<16751.41001.605346.333873@gargle.gargle.HOWL>
Message-ID: <200410151136.14601.foadi@ysbl.york.ac.uk>

On Friday 15 Oct 2004 11:02 am, you wrote:

>
> or unname(classi) -- which is slightly more expressive in this
> case and possibly more desirable in other situations.
>
> Martin Maechler, ETH Zurich
>

Thanks, Martin.
I've tried, like you suggested:

		un_classi <- unname(classi)

but nothing changed. By typing, for instance:

		un_classi[[1]]

I still obtained twice the values. But, if I type:

		un_classe_01 <- unname(classi[[1]])

the un_classe_01 is an unnamed vector.

Cheers,

james
-- 
Dr James Foadi
Structural Biology Laboratory
Department of Chemistry
University of York
YORK YO10 5YW
UK



From vito_ricci at yahoo.com  Fri Oct 15 12:37:47 2004
From: vito_ricci at yahoo.com (Vito Ricci)
Date: Fri, 15 Oct 2004 12:37:47 +0200 (CEST)
Subject: [R] Re: Testing for normality of residuals in a regression model
Message-ID: <20041015103747.55531.qmail@web41208.mail.yahoo.com>

Dear Federico,

see:  
? shapiro.test(stats)     Shapiro-Wilk Normality Test
and 
? jarque.bera.test(tseries)                  
Jarque-Bera Test

They are the most common tests used for normality
testing.

Ciao
Vito



Federico Gherardini wrote on Fri Oct 15 14:44:18 CEST
2004:

Hi all,

Is it possible to have a test value for assessing the
normality of 
residuals from a linear regression model, instead of
simply relying on 
qqplots?
I've tried to use fitdistr to try and fit the
residuals with a normal 
distribution, but fitdsitr only returns the parameters
of the 
distribution and the standard errors, not the p-value.
Am I missing 
something?

Cheers,

Federico



=====
Diventare costruttori di soluzioni

"The business of the statistician is to catalyze 
the scientific learning process."  
George E. P. Box


Visitate il portale http://www.modugno.it/
e in particolare la sezione su Palese http://www.modugno.it/archivio/cat_palese.shtml



From stecalza at tiscali.it  Fri Oct 15 12:50:57 2004
From: stecalza at tiscali.it (Stefano Calza)
Date: Fri, 15 Oct 2004 12:50:57 +0200
Subject: [R] Testing for normality of residuals in a regression model
In-Reply-To: <416FC622.9060601@pigrecodata.net>
References: <416FC622.9060601@pigrecodata.net>
Message-ID: <20041015105057.GA2892@med.unibs.it>

What about shapiro.test(resid(fit.object))

Stefano

On Fri, Oct 15, 2004 at 02:44:18PM +0200, Federico Gherardini wrote:
> Hi all,
> 
> Is it possible to have a test value for assessing the normality of 
> residuals from a linear regression model, instead of simply relying on 
> qqplots?
> I've tried to use fitdistr to try and fit the residuals with a normal 
> distribution, but fitdsitr only returns the parameters of the 
> distribution and the standard errors, not the p-value. Am I missing 
> something?
> 
> Cheers,
> 
> Federico
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html



From Luisr at frs.fo  Fri Oct 15 13:01:25 2004
From: Luisr at frs.fo (Luis Rideau Cruz)
Date: Fri, 15 Oct 2004 12:01:25 +0100
Subject: [R] length with missing values
Message-ID: <s16fbc21.048@ffdata.setur.fo>

R-help

I have a martix with missing values( in which I want the sample size by
column)
When I :

apply(matrix,2,length)

I get the length of the vector regardless of missing values.
I can't pass an argument to length in apply.

Alternatively I could 

ifelse ( is.na ( matrix [, "columns in matrix " ] ) , 0 , 1)

Is there any easier way?

Thank you



From dimitris.rizopoulos at med.kuleuven.ac.be  Fri Oct 15 13:10:12 2004
From: dimitris.rizopoulos at med.kuleuven.ac.be (Dimitris Rizopoulos)
Date: Fri, 15 Oct 2004 13:10:12 +0200
Subject: [R] Testing for normality of residuals in a regression model
References: <416FC622.9060601@pigrecodata.net>
Message-ID: <002401c4b2a7$8a64f050$b2133a86@www.domain>

Hi Frederico,

take also a look at the package "nortest":

help(package="nortest")

Best,
Dimitris

----
Dimitris Rizopoulos
Ph.D. Student
Biostatistical Centre
School of Public Health
Catholic University of Leuven

Address: Kapucijnenvoer 35, Leuven, Belgium
Tel: +32/16/396887
Fax: +32/16/337015
Web: http://www.med.kuleuven.ac.be/biostat/
     http://www.student.kuleuven.ac.be/~m0390867/dimitris.htm.


----- Original Message ----- 
From: "Federico Gherardini" <f.gherardini at pigrecodata.net>
To: <R-help at stat.math.ethz.ch>
Sent: Friday, October 15, 2004 2:44 PM
Subject: [R] Testing for normality of residuals in a regression model


> Hi all,
>
> Is it possible to have a test value for assessing the normality of 
> residuals from a linear regression model, instead of simply relying 
> on qqplots?
> I've tried to use fitdistr to try and fit the residuals with a 
> normal distribution, but fitdsitr only returns the parameters of the 
> distribution and the standard errors, not the p-value. Am I missing 
> something?
>
> Cheers,
>
> Federico
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
>



From Achim.Zeileis at wu-wien.ac.at  Fri Oct 15 13:20:17 2004
From: Achim.Zeileis at wu-wien.ac.at (Achim Zeileis)
Date: Fri, 15 Oct 2004 13:20:17 +0200 (CEST)
Subject: [R] length with missing values
In-Reply-To: <s16fbc21.048@ffdata.setur.fo>
References: <s16fbc21.048@ffdata.setur.fo>
Message-ID: <Pine.LNX.4.58.0410151319360.25329@thorin.ci.tuwien.ac.at>

On Fri, 15 Oct 2004, Luis Rideau Cruz wrote:

> R-help
>
> I have a martix with missing values( in which I want the sample size by
> column)
> When I :
>
> apply(matrix,2,length)
>
> I get the length of the vector regardless of missing values.
> I can't pass an argument to length in apply.
>
> Alternatively I could
>
> ifelse ( is.na ( matrix [, "columns in matrix " ] ) , 0 , 1)
>
> Is there any easier way?

You could omit the missing values before:
  apply(matrix, 2, function(y) length(na.omit(y)))

hth,
Z

> Thank you
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>



From adrian at maths.uwa.edu.au  Fri Oct 15 13:20:55 2004
From: adrian at maths.uwa.edu.au (Adrian Baddeley)
Date: Fri, 15 Oct 2004 19:20:55 +0800
Subject: [R] Filling polygons with points
Message-ID: <16751.45719.939664.269544@maths.uwa.edu.au>


<Michael.Wolf at bezreg-muenster.nrw.de> wrote :

 >Are there any possibilities to fill a polygon with a point pattern or
 >with a symbol pattern like '+' oder '-' instead of shading lines?

In `spatstat' there is code for testing whether points belong to
a spatial region. The region can consist of any number of polygons,
and the polygons may have holes, etc.

While splancs has similar functionality, spatstat additionally allows regions
defined by a binary image (e.g. a digital approximation to a circle). 

The code is available both as a function inside.owin(x,y,win)
and as a method for "[" for point pattern objects.

Example of "[":

	library(spatstat)
	data(demopat)
	w <- demopat$window               # an irregular polygonal window
	plot(w)

	g <- as.mask(w, dimyx=c(20,20))   # make a 20 x 20 grid
				            # enclosing the window

        g <- as.mask(w, eps = 200)       # or set absolute grid spacing 200

        p <- ppp(raster.x(g), raster.y(g), window=w)
                                            # convert to point pattern object
        p <- p[, w]   # clip points to window 

	plot(p)
	plot(p, pch="+")   # etc
	

To create a window object use owin() or as.owin().

Adrian Baddeley



From dimitris.rizopoulos at med.kuleuven.ac.be  Fri Oct 15 13:23:48 2004
From: dimitris.rizopoulos at med.kuleuven.ac.be (Dimitris Rizopoulos)
Date: Fri, 15 Oct 2004 13:23:48 +0200
Subject: [R] length with missing values
References: <001801c4b2a9$084e8fc0$ae133a86@www.domain>
Message-ID: <004201c4b2a9$710051c0$b2133a86@www.domain>

Hi Luis,

just use:

apply( mat, 2, function(x) length(x[!is.na(x)]) )

I hope it helps.

Best,
Dimitris

----
Dimitris Rizopoulos
Ph.D. Student
Biostatistical Centre
School of Public Health
Catholic University of Leuven

Address: Kapucijnenvoer 35, Leuven, Belgium
Tel: +32/16/396887
Fax: +32/16/337015
Web: http://www.med.kuleuven.ac.be/biostat/
     http://www.student.kuleuven.ac.be/~m0390867/dimitris.htm


> ----- Original Message ----- 
> From: "Luis Rideau Cruz" <Luisr at frs.fo>
> To: <r-help at stat.math.ethz.ch>
> Sent: Friday, October 15, 2004 1:01 PM
> Subject: [R] length with missing values
>
>
>> R-help
>>
>> I have a martix with missing values( in which I want the sample 
>> size by
>> column)
>> When I :
>>
>> apply(matrix,2,length)
>>
>> I get the length of the vector regardless of missing values.
>> I can't pass an argument to length in apply.
>>
>> Alternatively I could
>>
>> ifelse ( is.na ( matrix [, "columns in matrix " ] ) , 0 , 1)
>>
>> Is there any easier way?
>>
>> Thank you
>>
>> ______________________________________________
>> R-help at stat.math.ethz.ch mailing list
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide! 
>> http://www.R-project.org/posting-guide.html
>>
>
>



From ismith at mango-solutions.com  Fri Oct 15 13:26:07 2004
From: ismith at mango-solutions.com (Ian Smith)
Date: Fri, 15 Oct 2004 12:26:07 +0100
Subject: [R] length with missing values
In-Reply-To: <s16fbc21.048@ffdata.setur.fo>
Message-ID: <200410151126.i9FBQ4wX002511@hypatia.math.ethz.ch>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20041015/ed206499/attachment.pl

From sundar.dorai-raj at PDF.COM  Fri Oct 15 13:30:46 2004
From: sundar.dorai-raj at PDF.COM (Sundar Dorai-Raj)
Date: Fri, 15 Oct 2004 06:30:46 -0500
Subject: [R] length with missing values
In-Reply-To: <s16fbc21.048@ffdata.setur.fo>
References: <s16fbc21.048@ffdata.setur.fo>
Message-ID: <416FB4E6.2040207@pdf.com>



Luis Rideau Cruz wrote:

> R-help
> 
> I have a martix with missing values( in which I want the sample size by
> column)
> When I :
> 
> apply(matrix,2,length)
> 
> I get the length of the vector regardless of missing values.
> I can't pass an argument to length in apply.
> 
> Alternatively I could 
> 
> ifelse ( is.na ( matrix [, "columns in matrix " ] ) , 0 , 1)
> 
> Is there any easier way?
> 

I think you almost have it:

colSums(ifelse(is.na(x), 0, 1))

will return the number of non-NA elements in each column of x.

Is that what you want?

--sundar



From B.Rowlingson at lancaster.ac.uk  Fri Oct 15 13:28:01 2004
From: B.Rowlingson at lancaster.ac.uk (Barry Rowlingson)
Date: Fri, 15 Oct 2004 12:28:01 +0100
Subject: [R] length with missing values
In-Reply-To: <s16fbc21.048@ffdata.setur.fo>
References: <s16fbc21.048@ffdata.setur.fo>
Message-ID: <416FB441.9050701@lancaster.ac.uk>

Luis Rideau Cruz wrote:
> R-help
> 
> I have a martix with missing values( in which I want the sample size by
> column)
> When I :
> 
> apply(matrix,2,length)
> 
> I get the length of the vector regardless of missing values.
> I can't pass an argument to length in apply.
> 
> Alternatively I could 
> 
> ifelse ( is.na ( matrix [, "columns in matrix " ] ) , 0 , 1)
> 
> Is there any easier way?
> 

  Firstly, dont call your matrix 'matrix'. Would you call your dog 
'dog'? Anyway, it might clash with the function 'matrix'.

  > m
      [,1] [,2] [,3] [,4]
[1,]    1   NA    7   10
[2,]    2   NA    8   11
[3,]    3    6    9   NA

Here's one way:

  > apply(m,2,function(x){sum(!is.na(x))})
  [1] 3 1 3 2

  you can supply a function to apply() which gets a column (in this 
case) at a time. It returns a scalar.

  Another way is to apply 'sum' to the matrix of 0s and 1s got from 
!is.na(m):

  > apply(!is.na(m),2,sum)
  [1] 3 1 3 2


Doubtless greater R-souls than me will come up with faster and better ways.



From p.dalgaard at biostat.ku.dk  Fri Oct 15 13:43:30 2004
From: p.dalgaard at biostat.ku.dk (Peter Dalgaard)
Date: 15 Oct 2004 13:43:30 +0200
Subject: [R] length with missing values
In-Reply-To: <004201c4b2a9$710051c0$b2133a86@www.domain>
References: <001801c4b2a9$084e8fc0$ae133a86@www.domain>
	<004201c4b2a9$710051c0$b2133a86@www.domain>
Message-ID: <x21xg02om5.fsf@biostat.ku.dk>

"Dimitris Rizopoulos" <dimitris.rizopoulos at med.kuleuven.ac.be> writes:

> Hi Luis,
> 
> just use:
> 
> apply( mat, 2, function(x) length(x[!is.na(x)]) )


or:

apply(!is.na(mat),2,sum)

-- 
   O__  ---- Peter Dalgaard             Blegdamsvej 3  
  c/ /'_ --- Dept. of Biostatistics     2200 Cph. N   
 (*) \(*) -- University of Copenhagen   Denmark      Ph: (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)             FAX: (+45) 35327907



From dimitris.rizopoulos at med.kuleuven.ac.be  Fri Oct 15 13:50:26 2004
From: dimitris.rizopoulos at med.kuleuven.ac.be (Dimitris Rizopoulos)
Date: Fri, 15 Oct 2004 13:50:26 +0200
Subject: [R] length with missing values
References: <001801c4b2a9$084e8fc0$ae133a86@www.domain><004201c4b2a9$710051c0$b2133a86@www.domain>
	<x21xg02om5.fsf@biostat.ku.dk>
Message-ID: <005b01c4b2ad$29474740$b2133a86@www.domain>

or even:

colSums(!is.na(mat))

Best,
Dimitris


----- Original Message ----- 
From: "Peter Dalgaard" <p.dalgaard at biostat.ku.dk>
To: "Dimitris Rizopoulos" <dimitris.rizopoulos at med.kuleuven.ac.be>
Cc: ""Luis Rideau Cruz"" <Luisr at frs.fo>; <r-help at stat.math.ethz.ch>
Sent: Friday, October 15, 2004 1:43 PM
Subject: Re: [R] length with missing values


> "Dimitris Rizopoulos" <dimitris.rizopoulos at med.kuleuven.ac.be> 
> writes:
>
>> Hi Luis,
>>
>> just use:
>>
>> apply( mat, 2, function(x) length(x[!is.na(x)]) )
>
>
> or:
>
> apply(!is.na(mat),2,sum)
>
> -- 
>   O__  ---- Peter Dalgaard             Blegdamsvej 3
>  c/ /'_ --- Dept. of Biostatistics     2200 Cph. N
> (*) \(*) -- University of Copenhagen   Denmark      Ph: (+45) 
> 35327918
> ~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)             FAX: (+45) 
> 35327907
>



From petr.pikal at precheza.cz  Fri Oct 15 14:00:18 2004
From: petr.pikal at precheza.cz (Petr Pikal)
Date: Fri, 15 Oct 2004 14:00:18 +0200
Subject: [R] length with missing values
In-Reply-To: <s16fbc21.048@ffdata.setur.fo>
Message-ID: <416FD7F2.2944.10DC603@localhost>



On 15 Oct 2004 at 12:01, Luis Rideau Cruz wrote:

> R-help
> 
> I have a martix with missing values( in which I want the sample size
> by column) When I :
> 
> apply(matrix,2,length)

Hi

Try
apply(matrix, 2, function(x) sum(!is.na(x)))

matrix is reserved word, so it is preferable to use other name for 
your object.

Cheers
Petr

> 
> I get the length of the vector regardless of missing values.
> I can't pass an argument to length in apply.
> 
> Alternatively I could 
> 
> ifelse ( is.na ( matrix [, "columns in matrix " ] ) , 0 , 1)
> 
> Is there any easier way?
> 
> Thank you
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide!
> http://www.R-project.org/posting-guide.html

Petr Pikal
petr.pikal at precheza.cz



From ripley at stats.ox.ac.uk  Fri Oct 15 14:11:40 2004
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Fri, 15 Oct 2004 13:11:40 +0100 (BST)
Subject: [R] length with missing values
In-Reply-To: <416FB4E6.2040207@pdf.com>
Message-ID: <Pine.LNX.4.44.0410151308270.29582-100000@gannet.stats>

On Fri, 15 Oct 2004, Sundar Dorai-Raj wrote:

> Luis Rideau Cruz wrote:
> 
> > I have a martix with missing values( in which I want the sample size by
> > column)
> > When I :
> > 
> > apply(matrix,2,length)
> > 
> > I get the length of the vector regardless of missing values.
> > I can't pass an argument to length in apply.
> > 
> > Alternatively I could 
> > 
> > ifelse ( is.na ( matrix [, "columns in matrix " ] ) , 0 , 1)
> > 
> > Is there any easier way?
> > 
> 
> I think you almost have it:
> 
> colSums(ifelse(is.na(x), 0, 1))
> 
> will return the number of non-NA elements in each column of x.

colSums(!is.na(x)) is the same, and more obvious, I think.
Remember logical values are coerced to 0/1 in an arithmetical context.


-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From henric.nilsson at statisticon.se  Fri Oct 15 14:18:46 2004
From: henric.nilsson at statisticon.se (Henric Nilsson)
Date: Fri, 15 Oct 2004 14:18:46 +0200
Subject: [R] R plot problems
In-Reply-To: <007001c4b29c$93f5fa70$0b1a0e0a@PORTATILMARTA>
References: <8A910F1818425847A6D18C7832D207E502A9F8@ex115.smic-sh.com>
	<6.1.2.0.0.20041015112714.0ab6bea0@10.0.10.66>
	<007001c4b29c$93f5fa70$0b1a0e0a@PORTATILMARTA>
Message-ID: <6.1.2.0.0.20041015141117.0ac23d70@10.0.10.66>

At 10:51 2004-10-15 +0100, you wrote:

>If you are using base grafics, you use the argument srt (see help(par)),

How do you get a, say, 45 degree rotation of the axis labels using `srt'? 
The help page for par's `las' argument says

"Note that other string/character rotation (via argument srt to par) does 
not affect the axis labels"

Henric



From jfox at mcmaster.ca  Fri Oct 15 14:43:18 2004
From: jfox at mcmaster.ca (John Fox)
Date: Fri, 15 Oct 2004 08:43:18 -0400
Subject: [R] Testing for normality of residuals in a regression model
In-Reply-To: <416FC622.9060601@pigrecodata.net>
Message-ID: <20041015124319.ESMA26826.tomts10-srv.bellnexxia.net@JohnDesktop8300>

Dear Federico,

A problem with applying a standard test of normality to LS residuals is that
the residuals are correlated and heterskedastic even if the standard
assumptions of the model hold. In a large sample, this is unlikely to be
problematic (unless there's an unusual data configuration), but in a small
sample the effect could be nontrivial.

One approach is to use BLUS residuals, which transform the LS residuals to a
smaller set of uncorrelated, homoskedastic residuals (assuming the
correctness of the model). A search of R resources didn't turn up anything
for BLUS, but they shouldn't be hard to compute. This is a standard topic
covered in many econometrics texts.

You might consider the alternative of generating a bootstrapped confidence
envelope for the QQ plot; the qq.plot() function in the car package will do
this for a linear model.

I hope this helps,
 John

--------------------------------
John Fox
Department of Sociology
McMaster University
Hamilton, Ontario
Canada L8S 4M4
905-525-9140x23604
http://socserv.mcmaster.ca/jfox 
-------------------------------- 

> -----Original Message-----
> From: r-help-bounces at stat.math.ethz.ch 
> [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of 
> Federico Gherardini
> Sent: Friday, October 15, 2004 7:44 AM
> To: R-help at stat.math.ethz.ch
> Subject: [R] Testing for normality of residuals in a regression model
> 
> Hi all,
> 
> Is it possible to have a test value for assessing the 
> normality of residuals from a linear regression model, 
> instead of simply relying on qqplots?
> I've tried to use fitdistr to try and fit the residuals with 
> a normal distribution, but fitdsitr only returns the 
> parameters of the distribution and the standard errors, not 
> the p-value. Am I missing something?
> 
> Cheers,
> 
> Federico
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html



From MSchwartz at MedAnalytics.com  Fri Oct 15 14:47:22 2004
From: MSchwartz at MedAnalytics.com (Marc Schwartz)
Date: Fri, 15 Oct 2004 07:47:22 -0500
Subject: [R] R plot problems
In-Reply-To: <6.1.2.0.0.20041015141117.0ac23d70@10.0.10.66>
References: <8A910F1818425847A6D18C7832D207E502A9F8@ex115.smic-sh.com>
	<6.1.2.0.0.20041015112714.0ab6bea0@10.0.10.66>
	<007001c4b29c$93f5fa70$0b1a0e0a@PORTATILMARTA>
	<6.1.2.0.0.20041015141117.0ac23d70@10.0.10.66>
Message-ID: <1097844441.14574.20.camel@localhost.localdomain>

On Fri, 2004-10-15 at 07:18, Henric Nilsson wrote:
> At 10:51 2004-10-15 +0100, you wrote:
> 
> >If you are using base grafics, you use the argument srt (see help(par)),
> 
> How do you get a, say, 45 degree rotation of the axis labels using `srt'? 
> The help page for par's `las' argument says
> 
> "Note that other string/character rotation (via argument srt to par) does 
> not affect the axis labels"
> 
> Henric

See the new R FAQ:

7.27 How can I create rotated axis labels?

HTH,

Marc Schwartz



From maechler at stat.math.ethz.ch  Fri Oct 15 14:51:30 2004
From: maechler at stat.math.ethz.ch (Martin Maechler)
Date: Fri, 15 Oct 2004 14:51:30 +0200
Subject: [R] cluster analysis
In-Reply-To: <200410151136.14601.foadi@ysbl.york.ac.uk>
References: <200410151019.12406.foadi@ysbl.york.ac.uk>
	<Pine.GSO.3.95q.1041015113520.1894E-100000@sun11.math.uni-hamburg.de>
	<16751.41001.605346.333873@gargle.gargle.HOWL>
	<200410151136.14601.foadi@ysbl.york.ac.uk>
Message-ID: <16751.51154.160846.196511@gargle.gargle.HOWL>

>>>>> "James" == James Foadi <foadi at ysbl.york.ac.uk>
>>>>>     on Fri, 15 Oct 2004 11:36:14 +0100 writes:

    James> On Friday 15 Oct 2004 11:02 am, you wrote:
    >> 
    >> or unname(classi) -- which is slightly more expressive in this
    >> case and possibly more desirable in other situations.
    >> 
    >> Martin Maechler, ETH Zurich
    >> 

    James> Thanks, Martin.
    James> I've tried, like you suggested:

    James> un_classi <- unname(classi)

    James> but nothing changed. By typing, for instance:

    James> un_classi[[1]]

"of course" -- I just chimed in with Christian who proposed as.vector(.)
Since your 'classi' is a list with named vector as components,
you'd need something like

  un_classi <- lapply(classi, unname)

I'm sorry to have added more confusion.
OTOH, really, I think you should learn a bit more about basic
manipulation of R objects and study something like
"An Introduction to R".

Regards, Martin


    James> I still obtained twice the values. But, if I type:

    James> un_classe_01 <- unname(classi[[1]])

    James> the un_classe_01 is an unnamed vector.

(exactly, since it works on the *component* of a list)

    James> Cheers,

    James> james



From andy_liaw at merck.com  Fri Oct 15 14:58:18 2004
From: andy_liaw at merck.com (Liaw, Andy)
Date: Fri, 15 Oct 2004 08:58:18 -0400
Subject: [R] R plot problems
Message-ID: <3A822319EB35174CA3714066D590DCD504AF8545@usrymx25.merck.com>

Use text(), for which srt will work.  As an example:

plot(rnorm(20), rnorm(20), xaxt="n")
text(-2:2, rep(par("usr")[3], 5), 
     c("negative two", "negative one", "zero", "one", "two"),
     srt=45, xpd=NA, adj=1)

HTH,
Andy

> From: Henric Nilsson
> 
> At 10:51 2004-10-15 +0100, you wrote:
> 
> >If you are using base grafics, you use the argument srt (see 
> help(par)),
> 
> How do you get a, say, 45 degree rotation of the axis labels 
> using `srt'? 
> The help page for par's `las' argument says
> 
> "Note that other string/character rotation (via argument srt 
> to par) does 
> not affect the axis labels"
> 
> Henric
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
> 
>



From ramasamy at cancer.org.uk  Fri Oct 15 15:09:42 2004
From: ramasamy at cancer.org.uk (Adaikalavan Ramasamy)
Date: Fri, 15 Oct 2004 14:09:42 +0100
Subject: [R] C/C++
In-Reply-To: <3398909b041014173177b45d01@mail.gmail.com>
References: <3398909b041014173177b45d01@mail.gmail.com>
Message-ID: <1097845749.3286.17.camel@ndmpc126.ihs.ox.ac.uk>

Have you checked Section 5.14 (page 64) of R-extension titled "Using
these functions in your own C code".

Section 5.7 tells you about some of the available subroutines. 



On Fri, 2004-10-15 at 01:31, doktora v wrote:
> Hey everyone,
> 
> I have been looking for a while for ways to integrate R's wonderful
> functions into my C++ software, but I have not found anything
> concrete.
> 
> So finally, i post to this list to see if anyouse else knows about
> this, or has done it!? Is it possible? Are there C++ or C R libraries?
> Or is it sufficiently easy to build them?
> 
> your help is much appreciated!
> thanks
> doktora
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>



From rgutierrez at cmatoso.com  Fri Oct 15 15:54:04 2004
From: rgutierrez at cmatoso.com (Rafael Gutierrez)
Date: Fri, 15 Oct 2004 08:54:04 -0500
Subject: [R] edit plots from the ADE4 package
Message-ID: <s16f904d.056@proxy.cmatoso.com>

Dear R-Help

i have a newbie question, how edit size fonts and colors in the plots  (scatter.acm or dudi.acm function) of multiple correspondence analysis in the ADE4 Package?

thanks?


Rafael Gutierrez
Estad??stico
Unidad de Tecnolog??a Cerro Matoso S.A.
Tel. 4-7723350  Fax. 4-7723236

*************************************************************************************************

Este correo y sus anexos pueden ser confidenciales y estar protegidos por derechos de autor.

Estan dirigidos unica y exclusivamente para uso de el (los) destinatario(s). 

Si Usted por error lo ha recibido por favor notifiquelo inmediatamente al remitente y destruyalo de su sistema.

No debe copiar, ni imprimir, ni distribuir este correo o sus anexos, ni usarlos para proposito alguno ni dar a conocer su contenido a persona alguna.

Las opiniones, conclusiones y otra informacion contenida en este correo no relacionadas con el negocio oficial de Cerro Matoso, deben entenderse como personales y de ninguna manera son avaladas por la Compa??ia.

*************************************************************************************************

This message and any attachments may be confidential and may...{{dropped}}



From mayeul.kauffmann at tiscali.fr  Fri Oct 15 15:57:46 2004
From: mayeul.kauffmann at tiscali.fr (Mayeul KAUFFMANN)
Date: Fri, 15 Oct 2004 15:57:46 +0200
Subject: [R] constrained splines in GLM
Message-ID: <001101c4b2be$f371ac20$afca9853@amd>

Hi,
I would like to use constrained splines in a GLM model (Poisson link)
to take into account the nonlinear effect of some covariate. The
constraints I need are described below.
I have several variables that I need concurrently in the same model.

I looked at package mgcv but I do not know if/how I can use it in GLM (not
GAM) : I could
not manage to adapt the mono.con(mgcv) example to GLM.
The help for package fda is not complete.
Not sure that backSpline(splines) does what I need.
isoreg (modreg) seems to do univariate regressions.


Some of my covariates are linear.

Three covariates (x1,x2 and x3) must be transformed in a decreasing and
convex way like

this:

|o
|o
| o
|  o
|    o
|        o
|               ooooo
|-----------------

Currently, I use exp(-x1/alpha1)+exp(-x2/alpha2)+exp(-x3/alpha3), I try
several alpha's

and choose the best according to log-likelihood.


One variable should have only one local maximum (that is, the derivative
should be zero

only once, which is at the top), like this:
|
|          TOP
|          oo
|      o      o
|    o          o
|o o             o
|                 o o
|--------------------

with bs() or ns() and no constraint, I get:
|
|          TOP
|          oo
|o      o     o
|  o o          o
|                o
|                 o o
|--------------------
which is nonsense (note there are very few observations on the left part)


I also tried some parametric forms, choosing via log-likelihood. But with
four covariates,

it is a lot of parameters to try (several hours with little flexible
functions).



I am looking for something similar to ns or bs (package splines), which
are very

convenient to place in the formula of a GLM model. I tried them, adjusting
knots, but

could not manage what I want. Constraints on some derivatives may do the
trick, but I do

not know how to implement them in R.

Any help or comment would be greatly appreciated !

Mayeul KAUFFMANN
Universit?? Pierre Mend??s France - Grenoble
France



From f.gherardini at pigrecodata.net  Fri Oct 15 18:22:28 2004
From: f.gherardini at pigrecodata.net (Federico Gherardini)
Date: Fri, 15 Oct 2004 18:22:28 +0200
Subject: [R] Testing for normality of residuals in a regression model
In-Reply-To: <20041015124319.ESMA26826.tomts10-srv.bellnexxia.net@JohnDesktop8300>
References: <20041015124319.ESMA26826.tomts10-srv.bellnexxia.net@JohnDesktop8300>
Message-ID: <416FF944.1000007@pigrecodata.net>

Thank you very much for your suggestions! The residuals come from a gls 
model, because I had to correct for heteroscedasticity using a weighted 
regression... can I simply apply one of these tests (like shapiro.test) 
to the standardized residuals from my gls model?

Cheers,
Federico



From ivo_welch-rstat8783 at mailblocks.com  Fri Oct 15 16:13:39 2004
From: ivo_welch-rstat8783 at mailblocks.com (ivo_welch-rstat8783@mailblocks.com)
Date: Fri, 15 Oct 2004 07:13:39 -0700
Subject: [R] pdf device --- axes labels text disappeared?
In-Reply-To: <200410151007.i9FA4XAY011401@hypatia.math.ethz.ch>
References: <200410151007.i9FA4XAY011401@hypatia.math.ethz.ch>
Message-ID: <200410151413.i9FEDh8r018446@hypatia.math.ethz.ch>


Dear R Wizards:  Running R 1.9.1. on amd64.


Promise<- c(0,20,40); Expect<- c(0, 20, 0.2*20+.8*40 );

# this omits printing numbers on the axes labels.
pdf(file = "bug.pdf" );
plot(Promise, Expect, type="b", ylim=c(0,60));
dev.off();

# this works
postscript(file = "bug.eps" );
plot(Promise, Expect, type="b", ylim=c(0,60));
dev.off();


apologies if this has already been noted elsewhere---I looked but did 
not find it.  (probably googled for the wrong terms, if so.)  is this 
my bug, or R's bug or my R installation bug?  help appreciated.

/iaw



From kjetil at acelerate.com  Fri Oct 15 16:12:07 2004
From: kjetil at acelerate.com (Kjetil Brinchmann Halvorsen)
Date: Fri, 15 Oct 2004 10:12:07 -0400
Subject: [R] Testing for normality of residuals in a regression model
In-Reply-To: <20041015124319.ESMA26826.tomts10-srv.bellnexxia.net@JohnDesktop8300>
References: <20041015124319.ESMA26826.tomts10-srv.bellnexxia.net@JohnDesktop8300>
Message-ID: <416FDAB7.70404@acelerate.com>

John Fox wrote:

>Dear Federico,
>
>A problem with applying a standard test of normality to LS residuals is that
>the residuals are correlated and heterskedastic even if the standard
>assumptions of the model hold. In a large sample, this is unlikely to be
>problematic (unless there's an unusual data configuration), but in a small
>sample the effect could be nontrivial.
>
>One approach is to use BLUS residuals, which transform the LS residuals to a
>smaller set of uncorrelated, homoskedastic residuals (assuming the
>correctness of the model).
>
I'm not sure if this are BLUE residuals, but the following function 
transform to a
smaller set of independent, homoscedastic residuals and the calls 
shapiro.test:
I've proposed to make this a method for shapiro.test for "lm" objects, 
but it is
not accepted.

 shapiro.test.lm
function (obj)
{
    eff <- effects(obj)
    rank <- obj$rank
    df.r <- obj$df.residual
    if (df.r < 3)
        stop("To few degrees of freedom for residual for the test.")
    data.name <- deparse(substitute(obj))
    x <- eff[-(1:rank)]
    res <- shapiro.test(x)
    res$data.name <- data.name
    res$method <- paste(res$method, " for residuals of linear model")
    res
}

Kjetil


> A search of R resources didn't turn up anything
>for BLUS, but they shouldn't be hard to compute. This is a standard topic
>covered in many econometrics texts.
>
>You might consider the alternative of generating a bootstrapped confidence
>envelope for the QQ plot; the qq.plot() function in the car package will do
>this for a linear model.
>
>I hope this helps,
> John
>
>--------------------------------
>John Fox
>Department of Sociology
>McMaster University
>Hamilton, Ontario
>Canada L8S 4M4
>905-525-9140x23604
>http://socserv.mcmaster.ca/jfox 
>-------------------------------- 
>
>  
>
>>-----Original Message-----
>>From: r-help-bounces at stat.math.ethz.ch 
>>[mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of 
>>Federico Gherardini
>>Sent: Friday, October 15, 2004 7:44 AM
>>To: R-help at stat.math.ethz.ch
>>Subject: [R] Testing for normality of residuals in a regression model
>>
>>Hi all,
>>
>>Is it possible to have a test value for assessing the 
>>normality of residuals from a linear regression model, 
>>instead of simply relying on qqplots?
>>I've tried to use fitdistr to try and fit the residuals with 
>>a normal distribution, but fitdsitr only returns the 
>>parameters of the distribution and the standard errors, not 
>>the p-value. Am I missing something?
>>
>>Cheers,
>>
>>Federico
>>
>>______________________________________________
>>R-help at stat.math.ethz.ch mailing list
>>https://stat.ethz.ch/mailman/listinfo/r-help
>>PLEASE do read the posting guide! 
>>http://www.R-project.org/posting-guide.html
>>    
>>
>
>______________________________________________
>R-help at stat.math.ethz.ch mailing list
>https://stat.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>
>
>  
>


-- 

Kjetil Halvorsen.

Peace is the most effective weapon of mass construction.
               --  Mahdi Elmandjra



From kjetil at acelerate.com  Fri Oct 15 16:12:28 2004
From: kjetil at acelerate.com (Kjetil Brinchmann Halvorsen)
Date: Fri, 15 Oct 2004 10:12:28 -0400
Subject: [R] Testing for normality of residuals in a regression model
In-Reply-To: <20041015124319.ESMA26826.tomts10-srv.bellnexxia.net@JohnDesktop8300>
References: <20041015124319.ESMA26826.tomts10-srv.bellnexxia.net@JohnDesktop8300>
Message-ID: <416FDACC.4070203@acelerate.com>

John Fox wrote:

>Dear Federico,
>
>A problem with applying a standard test of normality to LS residuals is that
>the residuals are correlated and heterskedastic even if the standard
>assumptions of the model hold. In a large sample, this is unlikely to be
>problematic (unless there's an unusual data configuration), but in a small
>sample the effect could be nontrivial.
>
>One approach is to use BLUS residuals, which transform the LS residuals to a
>smaller set of uncorrelated, homoskedastic residuals (assuming the
>correctness of the model). A search of R resources didn't turn up anything
>for BLUS, but they shouldn't be hard to compute. This is a standard topic
>covered in many econometrics texts.
>
>You might consider the alternative of generating a bootstrapped confidence
>envelope for the QQ plot; the qq.plot() function in the car package will do
>this for a linear model.
>
>I hope this helps,
> John
>
>--------------------------------
>John Fox
>Department of Sociology
>McMaster University
>Hamilton, Ontario
>Canada L8S 4M4
>905-525-9140x23604
>http://socserv.mcmaster.ca/jfox 
>-------------------------------- 
>
>  
>
>>-----Original Message-----
>>From: r-help-bounces at stat.math.ethz.ch 
>>[mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of 
>>Federico Gherardini
>>Sent: Friday, October 15, 2004 7:44 AM
>>To: R-help at stat.math.ethz.ch
>>Subject: [R] Testing for normality of residuals in a regression model
>>
>>Hi all,
>>
>>Is it possible to have a test value for assessing the 
>>normality of residuals from a linear regression model, 
>>instead of simply relying on qqplots?
>>I've tried to use fitdistr to try and fit the residuals with 
>>a normal distribution, but fitdsitr only returns the 
>>parameters of the distribution and the standard errors, not 
>>the p-value. Am I missing something?
>>
>>Cheers,
>>
>>Federico
>>
>>______________________________________________
>>R-help at stat.math.ethz.ch mailing list
>>https://stat.ethz.ch/mailman/listinfo/r-help
>>PLEASE do read the posting guide! 
>>http://www.R-project.org/posting-guide.html
>>    
>>
>
>______________________________________________
>R-help at stat.math.ethz.ch mailing list
>https://stat.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>
>
>  
>


-- 

Kjetil Halvorsen.

Peace is the most effective weapon of mass construction.
               --  Mahdi Elmandjra



From talitaperciano at hotmail.com  Fri Oct 15 16:28:55 2004
From: talitaperciano at hotmail.com (Talita Leite)
Date: Fri, 15 Oct 2004 12:28:55 -0200
Subject: [R] Problems saving a graphic
Message-ID: <BAY14-F15ebfaxI1bbh0001cedf@hotmail.com>

Hi!

I'm with a little problem when saving a graphic into pdf. The graphic has 
the label: "Milh??es de toneladas". The problem is with the accent. After I 
save the graphic into pdf the "??" doesn't appear.
Somebody could help me?

Thanks,


Talita Perciano Costa Leite
Graduanda em Ci??ncia da Computa????o
Universidade Federal de Alagoas - UFAL
Departamento de Tecnologia da Informa????o - TCI
Constru????o de Conhecimento por Agrupamento de Dados - CoCADa



From trujillo at soton.ac.uk  Fri Oct 15 16:30:26 2004
From: trujillo at soton.ac.uk (Trujillo L.)
Date: Fri, 15 Oct 2004 15:30:26 +0100
Subject: [R] edit plots from the ADE4 package
Message-ID: <41310D8A2BD91B4CB42AB540EF248F59B75C88@exchange4.soton.ac.uk>

Hi

Try to change the parameters in the s.class function corresponding to col (which is 1 by default) and using the parameter font as a small positive integer determining a text font for characters and hence an interline spacing.

Leonardo Trujillo
Southamtpton Statistical Sciences Research Institute
trujillo at soton.ac.uk  

-----Original Message-----
From: r-help-bounces at stat.math.ethz.ch [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Rafael Gutierrez
Sent: 15 October 2004 14:54
To: r-help at stat.math.ethz.ch
Subject: [R] edit plots from the ADE4 package

Dear R-Help

i have a newbie question, how edit size fonts and colors in the plots  (scatter.acm or dudi.acm function) of multiple correspondence analysis in the ADE4 Package?

thanks?


Rafael Gutierrez
Estad??stico
Unidad de Tecnolog??a Cerro Matoso S.A.
Tel. 4-7723350  Fax. 4-7723236

*************************************************************************************************

Este correo y sus anexos pueden ser confidenciales y estar protegidos por derechos de autor.

Estan dirigidos unica y exclusivamente para uso de el (los) destinatario(s). 

Si Usted por error lo ha recibido por favor notifiquelo inmediatamente al remitente y destruyalo de su sistema.

No debe copiar, ni imprimir, ni distribuir este correo o sus anexos, ni usarlos para proposito alguno ni dar a conocer su contenido a persona alguna.

Las opiniones, conclusiones y otra informacion contenida en este correo no relacionadas con el negocio oficial de Cerro Matoso, deben entenderse como personales y de ninguna manera son avaladas por la Compa??ia.

*************************************************************************************************

This message and any attachments may be confidential and may...{{dropped}}

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html



From wwei at mdanderson.org  Fri Oct 15 16:34:24 2004
From: wwei at mdanderson.org (Auston Wei)
Date: Fri, 15 Oct 2004 14:34:24 +0000 (UTC)
Subject: [R] does rpart has similar function as edit.tree?
Message-ID: <loom.20041015T163227-402@post.gmane.org>

Hi, list,

The 'tree' package has edit.tree to change split variables by user. 
Does 'rpart' package have something similar? I couldn't find either in the 
manual or archive. Thanks in advance.

Best,
Auston



From Bernhard.Pfaff at drkw.com  Fri Oct 15 16:37:59 2004
From: Bernhard.Pfaff at drkw.com (Pfaff, Bernhard)
Date: Fri, 15 Oct 2004 16:37:59 +0200
Subject: [R] pdf device --- axes labels text disappeared?
Message-ID: <18D602BD42B7E24EB810D6454A58DB900A29BBD3@ibfftce505.de.ad.drkw.net>

> Dear R Wizards:  Running R 1.9.1. on amd64.
> 
> 
> Promise<- c(0,20,40); Expect<- c(0, 20, 0.2*20+.8*40 );
> 
> # this omits printing numbers on the axes labels.
> pdf(file = "bug.pdf" );
> plot(Promise, Expect, type="b", ylim=c(0,60));
> dev.off();
> 
> # this works
> postscript(file = "bug.eps" );
> plot(Promise, Expect, type="b", ylim=c(0,60));
> dev.off();
> 

Dear Ivo,

although I do not want to consider myself as an R wizard, but rather a
helpful soul and because you have provided R-code suited for direct
execution as it should be :-), here is my answer:

in both devices the numbers are printed below the axes. This is on:

platform i386-pc-mingw32
arch     i386           
os       mingw32        
system   i386, mingw32  
status                  
major    2              
minor    0.0            
year     2004           
month    10             
day      04             
language R              


HTH,
Bernhard

ps: Could the problem be related to your pdf encodings?
> 
> apologies if this has already been noted elsewhere---I looked but did 
> not find it.  (probably googled for the wrong terms, if so.)  is this 
> my bug, or R's bug or my R installation bug?  help appreciated.
> 
> /iaw
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
> 


--------------------------------------------------------------------------------
The information contained herein is confidential and is inte...{{dropped}}



From MSchwartz at MedAnalytics.com  Fri Oct 15 16:44:10 2004
From: MSchwartz at MedAnalytics.com (Marc Schwartz)
Date: Fri, 15 Oct 2004 09:44:10 -0500
Subject: [R] pdf device --- axes labels text disappeared?
In-Reply-To: <200410151413.i9FEDh8r018446@hypatia.math.ethz.ch>
References: <200410151007.i9FA4XAY011401@hypatia.math.ethz.ch>
	<200410151413.i9FEDh8r018446@hypatia.math.ethz.ch>
Message-ID: <1097851450.14574.25.camel@localhost.localdomain>

On Fri, 2004-10-15 at 09:13, ivo_welch-rstat8783 at mailblocks.com wrote:
> Dear R Wizards:  Running R 1.9.1. on amd64.
> 
> 
> Promise<- c(0,20,40); Expect<- c(0, 20, 0.2*20+.8*40 );
> 
> # this omits printing numbers on the axes labels.
> pdf(file = "bug.pdf" );
> plot(Promise, Expect, type="b", ylim=c(0,60));
> dev.off();
> 
> # this works
> postscript(file = "bug.eps" );
> plot(Promise, Expect, type="b", ylim=c(0,60));
> dev.off();
> 
> 
> apologies if this has already been noted elsewhere---I looked but did 
> not find it.  (probably googled for the wrong terms, if so.)  is this 
> my bug, or R's bug or my R installation bug?  help appreciated.
> 
> /iaw

Hey Ivo!

No problems here using R 2.0.0 on 32 bit Intel FC2. Both plots are fine.

I don't have a 1.9.1 install at the moment, but I would guess that we
would have heard about the problem prior to this.

That suggests a possible issue with your install or a change of some
sort with 64 bit R. Though again, I suspect other 64 bit users would
have commented on this already...

HTH,

Marc Schwartz



From ggrothendieck at myway.com  Fri Oct 15 16:47:25 2004
From: ggrothendieck at myway.com (Gabor Grothendieck)
Date: Fri, 15 Oct 2004 14:47:25 +0000 (UTC)
Subject: [R] Problem with number characters
References: <20041014193054.CDDF439FD@mprdmxin.myway.com>
	<416EE474.6030302@pdf.com>
	<16751.30736.477306.49343@gargle.gargle.HOWL>
Message-ID: <loom.20041015T164446-429@post.gmane.org>


Note that there are also regexp classes that define certain character
sets, most notably [:graph:] , which can make it easy to create 
appropriate regexps.  More is in ?regex .

Martin Maechler <maechler <at> stat.math.ethz.ch> writes:

: 
: >>>>> "Spencer" == Spencer Graves <spencer.graves <at> pdf.com>
: >>>>>     on Thu, 14 Oct 2004 13:41:24 -0700 writes:
: 
:     Spencer>   It looks like you have several non-printing
:     Spencer> characters.  "nchar" will give you the total number
:     Spencer> of characters in each character string.
: 
:     Spencer> "strsplit" can break character strings into single
:     Spencer> characters, and "%in%" can be used to classify
:     Spencer> them.
: 
: and you give nice coding examples:
: 
:     Spencer> Consider the following:
:     >> x <- "Draszt 0%/1???????iso8859-15????"
:     >> nx <- nchar(x)
:     >> x. <- strsplit(x, "")
:     >> length(x.[[1]])
:     Spencer> [1] 29
:     >> 
:     >> namechars <- c(letters, LETTERS, as.character(0:9), ".")
: 
: just to be precise:  If 'namechars' is supposed to mean
: ``characters valid in R object names'', then you should have
: added "_" as well:
: 
: namechars <- c(letters, LETTERS, as.character(0:9), ".", "_")
: 
:     >> punctuation <- c(",", "!", "+", "*", "&", "|")
:     >> legalchars <- c(namechars, punctuation)
: 
: and 'legalchars' would have to contain quite a bit more I
: presume, e.g. "$", " <at> ", ....
: (but that wouldn't have been a reason to write this e-mail..)
: 
:     >> legalx <- lapply(x., function(y)(y %in% legalchars))
:     >> x.[[1]][!legalx[[1]]]
:     Spencer> [1] " " "" "%" "/" "??" "?" "??" "??" "-" "" "??" "??"
:     >> 
:     >> sapply(legalx, sum)
:     Spencer> [1] 17
: 
:     Spencer> Will this give you ideas about what to do what you want?
:     Spencer> hope this helps. spencer graves
: 
: (and this too)
: 
: Martin Maechler, ETH Zurich
: 
: 
:     Spencer> Gabor Grothendieck wrote:
: 
:     >> Assuming that the problem is that your input file has 
:     >> additional embedded characters added by the data base
:     >> program you could try extracting just the text using
:     >> the UNIX strings program:
:     >> 
:     >> strings myfile.csv > myfile.txt
:     >> 
:     >> and see if myfile.txt works with R and if not check out
:     >> what the differences are between it and the .csv file.
:     >> 
:     >> Date:   Thu, 14 Oct 2004 11:31:33 -0700 
:     >> From:   Scott Waichler <scott.waichler <at> pnl.gov>
:     >> To:   <r-help <at> stat.math.ethz.ch> 
:     >> Subject:   [R] Problem with number characters 
:     >> 
:     >> 
:     >> I am trying to process text fields scanned in from a csv file that is
:     >> output from the Windows database program FileMakerPro. The characters
:     >> onscreen look like regular text, but R does not like their underlying 
binary form.
:     >> For example, one of text fields contains a name and a number, but
:     >> R recognizes the number as something other than what it appears
:     >> to be in plain text. The character string "Draszt 03" after being
:     >> read into R using scan and ="" becomes "Draszt 03" where the 3 is 
:     >> displayed in my R session as a superscript. Here is the result pasted
:     >> into this email I'm composing in emacs: "Draszt 0%/1???????iso8859-
15????"
:     >> Another clue for the knowledgable: when I try to display the vector 
element
:     >> causing trouble, I get
:     >> <CHARSXP: "Draszt 0%/1???????iso8859-15????">
:     >> where again the superscipt part is just "3" in my R session. I'm 
working in
:     >> Linux, R version 1.9.1, 2004-06-21. Your help will be much 
appreciated.
:     >> 
:     >> Scott Waichler
:     >> Pacific Northwest National Laboratory
:     >> scott.waichler <at> pnl.gov
: 
: ______________________________________________
: R-help <at> stat.math.ethz.ch mailing list
: https://stat.ethz.ch/mailman/listinfo/r-help
: PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
: 
:



From steffen.katzner at mail.gwdg.de  Fri Oct 15 16:45:40 2004
From: steffen.katzner at mail.gwdg.de (Steffen Katzner)
Date: Fri, 15 Oct 2004 16:45:40 +0200
Subject: [R] Within-Subjects ANOVA & TukeyHSD
Message-ID: <416FE294.3060802@mail.gwdg.de>

Is there a problem with using TukeyHSD for individual comparisons of 
means coming from a within-subjects ANOVA?
I always get: "no applicable method for TukeyHSD"
Thank you,  Steffen



From ripley at stats.ox.ac.uk  Fri Oct 15 16:51:51 2004
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Fri, 15 Oct 2004 15:51:51 +0100 (BST)
Subject: [R] pdf device --- axes labels text disappeared?
In-Reply-To: <200410151413.i9FEDh8r018446@hypatia.math.ethz.ch>
Message-ID: <Pine.LNX.4.44.0410151546350.24176-100000@gannet.stats>

No problems under R 2.0.0 or 1.9.1 on that platform.

Are you *sure* this is a bug in the PDF and not in your pdf reader?
Please read the PDF source file to find out.  Mine has lines like

/F2 1 Tf 12.00 0.00 -0.00 12.00 68.40 47.52 Tm (0) Tj
/F2 1 Tf 12.00 0.00 -0.00 12.00 144.39 47.52 Tm (10) Tj
/F2 1 Tf 12.00 0.00 -0.00 12.00 223.73 47.52 Tm (20) Tj
/F2 1 Tf 12.00 0.00 -0.00 12.00 303.06 47.52 Tm (30) Tj
/F2 1 Tf 12.00 0.00 -0.00 12.00 382.39 47.52 Tm (40) Tj


On Fri, 15 Oct 2004 ivo_welch-rstat8783 at mailblocks.com wrote:

> 
> Dear R Wizards:  Running R 1.9.1. on amd64.
> 
> 
> Promise<- c(0,20,40); Expect<- c(0, 20, 0.2*20+.8*40 );
> 
> # this omits printing numbers on the axes labels.
> pdf(file = "bug.pdf" );
> plot(Promise, Expect, type="b", ylim=c(0,60));
> dev.off();
> 
> # this works
> postscript(file = "bug.eps" );
> plot(Promise, Expect, type="b", ylim=c(0,60));
> dev.off();
> 
> 
> apologies if this has already been noted elsewhere---I looked but did 
> not find it.  (probably googled for the wrong terms, if so.)  is this 
> my bug, or R's bug or my R installation bug?  help appreciated.
> 
> /iaw
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
> 
> 

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From cswingle at iarc.uaf.edu  Fri Oct 15 17:18:39 2004
From: cswingle at iarc.uaf.edu (Christopher Swingley)
Date: Fri, 15 Oct 2004 07:18:39 -0800
Subject: [R] pdf device --- axes labels text disappeared?
In-Reply-To: <200410151413.i9FEDh8r018446@hypatia.math.ethz.ch>
References: <200410151007.i9FA4XAY011401@hypatia.math.ethz.ch>
	<200410151413.i9FEDh8r018446@hypatia.math.ethz.ch>
Message-ID: <20041015151839.GB27403@iarc.uaf.edu>

/iaw,

* <ivo_welch-rstat8783 at mailblocks.com> [2004-Oct-15 06:13 AKDT]:
> Dear R Wizards:  Running R 1.9.1. on amd64.
> 
> Promise<- c(0,20,40); Expect<- c(0, 20, 0.2*20+.8*40 );
> 
> # this omits printing numbers on the axes labels.
> pdf(file = "bug.pdf" );
> plot(Promise, Expect, type="b", ylim=c(0,60));
> dev.off();

Version 1.9.1  (2004-06-21), AMD64, Debian amd64 unstable port.

The plot I produced using these commands had all the numbers and lables.  
I used 'xpdf' version 3.00 to display the plot.  Maybe it's your PDF 
viewer, rather than R?

Chris
-- 
Christopher S. Swingley          email: cswingle at iarc.uaf.edu (work)
Intl. Arctic Research Center            cswingle at gmail.com (personal)
University of Alaska Fairbanks   www.frontier.iarc.uaf.edu/~cswingle/



From gunter.berton at gene.com  Fri Oct 15 17:19:05 2004
From: gunter.berton at gene.com (Berton Gunter)
Date: Fri, 15 Oct 2004 08:19:05 -0700
Subject: [R] Testing for normality of residuals in a regression model
In-Reply-To: <20041015124319.ESMA26826.tomts10-srv.bellnexxia.net@JohnDesktop8300>
Message-ID: <200410151519.i9FFJ5Tg028686@faraday.gene.com>

Quite right, John!

I have 2 additional questions:

1) Why test for normality of residuals? Suppose you reject -- then what?
(residual plots may give information on skewness, multi-modality, data
"anomalies" that can affect the data analysis).

2) Why test for normality? Is it EVER useful? Suppose you reject -- then
what?

(I am tempted to add a 3rd question -- why test at all? -- but that is
perhaps too iconoclastic and certainly off topic. Let the hounds remain
leashed for now.)

Cheers,

-- Bert Gunter
Genentech Non-Clinical Statistics
South San Francisco, CA
 
"The business of the statistician is to catalyze the scientific learning
process."  - George E. P. Box
 
 

> -----Original Message-----
> From: r-help-bounces at stat.math.ethz.ch 
> [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of John Fox
> Sent: Friday, October 15, 2004 5:43 AM
> To: 'Federico Gherardini'; R-help at stat.math.ethz.ch
> Subject: RE: [R] Testing for normality of residuals in a 
> regression model
> 
> Dear Federico,
> 
> A problem with applying a standard test of normality to LS 
> residuals is that
> the residuals are correlated and heterskedastic even if the standard
> assumptions of the model hold. In a large sample, this is 
> unlikely to be
> problematic (unless there's an unusual data configuration), 
> but in a small
> sample the effect could be nontrivial.
> 
> One approach is to use BLUS residuals, which transform the LS 
> residuals to a
> smaller set of uncorrelated, homoskedastic residuals (assuming the
> correctness of the model). A search of R resources didn't 
> turn up anything
> for BLUS, but they shouldn't be hard to compute. This is a 
> standard topic
> covered in many econometrics texts.
> 
> You might consider the alternative of generating a 
> bootstrapped confidence
> envelope for the QQ plot; the qq.plot() function in the car 
> package will do
> this for a linear model.
> 
> I hope this helps,
>  John
> 
> --------------------------------
> John Fox
> Department of Sociology
> McMaster University
> Hamilton, Ontario
> Canada L8S 4M4
> 905-525-9140x23604
> http://socserv.mcmaster.ca/jfox 
> -------------------------------- 
> 
> > -----Original Message-----
> > From: r-help-bounces at stat.math.ethz.ch 
> > [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of 
> > Federico Gherardini
> > Sent: Friday, October 15, 2004 7:44 AM
> > To: R-help at stat.math.ethz.ch
> > Subject: [R] Testing for normality of residuals in a 
> regression model
> > 
> > Hi all,
> > 
> > Is it possible to have a test value for assessing the 
> > normality of residuals from a linear regression model, 
> > instead of simply relying on qqplots?
> > I've tried to use fitdistr to try and fit the residuals with 
> > a normal distribution, but fitdsitr only returns the 
> > parameters of the distribution and the standard errors, not 
> > the p-value. Am I missing something?
> > 
> > Cheers,
> > 
> > Federico
> > 
> > ______________________________________________
> > R-help at stat.math.ethz.ch mailing list
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide! 
> > http://www.R-project.org/posting-guide.html
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
>



From erich.neuwirth at univie.ac.at  Fri Oct 15 17:27:00 2004
From: erich.neuwirth at univie.ac.at (Erich Neuwirth)
Date: Fri, 15 Oct 2004 17:27:00 +0200
Subject: [R] one more Rcmdr problem
In-Reply-To: <200410131829.i9DITvZ9024159@swork.sas.unibuc.ro>
References: <200410131829.i9DITvZ9024159@swork.sas.unibuc.ro>
Message-ID: <416FEC44.30506@univie.ac.at>

I did experience the same problem.
After installing R 2.0.0 patched and downloading the source for
Rcmdr_0.99-12 from John Fox's Web page
http://socserv.socsci.mcmaster.ca/jfox/Misc/Rcmdr/
and recompiling the package
it now works.

Ctrl-C, Ctrl-X, and Ctrl-V,  in the text boxes of Rcmdr now work.



Adrian Dusa wrote:
> Hello,
> 
> I'm using R 2.0.0 with the latest Rcmdr package installed from CRAN, on
> Windows XP Professional.
> 
> When trying to copy some commands or results, either from the upper or lower
> text window, this causes Rcmdr to crash:
> 
> "R for Windows GUI front-end has encountered a problem and needs to close"
> 
> Did anyone have the same problem? I don't think it's my system, as it
> happened to reinstall my Windows just a few days ago, and the same problem
> occurred in the former one.
> 
> Regards,
> Adrian
> 
> ~~~~~~~~~~~~~~~~~~~~~~~~~~~~
> Adrian Dusa
> Romanian Social Data Archive
> 1 Schitu Magureanu Bd.
> 050025 Bucharest sector 5
> Tel. +40 21 3126618\
>      +40 21 3153122/ int.101
> 
> 
> 


-- 
Erich Neuwirth, Computer Supported Didactics Working Group
Visit our SunSITE at http://sunsite.univie.ac.at
Phone: +43-1-4277-38624 Fax: +43-1-4277-9386



From f.gherardini at pigrecodata.net  Fri Oct 15 19:42:00 2004
From: f.gherardini at pigrecodata.net (Federico Gherardini)
Date: Fri, 15 Oct 2004 19:42:00 +0200
Subject: [R] Testing for normality of residuals in a regression model
In-Reply-To: <200410151519.i9FFJ5Tg028686@faraday.gene.com>
References: <200410151519.i9FFJ5Tg028686@faraday.gene.com>
Message-ID: <41700BE8.9000401@pigrecodata.net>

Berton Gunter wrote:

>Quite right, John!
>
>I have 2 additional questions:
>
>1) Why test for normality of residuals? Suppose you reject -- then what?
>(residual plots may give information on skewness, multi-modality, data
>"anomalies" that can affect the data analysis).
>  
>
Because I want to know if my model satisfies the basic assumptions of 
regression theory... in other words I want to know if I can "trust" my 
model.

Cheers,

Federico

>2) Why test for normality? Is it EVER useful? Suppose you reject -- then
>what?
>
>(I am tempted to add a 3rd question -- why test at all? -- but that is
>perhaps too iconoclastic and certainly off topic. Let the hounds remain
>leashed for now.)
>
>Cheers,
>
>-- Bert Gunter
>Genentech Non-Clinical Statistics
>South San Francisco, CA
> 
>
>  
>



From ivo_welch-rstat8783 at mailblocks.com  Fri Oct 15 17:44:38 2004
From: ivo_welch-rstat8783 at mailblocks.com (ivo_welch-rstat8783@mailblocks.com)
Date: Fri, 15 Oct 2004 08:44:38 -0700
Subject: [R] pdf device --- axes labels text disappeared?
In-Reply-To: <Pine.LNX.4.44.0410151546350.24176-100000@gannet.stats>
Message-ID: <200410151544.i9FFiudF012590@hypatia.math.ethz.ch>


Hi:  Thank you everyone.  You were all correct.  This turns out to be a 
viewer bug---and even more likely a viewer installation bug, not an 
xpdf bug, much less an R bug.  Apologies for having wasted everyone's 
time.

Regards, /ivo

---
ivo welch



From Mike.Prager at noaa.gov  Fri Oct 15 17:46:59 2004
From: Mike.Prager at noaa.gov (Mike Prager)
Date: Fri, 15 Oct 2004 11:46:59 -0400
Subject: [R] Problem with number characters
In-Reply-To: <loom.20041015T164446-429@post.gmane.org>
References: <20041014193054.CDDF439FD@mprdmxin.myway.com>
	<416EE474.6030302@pdf.com>
	<16751.30736.477306.49343@gargle.gargle.HOWL>
	<loom.20041015T164446-429@post.gmane.org>
Message-ID: <6.1.2.0.2.20041015114204.03d66b30@hermes.nos.noaa.gov>

Scott,

Has anyone suggested yet that some options might need adjusting in the 
Windows program that wrote the file?  In my experience, CSV files from 
Windows applications are typically pure ASCII (though yours clearly isn't).

Another possibility is that the program can export in plain ASCII with the 
fields in set positions.  Sometimes that can be done by capturing output 
intended for basic printers.  Either of those approaches might be simpler 
than trying to fiddle with the quirky output provided.  Reverse engineering 
is error prone.

MHP


At 10/15/2004 10:47 AM Friday, you wrote:

>Note that there are also regexp classes that define certain character
>sets, most notably [:graph:] , which can make it easy to create
>appropriate regexps.  More is in ?regex .
>
>Martin Maechler <maechler <at> stat.math.ethz.ch> writes:
>
>:
>: >>>>> "Spencer" == Spencer Graves <spencer.graves <at> pdf.com>
>: >>>>>     on Thu, 14 Oct 2004 13:41:24 -0700 writes:
>:
>:     Spencer>   It looks like you have several non-printing
>:     Spencer> characters.  "nchar" will give you the total number
>:     Spencer> of characters in each character string.
>:
>:     Spencer> "strsplit" can break character strings into single
>:     Spencer> characters, and "%in%" can be used to classify
>:     Spencer> them.
>:
>: and you give nice coding examples:
>:
>:     Spencer> Consider the following:
>:     >> x <- "Draszt 0%/1?????????????iso8859-15????????"
>:     >> nx <- nchar(x)
>:     >> x. <- strsplit(x, "")
>:     >> length(x.[[1]])
>:     Spencer> [1] 29
>:     >>
>:     >> namechars <- c(letters, LETTERS, as.character(0:9), ".")
>:
>: just to be precise:  If 'namechars' is supposed to mean
>: ``characters valid in R object names'', then you should have
>: added "_" as well:
>:
>: namechars <- c(letters, LETTERS, as.character(0:9), ".", "_")
>:
>:     >> punctuation <- c(",", "!", "+", "*", "&", "|")
>:     >> legalchars <- c(namechars, punctuation)
>:
>: and 'legalchars' would have to contain quite a bit more I
>: presume, e.g. "$", " <at> ", ....
>: (but that wouldn't have been a reason to write this e-mail..)
>:
>:     >> legalx <- lapply(x., function(y)(y %in% legalchars))
>:     >> x.[[1]][!legalx[[1]]]
>:     Spencer> [1] " " "" "%" "/" "????" "??" "????" "???" "-" "" "????" "????"
>:     >>
>:     >> sapply(legalx, sum)
>:     Spencer> [1] 17
>:
>:     Spencer> Will this give you ideas about what to do what you want?
>:     Spencer> hope this helps. spencer graves
>:
>: (and this too)
>:
>: Martin Maechler, ETH Zurich
>:
>:
>:     Spencer> Gabor Grothendieck wrote:
>:
>:     >> Assuming that the problem is that your input file has
>:     >> additional embedded characters added by the data base
>:     >> program you could try extracting just the text using
>:     >> the UNIX strings program:
>:     >>
>:     >> strings myfile.csv > myfile.txt
>:     >>
>:     >> and see if myfile.txt works with R and if not check out
>:     >> what the differences are between it and the .csv file.
>:     >>
>:     >> Date:   Thu, 14 Oct 2004 11:31:33 -0700
>:     >> From:   Scott Waichler <scott.waichler <at> pnl.gov>
>:     >> To:   <r-help <at> stat.math.ethz.ch>
>:     >> Subject:   [R] Problem with number characters
>:     >>
>:     >>
>:     >> I am trying to process text fields scanned in from a csv file that is
>:     >> output from the Windows database program FileMakerPro. The characters
>:     >> onscreen look like regular text, but R does not like their 
>underlying
>binary form.
>:     >> For example, one of text fields contains a name and a number, but
>:     >> R recognizes the number as something other than what it appears
>:     >> to be in plain text. The character string "Draszt 03" after being
>:     >> read into R using scan and ="" becomes "Draszt 03" where the 3 is
>:     >> displayed in my R session as a superscript. Here is the result pasted
>:     >> into this email I'm composing in emacs: "Draszt 0%/1?????????????iso8859-
>15????????"
>:     >> Another clue for the knowledgable: when I try to display the vector
>element
>:     >> causing trouble, I get
>:     >> <CHARSXP: "Draszt 0%/1?????????????iso8859-15????????">
>:     >> where again the superscipt part is just "3" in my R session. I'm
>working in
>:     >> Linux, R version 1.9.1, 2004-06-21. Your help will be much
>appreciated.
>:     >>
>:     >> Scott Waichler
>:     >> Pacific Northwest National Laboratory
>:     >> scott.waichler <at> pnl.gov
>:
>: ______________________________________________
>: R-help <at> stat.math.ethz.ch mailing list
>: https://stat.ethz.ch/mailman/listinfo/r-help
>: PLEASE do read the posting guide! 
>http://www.R-project.org/posting-guide.html
>:
>:
>
>______________________________________________
>R-help at stat.math.ethz.ch mailing list
>https://stat.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html


-- 
Michael Prager
NOAA Center for Coastal Fisheries and Habitat Research
Beaufort, North Carolina  28516  USA
http://shrimp.ccfhrb.noaa.gov/~mprager/

NOTE: Opinions expressed are personal, not official. No government
endorsement of any product is made or implied.



From mableu at eden.rutgers.edu  Fri Oct 15 17:57:06 2004
From: mableu at eden.rutgers.edu (Nebahat Noyan)
Date: Fri, 15 Oct 2004 11:57:06 -0400
Subject: [R] add-on package/windows installation probLem
Message-ID: <000f01c4b2cf$9ec19e40$c81c0680@userw2mpwlmw4r>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20041015/cd9429ab/attachment.pl

From trujillo at soton.ac.uk  Fri Oct 15 18:06:19 2004
From: trujillo at soton.ac.uk (Trujillo L.)
Date: Fri, 15 Oct 2004 17:06:19 +0100
Subject: [R] edit plots from the ADE4 package
Message-ID: <41310D8A2BD91B4CB42AB540EF248F59E73C82@exchange4.soton.ac.uk>

Hi again,

You could change the size font using the parameter clabel in both functions s.label and s.class.

For the typical example in Fine, 1996 you could notice that slabel(acm$co,clabel=1) produce different results than slabel(acm$co,clabel=0),slabel(acm$co,clabel=0.5),slabel(acm$co,clabel=4). The size of the point could be changed using cpoint. 

Using s.class you could use col=seq(length(levels(fac))) (instead change col = rep(1, length(levels(fac)))), which produces different colours according to the illustrative factor levels.

Leonardo Trujillo
University of Southampton
Southampton Statistical Sciences Research Institute
trujillo at soton.ac.uk

-----Original Message-----
From: r-help-bounces at stat.math.ethz.ch [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Rafael Gutierrez
Sent: 15 October 2004 14:54
To: r-help at stat.math.ethz.ch
Subject: [R] edit plots from the ADE4 package

Dear R-Help

i have a newbie question, how edit size fonts and colors in the plots  (scatter.acm or dudi.acm function) of multiple correspondence analysis in the ADE4 Package?

thanks?


Rafael Gutierrez
Estad??stico
Unidad de Tecnolog??a Cerro Matoso S.A.
Tel. 4-7723350  Fax. 4-7723236

*************************************************************************************************

Este correo y sus anexos pueden ser confidenciales y estar protegidos por derechos de autor.

Estan dirigidos unica y exclusivamente para uso de el (los) destinatario(s). 

Si Usted por error lo ha recibido por favor notifiquelo inmediatamente al remitente y destruyalo de su sistema.

No debe copiar, ni imprimir, ni distribuir este correo o sus anexos, ni usarlos para proposito alguno ni dar a conocer su contenido a persona alguna.

Las opiniones, conclusiones y otra informacion contenida en este correo no relacionadas con el negocio oficial de Cerro Matoso, deben entenderse como personales y de ninguna manera son avaladas por la Compa??ia.

*************************************************************************************************

This message and any attachments may be confidential and may...{{dropped}}

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html



From f.gherardini at pigrecodata.net  Fri Oct 15 20:24:08 2004
From: f.gherardini at pigrecodata.net (Federico Gherardini)
Date: Fri, 15 Oct 2004 20:24:08 +0200
Subject: [R] Testing for normality of residuals in a regression model
In-Reply-To: <200410151600.i9FG0ALC015119@ohm.gene.com>
References: <200410151600.i9FG0ALC015119@ohm.gene.com>
Message-ID: <417015C8.6050500@pigrecodata.net>

Berton Gunter wrote:

>>>Exactly! My point is that normality tests are useless for this purpose for
>>>reasons that are beyond what I can take up here. 
>>>
Thanks for your suggestions, I undesrtand that! Could you possibly give 
me some (not too complicated!)
links so that I can investigate this matter further?

Cheers,

Federico

>>>Hints: Balanced designs are
>>>robust to non-normality; independence (especially "clustering" of subjects
>>>due to systematic effects), not normality is usually the biggest real
>>>statistical problem; hypothesis tests will always reject when samples are
>>>large -- so what!; "trust" refers to prediction validity which has to do
>>>with study design and the validity/representativeness of the current data to
>>>future. 
>>>
>>>I know that all the stats 101 tests say to test for normality, but they're
>>>full of baloney!
>>>
>>>Of course, this is "free" advice -- so caveat emptor!
>>>
>>>Cheers,
>>>Bert
>>>
>>>      
>>>



From hb at maths.lth.se  Fri Oct 15 18:14:20 2004
From: hb at maths.lth.se (Henrik Bengtsson)
Date: Fri, 15 Oct 2004 18:14:20 +0200
Subject: [R] Building package compatible w/ R v1.9.1 and R v2.0.0?
Message-ID: <002c01c4b2d2$0875ca30$e502eb82@hblaptop>

Hi,

just in the process of updating my packages for R v2.0.0 and I have not had
time to followed the R v2.0.0 discussions so maybe my questions have already
been answered.

A concern I have is that when creating packages they should be backward
compatible with R v1.9.x for a while until all users and computers has
migrated to R v2.0.x. It is pretty straightforward to setup my packages so
that they will *build from source* smoothly under both R v1.9.1 and R
v2.0.0. 

But, is there an easy way to to build a binary (read Windows binary) so that
it will install on R v2.0.0 as well as Rv1.9.1, or do I "have to" provide
two seperate builds?  

As you already know, current status is that building under R v2.0.0 and
loading in R v1.9.1 gives the error:

Error in firstlib(which.lib.loc, package) :
        couldn't find function "lazyLoad"
In addition: Warning message:
package R.oo was built under R version 2.0.0

and building under R v1.9.1 and loading in R v2.0.0 gives the error:

Error in library(R.oo) : 'R.oo' is not a valid package -- installed < 2.0.0?

Just to give it a try, I tried to build the package under R v2.0.0 with
"LazyLoad: FALSE" in DESCRIPTION, but with the same error.

Best wishes

Henrik Bengtsson

Dept. of Mathematical Statistics @ Centre for Mathematical Sciences 
Lund Institute of Technology/Lund University, Sweden (+2h UTC)
+46 46 2229611 (off), +46 708 909208 (cell), +46 46 2224623 (fax)
h b @ m a t h s . l t h . s e, http://www.maths.lth.se/~hb/



From gunter.berton at gene.com  Fri Oct 15 18:25:45 2004
From: gunter.berton at gene.com (Berton Gunter)
Date: Fri, 15 Oct 2004 09:25:45 -0700
Subject: [R] Testing for normality of residuals in a regression model
In-Reply-To: <417015C8.6050500@pigrecodata.net>
Message-ID: <200410151625.i9FGPjvT018736@compton.gene.com>


> 
> Berton Gunter wrote:
> 
> >>>Exactly! My point is that normality tests are useless for 
> this purpose for
> >>>reasons that are beyond what I can take up here. 
> >>>
> Thanks for your suggestions, I undesrtand that! Could you 
> possibly give 
> me some (not too complicated!)
> links so that I can investigate this matter further?
> 
> Cheers,
> 
> Federico
>

1. This was meant as a private reply so I would not roil the list. In
future, when a reply takes a discussion off list, you should keep it off
list, please.

2. The writings of (and personal conversations with) John Tukey and George
Box are certainly primary influences, as are numerous other commentaries
over the year from folks like Leo Breiman, Jerry Friedman, David Freedman,
Persi Diaconis and many others. Box's original paper about robustness to
non-normality was around 1952, I think, but much of what I allude to is
statistical folklore, I think. Perhaps other list contributors might give
you some better specific references. 

Cheers,
Bert



From adi at sas.unibuc.ro  Fri Oct 15 18:43:30 2004
From: adi at sas.unibuc.ro (Adrian Dusa)
Date: Fri, 15 Oct 2004 19:43:30 +0300
Subject: [R] one more Rcmdr problem
In-Reply-To: <416FEC44.30506@univie.ac.at>
Message-ID: <200410152038.i9FKcF4n029153@swork.sas.unibuc.ro>

Brilliant. With R 2.0.0 patched and Rcmdr 0.9-12 all problems are gone.
Thank you all, and a special thank you for the R Commander; it saves a lot
of effort for teaching purposes.
Best regards,
Adrian

-----Original Message-----
From: Erich Neuwirth [mailto:erich.neuwirth at univie.ac.at] 
Sent: 15 octombrie 2004 18:27
To: Adrian Dusa
Cc: r-help at stat.math.ethz.ch
Subject: Re: [R] one more Rcmdr problem

I did experience the same problem.
After installing R 2.0.0 patched and downloading the source for
Rcmdr_0.99-12 from John Fox's Web page
http://socserv.socsci.mcmaster.ca/jfox/Misc/Rcmdr/
and recompiling the package
it now works.

Ctrl-C, Ctrl-X, and Ctrl-V,  in the text boxes of Rcmdr now work.



Adrian Dusa wrote:
> Hello,
> 
> I'm using R 2.0.0 with the latest Rcmdr package installed from CRAN, on
> Windows XP Professional.
> 
> When trying to copy some commands or results, either from the upper or
lower
> text window, this causes Rcmdr to crash:
> 
> "R for Windows GUI front-end has encountered a problem and needs to close"
> 
> Did anyone have the same problem? I don't think it's my system, as it
> happened to reinstall my Windows just a few days ago, and the same problem
> occurred in the former one.
> 
> Regards,
> Adrian
> 
> ~~~~~~~~~~~~~~~~~~~~~~~~~~~~
> Adrian Dusa
> Romanian Social Data Archive
> 1 Schitu Magureanu Bd.
> 050025 Bucharest sector 5
> Tel. +40 21 3126618\
>      +40 21 3153122/ int.101
> 
> 
> 


-- 
Erich Neuwirth, Computer Supported Didactics Working Group
Visit our SunSITE at http://sunsite.univie.ac.at
Phone: +43-1-4277-38624 Fax: +43-1-4277-9386



-- 
This message was scanned for spam and viruses by BitDefender
For more information please visit http://linux.bitdefender.com/



-- 
This message was scanned for spam and viruses by BitDefender
For more information please visit http://linux.bitdefender.com/



From rxg218 at psu.edu  Fri Oct 15 18:51:17 2004
From: rxg218 at psu.edu (Rajarshi Guha)
Date: Fri, 15 Oct 2004 12:51:17 -0400
Subject: [R] SJava 0.65 and R 2.0.0 (slightly long)
Message-ID: <1097859077.18812.25.camel@blue.chem.psu.edu>

Hi,
  I had some Java code that worked with SJava 0.65 under R 1.8.*. I'm
trying to get it to work with R 2.0.0. My JVM is Sun 1.5.0 (running on
Fedora Core 2)

I downloaded and installed SJava 0.65 in my personal directory using

R CMD INSTALL SJava-0.65.tar.gz -l ~/src/Rlibrary

and before starting R I sourced RJava.bsh. However after loading the
SJava library if  do javaConfig() I get

---------------------------------------------

$classPath
[1] "/omegahat/Jars/Environment.jar" "/.."
[3] "/omegahat/Jars/antlr.jar"       "/omegahat/Jars/jas.jar"
[5] "/omegahat/Jars/jhall.jar"
 
$properties
                                                                
EmbeddedInR
                                                                     
"true"
                                                      
InterfaceManagerClass
            
"org/omegahat/Interfaces/NativeInterface/OmegaInterfaceManager"
                                                  
ForeignReferenceBaseClass
                                    
"org/omegahat/R/Java/RForeignReference"
                                                              
java.compiler
                                                                     
"NONE"
                                                                 
OMEGA_HOME
                                                                          ""
                                                         
OmegahatSearchPath
".,${OMEGA_HOME}/Environment/Scripts/Run,${OMEGA_HOME}/Jars/Environment.jar"
                                                          
java.library.path
                                                                          ""
 
$libraryPath
[1] ""

--------------------------------------------

and doing .JavaInit() gives me:

JVM (nil) Env (nil)
[1] error initializing manager class Cannot find the Omegahat interface
manager class. Check you classpath!
Error in .JavaInit() : Couldn't start Java Virtual Machine: Cannot find
the Omegahat interface manager class. Check you classpath!

>From the output fof javaConfig() I was'nt surprised. However I thought
that the install would set the paths to the OmegaHat interface manager.

So I set the classpath to

/home/rajarshi/src/Rlibrary/SJava/org/omegahat/Jars/../Interfaces: \
/home/rajarshi/src/Rlibrary/SJava/org/omegahat/Jars/../R/Java: \
/home/rajarshi/src/Rlibrary/SJava/org/omegahat/Jars/antlr.jar: \
/home/rajarshi/src/Rlibrary/SJava/org/omegahat/Jars/Environment.jar: \
/home/rajarshi/src/Rlibrary/SJava/org/omegahat/Jars/jas.jar: \
/home/rajarshi/src/Rlibrary/SJava/org/omegahat/Jars/jhall.jar: \
/home/rajarshi/src/Rlibrary/SJava/org/omegahat/Jars/ROmegahatExamples.jar:

and then starting up R and doing javaConfig() I get the same output as
before (except that the above paths are now in the CLASSPATH).

I also noted that the OMEGA_HOME variable is set to "". This did not
seem to be right so I tried setting it to 

/home/rajarshi/src/Rlibrary/SJava/org/omegahat

from my shell but on starting up R and loading SJava and running
javaConfig() I see that OMEGA_HOME is still set to ""

Does anybody have any pointers as to whats going on? 

(I also tried SJava 0.68, but thought it installs fine and .JavaInit()
works, when I try the example on the SJava download page, my R session
crashes)

-------------------------------------------------------------------
Rajarshi Guha <rxg218 at psu.edu> <http://jijo.cjb.net>
GPG Fingerprint: 0CCA 8EE2 2EEB 25E2 AB04 06F7 1BB9 E634 9B87 56EE
-------------------------------------------------------------------
I'm related to people I don't relate to."
-Calvin



From ripley at stats.ox.ac.uk  Fri Oct 15 18:59:13 2004
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Fri, 15 Oct 2004 17:59:13 +0100 (BST)
Subject: [R] add-on package/windows installation probLem
In-Reply-To: <000f01c4b2cf$9ec19e40$c81c0680@userw2mpwlmw4r>
Message-ID: <Pine.LNX.4.44.0410151755460.25564-100000@gannet.stats>

You downloaded a .tar.gz file and treated it as a zip file, for some 
reason not in any R documentation.

On Fri, 15 Oct 2004, Nebahat Noyan wrote:

> Hi,
> 
> I have tried to download the bootstrap.tar.gz from the website below:
> 
> http://cran.r-project.org/src/contrib/Old/0.50/INDEX.html
> 
> I have extracted the folder to a zip file...and then use the load
> packages from local zip files. I use R2.0.0.0 and windows NT, i get the
> error:
> 
> 1: error -1 in extracting from zip file 
> 2: cannot open file `bootstrap/DESCRIPTION' 
> 
> ALso, when i use R CMD INSTALL with the package name bootstrap.tar.gz it
> gives a syntax error. Can you help me with this pLease?

Perhaps that is because the file you got is for R 0.50, not R 2.0.0 (sic)?
The bootstrap package has been withdrawn and does not work under R 2.0.0 
AFAIK.

> 
> Sincere Regards,
> Neba Noyan
> Research Assistant
> Center for Advanced Infrastructure and Transportation Institute
> Rutgers University
> 
> P.S:
> ReLated solutions  in the mailing List are below:

They are *not* related.

> > solved : 
> > before installing using menu option, go to "program files\R\rw1051\library" and 
> > make a directory "pubbias", then install the package from the local zip file 
> > option. 
> 
> > 1) the zip file might have invalid internal structure. Maybe it is 
> >missing the directory `pubbias' (although the unzip code should cope with 
> >that, and that's not the error code I would expect). 
> >2) This is not the first attempt to do something like this, and Windows 
> >had the location in use. (I have seen that one, and after a reboot it 
> >worked cleanly.) 
> 	[[alternative HTML version deleted]]

What does the posting guide say about that?

> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From ligges at statistik.uni-dortmund.de  Fri Oct 15 19:10:39 2004
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Fri, 15 Oct 2004 19:10:39 +0200
Subject: [R] add-on package/windows installation probLem
In-Reply-To: <000f01c4b2cf$9ec19e40$c81c0680@userw2mpwlmw4r>
References: <000f01c4b2cf$9ec19e40$c81c0680@userw2mpwlmw4r>
Message-ID: <4170048F.6070303@statistik.uni-dortmund.de>

Nebahat Noyan wrote:

> Hi,
> 
> I have tried to download the bootstrap.tar.gz from the website below:
> 
> http://cran.r-project.org/src/contrib/Old/0.50/INDEX.html

0. For sure you are joking ...          ^^^^^^^^
    The package is 7.5 years old (16-Apr-1997) -  too old to be 
installable by recent versions of R. But you might want to look into 
more recent packages on CRAN ....

1. Under Windows, you have to INSTALL via R CMD INSTALL with R >= 2.0.0 
(if there is no binary version available built with that version).

2. The solution in the mail given below was never the preferable one.

Uwe Ligges




> I have extracted the folder to a zip file...and then use the load packages from local zip files.
> I use R2.0.0.0 and windows NT, i get the error:
> 
> 1: error -1 in extracting from zip file 
> 2: cannot open file `bootstrap/DESCRIPTION' 
> 
> ALso, when i use R CMD INSTALL with the package name bootstrap.tar.gz it gives a syntax error.
> Can you help me with this pLease?
> 
> Sincere Regards,
> Neba Noyan
> Research Assistant
> Center for Advanced Infrastructure and Transportation Institute
> Rutgers University
> 
> P.S:
> ReLated solutions  in the mailing List are below:
> 
> 
>>solved : 
>>before installing using menu option, go to "program files\R\rw1051\library" and 
>>make a directory "pubbias", then install the package from the local zip file 
>>option. 
> 
> 
>>1) the zip file might have invalid internal structure. Maybe it is 
>>missing the directory `pubbias' (although the unzip code should cope with 
>>that, and that's not the error code I would expect). 
>>2) This is not the first attempt to do something like this, and Windows 
>>had the location in use. (I have seen that one, and after a reboot it 
>>worked cleanly.) 
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html



From ligges at statistik.uni-dortmund.de  Fri Oct 15 19:13:24 2004
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Fri, 15 Oct 2004 19:13:24 +0200
Subject: [R] Building package compatible w/ R v1.9.1 and R v2.0.0?
In-Reply-To: <002c01c4b2d2$0875ca30$e502eb82@hblaptop>
References: <002c01c4b2d2$0875ca30$e502eb82@hblaptop>
Message-ID: <41700534.9090909@statistik.uni-dortmund.de>

Henrik Bengtsson wrote:

> Hi,
> 
> just in the process of updating my packages for R v2.0.0 and I have not had
> time to followed the R v2.0.0 discussions so maybe my questions have already
> been answered.
> 
> A concern I have is that when creating packages they should be backward
> compatible with R v1.9.x for a while until all users and computers has
> migrated to R v2.0.x. It is pretty straightforward to setup my packages so
> that they will *build from source* smoothly under both R v1.9.1 and R
> v2.0.0. 
> 
> But, is there an easy way to to build a binary (read Windows binary) so that
> it will install on R v2.0.0 as well as Rv1.9.1, or do I "have to" provide
> two seperate builds?  

Yes. Same source might work, but you need separate binaries.

Uwe Ligges


> As you already know, current status is that building under R v2.0.0 and
> loading in R v1.9.1 gives the error:
> 
> Error in firstlib(which.lib.loc, package) :
>         couldn't find function "lazyLoad"
> In addition: Warning message:
> package R.oo was built under R version 2.0.0
> 
> and building under R v1.9.1 and loading in R v2.0.0 gives the error:
> 
> Error in library(R.oo) : 'R.oo' is not a valid package -- installed < 2.0.0?
> 
> Just to give it a try, I tried to build the package under R v2.0.0 with
> "LazyLoad: FALSE" in DESCRIPTION, but with the same error.
> 
> Best wishes
> 
> Henrik Bengtsson
> 
> Dept. of Mathematical Statistics @ Centre for Mathematical Sciences 
> Lund Institute of Technology/Lund University, Sweden (+2h UTC)
> +46 46 2229611 (off), +46 708 909208 (cell), +46 46 2224623 (fax)
> h b @ m a t h s . l t h . s e, http://www.maths.lth.se/~hb/
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html



From jfox at mcmaster.ca  Fri Oct 15 19:22:14 2004
From: jfox at mcmaster.ca (John Fox)
Date: Fri, 15 Oct 2004 13:22:14 -0400
Subject: [R] one more Rcmdr problem
In-Reply-To: <200410152038.i9FKcF4n029153@swork.sas.unibuc.ro>
Message-ID: <20041015172214.NXHJ25820.tomts20-srv.bellnexxia.net@JohnDesktop8300>

Dear Adrian,

Since I moved the discussion of this problem to the r-devel list, I should
have reported back to r-help (1) that the problem was general to tlctk (not
just the Rcmdr package), and (2) that Duncan Murdoch made a change to the
patched Windows version of R 2.0.0 that made the problem go away.

Regards,
 John

--------------------------------
John Fox
Department of Sociology
McMaster University
Hamilton, Ontario
Canada L8S 4M4
905-525-9140x23604
http://socserv.mcmaster.ca/jfox 
-------------------------------- 

> -----Original Message-----
> From: r-help-bounces at stat.math.ethz.ch 
> [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Adrian Dusa
> Sent: Friday, October 15, 2004 11:44 AM
> To: 'Erich Neuwirth'; 'John Fox'; 'Duncan Murdoch'
> Cc: r-help at stat.math.ethz.ch
> Subject: RE: [R] one more Rcmdr problem
> 
> Brilliant. With R 2.0.0 patched and Rcmdr 0.9-12 all problems 
> are gone.
> Thank you all, and a special thank you for the R Commander; 
> it saves a lot of effort for teaching purposes.
> Best regards,
> Adrian
> 
> -----Original Message-----
> From: Erich Neuwirth [mailto:erich.neuwirth at univie.ac.at]
> Sent: 15 octombrie 2004 18:27
> To: Adrian Dusa
> Cc: r-help at stat.math.ethz.ch
> Subject: Re: [R] one more Rcmdr problem
> 
> I did experience the same problem.
> After installing R 2.0.0 patched and downloading the source for
> Rcmdr_0.99-12 from John Fox's Web page
> http://socserv.socsci.mcmaster.ca/jfox/Misc/Rcmdr/
> and recompiling the package
> it now works.
> 
> Ctrl-C, Ctrl-X, and Ctrl-V,  in the text boxes of Rcmdr now work.
> 
> 
> 
> Adrian Dusa wrote:
> > Hello,
> > 
> > I'm using R 2.0.0 with the latest Rcmdr package installed 
> from CRAN, 
> > on Windows XP Professional.
> > 
> > When trying to copy some commands or results, either from 
> the upper or
> lower
> > text window, this causes Rcmdr to crash:
> > 
> > "R for Windows GUI front-end has encountered a problem and 
> needs to close"
> > 
> > Did anyone have the same problem? I don't think it's my 
> system, as it 
> > happened to reinstall my Windows just a few days ago, and the same 
> > problem occurred in the former one.
> > 
> > Regards,
> > Adrian
> > 
> > ~~~~~~~~~~~~~~~~~~~~~~~~~~~~
> > Adrian Dusa
> > Romanian Social Data Archive
> > 1 Schitu Magureanu Bd.
> > 050025 Bucharest sector 5
> > Tel. +40 21 3126618\
> >      +40 21 3153122/ int.101
> > 
> > 
> > 
> 
> 
> --
> Erich Neuwirth, Computer Supported Didactics Working Group 
> Visit our SunSITE at http://sunsite.univie.ac.at
> Phone: +43-1-4277-38624 Fax: +43-1-4277-9386
> 
> 
> 
> -- 
> This message was scanned for spam and viruses by BitDefender
> For more information please visit http://linux.bitdefender.com/
> 
> 
> 
> -- 
> This message was scanned for spam and viruses by BitDefender
> For more information please visit http://linux.bitdefender.com/
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html



From michele.alzetta at gmail.com  Fri Oct 15 19:41:43 2004
From: michele.alzetta at gmail.com (Michele Alzetta)
Date: Fri, 15 Oct 2004 19:41:43 +0200
Subject: [R] Mandrake rpm's for R 2.0.0
Message-ID: <8079280b0410151041139ca009@mail.gmail.com>

A binary RPM for Mandrake 10.0 official has been uploaded and will
soon be available on CRAN, as well as the SRPM with which users of
other flavours of Mandrake will be able to build (and hopefully
contribute) their own RPM's.

Please read the readme file if you have problems installing.

Downloads are also possible from my homepage:

http://www.webalice.it/michele.alzetta

-- 
Michele Alzetta



From andy_liaw at merck.com  Fri Oct 15 18:55:03 2004
From: andy_liaw at merck.com (Liaw, Andy)
Date: Fri, 15 Oct 2004 12:55:03 -0400
Subject: [R] Testing for normality of residuals in a regression
 model
Message-ID: <3A822319EB35174CA3714066D590DCD504AF854F@usrymx25.merck.com>

Let's see if I can get my stat 101 straight:

We learned that linear regression has a set of assumptions:

1. Linearity of the relationship between X and y.
2. Independence of errors.
3. Homoscedasticity (equal error variance).
4. Normality of errors.

Now, we should ask:  Why are they needed?  Can we get away with less?  What
if some of them are not met?

It should be clear why we need #1.

Without #2, I believe the least squares estimator is still unbias, but the
usual estimate of SEs for the coefficients are wrong, so the t-tests are
wrong.

Without #3, the coefficients are, again, still unbiased, but not as
efficient as can be.  Interval estimates for the prediction will surely be
wrong.

Without #4, well, it depends.  If the residual DF is sufficiently large, the
t-tests are still valid because of CLT.  You do need normality if you have
small residual DF.

The problem with normality tests, I believe, is that they usually have
fairly low power at small sample sizes, so that doesn't quite help.  There's
no free lunch:  A normality test with good power will usually have good
power against a fairly narrow class of alternatives, and almost no power
against others (directional test).  How do you decide what to use?

Has anyone seen a data set where the normality test on the residuals is
crucial in coming up with appriate analysis?

Cheers,
Andy

> From: Federico Gherardini
> 
> Berton Gunter wrote:
> 
> >>>Exactly! My point is that normality tests are useless for 
> this purpose for
> >>>reasons that are beyond what I can take up here. 
> >>>
> Thanks for your suggestions, I undesrtand that! Could you 
> possibly give 
> me some (not too complicated!)
> links so that I can investigate this matter further?
> 
> Cheers,
> 
> Federico
> 
> >>>Hints: Balanced designs are
> >>>robust to non-normality; independence (especially 
> "clustering" of subjects
> >>>due to systematic effects), not normality is usually the 
> biggest real
> >>>statistical problem; hypothesis tests will always reject 
> when samples are
> >>>large -- so what!; "trust" refers to prediction validity 
> which has to do
> >>>with study design and the validity/representativeness of 
> the current data to
> >>>future. 
> >>>
> >>>I know that all the stats 101 tests say to test for 
> normality, but they're
> >>>full of baloney!
> >>>
> >>>Of course, this is "free" advice -- so caveat emptor!
> >>>
> >>>Cheers,
> >>>Bert
> >>>
> >>>      
> >>>
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
> 
>



From jfox at mcmaster.ca  Fri Oct 15 19:48:49 2004
From: jfox at mcmaster.ca (John Fox)
Date: Fri, 15 Oct 2004 13:48:49 -0400
Subject: [R] Testing for normality of residuals in a regression model
In-Reply-To: <416FDAB7.70404@acelerate.com>
Message-ID: <20041015174852.QAVG2542.tomts22-srv.bellnexxia.net@JohnDesktop8300>

Dear Kjetil,

I don't believe that these are BLUS residuals, but since the last n - r
"effects" are projections onto an orthogonal basis for the residual
subspace, they should do just fine (as long as the basis vectors have the
same length, which I think is the case, but perhaps someone can confirm).
The general idea is to transform the LS residuals into an uncorrelated,
equal-variance set.

Regards,
 John

--------------------------------
John Fox
Department of Sociology
McMaster University
Hamilton, Ontario
Canada L8S 4M4
905-525-9140x23604
http://socserv.mcmaster.ca/jfox 
-------------------------------- 

> -----Original Message-----
> From: Kjetil Brinchmann Halvorsen [mailto:kjetil at acelerate.com] 
> Sent: Friday, October 15, 2004 9:12 AM
> To: John Fox
> Cc: 'Federico Gherardini'; R-help at stat.math.ethz.ch
> Subject: Re: [R] Testing for normality of residuals in a 
> regression model
> 
> John Fox wrote:
> 
> >Dear Federico,
> >
> >A problem with applying a standard test of normality to LS 
> residuals is 
> >that the residuals are correlated and heterskedastic even if the 
> >standard assumptions of the model hold. In a large sample, this is 
> >unlikely to be problematic (unless there's an unusual data 
> >configuration), but in a small sample the effect could be nontrivial.
> >
> >One approach is to use BLUS residuals, which transform the 
> LS residuals 
> >to a smaller set of uncorrelated, homoskedastic residuals 
> (assuming the 
> >correctness of the model).
> >
> I'm not sure if this are BLUE residuals, but the following 
> function transform to a smaller set of independent, 
> homoscedastic residuals and the calls
> shapiro.test:
> I've proposed to make this a method for shapiro.test for "lm" 
> objects, but it is not accepted.
> 
>  shapiro.test.lm
> function (obj)
> {
>     eff <- effects(obj)
>     rank <- obj$rank
>     df.r <- obj$df.residual
>     if (df.r < 3)
>         stop("To few degrees of freedom for residual for the test.")
>     data.name <- deparse(substitute(obj))
>     x <- eff[-(1:rank)]
>     res <- shapiro.test(x)
>     res$data.name <- data.name
>     res$method <- paste(res$method, " for residuals of linear model")
>     res
> }
> 
> Kjetil
> 
> 
> > A search of R resources didn't turn up anything for BLUS, but they 
> >shouldn't be hard to compute. This is a standard topic 
> covered in many 
> >econometrics texts.
> >
> >You might consider the alternative of generating a bootstrapped 
> >confidence envelope for the QQ plot; the qq.plot() function 
> in the car 
> >package will do this for a linear model.
> >
> >I hope this helps,
> > John
> >
> >--------------------------------
> >John Fox
> >Department of Sociology
> >McMaster University
> >Hamilton, Ontario
> >Canada L8S 4M4
> >905-525-9140x23604
> >http://socserv.mcmaster.ca/jfox
> >--------------------------------
> >
> >  
> >
> >>-----Original Message-----
> >>From: r-help-bounces at stat.math.ethz.ch 
> >>[mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Federico 
> >>Gherardini
> >>Sent: Friday, October 15, 2004 7:44 AM
> >>To: R-help at stat.math.ethz.ch
> >>Subject: [R] Testing for normality of residuals in a 
> regression model
> >>
> >>Hi all,
> >>
> >>Is it possible to have a test value for assessing the normality of 
> >>residuals from a linear regression model, instead of simply 
> relying on 
> >>qqplots?
> >>I've tried to use fitdistr to try and fit the residuals 
> with a normal 
> >>distribution, but fitdsitr only returns the parameters of the 
> >>distribution and the standard errors, not the p-value. Am I missing 
> >>something?
> >>
> >>Cheers,
> >>
> >>Federico
> >>
> >>______________________________________________
> >>R-help at stat.math.ethz.ch mailing list
> >>https://stat.ethz.ch/mailman/listinfo/r-help
> >>PLEASE do read the posting guide! 
> >>http://www.R-project.org/posting-guide.html
> >>    
> >>
> >
> >______________________________________________
> >R-help at stat.math.ethz.ch mailing list
> >https://stat.ethz.ch/mailman/listinfo/r-help
> >PLEASE do read the posting guide! 
> >http://www.R-project.org/posting-guide.html
> >
> >
> >  
> >
> 
> 
> -- 
> 
> Kjetil Halvorsen.
> 
> Peace is the most effective weapon of mass construction.
>                --  Mahdi Elmandjra
> 
> 
>



From jfox at mcmaster.ca  Fri Oct 15 19:51:54 2004
From: jfox at mcmaster.ca (John Fox)
Date: Fri, 15 Oct 2004 13:51:54 -0400
Subject: [R] Testing for normality of residuals in a regression model
In-Reply-To: <416FF944.1000007@pigrecodata.net>
Message-ID: <20041015175155.ZHWW1536.tomts25-srv.bellnexxia.net@JohnDesktop8300>

Dear Federico,

The problem is the same with GLS residuals -- even if the GLS transformation
produces homoskedastic errors, the residuals will be correlated and
heteroskedastic (with this problem tending to disappear in most instances as
n grows). The central point is that residuals don't behave quite the same as
errors.

Regards,
 John

--------------------------------
John Fox
Department of Sociology
McMaster University
Hamilton, Ontario
Canada L8S 4M4
905-525-9140x23604
http://socserv.mcmaster.ca/jfox 
-------------------------------- 

> -----Original Message-----
> From: r-help-bounces at stat.math.ethz.ch 
> [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of 
> Federico Gherardini
> Sent: Friday, October 15, 2004 11:22 AM
> To: R-help at stat.math.ethz.ch
> Subject: Re: [R] Testing for normality of residuals in a 
> regression model
> 
> Thank you very much for your suggestions! The residuals come 
> from a gls model, because I had to correct for 
> heteroscedasticity using a weighted regression... can I 
> simply apply one of these tests (like shapiro.test) to the 
> standardized residuals from my gls model?
> 
> Cheers,
> Federico
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
>



From jfox at mcmaster.ca  Fri Oct 15 19:56:49 2004
From: jfox at mcmaster.ca (John Fox)
Date: Fri, 15 Oct 2004 13:56:49 -0400
Subject: [R] Testing for normality of residuals in a regression model
In-Reply-To: <3A822319EB35174CA3714066D590DCD504AF854F@usrymx25.merck.com>
Message-ID: <20041015175650.QDGG2542.tomts22-srv.bellnexxia.net@JohnDesktop8300>

Dear Andy,

At the risk of muddying the waters (and certainly without wanting to
advocate the use of normality tests for residuals), I believe that your
point #4 is subject to misinterpretation: That is, while it is true that t-
and F-tests for regression coefficients in large sample retain their
validity well when the errors are non-normal, the efficiency of the LS
estimates can (depending upon the nature of the non-normality) be seriously
compromised, not only absolutely but in relation to alternatives (e.g.,
robust regression).

Regards,
 John

--------------------------------
John Fox
Department of Sociology
McMaster University
Hamilton, Ontario
Canada L8S 4M4
905-525-9140x23604
http://socserv.mcmaster.ca/jfox 
-------------------------------- 

> -----Original Message-----
> From: r-help-bounces at stat.math.ethz.ch 
> [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Liaw, Andy
> Sent: Friday, October 15, 2004 11:55 AM
> To: 'Federico Gherardini'; Berton Gunter
> Cc: R-help mailing list
> Subject: RE: [R] Testing for normality of residuals in a 
> regression model
> 
> Let's see if I can get my stat 101 straight:
> 
> We learned that linear regression has a set of assumptions:
> 
> 1. Linearity of the relationship between X and y.
> 2. Independence of errors.
> 3. Homoscedasticity (equal error variance).
> 4. Normality of errors.
> 
> Now, we should ask:  Why are they needed?  Can we get away 
> with less?  What if some of them are not met?
> 
> It should be clear why we need #1.
> 
> Without #2, I believe the least squares estimator is still 
> unbias, but the usual estimate of SEs for the coefficients 
> are wrong, so the t-tests are wrong.
> 
> Without #3, the coefficients are, again, still unbiased, but 
> not as efficient as can be.  Interval estimates for the 
> prediction will surely be wrong.
> 
> Without #4, well, it depends.  If the residual DF is 
> sufficiently large, the t-tests are still valid because of 
> CLT.  You do need normality if you have small residual DF.
> 
> The problem with normality tests, I believe, is that they 
> usually have fairly low power at small sample sizes, so that 
> doesn't quite help.  There's no free lunch:  A normality test 
> with good power will usually have good power against a fairly 
> narrow class of alternatives, and almost no power against 
> others (directional test).  How do you decide what to use?
> 
> Has anyone seen a data set where the normality test on the 
> residuals is crucial in coming up with appriate analysis?
> 
> Cheers,
> Andy
> 
> > From: Federico Gherardini
> > 
> > Berton Gunter wrote:
> > 
> > >>>Exactly! My point is that normality tests are useless for
> > this purpose for
> > >>>reasons that are beyond what I can take up here. 
> > >>>
> > Thanks for your suggestions, I undesrtand that! Could you possibly 
> > give me some (not too complicated!) links so that I can investigate 
> > this matter further?
> > 
> > Cheers,
> > 
> > Federico
> > 
> > >>>Hints: Balanced designs are
> > >>>robust to non-normality; independence (especially
> > "clustering" of subjects
> > >>>due to systematic effects), not normality is usually the
> > biggest real
> > >>>statistical problem; hypothesis tests will always reject
> > when samples are
> > >>>large -- so what!; "trust" refers to prediction validity
> > which has to do
> > >>>with study design and the validity/representativeness of
> > the current data to
> > >>>future. 
> > >>>
> > >>>I know that all the stats 101 tests say to test for
> > normality, but they're
> > >>>full of baloney!
> > >>>
> > >>>Of course, this is "free" advice -- so caveat emptor!
> > >>>
> > >>>Cheers,
> > >>>Bert
> > >>>
> > >>>      
> > >>>
> > 
> > ______________________________________________
> > R-help at stat.math.ethz.ch mailing list
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide! 
> > http://www.R-project.org/posting-guide.html
> > 
> >
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html



From harris41 at msu.edu  Fri Oct 15 20:00:47 2004
From: harris41 at msu.edu (Scott Harrison)
Date: Fri, 15 Oct 2004 14:00:47 -0400
Subject: [R] fidelity of generated raster images (R and perl)
In-Reply-To: <000301c4b212$188bff40$1bf1ba51@hblaptop>
References: <000301c4b212$188bff40$1bf1ba51@hblaptop>
Message-ID: <4170104F.8030201@msu.edu>

Wrap-up:

Different solutions were proposed for turning a matrix
into a bitmap.

Henrik Bengtsson <hb at maths.lth.se>
 >R.classes bundle from
 > (http://www.maths.lth.se/help/R/R.classes/)
 >MonochromeImage
 >R.classes is still not compatible with
 >2.0.0 (minor modification to DESCRIPTION etc
 > are needed) so you have use R v1.9.1 to try the below.
 >img <- MonochromeImage(1-bits) # 1=white, 0=black
 >image(img)
 >plot(img)
 >persp(img, phi=60, theta=150)
 >write(img, "foo.pbm") # Write directly

Martin Maechler <maechler at stat.math.ethz.ch>
 >I think the "most typical" alternative is to use image()

Roger.Bivand at nhh.no
 >Maybe use the write.pnm() function in the pixmap package
 >Another package you could explore is rimage, which looks relevant.

For the sake of being conventional and minimizing dependencies,
I had good success with the image() function.  It is very nice to know
there are other approaches if I need to optimize/customize what
I am doing more for this.

(As usual) learned a few more things about 'R' from the examples
provided by others.  Most grateful for your help.

Regards,
Scott



From ripley at stats.ox.ac.uk  Fri Oct 15 20:08:05 2004
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Fri, 15 Oct 2004 19:08:05 +0100 (BST)
Subject: [R] Testing for normality of residuals in a regression model
In-Reply-To: <3A822319EB35174CA3714066D590DCD504AF854F@usrymx25.merck.com>
Message-ID: <Pine.LNX.4.44.0410151903370.25694-100000@gannet.stats>

On Fri, 15 Oct 2004, Liaw, Andy wrote:

> Let's see if I can get my stat 101 straight:
> 
> We learned that linear regression has a set of assumptions:
> 
> 1. Linearity of the relationship between X and y.
> 2. Independence of errors.
> 3. Homoscedasticity (equal error variance).
> 4. Normality of errors.
> 
> Now, we should ask:  Why are they needed?  Can we get away with less?  What
> if some of them are not met?
> 
> It should be clear why we need #1.
> 
> Without #2, I believe the least squares estimator is still unbias, but the
> usual estimate of SEs for the coefficients are wrong, so the t-tests are
> wrong.
> 
> Without #3, the coefficients are, again, still unbiased, but not as
> efficient as can be.  Interval estimates for the prediction will surely be
> wrong.

The lost of efficiency is often quite small.

> Without #4, well, it depends.  If the residual DF is sufficiently large, the
> t-tests are still valid because of CLT.  You do need normality if you have
> small residual DF.

However, stats 901 or some such tells you that if the distributions have 
even slightly longer tails than the normal you can get much better 
estimates than OLS, and this happens even before a test of normality 
rejects on a sample size of thousands.

Robustness of efficiency is much more important than robustness of 
distribution, and I believe robustness concepts should be in stats 101.
(I was teaching them yesterday in the third lecture of a basic course, 
albeit a graduate course.)

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From andy_liaw at merck.com  Fri Oct 15 20:14:00 2004
From: andy_liaw at merck.com (Liaw, Andy)
Date: Fri, 15 Oct 2004 14:14:00 -0400
Subject: [R] Testing for normality of residuals in a regression
 model
Message-ID: <3A822319EB35174CA3714066D590DCD504AF8554@usrymx25.merck.com>

Hi John,

Your point is well taken.  I was only thinking about the shape of the
distribution, and neglected the cases of, say, symmetric long tailed
distributions.  However, I think I'd still argue that other tools are
probably more useful than normality tests (e.g., robust methods, as you
mentioned).

To take the point a bit further, let's say we test for normality and it's
rejected.  What do we do then?  Well, if the non-normality is caused by
outliers, we can try robust methods.  If not, what do we do?  We can try to
see if some sort of transformation would bring the residuals closer to
normally distributed, but if the interest is in inference on the
coefficients, those inferences on the `final' model are potentially invalid.
What's one to do then?

Also, I was told by someone very smart that fitting OLS to data with
heteroscedastic errors can make the residuals look `more normal' than they
really are...  Don't know how true that is, though.

Best,
Andy

> From: John Fox
> 
> Dear Andy,
> 
> At the risk of muddying the waters (and certainly without wanting to
> advocate the use of normality tests for residuals), I believe 
> that your
> point #4 is subject to misinterpretation: That is, while it 
> is true that t-
> and F-tests for regression coefficients in large sample retain their
> validity well when the errors are non-normal, the efficiency of the LS
> estimates can (depending upon the nature of the 
> non-normality) be seriously
> compromised, not only absolutely but in relation to 
> alternatives (e.g.,
> robust regression).
> 
> Regards,
>  John
> 
> --------------------------------
> John Fox
> Department of Sociology
> McMaster University
> Hamilton, Ontario
> Canada L8S 4M4
> 905-525-9140x23604
> http://socserv.mcmaster.ca/jfox 
> -------------------------------- 
> 
> > -----Original Message-----
> > From: r-help-bounces at stat.math.ethz.ch 
> > [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Liaw, Andy
> > Sent: Friday, October 15, 2004 11:55 AM
> > To: 'Federico Gherardini'; Berton Gunter
> > Cc: R-help mailing list
> > Subject: RE: [R] Testing for normality of residuals in a 
> > regression model
> > 
> > Let's see if I can get my stat 101 straight:
> > 
> > We learned that linear regression has a set of assumptions:
> > 
> > 1. Linearity of the relationship between X and y.
> > 2. Independence of errors.
> > 3. Homoscedasticity (equal error variance).
> > 4. Normality of errors.
> > 
> > Now, we should ask:  Why are they needed?  Can we get away 
> > with less?  What if some of them are not met?
> > 
> > It should be clear why we need #1.
> > 
> > Without #2, I believe the least squares estimator is still 
> > unbias, but the usual estimate of SEs for the coefficients 
> > are wrong, so the t-tests are wrong.
> > 
> > Without #3, the coefficients are, again, still unbiased, but 
> > not as efficient as can be.  Interval estimates for the 
> > prediction will surely be wrong.
> > 
> > Without #4, well, it depends.  If the residual DF is 
> > sufficiently large, the t-tests are still valid because of 
> > CLT.  You do need normality if you have small residual DF.
> > 
> > The problem with normality tests, I believe, is that they 
> > usually have fairly low power at small sample sizes, so that 
> > doesn't quite help.  There's no free lunch:  A normality test 
> > with good power will usually have good power against a fairly 
> > narrow class of alternatives, and almost no power against 
> > others (directional test).  How do you decide what to use?
> > 
> > Has anyone seen a data set where the normality test on the 
> > residuals is crucial in coming up with appriate analysis?
> > 
> > Cheers,
> > Andy
> > 
> > > From: Federico Gherardini
> > > 
> > > Berton Gunter wrote:
> > > 
> > > >>>Exactly! My point is that normality tests are useless for
> > > this purpose for
> > > >>>reasons that are beyond what I can take up here. 
> > > >>>
> > > Thanks for your suggestions, I undesrtand that! Could you 
> possibly 
> > > give me some (not too complicated!) links so that I can 
> investigate 
> > > this matter further?
> > > 
> > > Cheers,
> > > 
> > > Federico
> > > 
> > > >>>Hints: Balanced designs are
> > > >>>robust to non-normality; independence (especially
> > > "clustering" of subjects
> > > >>>due to systematic effects), not normality is usually the
> > > biggest real
> > > >>>statistical problem; hypothesis tests will always reject
> > > when samples are
> > > >>>large -- so what!; "trust" refers to prediction validity
> > > which has to do
> > > >>>with study design and the validity/representativeness of
> > > the current data to
> > > >>>future. 
> > > >>>
> > > >>>I know that all the stats 101 tests say to test for
> > > normality, but they're
> > > >>>full of baloney!
> > > >>>
> > > >>>Of course, this is "free" advice -- so caveat emptor!
> > > >>>
> > > >>>Cheers,
> > > >>>Bert
> > > >>>
> > > >>>      
> > > >>>
> > > 
> > > ______________________________________________
> > > R-help at stat.math.ethz.ch mailing list
> > > https://stat.ethz.ch/mailman/listinfo/r-help
> > > PLEASE do read the posting guide! 
> > > http://www.R-project.org/posting-guide.html
> > > 
> > >
> > 
> > ______________________________________________
> > R-help at stat.math.ethz.ch mailing list
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide! 
> > http://www.R-project.org/posting-guide.html
> 
> 
>



From spencer.graves at pdf.com  Fri Oct 15 20:18:26 2004
From: spencer.graves at pdf.com (Spencer Graves)
Date: Fri, 15 Oct 2004 11:18:26 -0700
Subject: [R] Testing for normality of residuals in a regression model
In-Reply-To: <3A822319EB35174CA3714066D590DCD504AF854F@usrymx25.merck.com>
References: <3A822319EB35174CA3714066D590DCD504AF854F@usrymx25.merck.com>
Message-ID: <41701472.1090602@pdf.com>

      OK, I'll expose myself: 

      I tend to do normal probability plots of residuals (usely deletion 
/ studentized residuals as described by Venables and Ripley in Modern 
Applied Statistics with S, 4th ed, MASS4).  If the plots look strange, I 
do something.  I'll check apparent outliers for coding and data entry 
errors, and I often delete those points from the analysis even if I 
can't find a reason why.  Robust regression will usually handle this 
type of problem, and I am gradually migrating to increasing use of 
robust regression, especially the procedures recommended by MASS4.  . 

      However, I recently encountered a situation that would be masked 
by standard use of robust regression without examining residual plots:  
A normal probability plot looked like three parallel straight lines with 
gaps, suggesting a mixture of 3 normal distributions with different 
means and a common standard deviation.  Further investigation revealed 
that an important 3-level explanatory variable that had been miscoded.  
When this was corrected, that variable entered the model and the gaps in 
the normal plot disappeared. 

      I tend NOT to use tests of normality for the reasons Andy 
mentioned.  Instead, I do various kinds of diagnostic plots and modify 
my model or investigate the data in response to what I see. 

      Comments?
      hope this helps.  spencer graves

Liaw, Andy wrote:

>Let's see if I can get my stat 101 straight:
>
>We learned that linear regression has a set of assumptions:
>
>1. Linearity of the relationship between X and y.
>2. Independence of errors.
>3. Homoscedasticity (equal error variance).
>4. Normality of errors.
>
>Now, we should ask:  Why are they needed?  Can we get away with less?  What
>if some of them are not met?
>
>It should be clear why we need #1.
>
>Without #2, I believe the least squares estimator is still unbias, but the
>usual estimate of SEs for the coefficients are wrong, so the t-tests are
>wrong.
>
>Without #3, the coefficients are, again, still unbiased, but not as
>efficient as can be.  Interval estimates for the prediction will surely be
>wrong.
>
>Without #4, well, it depends.  If the residual DF is sufficiently large, the
>t-tests are still valid because of CLT.  You do need normality if you have
>small residual DF.
>
>The problem with normality tests, I believe, is that they usually have
>fairly low power at small sample sizes, so that doesn't quite help.  There's
>no free lunch:  A normality test with good power will usually have good
>power against a fairly narrow class of alternatives, and almost no power
>against others (directional test).  How do you decide what to use?
>
>Has anyone seen a data set where the normality test on the residuals is
>crucial in coming up with appriate analysis?
>
>Cheers,
>Andy
>
>  
>
>>From: Federico Gherardini
>>
>>Berton Gunter wrote:
>>
>>    
>>
>>>>>Exactly! My point is that normality tests are useless for 
>>>>>          
>>>>>
>>this purpose for
>>    
>>
>>>>>reasons that are beyond what I can take up here. 
>>>>>
>>>>>          
>>>>>
>>Thanks for your suggestions, I undesrtand that! Could you 
>>possibly give 
>>me some (not too complicated!)
>>links so that I can investigate this matter further?
>>
>>Cheers,
>>
>>Federico
>>
>>    
>>
>>>>>Hints: Balanced designs are
>>>>>robust to non-normality; independence (especially 
>>>>>          
>>>>>
>>"clustering" of subjects
>>    
>>
>>>>>due to systematic effects), not normality is usually the 
>>>>>          
>>>>>
>>biggest real
>>    
>>
>>>>>statistical problem; hypothesis tests will always reject 
>>>>>          
>>>>>
>>when samples are
>>    
>>
>>>>>large -- so what!; "trust" refers to prediction validity 
>>>>>          
>>>>>
>>which has to do
>>    
>>
>>>>>with study design and the validity/representativeness of 
>>>>>          
>>>>>
>>the current data to
>>    
>>
>>>>>future. 
>>>>>
>>>>>I know that all the stats 101 tests say to test for 
>>>>>          
>>>>>
>>normality, but they're
>>    
>>
>>>>>full of baloney!
>>>>>
>>>>>Of course, this is "free" advice -- so caveat emptor!
>>>>>
>>>>>Cheers,
>>>>>Bert
>>>>>
>>>>>     
>>>>>
>>>>>          
>>>>>
>>______________________________________________
>>R-help at stat.math.ethz.ch mailing list
>>https://stat.ethz.ch/mailman/listinfo/r-help
>>PLEASE do read the posting guide! 
>>http://www.R-project.org/posting-guide.html
>>
>>
>>    
>>
>
>______________________________________________
>R-help at stat.math.ethz.ch mailing list
>https://stat.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>  
>

-- 
Spencer Graves, PhD, Senior Development Engineer
O:  (408)938-4420;  mobile:  (408)655-4567



From lisawang at uhnres.utoronto.ca  Fri Oct 15 21:39:17 2004
From: lisawang at uhnres.utoronto.ca (Lisa Wang)
Date: Fri, 15 Oct 2004 15:39:17 -0400
Subject: [R] categorical varibles in coxph
Message-ID: <41702765.75E95FDD@uhnres.utoronto.ca>

Hello,

I wonder when I do coxph in R:

coxph( Surv(start, stop, event) ~ x, data=test)

If x is a categorical varible (1,2,3,4,5), should I creat four dummy
varibles for it? if yes, how can I get the overall p value on x other
than for each dummy variable?

Thanks

Lisa Wang

Princess Margaret Hospital
Phone 416 946 4501



From aorchid at mac.com  Fri Oct 15 21:49:48 2004
From: aorchid at mac.com (Aric Gregson)
Date: Fri, 15 Oct 2004 15:49:48 -0400
Subject: [R] Using accrualReport.s in Hmisc/Design
Message-ID: <r02010400-1035-5F88AFA31EE311D99887000A959B3D22@[134.192.145.239]>

Hello,

I apologize for the post if this is not the appropriate place. I have
searched the R List archives and the Vanderbilt.edu website, but am not
able to understand how to use the accrualReport.s and associated scripts
available at the Vanderbilt site. It must be very simple, but I am at a
loss for ideas. Any suggestions or pointers in the direction of
examples?

thanks very much.

aric



From KalayliogluZ at imsweb.com  Fri Oct 15 21:52:11 2004
From: KalayliogluZ at imsweb.com (Kalaylioglu, Zeynep (IMS))
Date: Fri, 15 Oct 2004 15:52:11 -0400
Subject: [R] tree version 1.0-16
Message-ID: <782D2A81EC812642B857B03B506E0B4404B766@granite.omni.imsweb.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20041015/0230c043/attachment.pl

From br44114 at yahoo.com  Fri Oct 15 21:58:47 2004
From: br44114 at yahoo.com (bogdan romocea)
Date: Fri, 15 Oct 2004 12:58:47 -0700 (PDT)
Subject: [R] combine many .csv files into a single file/data frame
Message-ID: <20041015195848.26724.qmail@web50307.mail.yahoo.com>

Dear R users,

I have a few hundred .csv files which I need to put
together (full outer joins on a common variable) to do a
factor analysis. Each file may contain anywhere from a few
hundred to a few thousand rows. What would be the most
efficient way to do this in R? Please include some sample
code if applicable.

Thank you,
b.



From Russ at IDesireSuccess.com  Fri Oct 15 22:34:34 2004
From: Russ at IDesireSuccess.com (Russ Stiffler)
Date: Fri, 15 Oct 2004 16:34:34 -0400
Subject: [R] This might not be for you, or it might
Message-ID: <1097870157.36748@autocontactor.com>


Dear ,

Are you in the United States?

Are you serious about financial success?

Are you open to look at a home-based business?

Are you looking for something that will give you
serious rewards for your effort?

If you answered yes to these questions read on. 
If you are among the few that see the possibilies
this can change your life.

It is changing mine. I have been very successful as
an athlete and coach. I have helped my students 
become World and National Champions. But making
money has never been a strength of mine. 

That has now changed. I am on my way to making a
residual income of $100,000 a year.

Part of the reason I love this is that I get the chance
to use all the coaching skill and experience I have 
gained over the years. I get to help those few that are
ready to learn. I get to help them gain financial success.
And along the way I'll probably make a few great 
friends. What can be better than that?

Let me tell you why this is changing for me and maybe
that can help you see what I see.

The downfall of most companies is customer retention.
To maintain your income in most companies you must
constantly replace lost customers.

We are different.

Our historic customer retention rate is 92.4%

Now each new customer increases your income.

Our system and training program is designed to produce
a residual income of $100,000 in two to four years
working 10 to 12 hours per week.

Is 10 to 12 hours a week worth a lifetime residual income
of $100,000 to you?

If so this could be your time to succeed. Then it is time to
visit my Website to find out if we are a good fit:
http://www.MyTimeToWin.com

To Your Success,

Russ Stiffler

P.S. We are rolling out, right now, a new product that 
increases commission payouts by two to three times.








If you no longer wish to receive communication from us:
http://autocontactor.com/app/r.asp?ID=18836264&ARID=0

To update your contact information:
http://autocontactor.com/app/r.asp?c=1&ID=18836264



From f.harrell at vanderbilt.edu  Fri Oct 15 22:23:38 2004
From: f.harrell at vanderbilt.edu (Frank E Harrell Jr)
Date: Fri, 15 Oct 2004 16:23:38 -0400
Subject: [R] Using accrualReport.s in Hmisc/Design
In-Reply-To: <r02010400-1035-5F88AFA31EE311D99887000A959B3D22@[134.192.145.239]>
References: <r02010400-1035-5F88AFA31EE311D99887000A959B3D22@[134.192.145.239]>
Message-ID: <417031CA.3050201@vanderbilt.edu>

Aric Gregson wrote:
> Hello,
> 
> I apologize for the post if this is not the appropriate place. I have
> searched the R List archives and the Vanderbilt.edu website, but am not
> able to understand how to use the accrualReport.s and associated scripts
> available at the Vanderbilt site. It must be very simple, but I am at a
> loss for ideas. Any suggestions or pointers in the direction of
> examples?
> 
> thanks very much.
> 
> aric
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
> 

That is part of a new package for clinical trials reporting that is not 
packaged for use by 'outsiders' yet.  The R package name will be rreport.

-- 
Frank E Harrell Jr   Professor and Chair           School of Medicine
                      Department of Biostatistics   Vanderbilt University



From tlumley at u.washington.edu  Fri Oct 15 22:27:15 2004
From: tlumley at u.washington.edu (Thomas Lumley)
Date: Fri, 15 Oct 2004 13:27:15 -0700 (PDT)
Subject: [R] categorical varibles in coxph
In-Reply-To: <41702765.75E95FDD@uhnres.utoronto.ca>
References: <41702765.75E95FDD@uhnres.utoronto.ca>
Message-ID: <Pine.A41.4.61a.0410151324390.197642@homer12.u.washington.edu>

On Fri, 15 Oct 2004, Lisa Wang wrote:

> Hello,
>
> I wonder when I do coxph in R:
>
> coxph( Surv(start, stop, event) ~ x, data=test)
>
> If x is a categorical varible (1,2,3,4,5), should I creat four dummy
> varibles for it? if yes, how can I get the overall p value on x other
> than for each dummy variable?
>

No. Use
   coxph( Surv(start, stop, event) ~ factor(x), data=test)
or define x as a factor.

For an overal test use anova(): eg

> data(pbc)
> model<-coxph(Surv(time,status)~factor(edtrt)+bili, data=pbc)
> model
Call:
coxph(formula = Surv(time, status) ~ factor(edtrt) + bili, data = pbc)


                   coef exp(coef) se(coef)    z       p
factor(edtrt)0.5 0.629      1.88   0.2297 2.74 6.2e-03
factor(edtrt)1   1.664      5.28   0.2762 6.02 1.7e-09
bili             0.119      1.13   0.0129 9.29 0.0e+00

Likelihood ratio test=127  on 3 df, p=0  n= 418
> anova(model)
Analysis of Deviance Table
  Cox model: response is Surv(time, status)
Terms added sequentially (first to last)

                Df Deviance Resid. Df Resid. Dev
NULL                             418    1746.94
factor(edtrt)   2    62.13       416    1684.82
bili            1    65.11       415    1619.71
> summary(model)
Call:
coxph(formula = Surv(time, status) ~ factor(edtrt) + bili, data = pbc)

   n= 418

                   coef exp(coef) se(coef)    z       p
factor(edtrt)0.5 0.629      1.88   0.2297 2.74 6.2e-03
factor(edtrt)1   1.664      5.28   0.2762 6.02 1.7e-09
bili             0.119      1.13   0.0129 9.29 0.0e+00

                  exp(coef) exp(-coef) lower .95 upper .95
factor(edtrt)0.5      1.88      0.533      1.20      2.94
factor(edtrt)1        5.28      0.189      3.07      9.07
bili                  1.13      0.887      1.10      1.16

Rsquare= 0.262   (max possible= 0.985 )
Likelihood ratio test= 127  on 3 df,   p=0
Wald test            = 193  on 3 df,   p=0
Score (logrank) test = 281  on 3 df,   p=0



From jmacdon at med.umich.edu  Fri Oct 15 22:29:49 2004
From: jmacdon at med.umich.edu (James W. MacDonald)
Date: Fri, 15 Oct 2004 16:29:49 -0400
Subject: [R] tree version 1.0-16
In-Reply-To: <782D2A81EC812642B857B03B506E0B4404B766@granite.omni.imsweb.com>
References: <782D2A81EC812642B857B03B506E0B4404B766@granite.omni.imsweb.com>
Message-ID: <4170333D.8020500@med.umich.edu>

Kalaylioglu, Zeynep (IMS) wrote:
> Hi
> There is something weird going on with the "tree" package version
> 1.0-16.
> So I want to download an updated tree package. I found that R website
> has version 1.0-18. I want to try this one and see how the tree
> algorithm is performing.
> What is the best way to download tree package version 1.0-18 which can
> be found in index of src/contrib at www.r-project.org?
> So far I tried two different ways none of which has worked for me:
> 1) I tried to download it by running the command 
> install.packages("tree","C:\Program
> Files\R\rw1081\library","http://cran.r-project.org/src/contrib",destdir=
> "C:\Program <http://cran.r-project.org/src/contrib",destdir="C:/Program>
> Files\R\rw1081\library",installWithVers=TRUE)
> But it tries to connect to
> http://cran.r-project.org/src/contrib/bin/windows/contrib/1.8/PACKAGES
> <http://cran.r-project.org/src/contrib/bin/windows/contrib/1.8/PACKAGES>
> instead of the URL I enter above.

How about Packages --> Install package(s) from CRAN...? You are making 
things unnecessarily difficult.

>  
> 2) Also I tried to download it by copying the zip file first
>  tree_1.0-18.tar.gz

Zip files don't end with .tar.gz! These are tarred and gzipped source 
packages that you are almost certainly not set up to install. Your best 
bet is to try the simplest things first, which on Windows is to use the 
Packages menu.

Best,

Jim



> <http://cran.r-project.org/src/contrib/tree_1.0-18.tar.gz>   in my
> directory and then using the "Packages->Install package from local -zip
> files" in the R menu. 
>  
> How can I download this version of tree? Thanks.
>  
> Zeynep
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html


-- 
James W. MacDonald
Affymetrix and cDNA Microarray Core
University of Michigan Cancer Center
1500 E. Medical Center Drive
7410 CCGC
Ann Arbor MI 48109



From Ted.Harding at nessie.mcc.ac.uk  Fri Oct 15 22:41:44 2004
From: Ted.Harding at nessie.mcc.ac.uk ( (Ted Harding))
Date: Fri, 15 Oct 2004 21:41:44 +0100 (BST)
Subject: [R] combine many .csv files into a single file/data frame
In-Reply-To: <20041015195848.26724.qmail@web50307.mail.yahoo.com>
Message-ID: <XFMail.041015214144.Ted.Harding@nessie.mcc.ac.uk>

On 15-Oct-04 bogdan romocea wrote:
> Dear R users,
> 
> I have a few hundred .csv files which I need to put
> together (full outer joins on a common variable) to do a
> factor analysis. Each file may contain anywhere from a few
> hundred to a few thousand rows. What would be the most
> efficient way to do this in R? Please include some sample
> code if applicable.

If you're using Linux/Unix, you should consider using the 'join'
command. Simple example (jpoining files "j1" and "j2"):

$ cat j1
1 a
1 b
2 c
2 d
2 e
3 f
3 g
4 h

$ cat j2
1 A
1 B
2 C
3 D
3 E
3 F

$ join j1 j2
1 a A
1 a B
1 b A
1 b B
2 c C
2 d C
2 e C
3 f D
3 f E
3 f F
3 g D
3 g E
3 g F

See 'man join' for details of options which you can use to
adapt the command to your needs.

Hoping this helps,
Ted.


--------------------------------------------------------------------
E-Mail: (Ted Harding) <Ted.Harding at nessie.mcc.ac.uk>
Fax-to-email: +44 (0)870 094 0861  [NB: New number!]
Date: 15-Oct-04                                       Time: 21:41:44
------------------------------ XFMail ------------------------------



From gpagnon at emory.edu  Sat Oct 16 01:55:40 2004
From: gpagnon at emory.edu (Giuseppe Pagnoni)
Date: Fri, 15 Oct 2004 19:55:40 -0400
Subject: [R] power in a specific frequency band
Message-ID: <B7B01462-1F05-11D9-9D36-000D932922EA@emory.edu>

Dear R users

I have a really simple question (hoping for a really simple answer :-):

Having estimated the spectral density of a time series "x" (heart rate 
data) with:

x.pgram <- spectrum(x,method="pgram")

  I would like to compute the power in a specific energy band.

Assuming that frequency(x)=4 (Hz), and that I am interested in the band 
between f1 and f2, is the power in the band simply the following?

sum(x.pgram$spec[(x.pgram$freq > f1/frequency(x)) & (x.pgram$freq <= 
f2/frequency(x))])

If it is so, are the returned units the same units as the original time 
series, but squared (if x is bpm, then the power is in bpm^2)?

I own a copy of Venables and Ripley (MASS 2003), but I was not able to 
extract this information from the time series chapter....


thanks for any help
(please cc to my e-mail, if possible)


         giuseppe





Giuseppe Pagnoni
Dept. Psychiatry and Behavioral Sciences
Emory University
1639 Pierce Drive, Suite 4000
Atlanta, GA, 30322
tel: 404.712.8431
fax: 404.727.3233



From kjetil at acelerate.com  Fri Oct 15 21:38:11 2004
From: kjetil at acelerate.com (Kjetil Brinchmann Halvorsen)
Date: Fri, 15 Oct 2004 15:38:11 -0400
Subject: [R] Testing for normality of residuals in a regression model
In-Reply-To: <3A822319EB35174CA3714066D590DCD504AF8554@usrymx25.merck.com>
References: <3A822319EB35174CA3714066D590DCD504AF8554@usrymx25.merck.com>
Message-ID: <41702723.9050404@acelerate.com>

Liaw, Andy wrote:

>.
>
.
.
.

>
>Also, I was told by someone very smart that fitting OLS to data with
>heteroscedastic errors can make the residuals look `more normal' than they
>really are...  Don't know how true that is, though.  
>  
>
Certainly true, since the residuals will be a kind of average, so the 
CLT works.
(Think that is in Seber, Linear Regression Analysis, 1977)

Kjetil

>Best,
>Andy
>
>  
>

-- 

Kjetil Halvorsen.

Peace is the most effective weapon of mass construction.
               --  Mahdi Elmandjra



From Rajiv.Prasad at pnl.gov  Sat Oct 16 05:32:24 2004
From: Rajiv.Prasad at pnl.gov (Prasad, Rajiv)
Date: Fri, 15 Oct 2004 20:32:24 -0700
Subject: [R] Problem Compiling R-2.0.0 on Linux Alpha
Message-ID: <AF293AF0A07C8A44A6098DA99D03713008926F@pnlmse24.pnl.gov>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20041015/e9ac63a3/attachment.pl

From sun at cae.wisc.edu  Sat Oct 16 06:24:15 2004
From: sun at cae.wisc.edu (Sun)
Date: Fri, 15 Oct 2004 23:24:15 -0500
Subject: [R] 20,000 * 6 data values
References: <3A822319EB35174CA3714066D590DCD504AF854F@usrymx25.merck.com>
	<41701472.1090602@pdf.com>
Message-ID: <032201c4b337$ff01ae90$23719792@star>

Hello, Rusers:

What is the maximum number of data R can handle? Or I have to use SAS? I am
trying to do some microarray data analysis. But I am totally new. Did anyone
use R to do microarray analysis?

Many thanks,

Sun



From nusbj at hotmail.com  Sat Oct 16 06:50:06 2004
From: nusbj at hotmail.com (Zhen Pang)
Date: Sat, 16 Oct 2004 12:50:06 +0800
Subject: [R] sapply and loop
Message-ID: <BAY22-F16KtKufVma0o00002e37@hotmail.com>

Dear all,

I am doing 200 times simulation. For each time, I generate a matrix and 
define some function on this matrix to get a 6 dimension vector as my 
results.

As the loop should be slow, I generate 200 matrice first, and save them into 
a list named ma,
then I define zz<-sapply(ma, myfunction)

To my surprise, It almost costs me the same time to get my results if I 
directly use a loop from 1 to 200. Is it common? Can I improve any further?

Ps, how to count the exact time to finish my code?

Thanks.

Zhen



From merser at image.dk  Sat Oct 16 10:02:58 2004
From: merser at image.dk (=?iso-8859-1?Q?S=F8ren_Merser?=)
Date: Sat, 16 Oct 2004 10:02:58 +0200
Subject: [R] hashing
Message-ID: <000301c4b356$98551a50$8b00a8c0@IBM>

is hashing implemented in R
regards s??ren



From bobby.corpus at gmail.com  Sat Oct 16 10:35:08 2004
From: bobby.corpus at gmail.com (Bobby Corpus)
Date: Sat, 16 Oct 2004 16:35:08 +0800
Subject: [R] Problem with number characters
In-Reply-To: <200410141831.i9EIVX322036@snow.pnl.gov>
References: <200410141831.i9EIVX322036@snow.pnl.gov>
Message-ID: <3f1349b041016013511a530c4@mail.gmail.com>

Hi Scott,

What's the result of running  the linux "file" command on your input file?
Does it give "ISO-8859 text " or something else?

example:
[bobby at thor bobby]$ file test2.txt
test2.txt: ISO-8859 text

Best regards,

Bobby


On Thu, 14 Oct 2004 11:31:33 -0700, Scott Waichler
<scott.waichler at pnl.gov> wrote:
> I am trying to process text fields scanned in from a csv file that is
> output from the Windows database program FileMakerPro.  The characters
> onscreen look like regular text, but R does not like their underlying binary form.
> For example, one of text fields contains a name and a number, but
> R recognizes the number as something other than what it appears
> to be in plain text.  The character string "Draszt  03" after being
> read into R using scan and ="" becomes "Draszt 03" where the 3 is
> displayed in my R session as a superscript.  Here is the result pasted
> into this email I'm composing in emacs:  "Draszt 0%/1?iso8859-15??"
> Another clue for the knowledgable:  when I try to display the vector element
> causing trouble, I get
>   <CHARSXP: "Draszt 0%/1?iso8859-15??">
> where again the superscipt part is just "3" in my R session.  I'm working in
> Linux, R version 1.9.1, 2004-06-21.  Your help will be much appreciated.
> 
> Scott Waichler
> Pacific Northwest National Laboratory
> scott.waichler at pnl.gov
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>



From p.dalgaard at biostat.ku.dk  Sat Oct 16 10:33:19 2004
From: p.dalgaard at biostat.ku.dk (Peter Dalgaard)
Date: 16 Oct 2004 10:33:19 +0200
Subject: [R] Problem Compiling R-2.0.0 on Linux Alpha
In-Reply-To: <AF293AF0A07C8A44A6098DA99D03713008926F@pnlmse24.pnl.gov>
References: <AF293AF0A07C8A44A6098DA99D03713008926F@pnlmse24.pnl.gov>
Message-ID: <x2ekjznju8.fsf@biostat.ku.dk>

"Prasad, Rajiv" <Rajiv.Prasad at pnl.gov> writes:

> Error in "names<-.default"(`*tmp*`, value = c("R", "Platform", "Date",
> :
>         names attribute [4] must be the same length as the vector [3]
> Execution halted
> make[2]: *** [R] Error 1
> make[2]: Leaving directory
> `/home1/rajiv/software/src-7.2/R-2.0.0/src/library'
> make[1]: *** [R] Error 1
> make[1]: Leaving directory `/home1/rajiv/software/src-7.2/R-2.0.0/src'
> make: *** [R] Error 1
> 
> Does this indicate a problem with R, or something in my system
> installation?  A search on R help list archives did not turn up this
> particular error.

Specific to your architecture at least (not too many Alphas around,
but this works elsewhere). It's a pretty odd bug: It would seem to
come from inside .split_description (tools/R/admin.R) where it would
indicate that the Built field has less than three "; " separators, but
the paste construct in .install_package_description that creates the
field does seem to put them in. One conjecture is that a newline
sneaked in somehow...

I'd first look for malformed DESCRIPTION files in your build
directory, then maybe insert some debugging code into
.split_description (e.g. print(sys.status());print(Built) immediately
before the names<- bit).

Are you doing this in a clean directory, btw, or might you possibly be
picking up bits of a previous build?

-- 
   O__  ---- Peter Dalgaard             Blegdamsvej 3  
  c/ /'_ --- Dept. of Biostatistics     2200 Cph. N   
 (*) \(*) -- University of Copenhagen   Denmark      Ph: (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)             FAX: (+45) 35327907



From ripley at stats.ox.ac.uk  Sat Oct 16 10:51:48 2004
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Sat, 16 Oct 2004 09:51:48 +0100 (BST)
Subject: [R] Problem with number characters
In-Reply-To: <3f1349b041016013511a530c4@mail.gmail.com>
Message-ID: <Pine.LNX.4.44.0410160944220.7497-100000@gannet.stats>

Since ISO-8859 has many different encodings, you need something more
precise.  I am seeing 'iso8859-15' embedded in that email.  AFAIK GNU
`file' or any other Linux utility does not know the encoding of a file,
and it is guessing that it has `ISO-8859 text' when it sees 8-bit files
that are not apparently UTF-8 etc.

R assumes that text files are encoded in the current locale (and does not 
support multibyte locales). That is planned to change one day.

On Sat, 16 Oct 2004, Bobby Corpus wrote:

> Hi Scott,
> 
> What's the result of running  the linux "file" command on your input file?
> Does it give "ISO-8859 text " or something else?
> 
> example:
> [bobby at thor bobby]$ file test2.txt
> test2.txt: ISO-8859 text
> 
> Best regards,
> 
> Bobby
> 
> 
> On Thu, 14 Oct 2004 11:31:33 -0700, Scott Waichler
> <scott.waichler at pnl.gov> wrote:
> > I am trying to process text fields scanned in from a csv file that is
> > output from the Windows database program FileMakerPro.  The characters
> > onscreen look like regular text, but R does not like their underlying binary form.
> > For example, one of text fields contains a name and a number, but
> > R recognizes the number as something other than what it appears
> > to be in plain text.  The character string "Draszt  03" after being
> > read into R using scan and ="" becomes "Draszt 03" where the 3 is
> > displayed in my R session as a superscript.  Here is the result pasted
> > into this email I'm composing in emacs:  "Draszt 0%/1?iso8859-15??"
> > Another clue for the knowledgable:  when I try to display the vector element
> > causing trouble, I get
> >   <CHARSXP: "Draszt 0%/1?iso8859-15??">
> > where again the superscipt part is just "3" in my R session.  I'm working in
> > Linux, R version 1.9.1, 2004-06-21.  Your help will be much appreciated.

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From f.gherardini at pigrecodata.net  Sat Oct 16 13:31:37 2004
From: f.gherardini at pigrecodata.net (Federico Gherardini)
Date: Sat, 16 Oct 2004 13:31:37 +0200
Subject: [R] Testing for normality of residuals in a regression model
In-Reply-To: <Pine.LNX.4.44.0410151903370.25694-100000@gannet.stats>
References: <Pine.LNX.4.44.0410151903370.25694-100000@gannet.stats>
Message-ID: <41710699.5000900@pigrecodata.net>

Prof Brian Ripley wrote:

>>However, stats 901 or some such tells you that if the distributions have 
>>even slightly longer tails than the normal you can get much better 
>>estimates than OLS, and this happens even before a test of normality 
>>rejects on a sample size of thousands.
>>
>>Robustness of efficiency is much more important than robustness of 
>>distribution, and I believe robustness concepts should be in stats 101.
>>(I was teaching them yesterday in the third lecture of a basic course, 
>>albeit a graduate course.)
>>    
>>
This is a very interesting discussion. So you are basically saying that 
it's better to use robust regression methods, without having to worry 
too much about the distribution of residuals, instead of using standard 
methods and doing a lot of tests to check for normality? Did I get your 
point?


Cheers,

Federico



From maechler at stat.math.ethz.ch  Sat Oct 16 11:38:14 2004
From: maechler at stat.math.ethz.ch (Martin Maechler)
Date: Sat, 16 Oct 2004 11:38:14 +0200
Subject: [R] tree version 1.0-16
In-Reply-To: <782D2A81EC812642B857B03B506E0B4404B766@granite.omni.imsweb.com>
References: <782D2A81EC812642B857B03B506E0B4404B766@granite.omni.imsweb.com>
Message-ID: <16752.60422.885916.797779@gargle.gargle.HOWL>

>>>>> "ZKala" == Kalaylioglu, Zeynep (IMS) <KalayliogluZ at imsweb.com>
>>>>>     on Fri, 15 Oct 2004 15:52:11 -0400 writes:

    ZKala> Hi There is something weird going on with the "tree"
    ZKala> package version 1.0-16.  So I want to download an
    ZKala> updated tree package. I found that R website has
    ZKala> version 1.0-18. I want to try this one and see how
    ZKala> the tree algorithm is performing.  What is the best
    ZKala> way to download tree package version 1.0-18 which can
    ZKala> be found in index of src/contrib at
    ZKala> www.r-project.org?  So far I tried two different ways
    ZKala> none of which has worked for me: 1) I tried to
    ZKala> download it by running the command
    ZKala> install.packages("tree","C:\Program
    ZKala> Files\R\rw1081\library","http://cran.r-project.org/src/contrib",destdir=
    ZKala> "C:\Program
    ZKala> <http://cran.r-project.org/src/contrib",destdir="C:/Program>
    ZKala> Files\R\rw1081\library",installWithVers=TRUE) But it
    ZKala> tries to connect to
    ZKala> http://cran.r-project.org/src/contrib/bin/windows/contrib/1.8/PACKAGES
    ZKala> <http://cran.r-project.org/src/contrib/bin/windows/contrib/1.8/PACKAGES>
    ZKala> instead of the URL I enter above.

yes, I see  that your R version is 'R (for windows) 1.8.1'
and that's probably why it tries to get package for that
version.

If you want to upgrade packages, I strongly recommend to first
upgrade R to the current version 2.0.0 (or maybe "2.0.0 patched")
-- and then follow the advice from James MacDonald.

Martin Maechler, ETH Zurich
 
    ZKala> 2) Also I tried to download it by copying the zip
    ZKala> file first tree_1.0-18.tar.gz
    ZKala> <http://cran.r-project.org/src/contrib/tree_1.0-18.tar.gz>
    ZKala> in my directory and then using the "Packages->Install
    ZKala> package from local -zip files" in the R menu.
 
    ZKala> How can I download this version of tree? Thanks.
 
    ZKala> Zeynep

    ZKala> 	[[alternative HTML version deleted]]

    ZKala> ______________________________________________
    ZKala> R-help at stat.math.ethz.ch mailing list
    ZKala> https://stat.ethz.ch/mailman/listinfo/r-help PLEASE
    ZKala> do read the posting guide!
    ZKala> http://www.R-project.org/posting-guide.html



From Ted.Harding at nessie.mcc.ac.uk  Sat Oct 16 11:39:05 2004
From: Ted.Harding at nessie.mcc.ac.uk ( (Ted Harding))
Date: Sat, 16 Oct 2004 10:39:05 +0100 (BST)
Subject: [R] combine many .csv files into a single file/data frame
In-Reply-To: <20041015195848.26724.qmail@web50307.mail.yahoo.com>
Message-ID: <XFMail.041016103905.Ted.Harding@nessie.mcc.ac.uk>

On 15-Oct-04 bogdan romocea wrote:
> I have a few hundred .csv files which I need to put
> together (full outer joins on a common variable) to do a
> factor analysis. Each file may contain anywhere from a few
> hundred to a few thousand rows. What would be the most
> efficient way to do this in R? Please include some sample
> code if applicable.

Sorry, the example I posted last night was written too hastily
and does not illustrate your precise query well. Here is a
better one, with explanation.

$ cat j1
1,a
1,b
2,c
2,d
2,e
3,f
3,g
4,h
4,i
5,j

$ cat j2
1,A
1,B
1,C
2,D
2,E
4,F
4,G
5,H
6,I
6,J

$ join -t , -a 1 -a 2 -o 0,1.2,2.2 j1 j2
1,a,A
1,a,B
1,a,C
1,b,A
1,b,B
1,b,C
2,c,D
2,c,E
2,d,D
2,d,E
2,e,D
2,e,E
3,f,
3,g,
4,h,F
4,h,G
4,i,F
4,i,G
5,j,H
6,,I
6,,J

Explanation of options:

"-t ,"    Input and output field separator is "," (for CSV)
"-a 1"    Output a line for every line of j1 not matched in j2
"-a 2"    Output a line for every line of j2 not matched in j1
"-o 0,1.2,2.2"  Output field format specification:
    0 denotes the match (join) field (needed when using "-a")
    1.2 denotes field 2 from file 1 ("j1")
    2.2 denotes field 2 from file 2 ("j2")
"j1" and "j2" are of course the two files to be joined.

Using the "-a" option gives you the full outer join which you want.

This command only works for two files at a time (and you must give
two). To join several files you would have to loop through them on
the lines of

$ join -t , -a 1 -a 2 -o 0,1.2,2.2 j1 j2 > J

which creates a file "J" which is the full outer join of "j1", "j2".

Then

$ join -t , -a 1 -a 2 -o 0,1.2,2.2 J j3 > J

and so on through j4, j5, ...

For your "few hundred files" this is best done with a loop like

$ for i in * ; do join -t , -a 1 -a 2 -o 0,1.2,2.2 J $i > J ; done

having first done it for j1, j2 as above and put these two file
out of sight in a different directory. J itself also needs to
be out of sight otherwise it will get joined to itself at some
stage. E.g. where "J" is written above it could in fact be
written as "../joins/J" and similarly for "../joins/j1" and
"../joins/j2".

E.g. first move j1 & j2 to ../joins and then do

$ join -t , -a 1 -a 2 -o 0,1.2,2.2 ../joins/j1 ../joins/j2 > ../joins/J

and then

$ for i in * ; do
    join -t , -a 1 -a 2 -o 0,1.2,2.2 ../joins/J $i > ../joins/J
  done


Sorry for the previous sloppy response, and hoping the above
helps.

Best wishes,
Ted.



--------------------------------------------------------------------
E-Mail: (Ted Harding) <Ted.Harding at nessie.mcc.ac.uk>
Fax-to-email: +44 (0)870 094 0861  [NB: New number!]
Date: 16-Oct-04                                       Time: 10:39:05
------------------------------ XFMail ------------------------------



From segu at gmx.net  Sat Oct 16 11:51:11 2004
From: segu at gmx.net (Sepp Gurgel)
Date: Sat, 16 Oct 2004 11:51:11 +0200 (MEST)
Subject: [R] barplot
Message-ID: <3908.1097920271@www33.gmx.net>

I have a table with two columns, one with types of blood (A, B, AB or 0) and
one with the factor (negative = -1 or positive = 1).

How can I combine those two columns so that 7 bars are plotted (A, B, AB, 0,
-A, -B and -0)?

--



From mayeul.kauffmann at tiscali.fr  Sat Oct 16 11:50:39 2004
From: mayeul.kauffmann at tiscali.fr (Mayeul KAUFFMANN)
Date: Sat, 16 Oct 2004 11:50:39 +0200
Subject: [R] sapply and loop
Message-ID: <000301c4b365$9a9f81b0$d0599a53@amd>

Zhen,

>how to count the exact time ?

system.time(base)
Returns CPU (and other) times that expr used.


If you only need seconds, you can also do

date();zz<-sapply(ma, myfunction);date()

I do not know about how to reduce the time.
For very comlex iterations, I use for( ) myself, which maybe inneficient.

Mayeul KAUFFMANN
Universit?? Pierre Mend??s France
Grenoble
France



From nleonard at tartarus.uwa.edu.au  Sat Oct 16 12:12:37 2004
From: nleonard at tartarus.uwa.edu.au (Neil Leonard)
Date: Sat, 16 Oct 2004 18:12:37 +0800
Subject: [R] Cox PH Warning Message
Message-ID: <E792896E-1F5B-11D9-8E98-003065D5B8EC@tartarus.uwa.edu.au>

Hi,

Can anybody tell me what the message below means and how to overcome it.

Thanks,
Neil

Warning message:
X matrix deemed to be singular; variable 2 in: coxph(Surv(age_at_death, 
death) ~ project$pluralgp + project$yrborn +  .........
 >



From sdavis2 at mail.nih.gov  Sat Oct 16 12:14:18 2004
From: sdavis2 at mail.nih.gov (Sean Davis)
Date: Sat, 16 Oct 2004 06:14:18 -0400
Subject: [R] 20,000 * 6 data values
References: <3A822319EB35174CA3714066D590DCD504AF854F@usrymx25.merck.com><41701472.1090602@pdf.com>
	<032201c4b337$ff01ae90$23719792@star>
Message-ID: <000c01c4b368$e90c13b0$04653744@WATSON>

Sun,

There are hundreds using R for microarray analysis.  You probably want to
avail yourself of the bioconductor tools (http://www.bioconductor.org).
20,000 rows with 6 data values is a small data set, by microarray standards,
and is easily handled by R and the bioconductor tools.  I routinely analyze
a couple of hundred experiments with 40-50k rows, and most of the time, that
doesn't push R very hard (on my 2 processor, 4Gb machine, at least).

Sean

----- Original Message -----
From: "Sun" <sun at cae.wisc.edu>
To: "R-help mailing list" <R-help at stat.math.ethz.ch>
Sent: Saturday, October 16, 2004 12:24 AM
Subject: [R] 20,000 * 6 data values


> Hello, Rusers:
>
> What is the maximum number of data R can handle? Or I have to use SAS? I
am
> trying to do some microarray data analysis. But I am totally new. Did
anyone
> use R to do microarray analysis?
>
> Many thanks,
>
> Sun
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide!
http://www.R-project.org/posting-guide.html
>



From mayeul.kauffmann at tiscali.fr  Sat Oct 16 12:32:40 2004
From: mayeul.kauffmann at tiscali.fr (Mayeul KAUFFMANN)
Date: Sat, 16 Oct 2004 12:32:40 +0200
Subject: [R] Cox PH Warning Message
Message-ID: <000301c4b36b$77a12aa0$d0599a53@amd>

"Hi,
Can anybody tell me what the message below means and how to overcome it.
Thanks,
Neil
Warning message:
X matrix deemed to be singular; variable 2 in: coxph(Surv(age_at_death,
death) ~ project$pluralgp + project$yrborn +  ........."

Your 2nd covariate (yrborn) is colinear to the other covariates (or nearly
colinear). R drops it, but warns you. The results are OK.

Mayeul KAUFFMANN
Universit?? Pierre Mend??s France
Grenoble
France



From nleonard at tartarus.uwa.edu.au  Sat Oct 16 12:45:17 2004
From: nleonard at tartarus.uwa.edu.au (Neil Leonard)
Date: Sat, 16 Oct 2004 18:45:17 +0800
Subject: [R] Cox PH Warning Message
In-Reply-To: <000301c4b36b$77a12aa0$d0599a53@amd>
References: <000301c4b36b$77a12aa0$d0599a53@amd>
Message-ID: <77CC9C0D-1F60-11D9-8E98-003065D5B8EC@tartarus.uwa.edu.au>

This is the output. I think it means that there is a problem with 
project$pluralgpTriplet???

Neil

 > fm.CPH
Call:
coxph(formula = Surv(age_at_death, death) ~ project$pluralgp +
     project$yrborn + project$private + project$pcdead + project$mheight 
+
     project$ga2 + project$apgcode + project$apgar5 + project$VLBWgp +
     project$PEBW + project$LBWgp + project$LBWcat + project$totadm +
     project$totlos + project$sex + teen + project$DS + project$pcsborn +
     project$disadv + project$ecores + project$edoccu)


                                 coef exp(coef) se(coef)       z       p
project$pluralgpTwin       -0.509203     0.601  0.74415 -0.6843 4.9e-01
project$pluralgpTriplet           NA        NA  0.00000      NA      NA
project$yrborn1988-1992    -0.083348     0.920  0.22553 -0.3696 7.1e-01
project$private             0.260465     1.298  0.12384  2.1032 3.5e-02
project$pcdead             -0.058200     0.943  0.49418 -0.1178 9.1e-01
project$mheight             0.016361     1.016  0.01745  0.9378 3.5e-01
project$ga2                 0.104246     1.110  0.10162  1.0258 3.0e-01
project$apgcode4-7         -0.266885     0.766  0.50211 -0.5315 6.0e-01
project$apgcode1-3         -1.704620     0.182  1.12545 -1.5146 1.3e-01
project$apgar5             -0.427139     0.652  0.14099 -3.0296 2.4e-03
project$VLBWgp=>1500 grams  0.046203     1.047  0.65494  0.0705 9.4e-01
project$PEBW                0.015297     1.015  0.01356  1.1281 2.6e-01
project$LBWgp=>2500 grams  -0.257472     0.773  0.42496 -0.6059 5.4e-01
project$LBWcat             -0.222823     0.800  0.23938 -0.9308 3.5e-01
project$totadm              0.005836     1.006  0.01536  0.3800 7.0e-01
project$totlos              0.007342     1.007  0.00176  4.1664 3.1e-05
project$sexFemale          -0.016431     0.984  0.22803 -0.0721 9.4e-01
teen                        0.286282     1.331  0.34807  0.8225 4.1e-01
project$DSDown syndrome     1.193727     3.299  0.27227  4.3844 1.2e-05
project$pcsborn            -0.704743     0.494  0.75943 -0.9280 3.5e-01
project$disadv             -0.000573     0.999  0.00250 -0.2296 8.2e-01
project$ecores             -0.001588     0.998  0.00249 -0.6392 5.2e-01
project$edoccu              0.000590     1.001  0.00193  0.3054 7.6e-01

Likelihood ratio test=102  on 22 df, p=2.87e-12  n=2126 (1396 
observations deleted due to missing)
 >
On 16/10/2004, at 6:32 PM, Mayeul KAUFFMANN wrote:

> "Hi,
> Can anybody tell me what the message below means and how to overcome 
> it.
> Thanks,
> Neil
> Warning message:
> X matrix deemed to be singular; variable 2 in: coxph(Surv(age_at_death,
> death) ~ project$pluralgp + project$yrborn +  ........."
>
> Your 2nd covariate (yrborn) is colinear to the other covariates (or 
> nearly
> colinear). R drops it, but warns you. The results are OK.
>
> Mayeul KAUFFMANN
> Universit?? Pierre Mend??s France
> Grenoble
> France
>
>



From phgrosjean at sciviews.org  Sat Oct 16 13:18:48 2004
From: phgrosjean at sciviews.org (Philippe Grosjean)
Date: Sat, 16 Oct 2004 13:18:48 +0200
Subject: [R] Testing for normality of residuals in a regression model
In-Reply-To: <41710699.5000900@pigrecodata.net>
Message-ID: <200410161118.i9GBIr33016061@outmx002.isp.belgacom.be>

> Prof Brian Ripley wrote:
> 
> >>However, stats 901 or some such tells you that if the distributions 
> >>have even slightly longer tails than the normal you can get much 
> >>better estimates than OLS, and this happens even before a test of 
> >>normality rejects on a sample size of thousands.
> >>
> >>Robustness of efficiency is much more important than robustness of 
> >>distribution, and I believe robustness concepts should be 
> in stats 101.
> >>(I was teaching them yesterday in the third lecture of a 
> basic course, 
> >>albeit a graduate course.)
> >>    
> >>

Federico Gherardini answered:
> This is a very interesting discussion. So you are basically 
> saying that it's better to use robust regression methods, 
> without having to worry too much about the distribution of 
> residuals, instead of using standard methods and doing a lot 
> of tests to check for normality? Did I get your point?

My feeling is that symmetry is more important than, let's say kurtosis <> 0
in the error. Is this correct? Now the problem is: the lower number of
observations, the more severe an effect of non-normality (at least,
asymmetry?) could be on the regression AND at the same time, power of tests
to detect non normality drops. So, I can imagine easily situations where
non-normality is not detected, yet asymmetry is such that regression is
significantly biased... It is mainly a question of sample size from this
point of view... But not only:

Andy Liaw wrote:
> Also, I was told by someone very smart that fitting OLS to
> data with heteroscedastic errors can make the residuals look
> `more normal' than they really are...  Don't know how true
> that is, though. 

That very smart person is not me, but it happens that I experimented also a
little bit on this a while ago! Just experiment with artificial data, and
you will see what happens: residuals look often more normal that the error
distribution you introduced in your artificial data... Another consequence,
is a biased estimate of parameters. Indeed, both come together: parameters
are biased in a direction that lowers residuals sum of square, obviously,
but also in some circumstances, in a direction that make residuals looking
more normal... And that is not (how can it be?) taken into account in the
test of normality. That is, I believe, a second reason why non-normality of
error could not be detected, yet it has a major impact on the OLS
regression.

And I am pretty sure there are other reasons, like distribution of error
both in the dependent and in the independent variables, another violation of
the assumptions made for OLS...

Best regards,

Philippe

..............................................<??}))><........
 ) ) ) ) )
( ( ( ( (    Prof. Philippe Grosjean
 ) ) ) ) )
( ( ( ( (    Numerical Ecology of Aquatic Systems
 ) ) ) ) )   Mons-Hainaut University, Pentagone
( ( ( ( (    Academie Universitaire Wallonie-Bruxelles
 ) ) ) ) )   6, av du Champ de Mars, 7000 Mons, Belgium  
( ( ( ( (       
 ) ) ) ) )   phone: + 32.65.37.34.97, fax: + 32.65.37.33.12
( ( ( ( (    email: Philippe.Grosjean at umh.ac.be
 ) ) ) ) )      
( ( ( ( (    web:   http://www.umh.ac.be/~econum
 ) ) ) ) )
..............................................................



From ripley at stats.ox.ac.uk  Sat Oct 16 14:04:34 2004
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Sat, 16 Oct 2004 13:04:34 +0100 (BST)
Subject: [R] Testing for normality of residuals in a regression model
In-Reply-To: <200410161118.i9GBIr33016061@outmx002.isp.belgacom.be>
Message-ID: <Pine.LNX.4.44.0410161220410.7750-100000@gannet.stats>

I am assuming everyone is on R-help and doesn't want two copies so have 
trimmed the Cc: list to R-help.

On Sat, 16 Oct 2004, Philippe Grosjean wrote:

> > Prof Brian Ripley wrote:

[ Other contributions previously excised here without comment. ]

> > >>However, stats 901 or some such tells you that if the distributions 
> > >>have even slightly longer tails than the normal you can get much 
> > >>better estimates than OLS, and this happens even before a test of 
> > >>normality rejects on a sample size of thousands.
> > >>
> > >>Robustness of efficiency is much more important than robustness of 
> > >>distribution, and I believe robustness concepts should be 
> > in stats 101.
> > >>(I was teaching them yesterday in the third lecture of a  basic course, 
> > >>albeit a graduate course.)
> 
> Federico Gherardini answered:
> > This is a very interesting discussion. So you are basically 
> > saying that it's better to use robust regression methods, 
> > without having to worry too much about the distribution of 
> > residuals, instead of using standard methods and doing a lot 
> > of tests to check for normality? Did I get your point?
> 
> My feeling is that symmetry is more important than, let's say kurtosis <> 0
> in the error. Is this correct? Now the problem is: the lower number of
> observations, the more severe an effect of non-normality (at least,
> asymmetry?) could be on the regression AND at the same time, power of tests
> to detect non normality drops. So, I can imagine easily situations where
> non-normality is not detected, yet asymmetry is such that regression is
> significantly biased... 

Before you can even talk about bias you have to agree what it is you are
trying to estimate.  For asymmetric error distributions it is unlikely to
be the population mean, but if it is then least-squares linear regression
is unbiased provided only that the error distribution has a finite first
moment.  (Part of the so-called Gauss-Markov Theorem.  This seems to
suggest that Philippe's `easy imagination' is of impossible things.)

For contaminated normal distributions it is possibly the mean of the
uncontaminated normal component, and the latter seems the commonest aim of
mainstream robust methods, which do often assume symmetry.  (This may not
affect interpretation of coefficients other than the intercept.)  The
(non-linear) robust regression estimators may be biased for the population
mean but have a (much) smaller variability for long-tailed distributions.

There is a lot of careful discussion about this in the statistical
literature, and I don't believe that it is profitable for people to be
discussing this without knowing the literature, and probably not _here_
even then.

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From rpeng at jhsph.edu  Sat Oct 16 14:13:12 2004
From: rpeng at jhsph.edu (Roger D. Peng)
Date: Sat, 16 Oct 2004 08:13:12 -0400
Subject: [R] hashing
In-Reply-To: <000301c4b356$98551a50$8b00a8c0@IBM>
References: <000301c4b356$98551a50$8b00a8c0@IBM>
Message-ID: <41711058.30303@jhsph.edu>

In a sense, yes.  Elements of named vectors can be accessed via their names and 
that (internally) uses hashing, I believe.

-roger

S??ren Merser wrote:
> is hashing implemented in R
> regards s??ren
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
> 

-- 
Roger D. Peng
http://www.biostat.jhsph.edu/~rpeng/



From rpeng at jhsph.edu  Sat Oct 16 14:14:52 2004
From: rpeng at jhsph.edu (Roger D. Peng)
Date: Sat, 16 Oct 2004 08:14:52 -0400
Subject: [R] sapply and loop
In-Reply-To: <BAY22-F16KtKufVma0o00002e37@hotmail.com>
References: <BAY22-F16KtKufVma0o00002e37@hotmail.com>
Message-ID: <417110BC.1090007@jhsph.edu>

You can use system.time() to time your procedure.  There's no guarantee that 
sapply() will be faster than a for() loop, especially if you preallocate the 
matrices.

-roger

Zhen Pang wrote:

> Dear all,
> 
> I am doing 200 times simulation. For each time, I generate a matrix and 
> define some function on this matrix to get a 6 dimension vector as my 
> results.
> 
> As the loop should be slow, I generate 200 matrice first, and save them 
> into a list named ma,
> then I define zz<-sapply(ma, myfunction)
> 
> To my surprise, It almost costs me the same time to get my results if I 
> directly use a loop from 1 to 200. Is it common? Can I improve any further?
> 
> Ps, how to count the exact time to finish my code?
> 
> Thanks.
> 
> Zhen
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
> 

-- 
Roger D. Peng
http://www.biostat.jhsph.edu/~rpeng/



From andy_liaw at merck.com  Sat Oct 16 14:23:54 2004
From: andy_liaw at merck.com (Liaw, Andy)
Date: Sat, 16 Oct 2004 08:23:54 -0400
Subject: [R] sapply and loop
Message-ID: <3A822319EB35174CA3714066D590DCD504AF8561@usrymx25.merck.com>

Without seeing what myfunction is, it's almost impossible to tell.

In addition to system.time(), you might want to profile your code.  e.g.,

Rprof()
zz <- sapply(ma, myfunction)
Rprof(NULL)
summaryRprof()

HTH,
Andy

> From: Zhen Pang
> 
> Dear all,
> 
> I am doing 200 times simulation. For each time, I generate a 
> matrix and 
> define some function on this matrix to get a 6 dimension vector as my 
> results.
> 
> As the loop should be slow, I generate 200 matrice first, and 
> save them into 
> a list named ma,
> then I define zz<-sapply(ma, myfunction)
> 
> To my surprise, It almost costs me the same time to get my 
> results if I 
> directly use a loop from 1 to 200. Is it common? Can I 
> improve any further?
> 
> Ps, how to count the exact time to finish my code?
> 
> Thanks.
> 
> Zhen
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
> 
>



From nusbj at hotmail.com  Sat Oct 16 14:55:19 2004
From: nusbj at hotmail.com (Zhen Pang)
Date: Sat, 16 Oct 2004 20:55:19 +0800
Subject: [R] sapply and loop
Message-ID: <BAY22-F19kNzPZrIkyP00045c7f@hotmail.com>

Below is my code. myfunction is the myfunction I mentioned in my last email.

p0<-.2
rho0<-.2

nl<-200
simu<-200
set.seed(135)
setwd("d:/r")
options(warn=1)
ns<-rep(1,nl)
configuration<-runif(nl)
frequency<-c(0.0046,0.0057,0.0099,0.0139,0.0147,0.0148,0.0225,0.0321,0.0475,0.0766,0.1179,0.1529,0.1605,0.1424,0.0975,0.0542,0.0207,0.0086,0.0030)

for (i in 1:nl)
    {if (configuration[i]<=frequency[1]) {ns[i]<-1
         } else {for (j in 2:length(frequency))
                      {if (sum(frequency[1:(j-1)])<configuration[i] & 
configuration[i]<=sum(frequency[1:j]))       {ns[i]<-j}
                       }
                 }
     }

nu<-unique(ns)
k<-max(nu)
a<-vector(simu,mode="list")

for (si in 1:simu)
{
print ("si")
print (si)
data<-c(0,0,0)
for (iu in 1:length(nu))
   {fr<-length(ns[ns==nu[iu]])
    y<-rep(0,fr)
    for (ju in 1:fr)
       
{y[ju]<-rbinom(1,nu[iu],rbeta(1,(1-rho0)*p0/rho0,(1-rho0)*(1-p0)/rho0))
        }
    yu<-sort(unique(y))
    yy<-rep(0,length(yu))
    for (ku in 1:length(yu))
       {yy[ku]<-length(y[y==yu[ku]])
        }
    ma<-cbind(rep(nu[iu],length(yu)),yu,yy)
    data<-rbind(data,ma)
    }
data<-data[-1,]
a[[si]]<-data
}

myfunction<-function(data)
{
llb<-function(theta)
{
s <- apply(data, 1, function(data) {
n<-data[1]; y<-data[2] ; re<-data[3]
	p<-1/(1+exp(-theta[1]))
        t <- exp(theta[2])
	s <- log(choose(n,y))
	r<-c(0:(n-1))
	s <- s-sum(log(1+r*t))
	if (n-y-1>=0)
	{
	r<-c(0:(n-y-1))
	s <- s+sum(log(1-p+r*t))
	}
        if (y-1>=0)
        {r<-c(0:(y-1))
	 s<- s+sum(log(p+r*t))
	}
s*re
})
-sum(s)
}
est2<-optim(c(log(p0/(1-p0)),log(rho0/(1-rho0))),llb,hessian=T,control = 
list(maxit=5000000))
est2$par
}

zz<-sapply(a,myfunction)


If we move the myfucntion to the for(si in 1:simu) loop, results are the 
same and there are no time spare. Can you improve a little? Thanks.

Zhen



>From: "Liaw, Andy" <andy_liaw at merck.com>
>To: "'Zhen Pang'" <nusbj at hotmail.com>, r-help at stat.math.ethz.ch
>Subject: RE: [R] sapply and loop
>Date: Sat, 16 Oct 2004 08:23:54 -0400
>
>Without seeing what myfunction is, it's almost impossible to tell.
>
>In addition to system.time(), you might want to profile your code.  e.g.,
>
>Rprof()
>zz <- sapply(ma, myfunction)
>Rprof(NULL)
>summaryRprof()
>
>HTH,
>Andy
>
> > From: Zhen Pang
> >
> > Dear all,
> >
> > I am doing 200 times simulation. For each time, I generate a
> > matrix and
> > define some function on this matrix to get a 6 dimension vector as my
> > results.
> >
> > As the loop should be slow, I generate 200 matrice first, and
> > save them into
> > a list named ma,
> > then I define zz<-sapply(ma, myfunction)
> >
> > To my surprise, It almost costs me the same time to get my
> > results if I
> > directly use a loop from 1 to 200. Is it common? Can I
> > improve any further?
> >
> > Ps, how to count the exact time to finish my code?
> >
> > Thanks.
> >
> > Zhen
> >
> > ______________________________________________
> > R-help at stat.math.ethz.ch mailing list
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide!
> > http://www.R-project.org/posting-guide.html
> >
> >
>
>______________________________________________
>R-help at stat.math.ethz.ch mailing list
>https://stat.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide! 
>http://www.R-project.org/posting-guide.html



From ligges at statistik.uni-dortmund.de  Sat Oct 16 15:24:26 2004
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Sat, 16 Oct 2004 15:24:26 +0200
Subject: [R] tree version 1.0-16
In-Reply-To: <16752.60422.885916.797779@gargle.gargle.HOWL>
References: <782D2A81EC812642B857B03B506E0B4404B766@granite.omni.imsweb.com>
	<16752.60422.885916.797779@gargle.gargle.HOWL>
Message-ID: <4171210A.8010403@statistik.uni-dortmund.de>

Martin Maechler wrote:
>>>>>>"ZKala" == Kalaylioglu, Zeynep (IMS) <KalayliogluZ at imsweb.com>
>>>>>>    on Fri, 15 Oct 2004 15:52:11 -0400 writes:
> 
> 
>     ZKala> Hi There is something weird going on with the "tree"
>     ZKala> package version 1.0-16.  So I want to download an
>     ZKala> updated tree package. I found that R website has
>     ZKala> version 1.0-18. I want to try this one and see how
>     ZKala> the tree algorithm is performing.  What is the best
>     ZKala> way to download tree package version 1.0-18 which can
>     ZKala> be found in index of src/contrib at
>     ZKala> www.r-project.org?  So far I tried two different ways
>     ZKala> none of which has worked for me: 1) I tried to
>     ZKala> download it by running the command
>     ZKala> install.packages("tree","C:\Program
>     ZKala> Files\R\rw1081\library","http://cran.r-project.org/src/contrib",destdir=
>     ZKala> "C:\Program
>     ZKala> <http://cran.r-project.org/src/contrib",destdir="C:/Program>
>     ZKala> Files\R\rw1081\library",installWithVers=TRUE) But it
>     ZKala> tries to connect to
>     ZKala> http://cran.r-project.org/src/contrib/bin/windows/contrib/1.8/PACKAGES
>     ZKala> <http://cran.r-project.org/src/contrib/bin/windows/contrib/1.8/PACKAGES>
>     ZKala> instead of the URL I enter above.
> 
> yes, I see  that your R version is 'R (for windows) 1.8.1'
> and that's probably why it tries to get package for that
> version.
> 
> If you want to upgrade packages, I strongly recommend to first
> upgrade R to the current version 2.0.0 (or maybe "2.0.0 patched")
> -- and then follow the advice from James MacDonald.

Yes!

Let me add, that if you are going to stay with R-1.8.1 for some 
unbelievable reason, you can compile from sources (the one that ends 
with .tar.gz) yourself (follow .../src/gnuwin32/readme.packages). That 
might be much harder than upgrading, though - and we cannot promise that 
a recent package version works under outdated versions of R.

Uwe Ligges


> Martin Maechler, ETH Zurich
>  
>     ZKala> 2) Also I tried to download it by copying the zip
>     ZKala> file first tree_1.0-18.tar.gz
>     ZKala> <http://cran.r-project.org/src/contrib/tree_1.0-18.tar.gz>
>     ZKala> in my directory and then using the "Packages->Install
>     ZKala> package from local -zip files" in the R menu.
>  
>     ZKala> How can I download this version of tree? Thanks.
>  
>     ZKala> Zeynep
> 
>     ZKala> 	[[alternative HTML version deleted]]
> 
>     ZKala> ______________________________________________
>     ZKala> R-help at stat.math.ethz.ch mailing list
>     ZKala> https://stat.ethz.ch/mailman/listinfo/r-help PLEASE
>     ZKala> do read the posting guide!
>     ZKala> http://www.R-project.org/posting-guide.html
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html



From ligges at statistik.uni-dortmund.de  Sat Oct 16 15:38:21 2004
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Sat, 16 Oct 2004 15:38:21 +0200
Subject: [R] barplot
In-Reply-To: <3908.1097920271@www33.gmx.net>
References: <3908.1097920271@www33.gmx.net>
Message-ID: <4171244D.7010005@statistik.uni-dortmund.de>

Sepp Gurgel wrote:

> I have a table 

Do you mean a data.frame?

> with two columns, one with types of blood (A, B, AB or 0) and
> one with the factor (negative = -1 or positive = 1).

And from these you made a table?

   my.table <- table(my.data.frame)


> How can I combine those two columns so that 7 bars are plotted (A, B, AB, 0,
> -A, -B and -0)?

Now you can say

   blood.id <- outer(rownames(my.table), colnames(m.ytable),
                     paste, sep="")
   barplot(as.vector(my.table), names = blood.id)

Uwe Ligges



> --
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html



From phgrosjean at sciviews.org  Sat Oct 16 16:36:33 2004
From: phgrosjean at sciviews.org (Philippe Grosjean)
Date: Sat, 16 Oct 2004 16:36:33 +0200
Subject: [R] Lazy loading... advices
Message-ID: <200410161436.i9GEaeCh011129@outmx002.isp.belgacom.be>

Hello,

I am looking for more information about lazy loading introduced in R 2.0.0.
Doing
?lazyLoad
I got some and there is a 'see also' section that points to
'makeLazyLoading'... But I cannot reach this page.

My problem is: I recompiled a library that uses a lot of functions from
other libraries (of course I can give details if needed). I load it in my
computer: library(svGUI), and it takes something like 20 seconds to load. In
R 1.9.1 it took 3-4 seconds on the same machine (Windows XP). So, I try now
to understand the mechanism and to find a way to lower the loading time of
this library with lazy loading (its goal is to load faster, isn't, so I
probably do something wrong).

Any help or advice would be appreciated.

Here is a Rprof of library(svGUI) on my machine:

?? summaryRprof()
$by.self
                        self.time self.pct total.time total.pct
file.exists                  7.42     24.9       8.46      28.4
list.files                   7.24     24.3       7.32      24.6
file                         6.78     22.8       6.88      23.1
read.dcf                     1.42      4.8       9.24      31.0
file.info                    0.54      1.8       0.82       2.8
lapply                       0.52      1.7       8.96      30.1
inherits                     0.34      1.1      28.76      96.5
names                        0.34      1.1       0.38       1.3
names<-                      0.30      1.0       0.42       1.4
paste                        0.30      1.0       0.66       2.2
close.connection             0.24      0.8       0.24       0.8
.Call                        0.20      0.7       0.20       0.7
apply                        0.20      0.7       0.58       1.9
.find.package                0.18      0.6      18.14      60.9
[... More here]

$by.total
                        total.time total.pct self.time self.pct
library                      29.70      99.7      0.00      0.0
try                          29.64      99.5      0.10      0.3
f                            29.36      98.5      0.00      0.0
firstlib                     29.36      98.5      0.00      0.0
Require                      28.96      97.2      0.00      0.0
match                        28.80      96.6      0.08      0.3
inherits                     28.76      96.5      0.34      1.1
is.factor                    28.76      96.5      0.00      0.0
%in%                         28.60      96.0      0.00      0.0
installed.packages           28.60      96.0      0.00      0.0
unlist                       21.46      72.0      0.08      0.3
packageDescription           21.22      71.2      0.10      0.3
system.file                  19.18      64.4      0.08      0.3
.find.package                18.14      60.9      0.18      0.6
guiInstall                   11.82      39.7      0.00      0.0
read.dcf                      9.24      31.0      1.42      4.8
lapply                        8.96      30.1      0.52      1.7
file.exists                   8.46      28.4      7.42     24.9
FUN                           7.82      26.2      0.02      0.1
list.files                    7.32      24.6      7.24     24.3
.packages                     7.20      24.2      0.02      0.1
file                          6.88      23.1      6.78     22.8
require                       3.42      11.5      0.00      0.0
[... More here]

This is the description of my package (in the bundle SciViews):

Package: svGUI
Title: SciViews GUI API - Main GUI features
Description: Functions to communicate with a GUI client, to implement an
object browser, etc...
Bundle: SciViews
Version: 0.7-0
Date: 2004-10-10
Depends: utils, grDevices, graphics, stats, methods, tcltk, R2HTML, svMisc
Suggests: Hmisc, MASS, wxPython
Author: Philippe Grosjean & Eric Lecoutre
Maintainer: Philippe Grosjean <phgrosjean at sciviews.org>
BundleDescription: SciViews GUI API
  A series of packages to implement a full reusable GUI API for R.
License: GPL 2 or above
URL: http://www.sciviews.org/SciViews-R

Thank you.
Best,

Philippe Grosjean

..............................................<??}))><........
 ) ) ) ) )
( ( ( ( (    Prof. Philippe Grosjean
 ) ) ) ) )
( ( ( ( (    Numerical Ecology of Aquatic Systems
 ) ) ) ) )   Mons-Hainaut University, Pentagone
( ( ( ( (    Academie Universitaire Wallonie-Bruxelles
 ) ) ) ) )   6, av du Champ de Mars, 7000 Mons, Belgium  
( ( ( ( (       
 ) ) ) ) )   phone: + 32.65.37.34.97, fax: + 32.65.37.33.12
( ( ( ( (    email: Philippe.Grosjean at umh.ac.be
 ) ) ) ) )      
( ( ( ( (    web:   http://www.umh.ac.be/~econum
 ) ) ) ) )
..............................................................



From faberfedor at gmail.com  Sat Oct 16 18:00:13 2004
From: faberfedor at gmail.com (Faber Fedor)
Date: Sat, 16 Oct 2004 12:00:13 -0400
Subject: [R] moving functions between namespaces
Message-ID: <300ccfa50410160900643f399d@mail.gmail.com>

Hi all,

I'm a newbie wrt R but that's okay, since I don't do any programming
in R.  What I do need to do is to administrate the activites of an R
programmer, so my questions will be related to adminning R as opposed
to statistcal programming.  I hope that's okay with y'all. :-)

I've been through most of the docs that I could find (the PDFs,
StatsRUs, etc.) and I've found the answers to most of my questions,
but not to an important one: how do I move a function from one
namespace to another.

On the chance that I'm not using the right terminlogoy, here's my
situation: Programmer A has written a function foobar() which resides
in his namespace /home/progA/.RData.  I want to move foobar(), and
only foobar(), into the QA enviroment which means replicating foobar()
inside of /home/qa/.RData.

I've found that I can use sink() to copy foobar() to a file but 1) I
haven't found how to read it back in and 2) I figure you guys have a
more elegant way of doing this.

Any advice?



From bates at stat.wisc.edu  Sat Oct 16 18:15:02 2004
From: bates at stat.wisc.edu (Douglas Bates)
Date: Sat, 16 Oct 2004 11:15:02 -0500
Subject: [R] Lazy loading... advices
In-Reply-To: <200410161436.i9GEaeCh011129@outmx002.isp.belgacom.be>
References: <200410161436.i9GEaeCh011129@outmx002.isp.belgacom.be>
Message-ID: <41714906.3030608@stat.wisc.edu>

Philippe Grosjean wrote:
> Hello,
> 
> I am looking for more information about lazy loading introduced in R 2.0.0.
> Doing
> ?lazyLoad
> I got some and there is a 'see also' section that points to
> 'makeLazyLoading'... But I cannot reach this page.
> 
> My problem is: I recompiled a library that uses a lot of functions from
> other libraries (of course I can give details if needed). I load it in my
> computer: library(svGUI), and it takes something like 20 seconds to load. In
> R 1.9.1 it took 3-4 seconds on the same machine (Windows XP). So, I try now
> to understand the mechanism and to find a way to lower the loading time of
> this library with lazy loading (its goal is to load faster, isn't, so I
> probably do something wrong).
> 
> Any help or advice would be appreciated.
> 

Have you read the article about lazyLoad mechanism that Brian Ripley 
wrote for the latest R News issue?



From ripley at stats.ox.ac.uk  Sat Oct 16 18:15:34 2004
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Sat, 16 Oct 2004 17:15:34 +0100 (BST)
Subject: [R] moving functions between namespaces
In-Reply-To: <300ccfa50410160900643f399d@mail.gmail.com>
Message-ID: <Pine.LNX.4.44.0410161712070.24441-100000@gannet.stats>

On Sat, 16 Oct 2004, Faber Fedor wrote:

> Hi all,
> 
> I'm a newbie wrt R but that's okay, since I don't do any programming
> in R.  What I do need to do is to administrate the activites of an R
> programmer, so my questions will be related to adminning R as opposed
> to statistcal programming.  I hope that's okay with y'all. :-)
> 
> I've been through most of the docs that I could find (the PDFs,
> StatsRUs, etc.) and I've found the answers to most of my questions,
> but not to an important one: how do I move a function from one
> namespace to another.
> 
> On the chance that I'm not using the right terminlogoy, here's my

I think you mean `workspace', not `namespace', so the explanation was very 
helpful.

> situation: Programmer A has written a function foobar() which resides
> in his namespace /home/progA/.RData.  I want to move foobar(), and
> only foobar(), into the QA enviroment which means replicating foobar()
> inside of /home/qa/.RData.

> I've found that I can use sink() to copy foobar() to a file but 1) I
> haven't found how to read it back in and 2) I figure you guys have a
> more elegant way of doing this.

In /home/progA start R and run save(foobar, file="foobar.rda"). End.

In /home/qa start R and run load("/home/progA/foobar.rda").

If this is more than once off, your programmer should write a package 
containing the useful functions: see `Writing R Extensions'.

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From bates at stat.wisc.edu  Sat Oct 16 18:22:59 2004
From: bates at stat.wisc.edu (Douglas Bates)
Date: Sat, 16 Oct 2004 11:22:59 -0500
Subject: [R] moving functions between namespaces
In-Reply-To: <300ccfa50410160900643f399d@mail.gmail.com>
References: <300ccfa50410160900643f399d@mail.gmail.com>
Message-ID: <41714AE3.3030106@stat.wisc.edu>

Faber Fedor wrote:
> Hi all,
> 
> I'm a newbie wrt R but that's okay, since I don't do any programming
> in R.  What I do need to do is to administrate the activites of an R
> programmer, so my questions will be related to adminning R as opposed
> to statistcal programming.  I hope that's okay with y'all. :-)
> 
> I've been through most of the docs that I could find (the PDFs,
> StatsRUs, etc.) and I've found the answers to most of my questions,
> but not to an important one: how do I move a function from one
> namespace to another.
> 
> On the chance that I'm not using the right terminlogoy, here's my
> situation: Programmer A has written a function foobar() which resides
> in his namespace /home/progA/.RData.  I want to move foobar(), and
> only foobar(), into the QA enviroment which means replicating foobar()
> inside of /home/qa/.RData.
> 
> I've found that I can use sink() to copy foobar() to a file but 1) I
> haven't found how to read it back in and 2) I figure you guys have a
> more elegant way of doing this.
> 
> Any advice?

The term "namespace" now means something other than the way you use it 
but that's not important.

The simple way to do this is to have Programmer A start up R in 
/home/progA and save a copy of the function foobar to a file.

   save(foobar, file = "/home/qa/foobar.rda")

(Depending on the size of the file Programmer A may wish to add the 
optional argument compress = TRUE in that call.)

Now start R in /home/qa and load the saved copy

   load("/home/qa/foobar.rda")

then exit R.

I'm tempted to write more about variations on this mechanism but I think 
I'll just stay with the simple answer.



From ligges at statistik.uni-dortmund.de  Sat Oct 16 18:31:29 2004
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Sat, 16 Oct 2004 18:31:29 +0200
Subject: [R] Lazy loading... advices
In-Reply-To: <200410161436.i9GEaeCh011129@outmx002.isp.belgacom.be>
References: <200410161436.i9GEaeCh011129@outmx002.isp.belgacom.be>
Message-ID: <41714CE1.1050907@statistik.uni-dortmund.de>

Philippe Grosjean wrote:

> Hello,
> 
> I am looking for more information about lazy loading introduced in R 2.0.0.
> Doing
> ?lazyLoad
> I got some and there is a 'see also' section that points to
> 'makeLazyLoading'... But I cannot reach this page.
> 
> My problem is: I recompiled a library 

Philippe,

citing Doug Bates: "Someone named Martin Maechler will shortly be 
sending you email regarding the distinction between 'library' and 
'package'". ;-)

[from:
   install.packages("fortunes")
   library(fortunes)
   fortune("library")
]


 > that uses a lot of functions from
> other libraries (of course I can give details if needed). I load it in my
> computer: library(svGUI), and it takes something like 20 seconds to load. In
> R 1.9.1 it took 3-4 seconds on the same machine (Windows XP). So, I try now
> to understand the mechanism and to find a way to lower the loading time of
> this library with lazy loading (its goal is to load faster, isn't, so I
> probably do something wrong).

It might not always be faster. See Brian Ripley's article in the most 
recent R Newsletter. Probably you are using a lot of functions in the 
startup directly after the call to library() (in .First.lib() or 
.onLoad() or Hooks or whatever).
I think you want to specify "LazyLoad: no" in the package's DESCRIPTION 
file (cp. "Writing R Extensions").

Uwe




> Any help or advice would be appreciated.
> 
> Here is a Rprof of library(svGUI) on my machine:
> 
> ?? summaryRprof()
> $by.self
>                         self.time self.pct total.time total.pct
> file.exists                  7.42     24.9       8.46      28.4
> list.files                   7.24     24.3       7.32      24.6
> file                         6.78     22.8       6.88      23.1
> read.dcf                     1.42      4.8       9.24      31.0
> file.info                    0.54      1.8       0.82       2.8
> lapply                       0.52      1.7       8.96      30.1
> inherits                     0.34      1.1      28.76      96.5
> names                        0.34      1.1       0.38       1.3
> names<-                      0.30      1.0       0.42       1.4
> paste                        0.30      1.0       0.66       2.2
> close.connection             0.24      0.8       0.24       0.8
> .Call                        0.20      0.7       0.20       0.7
> apply                        0.20      0.7       0.58       1.9
> .find.package                0.18      0.6      18.14      60.9
> [... More here]
> 
> $by.total
>                         total.time total.pct self.time self.pct
> library                      29.70      99.7      0.00      0.0
> try                          29.64      99.5      0.10      0.3
> f                            29.36      98.5      0.00      0.0
> firstlib                     29.36      98.5      0.00      0.0
> Require                      28.96      97.2      0.00      0.0
> match                        28.80      96.6      0.08      0.3
> inherits                     28.76      96.5      0.34      1.1
> is.factor                    28.76      96.5      0.00      0.0
> %in%                         28.60      96.0      0.00      0.0
> installed.packages           28.60      96.0      0.00      0.0
> unlist                       21.46      72.0      0.08      0.3
> packageDescription           21.22      71.2      0.10      0.3
> system.file                  19.18      64.4      0.08      0.3
> .find.package                18.14      60.9      0.18      0.6
> guiInstall                   11.82      39.7      0.00      0.0
> read.dcf                      9.24      31.0      1.42      4.8
> lapply                        8.96      30.1      0.52      1.7
> file.exists                   8.46      28.4      7.42     24.9
> FUN                           7.82      26.2      0.02      0.1
> list.files                    7.32      24.6      7.24     24.3
> .packages                     7.20      24.2      0.02      0.1
> file                          6.88      23.1      6.78     22.8
> require                       3.42      11.5      0.00      0.0
> [... More here]
> 
> This is the description of my package (in the bundle SciViews):
> 
> Package: svGUI
> Title: SciViews GUI API - Main GUI features
> Description: Functions to communicate with a GUI client, to implement an
> object browser, etc...
> Bundle: SciViews
> Version: 0.7-0
> Date: 2004-10-10
> Depends: utils, grDevices, graphics, stats, methods, tcltk, R2HTML, svMisc
> Suggests: Hmisc, MASS, wxPython
> Author: Philippe Grosjean & Eric Lecoutre
> Maintainer: Philippe Grosjean <phgrosjean at sciviews.org>
> BundleDescription: SciViews GUI API
>   A series of packages to implement a full reusable GUI API for R.
> License: GPL 2 or above
> URL: http://www.sciviews.org/SciViews-R
> 
> Thank you.
> Best,
> 
> Philippe Grosjean
> 
> ..............................................<??}))><........
>  ) ) ) ) )
> ( ( ( ( (    Prof. Philippe Grosjean
>  ) ) ) ) )
> ( ( ( ( (    Numerical Ecology of Aquatic Systems
>  ) ) ) ) )   Mons-Hainaut University, Pentagone
> ( ( ( ( (    Academie Universitaire Wallonie-Bruxelles
>  ) ) ) ) )   6, av du Champ de Mars, 7000 Mons, Belgium  
> ( ( ( ( (       
>  ) ) ) ) )   phone: + 32.65.37.34.97, fax: + 32.65.37.33.12
> ( ( ( ( (    email: Philippe.Grosjean at umh.ac.be
>  ) ) ) ) )      
> ( ( ( ( (    web:   http://www.umh.ac.be/~econum
>  ) ) ) ) )
> ..............................................................
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html



From cstrato at aon.at  Sat Oct 16 18:21:31 2004
From: cstrato at aon.at (cstrato)
Date: Sat, 16 Oct 2004 18:21:31 +0200
Subject: [R] 20,000 * 6 data values
In-Reply-To: <032201c4b337$ff01ae90$23719792@star>
References: <3A822319EB35174CA3714066D590DCD504AF854F@usrymx25.merck.com>	<41701472.1090602@pdf.com>
	<032201c4b337$ff01ae90$23719792@star>
Message-ID: <41714A8B.7040500@aon.at>

R and especially Bioconductor are the "Gold Standard" for
microarry analysis, see: http://www.bioconductor.org/

Regards
Christian

Sun wrote:
> Hello, Rusers:
> 
> What is the maximum number of data R can handle? Or I have to use SAS? I am
> trying to do some microarray data analysis. But I am totally new. Did anyone
> use R to do microarray analysis?
> 
> Many thanks,
> 
> Sun
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
> 
>



From ggrothendieck at myway.com  Sat Oct 16 19:16:48 2004
From: ggrothendieck at myway.com (Gabor Grothendieck)
Date: Sat, 16 Oct 2004 13:16:48 -0400 (EDT)
Subject: [R] power in a specific frequency band
Message-ID: <20041016171648.538DA39A7@mprdmxin.myway.com>



Unfortunately the documentation seems somewhat sparse 
but you can figure out what it is doing by looking at the code 
of spec.pgram or just pumping a sine wave through it and 
comparing that to a manually generated periodogram, which 
we do below:

> tt <- 1:32 # time
> x <- sin(2*pi*tt/8) # sine save with period 8, i.e. freq = .125
> x <- x - mean(x)
> 
> # Calculate the periodogram manually using the fft
> round(Mod(fft(x))^2/32, 3)
 [1] 0 0 0 0 8 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 8 0 0 0
> 
> # Now recalculate using spectrum
> sp <- spectrum(x, taper = 0, detrend = FALSE)
> 
> # Note the power concentrated at sp$freq[4]=.125 in sp$spec[4].
> # Note that the fft version returns the mean in element 1 so element 2
> # of the fft version corresponds to element 1 of the spectrum version.
> # Also in comparing we see that the spectrum version omits the symmetric 
> # completion of the first half.
> round(sp$spec,2)
 [1] 0 0 0 8 0 0 0 0 0 0 0 0 0 0 0 0
> sp$freq
 [1] 0.03125 0.06250 0.09375 0.12500 0.15625 0.18750 0.21875 0.25000 0.28125
[10] 0.31250 0.34375 0.37500 0.40625 0.43750 0.46875 0.50000
> 1/sp$freq
 [1] 32.000000 16.000000 10.666667  8.000000  6.400000  5.333333  4.571429
 [8]  4.000000  3.555556  3.200000  2.909091  2.666667  2.461538  2.285714
[15]  2.133333  2.000000
> 
> # sum of squares vs. sum of periodogram values.
> # Note that power defined as sum of squares of demeaned series
> # equals twice sum of periodogram values returned by spectrum 
> # (since spec.pgram does not return the half that is the same 
> # by symmetry)
> sum(x*x)
[1] 16
> sum(sp$spec)
[1] 8



Date:   Fri, 15 Oct 2004 19:55:40 -0400 
From:   Giuseppe Pagnoni <gpagnon at emory.edu>
To:   <r-help at stat.math.ethz.ch> 
Subject:   [R] power in a specific frequency band 

 
Dear R users

I have a really simple question (hoping for a really simple answer :-):

Having estimated the spectral density of a time series "x" (heart rate 
data) with:

x.pgram <- spectrum(x,method="pgram")

I would like to compute the power in a specific energy band.

Assuming that frequency(x)=4 (Hz), and that I am interested in the band 
between f1 and f2, is the power in the band simply the following?

sum(x.pgram$spec[(x.pgram$freq > f1/frequency(x)) & (x.pgram$freq <= 
f2/frequency(x))])

If it is so, are the returned units the same units as the original time 
series, but squared (if x is bpm, then the power is in bpm^2)?

I own a copy of Venables and Ripley (MASS 2003), but I was not able to 
extract this information from the time series chapter....


thanks for any help
(please cc to my e-mail, if possible)


giuseppe





Giuseppe Pagnoni
Dept. Psychiatry and Behavioral Sciences
Emory University
1639 Pierce Drive, Suite 4000
Atlanta, GA, 30322
tel: 404.712.8431
fax: 404.727.3233



From ps at ph.ed.ac.uk  Sat Oct 16 20:19:42 2004
From: ps at ph.ed.ac.uk (Paul Stansell)
Date: Sat, 16 Oct 2004 19:19:42 +0100 (BST)
Subject: [R] problem with axis labels
Message-ID: <Pine.LNX.4.58.0410161910440.11233@mills.ph.ed.ac.uk>


Dear R users,

Below are some R commands which produce a y-axis label that is not wholly
in the viewing area of the eps file (or the x11 window).

I have tried experimenting with the postscript bounding box, and using
such R commands as over(phantom(0),...) but I make the whole y-label 
visible.

Does anyone have any suggestions for how I may fix this.  (Please CC
mailing list email to p.stansell at ed.ac.uk.)

Thanks,

Paul Stansell


label<-expression(italic(A)==union(italic(H)[italic(i)],i==1,4))
postscript("labelBug.eps",onefile=F,height=5,width=5,pointsize=12)
plot(1,1,xlab=label,ylab=label)
dev.off()



From assuncao.senra at portugalmail.com  Sat Oct 16 20:25:34 2004
From: assuncao.senra at portugalmail.com (assuncao.senra@portugalmail.com)
Date: Sat, 16 Oct 2004 19:25:34 +0100
Subject: [R] returning parameter values by CoCo 
Message-ID: <1097951134.4171679ec69e0@webmail3.portugalmail.pt>


Hello,
as anyone used CoCo?
I used 'backward' function to find a loglinear model for my data, wich I did. 
Now I??m trying to get the estimates of the parameters of the model. I 
tried 'returnExpression' but I can??t understand the output. Besides the same 
model I get the value TRUE.
Can someone explain me?
Thanks in advance.
Assun????o

__________________________________________________________



From maechler at stat.math.ethz.ch  Sat Oct 16 21:17:22 2004
From: maechler at stat.math.ethz.ch (Martin Maechler)
Date: Sat, 16 Oct 2004 21:17:22 +0200
Subject: [R] Lazy loading... advices
In-Reply-To: <41714CE1.1050907@statistik.uni-dortmund.de>
References: <200410161436.i9GEaeCh011129@outmx002.isp.belgacom.be>
	<41714CE1.1050907@statistik.uni-dortmund.de>
Message-ID: <16753.29634.146059.39284@gargle.gargle.HOWL>

>>>>> "UweL" == Uwe Ligges <ligges at statistik.uni-dortmund.de>
>>>>>     on Sat, 16 Oct 2004 18:31:29 +0200 writes:

    UweL> Philippe Grosjean wrote:
    >> Hello,
    >> 
    >> I am looking for more information about lazy loading
    >> introduced in R 2.0.0.  Doing ?lazyLoad I got some and
    >> there is a 'see also' section that points to
    >> 'makeLazyLoading'... But I cannot reach this page.
    >> 
    >> My problem is: I recompiled a library

    UweL> Philippe,

    UweL> citing Doug Bates: "Someone named Martin Maechler will
    UweL> shortly be sending you email regarding the distinction
    UweL> between 'library' and 'package'". ;-)

of course, now I have to .... 

Note that library(<package>) attaches and loads a *package* from
one of possibly several libraries { = the directories listed,
e.g., by .libPaths() !} 

    >> that uses a lot of functions from other libraries (of
    >> course I can give details if needed). I load it in my
    >> computer: library(svGUI), and it takes something like 20
    >> seconds to load. In R 1.9.1 it took 3-4 seconds on the
    >> same machine (Windows XP). So, I try now to understand
    >> the mechanism and to find a way to lower the loading time
    >> of this library with lazy loading (its goal is to load
    >> faster, isn't, so I probably do something wrong).

    UweL> It might not always be faster. See Brian Ripley's
    UweL> article in the most recent R Newsletter. Probably you
    UweL> are using a lot of functions in the startup directly
    UweL> after the call to library() (in .First.lib() or
    UweL> .onLoad() or Hooks or whatever).  I think you want to
    UweL> specify "LazyLoad: no" in the package's DESCRIPTION
    UweL> file (cp. "Writing R Extensions").

I'm pretty sure the increased startup time comes from the very
long 'Depends:' field in Philippe's package.
I'd bet that -- after reading Brian's article -- Philippe will
find that he can make the 'Depends' much smaller, e.g., by
moving entries to 'Suggests'.

Martin



From maechler at stat.math.ethz.ch  Sat Oct 16 21:47:18 2004
From: maechler at stat.math.ethz.ch (Martin Maechler)
Date: Sat, 16 Oct 2004 21:47:18 +0200
Subject: [R] problem with axis labels
In-Reply-To: <Pine.LNX.4.58.0410161910440.11233@mills.ph.ed.ac.uk>
References: <Pine.LNX.4.58.0410161910440.11233@mills.ph.ed.ac.uk>
Message-ID: <16753.31430.785104.754747@gargle.gargle.HOWL>

>>>>> "Paul" == Paul Stansell <ps at ph.ed.ac.uk>
>>>>>     on Sat, 16 Oct 2004 19:19:42 +0100 (BST) writes:

    Paul> Dear R users,

    Paul> Below are some R commands which produce a y-axis label
    Paul> that is not wholly in the viewing area of the eps file
    Paul> (or the x11 window).

yes, that has almost nothing to do with the specific device

  > label<-expression(italic(A)==union(italic(H)[italic(i)],i==1,4))
  > postscript("labelBug.eps",onefile=F,height=5,width=5,pointsize=12)
  > plot(1,1,xlab=label,ylab=label) 
  > dev.off()

    Paul> I have tried experimenting with the postscript
    Paul> bounding box, and using such R commands as
    Paul> over(phantom(0),...) but I make the whole y-label
    Paul> visible.

    Paul> Does anyone have any suggestions for how I may fix this. 

yes. You have to use a larger "left" margin (i.e. margin # 2),
see the 'mar' section in  help(par),
and also re-read the part on graphics in the "Introduction to R"
manual.

The following will look better {independently of the device,
postscript, X11, windows,...}:

  label <- expression(italic(A) == union(italic(H)[italic(i)],i==1,4))
  op <- par(mar = .1 + c(5,5,4,1))
  plot(1,1, xlab=label, ylab=label) 
  par(op)# to reset the graphics settings to the defaults

Martin



From Gregor.Gorjanc at bfro.uni-lj.si  Sun Oct 17 02:51:58 2004
From: Gregor.Gorjanc at bfro.uni-lj.si (Gorjanc Gregor)
Date: Sun, 17 Oct 2004 02:51:58 +0200
Subject: [R] Plotcorr: colour the ellipses to emphasize the differences 
Message-ID: <7FFEE688B57D7346BC6241C55900E7300FCF17@pollux.bfro.uni-lj.si>

Hello R users!

I began with R and I must say that it is really nice. I have data with a lot of variables
and have a problem to extract the pattern from correlation matrix. So I tried with plotcorr
and it went fine. While I was reading the help page of this function, I found that ellipse
display can be even better with use of different colors (the code is bellow). However I have
a problem to understand the process of generating the colors. The function cm.colors() with argument 11 produces scale of colors with 11 points. What is the meaning of [5*xc + 6])? If I ommit this part from the code, I see that ellipses bellow diagonal do not have the same color as above the diagonal. In given example numbers 5 and 6 are given (I think so) since there are 11 variables in dataset mtcars. 

How can one use this setup for other datasets? For example I have a dataset with 15 variables.

I would also like to know if it is possible to use some other scale of colors instead of cm.colors, rainbow, heat.colors, terrain.colors, topo.colors. I would like to have positive correlations in blue and nagative ones in red spectrum of colors. Is it possible?

Thank you!

# Colour the ellipses to emphasize the differences
corr.mtcars <- cor(mtcars)
ord <- order(corr.mtcars[1,])
xc <- corr.mtcars[ord, ord]
plotcorr( xc, col=cm.colors(11)[5*xc + 6])


With regards, Lep pozdrav
    Gregor GORJANC



From sun at cae.wisc.edu  Sun Oct 17 04:59:02 2004
From: sun at cae.wisc.edu (Sun)
Date: Sat, 16 Oct 2004 21:59:02 -0500
Subject: [R] how to draw a multivariate function
Message-ID: <002b01c4b3f5$41a37f90$23719792@star>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20041016/5e3c804d/attachment.pl

From spencer.graves at pdf.com  Sun Oct 17 06:06:39 2004
From: spencer.graves at pdf.com (Spencer Graves)
Date: Sat, 16 Oct 2004 21:06:39 -0700
Subject: [R] how to draw a multivariate function
In-Reply-To: <002b01c4b3f5$41a37f90$23719792@star>
References: <002b01c4b3f5$41a37f90$23719792@star>
Message-ID: <4171EFCF.4020700@pdf.com>

      Have you studied the documentation for "wireframe" including the 
examples?  See also the contour plot examples in Venables and Ripley 
(2002) Modern Applied Statistics with S, 4th ed. (Springer). 

      It might help if you write your function as follows: 

f <- function(x,y ,n, pa, pb) (factorial(n)/
     (factorial(x) * factorial(y) * factorial(n-x-y))*
                      pa^x * pb^y * ((1-pa-pb)^(n-x-y)))

      I hope this fills in enough gaps that you will be able to complete 
the remaining steps. 

      Good luck.  spencer graves

Sun wrote:

>Hi, Rusers:
>
>Thanks for answering my last questions. I am frustrated in plotting a trinomial pmf function 
>
>f(x,y | n, pa, pb) = factorial(n)/ (factorial(x) * factorial(y) * factorial (n-x-y))* pa^x * pb^y * ((1-pa-pb)^(n-x-y))
>
>obviously it is a bivariate function of x and y. But I have put a lot of time on this.
>
>**********************************
>x <- seq(0, n, len = n/2+1) # for now I set it to n/2 to control x+y <= n
>y <- seq(0, n, len = n/2+1)
>f = factorial(n)/ (factorial(x) * factorial(y) * factorial (n-x-y))* pa^x * pb^y * ((1-pa-pb)^(n-x-y))
>wireframe(f ~ x * y, shade = TRUE)
>**********************************
>
>well, but it plots nothing out.
>
>I wonder if you could help me? Seems R is hard to learn without your help.
>
>Many thanks,
>
>Sun
>	[[alternative HTML version deleted]]
>
>______________________________________________
>R-help at stat.math.ethz.ch mailing list
>https://stat.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>  
>

-- 
Spencer Graves, PhD, Senior Development Engineer
O:  (408)938-4420;  mobile:  (408)655-4567



From sun at cae.wisc.edu  Sun Oct 17 06:12:28 2004
From: sun at cae.wisc.edu (Sun)
Date: Sat, 16 Oct 2004 23:12:28 -0500
Subject: [R] how to draw a multivariate function
References: <86EED55AE819274B8308B62A225457D8CEC262@exch1.qsa.local>
Message-ID: <003f01c4b3ff$83e71b50$23719792@star>

Hi, All:

Thanks. Here is the code

n = 30
lamdaa = 4
lamdab = 1.5

pa = lamdaa/n
pb = lamdab/n

x <- seq(0, n/2, len = n/2+1)
y <- seq(0, n/2, len = n/2+1)
f = factorial(n)/ (factorial(x) * factorial(y) * factorial (n-x-y))* pa^x *
pb^y * ((1-pa-pb)^(n-x-y))
wireframe(f ~ x * y, shade = TRUE)

The above cannot show anything.
Just le t you know that now I changed to cloud, it can display something :)
cloud(f ~ x * y, shade = TRUE)

I have questions:

1.
what does x*y mean here? I don't think it is a vector dot multiplication. I
guess it will creat all rows of x and y for all possible combinations? Why
wireframe cannot show here?

2.
How to show the value on the cloud plot? I have no idea of how much the data
value is from the plot.

3. Where can I get resources of R? The help file seems not very helpful to
me. For example, the lm () function, its weighted least square option does
not say clearly the weight = standard deviation. It said it is to minimize
sum w*error^2, which mislead us to think it takes variance. I have to ask
experienced people. And everytime the answer depends on luck.

Thanks,

----- Original Message ----- 
From: "Andrew Ward" <Andrew.Ward at qsa.qld.edu.au>
To: "Sun" <sun at cae.wisc.edu>
Sent: Saturday, October 16, 2004 10:15 PM
Subject: RE: [R] how to draw a multivariate function


> Dear Sun,
>
> Could you please provide an example that can be run
> by readers of the list? What you've given is
> missing at least n and pa.
>
> Regards,
>
> Andrew C. Ward,                andrew.ward at qsa.qld.edu.au
> Senior Analyst (Quantitative), Tel: +61 7 3864 0439
> Queensland Studies Authority,  Fax: +61 7 3229 3318
> 295 Ann Street,
> Brisbane Qld 4000, Australia
>
> -----Original Message-----
> From: r-help-bounces at stat.math.ethz.ch
> [mailto:r-help-bounces at stat.math.ethz.ch]On Behalf Of Sun
> Sent: Sunday, 17 October 2004 12:59 PM
> To: R-help mailing list
> Subject: [R] how to draw a multivariate function
>
>
> Hi, Rusers:
>
> Thanks for answering my last questions. I am frustrated in plotting a
trinomial pmf function
>
> f(x,y | n, pa, pb) = factorial(n)/ (factorial(x) * factorial(y) *
factorial (n-x-y))* pa^x * pb^y * ((1-pa-pb)^(n-x-y))
>
> obviously it is a bivariate function of x and y. But I have put a lot of
time on this.
>
> **********************************
> x <- seq(0, n, len = n/2+1) # for now I set it to n/2 to control x+y <= n
> y <- seq(0, n, len = n/2+1)
> f = factorial(n)/ (factorial(x) * factorial(y) * factorial (n-x-y))* pa^x
* pb^y * ((1-pa-pb)^(n-x-y))
> wireframe(f ~ x * y, shade = TRUE)
> **********************************
>
> well, but it plots nothing out.
>
> I wonder if you could help me? Seems R is hard to learn without your help.
>
> Many thanks,
>
> Sun
> [[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide!
http://www.R-project.org/posting-guide.html
> ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
> This email (including any attached files) is for the intended
> recipient(s) only. If you received this email by mistake, please,
> as a courtesy, tell the sender, then delete this email.
>
> The views and opinions are the originator's and do not necessarily
> reflect those of the Queensland Studies Authority. All reasonable
> precautions have been taken to ensure that this email contained no
> viruses at the time it was sent.
>
>
>



From sun at cae.wisc.edu  Sun Oct 17 06:31:26 2004
From: sun at cae.wisc.edu (Sun)
Date: Sat, 16 Oct 2004 23:31:26 -0500
Subject: [R] how to draw a multivariate function
References: <002b01c4b3f5$41a37f90$23719792@star> <4171EFCF.4020700@pdf.com>
Message-ID: <005a01c4b402$2a210d80$23719792@star>

Thanks, Spencer. Turing it into a function seems really to make difference.

Well but I don't know what is the difference between

data1 = cbind (x, y, f(x,y))
cloud(data1)

and

cloud(f(x,y) ~ x *y)?

They draw different plots. I don't know which one is correct. I am really
confused.

Here are codes:
n = 30
lamdaa = 4
lamdab = 1.5

pa = lamdaa/n
pb = lamdab/n

x <- seq(0, n/2, len = n/2+1)
y <- seq(0, n/2, len = n/2+1)
f <- function(x,y) (factorial(n)/
     (factorial(x) * factorial(y) * factorial(n-x-y))*
                      pa^x * pb^y * ((1-pa-pb)^(n-x-y)))

data1 = cbind (x, y, f(x,y))
cloud(data1)

cloud(f(x,y) ~ x *y)

------------------------

Thanks a lot
----- Original Message ----- 
From: "Spencer Graves" <spencer.graves at pdf.com>
To: "Sun" <sun at cae.wisc.edu>
Cc: "R-help mailing list" <R-help at stat.math.ethz.ch>
Sent: Saturday, October 16, 2004 11:06 PM
Subject: Re: [R] how to draw a multivariate function


>       Have you studied the documentation for "wireframe" including the
> examples?  See also the contour plot examples in Venables and Ripley
> (2002) Modern Applied Statistics with S, 4th ed. (Springer).
>
>       It might help if you write your function as follows:
>
> f <- function(x,y ,n, pa, pb) (factorial(n)/
>      (factorial(x) * factorial(y) * factorial(n-x-y))*
>                       pa^x * pb^y * ((1-pa-pb)^(n-x-y)))
>
>       I hope this fills in enough gaps that you will be able to complete
> the remaining steps.
>
>       Good luck.  spencer graves
>
> Sun wrote:
>
> >Hi, Rusers:
> >
> >Thanks for answering my last questions. I am frustrated in plotting a
trinomial pmf function
> >
> >f(x,y | n, pa, pb) = factorial(n)/ (factorial(x) * factorial(y) *
factorial (n-x-y))* pa^x * pb^y * ((1-pa-pb)^(n-x-y))
> >
> >obviously it is a bivariate function of x and y. But I have put a lot of
time on this.
> >
> >**********************************
> >x <- seq(0, n, len = n/2+1) # for now I set it to n/2 to control x+y <= n
> >y <- seq(0, n, len = n/2+1)
> >f = factorial(n)/ (factorial(x) * factorial(y) * factorial (n-x-y))* pa^x
* pb^y * ((1-pa-pb)^(n-x-y))
> >wireframe(f ~ x * y, shade = TRUE)
> >**********************************
> >
> >well, but it plots nothing out.
> >
> >I wonder if you could help me? Seems R is hard to learn without your
help.
> >
> >Many thanks,
> >
> >Sun
> > [[alternative HTML version deleted]]
> >
> >______________________________________________
> >R-help at stat.math.ethz.ch mailing list
> >https://stat.ethz.ch/mailman/listinfo/r-help
> >PLEASE do read the posting guide!
http://www.R-project.org/posting-guide.html
> >
> >
>
> -- 
> Spencer Graves, PhD, Senior Development Engineer
> O:  (408)938-4420;  mobile:  (408)655-4567
>
>
>



From deepayan at stat.wisc.edu  Sun Oct 17 07:04:57 2004
From: deepayan at stat.wisc.edu (Deepayan Sarkar)
Date: Sun, 17 Oct 2004 00:04:57 -0500
Subject: [R] how to draw a multivariate function
In-Reply-To: <003f01c4b3ff$83e71b50$23719792@star>
References: <86EED55AE819274B8308B62A225457D8CEC262@exch1.qsa.local>
	<003f01c4b3ff$83e71b50$23719792@star>
Message-ID: <200410170004.57932.deepayan@stat.wisc.edu>

On Saturday 16 October 2004 23:12, Sun wrote:
> Hi, All:
>
> Thanks. Here is the code
>
> n = 30
> lamdaa = 4
> lamdab = 1.5
>
> pa = lamdaa/n
> pb = lamdab/n
>
> x <- seq(0, n/2, len = n/2+1)
> y <- seq(0, n/2, len = n/2+1)

Have you looked at what values of x and y these produce? They include 
non-integer values. Are you sure you want that?

> f = factorial(n)/ (factorial(x) * factorial(y) * factorial (n-x-y))*
> pa^x * pb^y * ((1-pa-pb)^(n-x-y))
> wireframe(f ~ x * y, shade = TRUE)
>
> The above cannot show anything.
> Just le t you know that now I changed to cloud, it can display
> something :) cloud(f ~ x * y, shade = TRUE)
>
> I have questions:
>
> 1.
> what does x*y mean here? I don't think it is a vector dot
> multiplication. I guess it will creat all rows of x and y for all
> possible combinations? Why wireframe cannot show here?

Why guess instead of reading the documentation and looking at the 
examples? There's a very relevant example in the help page for 
wireframe.

You clearly want to evaluate 'f' at all combinations of x and y, yet you 
seem to be evaluating it only along the diagonal (x = y). The correct 
way to do this is (as studying the examples should have suggested to 
you):

g <- expand.grid(x = seq(0, n), y = seq(0, n))
g$z <- dtrinom(g$x, g$y)
wireframe(z ~ x * y, data = g, shade = TRUE)

where dtrinom could be defined as 

dtrinom <- function(x, y)
{
    ifelse(x + y > n,
           NA,
           factorial(n)/ (factorial(x) * 
           factorial(y) * factorial (n-x-y))*
           pa^x * pb^y * ((1-pa-pb)^(n-x-y)))
}

although I would suggest working on the log scale for numerical 
stability:

dtrinom <- function(x, y)
{
    ifelse(x + y > n,
           NA,
           exp((lfactorial(n) - lfactorial(x) -
                lfactorial(y) - lfactorial(n-x-y)) + 
               x * log(pa) +  y * log(pb) +
               (n-x-y) * log(1-pa-pb)))
}


> 2.
> How to show the value on the cloud plot? I have no idea of how much
> the data value is from the plot.

Read the documentation for the 'scales' argument.

> 3. Where can I get resources of R? The help file seems not very
> helpful to me. For example, the lm () function, its weighted least
> square option does not say clearly the weight = standard deviation.
> It said it is to minimize sum w*error^2, which mislead us to think it
> takes variance. I have to ask experienced people. And everytime the
> answer depends on luck.

It's too bad you feel that way. Statistics, and in particular linear 
modeling, is a non-trivial subject, and R documentation is not supposed 
to serve as a textbook. If you don't understand what "minimizing 
'sum(w*e^2)'" means, you really do need help from 'experienced people'. 
Alternatively, look at the references listed in the help page for lm.

Hope that helps,

Deepayan


>
> Thanks,
>
> ----- Original Message -----
> From: "Andrew Ward" <Andrew.Ward at qsa.qld.edu.au>
> To: "Sun" <sun at cae.wisc.edu>
> Sent: Saturday, October 16, 2004 10:15 PM
> Subject: RE: [R] how to draw a multivariate function
>
> > Dear Sun,
> >
> > Could you please provide an example that can be run
> > by readers of the list? What you've given is
> > missing at least n and pa.



From sun at cae.wisc.edu  Sun Oct 17 07:27:32 2004
From: sun at cae.wisc.edu (Sun)
Date: Sun, 17 Oct 2004 00:27:32 -0500
Subject: [R] how to draw a multivariate function
References: <86EED55AE819274B8308B62A225457D8CEC262@exch1.qsa.local><003f01c4b3ff$83e71b50$23719792@star>
	<200410170004.57932.deepayan@stat.wisc.edu>
Message-ID: <002301c4b40a$00b5e5d0$23719792@star>

Hi, Thanks. But minimizing
> 'sum(w*e^2)'"

means w is the variance instead of the standard deviation. However, the
truth is that R takes standard deviation. R will square it!

R-help document is not that to be proud of. It is not very clear or helpful
sometimes.


----- Original Message ----- 
From: "Deepayan Sarkar" <deepayan at stat.wisc.edu>
To: <r-help at stat.math.ethz.ch>
Cc: "Sun" <sun at cae.wisc.edu>; "Andrew Ward" <Andrew.Ward at qsa.qld.edu.au>
Sent: Sunday, October 17, 2004 12:04 AM
Subject: Re: [R] how to draw a multivariate function


> On Saturday 16 October 2004 23:12, Sun wrote:
> > Hi, All:
> >
> > Thanks. Here is the code
> >
> > n = 30
> > lamdaa = 4
> > lamdab = 1.5
> >
> > pa = lamdaa/n
> > pb = lamdab/n
> >
> > x <- seq(0, n/2, len = n/2+1)
> > y <- seq(0, n/2, len = n/2+1)
>
> Have you looked at what values of x and y these produce? They include
> non-integer values. Are you sure you want that?
>
> > f = factorial(n)/ (factorial(x) * factorial(y) * factorial (n-x-y))*
> > pa^x * pb^y * ((1-pa-pb)^(n-x-y))
> > wireframe(f ~ x * y, shade = TRUE)
> >
> > The above cannot show anything.
> > Just le t you know that now I changed to cloud, it can display
> > something :) cloud(f ~ x * y, shade = TRUE)
> >
> > I have questions:
> >
> > 1.
> > what does x*y mean here? I don't think it is a vector dot
> > multiplication. I guess it will creat all rows of x and y for all
> > possible combinations? Why wireframe cannot show here?
>
> Why guess instead of reading the documentation and looking at the
> examples? There's a very relevant example in the help page for
> wireframe.
>
> You clearly want to evaluate 'f' at all combinations of x and y, yet you
> seem to be evaluating it only along the diagonal (x = y). The correct
> way to do this is (as studying the examples should have suggested to
> you):
>
> g <- expand.grid(x = seq(0, n), y = seq(0, n))
> g$z <- dtrinom(g$x, g$y)
> wireframe(z ~ x * y, data = g, shade = TRUE)
>
> where dtrinom could be defined as
>
> dtrinom <- function(x, y)
> {
>     ifelse(x + y > n,
>            NA,
>            factorial(n)/ (factorial(x) *
>            factorial(y) * factorial (n-x-y))*
>            pa^x * pb^y * ((1-pa-pb)^(n-x-y)))
> }
>
> although I would suggest working on the log scale for numerical
> stability:
>
> dtrinom <- function(x, y)
> {
>     ifelse(x + y > n,
>            NA,
>            exp((lfactorial(n) - lfactorial(x) -
>                 lfactorial(y) - lfactorial(n-x-y)) +
>                x * log(pa) +  y * log(pb) +
>                (n-x-y) * log(1-pa-pb)))
> }
>
>
> > 2.
> > How to show the value on the cloud plot? I have no idea of how much
> > the data value is from the plot.
>
> Read the documentation for the 'scales' argument.
>
> > 3. Where can I get resources of R? The help file seems not very
> > helpful to me. For example, the lm () function, its weighted least
> > square option does not say clearly the weight = standard deviation.
> > It said it is to minimize sum w*error^2, which mislead us to think it
> > takes variance. I have to ask experienced people. And everytime the
> > answer depends on luck.
>
> It's too bad you feel that way. Statistics, and in particular linear
> modeling, is a non-trivial subject, and R documentation is not supposed
> to serve as a textbook. If you don't understand what "minimizing
> 'sum(w*e^2)'" means, you really do need help from 'experienced people'.
> Alternatively, look at the references listed in the help page for lm.
>
> Hope that helps,
>
> Deepayan
>
>
> >
> > Thanks,
> >
> > ----- Original Message -----
> > From: "Andrew Ward" <Andrew.Ward at qsa.qld.edu.au>
> > To: "Sun" <sun at cae.wisc.edu>
> > Sent: Saturday, October 16, 2004 10:15 PM
> > Subject: RE: [R] how to draw a multivariate function
> >
> > > Dear Sun,
> > >
> > > Could you please provide an example that can be run
> > > by readers of the list? What you've given is
> > > missing at least n and pa.
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide!
http://www.R-project.org/posting-guide.html
>



From ggrothendieck at myway.com  Sun Oct 17 08:19:42 2004
From: ggrothendieck at myway.com (Gabor Grothendieck)
Date: Sun, 17 Oct 2004 06:19:42 +0000 (UTC)
Subject: [R] how to draw a multivariate function
References: <86EED55AE819274B8308B62A225457D8CEC262@exch1.qsa.local>
	<003f01c4b3ff$83e71b50$23719792@star>
Message-ID: <loom.20041017T073218-243@post.gmane.org>

Sun <sun <at> cae.wisc.edu> writes:

> 3. Where can I get resources of R? The help file seems not very helpful to
> me. For example, the lm () function, its weighted least square option does
> not say clearly the weight = standard deviation. It said it is to minimize
> sum w*error^2, which mislead us to think it takes variance. I have to ask
> experienced people. And everytime the answer depends on luck.

1. There are quite a few pointers under Documentation in the left hand
pane of the R web site at www.r-project.org .   

2. If you have something specific like weighted regression you can 
try the Search link at that same site or just use google, e.g. try 
these search words and you will find some examples of using lm with 
weights:
  r-help weight lm

3. Sometimes its easiest to check your understanding by just plugging
in some variables for which you know the answer and see what it does.  
For example,

R> set.seed(1)
R> y <- rnorm(10)
R> w <- runif(10)

R> # Run 1
R> coef(lm(w*y ~ w+0))  # +0 means no intercept
         w 
-0.1768129

R> # Run 2 
R> coef(lm(y~1,weight=w^2)) # ~1 means regression against intercept
(Intercept) 
 -0.1768129

R> # Run 3
R> coef(lm(y~1,weight=w))
(Intercept) 
 -0.1037307 

The first two runs give the same answer so it seems the help
page was correect because the first run finds the b that
minimizes sum((w*y - b*w)^2) and we can factor the w out to give
= sum(w^2 * (y-b)^2) which corresponds to Run 2, not Run 3.



From UYRI24 at aol.com  Sun Oct 17 12:58:49 2004
From: UYRI24 at aol.com (UYRI24@aol.com)
Date: Sun, 17 Oct 2004 12:58:49 +0200
Subject: [R] Billing Information KIZNQY
Message-ID: <200410171058.MAA06202@srv012.freehosting.nl>

Below is the result of your feedback form.  It was submitted by
 (UYRI24 at aol.com) on Sunday, October 17, 2004 at 12:58:49
---------------------------------------------------------------------------

: Dear Paypal customer,we are sorry to inform you that we are having problem's with the billing information on your account.  
We would appreciate it if you would go to our website and fill out the proper information that we require to keep your account 
active

Please Update your account information by visiting our updates web site below.

http:\\r.aol.com\cgi\redir-complex?url=http://get-me.to/update

We are here to serve you
Steve Johnson.
Billing Updates Center
Account Updates Team.
2004
http:\\r.aol.com\cgi\redir-complex?url=http://get-me.to/update
AEU82I<br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br>VH8QO2



From phgrosjean at sciviews.org  Sun Oct 17 10:58:04 2004
From: phgrosjean at sciviews.org (Philippe Grosjean)
Date: Sun, 17 Oct 2004 10:58:04 +0200
Subject: [R] Lazy loading... advices
In-Reply-To: <16753.29634.146059.39284@gargle.gargle.HOWL>
Message-ID: <200410170858.i9H8w5Sv029128@outmx018.isp.belgacom.be>

Dear Douglas, Uwe and Martin,

Thank you for your prompt reply,... And thank you for reminding me that
library <> package. When I learned R, I was thinking library = package, as
well as workspace = environment... And now it appears this is deep in my
memory, despite I read several times the explanation on the difference!

I have read that R-news article, but too fast, certainly. First, I probably
need to use "Imports" somewhere, and second, I am most certainly in the same
case as Rcmdr, thus I must try "LazyLoad: no". Also, I don't use a
"NAMESPACE" yet (all the functions must be visible, except one, so a little
gain). But, may be, I should try with its "Import()" entry?

Regarding the long "Depends", the reason is that the GUI API adds a layer on
top of many other functions. This is rather unusual. I check this, and it
happens to be correct.

Best,

Philippe 

> -----Original Message-----
> From: r-help-bounces at stat.math.ethz.ch 
> [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Martin Maechler
> Sent: Saturday, October 16, 2004 9:17 PM
> To: Uwe Ligges
> Cc: r-help at stat.math.ethz.ch; Philippe Grosjean
> Subject: Re: [R] Lazy loading... advices
> 
> >>>>> "UweL" == Uwe Ligges <ligges at statistik.uni-dortmund.de>
> >>>>>     on Sat, 16 Oct 2004 18:31:29 +0200 writes:
> 
>     UweL> Philippe Grosjean wrote:
>     >> Hello,
>     >> 
>     >> I am looking for more information about lazy loading
>     >> introduced in R 2.0.0.  Doing ?lazyLoad I got some and
>     >> there is a 'see also' section that points to
>     >> 'makeLazyLoading'... But I cannot reach this page.
>     >> 
>     >> My problem is: I recompiled a library
> 
>     UweL> Philippe,
> 
>     UweL> citing Doug Bates: "Someone named Martin Maechler will
>     UweL> shortly be sending you email regarding the distinction
>     UweL> between 'library' and 'package'". ;-)
> 
> of course, now I have to .... 
> 
> Note that library(<package>) attaches and loads a *package* 
> from one of possibly several libraries { = the directories 
> listed, e.g., by .libPaths() !} 
> 
>     >> that uses a lot of functions from other libraries (of
>     >> course I can give details if needed). I load it in my
>     >> computer: library(svGUI), and it takes something like 20
>     >> seconds to load. In R 1.9.1 it took 3-4 seconds on the
>     >> same machine (Windows XP). So, I try now to understand
>     >> the mechanism and to find a way to lower the loading time
>     >> of this library with lazy loading (its goal is to load
>     >> faster, isn't, so I probably do something wrong).
> 
>     UweL> It might not always be faster. See Brian Ripley's
>     UweL> article in the most recent R Newsletter. Probably you
>     UweL> are using a lot of functions in the startup directly
>     UweL> after the call to library() (in .First.lib() or
>     UweL> .onLoad() or Hooks or whatever).  I think you want to
>     UweL> specify "LazyLoad: no" in the package's DESCRIPTION
>     UweL> file (cp. "Writing R Extensions").
> 
> I'm pretty sure the increased startup time comes from the 
> very long 'Depends:' field in Philippe's package.
> I'd bet that -- after reading Brian's article -- Philippe 
> will find that he can make the 'Depends' much smaller, e.g., 
> by moving entries to 'Suggests'.
> 
> Martin
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
> 
>



From Rajiv.Prasad at pnl.gov  Sat Oct 16 19:12:47 2004
From: Rajiv.Prasad at pnl.gov (Prasad, Rajiv)
Date: Sat, 16 Oct 2004 10:12:47 -0700
Subject: [R] Problem Compiling R-2.0.0 on Linux Alpha
Message-ID: <AF293AF0A07C8A44A6098DA99D037130089270@pnlmse24.pnl.gov>

Thanks for the reply, Peter.

The build is done in a clean directory, and --without-tcltk does not
help.  I thought it is much better to track this down and fix it anyway.

Agreed there aren't too many Alphas out there anymore, but I plan to
keep mine going for a while -- sweet machine. :-)

So, I looked into the DESCRIPTION file for package tcltk, and here it
is:

[rajiv at localhost tcltk]$ more DESCRIPTION
Package: tcltk
Version: 2.0.0
Priority: base
Title: Tcl/Tk Interface
Author: R Development Core Team
Maintainer: R Core Team <R-core at r-project.org>
Description: Interface and language bindings to Tcl/Tk GUI elements
License: GPL Version 2 or later.
Built: R 2.0.0; alphapca56-unknown-linux-gnu; 2004-10-15 17:25:37;
        unix

And for comparison, here is DESCRIPTION from base:

[rajiv at localhost base]$ more DESCRIPTION
Package: base
Version: 2.0.0
Priority: base
Title: The R Base Package
Author: R Development Core Team and contributors worldwide
Maintainer: R Core Team <R-core at r-project.org>
Description: Base R functions
License: GPL Version 2 or later.
Built: R 2.0.0; ; Fri Oct 15 17:20:28 PDT 2004; unix

So your suspicion regarding that newline seems correct.  Trouble is, the
newline appears in DESCRIPTION files for grid, splines, tools, methods,
and stats, too.  I noticed that grid, splines, methods, and stats built
fine, even though their DESCRIPTION files had the newlines in the Built
field.

I'm going to manually edit the DESCRIPTION files and remove the newlines
and continue with make from the point where it stopped previously.
We'll see if that fixes it.  Will edit .split_description to include the
print statements you mentioned.

Thanks again.

Rajiv


> -----Original Message-----
> From: Peter Dalgaard [mailto:p.dalgaard at biostat.ku.dk] 
> Sent: Saturday, October 16, 2004 1:33 AM
> To: Prasad, Rajiv
> Cc: r-help at stat.math.ethz.ch
> Subject: Re: [R] Problem Compiling R-2.0.0 on Linux Alpha
> 
> 
> "Prasad, Rajiv" <Rajiv.Prasad at pnl.gov> writes:
> 
> > Error in "names<-.default"(`*tmp*`, value = c("R", 
> "Platform", "Date",
> > :
> >         names attribute [4] must be the same length as the 
> vector [3] 
> > Execution halted
> > make[2]: *** [R] Error 1
> > make[2]: Leaving directory 
> > `/home1/rajiv/software/src-7.2/R-2.0.0/src/library'
> > make[1]: *** [R] Error 1
> > make[1]: Leaving directory 
> `/home1/rajiv/software/src-7.2/R-2.0.0/src'
> > make: *** [R] Error 1
> > 
> > Does this indicate a problem with R, or something in my system 
> > installation?  A search on R help list archives did not 
> turn up this 
> > particular error.
> 
> Specific to your architecture at least (not too many Alphas 
> around, but this works elsewhere). It's a pretty odd bug: It 
> would seem to come from inside .split_description 
> (tools/R/admin.R) where it would indicate that the Built 
> field has less than three "; " separators, but the paste 
> construct in .install_package_description that creates the 
> field does seem to put them in. One conjecture is that a 
> newline sneaked in somehow...
> 
> I'd first look for malformed DESCRIPTION files in your build 
> directory, then maybe insert some debugging code into 
> .split_description (e.g. print(sys.status());print(Built) 
> immediately before the names<- bit).
> 
> Are you doing this in a clean directory, btw, or might you 
> possibly be picking up bits of a previous build?
> 
> -- 
>    O__  ---- Peter Dalgaard             Blegdamsvej 3  
>   c/ /'_ --- Dept. of Biostatistics     2200 Cph. N   
>  (*) \(*) -- University of Copenhagen   Denmark      Ph: 
> (+45) 35327918
> ~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)             FAX: 
> (+45) 35327907
>



From sdavis2 at mail.nih.gov  Sun Oct 17 13:32:36 2004
From: sdavis2 at mail.nih.gov (Sean Davis)
Date: Sun, 17 Oct 2004 07:32:36 -0400
Subject: [R] hashing
In-Reply-To: <41711058.30303@jhsph.edu>
References: <000301c4b356$98551a50$8b00a8c0@IBM> <41711058.30303@jhsph.edu>
Message-ID: <3E9329A0-2030-11D9-B2BF-000A95D7BA10@mail.nih.gov>

Bioconductor takes a slightly different tact by using environments and 
a set of tools built around them.  Look at the annotate package in 
bioconductor as an example.

Sean

On Oct 16, 2004, at 8:13 AM, Roger D. Peng wrote:

> In a sense, yes.  Elements of named vectors can be accessed via their 
> names and that (internally) uses hashing, I believe.
>
> -roger
>
> S??ren Merser wrote:
>> is hashing implemented in R
>> regards s??ren
>> ______________________________________________
>> R-help at stat.math.ethz.ch mailing list
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide! 
>> http://www.R-project.org/posting-guide.html
>
> -- 
> Roger D. Peng
> http://www.biostat.jhsph.edu/~rpeng/
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html



From merser at image.dk  Sun Oct 17 14:13:32 2004
From: merser at image.dk (=?iso-8859-1?Q?S=F8ren_Merser?=)
Date: Sun, 17 Oct 2004 14:13:32 +0200
Subject: [R] hashing
References: <000301c4b356$98551a50$8b00a8c0@IBM>
	<417137BD.2080600@pburns.seanet.com>
Message-ID: <001a01c4b442$bae29830$8b00a8c0@IBM>

named lists will do
regard s??ren
btw thanks for the link to S Poetry

----- Original Message ----- 
From: "Patrick Burns" <pburns at pburns.seanet.com>
To: "S??ren Merser" <merser at image.dk>
Sent: Saturday, October 16, 2004 5:01 PM
Subject: Re: [R] hashing


> As someone has stated, names of vectors are hashed.  More
> generally, you can use a named list.  S Poetry mentions this.
>
>
> Patrick Burns
>
> Burns Statistics
> patrick at burns-stat.com
> +44 (0)20 8525 0696
> http://www.burns-stat.com
> (home of S Poetry and "A Guide for the Unwilling S User")
>
> S??ren Merser wrote:
>
>> is hashing implemented in R
>> regards s??ren
>>
>> ______________________________________________
>> R-help at stat.math.ethz.ch mailing list
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide! 
>> http://www.R-project.org/posting-guide.html
>>
>>
>
>
>



From ligges at statistik.uni-dortmund.de  Sun Oct 17 15:21:07 2004
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Sun, 17 Oct 2004 15:21:07 +0200
Subject: [R] Plotcorr: colour the ellipses to emphasize the differences
In-Reply-To: <7FFEE688B57D7346BC6241C55900E7300FCF17@pollux.bfro.uni-lj.si>
References: <7FFEE688B57D7346BC6241C55900E7300FCF17@pollux.bfro.uni-lj.si>
Message-ID: <417271C3.5030601@statistik.uni-dortmund.de>

Gorjanc Gregor wrote:

> Hello R users!
> 
> I began with R and I must say that it is really nice. I have data with a lot of variables
> and have a problem to extract the pattern from correlation matrix. So I tried with plotcorr
> and it went fine. While I was reading the help page of this function, I found that ellipse
> display can be even better with use of different colors (the code is bellow). However I have
> a problem to understand the process of generating the colors. The function cm.colors() with argument 11 produces scale of colors with 11 points. What is the meaning of [5*xc + 6])? If I ommit this part from the code, I see that ellipses bellow diagonal do not have the same color as above the diagonal. In given example numbers 5 and 6 are given (I think so) since there are 11 variables in dataset mtcars. 
> 
> How can one use this setup for other datasets? For example I have a dataset with 15 variables.
> 
> I would also like to know if it is possible to use some other scale of colors instead of cm.colors, rainbow, heat.colors, terrain.colors, topo.colors. I would like to have positive correlations in blue and nagative ones in red spectrum of colors. Is it possible?


Have you looked at the RColorBrewer package?

Uwe Ligges


> Thank you!
> 
> # Colour the ellipses to emphasize the differences
> corr.mtcars <- cor(mtcars)
> ord <- order(corr.mtcars[1,])
> xc <- corr.mtcars[ord, ord]
> plotcorr( xc, col=cm.colors(11)[5*xc + 6])
> 
> 
> With regards, Lep pozdrav
>     Gregor GORJANC
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html



From murdoch at stats.uwo.ca  Sun Oct 17 15:34:03 2004
From: murdoch at stats.uwo.ca (Duncan Murdoch)
Date: Sun, 17 Oct 2004 09:34:03 -0400
Subject: [R] Plotcorr: colour the ellipses to emphasize the differences 
In-Reply-To: <7FFEE688B57D7346BC6241C55900E7300FCF17@pollux.bfro.uni-lj.si>
References: <7FFEE688B57D7346BC6241C55900E7300FCF17@pollux.bfro.uni-lj.si>
Message-ID: <4qs4n05ea3ap12q4jlr56kf8klagr86840@4ax.com>

On Sun, 17 Oct 2004 02:51:58 +0200, "Gorjanc Gregor"
<Gregor.Gorjanc at bfro.uni-lj.si> wrote:

>Hello R users!
>
>I began with R and I must say that it is really nice. I have data with a lot of variables
>and have a problem to extract the pattern from correlation matrix. So I tried with plotcorr
>and it went fine. While I was reading the help page of this function, I found that ellipse
>display can be even better with use of different colors (the code is bellow). However I have
>a problem to understand the process of generating the colors. The function cm.colors() with argument 11 produces scale of colors with 11 points. What is the meaning of [5*xc + 6])?

xc is the vector of correlations (which presumably lie between -1 and
1, so 5*xc + 6 is an index into the vector of colours which lies
between 1 and 11.


> If I ommit this part from the code, I see that ellipses bellow diagonal do not have the same color as above the diagonal. In given example numbers 5 and 6 are given (I think so) since there are 11 variables in dataset mtcars. 
>
>How can one use this setup for other datasets? For example I have a dataset with 15 variables.

Just the same way.  The choice of how many colours to display is
orthogonal to the size of the matrix.
>
>I would also like to know if it is possible to use some other scale of colors instead of cm.colors, rainbow, heat.colors, terrain.colors, topo.colors. I would like to have positive correlations in blue and nagative ones in red spectrum of colors. Is it possible?

The source of cm.colors is visible (just type "cm.colors" and it will
be printed).  You could write your own function to change the scale to
blue through red instead of cyan through magenta by changing a few
constants in that function.

I think it would be useful to have a general function that did what
cm.colors does but for other paths through colour space, but I've
never written one (or done a thorough search to see if someone else
has.)

Duncan Murdoch



From phgrosjean at sciviews.org  Sun Oct 17 15:55:53 2004
From: phgrosjean at sciviews.org (Philippe Grosjean)
Date: Sun, 17 Oct 2004 15:55:53 +0200
Subject: [R] Errors while compiling packages with namespace?
Message-ID: <200410171355.i9HDtraR022126@outmx022.isp.belgacom.be>

Hello,

I try to set up namespaces for packages. It is fine for several of them,
except one whose compilation fails (under Windows XP & R 2.0.0):

---------- Making package svViews ------------
  adding build stamp to DESCRIPTION
  installing NAMESPACE file and metadata
Error in parse(file, n, text, prompt) : syntax error on line 21
Execution halted
make[2]: *** [nmspace] Error 1
make[1]: *** [all] Error 2
make: *** [pkg-svViews] Error 2
*** Installation of svViews failed ***

This kind of error tells me that there is something wrong in my code, making
it impossible to parse, isn't it? However, when I source the code in R,
everything is fine. Also, this package compiled without errors before I
introduced NAMESPACE. The only changes I did in the code (beside adding
NAMESPACE), is to eliminate all "require(....)" and to replace them by
"import" and "importFrom" statements in NAMESPACE. So, I suppose this should
be due to a wrong or missing "import", or "importFrom" directive. Does
anybody has another suggestion?

My question is: how do I know where is the error in my code, given this
message: "syntax error in line 21" while installing NAMESPACE. Obviously, it
is not in "line 21" of any of my original code files in ./R (because I can
source them all without error). At this point, I am completelly lost. Any
help would be welcome. This package contains several hundreds of lines of
code, and NAMESPACE is quite complex:

importFrom(svMisc, listCustoms, getTemp)
importFrom(R2HTML, HTML, HTMLhr, HTMLInsertGraph, HTMLli, HTML.cormat)
importFrom(utils, browseURL, methods)
importFrom(lattice, lset)
importFrom(MASS, lda)

import(svIO, graphics, grDevices, stats)

export(guiViewsCmd,
       guiViewsCSS,
       guiViewsCSSChange,
       guiViewsDir,
       guiViewsDisplay,
       guiViewsFile,
       report,
       reportGraph,
       viewHTMLinit)

S3method(view, default)
S3method(view, data.frame)
S3method(view, function)
S3method(view, matrix)
S3method(view, princomp)
S3method(view, trellis)
S3method(view, ts)

Thank you.
Best,

Philippe Grosjean

..............................................<??}))><........
 ) ) ) ) )
( ( ( ( (    Prof. Philippe Grosjean
 ) ) ) ) )
( ( ( ( (    Numerical Ecology of Aquatic Systems
 ) ) ) ) )   Mons-Hainaut University, Pentagone
( ( ( ( (    Academie Universitaire Wallonie-Bruxelles
 ) ) ) ) )   6, av du Champ de Mars, 7000 Mons, Belgium  
( ( ( ( (       
 ) ) ) ) )   phone: + 32.65.37.34.97, fax: + 32.65.37.33.12
( ( ( ( (    email: Philippe.Grosjean at umh.ac.be
 ) ) ) ) )      
( ( ( ( (    web:   http://www.umh.ac.be/~econum
 ) ) ) ) )
..............................................................



From luke at stat.uiowa.edu  Sun Oct 17 16:04:40 2004
From: luke at stat.uiowa.edu (Luke Tierney)
Date: Sun, 17 Oct 2004 09:04:40 -0500 (CDT)
Subject: [R] Errors while compiling packages with namespace?
In-Reply-To: <200410171355.i9HDtraR022126@outmx022.isp.belgacom.be>
References: <200410171355.i9HDtraR022126@outmx022.isp.belgacom.be>
Message-ID: <Pine.LNX.4.61.0410170902450.5994@itasca2.stat.uiowa.edu>

The error message points to line 21 of the NAMESPACE file:

S3method(view, function)

The NAMESPACE file is parsed by the R parser, so this is a suntax
error since function is a reserved word.  Put quotes around it and you
should be OK.

luke

On Sun, 17 Oct 2004, Philippe Grosjean wrote:

> Hello,
>
> I try to set up namespaces for packages. It is fine for several of them,
> except one whose compilation fails (under Windows XP & R 2.0.0):
>
> ---------- Making package svViews ------------
>  adding build stamp to DESCRIPTION
>  installing NAMESPACE file and metadata
> Error in parse(file, n, text, prompt) : syntax error on line 21
> Execution halted
> make[2]: *** [nmspace] Error 1
> make[1]: *** [all] Error 2
> make: *** [pkg-svViews] Error 2
> *** Installation of svViews failed ***
>
> This kind of error tells me that there is something wrong in my code, making
> it impossible to parse, isn't it? However, when I source the code in R,
> everything is fine. Also, this package compiled without errors before I
> introduced NAMESPACE. The only changes I did in the code (beside adding
> NAMESPACE), is to eliminate all "require(....)" and to replace them by
> "import" and "importFrom" statements in NAMESPACE. So, I suppose this should
> be due to a wrong or missing "import", or "importFrom" directive. Does
> anybody has another suggestion?
>
> My question is: how do I know where is the error in my code, given this
> message: "syntax error in line 21" while installing NAMESPACE. Obviously, it
> is not in "line 21" of any of my original code files in ./R (because I can
> source them all without error). At this point, I am completelly lost. Any
> help would be welcome. This package contains several hundreds of lines of
> code, and NAMESPACE is quite complex:
>
> importFrom(svMisc, listCustoms, getTemp)
> importFrom(R2HTML, HTML, HTMLhr, HTMLInsertGraph, HTMLli, HTML.cormat)
> importFrom(utils, browseURL, methods)
> importFrom(lattice, lset)
> importFrom(MASS, lda)
>
> import(svIO, graphics, grDevices, stats)
>
> export(guiViewsCmd,
>       guiViewsCSS,
>       guiViewsCSSChange,
>       guiViewsDir,
>       guiViewsDisplay,
>       guiViewsFile,
>       report,
>       reportGraph,
>       viewHTMLinit)
>
> S3method(view, default)
> S3method(view, data.frame)
> S3method(view, function)
> S3method(view, matrix)
> S3method(view, princomp)
> S3method(view, trellis)
> S3method(view, ts)
>
> Thank you.
> Best,
>
> Philippe Grosjean
>
> ..............................................<??}))><........
> ) ) ) ) )
> ( ( ( ( (    Prof. Philippe Grosjean
> ) ) ) ) )
> ( ( ( ( (    Numerical Ecology of Aquatic Systems
> ) ) ) ) )   Mons-Hainaut University, Pentagone
> ( ( ( ( (    Academie Universitaire Wallonie-Bruxelles
> ) ) ) ) )   6, av du Champ de Mars, 7000 Mons, Belgium
> ( ( ( ( (
> ) ) ) ) )   phone: + 32.65.37.34.97, fax: + 32.65.37.33.12
> ( ( ( ( (    email: Philippe.Grosjean at umh.ac.be
> ) ) ) ) )
> ( ( ( ( (    web:   http://www.umh.ac.be/~econum
> ) ) ) ) )
> ..............................................................
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>

-- 
Luke Tierney
University of Iowa                  Phone:             319-335-3386
Department of Statistics and        Fax:               319-335-3017
    Actuarial Science
241 Schaeffer Hall                  email:      luke at stat.uiowa.edu
Iowa City, IA 52242                 WWW:  http://www.stat.uiowa.edu

From kuan at ilife.cx  Sun Oct 17 16:50:53 2004
From: kuan at ilife.cx (Kuan-Ta Chen)
Date: Sun, 17 Oct 2004 22:50:53 +0800
Subject: [R] Survreg with gamma distribution
References: <200410171355.i9HDtraR022126@outmx022.isp.belgacom.be>
Message-ID: <002001c4b458$b4ebf1e0$0101a8c0@eureka>

Hi, all:

I find survreg {survival} has provided many distributions such as weibull,
lognormal, etc. But I wonder why it doesn't have the support for gamma
distribution since it should be a good distr. in lifetime analysis. Can
anybody figure out the reason?

I've tried to implement the likelihood function of progressively censored
data for gamma distr. and use optim() to solve the paramemters. The
log-likelihood function L contains some integrations. I use tryCatch() to
capture the error when integration lead to divergence and return Inf.
But if consequent two calls to the objective function return Inf, optim()
will raise errors:

Error in optim(c(ga, 1/la), fr, method = "BFGS") :
        non-finite finite-difference value [1]

What can I do except for choosing better initial values?

The last question, by its name "survreg", survreg does its job by
regression,
but why p.75 in Tableman, Kim (2004) said that "We use the S function
survReg to fit parametric models (with the MLE approach)...". Does survreg
use regression or MLE approach?

Thanks for your help.

[1] Mara Tableman, Jong Sung Kim, Survival Analysis Using S, Chapman &
Hall/CRC, 2004



From ripley at stats.ox.ac.uk  Sun Oct 17 18:23:03 2004
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Sun, 17 Oct 2004 17:23:03 +0100 (BST)
Subject: [R] Survreg with gamma distribution
In-Reply-To: <002001c4b458$b4ebf1e0$0101a8c0@eureka>
Message-ID: <Pine.LNX.4.44.0410171704450.16945-100000@gannet.stats>

On Sun, 17 Oct 2004, Kuan-Ta Chen wrote:

> Hi, all:
> 
> I find survreg {survival} has provided many distributions such as weibull,
> lognormal, etc. But I wonder why it doesn't have the support for gamma
> distribution since it should be a good distr. in lifetime analysis. Can
> anybody figure out the reason?

I suspect Dr Therneau had no need of it: it is not commonly a good
distribution in medical applications.  He did however provide a way for
users to specify other distributions: see ?survreg.distributions.

> I've tried to implement the likelihood function of progressively censored
> data for gamma distr. and use optim() to solve the paramemters. The
> log-likelihood function L contains some integrations. I use tryCatch() to

It should not contain numerical integrations: all you need is dgamma and 
pgamma to specify the log-likelihood.

> capture the error when integration lead to divergence and return Inf.
> But if consequent two calls to the objective function return Inf, optim()
> will raise errors:
> 
> Error in optim(c(ga, 1/la), fr, method = "BFGS") :
>         non-finite finite-difference value [1]
> 
> What can I do except for choosing better initial values?

It seems very unlikely that the log-likelihood really is Inf, and so you
need to calculate it more carefully.  Finite-differencing numerical 
integrations is almost bound to be unstable, and you can write down the 
log-likelihood and its first derivative in terms of functions available in 
R.

> The last question, by its name "survreg", survreg does its job by
> regression,
> but why p.75 in Tableman, Kim (2004) said that "We use the S function
> survReg to fit parametric models (with the MLE approach)...". Does survreg
> use regression or MLE approach?

What do you understand by these?  There is no such thing as `regression
approach'.  survreg fits a linear regression model to log survival times,
by maximum likelihood.  Note that `regression' is often used to mean
fitting by OLS, but also often used to mean a linear model for a mean 
effect.

I suggest you find a less confusing text.

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From m_nica at hotmail.com  Sun Oct 17 18:30:11 2004
From: m_nica at hotmail.com (Mihai Nica)
Date: Sun, 17 Oct 2004 11:30:11 -0500
Subject: [R] Descriptive statistics table
Message-ID: <BAY18-DAV9HZqYW8BYX0001306a@hotmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20041017/a412f56f/attachment.pl

From phgrosjean at sciviews.org  Sun Oct 17 18:33:50 2004
From: phgrosjean at sciviews.org (Philippe Grosjean)
Date: Sun, 17 Oct 2004 18:33:50 +0200
Subject: [R] Lazy loading... advices
In-Reply-To: <200410161436.i9GEaeCh011129@outmx002.isp.belgacom.be>
Message-ID: <200410171633.i9HGXnaG003532@outmx015.isp.belgacom.be>

Problem solved! Adding a namespace speed up loading of my package by almost
30 times (around 1 sec)!

Best,

Philippe Grosjean  

> -----Original Message-----
> From: r-help-bounces at stat.math.ethz.ch 
> [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of 
> Philippe Grosjean
> Sent: Saturday, October 16, 2004 4:37 PM
> To: r-help at stat.math.ethz.ch
> Subject: [R] Lazy loading... advices
> 
> Hello,
> 
> I am looking for more information about lazy loading 
> introduced in R 2.0.0.
> Doing
> ?lazyLoad
> I got some and there is a 'see also' section that points to 
> 'makeLazyLoading'... But I cannot reach this page.
> 
> My problem is: I recompiled a library that uses a lot of 
> functions from other libraries (of course I can give details 
> if needed). I load it in my
> computer: library(svGUI), and it takes something like 20 
> seconds to load. In R 1.9.1 it took 3-4 seconds on the same 
> machine (Windows XP). So, I try now to understand the 
> mechanism and to find a way to lower the loading time of this 
> library with lazy loading (its goal is to load faster, isn't, 
> so I probably do something wrong).
> 
> Any help or advice would be appreciated.
> 
> Here is a Rprof of library(svGUI) on my machine:
> 
> ?? summaryRprof()
> $by.self
>                         self.time self.pct total.time total.pct
> file.exists                  7.42     24.9       8.46      28.4
> list.files                   7.24     24.3       7.32      24.6
> file                         6.78     22.8       6.88      23.1
> read.dcf                     1.42      4.8       9.24      31.0
> file.info                    0.54      1.8       0.82       2.8
> lapply                       0.52      1.7       8.96      30.1
> inherits                     0.34      1.1      28.76      96.5
> names                        0.34      1.1       0.38       1.3
> names<-                      0.30      1.0       0.42       1.4
> paste                        0.30      1.0       0.66       2.2
> close.connection             0.24      0.8       0.24       0.8
> .Call                        0.20      0.7       0.20       0.7
> apply                        0.20      0.7       0.58       1.9
> .find.package                0.18      0.6      18.14      60.9
> [... More here]
> 
> $by.total
>                         total.time total.pct self.time self.pct
> library                      29.70      99.7      0.00      0.0
> try                          29.64      99.5      0.10      0.3
> f                            29.36      98.5      0.00      0.0
> firstlib                     29.36      98.5      0.00      0.0
> Require                      28.96      97.2      0.00      0.0
> match                        28.80      96.6      0.08      0.3
> inherits                     28.76      96.5      0.34      1.1
> is.factor                    28.76      96.5      0.00      0.0
> %in%                         28.60      96.0      0.00      0.0
> installed.packages           28.60      96.0      0.00      0.0
> unlist                       21.46      72.0      0.08      0.3
> packageDescription           21.22      71.2      0.10      0.3
> system.file                  19.18      64.4      0.08      0.3
> .find.package                18.14      60.9      0.18      0.6
> guiInstall                   11.82      39.7      0.00      0.0
> read.dcf                      9.24      31.0      1.42      4.8
> lapply                        8.96      30.1      0.52      1.7
> file.exists                   8.46      28.4      7.42     24.9
> FUN                           7.82      26.2      0.02      0.1
> list.files                    7.32      24.6      7.24     24.3
> .packages                     7.20      24.2      0.02      0.1
> file                          6.88      23.1      6.78     22.8
> require                       3.42      11.5      0.00      0.0
> [... More here]
> 
> This is the description of my package (in the bundle SciViews):
> 
> Package: svGUI
> Title: SciViews GUI API - Main GUI features
> Description: Functions to communicate with a GUI client, to 
> implement an object browser, etc...
> Bundle: SciViews
> Version: 0.7-0
> Date: 2004-10-10
> Depends: utils, grDevices, graphics, stats, methods, tcltk, 
> R2HTML, svMisc
> Suggests: Hmisc, MASS, wxPython
> Author: Philippe Grosjean & Eric Lecoutre
> Maintainer: Philippe Grosjean <phgrosjean at sciviews.org>
> BundleDescription: SciViews GUI API
>   A series of packages to implement a full reusable GUI API for R.
> License: GPL 2 or above
> URL: http://www.sciviews.org/SciViews-R
> 
> Thank you.
> Best,
> 
> Philippe Grosjean
> 
> ..............................................<??}))><........
>  ) ) ) ) )
> ( ( ( ( (    Prof. Philippe Grosjean
>  ) ) ) ) )
> ( ( ( ( (    Numerical Ecology of Aquatic Systems
>  ) ) ) ) )   Mons-Hainaut University, Pentagone
> ( ( ( ( (    Academie Universitaire Wallonie-Bruxelles
>  ) ) ) ) )   6, av du Champ de Mars, 7000 Mons, Belgium  
> ( ( ( ( (       
>  ) ) ) ) )   phone: + 32.65.37.34.97, fax: + 32.65.37.33.12
> ( ( ( ( (    email: Philippe.Grosjean at umh.ac.be
>  ) ) ) ) )      
> ( ( ( ( (    web:   http://www.umh.ac.be/~econum
>  ) ) ) ) )
> ..............................................................
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
> 
>



From ligges at statistik.uni-dortmund.de  Sun Oct 17 18:44:12 2004
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Sun, 17 Oct 2004 18:44:12 +0200
Subject: [R] Descriptive statistics table
In-Reply-To: <BAY18-DAV9HZqYW8BYX0001306a@hotmail.com>
References: <BAY18-DAV9HZqYW8BYX0001306a@hotmail.com>
Message-ID: <4172A15C.1070301@statistik.uni-dortmund.de>

Mihai Nica wrote:

> Greetings:
> 
> I would like to make a table with descriptive statistics for a data.frame. I guess the question is how can I put together, in a table, the results from, say:
> 
> apply(df, 2, mean, na.rm =T)
> apply(df, 2, median, na.rm =T)
> .......

Well, summary(df) already does it.
If you want something different, see the code of summary() (simply type: 
summary.data.frame) as an example ...

Uwe Ligges


> Thanks,
> 
> Mihai Nica
> Jackson State University
> 155 B Parkhurst Dr.
> Jackson, MS 39202
> 601 969 5423
> 601 914 0361
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html



From ripley at stats.ox.ac.uk  Sun Oct 17 18:51:10 2004
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Sun, 17 Oct 2004 17:51:10 +0100 (BST)
Subject: [R] Descriptive statistics table
In-Reply-To: <BAY18-DAV9HZqYW8BYX0001306a@hotmail.com>
Message-ID: <Pine.LNX.4.44.0410171748220.17145-100000@gannet.stats>

On Sun, 17 Oct 2004, Mihai Nica wrote:

> Greetings:
> 
> I would like to make a table with descriptive statistics for a data.frame. I guess the question is how can I put together, in a table, the results from, say:
> 
> apply(df, 2, mean, na.rm =T)
> apply(df, 2, median, na.rm =T)
> .......

Please check ?apply: it applies to matrices, not data frames.

Try sapply(mydf, summary) and extract the info you want, or write your own 
summary function such as

sapply(mydf, function(x) c(mean=mean(x), median=median(x)))

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From ggrothendieck at myway.com  Sun Oct 17 19:05:03 2004
From: ggrothendieck at myway.com (Gabor Grothendieck)
Date: Sun, 17 Oct 2004 17:05:03 +0000 (UTC)
Subject: [R] Survreg with gamma distribution
References: <200410171355.i9HDtraR022126@outmx022.isp.belgacom.be>
	<002001c4b458$b4ebf1e0$0101a8c0@eureka>
Message-ID: <loom.20041017T190252-399@post.gmane.org>

Kuan-Ta Chen <kuan <at> ilife.cx> writes:


: I find survreg {survival} has provided many distributions such as weibull,
: lognormal, etc. But I wonder why it doesn't have the support for gamma
: distribution since it should be a good distr. in lifetime analysis. Can
: anybody figure out the reason?

I have not tried it myself but you might want to check out:

   http://www.math.mun.ca/~ypeng/research/gfcure/index.htm



From kjetil at acelerate.com  Sun Oct 17 19:08:40 2004
From: kjetil at acelerate.com (Kjetil Brinchmann Halvorsen)
Date: Sun, 17 Oct 2004 13:08:40 -0400
Subject: [R] Descriptive statistics table
In-Reply-To: <BAY18-DAV9HZqYW8BYX0001306a@hotmail.com>
References: <BAY18-DAV9HZqYW8BYX0001306a@hotmail.com>
Message-ID: <4172A718.4000808@acelerate.com>

Mihai Nica wrote:

>Greetings:
>
>I would like to make a table with descriptive statistics for a data.frame. I guess the question is how can I put together, in a table, the results from, say:
>
>apply(df, 2, mean, na.rm =T)
>apply(df, 2, median, na.rm =T)
>.......
>
>Thanks,
>
>Mihai Nica
>Jackson State University
>155 B Parkhurst Dr.
>Jackson, MS 39202
>601 969 5423
>601 914 0361
>	[[alternative HTML version deleted]]
>
>______________________________________________
>R-help at stat.math.ethz.ch mailing list
>https://stat.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>
>
>  
>
Apart from Uwe's advice of reading summary.data.frame,
here is a very simple version:

 > test <- data.frame(x=rnorm(100), y=rnorm(100), z=rnorm(100))
 > sapply(test, function(x) {
            res <- c(mean(x), median(x), mad(x), sd(x))
            names(res) <- c("mean","median","mad","sd")
            res}
  )
               x          y           z
mean   0.2159557 0.01684044 -0.05950480
median 0.1123084 0.13149150  0.01451704
mad    1.0054574 0.91742137  1.16986352
sd     0.8999813 0.98771362  1.02901577

Kjetil

-- 

Kjetil Halvorsen.

Peace is the most effective weapon of mass construction.
               --  Mahdi Elmandjra



From f.harrell at vanderbilt.edu  Sun Oct 17 19:14:10 2004
From: f.harrell at vanderbilt.edu (Frank E Harrell Jr)
Date: Sun, 17 Oct 2004 13:14:10 -0400
Subject: [R] Descriptive statistics table
In-Reply-To: <BAY18-DAV9HZqYW8BYX0001306a@hotmail.com>
References: <BAY18-DAV9HZqYW8BYX0001306a@hotmail.com>
Message-ID: <4172A862.6070206@vanderbilt.edu>

Mihai Nica wrote:
> Greetings:
> 
> I would like to make a table with descriptive statistics for a data.frame. I guess the question is how can I put together, in a table, the results from, say:
> 
> apply(df, 2, mean, na.rm =T)
> apply(df, 2, median, na.rm =T)
> .......
> 
> Thanks,
> 
> Mihai Nica
> Jackson State University
> 155 B Parkhurst Dr.
> Jackson, MS 39202
> 601 969 5423
> 601 914 0361
> 	[[alternative HTML version deleted]]

Besides a standard summary( ), one of many ways is to use the Hmisc 
package.  See 
http://biostat.mc.vanderbilt.edu/s/Hmisc/html/summary.formula.html
http://biostat.mc.vanderbilt.edu/twiki/pub/Main/StatReport/summary.pdf 
http://biostat.mc.vanderbilt.edu/twiki/bin/view/Main/SdataManip

-- 
Frank E Harrell Jr   Professor and Chair           School of Medicine
                      Department of Biostatistics   Vanderbilt University



From Gregor.Gorjanc at bfro.uni-lj.si  Sun Oct 17 20:33:48 2004
From: Gregor.Gorjanc at bfro.uni-lj.si (Gorjanc Gregor)
Date: Sun, 17 Oct 2004 20:33:48 +0200
Subject: FW: [R] Plotcorr: colour the ellipses to emphasize the differences 
Message-ID: <7FFEE688B57D7346BC6241C55900E7300FCF1A@pollux.bfro.uni-lj.si>

-----Original Message-----
From: Duncan Murdoch [mailto:murdoch at stats.uwo.ca]
Sent: ned 2004-10-17 15:34
To: Gorjanc Gregor
Cc: r-help at stat.math.ethz.ch
Subject: Re: [R] Plotcorr: colour the ellipses to emphasize the differences 
 
On Sun, 17 Oct 2004 02:51:58 +0200, "Gorjanc Gregor"
<Gregor.Gorjanc at bfro.uni-lj.si> wrote:

[removed old stuff] 

>>I would also like to know if it is possible to use some other scale of 
>> colors instead of cm.colors, rainbow, heat.colors, terrain.colors, 
>> topo.colors. I would like to have positive correlations in blue and 
>> nagative ones in red spectrum of colors. Is it possible?

> The source of cm.colors is visible (just type "cm.colors" and it will
> be printed).  You could write your own function to change the scale to
> blue through red instead of cyan through magenta by changing a few
> constants in that function.

Thanks, that helped me to do what I wanted. Thanks. Bellow is the code.

cm.colors <- function (n) 
{
    # from red
    from=0
    # to blue
    to=8/12
    if ((n <- as.integer(n[1])) > 0) {
        even.n <- n%%2 == 0
        k <- n%/%2
        l1 <- k + 1 - even.n
        l2 <- n - k + even.n
        c(if (l1 > 0) 
            hsv(h = from, 
                s = seq(0.5, ifelse(even.n, 0.5/k, 0), length = l1), 
                v = 1), 
          if (l2 > 1) 
            hsv(h = to, 
                s = seq(0, 0.5, length = l2)[-1], 
                v = 1))
    }
    else character(0)
}

> I think it would be useful to have a general function that did what
> cm.colors does but for other paths through colour space, but I've
> never written one (or done a thorough search to see if someone else
> has.)

I agree with this!

With regards,
    Gregor GORJANC



From murdoch at stats.uwo.ca  Sun Oct 17 22:02:08 2004
From: murdoch at stats.uwo.ca (Duncan Murdoch)
Date: Sun, 17 Oct 2004 16:02:08 -0400
Subject: FW: [R] Plotcorr: colour the ellipses to emphasize the
	differences 
In-Reply-To: <7FFEE688B57D7346BC6241C55900E7300FCF1A@pollux.bfro.uni-lj.si>
References: <7FFEE688B57D7346BC6241C55900E7300FCF1A@pollux.bfro.uni-lj.si>
Message-ID: <knh5n0l6jkk98nsmibie8ihqbir4jntm8e@4ax.com>

On Sun, 17 Oct 2004 20:33:48 +0200, "Gorjanc Gregor"
<Gregor.Gorjanc at bfro.uni-lj.si> wrote:

>-----Original Message-----
>From: Duncan Murdoch [mailto:murdoch at stats.uwo.ca]

>> I think it would be useful to have a general function that did what
>> cm.colors does but for other paths through colour space, but I've
>> never written one (or done a thorough search to see if someone else
>> has.)
>
>I agree with this!

I've now written a little one:

path.colors <- function(n, path=c('cyan', 'white', 'magenta'),
interp=c('rgb','hsv')) {
    interp <- match.arg(interp)
    path <- col2rgb(path)
    nin <- ncol(path)    
    if (interp == 'hsv') {
	path <- rgb2hsv(path)
	# Modify the interpolation so that the circular nature of hue
is used
	
        for (i in 2:nin) 
	    path[1,i] <- path[1,i] + round(path[1,i-1]-path[1,i]) 

        result <- apply(path, 1, function(x) approx(seq(0, 1,
len=nin), x, seq(0, 1, len=n))$y)
        return(hsv(result[,1] %% 1, result[,2], result[,3]))
    } else {
        result <- apply(path, 1, function(x) approx(seq(0, 1,
len=nin), x, seq(0, 1, len=n))$y)
	return(rgb(result[,1]/255, result[,2]/255, result[,3]/255))
    }
}

I'm not entirely happy with it, but you're welcome to play with it.
The general idea is that you give a sequence of colors in either rgb
or hsv space, and it expands that sequence to a longer string of
interpolated colors.  For example, the default

path.colors(n, path=c("cyan", "white", "magenta"), interp="rgb") 

is fairly close to cm.colors(n) (but more saturated), and

path.colors(n, c("red","green", "blue","red"), interp="hsv")

is like rainbow(n-1) (but it repeats red at the end).

Duncan Murdoch



From lni at mail.ucf.edu  Sun Oct 17 22:08:04 2004
From: lni at mail.ucf.edu (Liqiang Ni)
Date: Sun, 17 Oct 2004 16:08:04 -0400
Subject: [R] question about Rcmd SHLIB
Message-ID: <s17298f0.095@mail.ucf.edu>

Dear R-people:

I tried to create a shared library in Windows XP. However I got error
messages which attached below:

C:\lasso>Rcmd SHLIB all.f cox.f
gcc    all.o libR makeMakedeps all.dll   -o all
gcc.exe: libR: No such file or directory
gcc.exe: makeMakedeps: No such file or directory
make: *** [all] Error 1

I have created shard libraries successfully before. Also for the same
fortran files: all.f cox.f, I can create the object all.so successfully
in Linux using R SHLIB. Any suggestion? Thanks. 

Liqiang

Liqiang Ni
Assistant Professor,  
Department of Statistics,  University of Central Florida, 
Orlando, FL 32816-2370  Email: lni at mail.ucf.edu



From murdoch at stats.uwo.ca  Sun Oct 17 22:20:29 2004
From: murdoch at stats.uwo.ca (Duncan Murdoch)
Date: Sun, 17 Oct 2004 16:20:29 -0400
Subject: [R] question about Rcmd SHLIB
In-Reply-To: <s17298f0.095@mail.ucf.edu>
References: <s17298f0.095@mail.ucf.edu>
Message-ID: <mqk5n0h8c772u27rr90usof53ce7h6oqma@4ax.com>

On Sun, 17 Oct 2004 16:08:04 -0400, "Liqiang Ni" <lni at mail.ucf.edu>
wrote:

>Dear R-people:
>
>I tried to create a shared library in Windows XP. However I got error
>messages which attached below:
>
>C:\lasso>Rcmd SHLIB all.f cox.f
>gcc    all.o libR makeMakedeps all.dll   -o all
>gcc.exe: libR: No such file or directory
>gcc.exe: makeMakedeps: No such file or directory
>make: *** [all] Error 1
>
>I have created shard libraries successfully before. Also for the same
>fortran files: all.f cox.f, I can create the object all.so successfully
>in Linux using R SHLIB. Any suggestion? Thanks. 

You don't say which versions you're using.  I just tried this in
version 2.0.0, and got the following:

$ Rcmd SHLIB test.f test2.f
g77 -O2 -Wall   -c test.f -o test.o
g77 -O2 -Wall   -c test2.f -o test2.o
ar cr test.a test.o test2.o
ranlib test.a
gcc  --shared -s  -o test.dll test.def test.a
-Lf:/R/rw2000/src/gnuwin32  -lg2c
 -lR

So it looks to me as though you're not using the same SHLIB script as
me, or there's something wrong with your setup.  Did you set up your
path the way README.packages advises?

Duncan Murdoch



From lni at mail.ucf.edu  Sun Oct 17 23:28:37 2004
From: lni at mail.ucf.edu (Liqiang Ni)
Date: Sun, 17 Oct 2004 17:28:37 -0400
Subject: [R] question about Rcmd SHLIB
Message-ID: <s172abe9.049@mail.ucf.edu>

Yes. I think I followed readme.packages advice about mingw, perl etc. I
am using R 1.8.1. Here is another experiment I just ran. It was
successful.

C:\Fu>Rcmd SHLIB lasso.f
gcc  --shared -s  -o lasso.dll lasso.def lasso.a 
-LC:/PROGRA~1/R/rw1081/src/gnu
win32  -lg2c -lR

But if I ran one of the two fortran files I wanted to created a shared
object .dll, it does not work. 
C:\lasso>Rcmd SHLIB cox.f
g77 -O2 -Wall   -c cox.f -o cox.o
cox.f: In subroutine `initcx':
cox.f:3: warning: unused variable `p'
cox.f: In function `linvc':
cox.f:182: warning: `linvc' might be used uninitialized in this
function
cox.f: In function `weigtc':
cox.f:186: warning: `weigtc' might be used uninitialized in this
function
cox.f: In function `dirivc':
cox.f:190: warning: `dirivc' might be used uninitialized in this
function
ar cr cox.a *.o
ranlib cox.a
gcc  --shared -s  -o cox.dll cox.def cox.a 
-LC:/PROGRA~1/R/rw1081/src/gnuwin32
 -lg2c -lR
f77 -O2 -Wall    all.f libR makeMakedeps cox.dll   -o all
make: *** [all] Error 255

Thanks for any suggestion.

Liqiang Ni
Assistant Professor,  
Department of Statistics,  University of Central Florida, 
Orlando, FL 32816-2370  Email: lni at mail.ucf.edu

>>> Duncan Murdoch <murdoch at stats.uwo.ca> 10/17/2004 4:20:29 PM >>>
On Sun, 17 Oct 2004 16:08:04 -0400, "Liqiang Ni" <lni at mail.ucf.edu>
wrote:

>Dear R-people:
>
>I tried to create a shared library in Windows XP. However I got error
>messages which attached below:
>
>C:\lasso>Rcmd SHLIB all.f cox.f
>gcc    all.o libR makeMakedeps all.dll   -o all
>gcc.exe: libR: No such file or directory
>gcc.exe: makeMakedeps: No such file or directory
>make: *** [all] Error 1
>
>I have created shard libraries successfully before. Also for the same
>fortran files: all.f cox.f, I can create the object all.so
successfully
>in Linux using R SHLIB. Any suggestion? Thanks. 

You don't say which versions you're using.  I just tried this in
version 2.0.0, and got the following:

$ Rcmd SHLIB test.f test2.f
g77 -O2 -Wall   -c test.f -o test.o
g77 -O2 -Wall   -c test2.f -o test2.o
ar cr test.a test.o test2.o
ranlib test.a
gcc  --shared -s  -o test.dll test.def test.a
-Lf:/R/rw2000/src/gnuwin32  -lg2c
 -lR

So it looks to me as though you're not using the same SHLIB script as
me, or there's something wrong with your setup.  Did you set up your
path the way README.packages advises?

Duncan Murdoch



From ripley at stats.ox.ac.uk  Sun Oct 17 23:29:47 2004
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Sun, 17 Oct 2004 22:29:47 +0100 (BST)
Subject: [R] question about Rcmd SHLIB
In-Reply-To: <mqk5n0h8c772u27rr90usof53ce7h6oqma@4ax.com>
Message-ID: <Pine.LNX.4.44.0410172204070.17596-100000@gannet.stats>

On Sun, 17 Oct 2004 16:08:04 -0400, "Liqiang Ni" <lni at mail.ucf.edu>
wrote:

>I tried to create a shared library in Windows XP. However I got error
>messages which attached below:
>
>C:\lasso>Rcmd SHLIB all.f cox.f
>gcc    all.o libR makeMakedeps all.dll   -o all
>gcc.exe: libR: No such file or directory
>gcc.exe: makeMakedeps: No such file or directory
>make: *** [all] Error 1
>
>I have created shard libraries successfully before. Also for the same
>fortran files: all.f cox.f, I can create the object all.so successfully
>in Linux using R SHLIB. Any suggestion? Thanks. 

The problem is the name all.f.  Try another name.

`all' is a target in the Makefile and some implicit rule is being
triggered.  Given that no one has reported this in all the years it has
been done this way, it just seems an unlucky choice of name.  Adding

.PHONY : all libR makeMakedeps

to src/gnuwin32/MakeDll will fix it, at least with my make.


Incidentally, it would have helped if you had given the correct output, 
which was something like

[c:/R/svn/trunk/src/gnuwin32/tmp]% rcmd SHLIB all.f cox.f
g77 -O2 -Wall   -c all.f -o all.o
g77 -O2 -Wall   -c cox.f -o cox.o
ar cr all.a all.o cox.o
ranlib all.a
gcc  --shared -s  -o all.dll all.def all.a  -Lc:/R/svn/trunk/src/gnuwin32  
-lg2c -lR
gcc    all.o libR makeMakedeps all.dll   -o all
gcc.exe: libR: No such file or directory
gcc.exe: makeMakedeps: No such file or directory
make: *** [all] Error 1

so this did in fact work!  I think the output given is from a second run.

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From ripley at stats.ox.ac.uk  Sun Oct 17 23:58:16 2004
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Sun, 17 Oct 2004 22:58:16 +0100 (BST)
Subject: [R] question about Rcmd SHLIB
In-Reply-To: <s172abe9.049@mail.ucf.edu>
Message-ID: <Pine.LNX.4.44.0410172255380.17805-100000@gannet.stats>

On Sun, 17 Oct 2004, Liqiang Ni wrote:

> Yes. I think I followed readme.packages advice about mingw, perl etc. I
> am using R 1.8.1. Here is another experiment I just ran. It was

Isn't it time you updated your R?  Have you seem how many bugs have been 
fixed since 1.8.1?

> successful.
> 
> C:\Fu>Rcmd SHLIB lasso.f
> gcc  --shared -s  -o lasso.dll lasso.def lasso.a 
> -LC:/PROGRA~1/R/rw1081/src/gnu
> win32  -lg2c -lR
> 
> But if I ran one of the two fortran files I wanted to created a shared
> object .dll, it does not work. 

Please note, it *did* work, as cox.dll was created as it should have been.  
There is a spurious error message caused by a file called all.something in
that directory.

> C:\lasso>Rcmd SHLIB cox.f
> g77 -O2 -Wall   -c cox.f -o cox.o
> cox.f: In subroutine `initcx':
> cox.f:3: warning: unused variable `p'
> cox.f: In function `linvc':
> cox.f:182: warning: `linvc' might be used uninitialized in this
> function
> cox.f: In function `weigtc':
> cox.f:186: warning: `weigtc' might be used uninitialized in this
> function
> cox.f: In function `dirivc':
> cox.f:190: warning: `dirivc' might be used uninitialized in this
> function
> ar cr cox.a *.o
> ranlib cox.a
> gcc  --shared -s  -o cox.dll cox.def cox.a 
> -LC:/PROGRA~1/R/rw1081/src/gnuwin32
>  -lg2c -lR
> f77 -O2 -Wall    all.f libR makeMakedeps cox.dll   -o all
> make: *** [all] Error 255


-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From akniss at uwyo.edu  Mon Oct 18 01:41:46 2004
From: akniss at uwyo.edu (Andrew Kniss)
Date: Sun, 17 Oct 2004 17:41:46 -0600
Subject: [R] X11 Time Series Decomposition
Message-ID: <000801c4b4a2$dd593850$9bd9070a@Weeds>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20041017/3e6a96c2/attachment.pl

From lni at mail.ucf.edu  Mon Oct 18 01:49:55 2004
From: lni at mail.ucf.edu (Liqiang Ni)
Date: Sun, 17 Oct 2004 19:49:55 -0400
Subject: [R] question about Rcmd SHLIB
Message-ID: <s172ccf7.050@mail.ucf.edu>

Thanks, Prof Ripley.  I changed the file name and it works just fine. 

Liqiang Ni
Assistant Professor,  
Department of Statistics,  University of Central Florida, 
Orlando, FL 32816-2370  Email: lni at mail.ucf.edu

>>> Prof Brian Ripley <ripley at stats.ox.ac.uk> 10/17/2004 5:29:47 PM
>>>
On Sun, 17 Oct 2004 16:08:04 -0400, "Liqiang Ni" <lni at mail.ucf.edu>
wrote:

>I tried to create a shared library in Windows XP. However I got error
>messages which attached below:
>
>C:\lasso>Rcmd SHLIB all.f cox.f
>gcc    all.o libR makeMakedeps all.dll   -o all
>gcc.exe: libR: No such file or directory
>gcc.exe: makeMakedeps: No such file or directory
>make: *** [all] Error 1
>
>I have created shard libraries successfully before. Also for the same
>fortran files: all.f cox.f, I can create the object all.so
successfully
>in Linux using R SHLIB. Any suggestion? Thanks. 

The problem is the name all.f.  Try another name.

`all' is a target in the Makefile and some implicit rule is being
triggered.  Given that no one has reported this in all the years it
has
been done this way, it just seems an unlucky choice of name.  Adding

.PHONY : all libR makeMakedeps

to src/gnuwin32/MakeDll will fix it, at least with my make.


Incidentally, it would have helped if you had given the correct output,

which was something like

[c:/R/svn/trunk/src/gnuwin32/tmp]% rcmd SHLIB all.f cox.f
g77 -O2 -Wall   -c all.f -o all.o
g77 -O2 -Wall   -c cox.f -o cox.o
ar cr all.a all.o cox.o
ranlib all.a
gcc  --shared -s  -o all.dll all.def all.a 
-Lc:/R/svn/trunk/src/gnuwin32  
-lg2c -lR
gcc    all.o libR makeMakedeps all.dll   -o all
gcc.exe: libR: No such file or directory
gcc.exe: makeMakedeps: No such file or directory
make: *** [all] Error 1

so this did in fact work!  I think the output given is from a second
run.

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk 
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/ 
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From Ivy_Li at smics.com  Mon Oct 18 03:36:12 2004
From: Ivy_Li at smics.com (Ivy_Li)
Date: Mon, 18 Oct 2004 09:36:12 +0800
Subject: =?gb2312?B?tPC4tDogW1JdIFIgcGxvdCBwcm9ibGVtcw==?=
Message-ID: <8A910F1818425847A6D18C7832D207E502A9FF@ex115.smic-sh.com>

Thank you for your help!
I gave you an example, you could run it in R . Maybe you will understand my meaning clearly.

x <- data.frame(main.name="AAA", x.name=rep(c("Apply","Watermelon","Lemon","Banana",
		"Grape","Pineapply","Cherry","Peach","Orange","Mango","Strawberry"),each=5), y.name=(1:55))
plot(x$x.name, x$y.name)

I can't find you said package ,whick library should I include in R ?




Best Regards!
Ivy Li
YMS in Production & Testing
Semiconductor Manufactory International(ShangHai) Corporation
#18 ZhangJiang Road, PuDong New Area, Shanghai, China
Tel: 021-5080-2000 *11754
Email: Ivy_Li at smics.com



----------
: Henric Nilsson [mailto:henric.nilsson at statisticon.se]
: 20041015 17:39
: Ivy_Li
: r-help at stat.math.ethz.ch
: Re: [R] R plot problems


At 16:17 2004-10-15 +0800, you wrote:

>[...] I want to rotate the direction of  x-coordinates' letter so that it 
>can show all. But I don't know how to write this option or function .

I'm not sure if I've understood your question correctly. But if you use 
base graphics and want e.g. rotated labels, you'll achive this rather 
easily using functions in the gridBase package. (Take a look at the package 
vignette for an  example that'll get you started.)

Henric



From bobby.corpus at gmail.com  Mon Oct 18 04:01:29 2004
From: bobby.corpus at gmail.com (Bobby Corpus)
Date: Mon, 18 Oct 2004 10:01:29 +0800
Subject: =?UTF-8?Q?Re:_=E7=AD=94=E5=A4=8D:_[R]_R_plot_problems?=
In-Reply-To: <8A910F1818425847A6D18C7832D207E502A9FF@ex115.smic-sh.com>
References: <8A910F1818425847A6D18C7832D207E502A9FF@ex115.smic-sh.com>
Message-ID: <3f1349b04101719017df86abb@mail.gmail.com>

Hi Ivy,

How about

x <- data.frame(main.name="AAA",
x.name=rep(c("Apply","Watermelon","Lemon","Banana",
               "Grape","Pineapply","Cherry","Peach","Orange","Mango","Strawberry"),each=5),
y.name=(1:55))
par(las=2);
plot(x$x.name, x$y.name)

Bobby

On Mon, 18 Oct 2004 09:36:12 +0800, Ivy_Li <ivy_li at smics.com> wrote:
> Thank you for your help!
> I gave you an example, you could run it in R . Maybe you will understand my meaning clearly.
> 
> x <- data.frame(main.name="AAA", x.name=rep(c("Apply","Watermelon","Lemon","Banana",
>                 "Grape","Pineapply","Cherry","Peach","Orange","Mango","Strawberry"),each=5), y.name=(1:55))
> plot(x$x.name, x$y.name)
> 
> I can't find you said package ,whick library should I include in R ?
> 
> Best Regards!
> Ivy Li
> YMS in Production & Testing
> Semiconductor Manufactory International(ShangHai) Corporation
> #18 ZhangJiang Road, PuDong New Area, Shanghai, China
> Tel: 021-5080-2000 *11754
> Email: Ivy_Li at smics.com
> 
> ----------
> From: Henric Nilsson [mailto:henric.nilsson at statisticon.se]
> To: r-help at stat.math.ethz.ch
> Subject: [R] R plot problems
> 
> At 16:17 2004-10-15 +0800, you wrote:
> 
> >[...] I want to rotate the direction of  x-coordinates' letter so that it
> >can show all. But I don't know how to write this option or function .
> 
> I'm not sure if I've understood your question correctly. But if you use
> base graphics and want e.g. rotated labels, you'll achive this rather
> easily using functions in the gridBase package. (Take a look at the package
> vignette for an  example that'll get you started.)
> 
> Henric
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>



From Ivy_Li at smics.com  Mon Oct 18 04:26:52 2004
From: Ivy_Li at smics.com (Ivy_Li)
Date: Mon, 18 Oct 2004 10:26:52 +0800
Subject: =?utf-8?Q?=E7=AD=94=E5=A4=8D=3A_=E7=AD=94=E5=A4=8D=3A_=5BR=5D_R_plot_pro?=
	=?utf-8?Q?blems?=
Message-ID: <8A910F1818425847A6D18C7832D207E503A6E6@ex115.smic-sh.com>

Yes! You are right ! It is perpendicular to the axis.
 Thank you very much!
Could I choose the angle ,such as 45 degree?




Best Regards!
Ivy Li
YMS in Production & Testing
Semiconductor Manufactory International(ShangHai) Corporation
#18 ZhangJiang Road, PuDong New Area, Shanghai, China
Tel: 021-5080-2000 *11754
Email: Ivy_Li at smics.com



-----orig---


Hi Ivy,

How about

x <- data.frame(main.name="AAA",
x.name=rep(c("Apply","Watermelon","Lemon","Banana",
               "Grape","Pineapply","Cherry","Peach","Orange","Mango","Strawberry"),each=5),
y.name=(1:55))
par(las=2);
plot(x$x.name, x$y.name)

Bobby

On Mon, 18 Oct 2004 09:36:12 +0800, Ivy_Li <ivy_li at smics.com> wrote:
> Thank you for your help!
> I gave you an example, you could run it in R . Maybe you will understand my meaning clearly.
> 
> x <- data.frame(main.name="AAA", x.name=rep(c("Apply","Watermelon","Lemon","Banana",
>                 "Grape","Pineapply","Cherry","Peach","Orange","Mango","Strawberry"),each=5), y.name=(1:55))
> plot(x$x.name, x$y.name)
> 
> I can't find you said package ,whick library should I include in R ?
> 
> Best Regards!
> Ivy Li
> YMS in Production & Testing
> Semiconductor Manufactory International(ShangHai) Corporation
> #18 ZhangJiang Road, PuDong New Area, Shanghai, China
> Tel: 021-5080-2000 *11754
> Email: Ivy_Li at smics.com
> 
> -----
> 
> At 16:17 2004-10-15 +0800, you wrote:
> 
> >[...] I want to rotate the direction of  x-coordinates' letter so that it
> >can show all. But I don't know how to write this option or function .
> 
> I'm not sure if I've understood your question correctly. But if you use
> base graphics and want e.g. rotated labels, you'll achive this rather
> easily using functions in the gridBase package. (Take a look at the package
> vignette for an  example that'll get you started.)
> 
> Henric
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>



From bobby.corpus at gmail.com  Mon Oct 18 05:23:07 2004
From: bobby.corpus at gmail.com (Bobby Corpus)
Date: Mon, 18 Oct 2004 11:23:07 +0800
Subject: =?UTF-8?Q?Re:_=E7=AD=94=E5=A4=8D:_=E7=AD=94=E5=A4=8D:_[R]_?=
	=?UTF-8?Q?R_plot_pro_blems?=
In-Reply-To: <8A910F1818425847A6D18C7832D207E503A6E6@ex115.smic-sh.com>
References: <8A910F1818425847A6D18C7832D207E503A6E6@ex115.smic-sh.com>
Message-ID: <3f1349b04101720233f21b361@mail.gmail.com>

unfortunately, you cannot specify 45 degrees as value for "las".
It only takes the values 0,1,2,3:

0: always parallel to the axis [default],
1: always horizontal,
2: always perpendicular to the axis,
3: always vertical.



On Mon, 18 Oct 2004 10:26:52 +0800, Ivy_Li <ivy_li at smics.com> wrote:
> Yes! You are right ! It is perpendicular to the axis.
>  Thank you very much!
> Could I choose the angle ,such as 45 degree?
> 
> Best Regards!
> Ivy Li
> YMS in Production & Testing
> Semiconductor Manufactory International(ShangHai) Corporation
> #18 ZhangJiang Road, PuDong New Area, Shanghai, China
> Tel: 021-5080-2000 *11754
> Email: Ivy_Li at smics.com
> 
> -----
> : Bobby Corpus [mailto:bobby.corpus at gmail.com]
>  Re: ... [R] R plot problems
> 
> Hi Ivy,
> 
> How about
> 
> x <- data.frame(main.name="AAA",
> x.name=rep(c("Apply","Watermelon","Lemon","Banana",
>                "Grape","Pineapply","Cherry","Peach","Orange","Mango","Strawberry"),each=5),
> y.name=(1:55))
> par(las=2);
> plot(x$x.name, x$y.name)
> 
> Bobby
> 
> On Mon, 18 Oct 2004 09:36:12 +0800, Ivy_Li <ivy_li at smics.com> wrote:
> > Thank you for your help!
> > I gave you an example, you could run it in R . Maybe you will understand my meaning clearly.
> >
> > x <- data.frame(main.name="AAA", x.name=rep(c("Apply","Watermelon","Lemon","Banana",
> >                 "Grape","Pineapply","Cherry","Peach","Orange","Mango","Strawberry"),each=5), y.name=(1:55))
> > plot(x$x.name, x$y.name)
> >
> > I can't find you said package ,whick library should I include in R ?
> >
> > Best Regards!
> > Ivy Li
> > YMS in Production & Testing
> > Semiconductor Manufactory International(ShangHai) Corporation
> > #18 ZhangJiang Road, PuDong New Area, Shanghai, China
> > Tel: 021-5080-2000 *11754
> > Email: Ivy_Li at smics.com
> >
> > -----
> >
> > At 16:17 2004-10-15 +0800, you wrote:
> >
> > >[...] I want to rotate the direction of  x-coordinates' letter so that it
> > >can show all. But I don't know how to write this option or function .
> >
> > I'm not sure if I've understood your question correctly. But if you use
> > base graphics and want e.g. rotated labels, you'll achive this rather
> > easily using functions in the gridBase package. (Take a look at the package
> > vignette for an  example that'll get you started.)
> >
> > Henric
> >
> > ______________________________________________
> > R-help at stat.math.ethz.ch mailing list
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
> >
>



From maustin at amgen.com  Mon Oct 18 05:31:13 2004
From: maustin at amgen.com (Austin, Matt)
Date: Sun, 17 Oct 2004 20:31:13 -0700
Subject: ??: [R] R plot problems
Message-ID: <E7D5AB4811D20B489622AABA9C53859104E0D9DD@teal-exch.amgen.com>

The following should work, note I made x.name a factor.

x <- data.frame(main.name="AAA",
                x.name=factor(rep(c("Apply","Watermelon","Lemon","Banana",
 
"Grape","Pineapply","Cherry","Peach","Orange","Mango","Strawberry"),each=5))
,
                       y.name=(1:55))

plot(x$x.name, x$y.name, axes=FALSE)
axis(1, at=1:length(levels(x$x.name)), lab=FALSE)
axis(2)
box()
text(1:length(levels(x$x.name)), par('usr')[3]-par('cxy')[2]*.5,
levels(x$x.name), offset=1, xpd=TRUE, srt=-45, adj=0)


-----Original Message-----
From: r-help-bounces at stat.math.ethz.ch
[mailto:r-help-bounces at stat.math.ethz.ch]On Behalf Of Ivy_Li
Sent: Sunday, October 17, 2004 19:27 PM
To: Bobby Corpus
Cc: Henric Nilsson; r-help at stat.math.ethz.ch
Subject: ??: ??: [R] R plot problems


Yes! You are right ! It is perpendicular to the axis.
 Thank you very much!
Could I choose the angle ,such as 45 degree?




Best Regards!
Ivy Li?
YMS in Production & Testing
Semiconductor Manufactory International(ShangHai) Corporation
#18 ZhangJiang Road, PuDong New Area, Shanghai, China
Tel: 021-5080-2000 *11754
Email: Ivy_Li at smics.com



-----

Hi Ivy,

How about

x <- data.frame(main.name="AAA",
x.name=rep(c("Apply","Watermelon","Lemon","Banana",
 
"Grape","Pineapply","Cherry","Peach","Orange","Mango","Strawberry"),each=5),
y.name=(1:55))
par(las=2);
plot(x$x.name, x$y.name)

Bobby

On Mon, 18 Oct 2004 09:36:12 +0800, Ivy_Li <ivy_li at smics.com> wrote:
> Thank you for your help!
> I gave you an example, you could run it in R . Maybe you will understand
my meaning clearly.
> 
> x <- data.frame(main.name="AAA",
x.name=rep(c("Apply","Watermelon","Lemon","Banana",
>
"Grape","Pineapply","Cherry","Peach","Orange","Mango","Strawberry"),each=5),
y.name=(1:55))
> plot(x$x.name, x$y.name)
> 
> I can't find you said package ,whick library should I include in R ?
> 
> Best Regards!
> Ivy Li
> YMS in Production & Testing
> Semiconductor Manufactory International(ShangHai) Corporation
> #18 ZhangJiang Road, PuDong New Area, Shanghai, China
> Tel: 021-5080-2000 *11754
> Email: Ivy_Li at smics.com
> 
> ..........
> 
> At 16:17 2004-10-15 +0800, you wrote:
> 
> >[...] I want to rotate the direction of  x-coordinates' letter so that it
> >can show all. But I don't know how to write this option or function .
> 
> I'm not sure if I've understood your question correctly. But if you use
> base graphics and want e.g. rotated labels, you'll achive this rather
> easily using functions in the gridBase package. (Take a look at the
package
> vignette for an  example that'll get you started.)
> 
> Henric
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide!
http://www.R-project.org/posting-guide.html
>

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide!
http://www.R-project.org/posting-guide.html



From ggrothendieck at myway.com  Mon Oct 18 05:44:00 2004
From: ggrothendieck at myway.com (Gabor Grothendieck)
Date: Mon, 18 Oct 2004 03:44:00 +0000 (UTC)
Subject: =?utf-8?b?562U5aSNOg==?= =?utf-8?b?562U5aSNOg==?= [R] R plot
	problems
References: <8A910F1818425847A6D18C7832D207E503A6E6@ex115.smic-sh.com>
Message-ID: <loom.20041018T053845-572@post.gmane.org>


Check out:

  http://maths.newcastle.edu.au/~rking/R/help/02b/2873.html

Ivy_Li <Ivy_Li <at> smics.com> writes:

: 
: Yes! You are right ! It is perpendicular to the axis.
:  Thank you very much!
: Could I choose the angle ,such as 45 degree?
: 
: Best Regards!
: Ivy Li
: YMS in Production & Testing
: Semiconductor Manufactory International(ShangHai) Corporation
: #18 ZhangJiang Road, PuDong New Area, Shanghai, China
: Tel: 021-5080-2000 *11754
: Email: Ivy_Li <at> smics.com
: 
: 
: -----
: 
: 
: Hi Ivy,
: 
: How about
: 
: x <- data.frame(main.name="AAA",
: x.name=rep(c("Apply","Watermelon","Lemon","Banana",
:                "Grape","Pineapply","Cherry","Peach","Orange","Mango","Strawbe
rry"),each=5),
: y.name=(1:55))
: par(las=2);
: plot(x$x.name, x$y.name)
: 
: Bobby
: 
: On Mon, 18 Oct 2004 09:36:12 +0800, Ivy_Li <ivy_li <at> smics.com> wrote:
: > Thank you for your help!
: > I gave you an example, you could run it in R . Maybe you will understand 
my meaning clearly.
: > 
: > x <- data.frame(main.name="AAA", x.name=rep(c
("Apply","Watermelon","Lemon","Banana",
: 
:                 "Grape","Pineapply","Cherry","Peach","Orange","Mango","Strawb
erry"),each=5), y.name=(1:55))
: > plot(x$x.name, x$y.name)
: > 
: > I can't find you said package ,whick library should I include in R ?
: > 
: > Best Regards!
: > Ivy Li
: > YMS in Production & Testing
: > Semiconductor Manufactory International(ShangHai) Corporation
: > #18 ZhangJiang Road, PuDong New Area, Shanghai, China
: > Tel: 021-5080-2000 *11754
: > Email: Ivy_Li <at> smics.com
: > 
: > -----original----
: > 
: > At 16:17 2004-10-15 +0800, you wrote:
: > 
: > >[...] I want to rotate the direction of  x-coordinates' letter so that it
: > >can show all. But I don't know how to write this option or function .
: > 
: > I'm not sure if I've understood your question correctly. But if you use
: > base graphics and want e.g. rotated labels, you'll achive this rather
: > easily using functions in the gridBase package. (Take a look at the package
: > vignette for an  example that'll get you started.)
: > 
: > Henric
: > 
: > ______________________________________________
: > R-help <at> stat.math.ethz.ch mailing list
: > https://stat.ethz.ch/mailman/listinfo/r-help
: > PLEASE do read the posting guide! http://www.R-project.org/posting-
guide.html
: >
: 
: ______________________________________________
: R-help <at> stat.math.ethz.ch mailing list
: https://stat.ethz.ch/mailman/listinfo/r-help
: PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
: 
:



From josh8912 at yahoo.com  Mon Oct 18 06:04:51 2004
From: josh8912 at yahoo.com (JJ)
Date: Sun, 17 Oct 2004 21:04:51 -0700 (PDT)
Subject: [R] manual recreation of varConstPower using new fixed effects
	variables in nlme
Message-ID: <20041018040451.86267.qmail@web51708.mail.yahoo.com>

Hello, I am trying to design new variance structures
by using fixed effects variables in combination with
the VarPower function.  That is, I would like to
create and evaluate my own variance function in the
data frame and then incorporate it into the model
using varPower, with value=.5.  

As a start, I am trying to recreate the function of
VarConstPower by introducing two new variables in the
data frame, d1 and d2. I am using a self-made
function, fx, which contains a logistic equation.  It
all works just fine in combination with the built-in
varConstPower variance structure.  

I try to mimic the varConstPower structure by using
the varPower variance function:
  varPower(form= ~vartemp, fixed= .5)

and then passing the variables d1 and d2 to fx and
adding a statement in fx like:
  vartemp <- (d1+theta^d2)^2
where theta is my covariate.  
  
However, even though the built-in varConstPower
function does work with my data set, the substitute
function discussed above does not work, even with d1
and d2 set to the correct values.   After about five
passes through the fx function I get the error
message:

Error in MEEM(object, conLin, control$niterEM) :
Singularity in backsolve at level 0, block 1

All, fixed effect variables look correct at the time
the error occurs.  Can anyone tell me if there is some
reason why this kind of approach would not work?  It
seems like it should be straightforward, but I am
having no luck.

I actually pass the d1 and d2 variables to the
function as d1.1, d1.2, d1.3, and d2.1, d2.2, d2.3 to
take into account the three grouping levels I have. 
The three d1's are then condensed into a single d1
column with respect to group.  Same with the d2's. 

Thanks.  John

PS:  Can anyone tell me what the reStruct parameter
means?  In looking at the verbose output, I obtain a
reStruct parameter value of -0.8352462 (this is for my
grouping factor for the random effect).  Does anyone
know what it means?  The actual value of my single
random effect is different, as is its variance.



From Ivy_Li at smics.com  Mon Oct 18 06:54:27 2004
From: Ivy_Li at smics.com (Ivy_Li)
Date: Mon, 18 Oct 2004 12:54:27 +0800
Subject: ??: [R] R plot problems
Message-ID: <8A910F1818425847A6D18C7832D207E503A6E8@ex115.smic-sh.com>

About Time labels, I have tried you mentioned method , but It still show oddness number
I have an example . Mybe It  can explain my meaning clearly.
Thank you for helping me!

Time <- c("2004-08-05 09:08:48", "2004-08-13 20:53:38",
  	"2004-08-14 13:57:23", "2004-08-12 16:17:41", 
  	"2004-08-12 16:15:27", "2004-08-11 21:38:24",
  	"2004-08-12 14:28:41", "2004-08-18 18:04:47",
  	"2004-08-13 15:23:14", "2004-08-14 02:36:33")
Time <- as.POSIXlt(Time)
x <- data.frame(main.name="AAA", fruit.name=rep(c("Apply","Watermelon"),each=5), 
		x.name=Time, y.name=(1:10))
plot(as.numeric(x$x.name),as.character(x$y.name),pch=26,)
fruit.class <- table(x$fruit.name)
color.code <- c(611,552,656,121,451,481,28,652,32,550,90,401,150,12,520,8)
for(j in 1:length(fruit.class))
{
	fruit <- names(fruit.class)[j]
	lines(smooth.spline(x[x$fruit.name==fruit, "x.name"],
		x[x$fruit.name==fruit, "y.name"],df=5),
		col=colors()[color.code[j]],lwd=5)
}



Best Regards!
Ivy Li(??)
YMS in Production & Testing
Semiconductor Manufactory International(ShangHai) Corporation
#18 ZhangJiang Road, PuDong New Area, Shanghai, China
Tel: 021-5080-2000 *11754
Email: Ivy_Li at smics.com



-----????-----
???: Jim Lemon [mailto:bitwrit at ozemail.com.au]
????: 2004?10?17? 18:28
???: Ivy_Li
??: Re: [R] R plot problems


Ivy_Li wrote:
> Hello Everyboby:
>         Could I consult everyboby two problems about plot.
>         Recently, I am doing some analysis in plot.
>         Now I can draw boxplot in R , but the result's plot loss some of
> the x-coordinates. I want to rotate the direction of  x-coordinates' letter
> so that it can show all. But I don't know how to write this option or
> function . Or Could you carry out some other good measures to help me?

If you are trying to cram a lot of labels on an axis, the "staxlab" function 
in the "plotrix" package may help you.

> The
> second problem, x-axis in the plot I drawed is time , but the figure in it 
> is very oddness. For example, the time is 2004-9-29, the figure in the plot
> is 1095000000.Could anyone help me to settle these problems. Thank you very
> much!
>
This looks like a date problem. The date is being converted to one of the 
"seconds-since-x" timescales and printed as that value. Try converting your 
original dates to labels using "as.character".

Jim



From Ivy_Li at smics.com  Mon Oct 18 08:26:51 2004
From: Ivy_Li at smics.com (Ivy_Li)
Date: Mon, 18 Oct 2004 14:26:51 +0800
Subject: [R] How to draw x-axis time label.
Message-ID: <8A910F1818425847A6D18C7832D207E502AA13@ex115.smic-sh.com>

Hi everybody,
	Could I consult one problem?
	It is about plot 
	Now I do some analysis in plot . I need to draw a plot which x-axis is time . But when I run the funtion of plot . The x-label are very oddness number, such as the time is "2004-08-05 09:08:48", but the x-label is 1091800000. I don't know how to do . 
	I have an example . Mybe It  can explain my meaning clearly.
	Thank you for helping me!

Time <- c("2004-08-05 09:08:48", "2004-08-13 20:53:38",
  	"2004-08-14 13:57:23", "2004-08-12 16:17:41", 
  	"2004-08-12 16:15:27", "2004-08-11 21:38:24",
  	"2004-08-12 14:28:41", "2004-08-18 18:04:47",
  	"2004-08-13 15:23:14", "2004-08-14 02:36:33")
Time <- as.POSIXlt(Time)
x <- data.frame(main.name="AAA", fruit.name=rep(c("Apply","Watermelon"),each=5), 
		x.name=Time, y.name=(1:10))
plot(as.numeric(x$x.name),as.character(x$y.name),pch=26,)
fruit.class <- table(x$fruit.name)
color.code <- c(611,552,656,121,451,481,28,652,32,550,90,401,150,12,520,8)
for(j in 1:length(fruit.class))
{
	fruit <- names(fruit.class)[j]
	lines(smooth.spline(x[x$fruit.name==fruit, "x.name"],
		x[x$fruit.name==fruit, "y.name"],df=5),
		col=colors()[color.code[j]],lwd=5)
}

Best Regards!
Ivy Li
YMS in Production & Testing
Semiconductor Manufactory International(ShangHai) Corporation
#18 ZhangJiang Road, PuDong New Area, Shanghai, China
Tel: 021-5080-2000 *11754
Email: Ivy_Li at smics.com



From m.mamin at intershop.de  Mon Oct 18 09:03:08 2004
From: m.mamin at intershop.de (Marc Mamin)
Date: Mon, 18 Oct 2004 09:03:08 +0200
Subject: [R] How to draw x-axis time label.
In-Reply-To: <8A910F1818425847A6D18C7832D207E502AA13@ex115.smic-sh.com>
References: <8A910F1818425847A6D18C7832D207E502AA13@ex115.smic-sh.com>
Message-ID: <opsf10jiws19ufgc@mail.intershop.de>

Hello Ivy,
Your scale shows times in seconds since 1970 (I guess it is 1970)

usually, I don't draw the axis with plot (see xaxt="n"), but call axis 
aftewards, giving the ticks positions and the labels as strings, using 
format.

With your example:

format(Time,'%d %b %Y')
  [1] "05 Aug 2004" "13 Aug 2004" "14 Aug 2004" "12 Aug 2004" "12 Aug 2004"
  [6] "11 Aug 2004" "12 Aug 2004" "18 Aug 2004" "13 Aug 2004" "14 Aug 2004"

Cheers,

Marc


On Mon, 18 Oct 2004 14:26:51 +0800, Ivy_Li <Ivy_Li at smics.com> wrote:

> Hi everybody,
> 	Could I consult one problem?
> 	It is about plot
> 	Now I do some analysis in plot . I need to draw a plot which x-axis is 
> time . But when I run the funtion of plot . The x-label are very oddness 
> number, such as the time is "2004-08-05 09:08:48", but the x-label is 
> 1091800000. I don't know how to do .
> 	I have an example . Mybe It  can explain my meaning clearly.
> 	Thank you for helping me!
>
> Time <- c("2004-08-05 09:08:48", "2004-08-13 20:53:38",
>   	"2004-08-14 13:57:23", "2004-08-12 16:17:41",
>   	"2004-08-12 16:15:27", "2004-08-11 21:38:24",
>   	"2004-08-12 14:28:41", "2004-08-18 18:04:47",
>   	"2004-08-13 15:23:14", "2004-08-14 02:36:33")
> Time <- as.POSIXlt(Time)
> x <- data.frame(main.name="AAA", 
> fruit.name=rep(c("Apply","Watermelon"),each=5),
> 		x.name=Time, y.name=(1:10))
> plot(as.numeric(x$x.name),as.character(x$y.name),pch=26,)
> fruit.class <- table(x$fruit.name)
> color.code <- 
> c(611,552,656,121,451,481,28,652,32,550,90,401,150,12,520,8)
> for(j in 1:length(fruit.class))
> {
> 	fruit <- names(fruit.class)[j]
> 	lines(smooth.spline(x[x$fruit.name==fruit, "x.name"],
> 		x[x$fruit.name==fruit, "y.name"],df=5),
> 		col=colors()[color.code[j]],lwd=5)
> }
>
> Best Regards!
> Ivy Li
> YMS in Production & Testing
> Semiconductor Manufactory International(ShangHai) Corporation
> #18 ZhangJiang Road, PuDong New Area, Shanghai, China
> Tel: 021-5080-2000 *11754
> Email: Ivy_Li at smics.com
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html



--



From Ivy_Li at smics.com  Mon Oct 18 11:48:16 2004
From: Ivy_Li at smics.com (Ivy_Li)
Date: Mon, 18 Oct 2004 17:48:16 +0800
Subject: =?utf-8?Q?=E7=AD=94=E5=A4=8D=3A_=5BR=5D_How_to_draw_x-axis_time_?=
	=?utf-8?Q?label=2E?=
Message-ID: <8A910F1818425847A6D18C7832D207E503A6EC@ex115.smic-sh.com>

Thank you for helping me!
I try the "pretty" funtion to select the x-axis position value.Then I use the "format" funtion.

xax.pos <- pretty(as.numeric(x$x.name))
format(xax.pos,'%d %b %y')
> xax.pos
[1] 1091600000 1091800000 1092000000 1092200000 1092400000 1092600000 1092800000
[8] 1093000000

There are something wrong. I found the xax.pos has been changed to the number of second calculated since 1970 ,such as 1091600000.
So this problem was not solved. 

 

Best Regards!
Ivy Li
YMS in Production & Testing
Semiconductor Manufactory International(ShangHai) Corporation
#18 ZhangJiang Road, PuDong New Area, Shanghai, China
Tel: 021-5080-2000 *11754
Email: Ivy_Li at smics.com



-----orig

Hello Ivy,
Your scale shows times in seconds since 1970 (I guess it is 1970)

usually, I don't draw the axis with plot (see xaxt="n"), but call axis 
aftewards, giving the ticks positions and the labels as strings, using 
format.

With your example:

format(Time,'%d %b %Y')
  [1] "05 Aug 2004" "13 Aug 2004" "14 Aug 2004" "12 Aug 2004" "12 Aug 2004"
  [6] "11 Aug 2004" "12 Aug 2004" "18 Aug 2004" "13 Aug 2004" "14 Aug 2004"

Cheers,

Marc


On Mon, 18 Oct 2004 14:26:51 +0800, Ivy_Li <Ivy_Li at smics.com> wrote:

> Hi everybody,
> 	Could I consult one problem?
> 	It is about plot
> 	Now I do some analysis in plot . I need to draw a plot which x-axis is 
> time . But when I run the funtion of plot . The x-label are very oddness 
> number, such as the time is "2004-08-05 09:08:48", but the x-label is 
> 1091800000. I don't know how to do .
> 	I have an example . Mybe It  can explain my meaning clearly.
> 	Thank you for helping me!
>
> Time <- c("2004-08-05 09:08:48", "2004-08-13 20:53:38",
>   	"2004-08-14 13:57:23", "2004-08-12 16:17:41",
>   	"2004-08-12 16:15:27", "2004-08-11 21:38:24",
>   	"2004-08-12 14:28:41", "2004-08-18 18:04:47",
>   	"2004-08-13 15:23:14", "2004-08-14 02:36:33")
> Time <- as.POSIXlt(Time)
> x <- data.frame(main.name="AAA", 
> fruit.name=rep(c("Apply","Watermelon"),each=5),
> 		x.name=Time, y.name=(1:10))
> plot(as.numeric(x$x.name),as.character(x$y.name),pch=26,)
> fruit.class <- table(x$fruit.name)
> color.code <- 
> c(611,552,656,121,451,481,28,652,32,550,90,401,150,12,520,8)
> for(j in 1:length(fruit.class))
> {
> 	fruit <- names(fruit.class)[j]
> 	lines(smooth.spline(x[x$fruit.name==fruit, "x.name"],
> 		x[x$fruit.name==fruit, "y.name"],df=5),
> 		col=colors()[color.code[j]],lwd=5)
> }
>
> Best Regards!
> Ivy Li
> YMS in Production & Testing
> Semiconductor Manufactory International(ShangHai) Corporation
> #18 ZhangJiang Road, PuDong New Area, Shanghai, China
> Tel: 021-5080-2000 *11754
> Email: Ivy_Li at smics.com
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html



-- 
Using M2, Opera's revolutionary e-mail client: http://www.opera.com/m2/



From ripley at stats.ox.ac.uk  Mon Oct 18 12:18:01 2004
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Mon, 18 Oct 2004 11:18:01 +0100 (BST)
Subject: =?utf-8?Q?=E7=AD=94=E5=A4=8D=3A_=5BR=5D_How_to_draw_x-axis_time_?=
	=?utf-8?Q?label=2E?=
In-Reply-To: <8A910F1818425847A6D18C7832D207E503A6EC@ex115.smic-sh.com>
Message-ID: <Pine.LNX.4.44.0410181114360.1156-100000@gannet.stats>

On Mon, 18 Oct 2004, Ivy_Li wrote:

> Thank you for helping me!
> I try the "pretty" funtion to select the x-axis position value.Then I use the "format" funtion.
> 
> xax.pos <- pretty(as.numeric(x$x.name))
> format(xax.pos,'%d %b %y')
> > xax.pos
> [1] 1091600000 1091800000 1092000000 1092200000 1092400000 1092600000 1092800000
> [8] 1093000000
> 
> There are something wrong. I found the xax.pos has been changed to the
> number of second calculated since 1970 ,such as 1091600000. So this
> problem was not solved.

The problem is that _you_ changed this by the use of as.numeric.  Why are 
you changing a time to a number by as.numeric?  And why are you calling a 
time `name' -- it is rather confusing!

If you didn't do that, you would find that plot() by default does 
sensible things, and calling axis.POSIXct will give you added control.  
See its help page.

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From phgrosjean at sciviews.org  Mon Oct 18 12:41:06 2004
From: phgrosjean at sciviews.org (Philippe Grosjean)
Date: Mon, 18 Oct 2004 12:41:06 +0200
Subject: [R] Tinn-R 0.0.9 r1.09 is released
Message-ID: <200410181042.i9IAgpMu467176@hedwig1.umh.ac.be>

Tinn-R is a small (< 2 Mb), but feature-rich, text/code 
editor for Windows.
It is distributed under GPL. Tinn-R contains various tools to 
interact with R (submit code in whole, or line by line; list 
objects; clear the user workspace or graph devices; stop 
current computation; get help on a function, etc...). Among 
general features of Tinn-R: syntax highlighting for R code as 
well as for 20 other languages, bracket matching, project 
management (with multiple files), copy code with syntax 
coloring in RTF, HTML or TeX, etc...
 
Tinn-R is developped by Jos?? Cl??udio Faria 
(mailto:joseclaudio.faria at terra.com.br), Philippe Grosjean
(mailto:phgrosjean at sciviews.org) and contributors. If you 
want to give it a try, or upgrade to the new version, visit 
http://www.sciviews.org/Tinn-R.

Best regards,

Philippe Grosjean

..............................................<??}))><........
 ) ) ) ) )
( ( ( ( (    Prof. Philippe Grosjean
 ) ) ) ) )
( ( ( ( (    Numerical Ecology of Aquatic Systems
 ) ) ) ) )   Mons-Hainaut University, Pentagone
( ( ( ( (    Academie Universitaire Wallonie-Bruxelles
 ) ) ) ) )   6, av du Champ de Mars, 7000 Mons, Belgium
( ( ( ( (
 ) ) ) ) )   phone: + 32.65.37.34.97, fax: + 32.65.37.33.12
( ( ( ( (    email: Philippe.Grosjean at umh.ac.be
 ) ) ) ) )
( ( ( ( (    web:   http://www.umh.ac.be/~econum
 ) ) ) ) )
..............................................................



From nleonard at tartarus.uwa.edu.au  Mon Oct 18 13:37:40 2004
From: nleonard at tartarus.uwa.edu.au (Neil Leonard)
Date: Mon, 18 Oct 2004 19:37:40 +0800
Subject: [R] Recoding factors
Message-ID: <1E47B7DA-20FA-11D9-85CA-003065D5B8EC@tartarus.uwa.edu.au>

I'm having a bit of trouble recoding factors in a fields.

I have a field which has the factors Singleton, Twin and Triplet.
I want to recode the field to have only the factors Singleton and 
Multiple.

Any advice?

Cheers,
Neil



From ligges at statistik.uni-dortmund.de  Mon Oct 18 13:43:58 2004
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Mon, 18 Oct 2004 13:43:58 +0200
Subject: [R] Recoding factors
In-Reply-To: <1E47B7DA-20FA-11D9-85CA-003065D5B8EC@tartarus.uwa.edu.au>
References: <1E47B7DA-20FA-11D9-85CA-003065D5B8EC@tartarus.uwa.edu.au>
Message-ID: <4173AC7E.6020000@statistik.uni-dortmund.de>

Neil Leonard wrote:

> I'm having a bit of trouble recoding factors in a fields.
> 
> I have a field which has the factors Singleton, Twin and Triplet.
> I want to recode the field to have only the factors Singleton and Multiple.
 >
> Any advice?

Depends on what you are going to do with the Twin ...

Uwe Ligges



> Cheers,
> Neil
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html



From nleonard at tartarus.uwa.edu.au  Mon Oct 18 13:45:39 2004
From: nleonard at tartarus.uwa.edu.au (Neil Leonard)
Date: Mon, 18 Oct 2004 19:45:39 +0800
Subject: [R] Recoding factors
In-Reply-To: <4173AC7E.6020000@statistik.uni-dortmund.de>
References: <1E47B7DA-20FA-11D9-85CA-003065D5B8EC@tartarus.uwa.edu.au>
	<4173AC7E.6020000@statistik.uni-dortmund.de>
Message-ID: <3BEF707F-20FB-11D9-85CA-003065D5B8EC@tartarus.uwa.edu.au>

I want Twin and Triplet to both be recoded as Multiple

Neil

On 18/10/2004, at 7:43 PM, Uwe Ligges wrote:

> Neil Leonard wrote:
>
>> I'm having a bit of trouble recoding factors in a fields.
>> I have a field which has the factors Singleton, Twin and Triplet.
>> I want to recode the field to have only the factors Singleton and 
>> Multiple.
> >
>> Any advice?
>
> Depends on what you are going to do with the Twin ...
>
> Uwe Ligges
>
>
>
>> Cheers,
>> Neil
>> ______________________________________________
>> R-help at stat.math.ethz.ch mailing list
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide! 
>> http://www.R-project.org/posting-guide.html
>



From ripley at stats.ox.ac.uk  Mon Oct 18 13:57:03 2004
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Mon, 18 Oct 2004 12:57:03 +0100 (BST)
Subject: [R] Recoding factors
In-Reply-To: <4173AC7E.6020000@statistik.uni-dortmund.de>
Message-ID: <Pine.LNX.4.44.0410181254250.1930-100000@gannet.stats>

On Mon, 18 Oct 2004, Uwe Ligges wrote:

> Neil Leonard wrote:
> 
> > I'm having a bit of trouble recoding factors in a fields.
> > 
> > I have a field which has the factors Singleton, Twin and Triplet.
> > I want to recode the field to have only the factors Singleton and Multiple.
>  >
> > Any advice?
> 
> Depends on what you are going to do with the Twin ...

?levels should help, though.

> x <- factor(c("Singleton", "Twin", "Triplet"))
> levels(x) <- c("Singleton", "Multiple", "Multiple")
> x
[1] Singleton Multiple  Multiple
Levels: Singleton Multiple

might be what is intended.

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From rksh at soc.soton.ac.uk  Mon Oct 18 14:01:04 2004
From: rksh at soc.soton.ac.uk (Robin Hankin)
Date: Mon, 18 Oct 2004 13:01:04 +0100
Subject: [R] concatenating lists elementwise
Message-ID: <a06002006bd995f581d04@[139.166.242.29]>


Hi

How do I concatenate two lists element-by-element?

Example:

list.1 <- list(temperature=c("hot","cold") , size=c("big","medium"))
list.2 <- list(temperature=c("lukewarm")   , size=c("massive","tiny"))


list.wanted  <- list(temperature=c("hot","cold","lukewarm") ,
                       size=c("big","medium","massive","tiny"))



(the lists are dimnames() of an array).


-- 
Robin Hankin
Uncertainty Analyst
Southampton Oceanography Centre
SO14 3ZH
tel +44(0)23-8059-7743
initialDOTsurname at soc.soton.ac.uk (edit in obvious way; spam precaution)



From nleonard at tartarus.uwa.edu.au  Mon Oct 18 14:13:58 2004
From: nleonard at tartarus.uwa.edu.au (Neil Leonard)
Date: Mon, 18 Oct 2004 20:13:58 +0800
Subject: [R] Recoding factors
In-Reply-To: <Pine.LNX.4.44.0410181254250.1930-100000@gannet.stats>
References: <Pine.LNX.4.44.0410181254250.1930-100000@gannet.stats>
Message-ID: <30639205-20FF-11D9-85CA-003065D5B8EC@tartarus.uwa.edu.au>

Yes that's it.

Thanks
Neil

On 18/10/2004, at 7:57 PM, Prof Brian Ripley wrote:

> On Mon, 18 Oct 2004, Uwe Ligges wrote:
>
>> Neil Leonard wrote:
>>
>>> I'm having a bit of trouble recoding factors in a fields.
>>>
>>> I have a field which has the factors Singleton, Twin and Triplet.
>>> I want to recode the field to have only the factors Singleton and 
>>> Multiple.
>>>
>>> Any advice?
>>
>> Depends on what you are going to do with the Twin ...
>
> ?levels should help, though.
>
>> x <- factor(c("Singleton", "Twin", "Triplet"))
>> levels(x) <- c("Singleton", "Multiple", "Multiple")
>> x
> [1] Singleton Multiple  Multiple
> Levels: Singleton Multiple
>
> might be what is intended.
>
> -- 
> Brian D. Ripley,                  ripley at stats.ox.ac.uk
> Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
> University of Oxford,             Tel:  +44 1865 272861 (self)
> 1 South Parks Road,                     +44 1865 272866 (PA)
> Oxford OX1 3TG, UK                Fax:  +44 1865 272595
>



From ripley at stats.ox.ac.uk  Mon Oct 18 14:16:49 2004
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Mon, 18 Oct 2004 13:16:49 +0100 (BST)
Subject: [R] concatenating lists elementwise
In-Reply-To: <a06002006bd995f581d04@[139.166.242.29]>
Message-ID: <Pine.LNX.4.44.0410181314260.1930-100000@gannet.stats>

On Mon, 18 Oct 2004, Robin Hankin wrote:

> How do I concatenate two lists element-by-element?

> Example:
> 
> list.1 <- list(temperature=c("hot","cold") , size=c("big","medium"))
> list.2 <- list(temperature=c("lukewarm")   , size=c("massive","tiny"))
> 
> 
> list.wanted  <- list(temperature=c("hot","cold","lukewarm") ,
>                        size=c("big","medium","massive","tiny"))
> 
> 
> 
> (the lists are dimnames() of an array).

mapply(c, list.1, list.2)

(but it loses the list names, so copy them across).

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From sam.kemp2 at ntlworld.com  Mon Oct 18 14:31:57 2004
From: sam.kemp2 at ntlworld.com (Samuel Kemp)
Date: Mon, 18 Oct 2004 13:31:57 +0100
Subject: [R] nnet learning
Message-ID: <4173B7BD.9050801@ntlworld.com>

Hi,

I am trying to make a neural network learning a "noisy sine wave". 
Suppose I generate my data like so..

x <- seq(-2*pi, 2*pi, length=500)
y <- sin(x) + rnorm(500, sd=sqrt(0.075))

I then train the neural net on the first 400 points using

c <- nnet(as.matrix(x[1:400]),as.matrix(y[1:400]), size=3, maxit=10000, 
abstol=0.075, decay=0.007)

Inspecting the fit of the training set against the actual values using:

pmat<- plot(y[1:400])
lines(c$fitted.values, col="blue", lwd=2)

It seems as though neural net is not learning the negative values. I 
have tried running nnet several times, but each time I get the same 
problem. I have also tried upsampling, but no joy.

I suspect that I am not using nnet correctly. Can anyone provide any 
hints/solutions?

Any help appreciated.

Kind Regards,

Samuel Kemp.



From ripley at stats.ox.ac.uk  Mon Oct 18 14:48:55 2004
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Mon, 18 Oct 2004 13:48:55 +0100 (BST)
Subject: [R] nnet learning
In-Reply-To: <4173B7BD.9050801@ntlworld.com>
Message-ID: <Pine.LNX.4.44.0410181346360.10399-100000@gannet.stats>

On Mon, 18 Oct 2004, Samuel Kemp wrote:

> I am trying to make a neural network learning a "noisy sine wave". 
> Suppose I generate my data like so..
> 
> x <- seq(-2*pi, 2*pi, length=500)
> y <- sin(x) + rnorm(500, sd=sqrt(0.075))
> 
> I then train the neural net on the first 400 points using
> 
> c <- nnet(as.matrix(x[1:400]),as.matrix(y[1:400]), size=3, maxit=10000, 
> abstol=0.075, decay=0.007)
> 
> Inspecting the fit of the training set against the actual values using:
> 
> pmat<- plot(y[1:400])
> lines(c$fitted.values, col="blue", lwd=2)
> 
> It seems as though neural net is not learning the negative values. I 
> have tried running nnet several times, but each time I get the same 
> problem. I have also tried upsampling, but no joy.
> 
> I suspect that I am not using nnet correctly. Can anyone provide any 
> hints/solutions?

Yes, please read the documentation, as the posting guide asks you to.
Hint: it's on p. 246.  Compare your example with that given there.

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From xiyanlon at yahoo.com  Mon Oct 18 15:24:30 2004
From: xiyanlon at yahoo.com (Xiyan Lon)
Date: Mon, 18 Oct 2004 06:24:30 -0700 (PDT)
Subject: [R] why package gregmisc did not work
Message-ID: <20041018132430.46213.qmail@web52002.mail.yahoo.com>

Dear useR

I want to know why package gregmisc did not work very
well. I did not find this package in my library
directory.

> local({a <- CRAN.packages()
+ install.packages(select.list(a[,1],,TRUE),
.libPaths()[1], available=a, dependencies=TRUE)})
trying URL
`http://cran.r-project.org/bin/windows/contrib/2.0/PACKAGES'
Content type `text/plain; charset=iso-8859-1' length
21630 bytes
opened URL
downloaded 21Kb

trying URL
`http://cran.r-project.org/bin/windows/contrib/2.0/gregmisc_2.0.0.zip'
Content type `application/zip' length 687958 bytes
opened URL
downloaded 671Kb

bundle 'gregmisc' successfully unpacked and MD5 sums
checked

Delete downloaded files (y/N)? y

updating HTML package descriptions
> library(gregmisc)
Error in library(gregmisc) : There is no package
called 'gregmisc'

I use windows 2000 Professional and R version 2.0.0
> version
         _              
platform i386-pc-mingw32
arch     i386           
os       mingw32        
system   i386, mingw32  
status                  
major    2              
minor    0.0            
year     2004           
month    10             
day      04             
language R              
> 
Then I check:
http://cran.r-project.org/bin/windows/contrib/checkSummaryWin.html
package gregmisc only warning on R-1.9.1 not R-2.0.0

Best regards,
xiyanlon



From dimitris.rizopoulos at med.kuleuven.ac.be  Mon Oct 18 15:40:03 2004
From: dimitris.rizopoulos at med.kuleuven.ac.be (Dimitris Rizopoulos)
Date: Mon, 18 Oct 2004 15:40:03 +0200
Subject: [R] why package gregmisc did not work
References: <20041018132430.46213.qmail@web52002.mail.yahoo.com>
Message-ID: <002a01c4b517$f8ab2ee0$b2133a86@www.domain>

Hi xiyanlon,

It gives you the reason why `library(gregmisc)' doen't work:

bundle 'gregmisc' successfully unpacked and MD5 sums checked
^^^^^^

So it's no longer a library. Look at

http://tolstoy.newcastle.edu.au/R/packages/04/0056.html

for more info

Best,
Dimitris

----
Dimitris Rizopoulos
Ph.D. Student
Biostatistical Centre
School of Public Health
Catholic University of Leuven

Address: Kapucijnenvoer 35, Leuven, Belgium
Tel: +32/16/396887
Fax: +32/16/337015
Web: http://www.med.kuleuven.ac.be/biostat/
     http://www.student.kuleuven.ac.be/~m0390867/dimitris.htm




----- Original Message ----- 
From: "Xiyan Lon" <xiyanlon at yahoo.com>
To: <r-help at stat.math.ethz.ch>
Sent: Monday, October 18, 2004 3:24 PM
Subject: [R] why package gregmisc did not work


> Dear useR
>
> I want to know why package gregmisc did not work very
> well. I did not find this package in my library
> directory.
>
>> local({a <- CRAN.packages()
> + install.packages(select.list(a[,1],,TRUE),
> .libPaths()[1], available=a, dependencies=TRUE)})
> trying URL
> `http://cran.r-project.org/bin/windows/contrib/2.0/PACKAGES'
> Content type `text/plain; charset=iso-8859-1' length
> 21630 bytes
> opened URL
> downloaded 21Kb
>
> trying URL
> `http://cran.r-project.org/bin/windows/contrib/2.0/gregmisc_2.0.0.zip'
> Content type `application/zip' length 687958 bytes
> opened URL
> downloaded 671Kb
>
> bundle 'gregmisc' successfully unpacked and MD5 sums
> checked
>
> Delete downloaded files (y/N)? y
>
> updating HTML package descriptions
>> library(gregmisc)
> Error in library(gregmisc) : There is no package
> called 'gregmisc'
>
> I use windows 2000 Professional and R version 2.0.0
>> version
>         _
> platform i386-pc-mingw32
> arch     i386
> os       mingw32
> system   i386, mingw32
> status
> major    2
> minor    0.0
> year     2004
> month    10
> day      04
> language R
>>
> Then I check:
> http://cran.r-project.org/bin/windows/contrib/checkSummaryWin.html
> package gregmisc only warning on R-1.9.1 not R-2.0.0
>
> Best regards,
> xiyanlon
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
>



From ripley at stats.ox.ac.uk  Mon Oct 18 15:44:36 2004
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Mon, 18 Oct 2004 14:44:36 +0100 (BST)
Subject: [R] There is no package gregmisc (was why package gregmisc did
	not work)
In-Reply-To: <20041018132430.46213.qmail@web52002.mail.yahoo.com>
Message-ID: <Pine.LNX.4.44.0410181438590.27522-100000@gannet.stats>

As it says very clearly:

   There is no package called 'gregmisc'
               ^^^^^^^

There is a _bundle_ called 'gregmisc', containing packages gtools, gdata, 
gmodels and gplots.  There used to be a package called `gregmisc', but 
there is no more.

See http://cran.r-project.org/src/contrib/Descriptions/gregmisc.html


On Mon, 18 Oct 2004, Xiyan Lon wrote:

> Dear useR
> 
> I want to know why package gregmisc did not work very
> well. I did not find this package in my library
> directory.
> 
> > local({a <- CRAN.packages()
> + install.packages(select.list(a[,1],,TRUE),
> .libPaths()[1], available=a, dependencies=TRUE)})
> trying URL
> `http://cran.r-project.org/bin/windows/contrib/2.0/PACKAGES'
> Content type `text/plain; charset=iso-8859-1' length
> 21630 bytes
> opened URL
> downloaded 21Kb
> 
> trying URL
> `http://cran.r-project.org/bin/windows/contrib/2.0/gregmisc_2.0.0.zip'
> Content type `application/zip' length 687958 bytes
> opened URL
> downloaded 671Kb
> 
> bundle 'gregmisc' successfully unpacked and MD5 sums
> checked
> 
> Delete downloaded files (y/N)? y
> 
> updating HTML package descriptions
> > library(gregmisc)
> Error in library(gregmisc) : There is no package
> called 'gregmisc'

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From redestig at mpimp-golm.mpg.de  Mon Oct 18 15:51:19 2004
From: redestig at mpimp-golm.mpg.de (Henning Redestig)
Date: Mon, 18 Oct 2004 15:51:19 +0200
Subject: [R] Increasing computiation time per column using lapply
Message-ID: <4173CA57.1060207@mpimp-golm.mpg.de>

Hi,

Would be very glad for help on this problem. Using this code:

temp<-function(x, bins, tot) {
   return(as.numeric(lapply(split(x, bins), wtest, tot)));
}

wtest <- function(x, y) {
   return(wilcox.test(x,y)$p.value);
}

rs <- function(x, bins) {
   binCount <- length(split(x[,1], bins));
   tot <- as.numeric(x);
   result<-matrix(apply(x, 2, temp, bins, tot),
          nrow=binCount, byrow=F);
   rownames(result)<-names(split(x[,1], bins));
   colnames(result)<-colnames(x);
   return(result);
}


where x is a matrix and bins is the grouping vector which can be used to 
split every column in x I get

 >rs(x, bins)

to take ~100 s to execute if x has 22000 rows, 2 columns and bins split 
these in to 226 arrays of similar length. Thats all right but, if I 
instead increase to 3 columns it takes ~300 s and with 50 columns it 
takes > 13 h to execute. I can not understand why execution time doesnt 
increase linearly with the amount of columns. Memory status is all fine 
and I never need to start swapping.

I tried to remove the temp function and use a for-loop to iterate over 
the columns instead of using apply but it does not solve my problem.



Thanx!

/Henning, redestig at mpimp-golm.mpg.de



From oscar.moreno at att.net  Mon Oct 18 16:05:47 2004
From: oscar.moreno at att.net (Oscar A. Moreno)
Date: Mon, 18 Oct 2004 10:05:47 -0400
Subject: [R] multiv
Message-ID: <BD9945FB.26B%oscar.moreno@att.net>

What happened to multiv package?  I have installed R 2.0.0 in my Mac and I
could not find this package.  I see other things have been reorganized. I
have read "What's New" and I have used the search facility in the RHelp
panel to no avail.
Oscar A. Moreno
oscar.moreno at att.net



From xiyanlon at yahoo.com  Mon Oct 18 16:12:17 2004
From: xiyanlon at yahoo.com (Xiyan Lon)
Date: Mon, 18 Oct 2004 07:12:17 -0700 (PDT)
Subject: [R] There is no package gregmisc (was why package gregmisc did
	not work)
In-Reply-To: <Pine.LNX.4.44.0410181438590.27522-100000@gannet.stats>
Message-ID: <20041018141217.29606.qmail@web52008.mail.yahoo.com>

Thank to Dimitris and Prof. Brian Ripley.
Now I understand why I did not find package gregmisc
in my library directory.
http://tolstoy.newcastle.edu.au/R/packages/04/0056.html

It is announce a few week ago... smile.

Best wishess,
xiyanlon

--- Prof Brian Ripley <ripley at stats.ox.ac.uk> wrote:

> As it says very clearly:
> 
>    There is no package called 'gregmisc'
>                ^^^^^^^
> 
> There is a _bundle_ called 'gregmisc', containing
> packages gtools, gdata, 
> gmodels and gplots.  There used to be a package
> called `gregmisc', but 
> there is no more.
> 
> See
>
http://cran.r-project.org/src/contrib/Descriptions/gregmisc.html
> 
> 
> On Mon, 18 Oct 2004, Xiyan Lon wrote:
> 
> > Dear useR
> > 
> > I want to know why package gregmisc did not work
> very
> > well. I did not find this package in my library
> > directory.
> > 
> > > local({a <- CRAN.packages()
> > + install.packages(select.list(a[,1],,TRUE),
> > .libPaths()[1], available=a, dependencies=TRUE)})
> > trying URL
> >
>
`http://cran.r-project.org/bin/windows/contrib/2.0/PACKAGES'
> > Content type `text/plain; charset=iso-8859-1'
> length
> > 21630 bytes
> > opened URL
> > downloaded 21Kb
> > 
> > trying URL
> >
>
`http://cran.r-project.org/bin/windows/contrib/2.0/gregmisc_2.0.0.zip'
> > Content type `application/zip' length 687958 bytes
> > opened URL
> > downloaded 671Kb
> > 
> > bundle 'gregmisc' successfully unpacked and MD5
> sums
> > checked
> > 
> > Delete downloaded files (y/N)? y
> > 
> > updating HTML package descriptions
> > > library(gregmisc)
> > Error in library(gregmisc) : There is no package
> > called 'gregmisc'
> 
> -- 
> Brian D. Ripley,                 
> ripley at stats.ox.ac.uk
> Professor of Applied Statistics, 
> http://www.stats.ox.ac.uk/~ripley/
> University of Oxford,             Tel:  +44 1865
> 272861 (self)
> 1 South Parks Road,                     +44 1865
> 272866 (PA)
> Oxford OX1 3TG, UK                Fax:  +44 1865
> 272595
> 
>



From csillery at selway.umt.edu  Mon Oct 18 16:18:55 2004
From: csillery at selway.umt.edu (Katalin Csillery)
Date: Mon, 18 Oct 2004 08:18:55 -0600 (MDT)
Subject: [R] installing package 'kinship'
Message-ID: <Pine.OSF.4.21.0410180804530.230-100000@selway.umt.edu>



Dear All,

I have problem installing the 'kinship' package. I used the
'install.packages' from R command line and had no problem installing other
packages before.
Here is what I get:

* Installing *source* package 'kinship' ...
** libs
gcc -no-cpp-precomp -I/Library/Frameworks/R.framework/Resources/include
-I/usr/local/include   -fno-common  -g -O2 -c agfit6b.c -o agfit6b.o
gcc -no-cpp-precomp -I/Library/Frameworks/R.framework/Resources/include
-I/usr/local/include   -fno-common  -g -O2 -c bdsmatrix_index1.c -o
bdsmatrix_index1.o
gcc -no-cpp-precomp -I/Library/Frameworks/R.framework/Resources/include
-I/usr/local/include   -fno-common  -g -O2 -c bdsmatrix_index2.c -o
bdsmatrix_index2.o
gcc -no-cpp-precomp -I/Library/Frameworks/R.framework/Resources/include
-I/usr/local/include   -fno-common  -g -O2 -c bdsmatrix_index3.c -o
bdsmatrix_index3.o
gcc -no-cpp-precomp -I/Library/Frameworks/R.framework/Resources/include
-I/usr/local/include   -fno-common  -g -O2 -c bdsmatrix_prod.c -o
bdsmatrix_prod.o
gcc -no-cpp-precomp -I/Library/Frameworks/R.framework/Resources/include
-I/usr/local/include   -fno-common  -g -O2 -c bdsmatrix_prod2.c -o
bdsmatrix_prod2.o
gcc -no-cpp-precomp -I/Library/Frameworks/R.framework/Resources/include
-I/usr/local/include   -fno-common  -g -O2 -c bdsmatrix_prod3.c -o
bdsmatrix_prod3.o
gcc -no-cpp-precomp -I/Library/Frameworks/R.framework/Resources/include
-I/usr/local/include   -fno-common  -g -O2 -c chinv4.c -o chinv4.o
gcc -no-cpp-precomp -I/Library/Frameworks/R.framework/Resources/include
-I/usr/local/include   -fno-common  -g -O2 -c chinv5.c -o chinv5.o
gcc -no-cpp-precomp -I/Library/Frameworks/R.framework/Resources/include
-I/usr/local/include   -fno-common  -g -O2 -c cholesky4.c -o cholesky4.o
gcc -no-cpp-precomp -I/Library/Frameworks/R.framework/Resources/include
-I/usr/local/include   -fno-common  -g -O2 -c cholesky5.c -o cholesky5.o
gcc -no-cpp-precomp -I/Library/Frameworks/R.framework/Resources/include
-I/usr/local/include   -fno-common  -g -O2 -c chsolve4.c -o chsolve4.o
gcc -no-cpp-precomp -I/Library/Frameworks/R.framework/Resources/include
-I/usr/local/include   -fno-common  -g -O2 -c chsolve5.c -o chsolve5.o
gcc -no-cpp-precomp -I/Library/Frameworks/R.framework/Resources/include
-I/usr/local/include   -fno-common  -g -O2 -c coxfit6a.c -o coxfit6a.o
gcc -no-cpp-precomp -I/Library/Frameworks/R.framework/Resources/include
-I/usr/local/include   -fno-common  -g -O2 -c coxfit6b.c -o coxfit6b.o
gcc -no-cpp-precomp -I/Library/Frameworks/R.framework/Resources/include
-I/usr/local/include   -fno-common  -g -O2 -c coxfit6c.c -o coxfit6c.o
gcc -no-cpp-precomp -I/Library/Frameworks/R.framework/Resources/include
-I/usr/local/include   -fno-common  -g -O2 -c gchol.c -o gchol.o
gcc -no-cpp-precomp -I/Library/Frameworks/R.framework/Resources/include
-I/usr/local/include   -fno-common  -g -O2 -c gchol_bds.c -o gchol_bds.o
gcc -bundle -flat_namespace -undefined suppress -L/usr/local/lib -o
kinship.so agfit6b.o bdsmatrix_index1.o bdsmatrix_index2.o
bdsmatrix_index3.o bdsmatrix_prod.o bdsmatrix_prod2.o bdsmatrix_prod3.o
chinv4.o chinv5.o cholesky4.o cholesky5.o
chsolve4.o chsolve5.o coxfit6a.o coxfit6b.o coxfit6c.o gchol.o gchol_bds.o
-lcc_dynamic -framework R
ld: multiple definitions of symbol _coxfit6
agfit6b.o definition of _coxfit6 in section (__DATA,__common)
coxfit6a.o definition of _coxfit6 in section (__DATA,__common)
coxfit6b.o definition of _coxfit6 in section (__DATA,__common)
coxfit6c.o definition of _coxfit6 in section (__DATA,__common)
make: *** [kinship.so] Error 1
ERROR: compilation failed for package 'kinship'
** Removing
'/Library/Frameworks/R.framework/Versions/1.9.1/Resources/library/kinship'

Thanks for your help in advance, 
Kati



From kjetil at acelerate.com  Mon Oct 18 16:20:05 2004
From: kjetil at acelerate.com (Kjetil Brinchmann Halvorsen)
Date: Mon, 18 Oct 2004 10:20:05 -0400
Subject: [R] multiv
In-Reply-To: <BD9945FB.26B%oscar.moreno@att.net>
References: <BD9945FB.26B%oscar.moreno@att.net>
Message-ID: <4173D115.1010101@acelerate.com>

Oscar A. Moreno wrote:

>What happened to multiv package?  I have installed R 2.0.0 in my Mac and I
>  
>

multiv is ORPHANED.
If you want it you can go the orphaned subdirectory on CRAN

>could not find this package.  I see other things have been reorganized. I
>have read "What's New" and I have used the search facility in the RHelp
>panel to no avail.
>Oscar A. Moreno
>oscar.moreno at att.net
>
>______________________________________________
>R-help at stat.math.ethz.ch mailing list
>https://stat.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>
>
>  
>


-- 

Kjetil Halvorsen.

Peace is the most effective weapon of mass construction.
               --  Mahdi Elmandjra



From ccleland at optonline.net  Mon Oct 18 13:46:34 2004
From: ccleland at optonline.net (Chuck Cleland)
Date: Mon, 18 Oct 2004 07:46:34 -0400
Subject: [R] Recoding factors
In-Reply-To: <1E47B7DA-20FA-11D9-85CA-003065D5B8EC@tartarus.uwa.edu.au>
References: <1E47B7DA-20FA-11D9-85CA-003065D5B8EC@tartarus.uwa.edu.au>
Message-ID: <4173AD1A.2050608@optonline.net>

   See ?levels.  I think you want something like the following:

levels(myfact) <- c("Singleton", "Multiple", "Multiple")

Neil Leonard wrote:
> I'm having a bit of trouble recoding factors in a fields.
> 
> I have a field which has the factors Singleton, Twin and Triplet.
> I want to recode the field to have only the factors Singleton and Multiple.
> 
> Any advice?


-- 
Chuck Cleland, Ph.D.
NDRI, Inc.
71 West 23rd Street, 8th floor
New York, NY 10010
tel: (212) 845-4495 (Tu, Th)
tel: (732) 452-1424 (M, W, F)
fax: (917) 438-0894



From i.visser at uva.nl  Mon Oct 18 16:31:09 2004
From: i.visser at uva.nl (Ingmar Visser)
Date: Mon, 18 Oct 2004 16:31:09 +0200
Subject: [R] meta.summaries and se's of effect sizes
Message-ID: <BD99A04D.8DA8%i.visser@uva.nl>

Hi All,

I would like to use meta.summaries from package rmeta to do a meta-analysis.
I have available effect sizes as r's (which could be easily transformed to
effect sizes in terms of d's).

My problem is that I'm not sure what the se's of these r's should be ...

The r-values are themselves computed from F-tests and t-tests for various
studies. 

Are there R-functions that do meta-anlysis on r-values instead of d-values?

regards, ingmar



From ripley at stats.ox.ac.uk  Mon Oct 18 16:47:27 2004
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Mon, 18 Oct 2004 15:47:27 +0100 (BST)
Subject: [R] multiv
In-Reply-To: <BD9945FB.26B%oscar.moreno@att.net>
Message-ID: <Pine.LNX.4.44.0410181543310.13863-100000@gannet.stats>

This is a CRAN question, perhaps best addressed to the CRAN maintainers.
Did you check on CRAN?  Its source exists in the Archive, and it was
orphaned a while back.  I am not sure why it is not in the Orphaned area: 
probably timing.

On Mon, 18 Oct 2004, Oscar A. Moreno wrote:

> What happened to multiv package?  I have installed R 2.0.0 in my Mac and I
> could not find this package.  I see other things have been reorganized. I
> have read "What's New" and I have used the search facility in the RHelp
> panel to no avail.
> Oscar A. Moreno
> oscar.moreno at att.net

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From ripley at stats.ox.ac.uk  Mon Oct 18 16:54:16 2004
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Mon, 18 Oct 2004 15:54:16 +0100 (BST)
Subject: [R] installing package 'kinship'
In-Reply-To: <Pine.OSF.4.21.0410180804530.230-100000@selway.umt.edu>
Message-ID: <Pine.LNX.4.44.0410181547560.13863-100000@gannet.stats>

Please read the posting guide, and do tell us what OS this is.

I can guess it is MacOS X, an OS on which quite a number of packages do
not compile.  You need to change coxfit6.h to have the object extern in
all but one of the files it is included in.  That is a peculiarity of
MacOS X.


On Mon, 18 Oct 2004, Katalin  Csillery wrote:

> 
> 
> Dear All,
> 
> I have problem installing the 'kinship' package. I used the
> 'install.packages' from R command line and had no problem installing other
> packages before.
> Here is what I get:
> 
> * Installing *source* package 'kinship' ...
> ** libs
> gcc -no-cpp-precomp -I/Library/Frameworks/R.framework/Resources/include
> -I/usr/local/include   -fno-common  -g -O2 -c agfit6b.c -o agfit6b.o
> gcc -no-cpp-precomp -I/Library/Frameworks/R.framework/Resources/include
> -I/usr/local/include   -fno-common  -g -O2 -c bdsmatrix_index1.c -o
> bdsmatrix_index1.o
> gcc -no-cpp-precomp -I/Library/Frameworks/R.framework/Resources/include
> -I/usr/local/include   -fno-common  -g -O2 -c bdsmatrix_index2.c -o
> bdsmatrix_index2.o
> gcc -no-cpp-precomp -I/Library/Frameworks/R.framework/Resources/include
> -I/usr/local/include   -fno-common  -g -O2 -c bdsmatrix_index3.c -o
> bdsmatrix_index3.o
> gcc -no-cpp-precomp -I/Library/Frameworks/R.framework/Resources/include
> -I/usr/local/include   -fno-common  -g -O2 -c bdsmatrix_prod.c -o
> bdsmatrix_prod.o
> gcc -no-cpp-precomp -I/Library/Frameworks/R.framework/Resources/include
> -I/usr/local/include   -fno-common  -g -O2 -c bdsmatrix_prod2.c -o
> bdsmatrix_prod2.o
> gcc -no-cpp-precomp -I/Library/Frameworks/R.framework/Resources/include
> -I/usr/local/include   -fno-common  -g -O2 -c bdsmatrix_prod3.c -o
> bdsmatrix_prod3.o
> gcc -no-cpp-precomp -I/Library/Frameworks/R.framework/Resources/include
> -I/usr/local/include   -fno-common  -g -O2 -c chinv4.c -o chinv4.o
> gcc -no-cpp-precomp -I/Library/Frameworks/R.framework/Resources/include
> -I/usr/local/include   -fno-common  -g -O2 -c chinv5.c -o chinv5.o
> gcc -no-cpp-precomp -I/Library/Frameworks/R.framework/Resources/include
> -I/usr/local/include   -fno-common  -g -O2 -c cholesky4.c -o cholesky4.o
> gcc -no-cpp-precomp -I/Library/Frameworks/R.framework/Resources/include
> -I/usr/local/include   -fno-common  -g -O2 -c cholesky5.c -o cholesky5.o
> gcc -no-cpp-precomp -I/Library/Frameworks/R.framework/Resources/include
> -I/usr/local/include   -fno-common  -g -O2 -c chsolve4.c -o chsolve4.o
> gcc -no-cpp-precomp -I/Library/Frameworks/R.framework/Resources/include
> -I/usr/local/include   -fno-common  -g -O2 -c chsolve5.c -o chsolve5.o
> gcc -no-cpp-precomp -I/Library/Frameworks/R.framework/Resources/include
> -I/usr/local/include   -fno-common  -g -O2 -c coxfit6a.c -o coxfit6a.o
> gcc -no-cpp-precomp -I/Library/Frameworks/R.framework/Resources/include
> -I/usr/local/include   -fno-common  -g -O2 -c coxfit6b.c -o coxfit6b.o
> gcc -no-cpp-precomp -I/Library/Frameworks/R.framework/Resources/include
> -I/usr/local/include   -fno-common  -g -O2 -c coxfit6c.c -o coxfit6c.o
> gcc -no-cpp-precomp -I/Library/Frameworks/R.framework/Resources/include
> -I/usr/local/include   -fno-common  -g -O2 -c gchol.c -o gchol.o
> gcc -no-cpp-precomp -I/Library/Frameworks/R.framework/Resources/include
> -I/usr/local/include   -fno-common  -g -O2 -c gchol_bds.c -o gchol_bds.o
> gcc -bundle -flat_namespace -undefined suppress -L/usr/local/lib -o
> kinship.so agfit6b.o bdsmatrix_index1.o bdsmatrix_index2.o
> bdsmatrix_index3.o bdsmatrix_prod.o bdsmatrix_prod2.o bdsmatrix_prod3.o
> chinv4.o chinv5.o cholesky4.o cholesky5.o
> chsolve4.o chsolve5.o coxfit6a.o coxfit6b.o coxfit6c.o gchol.o gchol_bds.o
> -lcc_dynamic -framework R
> ld: multiple definitions of symbol _coxfit6
> agfit6b.o definition of _coxfit6 in section (__DATA,__common)
> coxfit6a.o definition of _coxfit6 in section (__DATA,__common)
> coxfit6b.o definition of _coxfit6 in section (__DATA,__common)
> coxfit6c.o definition of _coxfit6 in section (__DATA,__common)
> make: *** [kinship.so] Error 1
> ERROR: compilation failed for package 'kinship'
> ** Removing
> '/Library/Frameworks/R.framework/Versions/1.9.1/Resources/library/kinship'
> 
> Thanks for your help in advance, 
> Kati
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
> 
> 

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From bates at stat.wisc.edu  Mon Oct 18 17:14:25 2004
From: bates at stat.wisc.edu (Douglas Bates)
Date: Mon, 18 Oct 2004 10:14:25 -0500
Subject: [R] manual recreation of varConstPower using new fixed effects
	variables in nlme
In-Reply-To: <20041018040451.86267.qmail@web51708.mail.yahoo.com>
References: <20041018040451.86267.qmail@web51708.mail.yahoo.com>
Message-ID: <4173DDD1.7010202@stat.wisc.edu>

> PS:  Can anyone tell me what the reStruct parameter
> means?  In looking at the verbose output, I obtain a
> reStruct parameter value of -0.8352462 (this is for my
> grouping factor for the random effect).  Does anyone
> know what it means?  The actual value of my single
> random effect is different, as is its variance.

IIRC it is the logarithm of the relative precision of the random effects.



From tlumley at u.washington.edu  Mon Oct 18 17:28:57 2004
From: tlumley at u.washington.edu (Thomas Lumley)
Date: Mon, 18 Oct 2004 08:28:57 -0700 (PDT)
Subject: [R] Testing for normality of residuals in a regression model
In-Reply-To: <41702723.9050404@acelerate.com>
References: <3A822319EB35174CA3714066D590DCD504AF8554@usrymx25.merck.com>
	<41702723.9050404@acelerate.com>
Message-ID: <Pine.A41.4.61a.0410180827130.319578@homer08.u.washington.edu>

On Fri, 15 Oct 2004, Kjetil Brinchmann Halvorsen wrote:

> Liaw, Andy wrote:
>> Also, I was told by someone very smart that fitting OLS to data with
>> heteroscedastic errors can make the residuals look `more normal' than they
>> really are...  Don't know how true that is, though. 
> Certainly true, since the residuals will be a kind of average, so the CLT 
> works.

[Inserting some R content into the discussion]
An example of this can be seen by running qqnorm on the residuals from the 
Anscombe quartet of data sets (data(anscombe)).

 	-thomas



From 0034058 at fudan.edu.cn  Mon Oct 18 17:36:05 2004
From: 0034058 at fudan.edu.cn (rongguiwong)
Date: Mon, 18 Oct 2004 23:36:05 +0800
Subject: [R] how to study the code of R
Message-ID: <200410182336.05941.0034058@fudan.edu.cn>

i want to study R programming by studying the existing code from R itself,but 
i don't know how to read the code,can any one give me some guide?
my R is installed in /usr/lib/R/

[ronggui at mylinux ronggui]$ /usr/lib/R/
afm      bin      doc      etc      include  library  modules  share


> version
         _
platform i586-mandrake-linux-gnu
arch     i586
os       linux-gnu
system   i586, linux-gnu
status
major    1
minor    9.1
year     2004
month    06
day      21
language R
>



From bill.shipley at usherbrooke.ca  Mon Oct 18 17:41:45 2004
From: bill.shipley at usherbrooke.ca (Bill Shipley)
Date: Mon, 18 Oct 2004 11:41:45 -0400
Subject: [R] inferential test for smoother df?
Message-ID: <001501c4b528$f94e8b10$8c1ad284@BIO041>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20041018/1ebeecb9/attachment.pl

From tlumley at u.washington.edu  Mon Oct 18 17:48:40 2004
From: tlumley at u.washington.edu (Thomas Lumley)
Date: Mon, 18 Oct 2004 08:48:40 -0700 (PDT)
Subject: [R] Survreg with gamma distribution
In-Reply-To: <002001c4b458$b4ebf1e0$0101a8c0@eureka>
References: <200410171355.i9HDtraR022126@outmx022.isp.belgacom.be>
	<002001c4b458$b4ebf1e0$0101a8c0@eureka>
Message-ID: <Pine.A41.4.61a.0410180837430.319578@homer08.u.washington.edu>

On Sun, 17 Oct 2004, Kuan-Ta Chen wrote:

> Hi, all:
>
> I find survreg {survival} has provided many distributions such as weibull,
> lognormal, etc. But I wonder why it doesn't have the support for gamma
> distribution since it should be a good distr. in lifetime analysis. Can
> anybody figure out the reason?

Presumably the actual reason is because Terry Therneau didn't need to use 
the Gamma model.  However, all the distributions in survreg are 
location-scale families, which the Gamma is not, so the basic algorithm 
would have to be different.

> I've tried to implement the likelihood function of progressively censored
> data for gamma distr. and use optim() to solve the paramemters. The
> log-likelihood function L contains some integrations.

It shouldn't have to: we do have pgamma() built in (and digamma, trigamma, 
etc for derivatives).

>							I use tryCatch() to
> capture the error when integration lead to divergence and return Inf.
> But if consequent two calls to the objective function return Inf, optim()
> will raise errors:
>
> Error in optim(c(ga, 1/la), fr, method = "BFGS") :
>        non-finite finite-difference value [1]
>
> What can I do except for choosing better initial values?

Choose better initial values.  You should be able to get quite good 
initial values for regression coefficients by using survreg on a lognormal 
distribution, since Gamma and lognormal distributions agree pretty well 
except in the extreme tails.  You could then try getting the shape 
parameter by matching the variance of the Gamma to the variance of the 
fitted lognormal.

> The last question, by its name "survreg", survreg does its job by
> regression,
> but why p.75 in Tableman, Kim (2004) said that "We use the S function
> survReg to fit parametric models (with the MLE approach)...". Does survreg
> use regression or MLE approach?

Its job *is* regression. It uses maximum likelihood to fit a regression 
model.

 	-thomas



From reid_huntsinger at merck.com  Mon Oct 18 17:58:39 2004
From: reid_huntsinger at merck.com (Huntsinger, Reid)
Date: Mon, 18 Oct 2004 11:58:39 -0400
Subject: [R] concatenating lists elementwise
Message-ID: <D9A95B4B7B20354992E165EEADA31999056A9144@uswpmx00.merck.com>

I don't know of a way other than brute force:

> list.3 <- list()
> for (i in union(names(list.1),names(list.2))) 
      list.3[[i]] <- union(list.1[[i]],list.2[[i]])
> list.3
$temperature
[1] "hot"      "cold"     "lukewarm"

$size
[1] "big"     "medium"  "massive" "tiny"   

I suppose one could write a method for "merge" for lists.

Reid Huntsinger

-----Original Message-----
From: r-help-bounces at stat.math.ethz.ch
[mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Robin Hankin
Sent: Monday, October 18, 2004 8:01 AM
To: r-help at stat.math.ethz.ch
Subject: [R] concatenating lists elementwise



Hi

How do I concatenate two lists element-by-element?

Example:

list.1 <- list(temperature=c("hot","cold") , size=c("big","medium"))
list.2 <- list(temperature=c("lukewarm")   , size=c("massive","tiny"))


list.wanted  <- list(temperature=c("hot","cold","lukewarm") ,
                       size=c("big","medium","massive","tiny"))



(the lists are dimnames() of an array).


-- 
Robin Hankin
Uncertainty Analyst
Southampton Oceanography Centre
SO14 3ZH
tel +44(0)23-8059-7743
initialDOTsurname at soc.soton.ac.uk (edit in obvious way; spam precaution)

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide!
http://www.R-project.org/posting-guide.html



From tlumley at u.washington.edu  Mon Oct 18 18:01:43 2004
From: tlumley at u.washington.edu (Thomas Lumley)
Date: Mon, 18 Oct 2004 09:01:43 -0700 (PDT)
Subject: [R] meta.summaries and se's of effect sizes
In-Reply-To: <BD99A04D.8DA8%i.visser@uva.nl>
References: <BD99A04D.8DA8%i.visser@uva.nl>
Message-ID: <Pine.A41.4.61a.0410180855180.319578@homer08.u.washington.edu>

On Mon, 18 Oct 2004, Ingmar Visser wrote:

> Hi All,
>
> I would like to use meta.summaries from package rmeta to do a meta-analysis.
> I have available effect sizes as r's (which could be easily transformed to
> effect sizes in terms of d's).
>
> My problem is that I'm not sure what the se's of these r's should be ...
>

I'm not sure even what r-values and d-values are in this context.

meta.summaries takes a number of independent estimates of the same 
quantity together with standard errors and combines them to get a single 
estimate. It doesn't care what the quantity is.

 	-thomas



From tlumley at u.washington.edu  Mon Oct 18 18:14:49 2004
From: tlumley at u.washington.edu (Thomas Lumley)
Date: Mon, 18 Oct 2004 09:14:49 -0700 (PDT)
Subject: [R] sapply and loop
In-Reply-To: <BAY22-F16KtKufVma0o00002e37@hotmail.com>
References: <BAY22-F16KtKufVma0o00002e37@hotmail.com>
Message-ID: <Pine.A41.4.61a.0410180905030.319578@homer08.u.washington.edu>

On Sat, 16 Oct 2004, Zhen Pang wrote:

> Dear all,
>
> I am doing 200 times simulation. For each time, I generate a matrix and 
> define some function on this matrix to get a 6 dimension vector as my 
> results.
>
> As the loop should be slow, I generate 200 matrice first, and save them into 
> a list named ma,
> then I define zz<-sapply(ma, myfunction)
>
> To my surprise, It almost costs me the same time to get my results if I 
> directly use a loop from 1 to 200. Is it common? Can I improve any further?
>

It is quite common for a loop to be as fast as sapply(). After all, 
sapply() still has to run `myfunction' 200 times, and this is what takes 
most of the time, so there isn't any obvious reason why sapply() should be 
much faster.  sapply() and lapply() certainly can be faster than loops, 
but usually not by very much.

The surprising fact is that so many people *believe* the apply() functions 
are typically much faster than loops. It's probably a useful belief, since 
it encourages people to learn to use them (and they sometimes are faster).

You should try using Rprof() to find out which parts of your code are 
slow.

 	-thomas



From dsonneborn at ucdavis.edu  Mon Oct 18 18:33:35 2004
From: dsonneborn at ucdavis.edu (Dean Sonneborn)
Date: Mon, 18 Oct 2004 09:33:35 -0700
Subject: [R] x is not a open/high/low/close time series
Message-ID: <6.1.0.6.2.20041018092910.01b8b670@yellow.ucdavis.edu>


I am trying to create a high low close style chart but I keep getting the 
following error statement; "x is not a open/high/low/close time 
series".  I've used the function is.ts and R responds TRUE but the 
ohlcPlot  function gives the above error statement. I'm actually planning 
to plot some odds ratios with their confidence intervals and the 
hi-low-close chart should do the job if I can just figure out what the 
ohlcPlot  function is expecting the time series data to look like.
Thanks for the help
Dean Sonneborn



From kuan at ilife.cx  Mon Oct 18 19:01:41 2004
From: kuan at ilife.cx (Kuan-Ta Chen)
Date: Tue, 19 Oct 2004 01:01:41 +0800
Subject: [R] Survreg with gamma distribution
References: <200410171355.i9HDtraR022126@outmx022.isp.belgacom.be>
	<002001c4b458$b4ebf1e0$0101a8c0@eureka>
	<Pine.A41.4.61a.0410180837430.319578@homer08.u.washington.edu>
Message-ID: <004e01c4b534$24ae0e10$0101a8c0@eureka>


One million thanks to Prof. Ripley and Prof. Lumley. I think I now have more
understanding regarding survreg with gamma distribution. But one of my
problems is still there: in the text of Lee, Wang (2003), there are two
"kinds" of parametric fitting: 1) fitting of survival distributions (like
regular probabillity distribution fitting) 2) regression model fitting
(mostly assume an accelerated failure time model). Survreg {survival}
provides model fitting of (2). But I still have one problem regarding (1):
try to estimate the parameters of gamma distributions for some data.

For regular gamma distr. fitting, we could use fitdistr (mass) or use
optim()/mle() with log-likelihood composed by dgamma()/pgamma(). But because
the data contains (randomly) censored observations, the log-likelihood
function must be modified to include the effect of duration of censored
observations. To clarify, I've excerpted the log-likelihood function and two
equations of gamma and lambda by taking the first derivation. But
unfortunately, but the loglik function and equations contain integrations
and I can't analytically eliminate them. That's the reason why I used
integration in optim() and always got errors (since I don't have clues to
handle divergent integration.)

The excerpt is from Lee, Wang (2003) p.193 (sorry I don't have another way
to show the complicated equations):
http://kuan.ilife.cx/gammamle.jpg The authors suggest using numerical method
to solve the equation and I don't have any idea to eliminate the
integrations from these equations before optim(). Please give me some hint,
thanks.

Lee, Wang (2003): Elisa T. Lee, John Wenyu Wang, Statistical Methods for
Survival Data Analysis, 3rd edition, 2003

Best regards,
Kuan-Ta Chen

----- Original Message ----- 
From: "Thomas Lumley" <tlumley at u.washington.edu>
To: "Kuan-Ta Chen" <kuan at ilife.cx>
Cc: <r-help at stat.math.ethz.ch>
Sent: Monday, October 18, 2004 11:48 PM
Subject: Re: [R] Survreg with gamma distribution


> On Sun, 17 Oct 2004, Kuan-Ta Chen wrote:
>
> > Hi, all:
> >
> > I find survreg {survival} has provided many distributions such as
weibull,
> > lognormal, etc. But I wonder why it doesn't have the support for gamma
> > distribution since it should be a good distr. in lifetime analysis. Can
> > anybody figure out the reason?
>
> Presumably the actual reason is because Terry Therneau didn't need to use
> the Gamma model.  However, all the distributions in survreg are
> location-scale families, which the Gamma is not, so the basic algorithm
> would have to be different.
>
> > I've tried to implement the likelihood function of progressively
censored
> > data for gamma distr. and use optim() to solve the paramemters. The
> > log-likelihood function L contains some integrations.
>
> It shouldn't have to: we do have pgamma() built in (and digamma, trigamma,
> etc for derivatives).
>
> > I use tryCatch() to
> > capture the error when integration lead to divergence and return Inf.
> > But if consequent two calls to the objective function return Inf,
optim()
> > will raise errors:
> >
> > Error in optim(c(ga, 1/la), fr, method = "BFGS") :
> >        non-finite finite-difference value [1]
> >
> > What can I do except for choosing better initial values?
>
> Choose better initial values.  You should be able to get quite good
> initial values for regression coefficients by using survreg on a lognormal
> distribution, since Gamma and lognormal distributions agree pretty well
> except in the extreme tails.  You could then try getting the shape
> parameter by matching the variance of the Gamma to the variance of the
> fitted lognormal.
>
> > The last question, by its name "survreg", survreg does its job by
> > regression,
> > but why p.75 in Tableman, Kim (2004) said that "We use the S function
> > survReg to fit parametric models (with the MLE approach)...". Does
survreg
> > use regression or MLE approach?
>
> Its job *is* regression. It uses maximum likelihood to fit a regression
> model.
>
>   -thomas
>



From murdoch at stats.uwo.ca  Mon Oct 18 19:29:39 2004
From: murdoch at stats.uwo.ca (Duncan Murdoch)
Date: Mon, 18 Oct 2004 13:29:39 -0400
Subject: [R] how to study the code of R
In-Reply-To: <200410182336.05941.0034058@fudan.edu.cn>
References: <200410182336.05941.0034058@fudan.edu.cn>
Message-ID: <u9v7n0lkkjg116hsc0petcghvm2qrtun8r@4ax.com>

On Mon, 18 Oct 2004 23:36:05 +0800, rongguiwong <0034058 at fudan.edu.cn>
wrote :

>i want to study R programming by studying the existing code from R itself,but 
>i don't know how to read the code,can any one give me some guide?
>my R is installed in /usr/lib/R/
>
>[ronggui at mylinux ronggui]$ /usr/lib/R/
>afm      bin      doc      etc      include  library  modules  share

You can see some of it by typing a function name, but to see it all,
you need a source installation, which you don't have (the source is
mostly in the src subdirectory).  You can download one from
http://cran.r-project.org.

Duncan Murdoch



From tlumley at u.washington.edu  Mon Oct 18 19:37:45 2004
From: tlumley at u.washington.edu (Thomas Lumley)
Date: Mon, 18 Oct 2004 10:37:45 -0700 (PDT)
Subject: [R] Survreg with gamma distribution
In-Reply-To: <004e01c4b534$24ae0e10$0101a8c0@eureka>
References: <200410171355.i9HDtraR022126@outmx022.isp.belgacom.be>
	<002001c4b458$b4ebf1e0$0101a8c0@eureka>
	<Pine.A41.4.61a.0410180837430.319578@homer08.u.washington.edu>
	<004e01c4b534$24ae0e10$0101a8c0@eureka>
Message-ID: <Pine.A41.4.61a.0410181020030.135076@homer08.u.washington.edu>

On Tue, 19 Oct 2004, Kuan-Ta Chen wrote:

>
> One million thanks to Prof. Ripley and Prof. Lumley. I think I now have more
> understanding regarding survreg with gamma distribution. But one of my
> problems is still there: in the text of Lee, Wang (2003), there are two
> "kinds" of parametric fitting: 1) fitting of survival distributions (like
> regular probabillity distribution fitting) 2) regression model fitting
> (mostly assume an accelerated failure time model). Survreg {survival}
> provides model fitting of (2). But I still have one problem regarding (1):
> try to estimate the parameters of gamma distributions for some data.

There aren't really two separate kinds: 1 is a special case of 2, so 
survreg() can do 1.

> For regular gamma distr. fitting, we could use fitdistr (mass) or use
> optim()/mle() with log-likelihood composed by dgamma()/pgamma(). But because
> the data contains (randomly) censored observations, the log-likelihood
> function must be modified to include the effect of duration of censored
> observations.

Yes. The loglikelihood is
   pgamma(x,shape,scale=scale,lower.tail=FALSE,log.p=TRUE)
for a censored observation and
   dgamma(x,shape,scale=scale,log=TRUE)
for an uncensored observation. No integration necessary.

You might want to work with log(shape) and log(scale) instead, to avoid 
the boundaries at 0.

eg if your data were in variables "times" and "status"
ll <-function(logshape,logscale){
 	-sum(ifelse(status,
 		pgamma(times,exp(logshape),scale=exp(logscale),
 			lower.tail=FALSE,log.p=TRUE),
 		dgamma(times,exp(logshape),scale=exp(logscale),log=TRUE)
 	     ))
}

This works in mle() without too much sensitivity to starting values.

 	-thomas



From edd at debian.org  Mon Oct 18 20:03:21 2004
From: edd at debian.org (Dirk Eddelbuettel)
Date: Mon, 18 Oct 2004 13:03:21 -0500
Subject: [R] x is not a open/high/low/close time series
In-Reply-To: <6.1.0.6.2.20041018092910.01b8b670@yellow.ucdavis.edu>
References: <6.1.0.6.2.20041018092910.01b8b670@yellow.ucdavis.edu>
Message-ID: <20041018180321.GA6429@sonny.eddelbuettel.com>

On Mon, Oct 18, 2004 at 09:33:35AM -0700, Dean Sonneborn wrote:
> 
> I am trying to create a high low close style chart but I keep getting the 
> following error statement; "x is not a open/high/low/close time 
> series".  I've used the function is.ts and R responds TRUE but the 
> ohlcPlot  function gives the above error statement. I'm actually planning 
> to plot some odds ratios with their confidence intervals and the 
> hi-low-close chart should do the job if I can just figure out what the 
> ohlcPlot  function is expecting the time series data to look like.

If you do 'example(plotOHLC)', an object x is created, and a subsequent
'class(x)' tells you that it is of class 'mts' -- a multiple timeseries
object.

Similar code and functionality using another data type is in the its package
on CRAN. You may find that easier to use. Depending on your R skills, you
can also factor out the OHLC plotting code and apply to data.frames or other
data containers.

Hth, Dirk

-- 
Those are my principles, and if you don't like them... well, I have others.
                                                -- Groucho Marx



From rpeng at jhsph.edu  Mon Oct 18 20:44:03 2004
From: rpeng at jhsph.edu (Roger D. Peng)
Date: Mon, 18 Oct 2004 14:44:03 -0400
Subject: [R] how to study the code of R
In-Reply-To: <200410182336.05941.0034058@fudan.edu.cn>
References: <200410182336.05941.0034058@fudan.edu.cn>
Message-ID: <41740EF3.6030304@jhsph.edu>

You need to download the full source code for R.  The latest version 
(2.0.0) is available at

http://cran.r-project.org/src/base/R-2/R-2.0.0.tar.gz

-roger

rongguiwong wrote:
> i want to study R programming by studying the existing code from R itself,but 
> i don't know how to read the code,can any one give me some guide?
> my R is installed in /usr/lib/R/
> 
> [ronggui at mylinux ronggui]$ /usr/lib/R/
> afm      bin      doc      etc      include  library  modules  share
> 
> 
> 
>>version
> 
>          _
> platform i586-mandrake-linux-gnu
> arch     i586
> os       linux-gnu
> system   i586, linux-gnu
> status
> major    1
> minor    9.1
> year     2004
> month    06
> day      21
> language R
> 
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
> 

-- 
Roger D. Peng
http://www.biostat.jhsph.edu/~rpeng/



From liuwensui at gmail.com  Mon Oct 18 20:52:16 2004
From: liuwensui at gmail.com (Wensui Liu)
Date: Mon, 18 Oct 2004 14:52:16 -0400
Subject: [R] R crashes while running RandomForest
Message-ID: <1115a2b004101811527d83d621@mail.gmail.com>

I tried to use RF build a model with a continuous variable as the
dependent. However, whenever I ran it, R just crashed.

But when I catigorize the dependent variable as a binary variable and
ran the same model again. It just works.

Is there anyone who can help me out? I really appreciate it.



From gb at stat.umu.se  Mon Oct 18 21:10:49 2004
From: gb at stat.umu.se (Gran Brostrm)
Date: Mon, 18 Oct 2004 21:10:49 +0200
Subject: [R] Survreg with gamma distribution
In-Reply-To: <Pine.A41.4.61a.0410180837430.319578@homer08.u.washington.edu>
References: <200410171355.i9HDtraR022126@outmx022.isp.belgacom.be>
	<002001c4b458$b4ebf1e0$0101a8c0@eureka>
	<Pine.A41.4.61a.0410180837430.319578@homer08.u.washington.edu>
Message-ID: <20041018191049.GA15239@stat.umu.se>

On Mon, Oct 18, 2004 at 08:48:40AM -0700, Thomas Lumley wrote:
> On Sun, 17 Oct 2004, Kuan-Ta Chen wrote:
> 
> >Hi, all:
> >
> >I find survreg {survival} has provided many distributions such as weibull,
> >lognormal, etc. But I wonder why it doesn't have the support for gamma
> >distribution since it should be a good distr. in lifetime analysis. Can
> >anybody figure out the reason?
> 
> Presumably the actual reason is because Terry Therneau didn't need to use 
> the Gamma model.  However, all the distributions in survreg are 
> location-scale families, 

But only after a time transformation (usually the log transformation) in
most cases (exponential, Weibull, lognormal, ...) 

> which the Gamma is not, so the basic algorithm 
> would have to be different.
 
which also holds for the Gamma; log(Gamma) is a location-scale family. So
the basic algorithm should work after all? (Haven't tried it myself,
though.) 

G??ran
-- 
 G??ran Brostr??m                    tel: +46 90 786 5223
 Department of Statistics          fax: +46 90 786 6614
 Ume?? University                   http://www.stat.umu.se/egna/gb/
 SE-90187 Ume??, Sweden             e-mail: gb at stat.umu.se



From josh8912 at yahoo.com  Mon Oct 18 21:21:00 2004
From: josh8912 at yahoo.com (JJ)
Date: Mon, 18 Oct 2004 12:21:00 -0700 (PDT)
Subject: [R] manual recreation of varConstPower using new fixed effects
	variables in nlme
In-Reply-To: <4173DDD1.7010202@stat.wisc.edu>
Message-ID: <20041018192100.92909.qmail@web51706.mail.yahoo.com>

Thanks much, Dr. Bates.  That helps.  Would you have
any comment on the other part of my other question
regarding the manual recreation of varConstPower
function using new fixed effects?  Im guessing I
obtain a singularity error because the additional
fixed effects may not alter the return value of my
function, fx.  The goal of attempting new variance
equations is to obtain in the end a variance function
that is of the form:

(d1 + (covariant)^d2)^2 + U,

where U is a random effect.

Thanks again.  John




--- Douglas Bates <bates at stat.wisc.edu> wrote:

> > PS:  Can anyone tell me what the reStruct
> parameter
> > means?  In looking at the verbose output, I obtain
> a
> > reStruct parameter value of -0.8352462 (this is
> for my
> > grouping factor for the random effect).  Does
> anyone
> > know what it means?  The actual value of my single
> > random effect is different, as is its variance.
> 
> IIRC it is the logarithm of the relative precision
> of the random effects.
> 
> 
>



From tlumley at u.washington.edu  Mon Oct 18 21:57:09 2004
From: tlumley at u.washington.edu (Thomas Lumley)
Date: Mon, 18 Oct 2004 12:57:09 -0700 (PDT)
Subject: [R] Survreg with gamma distribution
In-Reply-To: <20041018191049.GA15239@stat.umu.se>
References: <200410171355.i9HDtraR022126@outmx022.isp.belgacom.be>
	<002001c4b458$b4ebf1e0$0101a8c0@eureka>
	<Pine.A41.4.61a.0410180837430.319578@homer08.u.washington.edu>
	<20041018191049.GA15239@stat.umu.se>
Message-ID: <Pine.A41.4.61a.0410181251010.135076@homer08.u.washington.edu>

On Mon, 18 Oct 2004, G?ran Brostr?m wrote:

> On Mon, Oct 18, 2004 at 08:48:40AM -0700, Thomas Lumley wrote:
>			  However, all the distributions in survreg are
>> location-scale families,
>
> But only after a time transformation (usually the log transformation) in
> most cases (exponential, Weibull, lognormal, ...)
>
>> which the Gamma is not, so the basic algorithm
>> would have to be different.
>
> which also holds for the Gamma; log(Gamma) is a location-scale family. So
> the basic algorithm should work after all? (Haven't tried it myself,
> though.)

I don't think the log(Gamma) is a location-scale family (though I may be 
missing something). For fixed shape parameter it is a location family, but 
not a scale family as the shape parameter varies:
   a) In survreg() the extreme-value [log(Weibull)] distributions are the 
location-scale family that contain the log(Exponential).
   b) The standardised skewness of log(Gamma) random variables varies with 
the shape parameter (by simulation), though not with the scale parameter.

 	-thomas

From arrayprofile at yahoo.com  Mon Oct 18 23:22:47 2004
From: arrayprofile at yahoo.com (array chip)
Date: Mon, 18 Oct 2004 14:22:47 -0700 (PDT)
Subject: [R] multinomial logistic regression with prior probability
Message-ID: <20041018212247.87183.qmail@web41204.mail.yahoo.com>

Hi,

Is there a way to specify proior probability in
multinom()? The function has a weight option for
individual cases, but I would like to specify prior
probability for each category of the response
variable, just like what lda() does. My data have
categorical independent variables, so I think lda() is
not suitable. Or are there other R package/function
that can do the work?

Thanks



From ok at cs.otago.ac.nz  Tue Oct 19 01:03:39 2004
From: ok at cs.otago.ac.nz (Richard A. O'Keefe)
Date: Tue, 19 Oct 2004 12:03:39 +1300 (NZDT)
Subject: [R] I seem to be doing Dcv wrong
Message-ID: <200410182303.i9IN3dXU455212@atlas.otago.ac.nz>

There is a paper
(1)	A new metric for categorical data, by
	S. H. Al-Harbi, G. P. McKeown, and V. J. Rayward-Smith, in
	Statistical Data Mining and Knowledge Discovery,
	ed. Hamparsum Bozdogan, CRC Press, 2004.

This paper introduced a distance measure Dcv for clustering.

I have _not_ read this paper myself, and do not as yet have access
to a copy of it, but I'm trying to make sense of another paper
which criticises it on the grounds that it does rather badly on both
simulated data and one real data set (the Mushroom data set).
Call this other paper (2).

According to (2) (which is not yet published or I'd cite it),
                              -1        T
    Dcv(x,y) = sqrt( Dh(x,y).C  .Dh(x,y) )

where
    x and y are rows of the same data frame,
    Dh(x,y) = ifelse(x == y, 0, 1) (so that sum(Dh(x,y)) is Hamming distance),
    C is the matrix of Cramer's V scores between the columns of the
      data frame.

This is an analogue of Mahalanobis distance, with C playing the role of
the variance-covariance matrix.

I wanted to try this out, so I wrote some R code.
In particular, I couldn't persuade myself that Dcv(x,y) was always
defined.  What if, for example, some element on the diagonal of C inverse
was negative?  Then for x,y differing only in the corresponding property,
the argument of sqrt() would be negative.  My mathematical skills were not
up to the task of proving that it couldn't happen, so experiment seemed
like a good idea.

First I read in the Mushroom data (8124 rows x 23 columns)
and deleted column 1 (the poisonous/edible class, to be predicted,
not clustered on) and column 17 (which has the same value in each row).
"mush" is the resulting data frame.

cramer <- function (x, y) {
    # x and y are factors of the same length.
    # cramer(x, y) is Cramer's V, measuring the association between x and y

    n <- length(x)                              # Number of observations
    o <- table(x, y)                            # Observed table
    e <- outer(rowSums(o), colSums(o)/n)        # Expected table
    X <- sum((o-e)**2/e)                        # Chi-squared
    sqrt(X/(n * (min(dim(o)) - 1)))             # Cramer's V
}

C <- matrix(nrow=21, ncol=21)
for (i in 1:21) {
    for (j in i:21) {
        x <- cramer(mush[,i], mush[,j])
        C[i,j] <- x
        C[j,i] <- x
    }
}

CI <- solve(C)					# find inverse of C

I checked my understanding of solve() by trying CI%*%C and C%*%CI and
got the identity matrix to good precision.

Now comes the fatal question:  _are_ any elements of the diagonal of
CI negative?

> sum(diag(CI) < 0)
[1] 3
> diag(CI)
 [1]  1.0614520  1.1689474  2.9640138  0.8785050  2.7083945 -3.0289056
 [7]  4.6415407  2.2881969  1.6595514  1.4349139  3.7682528  1.9774941
[13]  2.3342165 -0.9505056  1.5479098 -7.4458871  0.3985586  1.2470409
[19]  1.2829017  2.3821185  1.5060749

There seem to be three possibilities:
(1) The paper cited above has proposed a distance measure for
    categorical variables which is sometimes undefined.
(2) The paper I'm studying has misunderstood that paper and
    misrepresented what Dcv is really supposed to be.
(3) There is a mistake in my R code.

Ad (1), we're talking about a published paper, so I'd be reluctant to think so.
Ad (2), I've actually looked at the (Python) code for calculating Dcv in some
        detail.  There's an extra scaling step, which multiplies the diagonal
        elements by weights between 1/#values and 1 inclusive, but that only
        changes the magnitudes of the diagonal elements, not their sign.
Ad (3), I make mistakes as often as the next man (hello, Eccles!), which is
        why I'm asking here if anyone can see the mistake.

The only structucal properties of C that I am sure of are
 - all the elements are between 0 and 1
 - it is symmetric
 - the diagonal elements are all 1.

random.C.like.matrix <- function (n) {
    x <- matrix(runif(n*n), nrow=n, ncol=n)
    x <- (x + t(x)) * 0.5
    diag(x) <- 1
    x
}

count.negative.diagonal.elements <- function (x) {
    sum(diag(solve(x)) < 0)
}

> summary(sapply(1:100, function (i)
+    count.negative.diagonal.elements(random.C.like.matrix(21)) ))    
   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. 
   0.00    4.00    9.00    8.94   14.00   21.00 

So clearly there is nothing unusual about a matrix with those properties
having negative diagonal elements.  If Dcv is defined correctly in (2),
then C must have some additional structural property which ensures that
this can't happen, and there must be a mistake in my code that fails to
respect that structural property.



From kjetil at acelerate.com  Tue Oct 19 01:07:45 2004
From: kjetil at acelerate.com (Kjetil Brinchmann Halvorsen)
Date: Mon, 18 Oct 2004 19:07:45 -0400
Subject: [R] how to study the code of R
In-Reply-To: <200410182336.05941.0034058@fudan.edu.cn>
References: <200410182336.05941.0034058@fudan.edu.cn>
Message-ID: <41744CC1.9020907@acelerate.com>

rongguiwong wrote:

>i want to study R programming by studying the existing code from R itself,but 
>i don't know how to read the code,can any one give me some guide?
>my R is installed in /usr/lib/R/
>
>  
>
One trick which can be usefull for a start is to use debug.
Choose some function to learn, say lm.
Say
debug(lm)

and ten use lm in a small example. You can go by the evaluation of the 
code step by step,
it can be illuminating

Kjeti


>[ronggui at mylinux ronggui]$ /usr/lib/R/
>afm      bin      doc      etc      include  library  modules  share
>
>
>  
>
>>version
>>    
>>
>         _
>platform i586-mandrake-linux-gnu
>arch     i586
>os       linux-gnu
>system   i586, linux-gnu
>status
>major    1
>minor    9.1
>year     2004
>month    06
>day      21
>language R
>  
>
>
>______________________________________________
>R-help at stat.math.ethz.ch mailing list
>https://stat.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>
>
>  
>


-- 

Kjetil Halvorsen.

Peace is the most effective weapon of mass construction.
               --  Mahdi Elmandjra



From andy_liaw at merck.com  Tue Oct 19 01:30:55 2004
From: andy_liaw at merck.com (Liaw, Andy)
Date: Mon, 18 Oct 2004 19:30:55 -0400
Subject: [R] R crashes while running RandomForest
Message-ID: <3A822319EB35174CA3714066D590DCD504AF856C@usrymx25.merck.com>

Such problems are best directed to the package maintainer (me in this case).
Please state the version of R, the version of the package, and the OS you're
using.  Also be explicit on what you mean by "crash".  If possible, give the
example code (and possibly simulated data) that caused the problem.

[There are some memory problems in the current version on CRAN, 4.3-3, but
they do not cause `crash' AFAICT.]

Cheers,
Andy

> From: Wensui Liu
> 
> I tried to use RF build a model with a continuous variable as the
> dependent. However, whenever I ran it, R just crashed.
> 
> But when I catigorize the dependent variable as a binary variable and
> ran the same model again. It just works.
> 
> Is there anyone who can help me out? I really appreciate it.
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
> 
>



From ggrothendieck at myway.com  Tue Oct 19 01:34:06 2004
From: ggrothendieck at myway.com (Gabor Grothendieck)
Date: Mon, 18 Oct 2004 19:34:06 -0400 (EDT)
Subject: [R] x is not a open/high/low/close time series
Message-ID: <20041018233406.15E4A3B83@mprdmxin.myway.com>


Dean Sonneborn <dsonneborn <at> ucdavis.edu> writes:
> I am trying to create a high low close style chart but I keep getting the 
> following error statement; "x is not a open/high/low/close time 
> series".  

Using is.mts(x) and colnames(x) check that these hold:

- the object is an "mts"
- the names of the first, second, third and fourth columns are
  "Open", "High", "Low" and "Close", respectively



From zangshizhu at yahoo.com.cn  Tue Oct 19 03:10:45 2004
From: zangshizhu at yahoo.com.cn (shizhu zang)
Date: Tue, 19 Oct 2004 09:10:45 +0800 (CST)
Subject: [R] error in file(file, "r") 
Message-ID: <20041019011045.75399.qmail@web15804.mail.cnb.yahoo.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20041019/512c4a86/attachment.pl

From murdoch at stats.uwo.ca  Tue Oct 19 03:53:17 2004
From: murdoch at stats.uwo.ca (Duncan Murdoch)
Date: Mon, 18 Oct 2004 21:53:17 -0400
Subject: [R] error in file(file, "r") 
In-Reply-To: <20041019011045.75399.qmail@web15804.mail.cnb.yahoo.com>
References: <20041019011045.75399.qmail@web15804.mail.cnb.yahoo.com>
Message-ID: <sis8n0pscb2q4n3agdcpabevd2o19h81p7@4ax.com>

On Tue, 19 Oct 2004 09:10:45 +0800 (CST), shizhu zang
<zangshizhu at yahoo.com.cn> wrote:
>Hi, there. 
> I am trying to read a file into R, but I got following message 
>Error in file(file, "r") : unable to open connection In addition: Warning message: cannot open file `SwirlSample.txt' .
>In fact, my file has been already in the current working directory.
>My R is running on window operation system instead of UNIX.

What was the command you used?  What version of R are you running?
How did you determine that the file was in the working directory?

Not to pick on you in particular, but there does seem to have been a
rash of incomplete questions posted lately.  Please, before you post,
try to guess what the obvious questions will be, and answer them!

Duncan Murdoch



From nusbj at hotmail.com  Tue Oct 19 03:59:08 2004
From: nusbj at hotmail.com (Zhen Pang)
Date: Tue, 19 Oct 2004 09:59:08 +0800
Subject: [R] sapply and loop
Message-ID: <BAY22-F23XRPmcarcR40001645c@hotmail.com>

I tried to use Rprof(). As an example, I consider the following code (from 
Venables & Ripley, 1999).

     library(MASS); library(boot); library(nls)
     data(stormer)
     storm.fm <- nls(Time ~ b*Viscosity/(Wt - c), stormer,
                     start = c(b=29.401, c=2.2183))
     st <- cbind(stormer, fit=fitted(storm.fm))
     storm.bf <- function(rs, i) {
         st$Time <-  st$fit + rs[i]
         tmp <- nls(Time ~ (b * Viscosity)/(Wt - c), st,
                    start = coef(storm.fm))
         tmp$m$getAllPars()
     }
     rs <- scale(resid(storm.fm), scale = FALSE) # remove the mean
     Rprof("boot.out")
     storm.boot <- boot(rs, storm.bf, R = 4999) # pretty slow
     Rprof(NULL)

summaryRprof()
Error in summaryRprof() : no events were recorded

I am using R1.8.1 in windows. Why can't I get the results?

Zhen

>From: Thomas Lumley <tlumley at u.washington.edu>
>To: Zhen Pang <nusbj at hotmail.com>
>CC: r-help at stat.math.ethz.ch
>Subject: Re: [R] sapply and loop
>Date: Mon, 18 Oct 2004 09:14:49 -0700 (PDT)
>
>On Sat, 16 Oct 2004, Zhen Pang wrote:
>
>>Dear all,
>>
>>I am doing 200 times simulation. For each time, I generate a matrix and 
>>define some function on this matrix to get a 6 dimension vector as my 
>>results.
>>
>>As the loop should be slow, I generate 200 matrice first, and save them 
>>into a list named ma,
>>then I define zz<-sapply(ma, myfunction)
>>
>>To my surprise, It almost costs me the same time to get my results if I 
>>directly use a loop from 1 to 200. Is it common? Can I improve any 
>>further?
>>
>
>It is quite common for a loop to be as fast as sapply(). After all, 
>sapply() still has to run `myfunction' 200 times, and this is what takes 
>most of the time, so there isn't any obvious reason why sapply() should be 
>much faster.  sapply() and lapply() certainly can be faster than loops, but 
>usually not by very much.
>
>The surprising fact is that so many people *believe* the apply() functions 
>are typically much faster than loops. It's probably a useful belief, since 
>it encourages people to learn to use them (and they sometimes are faster).
>
>You should try using Rprof() to find out which parts of your code are slow.
>
>	-thomas



From s938611 at mail.yzu.edu.tw  Tue Oct 19 05:04:38 2004
From: s938611 at mail.yzu.edu.tw (pcscan)
Date: Tue, 19 Oct 2004 11:04:38 +0800
Subject: [R] Questions of t.test {stats}
Message-ID: <000501c4b588$5f57dd30$47138a8c@linuxkeyzthan9>

We are currently using the t-test in Package stats,

t.test(x, y = NULL, alternative = c("two.sided", "less", "greater"),
       mu = 0, paired = FALSE, var.equal = FALSE,
       conf.level = 0.95, ...)

but have some troubles :
1. why does the t-test take so a long time to perform a single test on a row
of a data.frame ? Is there any alternative function to perform t-test on all
the rows of a data.frame ?
2. We got different results on the following data with the argument
var.equal  setting as TRUE and FALSE respectively.
We are curious why the  "Welch Two Sample t-test" couldn't distinguish these
two vectors well.

Any help is greatly appreciated.


Sincerely. Liu Yu Ting

============================================================================
===============

x <-
c(-0.299611385,-0.164028986,-0.225545128,-0.244473171,-0.276619985,-0.276362
81,-0.289633015,-0.298994167,-0.27908886,-0.265612916,-0.262321082,-0.295753
768,-0.235677803,-0.283872306,-0.282174954,-0.241592817,-0.274716893,-0.2886
55752,-0.262166777,-0.263298345,-0.252239841,-0.298274078,-0.28958158,-0.187
174691,-0.26628157,-0.252034102,-0.248793703,-0.267207398,-0.289838754,-0.28
4283785,-0.118097619,-0.27898599,-0.265818655,-0.295085114,-0.246839177,-0.2
76105636,-0.293336328,-0.294210721,-0.259543597,0.18181929,-0.276311375,-0.2
48948008,-0.212583533,-0.247147786,-0.269573403,-0.27636281,-0.295445158,-0.
281146256,-0.27636281,-0.255840285,-0.292513369,-0.21664689,-0.228014003,-0.
238249548,-0.238300983,-0.238506723,-0.242004296,-0.213869405,-0.272916672,-
0.293233458,-0.239483986,-0.147672687,-0.289941624,-0.233774712,-0.237940939
,-0.276517115,-0.22431069,-0.217469848,0.461573717,-0.218858591,-0.280271863
,-0.290867452,-0.177144886,-0.179150847,-0.258463465,-0.269470533,-0.2482279
19,-0.221327466,-0.217418413,-0.290044494,-0.290610278,-0.260006512,-0.22261
3338,-0.275951331,0.015118775,0.116959879,-0.24509039,-0.092894518,0.5274618
26)

y <-
c(0.963784092,-0.266641614,4.623274441,1.6857758,-0.251159709,-0.090631382,1
.380355357,-0.117840445,7.213998979,3.404935937,1.444648983)

> t.test(x,y)

        Welch Two Sample t-test

data:  x and y
t = -2.8369, df = 10.009, p-value = 0.01763
alternative hypothesis: true difference in means is not equal to 0
95 percent confidence interval:
 -3.6338688 -0.4369566
sample estimates:
 mean of x  mean of y
-0.2180945  1.8173182

> t.test(x,y,var.equal=TRUE)

        Two Sample t-test

data:  x and y
t = -8.2473, df = 98, p-value = 7.507e-13
alternative hypothesis: true difference in means is not equal to 0
95 percent confidence interval:
 -2.525175 -1.545650
sample estimates:
 mean of x  mean of y
-0.2180945  1.8173182



From josh8912 at yahoo.com  Tue Oct 19 05:25:24 2004
From: josh8912 at yahoo.com (JJ)
Date: Mon, 18 Oct 2004 20:25:24 -0700 (PDT)
Subject: [R] manual recreation of varConstPower using new fixed effects
	variables in nlme
Message-ID: <20041019032525.66212.qmail@web51702.mail.yahoo.com>

Ah, I think Ive found a solution.  A variance 
function of the form:

(d1 + (covariant)^d2)^2 + U,

can be made by altering the line in varWeights.varComb
to read:

apply(as.data.frame(lapply(object, varWeights)), 1,
sum)

Then the function varComb can be used as in:

varComb(varPower(form = ~cov1|g1,value=list(...)),
varConstPower(form = ~g2|g2, fixed=
list(power=(c(...)))))

That seems to work just fine.  John



From ripley at stats.ox.ac.uk  Tue Oct 19 08:29:54 2004
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Tue, 19 Oct 2004 07:29:54 +0100 (BST)
Subject: [R] sapply and loop
In-Reply-To: <BAY22-F23XRPmcarcR40001645c@hotmail.com>
Message-ID: <Pine.LNX.4.44.0410190723090.27668-100000@gannet.stats>

On Tue, 19 Oct 2004, Zhen Pang wrote:

> I tried to use Rprof(). As an example, I consider the following code (from 
> Venables & Ripley, 1999).

I believe you parroted that from `Writing R Extensions', but failed to 
give proper credit!

>      library(MASS); library(boot); library(nls)
>      data(stormer)
>      storm.fm <- nls(Time ~ b*Viscosity/(Wt - c), stormer,
>                      start = c(b=29.401, c=2.2183))
>      st <- cbind(stormer, fit=fitted(storm.fm))
>      storm.bf <- function(rs, i) {
>          st$Time <-  st$fit + rs[i]
>          tmp <- nls(Time ~ (b * Viscosity)/(Wt - c), st,
>                     start = coef(storm.fm))
>          tmp$m$getAllPars()
>      }
>      rs <- scale(resid(storm.fm), scale = FALSE) # remove the mean
>      Rprof("boot.out")
>      storm.boot <- boot(rs, storm.bf, R = 4999) # pretty slow
>      Rprof(NULL)

At this point your unacknowledged copying went adrift.

> summaryRprof()
> Error in summaryRprof() : no events were recorded
> 
> I am using R1.8.1 in windows. Why can't I get the results?

Because you didn't do your homework, and didn't even follow your source.  
The 'file' arguments of Rprof and summaryProf have to agree: see their 
help pages (as the posting guide asks).

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From nusbj at hotmail.com  Tue Oct 19 08:49:42 2004
From: nusbj at hotmail.com (Zhen Pang)
Date: Tue, 19 Oct 2004 14:49:42 +0800
Subject: [R] sapply and loop
Message-ID: <BAY22-F10kcJse1fTvO0005bcee@hotmail.com>

I am sorry for neglecting the acknowledgement for `Writing R Extensions', 
since I think I am just citing the code from the orignal R-help. I fail to 
get the results when I use my own code. So I refer to this code where 
Rprof() appears. Anyway, I am sorry for this.

In fact, I have tried to whether I specify boot.out. Neither one works.

Rprof("boot.out")
     storm.boot <- boot(rs, storm.bf, R = 4999) # pretty slow
      Rprof(NULL)

summaryRprof()
Error in summaryRprof() : no events were recorded

summaryRprof("boot.out")
Error in summaryRprof("boot.out") : no events were recorded

     Rprof()
     storm.boot <- boot(rs, storm.bf, R = 4999) # pretty slow
      Rprof(NULL)

summaryRprof()
Error in summaryRprof() : no events were recorded


Zhen


>From: Prof Brian Ripley <ripley at stats.ox.ac.uk>
>To: Zhen Pang <nusbj at hotmail.com>
>CC: tlumley at u.washington.edu, r-help at stat.math.ethz.ch
>Subject: Re: [R] sapply and loop
>Date: Tue, 19 Oct 2004 07:29:54 +0100 (BST)
>
>On Tue, 19 Oct 2004, Zhen Pang wrote:
>
> > I tried to use Rprof(). As an example, I consider the following code 
>(from
> > Venables & Ripley, 1999).
>
>I believe you parroted that from `Writing R Extensions', but failed to
>give proper credit!
>
> >      library(MASS); library(boot); library(nls)
> >      data(stormer)
> >      storm.fm <- nls(Time ~ b*Viscosity/(Wt - c), stormer,
> >                      start = c(b=29.401, c=2.2183))
> >      st <- cbind(stormer, fit=fitted(storm.fm))
> >      storm.bf <- function(rs, i) {
> >          st$Time <-  st$fit + rs[i]
> >          tmp <- nls(Time ~ (b * Viscosity)/(Wt - c), st,
> >                     start = coef(storm.fm))
> >          tmp$m$getAllPars()
> >      }
> >      rs <- scale(resid(storm.fm), scale = FALSE) # remove the mean
> >      Rprof("boot.out")
> >      storm.boot <- boot(rs, storm.bf, R = 4999) # pretty slow
> >      Rprof(NULL)
>
>At this point your unacknowledged copying went adrift.
>
> > summaryRprof()
> > Error in summaryRprof() : no events were recorded
> >
> > I am using R1.8.1 in windows. Why can't I get the results?
>
>Because you didn't do your homework, and didn't even follow your source.
>The 'file' arguments of Rprof and summaryProf have to agree: see their
>help pages (as the posting guide asks).
>
>--
>Brian D. Ripley,                  ripley at stats.ox.ac.uk
>Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
>University of Oxford,             Tel:  +44 1865 272861 (self)
>1 South Parks Road,                     +44 1865 272866 (PA)
>Oxford OX1 3TG, UK                Fax:  +44 1865 272595
>
>______________________________________________
>R-help at stat.math.ethz.ch mailing list
>https://stat.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide! 
>http://www.R-project.org/posting-guide.html



From ockham at gmx.net  Tue Oct 19 06:54:59 2004
From: ockham at gmx.net (Bernhard Reiter)
Date: Tue, 19 Oct 2004 06:54:59 +0200
Subject: [R] gaussian error propagation
Message-ID: <200410190702.i9J72MXp010142@hypatia.math.ethz.ch>

Dear list,

I'm intending to use R (1.9.0 installed on my linux machine) for my applied
experimental physics course, requiring it to do rather basic statistical
things (like calculation of mean, std-deviation and some linear fitting).
There is, though, a task for which I haven't found a solution for after some
days of search: in experiments involving multiple (supposedly) independent
measurements I am required to calculate the total error according to
Gaussian error propagation law. 
[Sqrt (Sum of (squares of (partials(mean) times corresponding std-dev) ) )]
After failing to find an appropriate function, I've started trying to build
my own (something that takes a (mathematical) function, does the partial
derivations, takes the corresponding mean values and sd's (i.e. from a data
frame) as arguments and does the remaining necessary calculations. 
However, even my most basic experiments with D() and derive() failed, though
I stuck to the derive() help page and the FAQ, but (x and y unspecified,)
typing 
> D(expression(x^2), "x")
yields 0
And so does derive (that is, for the gradients; this time x and y set to
integer values, as R complains about failing objects otherwise), when, i.e.,
I do
> eval(derive(expression(x*y), c("x","y")))

I'd be very glad if anybody could help me with this rather strange behaviour
and tell me if/what I'm doing wrong and if maybe there's an even simpler way
to perform gaussian error propagation.

Greetings,
Ockham



From gb at stat.umu.se  Tue Oct 19 09:14:23 2004
From: gb at stat.umu.se (Gran Brostrm)
Date: Tue, 19 Oct 2004 09:14:23 +0200
Subject: [R] Survreg with gamma distribution
In-Reply-To: <Pine.A41.4.61a.0410181251010.135076@homer08.u.washington.edu>
References: <200410171355.i9HDtraR022126@outmx022.isp.belgacom.be>
	<002001c4b458$b4ebf1e0$0101a8c0@eureka>
	<Pine.A41.4.61a.0410180837430.319578@homer08.u.washington.edu>
	<20041018191049.GA15239@stat.umu.se>
	<Pine.A41.4.61a.0410181251010.135076@homer08.u.washington.edu>
Message-ID: <20041019071423.GA18486@stat.umu.se>

On Mon, Oct 18, 2004 at 12:57:09PM -0700, Thomas Lumley wrote:
> On Mon, 18 Oct 2004, G??ran Brostr??m wrote:
> 
> >On Mon, Oct 18, 2004 at 08:48:40AM -0700, Thomas Lumley wrote:
> >			  However, all the distributions in survreg are
> >>location-scale families,
> >
> >But only after a time transformation (usually the log transformation) in
> >most cases (exponential, Weibull, lognormal, ...)
> >
> >>which the Gamma is not, so the basic algorithm
> >>would have to be different.
> >
> >which also holds for the Gamma; log(Gamma) is a location-scale family. So
> >the basic algorithm should work after all? (Haven't tried it myself,
> >though.)
> 
> I don't think the log(Gamma) is a location-scale family (though I may be 
> missing something). 

You are not missing anything, but I was, apparently; I have always thought
of a shape parameter as follows: If the cdf of an rv  X  can be written
as F(x) = G((x/s)^p), then (s, p) is a scale-shape parameter. In that case,
the log transform (of  X) gives a location-scale family of distributions.

Obviously, the gamma cdf is not of the scale-shape form above, and so the
log transform does not give a location-scale family. I apologize for the
misinformation. 

G??ran



From Ted.Harding at nessie.mcc.ac.uk  Tue Oct 19 09:31:10 2004
From: Ted.Harding at nessie.mcc.ac.uk ( (Ted Harding))
Date: Tue, 19 Oct 2004 08:31:10 +0100 (BST)
Subject: [R] Questions of t.test {stats}
In-Reply-To: <000501c4b588$5f57dd30$47138a8c@linuxkeyzthan9>
Message-ID: <XFMail.041019082701.Ted.Harding@nessie.mcc.ac.uk>

On 19-Oct-04 pcscan wrote:
> We are currently using the t-test in Package stats,
> [...]
> but have some troubles :
> 1. why does the t-test take so a long time to perform a single test
>    on a row of a data.frame ? Is there any alternative function to
>    perform t-test on all the rows of a data.frame ?

Not sure what your problem is here. 't.test' is almost instantaneous
on my somewhat slow machine here.

> 2. We got different results on the following data with the argument
>    var.equal  setting as TRUE and FALSE respectively.
>    We are curious why the  "Welch Two Sample t-test" couldn't
>    distinguish these two vectors well.

The two samples have very different standard deviations, the SD of y
being about 18 times that of x. You would expect the Welch test to
give quite different results from the "var.equal=TRUE" case. In these
circumstances I would trust the Welch test rather than the other.

Best wishes,
Ted.


--------------------------------------------------------------------
E-Mail: (Ted Harding) <Ted.Harding at nessie.mcc.ac.uk>
Fax-to-email: +44 (0)870 094 0861  [NB: New number!]
Date: 19-Oct-04                                       Time: 08:27:01
------------------------------ XFMail ------------------------------



From ligges at statistik.uni-dortmund.de  Tue Oct 19 09:54:39 2004
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Tue, 19 Oct 2004 09:54:39 +0200
Subject: Question on Rprof(); was: Re: [R] sapply and loop
In-Reply-To: <BAY22-F10kcJse1fTvO0005bcee@hotmail.com>
References: <BAY22-F10kcJse1fTvO0005bcee@hotmail.com>
Message-ID: <4174C83F.9050108@statistik.uni-dortmund.de>

Zhen Pang wrote:

> I am sorry for neglecting the acknowledgement for `Writing R 
> Extensions', since I think I am just citing the code from the orignal 
> R-help. I fail to get the results when I use my own code. So I refer to 
> this code where Rprof() appears. Anyway, I am sorry for this.
> 
> In fact, I have tried to whether I specify boot.out. Neither one works.

Do you have read/write permissions? R-1.8.1 is pretty old. Please upgrade.



BTW: Rprof() to a non-existing directory
    Rprof("c:/DoesNotExist/temp.out")
under R-2.0.0, Windows, results in the GUI information message:
"Fatal error: can't open profile file", and R is closed afterwards.

I think we should change this behaviour to a simple error message by 
adding the lines

  if(!file.exists(dirname(filename)))
      stop("Cannot open file ", filename)

just before

  invisible(.Internal(Rprof(filename, append, interval)))



Uwe Ligges



> Rprof("boot.out")
>     storm.boot <- boot(rs, storm.bf, R = 4999) # pretty slow
>      Rprof(NULL)
> 
> summaryRprof()
> Error in summaryRprof() : no events were recorded
> 
> summaryRprof("boot.out")
> Error in summaryRprof("boot.out") : no events were recorded
> 
>     Rprof()
>     storm.boot <- boot(rs, storm.bf, R = 4999) # pretty slow
>      Rprof(NULL)
> 
> summaryRprof()
> Error in summaryRprof() : no events were recorded
> 
> 
> Zhen
> 
> 
>> From: Prof Brian Ripley <ripley at stats.ox.ac.uk>
>> To: Zhen Pang <nusbj at hotmail.com>
>> CC: tlumley at u.washington.edu, r-help at stat.math.ethz.ch
>> Subject: Re: [R] sapply and loop
>> Date: Tue, 19 Oct 2004 07:29:54 +0100 (BST)
>>
>> On Tue, 19 Oct 2004, Zhen Pang wrote:
>>
>> > I tried to use Rprof(). As an example, I consider the following code 
>> (from
>> > Venables & Ripley, 1999).
>>
>> I believe you parroted that from `Writing R Extensions', but failed to
>> give proper credit!
>>
>> >      library(MASS); library(boot); library(nls)
>> >      data(stormer)
>> >      storm.fm <- nls(Time ~ b*Viscosity/(Wt - c), stormer,
>> >                      start = c(b=29.401, c=2.2183))
>> >      st <- cbind(stormer, fit=fitted(storm.fm))
>> >      storm.bf <- function(rs, i) {
>> >          st$Time <-  st$fit + rs[i]
>> >          tmp <- nls(Time ~ (b * Viscosity)/(Wt - c), st,
>> >                     start = coef(storm.fm))
>> >          tmp$m$getAllPars()
>> >      }
>> >      rs <- scale(resid(storm.fm), scale = FALSE) # remove the mean
>> >      Rprof("boot.out")
>> >      storm.boot <- boot(rs, storm.bf, R = 4999) # pretty slow
>> >      Rprof(NULL)
>>
>> At this point your unacknowledged copying went adrift.
>>
>> > summaryRprof()
>> > Error in summaryRprof() : no events were recorded
>> >
>> > I am using R1.8.1 in windows. Why can't I get the results?
>>
>> Because you didn't do your homework, and didn't even follow your source.
>> The 'file' arguments of Rprof and summaryProf have to agree: see their
>> help pages (as the posting guide asks).
>>
>> -- 
>> Brian D. Ripley,                  ripley at stats.ox.ac.uk
>> Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
>> University of Oxford,             Tel:  +44 1865 272861 (self)
>> 1 South Parks Road,                     +44 1865 272866 (PA)
>> Oxford OX1 3TG, UK                Fax:  +44 1865 272595
>>
>> ______________________________________________
>> R-help at stat.math.ethz.ch mailing list
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide! 
>> http://www.R-project.org/posting-guide.html
> 
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html



From maechler at stat.math.ethz.ch  Tue Oct 19 10:11:51 2004
From: maechler at stat.math.ethz.ch (Martin Maechler)
Date: Tue, 19 Oct 2004 10:11:51 +0200
Subject: [R] gaussian error propagation
In-Reply-To: <200410190702.i9J72MXp010142@hypatia.math.ethz.ch>
References: <200410190702.i9J72MXp010142@hypatia.math.ethz.ch>
Message-ID: <16756.52295.294405.70525@gargle.gargle.HOWL>


>>>>> "Bernhard" == Bernhard Reiter <ockham at gmx.net>
>>>>>     on Tue, 19 Oct 2004 06:54:59 +0200 writes:

    Bernhard> Dear list, I'm intending to use R (1.9.0 installed
    Bernhard> on my linux machine) for my applied experimental
    Bernhard> physics course, requiring it to do rather basic
    Bernhard> statistical things (like calculation of mean,
    Bernhard> std-deviation and some linear fitting).  There is,
    Bernhard> though, a task for which I haven't found a
    Bernhard> solution for after some days of search: in
    Bernhard> experiments involving multiple (supposedly)
    Bernhard> independent measurements I am required to
    Bernhard> calculate the total error according to Gaussian
    Bernhard> error propagation law.  [Sqrt (Sum of (squares of
    Bernhard> (partials(mean) times corresponding std-dev) ) )]
    Bernhard> After failing to find an appropriate function,
    Bernhard> I've started trying to build my own (something
    Bernhard> that takes a (mathematical) function, does the
    Bernhard> partial derivations, takes the corresponding mean
    Bernhard> values and sd's (i.e. from a data frame) as
    Bernhard> arguments and does the remaining necessary
    Bernhard> calculations.  However, even my most basic
    Bernhard> experiments with D() and derive() failed, though I
    Bernhard> stuck to the derive() help page and the FAQ, but
    Bernhard> (x and y unspecified,) typing
    >> D(expression(x^2), "x")
    Bernhard> yields 0

not for me, see below
Have you accidentally redefined D() to be something else?

> (dd <- D(expression(x^2), "x"))
2 * x
> str(dd)
 language 2 * x
> x <- 1:3
> eval(dd)
[1] 2 4 6
>

    Bernhard>  And so does derive (that is, for the
    Bernhard> gradients; this time x and y set to integer
    Bernhard> values, as R complains about failing objects
    Bernhard> otherwise), when, i.e., I do
    >> eval(derive(expression(x*y), c("x","y")))

    Bernhard> I'd be very glad if anybody could help me with
    Bernhard> this rather strange behaviour and tell me if/what
    Bernhard> I'm doing wrong and if maybe there's an even
    Bernhard> simpler way to perform gaussian error propagation.

    Bernhard> Greetings, Ockham

    Bernhard> ______________________________________________
    Bernhard> R-help at stat.math.ethz.ch mailing list
    Bernhard> https://stat.ethz.ch/mailman/listinfo/r-help
    Bernhard> PLEASE do read the posting guide!
    Bernhard> http://www.R-project.org/posting-guide.html



From florence.combes at paris7.jussieu.fr  Tue Oct 19 10:10:16 2004
From: florence.combes at paris7.jussieu.fr (Florence Combes)
Date: Tue, 19 Oct 2004 10:10:16 +0200
Subject: [R] pb with aws package
Message-ID: <6.0.0.22.2.20041019100953.01dff378@paris7.jussieu.fr>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20041019/91dd8b6c/attachment.pl

From jost at cict.fr  Tue Oct 19 10:18:27 2004
From: jost at cict.fr (Christian Jost)
Date: Tue, 19 Oct 2004 10:18:27 +0200
Subject: [R] matrix of eigenvalues
Message-ID: <a06002006bd9a7b5ffb79@[130.120.104.141]>

I thought that the function
eigen(A)
will return a matrix with eigenvectors that are independent of each 
other (thus forming a base and the matrix being invertible). This 
seems not to be the case in the following example
A=matrix(c(1,2,0,1),nrow=2,byrow=T)
eigen(A) ->ev
solve(ev$vectors)

note that I try to get the upper triangular form with eigenvalues on 
the diagonal and (possibly) 1 just atop the eigenvalues to be used to 
solve a linear differential equation
x' = Ax, x(0)=x0
x(t) = P exp(D t) P^-1 x0
where D is this upper triangular form and P is the "passage matrix" 
(not sure about the correct english name) given by a base of 
eigenvectors. So the test would be
solve(ev$vectors) %*% A %*% ev$vectors - D
should be 0

Thanks for any help, Christian.

ps: please copy reply also to my address, my subscription to the 
R-help list seems to have delays
-- 
***********************************************************
http://cognition.ups-tlse.fr/vas-y.php?id=chj  jost at cict.fr
Christian Jost                                   (PhD, MdC)
Centre de Recherches sur la Cognition Animale
Universite Paul Sabatier, Bat IV R3
118 route de Narbonne
31062 Toulouse cedex 4, France
Tel: +33 5 61 55 64 37   Fax: +33 5 61 55 61 54



From ligges at statistik.uni-dortmund.de  Tue Oct 19 10:25:35 2004
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Tue, 19 Oct 2004 10:25:35 +0200
Subject: [R] pb with aws package
In-Reply-To: <6.0.0.22.2.20041019100953.01dff378@paris7.jussieu.fr>
References: <6.0.0.22.2.20041019100953.01dff378@paris7.jussieu.fr>
Message-ID: <4174CF7F.7060505@statistik.uni-dortmund.de>

Florence Combes wrote:

> Dear all,
> 
> [I work with a Windows XP Prof. installation, and the 2.0.0 R version]
> 
> I need to use the aws package, but I have the following error message:
> 
> 
>  > require(aws)
> Loading required package: aws
> Error in firstlib(which.lib.loc, package) :
>          couldn't find function "lazyLoad"
> [1] FALSE

So you are not loading the package with R-2.0.0, or your R-2.0.0 
installation is broken, or you have a library specified that contains 
outdated base packages.
Please re-install R-2.0.0 into a clean directory and check the setting 
that specifies in which libraries R looks for packages.

Uwe Ligges


> I download the .zip file (for Windows) today on the CRAN site, so I think I 
> have an update version; does someone has already encountered this problem, 
> do I miss something ?
> 
> Thanks a lot for your help,
> 
> Florence.
> 
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html



From mrufino at ipimar.ualg.pt  Tue Oct 19 10:50:29 2004
From: mrufino at ipimar.ualg.pt (Marta Rufino)
Date: Tue, 19 Oct 2004 09:50:29 +0100
Subject: [R] Questions of t.test {stats}
References: <000501c4b588$5f57dd30$47138a8c@linuxkeyzthan9>
Message-ID: <03fe01c4b5b8$afcb1b30$0b1a0e0a@PORTATILMARTA>

Dear colegue,

I am not sure if it is this what you want, but to apply the t.test to all
rows in a data frame, you can do like:

apply(dataframe, 1, t.test)

If you want to store the results in a nice data frame, to us after, then you
should do a function, for example:

#################################### wilcox CI plus median
median.ci.wilcox=function(y){
# print("example: use to construct graphs with 95% CI, using xYplot, from
Hmisc. use summarize to compute the CI in function of factors")
if (is.R()) {require(ctest)}
if(length(y)>5){
c=t.test(y, conf.int=T, conf.level=.95)
res=c(mean=c$estimate, Lower=c(c$conf.int[1]), Upper=c(c$conf.int[2]))}
else {res=c(median=NA, Lower=NA, Upper=NA)}
res
}



hope this helps,
All the best,
Marta


----- Original Message ----- 
From: "pcscan" <s938611 at mail.yzu.edu.tw>
To: <r-help at stat.math.ethz.ch>
Sent: Tuesday, October 19, 2004 4:04 AM
Subject: [R] Questions of t.test {stats}


> We are currently using the t-test in Package stats,
>
> t.test(x, y = NULL, alternative = c("two.sided", "less", "greater"),
>        mu = 0, paired = FALSE, var.equal = FALSE,
>        conf.level = 0.95, ...)
>
> but have some troubles :
> 1. why does the t-test take so a long time to perform a single test on a
row
> of a data.frame ? Is there any alternative function to perform t-test on
all
> the rows of a data.frame ?
> 2. We got different results on the following data with the argument
> var.equal  setting as TRUE and FALSE respectively.
> We are curious why the  "Welch Two Sample t-test" couldn't distinguish
these
> two vectors well.
>
> Any help is greatly appreciated.
>
>
> Sincerely. Liu Yu Ting
>
>
============================================================================
> ===============
>
> x <-
>
c(-0.299611385,-0.164028986,-0.225545128,-0.244473171,-0.276619985,-0.276362
>
81,-0.289633015,-0.298994167,-0.27908886,-0.265612916,-0.262321082,-0.295753
>
768,-0.235677803,-0.283872306,-0.282174954,-0.241592817,-0.274716893,-0.2886
>
55752,-0.262166777,-0.263298345,-0.252239841,-0.298274078,-0.28958158,-0.187
>
174691,-0.26628157,-0.252034102,-0.248793703,-0.267207398,-0.289838754,-0.28
>
4283785,-0.118097619,-0.27898599,-0.265818655,-0.295085114,-0.246839177,-0.2
>
76105636,-0.293336328,-0.294210721,-0.259543597,0.18181929,-0.276311375,-0.2
>
48948008,-0.212583533,-0.247147786,-0.269573403,-0.27636281,-0.295445158,-0.
>
281146256,-0.27636281,-0.255840285,-0.292513369,-0.21664689,-0.228014003,-0.
>
238249548,-0.238300983,-0.238506723,-0.242004296,-0.213869405,-0.272916672,-
>
0.293233458,-0.239483986,-0.147672687,-0.289941624,-0.233774712,-0.237940939
>
,-0.276517115,-0.22431069,-0.217469848,0.461573717,-0.218858591,-0.280271863
>
,-0.290867452,-0.177144886,-0.179150847,-0.258463465,-0.269470533,-0.2482279
>
19,-0.221327466,-0.217418413,-0.290044494,-0.290610278,-0.260006512,-0.22261
>
3338,-0.275951331,0.015118775,0.116959879,-0.24509039,-0.092894518,0.5274618
> 26)
>
> y <-
>
c(0.963784092,-0.266641614,4.623274441,1.6857758,-0.251159709,-0.090631382,1
> .380355357,-0.117840445,7.213998979,3.404935937,1.444648983)
>
> > t.test(x,y)
>
>         Welch Two Sample t-test
>
> data:  x and y
> t = -2.8369, df = 10.009, p-value = 0.01763
> alternative hypothesis: true difference in means is not equal to 0
> 95 percent confidence interval:
>  -3.6338688 -0.4369566
> sample estimates:
>  mean of x  mean of y
> -0.2180945  1.8173182
>
> > t.test(x,y,var.equal=TRUE)
>
>         Two Sample t-test
>
> data:  x and y
> t = -8.2473, df = 98, p-value = 7.507e-13
> alternative hypothesis: true difference in means is not equal to 0
> 95 percent confidence interval:
>  -2.525175 -1.545650
> sample estimates:
>  mean of x  mean of y
> -0.2180945  1.8173182
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide!
http://www.R-project.org/posting-guide.html



From r+Steven.Murdoch at cl.cam.ac.uk  Tue Oct 19 11:15:36 2004
From: r+Steven.Murdoch at cl.cam.ac.uk (Steven Murdoch)
Date: Tue, 19 Oct 2004 10:15:36 +0100
Subject: [R] Overlaying data on graphs
Message-ID: <20041019091536.GA17090@cl.cam.ac.uk>

An embedded message was scrubbed...
From: unknown sender
Subject: no subject
Date: no date
Size: 1608
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20041019/04458194/attachment.mht

From florence.combes at paris7.jussieu.fr  Tue Oct 19 11:16:44 2004
From: florence.combes at paris7.jussieu.fr (Florence Combes)
Date: Tue, 19 Oct 2004 11:16:44 +0200
Subject: [R] pb with aws package
In-Reply-To: <86EED55AE819274B8308B62A225457D8CEC28B@exch1.qsa.local>
References: <86EED55AE819274B8308B62A225457D8CEC28B@exch1.qsa.local>
Message-ID: <6.0.0.22.2.20041019111523.01dd24b0@paris7.jussieu.fr>

Dear Andrew,

> From Windows, using Packages->Install packages from CRAN
>will get the right version.


Following your advice, I did:

***************************************************************************************
 > local({a <- CRAN.packages()
+ install.packages(select.list(a[,1],,TRUE), .libPaths()[1], available=a)})
trying URL `http://cran.r-project.org/bin/windows/contrib/2.0/PACKAGES'
Content type `text/plain; charset=iso-8859-1' length 21691 bytes
opened URL
downloaded 21Kb

trying URL `http://cran.r-project.org/bin/windows/contrib/2.0/aws_1.3-0.zip'
Content type `application/zip' length 205666 bytes
opened URL
downloaded 200Kb

package 'aws' successfully unpacked and MD5 sums checked

Delete downloaded files (y/N)? N
The packages are in C:\DOCUME~1\Florence\LOCALS~1\Temp\Rtmp21463\Rinstdir20885
updating HTML package descriptions
***************************************************************************************
and then I still have the same error:

 > library(aws)
Error in firstlib(which.lib.loc, package) :
         couldn't find function "lazyLoad"
Error in library(aws) : .First.lib failed
 > require(aws)
Loading required package: aws
Error in firstlib(which.lib.loc, package) :
         couldn't find function "lazyLoad"
[1] FALSE


I can't understand what's happening because this problem apart, all is OK 
with the R version I have, and with all other packages I installed the same 
way.

Do I have to install the latest R-version for the aws package ?

Any help appreciated,

Florence.



>Dear Florence,
>
>You may have downloaded the wrong version.
> From Windows, using Packages->Install packages from CRAN
>will get the right version.
>
>Regards,
>
>Andrew C. Ward,                andrew.ward at qsa.qld.edu.au
>Senior Analyst (Quantitative), Tel: +61 7 3864 0439
>Queensland Studies Authority,  Fax: +61 7 3229 3318
>295 Ann Street,
>Brisbane Qld 4000, Australia



Le 10:28 AM 10/19/2004,Andrew Ward ??crit:
>Dear Florence,
>
>You may have downloaded the wrong version.
> From Windows, using Packages->Install packages from CRAN
>will get the right version.
>
>Regards,
>
>Andrew C. Ward,                andrew.ward at qsa.qld.edu.au
>Senior Analyst (Quantitative), Tel: +61 7 3864 0439
>Queensland Studies Authority,  Fax: +61 7 3229 3318
>295 Ann Street,
>Brisbane Qld 4000, Australia
>
>-----Original Message-----
>From: r-help-bounces at stat.math.ethz.ch
>[mailto:r-help-bounces at stat.math.ethz.ch]On Behalf Of Florence Combes
>Sent: Tuesday, 19 October 2004 6:10 PM
>To: r-help at stat.math.ethz.ch
>Subject: [R] pb with aws package
>
>
>Dear all,
>
>[I work with a Windows XP Prof. installation, and the 2.0.0 R version]
>
>I need to use the aws package, but I have the following error message:
>
>
>  > require(aws)
>Loading required package: aws
>Error in firstlib(which.lib.loc, package) :
>          couldn't find function "lazyLoad"
>[1] FALSE
>
>I download the .zip file (for Windows) today on the CRAN site, so I think I
>have an update version; does someone has already encountered this problem,
>do I miss something ?
>
>Thanks a lot for your help,
>
>Florence.
>
>
>         [[alternative HTML version deleted]]
>
>______________________________________________
>R-help at stat.math.ethz.ch mailing list
>https://stat.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
>This email (including any attached files) is for the intended
>recipient(s) only. If you received this email by mistake, please,
>as a courtesy, tell the sender, then delete this email.
>
>The views and opinions are the originator's and do not necessarily
>reflect those of the Queensland Studies Authority. All reasonable
>precautions have been taken to ensure that this email contained no
>viruses at the time it was sent.
>



From r+Steven.Murdoch at cl.cam.ac.uk  Tue Oct 19 11:18:59 2004
From: r+Steven.Murdoch at cl.cam.ac.uk (Steven Murdoch)
Date: Tue, 19 Oct 2004 10:18:59 +0100
Subject: [R] Overlaying data on graphs
Message-ID: <20041019091859.GB17090@cl.cam.ac.uk>

[sorry, my last message seems to have been corrupted - here it is
again.]

I currently have a graph from network measurements of latency against
time and would like to overlay some extra data on top, but with a
different y scale, but I am not sure how to do this in an automated
manner.

The base graph is generated by these commands:
 time=probe$rsec+(probe$rusec/1000000)
 diff=((probe$rsec-probe$ssec)*1000000)+(probe$rusec-probe$susec)
 plot(time[100:len],diff[100:len])

Then I would like to overlay data on the usage of the network. In this
data the x scale is the same (time), but the y axis is unrelated
(buffer size). The approach I have currently been using is simply
drawing on top of the graph:
 lines(usage$time,(usage$bytes)*m+c,type="h",col="blue")

The problem with this is that to make the lines visible, but not
taking up the whole height, I need to manually tweak the m and c
factors, based on the relative values of the usage and probe data.
Since I have a lot of graphs to generate, I would like to do this
automatically.

I haven't been able to do this, because I can't find out the ymin and
ymax of the graph generated by plot. If I could then it would be
trivial to scale the factors so they took up say, 10% of the vertical
space. There is par("yaxp"), but this shows the positions of the
tickmarks, but these can be different from the actual ymin and ymax.
Is there a way to do this?

The alternative would be to have two graphs, one on top of the other.
With the latency graph on top, taking up most of the space, and the
usage graph below, with the scale of the n axis being identical.
However I can't see how do this.

Does anyone have any advice?

Thanks in advance,
Steven Murdoch.



From nusbj at hotmail.com  Tue Oct 19 11:21:26 2004
From: nusbj at hotmail.com (Zhen Pang)
Date: Tue, 19 Oct 2004 17:21:26 +0800
Subject: Question on Rprof(); was: Re: [R] sapply and loop
Message-ID: <BAY22-F36YP0QzBxhJT000185d4@hotmail.com>

Yes. It should have something to do with read/write permissions, but it is 
not clear how it happens.

I can write file to C drive using R. I usually write my results matrix to a 
txt file in C drive.

For Rprof(), the boot.out file can be created, but only with one line

sample.interval=20000

The situation is the same even if I specify the directory to the D 
drive,where I have the full read/write permissions.

Anyway, I do success in my own laptops. Thanks.

Yours,

Zhen

>From: Uwe Ligges <ligges at statistik.uni-dortmund.de>
>To: Zhen Pang <nusbj at hotmail.com>
>CC: tlumley at u.washington.edu, ripley at stats.ox.ac.uk, 
>r-help at stat.math.ethz.ch
>Subject: Question on Rprof(); was: Re: [R] sapply and loop
>Date: Tue, 19 Oct 2004 09:54:39 +0200
>
>Zhen Pang wrote:
>
>>I am sorry for neglecting the acknowledgement for `Writing R Extensions', 
>>since I think I am just citing the code from the orignal R-help. I fail to 
>>get the results when I use my own code. So I refer to this code where 
>>Rprof() appears. Anyway, I am sorry for this.
>>
>>In fact, I have tried to whether I specify boot.out. Neither one works.
>
>Do you have read/write permissions? R-1.8.1 is pretty old. Please upgrade.
>
>
>
>BTW: Rprof() to a non-existing directory
>    Rprof("c:/DoesNotExist/temp.out")
>under R-2.0.0, Windows, results in the GUI information message:
>"Fatal error: can't open profile file", and R is closed afterwards.
>
>I think we should change this behaviour to a simple error message by adding 
>the lines
>
>  if(!file.exists(dirname(filename)))
>      stop("Cannot open file ", filename)
>
>just before
>
>  invisible(.Internal(Rprof(filename, append, interval)))
>
>
>
>Uwe Ligges
>
>
>
>>Rprof("boot.out")
>>     storm.boot <- boot(rs, storm.bf, R = 4999) # pretty slow
>>      Rprof(NULL)
>>
>>summaryRprof()
>>Error in summaryRprof() : no events were recorded
>>
>>summaryRprof("boot.out")
>>Error in summaryRprof("boot.out") : no events were recorded
>>
>>     Rprof()
>>     storm.boot <- boot(rs, storm.bf, R = 4999) # pretty slow
>>      Rprof(NULL)
>>
>>summaryRprof()
>>Error in summaryRprof() : no events were recorded
>>
>>
>>Zhen
>>
>>
>>>From: Prof Brian Ripley <ripley at stats.ox.ac.uk>
>>>To: Zhen Pang <nusbj at hotmail.com>
>>>CC: tlumley at u.washington.edu, r-help at stat.math.ethz.ch
>>>Subject: Re: [R] sapply and loop
>>>Date: Tue, 19 Oct 2004 07:29:54 +0100 (BST)
>>>
>>>On Tue, 19 Oct 2004, Zhen Pang wrote:
>>>
>>> > I tried to use Rprof(). As an example, I consider the following code 
>>>(from
>>> > Venables & Ripley, 1999).
>>>
>>>I believe you parroted that from `Writing R Extensions', but failed to
>>>give proper credit!
>>>
>>> >      library(MASS); library(boot); library(nls)
>>> >      data(stormer)
>>> >      storm.fm <- nls(Time ~ b*Viscosity/(Wt - c), stormer,
>>> >                      start = c(b=29.401, c=2.2183))
>>> >      st <- cbind(stormer, fit=fitted(storm.fm))
>>> >      storm.bf <- function(rs, i) {
>>> >          st$Time <-  st$fit + rs[i]
>>> >          tmp <- nls(Time ~ (b * Viscosity)/(Wt - c), st,
>>> >                     start = coef(storm.fm))
>>> >          tmp$m$getAllPars()
>>> >      }
>>> >      rs <- scale(resid(storm.fm), scale = FALSE) # remove the mean
>>> >      Rprof("boot.out")
>>> >      storm.boot <- boot(rs, storm.bf, R = 4999) # pretty slow
>>> >      Rprof(NULL)
>>>
>>>At this point your unacknowledged copying went adrift.
>>>
>>> > summaryRprof()
>>> > Error in summaryRprof() : no events were recorded
>>> >
>>> > I am using R1.8.1 in windows. Why can't I get the results?
>>>
>>>Because you didn't do your homework, and didn't even follow your source.
>>>The 'file' arguments of Rprof and summaryProf have to agree: see their
>>>help pages (as the posting guide asks).
>>>
>>>--
>>>Brian D. Ripley,                  ripley at stats.ox.ac.uk
>>>Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
>>>University of Oxford,             Tel:  +44 1865 272861 (self)
>>>1 South Parks Road,                     +44 1865 272866 (PA)
>>>Oxford OX1 3TG, UK                Fax:  +44 1865 272595
>>>
>>>______________________________________________
>>>R-help at stat.math.ethz.ch mailing list
>>>https://stat.ethz.ch/mailman/listinfo/r-help
>>>PLEASE do read the posting guide! 
>>>http://www.R-project.org/posting-guide.html
>>
>>
>>______________________________________________
>>R-help at stat.math.ethz.ch mailing list
>>https://stat.ethz.ch/mailman/listinfo/r-help
>>PLEASE do read the posting guide! 
>>http://www.R-project.org/posting-guide.html
>
>______________________________________________
>R-help at stat.math.ethz.ch mailing list
>https://stat.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide! 
>http://www.R-project.org/posting-guide.html



From Ivy_Li at smics.com  Tue Oct 19 11:23:20 2004
From: Ivy_Li at smics.com (Ivy_Li)
Date: Tue, 19 Oct 2004 17:23:20 +0800
Subject: =?gb2312?B?tPC4tDogtPC4tDogtPC4tDogW1JdIEhvdyB0byBkcmF3IHgtYXhpcyB0aW0=?=
	=?gb2312?B?ZSBsYWJlbC4=?=
Message-ID: <8A910F1818425847A6D18C7832D207E503A6EE@ex115.smic-sh.com>


Thank you ! 
I run your code. It have a graph with time(date) on x-axis. Thank you very much!
Well,  Would you like to do me a favor again? 
Most of times, the x-axis time data is lasted about 3 months. They are too much. I must select some of them marking in x-axis,and they are must reasonable and  human-readable. Now I have tried the "pretty " function, But as soon as I run it,  the format of data changed to "numeric" ,I don't know what should I do. Is there exist other functions can get the same result?



Best Regards!
Ivy Li
YMS in Production & Testing
Semiconductor Manufactory International(ShangHai) Corporation
#18 ZhangJiang Road, PuDong New Area, Shanghai, China
Tel: 021-5080-2000 *11754
Email: Ivy_Li at smics.com



-----orig


Hi Li chen,

I mean you should run the code I sent you in my first email. It's here
again:

plot(x$x.name,as.character(x$y.name),pch=26)

By running this you should have a graph with time(date) on x-axis.

Give it a go? Let me know if it is not working.
Huang Huan



Internet
Ivy_Li at smics.com - 10/19/2004 02:18 AM


To:    Huan HUANG

cc:


Subject: ????: [R] How to draw x-axis time label.


Put *time* on x-axis? Like this:

>plot(time(x$x.name),as.character(x$y.name),pch=26)

but the x-axis is not real time , just some number.

I forgot your question in your previous mail. :-)
My chinese name is Li Chen
hello!

Best Regards!
Ivy Li
YMS in Production & Testing
Semiconductor Manufactory International(ShangHai) Corporation
#18 ZhangJiang Road, PuDong New Area, Shanghai, China
Tel: 021-5080-2000 *11754
Email: Ivy_Li at smics.com



-----orig-------
: huan.huang at uk.bnpparibas.com
[mailto:huan.huang at uk.bnpparibas.com]
: Ivy_Li
: Re: ..: [R] How to draw x-axis time label.


Come on, Ivy. Did you try the line in my previous email? That will put
*time* on x-axis. Give it a go?



Internet
Ivy_Li at smics.com - 10/18/2004 10:24 AM


To:    Huan HUANG

cc:


Subject:      : [R] How to draw x-axis time label.



Thank you !
I know the oddness number are time calulated since 1970, but i need them to
be  x-axis .
What  should  I do ?

Best Regards!
Ivy Li
YMS in Production & Testing
Semiconductor Manufactory International(ShangHai) Corporation
#18 ZhangJiang Road, PuDong New Area, Shanghai, China
Tel: 021-5080-2000 *11754
Email: Ivy_Li at smics.com



-----orig-------
To : huan.huang at uk.bnpparibas.com

Hi Ivy (any chinese name?)

I ran your codes. I found if I try:

plot(x$x.name,as.character(x$y.name),pch=26,)

It would put time as the x axis. The *odd* numbers showed in yoru graph as
the moment are numbers of seconds of those dates since the beginning of
1970.
Hope it helps.
Huang Huan



Internet
Ivy_Li at smics.com@stat.math.ethz.ch - 10/18/2004 07:26 AM


Sent by:    r-help-bounces at stat.math.ethz.ch

To:    r-help

cc:


Subject:    [R] How to draw x-axis time label.


Hi everybody,
 Could I consult one problem?
 It is about plot
 Now I do some analysis in plot . I need to draw a plot which x-axis is
 time . But when I run the funtion of plot . The x-label are very oddness
 number, such as the time is "2004-08-05 09:08:48", but the x-label is
 1091800000. I don't know how to do .
 I have an example . Mybe It  can explain my meaning clearly.
 Thank you for helping me!

Time <- c("2004-08-05 09:08:48", "2004-08-13 20:53:38",
   "2004-08-14 13:57:23", "2004-08-12 16:17:41",
   "2004-08-12 16:15:27", "2004-08-11 21:38:24",
   "2004-08-12 14:28:41", "2004-08-18 18:04:47",
   "2004-08-13 15:23:14", "2004-08-14 02:36:33")
Time <- as.POSIXlt(Time)
x <- data.frame(main.name="AAA",
fruit.name=rep(c("Apply","Watermelon"),each=5),
  x.name=Time, y.name=(1:10))
plot(as.numeric(x$x.name),as.character(x$y.name),pch=26,)
fruit.class <- table(x$fruit.name)
color.code <- c(611,552,656,121,451,481,28,652,32,550,90,401,150,12,520,8)
for(j in 1:length(fruit.class))
{
 fruit <- names(fruit.class)[j]
 lines(smooth.spline(x[x$fruit.name==fruit, "x.name"],
  x[x$fruit.name==fruit, "y.name"],df=5),
  col=colors()[color.code[j]],lwd=5)
}

Best Regards!
Ivy Li
YMS in Production & Testing
Semiconductor Manufactory International(ShangHai) Corporation
#18 ZhangJiang Road, PuDong New Area, Shanghai, China
Tel: 021-5080-2000 *11754
Email: Ivy_Li at smics.com

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-help
 PLEASE do read the posting guide!
 http://www.R-project.org/posting-guide.html



From Roger.Bivand at nhh.no  Tue Oct 19 11:29:15 2004
From: Roger.Bivand at nhh.no (Roger Bivand)
Date: Tue, 19 Oct 2004 11:29:15 +0200 (CEST)
Subject: [R] error in file(file, "r") 
In-Reply-To: <20041019011045.75399.qmail@web15804.mail.cnb.yahoo.com>
Message-ID: <Pine.LNX.4.44.0410191105150.523-100000@reclus.nhh.no>

On Tue, 19 Oct 2004, shizhu zang wrote:

> Hi, there. 
>  I am trying to read a file into R, but I got following message 
> Error in file(file, "r") : unable to open connection In addition: Warning message: cannot open file `SwirlSample.txt' .
> In fact, my file has been already in the current working directory.
> My R is running on window operation system instead of UNIX.

There are a couple of things that even experienced users of Windows can
find useful to note: one is the nice little function called list.files(),
which shows you the files in the current working directory. It happens
that R is not started in the directory you think, and this shows you what
is in your working directory; you can see the name of the working 
directory with the getwd() function (R for Windows FAQ 2.12). 

Secondly, you can change your working directory either from the File menu,
or with the setwd() function. 

Thirdly, you can put file.choose() in place of the file name when you are
reading your file, which lets you choose the file interactively - this
also helps if you have difficulty understanding how R deals with
backslashes and forward slashes in file names with directory names (this 
is R for Windows FAQ 2.14). I find it helps to think of directory name 
separators like web page addresses, using forward slashes does work.

Finally, remember that Windows may hide the extension part of the file
name, so that the real name of the file may be different from what Windows
Explorer shows you (unsetting "Hide file extension for known file types"  
in the View tab of Folder alternatives under the Tools menu works, I'm not
sure if I've translated these right). If you have downloaded a file using
a web browser, you often find that it adds an extra extension (often
.txt), making it seem that the file isn't there, while it only has a
different name than the name Windows shows you. This can be abated by
always choosing "All files"  before saving. (I think these are among the
most frequently asked questions for people starting R under Windows).

> Thanks. 
> Shizhu Zang 
> Department of Biochemsitry 
> Peking University
> Beijing 
> China
> 
> 
> 
> 
> 
> ---------------------------------
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
> 

-- 
Roger Bivand
Economic Geography Section, Department of Economics, Norwegian School of
Economics and Business Administration, Breiviksveien 40, N-5045 Bergen,
Norway. voice: +47 55 95 93 55; fax +47 55 95 93 93
e-mail: Roger.Bivand at nhh.no



From mdowle at concordiafunds.com  Tue Oct 19 11:44:20 2004
From: mdowle at concordiafunds.com (Matthew Dowle)
Date: Tue, 19 Oct 2004 10:44:20 +0100
Subject: [R] RDCOMClient under R2.0.0 - "not a valid package"
Message-ID: <78166BFC5165D811AA0400065BF0324B314A12@wisconsin.concordia>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20041019/0e79949c/attachment.pl

From csillery at selway.umt.edu  Tue Oct 19 11:51:21 2004
From: csillery at selway.umt.edu (Katalin Csillery)
Date: Tue, 19 Oct 2004 03:51:21 -0600 (MDT)
Subject: [R] installing package 'kinship'
In-Reply-To: <Pine.LNX.4.44.0410181547560.13863-100000@gannet.stats>
Message-ID: <Pine.OSF.4.21.0410190227410.25048-100000@selway.umt.edu>


Thanks for getting back to me.
My version is OS 10.3.5, I use X11 and ess.
Could you please advise me exactly how do I need to change my coxfit6.h
file? How should I install the modified package? I don't know how to use
'install.packages' when the package I want to istall has actually been
downloaded to me computer.

Thanks in advance,
Kati

On Mon, 18 Oct 2004, Prof Brian Ripley wrote:

> Date: Mon, 18 Oct 2004 15:54:16 +0100 (BST)
> From: Prof Brian Ripley <ripley at stats.ox.ac.uk>
> To: Katalin  Csillery <csillery at selway.umt.edu>
> Cc: R-help <R-help at stat.math.ethz.ch>
> Subject: Re: [R] installing package 'kinship'
> 
> Please read the posting guide, and do tell us what OS this is.
> 
> I can guess it is MacOS X, an OS on which quite a number of packages do
> not compile.  You need to change coxfit6.h to have the object extern in
> all but one of the files it is included in.  That is a peculiarity of
> MacOS X.
> 
> 
> On Mon, 18 Oct 2004, Katalin  Csillery wrote:
> 
> > 
> > 
> > Dear All,
> > 
> > I have problem installing the 'kinship' package. I used the
> > 'install.packages' from R command line and had no problem installing other
> > packages before.
> > Here is what I get:
> > 
> > * Installing *source* package 'kinship' ...
> > ** libs
> > gcc -no-cpp-precomp -I/Library/Frameworks/R.framework/Resources/include
> > -I/usr/local/include   -fno-common  -g -O2 -c agfit6b.c -o agfit6b.o
> > gcc -no-cpp-precomp -I/Library/Frameworks/R.framework/Resources/include
> > -I/usr/local/include   -fno-common  -g -O2 -c bdsmatrix_index1.c -o
> > bdsmatrix_index1.o
> > gcc -no-cpp-precomp -I/Library/Frameworks/R.framework/Resources/include
> > -I/usr/local/include   -fno-common  -g -O2 -c bdsmatrix_index2.c -o
> > bdsmatrix_index2.o
> > gcc -no-cpp-precomp -I/Library/Frameworks/R.framework/Resources/include
> > -I/usr/local/include   -fno-common  -g -O2 -c bdsmatrix_index3.c -o
> > bdsmatrix_index3.o
> > gcc -no-cpp-precomp -I/Library/Frameworks/R.framework/Resources/include
> > -I/usr/local/include   -fno-common  -g -O2 -c bdsmatrix_prod.c -o
> > bdsmatrix_prod.o
> > gcc -no-cpp-precomp -I/Library/Frameworks/R.framework/Resources/include
> > -I/usr/local/include   -fno-common  -g -O2 -c bdsmatrix_prod2.c -o
> > bdsmatrix_prod2.o
> > gcc -no-cpp-precomp -I/Library/Frameworks/R.framework/Resources/include
> > -I/usr/local/include   -fno-common  -g -O2 -c bdsmatrix_prod3.c -o
> > bdsmatrix_prod3.o
> > gcc -no-cpp-precomp -I/Library/Frameworks/R.framework/Resources/include
> > -I/usr/local/include   -fno-common  -g -O2 -c chinv4.c -o chinv4.o
> > gcc -no-cpp-precomp -I/Library/Frameworks/R.framework/Resources/include
> > -I/usr/local/include   -fno-common  -g -O2 -c chinv5.c -o chinv5.o
> > gcc -no-cpp-precomp -I/Library/Frameworks/R.framework/Resources/include
> > -I/usr/local/include   -fno-common  -g -O2 -c cholesky4.c -o cholesky4.o
> > gcc -no-cpp-precomp -I/Library/Frameworks/R.framework/Resources/include
> > -I/usr/local/include   -fno-common  -g -O2 -c cholesky5.c -o cholesky5.o
> > gcc -no-cpp-precomp -I/Library/Frameworks/R.framework/Resources/include
> > -I/usr/local/include   -fno-common  -g -O2 -c chsolve4.c -o chsolve4.o
> > gcc -no-cpp-precomp -I/Library/Frameworks/R.framework/Resources/include
> > -I/usr/local/include   -fno-common  -g -O2 -c chsolve5.c -o chsolve5.o
> > gcc -no-cpp-precomp -I/Library/Frameworks/R.framework/Resources/include
> > -I/usr/local/include   -fno-common  -g -O2 -c coxfit6a.c -o coxfit6a.o
> > gcc -no-cpp-precomp -I/Library/Frameworks/R.framework/Resources/include
> > -I/usr/local/include   -fno-common  -g -O2 -c coxfit6b.c -o coxfit6b.o
> > gcc -no-cpp-precomp -I/Library/Frameworks/R.framework/Resources/include
> > -I/usr/local/include   -fno-common  -g -O2 -c coxfit6c.c -o coxfit6c.o
> > gcc -no-cpp-precomp -I/Library/Frameworks/R.framework/Resources/include
> > -I/usr/local/include   -fno-common  -g -O2 -c gchol.c -o gchol.o
> > gcc -no-cpp-precomp -I/Library/Frameworks/R.framework/Resources/include
> > -I/usr/local/include   -fno-common  -g -O2 -c gchol_bds.c -o gchol_bds.o
> > gcc -bundle -flat_namespace -undefined suppress -L/usr/local/lib -o
> > kinship.so agfit6b.o bdsmatrix_index1.o bdsmatrix_index2.o
> > bdsmatrix_index3.o bdsmatrix_prod.o bdsmatrix_prod2.o bdsmatrix_prod3.o
> > chinv4.o chinv5.o cholesky4.o cholesky5.o
> > chsolve4.o chsolve5.o coxfit6a.o coxfit6b.o coxfit6c.o gchol.o gchol_bds.o
> > -lcc_dynamic -framework R
> > ld: multiple definitions of symbol _coxfit6
> > agfit6b.o definition of _coxfit6 in section (__DATA,__common)
> > coxfit6a.o definition of _coxfit6 in section (__DATA,__common)
> > coxfit6b.o definition of _coxfit6 in section (__DATA,__common)
> > coxfit6c.o definition of _coxfit6 in section (__DATA,__common)
> > make: *** [kinship.so] Error 1
> > ERROR: compilation failed for package 'kinship'
> > ** Removing
> > '/Library/Frameworks/R.framework/Versions/1.9.1/Resources/library/kinship'
> > 
> > Thanks for your help in advance, 
> > Kati
> > 
> > ______________________________________________
> > R-help at stat.math.ethz.ch mailing list
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
> > 
> > 
> 
> -- 
> Brian D. Ripley,                  ripley at stats.ox.ac.uk
> Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
> University of Oxford,             Tel:  +44 1865 272861 (self)
> 1 South Parks Road,                     +44 1865 272866 (PA)
> Oxford OX1 3TG, UK                Fax:  +44 1865 272595
> 
>



From ligges at statistik.uni-dortmund.de  Tue Oct 19 12:03:34 2004
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Tue, 19 Oct 2004 12:03:34 +0200
Subject: [R] Overlaying data on graphs
In-Reply-To: <20041019091859.GB17090@cl.cam.ac.uk>
References: <20041019091859.GB17090@cl.cam.ac.uk>
Message-ID: <4174E676.3070005@statistik.uni-dortmund.de>


par("usr") is your friend - both to ask for the height and also for 
setting a new height.

Uwe Ligges


Steven Murdoch wrote:

> [sorry, my last message seems to have been corrupted - here it is
> again.]
> 
> I currently have a graph from network measurements of latency against
> time and would like to overlay some extra data on top, but with a
> different y scale, but I am not sure how to do this in an automated
> manner.
> 
> The base graph is generated by these commands:
>  time=probe$rsec+(probe$rusec/1000000)
>  diff=((probe$rsec-probe$ssec)*1000000)+(probe$rusec-probe$susec)
>  plot(time[100:len],diff[100:len])
> 
> Then I would like to overlay data on the usage of the network. In this
> data the x scale is the same (time), but the y axis is unrelated
> (buffer size). The approach I have currently been using is simply
> drawing on top of the graph:
>  lines(usage$time,(usage$bytes)*m+c,type="h",col="blue")
> 
> The problem with this is that to make the lines visible, but not
> taking up the whole height, I need to manually tweak the m and c
> factors, based on the relative values of the usage and probe data.
> Since I have a lot of graphs to generate, I would like to do this
> automatically.
> 
> I haven't been able to do this, because I can't find out the ymin and
> ymax of the graph generated by plot. If I could then it would be
> trivial to scale the factors so they took up say, 10% of the vertical
> space. There is par("yaxp"), but this shows the positions of the
> tickmarks, but these can be different from the actual ymin and ymax.
> Is there a way to do this?
> 
> The alternative would be to have two graphs, one on top of the other.
> With the latency graph on top, taking up most of the space, and the
> usage graph below, with the scale of the n axis being identical.
> However I can't see how do this.
> 
> Does anyone have any advice?
> 
> Thanks in advance,
> Steven Murdoch.
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html



From ripley at stats.ox.ac.uk  Tue Oct 19 12:07:01 2004
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Tue, 19 Oct 2004 11:07:01 +0100 (BST)
Subject: [R] installing package 'kinship'
In-Reply-To: <Pine.OSF.4.21.0410190227410.25048-100000@selway.umt.edu>
Message-ID: <Pine.LNX.4.44.0410191104220.10847-100000@gannet.stats>

On Tue, 19 Oct 2004, Katalin  Csillery wrote:

> 
> Thanks for getting back to me.
> My version is OS 10.3.5, I use X11 and ess.
> Could you please advise me exactly how do I need to change my coxfit6.h
> file? 

Ask someone who knows for sure about the peculiar features of MacOS X.

> How should I install the modified package? I don't know how to use
> 'install.packages' when the package I want to istall has actually been
> downloaded to me computer.

The help page says:

See Also:

     'INSTALL', 'REMOVE', 'library', '.packages', 'read.dcf'

so please do ?INSTALL (or read the `Installation and Administration 
Manual').


> 
> Thanks in advance,
> Kati
> 
> On Mon, 18 Oct 2004, Prof Brian Ripley wrote:
> 
> > Date: Mon, 18 Oct 2004 15:54:16 +0100 (BST)
> > From: Prof Brian Ripley <ripley at stats.ox.ac.uk>
> > To: Katalin  Csillery <csillery at selway.umt.edu>
> > Cc: R-help <R-help at stat.math.ethz.ch>
> > Subject: Re: [R] installing package 'kinship'
> > 
> > Please read the posting guide, and do tell us what OS this is.
> > 
> > I can guess it is MacOS X, an OS on which quite a number of packages do
> > not compile.  You need to change coxfit6.h to have the object extern in
> > all but one of the files it is included in.  That is a peculiarity of
> > MacOS X.
> > 
> > 
> > On Mon, 18 Oct 2004, Katalin  Csillery wrote:
> > 
> > > 
> > > 
> > > Dear All,
> > > 
> > > I have problem installing the 'kinship' package. I used the
> > > 'install.packages' from R command line and had no problem installing other
> > > packages before.
> > > Here is what I get:
> > > 
> > > * Installing *source* package 'kinship' ...
> > > ** libs
> > > gcc -no-cpp-precomp -I/Library/Frameworks/R.framework/Resources/include
> > > -I/usr/local/include   -fno-common  -g -O2 -c agfit6b.c -o agfit6b.o
> > > gcc -no-cpp-precomp -I/Library/Frameworks/R.framework/Resources/include
> > > -I/usr/local/include   -fno-common  -g -O2 -c bdsmatrix_index1.c -o
> > > bdsmatrix_index1.o
> > > gcc -no-cpp-precomp -I/Library/Frameworks/R.framework/Resources/include
> > > -I/usr/local/include   -fno-common  -g -O2 -c bdsmatrix_index2.c -o
> > > bdsmatrix_index2.o
> > > gcc -no-cpp-precomp -I/Library/Frameworks/R.framework/Resources/include
> > > -I/usr/local/include   -fno-common  -g -O2 -c bdsmatrix_index3.c -o
> > > bdsmatrix_index3.o
> > > gcc -no-cpp-precomp -I/Library/Frameworks/R.framework/Resources/include
> > > -I/usr/local/include   -fno-common  -g -O2 -c bdsmatrix_prod.c -o
> > > bdsmatrix_prod.o
> > > gcc -no-cpp-precomp -I/Library/Frameworks/R.framework/Resources/include
> > > -I/usr/local/include   -fno-common  -g -O2 -c bdsmatrix_prod2.c -o
> > > bdsmatrix_prod2.o
> > > gcc -no-cpp-precomp -I/Library/Frameworks/R.framework/Resources/include
> > > -I/usr/local/include   -fno-common  -g -O2 -c bdsmatrix_prod3.c -o
> > > bdsmatrix_prod3.o
> > > gcc -no-cpp-precomp -I/Library/Frameworks/R.framework/Resources/include
> > > -I/usr/local/include   -fno-common  -g -O2 -c chinv4.c -o chinv4.o
> > > gcc -no-cpp-precomp -I/Library/Frameworks/R.framework/Resources/include
> > > -I/usr/local/include   -fno-common  -g -O2 -c chinv5.c -o chinv5.o
> > > gcc -no-cpp-precomp -I/Library/Frameworks/R.framework/Resources/include
> > > -I/usr/local/include   -fno-common  -g -O2 -c cholesky4.c -o cholesky4.o
> > > gcc -no-cpp-precomp -I/Library/Frameworks/R.framework/Resources/include
> > > -I/usr/local/include   -fno-common  -g -O2 -c cholesky5.c -o cholesky5.o
> > > gcc -no-cpp-precomp -I/Library/Frameworks/R.framework/Resources/include
> > > -I/usr/local/include   -fno-common  -g -O2 -c chsolve4.c -o chsolve4.o
> > > gcc -no-cpp-precomp -I/Library/Frameworks/R.framework/Resources/include
> > > -I/usr/local/include   -fno-common  -g -O2 -c chsolve5.c -o chsolve5.o
> > > gcc -no-cpp-precomp -I/Library/Frameworks/R.framework/Resources/include
> > > -I/usr/local/include   -fno-common  -g -O2 -c coxfit6a.c -o coxfit6a.o
> > > gcc -no-cpp-precomp -I/Library/Frameworks/R.framework/Resources/include
> > > -I/usr/local/include   -fno-common  -g -O2 -c coxfit6b.c -o coxfit6b.o
> > > gcc -no-cpp-precomp -I/Library/Frameworks/R.framework/Resources/include
> > > -I/usr/local/include   -fno-common  -g -O2 -c coxfit6c.c -o coxfit6c.o
> > > gcc -no-cpp-precomp -I/Library/Frameworks/R.framework/Resources/include
> > > -I/usr/local/include   -fno-common  -g -O2 -c gchol.c -o gchol.o
> > > gcc -no-cpp-precomp -I/Library/Frameworks/R.framework/Resources/include
> > > -I/usr/local/include   -fno-common  -g -O2 -c gchol_bds.c -o gchol_bds.o
> > > gcc -bundle -flat_namespace -undefined suppress -L/usr/local/lib -o
> > > kinship.so agfit6b.o bdsmatrix_index1.o bdsmatrix_index2.o
> > > bdsmatrix_index3.o bdsmatrix_prod.o bdsmatrix_prod2.o bdsmatrix_prod3.o
> > > chinv4.o chinv5.o cholesky4.o cholesky5.o
> > > chsolve4.o chsolve5.o coxfit6a.o coxfit6b.o coxfit6c.o gchol.o gchol_bds.o
> > > -lcc_dynamic -framework R
> > > ld: multiple definitions of symbol _coxfit6
> > > agfit6b.o definition of _coxfit6 in section (__DATA,__common)
> > > coxfit6a.o definition of _coxfit6 in section (__DATA,__common)
> > > coxfit6b.o definition of _coxfit6 in section (__DATA,__common)
> > > coxfit6c.o definition of _coxfit6 in section (__DATA,__common)
> > > make: *** [kinship.so] Error 1
> > > ERROR: compilation failed for package 'kinship'
> > > ** Removing
> > > '/Library/Frameworks/R.framework/Versions/1.9.1/Resources/library/kinship'
> > > 
> > > Thanks for your help in advance, 
> > > Kati
> > > 
> > > ______________________________________________
> > > R-help at stat.math.ethz.ch mailing list
> > > https://stat.ethz.ch/mailman/listinfo/r-help
> > > PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
> > > 
> > > 
> > 
> > -- 
> > Brian D. Ripley,                  ripley at stats.ox.ac.uk
> > Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
> > University of Oxford,             Tel:  +44 1865 272861 (self)
> > 1 South Parks Road,                     +44 1865 272866 (PA)
> > Oxford OX1 3TG, UK                Fax:  +44 1865 272595
> > 
> > 
> 
> 
> 

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From vito_ricci at yahoo.com  Tue Oct 19 12:07:40 2004
From: vito_ricci at yahoo.com (Vito Ricci)
Date: Tue, 19 Oct 2004 12:07:40 +0200 (CEST)
Subject: [R] Time series reference card
Message-ID: <20041019100740.78437.qmail@web41211.mail.yahoo.com>

Hi,

I wish to informe all of you that on the CRAN is now
available a short document concerning the main R
functions for time series analysis.

PDF format:
http://cran.r-project.org/doc/contrib/Ricci-refcard-ts.pdf
DOC format:
http://cran.r-project.org/doc/contrib/Ricci-refcard-ts.doc

Best
Vito Ricci

=====
Diventare costruttori di soluzioni

"The business of the statistician is to catalyze 
the scientific learning process."  
George E. P. Box


Visitate il portale http://www.modugno.it/
e in particolare la sezione su Palese http://www.modugno.it/archivio/cat_palese.shtml



From ripley at stats.ox.ac.uk  Tue Oct 19 12:15:57 2004
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Tue, 19 Oct 2004 11:15:57 +0100 (BST)
Subject: [R] RDCOMClient under R2.0.0 - "not a valid package"
In-Reply-To: <78166BFC5165D811AA0400065BF0324B314A12@wisconsin.concordia>
Message-ID: <Pine.LNX.4.44.0410191109300.10847-100000@gannet.stats>

Try <duncan at wald.ucdavis.edu>.

I am afraid you do need to install the package from the sources for use 
under a later version of R than that used for www.omegahat.org.

On Tue, 19 Oct 2004, Matthew Dowle wrote:

> Both 'omega-devel at omegahat.org' and 'duncan at research.bell-labs.com' appear
> to be bouncing with 'user unknown' so trying r-help ....
> [ Btw, "mailing lists" link on omega page is returning "mailman CGI error".
> ]

Yes, it has been reported a few times.

> > > require("RDCOMClient")
> > Loading required package: RDCOMClient 
> > Error in library(package, character.only = TRUE, logical = TRUE,
> > warn.conflicts = warn.conflicts,  : 
> >         'RDCOMClient' is not a valid package -- installed < 2.0.0?
> > > version
> >          _              
> > platform i386-pc-mingw32
> > arch     i386           
> > os       mingw32        
> > system   i386, mingw32  
> > status   beta           
> > major    2              
> > minor    0.0            
> > year     2004           
> > month    09             
> > day      28             
> > language R              

It might be a good idea to use the final released version of 2.0.0.

> > I have re-installed RDCOMClient_0.8-1.zip under R2.0.0 but still getting
> > the error above.   Any ideas/suggestions much appreciated.  The file
> > ../library/RDCOMClient/Meta/package.rds appears to be missing?

And several other files will also be.

If you need to use the binaries on www.omegahat.org, you need to use R < 
2.0.0, and I keep 1.9.1 installed for such uses (Bioconductor, too).

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From zangshizhu at yahoo.com.cn  Tue Oct 19 12:30:36 2004
From: zangshizhu at yahoo.com.cn (shizhu zang)
Date: Tue, 19 Oct 2004 18:30:36 +0800 (CST)
Subject: [R] Error in grep
Message-ID: <20041019103036.17237.qmail@web15810.mail.cnb.yahoo.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20041019/b6912c9f/attachment.pl

From ernesto at ipimar.pt  Tue Oct 19 12:46:03 2004
From: ernesto at ipimar.pt (Ernesto Jardim)
Date: Tue, 19 Oct 2004 11:46:03 +0100
Subject: [R] Help with strange vertical line in plot
Message-ID: <1098182762.21703.16.camel@mordor.ipimar.pt>

Hi,

I'm ploting a portuguese map with "plot" but I'm getting a strange
vertical line. 

To avoid posting binary files, I've rolled the data file, an R script
and the resulted ps file in this tgz file
http://ernesto.freezope.org/ploterror.tgz.

Can someone give me an help on this ?

Thanks

EJ



From ripley at stats.ox.ac.uk  Tue Oct 19 13:04:57 2004
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Tue, 19 Oct 2004 12:04:57 +0100 (BST)
Subject: [R] Error in limma (was Error in grep)
In-Reply-To: <20041019103036.17237.qmail@web15810.mail.cnb.yahoo.com>
Message-ID: <Pine.LNX.4.44.0410191202120.10847-100000@gannet.stats>

On Tue, 19 Oct 2004, shizhu zang wrote:

> I used the function read.maimages in limma package to analyze the
> microarray data .but I got following message

> >RG <- read.maimages(targets$FileName, source="spot")
> Error in grep(pattern, x, ignore.case, extended, value, fixed) : 
>         invalid argument
> I don't know what is the matter 

It's an error from the limma package, not in grep (which is reporting that
it is being used incorrectly).  Please quote accurately.

It could be an error in your usage of limma, or in the package itself, but 
in either case you need to discuss it with the package maintainer in the 
first instance.


-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From Roger.Bivand at nhh.no  Tue Oct 19 13:34:48 2004
From: Roger.Bivand at nhh.no (Roger Bivand)
Date: Tue, 19 Oct 2004 13:34:48 +0200 (CEST)
Subject: [R] Help with strange vertical line in plot
In-Reply-To: <1098182762.21703.16.camel@mordor.ipimar.pt>
Message-ID: <Pine.LNX.4.44.0410191329200.523-100000@reclus.nhh.no>

On Tue, 19 Oct 2004, Ernesto Jardim wrote:

> Hi,
> 
> I'm ploting a portuguese map with "plot" but I'm getting a strange
> vertical line. 
> 
> To avoid posting binary files, I've rolled the data file, an R script
> and the resulted ps file in this tgz file
> http://ernesto.freezope.org/ploterror.tgz.
> 

It is the 

axis(2, outer=T, line=-1.25, cex=0.7, las=1)

line in the code, line= argument, set to -0.75 looks better? Is there a 
way to set line= from what par() knows?

Roger

> Can someone give me an help on this ?
> 
> Thanks
> 
> EJ
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
> 

-- 
Roger Bivand
Economic Geography Section, Department of Economics, Norwegian School of
Economics and Business Administration, Breiviksveien 40, N-5045 Bergen,
Norway. voice: +47 55 95 93 55; fax +47 55 95 93 93
e-mail: Roger.Bivand at nhh.no



From jonathan.williams at pharmacology.oxford.ac.uk  Tue Oct 19 13:35:51 2004
From: jonathan.williams at pharmacology.oxford.ac.uk (Jonathan Williams)
Date: Tue, 19 Oct 2004 12:35:51 +0100
Subject: [R] Fatal error: invalid home drive
Message-ID: <NGBBKJEMOMLJFCOIEGCEKEDNJMAA.jonathan.williams@pharm.ox.ac.uk>

Dear R-helpers,

I have just installed R2.0.0 onto my machine which already had R1.9.1
present.
I then tried to add to R2.0.0 a library called GLMM written by James McBroom
for R1.6.0. Unfortunately, R2.0.0 does not recognise the library, even
though
R1.9.1 does. This is not because I have used the wrong case in the call to
the
library:
> library('GLMM')
> "Error in library("GLMM") : 'GLMM' is not a valid package -- installed <
2.0.0?"

So, I was hoping to revert to using GLMM in R1.9.1. BUT, when I now try to
load
R1.9.1, I receive "Fatal error: invalid home drive".

Is there any way I can resurrect my R1.9.1, which is still present
physically
on my HDD? I am working with Windows XP on a 2.4GHz desktop with a large
HDD.

Is it now the case that R2.0.0 will only run packages which it has installed
itself, and not those that the user imports from earlier versions of R?

Thanks for your help,

Jonathan Williams



From ligges at statistik.uni-dortmund.de  Tue Oct 19 14:03:19 2004
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Tue, 19 Oct 2004 14:03:19 +0200
Subject: [R] Fatal error: invalid home drive
In-Reply-To: <NGBBKJEMOMLJFCOIEGCEKEDNJMAA.jonathan.williams@pharm.ox.ac.uk>
References: <NGBBKJEMOMLJFCOIEGCEKEDNJMAA.jonathan.williams@pharm.ox.ac.uk>
Message-ID: <41750287.30407@statistik.uni-dortmund.de>

Jonathan Williams wrote:

> Dear R-helpers,
> 
> I have just installed R2.0.0 onto my machine which already had R1.9.1
> present.
> I then tried to add to R2.0.0 a library called GLMM written by James McBroom
> for R1.6.0. Unfortunately, R2.0.0 does not recognise the library, even
> though
> R1.9.1 does. This is not because I have used the wrong case in the call to
> the
> library:
> 
>>library('GLMM')
>>"Error in library("GLMM") : 'GLMM' is not a valid package -- installed <
> 
> 2.0.0?"
> 
> So, I was hoping to revert to using GLMM in R1.9.1. BUT, when I now try to
> load
> R1.9.1, I receive "Fatal error: invalid home drive".

Please read the mailing list archives. I think this is a bug introduced 
by a Microsoft Windows XP patch some time (i.e. several months) ago. 
"R-1.9.1 patched" has a workaround.


> Is there any way I can resurrect my R1.9.1, which is still present
> physically
> on my HDD? I am working with Windows XP on a 2.4GHz desktop with a large
> HDD.
> 
> Is it now the case that R2.0.0 will only run packages which it has installed
> itself, and not those that the user imports from earlier versions of R?

Well, you have to re-install packages, because some changes in R-2.0.0 
make it necessary.

Please take the sources of "GLMM", read .../src/gnuwin32/readme.packages 
and follow the advice how to compile packages yourself.

Uwe Ligges


> Thanks for your help,
> 
> Jonathan Williams
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html



From bates at stat.wisc.edu  Tue Oct 19 14:27:39 2004
From: bates at stat.wisc.edu (Douglas Bates)
Date: Tue, 19 Oct 2004 07:27:39 -0500
Subject: [R] matrix of eigenvalues
In-Reply-To: <a06002006bd9a7b5ffb79@[130.120.104.141]>
References: <a06002006bd9a7b5ffb79@[130.120.104.141]>
Message-ID: <4175083B.6090605@stat.wisc.edu>

Christian Jost wrote:
> I thought that the function
> eigen(A)
> will return a matrix with eigenvectors that are independent of each 
> other (thus forming a base and the matrix being invertible). This seems 
> not to be the case in the following example
> A=matrix(c(1,2,0,1),nrow=2,byrow=T)
> eigen(A) ->ev
> solve(ev$vectors)
> 
> note that I try to get the upper triangular form with eigenvalues on the 
> diagonal and (possibly) 1 just atop the eigenvalues to be used to solve 
> a linear differential equation
> x' = Ax, x(0)=x0
> x(t) = P exp(D t) P^-1 x0
> where D is this upper triangular form and P is the "passage matrix" (not 
> sure about the correct english name) given by a base of eigenvectors. So 
> the test would be
> solve(ev$vectors) %*% A %*% ev$vectors - D
> should be 0
> 
> Thanks for any help, Christian.
> 
> ps: please copy reply also to my address, my subscription to the R-help 
> list seems to have delays

That particular matrix has repeated eigenvalues and a degenerate eigenspace.

 > A <- matrix(c(1,0,2,1),nc=2)
 > A
      [,1] [,2]
[1,]    1    2
[2,]    0    1
 > eigen(A)
$values
[1] 1 1

$vectors
      [,1]          [,2]
[1,]    1 -1.000000e+00
[2,]    0  1.110223e-16



From kjetil at acelerate.com  Tue Oct 19 14:31:51 2004
From: kjetil at acelerate.com (Kjetil Brinchmann Halvorsen)
Date: Tue, 19 Oct 2004 08:31:51 -0400
Subject: [R] matrix of eigenvalues
In-Reply-To: <a06002006bd9a7b5ffb79@[130.120.104.141]>
References: <a06002006bd9a7b5ffb79@[130.120.104.141]>
Message-ID: <41750937.6020900@acelerate.com>

Christian Jost wrote:

> I thought that the function
> eigen(A)
> will return a matrix with eigenvectors that are independent of each 
> other (thus forming a base and the matrix being invertible). This 
> seems not to be the case in the following example
> A=matrix(c(1,2,0,1),nrow=2,byrow=T)
> eigen(A) ->ev
> solve(ev$vectors)
>
I guess eigen tries to get independent eigenvectors, butr that is not 
always possible, and your matrix is a case of that.
Note that all eigenvectors of A are a multiple of (0,1)^T, so there 
cannot be two independent ones.

Kjetil

> note that I try to get the upper triangular form with eigenvalues on 
> the diagonal and (possibly) 1 just atop the eigenvalues to be used to 
> solve a linear differential equation
> x' = Ax, x(0)=x0
> x(t) = P exp(D t) P^-1 x0
> where D is this upper triangular form and P is the "passage matrix" 
> (not sure about the correct english name) given by a base of 
> eigenvectors. So the test would be
> solve(ev$vectors) %*% A %*% ev$vectors - D
> should be 0
>
> Thanks for any help, Christian.
>
> ps: please copy reply also to my address, my subscription to the 
> R-help list seems to have delays



-- 

Kjetil Halvorsen.

Peace is the most effective weapon of mass construction.
               --  Mahdi Elmandjra



From marchini at ifc.cnr.it  Tue Oct 19 15:05:35 2004
From: marchini at ifc.cnr.it (marchini)
Date: Tue, 19 Oct 2004 15:05:35 +0200
Subject: [R] AIC in GAM model
Message-ID: <HPEKKMAIJDHOLIFOCLOHOEOBCAAA.marchini@ifc.cnr.it>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20041019/6a6bc68e/attachment.pl

From ernesto at ipimar.pt  Tue Oct 19 14:47:24 2004
From: ernesto at ipimar.pt (Ernesto Jardim)
Date: Tue, 19 Oct 2004 13:47:24 +0100
Subject: [R] Help with strange vertical line in plot
In-Reply-To: <Pine.LNX.4.44.0410191329200.523-100000@reclus.nhh.no>
References: <Pine.LNX.4.44.0410191329200.523-100000@reclus.nhh.no>
Message-ID: <1098190044.21703.73.camel@mordor.ipimar.pt>

On Tue, 2004-10-19 at 12:34, Roger Bivand wrote:
> On Tue, 19 Oct 2004, Ernesto Jardim wrote:
> 
> > Hi,
> > 
> > I'm ploting a portuguese map with "plot" but I'm getting a strange
> > vertical line. 
> > 
> > To avoid posting binary files, I've rolled the data file, an R script
> > and the resulted ps file in this tgz file
> > http://ernesto.freezope.org/ploterror.tgz.
> > 
> 
> It is the 
> 
> axis(2, outer=T, line=-1.25, cex=0.7, las=1)
> 
> line in the code, line= argument, set to -0.75 looks better? Is there a 
> way to set line= from what par() knows?
> 
> Roger
> 
> > Can someone give me an help on this ?
> > 
> > Thanks
> > 
> > EJ
> > 


Yes, thank you.

EJ



From kuan at ilife.cx  Tue Oct 19 15:14:27 2004
From: kuan at ilife.cx (Kuan-Ta Chen)
Date: Tue, 19 Oct 2004 21:14:27 +0800
Subject: [R] Survreg with gamma distribution
References: <200410171355.i9HDtraR022126@outmx022.isp.belgacom.be>
	<002001c4b458$b4ebf1e0$0101a8c0@eureka>
	<Pine.A41.4.61a.0410180837430.319578@homer08.u.washington.edu>
	<004e01c4b534$24ae0e10$0101a8c0@eureka>
	<Pine.A41.4.61a.0410181020030.135076@homer08.u.washington.edu>
Message-ID: <005b01c4b5dd$90e05f80$0101a8c0@eureka>

Many thanks to Prof. Lumley. You are right, the likelihood function could be
written with builtin gamma distribution functions. Now the MLE works
completely fine. :-)

Best Regards,
Kuan-Ta Chen

----- Original Message ----- 
From: "Thomas Lumley" <tlumley at u.washington.edu>
To: "Kuan-Ta Chen" <kuan at ilife.cx>
Cc: <r-help at stat.math.ethz.ch>
Sent: Tuesday, October 19, 2004 1:37 AM
Subject: Re: [R] Survreg with gamma distribution


> On Tue, 19 Oct 2004, Kuan-Ta Chen wrote:
>
> >
> > One million thanks to Prof. Ripley and Prof. Lumley. I think I now have
more
> > understanding regarding survreg with gamma distribution. But one of my
> > problems is still there: in the text of Lee, Wang (2003), there are two
> > "kinds" of parametric fitting: 1) fitting of survival distributions
(like
> > regular probabillity distribution fitting) 2) regression model fitting
> > (mostly assume an accelerated failure time model). Survreg {survival}
> > provides model fitting of (2). But I still have one problem regarding
(1):
> > try to estimate the parameters of gamma distributions for some data.
>
> There aren't really two separate kinds: 1 is a special case of 2, so
> survreg() can do 1.
>
> > For regular gamma distr. fitting, we could use fitdistr (mass) or use
> > optim()/mle() with log-likelihood composed by dgamma()/pgamma(). But
because
> > the data contains (randomly) censored observations, the log-likelihood
> > function must be modified to include the effect of duration of censored
> > observations.
>
> Yes. The loglikelihood is
>    pgamma(x,shape,scale=scale,lower.tail=FALSE,log.p=TRUE)
> for a censored observation and
>    dgamma(x,shape,scale=scale,log=TRUE)
> for an uncensored observation. No integration necessary.
>
> You might want to work with log(shape) and log(scale) instead, to avoid
> the boundaries at 0.
>
> eg if your data were in variables "times" and "status"
> ll <-function(logshape,logscale){
>   -sum(ifelse(status,
>   pgamma(times,exp(logshape),scale=exp(logscale),
>   lower.tail=FALSE,log.p=TRUE),
>   dgamma(times,exp(logshape),scale=exp(logscale),log=TRUE)
>        ))
> }
>
> This works in mle() without too much sensitivity to starting values.
>
>   -thomas
>



From simon at stats.gla.ac.uk  Tue Oct 19 15:58:56 2004
From: simon at stats.gla.ac.uk (Simon Wood)
Date: Tue, 19 Oct 2004 14:58:56 +0100 (BST)
Subject: [R] AIC in GAM model
In-Reply-To: <HPEKKMAIJDHOLIFOCLOHOEOBCAAA.marchini@ifc.cnr.it>
References: <HPEKKMAIJDHOLIFOCLOHOEOBCAAA.marchini@ifc.cnr.it>
Message-ID: <Pine.LNX.4.58.0410191457050.25526@moon.stats.gla.ac.uk>

> How can I obtain AIC index when my model is a GAM?
> Thanks
um <- gam(y~s(x))
AIC(um)
- I think this works for packages mgcv (>1.1) and gam. 

best,
Simon



From ernesto at ipimar.pt  Tue Oct 19 15:56:08 2004
From: ernesto at ipimar.pt (Ernesto Jardim)
Date: Tue, 19 Oct 2004 14:56:08 +0100
Subject: [R] plot.ts: controling ylim and using expression in ylab
Message-ID: <1098194168.21703.88.camel@mordor.ipimar.pt>

Hi,

I'm using plot.ts for a multivariate time series and I want to define
ylim for all plots. In the past I had to hack plot.mts to do it. Now I'm
using the new plot.ts but I'm still having problems with ylim and it
seems to ignore the expressions for ylab.

Can someone help me on this ?

EJ

plot.ts(hke.ts, main="",xlab="year", ylab=c(expression(U[ICES]),
expression(bar(U)[y]), expression(hat(U)[y]), expression(tilde(U)[y]), 
expression(tilde(U)[y]*"*"), expression(B[ICES]), expression(SSB[ICES]),
expression(B[SURV]), expression(SSB[SURV])), ylim=c(0,3.5), nc=3)



From BNewquist at constellagroup.com  Tue Oct 19 16:26:28 2004
From: BNewquist at constellagroup.com (Brian Newquist)
Date: Tue, 19 Oct 2004 10:26:28 -0400
Subject: [R] Error message in mclust
Message-ID: <1BF5A584BBD24645A0524FA419524BCB09EC7B84@banyan.constellagroup.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20041019/18f09615/attachment.pl

From lecoutre at stat.ucl.ac.be  Tue Oct 19 16:57:27 2004
From: lecoutre at stat.ucl.ac.be (Eric Lecoutre)
Date: Tue, 19 Oct 2004 16:57:27 +0200
Subject: [R] [R-pkgs] R2HTML version 1.4-3
Message-ID: <6.0.1.1.2.20041019162257.034c6ec0@stat4ux.stat.ucl.ac.be>


Hi useRs,

I have uploaded on CRAN R2HTML version 1.4-3, which should be available 
soon for your favorite platform.

For R 2.0.0, I had to rewrite some parts of the code so that it works. I 
did that quickly (maybe too), letting some bugs.

Now, this is mere the good R 2.0 version, with some new functionalities:

- HTML.matrix and HTML.data.frame now can use several new output options
   as they call now 'format. This allows using a comma "," as decimal
   separator for example. All 'format' arguments are allowed and can be used
   as a single element (value applies to all columns) or a vector (provide
   values for each column). - Suggestion by Arne Henningsen

- HTML.data.frame now has a sortableDF option. When using it, a link to
   an external DHTML behavior file is done (suggestion by Tom Short, could be
   used by it's wonderful Rpad)

- Available options that user may want to change
  (default for every   matrix/DF):
	R2HTML.sortableDF  		(FALSE)
	R2HTML.format.digits  		(2)
	R2HTML.format.nsmall  		(0)
	R2HTML.format.big.mark		("")
	R2HTML.format.big.interval	(3)
	R2HTML.format.decimal.mark=	(Sys.localeconv()[["decimal_point"]])

- Bug fixed: HTML.table

Following modifications contributed by Philippe Grosjean

- Now uses NAMESPACE

- Renamed .First.lib() into .onLoad()

- In RweaveHTMLSetup(), commented if(!require(R2HTML))
       stop("R2HTML package is required.").
   This is not useful, becasue this function IS in R2HTML package!!!

- Changed utils:::XXX and utils::XXX into XXX everywhere, and added
   import(utils) in NAMESPACE instead.

Eric
Eric Lecoutre
UCL /  Institut de Statistique
Voie du Roman Pays, 20
1348 Louvain-la-Neuve
Belgium

tel: (+32)(0)10473050
lecoutre at stat.ucl.ac.be
http://www.stat.ucl.ac.be/ISpersonnel/lecoutre

If the statistics are boring, then you've got the wrong numbers. -Edward 
Tufte

_______________________________________________
R-packages mailing list
R-packages at stat.math.ethz.ch
https://stat.ethz.ch/mailman/listinfo/r-packages



From jmacdon at med.umich.edu  Tue Oct 19 17:41:00 2004
From: jmacdon at med.umich.edu (James W. MacDonald)
Date: Tue, 19 Oct 2004 11:41:00 -0400
Subject: [R] Question about version argument of require() and library()
Message-ID: <4175358C.9030502@med.umich.edu>

As far as I know, the following should work. Can anybody tell me 
if/where I am going wrong?

 > packageDescription("KEGG", field="Version")
[1] "1.6.4"
 > require(KEGG, version="1.6.4")
Loading required package: KEGG
[1] FALSE
Warning message:
There is no package called 'KEGG', version 1.6.4 in: library(package, 
character.only = TRUE, logical = TRUE, warn.conflicts = warn.conflicts,

TIA,

Jim


-- 
James W. MacDonald
Affymetrix and cDNA Microarray Core
University of Michigan Cancer Center
1500 E. Medical Center Drive
7410 CCGC
Ann Arbor MI 48109



From ripley at stats.ox.ac.uk  Tue Oct 19 18:05:42 2004
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Tue, 19 Oct 2004 17:05:42 +0100 (BST)
Subject: [R] Question about version argument of require() and library()
In-Reply-To: <4175358C.9030502@med.umich.edu>
Message-ID: <Pine.LNX.4.44.0410191658590.9519-100000@gannet.stats>

Did you install this package with --with-package-versions?  The `version'
argument picks up a particular versioned installed, not an ordinary
install of the right version.

We have rather little documentation on versioned installs.

On Tue, 19 Oct 2004, James W. MacDonald wrote:

> As far as I know, the following should work. Can anybody tell me 
> if/where I am going wrong?
> 
>  > packageDescription("KEGG", field="Version")
> [1] "1.6.4"
>  > require(KEGG, version="1.6.4")
> Loading required package: KEGG
> [1] FALSE
> Warning message:
> There is no package called 'KEGG', version 1.6.4 in: library(package, 
> character.only = TRUE, logical = TRUE, warn.conflicts = warn.conflicts,

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From jmacdon at med.umich.edu  Tue Oct 19 18:23:35 2004
From: jmacdon at med.umich.edu (James W. MacDonald)
Date: Tue, 19 Oct 2004 12:23:35 -0400
Subject: [R] Question about version argument of require() and library()
In-Reply-To: <Pine.LNX.4.44.0410191658590.9519-100000@gannet.stats>
References: <Pine.LNX.4.44.0410191658590.9519-100000@gannet.stats>
Message-ID: <41753F87.9050508@med.umich.edu>

Prof Brian Ripley wrote:

> Did you install this package with --with-package-versions?  The `version'
> argument picks up a particular versioned installed, not an ordinary
> install of the right version.

No I hadn't. That fixed it, and I do see the installWithVers argument to 
install.packages() for binary packages. Thanks for the help.

Best,

Jim


> 
> We have rather little documentation on versioned installs.
> 


-- 
James W. MacDonald
Affymetrix and cDNA Microarray Core
University of Michigan Cancer Center
1500 E. Medical Center Drive
7410 CCGC
Ann Arbor MI 48109



From fws4 at cdrh.fda.gov  Tue Oct 19 18:23:56 2004
From: fws4 at cdrh.fda.gov (Frank Samuelson)
Date: Tue, 19 Oct 2004 12:23:56 -0400
Subject: [R] lattice, xyplot layout questions
Message-ID: <cl3f2r$7re$1@sea.gmane.org>


1.  How do I get a grid in the background of my xyplots,
     like Figure 1 of
http://www.ci.tuwien.ac.at/Conferences/DSC-2001/Proceedings/Murrell.pdf

2. In my scatter xyplot I have multiple columns and rows of panels.
    I would like to have different y scales for panels in different
    rows (panels in same rows have same y scales).  In such a scenario,
    only the outside axes are necessary, just like the case where all
    y scales are  the same (scales$x$relation="same").

    Is there an easy way to do this, so no additional space or axis
    labels appear?

    I tried setting scales$y$relation="free" and
    scales$y$limits.  This puts space between every column
    and a y axis on every panel, which isn't necessary.
    I nuked the unwanted axes by setting scales$y$at appropriately,
    and I tried to rid myself of the space using between=list(x=0,y=0),
    but that didn't work.  Is there some other flag to try?

Thanks for any help.

-Frank



From arrayprofile at yahoo.com  Tue Oct 19 18:27:39 2004
From: arrayprofile at yahoo.com (array chip)
Date: Tue, 19 Oct 2004 09:27:39 -0700 (PDT)
Subject: [R] multinomial logistic regression with prior probability
Message-ID: <20041019162739.27849.qmail@web41215.mail.yahoo.com>

Hi,

Is there a way to specify proior probability in
multinom()? The function has a weight option for
individual cases, but I would like to specify prior
probability for each category of the response
variable, just like what lda() does. My data have
categorical independent variables, so I think lda() is
not suitable. Or are there other R package/function
that can do the work?

Thanks



From sundar.dorai-raj at PDF.COM  Tue Oct 19 18:44:44 2004
From: sundar.dorai-raj at PDF.COM (Sundar Dorai-Raj)
Date: Tue, 19 Oct 2004 09:44:44 -0700
Subject: [R] Error message in mclust
In-Reply-To: <1BF5A584BBD24645A0524FA419524BCB09EC7B84@banyan.constellagroup.com>
References: <1BF5A584BBD24645A0524FA419524BCB09EC7B84@banyan.constellagroup.com>
Message-ID: <4175447C.4060606@pdf.com>



Brian Newquist wrote:

> I keep on receiving the message below after submitting the following line
> using the mclust package.  m2 is a 99 X 1 column vector. 
> 
>  
> 
> *       em(modelName = "E", m2, mu = c(25, 50), sigmasq=10, pro = c(0.4,
> 0.6))
> 
>  
> 
> Error in as.double.default(data) : (list) object cannot be coerced to
> double.
> 
>  
> 
> Why do I receive this error?
> 

Brian,

Are you sure m2 is a vector and not a data.frame or list? I can 
replicate your error message as follows:

library(mclust)
# from ?em
data(iris)
irisMatrix <- as.matrix(iris[, 1:4])
irisClass <- iris[, 5]
msEst <- mstep(modelName = "EEE", data = irisMatrix,
                z = unmap(irisClass))
iris.em1 <- em(modelName = msEst$modelName, data = irisMatrix,
                mu = msEst$mu, Sigma = msEst$Sigma, pro = msEst$pro)
# not from ?em
iris.em2 <- em(modelName = msEst$modelName, data = iris[, 1:4],
                mu = msEst$mu, Sigma = msEst$Sigma, pro = msEst$pro)

The second call to em produces:

Error in as.double.default(data) : (list) object cannot be coerced to double

That said, it shouldn't matter that `data' is a data.frame since the 
help page says explicitly that data.frames are allowed as long as the 
variables are not categorical. The first for columns of iris are `numeric'.

R-2.0.0Patched with MCLUST 2.1-6

--sundar



From diana.chirac at free.fr  Tue Oct 19 18:49:55 2004
From: diana.chirac at free.fr (diana.chirac@free.fr)
Date: Tue, 19 Oct 2004 18:49:55 +0200
Subject: [R] indexing problem
Message-ID: <1098204595.417545b3ee327@imp4-q.free.fr>

Hi,

I have the following indexing problem, can you help me please ?

Given:
dm = a data.frame or a matrix dm,
idx = a 2 columns (or any number) matrix with the same number of rows as dm

I want get a subset of dm, for each row, the columns which
specified by idx.

thank you, diana



From rbaer at atsu.edu  Tue Oct 19 18:52:13 2004
From: rbaer at atsu.edu (Robert W. Baer, Ph.D.)
Date: Tue, 19 Oct 2004 11:52:13 -0500
Subject: [R] Matrix/Table col headings R 2.0.0
Message-ID: <010601c4b5fb$fbd474c0$3e80010a@BigBaer>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20041019/6a653834/attachment.pl

From sundar.dorai-raj at PDF.COM  Tue Oct 19 19:00:08 2004
From: sundar.dorai-raj at PDF.COM (Sundar Dorai-Raj)
Date: Tue, 19 Oct 2004 10:00:08 -0700
Subject: [R] indexing problem
In-Reply-To: <1098204595.417545b3ee327@imp4-q.free.fr>
References: <1098204595.417545b3ee327@imp4-q.free.fr>
Message-ID: <41754818.7020206@pdf.com>



diana.chirac at free.fr wrote:

> Hi,
> 
> I have the following indexing problem, can you help me please ?
> 
> Given:
> dm = a data.frame or a matrix dm,
> idx = a 2 columns (or any number) matrix with the same number of rows as dm
> 
> I want get a subset of dm, for each row, the columns which
> specified by idx.
> 
> thank you, diana
> 

Diana,
   From what I gather it appears as if you want to split dm by all the 
unique rows of idx? Is that right? If so, you can do the following:

x <- split(dm, do.call("paste", as.data.frame(idx))

This will split dm into a list with each element a subset of dm 
corresponding to a unique row in idx. The length of the x will be the 
number of unique rows in idx.

If this is not what you want, please provide an example and what you 
expect to see.

--sundar



From laura at env.leeds.ac.uk  Tue Oct 19 19:01:49 2004
From: laura at env.leeds.ac.uk (Laura Quinn)
Date: Tue, 19 Oct 2004 18:01:49 +0100 (BST)
Subject: [R] Slope of surface
Message-ID: <Pine.LNX.4.44.0410191759520.5125-100000@gw.env.leeds.ac.uk>

Hi,

Is there a neat way of working out the slope of a flat surface in R?
Given (x,y,z) co-ordinates of the four corners of a square, is there a
function which will allow me to calculate the "mean" slope of the surface
in a given direction?

Thanks in advance..

Laura

Laura Quinn
Institute of Atmospheric Science
School of Earth and Environment
University of Leeds
Leeds
LS2 9JT

tel: +44 113 343 1596
fax: +44 113 343 6716
mail: laura at env.leeds.ac.uk



From knicodem at jhsph.edu  Tue Oct 19 19:32:02 2004
From: knicodem at jhsph.edu (Kristin Kay Nicodemus)
Date: Tue, 19 Oct 2004 13:32:02 -0400
Subject: [R] pasting indexes to variables within loops
Message-ID: <78D5BFE5CDEC5A4CA1101197911F1EEAB529DD@XCH-VN01.sph.ad.jhsph.edu>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20041019/85d765df/attachment.pl

From spencer.graves at pdf.com  Tue Oct 19 19:41:58 2004
From: spencer.graves at pdf.com (Spencer Graves)
Date: Tue, 19 Oct 2004 10:41:58 -0700
Subject: [R] Schur decomposition?  (was:  matrix of eigenvalues)
In-Reply-To: <4175083B.6090605@stat.wisc.edu>
References: <a06002006bd9a7b5ffb79@[130.120.104.141]>
	<4175083B.6090605@stat.wisc.edu>
Message-ID: <417551E6.5090105@pdf.com>

      Does R have a function for the Schur decomposition?  The 
documentation for library(Matrix) describes a function "Schur", but it 
seems to be missing from the Windows version 0.8-14 (2004-09-14) and 
0.8-15 (2004-10-02). 

      The R 2.0.0 pat documentation for "eigen" refers to 
"http://www.netlib.org/lapack/lug/lapack_lug.html", and the description 
there for eigen analysis of a non-symmetric matrix says, "This problem 
can be solved via the Schur factorization of A, defined in the real case as
A = ZTZT,

where Z is an orthogonal matrix and T is an upper quasi-triangular 
matrix with 1-by-1 and 2-by-2 diagonal blocks, the 2-by-2 blocks 
corresponding to complex conjugate pairs of eigenvalues of A." 

      Thanks,
      Spencer Graves

Douglas Bates wrote:

> Christian Jost wrote:
>
>> I thought that the function
>> eigen(A)
>> will return a matrix with eigenvectors that are independent of each 
>> other (thus forming a base and the matrix being invertible). This 
>> seems not to be the case in the following example
>> A=matrix(c(1,2,0,1),nrow=2,byrow=T)
>> eigen(A) ->ev
>> solve(ev$vectors)
>>
>> note that I try to get the upper triangular form with eigenvalues on 
>> the diagonal and (possibly) 1 just atop the eigenvalues to be used to 
>> solve a linear differential equation
>> x' = Ax, x(0)=x0
>> x(t) = P exp(D t) P^-1 x0
>> where D is this upper triangular form and P is the "passage matrix" 
>> (not sure about the correct english name) given by a base of 
>> eigenvectors. So the test would be
>> solve(ev$vectors) %*% A %*% ev$vectors - D
>> should be 0
>>
>> Thanks for any help, Christian.
>>
>> ps: please copy reply also to my address, my subscription to the 
>> R-help list seems to have delays
>
>
> That particular matrix has repeated eigenvalues and a degenerate 
> eigenspace.
>
> > A <- matrix(c(1,0,2,1),nc=2)
> > A
>      [,1] [,2]
> [1,]    1    2
> [2,]    0    1
> > eigen(A)
> $values
> [1] 1 1
>
> $vectors
>      [,1]          [,2]
> [1,]    1 -1.000000e+00
> [2,]    0  1.110223e-16
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html


-- 
Spencer Graves, PhD, Senior Development Engineer
O:  (408)938-4420;  mobile:  (408)655-4567



From sway at tanox.com  Tue Oct 19 19:43:29 2004
From: sway at tanox.com (Shawn Way)
Date: Tue, 19 Oct 2004 12:43:29 -0500
Subject: [R] Sweave and Trellis in R 2.0.0patched (Windows)
Message-ID: <2DBF8A8E1A1AEE4AB3618AC4D6BF308807205E@houston.tanox.net>

 
 I've been using the following code to plot using Sweave in version
1.9.1

library(RODBC)
library(lattice)
channel <-odbcConnectExcel("h:/water.xls")
data <- sqlQuery(channel,"select * from `Sheet1$` where Test = 'TOC' and
(Valve='5010-05' or Valve='8030-V26' or Valve='1180-08' or
Valve='5040-08')")
odbcClose(channel)
srt <- order(as.POSIXct(data$'DateTime Sampled'))
data <- data[srt,]
data <- data[!is.na(data$Value),]
start1 <- strptime(c("6/1/2003"),format="%m/%d/%Y")
end1 <- strptime(c("5/1/2004"),format="%m/%d/%Y")
data.plot <- data[
                  data$'DateTime Sampled'>= start1
#                  & is.na(data$Exclude)
                  & data$'DateTime Sampled' <= end1
                  ,]
trellis.par.set(theme=col.whitebg())
print(xyplot(data.plot$Value~data.plot$'DateTime
Sampled'|data.plot$Valve,
           data=data.plot,
           panel= function(x,y) {
                      panel.xyplot(x,y,type="b")
                      panel.abline(h=500,col="red")
                    },
           layout=c(1,4),
           type="b",
           main="TOC Values for Various Systems",
           xlab="Date",
           ylab="TOC (ppb)"
          ))

But under version 2.0.0 it gives the following error:

> Sweave("M:\\Engineering\\Shawn\\BMF\\MECO\\Clean Steam\\32-04.rnw")
Writing to file 32-04.tex
Processing code chunks ...
 1 : term hide eps pdf
Error in "[<-"(`*tmp*`, pos.heights[[nm]], value = numeric(0)) : 
	nothing to replace with
Error in driver$runcode(drobj, chunk, chunkopts) : 
	Error in "[<-"(`*tmp*`, pos.heights[[nm]], value = numeric(0)) :

	nothing to replace with
>
Data.plot has the following structure:
> str(data.plot)
`data.frame':	436 obs. of  13 variables:
 $ DateTime Sampled:`POSIXct', format: chr  "2003-06-02 00:00:00"
"2003-06-03 00:00:00" "2003-06-03 00:00:00" "2003-06-03 00:00:00" ...
 $ DateTime Tested :`POSIXct', format: chr  "2003-06-02 00:00:00"
"2003-06-03 00:00:00" "2003-06-03 00:00:00" "2003-06-03 00:00:00" ...
 $ System          : Factor w/ 2 levels "CLS","HWFI": 2 1 2 2 2 2 1 2 2
2 ...
 $ Valve           : Factor w/ 4 levels "1180-08","5010-05",..: 1 2 1 3
1 3 4 1 3 3 ...
 $ Value           : num    5.75 231.00   1.10   8.25   8.70 ...
 $ LimitValue      : logi  NA NA NA NA NA NA NA NA NA NA NA NA ...
 $ IC              : logi  NA NA NA NA NA NA NA NA NA NA NA NA ...
 $ Test            : Factor w/ 1 level "TOC": 1 1 1 1 1 1 1 1 1 1 ...
 $ Sampled         : Factor w/ 4 levels "EV","HL","HL, EV",..: 1 4 1 1 1
1 1 1 1 1 ...
 $ Tested          : Factor w/ 3 levels "Contract","KO",..: 3 3 3 3 3 3
3 3 3 3 ...
 $ Verified        : Factor w/ 1 level "EV": NA NA NA NA NA NA NA NA NA
NA ...
 $ Comment         : Factor w/ 1 level "Test": NA NA NA NA NA NA NA NA
NA NA ...
 $ Exclude         : Factor w/ 1 level "X": NA NA NA NA NA NA NA NA NA
NA ...
>

When I remove the print(...) from the xyplot, the code chunks executes
fine (no plot but that is expected). This also does not happen when the
code is directly placed into R, only using Sweave().

Any thoughts?

Thanks for all your help and a wonderful (and useful) product.

________________________________

"Policies are many, Principles are few, Policies will change, Principles
never do." 
-John C. Maxwell

________________________________

Shawn Way, PE
Engineering Manager



From deepayan at stat.wisc.edu  Tue Oct 19 20:29:27 2004
From: deepayan at stat.wisc.edu (Deepayan Sarkar)
Date: Tue, 19 Oct 2004 13:29:27 -0500
Subject: [R] Sweave and Trellis in R 2.0.0patched (Windows)
In-Reply-To: <2DBF8A8E1A1AEE4AB3618AC4D6BF308807205E@houston.tanox.net>
References: <2DBF8A8E1A1AEE4AB3618AC4D6BF308807205E@houston.tanox.net>
Message-ID: <200410191329.27350.deepayan@stat.wisc.edu>

On Tuesday 19 October 2004 12:43, Shawn Way wrote:
>  I've been using the following code to plot using Sweave in version
> 1.9.1
>
> library(RODBC)
> library(lattice)
> channel <-odbcConnectExcel("h:/water.xls")
> data <- sqlQuery(channel,"select * from `Sheet1$` where Test = 'TOC'
> and (Valve='5010-05' or Valve='8030-V26' or Valve='1180-08' or
> Valve='5040-08')")
> odbcClose(channel)
> srt <- order(as.POSIXct(data$'DateTime Sampled'))
> data <- data[srt,]
> data <- data[!is.na(data$Value),]
> start1 <- strptime(c("6/1/2003"),format="%m/%d/%Y")
> end1 <- strptime(c("5/1/2004"),format="%m/%d/%Y")
> data.plot <- data[
>                   data$'DateTime Sampled'>= start1
> #                  & is.na(data$Exclude)
>                   & data$'DateTime Sampled' <= end1
>                   ,]
> trellis.par.set(theme=col.whitebg())
> print(xyplot(data.plot$Value~data.plot$'DateTime
> Sampled'|data.plot$Valve,
>            data=data.plot,
>            panel= function(x,y) {
>                       panel.xyplot(x,y,type="b")
>                       panel.abline(h=500,col="red")
>                     },
>            layout=c(1,4),
>            type="b",
>            main="TOC Values for Various Systems",
>            xlab="Date",
>            ylab="TOC (ppb)"
>           ))
>
> But under version 2.0.0 it gives the following error:
> > Sweave("M:\\Engineering\\Shawn\\BMF\\MECO\\Clean Steam\\32-04.rnw")
>
> Writing to file 32-04.tex
> Processing code chunks ...
>  1 : term hide eps pdf
> Error in "[<-"(`*tmp*`, pos.heights[[nm]], value = numeric(0)) :
>  nothing to replace with
> Error in driver$runcode(drobj, chunk, chunkopts) :
>  Error in "[<-"(`*tmp*`, pos.heights[[nm]], value = numeric(0)) :
>
>  nothing to replace with

This looks like code introduced in 2.0.0, so there's a good chance this 
is a bug. 

> Data.plot has the following structure:
> > str(data.plot)
>
> `data.frame': 436 obs. of  13 variables:
>  $ DateTime Sampled:`POSIXct', format: chr  "2003-06-02 00:00:00"
> "2003-06-03 00:00:00" "2003-06-03 00:00:00" "2003-06-03 00:00:00" ...
>  $ DateTime Tested :`POSIXct', format: chr  "2003-06-02 00:00:00"
> "2003-06-03 00:00:00" "2003-06-03 00:00:00" "2003-06-03 00:00:00" ...
>  $ System          : Factor w/ 2 levels "CLS","HWFI": 2 1 2 2 2 2 1 2
> 2 2 ...
>  $ Valve           : Factor w/ 4 levels "1180-08","5010-05",..: 1 2 1
> 3 1 3 4 1 3 3 ...
>  $ Value           : num    5.75 231.00   1.10   8.25   8.70 ...
>  $ LimitValue      : logi  NA NA NA NA NA NA NA NA NA NA NA NA ...
>  $ IC              : logi  NA NA NA NA NA NA NA NA NA NA NA NA ...
>  $ Test            : Factor w/ 1 level "TOC": 1 1 1 1 1 1 1 1 1 1 ...
>  $ Sampled         : Factor w/ 4 levels "EV","HL","HL, EV",..: 1 4 1
> 1 1 1 1 1 1 1 ...
>  $ Tested          : Factor w/ 3 levels "Contract","KO",..: 3 3 3 3 3
> 3 3 3 3 3 ...
>  $ Verified        : Factor w/ 1 level "EV": NA NA NA NA NA NA NA NA
> NA NA ...
>  $ Comment         : Factor w/ 1 level "Test": NA NA NA NA NA NA NA
> NA NA NA ...
>  $ Exclude         : Factor w/ 1 level "X": NA NA NA NA NA NA NA NA
> NA NA ...
>
>
> When I remove the print(...) from the xyplot, the code chunks
> executes fine (no plot but that is expected). This also does not
> happen when the code is directly placed into R, only using Sweave().

Which is a bit surprising. One explanation could be that this is 
something device specific. Could you try doing this interactively but 
in PDF and postscript devices?

Also, could you assign the result of the xyplot call to a variable, save 
it in a file (using save), and send it to me offline?

Deepayan



From diana.chirac at free.fr  Tue Oct 19 20:43:37 2004
From: diana.chirac at free.fr (diana)
Date: Tue, 19 Oct 2004 20:43:37 +0200
Subject: [R] indexing problem
References: <1098204595.417545b3ee327@imp4-q.free.fr>
	<41754818.7020206@pdf.com>
Message-ID: <41756059.9020405@free.fr>

ah sorry, here's an example:
 > dm = cbind(1:2,11:12,101:102)
 > dm
      [,1] [,2] [,3]
[1,]    1   11  101
[2,]    2   12  102

 > idx=cbind(c(1,2),c(2,3))
 > idx
      [,1] [,2]
[1,]    1    2
[2,]    2    3

the result I want to get:
1   11
12 102

that is: each row of idx gives the column index in dm

diana

Sundar Dorai-Raj wrote:
> 
> 
> diana.chirac at free.fr wrote:
> 
>> Hi,
>>
>> I have the following indexing problem, can you help me please ?
>>
>> Given:
>> dm = a data.frame or a matrix dm,
>> idx = a 2 columns (or any number) matrix with the same number of rows 
>> as dm
>>
>> I want get a subset of dm, for each row, the columns which
>> specified by idx.
>>
>> thank you, diana
>>
> 
> Diana,
>   From what I gather it appears as if you want to split dm by all the 
> unique rows of idx? Is that right? If so, you can do the following:
> 
> x <- split(dm, do.call("paste", as.data.frame(idx))
> 
> This will split dm into a list with each element a subset of dm 
> corresponding to a unique row in idx. The length of the x will be the 
> number of unique rows in idx.
> 
> If this is not what you want, please provide an example and what you 
> expect to see.
> 
> --sundar
> 
> 
> 
>



From tplate at blackmesacapital.com  Tue Oct 19 20:54:50 2004
From: tplate at blackmesacapital.com (Tony Plate)
Date: Tue, 19 Oct 2004 12:54:50 -0600
Subject: [R] indexing problem
In-Reply-To: <41756059.9020405@free.fr>
References: <1098204595.417545b3ee327@imp4-q.free.fr>
	<41754818.7020206@pdf.com> <41756059.9020405@free.fr>
Message-ID: <6.1.0.6.2.20041019125306.0b5b3c08@mailhost.blackmesacapital.com>

Maybe this does what you want:

 > dm <- cbind(1:2,11:12,101:102)
 > idx <- cbind(c(1,2),c(2,3))
 > row(idx)
      [,1] [,2]
[1,]    1    1
[2,]    2    2
 > cbind(as.vector(row(idx)), as.vector(idx))
      [,1] [,2]
[1,]    1    1
[2,]    2    2
[3,]    1    2
[4,]    2    3
 > dm[cbind(as.vector(row(idx)), as.vector(idx))]
[1]   1  12  11 102
 > array(dm[cbind(as.vector(row(idx)), as.vector(idx))], dim=dim(idx))
      [,1] [,2]
[1,]    1   11
[2,]   12  102
 >

At Tuesday 12:43 PM 10/19/2004, you wrote:
>ah sorry, here's an example:
> > dm = cbind(1:2,11:12,101:102)
> > dm
>      [,1] [,2] [,3]
>[1,]    1   11  101
>[2,]    2   12  102
>
> > idx=cbind(c(1,2),c(2,3))
> > idx
>      [,1] [,2]
>[1,]    1    2
>[2,]    2    3
>
>the result I want to get:
>1   11
>12 102
>
>that is: each row of idx gives the column index in dm
>
>diana
>
>Sundar Dorai-Raj wrote:
>>
>>diana.chirac at free.fr wrote:
>>
>>>Hi,
>>>
>>>I have the following indexing problem, can you help me please ?
>>>
>>>Given:
>>>dm = a data.frame or a matrix dm,
>>>idx = a 2 columns (or any number) matrix with the same number of rows as dm
>>>
>>>I want get a subset of dm, for each row, the columns which
>>>specified by idx.
>>>
>>>thank you, diana
>>Diana,
>>   From what I gather it appears as if you want to split dm by all the 
>> unique rows of idx? Is that right? If so, you can do the following:
>>x <- split(dm, do.call("paste", as.data.frame(idx))
>>This will split dm into a list with each element a subset of dm 
>>corresponding to a unique row in idx. The length of the x will be the 
>>number of unique rows in idx.
>>If this is not what you want, please provide an example and what you 
>>expect to see.
>>--sundar
>>
>
>______________________________________________
>R-help at stat.math.ethz.ch mailing list
>https://stat.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html



From Friedrich.Leisch at ci.tuwien.ac.at  Tue Oct 19 21:00:55 2004
From: Friedrich.Leisch at ci.tuwien.ac.at (Friedrich.Leisch@ci.tuwien.ac.at)
Date: Tue, 19 Oct 2004 21:00:55 +0200
Subject: [R] [R-pkgs] flexmix version 1.0-0 released
Message-ID: <16757.25703.394807.531238@galadriel.ci.tuwien.ac.at>

Dear useRs,

FlexMix version 1.0-0 has been released on CRAN. FlexMix implements a
general framework for finite mixtures of regression models using the
EM algorithm.  FlexMix provides the E-step and all data handling,
while the M-step can be supplied by the user to easily define new
models. Existing drivers implement mixtures of standard linear models,
generalized linear models and model-based clustering.

Compared to the porvious version on CRAN the code has not changed, but
the package has now an introductionary vignette, see

	vignette("flexmix-intro")

after installing the package. The vignette has also been published in
the Journal of Statistical Software, see http://www.jstatsoft.org.

Best regards,
Fritz Leisch

** DESCRIPTION *******************************************
Package: flexmix
Version: 1.0-0
Date: 2004-10-19
Author: Friedrich Leisch
Title: Flexible Mixture Modeling
Depends: R (>= 1.9.0), graphics, methods, stats, stats4
Suggests: MASS, ellipse, mvtnorm
License: GPL version 2.
URL: http://www.ci.tuwien.ac.at/~leisch/FlexMix

_______________________________________________
R-packages mailing list
R-packages at stat.math.ethz.ch
https://stat.ethz.ch/mailman/listinfo/r-packages



From deepayan at stat.wisc.edu  Tue Oct 19 21:27:21 2004
From: deepayan at stat.wisc.edu (Deepayan Sarkar)
Date: Tue, 19 Oct 2004 14:27:21 -0500
Subject: [R] lattice, xyplot layout questions
In-Reply-To: <cl3f2r$7re$1@sea.gmane.org>
References: <cl3f2r$7re$1@sea.gmane.org>
Message-ID: <200410191427.21778.deepayan@stat.wisc.edu>

On Tuesday 19 October 2004 11:23, Frank Samuelson wrote:
> 1.  How do I get a grid in the background of my xyplots,
>      like Figure 1 of
> http://www.ci.tuwien.ac.at/Conferences/DSC-2001/Proceedings/Murrell.p
>df

The traditional way is to use panel.grid. e.g.

 xyplot(<...>
        panel = function(x, y, ...) {
            panel.grid(h=, v=)
            panel.xyplot(x, y, ...)
        })

With R 2.0.0, you could also do

 xyplot(<...>, type = c("p", "g"))

as long as you are using the default panel function, but you won't be 
able to control where the grid lines are drawn.

> 2. In my scatter xyplot I have multiple columns and rows of panels.
>     I would like to have different y scales for panels in different
>     rows (panels in same rows have same y scales).  In such a
> scenario, only the outside axes are necessary, just like the case
> where all y scales are  the same (scales$x$relation="same").
>
>     Is there an easy way to do this, so no additional space or axis
>     labels appear?

Not easy, but it's possible.

>     I tried setting scales$y$relation="free" and
>     scales$y$limits.  This puts space between every column
>     and a y axis on every panel, which isn't necessary.
>     I nuked the unwanted axes by setting scales$y$at appropriately,
>     and I tried to rid myself of the space using
> between=list(x=0,y=0), but that didn't work.  Is there some other
> flag to try?

Yes, but only with 2.0.0. You basically need to change the 
'layout.widths' settings value. To make things slightly inconvenient, 
you will probably need to specify the limits (and tick positions) for 
all the panels (if you just specify relation="free", there's no 
guarantee all panels in a row will have the same limits -- unless of 
course your data is such that they are). Here's a (slightly artificial) 
example:

xyplot(as.numeric(variety) ~ yield | year * site, data = barley,
       layout=c(6,2),
       scales =
       list(y =
            list(relation="free", rot = 0,
                 limits = rep(list( c(0, 6), c( 5, 11 )), 
                              c(6, 6)),
                 at = rep(list( 1:5, NULL, 6:10, NULL ), 
                          c(1, 5, 1, 5) ))),
       par.settings =
       list(layout.widths = 
            list(axis.panel = rep(c(1, 0), c(1, 5)))))

If you leave out the 'par.settings' argument, this just has the effect 
of suppressing the unwanted axis labels (by setting the corresponding 
'at' to NULL), even though the space is allocated for them. 
layout.widths$axis.panel controls the space. They don't have to be 0 or 
1, any multiplier is allowed.

I Hope this makes sense.

Deepayan



From sundar.dorai-raj at PDF.COM  Tue Oct 19 21:26:06 2004
From: sundar.dorai-raj at PDF.COM (Sundar Dorai-Raj)
Date: Tue, 19 Oct 2004 12:26:06 -0700
Subject: [R] indexing problem
In-Reply-To: <41756059.9020405@free.fr>
References: <1098204595.417545b3ee327@imp4-q.free.fr>	<41754818.7020206@pdf.com>
	<41756059.9020405@free.fr>
Message-ID: <41756A4E.2050309@pdf.com>



diana wrote:
> ah sorry, here's an example:
>  > dm = cbind(1:2,11:12,101:102)
>  > dm
>      [,1] [,2] [,3]
> [1,]    1   11  101
> [2,]    2   12  102
> 
>  > idx=cbind(c(1,2),c(2,3))
>  > idx
>      [,1] [,2]
> [1,]    1    2
> [2,]    2    3
> 
> the result I want to get:
> 1   11
> 12 102
> 
> that is: each row of idx gives the column index in dm
> 
> diana


How about:

sapply(seq(nrow(dm)), function(i) dm[i, idx[i, ]])

--sundar

> 
> Sundar Dorai-Raj wrote:
> 
>>
>>
>> diana.chirac at free.fr wrote:
>>
>>> Hi,
>>>
>>> I have the following indexing problem, can you help me please ?
>>>
>>> Given:
>>> dm = a data.frame or a matrix dm,
>>> idx = a 2 columns (or any number) matrix with the same number of rows 
>>> as dm
>>>
>>> I want get a subset of dm, for each row, the columns which
>>> specified by idx.
>>>
>>> thank you, diana
>>>
>>
>> Diana,
>>   From what I gather it appears as if you want to split dm by all the 
>> unique rows of idx? Is that right? If so, you can do the following:
>>
>> x <- split(dm, do.call("paste", as.data.frame(idx))
>>
>> This will split dm into a list with each element a subset of dm 
>> corresponding to a unique row in idx. The length of the x will be the 
>> number of unique rows in idx.
>>
>> If this is not what you want, please provide an example and what you 
>> expect to see.
>>
>> --sundar
>>
>>
>>
>>
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html



From p.dalgaard at biostat.ku.dk  Tue Oct 19 21:39:35 2004
From: p.dalgaard at biostat.ku.dk (Peter Dalgaard)
Date: 19 Oct 2004 21:39:35 +0200
Subject: [R] Matrix/Table col headings R 2.0.0
In-Reply-To: <010601c4b5fb$fbd474c0$3e80010a@BigBaer>
References: <010601c4b5fb$fbd474c0$3e80010a@BigBaer>
Message-ID: <x23c0acxag.fsf@biostat.ku.dk>

"Robert W. Baer, Ph.D." <rbaer at atsu.edu> writes:

> #Produces Left justified column names
> library(ISwR)
> data(juul)
> attach(juul)
> sex.tan=table(sex,tanner)
> colnames(sex.tan)=c("I","II","IIII","IV","V")
> rownames(sex.tan)=c("M","F")
> sex.tan
> class(sex.tan)
> 
> # Finally, look at (left justified)
> as.table(caff.marital)
> 
> Somehow PD got right justified columns with this dataset. Is there a
> new way of doing things in version 2.0.0, my ignorance, a bug? I
> tried making the columns factors first, but in my hands this did not
> appear to help either. Thanks for any insight.

The book (which by the way does not have an example involving
"sex.tan" anywhere, but does have an example with the tranposed table)
is based on R 1.5.0 which was current in early 2002. I have check runs
done on October 1, 2002 with right justified labels, so I suppose this
got changed in between.

The label justification comes from printing of the character matrix
created by format() inside print.table(), so

   print(sex.tan, right=T)

gives you what you want.

The fundamental issue seems to be that we get left-justified printing
of strings that contains right-justified text. The effect is quite
clear in

> format(unclass(table(sex,tanner)))
   tanner
sex 1     2     3     4     5
  1 "291" " 55" " 34" " 41" "124"
  2 "224" " 48" " 38" " 40" "204"


-- 
   O__  ---- Peter Dalgaard             Blegdamsvej 3  
  c/ /'_ --- Dept. of Biostatistics     2200 Cph. N   
 (*) \(*) -- University of Copenhagen   Denmark      Ph: (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)             FAX: (+45) 35327907



From ligges at statistik.uni-dortmund.de  Tue Oct 19 21:59:13 2004
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Tue, 19 Oct 2004 21:59:13 +0200
Subject: [R] Slope of surface
In-Reply-To: <Pine.LNX.4.44.0410191759520.5125-100000@gw.env.leeds.ac.uk>
References: <Pine.LNX.4.44.0410191759520.5125-100000@gw.env.leeds.ac.uk>
Message-ID: <41757211.9070805@statistik.uni-dortmund.de>

Laura Quinn wrote:

> Hi,
> 
> Is there a neat way of working out the slope of a flat surface in R?
> Given (x,y,z) co-ordinates of the four corners of a square, 

three should be sufficient


 > s there a
> function which will allow me to calculate the "mean" slope of the surface
> in a given direction?

Along the axis, one way without any thinking is to apply a regression 
with e=0, the neater way is to calculate the true result simply. Just 
insert one formula into the other ...

Uwe Ligges


> Thanks in advance..
> 
> Laura
> 
> Laura Quinn
> Institute of Atmospheric Science
> School of Earth and Environment
> University of Leeds
> Leeds
> LS2 9JT
> 
> tel: +44 113 343 1596
> fax: +44 113 343 6716
> mail: laura at env.leeds.ac.uk
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html



From wolski at molgen.mpg.de  Tue Oct 19 22:10:51 2004
From: wolski at molgen.mpg.de (Witold Eryk Wolski)
Date: Tue, 19 Oct 2004 22:10:51 +0200
Subject: [R] plot.dendrogram and plot.hclust ZOOM into the height?
Message-ID: <417574CB.9020805@molgen.mpg.de>

Hi,

I clustered a distance matrix and would like to draw it using 
plot.hclust or plot.dendrogram.
The dendrogram is not informative because I have a few extremely small 
dissimilarities in the distance matrix (e.g. 0), but most of the other 
distances are in the range 1e10+-5000.
I would like to show the tree only for the height of 1e10+-5000 but 
unfortunately their are no parameter like xlim,ylim in the function?
Are there other ways to ZOOM?

Any suggestions?

/E

-- 
Dipl. bio-chem. Witold Eryk Wolski
MPI-Moleculare Genetic
Ihnestrasse 63-73 14195 Berlin
tel: 0049-30-83875219                 __("<    _
http://www.molgen.mpg.de/~wolski      \__/    'v'
http://r4proteomics.sourceforge.net    ||    /   \
mail: witek96 at users.sourceforge.net    ^^     m m
      wolski at molgen.mpg.de



From fws4 at cdrh.fda.gov  Tue Oct 19 22:11:58 2004
From: fws4 at cdrh.fda.gov (Frank Samuelson)
Date: Tue, 19 Oct 2004 16:11:58 -0400
Subject: [R] pasting indexes to variables within loops
In-Reply-To: <78D5BFE5CDEC5A4CA1101197911F1EEAB529DD@XCH-VN01.sph.ad.jhsph.edu>
References: <78D5BFE5CDEC5A4CA1101197911F1EEAB529DD@XCH-VN01.sph.ad.jhsph.edu>
Message-ID: <cl3sec$ig9$1@sea.gmane.org>


as.numeric(apply(haplo, 1 ,function(x) paste(x,collapse="")))
perhaps?

Kristin Kay Nicodemus wrote:
> Hi all,
>  
> Hope someone can help me.  I start out with a matrix called haplo with 600 rows and two columns.  To start with, the elements of the matrix are character strings of 20 numbers.  I then want to create separate objects, called ha1-ha600, that are the concatenated 20 + 20 numbers, as numeric.  I am having no trouble doing anything except getting the index i to paste to the object ha(i) in some fashion.  When I do substring(haplo[(i)...) it works fine, but I can't seem to create objects that are ha1 or ha2 etc., containing the 40 number row I just created in the loop.  I've tried paste but I can't seem to get it to do the right thing.
>  
> For this example, nc=20
>  
> for (i in 1:600) {
> tst<-substring(haplo[(i),1],1:nc,1:nc)
> a1<-as.numeric(tst)
> tst<-substring(haplo[(i),2],1:nc,1:nc)
> a2<-as.numeric(tst)
> ha<-append(a1,a2)
> ha(i)<-ha # DOESN'T WORK
> }
>  
> TIA,
> Kristin Nicodemus
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>



From clint at ecy.wa.gov  Tue Oct 19 22:11:20 2004
From: clint at ecy.wa.gov (Clint Bowman)
Date: Tue, 19 Oct 2004 13:11:20 -0700 (PDT)
Subject: [R] Slope of surface
In-Reply-To: <41757211.9070805@statistik.uni-dortmund.de>
Message-ID: <Pine.LNX.4.44.0410191310110.2780-100000@aeolus.ecy.wa.gov>

If I recall my computer graphics (decades ago at Utah), doesn't the cross 
product enter in here?  I'm hoping someone with more recent experience 
will have fewer cobwebs overlaying the knowledge here.

Clint

On Tue, 19 Oct 2004, Uwe Ligges wrote:

> Laura Quinn wrote:
> 
> > Hi,
> > 
> > Is there a neat way of working out the slope of a flat surface in R?
> > Given (x,y,z) co-ordinates of the four corners of a square, 
> 
> three should be sufficient
> 
> 
>  > s there a
> > function which will allow me to calculate the "mean" slope of the surface
> > in a given direction?
> 
> Along the axis, one way without any thinking is to apply a regression 
> with e=0, the neater way is to calculate the true result simply. Just 
> insert one formula into the other ...
> 
> Uwe Ligges
> 
> 
> > Thanks in advance..
> > 
> > Laura
> > 
> > Laura Quinn
> > Institute of Atmospheric Science
> > School of Earth and Environment
> > University of Leeds
> > Leeds
> > LS2 9JT
> > 
> > tel: +44 113 343 1596
> > fax: +44 113 343 6716
> > mail: laura at env.leeds.ac.uk
> > 
> > ______________________________________________
> > R-help at stat.math.ethz.ch mailing list
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
> 

-- 
Clint Bowman			INTERNET:	clint at ecy.wa.gov
Air Quality Modeler		INTERNET:	clint at math.utah.edu
Department of Ecology		VOICE:		(360) 407-6815
PO Box 47600			FAX:		(360) 407-7534
Olympia, WA 98504-7600



From diana.chirac at free.fr  Tue Oct 19 22:19:11 2004
From: diana.chirac at free.fr (diana)
Date: Tue, 19 Oct 2004 22:19:11 +0200
Subject: [R] indexing problem
References: <1098204595.417545b3ee327@imp4-q.free.fr>
	<41754818.7020206@pdf.com> <41756059.9020405@free.fr>
	<6.1.0.6.2.20041019125306.0b5b3c08@mailhost.blackmesacapital.com>
Message-ID: <417576BF.9050801@free.fr>

Tony, Sundar,
Thank you for your codes, they all work, but the "tricky" code of
Tony seems more efficient than the sapply() solution of Sundar,
especially for big size array. For a quick test:

 > dm = matrix(rnorm(100000*40),ncol=40)
 > idx=cbind(sample(1:ncol(dm), nrow(dm), replace=T),
             sample(1:ncol(dm), nrow(dm),replace=T),
             sample(1:ncol(dm), nrow(dm),replace=T))
 > str(idx)
int [1:100000, 1:3] 6 2 1 6 24 23 21 36 31 21 ...

 > system.time(a <- array(dm[cbind(as.vector(row(idx)), as.vector(idx))], dim=dim(idx)))
[1] 0.12 0.01 0.14   NA   NA

 > system.time(b <- t(sapply(seq(nrow(dm)), function(i) dm[i, idx[i, ]])))
[1] 1.60 0.00 1.69   NA   NA

 > all(a==b)
[1] TRUE

diana



From tlumley at u.washington.edu  Tue Oct 19 22:26:37 2004
From: tlumley at u.washington.edu (Thomas Lumley)
Date: Tue, 19 Oct 2004 13:26:37 -0700 (PDT)
Subject: [R] pasting indexes to variables within loops
In-Reply-To: <78D5BFE5CDEC5A4CA1101197911F1EEAB529DD@XCH-VN01.sph.ad.jhsph.edu>
References: <78D5BFE5CDEC5A4CA1101197911F1EEAB529DD@XCH-VN01.sph.ad.jhsph.edu>
Message-ID: <Pine.A41.4.61a.0410191324350.51112@homer03.u.washington.edu>

On Tue, 19 Oct 2004, Kristin Kay Nicodemus wrote:

> Hi all,
>
> Hope someone can help me.  I start out with a matrix called haplo with 
> 600 rows and two columns.  To start with, the elements of the matrix are 
> character strings of 20 numbers.  I then want to create separate 
> objects, called ha1-ha600, that are the concatenated 20 + 20 numbers, as 
> numeric.  I am having no trouble doing anything except getting the index 
> i to paste to the object ha(i) in some fashion.  When I do 
> substring(haplo[(i)...) it works fine, but I can't seem to create 
> objects that are ha1 or ha2 etc., containing the 40 number row I just 
> created in the loop.  I've tried paste but I can't seem to get it to do 
> the right thing.


If you really want to create 600 objects, then you want
   assign(paste("ha",i,sep=""), ha)
and to retrieve them
   get(paste("ha",i,sep=""))

Often it is better to keep things like this in a list.

 	-thomas



From john.fisler at latticesemi.com  Tue Oct 19 22:24:19 2004
From: john.fisler at latticesemi.com (John Fisler)
Date: Tue, 19 Oct 2004 13:24:19 -0700
Subject: [R] Changing the Y graph scale Maximum value.
Message-ID: <417577F3.5020602@latticesemi.com>

Hello,

I am new to R and have read the documents related to graphics but have 
not come across a description of how to change the maximum scale on a 
graph.  Below is sample code that sets up a plot window with a 0 Minimum 
to 10 Maximum, X and Y coordinate system:

x <- c(1,2,3,4,5,6,7,8,9,10)
y <- c(1,1.5,2,2.5,3,3.5,4,4.5,5,5.5)
plot(0:10, 0:10, type = "n" )# setting up coord. system
points(x, y, col = "red", cex = 1.5)

I now want to change the maximum Y axis scale to 6.  I am using the 
'points' function to plot 'x' and 'y' because my 'real' data is X and Y 
coordinate based.  When I try to change the maximum Y scale to 6 as in 
the following code segment, I get the following error:

plot(0:10, 0:6, type = "n" )# setting up coord. system
Error in xy.coords(x, y, xlabel, ylabel, log) :
         x and y lengths differ

How do I change the maximum Y scale to 6?

Thanks,

John Fisler



From deepayan at stat.wisc.edu  Tue Oct 19 22:28:56 2004
From: deepayan at stat.wisc.edu (Deepayan Sarkar)
Date: Tue, 19 Oct 2004 15:28:56 -0500
Subject: [R] Sweave and Trellis in R 2.0.0patched (Windows)
In-Reply-To: <2DBF8A8E1A1AEE4AB3618AC4D6BF30888D0E0C@houston.tanox.net>
References: <2DBF8A8E1A1AEE4AB3618AC4D6BF30888D0E0C@houston.tanox.net>
Message-ID: <200410191528.56651.deepayan@stat.wisc.edu>


On Tuesday 19 October 2004 13:49, you wrote:

[...]

>
> I've included the following files:
> Plt.dat - This is the xyplot using save()
> Full.data - This is the entire environment using save.image()

Thanks. Turns out this was a bug in trellis.par.set (a missing 
assignment), and can be reproduced (in a fresh R session) by 

> library(lattice)
> x11()  ## or any other device
> trellis.par.set(theme = col.whitebg())
> xyplot(1 ~ 1)
Error in "[<-"(`*tmp*`, pos.heights[[nm]], value = numeric(0)) : 
 nothing to replace with

The issue was that a call to trellis.device() needs to be made somewhere 
along the way so that the proper settings are initialized (which would 
then be overwritten by 'theme'). trellis.par.set was detecting if it 
needed to do this, but wasn't handling the situation correctly, leading 
to inconsistent settings.

This should be fixed in the next release of lattice. For now, a 
workaround (which is functionally equivalent to what you are doing now) 
is to replace 

trellis.par.set(theme = col.whitebg())

by 

lattice.options(default.theme = "col.whitebg")

Deepayan


[...]

> On Tuesday 19 October 2004 12:43, Shawn Way wrote:
> >  I've been using the following code to plot using Sweave in version
> > 1.9.1
> >
> > library(RODBC)
> > library(lattice)
> > channel <-odbcConnectExcel("h:/water.xls")
> > data <- sqlQuery(channel,"select * from `Sheet1$` where Test =
> > 'TOC' and (Valve='5010-05' or Valve='8030-V26' or Valve='1180-08'
> > or Valve='5040-08')")
> > odbcClose(channel)
> > srt <- order(as.POSIXct(data$'DateTime Sampled')) data <-
> > data[srt,] data <- data[!is.na(data$Value),]
> > start1 <- strptime(c("6/1/2003"),format="%m/%d/%Y")
> > end1 <- strptime(c("5/1/2004"),format="%m/%d/%Y")
> > data.plot <- data[
> >                   data$'DateTime Sampled'>= start1
> > #                  & is.na(data$Exclude)
> >                   & data$'DateTime Sampled' <= end1
> >                   ,]
> > trellis.par.set(theme=col.whitebg())
> > print(xyplot(data.plot$Value~data.plot$'DateTime
> > Sampled'|data.plot$Valve,
> >            data=data.plot,
> >            panel= function(x,y) {
> >                       panel.xyplot(x,y,type="b")
> >                       panel.abline(h=500,col="red")
> >                     },
> >            layout=c(1,4),
> >            type="b",
> >            main="TOC Values for Various Systems",
> >            xlab="Date",
> >            ylab="TOC (ppb)"
> >           ))
> >
> > But under version 2.0.0 it gives the following error:
> > > Sweave("M:\\Engineering\\Shawn\\BMF\\MECO\\Clean
> > > Steam\\32-04.rnw")
> >
> > Writing to file 32-04.tex
> > Processing code chunks ...
> >  1 : term hide eps pdf
> > Error in "[<-"(`*tmp*`, pos.heights[[nm]], value = numeric(0)) :
> >  nothing to replace with
> > Error in driver$runcode(drobj, chunk, chunkopts) :
> >  Error in "[<-"(`*tmp*`, pos.heights[[nm]], value = numeric(0)) :
> >
> >  nothing to replace with



From mayeul.kauffmann at tiscali.fr  Tue Oct 19 22:44:24 2004
From: mayeul.kauffmann at tiscali.fr (Mayeul KAUFFMANN)
Date: Tue, 19 Oct 2004 22:44:24 +0200
Subject: [R] Cox PH Warning Message
Message-ID: <000601c4b61c$6beead00$062d9b53@amd>

sorry for late answer.

"This is the output. I think it means that there is a problem with
project$pluralgpTriplet???
Neil"

Yes.
definitely. there are probably no triplet, or all triplet are coded as
twins too (and there are no real twins). Or the only triplet is the first
individual that dies (in fact, the latter should rather give an infinite
exp(coef), I think).
You also seem to use many qualitative variable. If a combination of two of
the other variables is identical to  "pluralgpTriplet", R drops the
variable. (This is true for any regression model.)
To find out which combination, you may regress pluralgpTriplet on all
other variables with lm(). You'll probably have R??=1 and some very low
p-values.

If the number of triplet is very small, I advise you group them with
twins, except if it is a major variable in your model.



Cheers,
Mayeul KAUFFMANN
Universit?? Pierre Mend??s France
Grenoble - France

 > fm.CPH
Call:
coxph(formula = Surv(age_at_death, death) ~ project$pluralgp +
     project$yrborn + project$private + project$pcdead + project$mheight
+
     project$ga2 + project$apgcode + project$apgar5 + project$VLBWgp +
     project$PEBW + project$LBWgp + project$LBWcat + project$totadm +
     project$totlos + project$sex + teen + project$DS + project$pcsborn +
     project$disadv + project$ecores + project$edoccu)


                                 coef exp(coef) se(coef)       z       p
project$pluralgpTwin       -0.509203     0.601  0.74415 -0.6843 4.9e-01
project$pluralgpTriplet           NA        NA  0.00000      NA      NA
project$yrborn1988-1992    -0.083348     0.920  0.22553 -0.3696 7.1e-01
project$private             0.260465     1.298  0.12384  2.1032 3.5e-02
project$pcdead             -0.058200     0.943  0.49418 -0.1178 9.1e-01
project$mheight             0.016361     1.016  0.01745  0.9378 3.5e-01
project$ga2                 0.104246     1.110  0.10162  1.0258 3.0e-01
project$apgcode4-7         -0.266885     0.766  0.50211 -0.5315 6.0e-01
project$apgcode1-3         -1.704620     0.182  1.12545 -1.5146 1.3e-01
project$apgar5             -0.427139     0.652  0.14099 -3.0296 2.4e-03
project$VLBWgp=>1500 grams  0.046203     1.047  0.65494  0.0705 9.4e-01
project$PEBW                0.015297     1.015  0.01356  1.1281 2.6e-01
project$LBWgp=>2500 grams  -0.257472     0.773  0.42496 -0.6059 5.4e-01
project$LBWcat             -0.222823     0.800  0.23938 -0.9308 3.5e-01
project$totadm              0.005836     1.006  0.01536  0.3800 7.0e-01
project$totlos              0.007342     1.007  0.00176  4.1664 3.1e-05
project$sexFemale          -0.016431     0.984  0.22803 -0.0721 9.4e-01
teen                        0.286282     1.331  0.34807  0.8225 4.1e-01
project$DSDown syndrome     1.193727     3.299  0.27227  4.3844 1.2e-05
project$pcsborn            -0.704743     0.494  0.75943 -0.9280 3.5e-01
project$disadv             -0.000573     0.999  0.00250 -0.2296 8.2e-01
project$ecores             -0.001588     0.998  0.00249 -0.6392 5.2e-01
project$edoccu              0.000590     1.001  0.00193  0.3054 7.6e-01



From wolski at molgen.mpg.de  Tue Oct 19 23:08:45 2004
From: wolski at molgen.mpg.de (Witold Eryk Wolski)
Date: Tue, 19 Oct 2004 23:08:45 +0200
Subject: [R] Re: plot.dendrogram and plot.hclust ZOOM into the height?
In-Reply-To: <417574CB.9020805@molgen.mpg.de>
References: <417574CB.9020805@molgen.mpg.de>
Message-ID: <4175825D.1020708@molgen.mpg.de>

Hi,

Just noted that
dendr2 <- cut(dendr1,h=10000)
plot(dendr2$upper)
Gives what I am looking for.
Sorry, Its quite late here.

/E

Witold Eryk Wolski wrote:

> Hi,
>
> I clustered a distance matrix and would like to draw it using 
> plot.hclust or plot.dendrogram.
> The dendrogram is not informative because I have a few extremely small 
> dissimilarities in the distance matrix (e.g. 0), but most of the other 
> distances are in the range 1e10+-5000.
> I would like to show the tree only for the height of 1e10+-5000 but 
> unfortunately their are no parameter like xlim,ylim in the function?
> Are there other ways to ZOOM?
>
> Any suggestions?
>
> /E
>


-- 
Dipl. bio-chem. Witold Eryk Wolski
MPI-Moleculare Genetic
Ihnestrasse 63-73 14195 Berlin
tel: 0049-30-83875219                 __("<    _
http://www.molgen.mpg.de/~wolski      \__/    'v'
http://r4proteomics.sourceforge.net    ||    /   \
mail: witek96 at users.sourceforge.net    ^^     m m
      wolski at molgen.mpg.de



From sundar.dorai-raj at PDF.COM  Tue Oct 19 23:11:10 2004
From: sundar.dorai-raj at PDF.COM (Sundar Dorai-Raj)
Date: Tue, 19 Oct 2004 14:11:10 -0700
Subject: [R] Changing the Y graph scale Maximum value.
In-Reply-To: <417577F3.5020602@latticesemi.com>
References: <417577F3.5020602@latticesemi.com>
Message-ID: <417582EE.9020602@pdf.com>



John Fisler wrote:

> Hello,
> 
> I am new to R and have read the documents related to graphics but have 
> not come across a description of how to change the maximum scale on a 
> graph.  Below is sample code that sets up a plot window with a 0 Minimum 
> to 10 Maximum, X and Y coordinate system:
> 
> x <- c(1,2,3,4,5,6,7,8,9,10)
> y <- c(1,1.5,2,2.5,3,3.5,4,4.5,5,5.5)
> plot(0:10, 0:10, type = "n" )# setting up coord. system
> points(x, y, col = "red", cex = 1.5)
> 
> I now want to change the maximum Y axis scale to 6.  I am using the 
> 'points' function to plot 'x' and 'y' because my 'real' data is X and Y 
> coordinate based.  When I try to change the maximum Y scale to 6 as in 
> the following code segment, I get the following error:
> 
> plot(0:10, 0:6, type = "n" )# setting up coord. system
> Error in xy.coords(x, y, xlabel, ylabel, log) :
>         x and y lengths differ
> 
> How do I change the maximum Y scale to 6?
> 
> Thanks,
> 
> John Fisler
> 

You should use ylim.

plot(0, 0, type = "n", xlim = c(0, 10), ylim = c(0, 6))

--sundar



From jeaneid at chass.utoronto.ca  Tue Oct 19 23:27:41 2004
From: jeaneid at chass.utoronto.ca (Jean Eid)
Date: Tue, 19 Oct 2004 17:27:41 -0400
Subject: [R] Changing the Y graph scale Maximum value.
In-Reply-To: <417577F3.5020602@latticesemi.com>
Message-ID: <Pine.SGI.4.40.0410191722490.47084116-100000@origin.chass.utoronto.ca>

If I understand you correctl, you just want to call the point of (0,10) to
be (0,6). if so have a look at text() function text(0,10, pos=2) or
something like that will do. The reason why you are getting the error
because your 0:10 vector is larger than 0:6 vector.

Hope this helps,

P.S. why are you not doing plot(x,y, col="red", cex=1.5, ylim=c(0,6))
but maybe I do not understand what you need....

Jean.

On Tue, 19 Oct 2004, John Fisler wrote:

> Hello,
>
> I am new to R and have read the documents related to graphics but have
> not come across a description of how to change the maximum scale on a
> graph.  Below is sample code that sets up a plot window with a 0 Minimum
> to 10 Maximum, X and Y coordinate system:
>
> x <- c(1,2,3,4,5,6,7,8,9,10)
> y <- c(1,1.5,2,2.5,3,3.5,4,4.5,5,5.5)
> plot(0:10, 0:10, type = "n" )# setting up coord. system
> points(x, y, col = "red", cex = 1.5)
>
> I now want to change the maximum Y axis scale to 6.  I am using the
> 'points' function to plot 'x' and 'y' because my 'real' data is X and Y
> coordinate based.  When I try to change the maximum Y scale to 6 as in
> the following code segment, I get the following error:
>
> plot(0:10, 0:6, type = "n" )# setting up coord. system
> Error in xy.coords(x, y, xlabel, ylabel, log) :
>          x and y lengths differ
>
> How do I change the maximum Y scale to 6?
>
> Thanks,
>
> John Fisler
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>



From BNewquist at constellagroup.com  Tue Oct 19 23:21:11 2004
From: BNewquist at constellagroup.com (Brian Newquist)
Date: Tue, 19 Oct 2004 17:21:11 -0400
Subject: [R] Windows XP crashes when running the EM algorithm in MCLUST
Message-ID: <1BF5A584BBD24645A0524FA419524BCB09EC8053@banyan.constellagroup.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20041019/55105973/attachment.pl

From jeaneid at chass.utoronto.ca  Wed Oct 20 00:12:07 2004
From: jeaneid at chass.utoronto.ca (Jean Eid)
Date: Tue, 19 Oct 2004 18:12:07 -0400
Subject: [R] densityplot and histogram
Message-ID: <Pine.SGI.4.40.0410191806170.47110897-100000@origin.chass.utoronto.ca>

Is there any function like par(new=T) for lattice. I want to plot a
histogram in percentages on the right hand side and also superimpose the
densityplot with its density scale on the lhs. so far I am only able to do
this
  histogram( temp[,2]~ temp[,1],nint=100,type="desnity",
               xlab = "Population Size",
               panel = function(x, ...) {
                   panel.histogram(x, ...)
                   panel.densityplot(x, col = "red", plot.points=F, lwd=2)
               } )

 If I change type="density" to type="percent" the scales for the
densityplot will be too low and all I see is a horizontal line at zero
(this is as expected) . However, I tried par(new=T) and nothing happens. I
want to be able to put percenstages on axis 2 and density values at axis
4.

Thank you

Jean,



From john.fisler at latticesemi.com  Wed Oct 20 00:52:58 2004
From: john.fisler at latticesemi.com (John Fisler)
Date: Tue, 19 Oct 2004 15:52:58 -0700
Subject: [R] Changing the Y graph scale Maximum value.
References: <Pine.SGI.4.40.0410191722490.47084116-100000@origin.chass.utoronto.ca>
Message-ID: <41759ACA.2050203@latticesemi.com>

Jean,

Thanks for your response.  I did not know about the xlim and ylim 
parameters until I got additional replys.  I used the following code 
and it worked:

plot(0, 0, type = "n", xlim = c(0, 10), ylim = c(0, 6))

John

Jean Eid wrote:
> If I understand you correctl, you just want to call the point of (0,10) to
> be (0,6). if so have a look at text() function text(0,10, pos=2) or
> something like that will do. The reason why you are getting the error
> because your 0:10 vector is larger than 0:6 vector.
> 
> Hope this helps,
> 
> P.S. why are you not doing plot(x,y, col="red", cex=1.5, ylim=c(0,6))
> but maybe I do not understand what you need....
> 
> Jean.
> 
> On Tue, 19 Oct 2004, John Fisler wrote:
> 
> 
>>Hello,
>>
>>I am new to R and have read the documents related to graphics but have
>>not come across a description of how to change the maximum scale on a
>>graph.  Below is sample code that sets up a plot window with a 0 Minimum
>>to 10 Maximum, X and Y coordinate system:
>>
>>x <- c(1,2,3,4,5,6,7,8,9,10)
>>y <- c(1,1.5,2,2.5,3,3.5,4,4.5,5,5.5)
>>plot(0:10, 0:10, type = "n" )# setting up coord. system
>>points(x, y, col = "red", cex = 1.5)
>>
>>I now want to change the maximum Y axis scale to 6.  I am using the
>>'points' function to plot 'x' and 'y' because my 'real' data is X and Y
>>coordinate based.  When I try to change the maximum Y scale to 6 as in
>>the following code segment, I get the following error:
>>
>>plot(0:10, 0:6, type = "n" )# setting up coord. system
>>Error in xy.coords(x, y, xlabel, ylabel, log) :
>>         x and y lengths differ
>>
>>How do I change the maximum Y scale to 6?
>>
>>Thanks,
>>
>>John Fisler
>>
>>______________________________________________
>>R-help at stat.math.ethz.ch mailing list
>>https://stat.ethz.ch/mailman/listinfo/r-help
>>PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>>
> 
>



From john.fisler at latticesemi.com  Wed Oct 20 00:53:54 2004
From: john.fisler at latticesemi.com (John Fisler)
Date: Tue, 19 Oct 2004 15:53:54 -0700
Subject: [R] Changing the Y graph scale Maximum value.
References: <417577F3.5020602@latticesemi.com> <417582EE.9020602@pdf.com>
Message-ID: <41759B02.60600@latticesemi.com>

Sundar,

Thanks for your response.  I used the command below:

plot(0, 0, type = "n", xlim = c(0, 10), ylim = c(0, 6))

to set the Y axis appropriately and it worked.

John

Sundar Dorai-Raj wrote:
> 
> 
> John Fisler wrote:
> 
>> Hello,
>>
>> I am new to R and have read the documents related to graphics but have 
>> not come across a description of how to change the maximum scale on a 
>> graph.  Below is sample code that sets up a plot window with a 0 
>> Minimum to 10 Maximum, X and Y coordinate system:
>>
>> x <- c(1,2,3,4,5,6,7,8,9,10)
>> y <- c(1,1.5,2,2.5,3,3.5,4,4.5,5,5.5)
>> plot(0:10, 0:10, type = "n" )# setting up coord. system
>> points(x, y, col = "red", cex = 1.5)
>>
>> I now want to change the maximum Y axis scale to 6.  I am using the 
>> 'points' function to plot 'x' and 'y' because my 'real' data is X and 
>> Y coordinate based.  When I try to change the maximum Y scale to 6 as 
>> in the following code segment, I get the following error:
>>
>> plot(0:10, 0:6, type = "n" )# setting up coord. system
>> Error in xy.coords(x, y, xlabel, ylabel, log) :
>>         x and y lengths differ
>>
>> How do I change the maximum Y scale to 6?
>>
>> Thanks,
>>
>> John Fisler
>>
> 
> You should use ylim.
> 
> plot(0, 0, type = "n", xlim = c(0, 10), ylim = c(0, 6))
> 
> --sundar
> 
>



From jeff.breiwick at noaa.gov  Wed Oct 20 01:33:40 2004
From: jeff.breiwick at noaa.gov (JMB)
Date: Tue, 19 Oct 2004 16:33:40 -0700
Subject: [R] Creating Packages with Windows under R 2.0.0
Message-ID: <cl48bn$iti$1@sea.gmane.org>

Hello,

I have a workspace that contains a number of functions I want to put in a
library. Under R 1.9.0 I did this fairly easily by putting them in a R
folder, creating a DESCRIPTION & TITLE file etc. I could then load my
library successfully.

Under R 2.0.0 this simple process apparently no longer works. I have used
package.skeleton() and have run R CMD INSTALL pkgname but still get various
errors. I have installed the various tools and do have a path to them. Is
there not a simple way to create a library consisting of just a group of
functions?  I know the DESCRIPTION file is different under R 2.0.0 but what
else do I have to consider? I've read most of the pertinent documentation
and though it is somewhat helpful it is not always helpful if you haven't
been able to get these procedures to run correctly.

Jeff Breiwick
National Marine Mammal Laboratory
Natl. Marine Fish. Serv.
7600 Sand Point Way NE, Bldg 4
Seattle, WA 98115  USA



From Ted.Harding at nessie.mcc.ac.uk  Wed Oct 20 01:49:33 2004
From: Ted.Harding at nessie.mcc.ac.uk ( (Ted Harding))
Date: Wed, 20 Oct 2004 00:49:33 +0100 (BST)
Subject: [R] Slope of surface
In-Reply-To: <Pine.LNX.4.44.0410191759520.5125-100000@gw.env.leeds.ac.uk>
Message-ID: <XFMail.041020004933.Ted.Harding@nessie.mcc.ac.uk>

On 19-Oct-04 Laura Quinn wrote:
> Is there a neat way of working out the slope of a flat surface in R?
> Given (x,y,z) co-ordinates of the four corners of a square, is there a
> function which will allow me to calculate the "mean" slope of the
> surface in a given direction?

Presumably you mean the z-slope in a direction defined by changes
in x and y.

Suppose the corners of the square (doesn't have to be square but
at least we know where we stand) are numbered in order round the
square as

  (x1,y2,z1), (x2,y2,z3), (x3,y3,z3), (x4,y4,z4)

(As Uwe suggests, you only need three). It makes it simpler and
doesn't change the problem to suppose that (x1,y1,z1) = (0,0,0).

Let your direction be defined by a move in the (x,y) plane from
(0,0,0) to (u,v,0), with the corresponding point in the square
given by (u,v,w). Then the slope is w/sqrt(u^2 + v^2).

Clearly w is linear in u and v with zero "intercept". Hence

  w = a*u + b*v

and at points [2] and [4] you have

  a*x2 + b*y2 = z2
  a*x4 + b*x4 = z4

so, if you let M = matrix(c(x2,x4,y2,y4),nrow=2)
and            A = c(z2,z4)

then (a,b) is given by solve(M,A). Provided the columns of M
are linearly independent (ensured by "square") this has a clean
solution for (a,b) and then you have it. (Note point [3] has not
been used.)

I think!

Best wishes,
Ted.


--------------------------------------------------------------------
E-Mail: (Ted Harding) <Ted.Harding at nessie.mcc.ac.uk>
Fax-to-email: +44 (0)870 094 0861  [NB: New number!]
Date: 20-Oct-04                                       Time: 00:49:33
------------------------------ XFMail ------------------------------



From vograno at evafunds.com  Wed Oct 20 02:20:39 2004
From: vograno at evafunds.com (Vadim Ogranovich)
Date: Tue, 19 Oct 2004 17:20:39 -0700
Subject: [R] help.search intersection
Message-ID: <C698D707214E6F4AB39AB7096C3DE5A56D8E81@phost015.EVAFUNDS.intermedia.net>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20041019/e466b7b5/attachment.pl

From nleonard at tartarus.uwa.edu.au  Wed Oct 20 03:07:59 2004
From: nleonard at tartarus.uwa.edu.au (Neil Leonard)
Date: Wed, 20 Oct 2004 09:07:59 +0800
Subject: [R] Replacing data from one data frame to another
Message-ID: <7BFB4384-2234-11D9-93F5-003065D5B8EC@tartarus.uwa.edu.au>

Hi,

I have a data frame which contains two fields, 'prev' and 'mchnum'.

I have another data frame which also contains 'mchnum' and 'prev' and 
some other fields. The 'prev' data in this field is corrupted and I 
want to replace it with the data from the other data frame for matching 
values of 'mchnum' (which are unique). Is there an easier way of doing 
this than having two for loops and going through each field looking for 
matching values of 'mchnum' and then replacing the 'prev' value when 
they match.


Thanks
Neil



From thefishfinger at ihug.com.au  Wed Oct 20 03:19:58 2004
From: thefishfinger at ihug.com.au (Sam Ferguson)
Date: Wed, 20 Oct 2004 11:19:58 +1000
Subject: [R] tuneR - readWave
In-Reply-To: <4174E676.3070005@statistik.uni-dortmund.de>
References: <20041019091859.GB17090@cl.cam.ac.uk>
	<4174E676.3070005@statistik.uni-dortmund.de>
Message-ID: <opsf49zkvro1fugt@samferguson-g5.arch.usyd.edu.au>

Hi R-people,

I'm trying to use the fantastic tuneR package to read wave files for  
analysis. I have followed some of the demos but am having trouble getting  
readWave to work on any wave file not saved by tuneR. As per the help  
example Wobj2 <- readWave("testfile.wav") works when I have used writeWave  
to save testfile.wav. However I cannot find any other wav file that is  
satisfactory for tuneR to load. I have saved a short pink noise file from  
protools in (bwf)wav format 16/44.1 Khz and  when i say x <-  
readWave("pinkNoisePT.wav") I get :-

Error in readBin(con, int, n = N, size = bytes, signed = (bytes == 2)) :
	invalid value of `n'
In addition: Warning message:
Looks like 'pinkNoisePT.wav' is not a valid wave file. in:  
readWave("pinkNoisePT.wav")


With another file that quicktime says is also 16/44.1 :-

Error in readBin(con, int, n = N, size = bytes, signed = (bytes == 2)) :
	That size is unknown on this machine
In addition: Warning messages:
1: Looks like 'voice.wav' is not a valid wave file. in:  
readWave("voice.wav")
2: NAs produced by integer overflow in: sample.rate * block.align
3: Wave file 'voice.wav' seems to be corrupted. in: readWave("voice.wav")

These both play happily in quicktime. I have searched for quite some time,  
but cannot locate what I am doing wrong ???

I have similar problems with my Windows XP box though my main box is  
MacOsX 10.3.4, R2.0.0.0 and tuneR compiled for 2.0.0.0

Thanks to Uwe L for what appears to be a fantastic package of analysis  
functions.

Sam Ferguson



From ggrothendieck at myway.com  Wed Oct 20 05:54:34 2004
From: ggrothendieck at myway.com (Gabor Grothendieck)
Date: Tue, 19 Oct 2004 23:54:34 -0400 (EDT)
Subject: [R] Replacing data from one data frame to another
Message-ID: <20041020035434.331B33966@mprdmxin.myway.com>



Check out the merge command.

From:   Neil Leonard <nleonard at tartarus.uwa.edu.au>

Hi,

I have a data frame which contains two fields, 'prev' and 'mchnum'.

I have another data frame which also contains 'mchnum' and 'prev' and 
some other fields. The 'prev' data in this field is corrupted and I 
want to replace it with the data from the other data frame for matching 
values of 'mchnum' (which are unique). Is there an easier way of doing 
this than having two for loops and going through each field looking for 
matching values of 'mchnum' and then replacing the 'prev' value when 
they match.


Thanks
Neil



From ripley at stats.ox.ac.uk  Wed Oct 20 08:29:21 2004
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Wed, 20 Oct 2004 07:29:21 +0100 (BST)
Subject: [R] Windows XP crashes when running the EM algorithm in MCLUST
In-Reply-To: <1BF5A584BBD24645A0524FA419524BCB09EC8053@banyan.constellagroup.com>
Message-ID: <Pine.LNX.4.44.0410200727200.10429-100000@gannet.stats>

On Tue, 19 Oct 2004, Brian Newquist wrote:

> Whenever I submit the following command >result <- em("E", m1, mu=c(25, 50),
> sigmasq=10, pro=c(0.49,0.51)) in MCLUST ,  I experience a problem with my
> Windows XP.  Has anyone had this type of problem?
> 
> I receive the general "send report" dialogue box and my program is no longer
> able to run.  Should I change any of my system settings?  Let me know if you
> think of anything that might help.

Please read the section on BUGS in the FAQ and then send a reproducible
example to the package maintainer.

> 	[[alternative HTML version deleted]]
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html

which asks for no HTML and discusses reporting problems.

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From ripley at stats.ox.ac.uk  Wed Oct 20 08:31:27 2004
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Wed, 20 Oct 2004 07:31:27 +0100 (BST)
Subject: [R] densityplot and histogram
In-Reply-To: <Pine.SGI.4.40.0410191806170.47110897-100000@origin.chass.utoronto.ca>
Message-ID: <Pine.LNX.4.44.0410200729460.10429-100000@gannet.stats>

See the arguments for print.trellis, especially 'split' and 'more'.

On Tue, 19 Oct 2004, Jean Eid wrote:

> Is there any function like par(new=T) for lattice. I want to plot a
> histogram in percentages on the right hand side and also superimpose the
> densityplot with its density scale on the lhs.

[...]

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From jeff.breiwick at noaa.gov  Wed Oct 20 01:33:40 2004
From: jeff.breiwick at noaa.gov (JMB)
Date: Tue, 19 Oct 2004 23:33:40 -0000
Subject: [R] Creating Packages with Windows under R 2.0.0
Message-ID: <cl48bn$iti$1@sea.gmane.org>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20041019/f70d4571/attachment.pl

From tlumley at u.washington.edu  Tue Oct 19 22:26:37 2004
From: tlumley at u.washington.edu (Thomas Lumley)
Date: Tue, 19 Oct 2004 20:26:37 -0000
Subject: [R] pasting indexes to variables within loops
Message-ID: <Pine.A41.4.61a.0410191324350.51112@homer03.u.washington.edu>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20041019/b4635c4e/attachment.pl

From john.fisler at latticesemi.com  Tue Oct 19 22:24:19 2004
From: john.fisler at latticesemi.com (John Fisler)
Date: Tue, 19 Oct 2004 20:24:19 -0000
Subject: [R] Changing the Y graph scale Maximum value.
Message-ID: <417577F3.5020602@latticesemi.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20041019/e24eb8a0/attachment.pl

From BNewquist at constellagroup.com  Tue Oct 19 23:21:11 2004
From: BNewquist at constellagroup.com (Brian Newquist)
Date: Tue, 19 Oct 2004 21:21:11 -0000
Subject: [R] Windows XP crashes when running the EM algorithm in MCLUST
Message-ID: <1BF5A584BBD24645A0524FA419524BCB09EC8053@banyan.constellagroup.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20041019/55105973/attachment-0001.pl

From john.fisler at latticesemi.com  Wed Oct 20 00:53:54 2004
From: john.fisler at latticesemi.com (John Fisler)
Date: Tue, 19 Oct 2004 22:53:54 -0000
Subject: [R] Changing the Y graph scale Maximum value.
Message-ID: <41759B02.60600@latticesemi.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20041019/7a2fd5ca/attachment.pl

From rbaer at atsu.edu  Tue Oct 19 18:52:13 2004
From: rbaer at atsu.edu (Robert W. Baer, Ph.D.)
Date: Tue, 19 Oct 2004 16:52:13 -0000
Subject: [R] Matrix/Table col headings R 2.0.0
Message-ID: <010601c4b5fb$fbd474c0$3e80010a@BigBaer>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20041019/6a653834/attachment-0001.pl

From thefishfinger at ihug.com.au  Wed Oct 20 03:19:58 2004
From: thefishfinger at ihug.com.au (Sam Ferguson)
Date: Wed, 20 Oct 2004 01:19:58 -0000
Subject: [R] tuneR - readWave
Message-ID: <opsf49zkvro1fugt@samferguson-g5.arch.usyd.edu.au>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20041020/63c0a7d7/attachment.pl

From ripley at stats.ox.ac.uk  Wed Oct 20 08:42:57 2004
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Wed, 20 Oct 2004 07:42:57 +0100 (BST)
Subject: [R] Replacing data from one data frame to another
In-Reply-To: <7BFB4384-2234-11D9-93F5-003065D5B8EC@tartarus.uwa.edu.au>
Message-ID: <Pine.LNX.4.44.0410200739000.10429-100000@gannet.stats>

Using merge() on 'mchnum' will give you a data frame with 'prev' from 
each.

You can also use match directly.  Suppose your dfs are A and B.  I think 
you want

ind <- match(B$mchnum, A$mchnum)
B$prev <- A$prev[ind]


On Wed, 20 Oct 2004, Neil Leonard wrote:

> I have a data frame which contains two fields, 'prev' and 'mchnum'.
> 
> I have another data frame which also contains 'mchnum' and 'prev' and 
> some other fields. The 'prev' data in this field is corrupted and I 
> want to replace it with the data from the other data frame for matching 
> values of 'mchnum' (which are unique). Is there an easier way of doing 
> this than having two for loops and going through each field looking for 
> matching values of 'mchnum' and then replacing the 'prev' value when 
> they match.

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From ligges at statistik.uni-dortmund.de  Wed Oct 20 08:53:09 2004
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Wed, 20 Oct 2004 08:53:09 +0200
Subject: [R] tuneR - readWave
In-Reply-To: <opsf49zkvro1fugt@samferguson-g5.arch.usyd.edu.au>
References: <20041019091859.GB17090@cl.cam.ac.uk>	<4174E676.3070005@statistik.uni-dortmund.de>
	<opsf49zkvro1fugt@samferguson-g5.arch.usyd.edu.au>
Message-ID: <41760B55.9010601@statistik.uni-dortmund.de>

Sam Ferguson wrote:
> Hi R-people,
> 
> I'm trying to use the fantastic tuneR package to read wave files for  
> analysis. I have followed some of the demos but am having trouble 
> getting  readWave to work on any wave file not saved by tuneR. As per 
> the help  example Wobj2 <- readWave("testfile.wav") works when I have 
> used writeWave  to save testfile.wav. However I cannot find any other 
> wav file that is  satisfactory for tuneR to load. I have saved a short 
> pink noise file from  protools in (bwf)wav format 16/44.1 Khz and  when 
> i say x <-  readWave("pinkNoisePT.wav") I get :-
> 
> Error in readBin(con, int, n = N, size = bytes, signed = (bytes == 2)) :
>     invalid value of `n'
> In addition: Warning message:
> Looks like 'pinkNoisePT.wav' is not a valid wave file. in:  
> readWave("pinkNoisePT.wav")
> 
> 
> With another file that quicktime says is also 16/44.1 :-
> 
> Error in readBin(con, int, n = N, size = bytes, signed = (bytes == 2)) :
>     That size is unknown on this machine
> In addition: Warning messages:
> 1: Looks like 'voice.wav' is not a valid wave file. in:  
> readWave("voice.wav")
> 2: NAs produced by integer overflow in: sample.rate * block.align
> 3: Wave file 'voice.wav' seems to be corrupted. in: readWave("voice.wav")
> 
> These both play happily in quicktime. I have searched for quite some 
> time,  but cannot locate what I am doing wrong ???
> 
> I have similar problems with my Windows XP box though my main box is  
> MacOsX 10.3.4, R2.0.0.0 and tuneR compiled for 2.0.0.0
> 
> Thanks to Uwe L for what appears to be a fantastic package of analysis  
> functions.
> 
> Sam Ferguson
> 


Well, it's not that fantastic yet, because most of the stuff intended to 
be included is either not yet included or still heavily in development.
Anyway, reading Wave files should work, iff the Wave files follows the 
Wave file definition I have read to implement that function.
It should work with Wave files generated by Microsoft software, some Mac 
studio software from the musicians at our university, and also by 
WaveLab (all tested). Looks like the Wave formats you are using are 
different.

Please can you upload small example Wave files to a website or sent them 
in a private message? Or even better, can you send the Wave file 
specification your software has implemented?
I might be able to fix the problem.

Please note that package specific questions are ought to be send to the 
package maintainer in the first place.

Uwe Ligges



From ligges at statistik.uni-dortmund.de  Wed Oct 20 09:03:23 2004
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Wed, 20 Oct 2004 09:03:23 +0200
Subject: [R] Creating Packages with Windows under R 2.0.0
In-Reply-To: <cl48bn$iti$1@sea.gmane.org>
References: <cl48bn$iti$1@sea.gmane.org>
Message-ID: <41760DBB.2030501@statistik.uni-dortmund.de>

JMB wrote:

> Hello,
> 
> I have a workspace that contains a number of functions I want to put in a
> library. 

It's a *package*.


 > Under R 1.9.0 I did this fairly easily by putting them in a R
> folder, creating a DESCRIPTION & TITLE file etc. I could then load my
> library successfully.
> 
> Under R 2.0.0 this simple process apparently no longer works. I have used
> package.skeleton() and have run R CMD INSTALL pkgname but still get various
> errors. I have installed the various tools and do have a path to them. Is
> there not a simple way to create a library consisting of just a group of
> functions?  

Yes, you already mentioned it: Use R CMD INSTALL.

> I know the DESCRIPTION file is different under R 2.0.0 

no, not necessarily

 > but what
> else do I have to consider? I've read most of the pertinent documentation

Let me guess: Are you on windows (at least your mail header says so)?
Then please read .../src/gnuwin32/readme.package *completely*!

What does the error message say?

As each R-help message tells us:
"PLEASE do read the posting guide! 
http://www.R-project.org/posting-guide.html"


Uwe Ligges


> and though it is somewhat helpful it is not always helpful if you haven't
> been able to get these procedures to run correctly.
> 
> Jeff Breiwick
> National Marine Mammal Laboratory
> Natl. Marine Fish. Serv.
> 7600 Sand Point Way NE, Bldg 4
> Seattle, WA 98115  USA
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html



From petr.pikal at precheza.cz  Wed Oct 20 09:05:13 2004
From: petr.pikal at precheza.cz (Petr Pikal)
Date: Wed, 20 Oct 2004 09:05:13 +0200
Subject: [R] sapply and loop
In-Reply-To: <BAY22-F10kcJse1fTvO0005bcee@hotmail.com>
Message-ID: <41762A49.28074.386FD0@localhost>



On 19 Oct 2004 at 14:49, Zhen Pang wrote:

> I am sorry for neglecting the acknowledgement for `Writing R
> Extensions', since I think I am just citing the code from the orignal
> R-help. I fail to get the results when I use my own code. So I refer
> to this code where Rprof() appears. Anyway, I am sorry for this.
> 
> In fact, I have tried to whether I specify boot.out. Neither one
> works.
> 
> Rprof("boot.out")
>      storm.boot <- boot(rs, storm.bf, R = 4999) # pretty slow
>       Rprof(NULL)
> 
> summaryRprof()
> Error in summaryRprof() : no events were recorded
> 
> summaryRprof("boot.out")
> Error in summaryRprof("boot.out") : no events were recorded

Hi

works for me without any problems. So you have probably 
mismatch in your directories (where is file boot.out?, is it in your 
working directory?)

Cheers
Petr



> 
>      Rprof()
>      storm.boot <- boot(rs, storm.bf, R = 4999) # pretty slow
>       Rprof(NULL)
> 
> summaryRprof()
> Error in summaryRprof() : no events were recorded
> 
> 
> Zhen
> 
> 
> >From: Prof Brian Ripley <ripley at stats.ox.ac.uk>
> >To: Zhen Pang <nusbj at hotmail.com>
> >CC: tlumley at u.washington.edu, r-help at stat.math.ethz.ch
> >Subject: Re: [R] sapply and loop
> >Date: Tue, 19 Oct 2004 07:29:54 +0100 (BST)
> >
> >On Tue, 19 Oct 2004, Zhen Pang wrote:
> >
> > > I tried to use Rprof(). As an example, I consider the following
> > > code 
> >(from
> > > Venables & Ripley, 1999).
> >
> >I believe you parroted that from `Writing R Extensions', but failed
> >to give proper credit!
> >
> > >      library(MASS); library(boot); library(nls)
> > >      data(stormer)
> > >      storm.fm <- nls(Time ~ b*Viscosity/(Wt - c), stormer,
> > >                      start = c(b=29.401, c=2.2183))
> > >      st <- cbind(stormer, fit=fitted(storm.fm))
> > >      storm.bf <- function(rs, i) {
> > >          st$Time <-  st$fit + rs[i]
> > >          tmp <- nls(Time ~ (b * Viscosity)/(Wt - c), st,
> > >                     start = coef(storm.fm))
> > >          tmp$m$getAllPars()
> > >      }
> > >      rs <- scale(resid(storm.fm), scale = FALSE) # remove the mean
> > >      Rprof("boot.out") storm.boot <- boot(rs, storm.bf, R = 4999)
> > >      # pretty slow Rprof(NULL)
> >
> >At this point your unacknowledged copying went adrift.
> >
> > > summaryRprof()
> > > Error in summaryRprof() : no events were recorded
> > >
> > > I am using R1.8.1 in windows. Why can't I get the results?
> >
> >Because you didn't do your homework, and didn't even follow your
> >source. The 'file' arguments of Rprof and summaryProf have to agree:
> >see their help pages (as the posting guide asks).
> >
> >--
> >Brian D. Ripley,                  ripley at stats.ox.ac.uk
> >Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
> >University of Oxford,             Tel:  +44 1865 272861 (self) 1
> >South Parks Road,                     +44 1865 272866 (PA) Oxford OX1
> >3TG, UK                Fax:  +44 1865 272595
> >
> >______________________________________________
> >R-help at stat.math.ethz.ch mailing list
> >https://stat.ethz.ch/mailman/listinfo/r-help
> >PLEASE do read the posting guide! 
> >http://www.R-project.org/posting-guide.html
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide!
> http://www.R-project.org/posting-guide.html

Petr Pikal
petr.pikal at precheza.cz



From ramagopa at etek.chalmers.se  Wed Oct 20 10:16:13 2004
From: ramagopa at etek.chalmers.se (Sivakumar Ramagopal)
Date: Wed, 20 Oct 2004 10:16:13 +0200 (CEST)
Subject: [R] Problem with Perl script calling R function
Message-ID: <63365.81.226.218.63.1098260173.squirrel@81.226.218.63>

Hi,

I'm a student doing my masters thesis and trying to work with S-Net
version 1.0 (tool used for statistical analysis and visualizations of
internet traffic) that is written in R.

The problem is this: I have a perl script calling a function defined in an
R script that is located in a directly different from where the perl
script is. This gives the following error.

>>> echo
'read.flow("BLDemo.2000.03.08.h12.m30.35.tcppac.gz",type="tcppac")' |
/cab/ce/user/ramagopa/lib/bin/R --no-save --no-restore --quiet --slave
Error: couldn't find function "read.flow"
Execution halted

Platform details:
-----------------
SNet-1.0
R-1.4.1 (I have to use this version because the version of SNet requires
only this).
R-recommended-1.4.1
perl-5.8.5

Could someone help me out with this problem? Additionally, I'd really
appreciate it if someone can point me to an SNet mailing list; I haven't
found one on the web.

Thanks,
Shiva



From Sebastian.Leuzinger at unibas.ch  Wed Oct 20 10:31:32 2004
From: Sebastian.Leuzinger at unibas.ch (Sebastian Leuzinger)
Date: Wed, 20 Oct 2004 10:31:32 +0200
Subject: [R] common axis label in multiple plot area
Message-ID: <41762264.2060604@unibas.ch>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20041020/d5865b53/attachment.pl

From r.g.brown at cefas.co.uk  Wed Oct 20 10:38:38 2004
From: r.g.brown at cefas.co.uk (Robert Brown FM CEFAS)
Date: Wed, 20 Oct 2004 09:38:38 +0100
Subject: [R] Selecting from a character vector with logical operators
Message-ID: <3589BC4D64C84341AE0C258244F977A2B60B55@expressa.corp.cefas.co.uk>

I'm trying to select a subset from character vector using the logical operator >, but get the following error 

6356     85     SOL   1     25     1   38E6
6357     85     SOL   1     27     1   38E6
6910     95     SOL   1     25     1   38E6
7152     98     SOL   1     19     1   38E6
7153     98     SOL   1     22     2   38E6
7154     98     SOL   1     28     1   38E6
> ca11c93SOL1VIIa<-ca11c93SOL1[ca11c93SOL1$RECTAN>"32Z9",]
Warning message: 
">" not meaningful for factors in: Ops.factor(ca11c93SOL1$RECTAN, "32Z9") 

(note RECTAN is the name assigned to the 7 th column)

Do these operators act on chacters vectors as opposed to numeric vectors and if so what are the rules for this?

Regards,

Robert Brown



From B.Rowlingson at lancaster.ac.uk  Wed Oct 20 11:24:11 2004
From: B.Rowlingson at lancaster.ac.uk (Barry Rowlingson)
Date: Wed, 20 Oct 2004 10:24:11 +0100
Subject: [R] Slope of surface
In-Reply-To: <XFMail.041020004933.Ted.Harding@nessie.mcc.ac.uk>
References: <XFMail.041020004933.Ted.Harding@nessie.mcc.ac.uk>
Message-ID: <41762EBB.30806@lancaster.ac.uk>

(Ted Harding) wrote:
> On 19-Oct-04 Laura Quinn wrote:
> 
>>Is there a neat way of working out the slope of a flat surface in R?

>>Given (x,y,z) co-ordinates of the four corners of a square, is there a
>>function which will allow me to calculate the "mean" slope of the
>>surface in a given direction?

  I had a quick look at the help for r.slope.aspect for the GRASS GIS 
[1]. To compute the slope of a raster grid it uses 'Horn's Formula' [2] 
which it says works by taking the 3x3 neighbourhood of the cell in 
question. If you really want a slope map over a big grid, this might be 
a better way to do it.


Barry


[1] 
http://supportweb.cs.bham.ac.uk/documentation/grass5/html/r.slope.aspect.html

[2] Horn, B. K. P. (1981). Hill Shading and the Reflectance Map, 
Proceedings of the IEEE, 69(1):14-47.



From ccleland at optonline.net  Wed Oct 20 11:49:48 2004
From: ccleland at optonline.net (Chuck Cleland)
Date: Wed, 20 Oct 2004 05:49:48 -0400
Subject: [R] Selecting from a character vector with logical operators
In-Reply-To: <3589BC4D64C84341AE0C258244F977A2B60B55@expressa.corp.cefas.co.uk>
References: <3589BC4D64C84341AE0C258244F977A2B60B55@expressa.corp.cefas.co.uk>
Message-ID: <417634BC.4060408@optonline.net>

   With a character vector it works fine, but your ca11c93SOL1$RECTAN 
seems to be a factor.

 > letters[letters > "j"]
  [1] "k" "l" "m" "n" "o" "p" "q" "r" "s" "t" "u" "v" "w" "x" "y" "z"

 > letfact <- as.factor(letters)
 > letfact[letfact > "j"]
  [1] <NA> <NA> <NA> <NA> <NA> <NA> <NA> <NA> <NA> <NA> <NA> <NA> <NA> 
<NA> <NA>
[16] <NA> <NA> <NA> <NA> <NA> <NA> <NA> <NA> <NA> <NA> <NA>
Levels: a b c d e f g h i j k l m n o p q r s t u v w x y z
Warning message:
">" not meaningful for factors in: Ops.factor(letfact, "j")

Robert Brown FM CEFAS wrote:
> I'm trying to select a subset from character vector using the logical operator >, but get the following error 
> 
> 6356     85     SOL   1     25     1   38E6
> 6357     85     SOL   1     27     1   38E6
> 6910     95     SOL   1     25     1   38E6
> 7152     98     SOL   1     19     1   38E6
> 7153     98     SOL   1     22     2   38E6
> 7154     98     SOL   1     28     1   38E6
> 
>>ca11c93SOL1VIIa<-ca11c93SOL1[ca11c93SOL1$RECTAN>"32Z9",]
> 
> Warning message: 
> ">" not meaningful for factors in: Ops.factor(ca11c93SOL1$RECTAN, "32Z9") 
> 
> (note RECTAN is the name assigned to the 7 th column)
> 
> Do these operators act on chacters vectors as opposed to numeric vectors and if so what are the rules for this?
> 
> Regards,
> 
> Robert Brown
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
> 

-- 
Chuck Cleland, Ph.D.
NDRI, Inc.
71 West 23rd Street, 8th floor
New York, NY 10010
tel: (212) 845-4495 (Tu, Th)
tel: (732) 452-1424 (M, W, F)
fax: (917) 438-0894



From sdavis2 at mail.nih.gov  Wed Oct 20 12:06:42 2004
From: sdavis2 at mail.nih.gov (Sean Davis)
Date: Wed, 20 Oct 2004 06:06:42 -0400
Subject: [R] Problem with Perl script calling R function
In-Reply-To: <63365.81.226.218.63.1098260173.squirrel@81.226.218.63>
References: <63365.81.226.218.63.1098260173.squirrel@81.226.218.63>
Message-ID: <BDB26A13-227F-11D9-BE4E-000A95D7BA10@mail.nih.gov>

It seems that read.flow is not present.  Perhaps it is in another 
library that needs to be loaded?  I've not used S-Net or R-1.4.1, 
though.

Sean

On Oct 20, 2004, at 4:16 AM, Sivakumar Ramagopal wrote:

> Hi,
>
> I'm a student doing my masters thesis and trying to work with S-Net
> version 1.0 (tool used for statistical analysis and visualizations of
> internet traffic) that is written in R.
>
> The problem is this: I have a perl script calling a function defined 
> in an
> R script that is located in a directly different from where the perl
> script is. This gives the following error.
>
>>>> echo
> 'read.flow("BLDemo.2000.03.08.h12.m30.35.tcppac.gz",type="tcppac")' |
> /cab/ce/user/ramagopa/lib/bin/R --no-save --no-restore --quiet --slave
> Error: couldn't find function "read.flow"
> Execution halted
>
> Platform details:
> -----------------
> SNet-1.0
> R-1.4.1 (I have to use this version because the version of SNet 
> requires
> only this).
> R-recommended-1.4.1
> perl-5.8.5
>
> Could someone help me out with this problem? Additionally, I'd really
> appreciate it if someone can point me to an SNet mailing list; I 
> haven't
> found one on the web.
>
> Thanks,
> Shiva
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html



From ramagopa at etek.chalmers.se  Wed Oct 20 12:18:50 2004
From: ramagopa at etek.chalmers.se (Sivakumar Ramagopal)
Date: Wed, 20 Oct 2004 12:18:50 +0200 (CEST)
Subject: [R] Problem with Perl script calling R function
In-Reply-To: <BDB26A13-227F-11D9-BE4E-000A95D7BA10@mail.nih.gov>
References: <63365.81.226.218.63.1098260173.squirrel@81.226.218.63>
	<BDB26A13-227F-11D9-BE4E-000A95D7BA10@mail.nih.gov>
Message-ID: <62197.81.226.218.63.1098267530.squirrel@81.226.218.63>

read.flow is defined in another file (in my case its called 'SNet',
without any extensions) in another directory.

The perl script is located in /usr/lib/R/library/SNet/exec/. This
directory is in my PATH.

The SNet file is in /usr/lib/R/library/SNet/R/.

Regards,
Shiva


> It seems that read.flow is not present.  Perhaps it is in another
> library that needs to be loaded?  I've not used S-Net or R-1.4.1,
> though.
>
> Sean
>
> On Oct 20, 2004, at 4:16 AM, Sivakumar Ramagopal wrote:
>
>> Hi,
>>
>> I'm a student doing my masters thesis and trying to work with S-Net
>> version 1.0 (tool used for statistical analysis and visualizations of
>> internet traffic) that is written in R.
>>
>> The problem is this: I have a perl script calling a function defined
>> in an
>> R script that is located in a directly different from where the perl
>> script is. This gives the following error.
>>
>>>>> echo
>> 'read.flow("BLDemo.2000.03.08.h12.m30.35.tcppac.gz",type="tcppac")' |
>> /cab/ce/user/ramagopa/lib/bin/R --no-save --no-restore --quiet --slave
>> Error: couldn't find function "read.flow"
>> Execution halted
>>
>> Platform details:
>> -----------------
>> SNet-1.0
>> R-1.4.1 (I have to use this version because the version of SNet
>> requires
>> only this).
>> R-recommended-1.4.1
>> perl-5.8.5
>>
>> Could someone help me out with this problem? Additionally, I'd really
>> appreciate it if someone can point me to an SNet mailing list; I
>> haven't
>> found one on the web.
>>
>> Thanks,
>> Shiva
>>
>> ______________________________________________
>> R-help at stat.math.ethz.ch mailing list
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide!
>> http://www.R-project.org/posting-guide.html
>
>



From gavin.simpson at ucl.ac.uk  Wed Oct 20 12:38:47 2004
From: gavin.simpson at ucl.ac.uk (Gavin Simpson)
Date: Wed, 20 Oct 2004 11:38:47 +0100
Subject: [R] Selecting from a character vector with logical operators
In-Reply-To: <417634BC.4060408@optonline.net>
References: <3589BC4D64C84341AE0C258244F977A2B60B55@expressa.corp.cefas.co.uk>
	<417634BC.4060408@optonline.net>
Message-ID: <41764037.3060208@ucl.ac.uk>

Chuck Cleland wrote:
>   With a character vector it works fine, but your ca11c93SOL1$RECTAN 
> seems to be a factor.
> 
>  > letters[letters > "j"]
>  [1] "k" "l" "m" "n" "o" "p" "q" "r" "s" "t" "u" "v" "w" "x" "y" "z"
> 
>  > letfact <- as.factor(letters)
>  > letfact[letfact > "j"]
>  [1] <NA> <NA> <NA> <NA> <NA> <NA> <NA> <NA> <NA> <NA> <NA> <NA> <NA> 
> <NA> <NA>
> [16] <NA> <NA> <NA> <NA> <NA> <NA> <NA> <NA> <NA> <NA> <NA>
> Levels: a b c d e f g h i j k l m n o p q r s t u v w x y z
> Warning message:
> ">" not meaningful for factors in: Ops.factor(letfact, "j")

You need an ordered factor for this to work:

 > letfact <- as.ordered(letters)
 > letfact[letfact > "j"]
  [1] k l m n o p q r s t u v w x y z
26 Levels: a < b < c < d < e < f < g < h < i < j < k < l < m < n < o < 
... < z

Gav

-- 
%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%
Gavin Simpson                     [T] +44 (0)20 7679 5522
ENSIS Research Fellow             [F] +44 (0)20 7679 7565
ENSIS Ltd. & ECRC                 [E] gavin.simpson at ucl.ac.uk
UCL Department of Geography       [W] http://www.ucl.ac.uk/~ucfagls/cv/
26 Bedford Way                    [W] http://www.ucl.ac.uk/~ucfagls/
London.  WC1H 0AP.
%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%



From Roger.Bivand at nhh.no  Wed Oct 20 13:13:49 2004
From: Roger.Bivand at nhh.no (Roger Bivand)
Date: Wed, 20 Oct 2004 13:13:49 +0200 (CEST)
Subject: [R] Slope of surface
In-Reply-To: <41762EBB.30806@lancaster.ac.uk>
Message-ID: <Pine.LNX.4.44.0410201304230.1221-100000@reclus.nhh.no>

On Wed, 20 Oct 2004, Barry Rowlingson wrote:

> (Ted Harding) wrote:
> > On 19-Oct-04 Laura Quinn wrote:
> > 
> >>Is there a neat way of working out the slope of a flat surface in R?
> 
> >>Given (x,y,z) co-ordinates of the four corners of a square, is there a
> >>function which will allow me to calculate the "mean" slope of the
> >>surface in a given direction?
> 
>   I had a quick look at the help for r.slope.aspect for the GRASS GIS 
> [1]. To compute the slope of a raster grid it uses 'Horn's Formula' [2] 
> which it says works by taking the 3x3 neighbourhood of the cell in 
> question. If you really want a slope map over a big grid, this might be 
> a better way to do it.

Yes, a more recent paper runs different methods against each other (Jones, 
K. H. 1998. A comparison of algorithms used to compute hill slope as a 
property of the DEM. Computers & Geosciences 24 (4), 315-323.), and finds 
that the method due to Zevenbergen & Thorne (Zevenbergen, L. W., Thorne, 
C. R. 1987. Quantitative analysis of land surface topography. Earth 
Surface Processes and Landforms, 12 (1), 47-56.) is generally pretty good, 
based on the same 3x3 moving window. The DEM will usually then give you 
not only slope and aspect, but also plane and profile curvature. As far as 
I remember, the algorithm is described in Burrough & McDonnell Principles 
of GIS (1998). This lecture page is quite informative:

http://www.geog.ubc.ca/courses/geog516/talks_2001/slope_calculation.html

What I'm not sure about is whether this answers Laura's question about 
slope "in a given direction", though - all these methods give slope in the 
aspect direction, not at an arbitrary angle.

Roger

> 
> 
> Barry
> 
> 
> [1] 
> http://supportweb.cs.bham.ac.uk/documentation/grass5/html/r.slope.aspect.html
> 
> [2] Horn, B. K. P. (1981). Hill Shading and the Reflectance Map, 
> Proceedings of the IEEE, 69(1):14-47.
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
> 

-- 
Roger Bivand
Economic Geography Section, Department of Economics, Norwegian School of
Economics and Business Administration, Breiviksveien 40, N-5045 Bergen,
Norway. voice: +47 55 95 93 55; fax +47 55 95 93 93
e-mail: Roger.Bivand at nhh.no



From M.Mamin at intershop.de  Wed Oct 20 13:25:14 2004
From: M.Mamin at intershop.de (Marc Mamin)
Date: Wed, 20 Oct 2004 13:25:14 +0200
Subject: [R] does R provides a wait or pause function?
Message-ID: <A03188C6623C0D46A703CB5AA59907F201C11C19@JENMAIL01.ad.intershop.net>


Hello,

I'd like to insert a "wait" function in my code.

The reason is that I output timestamped  files and I want to ensure that at least 1 second separes 2 files to avoid overwriting the previous file.

Thanks,

Marc Mamin



From ripley at stats.ox.ac.uk  Wed Oct 20 13:45:43 2004
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Wed, 20 Oct 2004 12:45:43 +0100 (BST)
Subject: [R] does R provides a wait or pause function?
In-Reply-To: <A03188C6623C0D46A703CB5AA59907F201C11C19@JENMAIL01.ad.intershop.net>
Message-ID: <Pine.LNX.4.44.0410201243090.10553-100000@gannet.stats>

On Wed, 20 Oct 2004, Marc Mamin wrote:

> I'd like to insert a "wait" function in my code.
> 
> The reason is that I output timestamped files and I want to ensure that
> at least 1 second separes 2 files to avoid overwriting the previous
> file.

?Sys.sleep (The Unix equivalent command and system call are both called
`sleep'.)

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From p.dalgaard at biostat.ku.dk  Wed Oct 20 13:42:53 2004
From: p.dalgaard at biostat.ku.dk (Peter Dalgaard)
Date: 20 Oct 2004 13:42:53 +0200
Subject: [R] does R provides a wait or pause function?
In-Reply-To: <A03188C6623C0D46A703CB5AA59907F201C11C19@JENMAIL01.ad.intershop.net>
References: <A03188C6623C0D46A703CB5AA59907F201C11C19@JENMAIL01.ad.intershop.net>
Message-ID: <x2y8i1d39e.fsf@biostat.ku.dk>

"Marc Mamin" <M.Mamin at intershop.de> writes:

> Hello,
> 
> I'd like to insert a "wait" function in my code.
> 
> The reason is that I output timestamped  files and I want to ensure that at least 1 second separes 2 files to avoid overwriting the previous file.

Sys.sleep(1)

-- 
   O__  ---- Peter Dalgaard             Blegdamsvej 3  
  c/ /'_ --- Dept. of Biostatistics     2200 Cph. N   
 (*) \(*) -- University of Copenhagen   Denmark      Ph: (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)             FAX: (+45) 35327907



From andy_liaw at merck.com  Wed Oct 20 14:15:40 2004
From: andy_liaw at merck.com (Liaw, Andy)
Date: Wed, 20 Oct 2004 08:15:40 -0400
Subject: [R] Q about strsplit and regexp
Message-ID: <3A822319EB35174CA3714066D590DCD504AF857D@usrymx25.merck.com>

Dear R-help,

This one is probably a piece of cake for regexp masters.  I'd like to split
a character vector (for simplicity, say of length one for now) that contains
fields that are delimited by arbitrary number of white spaces (e.g., "  a b
c ").  How do I get the character vector that contain the fields?  In the
example I gave, I've tried:

> strsplit("  a b    c ", " +")
[[1]]
[1] ""  "a" "b" "c"

I do not want that empty character in the beginning, but couldn't figure out
how to strip the starting white spaces, other than something ugly like:

> strsplit(sub("^ +", "", "  a b    c "), " +")
[[1]]
[1] "a" "b" "c"

Can some kind soul point me to a simpler way?  TIA!!

Best,
Andy

Andy Liaw, PhD
Biometrics Research      PO Box 2000, RY33-300     
Merck Research Labs           Rahway, NJ 07065
andy_liaw <at> merck.com          732-594-0820



From lperepol at necessity.org  Wed Oct 20 14:18:10 2004
From: lperepol at necessity.org (lawrence Perepolkin)
Date: Wed, 20 Oct 2004 08:18:10 -0400
Subject: [R] Plotting a 3D surface 
Message-ID: <000001c4b69e$de51d750$6a02a8c0@win1>

Hi 

Does R have a function or has someone written a function to draw a 3d
surface from a scatter plot of values using either ksmooth or locpoly. OR a
transform function a that merges x relation z and y relation z to (x,y)
relation z?  

I tried out scatterplot3d but it seems it would take a bit of work to get
scatterplot3d to draw a curved surface.


Lawrence



From dataanalytics at rediffmail.com  Wed Oct 20 14:19:17 2004
From: dataanalytics at rediffmail.com (Arin Basu)
Date: 20 Oct 2004 12:19:17 -0000
Subject: [R] Drawing multiple line plots
Message-ID: <20041020121917.22347.qmail@webmail46.rediffmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20041020/d4729d82/attachment.pl

From murdoch at stats.uwo.ca  Wed Oct 20 14:41:17 2004
From: murdoch at stats.uwo.ca (Duncan Murdoch)
Date: Wed, 20 Oct 2004 08:41:17 -0400
Subject: [R] Plotting a 3D surface 
In-Reply-To: <000001c4b69e$de51d750$6a02a8c0@win1>
References: <000001c4b69e$de51d750$6a02a8c0@win1>
Message-ID: <7omcn0d64haodsinknl4ec7aovb2sqfkgp@4ax.com>

On Wed, 20 Oct 2004 08:18:10 -0400, "lawrence Perepolkin"
<lperepol at necessity.org> wrote :

>Hi 
>
>Does R have a function or has someone written a function to draw a 3d
>surface from a scatter plot of values using either ksmooth or locpoly. OR a
>transform function a that merges x relation z and y relation z to (x,y)
>relation z?  
>
>I tried out scatterplot3d but it seems it would take a bit of work to get
>scatterplot3d to draw a curved surface.

persp() or functions in the rgl package can draw the curved surface,
but you'll need to calculate it yourself first.  You should fit a
model to the surface then use predict to find the predictions at every
grid point; that's the sort of data that persp() wants.  As far as I
know both ksmooth and locpoly are one dimensional scatterplot
smoothers, but other functions (e.g. gam() in mgcv) can do higher
dimensional smoothing.

Duncan murdoch



From ccleland at optonline.net  Wed Oct 20 14:45:19 2004
From: ccleland at optonline.net (Chuck Cleland)
Date: Wed, 20 Oct 2004 08:45:19 -0400
Subject: [R] Drawing multiple line plots
In-Reply-To: <20041020121917.22347.qmail@webmail46.rediffmail.com>
References: <20041020121917.22347.qmail@webmail46.rediffmail.com>
Message-ID: <41765DDF.5080202@optonline.net>

?matplot

   For example:

myFrame <- data.frame(lowest = c(0.107, 0.091, 0.126),
                       second = c(0.115, 0.107, 0.103),
                       third = c(0.123, 0.115, 0.126),
                       fourth = c(0.115, 0.142, 0.129),
                       highest = c(0.166, 0.179, 0.142),
                       sig = c(0.000, 0.000, 0.031))

par(las=1)

matplot(t(myFrame[,-6]), type="l", xaxt="n", ylab="INMET",
                          col=c("black", "red", "blue"), lty=c(1,1,1))

axis(side=1, at=1:5, names(myFrame)[1:5])

legend(1, 0.18, c("INAS", "MMA", "DMA"), lty=c(1,1,1),
                 col=c("black", "red", "blue"))

Arin Basu wrote:
> Hi All:
> 
> Greetings, and best wishes from the festive times here at Kolkata, India -- the time of Durga Puja celebrations. 
> 
> I seek your advice as I try plotting lines for my data. The problem:
> 
> I have created a dataframe that looks like this (name: myFrame):
> 
>           lowest second third fourth highest    significance
> INAS      0.107  0.115 0.123  0.115   0.166 0.000
> MMA       0.091  0.107 0.115  0.142   0.179 0.000
> DMA       0.126  0.103 0.126  0.129   0.142 0.031
> 
> The numbers in the dataframe indicate values of a continuous variable INMET (indicating ratio of inorganic arsenic vesus methylated arsenic species) for the respective quintiles of variables INAS, MMA, and DMA; "lowest" through "highest" indicate quintile levels of INAS, 
> MMA, and DMA. INAS, MMA, and DMA are discrete ordinal variables (factors, each variable has following levels: lowest, ..., highest). Significance indicates significance levels. 
> 
>>From this dataframe, I want to plot INAS, MMA, and DMA (x-axis : levels "lowest" through "highest", y-axis: values of INMET) as separate lineplots in the same graphic window. I can individually plot lines for INAS,MMA, and DMA; however, I want the three lines to appear in the same graph (say in different colors). 
> 
> Searching the archives and the documentation, I got to interaction.plot(), but this is not what I want. 
> 
> I can export the dataframe to a spreadsheet program and can draw such a graph (one graph with multiple lines, one for INAS, one for MMA, and one for DMA), but would prefer to do it in R. 
> 
> I looked for, but could not locate how to do that in either the R documentation, or the archives of mailing list postings. I suspect I am missing something very obvious; if there is a solution that I was inept to look up in the documentation or archives, I duly apologize.
> 
> Would greatly appreciate your advice.
> 
> Kind regards,
> Arin Basu
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
> 

-- 
Chuck Cleland, Ph.D.
NDRI, Inc.
71 West 23rd Street, 8th floor
New York, NY 10010
tel: (212) 845-4495 (Tu, Th)
tel: (732) 452-1424 (M, W, F)
fax: (917) 438-0894



From Jean-Pierre.Mueller at unil.ch  Wed Oct 20 14:49:00 2004
From: Jean-Pierre.Mueller at unil.ch (Jean-Pierre Muller)
Date: Wed, 20 Oct 2004 14:49:00 +0200
Subject: [R] Q about strsplit and regexp
In-Reply-To: <3A822319EB35174CA3714066D590DCD504AF857D@usrymx25.merck.com>
References: <3A822319EB35174CA3714066D590DCD504AF857D@usrymx25.merck.com>
Message-ID: <6A13537E-2296-11D9-8054-000D93AE2752@dssp.unil.ch>

Hello,

in the function ttda.segmentation of ttda 
<http://wwwpeople.unil.ch/jean-pierre.mueller/>

i use:

     #compute occurences
     occurences <- unlist(strsplit(textlines[1:length(textlines)],
         grep.sep, TRUE))
     #delete empty lines
     occurences <- occurences[nchar(occurences) > 0]

HTH.


Le 20 oct. 04, ?? 14:15, Liaw, Andy a ??crit :

> Dear R-help,
>
> This one is probably a piece of cake for regexp masters.  I'd like to 
> split
> a character vector (for simplicity, say of length one for now) that 
> contains
> fields that are delimited by arbitrary number of white spaces (e.g., " 
>  a b
> c ").  How do I get the character vector that contain the fields?  In 
> the
> example I gave, I've tried:
>
>> strsplit("  a b    c ", " +")
> [[1]]
> [1] ""  "a" "b" "c"
>
> I do not want that empty character in the beginning, but couldn't 
> figure out
> how to strip the starting white spaces, other than something ugly like:
>
>> strsplit(sub("^ +", "", "  a b    c "), " +")
> [[1]]
> [1] "a" "b" "c"
>
> Can some kind soul point me to a simpler way?  TIA!!
>
> Best,
> Andy
>
> Andy Liaw, PhD
> Biometrics Research      PO Box 2000, RY33-300
> Merck Research Labs           Rahway, NJ 07065
> andy_liaw <at> merck.com          732-594-0820
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
>
>
-- 
Jean-Pierre M??ller
SSP / BFSH2 / UNIL / CH - 1015 Lausanne
Voice:+41 21 692 3116 / Fax:+41 21 692 3115

Please avoid sending me Word or PowerPoint attachments.
  See http://www.fsf.org/philosophy/no-word-attachments.html
S'il vous pla??t, ??vitez de m'envoyer des attachements au format Word ou 
PowerPoint.
  Voir http://www.fsf.org/philosophy/no-word-attachments.fr.html



From jfox at mcmaster.ca  Wed Oct 20 14:54:13 2004
From: jfox at mcmaster.ca (John Fox)
Date: Wed, 20 Oct 2004 08:54:13 -0400
Subject: [R] Plotting a 3D surface 
In-Reply-To: <000001c4b69e$de51d750$6a02a8c0@win1>
Message-ID: <20041020125413.EIYC26826.tomts10-srv.bellnexxia.net@JohnDesktop8300>

Dear Lawrence,

You might take a look at the scatter3d function in the Rcmdr package, which
uses the rgl package. scatter3d() includes some smoothing, but not by
ksmooth() or locpoly(), so you might have to alter it for your purposes, but
that shouldn't be hard.

I hope that this helps,
 John

--------------------------------
John Fox
Department of Sociology
McMaster University
Hamilton, Ontario
Canada L8S 4M4
905-525-9140x23604
http://socserv.mcmaster.ca/jfox 
-------------------------------- 

> -----Original Message-----
> From: r-help-bounces at stat.math.ethz.ch 
> [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of 
> lawrence Perepolkin
> Sent: Wednesday, October 20, 2004 7:18 AM
> To: R-help at stat.math.ethz.ch
> Subject: [R] Plotting a 3D surface 
> 
> Hi 
> 
> Does R have a function or has someone written a function to 
> draw a 3d surface from a scatter plot of values using either 
> ksmooth or locpoly. OR a transform function a that merges x 
> relation z and y relation z to (x,y) relation z?  
> 
> I tried out scatterplot3d but it seems it would take a bit of 
> work to get scatterplot3d to draw a curved surface.
> 
> 
> Lawrence
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html



From ligges at statistik.uni-dortmund.de  Wed Oct 20 15:00:32 2004
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Wed, 20 Oct 2004 15:00:32 +0200
Subject: [R] common axis label in multiple plot area
In-Reply-To: <41762264.2060604@unibas.ch>
References: <41762264.2060604@unibas.ch>
Message-ID: <41766170.6070009@statistik.uni-dortmund.de>

Sebastian Leuzinger wrote:

> Hello
> A very short question: Using multiple plots with par(mfrow=c(3,3)), how 
> can I get R to indicate one common y- and x-axis label? I tried to use 
> text() in par, but this is then overwritten when I plot the graphs.

Use par(oma=...) to set the outer margins > 0 and use mtext(..., 
outer=TRUE) to write into the outer margins.

Uwe Ligges



From jps at sanger.ac.uk  Wed Oct 20 15:00:30 2004
From: jps at sanger.ac.uk (Jason Skelton)
Date: Wed, 20 Oct 2004 14:00:30 +0100
Subject: [R] heatmap.2 ordering & color key
Message-ID: <4176616E.5040801@sanger.ac.uk>

HI All

sorry if this question has already been asked but I couldn't find 
anything that answered my question

I have 24 columns of data that I'm trying to plot in heatmap.2 
(gregmisc) and I'm having difficulty ordering them except in numerical 
sequence:
I have transposed my matrix so it will appear with the dendrogram I want 
appearing at the top of the heatmap
If I use either of these orders the ordering works:
order <- c(1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24)
order2 <- c(24,23,22,21,20,19,18,17,16,15,14,13,12,11,10,9,8,7,6,5,4,3,2,1)
using the command:
heatmap.2(tmatrix, clusteredmatrixasdendrogram, col=bluered600, 
dendrogram="column", scale="none", trace="none", Rowv=order)
however If I start mixing the numbers up e.g:

order3 <- c(1,4,7,10,2,5,8,11,3,6,9,12,13,16,19,22,14,17,20,23,15,18,21,24)
or
order4 <- c(24,21,18,15,23,20,17,14,22,19,16,13,12,9,6,3,11,8,5,2,10,7,4,1)

The heatmap is reordered but NOT how I have specified:
The actual order it returns in either case is the same:

1,5,9,2,6,10,3,7,11,4,8,12,13,17,21,14,18,22,15,19,23,16,20,24 which I'm 
completely confused about as I'm not plotting a dendrogram for the rows
or letting the heatmap function draw its own so where is it getting the 
order from ? and is it possible to plot it like I'm trying to ?

My second question is also from the gregmisc package
I'm using bluered600 <- bluered(600) to specify the range of colors plotted
my data ranges from -60 to +80 but I would like to specify that the 
colours range from blue to red with white being zero
however because of the unequal distribution zero is shifted into the red 
area (hope that makes sense)
I know I can specify colors for "low" "medium" and "high", in some 
graphics functions but this isn't what I'm trying to do
unless my data is evenly distributed i.e -60 to +60 I can't plot exactly 
how I want to


If anyone has any ideas that would be fantastic

Cheers

Jason


-- 
--------------------------------
Jason Skelton
Pathogen Microarrays
Wellcome Trust Sanger Institute
Hinxton
Cambridge
CB10 1SA

Tel +44(0)1223 834244 Ext 7123
Fax +44(0)1223 494919



From dimitris.rizopoulos at med.kuleuven.ac.be  Wed Oct 20 15:03:53 2004
From: dimitris.rizopoulos at med.kuleuven.ac.be (Dimitris Rizopoulos)
Date: Wed, 20 Oct 2004 15:03:53 +0200
Subject: [R] Q about strsplit and regexp
References: <3A822319EB35174CA3714066D590DCD504AF857D@usrymx25.merck.com>
Message-ID: <001e01c4b6a5$4c7f80a0$0540210a@www.domain>

Hi Andy,

may be something like:

x <- "  a b     c "
##########
nx <- nchar(x)
x. <- substring(x, 1:nx, 1:nx)
x.[x.!=" "]

could be helpful.

Best,
Dimitris

----
Dimitris Rizopoulos
Ph.D. Student
Biostatistical Centre
School of Public Health
Catholic University of Leuven

Address: Kapucijnenvoer 35, Leuven, Belgium
Tel: +32/16/396887
Fax: +32/16/337015
Web: http://www.med.kuleuven.ac.be/biostat/
     http://www.student.kuleuven.ac.be/~m0390867/dimitris.htm


----- Original Message ----- 
From: "Liaw, Andy" <andy_liaw at merck.com>
To: "R-Help" <r-help at r-project.org>
Sent: Wednesday, October 20, 2004 2:15 PM
Subject: [R] Q about strsplit and regexp


> Dear R-help,
>
> This one is probably a piece of cake for regexp masters.  I'd like 
> to split
> a character vector (for simplicity, say of length one for now) that 
> contains
> fields that are delimited by arbitrary number of white spaces (e.g., 
> "  a b
> c ").  How do I get the character vector that contain the fields? 
> In the
> example I gave, I've tried:
>
>> strsplit("  a b    c ", " +")
> [[1]]
> [1] ""  "a" "b" "c"
>
> I do not want that empty character in the beginning, but couldn't 
> figure out
> how to strip the starting white spaces, other than something ugly 
> like:
>
>> strsplit(sub("^ +", "", "  a b    c "), " +")
> [[1]]
> [1] "a" "b" "c"
>
> Can some kind soul point me to a simpler way?  TIA!!
>
> Best,
> Andy
>
> Andy Liaw, PhD
> Biometrics Research      PO Box 2000, RY33-300
> Merck Research Labs           Rahway, NJ 07065
> andy_liaw <at> merck.com          732-594-0820
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
>



From jmoreira at fe.up.pt  Wed Oct 20 15:04:33 2004
From: jmoreira at fe.up.pt (jmoreira@fe.up.pt)
Date: Wed, 20 Oct 2004 14:04:33 +0100
Subject: [R] date and time format for nonparametric regression
Message-ID: <1098277473.4176626121ade@webmail.fe.up.pt>


Hello,

I have my time series data in Oracle. I want to read the data to test some 
nonparametric regression techniques, such as, neural networks, support 
vectors, random forests, etc. My question is: does someone have experience on 
what is the best way to use the date data? That is, shall I read the date 
(date and time) as a single variable, or is it better to split date and time 
in different variables? The answer is the same for all non parametric 
regression techniques? For time after midnight, the descontinuity from 24 
hours to 0 hours doesn't create problems when using those algorithms?

Thanks in advance

Joao Moreira



From jfox at mcmaster.ca  Wed Oct 20 15:07:28 2004
From: jfox at mcmaster.ca (John Fox)
Date: Wed, 20 Oct 2004 09:07:28 -0400
Subject: [R] Q about strsplit and regexp
In-Reply-To: <3A822319EB35174CA3714066D590DCD504AF857D@usrymx25.merck.com>
Message-ID: <20041020130728.ZULM1536.tomts25-srv.bellnexxia.net@JohnDesktop8300>

Dear Andy,

This is something that I sometimes want to do, so I have a little utility
that trims blanks and tabs from the beginnings and ends of strings:

trim.ws <- function(text) gsub("^[\ \t]", "", gsub("[\ \t]*$", "", text))

Regards,
 John

--------------------------------
John Fox
Department of Sociology
McMaster University
Hamilton, Ontario
Canada L8S 4M4
905-525-9140x23604
http://socserv.mcmaster.ca/jfox 
-------------------------------- 

> -----Original Message-----
> From: r-help-bounces at stat.math.ethz.ch 
> [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Liaw, Andy
> Sent: Wednesday, October 20, 2004 7:16 AM
> To: R-Help
> Subject: [R] Q about strsplit and regexp
> 
> Dear R-help,
> 
> This one is probably a piece of cake for regexp masters.  I'd 
> like to split a character vector (for simplicity, say of 
> length one for now) that contains fields that are delimited 
> by arbitrary number of white spaces (e.g., "  a b c ").  How 
> do I get the character vector that contain the fields?  In 
> the example I gave, I've tried:
> 
> > strsplit("  a b    c ", " +")
> [[1]]
> [1] ""  "a" "b" "c"
> 
> I do not want that empty character in the beginning, but 
> couldn't figure out how to strip the starting white spaces, 
> other than something ugly like:
> 
> > strsplit(sub("^ +", "", "  a b    c "), " +")
> [[1]]
> [1] "a" "b" "c"
> 
> Can some kind soul point me to a simpler way?  TIA!!
> 
> Best,
> Andy
> 
> Andy Liaw, PhD
> Biometrics Research      PO Box 2000, RY33-300     
> Merck Research Labs           Rahway, NJ 07065
> andy_liaw <at> merck.com          732-594-0820
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html



From petr.pikal at precheza.cz  Wed Oct 20 15:12:12 2004
From: petr.pikal at precheza.cz (Petr Pikal)
Date: Wed, 20 Oct 2004 15:12:12 +0200
Subject: [R] common axis label in multiple plot area
In-Reply-To: <41762264.2060604@unibas.ch>
Message-ID: <4176804C.22027.1882BFF@localhost>

Hi

you can use outer margins and mtext for common labeling of 
multiple figures

see ?par    "oma" and ?mtext

Cheers
Petr


On 20 Oct 2004 at 10:31, Sebastian Leuzinger wrote:

> Hello
> A very short question: Using multiple plots with par(mfrow=c(3,3)),
> how can I get R to indicate one common y- and x-axis label? I tried to
> use text() in par, but this is then overwritten when I plot the
> graphs. -- Sebastian Leuzinger Institute of Botany, University of
> Basel Sch??nbeinstr. 6 CH-4056 Basel Ph. 0041 (0) 61 267 3511 fax  0041
> (0) 61 2673504 email: Sebastian.Leuzinger at unibas.ch
> <mailto:Sebastian.Leuzinger at unibas.ch> web: 
> http://www.unibas.ch/botschoen/leuzinger/e.shtml
> <http://www.unibas.ch/botschoen/leuzinger/d.shtml>
> 
>  [[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide!
> http://www.R-project.org/posting-guide.html

Petr Pikal
petr.pikal at precheza.cz



From secchi at sssup.it  Wed Oct 20 15:22:39 2004
From: secchi at sssup.it (Angelo Secchi)
Date: Wed, 20 Oct 2004 15:22:39 +0200
Subject: [R] Robust regression with groups
In-Reply-To: <000b01c481ba$650a4ad0$0800a8c0@olau>
References: <200408131009.i7DA7EHZ024157@hypatia.math.ethz.ch>
	<000b01c481ba$650a4ad0$0800a8c0@olau>
Message-ID: <20041020152239.406bc7bc.secchi@sssup.it>



Hi,
I have data on a group of subjects in different years. I should assume
that observations regarding different individuals are independent but
observations for the same individual in different years are not and I
would like to have an estimated standard error (and variance-covariance
matrix) taking into account this problem.

More in general is there a way in R to run a (robust)regression having
different groups in the observations and specifying that the observation
are independent across groups but not necessarily independent within
groups?

Thanks
a.



From supton at referentia.com  Wed Oct 20 15:32:32 2004
From: supton at referentia.com (Stephen Upton)
Date: Wed, 20 Oct 2004 09:32:32 -0400
Subject: [R] Q about strsplit and regexp
In-Reply-To: <3A822319EB35174CA3714066D590DCD504AF857D@usrymx25.merck.com>
Message-ID: <200410201332.i9KDWLV93860@mail2.referentia.com>

Hi Andy,

A slight variation on Jean-Pierre's:

x <- unlist(strsplit("  a b    c ","[[:space:]]"))
x <- x[nchar(x) > 0]

HTH
steve

> -----Original Message-----
> From: r-help-bounces at stat.math.ethz.ch [mailto:r-help-
> bounces at stat.math.ethz.ch] On Behalf Of Liaw, Andy
> Sent: Wednesday, October 20, 2004 8:16 AM
> To: R-Help
> Subject: [R] Q about strsplit and regexp
> 
> Dear R-help,
> 
> This one is probably a piece of cake for regexp masters.  I'd like to
> split
> a character vector (for simplicity, say of length one for now) that
> contains
> fields that are delimited by arbitrary number of white spaces (e.g., "  a
> b
> c ").  How do I get the character vector that contain the fields?  In the
> example I gave, I've tried:
> 
> > strsplit("  a b    c ", " +")
> [[1]]
> [1] ""  "a" "b" "c"
> 
> I do not want that empty character in the beginning, but couldn't figure
> out
> how to strip the starting white spaces, other than something ugly like:
> 
> > strsplit(sub("^ +", "", "  a b    c "), " +")
> [[1]]
> [1] "a" "b" "c"
> 
> Can some kind soul point me to a simpler way?  TIA!!
> 
> Best,
> Andy
> 
> Andy Liaw, PhD
> Biometrics Research      PO Box 2000, RY33-300
> Merck Research Labs           Rahway, NJ 07065
> andy_liaw <at> merck.com          732-594-0820
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-
> guide.html



From andy_liaw at merck.com  Wed Oct 20 15:43:15 2004
From: andy_liaw at merck.com (Liaw, Andy)
Date: Wed, 20 Oct 2004 09:43:15 -0400
Subject: [R] Q about strsplit and regexp
Message-ID: <3A822319EB35174CA3714066D590DCD504AF8583@usrymx25.merck.com>

Thanks to Barry Rawlingson, Peter Dalgaard, Jean-Pierre Muller, Dimitris
Rizopoulos, John Fox, and Stephen Upton for comments and suggestions.  Looks
like there's no easier way than to strip the spaces before splitting the
fields.  Several people suggested deleting the empty strings afterwards.  In
my particular application, there are typically thousands of fields, and I'd
think stripping leading (and maybe trailing) spaces in the original string
should be more efficient than computing nchar() on all fields afterwards.
(Although in reality it hardly makes any difference for me:  I'm only doing
this once, not gazillion times...)

So, in summary, I'm sticking with what I had originally.  Prof. Fox's
function for nuking leading and trailing white spaces will come in handy,
though.

Thanks again to all!

Best,
Andy

> From: Liaw, Andy
> 
> Dear R-help,
> 
> This one is probably a piece of cake for regexp masters.  I'd 
> like to split
> a character vector (for simplicity, say of length one for 
> now) that contains
> fields that are delimited by arbitrary number of white spaces 
> (e.g., "  a b
> c ").  How do I get the character vector that contain the 
> fields?  In the
> example I gave, I've tried:
> 
> > strsplit("  a b    c ", " +")
> [[1]]
> [1] ""  "a" "b" "c"
> 
> I do not want that empty character in the beginning, but 
> couldn't figure out
> how to strip the starting white spaces, other than something 
> ugly like:
> 
> > strsplit(sub("^ +", "", "  a b    c "), " +")
> [[1]]
> [1] "a" "b" "c"
> 
> Can some kind soul point me to a simpler way?  TIA!!
> 
> Best,
> Andy
> 
> Andy Liaw, PhD
> Biometrics Research      PO Box 2000, RY33-300     
> Merck Research Labs           Rahway, NJ 07065
> andy_liaw <at> merck.com          732-594-0820
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
> 
> 
> --------------------------------------------------------------
> ----------------
> Notice:  This e-mail message, together with any attachments, 
> contains information of Merck & Co., Inc. (One Merck Drive, 
> Whitehouse Station, New Jersey, USA 08889), and/or its 
> affiliates (which may be known outside the United States as 
> Merck Frosst, Merck Sharp & Dohme or MSD and in Japan, as 
> Banyu) that may be confidential, proprietary copyrighted 
> and/or legally privileged. It is intended solely for the use 
> of the individual or entity named on this message.  If you 
> are not the intended recipient, and have received this 
> message in error, please notify us immediately by reply 
> e-mail and then delete it from your system.
> --------------------------------------------------------------
> ----------------
>



From ahenningsen at email.uni-kiel.de  Wed Oct 20 15:52:54 2004
From: ahenningsen at email.uni-kiel.de (Arne Henningsen)
Date: Wed, 20 Oct 2004 15:52:54 +0200
Subject: [R] Q about strsplit and regexp
In-Reply-To: <3A822319EB35174CA3714066D590DCD504AF857D@usrymx25.merck.com>
References: <3A822319EB35174CA3714066D590DCD504AF857D@usrymx25.merck.com>
Message-ID: <200410201552.54216.ahenningsen@email.uni-kiel.de>

Dear Andy,

I also don't know a regular expression that does what you want. However, if 
you have to do this several times, you can avoid the 'ugly' command by:
> mystrsplit <- function( str ) strsplit(sub("^ +", "", str), " +")
> mystrsplit( "  a b    c ")
[[1]]
[1] "a" "b" "c"
> mystrsplit( " d  e   f")
[[1]]
[1] "d" "e" "f"

All the best,
Arne

On Wednesday 20 October 2004 14:15, Liaw, Andy wrote:
> Dear R-help,
>
> This one is probably a piece of cake for regexp masters.  I'd like to split
> a character vector (for simplicity, say of length one for now) that
> contains fields that are delimited by arbitrary number of white spaces
> (e.g., "  a b c ").  How do I get the character vector that contain the
> fields?  In the
>
> example I gave, I've tried:
> > strsplit("  a b    c ", " +")
>
> [[1]]
> [1] ""  "a" "b" "c"
>
> I do not want that empty character in the beginning, but couldn't figure
> out
>
> how to strip the starting white spaces, other than something ugly like:
> > strsplit(sub("^ +", "", "  a b    c "), " +")
>
> [[1]]
> [1] "a" "b" "c"
>
> Can some kind soul point me to a simpler way?  TIA!!
>
> Best,
> Andy
>
> Andy Liaw, PhD
> Biometrics Research      PO Box 2000, RY33-300
> Merck Research Labs           Rahway, NJ 07065
> andy_liaw <at> merck.com          732-594-0820
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide!
> http://www.R-project.org/posting-guide.html

-- 
Arne Henningsen
Department of Agricultural Economics
University of Kiel
Olshausenstr. 40
D-24098 Kiel (Germany)
Tel: +49-431-880 4445
Fax: +49-431-880 1397
ahenningsen at agric-econ.uni-kiel.de
http://www.uni-kiel.de/agrarpol/ahenningsen/



From bob.ohara at helsinki.fi  Wed Oct 20 15:56:48 2004
From: bob.ohara at helsinki.fi (Anon.)
Date: Wed, 20 Oct 2004 16:56:48 +0300
Subject: [R] Odd behaviour with scale()
Message-ID: <41766EA0.2050505@helsinki.fi>

Moi!

A student here has been getting a bit irritated with some side effects 
of scale() (OS is Windows XP, the behaviour occurs in R 2.0.0, but not 
1.7.1).  The problem is that she scales a variable in a data frame, then 
does a regression, and tries to get some predictions for some new data.  
However, at this point she gets an error (see the example below).  This 
seems to be because the scaled variable in the new data frame does not 
have the center and scale attributes, but the one in the old data frame 
does.

The work-around is to put the scaled variable intro a new data frame, 
which again won't have the attributes.  But it seems odd to me that 
whether a scale()'d variable has attributes depends on where it's 
placed.  I presume that this is because I'm not understanding something 
about the way R is working, rather than it being a bug.  Would anyone 
care to enlighten me?

 > Data1=data.frame(xx=1:10, yy=2.1:12)
 > Data1$xx=scale(Data1$xx)
 >
 > reg1=lm(yy~xx, data=Data1)
 > New=data.frame(xx=2:4)
 > b=predict(reg1, New, se.fit=T)
Error: variable 'xx' was fitted with nmatrix.1 but numeric was supplied
 >
 > New=data.frame(xx=scale(2:4, center=5.5, scale=3.02))
 > b=predict(reg1, New, se.fit=T)
Error: variable 'xx' was fitted with nmatrix.1 but numeric was supplied
 >

Bob

-- 
Bob O'Hara
Department of Mathematics and Statistics
P.O. Box 68 (Gustaf H??llstr??min katu 2b)
FIN-00014 University of Helsinki
Finland

Telephone: +358-9-191 51479
Mobile: +358 50 599 0540
Fax:  +358-9-191 51400
WWW:  http://www.RNI.Helsinki.FI/~boh/
Journal of Negative Results - EEB: www.jnr-eeb.org



From dimitris.rizopoulos at med.kuleuven.ac.be  Wed Oct 20 16:08:21 2004
From: dimitris.rizopoulos at med.kuleuven.ac.be (Dimitris Rizopoulos)
Date: Wed, 20 Oct 2004 16:08:21 +0200
Subject: [R] Robust regression with groups
References: <200408131009.i7DA7EHZ024157@hypatia.math.ethz.ch><000b01c481ba$650a4ad0$0800a8c0@olau>
	<20041020152239.406bc7bc.secchi@sssup.it>
Message-ID: <007b01c4b6ae$45f586e0$0540210a@www.domain>

Hi Angelo,

There are two possible options (at least to my knowledge):

1. to use a random-effects model, either using `lme' (packages: nlme, 
lme4) if you have normal data or `glmmPQL' (package: MASS) or `GLMM' 
(package: lme4) or `glmmML' (package:glmmML) if you cannot use the 
normal distribution.

2. to use a gee model with a robust (sandwich) std.error estimation. 
See at `gee' (package: gee) and `geese' (package: geepack).

I hope this helps.

Best,
Dimitris

----
Dimitris Rizopoulos
Ph.D. Student
Biostatistical Centre
School of Public Health
Catholic University of Leuven

Address: Kapucijnenvoer 35, Leuven, Belgium
Tel: +32/16/396887
Fax: +32/16/337015
Web: http://www.med.kuleuven.ac.be/biostat/
     http://www.student.kuleuven.ac.be/~m0390867/dimitris.htm




----- Original Message ----- 
From: "Angelo Secchi" <secchi at sssup.it>
To: <r-help at stat.math.ethz.ch>
Sent: Wednesday, October 20, 2004 3:22 PM
Subject: [R] Robust regression with groups


>
>
> Hi,
> I have data on a group of subjects in different years. I should 
> assume
> that observations regarding different individuals are independent 
> but
> observations for the same individual in different years are not and 
> I
> would like to have an estimated standard error (and 
> variance-covariance
> matrix) taking into account this problem.
>
> More in general is there a way in R to run a (robust)regression 
> having
> different groups in the observations and specifying that the 
> observation
> are independent across groups but not necessarily independent within
> groups?
>
> Thanks
> a.
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
>



From aorchid at mac.com  Wed Oct 20 16:12:35 2004
From: aorchid at mac.com (Aric Gregson)
Date: Wed, 20 Oct 2004 10:12:35 -0400
Subject: [R] R Cocoa won't load in Mac 10.3
Message-ID: <r02010400-1035-17BBA23022A211D99DC0000A959B3D22@[10.0.1.205]>

I apologize for the post, but I have not been able to find reference
to this searching the R-archives, web site and WWW.

I installed R-cocoa 2.0.0 using the binary installer package on Mac
OS 10.3. When I try to start the program from the GUI I get the
following error message:

    Unable to start R: R_HOME is not set. Please set all required
    environment variables before running this program.
    
It is set to the proper location according to all the documentation I
can find. I have R_HOME_DIR=/Library/Frameworks/R.framework/Resources.

R 2.0.0 will launch from the command-line without any difficulty.

Any ideas or directions to take to resolve this would be greatly
appreciated.

aric



From jost at cict.fr  Wed Oct 20 16:26:11 2004
From: jost at cict.fr (Christian Jost)
Date: Wed, 20 Oct 2004 16:26:11 +0200
Subject: [R] matrix of eigenvalues
In-Reply-To: <4175083B.6090605@stat.wisc.edu>
References: <a06002006bd9a7b5ffb79@[130.120.104.141]>
	<4175083B.6090605@stat.wisc.edu>
Message-ID: <a0600200bbd9c1ce167c6@[130.120.104.141]>

Thanks for all the replys. I understand now that eigen does not 
return a matrix with linearly independent eigenvectors (which we 
would need to get its inverse).

The question whether there is a function in R that does so remains. 
As an example, consider the problem
dx1/dt = -x1 - 4 x2
dx2/dt =  x1 + 3 x2
corresponding to the matrix system
dx/dt = A x, x(0) = x0 = c(1,2) (for example)

Analysing this system in R gives
A = matrix(cbind(c(-1,1),c(-4,3)),nrow=2)
for which eigen() returns a singular eigenvector matrix
det(eigen(A)$vectors)

Now, if I tried to solve the system by hand, I would find one 
eigenvalue of value 1 and an associated eigenvector v1=[2, -1]. I can 
complement this eigenvector to get a base, e.g. v2=[0,1] (which is 
linearly independent of v1). Defining now a passage matrix as
P = matrix(cbind(c(2,-1),c(0,1)),nrow=2)
I can compute the inverse
invP = solve(P)
and
invP %*% A %*% P -> D
will return an upper triangular matrix (with eigenvalues on the 
diagonal) for which the exponential can easily be computed by hand

expDt = diag(exp(t),2,2) %*% matrix(c(1,0,-2*t,1),nrow=2)
and the solution of the original system becomes

x(t) = P %*% expDt %*% invP %*% x0
and I can simulate a trajectory

times = seq(0,2,by=0.1);
x0 = c(1,2);
sol = matrix(0,nrow=2,ncol=length(times));
for (i in 1:length(times)) {
   t = times[i];
   expDt = diag(exp(t),2,2) %*% matrix(c(1,0,-2*t,1),nrow=2);
   sol[,i] = P %*% expDt %*% invP %*% x0
}
plot(times,sol[1,],type='l',ylim=c(min(sol),max(sol)))
lines(times,sol[2,],lty='dashed')

Thanks to all who followed down here. Now, is there a function in R 
that will construct my P automatically? Or should I apply directly 
the function mexp(A) (from package mrutil) and completely bypass the 
passage matrix stuff (which my math teachers like so much ;-)

Best, Christian.

>Christian Jost wrote:
>>I thought that the function
>>eigen(A)
>>will return a matrix with eigenvectors that are independent of each 
>>other (thus forming a base and the matrix being invertible). This 
>>seems not to be the case in the following example
>>A=matrix(c(1,2,0,1),nrow=2,byrow=T)
>>eigen(A) ->ev
>>solve(ev$vectors)
>>
>>note that I try to get the upper triangular form with eigenvalues 
>>on the diagonal and (possibly) 1 just atop the eigenvalues to be 
>>used to solve a linear differential equation
>>x' = Ax, x(0)=x0
>>x(t) = P exp(D t) P^-1 x0
>>where D is this upper triangular form and P is the "passage matrix" 
>>(not sure about the correct english name) given by a base of 
>>eigenvectors. So the test would be
>>solve(ev$vectors) %*% A %*% ev$vectors - D
>>should be 0
>>
>>Thanks for any help, Christian.
>>
>>ps: please copy reply also to my address, my subscription to the 
>>R-help list seems to have delays
>
>That particular matrix has repeated eigenvalues and a degenerate eigenspace.
>
>>  A <- matrix(c(1,0,2,1),nc=2)
>>  A
>      [,1] [,2]
>[1,]    1    2
>[2,]    0    1
>>  eigen(A)
>$values
>[1] 1 1
>
>$vectors
>      [,1]          [,2]
>[1,]    1 -1.000000e+00
>[2,]    0  1.110223e-16


-- 
***********************************************************
http://cognition.ups-tlse.fr/vas-y.php?id=chj  jost at cict.fr
Christian Jost                                   (PhD, MdC)
Centre de Recherches sur la Cognition Animale
Universite Paul Sabatier, Bat IV R3
118 route de Narbonne
31062 Toulouse cedex 4, France
Tel: +33 5 61 55 64 37   Fax: +33 5 61 55 61 54



From msong at galton.uchicago.edu  Wed Oct 20 16:29:34 2004
From: msong at galton.uchicago.edu (Minsun Song)
Date: Wed, 20 Oct 2004 09:29:34 -0500 (CDT)
Subject: [R] Question about the variogram
Message-ID: <Pine.LNX.4.44.0410200927530.6642-100000@porthos.uchicago.edu>


Hello 

Could I ask questions about the command variogram in package gstat??
In the variogram command options, I can not find how I can control "h"
which is difference between points,
If you can answer, I will really appreciate for that.
Thanks.
                                                                                
Minsun



From dimitris.rizopoulos at med.kuleuven.ac.be  Wed Oct 20 16:37:10 2004
From: dimitris.rizopoulos at med.kuleuven.ac.be (Dimitris Rizopoulos)
Date: Wed, 20 Oct 2004 16:37:10 +0200
Subject: [R] Odd behaviour with scale()
References: <41766EA0.2050505@helsinki.fi>
Message-ID: <00a201c4b6b2$4b657230$0540210a@www.domain>

Hi Bob,

note that `scale()' returns a matrix! This should work:

Data1$xx <- scale(Data1$xx)
sapply(Data1, data.class)
####
Data1$xx <- c(scale(Data1$xx))
sapply(Data1, data.class)

I hope it helps.

Best,
Dimitris

----
Dimitris Rizopoulos
Ph.D. Student
Biostatistical Centre
School of Public Health
Catholic University of Leuven

Address: Kapucijnenvoer 35, Leuven, Belgium
Tel: +32/16/396887
Fax: +32/16/337015
Web: http://www.med.kuleuven.ac.be/biostat/
     http://www.student.kuleuven.ac.be/~m0390867/dimitris.htm


----- Original Message ----- 
From: "Anon." <bob.ohara at helsinki.fi>
To: <r-help at stat.math.ethz.ch>
Sent: Wednesday, October 20, 2004 3:56 PM
Subject: [R] Odd behaviour with scale()


> Moi!
>
> A student here has been getting a bit irritated with some side 
> effects of scale() (OS is Windows XP, the behaviour occurs in R 
> 2.0.0, but not 1.7.1).  The problem is that she scales a variable in 
> a data frame, then does a regression, and tries to get some 
> predictions for some new data.  However, at this point she gets an 
> error (see the example below).  This seems to be because the scaled 
> variable in the new data frame does not have the center and scale 
> attributes, but the one in the old data frame does.
>
> The work-around is to put the scaled variable intro a new data 
> frame, which again won't have the attributes.  But it seems odd to 
> me that whether a scale()'d variable has attributes depends on where 
> it's placed.  I presume that this is because I'm not understanding 
> something about the way R is working, rather than it being a bug. 
> Would anyone care to enlighten me?
>
> > Data1=data.frame(xx=1:10, yy=2.1:12)
> > Data1$xx=scale(Data1$xx)
> >
> > reg1=lm(yy~xx, data=Data1)
> > New=data.frame(xx=2:4)
> > b=predict(reg1, New, se.fit=T)
> Error: variable 'xx' was fitted with nmatrix.1 but numeric was 
> supplied
> >
> > New=data.frame(xx=scale(2:4, center=5.5, scale=3.02))
> > b=predict(reg1, New, se.fit=T)
> Error: variable 'xx' was fitted with nmatrix.1 but numeric was 
> supplied
> >
>
> Bob
>
> -- 
> Bob O'Hara
> Department of Mathematics and Statistics
> P.O. Box 68 (Gustaf H??llstr??min katu 2b)
> FIN-00014 University of Helsinki
> Finland
>
> Telephone: +358-9-191 51479
> Mobile: +358 50 599 0540
> Fax:  +358-9-191 51400
> WWW:  http://www.RNI.Helsinki.FI/~boh/
> Journal of Negative Results - EEB: www.jnr-eeb.org
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
>



From andy_liaw at merck.com  Wed Oct 20 16:40:28 2004
From: andy_liaw at merck.com (Liaw, Andy)
Date: Wed, 20 Oct 2004 10:40:28 -0400
Subject: [R] Odd behaviour with scale()
Message-ID: <3A822319EB35174CA3714066D590DCD504AF8586@usrymx25.merck.com>

The problem is that scale() returns a matrix, even if only a vector is
supplied.  Thus the regression model actually has a matrix (scaled xx) as
the predictor.  See:

> str(Data1)
`data.frame':   10 obs. of  2 variables:
 $ xx: num [1:10, 1] -1.486 -1.156 -0.826 -0.495 -0.165 ...
  ..- attr(*, "scaled:center")= num 5.5
  ..- attr(*, "scaled:scale")= num 3.03
 $ yy: num  2.1 3.1 4.1 5.1 6.1 7.1 8.1 9.1 10.1 11.1

and note that xx is not `num [1:10]',  but `num [1:10, 1]'.  If you give a
single column matrix as xx to predict(), it would work:

> predict(reg1, data.frame(xx=I(matrix(2:4, ncol=1))), se.fit=T)
$fit
       1        2        3 
12.65530 15.68295 18.71060 

$se.fit
           1            2            3 
4.582710e-16 6.513913e-16 8.510747e-16 

$df
[1] 8

$residual.scale
[1] 6.210772e-16

HTH,
Andy

> From: Anon.
> 
> Moi!
> 
> A student here has been getting a bit irritated with some 
> side effects 
> of scale() (OS is Windows XP, the behaviour occurs in R 
> 2.0.0, but not 
> 1.7.1).  The problem is that she scales a variable in a data 
> frame, then 
> does a regression, and tries to get some predictions for some 
> new data.  
> However, at this point she gets an error (see the example 
> below).  This 
> seems to be because the scaled variable in the new data frame 
> does not 
> have the center and scale attributes, but the one in the old 
> data frame 
> does.
> 
> The work-around is to put the scaled variable intro a new data frame, 
> which again won't have the attributes.  But it seems odd to me that 
> whether a scale()'d variable has attributes depends on where it's 
> placed.  I presume that this is because I'm not understanding 
> something 
> about the way R is working, rather than it being a bug.  Would anyone 
> care to enlighten me?
> 
>  > Data1=data.frame(xx=1:10, yy=2.1:12)
>  > Data1$xx=scale(Data1$xx)
>  >
>  > reg1=lm(yy~xx, data=Data1)
>  > New=data.frame(xx=2:4)
>  > b=predict(reg1, New, se.fit=T)
> Error: variable 'xx' was fitted with nmatrix.1 but numeric 
> was supplied
>  >
>  > New=data.frame(xx=scale(2:4, center=5.5, scale=3.02))
>  > b=predict(reg1, New, se.fit=T)
> Error: variable 'xx' was fitted with nmatrix.1 but numeric 
> was supplied
>  >
> 
> Bob
> 
> -- 
> Bob O'Hara
> Department of Mathematics and Statistics
> P.O. Box 68 (Gustaf H??llstr??min katu 2b)
> FIN-00014 University of Helsinki
> Finland
> 
> Telephone: +358-9-191 51479
> Mobile: +358 50 599 0540
> Fax:  +358-9-191 51400
> WWW:  http://www.RNI.Helsinki.FI/~boh/
> Journal of Negative Results - EEB: www.jnr-eeb.org
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
> 
>



From tlumley at u.washington.edu  Wed Oct 20 16:47:05 2004
From: tlumley at u.washington.edu (Thomas Lumley)
Date: Wed, 20 Oct 2004 07:47:05 -0700 (PDT)
Subject: [R] help.search intersection
In-Reply-To: <C698D707214E6F4AB39AB7096C3DE5A56D8E81@phost015.EVAFUNDS.intermedia.net>
References: <C698D707214E6F4AB39AB7096C3DE5A56D8E81@phost015.EVAFUNDS.intermedia.net>
Message-ID: <Pine.A41.4.61b.0410200740350.378018@homer07.u.washington.edu>

On Tue, 19 Oct 2004, Vadim Ogranovich wrote:

> Hi,
>
> Is it possible to search for help pages that meet more than one criteria
> at a time? Say I want to search for all help pages that mention
> "cross-validation" AND "bootstrap". How do I do this?
>

I think you have to intersect them afterwards

a<-help.search("bootstrap")
b<-help.search("cross-validation")
both<-intersect(a$matches[,1],b$matches[,1])
a$matches<-a$matches[both,,drop=FALSE]
a

 	-thomas



From h.wickham at gmail.com  Wed Oct 20 17:04:57 2004
From: h.wickham at gmail.com (hadley wickham)
Date: Wed, 20 Oct 2004 10:04:57 -0500
Subject: [R] R Cocoa won't load in Mac 10.3
In-Reply-To: <r02010400-1035-17BBA23022A211D99DC0000A959B3D22@10.0.1.205>
References: <r02010400-1035-17BBA23022A211D99DC0000A959B3D22@10.0.1.205>
Message-ID: <f8e6ff0504102008047992842d@mail.gmail.com>

> I installed R-cocoa 2.0.0 using the binary installer package on Mac
> OS 10.3. When I try to start the program from the GUI I get the
> following error message:
> 
>     Unable to start R: R_HOME is not set. Please set all required
>     environment variables before running this program.


I had the same problem, which was solved with a reboot.
Hadley



From stefan.ulrych at ubs.com  Wed Oct 20 17:07:43 2004
From: stefan.ulrych at ubs.com (stefan.ulrych@ubs.com)
Date: Wed, 20 Oct 2004 17:07:43 +0200
Subject: [R] Inserting Date Field into Oracle table using ROracle
Message-ID: <BD1F20106799784C8150ED2B8112444602904D0F@NZURC900PEX1.ubsgs.ubsgroup.net>

It is possible to insert date and time information as a 
character string into an Oracle DATE data type 
with ROracle. In an older e-mail it has been stated 
that it is only possible to insert the date as a string
into a DATE field.

The solution is as follows:
Before inserting the character string the default 
value for the NLS_DATE_FORMAT must be changed in Oracle, 
e.g.

...
res<-dbSendQuery(con,"alter session set NLS_DATE_FORMAT = 'dd-mm-yyyy hh24:mi:ss'")
dbClearResult(res)
...

After you have processed these commands you can insert a
character string that matches the above format into an
Oracle DATE data field with the dbPrepareStatement and
dbExecStatement commmands.

You can define also other formats using the Oracle date and 
time format rules.

Regards
Stefan Ulrych

Visit our website at http://www.ubs.com

This message contains confidential information and is intend...{{dropped}}



From sluque at mun.ca  Wed Oct 20 17:13:43 2004
From: sluque at mun.ca (Sebastian Luque)
Date: Wed, 20 Oct 2004 10:13:43 -0500
Subject: [R] par() defaults in Rprofile
Message-ID: <cl5v6p$m79$2@sea.gmane.org>

Dear List,

I've tried to set default par() in .Rprofile by putting the following in
the .First function:

  setHook(packageEvent("graphics", "onLoad"),
          function(...) {graphics::par(cex.axis=1.5, cex.lab=2, las=1)}
          )

My goal was to set par() defaults without opening a device everytime at
startup. However, the next plot I create doesn't show these defaults. Any
suggestions? Thanks in advance.

OS = GNU/Linux Debian
R version = 2.0
-- 
Best wishes,
Sebastian



From gunter.berton at gene.com  Wed Oct 20 17:42:23 2004
From: gunter.berton at gene.com (Berton Gunter)
Date: Wed, 20 Oct 2004 08:42:23 -0700
Subject: [R] Robust regression with groups
In-Reply-To: <007b01c4b6ae$45f586e0$0540210a@www.domain>
Message-ID: <200410201542.i9KFgNdw007037@ohm.gene.com>

Angelo and Folks:

Beware! It is not at all clear what you mean by "robust" regression. The
sandwich estimator is often said to be "robust" to model misspecification in
the sense that it converges to the correct covariance matrix whether or not
the correlation structure in the GEE has been correctly specified (as
Dmitris implied). Is this what you mean? Mixed effect models are often said
to be "robust" in the sense that individual group "estimators" (blups) are
shrunk toward the overall fixed effect estimates. Is this what you mean?

In other applications, "robustness" can mean insensitivity to distributional
assumptions. Mixed effects models for continupus responses commonly assume
normality (as the estimates solve likelihood equations), as do GLMM's for
the random effects. I know of no definitive work that has examined
sensitivity of estimates (or inferences, which are, at best, asymptotic
anyway) to those assumptions. (in the simple independent errors case, it is
usually the case that estimates are not at all sensitive). However, I am a
novice here, so others may be able to illuminate the issue more.

Finally, "robustness" is often used to mean "outlier resistance." Here the
situation is yet murkier. Do you mean resistance to individual "outlying"
observations within a subject or resistance to outlying subjects? Shrinkage
should help with both, but, again, I know of no definitive work, especially
regarding resistance to individual extreme values. Given the sensitivity of
covariance estimates to heavy tails and the consequent inferential
inefficiency, this presumably could be a problem. Finding methods that could
deal with this may be nearly impossible, as you are adding yet another layer
of nonlinear estimation (that of determining optimal case weights/parameters
for mixture contamination models/or whatever...) to the problem; it is easy
to come up with examples where the data are inherently ambiguous and
parameter estimates for resistant case weights and the model would trade off
with each other depending on starting values. That is, too many nonlinear
parameters are being estimated and the model estimates are therefore
unstable.

Again, I am happy to leave more definitive resolution and correction of any
errors in my comments to the experts, but, at the least, I think you need to
think more and communicate more clearly about what you mean by "robust." 

Cheers,

-- Bert Gunter
Genentech Non-Clinical Statistics
South San Francisco, CA
 
"The business of the statistician is to catalyze the scientific learning
process."  - George E. P. Box
 
 

> -----Original Message-----
> From: r-help-bounces at stat.math.ethz.ch 
> [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of 
> Dimitris Rizopoulos
> Sent: Wednesday, October 20, 2004 7:08 AM
> To: Angelo Secchi
> Cc: r-help at stat.math.ethz.ch
> Subject: Re: [R] Robust regression with groups
> 
> Hi Angelo,
> 
> There are two possible options (at least to my knowledge):
> 
> 1. to use a random-effects model, either using `lme' (packages: nlme, 
> lme4) if you have normal data or `glmmPQL' (package: MASS) or `GLMM' 
> (package: lme4) or `glmmML' (package:glmmML) if you cannot use the 
> normal distribution.
> 
> 2. to use a gee model with a robust (sandwich) std.error estimation. 
> See at `gee' (package: gee) and `geese' (package: geepack).
> 
> I hope this helps.
> 
> Best,
> Dimitris
> 
> ----
> Dimitris Rizopoulos
> Ph.D. Student
> Biostatistical Centre
> School of Public Health
> Catholic University of Leuven
> 
> Address: Kapucijnenvoer 35, Leuven, Belgium
> Tel: +32/16/396887
> Fax: +32/16/337015
> Web: http://www.med.kuleuven.ac.be/biostat/
>      http://www.student.kuleuven.ac.be/~m0390867/dimitris.htm
> 
> 
> 
> 
> ----- Original Message ----- 
> From: "Angelo Secchi" <secchi at sssup.it>
> To: <r-help at stat.math.ethz.ch>
> Sent: Wednesday, October 20, 2004 3:22 PM
> Subject: [R] Robust regression with groups
> 
> 
> >
> >
> > Hi,
> > I have data on a group of subjects in different years. I should 
> > assume
> > that observations regarding different individuals are independent 
> > but
> > observations for the same individual in different years are not and 
> > I
> > would like to have an estimated standard error (and 
> > variance-covariance
> > matrix) taking into account this problem.
> >
> > More in general is there a way in R to run a (robust)regression 
> > having
> > different groups in the observations and specifying that the 
> > observation
> > are independent across groups but not necessarily independent within
> > groups?
> >
> > Thanks
> > a.
> >
> > ______________________________________________
> > R-help at stat.math.ethz.ch mailing list
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide! 
> > http://www.R-project.org/posting-guide.html
> >
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
>



From ligges at statistik.uni-dortmund.de  Wed Oct 20 17:49:22 2004
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Wed, 20 Oct 2004 17:49:22 +0200
Subject: [R] par() defaults in Rprofile
In-Reply-To: <cl5v6p$m79$2@sea.gmane.org>
References: <cl5v6p$m79$2@sea.gmane.org>
Message-ID: <41768902.2060301@statistik.uni-dortmund.de>

Sebastian Luque wrote:

> Dear List,
> 
> I've tried to set default par() in .Rprofile by putting the following in
> the .First function:
> 
>   setHook(packageEvent("graphics", "onLoad"),
>           function(...) {graphics::par(cex.axis=1.5, cex.lab=2, las=1)}
>           )
> 
> My goal was to set par() defaults without opening a device everytime at
> startup. However, the next plot I create doesn't show these defaults. Any
> suggestions? Thanks in advance.
> 
> OS = GNU/Linux Debian
> R version = 2.0


The par settings are for the current device. You have to apply them 
after a device has been opened.

What you want is a device function like

mydevice() that opens a device and sets the par() stuff, I guess.

Uwe Ligges



From ripley at stats.ox.ac.uk  Wed Oct 20 17:52:55 2004
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Wed, 20 Oct 2004 16:52:55 +0100 (BST)
Subject: [R] par() defaults in Rprofile
In-Reply-To: <cl5v6p$m79$2@sea.gmane.org>
Message-ID: <Pine.LNX.4.44.0410201649270.11095-100000@gannet.stats>

On Wed, 20 Oct 2004, Sebastian Luque wrote:

> Dear List,
> 
> I've tried to set default par() in .Rprofile by putting the following in
> the .First function:
> 
>   setHook(packageEvent("graphics", "onLoad"),
>           function(...) {graphics::par(cex.axis=1.5, cex.lab=2, las=1)}
>           )
> 
> My goal was to set par() defaults without opening a device everytime at
> startup. However, the next plot I create doesn't show these defaults. Any
> suggestions? Thanks in advance.

par() only applies to the current device.  You will have to specify your 
own customized device, something like

X11 <- function(...)
{
    grDevices::X11(...)
    par(cex.axis=1.5, cex.lab=2, las=1)
}

perhaps?  (Namespace issues might defeat you here.)

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From spencer.graves at pdf.com  Wed Oct 20 18:07:54 2004
From: spencer.graves at pdf.com (Spencer Graves)
Date: Wed, 20 Oct 2004 09:07:54 -0700
Subject: [R] Selecting from a character vector with logical operators
In-Reply-To: <41764037.3060208@ucl.ac.uk>
References: <3589BC4D64C84341AE0C258244F977A2B60B55@expressa.corp.cefas.co.uk>	<417634BC.4060408@optonline.net>
	<41764037.3060208@ucl.ac.uk>
Message-ID: <41768D5A.4040409@pdf.com>

      One can, of course, convert the factor to character as: 

> letfac <- factor(letters)
> letfac[as.character(letfac)>"j"]
 [1] k l m n o p q r s t u v w x y z
Levels: a b c d e f g h i j k l m n o p q r s t u v w x y z

	  Alternatively, one can select from the levels:  

> letfac[levels(letfac)[letfac]>"j"]
 [1] k l m n o p q r s t u v w x y z
Levels: a b c d e f g h i j k l m n o p q r s t u v w x y z
> 
	  hope this helps.  spencer graves 

Gavin Simpson wrote:

> Chuck Cleland wrote:
>
>>   With a character vector it works fine, but your ca11c93SOL1$RECTAN 
>> seems to be a factor.
>>
>>  > letters[letters > "j"]
>>  [1] "k" "l" "m" "n" "o" "p" "q" "r" "s" "t" "u" "v" "w" "x" "y" "z"
>>
>>  > letfact <- as.factor(letters)
>>  > letfact[letfact > "j"]
>>  [1] <NA> <NA> <NA> <NA> <NA> <NA> <NA> <NA> <NA> <NA> <NA> <NA> <NA> 
>> <NA> <NA>
>> [16] <NA> <NA> <NA> <NA> <NA> <NA> <NA> <NA> <NA> <NA> <NA>
>> Levels: a b c d e f g h i j k l m n o p q r s t u v w x y z
>> Warning message:
>> ">" not meaningful for factors in: Ops.factor(letfact, "j")
>
>
> You need an ordered factor for this to work:
>
> > letfact <- as.ordered(letters)
> > letfact[letfact > "j"]
>  [1] k l m n o p q r s t u v w x y z
> 26 Levels: a < b < c < d < e < f < g < h < i < j < k < l < m < n < o < 
> ... < z
>
> Gav
>

-- 
Spencer Graves, PhD, Senior Development Engineer
O:  (408)938-4420;  mobile:  (408)655-4567



From vito_ricci at yahoo.com  Wed Oct 20 18:17:22 2004
From: vito_ricci at yahoo.com (Vito Ricci)
Date: Wed, 20 Oct 2004 18:17:22 +0200 (CEST)
Subject: [R] R & Graphs
Message-ID: <20041020161722.85496.qmail@web41208.mail.yahoo.com>

Dear R-users,

I'm finding for a R-package concerning graphs. Is
there some kind of that package? I've a set of
correlation coeffients between several variable and I
wish to built a graph to link variables correlated.

Many thanks.

Best,

Vito

=====
Diventare costruttori di soluzioni

"The business of the statistician is to catalyze 
the scientific learning process."  
George E. P. Box


Visitate il portale http://www.modugno.it/
e in particolare la sezione su Palese http://www.modugno.it/archivio/cat_palese.shtml



From jin at entelos.com  Wed Oct 20 18:25:40 2004
From: jin at entelos.com (Yisheng Jin)
Date: Wed, 20 Oct 2004 09:25:40 -0700
Subject: [R] Error when intstalling R on intel box running linux
Message-ID: <B56908D27FB07B48A05FCC6FC91A0467034AD4E0@mithril.entelos.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20041020/4b959906/attachment.pl

From rgentlem at jimmy.harvard.edu  Wed Oct 20 18:30:29 2004
From: rgentlem at jimmy.harvard.edu (Robert Gentleman)
Date: Wed, 20 Oct 2004 12:30:29 -0400
Subject: [R] R & Graphs
In-Reply-To: <20041020161722.85496.qmail@web41208.mail.yahoo.com>;
	from vito_ricci@yahoo.com on Wed, Oct 20, 2004 at 06:17:22PM +0200
References: <20041020161722.85496.qmail@web41208.mail.yahoo.com>
Message-ID: <20041020123029.I20510@jimmy.harvard.edu>

>From Bioconductor:
  graph, Rgraphviz, and RBGL..

On Wed, Oct 20, 2004 at 06:17:22PM +0200, Vito Ricci wrote:
> Dear R-users,
> 
> I'm finding for a R-package concerning graphs. Is
> there some kind of that package? I've a set of
> correlation coeffients between several variable and I
> wish to built a graph to link variables correlated.
> 
> Many thanks.
> 
> Best,
> 
> Vito
> 
> =====
> Diventare costruttori di soluzioni
> 
> "The business of the statistician is to catalyze 
> the scientific learning process."  
> George E. P. Box
> 
> 
> Visitate il portale http://www.modugno.it/
> e in particolare la sezione su Palese http://www.modugno.it/archivio/cat_palese.shtml
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html

-- 
+---------------------------------------------------------------------------+
| Robert Gentleman                 phone : (617) 632-5250                   |
| Associate Professor              fax:   (617)  632-2444                   |
| Department of Biostatistics      office: M1B20                            |
| Harvard School of Public Health  email: rgentlem at jimmy.harvard.edu        |
+---------------------------------------------------------------------------+



From aorchid at mac.com  Wed Oct 20 18:31:31 2004
From: aorchid at mac.com (Aric Gregson)
Date: Wed, 20 Oct 2004 12:31:31 -0400
Subject: [R] R Cocoa won't load in Mac 10.3
In-Reply-To: <f8e6ff0504102008047992842d@mail.gmail.com>
Message-ID: <r02010400-1035-86A6D83C22B511D9AA8B000A959B3D22@[134.192.145.239]>

On 10/20/04 10:04 hadley wickham sent the following:

>> I installed R-cocoa 2.0.0 using the binary installer package on Mac
>> OS 10.3. When I try to start the program from the GUI I get the
>> following error message:
>> 
>>     Unable to start R: R_HOME is not set. Please set all required
>>     environment variables before running this program.
>
>
>I had the same problem, which was solved with a reboot.

Thanks very much. A reboot solved this problem for me as well. 

This should be mentioned in the installation instructions, since I don't
usually reboot after installing software. Also noted permissions errors
after installation, though I cannot guarantee that they are from the R
installation package. 

aric



From eric.pellegrini at chemie.uni-erlangen.de  Wed Oct 20 18:54:29 2004
From: eric.pellegrini at chemie.uni-erlangen.de (Eric Pellegrini)
Date: Wed, 20 Oct 2004 18:54:29 +0200
Subject: [R] apply function
Message-ID: <200410201854.29721.eric.pellegrini@chemie.uni-erlangen.de>

Hi all,

I have a question about apply function. Is that possible to pass some 
non-default arguments in the function we want to apply ?

For example:

if "mat" is a matrix and I want to use the "tabulate" function on its row.

The command apply(mat,1,tabulate) works but I have problem with this one 
apply(mat, 1, tabulate(nbins=4)).

Any clue ?

Thanks,

	Eric



-- 
Eric Pellegrini, PhD
Computer-Chemie-Centrum
University of Erlangen-N??rnberg
N??gelbachstra??e, 25
D-91052 Erlangen
Germany



From spencer.graves at pdf.com  Wed Oct 20 18:56:50 2004
From: spencer.graves at pdf.com (Spencer Graves)
Date: Wed, 20 Oct 2004 09:56:50 -0700
Subject: [R] Plotting a 3D surface
In-Reply-To: <20041020125413.EIYC26826.tomts10-srv.bellnexxia.net@JohnDesktop8300>
References: <20041020125413.EIYC26826.tomts10-srv.bellnexxia.net@JohnDesktop8300>
Message-ID: <417698D2.7060306@pdf.com>

      Venables and Ripley (2002) Modern Applied Statistics with S, 4th 
ed. (Springer, ch.4, pp. 76 & 94) provide an example doing just that.  
Under Windows, open "C:\Program 
Files\R\rw2000pat\library\MASS\scripts\ch04.R", and run that file. 

      hope this helps.  spencer graves    

John Fox wrote:

>Dear Lawrence,
>
>You might take a look at the scatter3d function in the Rcmdr package, which
>uses the rgl package. scatter3d() includes some smoothing, but not by
>ksmooth() or locpoly(), so you might have to alter it for your purposes, but
>that shouldn't be hard.
>
>I hope that this helps,
> John
>
>--------------------------------
>John Fox
>Department of Sociology
>McMaster University
>Hamilton, Ontario
>Canada L8S 4M4
>905-525-9140x23604
>http://socserv.mcmaster.ca/jfox 
>-------------------------------- 
>
>  
>
>>-----Original Message-----
>>From: r-help-bounces at stat.math.ethz.ch 
>>[mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of 
>>lawrence Perepolkin
>>Sent: Wednesday, October 20, 2004 7:18 AM
>>To: R-help at stat.math.ethz.ch
>>Subject: [R] Plotting a 3D surface 
>>
>>Hi 
>>
>>Does R have a function or has someone written a function to 
>>draw a 3d surface from a scatter plot of values using either 
>>ksmooth or locpoly. OR a transform function a that merges x 
>>relation z and y relation z to (x,y) relation z?  
>>
>>I tried out scatterplot3d but it seems it would take a bit of 
>>work to get scatterplot3d to draw a curved surface.
>>
>>
>>Lawrence
>>
>>______________________________________________
>>R-help at stat.math.ethz.ch mailing list
>>https://stat.ethz.ch/mailman/listinfo/r-help
>>PLEASE do read the posting guide! 
>>http://www.R-project.org/posting-guide.html
>>    
>>
>
>______________________________________________
>R-help at stat.math.ethz.ch mailing list
>https://stat.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>  
>

-- 
Spencer Graves, PhD, Senior Development Engineer
O:  (408)938-4420;  mobile:  (408)655-4567



From Stefano.Guazzetti at ausl.re.it  Wed Oct 20 19:01:43 2004
From: Stefano.Guazzetti at ausl.re.it (Guazzetti Stefano)
Date: Wed, 20 Oct 2004 19:01:43 +0200
Subject: R: [R] apply function
Message-ID: <B8A1EED732379B44A7E59D22E82E4442331C98@IMHOTEP.ausl.org>

you seem to need

apply(mat, 1, tabulate, nbins=4)

isnt it?

Stefano

-----Messaggio originale-----
Da: r-help-bounces at stat.math.ethz.ch
[mailto:r-help-bounces at stat.math.ethz.ch]Per conto di Eric Pellegrini
Inviato: mercoled?? 20 ottobre 2004 18.54
A: R-help at stat.math.ethz.ch
Oggetto: [R] apply function


Hi all,

I have a question about apply function. Is that possible to pass some 
non-default arguments in the function we want to apply ?

For example:

if "mat" is a matrix and I want to use the "tabulate" function on its row.

The command apply(mat,1,tabulate) works but I have problem with this one 
apply(mat, 1, tabulate(nbins=4)).

Any clue ?

Thanks,

	Eric



-- 
Eric Pellegrini, PhD
Computer-Chemie-Centrum
University of Erlangen-N??rnberg
N??gelbachstra??e, 25
D-91052 Erlangen
Germany

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html



From andy_liaw at merck.com  Wed Oct 20 19:01:40 2004
From: andy_liaw at merck.com (Liaw, Andy)
Date: Wed, 20 Oct 2004 13:01:40 -0400
Subject: [R] apply function
Message-ID: <3A822319EB35174CA3714066D590DCD504AF858C@usrymx25.merck.com>

Try apply(mat, 1, tabulate, nbins=4).

HTH,
Andy

> From: Eric Pellegrini
> 
> Hi all,
> 
> I have a question about apply function. Is that possible to pass some 
> non-default arguments in the function we want to apply ?
> 
> For example:
> 
> if "mat" is a matrix and I want to use the "tabulate" 
> function on its row.
> 
> The command apply(mat,1,tabulate) works but I have problem 
> with this one 
> apply(mat, 1, tabulate(nbins=4)).
> 
> Any clue ?
> 
> Thanks,
> 
> 	Eric
> 
> 
> 
> -- 
> Eric Pellegrini, PhD
> Computer-Chemie-Centrum
> University of Erlangen-N??rnberg
> N??gelbachstra??e, 25
> D-91052 Erlangen
> Germany
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
> 
>



From abunn at whrc.org  Wed Oct 20 19:11:18 2004
From: abunn at whrc.org (Andy Bunn)
Date: Wed, 20 Oct 2004 13:11:18 -0400
Subject: [R] R & Graphs
In-Reply-To: <20041020161722.85496.qmail@web41208.mail.yahoo.com>
Message-ID: <NEBBIPHDAMMOKDKPOFFIEENNCKAA.abunn@whrc.org>

In addition, look at "Laying Out Pathways With Rgraphviz" in R News which
describes the Rgraphviz packages on Bioconductor.

http://cran.r-project.org/doc/Rnews/Rnews_2004-2.pdf

HTH,
Andy



From p.dalgaard at biostat.ku.dk  Wed Oct 20 19:11:18 2004
From: p.dalgaard at biostat.ku.dk (Peter Dalgaard)
Date: 20 Oct 2004 19:11:18 +0200
Subject: [R] apply function
In-Reply-To: <200410201854.29721.eric.pellegrini@chemie.uni-erlangen.de>
References: <200410201854.29721.eric.pellegrini@chemie.uni-erlangen.de>
Message-ID: <x2d5zdco21.fsf@biostat.ku.dk>

Eric Pellegrini <eric.pellegrini at chemie.uni-erlangen.de> writes:

> Hi all,
> 
> I have a question about apply function. Is that possible to pass some 
> non-default arguments in the function we want to apply ?
> 
> For example:
> 
> if "mat" is a matrix and I want to use the "tabulate" function on its row.
> 
> The command apply(mat,1,tabulate) works but I have problem with this one 
> apply(mat, 1, tabulate(nbins=4)).
> 
> Any clue ?

You might have gotten one by R'ing TFHP.... 

 apply(mat, 1, tabulate, nbins=4)

There's even an example!

-- 
   O__  ---- Peter Dalgaard             Blegdamsvej 3  
  c/ /'_ --- Dept. of Biostatistics     2200 Cph. N   
 (*) \(*) -- University of Copenhagen   Denmark      Ph: (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)             FAX: (+45) 35327907



From spencer.graves at pdf.com  Wed Oct 20 19:32:35 2004
From: spencer.graves at pdf.com (Spencer Graves)
Date: Wed, 20 Oct 2004 10:32:35 -0700
Subject: [R] matrix of eigenvalues
In-Reply-To: <a0600200bbd9c1ce167c6@[130.120.104.141]>
References: <a06002006bd9a7b5ffb79@[130.120.104.141]>	<4175083B.6090605@stat.wisc.edu>
	<a0600200bbd9c1ce167c6@[130.120.104.141]>
Message-ID: <4176A133.2060103@pdf.com>

      I think you need the Schur decomposition, which seems currently 
not to be available in R.  The documentation for the the Matrix package 
describes a "Schur" function, but it's not available in the current 
Matrix package, as mentioned in my post on this issue yesterday (subj:  
Schur decomposition). 

      Moreover, Lindsey's mexp in rmutils won't work either: 

 > A = matrix(cbind(c(-1,1),c(-4,3)),nrow=2)
 > mexp(A)
Error in solve.default(z$vectors) : system is computationally singular: 
reciprocal condition number = 4.13756e-017
 > mexp(A, "series")
Error in t * x : non-numeric argument to binary operator
 >
      Have you considered adding a little noise: 
 > mexp(A+1e-6*rnorm(4))
          [,1]       [,2]
[1,] -2.718284 -10.873139
[2,]  2.718284   8.154859
 > mexp(A+1e-6*rnorm(4))
                        [,1]                     [,2]
[1,] -2.718284-1.060041e-12i -10.873126-1.575184e-12i
[2,]  2.718284+7.492895e-13i   8.154846+1.578515e-12i
 > mexp(A+1e-6*rnorm(4))
                        [,1]                     [,2]
[1,] -2.718284+6.146195e-13i -10.873127+1.586731e-12i
[2,]  2.718284-4.004574e-13i   8.154847-8.515411e-13i
 > mexp(A+1e-6*rnorm(4))
                        [,1]                     [,2]
[1,] -2.718283+3.077538e-13i -10.873130+2.433609e-13i
[2,]  2.718283-3.197442e-14i   8.154847-2.782219e-13i
 >
      hope this helps.  spencer graves
(I believe this is described in one of Richard Bellman's matrix analysis 
book.) 

Christian Jost wrote:

> Thanks for all the replys. I understand now that eigen does not return 
> a matrix with linearly independent eigenvectors (which we would need 
> to get its inverse).
>
> The question whether there is a function in R that does so remains. As 
> an example, consider the problem
> dx1/dt = -x1 - 4 x2
> dx2/dt =  x1 + 3 x2
> corresponding to the matrix system
> dx/dt = A x, x(0) = x0 = c(1,2) (for example)
>
> Analysing this system in R gives
> A = matrix(cbind(c(-1,1),c(-4,3)),nrow=2)
> for which eigen() returns a singular eigenvector matrix
> det(eigen(A)$vectors)
>
> Now, if I tried to solve the system by hand, I would find one 
> eigenvalue of value 1 and an associated eigenvector v1=[2, -1]. I can 
> complement this eigenvector to get a base, e.g. v2=[0,1] (which is 
> linearly independent of v1). Defining now a passage matrix as
> P = matrix(cbind(c(2,-1),c(0,1)),nrow=2)
> I can compute the inverse
> invP = solve(P)
> and
> invP %*% A %*% P -> D
> will return an upper triangular matrix (with eigenvalues on the 
> diagonal) for which the exponential can easily be computed by hand
>
> expDt = diag(exp(t),2,2) %*% matrix(c(1,0,-2*t,1),nrow=2)
> and the solution of the original system becomes
>
> x(t) = P %*% expDt %*% invP %*% x0
> and I can simulate a trajectory
>
> times = seq(0,2,by=0.1);
> x0 = c(1,2);
> sol = matrix(0,nrow=2,ncol=length(times));
> for (i in 1:length(times)) {
>   t = times[i];
>   expDt = diag(exp(t),2,2) %*% matrix(c(1,0,-2*t,1),nrow=2);
>   sol[,i] = P %*% expDt %*% invP %*% x0
> }
> plot(times,sol[1,],type='l',ylim=c(min(sol),max(sol)))
> lines(times,sol[2,],lty='dashed')
>
> Thanks to all who followed down here. Now, is there a function in R 
> that will construct my P automatically? Or should I apply directly the 
> function mexp(A) (from package mrutil) and completely bypass the 
> passage matrix stuff (which my math teachers like so much ;-)
>
> Best, Christian.
>
>> Christian Jost wrote:
>>
>>> I thought that the function
>>> eigen(A)
>>> will return a matrix with eigenvectors that are independent of each 
>>> other (thus forming a base and the matrix being invertible). This 
>>> seems not to be the case in the following example
>>> A=matrix(c(1,2,0,1),nrow=2,byrow=T)
>>> eigen(A) ->ev
>>> solve(ev$vectors)
>>>
>>> note that I try to get the upper triangular form with eigenvalues on 
>>> the diagonal and (possibly) 1 just atop the eigenvalues to be used 
>>> to solve a linear differential equation
>>> x' = Ax, x(0)=x0
>>> x(t) = P exp(D t) P^-1 x0
>>> where D is this upper triangular form and P is the "passage matrix" 
>>> (not sure about the correct english name) given by a base of 
>>> eigenvectors. So the test would be
>>> solve(ev$vectors) %*% A %*% ev$vectors - D
>>> should be 0
>>>
>>> Thanks for any help, Christian.
>>>
>>> ps: please copy reply also to my address, my subscription to the 
>>> R-help list seems to have delays
>>
>>
>> That particular matrix has repeated eigenvalues and a degenerate 
>> eigenspace.
>>
>>>  A <- matrix(c(1,0,2,1),nc=2)
>>>  A
>>
>>      [,1] [,2]
>> [1,]    1    2
>> [2,]    0    1
>>
>>>  eigen(A)
>>
>> $values
>> [1] 1 1
>>
>> $vectors
>>      [,1]          [,2]
>> [1,]    1 -1.000000e+00
>> [2,]    0  1.110223e-16
>
>
>

-- 
Spencer Graves, PhD, Senior Development Engineer
O:  (408)938-4420;  mobile:  (408)655-4567



From aolinto_r at bignet.com.br  Wed Oct 20 20:23:04 2004
From: aolinto_r at bignet.com.br (Antonio Olinto)
Date: Wed, 20 Oct 2004 15:23:04 -0300
Subject: [R] probability scale at y axis
Message-ID: <1098296584.4176ad08c94c8@webmail2.bignet.com.br>

Hi,

I'd like to make a plot with a probability scale at y axis, just like in the old
probability paper.

I couldn't find how make it in R. Thanks for any hint.

Best regards,

Antonio Olinto



-------------------------------------------------
WebMail Bignet - O seu provedor do litoral
www.bignet.com.br



From andy_liaw at merck.com  Wed Oct 20 20:31:27 2004
From: andy_liaw at merck.com (Liaw, Andy)
Date: Wed, 20 Oct 2004 14:31:27 -0400
Subject: [R] probability scale at y axis
Message-ID: <3A822319EB35174CA3714066D590DCD504AF8596@usrymx25.merck.com>

Do you mean something like:

y <- sort(rnorm(100))
plot(y, yaxt="n")
p <- c(0.01, 0.05, 0.5, 0.95, 0.99)
axis(2, at=qnorm(p), label=p, las=1)

??

Andy

> From: Antonio Olinto
> 
> Hi,
> 
> I'd like to make a plot with a probability scale at y axis, 
> just like in the old
> probability paper.
> 
> I couldn't find how make it in R. Thanks for any hint.
> 
> Best regards,
> 
> Antonio Olinto
> 
> 
> 
> -------------------------------------------------
> WebMail Bignet - O seu provedor do litoral
> www.bignet.com.br
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
> 
>



From deepayan at stat.wisc.edu  Wed Oct 20 22:12:00 2004
From: deepayan at stat.wisc.edu (Deepayan Sarkar)
Date: Wed, 20 Oct 2004 15:12:00 -0500
Subject: [R] densityplot and histogram
In-Reply-To: <Pine.SGI.4.40.0410191806170.47110897-100000@origin.chass.utoronto.ca>
References: <Pine.SGI.4.40.0410191806170.47110897-100000@origin.chass.utoronto.ca>
Message-ID: <200410201512.00533.deepayan@stat.wisc.edu>

On Tuesday 19 October 2004 17:12, Jean Eid wrote:
> Is there any function like par(new=T) for lattice. I want to plot a
> histogram in percentages on the right hand side and also superimpose the
> densityplot with its density scale on the lhs. so far I am only able to do
> this
>   histogram( temp[,2]~ temp[,1],nint=100,type="desnity",
>                xlab = "Population Size",
>                panel = function(x, ...) {
>                    panel.histogram(x, ...)
>                    panel.densityplot(x, col = "red", plot.points=F, lwd=2)
>                } )
>
>  If I change type="density" to type="percent" the scales for the
> densityplot will be too low and all I see is a horizontal line at zero
> (this is as expected) . However, I tried par(new=T) and nothing happens. I
> want to be able to put percenstages on axis 2 and density values at axis
> 4.

I don't think par(new=T) is what you should be looking for (and incidentally, 
par settings have no effect on lattice plots). It's possible to add axes 
after the plot (easier than it was before 2.0.0), but the design of lattice 
doesn't allow you to easily allocate enough space for the second set of axes. 
You can still do it, but it would be kludgy.

Here's an example (you need to know what 'mult' should be -- it's the factor 
that converts the density scale to the percent scale -- it would depend on 
the widths of the bins and the length of x):


x <- rnorm(200)
mult <- 60 ## meaningless in this case

histogram(x, type = "percent",
          panel = function(x, ...) {
              panel.histogram(x, ...)
              d <- density(x)
              panel.lines(d$x, mult * d$y, col = 'red')
          },
          scales = list(y = list(tck = c(1, 0))),
          par.settings = list(layout.widths = list(right.padding = 5)))

trellis.focus("panel", 1, 1, clip.off = TRUE)

at <- pretty(c(0, 25)/mult)
panel.axis(side = "right",
           at = mult * at, labels = at,
           outside = TRUE)
trellis.unfocus()



From Ted.Harding at nessie.mcc.ac.uk  Wed Oct 20 21:40:34 2004
From: Ted.Harding at nessie.mcc.ac.uk ( (Ted Harding))
Date: Wed, 20 Oct 2004 20:40:34 +0100 (BST)
Subject: [R] probability scale at y axis
In-Reply-To: <3A822319EB35174CA3714066D590DCD504AF8596@usrymx25.merck.com>
Message-ID: <XFMail.041020204034.Ted.Harding@nessie.mcc.ac.uk>

Well, I think he probably meant something more like

 x <- sort(rnorm(100))
 y<-0.5+(0:99)
 p <- c(0.01, 0.05, 0.5, 0.95, 0.99)
 plot(x,qnorm(y/100), yaxt="n")
 axis(2, at=qnorm(p), label=p, las=1)

(From your plot, Andy, I would have inferred that the data
were not normal! However, I like the elegance of your uses
of "plot" and "axis".)

Best wishes,
Ted.

On 20-Oct-04 Liaw, Andy wrote:
> Do you mean something like:
> 
> y <- sort(rnorm(100))
> plot(y, yaxt="n")
> p <- c(0.01, 0.05, 0.5, 0.95, 0.99)
> axis(2, at=qnorm(p), label=p, las=1)
> 
> ??
> 
> Andy
> 
>> From: Antonio Olinto
>> 
>> Hi,
>> 
>> I'd like to make a plot with a probability scale at y axis, 
>> just like in the old
>> probability paper.
>> 
>> I couldn't find how make it in R. Thanks for any hint.
>> 
>> Best regards,
>> 
>> Antonio Olinto
>> 
>> 
>> 
>> -------------------------------------------------
>> WebMail Bignet - O seu provedor do litoral
>> www.bignet.com.br
>> 
>> ______________________________________________
>> R-help at stat.math.ethz.ch mailing list
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide! 
>> http://www.R-project.org/posting-guide.html
>> 
>>
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide!
> http://www.R-project.org/posting-guide.html

--------------------------------------------------------------------
E-Mail: (Ted Harding) <Ted.Harding at nessie.mcc.ac.uk>
Fax-to-email: +44 (0)870 094 0861  [NB: New number!]
Date: 20-Oct-04                                       Time: 20:40:34
------------------------------ XFMail ------------------------------



From Ted.Harding at nessie.mcc.ac.uk  Wed Oct 20 22:55:52 2004
From: Ted.Harding at nessie.mcc.ac.uk ( (Ted Harding))
Date: Wed, 20 Oct 2004 21:55:52 +0100 (BST)
Subject: [R] probability scale at y axis
In-Reply-To: <XFMail.041020204034.Ted.Harding@nessie.mcc.ac.uk>
Message-ID: <XFMail.041020215552.Ted.Harding@nessie.mcc.ac.uk>

Now that I think of it:

 qqplot(x,qnorm(0.005+0.01*(0:99)),yaxt="n")
 axis(2, at=qnorm(p), label=p, las=1)

Ted.

On 20-Oct-04 Ted Harding wrote:
> Well, I think he probably meant something more like
> 
>  x <- sort(rnorm(100))
>  y<-0.5+(0:99)
>  p <- c(0.01, 0.05, 0.5, 0.95, 0.99)
>  plot(x,qnorm(y/100), yaxt="n")
>  axis(2, at=qnorm(p), label=p, las=1)
> 
> (From your plot, Andy, I would have inferred that the data
> were not normal! However, I like the elegance of your uses
> of "plot" and "axis".)
> 
> Best wishes,
> Ted.
> 
> On 20-Oct-04 Liaw, Andy wrote:
>> Do you mean something like:
>> 
>> y <- sort(rnorm(100))
>> plot(y, yaxt="n")
>> p <- c(0.01, 0.05, 0.5, 0.95, 0.99)
>> axis(2, at=qnorm(p), label=p, las=1)
>> 
>> ??
>> 
>> Andy
>> 
>>> From: Antonio Olinto
>>> 
>>> Hi,
>>> 
>>> I'd like to make a plot with a probability scale at y axis, 
>>> just like in the old
>>> probability paper.
>>> 
>>> I couldn't find how make it in R. Thanks for any hint.
>>> 
>>> Best regards,
>>> 
>>> Antonio Olinto
>>> 
>>> 
>>> 
>>> -------------------------------------------------
>>> WebMail Bignet - O seu provedor do litoral
>>> www.bignet.com.br
>>> 
>>> ______________________________________________
>>> R-help at stat.math.ethz.ch mailing list
>>> https://stat.ethz.ch/mailman/listinfo/r-help
>>> PLEASE do read the posting guide! 
>>> http://www.R-project.org/posting-guide.html
>>> 
>>>
>> 
>> ______________________________________________
>> R-help at stat.math.ethz.ch mailing list
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide!
>> http://www.R-project.org/posting-guide.html
> 
> --------------------------------------------------------------------
> E-Mail: (Ted Harding) <Ted.Harding at nessie.mcc.ac.uk>
> Fax-to-email: +44 (0)870 094 0861  [NB: New number!]
> Date: 20-Oct-04                                       Time: 20:40:34
> ------------------------------ XFMail ------------------------------

--------------------------------------------------------------------
E-Mail: (Ted Harding) <Ted.Harding at nessie.mcc.ac.uk>
Fax-to-email: +44 (0)870 094 0861  [NB: New number!]
Date: 20-Oct-04                                       Time: 21:55:52
------------------------------ XFMail ------------------------------



From cjlu at mail.ncku.edu.tw  Thu Oct 21 04:10:10 2004
From: cjlu at mail.ncku.edu.tw (cjlu)
Date: Thu, 21 Oct 2004 10:10:10 +0800
Subject: [R] How to calculate a double integral ...?
Message-ID: <20041021015643.M79703@mail.ncku.edu.tw>

Dear R-Friends, 

How can I calculate a double integral like

  \int_a^b \int_c^y g(x, y) dx dy

where a, b, c are constants, g(x, y), e.g.,
g(x, y) = tan(x + y).

I tried to nested integrate() and adapt(),
but none of them working, seemingly due to the 
limits can not be specified constants.


Best regards,

C. Joseph Lu
Department of Statistics
National Cheng-Kung University
Voice: (886)-6-275-7575 Ext. 53638
  Fax: (886)-6-234-2469



From ggrothendieck at myway.com  Thu Oct 21 07:45:42 2004
From: ggrothendieck at myway.com (Gabor Grothendieck)
Date: Thu, 21 Oct 2004 05:45:42 +0000 (UTC)
Subject: [R] Q about strsplit and regexp
References: <3A822319EB35174CA3714066D590DCD504AF8583@usrymx25.merck.com>
Message-ID: <loom.20041021T074136-827@post.gmane.org>

Liaw, Andy <andy_liaw <at> merck.com> writes:

: > I do not want that empty character in the beginning, but 
: > couldn't figure out
: > how to strip the starting white spaces, other than something 
: > ugly like:
: > 
: > > strsplit(sub("^ +", "", "  a b    c "), " +")
: > [[1]]
: > [1] "a" "b" "c"

: 
: Looks like there's no easier way than to strip the spaces before 
: splitting the fields.  

Here is a way to split it up without first stripping away leading spaces:

  scan(textConnection(" a b    c "), what = "")



From ggrothendieck at myway.com  Thu Oct 21 07:55:43 2004
From: ggrothendieck at myway.com (Gabor Grothendieck)
Date: Thu, 21 Oct 2004 05:55:43 +0000 (UTC)
Subject: [R] Q about strsplit and regexp
References: <3A822319EB35174CA3714066D590DCD504AF857D@usrymx25.merck.com>
	<20041020130728.ZULM1536.tomts25-srv.bellnexxia.net@JohnDesktop8300>
Message-ID: <loom.20041021T075200-100@post.gmane.org>

John Fox <jfox <at> mcmaster.ca> writes:

: This is something that I sometimes want to do, so I have a little utility
: that trims blanks and tabs from the beginnings and ends of strings:
: 
: trim.ws <- function(text) gsub("^[\ \t]", "", gsub("[\ \t]*$", "", text))
: 

This can be reduced to a single gsub like this:

  gsub("^[[:space:]]+|[[:space:]]+$", "", text)



From ggrothendieck at myway.com  Thu Oct 21 08:11:18 2004
From: ggrothendieck at myway.com (Gabor Grothendieck)
Date: Thu, 21 Oct 2004 06:11:18 +0000 (UTC)
Subject: [R] How to calculate a double integral ...?
References: <20041021015643.M79703@mail.ncku.edu.tw>
Message-ID: <loom.20041021T080712-416@post.gmane.org>

cjlu <cjlu <at> mail.ncku.edu.tw> writes:

: 
: Dear R-Friends, 
: 
: How can I calculate a double integral like
: 
:   \int_a^b \int_c^y g(x, y) dx dy
: 
: where a, b, c are constants, g(x, y), e.g.,
: g(x, y) = tan(x + y).
: 
: I tried to nested integrate() and adapt(),
: but none of them working, seemingly due to the 
: limits can not be specified constants.
: 

Integrate

   g2 <- function(x, y) g(x, y) * (x > y)

over (a,b) x (a,c).



From coulon at toulouse.inra.fr  Thu Oct 21 08:37:52 2004
From: coulon at toulouse.inra.fr (=?iso-8859-1?Q?Aur=E9lie_Coulon?=)
Date: Thu, 21 Oct 2004 08:37:52 +0200
Subject: [R] loess significance
Message-ID: <007201c4b738$7d8e8400$d3626393@IRGMCOUL>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20041021/700f469d/attachment.pl

From john.maindonald at anu.edu.au  Thu Oct 21 08:49:31 2004
From: john.maindonald at anu.edu.au (John Maindonald)
Date: Thu, 21 Oct 2004 16:49:31 +1000
Subject: [R] "Using R for Data Analysis and Graphics ..."
Message-ID: <5CA59B22-232D-11D9-9B9D-000A95CDA0F2@anu.edu.au>

A revised version of this document, now designed to reflect
version 2.0.0 of R, is available from CRAN sites, under
Documentation | Contributed.  Data sets are available
from my web page directory:
   http://wwwmaths.anu.edu.au/~johnm/r/dsets
Preferably, retrieve the image file usingR.Rdata

The output has, mostly, not been revised.  Thus it will in
some cases reflect an earlier version of R.  This is a task
for some later time.

John Maindonald             email: john.maindonald at anu.edu.au
phone : +61 2 (6125)3473    fax  : +61 2(6125)5549
Centre for Bioinformation Science, Room 1194,
John Dedman Mathematical Sciences Building (Building 27)
Australian National University, Canberra ACT 0200.



From vito.muggeo at giustizia.it  Thu Oct 21 09:57:52 2004
From: vito.muggeo at giustizia.it (Vito Muggeo)
Date: Thu, 21 Oct 2004 09:57:52 +0200
Subject: R: [R] loess significance
References: <007201c4b738$7d8e8400$d3626393@IRGMCOUL>
Message-ID: <00bd01c4b743$adf7b340$5c13070a@PROCGEN>

Dear Aur??lie,
I think that for *fixed* (i.e. assumed known) amount of smoothing, you can
use a simple LRT by comparing the two candidate models.

BTW, have a look to the mgcv or gam packages for a general model-based
approach.


----- Original Message -----
From: Aur??lie Coulon <coulon at toulouse.inra.fr>
To: <r-help at stat.math.ethz.ch>
Sent: Thursday, October 21, 2004 8:37 AM
Subject: [R] loess significance


Dear list,

I would like to know it is possible to test the significance of a loess ; in
other words, I would like to know if the loess I got is significantly
different from a linear model.

Thanks.


Aur??lie Coulon.



[[alternative HTML version deleted]]

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide!
http://www.R-project.org/posting-guide.html



From eric.esposito at gazdefrance.com  Thu Oct 21 11:07:41 2004
From: eric.esposito at gazdefrance.com (Eric ESPOSITO)
Date: Thu, 21 Oct 2004 11:07:41 +0200
Subject: [R] Insert image in Rd documentation files
Message-ID: <OF2B90D085.64C4F5C6-ON41256F34.00374AA8-41256F34.0037A270@notes.edfgdf.fr>

Hello,

Does anyone know if it is posssible to insert a picture (ps or jpg for example) in documentation files with Rd extension. I guess it's impossible but maybe there is a trick.

Thank you!


Eric Esposito
==================================
Direction de la Recherche
P??le Economie, Statistisques et Sociologie
361 avenue du Pr??sident Wilson - BP 33
93211 Saint-Denis La Plaine cedex
==================================
Tel: 01.49.22.53.98
Fax: 01.49.22.57.10



From ligges at statistik.uni-dortmund.de  Thu Oct 21 11:55:57 2004
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Thu, 21 Oct 2004 11:55:57 +0200
Subject: [R] Insert image in Rd documentation files
In-Reply-To: <OF2B90D085.64C4F5C6-ON41256F34.00374AA8-41256F34.0037A270@notes.edfgdf.fr>
References: <OF2B90D085.64C4F5C6-ON41256F34.00374AA8-41256F34.0037A270@notes.edfgdf.fr>
Message-ID: <417787AD.2070204@statistik.uni-dortmund.de>

Eric ESPOSITO wrote:

> Hello,
> 
> Does anyone know if it is posssible to insert a picture (ps or jpg for example) in documentation files with Rd extension. I guess it's impossible but maybe there is a trick.

I think it is not (easily) possible. Instead, you might want to write an 
additional vignette using Sweave ...

Uwe Ligges


> Thank you!
> 
> 
> Eric Esposito
> ==================================
> Direction de la Recherche
> P??le Economie, Statistisques et Sociologie
> 361 avenue du Pr??sident Wilson - BP 33
> 93211 Saint-Denis La Plaine cedex
> ==================================
> Tel: 01.49.22.53.98
> Fax: 01.49.22.57.10
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html



From dimitris.rizopoulos at med.kuleuven.ac.be  Thu Oct 21 12:11:22 2004
From: dimitris.rizopoulos at med.kuleuven.ac.be (Dimitris Rizopoulos)
Date: Thu, 21 Oct 2004 12:11:22 +0200
Subject: [R] Robust regression with groups
References: <200410201542.i9KFgNdw007037@ohm.gene.com>
Message-ID: <002001c4b756$50cf7460$0540210a@www.domain>

Hi Bert,

Regarding the sensitivity in the choice of the random-effects 
distribution, I know that usually estimates of the fixed-effects (and 
their std.errors) do not have serious problems, even if you assume 
normality where in fact you have log-normality. However, you do have a 
problem in the EB estimates of the random-effects.

More info could be found in:

G. Verbeke and E. Lesaffre (1996). A linear mixed-effects model with 
heterogeneity in the random-effects population, JASA, 91, 217-221.

W. Ghidey, E. Lesaffre and P. Eilers (2004). Smooth random-effects 
distribution in linear mixed model, Biometrics, 60, 945-953.
(to appear in December)

Best,
Dimitris

----
Dimitris Rizopoulos
Ph.D. Student
Biostatistical Centre
School of Public Health
Catholic University of Leuven

Address: Kapucijnenvoer 35, Leuven, Belgium
Tel: +32/16/396887
Fax: +32/16/337015
Web: http://www.med.kuleuven.ac.be/biostat/
     http://www.student.kuleuven.ac.be/~m0390867/dimitris.htm


----- Original Message ----- 
From: "Berton Gunter" <gunter.berton at gene.com>
To: "'Dimitris Rizopoulos'" <dimitris.rizopoulos at med.kuleuven.ac.be>; 
"'Angelo Secchi'" <secchi at sssup.it>
Cc: <r-help at stat.math.ethz.ch>
Sent: Wednesday, October 20, 2004 5:42 PM
Subject: RE: [R] Robust regression with groups


> Angelo and Folks:
>
> Beware! It is not at all clear what you mean by "robust" regression. 
> The
> sandwich estimator is often said to be "robust" to model 
> misspecification in
> the sense that it converges to the correct covariance matrix whether 
> or not
> the correlation structure in the GEE has been correctly specified 
> (as
> Dmitris implied). Is this what you mean? Mixed effect models are 
> often said
> to be "robust" in the sense that individual group "estimators" 
> (blups) are
> shrunk toward the overall fixed effect estimates. Is this what you 
> mean?
>
> In other applications, "robustness" can mean insensitivity to 
> distributional
> assumptions. Mixed effects models for continupus responses commonly 
> assume
> normality (as the estimates solve likelihood equations), as do 
> GLMM's for
> the random effects. I know of no definitive work that has examined
> sensitivity of estimates (or inferences, which are, at best, 
> asymptotic
> anyway) to those assumptions. (in the simple independent errors 
> case, it is
> usually the case that estimates are not at all sensitive). However, 
> I am a
> novice here, so others may be able to illuminate the issue more.
>
> Finally, "robustness" is often used to mean "outlier resistance." 
> Here the
> situation is yet murkier. Do you mean resistance to individual 
> "outlying"
> observations within a subject or resistance to outlying subjects? 
> Shrinkage
> should help with both, but, again, I know of no definitive work, 
> especially
> regarding resistance to individual extreme values. Given the 
> sensitivity of
> covariance estimates to heavy tails and the consequent inferential
> inefficiency, this presumably could be a problem. Finding methods 
> that could
> deal with this may be nearly impossible, as you are adding yet 
> another layer
> of nonlinear estimation (that of determining optimal case 
> weights/parameters
> for mixture contamination models/or whatever...) to the problem; it 
> is easy
> to come up with examples where the data are inherently ambiguous and
> parameter estimates for resistant case weights and the model would 
> trade off
> with each other depending on starting values. That is, too many 
> nonlinear
> parameters are being estimated and the model estimates are therefore
> unstable.
>
> Again, I am happy to leave more definitive resolution and correction 
> of any
> errors in my comments to the experts, but, at the least, I think you 
> need to
> think more and communicate more clearly about what you mean by 
> "robust."
>
> Cheers,
>
> -- Bert Gunter
> Genentech Non-Clinical Statistics
> South San Francisco, CA
>
> "The business of the statistician is to catalyze the scientific 
> learning
> process."  - George E. P. Box
>
>
>
>> -----Original Message-----
>> From: r-help-bounces at stat.math.ethz.ch
>> [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of
>> Dimitris Rizopoulos
>> Sent: Wednesday, October 20, 2004 7:08 AM
>> To: Angelo Secchi
>> Cc: r-help at stat.math.ethz.ch
>> Subject: Re: [R] Robust regression with groups
>>
>> Hi Angelo,
>>
>> There are two possible options (at least to my knowledge):
>>
>> 1. to use a random-effects model, either using `lme' (packages: 
>> nlme,
>> lme4) if you have normal data or `glmmPQL' (package: MASS) or 
>> `GLMM'
>> (package: lme4) or `glmmML' (package:glmmML) if you cannot use the
>> normal distribution.
>>
>> 2. to use a gee model with a robust (sandwich) std.error 
>> estimation.
>> See at `gee' (package: gee) and `geese' (package: geepack).
>>
>> I hope this helps.
>>
>> Best,
>> Dimitris
>>
>> ----
>> Dimitris Rizopoulos
>> Ph.D. Student
>> Biostatistical Centre
>> School of Public Health
>> Catholic University of Leuven
>>
>> Address: Kapucijnenvoer 35, Leuven, Belgium
>> Tel: +32/16/396887
>> Fax: +32/16/337015
>> Web: http://www.med.kuleuven.ac.be/biostat/
>>      http://www.student.kuleuven.ac.be/~m0390867/dimitris.htm
>>
>>
>>
>>
>> ----- Original Message ----- 
>> From: "Angelo Secchi" <secchi at sssup.it>
>> To: <r-help at stat.math.ethz.ch>
>> Sent: Wednesday, October 20, 2004 3:22 PM
>> Subject: [R] Robust regression with groups
>>
>>
>> >
>> >
>> > Hi,
>> > I have data on a group of subjects in different years. I should
>> > assume
>> > that observations regarding different individuals are independent
>> > but
>> > observations for the same individual in different years are not 
>> > and
>> > I
>> > would like to have an estimated standard error (and
>> > variance-covariance
>> > matrix) taking into account this problem.
>> >
>> > More in general is there a way in R to run a (robust)regression
>> > having
>> > different groups in the observations and specifying that the
>> > observation
>> > are independent across groups but not necessarily independent 
>> > within
>> > groups?
>> >
>> > Thanks
>> > a.
>> >
>> > ______________________________________________
>> > R-help at stat.math.ethz.ch mailing list
>> > https://stat.ethz.ch/mailman/listinfo/r-help
>> > PLEASE do read the posting guide!
>> > http://www.R-project.org/posting-guide.html
>> >
>>
>> ______________________________________________
>> R-help at stat.math.ethz.ch mailing list
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide!
>> http://www.R-project.org/posting-guide.html
>>
>



From Achim.Zeileis at wu-wien.ac.at  Thu Oct 21 12:11:25 2004
From: Achim.Zeileis at wu-wien.ac.at (Achim Zeileis)
Date: Thu, 21 Oct 2004 12:11:25 +0200
Subject: [R] Re: [R--gR] smooth="periodic.lowess"???
In-Reply-To: <200410210944.i9L9iVER007900@hypatia.math.ethz.ch>
References: <200410210944.i9L9iVER007900@hypatia.math.ethz.ch>
Message-ID: <20041021121125.458f0b47.Achim.Zeileis@wu-wien.ac.at>

Moved to R-help from R-SIG-gR.

On Thu, 21 Oct 2004 17:44:27 +0800 XP Sun wrote:

> hi, everyone,
> 
> this is the first time for me to use R,
> would you like to tell me how to use this command? thank you in
> advance.
> 
> fit <- principal.curve(y, plot = TRUE, smooth="periodic.lowess",
> iter=0)
> 
> when i uesd the parameter 
> 
>    smooth="periodic.lowess", 
> 
> i met a error. the message is:
> 
>    Error in cbind(...) : number of rows of matrices must match (see
>    arg 2)
> 
> any help is appreciated.

Several comments:
  1. Why did you send this request to R-SIG-gR?
     This is a SIG (special interest group) on gR (graphical models
     in R) and your request does not seem to be related to that!
  2. A better place to ask questions like the above is R-help.
     But one should read the posting guide before doing so.
       http://www.R-project.org/posting-guide.html
  3. If you read the posting guide, it will tell you that:
     If you have problems with some function, please do give a 
     reproducible example (including your version of R and OS,
     indicate add-on packages you use, etc.).

For your request above, there is nothing much that we could do. After I
had found out that this is about a function in the package princurve (do
note that there is a second CRAN package pcurve for the same task), all
I can say is that

  R> library(princurve)
  R> example(principal.curve)
  R> fit <- principal.curve(x, plot = TRUE, smooth="periodic.lowess",
       iter= 0)

gives some warnings but does not throw an error as you indicated above.
This is on Linux with today's R-devel (2.1.0 to be) with the current
CRAN version of princurve.
Z


> best, 
> 
> ????????????????????????????????Sun
> ????????????????????????????????xpsun at ict.ac.cn
> ????????????????????????????????????????2004-10-21
> 
> _______________________________________________
> R-sig-gR mailing list
> R-sig-gR at stat.math.ethz.ch
> https://stat.ethz.ch/mailman/listinfo/r-sig-gr
>



From eric.esposito at gazdefrance.com  Thu Oct 21 13:19:39 2004
From: eric.esposito at gazdefrance.com (Eric ESPOSITO)
Date: Thu, 21 Oct 2004 13:19:39 +0200
Subject: [R] Problem with vignettes
Message-ID: <OF2253A963.06ED5645-ON41256F34.00434A9E-41256F34.0043B790@notes.edfgdf.fr>

Hello,

I'm trying to insert a vignette as a word document in my package. After compiling and installing the package, the word file is in the doc directory, but no link in the html
index for vignettes is created. I tried with a pdf file and there is the same problem. It seems that only sweave format files are taken into account, is it normal? Is there
something special to do to take other file formats into account?

Thank you


Eric Esposito
==================================
Direction de la Recherche
P??le Economie, Statistisques et Sociologie
361 avenue du Pr??sident Wilson - BP 33
93211 Saint-Denis La Plaine cedex
==================================
Tel: 01.49.22.53.98
Fax: 01.49.22.57.10



From fprass at yahoo.com.br  Thu Oct 21 13:48:46 2004
From: fprass at yahoo.com.br (Fernando Prass)
Date: Thu, 21 Oct 2004 08:48:46 -0300 (ART)
Subject: [R] Cluster Analysis: Density-Based Method
Message-ID: <20041021114846.71967.qmail@web42005.mail.yahoo.com>

Hi people,

Does anybody know some Density-Based Method for clustering implemented in R?

Thanks,

Fernando Prass
		
_______________________________________________________ 



From h.andersson at nioo.knaw.nl  Thu Oct 21 14:12:52 2004
From: h.andersson at nioo.knaw.nl (Henrik Andersson)
Date: Thu, 21 Oct 2004 14:12:52 +0200
Subject: [R] Arrow heads at the end of axes
Message-ID: <cl8952$ino$1@sea.gmane.org>

I would like to have arrow heads at the end of my axes, since I am 
plotting variable where the absolute amount is irrelevant, there is not 
supposed to be numbers on the axes.

An imperfect example:
plot(rnorm(10),bty='l',xaxt='n',yaxt='n',ylab='',xlab='',type='l')
abline(h=0)

Like this but without, the xaxis and with arrrowheads

More like this in fact,
LaTeX Picture example:

\setlength{\unitlength}{1.3cm}
\begin{picture}(4.3,3.6)(-2.5,-0.25)
\put(-2,1.8){\vector(1,0){4.4}}
\put(2.5,1.7){$x$}
\put(-2,0){\vector(0,1){3.2}}
\put(-2,3.35){\makebox(0,0){$y$}}
\end{picture}

---------------------------------------------
Henrik Andersson
Netherlands Institute of Ecology -
Centre for Estuarine and Marine Ecology
P.O. Box 140
4400 AC Yerseke
Phone: +31 113 577473
h.andersson at nioo.knaw.nl
http://www.nioo.knaw.nl/ppages/handersson



From emanuela001 at interfree.it  Thu Oct 21 14:31:01 2004
From: emanuela001 at interfree.it (emanuela001@interfree.it)
Date: 21 Oct 2004 12:31:01 -0000
Subject: [R] inverse gaussian distribution of frailty variable
Message-ID: <20041021123101.17071.qmail@community22.interfree.it>


Hello,

I'm Emanuela, I'm implemented a survival analysis and I'm trying to use a frailty model with inverse gaussian distribution, but I'm not able to find the right code, because it seems to be only a gamma and a gaussian distribution. Is there also the inverse gaussian distribution? 

Thanks a lot

Emanuela Rossi

-------------------------------------------------------------------------



From kjetil at acelerate.com  Thu Oct 21 14:34:10 2004
From: kjetil at acelerate.com (Kjetil Brinchmann Halvorsen)
Date: Thu, 21 Oct 2004 08:34:10 -0400
Subject: [R] Cluster Analysis: Density-Based Method
In-Reply-To: <20041021114846.71967.qmail@web42005.mail.yahoo.com>
References: <20041021114846.71967.qmail@web42005.mail.yahoo.com>
Message-ID: <4177ACC2.6070604@acelerate.com>

Fernando Prass wrote:

>Hi people,
>
>Does anybody know some Density-Based Method for clustering implemented in R?
>  
>
Have you looked at CRAN package mclust?

>Thanks,
>
>Fernando Prass
>
>		
>_______________________________________________________ 
>
>______________________________________________
>R-help at stat.math.ethz.ch mailing list
>https://stat.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>
>
>  
>


-- 

Kjetil Halvorsen.

Peace is the most effective weapon of mass construction.
               --  Mahdi Elmandjra



From MSchwartz at MedAnalytics.com  Thu Oct 21 14:35:34 2004
From: MSchwartz at MedAnalytics.com (Marc Schwartz)
Date: Thu, 21 Oct 2004 07:35:34 -0500
Subject: [R] Arrow heads at the end of axes
In-Reply-To: <cl8952$ino$1@sea.gmane.org>
References: <cl8952$ino$1@sea.gmane.org>
Message-ID: <1098362134.2041.4.camel@orion.localdomain>

On Thu, 2004-10-21 at 07:12, Henrik Andersson wrote:
> I would like to have arrow heads at the end of my axes, since I am 
> plotting variable where the absolute amount is irrelevant, there is not 
> supposed to be numbers on the axes.
> 
> An imperfect example:
> plot(rnorm(10),bty='l',xaxt='n',yaxt='n',ylab='',xlab='',type='l')
> abline(h=0)
> 
> Like this but without, the xaxis and with arrrowheads
> 
> More like this in fact,
> LaTeX Picture example:
> 
> \setlength{\unitlength}{1.3cm}
> \begin{picture}(4.3,3.6)(-2.5,-0.25)
> \put(-2,1.8){\vector(1,0){4.4}}
> \put(2.5,1.7){$x$}
> \put(-2,0){\vector(0,1){3.2}}
> \put(-2,3.35){\makebox(0,0){$y$}}
> \end{picture}

You can do something like this:

plot(rnorm(10), axes = FALSE)

# Get the axis ranges
u <- par("usr")

# Use arrows() to draw the axis lines, adding the arrowheads
# Use the par("usr") values to specify the end points of the lines
# Setting 'xpd = TRUE' allows for the arrowhead to be drawn outside
# the plot region as required here
arrows(u[1], u[3], u[2], u[3], code = 2, xpd = TRUE)
arrows(u[1], u[3], u[1], u[4], code = 2, xpd = TRUE)

See ?arrows for additional options to define the arrows.

HTH,

Marc Schwartz



From fprass at yahoo.com.br  Thu Oct 21 14:47:41 2004
From: fprass at yahoo.com.br (Fernando Prass)
Date: Thu, 21 Oct 2004 09:47:41 -0300 (ART)
Subject: [R] Cluster Analysis: Density-Based Method
In-Reply-To: <4177ACC2.6070604@acelerate.com>
Message-ID: <20041021124741.13881.qmail@web42006.mail.yahoo.com>

Yes, but mclust don't have a density-based algorithm. Mclust have the algorithm
BIC, that is a model-based method...

Fernando Prass

 --- Kjetil Brinchmann Halvorsen <kjetil at acelerate.com> escreveu: 
> Fernando Prass wrote:
> 
> >Hi people,
> >
> >Does anybody know some Density-Based Method for clustering implemented in R?
> >  
> >
> Have you looked at CRAN package mclust?
> 
> >Thanks,
> >
> >Fernando Prass
	
_______________________________________________________ 



From ligges at statistik.uni-dortmund.de  Thu Oct 21 15:05:52 2004
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Thu, 21 Oct 2004 15:05:52 +0200
Subject: [R] Error when intstalling R on intel box running linux
In-Reply-To: <B56908D27FB07B48A05FCC6FC91A0467034AD4E0@mithril.entelos.com>
References: <B56908D27FB07B48A05FCC6FC91A0467034AD4E0@mithril.entelos.com>
Message-ID: <4177B430.8000201@statistik.uni-dortmund.de>

Yisheng Jin wrote:
> Hi there,
> 
>  
> 
> I have been trying to install R-2.0.0 on my Linux box (Intel chip) from
> source.

Which Linux? Which version of gcc? Which version auf the binutils?

Uwe Ligges


>  
> 
> I am getting the following error message when running 'make':
> 
>  
> 
> make[3]: Entering directory `/devtop/private/jin/R-2.0.0/src/appl'
> 
> g77 -mieee-fp -fPIC  -g -O2 -c ch2inv.f -o ch2inv.lo
> 
> /tmp/ccm8Zyp4.s: Assembler messages:
> 
> /tmp/ccm8Zyp4.s:165: Error: junk `(%ebx)' after expression
> 
> make[3]: *** [ch2inv.lo] Error 1
> 
> make[3]: Leaving directory `/devtop/private/jin/R-2.0.0/src/appl'
> 
> make[2]: *** [R] Error 2
> 
> make[2]: Leaving directory `/devtop/private/jin/R-2.0.0/src/appl'
> 
> make[1]: *** [R] Error 1
> 
> make[1]: Leaving directory `/devtop/private/jin/R-2.0.0/src'
> 
> make: *** [R] Error 1
> 
>  
> 
> What shall I do to fix?  Thanks.
> 
>  
> 
> Jim
> 
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html



From i.visser at uva.nl  Thu Oct 21 15:05:36 2004
From: i.visser at uva.nl (Ingmar Visser)
Date: Thu, 21 Oct 2004 15:05:36 +0200
Subject: [R] Cluster Analysis: Density-Based Method
In-Reply-To: <20041021124741.13881.qmail@web42006.mail.yahoo.com>
Message-ID: <BD9D80C0.8FCE%i.visser@uva.nl>

maybe ?kmeans is what you're looking for ...
ingmar

On 10/21/04 2:47 PM, "Fernando Prass" <fprass at yahoo.com.br> wrote:

> Yes, but mclust don't have a density-based algorithm. Mclust have the
> algorithm
> BIC, that is a model-based method...
> 
> Fernando Prass
> 
> --- Kjetil Brinchmann Halvorsen <kjetil at acelerate.com> escreveu:
>> Fernando Prass wrote:
>> 
>>> Hi people,
>>> 
>>> Does anybody know some Density-Based Method for clustering implemented in R?
>>>  
>>> 
>> Have you looked at CRAN package mclust?
>> 
>>> Thanks,
>>> 
>>> Fernando Prass
> 
> 
> 
> 
> _______________________________________________________
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>



From fprass at yahoo.com.br  Thu Oct 21 15:18:23 2004
From: fprass at yahoo.com.br (Fernando Prass)
Date: Thu, 21 Oct 2004 10:18:23 -0300 (ART)
Subject: [R] Cluster Analysis: Density-Based Method
In-Reply-To: <BD9D80C0.8FCE%i.visser@uva.nl>
Message-ID: <20041021131823.18478.qmail@web42003.mail.yahoo.com>

No, kmeans is a partition method. I need a model-based method, like DBSCAN or
DENCLUE algorithm...

Fernando Prass

 --- Ingmar Visser <i.visser at uva.nl> escreveu: 
> maybe ?kmeans is what you're looking for ...
> ingmar
> 
> On 10/21/04 2:47 PM, "Fernando Prass" <fprass at yahoo.com.br> wrote:
> 
> > Yes, but mclust don't have a density-based algorithm. Mclust have the
> > algorithm
> > BIC, that is a model-based method...
> > 
> > Fernando Prass
> > 
> > --- Kjetil Brinchmann Halvorsen <kjetil at acelerate.com> escreveu:
> >> Fernando Prass wrote:
> >> 
> >>> Hi people,
> >>> 
> >>> Does anybody know some Density-Based Method for clustering implemented in
> R?
> >>>  
> >>> 
> >> Have you looked at CRAN package mclust?
> >> 
> >>> Thanks,
> >>> 
> >>> Fernando Prass
> > 
> > 
> > 
> > 
> > 
> > 
> > _______________________________________________________
> > 
> > ______________________________________________
> > R-help at stat.math.ethz.ch mailing list
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide!
> http://www.R-project.org/posting-guide.html
> > 
> 
>  
		
_______________________________________________________ 



From andy_liaw at merck.com  Thu Oct 21 15:18:54 2004
From: andy_liaw at merck.com (Liaw, Andy)
Date: Thu, 21 Oct 2004 09:18:54 -0400
Subject: [R] Cluster Analysis: Density-Based Method
Message-ID: <3A822319EB35174CA3714066D590DCD50994E1E7@usrymx25.merck.com>

I'm no expert in this, but mclust is `density-based' because it estimates
the density with a mixture of Gaussians.  If this is not what you want, you
should clarify what you mean by `density-based'.  Do you mean an algorithm
based on kernel estimator of the density?

Andy

> From: Fernando Prass
> 
> Yes, but mclust don't have a density-based algorithm. Mclust 
> have the algorithm
> BIC, that is a model-based method...
> 
> Fernando Prass
> 
>  --- Kjetil Brinchmann Halvorsen <kjetil at acelerate.com> escreveu: 
> > Fernando Prass wrote:
> > 
> > >Hi people,
> > >
> > >Does anybody know some Density-Based Method for clustering 
> implemented in R?
> > >  
> > >
> > Have you looked at CRAN package mclust?
> > 
> > >Thanks,
> > >
> > >Fernando Prass
> 
> 
> 
> 	
> 	
> 		
> _______________________________________________________ 
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
> 
>



From ligges at statistik.uni-dortmund.de  Thu Oct 21 15:20:21 2004
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Thu, 21 Oct 2004 15:20:21 +0200
Subject: [R] Matrix/Table col headings R 2.0.0
In-Reply-To: <010601c4b5fb$fbd474c0$3e80010a@BigBaer>
References: <010601c4b5fb$fbd474c0$3e80010a@BigBaer>
Message-ID: <4177B795.4060203@statistik.uni-dortmund.de>

Robert W. Baer, Ph.D. wrote:

> I have been looking at some 'table' examples in Peter Dalgaard's ISwR book, and I am confused by how to get right justification of my table headings when I use the tables() command.  Compare the following:
> 
> # Produces right justfified column names
> caff.marital=matrix(c(652,1537,598,242,36,46,38,21,218,327,106,67),nrow=3,byrow=T)
> colnames(caff.marital)=c("0","1-150","151-300",">300")
> rownames(caff.marital)=c("Married","Prev.married","Single")
> caff.marital
> class(caff.marital)
> 
> #Produces Left justified column names
> library(ISwR)
> data(juul)
> attach(juul)
> sex.tan=table(sex,tanner)
> colnames(sex.tan)=c("I","II","IIII","IV","V")
> rownames(sex.tan)=c("M","F")
> sex.tan
> class(sex.tan)
> 
> # Finally, look at (left justified)
> as.table(caff.marital)
> 
> Somehow PD got right justified columns with this dataset.  Is there a new way of doing things in version 2.0.0, my ignorance, a bug?  I tried making the columns factors first, but in my hands this did not appear to help either.  Thanks for any insight.

[Since this question seems to be unanswered:]

This is a question related to Peter Dalgaard's book, so why don't you 
ask the author rather than the whole world?

Note that caff.materia is a matrix while as.table(caff.marital) is a 
table (and has a different print method).

Why does the justification matter here? I'm much too lazy to look for 
changes in R related to your question - but probably you have done 
yourself before asking the question...???
And why don't you cite the example you are referring to (e.g. by a page 
number!!!) exactly? I'm also too lazy to read Peter's book once more 
just in order to find the example you are talking about!

Uwe Ligges


>>version
> 
>       _              
> platform i386-pc-mingw32
> arch     i386           
> os       mingw32        
> system   i386, mingw32  
> status                  
> major    2              
> minor    0.0            
> year     2004           
> month    10             
> day      04             
> language R 
> 
>   ___________________________
> Robert W. Baer, Ph.D.
> Department of Physiology
> A. T. Still University of Health Science
> 800 W. Jefferson St. 
> Kirksville, MO 63501-1497 USA
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
> 
> 	[[alternative HTML version deleted]]



From ligges at statistik.uni-dortmund.de  Thu Oct 21 15:24:05 2004
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Thu, 21 Oct 2004 15:24:05 +0200
Subject: [R] Problem with vignettes
In-Reply-To: <OF2253A963.06ED5645-ON41256F34.00434A9E-41256F34.0043B790@notes.edfgdf.fr>
References: <OF2253A963.06ED5645-ON41256F34.00434A9E-41256F34.0043B790@notes.edfgdf.fr>
Message-ID: <4177B875.2020601@statistik.uni-dortmund.de>

Eric ESPOSITO wrote:

> Hello,
> 
> I'm trying to insert a vignette as a word document in my package. After compiling and installing the package, the word file is in the doc directory, but no link in the html
> index for vignettes is created. I tried with a pdf file and there is the same problem. It seems that only sweave format files are taken into account, is it normal? Is there
> something special to do to take other file formats into account?

Please read Section 1.4 (Writing package vignettes) of the manual 
"Writing R Extensions".

Uwe Ligges


> Thank you
> 
> 
> Eric Esposito
> ==================================
> Direction de la Recherche
> P??le Economie, Statistisques et Sociologie
> 361 avenue du Pr??sident Wilson - BP 33
> 93211 Saint-Denis La Plaine cedex
> ==================================
> Tel: 01.49.22.53.98
> Fax: 01.49.22.57.10
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html



From jeaneid at chass.utoronto.ca  Thu Oct 21 15:46:42 2004
From: jeaneid at chass.utoronto.ca (Jean Eid)
Date: Thu, 21 Oct 2004 09:46:42 -0400
Subject: [R] densityplot and histogram
In-Reply-To: <200410201512.00533.deepayan@stat.wisc.edu>
Message-ID: <Pine.SGI.4.40.0410210939460.47402748-100000@origin.chass.utoronto.ca>

Deepayan,

Thank you so much,... works like a charm. However, I have two more
questions:

a) in ?panel.axis is says
...: certain graphical parameters (fonts, color, etc) can be
          supplied. See the formal argument list for valid names.
I tried to change your example to have to have
	panel.axis(side = "right",
	           at = mult * at, labels = at,
        	   outside = TRUE, tck=-.5, font=2)

However, it does output an error:
	Error in panel.axis(side = "right", at = mult * at, labels = at, outside = TRUE,  :
	unused argument(s) (font ...)

When you say "See the formal argument for valid names" where do I see
these, and is there a pdf tutorial on the lattice package that I can take
a look at.


b) How do I change the xlab and yalb to bold fonts.



Thank you so much for all the help,


Jean,


On Wed, 20 Oct 2004, Deepayan Sarkar wrote:

> On Tuesday 19 October 2004 17:12, Jean Eid wrote:
> > Is there any function like par(new=T) for lattice. I want to plot a
> > histogram in percentages on the right hand side and also superimpose the
> > densityplot with its density scale on the lhs. so far I am only able to do
> > this
> >   histogram( temp[,2]~ temp[,1],nint=100,type="desnity",
> >                xlab = "Population Size",
> >                panel = function(x, ...) {
> >                    panel.histogram(x, ...)
> >                    panel.densityplot(x, col = "red", plot.points=F, lwd=2)
> >                } )
> >
> >  If I change type="density" to type="percent" the scales for the
> > densityplot will be too low and all I see is a horizontal line at zero
> > (this is as expected) . However, I tried par(new=T) and nothing happens. I
> > want to be able to put percenstages on axis 2 and density values at axis
> > 4.
>
> I don't think par(new=T) is what you should be looking for (and incidentally,
> par settings have no effect on lattice plots). It's possible to add axes
> after the plot (easier than it was before 2.0.0), but the design of lattice
> doesn't allow you to easily allocate enough space for the second set of axes.
> You can still do it, but it would be kludgy.
>
> Here's an example (you need to know what 'mult' should be -- it's the factor
> that converts the density scale to the percent scale -- it would depend on
> the widths of the bins and the length of x):
>
>
> x <- rnorm(200)
> mult <- 60 ## meaningless in this case
>
> histogram(x, type = "percent",
>           panel = function(x, ...) {
>               panel.histogram(x, ...)
>               d <- density(x)
>               panel.lines(d$x, mult * d$y, col = 'red')
>           },
>           scales = list(y = list(tck = c(1, 0))),
>           par.settings = list(layout.widths = list(right.padding = 5)))
>
> trellis.focus("panel", 1, 1, clip.off = TRUE)
>
> at <- pretty(c(0, 25)/mult)
> panel.axis(side = "right",
>            at = mult * at, labels = at,
>            outside = TRUE)
> trellis.unfocus()
>
>



From fm3a004 at math.uni-hamburg.de  Thu Oct 21 15:51:02 2004
From: fm3a004 at math.uni-hamburg.de (Christian Hennig)
Date: Thu, 21 Oct 2004 15:51:02 +0200 (MEST)
Subject: [R] Cluster Analysis: Density-Based Method
In-Reply-To: <20041021131823.18478.qmail@web42003.mail.yahoo.com>
Message-ID: <Pine.GSO.3.95q.1041021153410.5006I-100000@sun11.math.uni-hamburg.de>

Dear Fernando,

below you find a DBSCAN function I wrote for my own purposes.
It comes with no warranty and without proper documentation, but I followed
the notation of the original KDD-96 DBSCAN paper.
For large data sets, it may be slow.

Best,
Christian

On Thu, 21 Oct 2004, Fernando Prass wrote:

> No, kmeans is a partition method. I need a model-based method, like DBSCAN or
> DENCLUE algorithm...
> 
> Fernando Prass

distvector <- function(x,data){
  ddata <- t(data)-x
  dv <- apply(ddata^2,2,sum)
}

# data may be nxp or distance matrix
# eps is the dbscan distance cutoff parameter
# MinPts is the minimum size of a cluster
# scale: Should the data be scaled?
# distances: has to be TRUE if data is a distance matrix
# showplot: Should the computation process be visualized? 
# countmode: dbscan gives messages when processing point no. (countmode)
dbscan <- function(data,eps,MinPts=5, scale=FALSE, distances=FALSE,
                   showplot=FALSE,
                   countmode=c(1,2,3,5,10,100,1000,5000,10000,50000)){
  data <- as.matrix(data)
  n <- nrow(data)
  if (scale) data <- scale(data)
  unregpoints <- rep(0,n)
  e2 <- eps^2
  cv <- rep(0,n)
  cn <- 0
  i <- 1
  for (i in 1:n){
    if (i %in% countmode) cat("Processing point ", i," of ",n, ".\n")
    unclass <- cv<1
    if (cv[i]==0){
      if (distances) seeds <- data[i,]<=eps
      else{
        seeds <- rep(FALSE,n)
        seeds[unclass] <- distvector(data[i,],data[unclass,])<=e2
      }
      if (sum(seeds)+unregpoints[i]<MinPts) cv[i] <- (-1)
      else{
        cn <- cn+1
        cv[i] <- cn
        seeds[i] <- unclass[i] <- FALSE
        unregpoints[seeds] <- unregpoints[seeds]+1
        while (sum(seeds)>0){
          if (showplot) plot(data,col=1+cv)
          unclass[seeds] <- FALSE
          cv[seeds] <- cn
          ap <- (1:n)[seeds]
#          print(ap)
          seeds <- rep(FALSE,n)          
          for (j in ap){
#            if (showplot) plot(data,col=1+cv)
            jseeds <- rep(FALSE,n)          
            if (distances) jseeds[unclass] <- data[j,unclass]<=eps
            else{
              jseeds[unclass] <- distvector(data[j,],data[unclass,])<=e2
            }
            unregpoints[jseeds] <- unregpoints[jseeds]+1
#            if (cn==1)
#              cat(j," sum seeds=",sum(seeds)," unreg=",unregpoints[j],
#                  " newseeds=",sum(cv[jseeds]==0),"\n")
            if (sum(jseeds)+unregpoints[j]>=MinPts){              
              seeds[jseeds] <- cv[jseeds]==0
              cv[jseeds & cv<0] <- cn
            }
          } # for j
        } # while sum seeds>0
      } # else (sum seeds + ... >= MinPts)
    } # if cv==0
  } # for i
  if (sum(cv==(-1))>0){
    noisenumber <- cn+1
    cv[cv==(-1)] <- noisenumber
  }
  else
    noisenumber <- FALSE
  out <- list(classification=cv, noisenumber=noisenumber,
              eps=eps, MinPts=MinPts, unregpoints=unregpoints)
  out
} # dbscan
# classification: classification vector
# noisenumber: number in the classification vector indicating noise points
# unregpoints: ignore...

***********************************************************************
Christian Hennig
Fachbereich Mathematik-SPST/ZMS, Universitaet Hamburg
hennig at math.uni-hamburg.de, http://www.math.uni-hamburg.de/home/hennig/
#######################################################################
ich empfehle www.boag-online.de



From h.andersson at nioo.knaw.nl  Thu Oct 21 16:04:09 2004
From: h.andersson at nioo.knaw.nl (Henrik Andersson)
Date: Thu, 21 Oct 2004 16:04:09 +0200
Subject: [R] Arrow heads at the end of axes
In-Reply-To: <1098362134.2041.4.camel@orion.localdomain>
References: <cl8952$ino$1@sea.gmane.org>
	<1098362134.2041.4.camel@orion.localdomain>
Message-ID: <cl8flo$75n$1@sea.gmane.org>

Marc Schwartz wrote:

> On Thu, 2004-10-21 at 07:12, Henrik Andersson wrote:
> 
>>I would like to have arrow heads at the end of my axes, since I am 
>>plotting variable where the absolute amount is irrelevant, there is not 
>>supposed to be numbers on the axes.
>>
...
> 
> 
> You can do something like this:
> 
> plot(rnorm(10), axes = FALSE)
> 
> # Get the axis ranges
> u <- par("usr")
> 
> # Use arrows() to draw the axis lines, adding the arrowheads
> # Use the par("usr") values to specify the end points of the lines
> # Setting 'xpd = TRUE' allows for the arrowhead to be drawn outside
> # the plot region as required here
> arrows(u[1], u[3], u[2], u[3], code = 2, xpd = TRUE)
> arrows(u[1], u[3], u[1], u[4], code = 2, xpd = TRUE)
> 
> See ?arrows for additional options to define the arrows.
> 
> HTH,
> 
> Marc Schwartz

Thanks for pointing the direction (seriously), the looks of the 
arrowheads could still use some improvements, so I used a polygon to 
draw an arrowhead which thanks to your suggestion was possible outside 
the plot region.

Is it my Internet English or normal English that is bad, but was is the 
expansion of 'HTH' ?

Cheers, Henrik



From jeaneid at chass.utoronto.ca  Thu Oct 21 16:15:15 2004
From: jeaneid at chass.utoronto.ca (Jean Eid)
Date: Thu, 21 Oct 2004 10:15:15 -0400
Subject: [R] densityplot and histogram
In-Reply-To: <Pine.SGI.4.40.0410210939460.47402748-100000@origin.chass.utoronto.ca>
Message-ID: <Pine.SGI.4.40.0410211010330.47368960-100000@origin.chass.utoronto.ca>

I figured out both questions,.. The idea is to set them with
trellis.par.set(). However, I still would like to know if there is a
tutorial for lattice package.

Jean,

On Thu, 21 Oct 2004, Jean Eid wrote:

> Deepayan,
>
> Thank you so much,... works like a charm. However, I have two more
> questions:
>
> a) in ?panel.axis is says
> ...: certain graphical parameters (fonts, color, etc) can be
>           supplied. See the formal argument list for valid names.
> I tried to change your example to have to have
> 	panel.axis(side = "right",
> 	           at = mult * at, labels = at,
>         	   outside = TRUE, tck=-.5, font=2)
>
> However, it does output an error:
> 	Error in panel.axis(side = "right", at = mult * at, labels = at, outside = TRUE,  :
> 	unused argument(s) (font ...)
>
> When you say "See the formal argument for valid names" where do I see
> these, and is there a pdf tutorial on the lattice package that I can take
> a look at.
>
>
> b) How do I change the xlab and yalb to bold fonts.
>
>
>
> Thank you so much for all the help,
>
>
> Jean,
>
>
> On Wed, 20 Oct 2004, Deepayan Sarkar wrote:
>
> > On Tuesday 19 October 2004 17:12, Jean Eid wrote:
> > > Is there any function like par(new=T) for lattice. I want to plot a
> > > histogram in percentages on the right hand side and also superimpose the
> > > densityplot with its density scale on the lhs. so far I am only able to do
> > > this
> > >   histogram( temp[,2]~ temp[,1],nint=100,type="desnity",
> > >                xlab = "Population Size",
> > >                panel = function(x, ...) {
> > >                    panel.histogram(x, ...)
> > >                    panel.densityplot(x, col = "red", plot.points=F, lwd=2)
> > >                } )
> > >
> > >  If I change type="density" to type="percent" the scales for the
> > > densityplot will be too low and all I see is a horizontal line at zero
> > > (this is as expected) . However, I tried par(new=T) and nothing happens. I
> > > want to be able to put percenstages on axis 2 and density values at axis
> > > 4.
> >
> > I don't think par(new=T) is what you should be looking for (and incidentally,
> > par settings have no effect on lattice plots). It's possible to add axes
> > after the plot (easier than it was before 2.0.0), but the design of lattice
> > doesn't allow you to easily allocate enough space for the second set of axes.
> > You can still do it, but it would be kludgy.
> >
> > Here's an example (you need to know what 'mult' should be -- it's the factor
> > that converts the density scale to the percent scale -- it would depend on
> > the widths of the bins and the length of x):
> >
> >
> > x <- rnorm(200)
> > mult <- 60 ## meaningless in this case
> >
> > histogram(x, type = "percent",
> >           panel = function(x, ...) {
> >               panel.histogram(x, ...)
> >               d <- density(x)
> >               panel.lines(d$x, mult * d$y, col = 'red')
> >           },
> >           scales = list(y = list(tck = c(1, 0))),
> >           par.settings = list(layout.widths = list(right.padding = 5)))
> >
> > trellis.focus("panel", 1, 1, clip.off = TRUE)
> >
> > at <- pretty(c(0, 25)/mult)
> > panel.axis(side = "right",
> >            at = mult * at, labels = at,
> >            outside = TRUE)
> > trellis.unfocus()
> >
> >
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>



From p.dalgaard at biostat.ku.dk  Thu Oct 21 16:08:34 2004
From: p.dalgaard at biostat.ku.dk (Peter Dalgaard)
Date: 21 Oct 2004 16:08:34 +0200
Subject: [R] Matrix/Table col headings R 2.0.0
In-Reply-To: <4177B795.4060203@statistik.uni-dortmund.de>
References: <010601c4b5fb$fbd474c0$3e80010a@BigBaer>
	<4177B795.4060203@statistik.uni-dortmund.de>
Message-ID: <x2y8i0kvtp.fsf@biostat.ku.dk>

Uwe Ligges <ligges at statistik.uni-dortmund.de> writes:

> Robert W. Baer, Ph.D. wrote:

> > Somehow PD got right justified columns with this dataset.  Is there
> > a new way of doing things in version 2.0.0, my ignorance, a bug?  I
> > tried making the columns factors first, but in my hands this did not
> > appear to help either.  Thanks for any insight.
> 
> [Since this question seems to be unanswered:]

[It wasn't...]
 
> This is a question related to Peter Dalgaard's book, so why don't you
> ask the author rather than the whole world?

And make me responsible for tracking all post-publication changes in
R? (Now, what I should have had was regression tests in the ISwR
package failing when script output changed, but, well, "The road to
hell, etc...")
 
> Why does the justification matter here? I'm much too lazy to look for
> changes in R related to your question - but probably you have done
> yourself before asking the question...???

This particular change is very hard to find in NEWS, and even in the
SVN logs. It's a side effect of using format() inside print.table,
conspiring with R's default for string printing. This results in
getting right-justified strings (left-padded with spaces) printed
left-justified so that the names line up with the leading whitespace.
Try

  print(as.table(matrix(c(1,2,1,1e4),2)))


> And why don't you cite the example you are referring to (e.g. by a
> page number!!!) exactly? I'm also too lazy to read Peter's book once
> more just in order to find the example you are talking about!

It actually was a paraphrased example, as those who can grep the TeX
sources found out...

-- 
   O__  ---- Peter Dalgaard             Blegdamsvej 3  
  c/ /'_ --- Dept. of Biostatistics     2200 Cph. N   
 (*) \(*) -- University of Copenhagen   Denmark      Ph: (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)             FAX: (+45) 35327907



From maechler at stat.math.ethz.ch  Thu Oct 21 16:17:13 2004
From: maechler at stat.math.ethz.ch (Martin Maechler)
Date: Thu, 21 Oct 2004 16:17:13 +0200
Subject: [R] Cluster Analysis: Density-Based Method
In-Reply-To: <3A822319EB35174CA3714066D590DCD50994E1E7@usrymx25.merck.com>
References: <3A822319EB35174CA3714066D590DCD50994E1E7@usrymx25.merck.com>
Message-ID: <16759.50409.325192.611692@gargle.gargle.HOWL>

>>>>> "AndyL" == Liaw, Andy <andy_liaw at merck.com>
>>>>>     on Thu, 21 Oct 2004 09:18:54 -0400 writes:

    AndyL> I'm no expert in this, but mclust is `density-based'
    AndyL> because it estimates the density with a mixture of
    AndyL> Gaussians.  If this is not what you want, you should
    AndyL> clarify what you mean by `density-based'.  Do you
    AndyL> mean an algorithm based on kernel estimator of the density?

yes, kernel or other "nonparametric" density estimator, is what
is usually meant in these contexts.
[ Of course, many "nonparametric" estimators can be seen to live
  in finite-dimensional spaces, so the difference to an explicit
 "flexible" / "high dimensional" method isn't that big.. ]

Martin

    >> From: Fernando Prass
    >> 
    >> Yes, but mclust don't have a density-based algorithm. Mclust 
    >> have the algorithm
    >> BIC, that is a model-based method...
    >> 
    >> Fernando Prass
    >> 
    >> --- Kjetil Brinchmann Halvorsen <kjetil at acelerate.com> escreveu: 
    >> > Fernando Prass wrote:
    >> > 
    >> > >Hi people,
    >> > >
    >> > >Does anybody know some Density-Based Method for clustering 
    >> implemented in R?
    >> > >  
    >> > >
    >> > Have you looked at CRAN package mclust?
    >> > 
    >> > >Thanks,
    >> > >
    >> > >Fernando Prass
    >> 
    >> 
    >> 
    >> 
    >> 
    >> 
    >> _______________________________________________________ 
    >> 
    >> ______________________________________________
    >> R-help at stat.math.ethz.ch mailing list
    >> https://stat.ethz.ch/mailman/listinfo/r-help
    >> PLEASE do read the posting guide! 
    >> http://www.R-project.org/posting-guide.html
    >> 
    >> 

    AndyL> ______________________________________________
    AndyL> R-help at stat.math.ethz.ch mailing list
    AndyL> https://stat.ethz.ch/mailman/listinfo/r-help
    AndyL> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html



From MSchwartz at MedAnalytics.com  Thu Oct 21 16:27:40 2004
From: MSchwartz at MedAnalytics.com (Marc Schwartz)
Date: Thu, 21 Oct 2004 09:27:40 -0500
Subject: [R] Arrow heads at the end of axes
In-Reply-To: <cl8flo$75n$1@sea.gmane.org>
References: <cl8952$ino$1@sea.gmane.org>
	<1098362134.2041.4.camel@orion.localdomain>
	<cl8flo$75n$1@sea.gmane.org>
Message-ID: <1098368859.2041.17.camel@orion.localdomain>

On Thu, 2004-10-21 at 09:04, Henrik Andersson wrote:
> Marc Schwartz wrote:
> 
> > On Thu, 2004-10-21 at 07:12, Henrik Andersson wrote:
> > 
> >>I would like to have arrow heads at the end of my axes, since I am 
> >>plotting variable where the absolute amount is irrelevant, there is not 
> >>supposed to be numbers on the axes.
> >>
> ...
> > 
> > 
> > You can do something like this:
> > 
> > plot(rnorm(10), axes = FALSE)
> > 
> > # Get the axis ranges
> > u <- par("usr")
> > 
> > # Use arrows() to draw the axis lines, adding the arrowheads
> > # Use the par("usr") values to specify the end points of the lines
> > # Setting 'xpd = TRUE' allows for the arrowhead to be drawn outside
> > # the plot region as required here
> > arrows(u[1], u[3], u[2], u[3], code = 2, xpd = TRUE)
> > arrows(u[1], u[3], u[1], u[4], code = 2, xpd = TRUE)
> > 
> > See ?arrows for additional options to define the arrows.
> > 
> > HTH,
> > 
> > Marc Schwartz
> 
> Thanks for pointing the direction (seriously), the looks of the 
> arrowheads could still use some improvements, so I used a polygon to 
> draw an arrowhead which thanks to your suggestion was possible outside 
> the plot region.
> 
> Is it my Internet English or normal English that is bad, but was is the 
> expansion of 'HTH' ?
> 
> Cheers, Henrik


Hope That Helps

A common internet acronym that I suppose we take for granted sometimes.

I had also looked at the possible plot symbols as an alternative. You
could use 'pch = 2' (or 17 or 24) for the y axis (an upward pointing
triangle). However, there is really not an equivalent "side pointing"
triangle for the x axis. 

polygon() is certainly an alternative.

Best regards,

Marc



From maechler at stat.math.ethz.ch  Thu Oct 21 16:49:19 2004
From: maechler at stat.math.ethz.ch (Martin Maechler)
Date: Thu, 21 Oct 2004 16:49:19 +0200
Subject: [R] Arrow heads at the end of axes
In-Reply-To: <cl8flo$75n$1@sea.gmane.org>
References: <cl8952$ino$1@sea.gmane.org>
	<1098362134.2041.4.camel@orion.localdomain>
	<cl8flo$75n$1@sea.gmane.org>
Message-ID: <16759.52335.144471.797559@gargle.gargle.HOWL>

>>>>> "Henrik" == Henrik Andersson <h.andersson at nioo.knaw.nl>
>>>>>     on Thu, 21 Oct 2004 16:04:09 +0200 writes:

    Henrik> Marc Schwartz wrote:
    >> On Thu, 2004-10-21 at 07:12, Henrik Andersson wrote:
    >> 
    >>> I would like to have arrow heads at the end of my axes, since I am 
    >>> plotting variable where the absolute amount is irrelevant, there is not 
    >>> supposed to be numbers on the axes.
    >>> 
    Henrik> ...
    >> 
    >> 
    >> You can do something like this:
    >> 
    >> plot(rnorm(10), axes = FALSE)
    >> 
    >> # Get the axis ranges
    >> u <- par("usr")
    >> 
    >> # Use arrows() to draw the axis lines, adding the arrowheads
    >> # Use the par("usr") values to specify the end points of the lines
    >> # Setting 'xpd = TRUE' allows for the arrowhead to be drawn outside
    >> # the plot region as required here
    >> arrows(u[1], u[3], u[2], u[3], code = 2, xpd = TRUE)
    >> arrows(u[1], u[3], u[1], u[4], code = 2, xpd = TRUE)
    >> 
    >> See ?arrows for additional options to define the arrows.
    >> 
    >> HTH,
    >> 
    >> Marc Schwartz

    Henrik> Thanks for pointing the direction (seriously), the
    Henrik> looks of the arrowheads could still use some
    Henrik> improvements, so I used a polygon to draw an
    Henrik> arrowhead which thanks to your suggestion was
    Henrik> possible outside the plot region.

If you are not pleased with the standard arrows(),
I also would like you to note the  p.arrows() function
in CRAN package 'sfsmisc' written by Andreas Ruckstuhl 10 years
ago.  It draws "nicer" (and slower) arrows and allows some
customization of that  [and I see that its help page could use
some retouching..].

    Henrik> Is it my Internet English or normal English that is
    Henrik> bad, but was is the expansion of 'HTH' ?

you can google for "HTH acronym" ... I get it as 1st match already

"Hope that helps" (or "this"),
Martin



From secchi at sssup.it  Thu Oct 21 16:57:56 2004
From: secchi at sssup.it (Angelo Secchi)
Date: Thu, 21 Oct 2004 16:57:56 +0200
Subject: [R] Robust regression with groups
In-Reply-To: <002001c4b756$50cf7460$0540210a@www.domain>
References: <200410201542.i9KFgNdw007037@ohm.gene.com>
	<002001c4b756$50cf7460$0540210a@www.domain>
Message-ID: <20041021165756.1d0811ee.secchi@sssup.it>


Hi,
Bert you are definitely right I've been confuse
and unclear on the nature of my problem (sorry about that).

In my message "robust regression" was referred to techniques able to
deal (when you estimate the variance of your coefficients) with
departures from the set of assumptions in a standard linear regression,
like for example the presence of heteroskedaciticy. In this case the
robust estimator of the variance of \beta (i.e. the coefficients) is
obtained considering a correction that take into account the
contribution from each observation to the score(d(ln L)/d\beta). Now I
would like to consider also the possibility that observations are not
independent as they are but they can be divided into groups that are
independent. In this case to obtain an estimator for the variance
that take into account this departure from the standard assumptions I
need a correction that take into account the contribution of each group
(and not of each observation) to the score(d(ln L)/d\beta). In summary,
I do not need more sophisticated way to estimate my coefficients but
only a routine to obtain a meaningful estimate for the variance of them.
Does this routine already exist in R?

Thanks,
a.

PS Thanks Dimitris but it seems that I cannot use a random effects model
since the Hausmann specification test casts doubt on the assumptions
justifying the use of a GLS estimator.






On Thu, 21 Oct 2004 12:11:22 +0200
"Dimitris Rizopoulos" <dimitris.rizopoulos at med.kuleuven.ac.be> wrote:

> Hi Bert,
> 
> Regarding the sensitivity in the choice of the random-effects 
> distribution, I know that usually estimates of the fixed-effects (and 
> their std.errors) do not have serious problems, even if you assume 
> normality where in fact you have log-normality. However, you do have a
> problem in the EB estimates of the random-effects.
> 
> More info could be found in:
> 
> G. Verbeke and E. Lesaffre (1996). A linear mixed-effects model with 
> heterogeneity in the random-effects population, JASA, 91, 217-221.
> 
> W. Ghidey, E. Lesaffre and P. Eilers (2004). Smooth random-effects 
> distribution in linear mixed model, Biometrics, 60, 945-953.
> (to appear in December)
> 
> Best,
> Dimitris
> 
> ----
> Dimitris Rizopoulos
> Ph.D. Student
> Biostatistical Centre
> School of Public Health
> Catholic University of Leuven
> 
> Address: Kapucijnenvoer 35, Leuven, Belgium
> Tel: +32/16/396887
> Fax: +32/16/337015
> Web: http://www.med.kuleuven.ac.be/biostat/
>      http://www.student.kuleuven.ac.be/~m0390867/dimitris.htm
> 
> 
> ----- Original Message ----- 
> From: "Berton Gunter" <gunter.berton at gene.com>
> To: "'Dimitris Rizopoulos'" <dimitris.rizopoulos at med.kuleuven.ac.be>; 
> "'Angelo Secchi'" <secchi at sssup.it>
> Cc: <r-help at stat.math.ethz.ch>
> Sent: Wednesday, October 20, 2004 5:42 PM
> Subject: RE: [R] Robust regression with groups
> 
> 
> > Angelo and Folks:
> >
> > Beware! It is not at all clear what you mean by "robust" regression.
> > The
> > sandwich estimator is often said to be "robust" to model 
> > misspecification in
> > the sense that it converges to the correct covariance matrix whether
> > or not
> > the correlation structure in the GEE has been correctly specified 
> > (as
> > Dmitris implied). Is this what you mean? Mixed effect models are 
> > often said
> > to be "robust" in the sense that individual group "estimators" 
> > (blups) are
> > shrunk toward the overall fixed effect estimates. Is this what you 
> > mean?
> >
> > In other applications, "robustness" can mean insensitivity to 
> > distributional
> > assumptions. Mixed effects models for continupus responses commonly 
> > assume
> > normality (as the estimates solve likelihood equations), as do 
> > GLMM's for
> > the random effects. I know of no definitive work that has examined
> > sensitivity of estimates (or inferences, which are, at best, 
> > asymptotic
> > anyway) to those assumptions. (in the simple independent errors 
> > case, it is
> > usually the case that estimates are not at all sensitive). However, 
> > I am a
> > novice here, so others may be able to illuminate the issue more.
> >
> > Finally, "robustness" is often used to mean "outlier resistance." 
> > Here the
> > situation is yet murkier. Do you mean resistance to individual 
> > "outlying"
> > observations within a subject or resistance to outlying subjects? 
> > Shrinkage
> > should help with both, but, again, I know of no definitive work, 
> > especially
> > regarding resistance to individual extreme values. Given the 
> > sensitivity of
> > covariance estimates to heavy tails and the consequent inferential
> > inefficiency, this presumably could be a problem. Finding methods 
> > that could
> > deal with this may be nearly impossible, as you are adding yet 
> > another layer
> > of nonlinear estimation (that of determining optimal case 
> > weights/parameters
> > for mixture contamination models/or whatever...) to the problem; it 
> > is easy
> > to come up with examples where the data are inherently ambiguous and
> > parameter estimates for resistant case weights and the model would 
> > trade off
> > with each other depending on starting values. That is, too many 
> > nonlinear
> > parameters are being estimated and the model estimates are therefore
> > unstable.
> >
> > Again, I am happy to leave more definitive resolution and correction
> > of any
> > errors in my comments to the experts, but, at the least, I think you
> > need to
> > think more and communicate more clearly about what you mean by 
> > "robust."
> >
> > Cheers,
> >
> > -- Bert Gunter
> > Genentech Non-Clinical Statistics
> > South San Francisco, CA
> >
> > "The business of the statistician is to catalyze the scientific 
> > learning
> > process."  - George E. P. Box
> >
> >
> >
> >> -----Original Message-----
> >> From: r-help-bounces at stat.math.ethz.ch
> >> [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of
> >> Dimitris Rizopoulos
> >> Sent: Wednesday, October 20, 2004 7:08 AM
> >> To: Angelo Secchi
> >> Cc: r-help at stat.math.ethz.ch
> >> Subject: Re: [R] Robust regression with groups
> >>
> >> Hi Angelo,
> >>
> >> There are two possible options (at least to my knowledge):
> >>
> >> 1. to use a random-effects model, either using `lme' (packages: 
> >> nlme,
> >> lme4) if you have normal data or `glmmPQL' (package: MASS) or 
> >> `GLMM'
> >> (package: lme4) or `glmmML' (package:glmmML) if you cannot use the
> >> normal distribution.
> >>
> >> 2. to use a gee model with a robust (sandwich) std.error 
> >> estimation.
> >> See at `gee' (package: gee) and `geese' (package: geepack).
> >>
> >> I hope this helps.
> >>
> >> Best,
> >> Dimitris
> >>
> >> ----
> >> Dimitris Rizopoulos
> >> Ph.D. Student
> >> Biostatistical Centre
> >> School of Public Health
> >> Catholic University of Leuven
> >>
> >> Address: Kapucijnenvoer 35, Leuven, Belgium
> >> Tel: +32/16/396887
> >> Fax: +32/16/337015
> >> Web: http://www.med.kuleuven.ac.be/biostat/
> >>      http://www.student.kuleuven.ac.be/~m0390867/dimitris.htm
> >>
> >>
> >>
> >>
> >> ----- Original Message ----- 
> >> From: "Angelo Secchi" <secchi at sssup.it>
> >> To: <r-help at stat.math.ethz.ch>
> >> Sent: Wednesday, October 20, 2004 3:22 PM
> >> Subject: [R] Robust regression with groups
> >>
> >>
> >> >
> >> >
> >> > Hi,
> >> > I have data on a group of subjects in different years. I should
> >> > assume
> >> > that observations regarding different individuals are independent
> >> > but
> >> > observations for the same individual in different years are not 
> >> > and
> >> > I
> >> > would like to have an estimated standard error (and
> >> > variance-covariance
> >> > matrix) taking into account this problem.
> >> >
> >> > More in general is there a way in R to run a (robust)regression
> >> > having
> >> > different groups in the observations and specifying that the
> >> > observation
> >> > are independent across groups but not necessarily independent 
> >> > within
> >> > groups?
> >> >
> >> > Thanks
> >> > a.
> >> >
> >> > ______________________________________________
> >> > R-help at stat.math.ethz.ch mailing list
> >> > https://stat.ethz.ch/mailman/listinfo/r-help
> >> > PLEASE do read the posting guide!
> >> > http://www.R-project.org/posting-guide.html
> >> >
> >>
> >> ______________________________________________
> >> R-help at stat.math.ethz.ch mailing list
> >> https://stat.ethz.ch/mailman/listinfo/r-help
> >> PLEASE do read the posting guide!
> >> http://www.R-project.org/posting-guide.html
> >>
> > 
> 


--
========================================================
 Angelo Secchi                     PGP Key ID:EA280337
========================================================
  Current Position:
  Research Fellow Scuola Superiore S.Anna
  Piazza Martiri della Liberta' 33, Pisa, 56127 Italy
  ph.: +39 050 883365
  email: secchi at sssup.it	www.sssup.it/~secchi/



From andy_liaw at merck.com  Thu Oct 21 16:53:59 2004
From: andy_liaw at merck.com (Liaw, Andy)
Date: Thu, 21 Oct 2004 10:53:59 -0400
Subject: [R] Cluster Analysis: Density-Based Method
Message-ID: <3A822319EB35174CA3714066D590DCD50994E1EA@usrymx25.merck.com>

> From: Martin Maechler
> 
> >>>>> "AndyL" == Liaw, Andy <andy_liaw at merck.com>
> >>>>>     on Thu, 21 Oct 2004 09:18:54 -0400 writes:
> 
>     AndyL> I'm no expert in this, but mclust is `density-based'
>     AndyL> because it estimates the density with a mixture of
>     AndyL> Gaussians.  If this is not what you want, you should
>     AndyL> clarify what you mean by `density-based'.  Do you
>     AndyL> mean an algorithm based on kernel estimator of the density?
> 
> yes, kernel or other "nonparametric" density estimator, is what
> is usually meant in these contexts.
> [ Of course, many "nonparametric" estimators can be seen to live
>   in finite-dimensional spaces, so the difference to an explicit
>  "flexible" / "high dimensional" method isn't that big.. ]
>
> Martin


Yes.  However, after reading
ftp://ftp.stat.rice.edu/pub/scottdw/TECH/ipra.ps (David Scott's `From
Kernels to Mixtures' published in Technometrics in 2000, I believe the Tukey
memorial issue) I thought the line between kernel densities and mixture
models is rather gray...

Best,
Andy

>     >> From: Fernando Prass
>     >> 
>     >> Yes, but mclust don't have a density-based algorithm. Mclust 
>     >> have the algorithm
>     >> BIC, that is a model-based method...
>     >> 
>     >> Fernando Prass
>     >> 
>     >> --- Kjetil Brinchmann Halvorsen <kjetil at acelerate.com> 
> escreveu: 
>     >> > Fernando Prass wrote:
>     >> > 
>     >> > >Hi people,
>     >> > >
>     >> > >Does anybody know some Density-Based Method for clustering 
>     >> implemented in R?
>     >> > >  
>     >> > >
>     >> > Have you looked at CRAN package mclust?
>     >> > 
>     >> > >Thanks,
>     >> > >
>     >> > >Fernando Prass
>     >> 
>     >> 
>     >> 
>     >> 
>     >> 
>     >> 
>     >> _______________________________________________________ 
>     >> 
>     >> ______________________________________________
>     >> R-help at stat.math.ethz.ch mailing list
>     >> https://stat.ethz.ch/mailman/listinfo/r-help
>     >> PLEASE do read the posting guide! 
>     >> http://www.R-project.org/posting-guide.html
>     >> 
>     >> 
> 
>     AndyL> ______________________________________________
>     AndyL> R-help at stat.math.ethz.ch mailing list
>     AndyL> https://stat.ethz.ch/mailman/listinfo/r-help
>     AndyL> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
> 
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
> 
>



From GBLEVINS at marketsolutionsgroup.com  Thu Oct 21 16:55:24 2004
From: GBLEVINS at marketsolutionsgroup.com (Greg Blevins)
Date: Thu, 21 Oct 2004 09:55:24 -0500
Subject: [R] Hmisc: Using stratified weighted means (wtd.mean) within a
	function
Message-ID: <s177879f.045@mail.marketsolutionsgroup.com>

Hello list,

I have the following function which, as you can see, uses mean:

meanratings <-  round(apply(stack03[,c(102:121)],2,function(x) (tapply(x ,actcode, mean, na.rm=T))), digits=1)

The above function yields the following output:

   q27a q27b q27c q27d q27e q27f q27g q27h q27i q27j q27k q27l q27m q27o q27p
1   7.8  8.1  7.7  7.9  7.9  NaN  NaN  8.4  7.8  7.0  7.6  NaN  NaN  7.1  6.0
2   8.3  8.6  8.5  8.2  8.3  NaN  NaN  8.8  NaN  7.5  NaN  7.7  7.4  8.1  7.4
3   8.1  8.5  7.7  8.2  7.8  NaN  NaN  8.2  8.3  7.1  7.8  NaN  NaN  7.7  6.3
4   7.9  8.1  7.0  7.6  8.1  8.1  NaN  8.5  NaN  7.2  NaN  NaN  NaN  7.6  7.2
5   7.9  8.2  7.5  7.8  7.8  8.2  NaN  8.5  NaN  7.3  NaN  NaN  NaN  8.0  7.2
6   7.6  8.2  7.2  7.2  7.2  8.1  NaN  8.1  8.2  6.7  NaN  7.8  NaN  7.5  6.9
7   8.2  8.3  7.5  8.2  7.2  7.9  NaN  8.2  NaN  7.2  NaN  NaN  NaN  8.0  6.7
8   7.3  7.5  7.1  7.0  6.6  8.0  7.3  7.7  NaN  6.5  NaN  NaN  NaN  6.8  7.1
9   7.6  7.9  7.6  7.1  7.2  NaN  NaN  7.9  8.6  7.7  NaN  7.9  NaN  7.2  7.1
10  8.2  8.2  8.1  8.0  7.5  NaN  NaN  8.2  NaN  7.5  NaN  NaN  NaN  7.5  7.0   etc.

For my current situation, I need to have this function use a weighted mean and I am trying to incorporate the 'wtd.mean' function from Hmisc.  I have tried various scenarios relying on the documentation for 'wtd.mean' in the Hmisc (shown below), but to no avail.  Any assistance would be much appreciated.

set.seed(1)
x <- runif(500)
wts <- sample(1:6, 500, TRUE)
xg <- cut2(x,g=4)
# Here is a method for getting stratified weighted means
y <- runif(500)
g <- function(y) wtd.mean(y[,1],y[,2])
summarize(cbind(y, wts), llist(xg), g, stat.name='y')


Greg Blevins
The Market Solutions Group
Windows XP, 512 memory, Pentium 4.



From rbaer at atsu.edu  Thu Oct 21 17:06:28 2004
From: rbaer at atsu.edu (Robert W. Baer, Ph.D.)
Date: Thu, 21 Oct 2004 10:06:28 -0500
Subject: [R] Matrix/Table col headings R 2.0.0
References: <010601c4b5fb$fbd474c0$3e80010a@BigBaer>
	<4177B795.4060203@statistik.uni-dortmund.de>
Message-ID: <001501c4b77f$8a6b6480$3e80010a@BigBaer>

Uwe Ligges <ligges at statistik.uni-dortmund.de> writes:
> Somehow PD got right justified columns with this dataset.  Is there a new
way of doing things in version 2.0.0, my ignorance, a bug?  I tried making
the columns factors first, but in my hands this did not appear to help
either.  Thanks for any insight.
>
> [Since this question seems to be unanswered:]

Peter provided a rapid and useful answer on this list.   The answer included
a formating work-around, and the whole process used very little 'list
bandwidth'.

> This is a question related to Peter Dalgaard's book, so why don't you
> ask the author rather than the whole world?
It was not question about his book.  It was about seeming change to the
behavior of R (for the worse in my mind).  I simply paraphrased his code for
my working example and wanted to be sure to credit where due.

Let me end this with a heart-felt thanks to you, to Peter, and to the many
other R experts who regularly teach me things by answering questions like
mine on this list.

Rob Baer



From wss.wjs at msa.hinet.net  Thu Oct 21 17:26:02 2004
From: wss.wjs at msa.hinet.net (Cindy)
Date: Thu, 21 Oct 2004 23:26:02 +0800
Subject: [R] I have a question
Message-ID: <000801c4b782$475b1700$9d68fea9@wjs>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20041021/ec0fc3b7/attachment.pl

From tlumley at u.washington.edu  Thu Oct 21 17:23:26 2004
From: tlumley at u.washington.edu (Thomas Lumley)
Date: Thu, 21 Oct 2004 08:23:26 -0700 (PDT)
Subject: [R] inverse gaussian distribution of frailty variable
In-Reply-To: <20041021123101.17071.qmail@community22.interfree.it>
References: <20041021123101.17071.qmail@community22.interfree.it>
Message-ID: <Pine.A41.4.61b.0410210823100.121554@homer06.u.washington.edu>

On Thu, 21 Oct 2004 emanuela001 at interfree.it wrote:

>
> Hello,
>
> I'm Emanuela, I'm implemented a survival analysis and I'm trying to use 
> a frailty model with inverse gaussian distribution, but I'm not able to 
> find the right code, because it seems to be only a gamma and a gaussian 
> distribution. Is there also the inverse gaussian distribution?

Not in the survival package.

 	-thomas



From bsl8096 at liverpool.ac.uk  Thu Oct 21 17:32:16 2004
From: bsl8096 at liverpool.ac.uk (Brian Lane)
Date: Thu, 21 Oct 2004 16:32:16 +0100
Subject: [R] Limma B-statistics
Message-ID: <2723CF02609F531A6126B2DD@182105-93607r.liv.ac.uk>

Hi,
I need some help with the interpretation of B statistics generated by 
eBayes in the limma package.

I want to compare gene expression in three groups of Affy samples. The 
expression set has been imported from GeneSpring using GSload.expBC and a 
linear model fitted to the data using a design based on the three groups 
(6, 5, and 5 samples in each group, respectively). I have then made 3 
contrasts to cover all possible comparisons within the data set, and 
generated empirical Bayes statistics using eBayes. I've then used 
classifyTestsF to classify each gene according to the contrasts.

The results of all this are 23 significantly differentially expressed 
genes. The moderated t-values for all these 23 genes are <0.01. However, 
all the B-values are <0 (average -3!). In fact, a volcano plot of log-odds 
and fold-change in the three contrasts show that all the B-values are 
negative.

My understanding is that B<0 implies the gene is more likely to not be 
differentially expressed than to be differentially expressed. If this is 
the case, should I take the "significant genes" seriously? If not, is there 
any reason why the B-values should all be negative or does this simply 
reflect the fact that there is little evidence of differential expression 
in the data set as a whole?

Regards,
Brian Lane
Dept of Haematology
Liverpool University



From gunter.berton at gene.com  Thu Oct 21 18:28:10 2004
From: gunter.berton at gene.com (Berton Gunter)
Date: Thu, 21 Oct 2004 09:28:10 -0700
Subject: [R] Robust regression with groups
In-Reply-To: <20041021165756.1d0811ee.secchi@sssup.it>
Message-ID: <200410211628.i9LGSATq005324@hertz.gene.com>

Angelo:

If I understand you correctly, what you want is exactly the mixed effects
model that Dmitris has already suggested. As you appear to be confused about
the underlying statistical concepts, I suggest that you read at least the
first and fourth chapters of MIXED-EFFECTS MODELS IN S AND S-PLUS by Bates
and Pinheiro. Chapter 10 of MASS (4th Edition) by Venables and Ripley (which
I would unequivocally say should be on every S language user's shelf)
contains a much terser overview, but consequently requires a stronger
statistical background to understand.

My apologies if I have misunderstood, but the references are good ones
anyway.

Cheers,
Bert

-- Bert Gunter
Genentech Non-Clinical Statistics
South San Francisco, CA
 
"The business of the statistician is to catalyze the scientific learning
process."  - George E. P. Box
 
 

> -----Original Message-----
> From: r-help-bounces at stat.math.ethz.ch 
> [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Angelo Secchi
> Sent: Thursday, October 21, 2004 7:58 AM
> To: r-help at stat.math.ethz.ch
> Subject: Re: [R] Robust regression with groups
> 
> 
> Hi,
> Bert you are definitely right I've been confuse
> and unclear on the nature of my problem (sorry about that).
> 
> In my message "robust regression" was referred to techniques able to
> deal (when you estimate the variance of your coefficients) with
> departures from the set of assumptions in a standard linear 
> regression,
> like for example the presence of heteroskedaciticy. In this case the
> robust estimator of the variance of \beta (i.e. the coefficients) is
> obtained considering a correction that take into account the
> contribution from each observation to the score(d(ln L)/d\beta). Now I
> would like to consider also the possibility that observations are not
> independent as they are but they can be divided into groups that are
> independent. In this case to obtain an estimator for the variance
> that take into account this departure from the standard assumptions I
> need a correction that take into account the contribution of 
> each group
> (and not of each observation) to the score(d(ln L)/d\beta). 
> In summary,
> I do not need more sophisticated way to estimate my coefficients but
> only a routine to obtain a meaningful estimate for the 
> variance of them.
> Does this routine already exist in R?
> 
> Thanks,
> a.
> 
> PS Thanks Dimitris but it seems that I cannot use a random 
> effects model
> since the Hausmann specification test casts doubt on the assumptions
> justifying the use of a GLS estimator.
> 
> 
> 
>



From francoisromain at free.fr  Thu Oct 21 18:40:29 2004
From: francoisromain at free.fr (=?ISO-8859-1?Q?Romain_Fran=E7ois?=)
Date: Thu, 21 Oct 2004 18:40:29 +0200
Subject: [R] an introduction to R in french
Message-ID: <4177E67D.1070501@free.fr>

Hello wizaRds !

I am looking for a french and recent version of "An introduction to R". 
Does anybody know where i could find on of these.

Thanks.

-- 
Romain Fran??ois
25, avenue Guy Moquet
94 400 Vitry sur seine
FRANCE
_______________________
_______________________

francoisromain at free.fr
01 46 80 65 60
06 18 39 14 69



From bates at wisc.edu  Thu Oct 21 18:57:22 2004
From: bates at wisc.edu (Douglas Bates)
Date: Thu, 21 Oct 2004 11:57:22 -0500
Subject: [R] Schur decomposition? 
In-Reply-To: <417551E6.5090105@pdf.com>
References: <a06002006bd9a7b5ffb79@[130.120.104.141]>
	<4175083B.6090605@stat.wisc.edu> <417551E6.5090105@pdf.com>
Message-ID: <4177EA72.9000501@wisc.edu>

Spencer Graves wrote:
>      Does R have a function for the Schur decomposition?  The 
> documentation for library(Matrix) describes a function "Schur", but it 
> seems to be missing from the Windows version 0.8-14 (2004-09-14) and 
> 0.8-15 (2004-10-02).
>      The R 2.0.0 pat documentation for "eigen" refers to 
> "http://www.netlib.org/lapack/lug/lapack_lug.html", and the description 
> there for eigen analysis of a non-symmetric matrix says, "This problem 
> can be solved via the Schur factorization of A, defined in the real case as
> A = ZTZT,
> 
> where Z is an orthogonal matrix and T is an upper quasi-triangular 
> matrix with 1-by-1 and 2-by-2 diagonal blocks, the 2-by-2 blocks 
> corresponding to complex conjugate pairs of eigenvalues of A."
>      Thanks,
>      Spencer Graves

That documentation entry was a note to myself to add the Schur 
factorization to the Matrix package but I have not yet done so.  If 
anyone can suggest a reasonable data structure for the result, it would 
be fairly easy to add the Schur decomposition.



From spencer.graves at pdf.com  Thu Oct 21 18:57:43 2004
From: spencer.graves at pdf.com (Spencer Graves)
Date: Thu, 21 Oct 2004 09:57:43 -0700
Subject: [R] Robust regression with groups
In-Reply-To: <200410211628.i9LGSATq005324@hertz.gene.com>
References: <200410211628.i9LGSATq005324@hertz.gene.com>
Message-ID: <4177EA87.8020303@pdf.com>

Hi, Angelo: 

      Have you plotted the data in creative ways, e.g., normal 
probability plots and plots vs. time with a separate line for each 
subject and with separate line types colors and plotting symbols for the 
different experimental / treatment groups?  [If the response variable(s) 
are all positive, I would also try the same thing using log="y".  If the 
responses were percentages, I'd transform to empirical logits 
log(y/(1-y)) with some adjustment to "y" to shrink it away from 0 and 
1.]  I always want to do the simple things first.  Plots like this too 
often show me that my favorite model is not appropriate.  I've sometimes 
skipped this step only to be forced back to it after getting nonsense 
fits.  The Gods may have smiled upon Pygmalion, turning his beloved 
creation into a flesh and blood woman.  I more often encounter the 
"great tragedy of science:  a beautiful theory slain by an ugly fact." 

      If these plot do NOT show wild outliers, then I would think that 
"lme" would be precisely what you want, as Bert suggested.  Years ago 
George Box said he thought that unmodelled autocorrelation was harder to 
detect and potentially more damaging than nonnormality.  He said 
something to the effect, "Why worry about mice when there are tigers 
about?" 

      hope this helps.  spencer graves

Berton Gunter wrote:

>Angelo:
>
>If I understand you correctly, what you want is exactly the mixed effects
>model that Dmitris has already suggested. As you appear to be confused about
>the underlying statistical concepts, I suggest that you read at least the
>first and fourth chapters of MIXED-EFFECTS MODELS IN S AND S-PLUS by Bates
>and Pinheiro. Chapter 10 of MASS (4th Edition) by Venables and Ripley (which
>I would unequivocally say should be on every S language user's shelf)
>contains a much terser overview, but consequently requires a stronger
>statistical background to understand.
>
>My apologies if I have misunderstood, but the references are good ones
>anyway.
>
>Cheers,
>Bert
>
>-- Bert Gunter
>Genentech Non-Clinical Statistics
>South San Francisco, CA
> 
>"The business of the statistician is to catalyze the scientific learning
>process."  - George E. P. Box
> 
> 
>
>  
>
>>-----Original Message-----
>>From: r-help-bounces at stat.math.ethz.ch 
>>[mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Angelo Secchi
>>Sent: Thursday, October 21, 2004 7:58 AM
>>To: r-help at stat.math.ethz.ch
>>Subject: Re: [R] Robust regression with groups
>>
>>
>>Hi,
>>Bert you are definitely right I've been confuse
>>and unclear on the nature of my problem (sorry about that).
>>
>>In my message "robust regression" was referred to techniques able to
>>deal (when you estimate the variance of your coefficients) with
>>departures from the set of assumptions in a standard linear 
>>regression,
>>like for example the presence of heteroskedaciticy. In this case the
>>robust estimator of the variance of \beta (i.e. the coefficients) is
>>obtained considering a correction that take into account the
>>contribution from each observation to the score(d(ln L)/d\beta). Now I
>>would like to consider also the possibility that observations are not
>>independent as they are but they can be divided into groups that are
>>independent. In this case to obtain an estimator for the variance
>>that take into account this departure from the standard assumptions I
>>need a correction that take into account the contribution of 
>>each group
>>(and not of each observation) to the score(d(ln L)/d\beta). 
>>In summary,
>>I do not need more sophisticated way to estimate my coefficients but
>>only a routine to obtain a meaningful estimate for the 
>>variance of them.
>>Does this routine already exist in R?
>>
>>Thanks,
>>a.
>>
>>PS Thanks Dimitris but it seems that I cannot use a random 
>>effects model
>>since the Hausmann specification test casts doubt on the assumptions
>>justifying the use of a GLS estimator.
>>
>>
>>
>>
>>    
>>
>
>______________________________________________
>R-help at stat.math.ethz.ch mailing list
>https://stat.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>  
>

-- 
Spencer Graves, PhD, Senior Development Engineer
O:  (408)938-4420;  mobile:  (408)655-4567



From NordlDJ at dshs.wa.gov  Thu Oct 21 19:04:52 2004
From: NordlDJ at dshs.wa.gov (Nordlund, Dan)
Date: Thu, 21 Oct 2004 10:04:52 -0700
Subject: [R] an introduction to R in french
Message-ID: <592E8923DB6EA348BE8E33FCAADEFFFC0A648B@dshs-exch2>

Here is a website that was mentioned on this list some time ago:

http://zoonek2.free.fr/UNIX/48_R/all.html

In addition there is a French introduction on CRAN in the contributed
documentation area.

Daniel J. Nordlund
Research and Data Analysis
Washington State Department of Social and Health Services
Olympia, WA  98504-5204


-----Original Message-----
From: Romain Fran??ois [mailto:francoisromain at free.fr] 
Sent: Thursday, October 21, 2004 9:40 AM
To: RHELP
Subject: [R] an introduction to R in french

Hello wizaRds !

I am looking for a french and recent version of "An introduction to R". 
Does anybody know where i could find on of these.

Thanks.

-- 
Romain Fran??ois
25, avenue Guy Moquet
94 400 Vitry sur seine
FRANCE
_______________________
_______________________

francoisromain at free.fr
01 46 80 65 60
06 18 39 14 69

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide!
http://www.R-project.org/posting-guide.html



From bates at wisc.edu  Thu Oct 21 19:12:29 2004
From: bates at wisc.edu (Douglas Bates)
Date: Thu, 21 Oct 2004 12:12:29 -0500
Subject: [R] matrix of eigenvalues
In-Reply-To: <4176A133.2060103@pdf.com>
References: <a06002006bd9a7b5ffb79@[130.120.104.141]>
	<4175083B.6090605@stat.wisc.edu>
	<a0600200bbd9c1ce167c6@[130.120.104.141]>
	<4176A133.2060103@pdf.com>
Message-ID: <4177EDFD.6050408@wisc.edu>

Spencer Graves wrote:
>      I think you need the Schur decomposition, which seems currently not 
> to be available in R.  The documentation for the the Matrix package 
> describes a "Schur" function, but it's not available in the current 
> Matrix package, as mentioned in my post on this issue yesterday (subj:  
> Schur decomposition).
>      Moreover, Lindsey's mexp in rmutils won't work either:
>  > A = matrix(cbind(c(-1,1),c(-4,3)),nrow=2)
>  > mexp(A)
> Error in solve.default(z$vectors) : system is computationally singular: 
> reciprocal condition number = 4.13756e-017
>  > mexp(A, "series")
> Error in t * x : non-numeric argument to binary operator
>  >
>      Have you considered adding a little noise: > mexp(A+1e-6*rnorm(4))
>          [,1]       [,2]
> [1,] -2.718284 -10.873139
> [2,]  2.718284   8.154859
>  > mexp(A+1e-6*rnorm(4))
>                        [,1]                     [,2]
> [1,] -2.718284-1.060041e-12i -10.873126-1.575184e-12i
> [2,]  2.718284+7.492895e-13i   8.154846+1.578515e-12i
>  > mexp(A+1e-6*rnorm(4))
>                        [,1]                     [,2]
> [1,] -2.718284+6.146195e-13i -10.873127+1.586731e-12i
> [2,]  2.718284-4.004574e-13i   8.154847-8.515411e-13i
>  > mexp(A+1e-6*rnorm(4))
>                        [,1]                     [,2]
> [1,] -2.718283+3.077538e-13i -10.873130+2.433609e-13i
> [2,]  2.718283-3.197442e-14i   8.154847-2.782219e-13i
>  >
>      hope this helps.  spencer graves
> (I believe this is described in one of Richard Bellman's matrix analysis 
> book.)

I rather suspected that that original question was about the matrix 
exponential and linear systems of differential equations.  The solution 
to such a system can only be written using the matrix exponential for 
diagonalizable systems and this is the classic example of a 
nondiagonalizable system.

Calculation of the matrix exponential is an operation that seems 
straightforward in theory and can be very difficult in practice.  Moler 
and van Loan have a classic paper on "Nineteen Dubious Ways to Calculate 
the Matrix Exponential" which I would recommend reading.

In the proposal for updated LAPACK/ScaLAPACK libraries, Demmel and 
Dongarra state that they will include code for the matrix exponential in 
their proposed package.



From gunter.berton at gene.com  Thu Oct 21 19:13:28 2004
From: gunter.berton at gene.com (Berton Gunter)
Date: Thu, 21 Oct 2004 10:13:28 -0700
Subject: [R] Robust regression with groups
In-Reply-To: <4177EA87.8020303@pdf.com>
Message-ID: <200410211713.i9LHDSho018332@volta.gene.com>


Quick addendum to Spencer's excellent advice: many of the plots he suggests
can be more or less automatically made by converting your data to an
appropriate groupedData object and invoking the relevant plot() methods on
them. Chapter 3 of Bates and Pinheiro provides details and numerous examples
as does the MASS chapter.


-- Bert Gunter
Genentech Non-Clinical Statistics
South San Francisco, CA
 
"The business of the statistician is to catalyze the scientific learning
process."  - George E. P. Box
 
 

> -----Original Message-----
> From: Spencer Graves [mailto:spencer.graves at pdf.com] 
> Sent: Thursday, October 21, 2004 9:58 AM
> To: Berton Gunter
> Cc: 'Angelo Secchi'; r-help at stat.math.ethz.ch
> Subject: Re: [R] Robust regression with groups
> 
> Hi, Angelo: 
> 
>       Have you plotted the data in creative ways, e.g., normal 
> probability plots and plots vs. time with a separate line for each 
> subject and with separate line types colors and plotting 
> symbols for the 
> different experimental / treatment groups?  [If the response 
> variable(s) 
> are all positive, I would also try the same thing using 
> log="y".  If the 
> responses were percentages, I'd transform to empirical logits 
> log(y/(1-y)) with some adjustment to "y" to shrink it away from 0 and 
> 1.]  I always want to do the simple things first.  Plots like 
> this too 
> often show me that my favorite model is not appropriate.  
> I've sometimes 
> skipped this step only to be forced back to it after getting nonsense 
> fits.  The Gods may have smiled upon Pygmalion, turning his beloved 
> creation into a flesh and blood woman.  I more often encounter the 
> "great tragedy of science:  a beautiful theory slain by an 
> ugly fact." 
> 
>       If these plot do NOT show wild outliers, then I would 
> think that 
> "lme" would be precisely what you want, as Bert suggested.  Years ago 
> George Box said he thought that unmodelled autocorrelation 
> was harder to 
> detect and potentially more damaging than nonnormality.  He said 
> something to the effect, "Why worry about mice when there are tigers 
> about?" 
> 
>       hope this helps.  spencer graves



From pierre.bady at univ-lyon1.fr  Thu Oct 21 19:35:45 2004
From: pierre.bady at univ-lyon1.fr (Pierre BADY)
Date: Thu, 21 Oct 2004 19:35:45 +0200
Subject: [R] an introduction to R in french
In-Reply-To: <4177E67D.1070501@free.fr>
Message-ID: <5.1.0.14.2.20041021193518.01bceef8@pop.univ-lyon1.fr>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20041021/3a78247d/attachment.pl

From andy_liaw at merck.com  Thu Oct 21 19:43:50 2004
From: andy_liaw at merck.com (Liaw, Andy)
Date: Thu, 21 Oct 2004 13:43:50 -0400
Subject: [R] densityplot and histogram
Message-ID: <3A822319EB35174CA3714066D590DCD50994E1F4@usrymx25.merck.com>

> From: Jean Eid
> 
> I figured out both questions,.. The idea is to set them with
> trellis.par.set(). However, I still would like to know if there is a
> tutorial for lattice package.
> 
> Jean,

Even though it's mostly about Trellis in S, the documents on
http://netlib.bell-labs.com/cm/ms/departments/sia/project/trellis/, I
believe, are largely applicable to lattice as well, thanks to Deepayan's
hard work to keep lattice largely compatible with trellis.

HTH,
Andy



From spencer.graves at pdf.com  Thu Oct 21 20:26:01 2004
From: spencer.graves at pdf.com (Spencer Graves)
Date: Thu, 21 Oct 2004 11:26:01 -0700
Subject: [R] Schur decomposition?
In-Reply-To: <4177EA72.9000501@wisc.edu>
References: <a06002006bd9a7b5ffb79@[130.120.104.141]>	<4175083B.6090605@stat.wisc.edu>
	<417551E6.5090105@pdf.com> <4177EA72.9000501@wisc.edu>
Message-ID: <4177FF39.5080301@pdf.com>

Hi, Doug: 

      What about returning list(T., Z) or list(quT, Z), where T. or quT 
is quasi-upper triangular and Z is orthogonal, consistent with the 
documentation?  This would make it easier for people like me to build an 
intuition for its properties by playing with it.  If it finds 
substantial use, someone may later suggest a special storage format for 
the quasi-upper-triangular part. 

      Thank for your work on this. 
      Spencer Graves

Douglas Bates wrote:

> Spencer Graves wrote:
>
>>      Does R have a function for the Schur decomposition?  The 
>> documentation for library(Matrix) describes a function "Schur", but 
>> it seems to be missing from the Windows version 0.8-14 (2004-09-14) 
>> and 0.8-15 (2004-10-02).
>>      The R 2.0.0 pat documentation for "eigen" refers to 
>> "http://www.netlib.org/lapack/lug/lapack_lug.html", and the 
>> description there for eigen analysis of a non-symmetric matrix says, 
>> "This problem can be solved via the Schur factorization of A, defined 
>> in the real case as
>> A = ZTZT,
>>
>> where Z is an orthogonal matrix and T is an upper quasi-triangular 
>> matrix with 1-by-1 and 2-by-2 diagonal blocks, the 2-by-2 blocks 
>> corresponding to complex conjugate pairs of eigenvalues of A."
>>      Thanks,
>>      Spencer Graves
>
>
> That documentation entry was a note to myself to add the Schur 
> factorization to the Matrix package but I have not yet done so.  If 
> anyone can suggest a reasonable data structure for the result, it 
> would be fairly easy to add the Schur decomposition.
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
>

-- 
Spencer Graves, PhD, Senior Development Engineer
O:  (408)938-4420;  mobile:  (408)655-4567



From fprass at yahoo.com.br  Thu Oct 21 20:27:45 2004
From: fprass at yahoo.com.br (Fernando Prass)
Date: Thu, 21 Oct 2004 15:27:45 -0300 (ART)
Subject: [R] Cluster Analysis: Density-Based Method
Message-ID: <20041021182745.65620.qmail@web42004.mail.yahoo.com>

Andy,

I can be wrong, I'm no expert too, but density estimation is different of
density-model. MClust is a model-basead method because use model statistics
from clustering data (more information in
ftp://ftp.u.washington.edu/public/mclust/tr415R.pdf).

I need some package that implement algorithms like OPTICIS, DBSCAN or
DENCLUE...

Fernando Prass

 
>  --- "Liaw, Andy" <andy_liaw at merck.com> escreveu: 
> > I'm no expert in this, but mclust is `density-based' because it estimates
> > the density with a mixture of Gaussians.  If this is not what you want, you
> > should clarify what you mean by `density-based'.  Do you mean an algorithm
> > based on kernel estimator of the density?
> > 
> > Andy
> > 
> > > From: Fernando Prass
> > > 
> > > Yes, but mclust don't have a density-based algorithm. Mclust 
> > > have the algorithm
> > > BIC, that is a model-based method...
> > > 
> > > Fernando Prass
> > >



From jost at cict.fr  Thu Oct 21 20:30:34 2004
From: jost at cict.fr (Christian Jost)
Date: Thu, 21 Oct 2004 20:30:34 +0200
Subject: [R] matrix of eigenvalues
In-Reply-To: <4177EDFD.6050408@wisc.edu>
References: <a06002006bd9a7b5ffb79@[130.120.104.141]>
	<4175083B.6090605@stat.wisc.edu>
	<a0600200bbd9c1ce167c6@[130.120.104.141]> <4176A133.2060103@pdf.com>
	<4177EDFD.6050408@wisc.edu>
Message-ID: <a0600201cbd9dae9ef5c8@[130.120.104.141]>

At 12:12 -0500 21/10/04, Douglas Bates wrote:
>Spencer Graves wrote:
>>      I think you need the Schur decomposition, which seems 
>>currently not to be available in R.  The documentation for the the 
>>Matrix package describes a "Schur" function, but it's not available 
>>in the current Matrix package, as mentioned in my post on this 
>>issue yesterday (subj:  Schur decomposition).
>>      Moreover, Lindsey's mexp in rmutils won't work either:
>>  > A = matrix(cbind(c(-1,1),c(-4,3)),nrow=2)
>>  > mexp(A)
>>Error in solve.default(z$vectors) : system is computationally 
>>singular: reciprocal condition number = 4.13756e-017
>>  > mexp(A, "series")
>>Error in t * x : non-numeric argument to binary operator
>>  >
>>      Have you considered adding a little noise: > mexp(A+1e-6*rnorm(4))
>>          [,1]       [,2]
>>[1,] -2.718284 -10.873139
>>[2,]  2.718284   8.154859
>>  > mexp(A+1e-6*rnorm(4))
>>                        [,1]                     [,2]
>>[1,] -2.718284-1.060041e-12i -10.873126-1.575184e-12i
>>[2,]  2.718284+7.492895e-13i   8.154846+1.578515e-12i
>>  > mexp(A+1e-6*rnorm(4))
>>                        [,1]                     [,2]
>>[1,] -2.718284+6.146195e-13i -10.873127+1.586731e-12i
>>[2,]  2.718284-4.004574e-13i   8.154847-8.515411e-13i
>>  > mexp(A+1e-6*rnorm(4))
>>                        [,1]                     [,2]
>>[1,] -2.718283+3.077538e-13i -10.873130+2.433609e-13i
>>[2,]  2.718283-3.197442e-14i   8.154847-2.782219e-13i
>>  >
>>      hope this helps.  spencer graves
>>(I believe this is described in one of Richard Bellman's matrix 
>>analysis book.)
>
>I rather suspected that that original question was about the matrix 
>exponential and linear systems of differential equations.  The 
>solution to such a system can only be written using the matrix 
>exponential for diagonalizable systems and this is the classic 
>example of a nondiagonalizable system.
>
>Calculation of the matrix exponential is an operation that seems 
>straightforward in theory and can be very difficult in practice. 
>Moler and van Loan have a classic paper on "Nineteen Dubious Ways to 
>Calculate the Matrix Exponential" which I would recommend reading.

interesting, I would not have suspected such difficulties based on 
the technical definition of the matrix exponential I know
exp(A) = Identity + A + A^2/2! + A^3/3! ...
which does not at all require A to be diagonalizable.

Now, the original question was indeed how to solve numerically a 
linear ODE system with non-diagonalizable matrix. The example I gave 
in another reply suggests that this could be done by taking all 
linearly independent eigenvectors, complement them to a full base P, 
then compute D=invP %*% A %*% P (upper triangular matrix), decompose 
it into a diagonal matrix + an upper triangular matrix with 0 on the 
diagonal for which exp(D) would be rather straightforward to compute 
as long as the system is not too large.

What I do not know is whether the complementing to a full base is 
feasible. Any idea?

Thanks, Christian.

-- 
***********************************************************
http://cognition.ups-tlse.fr/vas-y.php?id=chj  jost at cict.fr
Christian Jost                                   (PhD, MdC)
Centre de Recherches sur la Cognition Animale
Universite Paul Sabatier, Bat IV R3
118 route de Narbonne
31062 Toulouse cedex 4, France
Tel: +33 5 61 55 64 37   Fax: +33 5 61 55 61 54



From spencer.graves at pdf.com  Thu Oct 21 20:46:12 2004
From: spencer.graves at pdf.com (Spencer Graves)
Date: Thu, 21 Oct 2004 11:46:12 -0700
Subject: [R] matrix of eigenvalues
In-Reply-To: <a0600201cbd9dae9ef5c8@[130.120.104.141]>
References: <a06002006bd9a7b5ffb79@[130.120.104.141]>
	<4175083B.6090605@stat.wisc.edu>
	<a0600200bbd9c1ce167c6@[130.120.104.141]>
	<4176A133.2060103@pdf.com> <4177EDFD.6050408@wisc.edu>
	<a0600201cbd9dae9ef5c8@[130.120.104.141]>
Message-ID: <417803F4.7080507@pdf.com>

      That sounds like what the default method of "mexp" does, though I 
haven't checked it to be sure.  That works fine if the eigenvalue part 
of the Jordan canonical form is diagonal.  That does not hold for your 
examples.  However, the theorem I mentioned from one of Bellman's books 
says that any matrix can be approximated arbitrarily closely with 
another matrix with unique eigenvalues, for which the default method of 
"mexp" seems to work.  For more, see the classic paper by Moler and van 
Loan that Doug mentioned on "Nineteen Dubious Ways to Calculate the 
Matrix Exponential". 

      hope this helps. 
      spencer graves

Christian Jost wrote:

> At 12:12 -0500 21/10/04, Douglas Bates wrote:
>
>> Spencer Graves wrote:
>>
>>>      I think you need the Schur decomposition, which seems currently 
>>> not to be available in R.  The documentation for the the Matrix 
>>> package describes a "Schur" function, but it's not available in the 
>>> current Matrix package, as mentioned in my post on this issue 
>>> yesterday (subj:  Schur decomposition).
>>>      Moreover, Lindsey's mexp in rmutils won't work either:
>>>  > A = matrix(cbind(c(-1,1),c(-4,3)),nrow=2)
>>>  > mexp(A)
>>> Error in solve.default(z$vectors) : system is computationally 
>>> singular: reciprocal condition number = 4.13756e-017
>>>  > mexp(A, "series")
>>> Error in t * x : non-numeric argument to binary operator
>>>  >
>>>      Have you considered adding a little noise: > mexp(A+1e-6*rnorm(4))
>>>          [,1]       [,2]
>>> [1,] -2.718284 -10.873139
>>> [2,]  2.718284   8.154859
>>>  > mexp(A+1e-6*rnorm(4))
>>>                        [,1]                     [,2]
>>> [1,] -2.718284-1.060041e-12i -10.873126-1.575184e-12i
>>> [2,]  2.718284+7.492895e-13i   8.154846+1.578515e-12i
>>>  > mexp(A+1e-6*rnorm(4))
>>>                        [,1]                     [,2]
>>> [1,] -2.718284+6.146195e-13i -10.873127+1.586731e-12i
>>> [2,]  2.718284-4.004574e-13i   8.154847-8.515411e-13i
>>>  > mexp(A+1e-6*rnorm(4))
>>>                        [,1]                     [,2]
>>> [1,] -2.718283+3.077538e-13i -10.873130+2.433609e-13i
>>> [2,]  2.718283-3.197442e-14i   8.154847-2.782219e-13i
>>>  >
>>>      hope this helps.  spencer graves
>>> (I believe this is described in one of Richard Bellman's matrix 
>>> analysis book.)
>>
>>
>> I rather suspected that that original question was about the matrix 
>> exponential and linear systems of differential equations.  The 
>> solution to such a system can only be written using the matrix 
>> exponential for diagonalizable systems and this is the classic 
>> example of a nondiagonalizable system.
>>
>> Calculation of the matrix exponential is an operation that seems 
>> straightforward in theory and can be very difficult in practice. 
>> Moler and van Loan have a classic paper on "Nineteen Dubious Ways to 
>> Calculate the Matrix Exponential" which I would recommend reading.
>
>
> interesting, I would not have suspected such difficulties based on the 
> technical definition of the matrix exponential I know
> exp(A) = Identity + A + A^2/2! + A^3/3! ...
> which does not at all require A to be diagonalizable.
>
> Now, the original question was indeed how to solve numerically a 
> linear ODE system with non-diagonalizable matrix. The example I gave 
> in another reply suggests that this could be done by taking all 
> linearly independent eigenvectors, complement them to a full base P, 
> then compute D=invP %*% A %*% P (upper triangular matrix), decompose 
> it into a diagonal matrix + an upper triangular matrix with 0 on the 
> diagonal for which exp(D) would be rather straightforward to compute 
> as long as the system is not too large.
>
> What I do not know is whether the complementing to a full base is 
> feasible. Any idea?
>
> Thanks, Christian.
>

-- 
Spencer Graves, PhD, Senior Development Engineer
O:  (408)938-4420;  mobile:  (408)655-4567



From fprass at yahoo.com.br  Thu Oct 21 20:50:36 2004
From: fprass at yahoo.com.br (Fernando Prass)
Date: Thu, 21 Oct 2004 15:50:36 -0300 (ART)
Subject: [R] Cluster Analysis: Density-Based Method
In-Reply-To: <Pine.GSO.3.95q.1041021153410.5006I-100000@sun11.math.uni-hamburg.de>
Message-ID: <20041021185036.72069.qmail@web42008.mail.yahoo.com>

Dear Christian,

Do you have DBSCAN algorithm in R? Can you send me? I need it for my 
dissertation (ms degree).

Thanks,

Fernando

 --- Christian Hennig <fm3a004 at math.uni-hamburg.de> escreveu: 
> Dear Fernando,
> 
> below you find a DBSCAN function I wrote for my own purposes.
> It comes with no warranty and without proper documentation, but I followed
> the notation of the original KDD-96 DBSCAN paper.
> For large data sets, it may be slow.
> 
> Best,
> Christian
> 
> On Thu, 21 Oct 2004, Fernando Prass wrote:
> 
> > No, kmeans is a partition method. I need a model-based method, like DBSCAN
> or
> > DENCLUE algorithm...
> > 
> > Fernando Prass
> 
> distvector <- function(x,data){
>   ddata <- t(data)-x
>   dv <- apply(ddata^2,2,sum)
> }
> 
> # data may be nxp or distance matrix
> # eps is the dbscan distance cutoff parameter
> # MinPts is the minimum size of a cluster
> # scale: Should the data be scaled?
> # distances: has to be TRUE if data is a distance matrix
> # showplot: Should the computation process be visualized? 
> # countmode: dbscan gives messages when processing point no. (countmode)
> dbscan <- function(data,eps,MinPts=5, scale=FALSE, distances=FALSE,
>                    showplot=FALSE,
>                    countmode=c(1,2,3,5,10,100,1000,5000,10000,50000)){
>   data <- as.matrix(data)
>   n <- nrow(data)
>   if (scale) data <- scale(data)
>   unregpoints <- rep(0,n)
>   e2 <- eps^2
>   cv <- rep(0,n)
>   cn <- 0
>   i <- 1
>   for (i in 1:n){
>     if (i %in% countmode) cat("Processing point ", i," of ",n, ".\n")
>     unclass <- cv<1
>     if (cv[i]==0){
>       if (distances) seeds <- data[i,]<=eps
>       else{
>         seeds <- rep(FALSE,n)
>         seeds[unclass] <- distvector(data[i,],data[unclass,])<=e2
>       }
>       if (sum(seeds)+unregpoints[i]<MinPts) cv[i] <- (-1)
>       else{
>         cn <- cn+1
>         cv[i] <- cn
>         seeds[i] <- unclass[i] <- FALSE
>         unregpoints[seeds] <- unregpoints[seeds]+1
>         while (sum(seeds)>0){
>           if (showplot) plot(data,col=1+cv)
>           unclass[seeds] <- FALSE
>           cv[seeds] <- cn
>           ap <- (1:n)[seeds]
> #          print(ap)
>           seeds <- rep(FALSE,n)          
>           for (j in ap){
> #            if (showplot) plot(data,col=1+cv)
>             jseeds <- rep(FALSE,n)          
>             if (distances) jseeds[unclass] <- data[j,unclass]<=eps
>             else{
>               jseeds[unclass] <- distvector(data[j,],data[unclass,])<=e2
>             }
>             unregpoints[jseeds] <- unregpoints[jseeds]+1
> #            if (cn==1)
> #              cat(j," sum seeds=",sum(seeds)," unreg=",unregpoints[j],
> #                  " newseeds=",sum(cv[jseeds]==0),"\n")
>             if (sum(jseeds)+unregpoints[j]>=MinPts){              
>               seeds[jseeds] <- cv[jseeds]==0
>               cv[jseeds & cv<0] <- cn
>             }
>           } # for j
>         } # while sum seeds>0
>       } # else (sum seeds + ... >= MinPts)
>     } # if cv==0
>   } # for i
>   if (sum(cv==(-1))>0){
>     noisenumber <- cn+1
>     cv[cv==(-1)] <- noisenumber
>   }
>   else
>     noisenumber <- FALSE
>   out <- list(classification=cv, noisenumber=noisenumber,
>               eps=eps, MinPts=MinPts, unregpoints=unregpoints)
>   out
> } # dbscan
> # classification: classification vector
> # noisenumber: number in the classification vector indicating noise points
> # unregpoints: ignore...
> 
> ***********************************************************************
> Christian Hennig
> Fachbereich Mathematik-SPST/ZMS, Universitaet Hamburg
> hennig at math.uni-hamburg.de, http://www.math.uni-hamburg.de/home/hennig/
> #######################################################################
> ich empfehle www.boag-online.de
> 
>



From francoisromain at free.fr  Thu Oct 21 21:51:24 2004
From: francoisromain at free.fr (=?ISO-8859-1?Q?Romain_Fran=E7ois?=)
Date: Thu, 21 Oct 2004 21:51:24 +0200
Subject: [R] I have a question
In-Reply-To: <000801c4b782$475b1700$9d68fea9@wjs>
References: <000801c4b782$475b1700$9d68fea9@wjs>
Message-ID: <4178133C.5080106@free.fr>

I'm not quite sure I got your question right, but here what I think of it :
   -  if  x  is your vector of ??
   -  and   y   your vector of dollars(i assume that was what you meant)

You could just perform a t-test on x and y/2, or a wilcoxon test :
 > t.test(x,y/2)
 > wilcox.test(x,y/2)

Cindy a ??crit :

>Hello!
>
>Could you tell me how to write R programming code about "Test the hypothesis that ??l=0.5 ?"
>
>Thank you!
>
>Cindy
>
>
>---
>
>
>
>
>	[[alternative HTML version deleted]]
>
>______________________________________________
>R-help at stat.math.ethz.ch mailing list
>https://stat.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>
>
>  
>

-- 
Romain Fran??ois
25, avenue Guy Moquet
94 400 Vitry sur seine
FRANCE
_______________________
_______________________

francoisromain at free.fr
01 46 80 65 60
06 18 39 14 69



From HWei at ms.soph.uab.edu  Thu Oct 21 22:14:00 2004
From: HWei at ms.soph.uab.edu (Hairong Wei)
Date: Thu, 21 Oct 2004 15:14:00 -0500
Subject: [R] RMA question
Message-ID: <4E4693F3D36946479044A8997EB05BD2011433BF@phealth4>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20041021/30ceedfd/attachment.pl

From ramasamy at cancer.org.uk  Thu Oct 21 22:57:31 2004
From: ramasamy at cancer.org.uk (Adaikalavan Ramasamy)
Date: Thu, 21 Oct 2004 21:57:31 +0100
Subject: [R] RMA question
In-Reply-To: <4E4693F3D36946479044A8997EB05BD2011433BF@phealth4>
References: <4E4693F3D36946479044A8997EB05BD2011433BF@phealth4>
Message-ID: <1098392250.3368.7.camel@ramasamy.stats>

1. This question is more appropriate for the BioConductor mailing list.

2. Any Affymetrix pre-processing can be split into background
correction, normalisation and summary stages. The combination of rma
background, quantile normalisation and median polish summary gives rise
to RMA. Therefore quantile normalisation is a defining feature of RMA. 
( which is probably why when the authors of RMA improved the background
stage, they called it GC-RMA instead ).

3. You can certainly mix and match any stages you like using the
expresso function. See help(expresso).
 


On Thu, 2004-10-21 at 21:14, Hairong Wei wrote:
> Can anybody explain why RMA has to have a default normalization method:
> quantile-quantile?  Why don't leave the choices to users? 
> 
> If I just want to use RMA to do a background correction without
> normalization, how should I specify the ? in the normalize.method="?" ?
> 
>  
> 
> Hairong 
> 
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>



From p.dalgaard at biostat.ku.dk  Thu Oct 21 22:55:00 2004
From: p.dalgaard at biostat.ku.dk (Peter Dalgaard)
Date: 21 Oct 2004 22:55:00 +0200
Subject: [R] matrix of eigenvalues
In-Reply-To: <417803F4.7080507@pdf.com>
References: <a06002006bd9a7b5ffb79@[130.120.104.141]>
	<4175083B.6090605@stat.wisc.edu>
	<a0600200bbd9c1ce167c6@[130.120.104.141]> <4176A133.2060103@pdf.com>
	<4177EDFD.6050408@wisc.edu> <a0600201cbd9dae9ef5c8@[130.120.104.141]>
	<417803F4.7080507@pdf.com>
Message-ID: <x27jpj94gr.fsf@biostat.ku.dk>

Spencer Graves <spencer.graves at pdf.com> writes:

>       That sounds like what the default method of "mexp" does, though
> I haven't checked it to be sure.  That works fine if the eigenvalue
> part of the Jordan canonical form is diagonal.  That does not hold for
> your examples.  However, the theorem I mentioned from one of Bellman's
> books says that any matrix can be approximated arbitrarily closely
> with another matrix with unique eigenvalues, for which the default
> method of "mexp" seems to work.  For more, see the classic paper by
> Moler and van Loan that Doug mentioned on "Nineteen Dubious Ways to
> Calculate the Matrix Exponential".     hope this helps.     spencer
> graves

An old item on my TODO list is to lift the version of mexp in Octave
into R. As far as I know, that one is just about as robust as they
come. Unfortunately, it is in C++. 

BTW, there's a "Son of 19Dubious" paper, celebrating its 25th
anniversary. I forget its exact coordinates, except that it is from
1978 + 25 = 2003... Someone mentioned it last time the topic came up.


-- 
   O__  ---- Peter Dalgaard             Blegdamsvej 3  
  c/ /'_ --- Dept. of Biostatistics     2200 Cph. N   
 (*) \(*) -- University of Copenhagen   Denmark      Ph: (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)             FAX: (+45) 35327907



From ligges at statistik.uni-dortmund.de  Thu Oct 21 23:04:36 2004
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Thu, 21 Oct 2004 23:04:36 +0200
Subject: [R] RMA question
In-Reply-To: <4E4693F3D36946479044A8997EB05BD2011433BF@phealth4>
References: <4E4693F3D36946479044A8997EB05BD2011433BF@phealth4>
Message-ID: <41782464.203@statistik.uni-dortmund.de>

Hairong Wei wrote:

> Can anybody explain why RMA has to have a default normalization method:
> quantile-quantile?  Why don't leave the choices to users? 
> 
> If I just want to use RMA to do a background correction without
> normalization, how should I specify the ? in the normalize.method="?" ?

What is RMA? It's not in base R, AFAIK.
Do you mean function rma() in the Bioconductor package "affy"? In this 
case please ask for help on the Bioconductor list.

Uwe Ligges


>  
> 
> Hairong 
> 
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html



From chao_zhu at hotmail.com  Thu Oct 21 21:25:03 2004
From: chao_zhu at hotmail.com (Zhu Chao)
Date: Thu, 21 Oct 2004 19:25:03 +0000
Subject: [R] Question about calling C function from R
Message-ID: <BAY24-F29ch9wPiyAph000004fd@hotmail.com>

Hi,all

I wrote a C function and now I want to plug in this in R. I know how to do 
this in Unix version R by typing something like "R CMD SHLIB foo.c" to 
compile this C code. Then I can load it by using dyn.load. But I don't know 
how to do it in a windows version R. Anyone can help me out here? I really 
appreciate.

Thanks a lot!

Chao



From efg at stowers-institute.org  Fri Oct 22 00:11:21 2004
From: efg at stowers-institute.org (Earl F. Glynn)
Date: Thu, 21 Oct 2004 17:11:21 -0500
Subject: [R] p.adjust with "hommel" method doesn't allow NAs but other
	methods do?
Message-ID: <cl9c6a$vfc$1@sea.gmane.org>

Is this expected behavior?

I've got 77 NAs in a vector of 6875 p values:

# Hommel method fails
> p <- p.adjust(r$p,"hommel")
Error: NAs are not allowed in subscripted assignments
In addition: Warning message:
longer object length
        is not a multiple of shorter object length in: n * p/(1:n)

# These work fine
> p <- p.adjust(r$p,"none")
> p <- p.adjust(r$p,"bonferroni")
> p <- p.adjust(r$p,"holm")
> p <- p.adjust(r$p,"hochberg")
> p <- p.adjust(r$p,"fdr")

> sum(is.na(r$p))
[1] 77
> length(r$p)
[1] 6875

When I removed these NAs myself, why does the "hommel" method take perhaps
300 times (or more) longer than the other methods?  The Hommel method took
over 5 minutes for this set of 6875 when the other methods were less than a
second.

I'm using Windows 2000 and R 2.0.0

efg



From andy_liaw at merck.com  Fri Oct 22 00:02:34 2004
From: andy_liaw at merck.com (Liaw, Andy)
Date: Thu, 21 Oct 2004 18:02:34 -0400
Subject: [R] Question about calling C function from R
Message-ID: <3A822319EB35174CA3714066D590DCD50994E1FB@usrymx25.merck.com>

> From: Zhu Chao
> 
> Hi,all
> 
> I wrote a C function and now I want to plug in this in R. I 
> know how to do 
> this in Unix version R by typing something like "R CMD SHLIB 
> foo.c" to 
> compile this C code. Then I can load it by using dyn.load. 
> But I don't know 
> how to do it in a windows version R. Anyone can help me out 
> here? I really 
> appreciate.

It's essentially the same, except you load the .dll file, rather than the
.so.  However, before you can do that, you need to follow the direction in
README.packages and install the tools and compilers mentioned there.

Andy
 
> Thanks a lot!
> 
> Chao
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
> 
>



From lane.alencar at itelefonica.com.br  Fri Oct 22 02:10:15 2004
From: lane.alencar at itelefonica.com.br (lane.alencar)
Date: Thu, 21 Oct 2004 21:10:15 -0300
Subject: [R] wavelets
Message-ID: <I5YLT3$070B69B663AA08B29908BA17BB1D2147@itelefonica.com.br>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20041021/d7a8af8d/attachment.pl

From sdfrost at ucsd.edu  Fri Oct 22 02:34:59 2004
From: sdfrost at ucsd.edu (Simon Frost)
Date: Thu, 21 Oct 2004 17:34:59 -0700
Subject: [R] Assist on R-2.0.0 /64 bit AMD/SuSE 9.1
Message-ID: <200410211734.59053.sdfrost@ucsd.edu>

Hi Folks,

I compiled and installed Assist-0.1.0; I'm using R-2.0.0 running on SuSe 
Linux, on a dual AMD64 machine. However, I (apparently randomly) get 
segmentation faults when I use cubic2. Any ideas?

Thanks
Simon
-- 
Simon D.W. Frost, MA DPhil
Adjunct Assistant Professor
Department of Pathology
University of California, San Diego
UCSD Antiviral Research Center
150 W. Washington St.
San Diego, CA 92103
USA
Tel: +1 619 543 8080 Ext 275
Fax: +1 619 298 0177
Email: sdfrost at ucsd.edu



From kylie.lange at flinders.edu.au  Fri Oct 22 03:47:34 2004
From: kylie.lange at flinders.edu.au (Kylie Lange)
Date: Fri, 22 Oct 2004 11:17:34 +0930
Subject: [R] p-values for the dip test
Message-ID: <5.2.1.1.0.20041022111521.00b865d0@post.flinders.edu.au>


Hi all,

I am using Hartigan & Hartigan's [1] "dip test" of unimodality via the 
diptest package in R. The function dip() returns the value of the test 
statistic but I am having problems calculating the p-value associated with 
that value. I'm hoping someone here is familiar with this process and can 
explain it.

In the original article there is an example using n=63 and a dip value of 
0.059 - which I am able to replicate using dip(). However the article then 
states:

"The dip illustrated in Figure 1 is 0.059 which has a tail probability 
about 10% from Table 1. ( sqrt(63)*D*F(63) is distributed approximately as 
sqrt(100)*D*F(100) which has 90% point 0.474. Thus D*F(63) has 90% point 
0.060. )"

It is the value of 0.474 that I am unable to get. Table 1 is a table of 
percentage points for various sample sizes. The same table is provided in 
the diptest package as 'qDiptab' (but at greater accuracy). n=63 is not 
tabled but n=50 and 100 are. In the table for n=100 the value given for 90% 
is 0.0471 so where does the 0.474 come from?

Any help appreciated!
Kylie.

[1] Hartigan JA & Hartigan PM. "The Dip Test of Unimodality", The Annals of 
Statistics, v13(1):70-84, 1985.



From bwooliscroft at e3.net.nz  Fri Oct 22 04:28:01 2004
From: bwooliscroft at e3.net.nz (Ben Wooliscroft)
Date: Fri, 22 Oct 2004 15:28:01 +1300
Subject: [R] <no subject>
Message-ID: <BD9ED781.8DEF%bwooliscroft@e3.net.nz>



From ripley at stats.ox.ac.uk  Fri Oct 22 08:29:40 2004
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Fri, 22 Oct 2004 07:29:40 +0100 (BST)
Subject: [R] Assist on R-2.0.0 /64 bit AMD/SuSE 9.1
In-Reply-To: <200410211734.59053.sdfrost@ucsd.edu>
Message-ID: <Pine.LNX.4.44.0410220722190.1539-100000@gannet.stats>

What is Assist-0.1.0?  It's not part of R and it is not a CRAN nor 
Bioconductor package.  

There is a CRAN package `assist', and if you mean that please follow the
advice in the posting guide and contact the maintainer.  Now, that package
says

Title: A Suite of S-Plus Functions Implementing Smoothing Splines

and S-PLUS functions don't run on 64-bit versions of R.  I suggest you and
the maintainer look for the use of `long *' in .C calls: S-PLUS uses `long 
*' where R uses `int *'.

On Thu, 21 Oct 2004, Simon Frost wrote:

> I compiled and installed Assist-0.1.0; I'm using R-2.0.0 running on SuSe 
> Linux, on a dual AMD64 machine. However, I (apparently randomly) get 
> segmentation faults when I use cubic2. Any ideas?

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From Ivy_Li at smics.com  Fri Oct 22 08:34:50 2004
From: Ivy_Li at smics.com (Ivy_Li)
Date: Fri, 22 Oct 2004 14:34:50 +0800
Subject: =?gb2312?B?tPC4tDogW1JdIEhvdyB0byBkcmF3IHgtYXhpcyB0aW1lIGxhYmVsLg==?=
Message-ID: <8A910F1818425847A6D18C7832D207E502AA82@ex115.smic-sh.com>

Thank you for helping me.
I have modified this part of code.

Time <- c("2004-08-05 09:08:48", "2004-08-13 20:53:38",
  	"2004-08-14 13:57:23", "2004-08-12 16:17:41", 
  	"2004-08-12 16:15:27", "2004-08-11 21:38:24",
  	"2004-08-12 14:28:41", "2004-08-18 18:04:47",
  	"2004-08-13 15:23:14", "2004-08-14 02:36:33")
Time <- as.POSIXct(Time)
x <- data.frame(main.name="AAA", fruit.name=rep(c("Apply","Watermelon"),each=5), 
		x.name=Time, y.name=(1:10))
par(las=3,mai=c(1,.7,.5,.3))
plot(x$x.name,as.character(x$y.name),pch=26, xlab="Process Time", xaxt = 'n')
axis.POSIXct(1, at=seq(min(x$x.name), max(x$x.name), "days"),format="%d /%m")
fruit.class <- table(x$fruit.name)
color.code <- c(611,552,656,121,451,481,28,652,32,550,90,401,150,12,520,8)
for(j in 1:length(fruit.class))
{
	fruit <- names(fruit.class)[j]
	lines(smooth.spline(x[x$fruit.name==fruit, "x.name"],
		x[x$fruit.name==fruit, "y.name"],df=5),
		col=colors()[color.code[j]],lwd=5)
}

Run above code , you will find the "Process Time" is close to the x-label, I want to move it down . And How can I reduce font?
Thank you very much!


Best Regards!
Ivy Li
YMS in Production & Testing
Semiconductor Manufactory International(ShangHai) Corporation
#18 ZhangJiang Road, PuDong New Area, Shanghai, China
Tel: 021-5080-2000 *11754
Email: Ivy_Li at smics.com



-----orig---


In this case,  don't draw x-axis:

plot(x$x.name,as.character(x$y.name),pch=26, xaxt = 'n')

Then check out functin mtext() (to draw the xaxis yourself. It's not
diffucult but you have to try).
Luck
Huang Huan



Internet
Ivy_Li at smics.com - 10/19/2004 10:23 AM


To:    Huan HUANG

cc:    r-help


Subject:    ?: ?: ?: [R] How to draw x-axis time label.



Thank you !
I run your code. It have a graph with time(date) on x-axis. Thank you very
much!
Well,  Would you like to do me a favor again?
Most of times, the x-axis time data is lasted about 3 months. They are too
much. I must select some of them marking in x-axis,and they are must
reasonable and  human-readable. Now I have tried the "pretty " function,
But as soon as I run it,  the format of data changed to "numeric" ,I don't
know what should I do. Is there exist other functions can get the same
result?



Best Regards!
Ivy Li
YMS in Production & Testing
Semiconductor Manufactory International(ShangHai) Corporation
#18 ZhangJiang Road, PuDong New Area, Shanghai, China
Tel: 021-5080-2000 *11754
Email: Ivy_Li at smics.com



----------
: huan.huang at uk.bnpparibas.com
[mailto:huan.huang at uk.bnpparibas.com]
: 20041019 16:43
: Ivy_Li
: Re: ?: ?: [R] How to draw x-axis time label.


Hi Li chen,

I mean you should run the code I sent you in my first email. It's here
again:

plot(x$x.name,as.character(x$y.name),pch=26)

By running this you should have a graph with time(date) on x-axis.

Give it a go? Let me know if it is not working.
Huang Huan



Internet
Ivy_Li at smics.com - 10/19/2004 02:18 AM


To:    Huan HUANG

cc:


Subject:    ?: ?: [R] How to draw x-axis time label.


Put *time* on x-axis? Like this:

>plot(time(x$x.name),as.character(x$y.name),pch=26)

but the x-axis is not real time , just some number.

I forgot your question in your previous mail. :-)
My chinese name is Li Chen
hello!

Best Regards!
Ivy Li
YMS in Production & Testing
Semiconductor Manufactory International(ShangHai) Corporation
#18 ZhangJiang Road, PuDong New Area, Shanghai, China
Tel: 021-5080-2000 *11754
Email: Ivy_Li at smics.com



----------
: huan.huang at uk.bnpparibas.com
[mailto:huan.huang at uk.bnpparibas.com]
: 20041018 18:35
: Ivy_Li
: Re: ?: [R] How to draw x-axis time label.


Come on, Ivy. Did you try the line in my previous email? That will put
*time* on x-axis. Give it a go?



Internet
Ivy_Li at smics.com - 10/18/2004 10:24 AM


To:    Huan HUANG

cc:


Subject:    ?: [R] How to draw x-axis time label.



Thank you !
I know the oddness number are time calulated since 1970, but i need them to
be  x-axis .
What  should  I do ?

Best Regards!
Ivy Li
YMS in Production & Testing
Semiconductor Manufactory International(ShangHai) Corporation
#18 ZhangJiang Road, PuDong New Area, Shanghai, China
Tel: 021-5080-2000 *11754
Email: Ivy_Li at smics.com



----------
: huan.huang at uk.bnpparibas.com
[mailto:huan.huang at uk.bnpparibas.com]
: 20041018 16:50
: Ivy_Li
: Re: [R] How to draw x-axis time label.


Hi Ivy (any chinese name?)

I ran your codes. I found if I try:

plot(x$x.name,as.character(x$y.name),pch=26,)

It would put time as the x axis. The *odd* numbers showed in yoru graph as
the moment are numbers of seconds of those dates since the beginning of
1970.
Hope it helps.
Huang Huan



Internet
Ivy_Li at smics.com@stat.math.ethz.ch - 10/18/2004 07:26 AM


Sent by:    r-help-bounces at stat.math.ethz.ch

To:    r-help

cc:


Subject:    [R] How to draw x-axis time label.


Hi everybody,
 Could I consult one problem?
 It is about plot
 Now I do some analysis in plot . I need to draw a plot which x-axis is
 time . But when I run the funtion of plot . The x-label are very oddness
 number, such as the time is "2004-08-05 09:08:48", but the x-label is
 1091800000. I don't know how to do .
 I have an example . Mybe It  can explain my meaning clearly.
 Thank you for helping me!

Time <- c("2004-08-05 09:08:48", "2004-08-13 20:53:38",
   "2004-08-14 13:57:23", "2004-08-12 16:17:41",
   "2004-08-12 16:15:27", "2004-08-11 21:38:24",
   "2004-08-12 14:28:41", "2004-08-18 18:04:47",
   "2004-08-13 15:23:14", "2004-08-14 02:36:33")
Time <- as.POSIXlt(Time)
x <- data.frame(main.name="AAA",
fruit.name=rep(c("Apply","Watermelon"),each=5),
  x.name=Time, y.name=(1:10))
plot(as.numeric(x$x.name),as.character(x$y.name),pch=26,)
fruit.class <- table(x$fruit.name)
color.code <- c(611,552,656,121,451,481,28,652,32,550,90,401,150,12,520,8)
for(j in 1:length(fruit.class))
{
 fruit <- names(fruit.class)[j]
 lines(smooth.spline(x[x$fruit.name==fruit, "x.name"],
  x[x$fruit.name==fruit, "y.name"],df=5),
  col=colors()[color.code[j]],lwd=5)
}

Best Regards!
Ivy Li
YMS in Production & Testing
Semiconductor Manufactory International(ShangHai) Corporation
#18 ZhangJiang Road, PuDong New Area, Shanghai, China
Tel: 021-5080-2000 *11754
Email: Ivy_Li at smics.com

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-help
 PLEASE do read the posting guide!
 http://www.R-project.org/posting-guide.html







????????????????????????????????????????????????????

????????????????????????????????????????????????????????

???????????????????????????????????????????????????????????

?????????????????????????????????????????????????????????

???????????????????????????????????????????????????????????

???????????????????????????????????????????????????????????????

?????????????????????????????????????????????????

????????????????????????????????????????????????????

?????????????????????????????????????????????????



??????????????????????????????????????????????????????????????????????????????????????????????






?????????????????????????????????????????????????????

?????????????????????????????????????????????????????????

???????????????????????????????????????????????????????????

???????????????



???????????????????????????????????????????????????????????

?????????????????????????????????????????????????????????

????????????????????????????????????????????????????????????

???????????????

??

???????????????????????????????????????????????????????

??????????????????????????????????????????????








????????????????????????????????????????????????????

????????????????????????????????????????????????????????

???????????????????????????????????????????????????????????

?????????????????????????????????????????????????????????

???????????????????????????????????????????????????????????

???????????????????????????????????????????????????????????????

?????????????????????????????????????????????????

????????????????????????????????????????????????????

?????????????????????????????????????????????????



??????????????????????????????????????????????????????????????????????????????????????????????





?????????????????????????????????????????????????????

?????????????????????????????????????????????????????????

???????????????????????????????????????????????????????????

???????????????



???????????????????????????????????????????????????????????

?????????????????????????????????????????????????????????

????????????????????????????????????????????????????????????

???????????????

??

???????????????????????????????????????????????????????

??????????????????????????????????????????????








????????????????????????????????????????????????????

????????????????????????????????????????????????????????

???????????????????????????????????????????????????????????

?????????????????????????????????????????????????????????

???????????????????????????????????????????????????????????

???????????????????????????????????????????????????????????????

?????????????????????????????????????????????????

????????????????????????????????????????????????????

?????????????????????????????????????????????????



??????????????????????????????????????????????????????????????????????????????????????????????




?????????????????????????????????????????????????????

?????????????????????????????????????????????????????????

???????????????????????????????????????????????????????????

???????????????



???????????????????????????????????????????????????????????

?????????????????????????????????????????????????????????

????????????????????????????????????????????????????????????

???????????????

??

???????????????????????????????????????????????????????

??????????????????????????????????????????????








????????????????????????????????????????????????????

????????????????????????????????????????????????????????

???????????????????????????????????????????????????????????

?????????????????????????????????????????????????????????

???????????????????????????????????????????????????????????

???????????????????????????????????????????????????????????????

?????????????????????????????????????????????????

????????????????????????????????????????????????????

?????????????????????????????????????????????????



??????????????????????????????????????????????????????????????????????????????????????????????



?????????????????????????????????????????????????????

?????????????????????????????????????????????????????????

???????????????????????????????????????????????????????????

???????????????



???????????????????????????????????????????????????????????

?????????????????????????????????????????????????????????

????????????????????????????????????????????????????????????

???????????????

??

???????????????????????????????????????????????????????

??????????????????????????????????????????????



From ripley at stats.ox.ac.uk  Fri Oct 22 09:06:58 2004
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Fri, 22 Oct 2004 08:06:58 +0100 (BST)
Subject: =?gb2312?B?tPC4tDogW1JdIEhvdyB0byBkcmF3IHgtYXhpcyB0aW1lIGxhYmVsLg==?=
In-Reply-To: <8A910F1818425847A6D18C7832D207E502AA82@ex115.smic-sh.com>
Message-ID: <Pine.LNX.4.44.0410220802420.15322-100000@gannet.stats>

On Fri, 22 Oct 2004, Ivy_Li wrote:

> Thank you for helping me.
> I have modified this part of code.
> 
> Time <- c("2004-08-05 09:08:48", "2004-08-13 20:53:38",
>   	"2004-08-14 13:57:23", "2004-08-12 16:17:41", 
>   	"2004-08-12 16:15:27", "2004-08-11 21:38:24",
>   	"2004-08-12 14:28:41", "2004-08-18 18:04:47",
>   	"2004-08-13 15:23:14", "2004-08-14 02:36:33")
> Time <- as.POSIXct(Time)
> x <- data.frame(main.name="AAA", fruit.name=rep(c("Apply","Watermelon"),each=5), 
> 		x.name=Time, y.name=(1:10))
> par(las=3,mai=c(1,.7,.5,.3))
> plot(x$x.name,as.character(x$y.name),pch=26, xlab="Process Time", xaxt = 'n')
> axis.POSIXct(1, at=seq(min(x$x.name), max(x$x.name), "days"),format="%d /%m")
> fruit.class <- table(x$fruit.name)
> color.code <- c(611,552,656,121,451,481,28,652,32,550,90,401,150,12,520,8)
> for(j in 1:length(fruit.class))
> {
> 	fruit <- names(fruit.class)[j]
> 	lines(smooth.spline(x[x$fruit.name==fruit, "x.name"],
> 		x[x$fruit.name==fruit, "y.name"],df=5),
> 		col=colors()[color.code[j]],lwd=5)
> }
> 
> Run above code , you will find the "Process Time" is close to the
> x-label, I want to move it down . And How can I reduce font? Thank you

"Process Time" *is* the x-label!  You will find all this discussed in "An
Introduction to R": you need to change the margins and to use title(xlab=,
line=) to place the x-label in another position.  And the font *size* is
set by cex*: see ?par and the "An Introduction to R" manual.

[Many, many unneeded lines removed.]


-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From maechler at stat.math.ethz.ch  Fri Oct 22 09:23:10 2004
From: maechler at stat.math.ethz.ch (Martin Maechler)
Date: Fri, 22 Oct 2004 09:23:10 +0200
Subject: [R] p-values for the dip test
In-Reply-To: <5.2.1.1.0.20041022111521.00b865d0@post.flinders.edu.au>
References: <5.2.1.1.0.20041022111521.00b865d0@post.flinders.edu.au>
Message-ID: <16760.46430.62149.423510@gargle.gargle.HOWL>

Hmm, this is rather about reading the (Hartigan)^2 paper ...

>>>>> "Kylie" == Kylie Lange <kylie.lange at flinders.edu.au>
>>>>>     on Fri, 22 Oct 2004 11:17:34 +0930 writes:

    Kylie> Hi all,

    Kylie> I am using Hartigan & Hartigan's [1] "dip test" of
    Kylie> unimodality via the diptest package in R. The
    Kylie> function dip() returns the value of the test
    Kylie> statistic but I am having problems calculating the
    Kylie> p-value associated with that value. I'm hoping
    Kylie> someone here is familiar with this process and can
    Kylie> explain it.

    Kylie> In the original article there is an example using
    Kylie> n=63 and a dip value of 0.059 - which I am able to
    Kylie> replicate using dip(). However the article then
    Kylie> states:

    Kylie> "The dip illustrated in Figure 1 is 0.059 which has a
    Kylie> tail probability about 10% from Table 1. (
    Kylie> sqrt(63)*D*F(63) is distributed approximately as
    Kylie> sqrt(100)*D*F(100) which has 90% point 0.474. Thus
    Kylie> D*F(63) has 90% point 0.060. )"

note:  sqrt(100) * D(F_100) =  10 * D(F_100)

       i.e. D(F_100) would have 90* point 0.0474

but the second "4" is a simple typo, (not the only one in the
paper, IIRC!);  "s/4/1/" --> you get the correct 0.0471

    Kylie> It is the value of 0.474 that I am unable to
    Kylie> get. Table 1 is a table of percentage points for
    Kylie> various sample sizes. The same table is provided in
    Kylie> the diptest package as 'qDiptab' (but at greater
    Kylie> accuracy). n=63 is not tabled but n=50 and 100
    Kylie> are. In the table for n=100 the value given for 90%
    Kylie> is 0.0471 so where does the 0.474 come from?

(see above).

    Kylie> Any help appreciated!  

you're welcome.
Martin Maechler, ETH Zurich ('diptest' maintainer)


    Kylie> [1] Hartigan JA & Hartigan PM. "The Dip Test of
    Kylie> Unimodality", The Annals of Statistics, v13(1):70-84, 1985.



From pbrouilly at gphy.campus.univ-poitiers.fr  Fri Oct 22 10:19:43 2004
From: pbrouilly at gphy.campus.univ-poitiers.fr (pbrouilly@gphy.campus.univ-poitiers.fr)
Date: Fri, 22 Oct 2004 10:19:43 +0200
Subject: [R] Vector and String
Message-ID: <1098433183.4178c29f79370@gphy.campus.univ-poitiers.fr>

Hi everybody,

I'm using a vector to strore some strings.

I create a vector named seq2 :
> seq2<-vector(length=0)

I have a string named b
> b
[1] CATGGTAGGAATAC

I put b in the vector
> seq2<-c(seq2,b)

Finally I edit seq2
> seq2
[1] "14760"

My question is why I obtained a number (14760) when I add a string (CATGGTAGGAATAC)

I have an idea : when I edit b, I see ( [1] CATGGTAGGAATAC ) 
and not ([1] "CATGGTAGGAATAC") 

How can I convert b to a true string with "" 

Thank you very much



From ripley at stats.ox.ac.uk  Fri Oct 22 10:55:14 2004
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Fri, 22 Oct 2004 09:55:14 +0100 (BST)
Subject: [R] Vector and String
In-Reply-To: <1098433183.4178c29f79370@gphy.campus.univ-poitiers.fr>
Message-ID: <Pine.LNX.4.44.0410220950390.24043-100000@gannet.stats>

On Fri, 22 Oct 2004 pbrouilly at gphy.campus.univ-poitiers.fr wrote:

> Hi everybody,
> 
> I'm using a vector to strore some strings.
> 
> I create a vector named seq2 :
> > seq2<-vector(length=0)

> typeof(seq2)
[1] "logical"

so you created a *logical vector*.  See ?vector.

> I have a string named b
> > b
> [1] CATGGTAGGAATAC

I think you have a factor, not a `string'.  What does str(b) say?

> I put b in the vector
> > seq2<-c(seq2,b)
> 
> Finally I edit seq2
> > seq2
> [1] "14760"
> 
> My question is why I obtained a number (14760) when I add a string (CATGGTAGGAATAC)
> 
> I have an idea : when I edit b, I see ( [1] CATGGTAGGAATAC ) 
> and not ([1] "CATGGTAGGAATAC") 
> 
> How can I convert b to a true string with "" 

seq2 <- character()
seq2 <- c(seq2, as.character(b))



-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From m.mamin at intershop.de  Fri Oct 22 11:30:52 2004
From: m.mamin at intershop.de (Marc Mamin)
Date: Fri, 22 Oct 2004 11:30:52 +0200
Subject: [R] Vector and String
In-Reply-To: <Pine.LNX.4.44.0410220950390.24043-100000@gannet.stats>
References: <Pine.LNX.4.44.0410220950390.24043-100000@gannet.stats>
Message-ID: <opsf9l1qqc19ufgc@mail.intershop.de>

Hello,

I'm looking for a procedure to detect trend changes or significant signals 
in time series as in the attached example.

Could you point me to a library or reference I can start with?

Thanks,

Marc Mamin

From ripley at stats.ox.ac.uk  Fri Oct 22 11:48:10 2004
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Fri, 22 Oct 2004 10:48:10 +0100 (BST)
Subject: libraries and references (was `[R] Vector and String')
In-Reply-To: <opsf9l1qqc19ufgc@mail.intershop.de>
Message-ID: <Pine.LNX.4.44.0410221039180.18808-100000@gannet.stats>

Have you read the R-posting guide which specifically asks you *not* to 
tack completely separate questions on to other threads, and to use a 
relevant subject line?

What has this to do with `[R] Vector and String'?

On Fri, 22 Oct 2004, Marc Mamin wrote:

> I'm looking for a procedure to detect trend changes or significant signals 
> in time series as in the attached example.
> 
> Could you point me to a library or reference I can start with?

Two good references are the R posting guide at
http://www.r-project.org/posting-guide.html and the R FAQ at
http://cran.r-project.org/doc/FAQ/R-FAQ.html which lists some very
relevant R >packages< (and which the posting guide asks you to consult).
Please do start with those.

There are lots of good libraries in the world, but you need to visit one 
which will admit you.  They will probably insist that you read their usage 
conditions first.  So does this list.

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From Bernhard.Pfaff at drkw.com  Fri Oct 22 12:16:58 2004
From: Bernhard.Pfaff at drkw.com (Pfaff, Bernhard)
Date: Fri, 22 Oct 2004 12:16:58 +0200
Subject: libraries and references (was `[R] Vector and String')
Message-ID: <29E0BC0C716A584582941615CF9FFB0902585C6C@ibfftce107.de.ad.drkw.net>

> > I'm looking for a procedure to detect trend changes or 
> significant signals 
> > in time series as in the attached example.

Dear Marc,

it seems that your example have been removed, as stated in the posting
guide. Hence, I can only guess what your problem is and offer:

help.search("Goldfeld")
help.search("Chow")
help.search("structural change")

May be you can provide your example directly in a mail?

HTH,
Bernhard

> > 
> > Could you point me to a library or reference I can start with?
> 
> Two good references are the R posting guide at
> http://www.r-project.org/posting-guide.html and the R FAQ at
> http://cran.r-project.org/doc/FAQ/R-FAQ.html which lists some very
> relevant R >packages< (and which the posting guide asks you 
> to consult).
> Please do start with those.
> 
> There are lots of good libraries in the world, but you need 
> to visit one 
> which will admit you.  They will probably insist that you 
> read their usage 
> conditions first.  So does this list.
> 
> -- 
> Brian D. Ripley,                  ripley at stats.ox.ac.uk
> Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
> University of Oxford,             Tel:  +44 1865 272861 (self)
> 1 South Parks Road,                     +44 1865 272866 (PA)
> Oxford OX1 3TG, UK                Fax:  +44 1865 272595
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
http://www.R-project.org/posting-guide.html


--------------------------------------------------------------------------------
The information contained herein is confidential and is inte...{{dropped}}



From gilles.guillot at inapg.inra.fr  Fri Oct 22 12:16:22 2004
From: gilles.guillot at inapg.inra.fr (Gilles GUILLOT)
Date: Fri, 22 Oct 2004 12:16:22 +0200
Subject: [R] building an R package : where and how should my fortran library
	be loaded ?
Message-ID: <200410221216.22215.gilles.guillot@inapg.inra.fr>

Hi ,

I'm currently trying to make available 
a few fortran subroutines and R functions 
(which make interface to these subroutines),
as an R package.
I'm doing it under linux with R 1.9.0 
(but hope to do it for windows too).
I have trouble for loading my fortran code.
Here are the steps involved :

1) In R: 
                                      # Create tree for package
                                        # and include my R functions in it
R> package.skeleton(name="Geneland",
                 list=c("mcmcFmodel",
                   "simFmodel",
                   "rdiscr",
                   "tessel.post"),
                 path="/home/guillot/projets/flux/package/",
                 force=T)

                                        # copy my fortran code in the tree
R>    system("cp /home/guillot/projets/flux/package/fortran/*.f   
/home/guillot/projets/flux/package/Geneland/src/")

R> q()

2) In a Unix shell
$ R CMD build Geneland
$ R CMD check Geneland
$ R CMD build Geneland
$ R CMD INSTALL Geneland

3) Back to R
R> library("Geneland")

So far, it's OK, pure R functions work fine.
But  R functions calling  some Fortran via .Fortran
do not work, I get an error message like :
Error in .Fortran("mcmc", ...
Fortran function name not in load table

If I make 
R> dyn.load("/usr/lib/R/library/Geneland/libs/Geneland.so")

then it works .
I was thinking that my Geneland.so was loaded automatically
by the command library("Geneland")
Obviously, it's not.
Where and how should the library loaded ?


Gilles

_____________________________________________________________________
Gilles GUILLOT
INRA -D??partement Math??matiques et Informatique Appliqu??es

Unit?? de Mixte de Recherche INRA - INAPG - ENGREF
Institut National Agronomique de Paris-Grignon
16 rue Claude Bernard
75231 Paris cedex 5

Aile Claude Bernard
Niveau cours +3 ??tages
tel : +33 (0)1 44 08 72 71
fax : +33 (0)1 44 08 16 66
http://www.inapg.fr/ens_rech/mathinfo/personnel/guillot/welcome.html
______________________________________________________________________
-- 
_____________________________________________________________________
Gilles GUILLOT
INRA -D??partement Math??matiques et Informatique Appliqu??es

Unit?? de Mixte de Recherche INRA - INAPG - ENGREF
Institut National Agronomique de Paris-Grignon
16 rue Claude Bernard
75231 Paris cedex 5

Aile Claude Bernard
Niveau cours +3 ??tages
tel : +33 (0)1 44 08 72 71
fax : +33 (0)1 44 08 16 66
http://www.inapg.fr/ens_rech/mathinfo/personnel/guillot/welcome.html



From ripley at stats.ox.ac.uk  Fri Oct 22 12:42:20 2004
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Fri, 22 Oct 2004 11:42:20 +0100 (BST)
Subject: [R] building an R package : where and how should my fortran
	library be loaded ?
In-Reply-To: <200410221216.22215.gilles.guillot@inapg.inra.fr>
Message-ID: <Pine.LNX.4.44.0410221135540.5265-100000@gannet.stats>

On Fri, 22 Oct 2004, Gilles GUILLOT wrote:

> I'm currently trying to make available 
> a few fortran subroutines and R functions 
> (which make interface to these subroutines),
> as an R package.
> I'm doing it under linux with R 1.9.0 

Please update, as few of us have such an old system in use.

> (but hope to do it for windows too).
> I have trouble for loading my fortran code.

[...]

> I was thinking that my Geneland.so was loaded automatically
> by the command library("Geneland")

`Thinking'?  `Hoping', perhaps, but where did you read that?

> Obviously, it's not.
> Where and how should the library loaded ?

By your R code, via library.dynam.  A common `spell' is for package foo to 
include in foo/src/zzz.R

.First.lib <- function(libpath, pkgname)  
    library.dynam("foo", pkgname, libpath)

and it described in `Writing R Extensions'.  That also explains the 
different rules for packages with namespaces.

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From lecoutre at stat.ucl.ac.be  Fri Oct 22 12:39:04 2004
From: lecoutre at stat.ucl.ac.be (Eric Lecoutre)
Date: Fri, 22 Oct 2004 12:39:04 +0200
Subject: [R] cor, cov, method "pairwise.complete.obs"
Message-ID: <6.0.1.1.2.20041022123435.02006ec0@stat4ux.stat.ucl.ac.be>


Hi UseRs,

I don't want to die beeing idiot...

I dont understand the different results between:
cor() and cov2cov(cov()).

See this little example:

 > x=matrix(c(0.5,0.2,0.3,0.1,0.4,NA,0.7,0.2,0.6,0.1,0.4,0.9),ncol=3)
 > cov2cor(cov(x,use="pairwise.complete.obs"))
            [,1]       [,2]       [,3]
[1,]  1.0000000  0.4653400 -0.1159542
[2,]  0.4653400  1.0000000 -0.7278728
[3,] -0.1159542 -0.7278728  1.0000000
 > cor(x,use="pairwise.complete.obs")
            [,1]       [,2]       [,3]
[1,]  1.0000000  0.3973597 -0.1159542
[2,]  0.3973597  1.0000000 -0.9736842
[3,] -0.1159542 -0.9736842  1.0000000


My question arises in a context where cor(mydata, 
use="pairwise.complete.obs")  returns correlations on diagonal that are 
near 0.95 (where as my data do have 100 observations and only 12 missing 
values...).

Do cor() and cov() handle the same way the argument "pairwise.complete.obs"?

Eric

  R.version
          _
platform i386-pc-mingw32
arch     i386
os       mingw32
system   i386, mingw32
status
major    2
minor    0.0
year     2004
month    10
day      04
language R


Eric Lecoutre
UCL /  Institut de Statistique
Voie du Roman Pays, 20
1348 Louvain-la-Neuve
Belgium

tel: (+32)(0)10473050
lecoutre at stat.ucl.ac.be
http://www.stat.ucl.ac.be/ISpersonnel/lecoutre

If the statistics are boring, then you've got the wrong numbers. -Edward 
Tufte



From vito_ricci at yahoo.com  Fri Oct 22 13:21:27 2004
From: vito_ricci at yahoo.com (Vito Ricci)
Date: Fri, 22 Oct 2004 13:21:27 +0200 (CEST)
Subject: [R] Convert a list in a dataframe
Message-ID: <20041022112127.63039.qmail@web41207.mail.yahoo.com>

Hi,

I've a list containing parameters (intercepts &
coefficients) of 12 regressions fitted

> coeff
[[1]]
 (Intercept)         anno 
-427017.1740     217.0588 

[[2]]
 (Intercept)         anno 
-39625.82146     21.78025 

.....

[[12]]
(Intercept)        anno 
257605.0343   -129.7646 

I want create a data frame with two columns (intercept
and anno)using data in these list.

Any help will be appreciated.
Best
Vito



=====
Diventare costruttori di soluzioni

"The business of the statistician is to catalyze 
the scientific learning process."  
George E. P. Box


Visitate il portale http://www.modugno.it/
e in particolare la sezione su Palese http://www.modugno.it/archivio/cat_palese.shtml



From busscher at uni-kassel.de  Fri Oct 22 13:24:07 2004
From: busscher at uni-kassel.de (Nicolaas Busscher)
Date: Fri, 22 Oct 2004 13:24:07 +0200
Subject: [R] grouping for lme with nested repeated measurements
Message-ID: <20041022132407.5b660b1e.busscher@uni-kassel.de>

I am using lme to handle repeated measurements.
So far i can follow the examples from the book from pinheiro and bates.
Now i get the problem , that i have "nested" repeated measuremnts, and i cant find out how to do the grouping part of the lme formula.

1.The simple problem ist that i have different Samples , from which i make repeated measurements (each sample is measured 6 times) and i repeat this experiment over several Days, so i get the lme grouping term "random=~1|Days/Sample".

2. Now i am measuring with 2 different measuring Apparatus the same Sample each 6 times, to see how big the difference from the appratus is. 
Because Apparatus is on the level of repeated measurements i cant write Days/Sample/Apparatus. the lme function offers a list() feature to design the grouping, but i didnt understand this, if it is the solution to the problem.

Maybe somebody had the same Problem?
best greetings
N.Busscher
-- 
#---------------------------------------------------
##!! Achtung neue E-Mail adresse: busscher at uni-kassel.de
##   Neue Telephon Nummer
#---------------------------------------------------
Dr.Nicolaas Busscher Universit??t GH Kassel
Nordbahnhofstrasse: 1a, D-37213 Witzenhausen
Phone: 0049-(0)5542-98-1721, Fax: 0049-(0)5542-98-1713



From p.dalgaard at biostat.ku.dk  Fri Oct 22 13:52:07 2004
From: p.dalgaard at biostat.ku.dk (Peter Dalgaard)
Date: 22 Oct 2004 13:52:07 +0200
Subject: [R] cor, cov, method "pairwise.complete.obs"
In-Reply-To: <6.0.1.1.2.20041022123435.02006ec0@stat4ux.stat.ucl.ac.be>
References: <6.0.1.1.2.20041022123435.02006ec0@stat4ux.stat.ucl.ac.be>
Message-ID: <x2u0sndl7c.fsf@biostat.ku.dk>

Eric Lecoutre <lecoutre at stat.ucl.ac.be> writes:

> Hi UseRs,
> 
> I don't want to die beeing idiot...

With age, most of us come to realise that that is the only possible
outcome...
 
> I dont understand the different results between:
> cor() and cov2cov(cov()).
> 
> See this little example:
> 
>  > x=matrix(c(0.5,0.2,0.3,0.1,0.4,NA,0.7,0.2,0.6,0.1,0.4,0.9),ncol=3)
>  > cov2cor(cov(x,use="pairwise.complete.obs"))
>             [,1]       [,2]       [,3]
> [1,]  1.0000000  0.4653400 -0.1159542
> [2,]  0.4653400  1.0000000 -0.7278728
> [3,] -0.1159542 -0.7278728  1.0000000
>  > cor(x,use="pairwise.complete.obs")
>             [,1]       [,2]       [,3]
> [1,]  1.0000000  0.3973597 -0.1159542
> [2,]  0.3973597  1.0000000 -0.9736842
> [3,] -0.1159542 -0.9736842  1.0000000
> 
> 
> My question arises in a context where cor(mydata,
> use="pairwise.complete.obs")  returns correlations on diagonal that
> are near 0.95 (where as my data do have 100 observations and only 12
> missing values...).

[Eh? "correlations on diagonal" are usually 1.00 for me!]
 
> Do cor() and cov() handle the same way the argument "pairwise.complete.obs"?

Obviously not.

It's not massively hard to experiment your way out of this. Consider

> cov(x,use="pairwise.complete.obs")
             [,1]        [,2]         [,3]
[1,]  0.029166667  0.02000000 -0.006666667
[2,]  0.020000000  0.06333333 -0.061666667
[3,] -0.006666667 -0.06166667  0.113333333

The diagonal elements of this is

> apply(x,2,var,na.rm=T)
[1] 0.02916667 0.06333333 0.11333333

Now, for the correlation calculation, you would arguably need

> var(x[!is.na(x[,2]),1])
[1] 0.04

and we have

> 0.02/sqrt(0.029166667*0.06333333)
[1] 0.4653401

> 0.02/sqrt(0.04*0.06333333)
[1] 0.3973597

which are the two versions of the correlation that you see. 

I.e., with cov(), you only require pairwise completeness for the
off-diagonal terms and remove individual NAs for the diagonal terms
(would be a bit difficult even to define them otherwise!). With cor()
the variances are computed separately for each (x,y) pair.

-- 
   O__  ---- Peter Dalgaard             Blegdamsvej 3  
  c/ /'_ --- Dept. of Biostatistics     2200 Cph. N   
 (*) \(*) -- University of Copenhagen   Denmark      Ph: (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)             FAX: (+45) 35327907



From andy_liaw at merck.com  Fri Oct 22 13:56:43 2004
From: andy_liaw at merck.com (Liaw, Andy)
Date: Fri, 22 Oct 2004 07:56:43 -0400
Subject: [R] Convert a list in a dataframe
Message-ID: <3A822319EB35174CA3714066D590DCD50994E201@usrymx25.merck.com>

Try something like:

as.data.frame(do.call("rbind", coeff))

HTH,
Andy

> From: Vito Ricci
> 
> Hi,
> 
> I've a list containing parameters (intercepts &
> coefficients) of 12 regressions fitted
> 
> > coeff
> [[1]]
>  (Intercept)         anno 
> -427017.1740     217.0588 
> 
> [[2]]
>  (Intercept)         anno 
> -39625.82146     21.78025 
> 
> .....
> 
> [[12]]
> (Intercept)        anno 
> 257605.0343   -129.7646 
> 
> I want create a data frame with two columns (intercept
> and anno)using data in these list.
> 
> Any help will be appreciated.
> Best
> Vito
> 
> 
> 
> =====
> Diventare costruttori di soluzioni
> 
> "The business of the statistician is to catalyze 
> the scientific learning process."  
> George E. P. Box
> 
> 
> Visitate il portale http://www.modugno.it/
> e in particolare la sezione su Palese 
> http://www.modugno.it/archivio/cat_palese.shtm> l
> 
> 
> ______________________________________________
> 
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
> 
>



From p.dalgaard at biostat.ku.dk  Fri Oct 22 14:09:47 2004
From: p.dalgaard at biostat.ku.dk (Peter Dalgaard)
Date: 22 Oct 2004 14:09:47 +0200
Subject: [R] Convert a list in a dataframe
In-Reply-To: <20041022112127.63039.qmail@web41207.mail.yahoo.com>
References: <20041022112127.63039.qmail@web41207.mail.yahoo.com>
Message-ID: <x2pt3bdkdw.fsf@biostat.ku.dk>

Vito Ricci <vito_ricci at yahoo.com> writes:

> Hi,
> 
> I've a list containing parameters (intercepts &
> coefficients) of 12 regressions fitted
> 
> > coeff
> [[1]]
>  (Intercept)         anno 
> -427017.1740     217.0588 
> 
> [[2]]
>  (Intercept)         anno 
> -39625.82146     21.78025 
> 
> .....
> 
> [[12]]
> (Intercept)        anno 
> 257605.0343   -129.7646 
> 
> I want create a data frame with two columns (intercept
> and anno)using data in these list.

data.frame(do.call("rbind",coeff))

Did this result from an lapply() construct? If so, consider using
sapply(); then you can just do

data.frame(t(coeff))

-- 
   O__  ---- Peter Dalgaard             Blegdamsvej 3  
  c/ /'_ --- Dept. of Biostatistics     2200 Cph. N   
 (*) \(*) -- University of Copenhagen   Denmark      Ph: (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)             FAX: (+45) 35327907



From ligges at statistik.uni-dortmund.de  Fri Oct 22 14:14:21 2004
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Fri, 22 Oct 2004 14:14:21 +0200
Subject: [R] Convert a list in a dataframe
In-Reply-To: <20041022112127.63039.qmail@web41207.mail.yahoo.com>
References: <20041022112127.63039.qmail@web41207.mail.yahoo.com>
Message-ID: <4178F99D.1020302@statistik.uni-dortmund.de>

Vito Ricci wrote:

> Hi,
> 
> I've a list containing parameters (intercepts &
> coefficients) of 12 regressions fitted
> 
> 
>>coeff
> 
> [[1]]
>  (Intercept)         anno 
> -427017.1740     217.0588 
> 
> [[2]]
>  (Intercept)         anno 
> -39625.82146     21.78025 
> 
> .....
> 
> [[12]]
> (Intercept)        anno 
> 257605.0343   -129.7646 
> 
> I want create a data frame with two columns (intercept
> and anno)using data in these list.


coeffD <- as.data.frame(t(matrix(unlist(coeff), , 2)))
names(coeffD) <- names(coeff[[1]])

Uwe Ligges



> Any help will be appreciated.
> Best
> Vito
> 
> 
> 
> =====
> Diventare costruttori di soluzioni
> 
> "The business of the statistician is to catalyze 
> the scientific learning process."  
> George E. P. Box
> 
> 
> Visitate il portale http://www.modugno.it/
> e in particolare la sezione su Palese http://www.modugno.it/archivio/cat_palese.shtml
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html



From luk111111 at yahoo.com  Fri Oct 22 14:35:31 2004
From: luk111111 at yahoo.com (lu kan)
Date: Fri, 22 Oct 2004 05:35:31 -0700 (PDT)
Subject: [R] confidence interval
In-Reply-To: <4178F99D.1020302@statistik.uni-dortmund.de>
Message-ID: <20041022123531.61321.qmail@web61301.mail.yahoo.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20041022/ded9c6b2/attachment.pl

From jeaneid at chass.utoronto.ca  Fri Oct 22 15:20:53 2004
From: jeaneid at chass.utoronto.ca (Jean Eid)
Date: Fri, 22 Oct 2004 09:20:53 -0400
Subject: [R] bquote inside a legend
Message-ID: <Pine.SGI.4.40.0410220913520.47548610-100000@origin.chass.utoronto.ca>

I am trying to automate an outide of the plot region table that has the
correlation coef. of the various variables in the plot. I am currently
using bquote to get the greek/latex representation of $\rho$. what I want
so what I have is the following

  rhoa <- cor(x, data1$No.Msa.Hosp)
  rhon <- cor(x, data1$N.Not.Profit.msa)
  rhop <- cor(x, data1$N.Profit.msa)

 temp <- bquote(list(rho["Not-profit]==.(rhon),rho["Profit"]==.(rhop),
                rho["All"]==.(rhoa)))

everything works fine but the problem is that the following legend call
  legend(par("usr")[2]-a1,3*par("usr")[3]-b1,ex12, ncol=2)

does not respect the ncol argument. I am guessing it is respecting it but
bquote is of dimension 1 i.e just a big string. Does anyone know of a way
to break this into two columns


Thank you



From ligges at statistik.uni-dortmund.de  Fri Oct 22 15:42:12 2004
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Fri, 22 Oct 2004 15:42:12 +0200
Subject: [R] confidence interval
In-Reply-To: <20041022123531.61321.qmail@web61301.mail.yahoo.com>
References: <20041022123531.61321.qmail@web61301.mail.yahoo.com>
Message-ID: <41790E34.3080809@statistik.uni-dortmund.de>

lu kan wrote:

> I used a dataset to fit a linear model. For each new sample, a precition can be obtained using the fitted model. Now I wonder is there any way to calculate the confidence interval of the precition for new samples? Any suggestion or pointing to a reference would be very appreciated.
>  
> Lu

You already found predict(), so read ?predict.lm carefully and find that 
there is a argument "interval".

Uwe Ligges



From buddanaak at ornl.gov  Fri Oct 22 15:38:53 2004
From: buddanaak at ornl.gov (Aruna Buddana)
Date: Fri, 22 Oct 2004 09:38:53 -0400
Subject: [R] Incompatibility between R-2.0.0 and Rggobi_1.0-0.
Message-ID: <41790D6D.2040405@ornl.gov>

Hello,
I was trying to install Rggobi in the latest version of R and it gave me 
a compilation error for
 R CMD INSTALL Rggobi_1.0-0.tar.gz

RSGGobi.C: In function RS_GGOBI_init
(R_IsNaNorNA) undeclared (line 77)


I searched for this function in R src files and it was abandoned in 
~/R-2.0.0/src/include/R_ext/Arith.h file.
Only functions

int R_IsNA(double);        /* True for R's NA only */
int R_IsNaN(double);        /* True for special NaN, *not* for NA */


are declared leaving out R_IsNaNorNA.
This funcion was declared in R-1.9.1.

So I changed (R_IsNaNorNA) to (R_IsNA) in  ~/Rggobi/src. Then I built 
Rggobi and installed it using R CMD

Can anyone tell me if there is any other way to do it? Was I right?

Thanks,
Sincerely,
Aruna Buddana,
Intern, Statistics and Data Sciences
Oak Ridge National Laboratories.



From wolfram at fischer-zim.ch  Fri Oct 22 15:46:19 2004
From: wolfram at fischer-zim.ch (Wolfram Fischer)
Date: Fri, 22 Oct 2004 15:46:19 +0200
Subject: [R] ave gives unexpected NA's
Message-ID: <20041022134619.GA3452@s1x.local>

[R 2.0.0 on Linux]

I tried:
> df <- data.frame(
     grp1=factor( c('A' ,'A' ,'A' ,'D', 'D' ) ) ,
     grp2=factor( c('a1','a2','a2','d1','d1') )
     )
> df
  grp1 grp2 val
1    A   a1   1
2    A   a2   2
3    A   a2   4
4    D   d1   8
5    D   d1  16

I got:
> with( df, ave( val, grp1, grp2, FUN=sum ) )
[1]  1 24 24 NA NA

I have expected to get:
[1]  1 6 24

Do I misunderstand something with `ave' or is there a bug?

Thanks - Wolfram



From Dominic.Kay at Sun.COM  Fri Oct 22 15:49:55 2004
From: Dominic.Kay at Sun.COM (Dominic Kay)
Date: Fri, 22 Oct 2004 14:49:55 +0100
Subject: [R] Interval Arithmetic & Queueing Models
Message-ID: <41791003.9040505@Sun.COM>

I want to do two things in R and have searched the mail archive and 
packages to see if they have been done before. I could not see anything. 
If anyone has tackled these problem domains before I would be grateful 
for help.

1.   Has anyone implemented interval arithmetic (see generally 
http://www.cs.utep.edu/interval-comp/ ) either directly in R code or by 
linking in an external C++/Fortran library?

2.  Has anyone implemented queueing modelling ( see e.g the software at 
http://www.perfdynamics.com/ ).

I would be grateful for pointers. Please include me as an addressee on 
any response as I am not on this mailing list.
regards
Dominic



From ripley at stats.ox.ac.uk  Fri Oct 22 16:08:09 2004
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Fri, 22 Oct 2004 15:08:09 +0100 (BST)
Subject: [R] Incompatibility between R-2.0.0 and Rggobi_1.0-0.
In-Reply-To: <41790D6D.2040405@ornl.gov>
Message-ID: <Pine.LNX.4.44.0410221505320.6079-100000@gannet.stats>

This is discussed in 200update.txt file on developer.r-project.org, which 
tells you the right way to do it.

BTW, the incompatility is of Rggobi_1.0-0 with R-2.0.0 and not the other 
way round.

On Fri, 22 Oct 2004, Aruna Buddana wrote:

> Hello,
> I was trying to install Rggobi in the latest version of R and it gave me 
> a compilation error for
>  R CMD INSTALL Rggobi_1.0-0.tar.gz
> 
> RSGGobi.C: In function RS_GGOBI_init
> (R_IsNaNorNA) undeclared (line 77)
> 
> 
> I searched for this function in R src files and it was abandoned in 
> ~/R-2.0.0/src/include/R_ext/Arith.h file.
> Only functions
> 
> int R_IsNA(double);        /* True for R's NA only */
> int R_IsNaN(double);        /* True for special NaN, *not* for NA */
> 
> 
> are declared leaving out R_IsNaNorNA.
> This funcion was declared in R-1.9.1.
> 
> So I changed (R_IsNaNorNA) to (R_IsNA) in  ~/Rggobi/src. Then I built 
> Rggobi and installed it using R CMD
> 
> Can anyone tell me if there is any other way to do it? Was I right?

No.


-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From ligges at statistik.uni-dortmund.de  Fri Oct 22 16:41:16 2004
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Fri, 22 Oct 2004 16:41:16 +0200
Subject: [R] bquote inside a legend
In-Reply-To: <Pine.SGI.4.40.0410220913520.47548610-100000@origin.chass.utoronto.ca>
References: <Pine.SGI.4.40.0410220913520.47548610-100000@origin.chass.utoronto.ca>
Message-ID: <41791C0C.2040500@statistik.uni-dortmund.de>

Jean Eid wrote:

> I am trying to automate an outide of the plot region table that has the
> correlation coef. of the various variables in the plot. I am currently
> using bquote to get the greek/latex representation of $\rho$. what I want
> so what I have is the following
> 
>   rhoa <- cor(x, data1$No.Msa.Hosp)
>   rhon <- cor(x, data1$N.Not.Profit.msa)
>   rhop <- cor(x, data1$N.Profit.msa)
> 
>  temp <- bquote(list(rho["Not-profit]==.(rhon),rho["Profit"]==.(rhop),
>                 rho["All"]==.(rhoa)))
> 
> everything works fine but the problem is that the following legend call
>   legend(par("usr")[2]-a1,3*par("usr")[3]-b1,ex12, ncol=2)
> 
> does not respect the ncol argument. I am guessing it is respecting it but
> bquote is of dimension 1 i.e just a big string. Does anyone know of a way
> to break this into two columns


There is an example how to work with mathematical annotation in legend() 
in the Help Desk in R News 2/3.
Please specify easily reproducible examples (not examples with data we 
do not have) - according to the posting guide!

What you are probably going to do is something like:

plot(1:10)
rhoa <- 1
rhon <- 2
rhop <- 3
exprhoa <- substitute(rho["All"]==.(rhoa), list(rhoa = rhoa))
exprhon <- substitute(rho["Not-profit"]==.(rhon), list(rhon = rhon))
exprhop <- substitute(rho["Profit"]==.(rhop), list(rhop = rhop))
legend(2, 8, do.call("expression", list(exprhon, exprhop, exprhoa)), 
ncol = 2)


Uwe Ligges


> 
> Thank you
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html



From adi at roda.ro  Fri Oct 22 16:43:43 2004
From: adi at roda.ro (Adrian Dusa)
Date: Fri, 22 Oct 2004 17:43:43 +0300
Subject: [R] console under Mandrake
Message-ID: <200410221444.i9MEiCpR023183@hypatia.math.ethz.ch>

Hello,

I recently compiled R 2.0.0 under Mandrake 9, but it won't run unless in a
terminal; is there a way to run it in a console, like in Windows?

TIA,
Adrian

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
Adrian Dusa
Romanian Social Data Archive
1, Schitu Magureanu Bd.
050025 Bucharest sector 5
Romania
Tel./Fax: +40 (21) 312.66.18\ 
          +40 (21) 312.02.10/ int.101



From ripley at stats.ox.ac.uk  Fri Oct 22 17:05:54 2004
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Fri, 22 Oct 2004 16:05:54 +0100 (BST)
Subject: [R] console under Mandrake
In-Reply-To: <200410221444.i9MEiCpR023183@hypatia.math.ethz.ch>
Message-ID: <Pine.LNX.4.44.0410221600330.6201-100000@gannet.stats>

On Fri, 22 Oct 2004, Adrian Dusa wrote:

> I recently compiled R 2.0.0 under Mandrake 9, but it won't run unless in a
> terminal; is there a way to run it in a console, like in Windows?

Yes.  Did you read the manual the INSTALL file pointed you to?
See appendix B.6 in the version I am looking at.

Or see `An Introduction to R' appendix B.1 and look for --gui.

[To run the GNOME console I think you need R-patched, not 2.0.0 as 
distributed.]

Another approach is to run John Fox's Rcmdr package that provides a 
console.

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From p.dalgaard at biostat.ku.dk  Fri Oct 22 17:04:10 2004
From: p.dalgaard at biostat.ku.dk (Peter Dalgaard)
Date: 22 Oct 2004 17:04:10 +0200
Subject: [R] ave gives unexpected NA's
In-Reply-To: <20041022134619.GA3452@s1x.local>
References: <20041022134619.GA3452@s1x.local>
Message-ID: <x2d5zaeqvp.fsf@biostat.ku.dk>

Wolfram Fischer <wolfram at fischer-zim.ch> writes:

> [R 2.0.0 on Linux]
> 
> I tried:
> > df <- data.frame(
>      grp1=factor( c('A' ,'A' ,'A' ,'D', 'D' ) ) ,
>      grp2=factor( c('a1','a2','a2','d1','d1') )
>      )
> > df
>   grp1 grp2 val
> 1    A   a1   1
> 2    A   a2   2
> 3    A   a2   4
> 4    D   d1   8
> 5    D   d1  16
> 
> I got:
> > with( df, ave( val, grp1, grp2, FUN=sum ) )
> [1]  1 24 24 NA NA
> 
> I have expected to get:
> [1]  1 6 24
> 
> Do I misunderstand something with `ave' or is there a bug?

Both. You should have expected 1 6 6 24 24.

Apparently the culprit is this line inside ave()

  unlist(lapply(split(x, g), FUN))[g]

which acts up if there are empty levels of g. It works with

  unlist(lapply(split(x, g), FUN))[as.character(g)]

but that doesn't strike me as the right solution.

-- 
   O__  ---- Peter Dalgaard             Blegdamsvej 3  
  c/ /'_ --- Dept. of Biostatistics     2200 Cph. N   
 (*) \(*) -- University of Copenhagen   Denmark      Ph: (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)             FAX: (+45) 35327907



From luk111111 at yahoo.com  Fri Oct 22 17:41:09 2004
From: luk111111 at yahoo.com (lu kan)
Date: Fri, 22 Oct 2004 08:41:09 -0700 (PDT)
Subject: [R] confidence interval
In-Reply-To: <41790E34.3080809@statistik.uni-dortmund.de>
Message-ID: <20041022154109.68629.qmail@web61309.mail.yahoo.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20041022/d3e924ea/attachment.pl

From p.dalgaard at biostat.ku.dk  Fri Oct 22 18:10:34 2004
From: p.dalgaard at biostat.ku.dk (Peter Dalgaard)
Date: 22 Oct 2004 18:10:34 +0200
Subject: [R] console under Mandrake
In-Reply-To: <Pine.LNX.4.44.0410221600330.6201-100000@gannet.stats>
References: <Pine.LNX.4.44.0410221600330.6201-100000@gannet.stats>
Message-ID: <x28y9yent1.fsf@biostat.ku.dk>

Prof Brian Ripley <ripley at stats.ox.ac.uk> writes:

> On Fri, 22 Oct 2004, Adrian Dusa wrote:
> 
> > I recently compiled R 2.0.0 under Mandrake 9, but it won't run unless in a
> > terminal; is there a way to run it in a console, like in Windows?
> 
> Yes.  Did you read the manual the INSTALL file pointed you to?
> See appendix B.6 in the version I am looking at.
> 
> Or see `An Introduction to R' appendix B.1 and look for --gui.
> 
> [To run the GNOME console I think you need R-patched, not 2.0.0 as 
> distributed.]
> 
> Another approach is to run John Fox's Rcmdr package that provides a 
> console.

Actually, that one is more of a script submission device. (It has
nowhere to just type and press Enter, you need to edit, select, and
press Submit. Which is a good thing for some modes of operation.)

You'll find that the Linux consoles are not nearly as developed as the
Windows one and hardly anyone is using --gui. There are two good reasons:

1) It's really not that horrible to use the command line in a terminal
   window on Linux. 

2) Many people like to use ESS (see the FAQ) and run everything from
   Emacs.

and of course the bad reason: That there isn't much there. However,
had there been a real need, someone would likely have put in the
relevant improvements. 

-- 
   O__  ---- Peter Dalgaard             Blegdamsvej 3  
  c/ /'_ --- Dept. of Biostatistics     2200 Cph. N   
 (*) \(*) -- University of Copenhagen   Denmark      Ph: (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)             FAX: (+45) 35327907



From stefan.albrecht at allianz.com  Fri Oct 22 18:14:20 2004
From: stefan.albrecht at allianz.com (stefan.albrecht@allianz.com)
Date: Fri, 22 Oct 2004 18:14:20 +0200
Subject: [R] invalid for factors not really working
Message-ID: <OF8A62A317.E41B8350-ONC1256F35.005847C3-C1256F35.005933DB@inside.allianz.de>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20041022/c1610b61/attachment.pl

From andy_liaw at merck.com  Fri Oct 22 18:35:02 2004
From: andy_liaw at merck.com (Liaw, Andy)
Date: Fri, 22 Oct 2004 12:35:02 -0400
Subject: [R] confidence interval
Message-ID: <3A822319EB35174CA3714066D590DCD50994E208@usrymx25.merck.com>

No, but this question has been asked fairly recently and work-around posted.
Please search the archive.

Andy

> From: lu kan
> 
> Is there the similar thing for non-linear 'nls'? 
> 
> Lu
> Uwe Ligges <ligges at statistik.uni-dortmund.de> wrote:
> lu kan wrote:
> 
> > I used a dataset to fit a linear model. For each new 
> sample, a precition can be obtained using the fitted model. 
> Now I wonder is there any way to calculate the confidence 
> interval of the precition for new samples? Any suggestion or 
> pointing to a reference would be very appreciated.
> > 
> > Lu
> 
> You already found predict(), so read ?predict.lm carefully 
> and find that 
> there is a argument "interval".
> 
> Uwe Ligges
> 
> 		
> ---------------------------------
> 
> vote.yahoo.com - Register online to vote today!
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
> 
>



From sdfrost at ucsd.edu  Fri Oct 22 18:38:27 2004
From: sdfrost at ucsd.edu (Simon Frost)
Date: Fri, 22 Oct 2004 09:38:27 -0700
Subject: [R] Assist on R-2.0.0 /64 bit AMD/SuSE 9.1
In-Reply-To: <Pine.LNX.4.44.0410220722190.1539-100000@gannet.stats>
References: <Pine.LNX.4.44.0410220722190.1539-100000@gannet.stats>
Message-ID: <200410220938.27062.sdfrost@ucsd.edu>

Dear Brian,

Apologies;I do mean 'assist' rather than 'Assist'; there are versions for R 
and for SPlus. I doubt whether it's to do with calling conventions; the 
functions sometimes work, sometimes they don't, and cause a segmentation 
fault.

Thanks
Simon
-- 
Simon D.W. Frost, MA DPhil
Adjunct Assistant Professor
Department of Pathology
University of California, San Diego
UCSD Antiviral Research Center
150 W. Washington St.
San Diego, CA 92103
USA
Tel: +1 619 543 8080 Ext 275
Fax: +1 619 298 0177
Email: sdfrost at ucsd.edu



From songj at ucalgary.ca  Fri Oct 22 18:58:30 2004
From: songj at ucalgary.ca (songj@ucalgary.ca)
Date: Fri, 22 Oct 2004 10:58:30 -0600
Subject: [R] R
Message-ID: <200410221658.i9MGwUr28797@smtp2.ucalgary.ca>

Hi, Everyone:

We are installing R package in our Sun Solaris 9, and falling in trouble. We 
downloaded all package from bioconductor, the R can not be installed in Sun 
Solaris 9. We greatly appreciated if you could give us any suggestions. 
Thanks.

Yours

Song



From a.buness at dkfz-heidelberg.de  Fri Oct 22 19:05:05 2004
From: a.buness at dkfz-heidelberg.de (Andreas Buness)
Date: Fri, 22 Oct 2004 19:05:05 +0200
Subject: [R] How to save a complete image of the current state of R  ? 
Message-ID: <41793DC1.60304@dkfz-heidelberg.de>

Hello,

I like to save the complete state of R, i.e. including
all environments, objects/workspaces, loaded packages etc..

This wish has arisen since I am not able to reproduce
an error which occurs when running R CMD check.

Many thanks for your advice in advance.
Best Regards
Andreas



From kbartz at loyaltymatrix.com  Fri Oct 22 19:19:43 2004
From: kbartz at loyaltymatrix.com (Kevin Bartz)
Date: Fri, 22 Oct 2004 10:19:43 -0700
Subject: [R] How to save a complete image of the current state of R  ?
In-Reply-To: <41793DC1.60304@dkfz-heidelberg.de>
References: <41793DC1.60304@dkfz-heidelberg.de>
Message-ID: <4179412F.5070609@loyaltymatrix.com>

Andreas Buness wrote:
> Hello,
> 
> I like to save the complete state of R, i.e. including
> all environments, objects/workspaces, loaded packages etc..
> 
> This wish has arisen since I am not able to reproduce
> an error which occurs when running R CMD check.
> 
> Many thanks for your advice in advance.
> Best Regards
> Andreas
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
> 

Does save.image meet your needs?

Kevin



From ripley at stats.ox.ac.uk  Fri Oct 22 19:20:09 2004
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Fri, 22 Oct 2004 18:20:09 +0100 (BST)
Subject: [R] confidence interval
In-Reply-To: <20041022154109.68629.qmail@web61309.mail.yahoo.com>
Message-ID: <Pine.LNX.4.44.0410221715140.6592-100000@gannet.stats>

On Fri, 22 Oct 2004, lu kan wrote:

> Is there the similar thing for non-linear 'nls'? 

Do read the documentation for predict.nls, which also has an argument
"interval", which is what you asked.

And this time, read even more carefully.  Then look in the recent
archives, as the posting guide asks, for this question was discussed
recently.

> Uwe Ligges <ligges at statistik.uni-dortmund.de> wrote:
> lu kan wrote:
> 
> > I used a dataset to fit a linear model. For each new sample, a precition can be obtained using the fitted model. Now I wonder is there any way to calculate the confidence interval of the precition for new samples? Any suggestion or pointing to a reference would be very appreciated.
> 
> You already found predict(), so read ?predict.lm carefully and find that 
> there is a argument "interval".

> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From ligges at statistik.uni-dortmund.de  Fri Oct 22 19:20:35 2004
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Fri, 22 Oct 2004 19:20:35 +0200
Subject: [R] How to save a complete image of the current state of R  ?
In-Reply-To: <41793DC1.60304@dkfz-heidelberg.de>
References: <41793DC1.60304@dkfz-heidelberg.de>
Message-ID: <41794163.2050405@statistik.uni-dortmund.de>

Andreas Buness wrote:
> Hello,
> 
> I like to save the complete state of R, i.e. including
> all environments, objects/workspaces, loaded packages etc..

I think what you are going to do is not available.

You can save the workspace, attach the data and load formerly loaded 
packages again, of course.

Uwe Ligges

> This wish has arisen since I am not able to reproduce
> an error which occurs when running R CMD check.
> 
> Many thanks for your advice in advance.
> Best Regards
> Andreas
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html



From ligges at statistik.uni-dortmund.de  Fri Oct 22 19:20:47 2004
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Fri, 22 Oct 2004 19:20:47 +0200
Subject: [R] not-mentioned problems on installing R under Solaris; was: R
In-Reply-To: <200410221658.i9MGwUr28797@smtp2.ucalgary.ca>
References: <200410221658.i9MGwUr28797@smtp2.ucalgary.ca>
Message-ID: <4179416F.40302@statistik.uni-dortmund.de>

songj at ucalgary.ca wrote:

> Hi, Everyone:
> 
> We are installing R package in our Sun Solaris 9, and falling in trouble. We 
> downloaded all package from bioconductor, the R can not be installed in Sun 
> Solaris 9. We greatly appreciated if you could give us any suggestions. 
> Thanks.
> 
> Yours
> 
> Song
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html

Please read the line above which is appended at each R-help message for 
very well reasons. So "PLEASE do read the posting guide! 
http://www.R-project.org/posting-guide.html"!

Please use a sensible subject!

Please tell us about your problems. Many users have installed both R and 
Bioconductor packages under Solaris.

So please specify the error messages that appear when you try to install 
R. And provide all information about versions and compiler you are using.

Also, please send Bioconductor related messages to the Bioconductor 
list. (BTW: people on that list - and every list I am aware of - also 
will appreciate messages compose along the guidlines of the posting guide.)

Uwe Ligges



From ramasamy at cancer.org.uk  Fri Oct 22 19:22:05 2004
From: ramasamy at cancer.org.uk (Adaikalavan Ramasamy)
Date: Fri, 22 Oct 2004 18:22:05 +0100
Subject: [R] R
In-Reply-To: <200410221658.i9MGwUr28797@smtp2.ucalgary.ca>
References: <200410221658.i9MGwUr28797@smtp2.ucalgary.ca>
Message-ID: <1098465725.3131.22.camel@ramasamy.stats>

You will need to get R working first before installing BioConductor
packages.

Please provide a more useful output (like the error message) if you want
useful help.

Have you tried reading the R manual or searching the mail archives [2].

[1] http://cran.r-project.org/doc/manuals/R-admin.pdf
[2] http://cran.r-project.org/search.html


On Fri, 2004-10-22 at 17:58, songj at ucalgary.ca wrote:
> Hi, Everyone:
> 
> We are installing R package in our Sun Solaris 9, and falling in trouble. We 
> downloaded all package from bioconductor, the R can not be installed in Sun 
> Solaris 9. We greatly appreciated if you could give us any suggestions. 
> Thanks.
> 
> Yours
> 
> Song
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>



From ramasamy at cancer.org.uk  Fri Oct 22 19:23:56 2004
From: ramasamy at cancer.org.uk (Adaikalavan Ramasamy)
Date: Fri, 22 Oct 2004 18:23:56 +0100
Subject: [R] How to save a complete image of the current state of R  ?
In-Reply-To: <41793DC1.60304@dkfz-heidelberg.de>
References: <41793DC1.60304@dkfz-heidelberg.de>
Message-ID: <1098465836.3131.24.camel@ramasamy.stats>

Perhaps save.image(file="lala.rda", compress=T) followed by
load("lala.rda") to re-load it.


On Fri, 2004-10-22 at 18:05, Andreas Buness wrote:
> Hello,
> 
> I like to save the complete state of R, i.e. including
> all environments, objects/workspaces, loaded packages etc..
> 
> This wish has arisen since I am not able to reproduce
> an error which occurs when running R CMD check.
> 
> Many thanks for your advice in advance.
> Best Regards
> Andreas
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>



From jsekhon at fas.harvard.edu  Fri Oct 22 19:36:22 2004
From: jsekhon at fas.harvard.edu (Jasjeet Singh Sekhon)
Date: Fri, 22 Oct 2004 13:36:22 -0400
Subject: [R] [R-pkgs] New Package for Multivariate and Propensity Score
	Matching
Message-ID: <16761.17686.677020.587408@ls01.fas.harvard.edu>


"Matching" version 0.48 is now available on CRAN.

Matching provides functions for estimating causal effects by
multivariate and propensity score matching. The package includes a
variety of univariate and multivariate tests to determine if balance
has been obtained by the matching procedure. These tests can also be
used to determine if an experiment or quasi-experiment is balanced on
baseline covariates.  The functions provide valid standard errors and
allow one to estimate various estimands.

For documentation and further details see:

http://jsekhon.fas.harvard.edu/matching

Cheers,
Jas.

======================================
Jasjeet S. Sekhon
Associate Professor
Harvard University
Center for Basic Research in the 
  Social Sciences
jasjeet_sekhon at harvard.edu
http://jsekhon.fas.harvard.edu/
Office: 617.496.2426 Fax: 617.507.5524

_______________________________________________
R-packages mailing list
R-packages at stat.math.ethz.ch
https://stat.ethz.ch/mailman/listinfo/r-packages



From 0034058 at fudan.edu.cn  Fri Oct 22 19:22:32 2004
From: 0034058 at fudan.edu.cn (rongguiwong)
Date: Sat, 23 Oct 2004 01:22:32 +0800
Subject: [R] R
In-Reply-To: <200410221658.i9MGwUr28797@smtp2.ucalgary.ca>
References: <200410221658.i9MGwUr28797@smtp2.ucalgary.ca>
Message-ID: <200410230122.32926.0034058@fudan.edu.cn>

i think install.packages may help.
for example,if i want package car from www.cran.r-project.org
i can use 
>install.packages("car")
you can use ?install.packages to see more details.


 20041023  00:58songj at ucalgary.ca 
> Hi, Everyone:
>
> We are installing R package in our Sun Solaris 9, and falling in trouble.
> We downloaded all package from bioconductor, the R can not be installed in
> Sun Solaris 9. We greatly appreciated if you could give us any suggestions.
> Thanks.
>
> Yours
>
> Song
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide!
> http://www.R-project.org/posting-guide.html



From adi at roda.ro  Fri Oct 22 20:54:09 2004
From: adi at roda.ro (Adrian Dusa)
Date: Fri, 22 Oct 2004 21:54:09 +0300
Subject: [R] console under Mandrake
In-Reply-To: <x28y9yent1.fsf@biostat.ku.dk>
Message-ID: <200410221854.i9MIsXWC024294@hypatia.math.ethz.ch>

Thank you for your both answers,

As you might have guessed, I am initiating myself in the Linux wizardry. I
have absolutely nothing against command line, I was just heavily used to the
Windows console mode; a terminal window is just fine. Actually, I only used
the console to install packages from CRAN, anyway; I'm sure I'll find the
commands for this.
I read the manuals; it probably didn't work for me because I do not use GNOME
but KDE.

So command line it is... and most probably ESS.
Best regards,
Adrian

-----Original Message-----
From: Peter Dalgaard [mailto:p.dalgaard at biostat.ku.dk]
Sent: 22 octombrie 2004 19:11
To: Adrian Dusa; r-help at stat.math.ethz.ch
Subject: Re: [R] console under Mandrake

Prof Brian Ripley <ripley at stats.ox.ac.uk> writes:

> On Fri, 22 Oct 2004, Adrian Dusa wrote:
>
> > I recently compiled R 2.0.0 under Mandrake 9, but it won't run unless in a
> > terminal; is there a way to run it in a console, like in Windows?
>
> Yes.  Did you read the manual the INSTALL file pointed you to?
> See appendix B.6 in the version I am looking at.
>
> Or see `An Introduction to R' appendix B.1 and look for --gui.
>
> [To run the GNOME console I think you need R-patched, not 2.0.0 as
> distributed.]
>
> Another approach is to run John Fox's Rcmdr package that provides a
> console.

Actually, that one is more of a script submission device. (It has
nowhere to just type and press Enter, you need to edit, select, and
press Submit. Which is a good thing for some modes of operation.)

You'll find that the Linux consoles are not nearly as developed as the
Windows one and hardly anyone is using --gui. There are two good reasons:

1) It's really not that horrible to use the command line in a terminal
   window on Linux.

2) Many people like to use ESS (see the FAQ) and run everything from
   Emacs.

and of course the bad reason: That there isn't much there. However,
had there been a real need, someone would likely have put in the
relevant improvements.

--
   O__  ---- Peter Dalgaard             Blegdamsvej 3
  c/ /'_ --- Dept. of Biostatistics     2200 Cph. N
 (*) \(*) -- University of Copenhagen   Denmark      Ph: (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)             FAX: (+45) 35327907



From mark.threefoot at amd.com  Sat Oct 23 00:01:21 2004
From: mark.threefoot at amd.com (Threefoot, Mark)
Date: Fri, 22 Oct 2004 15:01:21 -0700
Subject: [R] R2HTML installation on R 1.9.1
Message-ID: <D484166DB9E4A94BBF3DCC4339B911630153B2F0@SSVLEXMB1.amd.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20041022/450452f7/attachment.pl

From sundar.dorai-raj at PDF.COM  Sat Oct 23 00:08:20 2004
From: sundar.dorai-raj at PDF.COM (Sundar Dorai-Raj)
Date: Fri, 22 Oct 2004 15:08:20 -0700
Subject: [R] R2HTML installation on R 1.9.1
In-Reply-To: <D484166DB9E4A94BBF3DCC4339B911630153B2F0@SSVLEXMB1.amd.com>
References: <D484166DB9E4A94BBF3DCC4339B911630153B2F0@SSVLEXMB1.amd.com>
Message-ID: <417984D4.9040408@pdf.com>



Threefoot, Mark wrote:

> Hello,
> 
> I am trying to install R2HTML version 1.4-3, but am getting errors when I try to use it.  I am running R 1.9.1 on Redhat Linux Ent 3 AS x86_64.  I installed R2HTML using the following command "R CMD INSTALL R2HTML_1.4-3.tar.gz".  I did not get any errors when installing it.  When I launch R and type library('R2HTML'), I get the following error:
> 
> 
>>library('R2HTML')
> 
> Error in loadNamespace(i, c(lib.loc, .libPaths()), keep.source) :
>         There is no package called 'grDevices'
> Error in library("R2HTML") : package/namespace load failed
> 
> Is there a dependency that I need to install prior to using R2HTML?  I could not find any dependencies in any of the R2HTML documentation.  I also could not find a package called "grDevices" on CRAN.  
> 
> Thanks for your help,
> Mark
> 

grDevices is new for R-2.0.0. Either upgrade R or find an older version 
of R2HTML.

http://cran.r-project.org/src/contrib/Archive/

--sundar



From dray at biomserv.univ-lyon1.fr  Sat Oct 23 00:10:44 2004
From: dray at biomserv.univ-lyon1.fr (Stephane DRAY)
Date: Fri, 22 Oct 2004 18:10:44 -0400
Subject: [R] Evaluate a function for various value of parameters
Message-ID: <5.2.1.1.0.20041022181038.01ef6f78@biomserv.univ-lyon1.fr>

Hello list,

I have a problem ... and do not know how to solve it.
I would like create a function that estimate the quality of the fit of 
different functions with different values of parameters.
This problem is related to the following one:
I would like to create a function "evaluatemyfunction" which evaluate a 
function f for different values of parameters. For instance, if
f=function(a,b) a*b
evaluatemyfunction(f,1:10,5) would return:
5
10
15...

I would like that f could have a variable number of arguments. I have begin 
to write a function:

evaluatemyfunction=function(f,...){
b=match.call(expand.dot=F)$...
myg=expand.grid(lapply(b,eval)) #contains all combinations of parameters
for (i in 1:nrow(myg)){

argsasname=paste(paste(names(myg),myg[1,],sep="="),collapse=",")
# ??????
}

}

I have try to construct arguments of the function as character, then parse 
and pass it to f but parse doesn't work:

 > parse(text=paste(paste(names(myg),myg[i,],sep="="),collapse=","))
Error in parse(file, n, text, prompt) : parse error

Any ideas ??

Thanks in advance,

sincerely
St??phane DRAY
-------------------------------------------------------------------------------------------------- 

D??partement des Sciences Biologiques
Universit?? de Montr??al, C.P. 6128, succursale centre-ville
Montr??al, Qu??bec H3C 3J7, Canada

Tel : (514) 343-6111 poste 1233         Fax : (514) 343-2293
E-mail : stephane.dray at umontreal.ca
-------------------------------------------------------------------------------------------------- 

Web                                          http://www.steph280.freesurf.fr/



From pauljohn at ku.edu  Sat Oct 23 00:16:37 2004
From: pauljohn at ku.edu (Paul Johnson)
Date: Fri, 22 Oct 2004 17:16:37 -0500
Subject: [R] dotplot & lattice problems: y axis values and bg color output in
 jpg
Message-ID: <417986C5.7060800@ku.edu>

I have a linux system with Fedora Core 2 and R-2.0.

I was comparing plots made with plot() and dotplot() and discovered a 
problem. Although the dots are positioned correctly, the numerical 
labels in the dotplot y axis are not correct.

I put copies here:

http://lark.cc.ku.edu/~pauljohn/R/plotTrouble1.jpg

That is the "correct" one from plot, with the higest value on y showing 
at 18.

http://lark.cc.ku.edu/~pauljohn/R/plotTrouble2.jpg

That is the dotplot one. The picture is basically the same, except the 
numbers on the y axis only go up to 8.  But the dots are in the correct 
spots and the x axis is labeled correctly.

On the screen, the plots have white backgrounds, but the picture from 
the dotplot turns out gray. Why is that?  Sometimes, if I run another 
lattice plot in the same session, the colors change to the dark 
background, and I suppose the same trouble is happening.  Isn't 
trellis.par.set() going to remain in place for all following trellis plots?


Here's the code I used to make these 2 graphs.

x11(height=6.5,width=6.5)
data2003<- subset(elaine1,POLCYEAR==2003)
plot(data2003$OLDCRASH,data2003$RENUCYC)

modall <- lm(RENUCYC~OLDCRASH,data=data2003)
abline(modall)

dev.copy(device=jpeg,file="plotTrouble1.jpg")
dev.off()
dev.off()

trellis.par.set(theme=col.whitebg(),height=9,width=6.5)
dotplot (RENUCYC~OLDCRASH, data=elaine1, xlab="ECR", 
as.table=T,subset=POLCYEAR==2003,
         panel=function(x,y)
         {
           panel.dotplot(x,y,cex=0.2);
           panel.lmline(x,y);
         }
        )

jpeg(file="plotTrouble2.jpg",bg="white",height=480,width=480)
trellis.last.object()
dev.off()
dev.off()


I tried to force the ylim on the dotplot up to 18, but it just produced 
this even uglier result.

http://lark.cc.ku.edu/~pauljohn/R/plotTrouble3.jpg

-- 
Paul E. Johnson                       email: pauljohn at ku.edu
Dept. of Political Science            http://lark.cc.ku.edu/~pauljohn
1541 Lilac Lane, Rm 504
University of Kansas                  Office: (785) 864-9086
Lawrence, Kansas 66044-3177           FAX: (785) 864-5700



From olafm at tako.de  Sat Oct 23 00:29:55 2004
From: olafm at tako.de (Olaf Mersmann)
Date: Sat, 23 Oct 2004 00:29:55 +0200
Subject: [R] Evaluate a function for various value of parameters
In-Reply-To: <5.2.1.1.0.20041022181038.01ef6f78@biomserv.univ-lyon1.fr>
References: <5.2.1.1.0.20041022181038.01ef6f78@biomserv.univ-lyon1.fr>
Message-ID: <20041022222955.GA26058@kimberly.tako.de>

Hi Stephane,

* Stephane DRAY <dray at biomserv.univ-lyon1.fr> [041023 00:20]:
> Hello list,
> 
> I have a problem ... and do not know how to solve it.
> I would like create a function that estimate the quality of the fit of 
> different functions with different values of parameters.
> This problem is related to the following one:
> I would like to create a function "evaluatemyfunction" which evaluate a 
> function f for different values of parameters. For instance, if
> f=function(a,b) a*b
> evaluatemyfunction(f,1:10,5) would return:
> 5
> 10
> 15...
> 
> I would like that f could have a variable number of arguments. I have begin 
> to write a function:

That function has already been written for you. Lookup 'mapply' in the
online help.

HTH
Olaf



From sundar.dorai-raj at PDF.COM  Sat Oct 23 00:36:52 2004
From: sundar.dorai-raj at PDF.COM (Sundar Dorai-Raj)
Date: Fri, 22 Oct 2004 15:36:52 -0700
Subject: [R] dotplot & lattice problems: y axis values and bg color output
	in jpg
In-Reply-To: <417986C5.7060800@ku.edu>
References: <417986C5.7060800@ku.edu>
Message-ID: <41798B84.8060508@pdf.com>



Paul Johnson wrote:

> I have a linux system with Fedora Core 2 and R-2.0.
> 
> I was comparing plots made with plot() and dotplot() and discovered a 
> problem. Although the dots are positioned correctly, the numerical 
> labels in the dotplot y axis are not correct.
> 
> I put copies here:
> 
> http://lark.cc.ku.edu/~pauljohn/R/plotTrouble1.jpg
> 
> That is the "correct" one from plot, with the higest value on y showing 
> at 18.
> 
> http://lark.cc.ku.edu/~pauljohn/R/plotTrouble2.jpg
> 
> That is the dotplot one. The picture is basically the same, except the 
> numbers on the y axis only go up to 8.  But the dots are in the correct 
> spots and the x axis is labeled correctly.
> 
> On the screen, the plots have white backgrounds, but the picture from 
> the dotplot turns out gray. Why is that?  Sometimes, if I run another 
> lattice plot in the same session, the colors change to the dark 
> background, and I suppose the same trouble is happening.  Isn't 
> trellis.par.set() going to remain in place for all following trellis plots?
> 
> 
> Here's the code I used to make these 2 graphs.
> 
> x11(height=6.5,width=6.5)
> data2003<- subset(elaine1,POLCYEAR==2003)
> plot(data2003$OLDCRASH,data2003$RENUCYC)
> 
> modall <- lm(RENUCYC~OLDCRASH,data=data2003)
> abline(modall)
> 
> dev.copy(device=jpeg,file="plotTrouble1.jpg")
> dev.off()
> dev.off()
> 
> trellis.par.set(theme=col.whitebg(),height=9,width=6.5)
> dotplot (RENUCYC~OLDCRASH, data=elaine1, xlab="ECR", 
> as.table=T,subset=POLCYEAR==2003,
>         panel=function(x,y)
>         {
>           panel.dotplot(x,y,cex=0.2);
>           panel.lmline(x,y);
>         }
>        )
> 
> jpeg(file="plotTrouble2.jpg",bg="white",height=480,width=480)
> trellis.last.object()
> dev.off()
> dev.off()
> 
> 
> I tried to force the ylim on the dotplot up to 18, but it just produced 
> this even uglier result.
> 
> http://lark.cc.ku.edu/~pauljohn/R/plotTrouble3.jpg
> 

Paul,

dotplot is not doing what you think it's doing. RENUCYC is being forced 
to a factor with 9 levels. I think what you intending to use is xyplot. 
Just

trellis.par.set(theme=col.whitebg(),height=9,width=6.5)
xyplot(RENUCYC~OLDCRASH, data=elaine1, xlab="ECR",
        as.table=T,subset=POLCYEAR==2003,
        panel=function(x,y) {
          panel.dotplot(x,y,cex=0.2) #; no need for semi-colons
          panel.lmline(x,y)
        })

As for your second question, when you call jpeg a new device is called. 
You need to reset the theme after opening the device, or just use 
trellis.device:

trellis.device(jpeg, file="plotTrouble2.jpg",
                theme = col.whitebg(),height=480,width=480)
trellis.last.object()
dev.off()


--sundar



From olafm at tako.de  Sat Oct 23 00:38:31 2004
From: olafm at tako.de (Olaf Mersmann)
Date: Sat, 23 Oct 2004 00:38:31 +0200
Subject: [R] dotplot & lattice problems: y axis values and bg color output
	in jpg
In-Reply-To: <417986C5.7060800@ku.edu>
References: <417986C5.7060800@ku.edu>
Message-ID: <20041022223831.GA26780@kimberly.tako.de>

Hi Paul.
* Paul Johnson <pauljohn at ku.edu> [041023 00:24]:
*snip*

> That is the dotplot one. The picture is basically the same, except the 
> numbers on the y axis only go up to 8.  But the dots are in the correct 
> spots and the x axis is labeled correctly.

Unless RENUCYC is a factor, a dotplot makes little sense. Afaik you're
looking for an xyplot. 

> On the screen, the plots have white backgrounds, but the picture from 
> the dotplot turns out gray. Why is that?  

It's the default lattice color sheme which makes for easier viewing on
computer screens.

> Sometimes, if I run another
> lattice plot in the same session, the colors change to the dark 
> background, and I suppose the same trouble is happening.  Isn't 
> trellis.par.set() going to remain in place for all following trellis plots?

To quote the manual:
"Once a device is open, it's settings can be modified. When another
instance of the same device is opened later using trellis.device, the
settings for that device are reset to its defaults, unless otherwise
specified in the call to trellis.device."

HTH
Olaf



From deepayan at stat.wisc.edu  Sat Oct 23 01:26:27 2004
From: deepayan at stat.wisc.edu (Deepayan Sarkar)
Date: Fri, 22 Oct 2004 18:26:27 -0500
Subject: [R] dotplot & lattice problems: y axis values and bg color output
	in jpg
In-Reply-To: <417986C5.7060800@ku.edu>
References: <417986C5.7060800@ku.edu>
Message-ID: <200410221826.27717.deepayan@stat.wisc.edu>

On Friday 22 October 2004 17:16, Paul Johnson wrote:
> I have a linux system with Fedora Core 2 and R-2.0.
>
> I was comparing plots made with plot() and dotplot() and discovered a
> problem. Although the dots are positioned correctly, the numerical
> labels in the dotplot y axis are not correct.

That's comparing apples and oranges, sort of. You should use xyplot 
instead of dotplot if you want the equivalent of plot (more precisely, 
plot.default).

Although it's not obvious from your code, I'll bet your Y variable is 
numeric with 9 unique values. dotplot treats one of its variables as a 
factor. If neither variable are factors, it by default chooses the Y 
variable is a factor. If it is numeric, as in your case, it converts it 
into a 'shingle' (see ?shingle). This has almost the same effect as 
coercion to a factor, except the Y values are labeled 1,2,... instead 
of the levels.

> I put copies here:
>
> http://lark.cc.ku.edu/~pauljohn/R/plotTrouble1.jpg
>
> That is the "correct" one from plot, with the higest value on y
> showing at 18.
>
> http://lark.cc.ku.edu/~pauljohn/R/plotTrouble2.jpg
>
> That is the dotplot one. The picture is basically the same, except
> the numbers on the y axis only go up to 8.  But the dots are in the
> correct spots and the x axis is labeled correctly.

They are not the same. The Y-spacing between the unique values are all 
equal in dotplot, unlike in the plot output.

> On the screen, the plots have white backgrounds, but the picture from
> the dotplot turns out gray. Why is that?  Sometimes, if I run another
> lattice plot in the same session, the colors change to the dark
> background, and I suppose the same trouble is happening.  Isn't
> trellis.par.set() going to remain in place for all following trellis
> plots?

Trellis settings are very much device-specific, and trellis.par.set only 
affects the currently open device (much like par). You call it to 
change the x11 settings, but you haven't changed the jpeg settings 
anywhere, so the defaults are used.

What happens for new instances of the same device is a bit more subtle. 
Once you change settings for a particular device type (say "x11"), 
lattice remembers these settings. However, if you open a new instance 
of this device with, say

trellis.device(x11)

the "x11" settings revert back to the x11 defaults (this behaviour can 
be suppressed by an argument to trellis.device). Now, even if you don't 
do this explicitly, any attempt to create a Trellis plot while there's 
no currently active device results in a call to trellis.device, and 
hence results in a new instance with the settings reset to defaults. 
However, the settings are NOT reset if the device is already open.

An example to make things a bit less confusing (hopefully):

p <- xyplot(1~1)
x11() 
p  ## default (grey) background
trellis.par.set(col.whitebg())
p  ## 'white' (actually transparent) background
dev.off()

x11()
p ## still white background, settings retained
dev.off()

p ## new instance of x11 started, settings reset, 
  ## hence grey background


If you find this surprising, it's basically because lattice has no way 
of knowing that a new device has been opened unless it's done through 
trellis.device. Otherwise, the idea is to be similar to par and reset 
the defaults everytime.

Deepayan



From cjosephlu at seed.net.tw  Sat Oct 23 02:57:39 2004
From: cjosephlu at seed.net.tw (cjosephlu@seed.net.tw)
Date: Sat, 23 Oct 2004 08:57:39 +0800
Subject: [R] How to calculate a double integral ...?
In-Reply-To: <loom.20041021T080712-416@post.gmane.org>
References: <20041021015643.M79703@mail.ncku.edu.tw>
	<loom.20041021T080712-416@post.gmane.org>
Message-ID: <1098493059.4179ac83eb301@webmail.seed.net.tw>

Gabor Grothendieck <ggrothendieck at myway.com> writes:
> cjlu <cjlu <at> mail.ncku.edu.tw> writes:
> : How can I calculate a double integral like
> : 
> :   \int_a^b \int_c^y g(x, y) dx dy
> : 
> : where a, b, c are constants, g(x, y), e.g.,
> : g(x, y) = tan(x + y).
> :  
> Integrate
> 
>    g2 <- function(x, y) g(x, y) * (x > y)
> 
> over (a,b) x (a,c).
> 
Thanks Gabor for your prompt and helpful response!

Later on, I realized that such an integration can be done by
the following way, provided function g(x, y) takes vector input, e.g.,
g(x, y) = tan(x + y)

> integrate(function(y) {
+   sapply(y, function(y) {
+     integrate(function(x) tan(x + y), -.5, y)$value
+   })
+ }, 0, .5)
0.07238855 with absolute error < 1.1e-15

if g(x, y) can only take scalar input, we can do

> integrate(function(y) {
+   sapply(y, function(y) {
+     integrate(function(x) {
+       sapply(x, function(x) tan(x + y))
+     }, -.5, y)$value
+   })
+ }, 0, .5)
0.07238855 with absolute error < 1.1e-15

Best regards,

C. Joseph Lu
Department of Statistics
National Cheng-Kung University
Tainan
Taiwan, ROC

-------------------------------------------------
This mail sent through Seednet Webmail
http://webmail.seed.net.tw



From 0034058 at fudan.edu.cn  Sat Oct 23 03:26:35 2004
From: 0034058 at fudan.edu.cn (rongguiwong)
Date: Sat, 23 Oct 2004 09:26:35 +0800
Subject: [R] Evaluate a function for various value of parameters
In-Reply-To: <5.2.1.1.0.20041022181038.01ef6f78@biomserv.univ-lyon1.fr>
References: <5.2.1.1.0.20041022181038.01ef6f78@biomserv.univ-lyon1.fr>
Message-ID: <200410230926.35299.0034058@fudan.edu.cn>

 20041023  06:10Stephane DRAY 
> Hello list,
>
> I have a problem ... and do not know how to solve it.
> I would like create a function that estimate the quality of the fit of
> different functions with different values of parameters.
> This problem is related to the following one:
> I would like to create a function "evaluatemyfunction" which evaluate a
> function f for different values of parameters. For instance, if
> f=function(a,b) a*b
> evaluatemyfunction(f,1:10,5) would return:
> 5
> 10
> 15...
> mapply("*",1:10,5)
 [1]  5 10 15 20 25 30 35 40 45 50
> outer(1:10,5,"*")
      [,1]
 [1,]    5
 [2,]   10
 [3,]   15
 [4,]   20
 [5,]   25
 [6,]   30
 [7,]   35
 [8,]   40
 [9,]   45
[10,]   50
is these meet your need?
try
>?outer
>?mapply
to see more details.



From ggrothendieck at myway.com  Sat Oct 23 03:24:41 2004
From: ggrothendieck at myway.com (Gabor Grothendieck)
Date: Sat, 23 Oct 2004 01:24:41 +0000 (UTC)
Subject: [R] Evaluate a function for various value of parameters
References: <5.2.1.1.0.20041022181038.01ef6f78@biomserv.univ-lyon1.fr>
Message-ID: <loom.20041023T032057-731@post.gmane.org>

Stephane DRAY <dray <at> biomserv.univ-lyon1.fr> writes:
: I would like to create a function "evaluatemyfunction" which evaluate a 
: function f for different values of parameters. For instance, if
: f=function(a,b) a*b
: evaluatemyfunction(f,1:10,5) would return:
: 5
: 10
: 15...
: 

This is not a general solution (Olaf has already provided that) but 
in some cases you may simply be able to write your function f so that 
it takes vector arguments.  For example, with the f you have above 
one could write f(1:10,5) .



From ggrothendieck at myway.com  Sat Oct 23 04:40:58 2004
From: ggrothendieck at myway.com (Gabor Grothendieck)
Date: Sat, 23 Oct 2004 02:40:58 +0000 (UTC)
Subject: [R] Evaluate a function for various value of parameters
References: <5.2.1.1.0.20041022181038.01ef6f78@biomserv.univ-lyon1.fr>
	<loom.20041023T032057-731@post.gmane.org>
Message-ID: <loom.20041023T035348-927@post.gmane.org>

Gabor Grothendieck <ggrothendieck <at> myway.com> writes:

: 
: Stephane DRAY <dray <at> biomserv.univ-lyon1.fr> writes:
: : I would like to create a function "evaluatemyfunction" which evaluate a 
: : function f for different values of parameters. For instance, if
: : f=function(a,b) a*b
: : evaluatemyfunction(f,1:10,5) would return:
: : 5
: : 10
: : 15...
: : 
: 
: This is not a general solution (Olaf has already provided that) but 
: in some cases you may simply be able to write your function f so that 
: it takes vector arguments.  For example, with the f you have above 
: one could write f(1:10,5) .


Sorry, I missed the last part of your question where you
provided some code.  That code shows that you want all
combos, not just parallel evaluation of the args so here
it is again.

This answer creates a grid and evaluates.  It makes use of 
do.call and mapply. 

The first two lines of the body collect the args into a list and
create the grid.  The third line reestablishes the names of the
columns since expand.grid may not give the ones we want.
Lastly we preface the grid with the function name and mapply
the result.

	evaluatemyfunction <- function(f, ...) {
	   args <- list(...)
	   grid <- do.call("expand.grid", args)
	   names(grid) <- names(args)
	   do.call("mapply", append("f", grid))
	}



From binabina at bellsouth.net  Sat Oct 23 05:41:28 2004
From: binabina at bellsouth.net (zubin)
Date: Fri, 22 Oct 2004 23:41:28 -0400
Subject: [R] JDBC DB access in R
Message-ID: <MBBBIIHJANJBMHLGMACKCECNCHAA.binabina@bellsouth.net>

Hello, i see packages referencing using ODBC access to databases via R, are
there any packages that use JDBC to submit SQL to a database and return into
a data frame?  

-zubin


From ajayshah at mayin.org  Sat Oct 23 05:21:25 2004
From: ajayshah at mayin.org (Ajay Shah)
Date: Sat, 23 Oct 2004 08:51:25 +0530
Subject: [R] Puzzled at a trivial function using Hmisc::setps
Message-ID: <20041023032125.GA9285@igidr.ac.in>

I have done:

  library(Hmisc)
  mysetps <- function(f) {setps(filename=f, w=4.5)}

Now when I set about doing

  mysetps("a")
  blah
  mysetps("b")
  blah

etc.

Problem: I find that only one file "f.ps" gets created.

My intent had been to create "a.ps", "b.ps", etc., all using the
argument w=4.5 with the setps() function of Hmisc::setps.

Just in case it was R being unhappy with one-letter variable names, as
sometimes happens, I also replaced "f" with "file" and that didn't help.

-- 
Ajay Shah                                                   Consultant
ajayshah at mayin.org                      Department of Economic Affairs
http://www.mayin.org/ajayshah           Ministry of Finance, New Delhi



From ggrothendieck at myway.com  Sat Oct 23 08:22:07 2004
From: ggrothendieck at myway.com (Gabor Grothendieck)
Date: Sat, 23 Oct 2004 06:22:07 +0000 (UTC)
Subject: [R] Puzzled at a trivial function using Hmisc::setps
References: <20041023032125.GA9285@igidr.ac.in>
Message-ID: <loom.20041023T082003-841@post.gmane.org>

Ajay Shah <ajayshah <at> mayin.org> writes:

: 
: I have done:
: 
:   library(Hmisc)
:   mysetps <- function(f) {setps(filename=f, w=4.5)}
: 
: Now when I set about doing
: 
:   mysetps("a")
:   blah
:   mysetps("b")
:   blah
: 
: etc.
: 
: Problem: I find that only one file "f.ps" gets created.
: 
: My intent had been to create "a.ps", "b.ps", etc., all using the
: argument w=4.5 with the setps() function of Hmisc::setps.
: 
: Just in case it was R being unhappy with one-letter variable names, as
: sometimes happens, I also replaced "f" with "file" and that didn't help.
: 

setps assumes the filename is unquoted unless you specify 
type = "char" like this:

mysetps <- function(f) {setps(filename=f, w=4.5, type = "char")}



From ripley at stats.ox.ac.uk  Sat Oct 23 09:02:41 2004
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Sat, 23 Oct 2004 08:02:41 +0100 (BST)
Subject: [R] JDBC DB access in R
In-Reply-To: <MBBBIIHJANJBMHLGMACKCECNCHAA.binabina@bellsouth.net>
Message-ID: <Pine.LNX.4.44.0410230759100.20303-100000@gannet.stats>

The J is for Java (I presume, you did't actually say), and R is written in
C.  So `all' you need to do is to call a Java wrapper from R and call JDBC
from that.  I believe it has been done via RSJava.

But if all you want to do is to retrieve a dataframe, isn't this rather 
hard work?

On Fri, 22 Oct 2004, zubin wrote:

> Hello, i see packages referencing using ODBC access to databases via R, are
> there any packages that use JDBC to submit SQL to a database and return into
> a data frame?  

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From ripley at stats.ox.ac.uk  Sat Oct 23 09:17:05 2004
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Sat, 23 Oct 2004 08:17:05 +0100 (BST)
Subject: [R] Puzzled at a trivial function using Hmisc::setps
In-Reply-To: <20041023032125.GA9285@igidr.ac.in>
Message-ID: <Pine.LNX.4.44.0410230804110.20303-100000@gannet.stats>

Most of us have no idea what this does.  Please ask questions about Hmisc 
of its author (Frank Harrell), or write them in a way accessible to people 
not familiar in detail with the package.

The posting guide asks you to read the help page, and the answer is on 
that help page, although in a less-than-obvious way.  Since the problem is 
with the Hmisc documentation, this really does need to be addressed to the 
author.

Reading the source shows you need setps(filename=f, w=4.5, type="char"),
but the help says

filename: character string specifying file prefix.  For 'setps' or
          'setpdf' omit surrounding quotes unless 'type="char"'.

    type: set 'type=1' to use black on white background, smaller
          pointsize, and other settings that are good for making
          overhead transparencies and graphs to include in reports.
          Set 'type=3' for 5" x 7" landscape plots, and 'time=4' for
          overheads. For 'setps' and 'setpdf', specifies whether
          'filename' is quoted or not.

without telling you what values are accepted or what they do (and indeed
without accurately describing the effect, which is

    filebase <- if (type == "char")
        filename
    else as.character(substitute(filename))

.)


On Sat, 23 Oct 2004, Ajay Shah wrote:

> I have done:
> 
>   library(Hmisc)
>   mysetps <- function(f) {setps(filename=f, w=4.5)}
> 
> Now when I set about doing
> 
>   mysetps("a")
>   blah
>   mysetps("b")
>   blah
> 
> etc.
> 
> Problem: I find that only one file "f.ps" gets created.
> 
> My intent had been to create "a.ps", "b.ps", etc., all using the
> argument w=4.5 with the setps() function of Hmisc::setps.
> 
> Just in case it was R being unhappy with one-letter variable names, as
> sometimes happens, I also replaced "f" with "file" and that didn't help.

*R* has no idea how many letters a variable name has, but you are using 
Hmisc here.

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From binabina at bellsouth.net  Sat Oct 23 14:13:01 2004
From: binabina at bellsouth.net (zubin)
Date: Sat, 23 Oct 2004 08:13:01 -0400
Subject: [R] JDBC DB access in R
In-Reply-To: <Pine.LNX.4.44.0410230759100.20303-100000@gannet.stats>
Message-ID: <MBBBIIHJANJBMHLGMACKOECNCHAA.binabina@bellsouth.net>

Thanks for the reply - thru R i have large data sets in oracle, mssql,
postgres and we have jdbc drivers for all of them - most of these systems
are on UNIX/LINUX and ODBC is not used - we use JDBC to hit the DB's usually
in java applications we have.  other than that we are a large SAS shop and i
am trying to find an alternative to SAS, i think R has a good chance.

For R and SQL - i assume using the wrapper below is the easiest way to hit
databases via SQL - are there any other ways i am missing?  or is JDBC route
the easiest when ODBC is not available?

not sure if R contains some standard database routines that i am not seeing?

also

does R have some good data manipulations function - if so, could you point
me in the right direction (similiar to DATA steps in SAS)

-thx!


-----Original Message-----
From: Prof Brian Ripley [mailto:ripley at stats.ox.ac.uk]
Sent: Saturday, October 23, 2004 3:03 AM
To: zubin
Cc: r-help at stat.math.ethz.ch
Subject: Re: [R] JDBC DB access in R


The J is for Java (I presume, you did't actually say), and R is written in
C.  So `all' you need to do is to call a Java wrapper from R and call JDBC
from that.  I believe it has been done via RSJava.

But if all you want to do is to retrieve a dataframe, isn't this rather
hard work?

On Fri, 22 Oct 2004, zubin wrote:

> Hello, i see packages referencing using ODBC access to databases via R,
are
> there any packages that use JDBC to submit SQL to a database and return
into
> a data frame?

--
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From sarah_english82 at yahoo.co.uk  Sat Oct 23 14:34:04 2004
From: sarah_english82 at yahoo.co.uk (sarah english)
Date: Sat, 23 Oct 2004 13:34:04 +0100 (BST)
Subject: [R] Plotting Bivariate Normal Data
Message-ID: <20041023123404.21350.qmail@web25101.mail.ukl.yahoo.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20041023/7d406afb/attachment.pl

From baysmacs at blueyonder.co.uk  Sat Oct 23 19:31:39 2004
From: baysmacs at blueyonder.co.uk ( Andrew Philpott)
Date: Sat, 23 Oct 2004 18:31:39 +0100
Subject: [R] Maple files in R
Message-ID: <ECECIKEIHOEPEIHLCNKIIEMOCAAA.baysmacs@blueyonder.co.uk>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20041023/4cae435c/attachment.pl

From edd at debian.org  Sat Oct 23 20:04:54 2004
From: edd at debian.org (Dirk Eddelbuettel)
Date: Sat, 23 Oct 2004 13:04:54 -0500
Subject: [R] JDBC DB access in R
In-Reply-To: <MBBBIIHJANJBMHLGMACKOECNCHAA.binabina@bellsouth.net>
References: <Pine.LNX.4.44.0410230759100.20303-100000@gannet.stats>
	<MBBBIIHJANJBMHLGMACKOECNCHAA.binabina@bellsouth.net>
Message-ID: <20041023180454.GA31182@sonny.eddelbuettel.com>

On Sat, Oct 23, 2004 at 08:13:01AM -0400, zubin wrote:
> Thanks for the reply - thru R i have large data sets in oracle, mssql,
> postgres and we have jdbc drivers for all of them - most of these systems
> are on UNIX/LINUX and ODBC is not used - we use JDBC to hit the DB's usually
> in java applications we have.  other than that we are a large SAS shop and i
> am trying to find an alternative to SAS, i think R has a good chance.

Oracle and Postgres have binary connection packages on CRAN, they are likely
to be more efficient than JDBC or ODBC.

For Sybase, we managed to get something working (on Solaris) using the
FreeTDS interface. We actually used FreeTDS and one of the Unix ODBC
packages so that our "client" code would run unchanged from windows and unix
programs -- using the ODBC abstraction.

I can't speak to JDBC.

> For R and SQL - i assume using the wrapper below is the easiest way to hit
> databases via SQL - are there any other ways i am missing?  or is JDBC route
> the easiest when ODBC is not available?

Probably rather the hardest, as Brian told you.

> not sure if R contains some standard database routines that i am not seeing?

Yup, see above.

> also
> 
> does R have some good data manipulations function - if so, could you point
> me in the right direction (similiar to DATA steps in SAS)

Loads of them, and many tricks are posted in various place. You could start
with the writeups provided by Frank Harrell.

Hth, Dirk
 
> -thx!
> 
> 
> -----Original Message-----
> From: Prof Brian Ripley [mailto:ripley at stats.ox.ac.uk]
> Sent: Saturday, October 23, 2004 3:03 AM
> To: zubin
> Cc: r-help at stat.math.ethz.ch
> Subject: Re: [R] JDBC DB access in R
> 
> 
> The J is for Java (I presume, you did't actually say), and R is written in
> C.  So `all' you need to do is to call a Java wrapper from R and call JDBC
> from that.  I believe it has been done via RSJava.
> 
> But if all you want to do is to retrieve a dataframe, isn't this rather
> hard work?
> 
> On Fri, 22 Oct 2004, zubin wrote:
> 
> > Hello, i see packages referencing using ODBC access to databases via R,
> are
> > there any packages that use JDBC to submit SQL to a database and return
> into
> > a data frame?
> 
> --
> Brian D. Ripley,                  ripley at stats.ox.ac.uk
> Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
> University of Oxford,             Tel:  +44 1865 272861 (self)
> 1 South Parks Road,                     +44 1865 272866 (PA)
> Oxford OX1 3TG, UK                Fax:  +44 1865 272595
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html

-- 
Those are my principles, and if you don't like them... well, I have others.
                                                -- Groucho Marx



From graumann at caltech.edu  Sat Oct 23 20:04:25 2004
From: graumann at caltech.edu (Johannes Graumann)
Date: Sat, 23 Oct 2004 11:04:25 -0700
Subject: [R] Legend/Substitute/Plotmath problem
Message-ID: <20041023110425.2b58d3df@localhost>

Hello,

I seem unable to construct a legend which contains a substitution as
well as math symbols. I'm trying to do the following:

strain2 <- "YJG48"

legend.txt <- c(
	substitute(
		strain *
		%==% *
		"YJG45, rpn10" *
		%Delta%,
		list(strain=strain2)
	),
	"Verhulst/Logistic",
	"Malthus"
)
legend(
	100,2.5,
	legend.txt,
	cex=0.75,
	bty="n",
	pch=c(20,NA,NA),
	lty=c(NA,1,2)

I derived this from the following "text" command which works, 

text(
	160,2.2,
	cex=0.5,
	adj=0,
	substitute(
		OD[600][~nm] * 
		" of 2 at " *
		time2_2 *
		" min" 
		%~~% 
		time2h_2 * 
		" h",
		list(
			time2_2=round(time2_2,digits=0),
			time2h_2=round(time2_2/60,digits=1)
		)
	)
)

But I can't get it to work in the context of "legend".
Any hints/ideas?

Thanks, Joh



From xpsun at ict.ac.cn  Sun Oct 24 09:24:23 2004
From: xpsun at ict.ac.cn (XP Sun)
Date: Sun, 24 Oct 2004 15:24:23 +0800
Subject: [R] How to use a matrix in pcurve?
Message-ID: <200410240724.i9O7OUa5019660@hypatia.math.ethz.ch>

Hi, Everyone,

I want to calculate the principal curve of a points set.
First I read the points'coordinate with function "scan", 
then converted it to matrix with the function "matrix",
and fit the curve with function "principal.curve".

Here is my data in the file "bmn007.data": 
0.023603	 -0.086540	 -0.001533
0.024349	 -0.083877	 -0.001454
..
..
0.025004	 -0.083690	 -0.001829
0.025562	 -0.083877	 -0.001857
0.026100	 -0.083877	 0.000090
0.025965	 -0.083877	 0.002574

and the code as follow:

pp <- scan("bmn007.data", quiet= TRUE)
x <- matrix(pp, nc=2, byrow=TRUE)
fit <- principal.curve(x, plot = TRUE)
points(fit,col="red")

By now, I got a right result. 
But when i changed to use pcurve with matrix x as "pcurve(x)",
an error was thrown as following:

Estimating starting configuration using : CA
Error in h %*% diag(sqrt(d)) : non-conformable arguments

How to convert a matrix to the format could be accepted by "pcurve"?
Any help appreciated!

Regards
- Sun



From jari.oksanen at oulu.fi  Sun Oct 24 09:49:18 2004
From: jari.oksanen at oulu.fi (Jari Oksanen)
Date: Sun, 24 Oct 2004 10:49:18 +0300
Subject: [R] How to use a matrix in pcurve?
In-Reply-To: <200410240724.i9O7OUa5019660@hypatia.math.ethz.ch>
References: <200410240724.i9O7OUa5019660@hypatia.math.ethz.ch>
Message-ID: <3585ADB0-2591-11D9-972D-000A95C76CA8@oulu.fi>

Sun,
On 24 Oct 2004, at 10:24, XP Sun wrote:

> Hi, Everyone,
>
> I want to calculate the principal curve of a points set.
> First I read the points'coordinate with function "scan",
> then converted it to matrix with the function "matrix",
> and fit the curve with function "principal.curve".
>
> Here is my data in the file "bmn007.data":
> 0.023603	 -0.086540	 -0.001533
> 0.024349	 -0.083877	 -0.001454
> ..
> ..
> 0.025004	 -0.083690	 -0.001829
> 0.025562	 -0.083877	 -0.001857
> 0.026100	 -0.083877	 0.000090
> 0.025965	 -0.083877	 0.002574
>
> and the code as follow:
>
> pp <- scan("bmn007.data", quiet= TRUE)
> x <- matrix(pp, nc=2, byrow=TRUE)
> fit <- principal.curve(x, plot = TRUE)
> points(fit,col="red")
>
> By now, I got a right result.
> But when i changed to use pcurve with matrix x as "pcurve(x)",
> an error was thrown as following:
>
> Estimating starting configuration using : CA
> Error in h %*% diag(sqrt(d)) : non-conformable arguments
>
> How to convert a matrix to the format could be accepted by "pcurve"?
> Any help appreciated!
>
Sun,

The canonical answer is "ask De'ath" (the author of the package). The 
rest is guessing. It seems that pcurve uses correspondence analysis 
("CA") to estimate the starting configuration. CA doesn't handle cases 
where any of the marginal sums (row or column sums) are negative or 
zero. Do you have this kind of cases? If so, can you get rid of them? 
Does pcurve have another option than CA for getting the starting 
configuration?

cheers, jari oksanen
--
Jari Oksanen, Oulu, Finland



From renaud.lancelot at cirad.fr  Sun Oct 24 16:07:13 2004
From: renaud.lancelot at cirad.fr (Renaud Lancelot)
Date: Sun, 24 Oct 2004 16:07:13 +0200
Subject: [R] lme4, GLMM, var-cov matrix of fixed effects
Message-ID: <417BB711.4080800@cirad.fr>

Dear all,

1. How do I compute the fixed-effects var-cov matrix from a GLMM object 
(lme4 package) fitted with the Laplace method?

details: the vcov method for GLMM objects does not seem to work in this 
case: returns a 0 * 0 matrix:

 > fm1 <- GLMM(immun ~ kid2p + mom25p + ord,
+          family = binomial, data = guImmun, random = ~ 1 | comm, 
method = "Laplace")
Using optimizer nlm

 > fm1
Generalized Linear Mixed Model

Family: binomial family with logit link
Fixed: immun ~ kid2p + mom25p + ord
Data: guImmun
  log-likelihood:  -1400.842
Random effects:
      Groups        Name    Variance    Std.Dev.
        comm (Intercept)     0.61621     0.78499
# of obs: 2159, groups: comm, 161

Estimated scale (compare to 1)  0.9702206

Fixed effects:
              Estimate Std. Error z value  Pr(>|z|)
(Intercept) -0.970969   0.162467 -5.9764 2.281e-09
kid2pY       0.991837   0.119242  8.3178 < 2.2e-16
mom25pY     -0.052423   0.129561 -0.4046    0.6858
ord23       -0.094408   0.140715 -0.6709    0.5023
ord46        0.040217   0.167089  0.2407    0.8098
ord7p        0.013805   0.206611  0.0668    0.9467
      (Intercept)    kid2pY     mom25pY       ord23      ord46      ord7p
[1,]   -0.970969 0.9918369 -0.05242273 -0.09440838 0.04021697 0.01380546

 > vcov(fm1)
<0 x 0 matrix>

#############

2. Is it relevant and efficient to use this var-cov matrix to perform 
Wald-like tests for the significance of one or more fixed effects ?


version information:

# R         _
platform i386-pc-mingw32
arch     i386
os       mingw32
system   i386, mingw32
status
major    2
minor    0.0
year     2004
month    10
day      04
language R

# lme4
Package:       lme4
Version:       0.6-9
Date:          2004-10-04

# Matrix
Package:       Matrix
Version:       0.8-15
Date:          2004-10-02

#########

Thanks,

Renaud


-- 
Dr Renaud Lancelot, v??t??rinaire
C/0 Ambassade de France - SCAC
BP 834 Antananarivo 101 - Madagascar

e-mail: renaud.lancelot at cirad.fr
tel.:   +261 32 40 165 53 (cell)
         +261 20 22 665 36 ext. 225 (work)
         +261 20 22 494 37 (home)



From jfox at mcmaster.ca  Sun Oct 24 18:37:09 2004
From: jfox at mcmaster.ca (John Fox)
Date: Sun, 24 Oct 2004 12:37:09 -0400
Subject: [R] Plotting Bivariate Normal Data
In-Reply-To: <20041023123404.21350.qmail@web25101.mail.ukl.yahoo.com>
Message-ID: <20041024163708.HIZC29162.tomts5-srv.bellnexxia.net@JohnDesktop8300>

Dear Sarah,

If the data are allegedly bivariate normal, then they are probably two
vectors, not one. Assuming that this is the case, I know of nothing quite as
neat as a univariate QQ plot to check visually for bivariate normality
(perhaps someone else has a suggestion here), but you could superimpose
bivariate-normal contours on a scatterplot of the data, perhaps along with a
bivariate density estimate. The car and ellipse packages can do the former,
while the locfit and sm packages (and possibly others) can do the latter.

I hope this helps,
 John

--------------------------------
John Fox
Department of Sociology
McMaster University
Hamilton, Ontario
Canada L8S 4M4
905-525-9140x23604
http://socserv.mcmaster.ca/jfox 
-------------------------------- 

> -----Original Message-----
> From: r-help-bounces at stat.math.ethz.ch 
> [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of sarah english
> Sent: Saturday, October 23, 2004 7:34 AM
> To: r-help at stat.math.ethz.ch
> Subject: [R] Plotting Bivariate Normal Data
> 
> 
> Dear list 
>  
> I have a vector of values that allegedly have a bivariate 
> normal distribution. 
> 
>  
> 
> I  want to create a plot that shows the values I have 
> obtained, and the  bivariate normal distribution curve for the data.
> 
> 
> Is there a  way of doing this in R? 
> 
> 
>  
> 
> Many thanks for your help, 
> 
> Sarah.
> 
> 
> 
> 		
> ---------------------------------
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html



From ckjmaner at carolina.rr.com  Sun Oct 24 21:10:59 2004
From: ckjmaner at carolina.rr.com (Charles and Kimberly Maner)
Date: Sun, 24 Oct 2004 15:10:59 -0400
Subject: [R] JDBC DB access in R
Message-ID: <200410241911.i9OJBJkd002769@ms-smtp-03-eri0.southeast.rr.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20041024/7307b15a/attachment.pl

From attenka at utu.fi  Sun Oct 24 22:32:49 2004
From: attenka at utu.fi (Atte Tenkanen)
Date: Sun, 24 Oct 2004 16:32:49 -0400
Subject: [R] From soundcard to R?
Message-ID: <417C1171.5040004@utu.fi>

Is it possible to input data from sound card of the computer to R? What 
do I need to know about my computer (it's linux pc)? Can I get some real 
time graphical information this way, spektrum for example?

Atte



From Gregor.Gorjanc at bfro.uni-lj.si  Mon Oct 25 00:04:30 2004
From: Gregor.Gorjanc at bfro.uni-lj.si (Gorjanc Gregor)
Date: Mon, 25 Oct 2004 00:04:30 +0200
Subject: [R] Installing packages on Debian linux
Message-ID: <7FFEE688B57D7346BC6241C55900E7300FCF2B@pollux.bfro.uni-lj.si>

Hello R users!

I am running R on windows and linux (Debian). Until recently I found
the function 

install.packages('R-package_name')

which is just great and simple. It is standard for installing packages
for me under windows. Under Debian linux  I got used to installing packages
vie apt-get tool and also R CMD INSTALL. As I can see R is using the 
same/similiar packaging system (dependencies, ...) as Debian. Is it wise
to mix all three methods? 

Which method is prefered for installation of R packages under Debian linux?
- apt-get install R-package_name
- install.packages('R-package_name')

I actually never tried with install.packages under linux. Where does it put
the filesm under /usr/local or standard path?


Thanks!

--
Lep pozdrav / With regards,
    Gregor GORJANC

---------------------------------------------------------------
University of Ljubljana
Biotechnical Faculty       URI: http://www.bfro.uni-lj.si
Zootechnical Department    email: gregor.gorjanc <at> bfro.uni-lj.si
Groblje 3                  tel: +386 (0)1 72 17 861
SI-1230 Domzale            fax: +386 (0)1 72 41 005
Slovenia



From kjetil at acelerate.com  Mon Oct 25 03:40:16 2004
From: kjetil at acelerate.com (Kjetil Brinchmann Halvorsen)
Date: Sun, 24 Oct 2004 21:40:16 -0400
Subject: [R] Plotting Bivariate Normal Data
In-Reply-To: <20041024163708.HIZC29162.tomts5-srv.bellnexxia.net@JohnDesktop8300>
References: <20041024163708.HIZC29162.tomts5-srv.bellnexxia.net@JohnDesktop8300>
Message-ID: <417C5980.4080907@acelerate.com>

John Fox wrote:

>Dear Sarah,
>
>If the data are allegedly bivariate normal, then they are probably two
>vectors, not one. Assuming that this is the case, I know of nothing quite as
>neat as a univariate QQ plot to check visually for bivariate normality
>(perhaps someone else has a suggestion here), but you could superimpose
>  
>
What about using Independent component analysis to find the most 
non-normal linear combination
and make a qq-plot of that?

Something like:

library(fastICA)    # on CRAN
x <- matrix(rchisq(10000, df=2), 1000, 10)
P <- svd(matrix(rnorm(100), 10,10))$u    #  random orthogonal matrix
y <- x %*% P
 y.ica <- fastICA(y, 2)
 qqnorm(y.ica$S[,1])

But I have never seen this suggested in print, although it seems quite
natural. Anybody knows if something like this is considered a reasonable 
test
of multivariate normality?

Kjetil






>bivariate-normal contours on a scatterplot of the data, perhaps along with a
>bivariate density estimate. The car and ellipse packages can do the former,
>while the locfit and sm packages (and possibly others) can do the latter.
>
>I hope this helps,
> John
>
>--------------------------------
>John Fox
>Department of Sociology
>McMaster University
>Hamilton, Ontario
>Canada L8S 4M4
>905-525-9140x23604
>http://socserv.mcmaster.ca/jfox 
>-------------------------------- 
>
>  
>
>>-----Original Message-----
>>From: r-help-bounces at stat.math.ethz.ch 
>>[mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of sarah english
>>Sent: Saturday, October 23, 2004 7:34 AM
>>To: r-help at stat.math.ethz.ch
>>Subject: [R] Plotting Bivariate Normal Data
>>
>>
>>Dear list 
>> 
>>I have a vector of values that allegedly have a bivariate 
>>normal distribution. 
>>
>> 
>>
>>I  want to create a plot that shows the values I have 
>>obtained, and the  bivariate normal distribution curve for the data.
>>
>>
>>Is there a  way of doing this in R? 
>>
>>
>> 
>>
>>Many thanks for your help, 
>>
>>Sarah.
>>
>>
>>
>>		
>>---------------------------------
>>
>>	[[alternative HTML version deleted]]
>>
>>______________________________________________
>>R-help at stat.math.ethz.ch mailing list
>>https://stat.ethz.ch/mailman/listinfo/r-help
>>PLEASE do read the posting guide! 
>>http://www.R-project.org/posting-guide.html
>>    
>>
>
>______________________________________________
>R-help at stat.math.ethz.ch mailing list
>https://stat.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>
>
>  
>


-- 

Kjetil Halvorsen.

Peace is the most effective weapon of mass construction.
               --  Mahdi Elmandjra



From MSchwartz at MedAnalytics.com  Mon Oct 25 03:59:40 2004
From: MSchwartz at MedAnalytics.com (Marc Schwartz)
Date: Sun, 24 Oct 2004 20:59:40 -0500
Subject: [R] OT: Linux Trojan Warning - Red Hat and Fedora Users
Message-ID: <1098669579.28228.11.camel@localhost.localdomain>

Greetings all,

Just a heads up to the masses that there is a new Linux trojan warning
this weekend.

An e-mail purporting to come from Red Hat's Security Team contains
information on a vulnerability in fileutils, with a URL to a site that
contains a patch.

The site is http://www.fedora-redhat.com/

A whois of the domain shows that it is owned by Yahoo, not Red Hat:

http://reports.internic.net/cgi/whois?whois_nic=fedora-redhat.com&type=domain

More information is showing up on the web:

http://it.slashdot.org/it/04/10/24/2352234.shtml?tid=172&tid=110&tid=218&tid=106

and Red Hat is aware of the issue:

http://www.redhat.com/security/

Best regards to all,

Marc Schwartz



From sluque at mun.ca  Mon Oct 25 05:35:50 2004
From: sluque at mun.ca (Sebastian Luque)
Date: Sun, 24 Oct 2004 22:35:50 -0500
Subject: [R] Installing packages on Debian linux
References: <7FFEE688B57D7346BC6241C55900E7300FCF2B@pollux.bfro.uni-lj.si>
Message-ID: <clhs5n$rbd$1@sea.gmane.org>

Gorjanc Gregor wrote:

> same/similiar packaging system (dependencies, ...) as Debian. Is it wise
> to mix all three methods?

Mixing R CMD and install.packages() doesn't pose any problems, AFAIK, but
mixing any of them with apt-get does. The problem is that apt-get doesn't
know what you're doing with R CMD or install.packages() or remove.packages(),
so you may end up with duplicate packages (possibly in different places, see
below). I generally try to leave as much package management up to the system,
so I choose apt-get whenever possible. However, not all CRAN packages are
available from Debian yet, so you may end up having to use install.packages()
to install some packages.


> Which method is prefered for installation of R packages under Debian linux?
> - apt-get install R-package_name
> - install.packages('R-package_name')
> 
> I actually never tried with install.packages under linux. Where does it put
> the filesm under /usr/local or standard path?

install.packages() will use /usr/local/lib/R/site-library as default, but you
can change that with the lib argument. apt-get will put them
in /usr/lib/R/site-library.

I have a question to add myself though: is it still necessary to add a line
like:

deb http://cran.r-project.org/bin/linux/debian woody main

to /etc/apt/sources.list, as indicated in the CRAN mirrors? This might be
redundant, as a lot of packages are already available through the Debian
mirrors anyway, but I haven't checked how the list compares with the one in
CRAN.

Dirk? (Dirk Eddelbuettel is the maintainer of R Debian packages)

-- 
Best wishes,
Sebastian



From matt.kowgier at utoronto.ca  Mon Oct 25 08:02:06 2004
From: matt.kowgier at utoronto.ca (matt.kowgier@utoronto.ca)
Date: Mon, 25 Oct 2004 02:02:06 -0400
Subject: [R] sample variogram construction
Message-ID: <1098684126.417c96de334e1@webmail.utoronto.ca>

Hi

Im attempting to build a sample variogram for 300 obersvations
of longitudinal data. So what I need to do is compute the half 
squared differences  between pairs of residuals (for instance
if a subject has 4 obersvations, this is 4 choose 2 paird differences)
for each subject.
Also, then I need the corresponding time differences within each
individual. So the end result will be a 300 by 2 matrix with
columns corresponding to paired difference residuals within subject
and time differences within subject.
Basically im having trouble coding this kind of matrix in R,
if anyone can help me out or give me some tips id appreciate it.

Thanks.
Stuck in the for loop
student



From Lorenz.Gygax at fat.admin.ch  Mon Oct 25 08:11:21 2004
From: Lorenz.Gygax at fat.admin.ch (Lorenz.Gygax@fat.admin.ch)
Date: Mon, 25 Oct 2004 08:11:21 +0200
Subject: [R] grouping for lme with nested repeated measurements
Message-ID: <BF74FADD4B44554CA7E53D0B5242CD6A01FC6232@evd-s7014.bk.evdad.admin.ch>


> 1.The simple problem ist that i have different Samples , from 
> which i make repeated measurements (each sample is measured 6 
> times) and i repeat this experiment over several Days, so i 
> get the lme grouping term "random=~1|Days/Sample".

I would rather specify this as: random= ~ 1 | Sample/Days, but I
am not quite sure how this affects the model.

> 2. Now i am measuring with 2 different measuring Apparatus 
> the same Sample each 6 times, to see how big the difference 
> from the appratus is. 
> Because Apparatus is on the level of repeated measurements i 
> cant write Days/Sample/Apparatus. the lme function offers a 
> list() feature to design the grouping, but i didnt understand 
> this, if it is the solution to the problem.

Why would you want to include the Apparatus in the random effect?
I assume that you are interested in differences and thus, this
is a fixed effect:

lme (fixed= response ~ apparatus, data= XX, random= ~ 1 | Sample/Days)

Lorenz
- 
Lorenz Gygax,  lorenz.gygax at fat.admin.ch      
Centre for proper housing of ruminants and pigs
Swiss Federal Veterinary Office



From zangshizhu at yahoo.com.cn  Mon Oct 25 08:24:33 2004
From: zangshizhu at yahoo.com.cn (shizhu zang)
Date: Mon, 25 Oct 2004 14:24:33 +0800 (CST)
Subject: [R] unable to open connection
Message-ID: <20041025062433.10601.qmail@web15805.mail.cnb.yahoo.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20041025/f4d68f5e/attachment.pl

From gerifalte28 at hotmail.com  Mon Oct 25 08:24:48 2004
From: gerifalte28 at hotmail.com (F Z)
Date: Mon, 25 Oct 2004 06:24:48 +0000
Subject: [R] grouping for lme with nested repeated measurements
Message-ID: <BAY2-F42pKDre7SfwAo000192fd@hotmail.com>

A variable is considered to be nested when it is unique within that 
combination.  i.e. if your measuring Apparatus was different at each sample. 
  Hence, as Lorenz suggested it is reasonable to treat Apparatus as fixed, 
specially if you are considering making comparisons between the 2 apparatus.

I hope that this helps


Francisco

>From: Lorenz.Gygax at fat.admin.ch
>To: r-help at stat.math.ethz.ch
>Subject: RE: [R] grouping for lme with nested repeated measurements
>Date: Mon, 25 Oct 2004 08:11:21 +0200
>
>
> > 1.The simple problem ist that i have different Samples , from
> > which i make repeated measurements (each sample is measured 6
> > times) and i repeat this experiment over several Days, so i
> > get the lme grouping term "random=~1|Days/Sample".
>
>I would rather specify this as: random= ~ 1 | Sample/Days, but I
>am not quite sure how this affects the model.
>
> > 2. Now i am measuring with 2 different measuring Apparatus
> > the same Sample each 6 times, to see how big the difference
> > from the appratus is.
> > Because Apparatus is on the level of repeated measurements i
> > cant write Days/Sample/Apparatus. the lme function offers a
> > list() feature to design the grouping, but i didnt understand
> > this, if it is the solution to the problem.
>
>Why would you want to include the Apparatus in the random effect?
>I assume that you are interested in differences and thus, this
>is a fixed effect:
>
>lme (fixed= response ~ apparatus, data= XX, random= ~ 1 | Sample/Days)
>
>Lorenz
>-
>Lorenz Gygax,  lorenz.gygax at fat.admin.ch
>Centre for proper housing of ruminants and pigs
>Swiss Federal Veterinary Office
>
>______________________________________________
>R-help at stat.math.ethz.ch mailing list
>https://stat.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide! 
>http://www.R-project.org/posting-guide.html



From maechler at stat.math.ethz.ch  Mon Oct 25 08:43:12 2004
From: maechler at stat.math.ethz.ch (Martin Maechler)
Date: Mon, 25 Oct 2004 08:43:12 +0200
Subject: [R] Plotting Bivariate Normal Data
In-Reply-To: <20041024163708.HIZC29162.tomts5-srv.bellnexxia.net@JohnDesktop8300>
References: <20041023123404.21350.qmail@web25101.mail.ukl.yahoo.com>
	<20041024163708.HIZC29162.tomts5-srv.bellnexxia.net@JohnDesktop8300>
Message-ID: <16764.41088.224111.571644@gargle.gargle.HOWL>

>>>>> "JohnF" == John Fox <jfox at mcmaster.ca>
>>>>>     on Sun, 24 Oct 2004 12:37:09 -0400 writes:

    JohnF> Dear Sarah, If the data are allegedly bivariate
    JohnF> normal, then they are probably two vectors, not
    JohnF> one. Assuming that this is the case, I know of
    JohnF> nothing quite as neat as a univariate QQ plot to
    JohnF> check visually for bivariate normality (perhaps
    JohnF> someone else has a suggestion here), but you could
    JohnF> superimpose bivariate-normal contours on a
    JohnF> scatterplot of the data, perhaps along with a
    JohnF> bivariate density estimate. The car and ellipse
    JohnF> packages can do the former, while the locfit and sm
    JohnF> packages (and possibly others) can do the latter.

Since one of the more severe and common deviations from
normality is "long tailed"ness (in all it's vaguety), we have
been recommending to QQ-plot mahalanobis distances against chi
squared quantiles - even before looking at the univariate
QQ plots.

Exactly for this reason, in R,
	example(mahalanobis)
shows a version of how to do this!

Martin Maechler, ETH Zurich



From ripley at stats.ox.ac.uk  Mon Oct 25 09:11:30 2004
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Mon, 25 Oct 2004 08:11:30 +0100 (BST)
Subject: [R] unable to open connection
In-Reply-To: <20041025062433.10601.qmail@web15805.mail.cnb.yahoo.com>
Message-ID: <Pine.LNX.4.44.0410250808240.15035-100000@gannet.stats>

Actually, you are unable to connect to the Internet from R on your 
computer.  Perhaps you are not connected, or need to set a proxy?

Please read the rw-FAQ, as the posting guide asks.

Note that right now you there are no packages on CRAN and Bioconductor has
not yet released packages for R 2.0.x.  The first is a short-term problem,
and the second needs a little patience (look at
http://www.bioconductor.org which has variously mentioned Oct 25 and Nov
1).

On Mon, 25 Oct 2004, shizhu zang wrote:

> Hi , there:
> I used function source to download the package 
> but found
>  
> > source("http://www.bioconductor.org/getBioC.R")
> Error in file(file, "r") : unable to open connection
> In addition: Warning message: 
> unable to resolve 'www.bioconductor.org'. 
>  
>  
> Then I downloaded the packages from CRAN
> and found 
> > local({a <- CRAN.packages()
> + install.packages(select.list(a[,1],,TRUE), .libPaths()[1], available=a, dependencies=TRUE)})
> trying URL `http://cran.r-project.org/bin/windows/contrib/2.0/PACKAGES'
> Error in download.file(url = paste(contriburl, "PACKAGES", sep = "/"),  : 
>         cannot open URL `http://cran.r-project.org/bin/windows/contrib/2.0/PACKAGES'
> In addition: Warning message: 
> unable to resolve 'cran.r-project.org'. 
> 
> I  use the Window operation system
> I don't konw what is the matter

> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From YiYao_Jiang at smics.com  Mon Oct 25 09:16:34 2004
From: YiYao_Jiang at smics.com (YiYao_Jiang)
Date: Mon, 25 Oct 2004 15:16:34 +0800
Subject: [R] Multiple formula in one block
Message-ID: <6DB56153640A0A41A4FF6C5B9761609306D823@ex103.smic-sh.com>

Hi Everybody:

I want to draw some chart using command "histogram" and add another curve in it.
Example:

require(stats)
data(singer)
library(lattice)
histogram( ~ height | voice.part, data = singer, nint = 17,  endpoints = c(59.5, 76.5), layout = c(2,4), aspect = .5,  xlab = "Height (inches)")

Now I got a chart of 8 blocks, then I need add :" abline(v=mean(singer$height), col=2, lwd=2) " or any other curve to every block.
How can I do to achieve it. Thanks.
 

Best Regards

YiYao Jiang  

Product Division/ product Testing Department
Semiconductor Manufacturing International Corporation
18 ZhangJiang Road, PuDong New Area, Shanghai  ZIP: 201203
Tel:86-21-5080-2000 Ext. 15173



From dimitris.rizopoulos at med.kuleuven.ac.be  Mon Oct 25 09:39:18 2004
From: dimitris.rizopoulos at med.kuleuven.ac.be (Dimitris Rizopoulos)
Date: Mon, 25 Oct 2004 09:39:18 +0200
Subject: [R] Multiple formula in one block
References: <6DB56153640A0A41A4FF6C5B9761609306D823@ex103.smic-sh.com>
Message-ID: <00b001c4ba65$bc30e240$0540210a@www.domain>

Hi YiYao,

you need the `?panel.abline()' function, somehing like:

panel=function(x, breaks, equal.widths, type, ...){
            panel.histogram(x, breaks, equal.widths, type, ...)
            panel.abline(v=mean(singer$height))
        }

inside the `histogram()' function could do the trick.

I hope it helps.

Best,
Dimitris

----
Dimitris Rizopoulos
Ph.D. Student
Biostatistical Centre
School of Public Health
Catholic University of Leuven

Address: Kapucijnenvoer 35, Leuven, Belgium
Tel: +32/16/396887
Fax: +32/16/337015
Web: http://www.med.kuleuven.ac.be/biostat/
     http://www.student.kuleuven.ac.be/~m0390867/dimitris.htm


----- Original Message ----- 
From: "YiYao_Jiang" <YiYao_Jiang at smics.com>
To: <r-help at stat.math.ethz.ch>
Cc: "YiYao_Jiang" <YiYao_Jiang at smics.com>
Sent: Monday, October 25, 2004 9:16 AM
Subject: [R] Multiple formula in one block


> Hi Everybody:
>
> I want to draw some chart using command "histogram" and add another 
> curve in it.
> Example:
>
> require(stats)
> data(singer)
> library(lattice)
> histogram( ~ height | voice.part, data = singer, nint = 17, 
> endpoints = c(59.5, 76.5), layout = c(2,4), aspect = .5,  xlab = 
> "Height (inches)")
>
> Now I got a chart of 8 blocks, then I need add :" 
> abline(v=mean(singer$height), col=2, lwd=2) " or any other curve to 
> every block.
> How can I do to achieve it. Thanks.
>
>
> Best Regards
>
> YiYao Jiang
>
> Product Division/ product Testing Department
> Semiconductor Manufacturing International Corporation
> 18 ZhangJiang Road, PuDong New Area, Shanghai  ZIP: 201203
> Tel:86-21-5080-2000 Ext. 15173
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
>



From YiYao_Jiang at smics.com  Mon Oct 25 09:45:15 2004
From: YiYao_Jiang at smics.com (YiYao_Jiang)
Date: Mon, 25 Oct 2004 15:45:15 +0800
Subject: =?gb2312?B?tPC4tDogW1JdIE11bHRpcGxlIGZvcm11bGEgaW4gb25lIGJsb2Nr?=
Message-ID: <6DB56153640A0A41A4FF6C5B9761609306EB66@ex103.smic-sh.com>

Hi Dimitris:

Thanks for your help, I will try.

BR
Yiyao


----------
: Dimitris Rizopoulos [mailto:dimitris.rizopoulos at med.kuleuven.ac.
be]
: 20041025 15:39
: YiYao_Jiang
: r-help at stat.math.ethz.ch
: Re: [R] Multiple formula in one block


Hi YiYao,

you need the `?panel.abline()' function, somehing like:

panel=function(x, breaks, equal.widths, type, ...){
            panel.histogram(x, breaks, equal.widths, type, ...)
            panel.abline(v=mean(singer$height))
        }

inside the `histogram()' function could do the trick.

I hope it helps.

Best,
Dimitris

----
Dimitris Rizopoulos
Ph.D. Student
Biostatistical Centre
School of Public Health
Catholic University of Leuven

Address: Kapucijnenvoer 35, Leuven, Belgium
Tel: +32/16/396887
Fax: +32/16/337015
Web: http://www.med.kuleuven.ac.be/biostat/
     http://www.student.kuleuven.ac.be/~m0390867/dimitris.htm


----- Original Message ----- 
From: "YiYao_Jiang" <YiYao_Jiang at smics.com>
To: <r-help at stat.math.ethz.ch>
Cc: "YiYao_Jiang" <YiYao_Jiang at smics.com>
Sent: Monday, October 25, 2004 9:16 AM
Subject: [R] Multiple formula in one block


> Hi Everybody:
>
> I want to draw some chart using command "histogram" and add another 
> curve in it.
> Example:
>
> require(stats)
> data(singer)
> library(lattice)
> histogram( ~ height | voice.part, data = singer, nint = 17, 
> endpoints = c(59.5, 76.5), layout = c(2,4), aspect = .5,  xlab = 
> "Height (inches)")
>
> Now I got a chart of 8 blocks, then I need add :" 
> abline(v=mean(singer$height), col=2, lwd=2) " or any other curve to 
> every block.
> How can I do to achieve it. Thanks.
>
>
> Best Regards
>
> YiYao Jiang
>
> Product Division/ product Testing Department
> Semiconductor Manufacturing International Corporation
> 18 ZhangJiang Road, PuDong New Area, Shanghai  ZIP: 201203
> Tel:86-21-5080-2000 Ext. 15173
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
>



From maechler at stat.math.ethz.ch  Mon Oct 25 09:56:35 2004
From: maechler at stat.math.ethz.ch (Martin Maechler)
Date: Mon, 25 Oct 2004 09:56:35 +0200
Subject: [R] Legend/Substitute/Plotmath problem
In-Reply-To: <20041023110425.2b58d3df@localhost>
References: <20041023110425.2b58d3df@localhost>
Message-ID: <16764.45491.531701.421033@gargle.gargle.HOWL>

>>>>> "Johannes" == Johannes Graumann <graumann at caltech.edu>
>>>>>     on Sat, 23 Oct 2004 11:04:25 -0700 writes:
>>>>> "Johannes" == Johannes Graumann <graumann at caltech.edu>
>>>>>     on Sat, 23 Oct 2004 11:04:25 -0700 writes:

    Johannes> Hello,

    Johannes> I seem unable to construct a legend which contains
    Johannes> a substitution as well as math symbols. I'm trying
    Johannes> to do the following:

    >> strain2 <- "YJG48"

    >> legend.txt <- c(
    >> 	substitute(
    >> 		strain *
    >> 		%==% *
    >> 		"YJG45, rpn10" *
    >> 		%Delta%,
    >> 		list(strain=strain2)
    >> 	),
    >> 	"Verhulst/Logistic",
    >> 	"Malthus"
    >> )


    Johannes>     .....................

Do try to break down a problem into simple things --
particularly when you have problems!

This substitute() call is simply invalid:

  > ss <- substitute( strain * %==% * "YJG45, rpn10" * %Delta%, list(strain=strain2) )
  Error: syntax error

and the 'syntax error' should give you a clue:  
The first argument of substitute must be a syntactically correct
R expression.

Now you try more and more simple things till you 'see it' :

Why should I expect  'A * %==% B'  to be valid syntax?
Both '*' and '%==%' are (diadic) operators: You can't juxtapose
them, as well as you can't write  'A * = B'.
Then, '%Delta%' (like any other '%foo%' !!) is a diadic operator
too and hence can't be juxtaposed to '*'. But I'm pretty sure
you rather mean (greek) 'Delta'.

Hence:
 ss <- substitute( strain %==% "YJG45, rpn10" * Delta, list(strain=strain2) )

---

Once you have the expression you can go further;
still step by step :

  > c(ss, "Verhulst")
  [[1]]
  "YJG48" %==% "YJG45, rpn10" * Delta

  [[2]]
  [1] "Verhulst"

Hmm, a list; that won't work.
You do need to pass either a "character" vector or an
expression, i.e., an expression of length 3 in our case.
We must build the expression somewhat manually:

  > e <- expression(1, "Verhulst", "Malthus")# '1' is a place holder
    expression(1, "Verhulst", "Malthus")
  > e[[1]] <- ss  ## that's the trick!

  > str(e)
    expression("YJG48" %==% "YJG45, rpn10" * Delta, "Verhulst", "Malthus")

  > plot(1); legend(1,1, leg = e)

---

Maybe something to be added as an example to help(legend) or rather
to help(expression) ?

HTH,
Martin Maechler, ETH Zurich



From annap at mate.polimi.it  Mon Oct 25 10:05:35 2004
From: annap at mate.polimi.it (Anna Maria Paganoni)
Date: Mon, 25 Oct 2004 10:05:35 +0200
Subject: [R] library gregmisc
Message-ID: <6.1.0.6.1.20041025100211.02670b40@pop2.mate.polimi.it>

I write to ask you an help about the package gregmisc.
I saw the instructions, and I need some functionalities of this package, 
but I am unable ti download it.
On friday I was able to download thte .zip, but R does not install this 
package, today there is no possibility to download it.
What I have to do?
Thanks
Anna Maria Paganoni
					
						Anna



From ligges at statistik.uni-dortmund.de  Mon Oct 25 10:10:06 2004
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Mon, 25 Oct 2004 10:10:06 +0200
Subject: [R] unable to open connection
In-Reply-To: <Pine.LNX.4.44.0410250808240.15035-100000@gannet.stats>
References: <Pine.LNX.4.44.0410250808240.15035-100000@gannet.stats>
Message-ID: <417CB4DE.10509@statistik.uni-dortmund.de>



Prof Brian Ripley wrote:

> Actually, you are unable to connect to the Internet from R on your 
> computer.  Perhaps you are not connected, or need to set a proxy?
> 
> Please read the rw-FAQ, as the posting guide asks.
> 
> Note that right now you there are no packages on CRAN and Bioconductor has
> not yet released packages for R 2.0.x.  The first is a short-term problem,

Yes, unfotunately, there was a network breakdown caused by a broken 
router (that one that connects the whole building with the world) over 
the weekend - and our department has no access to that machine. The 
problem is solved now.

Windows binaries will soon be back on CRAN.

Uwe Ligges


> and the second needs a little patience (look at
> http://www.bioconductor.org which has variously mentioned Oct 25 and Nov
> 1).
> 
> On Mon, 25 Oct 2004, shizhu zang wrote:
> 
> 
>>Hi , there:
>>I used function source to download the package 
>>but found
>> 
>>
>>>source("http://www.bioconductor.org/getBioC.R")
>>
>>Error in file(file, "r") : unable to open connection
>>In addition: Warning message: 
>>unable to resolve 'www.bioconductor.org'. 
>> 
>> 
>>Then I downloaded the packages from CRAN
>>and found 
>>
>>>local({a <- CRAN.packages()
>>
>>+ install.packages(select.list(a[,1],,TRUE), .libPaths()[1], available=a, dependencies=TRUE)})
>>trying URL `http://cran.r-project.org/bin/windows/contrib/2.0/PACKAGES'
>>Error in download.file(url = paste(contriburl, "PACKAGES", sep = "/"),  : 
>>        cannot open URL `http://cran.r-project.org/bin/windows/contrib/2.0/PACKAGES'
>>In addition: Warning message: 
>>unable to resolve 'cran.r-project.org'. 
>>
>>I  use the Window operation system
>>I don't konw what is the matter
> 
> 
>>PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
> 
>



From ripley at stats.ox.ac.uk  Mon Oct 25 10:15:06 2004
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Mon, 25 Oct 2004 09:15:06 +0100 (BST)
Subject: [R] library gregmisc
In-Reply-To: <6.1.0.6.1.20041025100211.02670b40@pop2.mate.polimi.it>
Message-ID: <Pine.LNX.4.44.0410250911350.24295-100000@gannet.stats>

Gregmisc is a *bundle*, not a library, not a package.  It unpacks into 
four packages: see

http://cran.r-project.org/src/contrib/Descriptions/gregmisc.html

I guess you are using Windows (see the posting guide, and tell us).
There is a temporary problem on the CRAN master.  Set

options(CRAN="http://cran.ch.r-project.org")

and it works.


On Mon, 25 Oct 2004, Anna Maria Paganoni wrote:

> I write to ask you an help about the package gregmisc.
> I saw the instructions, and I need some functionalities of this package, 
> but I am unable ti download it.
> On friday I was able to download thte .zip, but R does not install this 
> package, today there is no possibility to download it.
> What I have to do?

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From Friedrich.Leisch at tuwien.ac.at  Mon Oct 25 10:24:09 2004
From: Friedrich.Leisch at tuwien.ac.at (Friedrich.Leisch@tuwien.ac.at)
Date: Mon, 25 Oct 2004 10:24:09 +0200
Subject: [R] unable to open connection
In-Reply-To: <417CB4DE.10509@statistik.uni-dortmund.de>
References: <Pine.LNX.4.44.0410250808240.15035-100000@gannet.stats>
	<417CB4DE.10509@statistik.uni-dortmund.de>
Message-ID: <16764.47145.573612.252303@galadriel.ci.tuwien.ac.at>

>>>>> On Mon, 25 Oct 2004 10:10:06 +0200,
>>>>> Uwe Ligges (UL) wrote:

  > Prof Brian Ripley wrote:

  >> Actually, you are unable to connect to the Internet from R on your 
  >> computer.  Perhaps you are not connected, or need to set a proxy?
  >> 
  >> Please read the rw-FAQ, as the posting guide asks.
  >> 
  >> Note that right now you there are no packages on CRAN and Bioconductor has
  >> not yet released packages for R 2.0.x.  The first is a short-term problem,

  > Yes, unfotunately, there was a network breakdown caused by a broken 
  > router (that one that connects the whole building with the world) over 
  > the weekend - and our department has no access to that machine. The 
  > problem is solved now.

  > Windows binaries will soon be back on CRAN.

They are now on the CRAN master site, mirrors may take a day or two to
recover.

Best,
Fritz Leisch

-- 
-------------------------------------------------------------------
                        Friedrich Leisch 
Institut f??r Statistik                     Tel: (+43 1) 58801 10715
Technische Universit??t Wien                Fax: (+43 1) 58801 10798
Wiedner Hauptstra??e 8-10/1071
A-1040 Wien, Austria             http://www.ci.tuwien.ac.at/~leisch



From a.buness at dkfz-heidelberg.de  Mon Oct 25 10:27:59 2004
From: a.buness at dkfz-heidelberg.de (Andreas Buness)
Date: Mon, 25 Oct 2004 10:27:59 +0200
Subject: [R] How to save a complete image of the current state of R  ?
In-Reply-To: <Pine.LNX.4.44.0410221842230.19140-100000@gannet.stats>
References: <Pine.LNX.4.44.0410221842230.19140-100000@gannet.stats>
Message-ID: <417CB90F.2020905@dkfz-heidelberg.de>

Thanks for your reply.

Considering the reproducibility of R this is an important issue.

Regarding my particular problem, i.e. the package build
failure with "R CMD check package" in the examples section,
I do not know how to proceed.  I have tried the following
strategy: direct installation of the package via
"R CMD INSTALL packageDirectory"
and running of the examples after loading the package
or running the examples file which was generated by the
failing "R CMD check" command
-  but the error is not reproducible.

Any ideas how I can track the problem or get interactive
access to the actual state of R during the check procedure ?

Best Regards
Andreas



Prof Brian Ripley wrote:
> On Fri, 22 Oct 2004, Uwe Ligges wrote:
> 
> 
>>Andreas Buness wrote:
>>
>>>Hello,
>>>
>>>I like to save the complete state of R, i.e. including
>>>all environments, objects/workspaces, loaded packages etc..
>>
>>I think what you are going to do is not available.
> 
> 
> It is impossible.  You have no access to e.g. the internal state of the 
> Box-Muller generator, the objects pointed to by external references, the 
> versions of namespaces, and many other things.
> 
> 
>>You can save the workspace, attach the data and load formerly loaded 
>>packages again, of course.
>>
>>Uwe Ligges
>>
>>
>>>This wish has arisen since I am not able to reproduce
>>>an error which occurs when running R CMD check.
>>>
>>>Many thanks for your advice in advance.
>>>Best Regards
>>>Andreas
>>>



From Kevin.Wang at maths.anu.edu.au  Mon Oct 25 11:14:29 2004
From: Kevin.Wang at maths.anu.edu.au (Kevin Wang)
Date: Mon, 25 Oct 2004 19:14:29 +1000 (EST)
Subject: [R] library gregmisc
In-Reply-To: <6.1.0.6.1.20041025100211.02670b40@pop2.mate.polimi.it>
References: <6.1.0.6.1.20041025100211.02670b40@pop2.mate.polimi.it>
Message-ID: <Pine.GSO.4.58.0410251913550.48@yin>

Hi,

On Mon, 25 Oct 2004, Anna Maria Paganoni wrote:

> I write to ask you an help about the package gregmisc.
> I saw the instructions, and I need some functionalities of this package,
> but I am unable ti download it.
> On friday I was able to download thte .zip, but R does not install this
> package, today there is no possibility to download it.

What do you mean R doesn't install the bundle?  How did you try to install
it?

Kev


--------------------------------
Ko-Kang Kevin Wang
PhD Student
Centre for Mathematics and its Applications
Building 27, Room 1004
Mathematical Sciences Institute (MSI)
Australian National University
Canberra, ACT 0200
Australia

Homepage: http://wwwmaths.anu.edu.au/~wangk/
Ph (W): +61-2-6125-2431
Ph (H): +61-2-6125-7407
Ph (M): +61-40-451-8301



From Matthias.Kohl at uni-bayreuth.de  Mon Oct 25 11:41:59 2004
From: Matthias.Kohl at uni-bayreuth.de (Matthias.Kohl@uni-bayreuth.de)
Date: Mon, 25 Oct 2004 11:41:59 +0200 (MEST)
Subject: [R] usage and behavior of 'setIs'
Message-ID: <1118.132.180.246.101.1098697319.squirrel@mail.uni-bayreuth.de>

Hello,

am I using 'setIs' in the correct way in the subsequent (artifical) example?

Do I have to specify explicit 'setAs' for 'list' and 'vector' or
should this work automatically, since "getClass("List1")" states
an explicit coerce also for these classes.

I'm working with R 2.0.0 Patched (2004-10-06) on windows 2000.

Thanks for your advice,
Matthias


# example
setClass(Class = "List1", representation(List = "list"))
setClass(Class = "List2", contains = "list")

setIs(class1 = "List1", class2 = "List2",
    coerce = function(obj){ new("List2", obj at List) },
    replace = function(obj, value){
        obj at List <- value
    })

getClass("List1")
# states explicit coerce for 'list' and 'vector'
getClass("List2")
L1 <- new("List1", List = list("a"))

# all TRUE
is(L1, "List2")
is(L1, "list")
is(L1, "vector")

as(L1, "List2") # works

# both return 'list()'
# why not a 'list' with entry "a"?
# Is there an additional 'setAs' needed?
as(L1, "list")
as(L1, "vector")

L2 <- as(L1, "List2")
as(L2, "list") # works
as(L2, "vector") # works



From WeiQiang.Li at seagate.com  Mon Oct 25 11:43:10 2004
From: WeiQiang.Li at seagate.com (WeiQiang.Li@seagate.com)
Date: Mon, 25 Oct 2004 17:43:10 +0800
Subject: [R] How to calculate Adjusted SS 
Message-ID: <OFF489E33F.FC9A3918-ON48256F38.00351EC0-48256F38.003555C5@seagate.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20041025/913d21fa/attachment.pl

From wolski at molgen.mpg.de  Mon Oct 25 12:02:02 2004
From: wolski at molgen.mpg.de (Witold Eryk Wolski)
Date: Mon, 25 Oct 2004 12:02:02 +0200
Subject: [R] usage and behavior of 'setIs'
In-Reply-To: <1118.132.180.246.101.1098697319.squirrel@mail.uni-bayreuth.de>
References: <1118.132.180.246.101.1098697319.squirrel@mail.uni-bayreuth.de>
Message-ID: <417CCF1A.3070305@molgen.mpg.de>

Hi Matthias,

A similar problem to yours (with one level of inheritance less) was 
disccussed this month on the r-devel list.
You find an answer from JChambers here:

https://stat.ethz.ch/pipermail/r-devel/2004-October/030980.html

And yes specifying _setAs_  to each _setIs_ with the coerce and replace 
is a _hack_ which is with this version of methods necessary when 
inherting from Old Classes.

/E


Matthias.Kohl at uni-bayreuth.de wrote:

>Hello,
>
>am I using 'setIs' in the correct way in the subsequent (artifical) example?
>
>Do I have to specify explicit 'setAs' for 'list' and 'vector' or
>should this work automatically, since "getClass("List1")" states
>an explicit coerce also for these classes.
>
>I'm working with R 2.0.0 Patched (2004-10-06) on windows 2000.
>
>Thanks for your advice,
>Matthias
>
>
># example
>setClass(Class = "List1", representation(List = "list"))
>setClass(Class = "List2", contains = "list")
>
>setIs(class1 = "List1", class2 = "List2",
>    coerce = function(obj){ new("List2", obj at List) },
>    replace = function(obj, value){
>        obj at List <- value
>    })
>
>getClass("List1")
># states explicit coerce for 'list' and 'vector'
>getClass("List2")
>L1 <- new("List1", List = list("a"))
>
># all TRUE
>is(L1, "List2")
>is(L1, "list")
>is(L1, "vector")
>
>as(L1, "List2") # works
>
># both return 'list()'
># why not a 'list' with entry "a"?
># Is there an additional 'setAs' needed?
>as(L1, "list")
>as(L1, "vector")
>
>L2 <- as(L1, "List2")
>as(L2, "list") # works
>as(L2, "vector") # works
>
>______________________________________________
>R-help at stat.math.ethz.ch mailing list
>https://stat.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>
>  
>


-- 
Dipl. bio-chem. Witold Eryk Wolski
MPI-Moleculare Genetic
Ihnestrasse 63-73 14195 Berlin
tel: 0049-30-83875219                 __("<    _
http://www.molgen.mpg.de/~wolski      \__/    'v'
http://r4proteomics.sourceforge.net    ||    /   \
mail: witek96 at users.sourceforge.net    ^^     m m
      wolski at molgen.mpg.de



From Matthias.Kohl at uni-bayreuth.de  Mon Oct 25 12:34:42 2004
From: Matthias.Kohl at uni-bayreuth.de (Matthias.Kohl@uni-bayreuth.de)
Date: Mon, 25 Oct 2004 12:34:42 +0200 (MEST)
Subject: [R] usage and behavior of 'setIs'
In-Reply-To: <417CCF1A.3070305@molgen.mpg.de>
References: <1118.132.180.246.101.1098697319.squirrel@mail.uni-bayreuth.de>
	<417CCF1A.3070305@molgen.mpg.de>
Message-ID: <1145.132.180.246.124.1098700482.squirrel@mail.uni-bayreuth.de>

Thank you,

Matthias

> Hi Matthias,
>
> A similar problem to yours (with one level of inheritance less) was
> disccussed this month on the r-devel list.
> You find an answer from JChambers here:
>
> https://stat.ethz.ch/pipermail/r-devel/2004-October/030980.html
>
> And yes specifying _setAs_  to each _setIs_ with the coerce and replace
> is a _hack_ which is with this version of methods necessary when
> inherting from Old Classes.
>
> /E
>
>
> Matthias.Kohl at uni-bayreuth.de wrote:
>
>>Hello,
>>
>>am I using 'setIs' in the correct way in the subsequent (artifical)
>> example?
>>
>>Do I have to specify explicit 'setAs' for 'list' and 'vector' or
>>should this work automatically, since "getClass("List1")" states
>>an explicit coerce also for these classes.
>>
>>I'm working with R 2.0.0 Patched (2004-10-06) on windows 2000.
>>
>>Thanks for your advice,
>>Matthias
>>
>>
>># example
>>setClass(Class = "List1", representation(List = "list"))
>>setClass(Class = "List2", contains = "list")
>>
>>setIs(class1 = "List1", class2 = "List2",
>>    coerce = function(obj){ new("List2", obj at List) },
>>    replace = function(obj, value){
>>        obj at List <- value
>>    })
>>
>>getClass("List1")
>># states explicit coerce for 'list' and 'vector'
>>getClass("List2")
>>L1 <- new("List1", List = list("a"))
>>
>># all TRUE
>>is(L1, "List2")
>>is(L1, "list")
>>is(L1, "vector")
>>
>>as(L1, "List2") # works
>>
>># both return 'list()'
>># why not a 'list' with entry "a"?
>># Is there an additional 'setAs' needed?
>>as(L1, "list")
>>as(L1, "vector")
>>
>>L2 <- as(L1, "List2")
>>as(L2, "list") # works
>>as(L2, "vector") # works
>>
>>______________________________________________
>>R-help at stat.math.ethz.ch mailing list
>>https://stat.ethz.ch/mailman/listinfo/r-help
>>PLEASE do read the posting guide!
>> http://www.R-project.org/posting-guide.html
>>
>>
>>
>
>
> --
> Dipl. bio-chem. Witold Eryk Wolski
> MPI-Moleculare Genetic
> Ihnestrasse 63-73 14195 Berlin
> tel: 0049-30-83875219                 __("<    _
> http://www.molgen.mpg.de/~wolski      \__/    'v'
> http://r4proteomics.sourceforge.net    ||    /   \
> mail: witek96 at users.sourceforge.net    ^^     m m
>       wolski at molgen.mpg.de
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide!
> http://www.R-project.org/posting-guide.html



From ligges at statistik.uni-dortmund.de  Mon Oct 25 12:57:08 2004
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Mon, 25 Oct 2004 12:57:08 +0200
Subject: [R] Legend/Substitute/Plotmath problem
In-Reply-To: <16764.45491.531701.421033@gargle.gargle.HOWL>
References: <20041023110425.2b58d3df@localhost>
	<16764.45491.531701.421033@gargle.gargle.HOWL>
Message-ID: <417CDC04.3060505@statistik.uni-dortmund.de>

Martin Maechler wrote:

>>>>>>"Johannes" == Johannes Graumann <graumann at caltech.edu>
>>>>>>    on Sat, 23 Oct 2004 11:04:25 -0700 writes:
>>>>>>"Johannes" == Johannes Graumann <graumann at caltech.edu>
>>>>>>    on Sat, 23 Oct 2004 11:04:25 -0700 writes:
> 
> 
>     Johannes> Hello,
> 
>     Johannes> I seem unable to construct a legend which contains
>     Johannes> a substitution as well as math symbols. I'm trying
>     Johannes> to do the following:
> 
>     >> strain2 <- "YJG48"
> 
>     >> legend.txt <- c(
>     >> 	substitute(
>     >> 		strain *
>     >> 		%==% *
>     >> 		"YJG45, rpn10" *
>     >> 		%Delta%,
>     >> 		list(strain=strain2)
>     >> 	),
>     >> 	"Verhulst/Logistic",
>     >> 	"Malthus"
>     >> )
> 
> 
>     Johannes>     .....................
> 
> Do try to break down a problem into simple things --
> particularly when you have problems!
> 
> This substitute() call is simply invalid:
> 
>   > ss <- substitute( strain * %==% * "YJG45, rpn10" * %Delta%, list(strain=strain2) )
>   Error: syntax error
> 
> and the 'syntax error' should give you a clue:  
> The first argument of substitute must be a syntactically correct
> R expression.
> 
> Now you try more and more simple things till you 'see it' :
> 
> Why should I expect  'A * %==% B'  to be valid syntax?
> Both '*' and '%==%' are (diadic) operators: You can't juxtapose
> them, as well as you can't write  'A * = B'.
> Then, '%Delta%' (like any other '%foo%' !!) is a diadic operator
> too and hence can't be juxtaposed to '*'. But I'm pretty sure
> you rather mean (greek) 'Delta'.
> 
> Hence:
>  ss <- substitute( strain %==% "YJG45, rpn10" * Delta, list(strain=strain2) )
> 
> ---
> 
> Once you have the expression you can go further;
> still step by step :
> 
>   > c(ss, "Verhulst")
>   [[1]]
>   "YJG48" %==% "YJG45, rpn10" * Delta
> 
>   [[2]]
>   [1] "Verhulst"
> 
> Hmm, a list; that won't work.
> You do need to pass either a "character" vector or an
> expression, i.e., an expression of length 3 in our case.
> We must build the expression somewhat manually:
> 
>   > e <- expression(1, "Verhulst", "Malthus")# '1' is a place holder
>     expression(1, "Verhulst", "Malthus")
>   > e[[1]] <- ss  ## that's the trick!
> 
>   > str(e)
>     expression("YJG48" %==% "YJG45, rpn10" * Delta, "Verhulst", "Malthus")
> 
>   > plot(1); legend(1,1, leg = e)
> 
> ---
> 
> Maybe something to be added as an example to help(legend) or rather
> to help(expression) ?

Martin, a small example is given in the Help Desk in R News 2 (3). Maybe 
you want to include it ...

Uwe



> HTH,
> Martin Maechler, ETH Zurich
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html



From danbebber at forestecology.co.uk  Mon Oct 25 12:57:22 2004
From: danbebber at forestecology.co.uk (Dan Bebber)
Date: Mon, 25 Oct 2004 11:57:22 +0100
Subject: [R] sample variogram construction
In-Reply-To: <200410251012.i9PA5nf9023131@hypatia.math.ethz.ch>
Message-ID: <000401c4ba81$67c6dd10$7d2501a3@plants.ox.ac.uk>

Hi Matt,

there are several R packages that will compute the sample variogram for you.
Check out GeoR, sgeostat, nlme, spatial. There's no point in recoding the
whole lot yourself, unless as a learning excercise.

D

p.s. For time series autocorrelations, you could use acf in package stats.

Message: 9
Date: Mon, 25 Oct 2004 02:02:06 -0400
From: matt.kowgier at utoronto.ca
Subject: [R] sample variogram construction
To: r-help at stat.math.ethz.ch
Message-ID: <1098684126.417c96de334e1 at webmail.utoronto.ca>
Content-Type: text/plain; charset=US-ASCII

Hi

Im attempting to build a sample variogram for 300 obersvations of
longitudinal data. So what I need to do is compute the half 
squared differences  between pairs of residuals (for instance if a subject
has 4 obersvations, this is 4 choose 2 paird differences) for each subject.
Also, then I need the corresponding time differences within each individual.
So the end result will be a 300 by 2 matrix with columns corresponding to
paired difference residuals within subject and time differences within
subject. Basically im having trouble coding this kind of matrix in R, if
anyone can help me out or give me some tips id appreciate it.

Thanks.
Stuck in the for loop
student



From ligges at statistik.uni-dortmund.de  Mon Oct 25 13:03:05 2004
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Mon, 25 Oct 2004 13:03:05 +0200
Subject: [R] From soundcard to R?
In-Reply-To: <417C1171.5040004@utu.fi>
References: <417C1171.5040004@utu.fi>
Message-ID: <417CDD69.4060505@statistik.uni-dortmund.de>

Atte Tenkanen wrote:

> Is it possible to input data from sound card of the computer to R? What 
> do I need to know about my computer (it's linux pc)? Can I get some real 
> time graphical information this way, spektrum for example?

I guess it is possible to get data directly from the sound card using 
connections, but you cannot get real time graphical output.

Uwe Ligges


> Atte
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html



From ripley at stats.ox.ac.uk  Mon Oct 25 14:01:04 2004
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Mon, 25 Oct 2004 13:01:04 +0100 (BST)
Subject: [R] Assist on R-2.0.0 /64 bit AMD/SuSE 9.1
In-Reply-To: <200410220938.27062.sdfrost@ucsd.edu>
Message-ID: <Pine.LNX.4.44.0410251159480.326-100000@gannet.stats>

R CMD check assist segfaults on AMD64 unless long is replaced by int in
ker.c. If it is, it passes R CMD check.

Next time please do not dismiss informed advice out of hand, but try it 
for yourself.

Chunlei Ke: please update your package.

On Fri, 22 Oct 2004, Simon Frost wrote:

> Dear Brian,
> 
> Apologies;I do mean 'assist' rather than 'Assist'; there are versions for R 
> and for SPlus. I doubt whether it's to do with calling conventions; the 
> functions sometimes work, sometimes they don't, and cause a segmentation 
> fault.
> 
> Thanks
> Simon
> 

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From Robert at sanctumfi.com  Mon Oct 25 14:28:29 2004
From: Robert at sanctumfi.com (Robert Sams)
Date: Mon, 25 Oct 2004 13:28:29 +0100
Subject: [R] functions for special na handling?
Message-ID: <E585EABA11227445B918BFB74C1A4D36015958@sanctum01.sanctumfi.com>

hi,

i need a function that can replace na's in univariate ts objects with a value interpolated linearly 
using the adjacent non-na values. for example, the function i need (myfun) would do the following

> x <- ts(10,11.4,NA,12,9.7)
> y <- myfun(x)
> y
10 11.4 11.7 12 9.7

i can code an na.action method myself to accomplish this, but has someone else already coded routines 
of this nature?

many thanks,
robert


Robert Sams

SANCTUM FI LLP
Charles House
18B Charles Street
Mayfair
London W1J 5DU
Tel: +44 (0) 207 667 6360
Dir: +44 (0) 207 667 6363
fax: +44 (0) 207 667 6460
email: robert at sanctumfi.com

Authorised and Regulated by the FSA. 

Sending encrypted mail:

See http://pgp.mit.edu (search string 'sanctumfi') for updates.

-----BEGIN PGP PUBLIC KEY BLOCK-----
Version: GnuPG v1.0.6 (GNU/Linux)
Comment: For info see http://www.gnupg.org

mQGiBD/xA1kRBAC2RUt8WyHDjXDoO1eu2Mli83cuEV37FicaBs/Wj5ry1QIz1drj
ubP25QQvu0lYOOnP7iS48bcOVP77uQYqLbsvzQ4fChFLCg9O3D4ourolZEK69ooJ
74r87PPV/LEnURL4T8E7QCDrRmylJ1iCffgJ9JWkAN4qUG+6fFuOyiqxDwCgiMkM
oLaYHjl3unc7Anx59xAlm+MD/R1EdXM9uewCj2kS3SdN+akklg6QVy+gTl3+HGzc
MC6ZcLsk1uIklkvCfoYDN7oeC3uVGik7QYkE3n02vfAMGjW7AqsQEoblzi3QscoX
Kzitd04NcWeDYIXRThCCydIJ64DdpF293ewJf2fRykmBdAbV0oaqL+zgdrRqFxYJ
m2d9A/9m3DoHwXBXPyqueX8naY5hmmeG+mihkdI4H4MBmaPJZW9DnZxor6P2Nm/X
muDSf7aZ1t9J0t75oEY/SyjoCYhWlMJS+wnOUq3u5XRNRyo9oI1qQaF5zw8ZmIzW
x7nfQMlXpXpETGwbNxZuUoucbN1cDrZBymHxlI5AiBCZ/fya3rQzUm9iZXJ0IFNh
bXMgKFNhbmN0dW0gRkkgTExQKSA8cm9iZXJ0QHNhbmN0dW1maS5jb20+iF0EExEC
AB0FAj/xA1kFCQHhM4AFCwcKAwQDFQMCAxYCAQIXgAAKCRD3WGga9bCIIPV6AJ9t
cDQkN8jW8CxzbU5K2O3dxaMlKwCeNOYQsAgW7S1qAlM345QpDXxbjgC5Ag0EP/ED
bRAIANoxq2NjQFdrUMSlQEaitR3pFTmOC7n2rICBAbU/hxlVs1PFSyh6Tr00vzFM
py6n4uBCOzrz3b5u5YbRaQzs8ipkqnSzoDD6GKfMEEWYQvZ76kkShWt5zDkLQ2X4
V4X+xJ0iuFT+9cK7VuJ102pjsAwltDUGPKsSwWqs55tzBN8BwAxqNMxRtNYbOTAB
Dpnm1BsiZ1TLqHIr4a1t2ZEuqhKV0HEP9VugP9XQz9u1f5QZrriNW/foxBwLuXS7
g+945IGXZq/qxHzgQjJQhC3jIaHUrchrQxy6snoQxgAnuO2/g9SLI8BBsvpVZ+Ac
mpkhPtT4pGujwsK/oRFDloAb7BsAAwYIAMlQrB2GPn1ZNFIf9zN+euTv2jWx5Hv4
ZEhqeBTqq00KCT3NSrnOHBTX4x6F4L+ofRzl2L5zIi685wWTpLgqQI7hzKvAxerJ
xe1qpz5GfGX976uaqxEfzwQZqcZB2iihhjeOUTxalSWdkX73yNtRmLLikTr0U3E0
v0dB1laMqldYub4X+GeH7tAeQGqYfS6Y+BdNDWfIcwADM0ggLIbNsw+IsjdQNOpq
5R4p1E2o5kfvafIFLpMOZACKKdTBkfiAqOZq8ezDpNHwLrRG4RvQ1K80pGGqaikI
XFbJIthvimA5w4MjvenuIn367zj+bz5eFE7YeQ0KG7NAdg2DkxD4W9CITAQYEQIA
DAUCP/EDbQUJAeEzgAAKCRD3WGga9bCIIL8RAJ4o9zXtkqK5RMKXxJTmZejtDjTC
kwCdFevBc9z4ermWaKb9BsDU7OYdgM8=
=6Y3T
-----END PGP PUBLIC KEY BLOCK-----



From smitaj at ukzn.ac.za  Mon Oct 25 14:42:52 2004
From: smitaj at ukzn.ac.za (AJ Smit)
Date: Mon, 25 Oct 2004 14:42:52 +0200
Subject: [R] aov documentation page: question
Message-ID: <417CF4CC.90202@ukzn.ac.za>

Dear all

I was looking at the aov documentation page and came across the 
following which seems like a contradiction to me:

" This provides a wrapper to |lm| for fitting linear models to balanced 
or unbalanced experimental designs." (I presume 'This' refers to aov)

and

"|aov| is designed for balanced designs, and the results can be hard to 
interpret without balance"

So, do I use it for unbalanced designs?

Thanks,
AJ

-- 
Dr Albertus J. Smit
(Marine Ecologist/Algal Biologist)
School of Biology and Conservation Science
Faculty of Science
University of KwaZulu-Natal
Westville Campus
P/B X54001
Durban
4000
South Africa

Tel. (031) 260 7472
Fax (031) 260 7364


--------------------------------------------------------------------
Please find our disclaimer at http://www.ukzn.ac.za/disclaimer
--------------------------------------------------------------------
<<<<gwavasig>>>>

From ripley at stats.ox.ac.uk  Mon Oct 25 14:59:06 2004
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Mon, 25 Oct 2004 13:59:06 +0100 (BST)
Subject: [R] aov documentation page: question
In-Reply-To: <417CF4CC.90202@ukzn.ac.za>
Message-ID: <Pine.LNX.4.44.0410251350010.710-100000@gannet.stats>

On Mon, 25 Oct 2004, AJ Smit wrote:

> I was looking at the aov documentation page and came across the 
> following which seems like a contradiction to me:
>
> " This provides a wrapper to |lm| for fitting linear models to balanced 
> or unbalanced experimental designs." (I presume 'This' refers to aov)
> 
> and
>
> "|aov| is designed for balanced designs, and the results can be hard to 
> interpret without balance"

and that is part of a Note, not even a whole sentence of it.  In 
particular, the note has extra force for the multistratum designs 
mentioned there.

> So, do I use it for unbalanced designs?

You _can_ use it, but the results _can_ be hard to interpret.  That is not
a `contradiction' -- perhaps you need to consult your dictionary?  You may
do better to use something else, like lme(), hence the `Note'.

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From p.dalgaard at biostat.ku.dk  Mon Oct 25 15:02:42 2004
From: p.dalgaard at biostat.ku.dk (Peter Dalgaard)
Date: 25 Oct 2004 15:02:42 +0200
Subject: [R] aov documentation page: question
In-Reply-To: <417CF4CC.90202@ukzn.ac.za>
References: <417CF4CC.90202@ukzn.ac.za>
Message-ID: <x2r7nn6jd9.fsf@biostat.ku.dk>

AJ Smit <smitaj at ukzn.ac.za> writes:

> Dear all
> 
> I was looking at the aov documentation page and came across the
> following which seems like a contradiction to me:
> 
> " This provides a wrapper to |lm| for fitting linear models to
> balanced or unbalanced experimental designs." (I presume 'This' refers
> to aov)
> 
> and
> 
> "|aov| is designed for balanced designs, and the results can be hard
> to interpret without balance"
> 
> So, do I use it for unbalanced designs?

If you know how to interpret the results...

In general, I would avoid it if there is unbalance in the random
effects, but some designs with unbalance in the fixed effects are
manageable. 

-- 
   O__  ---- Peter Dalgaard             Blegdamsvej 3  
  c/ /'_ --- Dept. of Biostatistics     2200 Cph. N   
 (*) \(*) -- University of Copenhagen   Denmark      Ph: (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)             FAX: (+45) 35327907



From Timur.Elzhov at jinr.ru  Mon Oct 25 15:32:03 2004
From: Timur.Elzhov at jinr.ru (Timur Elzhov)
Date: Mon, 25 Oct 2004 17:32:03 +0400
Subject: [R] `hist()' the weighted value
Message-ID: <20041025133203.GA2660@nf034.jinr.ru>

Hello useRs,

I have the numeric vector `x' and the weights `w' for it of the same
length. I'd like to have hist of `x' weighted with `w'.
How to do that? I found nothing about weights in the hist help page..

Thanks.


--
WBR,
Timur.



From drewbrewit at yahoo.com  Mon Oct 25 15:46:32 2004
From: drewbrewit at yahoo.com (Nick Drew)
Date: Mon, 25 Oct 2004 06:46:32 -0700 (PDT)
Subject: [R] .Rprofile and RODBC
Message-ID: <20041025134632.58071.qmail@web50901.mail.yahoo.com>

Since installing R 2.0 I've had the following 2 issues
I can't find solutions for. I'm using Windows XP.

1) R 1.9 continues to read my .Rprofile file in my
home directory but R 2.0 does not. How do I get R 2.0
to do this?

2) The following function no longer works in R 2.0 but
continues to work in R 1.9:


getExcel <- function (x) {
 invisible(require(RODBC))
 x <- sqlFetch(odbcConnectExcel(file.choose()),
"exceldata", na.strings = "NA")
 odbcCloseAll() #closes the connection
 invisible(edit(x))
}


I use this function to choose an Excel file and open
it using an ODBC connection. Why does this not work
now?


Thanks in advance for you help.

~Nick



From 0034058 at fudan.edu.cn  Mon Oct 25 15:57:43 2004
From: 0034058 at fudan.edu.cn (rongguiwong)
Date: Mon, 25 Oct 2004 21:57:43 +0800
Subject: [R] `hist()' the weighted value
In-Reply-To: <20041025133203.GA2660@nf034.jinr.ru>
References: <20041025133203.GA2660@nf034.jinr.ru>
Message-ID: <200410252157.43751.0034058@fudan.edu.cn>

how about
>hist(rep(x,w))
i think it works.

> I have the numeric vector `x' and the weights `w' for it of the same
> length. I'd like to have hist of `x' weighted with `w'.
> How to do that? I found nothing about weights in the hist help page..



From h.andersson at nioo.knaw.nl  Mon Oct 25 15:56:37 2004
From: h.andersson at nioo.knaw.nl (Henrik Andersson)
Date: Mon, 25 Oct 2004 15:56:37 +0200
Subject: [R] Reading sections of data files based on pattern matching
Message-ID: <clj0nn$mil$1@sea.gmane.org>

I am about to write general functions to read the output of simulations 
models.

These model generate output files with different sections which I want 
to analyze plot etc.

Since this will be used many people at the department I wanted to make 
sure that will do this in the best way.

For instance I want to read a snippets of data from a text that look 
like this.
-------------------------------
Lots of stuff
...
@@Start Values@@
	Column1 Column2 Column3 ...
Row1	1	2	3 ...
...
@@End Values@@

More stuff
...
@@Start OtherValues@@
	Column1 Column2 Column3 ...
Row1	1	2	3 ...
...
@@End OtherValues@@


I looked in the help files and found grep which operates on character 
strings, do I have to like this then?

1. Read file with readLines("foo.txt")
2. grep this object for the start and end of each section ->startline & 
stopline
3. Read the file again with 
read.table("foo.txt",skip=startline,nrows=stoplin-startline)

Or is there a more beautiful way?

Cheers,
---------------------------------------------
Henrik Andersson
Netherlands Institute of Ecology -
Centre for Estuarine and Marine Ecology
P.O. Box 140
4400 AC Yerseke
Phone: +31 113 577473
h.andersson at nioo.knaw.nl
http://www.nioo.knaw.nl/ppages/handersson



From dataanalytics at rediffmail.com  Mon Oct 25 16:33:57 2004
From: dataanalytics at rediffmail.com (Arin Basu)
Date: 25 Oct 2004 14:33:57 -0000
Subject: [R] Intro to R: lecture presentation
Message-ID: <20041025143357.2740.qmail@webmail29.rediffmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20041025/c685496d/attachment.pl

From Alicia.Amadoz at uv.es  Mon Oct 25 16:55:33 2004
From: Alicia.Amadoz at uv.es (Alicia Amadoz)
Date: Mon, 25 Oct 2004 16:55:33 +0200 (CEST)
Subject: [R] problem with installation on Linux (beginners)
Message-ID: <7473758598amadoz@uv.es>

Hello,

I'm new to the installation R on Linux and I've followed the
instructions on the R Installation and Administration Manual and on the
FAQs. I also have changed the path at the .bash_profile file but when I
try to type R at the shell prompt, it says:

Fatal error: R home directory is not defined

I've tried many things but all the time it says the same. Any help would
be appreciated.

Regards,
Alicia



From ggrothendieck at myway.com  Mon Oct 25 17:06:04 2004
From: ggrothendieck at myway.com (Gabor Grothendieck)
Date: Mon, 25 Oct 2004 15:06:04 +0000 (UTC)
Subject: [R] Reading sections of data files based on pattern matching
References: <clj0nn$mil$1@sea.gmane.org>
Message-ID: <loom.20041025T170414-224@post.gmane.org>

Henrik Andersson <h.andersson <at> nioo.knaw.nl> writes:

: 
: I am about to write general functions to read the output of simulations 
: models.
: 
: These model generate output files with different sections which I want 
: to analyze plot etc.
: 
: Since this will be used many people at the department I wanted to make 
: sure that will do this in the best way.
: 
: For instance I want to read a snippets of data from a text that look 
: like this.
: -------------------------------
: Lots of stuff
: ...
:  <at>  <at> Start Values <at>  <at> 
: 	Column1 Column2 Column3 ...
: Row1	1	2	3 ...
: ...
:  <at>  <at> End Values <at>  <at> 
: 
: More stuff
: ...
:  <at>  <at> Start OtherValues <at>  <at> 
: 	Column1 Column2 Column3 ...
: Row1	1	2	3 ...
: ...
:  <at>  <at> End OtherValues <at>  <at> 
: 
: 
: I looked in the help files and found grep which operates on character 
: strings, do I have to like this then?
: 
: 1. Read file with readLines("foo.txt")
: 2. grep this object for the start and end of each section ->startline & 
: stopline
: 3. Read the file again with 
: read.table("foo.txt",skip=startline,nrows=stoplin-startline)
: 
: Or is there a more beautiful way?


You could adapt the following to your situation (i.e. multiple sections
rather than just one):

https://www.stat.math.ethz.ch/pipermail/r-help/2003-November/040184.html

Also regarding your example, one potential gotcha to be aware of is
that skip= skips lines but nrow= counts rows of the data frame so they 
are slightly different concepts.



From p.dalgaard at biostat.ku.dk  Mon Oct 25 17:04:18 2004
From: p.dalgaard at biostat.ku.dk (Peter Dalgaard)
Date: 25 Oct 2004 17:04:18 +0200
Subject: [R] problem with installation on Linux (beginners)
In-Reply-To: <7473758598amadoz@uv.es>
References: <7473758598amadoz@uv.es>
Message-ID: <x2mzya7sb1.fsf@biostat.ku.dk>

"Alicia Amadoz" <Alicia.Amadoz at uv.es> writes:

> Hello,
> 
> I'm new to the installation R on Linux and I've followed the
> instructions on the R Installation and Administration Manual and on the
> FAQs. I also have changed the path at the .bash_profile file but when I
> try to type R at the shell prompt, it says:
> 
> Fatal error: R home directory is not defined
> 
> I've tried many things but all the time it says the same. Any help would
> be appreciated.

What happens if you type 'which R'? What is the version of R and what
is in your path?

I suspect you are somehow trying to run the R binary, bypassing the R
shell script, which sets things like R_HOME.

-- 
   O__  ---- Peter Dalgaard             Blegdamsvej 3  
  c/ /'_ --- Dept. of Biostatistics     2200 Cph. N   
 (*) \(*) -- University of Copenhagen   Denmark      Ph: (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)             FAX: (+45) 35327907



From suzette at sdac.harvard.edu  Mon Oct 25 17:16:15 2004
From: suzette at sdac.harvard.edu (Suzette Blanchard)
Date: Mon, 25 Oct 2004 11:16:15 -0400 (EDT)
Subject: [R] CRAN packages
Message-ID: <Pine.GSO.4.40.0410251111010.19132-100000@sdac.harvard.edu>


I am not able to download packages ("xtable") from CRAN
not even just a .zip file,   I get that the page can not be
found.  Please could you help?
	Suzette



=================================
Suzette Blanchard, Ph.D.
Research Scientist
Frontier Science



From tom_woody at swissinfo.org  Mon Oct 25 17:19:41 2004
From: tom_woody at swissinfo.org (=?ISO-8859-15?Q?Thomas_Sch=F6nhoff?=)
Date: Mon, 25 Oct 2004 17:19:41 +0200
Subject: [R] Intro to R: lecture presentation
In-Reply-To: <20041025143357.2740.qmail@webmail29.rediffmail.com>
References: <20041025143357.2740.qmail@webmail29.rediffmail.com>
Message-ID: <417D198D.3020603@swissinfo.org>

Hello Arin,


Arin Basu schrieb:

> A couple of weeks back, I asked a question on the list that I was invited to provide an introductory lecture on R to a group of academicians in Kolkata. I thank all of you who had generously guided me in providing me web links and words to the wise. 
> 
> Time to give back. I did the presentation on introduction to R and uploaded the presentation files at the following site:
> 
> http://www.aloofhosting.com/arinbasu/Rtutorial/Rintroweb_files/frame.htm
> 
> A shortened form of url is here (easy for cutting and pasting):
> 
> http://tinyurl.com/3zsr6
> 
> Would greatly appreciate your feedbacks/opinions/advices on errors/omissions.

Both links don't work for me, simply get an empty page and a black frame 
on both websites (IE optimzed?) running Mozilla-1.7.x on Debian GNU/Linux.

sincerely

Thomas



From ramasamy at cancer.org.uk  Mon Oct 25 17:25:14 2004
From: ramasamy at cancer.org.uk (Adaikalavan Ramasamy)
Date: Mon, 25 Oct 2004 16:25:14 +0100
Subject: [R] problem with installation on Linux (beginners)
In-Reply-To: <7473758598amadoz@uv.es>
References: <7473758598amadoz@uv.es>
Message-ID: <1098717914.3686.14.camel@ndmpc126.ihs.ox.ac.uk>

I presume R when executed from its bin directory works fine.

Are you actually using bash shell. Try 'echo $SHELL'.

If yes, then try 'which R' and see if it pointing to the right path.



On Mon, 2004-10-25 at 15:55, Alicia Amadoz wrote:
> Hello,
> 
> I'm new to the installation R on Linux and I've followed the
> instructions on the R Installation and Administration Manual and on the
> FAQs. I also have changed the path at the .bash_profile file but when I
> try to type R at the shell prompt, it says:
> 
> Fatal error: R home directory is not defined
> 
> I've tried many things but all the time it says the same. Any help would
> be appreciated.
> 
> Regards,
> Alicia
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>



From R.P.Clement at westminster.ac.uk  Mon Oct 25 17:19:33 2004
From: R.P.Clement at westminster.ac.uk (Ross Clement)
Date: 25 Oct 2004 16:19:33 +0100
Subject: [R] problem with installation on Linux (beginners)
In-Reply-To: <x2mzya7sb1.fsf@biostat.ku.dk>
References: <7473758598amadoz@uv.es>  <x2mzya7sb1.fsf@biostat.ku.dk>
Message-ID: <1098717573.3140.14.camel@staff-pc01.harrowscs.westminster.ac.uk>

On Mon, 2004-10-25 at 16:04, Peter Dalgaard wrote:
> "Alicia Amadoz" <Alicia.Amadoz at uv.es> writes:
> What happens if you type 'which R'? What is the version of R and what
> is in your path?
> 
> I suspect you are somehow trying to run the R binary, bypassing the R
> shell script, which sets things like R_HOME.

$ cd /usr/lib/R
$ ./R
(R works perfectly normally)
$ ./R.bin



From R.P.Clement at westminster.ac.uk  Mon Oct 25 17:22:35 2004
From: R.P.Clement at westminster.ac.uk (Ross Clement)
Date: 25 Oct 2004 16:22:35 +0100
Subject: [R] problem with installation on Linux (beginners)
In-Reply-To: <x2mzya7sb1.fsf@biostat.ku.dk>
References: <7473758598amadoz@uv.es>  <x2mzya7sb1.fsf@biostat.ku.dk>
Message-ID: <1098717755.3140.18.camel@staff-pc01.harrowscs.westminster.ac.uk>

Please excuse my previous email. I was going to follow up Peter's post
showing that if I run the R binary directly I get exactly the error
message reported by the original poster. Unfortunately, after I decided
that was too trivial to post, as my mouse headed towards the X button to
kill the email for some reason the send button was depressed and the
email sent.

Apologies.

Cheers,

Ross-c



From p.dalgaard at biostat.ku.dk  Mon Oct 25 17:26:49 2004
From: p.dalgaard at biostat.ku.dk (Peter Dalgaard)
Date: 25 Oct 2004 17:26:49 +0200
Subject: [R] CRAN packages
In-Reply-To: <Pine.GSO.4.40.0410251111010.19132-100000@sdac.harvard.edu>
References: <Pine.GSO.4.40.0410251111010.19132-100000@sdac.harvard.edu>
Message-ID: <x2is8y7r9i.fsf@biostat.ku.dk>

Suzette Blanchard <suzette at sdac.harvard.edu> writes:

> I am not able to download packages ("xtable") from CRAN
> not even just a .zip file,   I get that the page can not be
> found.  Please could you help?
> 	Suzette

Which CRAN mirror? There was a messup over the weekend where a router
in Dortmund went down and the mirroring software misbehaved, deleting
all the Windows packages from CRAN. However, CRAN master
(cran.r-project.org) is back to normal now, and its mirrors should
follow in a day or two.


-- 
   O__  ---- Peter Dalgaard             Blegdamsvej 3  
  c/ /'_ --- Dept. of Biostatistics     2200 Cph. N   
 (*) \(*) -- University of Copenhagen   Denmark      Ph: (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)             FAX: (+45) 35327907



From sundar.dorai-raj at PDF.COM  Mon Oct 25 17:30:41 2004
From: sundar.dorai-raj at PDF.COM (Sundar Dorai-Raj)
Date: Mon, 25 Oct 2004 10:30:41 -0500
Subject: [R] CRAN packages
In-Reply-To: <Pine.GSO.4.40.0410251111010.19132-100000@sdac.harvard.edu>
References: <Pine.GSO.4.40.0410251111010.19132-100000@sdac.harvard.edu>
Message-ID: <417D1C21.5050006@pdf.com>



Suzette Blanchard wrote:
> I am not able to download packages ("xtable") from CRAN
> not even just a .zip file,   I get that the page can not be
> found.  Please could you help?
> 	Suzette
> 
> 
> 
> =================================
> Suzette Blanchard, Ph.D.
> Research Scientist
> Frontier Science
> 

Hi Suzette,
   Please supply more information such as how you are downloading the 
package, what operating system you are using, and which version of R you 
have installed (or in general, read the posting guide). The easiest way 
to install a package is within R using ?install.packages. For example,

install.packages("xtable")

works perfectly fine for me in R-2.0.0 patched on Win2K.

--sundar



From dimitris.rizopoulos at med.kuleuven.ac.be  Mon Oct 25 17:35:02 2004
From: dimitris.rizopoulos at med.kuleuven.ac.be (Dimitris Rizopoulos)
Date: Mon, 25 Oct 2004 17:35:02 +0200
Subject: [R] CRAN packages
References: <Pine.GSO.4.40.0410251111010.19132-100000@sdac.harvard.edu>
Message-ID: <001201c4baa8$417141b0$0540210a@www.domain>

Hi Suzette,

did you try several CRAN mirrors, for instance I'm able to download it 
from a UK mirror:

http://www.stats.bris.ac.uk/R/src/contrib/Descriptions/xtable.html

I hope it helps.

Best,
Dimitris

----
Dimitris Rizopoulos
Ph.D. Student
Biostatistical Centre
School of Public Health
Catholic University of Leuven

Address: Kapucijnenvoer 35, Leuven, Belgium
Tel: +32/16/396887
Fax: +32/16/337015
Web: http://www.med.kuleuven.ac.be/biostat/
     http://www.student.kuleuven.ac.be/~m0390867/dimitris.htm


----- Original Message ----- 
From: "Suzette Blanchard" <suzette at sdac.harvard.edu>
To: <r-help at stat.math.ethz.ch>
Sent: Monday, October 25, 2004 5:16 PM
Subject: [R] CRAN packages


>
> I am not able to download packages ("xtable") from CRAN
> not even just a .zip file,   I get that the page can not be
> found.  Please could you help?
> Suzette
>
>
>
> =================================
> Suzette Blanchard, Ph.D.
> Research Scientist
> Frontier Science
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
>



From wolski at molgen.mpg.de  Mon Oct 25 17:34:08 2004
From: wolski at molgen.mpg.de (Witold Eryk Wolski)
Date: Mon, 25 Oct 2004 17:34:08 +0200
Subject: [R] Intro to R: lecture presentation
In-Reply-To: <417D198D.3020603@swissinfo.org>
References: <20041025143357.2740.qmail@webmail29.rediffmail.com>
	<417D198D.3020603@swissinfo.org>
Message-ID: <417D1CF0.5060202@molgen.mpg.de>

Same behavior with Firefox 1.0Pr.
/E

Thomas Sch??nhoff wrote:

> Hello Arin,
>
>
> Arin Basu schrieb:
>
>> A couple of weeks back, I asked a question on the list that I was 
>> invited to provide an introductory lecture on R to a group of 
>> academicians in Kolkata. I thank all of you who had generously guided 
>> me in providing me web links and words to the wise.
>> Time to give back. I did the presentation on introduction to R and 
>> uploaded the presentation files at the following site:
>>
>> http://www.aloofhosting.com/arinbasu/Rtutorial/Rintroweb_files/frame.htm
>>
>> A shortened form of url is here (easy for cutting and pasting):
>>
>> http://tinyurl.com/3zsr6
>>
>> Would greatly appreciate your feedbacks/opinions/advices on 
>> errors/omissions.
>
>
> Both links don't work for me, simply get an empty page and a black 
> frame on both websites (IE optimzed?) running Mozilla-1.7.x on Debian 
> GNU/Linux.
>
> sincerely
>
> Thomas
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
>


-- 
Dipl. bio-chem. Witold Eryk Wolski
MPI-Moleculare Genetic
Ihnestrasse 63-73 14195 Berlin
tel: 0049-30-83875219                 __("<    _
http://www.molgen.mpg.de/~wolski      \__/    'v'
http://r4proteomics.sourceforge.net    ||    /   \
mail: witek96 at users.sourceforge.net    ^^     m m
      wolski at molgen.mpg.de



From baron at psych.upenn.edu  Mon Oct 25 17:39:41 2004
From: baron at psych.upenn.edu (Jonathan Baron)
Date: Mon, 25 Oct 2004 11:39:41 -0400
Subject: [R] Intro to R: lecture presentation
In-Reply-To: <417D198D.3020603@swissinfo.org>
References: <20041025143357.2740.qmail@webmail29.rediffmail.com>
	<417D198D.3020603@swissinfo.org>
Message-ID: <20041025153941.GA16439@psych>

On 10/25/04 17:19, Thomas Schnhoff wrote:
>Both links don't work for me, simply get an empty page and a black frame
>on both websites (IE optimzed?) running Mozilla-1.7.x on Debian GNU/Linux.

If you are interested in seeing what you're missing, go to
View/Use_style in Mozilla and click "None".  I'm sure it doesn't
show up "properly," but really "proper" should refer to
conformity with w3c style, and this is a long way from that,
being "optimized" for a browser that many of us cannot use (and
don't want to use) and is pretty dangerous for the rest.

Jon
-- 
Jonathan Baron, Professor of Psychology, University of Pennsylvania
Home page: http://www.sas.upenn.edu/~baron



From ripley at stats.ox.ac.uk  Mon Oct 25 17:41:20 2004
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Mon, 25 Oct 2004 16:41:20 +0100 (BST)
Subject: [R] CRAN packages
In-Reply-To: <Pine.GSO.4.40.0410251111010.19132-100000@sdac.harvard.edu>
Message-ID: <Pine.LNX.4.44.0410251638510.12584-100000@gannet.stats>

Which mirror of CRAN?  I presume this is for Windows, since you mention 
.zip.

There was a problem over the w/end, now restored on the main site.

	http://cran.r-project.org/bin/windows/contrib/2.0/xtable_1.2-4.zip

is there.  Try flushing any web cache if you can't see it.

On Mon, 25 Oct 2004, Suzette Blanchard wrote:

> I am not able to download packages ("xtable") from CRAN
> not even just a .zip file,   I get that the page can not be
> found.  Please could you help?

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From ggrothendieck at myway.com  Mon Oct 25 17:52:19 2004
From: ggrothendieck at myway.com (Gabor Grothendieck)
Date: Mon, 25 Oct 2004 15:52:19 +0000 (UTC)
Subject: [R] Intro to R: lecture presentation
References: <20041025143357.2740.qmail@webmail29.rediffmail.com>
	<417D198D.3020603@swissinfo.org>
Message-ID: <loom.20041025T174751-165@post.gmane.org>

Thomas Sch??nhoff <tom_woody <at> swissinfo.org> writes:

: 
: Hello Arin,
: 
: Arin Basu schrieb:
: 
: > A couple of weeks back, I asked a question on the list that I was invited 
to provide an introductory lecture
: on R to a group of academicians in Kolkata. I thank all of you who had 
generously guided me in providing me web
: links and words to the wise. 
: > 
: > Time to give back. I did the presentation on introduction to R and 
uploaded the presentation files at the
: following site:
: > 
: > http://www.aloofhosting.com/arinbasu/Rtutorial/Rintroweb_files/frame.htm
: > 
: > A shortened form of url is here (easy for cutting and pasting):
: > 
: > http://tinyurl.com/3zsr6
: > 
: > Would greatly appreciate your feedbacks/opinions/advices on 
errors/omissions.
: 
: Both links don't work for me, simply get an empty page and a black frame 
: on both websites (IE optimzed?) running Mozilla-1.7.x on Debian GNU/Linux.

The first link worked for me using Windows XP and Internet Explorer 6.
Using Mozilla on the same machine it seemed to download something but 
Mozilla was unable to render it (got same as you) so its likely 
using IE-specific constructs.



From sundar.dorai-raj at PDF.COM  Mon Oct 25 18:17:13 2004
From: sundar.dorai-raj at PDF.COM (Sundar Dorai-Raj)
Date: Mon, 25 Oct 2004 11:17:13 -0500
Subject: [R] Intro to R: lecture presentation
In-Reply-To: <417D1CF0.5060202@molgen.mpg.de>
References: <20041025143357.2740.qmail@webmail29.rediffmail.com>	<417D198D.3020603@swissinfo.org>
	<417D1CF0.5060202@molgen.mpg.de>
Message-ID: <417D2709.30601@pdf.com>



Witold Eryk Wolski wrote:

> Same behavior with Firefox 1.0Pr.
> /E
> 
> Thomas Sch??nhoff wrote:
> 
>> Hello Arin,
>>
>>
>> Arin Basu schrieb:
>>
>>> A couple of weeks back, I asked a question on the list that I was 
>>> invited to provide an introductory lecture on R to a group of 
>>> academicians in Kolkata. I thank all of you who had generously guided 
>>> me in providing me web links and words to the wise.
>>> Time to give back. I did the presentation on introduction to R and 
>>> uploaded the presentation files at the following site:
>>>
>>> http://www.aloofhosting.com/arinbasu/Rtutorial/Rintroweb_files/frame.htm
>>>
>>> A shortened form of url is here (easy for cutting and pasting):
>>>
>>> http://tinyurl.com/3zsr6
>>>
>>> Would greatly appreciate your feedbacks/opinions/advices on 
>>> errors/omissions.
>>
>>
>>
>> Both links don't work for me, simply get an empty page and a black 
>> frame on both websites (IE optimzed?) running Mozilla-1.7.x on Debian 
>> GNU/Linux.
>>
>> sincerely
>>
>> Thomas
>>

Yes, these links only work in Internet Explorer (which I don't use 
unless forced to) since I think the web pages were created with 
PowerPoint. The orignal poster should consider a different format for 
publication.

--sundar



From ligges at statistik.uni-dortmund.de  Mon Oct 25 18:25:04 2004
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Mon, 25 Oct 2004 18:25:04 +0200
Subject: [R] .Rprofile and RODBC
In-Reply-To: <20041025134632.58071.qmail@web50901.mail.yahoo.com>
References: <20041025134632.58071.qmail@web50901.mail.yahoo.com>
Message-ID: <417D28E0.1080409@statistik.uni-dortmund.de>

Nick Drew wrote:

> Since installing R 2.0 I've had the following 2 issues
> I can't find solutions for. I'm using Windows XP.
> 
> 1) R 1.9 continues to read my .Rprofile file in my
> home directory but R 2.0 does not. How do I get R 2.0
> to do this?

Why do you think it does not? What is in there which you think is not 
executed?
You are talking about R-2.0.0 and R-1.9.1?



> 2) The following function no longer works in R 2.0 but
> continues to work in R 1.9:
> 
> 
> getExcel <- function (x) {
>  invisible(require(RODBC))
>  x <- sqlFetch(odbcConnectExcel(file.choose()),
> "exceldata", na.strings = "NA")
>  odbcCloseAll() #closes the connection
>  invisible(edit(x))
> }
> 

Works for me. What does the error message say? Do you have a recent 
version of RODBC compiled for R-2.0.0?

Uwe Ligges


> I use this function to choose an Excel file and open
> it using an ODBC connection. Why does this not work
> now?
> 
> 
> Thanks in advance for you help.
> 
> ~Nick
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html



From partha_bagchi at hgsi.com  Mon Oct 25 17:52:32 2004
From: partha_bagchi at hgsi.com (partha_bagchi@hgsi.com)
Date: Mon, 25 Oct 2004 11:52:32 -0400
Subject: [R] Intro to R: lecture presentation
Message-ID: <OF925D137E.7F7424A7-ON85256F38.00571C82-85256F38.0057356A@hgsi.com>

I get the same with IE6 on Windows XP.






Thomas Sch??nhoff <tom_woody at swissinfo.org>
Sent by: r-help-bounces at stat.math.ethz.ch
10/25/2004 11:19 AM

 
        To:     R User-Liste <r-help at stat.math.ethz.ch>
        cc: 
        Subject:        Re: [R] Intro to R: lecture presentation


Hello Arin,


Arin Basu schrieb:

> A couple of weeks back, I asked a question on the list that I was 
invited to provide an introductory lecture on R to a group of academicians 
in Kolkata. I thank all of you who had generously guided me in providing 
me web links and words to the wise.
>
> Time to give back. I did the presentation on introduction to R and 
uploaded the presentation files at the following site:
>
> http://www.aloofhosting.com/arinbasu/Rtutorial/Rintroweb_files/frame.htm
>
> A shortened form of url is here (easy for cutting and pasting):
>
> http://tinyurl.com/3zsr6
>
> Would greatly appreciate your feedbacks/opinions/advices on 
errors/omissions.

Both links don't work for me, simply get an empty page and a black frame
on both websites (IE optimzed?) running Mozilla-1.7.x on Debian GNU/Linux.

sincerely

Thomas

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html



From halldor at vedur.is  Mon Oct 25 18:53:38 2004
From: halldor at vedur.is (=?ISO-8859-1?Q?Halldor_Bj=F6rnsson?=)
Date: Mon, 25 Oct 2004 16:53:38 +0000
Subject: [R] printing ISO/8859-1 characters
Message-ID: <417D2F92.9060406@vedur.is>

Hi,
I ran into an odd problem with the print command for R-2.0 on a windows 
machine.

The icelandic character thorn (??,??) which is included in in the Latin-1
character set [iso/8859-1 char# 222 (upper case) and #254 (lower case)]
prints out incorrectly. Instead of getting the correct
character I get the octal codes for upper and lower case thorn (\336 or 
\376). This only happens on a windows machine, but not on a linux box.

This is not some problem generic to the Latin-1 characters. I get
all accented characters correctly and also the character eth (???).
Its only thorn thats problematic...

This problem also shows up on the commandline a <-"??" works
ok, but print(a) or just:
a
returns the octal number.

However, when using THORN in plotlabels it works fine.

If anyone knows how to fix this I would be very happy to hear from them...

Thanks
Halldor
-- 
------------------------------------------
Halldor Bjornsson   (halldor at vedur.is)
Vedurstofa Islands (Icelandic Met. Office)
Bustadavegur 9, IS-150, Reykjavik, Iceland



From garbade at mip.paed.uni-muenchen.de  Mon Oct 25 19:02:42 2004
From: garbade at mip.paed.uni-muenchen.de (garbade@mip.paed.uni-muenchen.de)
Date: Mon, 25 Oct 2004 19:02:42 +0200 (CEST)
Subject: [R] box() and hist()
Message-ID: <50486.129.206.90.2.1098723762.squirrel@www.paed.uni-muenchen.de>

Hi,
does anybody know why the following is not working:

> hist(rnorm(200))
> box(bty="o")

gives me a box without rounded corners.

System:
> R.Version()
$platform
[1] "i386-pc-mingw32"

$arch
[1] "i386"

$os
[1] "mingw32"

$system
[1] "i386, mingw32"

$status
[1] ""

$major
[1] "2"

$minor
[1] "0.0"

$year
[1] "2004"

$month
[1] "10"

$day
[1] "04"

$language
[1] "R"

Thanks, Sven



From gilles.guillot at inapg.inra.fr  Mon Oct 25 19:10:27 2004
From: gilles.guillot at inapg.inra.fr (Gilles GUILLOT)
Date: Mon, 25 Oct 2004 19:10:27 +0200
Subject: [R] building a package under windows
Message-ID: <200410251910.27536.gilles.guillot@inapg.inra.fr>

Hi,

I have a package of my own which seems to work fine under linux.
I want to make a compiled version for windows.
(I work with windows 2000 and R 2.0)

I have followed the steps described in the file 
readme.packages (in the top-level directory of the binary installation)
and I had a trouble at this step (which aim is not documented):

C:\Program Files\R\rw2000\src\gnuwin32>make libR.a libRblas.a
dlltool -k --as as   --dllname R.dll --def R.exp --output-lib libR.a
make: *** [libR.a] Error 255

But still, I tried to build my package by 
C:\Documents and Settings\guillot\Mes documents\package>Rcmd check geneland

and got :
 
* checking for working latex ...latex: not found
 NO
* using log directory 'C:/Documents and Settings/guillot/Mes 
documents/package/
eneland.Rcheck'
* checking for file 'geneland/DESCRIPTION' ... OK
* checking if this is a source package ... OK

installing R.css in C:/DOCUME~1/guillot/MESDOC~1/package/GENELA~1.RCH


---------- Making package geneland ------------
  adding build stamp to DESCRIPTION
  making DLL ...
make[4]: *** [libR.a] Error 255
make[3]: *** [libR] Error 2
make[2]: *** [srcDynlib] Error 2
make[1]: *** [all] Error 2
make: *** [pkg-geneland] Error 2
*** Installation of geneland failed ***

Removing 'C:/DOCUME~1/guillot/MESDOC~1/package/GENELA~1.RCH/geneland'
 ERROR
Installation failed.



What is wrong here ?
 
Gilles

-- 
_____________________________________________________________________
Gilles GUILLOT
INRA -D??partement Math??matiques et Informatique Appliqu??es

Unit?? de Mixte de Recherche INRA - INAPG - ENGREF
Institut National Agronomique de Paris-Grignon
16 rue Claude Bernard
75231 Paris cedex 5

Aile Claude Bernard
Niveau cours +3 ??tages
tel : +33 (0)1 44 08 72 71
fax : +33 (0)1 44 08 16 66
http://www.inapg.fr/ens_rech/mathinfo/personnel/guillot/welcome.html



From ripley at stats.ox.ac.uk  Mon Oct 25 19:21:23 2004
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Mon, 25 Oct 2004 18:21:23 +0100 (BST)
Subject: [R] printing ISO/8859-1 characters
In-Reply-To: <417D2F92.9060406@vedur.is>
Message-ID: <Pine.LNX.4.44.0410251814280.12905-100000@gannet.stats>

On Mon, 25 Oct 2004, Halldor Bj??rnsson wrote:

> Hi,
> I ran into an odd problem with the print command for R-2.0 on a windows 
> machine.
> 
> The icelandic character thorn (??,??) which is included in in the Latin-1
> character set [iso/8859-1 char# 222 (upper case) and #254 (lower case)]
> prints out incorrectly. Instead of getting the correct
> character I get the octal codes for upper and lower case thorn (\336 or 
> \376). This only happens on a windows machine, but not on a linux box.
> 
> This is not some problem generic to the Latin-1 characters. I get
> all accented characters correctly and also the character eth (???).
> Its only thorn thats problematic...
> 
> This problem also shows up on the commandline a <-"??" works
> ok, but print(a) or just:
> a
> returns the octal number.
> 
> However, when using THORN in plotlabels it works fine.
> 
> If anyone knows how to fix this I would be very happy to hear from them...

It's a bug in your version of Windows.  print() tests for printable 
characters, by  if(isprint((int)*p))  in src/utils/printutils.c, and
that in turn asks Windows about your locale.  (The Rgui console used to do 
this, but Windows crashed too often when it did.)

Assuming your machine is set correctly to an Icelandic locale, the only 
solution I know of is to write your own replacement for isprint and 
compile that into a build of R.  If you can, it might be worth trying 
another version of Windows: we saw something similar in Czech that worked 
in Windows XP but not 2000.

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From graumann at caltech.edu  Mon Oct 25 19:21:37 2004
From: graumann at caltech.edu (Johannes Graumann)
Date: Mon, 25 Oct 2004 10:21:37 -0700
Subject: [R] Legend/Substitute/Plotmath problem
References: <20041023110425.2b58d3df@localhost>
	<16764.45491.531701.421033@gargle.gargle.HOWL>
Message-ID: <20041025102137.1cba222c@localhost>

Thank you so much ... works now ... sooo much to learn ...

Joh

On Mon, 25 Oct 2004 09:56:35 +0200
Martin Maechler <maechler at stat.math.ethz.ch> wrote:

> >>>>> "Johannes" == Johannes Graumann <graumann at caltech.edu>
> >>>>>     on Sat, 23 Oct 2004 11:04:25 -0700 writes:
> >>>>> "Johannes" == Johannes Graumann <graumann at caltech.edu>
> >>>>>     on Sat, 23 Oct 2004 11:04:25 -0700 writes:
> 
>     Johannes> Hello,
> 
>     Johannes> I seem unable to construct a legend which contains
>     Johannes> a substitution as well as math symbols. I'm trying
>     Johannes> to do the following:
> 
>     >> strain2 <- "YJG48"
> 
>     >> legend.txt <- c(
>     >> 	substitute(
>     >> 		strain *
>     >> 		%==% *
>     >> 		"YJG45, rpn10" *
>     >> 		%Delta%,
>     >> 		list(strain=strain2)
>     >> 	),
>     >> 	"Verhulst/Logistic",
>     >> 	"Malthus"
>     >> )
> 
> 
>     Johannes>     .....................
> 
> Do try to break down a problem into simple things --
> particularly when you have problems!
> 
> This substitute() call is simply invalid:
> 
>   > ss <- substitute( strain * %==% * "YJG45, rpn10" * %Delta%,
>   > list(strain=strain2) )
>   Error: syntax error
> 
> and the 'syntax error' should give you a clue:  
> The first argument of substitute must be a syntactically correct
> R expression.
> 
> Now you try more and more simple things till you 'see it' :
> 
> Why should I expect  'A * %==% B'  to be valid syntax?
> Both '*' and '%==%' are (diadic) operators: You can't juxtapose
> them, as well as you can't write  'A * = B'.
> Then, '%Delta%' (like any other '%foo%' !!) is a diadic operator
> too and hence can't be juxtaposed to '*'. But I'm pretty sure
> you rather mean (greek) 'Delta'.
> 
> Hence:
>  ss <- substitute( strain %==% "YJG45, rpn10" * Delta,
>  list(strain=strain2) )
> 
> ---
> 
> Once you have the expression you can go further;
> still step by step :
> 
>   > c(ss, "Verhulst")
>   [[1]]
>   "YJG48" %==% "YJG45, rpn10" * Delta
> 
>   [[2]]
>   [1] "Verhulst"
> 
> Hmm, a list; that won't work.
> You do need to pass either a "character" vector or an
> expression, i.e., an expression of length 3 in our case.
> We must build the expression somewhat manually:
> 
>   > e <- expression(1, "Verhulst", "Malthus")# '1' is a place holder
>     expression(1, "Verhulst", "Malthus")
>   > e[[1]] <- ss  ## that's the trick!
> 
>   > str(e)
>     expression("YJG48" %==% "YJG45, rpn10" * Delta, "Verhulst",
>     "Malthus")
> 
>   > plot(1); legend(1,1, leg = e)
> 
> ---
> 
> Maybe something to be added as an example to help(legend) or rather
> to help(expression) ?
> 
> HTH,
> Martin Maechler, ETH Zurich
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide!
> http://www.R-project.org/posting-guide.html
>



From p.dalgaard at biostat.ku.dk  Mon Oct 25 19:20:38 2004
From: p.dalgaard at biostat.ku.dk (Peter Dalgaard)
Date: 25 Oct 2004 19:20:38 +0200
Subject: [R] printing ISO/8859-1 characters
In-Reply-To: <417D2F92.9060406@vedur.is>
References: <417D2F92.9060406@vedur.is>
Message-ID: <x2k6ter9y1.fsf@biostat.ku.dk>

Halldor Bj??rnsson <halldor at vedur.is> writes:

> Hi,
> I ran into an odd problem with the print command for R-2.0 on a
> windows machine.
> 
> The icelandic character thorn (??,??) which is included in in the Latin-1
> character set [iso/8859-1 char# 222 (upper case) and #254 (lower case)]
> prints out incorrectly. Instead of getting the correct
> character I get the octal codes for upper and lower case thorn (\336
> or \376). This only happens on a windows machine, but not on a linux
> box.
> 
> This is not some problem generic to the Latin-1 characters. I get
> all accented characters correctly and also the character eth (???).
> Its only thorn thats problematic...
> 
> This problem also shows up on the commandline a <-"??" works
> ok, but print(a) or just:
> a
> returns the octal number.
> 
> However, when using THORN in plotlabels it works fine.
> 
> If anyone knows how to fix this I would be very happy to hear from them...

It happens if (and presumably only if) isprint('??') is 0 in the
internal C code. We have heard about similar bugs in Microsoft DLLs
before. It is not a bug in R as such, but it might help to replace - I
think it is - msvcrt.dll.

-- 
   O__  ---- Peter Dalgaard             Blegdamsvej 3  
  c/ /'_ --- Dept. of Biostatistics     2200 Cph. N   
 (*) \(*) -- University of Copenhagen   Denmark      Ph: (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)             FAX: (+45) 35327907



From br44114 at yahoo.com  Mon Oct 25 19:27:44 2004
From: br44114 at yahoo.com (bogdan romocea)
Date: Mon, 25 Oct 2004 10:27:44 -0700 (PDT)
Subject: [R] output processing / ARMA order identification
Message-ID: <20041025172744.24586.qmail@web50310.mail.yahoo.com>

Dear R users,

I need to fit an ARMA model. As far as I've seen, EACF (extended ACF)
is not available in R. 

1. Let's say I fit a series of ARMA models in a loop. Given the
code/output included below, how do I pull 'Model' and 'Fit' (AIC)
from each summary() so that I can combine them into an array/data
frame to be sorted by AIC?

2. Apart from EACF, are you aware perhaps of another function in R
that can help solve the issue of ARMA order identification?

Thank you,
b.


> arma <- arma(var, order=c(1,1), lag=NULL, coef=NULL, 
+ include.intercept = TRUE, series = NULL)
> summary(arma)

Call:
arma(x = var, order = c(1, 1), lag = NULL, coef = NULL,
include.intercept = TRUE,     series = NULL)

Model:
ARMA(1,1)

Residuals:
     Min       1Q   Median       3Q      Max 
-686.092  -68.499    4.024   65.531  509.171 

Coefficient(s):
           Estimate  Std. Error  t value Pr(>|t|)    
ar1        0.990653    0.003724  265.987   <2e-16 ***
ma1       -0.019562    0.030110   -0.650   0.5159    
intercept 90.940774   36.914682    2.464   0.0138 *  
---
Signif. codes:  0 `***' 0.001 `**' 0.01 `*' 0.05 `.' 0.1 ` ' 1 

Fit:
sigma^2 estimated as 14193,  Conditional Sum-of-Squares = 17116373, 
AIC = 14983.22



From matt.kowgier at utoronto.ca  Mon Oct 25 19:32:29 2004
From: matt.kowgier at utoronto.ca (matt.kowgier@utoronto.ca)
Date: Mon, 25 Oct 2004 13:32:29 -0400
Subject: [R] sample variogram construction
In-Reply-To: <000401c4ba81$67c6dd10$7d2501a3@plants.ox.ac.uk>
References: <000401c4ba81$67c6dd10$7d2501a3@plants.ox.ac.uk>
Message-ID: <1098725549.417d38ad2c28c@webmail.utoronto.ca>

Okay thanks!
No I have the following difficulty implementing a variogram in the
nlme package:

> cd1 <- lme(count ~ time, data=cd4,random= ~ time | id)
> plot(Variogram(cd1, form= ~ time | id, robust=TRUE))
Error in as.array(X) : attempt to set an attribute on NULL

Im not sure how to fix this, and apply this function to
a longitudinal data with unequal times?

Any help would be appreciated


Quoting Dan Bebber <danbebber at forestecology.co.uk>:

> Hi Matt,
> 
> there are several R packages that will compute the sample variogram for
> you.
> Check out GeoR, sgeostat, nlme, spatial. There's no point in recoding the
> whole lot yourself, unless as a learning excercise.
> 
> D
> 
> p.s. For time series autocorrelations, you could use acf in package stats.
> 
> Message: 9
> Date: Mon, 25 Oct 2004 02:02:06 -0400
> From: matt.kowgier at utoronto.ca
> Subject: [R] sample variogram construction
> To: r-help at stat.math.ethz.ch
> Message-ID: <1098684126.417c96de334e1 at webmail.utoronto.ca>
> Content-Type: text/plain; charset=US-ASCII
> 
> Hi
> 
> Im attempting to build a sample variogram for 300 obersvations of
> longitudinal data. So what I need to do is compute the half 
> squared differences  between pairs of residuals (for instance if a subject
> has 4 obersvations, this is 4 choose 2 paird differences) for each subject.
> Also, then I need the corresponding time differences within each
> individual.
> So the end result will be a 300 by 2 matrix with columns corresponding to
> paired difference residuals within subject and time differences within
> subject. Basically im having trouble coding this kind of matrix in R, if
> anyone can help me out or give me some tips id appreciate it.
> 
> Thanks.
> Stuck in the for loop
> student
> 
>



From p.dalgaard at biostat.ku.dk  Mon Oct 25 19:39:41 2004
From: p.dalgaard at biostat.ku.dk (Peter Dalgaard)
Date: 25 Oct 2004 19:39:41 +0200
Subject: [R] box() and hist()
In-Reply-To: <50486.129.206.90.2.1098723762.squirrel@www.paed.uni-muenchen.de>
References: <50486.129.206.90.2.1098723762.squirrel@www.paed.uni-muenchen.de>
Message-ID: <x2breqr92a.fsf@biostat.ku.dk>

garbade at mip.paed.uni-muenchen.de writes:

> Hi,
> does anybody know why the following is not working:
> 
> > hist(rnorm(200))
> > box(bty="o")
> 
> gives me a box without rounded corners.

Because that isn't what it is supposed to do. Did you expect "7" to
give you a slanting right edge? And which letter should represent a
full rectangle?


-- 
   O__  ---- Peter Dalgaard             Blegdamsvej 3  
  c/ /'_ --- Dept. of Biostatistics     2200 Cph. N   
 (*) \(*) -- University of Copenhagen   Denmark      Ph: (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)             FAX: (+45) 35327907



From murdoch at stats.uwo.ca  Mon Oct 25 19:44:31 2004
From: murdoch at stats.uwo.ca (Duncan Murdoch)
Date: Mon, 25 Oct 2004 13:44:31 -0400
Subject: [R] Reading sections of data files based on pattern matching
In-Reply-To: <clj0nn$mil$1@sea.gmane.org>
References: <clj0nn$mil$1@sea.gmane.org>
Message-ID: <aa3qn056hgvlgv08lc27kkgtttgasnvfj3@4ax.com>

On Mon, 25 Oct 2004 15:56:37 +0200, Henrik Andersson
<h.andersson at nioo.knaw.nl> wrote :

>I am about to write general functions to read the output of simulations 
>models.
>
>These model generate output files with different sections which I want 
>to analyze plot etc.
>
>Since this will be used many people at the department I wanted to make 
>sure that will do this in the best way.
>
>For instance I want to read a snippets of data from a text that look 
>like this.
>-------------------------------
>Lots of stuff
>...
>@@Start Values@@
>	Column1 Column2 Column3 ...
>Row1	1	2	3 ...
>...
>@@End Values@@
>
>More stuff
>...
>@@Start OtherValues@@
>	Column1 Column2 Column3 ...
>Row1	1	2	3 ...
>...
>@@End OtherValues@@
>
>
>I looked in the help files and found grep which operates on character 
>strings, do I have to like this then?
>
>1. Read file with readLines("foo.txt")
>2. grep this object for the start and end of each section ->startline & 
>stopline
>3. Read the file again with 
>read.table("foo.txt",skip=startline,nrows=stoplin-startline)
>
>Or is there a more beautiful way?

I would avoid putting mixing multiple tables in the same file.  I
think you'll run into fewer problems if you put each table into a
separate file, and generate an index file to list all the tables.
Each of the files in your scheme would then become a subdirectory in
my scheme.

If the multiplicity of files is a problem, you could use zip or winzip
to put them all into a zip file; R can extract a file from one of
those using zip.file.extract.

Duncan Murdoch



From ripley at stats.ox.ac.uk  Mon Oct 25 19:45:03 2004
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Mon, 25 Oct 2004 18:45:03 +0100 (BST)
Subject: [R] building a package under windows
In-Reply-To: <200410251910.27536.gilles.guillot@inapg.inra.fr>
Message-ID: <Pine.LNX.4.44.0410251839510.12905-100000@gannet.stats>

On Mon, 25 Oct 2004, Gilles GUILLOT wrote:

> Hi,
> 
> I have a package of my own which seems to work fine under linux.
> I want to make a compiled version for windows.
> (I work with windows 2000 and R 2.0)

There is no such version of R.

> I have followed the steps described in the file 
> readme.packages (in the top-level directory of the binary installation)

Clearly you haven't, as you don't have latex in your path, and I don't 
think you have the proper compilers either.

> and I had a trouble at this step (which aim is not documented):

Yes, it is: it makes the libraries libR.a libRblas.a!  Pretty
self-explanatory don't you think?

> C:\Program Files\R\rw2000\src\gnuwin32>make libR.a libRblas.a
> dlltool -k --as as   --dllname R.dll --def R.exp --output-lib libR.a
> make: *** [libR.a] Error 255

So something is wrong with your compiler installation (if you have one), 
for dlltool or some of its components are missing.

> But still, I tried to build my package by 
> C:\Documents and Settings\guillot\Mes documents\package>Rcmd check geneland
> 
> and got :
>  
> * checking for working latex ...latex: not found

and you are missing some more tools.

>  NO
> * using log directory 'C:/Documents and Settings/guillot/Mes 
> documents/package/
> eneland.Rcheck'
> * checking for file 'geneland/DESCRIPTION' ... OK
> * checking if this is a source package ... OK
> 
> installing R.css in C:/DOCUME~1/guillot/MESDOC~1/package/GENELA~1.RCH
> 
> 
> ---------- Making package geneland ------------
>   adding build stamp to DESCRIPTION
>   making DLL ...
> make[4]: *** [libR.a] Error 255
> make[3]: *** [libR] Error 2
> make[2]: *** [srcDynlib] Error 2
> make[1]: *** [all] Error 2
> make: *** [pkg-geneland] Error 2
> *** Installation of geneland failed ***
> 
> Removing 'C:/DOCUME~1/guillot/MESDOC~1/package/GENELA~1.RCH/geneland'
>  ERROR
> Installation failed.
> 
> What is wrong here ?

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From MSchwartz at MedAnalytics.com  Mon Oct 25 19:55:19 2004
From: MSchwartz at MedAnalytics.com (Marc Schwartz)
Date: Mon, 25 Oct 2004 12:55:19 -0500
Subject: [R] box() and hist()
In-Reply-To: <50486.129.206.90.2.1098723762.squirrel@www.paed.uni-muenchen.de>
References: <50486.129.206.90.2.1098723762.squirrel@www.paed.uni-muenchen.de>
Message-ID: <1098726919.28228.39.camel@localhost.localdomain>

On Mon, 2004-10-25 at 12:02, garbade at mip.paed.uni-muenchen.de wrote:
> Hi,
> does anybody know why the following is not working:
> 
> > hist(rnorm(200))
> > box(bty="o")
> 
> gives me a box without rounded corners.

You will not get rounded corners with box().

The 'bty' argument determines how many sides of the plot region are
drawn using box().

"o" gives you all four sides (1:4)

"l" gives you left and lower (2 and 1)

"7" gives you upper and right (3:4)

"c" gives you all except right (1:2, 3)

"u" gives you all except upper (1:2, 4)

"]" gives you all except left (1, 3:4)

Try this as an example:

plot(1:10, axes = FALSE)
box(bty = "7")

Substitute each of the bty types to review further.

HTH,

Marc Schwartz



From ligges at statistik.uni-dortmund.de  Mon Oct 25 19:56:40 2004
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Mon, 25 Oct 2004 19:56:40 +0200
Subject: [R] building a package under windows
In-Reply-To: <200410251910.27536.gilles.guillot@inapg.inra.fr>
References: <200410251910.27536.gilles.guillot@inapg.inra.fr>
Message-ID: <417D3E58.6010003@statistik.uni-dortmund.de>

Gilles GUILLOT wrote:

> Hi,
> 
> I have a package of my own which seems to work fine under linux.
> I want to make a compiled version for windows.
> (I work with windows 2000 and R 2.0)
 >
> I have followed the steps described in the file 
> readme.packages (in the top-level directory of the binary installation)
> and I had a trouble at this step (which aim is not documented):
> 
> C:\Program Files\R\rw2000\src\gnuwin32>make libR.a libRblas.a
> dlltool -k --as as   --dllname R.dll --def R.exp --output-lib libR.a
> make: *** [libR.a] Error 255


Hmm, don't know at this place (if the paths are correct and in the 
correct order, and all tools are installed).
I'd rather try to build R from scratch, and you'll get libR.a.

> But still, I tried to build my package by 
> C:\Documents and Settings\guillot\Mes documents\package>Rcmd check geneland
> 
> and got :
>  
> * checking for working latex ...latex: not found
>  NO

So install LaTeX.


> * using log directory 'C:/Documents and Settings/guillot/Mes 
> documents/package/
> eneland.Rcheck'

Does it really read "eneland"? Then something is wrong with the tools 
you have installed, I presume. And you have left out at least one 
sentence of "readme.packages" ...

Uwe Ligges


One last comment: Even if it works on some machines, I do never trust 
paths with blanks in it.


> * checking for file 'geneland/DESCRIPTION' ... OK
> * checking if this is a source package ... OK
> 
> installing R.css in C:/DOCUME~1/guillot/MESDOC~1/package/GENELA~1.RCH
> 
> 
> ---------- Making package geneland ------------
>   adding build stamp to DESCRIPTION
>   making DLL ...
> make[4]: *** [libR.a] Error 255
> make[3]: *** [libR] Error 2
> make[2]: *** [srcDynlib] Error 2
> make[1]: *** [all] Error 2
> make: *** [pkg-geneland] Error 2
> *** Installation of geneland failed ***
> 
> Removing 'C:/DOCUME~1/guillot/MESDOC~1/package/GENELA~1.RCH/geneland'
>  ERROR
> Installation failed.
> 
> 
> 
> What is wrong here ?
>  
> Gilles
>



From gregory.r.warnes at pfizer.com  Mon Oct 25 20:10:32 2004
From: gregory.r.warnes at pfizer.com (Warnes, Gregory R)
Date: Mon, 25 Oct 2004 14:10:32 -0400 (EDT)
Subject: [R] How to save a complete image of the current state of R  ?
Message-ID: <915D2D65A9986440A277AC5C98AA466F0A1B5D@groamrexm02.amer.pfizer.
	com>

Date: Mon, 25 Oct 2004 14:10:15 -0400
MIME-Version: 1.0
X-Mailer: Internet Mail Service (5.5.2654.89)
Content-Type: text/plain; charset="iso-8859-1"


It is possible to save all of these items using the save.session and
restore.session functions provided by the 'session' package available from
CRAN.

-Greg

> -----Original Message-----
> From: r-help-bounces at stat.math.ethz.ch
> [mailto:r-help-bounces at stat.math.ethz.ch]On Behalf Of Andreas Buness
> Sent: Friday, October 22, 2004 1:05 PM
> To: r-help at stat.math.ethz.ch
> Subject: [R] How to save a complete image of the current state of R ? 
> 
> 
> Hello,
> 
> I like to save the complete state of R, i.e. including
> all environments, objects/workspaces, loaded packages etc..
> 
> This wish has arisen since I am not able to reproduce
> an error which occurs when running R CMD check.
> 
> Many thanks for your advice in advance.
> Best Regards
> Andreas
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
> 


LEGAL NOTICE\ Unless expressly stated otherwise, this messag...{{dropped}}



From graumann at caltech.edu  Mon Oct 25 20:19:38 2004
From: graumann at caltech.edu (Johannes Graumann)
Date: Mon, 25 Oct 2004 11:19:38 -0700
Subject: [R] par("usr") trouble in multiplot axis scaling
Message-ID: <20041025111938.174d51c3@localhost>

Hello,

I'm blotting a series of growth curves into a multiplot environment
created with layout().
since I want the four plots to be easily visually comparable, I do the
following:

#first plot
plot(x,y,<stuff>)
standarduser<-par()$usr
...
<some fitting>
...
lines(spline(x, <fitted_equation>))

#everything all right till here
# second plot
plot(x,y,<stuff>)
par(usr=standarduser)
...
<some fitting>
...
lines(spline(x, <fitted_equation>))

The problem here is, that the axis of the second plot seem to be scaled
according to the parameters of the first, BUT the fitted curve in the
second plot isn't!

Any idea about what I'm doing wrong?

Please help this newbie out of his misery!

Joh



From kshe4 at student.monash.edu  Mon Oct 25 20:20:22 2004
From: kshe4 at student.monash.edu (Kunal Shetty)
Date: Mon, 25 Oct 2004 18:20:22 +0000
Subject: [R] Ref: Variable scope or function behaviour or array reassign
Message-ID: <220.253.19.67.1098727156.58963@my.monash.edu.au>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20041025/a82cb87a/attachment.pl

From ripley at stats.ox.ac.uk  Mon Oct 25 20:24:25 2004
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Mon, 25 Oct 2004 19:24:25 +0100 (BST)
Subject: [R] output processing / ARMA order identification
In-Reply-To: <20041025172744.24586.qmail@web50310.mail.yahoo.com>
Message-ID: <Pine.LNX.4.44.0410251920570.13013-100000@gannet.stats>

On Mon, 25 Oct 2004, bogdan romocea wrote:

> Dear R users,
> 
> I need to fit an ARMA model. As far as I've seen, EACF (extended ACF)
> is not available in R. 

But PACF is as well as ACF.

> 1. Let's say I fit a series of ARMA models in a loop. Given the
> code/output included below, how do I pull 'Model' and 'Fit' (AIC)
> from each summary() so that I can combine them into an array/data
> frame to be sorted by AIC?

Look at the examples in tests/ts-tests.R.  Or read the arima() help page 
more carefully, as it tells you where the results are (and they are not in 
summary(), which does not work for arima fits!).

> 2. Apart from EACF, are you aware perhaps of another function in R
> that can help solve the issue of ARMA order identification?

It's a rather old-fashioned idea.  Just fit all the target models and 
compare their performance.

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From gunter.berton at gene.com  Mon Oct 25 20:31:00 2004
From: gunter.berton at gene.com (Berton Gunter)
Date: Mon, 25 Oct 2004 11:31:00 -0700
Subject: [R] Plotting Bivariate Normal Data
Message-ID: <200410251831.i9PIV0no029639@ohm.gene.com>

 
Just a little addendum to Martin's comments below. It is well known that
using LS centers and covariances for the M-distances is generally not a good
way to do this, as these statistics, themselves, are distorted by the long
"tails" (do > 1D distributions have "tails"?)  so that the problems are
hidden (see Brian Ripley's comments on the R-Help "robust regression with
groups" thread  from last week). Hence, one should use a resistant center
(the medioid, say) and a resistant covariance matrix (e.g., from cov.rob())
to compute the M-distances.

... But then, this begs the question: Why do normality testing at all?
(again, see BR's comments). Better to use robust/resistant statistical
procedures for estimation from the beginning, though, unfortunately, this
shatters the nice simple mathematical framework for inference. 

-- Bert Gunter
Genentech Non-Clinical Statistics
South San Francisco, CA
 
"The business of the statistician is to catalyze the scientific learning
process."  - George E. P. Box
 
 

> Since one of the more severe and common deviations from
> normality is "long tailed"ness (in all it's vaguety), we have
> been recommending to QQ-plot mahalanobis distances against chi
> squared quantiles - even before looking at the univariate
> QQ plots.
> 
> Exactly for this reason, in R,
> 	example(mahalanobis)
> shows a version of how to do this!
> 
> Martin Maechler, ETH Zurich
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
>



From das at cshl.edu  Mon Oct 25 21:13:21 2004
From: das at cshl.edu (Rajdeep Das)
Date: Mon, 25 Oct 2004 15:13:21 -0400
Subject: [R] Feature selection
Message-ID: <002c01c4bac6$b196fd90$1c0b308f@artney>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20041025/cf015005/attachment.pl

From ligges at statistik.uni-dortmund.de  Mon Oct 25 21:24:30 2004
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Mon, 25 Oct 2004 21:24:30 +0200
Subject: [R] par("usr") trouble in multiplot axis scaling
In-Reply-To: <20041025111938.174d51c3@localhost>
References: <20041025111938.174d51c3@localhost>
Message-ID: <417D52EE.9040801@statistik.uni-dortmund.de>

Johannes Graumann wrote:
> Hello,
> 
> I'm blotting a series of growth curves into a multiplot environment
> created with layout().
> since I want the four plots to be easily visually comparable, I do the
> following:
> 
> #first plot
> plot(x,y,<stuff>)
> standarduser<-par()$usr
> ...
> <some fitting>
> ...
> lines(spline(x, <fitted_equation>))
> 
> #everything all right till here
> # second plot
> plot(x,y,<stuff>)
> par(usr=standarduser)
> ...
> <some fitting>
> ...
> lines(spline(x, <fitted_equation>))
> 
> The problem here is, that the axis of the second plot seem to be scaled
> according to the parameters of the first, BUT the fitted curve in the
> second plot isn't!
> 
> Any idea about what I'm doing wrong?
> 
> Please help this newbie out of his misery!

Can you specify a very simple reproducible example please?

Uwe Ligges

> Joh
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html



From ligges at statistik.uni-dortmund.de  Mon Oct 25 21:29:58 2004
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Mon, 25 Oct 2004 21:29:58 +0200
Subject: [R] Ref: Variable scope or function behaviour or array reassign
In-Reply-To: <220.253.19.67.1098727156.58963@my.monash.edu.au>
References: <220.253.19.67.1098727156.58963@my.monash.edu.au>
Message-ID: <417D5436.4030105@statistik.uni-dortmund.de>

Kunal Shetty wrote:

> Dear R- helpers
> 
> Following a draft structure of the R script for which I am facing problem
> 
> Step 1
> x <-   of type array with original values
> y <-   of type array  with original values
> 
> 
> Step 2
> 
> for (ctr in 1:10) {
> 
> # my problem here the both x and y still show the original values from step 1
> # in spite of making changes to the old values of the arrays x and y in the function
>    function (x,y)  ???
> 
>     }
> 
> 
> step3
> 
> 
> output < - function(parX,parY){
> 
>           Variables for New X and Y
> 	newx <- array(parX, dim=c(1,length(parX)))
> 	newy <- array(parY, dim=c(1,length(parY)))
> 
>                 # make some calculation and updated some arrays element in the newX and   # #newY
> 
> 
>                        # finally assign the global  original values x and y with newX and newY
>                         
>                        x<- newx
> 	           y<- newy
>                       # if print here I can see the new values
>                   
>                        # but when the function gets called the second time  the original values of 
>            #  x and y get called hence failing my motive of passing update values of                  #the arrays to the function each time   ???
> }
> I believe there something to deal with env or new.env.... but never could get the concept of the variable scope.....where the fact with the function the x and y get updated but while calling from the main loop the x and y are pointing to old values
> 
>   also I am keen to know as to is there a way to clear the array of old values and reasign new values..or the <- operator take care of it by overwriting
> 
> regards
> Kunal
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html


I don't understand your question completely. Anyway, I guess you have 
missed the point that you can pass arguments to a function and that you 
can return an object (e.g. a list of other objects) from the function 
(please read "An Introduction to R" for more details).
Assign the value returned by a function it to an object in the calling 
function. In most circumstances, don't think too much about accessing 
environments, because you don't want to do it! The mechanisms mentioned 
above are sufficient in most circumstances.

Uwe Ligges



From MSchwartz at MedAnalytics.com  Mon Oct 25 21:25:55 2004
From: MSchwartz at MedAnalytics.com (Marc Schwartz)
Date: Mon, 25 Oct 2004 14:25:55 -0500
Subject: [R] par("usr") trouble in multiplot axis scaling
In-Reply-To: <20041025111938.174d51c3@localhost>
References: <20041025111938.174d51c3@localhost>
Message-ID: <1098732355.28228.57.camel@localhost.localdomain>

On Mon, 2004-10-25 at 13:19, Johannes Graumann wrote:
> Hello,
> 
> I'm blotting a series of growth curves into a multiplot environment
> created with layout().
> since I want the four plots to be easily visually comparable, I do the
> following:
> 
> #first plot
> plot(x,y,<stuff>)
> standarduser<-par()$usr
> ...
> <some fitting>
> ...
> lines(spline(x, <fitted_equation>))
> 
> #everything all right till here
> # second plot
> plot(x,y,<stuff>)
> par(usr=standarduser)
> ...
> <some fitting>
> ...
> lines(spline(x, <fitted_equation>))
> 
> The problem here is, that the axis of the second plot seem to be scaled
> according to the parameters of the first, BUT the fitted curve in the
> second plot isn't!
> 
> Any idea about what I'm doing wrong?
> 
> Please help this newbie out of his misery!
> 
> Joh

If I am correctly understanding what you are doing and what you want,
you would like each of the four plots to have the same axis ranges?

Part of the problem, I think, is that in your second plot(), the axis
ranges are automatically set based upon the ranges of your x and y data
in that call. These presumably are different than the x and y values in
your first plot? 

Thus, the initial plot region scales are going to be different for each
plot. By default, this will be range(x) +/- 4% and range(y) +/- 4%.

When you force the second plot region's values to be 'standarduser',
your underlying x,y plot, having already been drawn, and the new lines
to be added are then on different scales in the same plot.

If my assumptions are correct, you would be better off calling plot()
each time using the 'xlim' and 'ylim' arguments to explicitly define the
axis ranges with known common values.

For example, if you know that the range of all x values is r.x and the
range of all y values is r.y:

#first plot
plot(x, y, <stuff>, xlim = r.x, ylim = r.y)
...
<some fitting>
...
lines(spline(x, <fitted_equation>))


# second plot
plot(x, y, <stuff>, xlim = r.x, ylim = r.y)
...
<some fitting>
...
lines(spline(x, <fitted_equation>))


This gets around the need to manipulate the pars directly and hopefully
less confusion in reading the code. The key is knowing the common ranges
of your x and y values in advance.

Does that help?

Marc Schwartz



From graumann at its.caltech.edu  Mon Oct 25 21:40:06 2004
From: graumann at its.caltech.edu (Johannes Graumann)
Date: Mon, 25 Oct 2004 12:40:06 -0700
Subject: [R] par("usr") trouble in multiplot axis scaling
In-Reply-To: <1098732355.28228.57.camel@localhost.localdomain>
References: <20041025111938.174d51c3@localhost>
	<1098732355.28228.57.camel@localhost.localdomain>
Message-ID: <20041025124006.464d3ea3@localhost>

You were right! 'ylim' does what I want!
Thanks for deciphering my cryptic scribble and helping out!

Joh

On Mon, 25 Oct 2004 14:25:55 -0500
Marc Schwartz <MSchwartz at MedAnalytics.com> wrote:

> On Mon, 2004-10-25 at 13:19, Johannes Graumann wrote:
> > Hello,
> > 
> > I'm blotting a series of growth curves into a multiplot environment
> > created with layout().
> > since I want the four plots to be easily visually comparable, I do
> > the following:
> > 
> > #first plot
> > plot(x,y,<stuff>)
> > standarduser<-par()$usr
> > ...
> > <some fitting>
> > ...
> > lines(spline(x, <fitted_equation>))
> > 
> > #everything all right till here
> > # second plot
> > plot(x,y,<stuff>)
> > par(usr=standarduser)
> > ...
> > <some fitting>
> > ...
> > lines(spline(x, <fitted_equation>))
> > 
> > The problem here is, that the axis of the second plot seem to be
> > scaled according to the parameters of the first, BUT the fitted
> > curve in the second plot isn't!
> > 
> > Any idea about what I'm doing wrong?
> > 
> > Please help this newbie out of his misery!
> > 
> > Joh
> 
> If I am correctly understanding what you are doing and what you want,
> you would like each of the four plots to have the same axis ranges?
> 
> Part of the problem, I think, is that in your second plot(), the axis
> ranges are automatically set based upon the ranges of your x and y
> data in that call. These presumably are different than the x and y
> values in your first plot? 
> 
> Thus, the initial plot region scales are going to be different for
> each plot. By default, this will be range(x) +/- 4% and range(y) +/-
> 4%.
> 
> When you force the second plot region's values to be 'standarduser',
> your underlying x,y plot, having already been drawn, and the new lines
> to be added are then on different scales in the same plot.
> 
> If my assumptions are correct, you would be better off calling plot()
> each time using the 'xlim' and 'ylim' arguments to explicitly define
> the axis ranges with known common values.
> 
> For example, if you know that the range of all x values is r.x and the
> range of all y values is r.y:
> 
> #first plot
> plot(x, y, <stuff>, xlim = r.x, ylim = r.y)
> ...
> <some fitting>
> ...
> lines(spline(x, <fitted_equation>))
> 
> 
> # second plot
> plot(x, y, <stuff>, xlim = r.x, ylim = r.y)
> ...
> <some fitting>
> ...
> lines(spline(x, <fitted_equation>))
> 
> 
> This gets around the need to manipulate the pars directly and
> hopefully less confusion in reading the code. The key is knowing the
> common ranges of your x and y values in advance.
> 
> Does that help?
> 
> Marc Schwartz
> 
> 
>



From kshe4 at student.monash.edu  Mon Oct 25 22:08:05 2004
From: kshe4 at student.monash.edu (Kunal Shetty)
Date: Mon, 25 Oct 2004 20:08:05 +0000
Subject: [R] Ref: Variable scope or function behaviour or array reassign
References: <417D5436.4030105@statistik.uni-dortmund.de>
Message-ID: <220.253.19.67.1098734867.99576@my.monash.edu.au>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20041025/4aeb9f24/attachment.pl

From sundar.dorai-raj at PDF.COM  Mon Oct 25 22:36:54 2004
From: sundar.dorai-raj at PDF.COM (Sundar Dorai-Raj)
Date: Mon, 25 Oct 2004 15:36:54 -0500
Subject: [R] functions for special na handling?
In-Reply-To: <E585EABA11227445B918BFB74C1A4D36015958@sanctum01.sanctumfi.com>
References: <E585EABA11227445B918BFB74C1A4D36015958@sanctum01.sanctumfi.com>
Message-ID: <417D63E6.7040305@pdf.com>



Robert Sams wrote:

> hi,
> 
> i need a function that can replace na's in univariate ts objects with a value interpolated linearly 
> using the adjacent non-na values. for example, the function i need (myfun) would do the following
> 
> 
>>x <- ts(10,11.4,NA,12,9.7)
>>y <- myfun(x)
>>y
> 
> 10 11.4 11.7 12 9.7
> 
> i can code an na.action method myself to accomplish this, but has someone else already coded routines 
> of this nature?
> 
> many thanks,
> robert
> 
> 
> Robert Sams

Hi Robert,
   Will the following do?

na.approx <- function(x) {
   na <- is.na(x)
   if(all(!na)) return(x)
   i <- seq(along = x)
   x[na] <- approx(i[!na], x[!na], i[na])$y
   x
}

 > x <- c(10,11.4,NA,12,9.7)
 > na.approx(x)
[1] 10.0 11.4 11.7 12.0  9.7


--sundar



From sundar.dorai-raj at PDF.COM  Mon Oct 25 22:49:02 2004
From: sundar.dorai-raj at PDF.COM (Sundar Dorai-Raj)
Date: Mon, 25 Oct 2004 15:49:02 -0500
Subject: [R] functions for special na handling?
In-Reply-To: <417D63E6.7040305@pdf.com>
References: <E585EABA11227445B918BFB74C1A4D36015958@sanctum01.sanctumfi.com>
	<417D63E6.7040305@pdf.com>
Message-ID: <417D66BE.1030204@pdf.com>



Sundar Dorai-Raj wrote:

> 
> 
> Robert Sams wrote:
> 
>> hi,
>>
>> i need a function that can replace na's in univariate ts objects with 
>> a value interpolated linearly using the adjacent non-na values. for 
>> example, the function i need (myfun) would do the following
>>
>>
>>> x <- ts(10,11.4,NA,12,9.7)
>>> y <- myfun(x)
>>> y
>>
>>
>> 10 11.4 11.7 12 9.7
>>
>> i can code an na.action method myself to accomplish this, but has 
>> someone else already coded routines of this nature?
>>
>> many thanks,
>> robert
>>
>>
>> Robert Sams
> 
> 
> Hi Robert,
>   Will the following do?
> 
> na.approx <- function(x) {
>   na <- is.na(x)
>   if(all(!na)) return(x)
>   i <- seq(along = x)
>   x[na] <- approx(i[!na], x[!na], i[na])$y
>   x
> }
> 
>  > x <- c(10,11.4,NA,12,9.7)
>  > na.approx(x)
> [1] 10.0 11.4 11.7 12.0  9.7
> 
> 

One other note: na.approx as written above will return NA if the first 
or last element of your vector is missing. See ?approx on how to change 
this behaviour.

--sundar



From Melania.Pintilie at uhn.on.ca  Mon Oct 25 22:53:39 2004
From: Melania.Pintilie at uhn.on.ca (Pintilie, Melania)
Date: Mon, 25 Oct 2004 16:53:39 -0400
Subject: [R] Question on bioconductor: reading affymetrix data
Message-ID: <D57F6E567EA2744C90085552E5C7CF4234D8BD@UHNVMAIL001.uhn.ca>

Hi everyone,

My purpose is to read a .CEL file into R.

The .CEL file was created from a .CAB by using DTT software found on
Affymetrix website 
I read the .CEL file in R using ReadAffy as follows:
> d2=ReadAffy(widget=T)
and I complete the fields as required.
It does not complain. For example I could find the description:
> description(d2)
Experimenter name: BB 
Laboratory: FFL 
Contact information: 
 
Title: Heter 
URL: www.bioconductor.org 

A 1 word abstract is available. Use 'abstract' method.

Information is available on: samples, preprocessing 


But if I want to see the values for MM I get an error:

> mm(d2)[1:2]
Note: You did not specify a download type.  Using a default value of: Win32 
This will be fine for almost all users
 
Error in if ((is.null(deps)) || (length(deps) == 0) || (deps == ""))
return(NULL) : 
        missing value where TRUE/FALSE needed

I appreciate any ideas you may have. Thank you.



Melania
Rm 15-433, ext 4886

Fax: (416) 946-2048



From tobias.sing at mpi-sb.mpg.de  Mon Oct 25 23:15:21 2004
From: tobias.sing at mpi-sb.mpg.de (Tobias Sing)
Date: Mon, 25 Oct 2004 23:15:21 +0200
Subject: [R] building a package under windows
In-Reply-To: <200410251910.27536.gilles.guillot@inapg.inra.fr>
References: <200410251910.27536.gilles.guillot@inapg.inra.fr>
Message-ID: <200410252315.21009.tobias.sing@mpi-sb.mpg.de>

As an alternative to building the package under Windows you might go for the 
quick-and-dirty solution suggested by Peter Dalgaard a while ago on this 
list: simply zip the installed Linux package. The package can be installed 
under Windows using this zip file (worked fine for me with R 1.9.1).
Here is the original posting:
http://tolstoy.newcastle.edu.au/R/help/04/06/1555.html
Good luck,
  Tobias.

On Monday 25 October 2004 19:10, Gilles GUILLOT wrote:
> Hi,
>
> I have a package of my own which seems to work fine under linux.
> I want to make a compiled version for windows.
> (I work with windows 2000 and R 2.0
> [...]


__________________________________________________________________________
Tobias Sing                         phone: +49 681 9325 315   
Max-Planck-Institut f??r Informatik  fax  : +49 681 9325 399   
Stuhlsatzenhausweg 85               email: tobias.sing at mpi-sb.mpg.de  
66123 Saarbr??cken, Germany          web  : http://www.mpi-sb.mpg.de/~tsing



From dataanalytics at rediffmail.com  Mon Oct 25 20:13:03 2004
From: dataanalytics at rediffmail.com (Arin Basu)
Date: 25 Oct 2004 18:13:03 -0000
Subject: [R] Revision: post on Intro to R lecture
Message-ID: <20041025181303.6836.qmail@webmail32.rediffmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20041025/d1d0eca5/attachment.pl

From clint at ecy.wa.gov  Tue Oct 26 00:25:21 2004
From: clint at ecy.wa.gov (Clint Bowman)
Date: Mon, 25 Oct 2004 15:25:21 -0700 (PDT)
Subject: [R] Revision: post on Intro to R lecture
In-Reply-To: <20041025181303.6836.qmail@webmail32.rediffmail.com>
Message-ID: <Pine.LNX.4.44.0410251523210.2780-100000@aeolus.ecy.wa.gov>

Much improved (patched Firefox 1.0pre on RH9)

On 25 Oct 2004, Arin Basu wrote:

> Hi All:
> 
> This follows my earlier post on webized slides on lecture presentation on introducing R. I learned that in Mozilla (Firefox) browsers, the slides did not show up. Sorry for the no show. As a reluctant windows user, I kind of carelessly clicked through Powerpoint to convert the presentation file to its html form, unwittingly leading to the mess.
> 
> See if it got corrected now (I do not have firefox yet in my computer, so no way of knowing whether it works), and please let me know. Also, made some changes and reformatted the original slides to make them other browser compatible, thanks to comments from Gabor Grothendiek, Stuart Lesk, and Sundar Dorai-Raj.
> 
> Here is the URL again:
> 
> http://www.aloofhosting.com/arinbasu/Rintroweb.htm  
> 
> /Arin Basu
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
> 

-- 
Clint Bowman			INTERNET:	clint at ecy.wa.gov
Air Quality Modeler		INTERNET:	clint at math.utah.edu
Department of Ecology		VOICE:		(360) 407-6815
PO Box 47600			FAX:		(360) 407-7534
Olympia, WA 98504-7600



From rolf at math.unb.ca  Tue Oct 26 00:42:33 2004
From: rolf at math.unb.ca (Rolf Turner)
Date: Mon, 25 Oct 2004 19:42:33 -0300 (ADT)
Subject: [R] Scoping and nls.
Message-ID: <200410252242.i9PMgXYM001348@erdos.math.unb.ca>


A colleague of mine is trying to use nls() to effect an optimization,
and is encountering a scoping problem.  I should know how to solve it
for him but .... well, I just don't.

I also had a quick scrounge of the archives --- I know I've seen this
topic addressed before --- but I couldn't track it down.

So here's a toy example that demonstrates the problem:

hhh <- function(y,x) {
        g <- function(a,b,x) {1/(1+a^2 + b^2*exp(x))}
        nls(y~g(a,b,x),data=data.frame(x=x,y=y))
}
set.seed(123)
x <- runif(50,1,10)
y <- 1/(1+16 + 36*exp(x)) + rnorm(50,0,0.1)
hhh(y,x)

which results in an error

	Error in get(x, envir, mode, inherits) : variable "g" was not found

So can one assign the function ``g'' somewhere where it **can**
be found?  I.e. use something like

	assign("g",g,envir=??????)

Or is there some other magic incantation that can be used here
to get nls to deal with a ``g'' defined inside the function
which call nsl()?

In the real problem the function g is a complicated gadget, taking
different forms in different parts of its domain, so it is
inconvenient-to-impossible to specify it explicitly in the formula in
the call to nls().

Thanks for any help you can give.

				cheers,

					Rolf Turner
					rolf at math.unb.ca



From jmacdon at med.umich.edu  Tue Oct 26 00:42:41 2004
From: jmacdon at med.umich.edu (James MacDonald)
Date: Mon, 25 Oct 2004 18:42:41 -0400
Subject: [R] Question on bioconductor: reading affymetrix data
Message-ID: <s17d492c.097@med-gwia-01a.med.umich.edu>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20041025/8644df2f/attachment.pl

From jfox at mcmaster.ca  Tue Oct 26 01:23:28 2004
From: jfox at mcmaster.ca (John Fox)
Date: Mon, 25 Oct 2004 19:23:28 -0400
Subject: [R] Plotting Bivariate Normal Data
In-Reply-To: <200410251831.i9PIV0no029639@ohm.gene.com>
Message-ID: <20041025232327.SJDR25820.tomts20-srv.bellnexxia.net@JohnDesktop8300>

Dear Bert,

The data.ellipse() function in the car package optionally uses the
covariance matrix and location vector returned by cov.trob() (from the MASS
package). I believe that any 1D function of the two variables is potentially
problematic. As to why do it -- comparing the bivariate distribution to the
bivariate normal might be an interesting way to think about the shape of the
distribution.

Regards,
 John

--------------------------------
John Fox
Department of Sociology
McMaster University
Hamilton, Ontario
Canada L8S 4M4
905-525-9140x23604
http://socserv.mcmaster.ca/jfox 
-------------------------------- 

> -----Original Message-----
> From: r-help-bounces at stat.math.ethz.ch 
> [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Berton Gunter
> Sent: Monday, October 25, 2004 1:31 PM
> To: R-Help
> Subject: Re: [R] Plotting Bivariate Normal Data
> 
>  
> Just a little addendum to Martin's comments below. It is well 
> known that using LS centers and covariances for the 
> M-distances is generally not a good way to do this, as these 
> statistics, themselves, are distorted by the long "tails" (do 
> > 1D distributions have "tails"?)  so that the problems are 
> hidden (see Brian Ripley's comments on the R-Help "robust 
> regression with groups" thread  from last week). Hence, one 
> should use a resistant center (the medioid, say) and a 
> resistant covariance matrix (e.g., from cov.rob()) to compute 
> the M-distances.
> 
> ... But then, this begs the question: Why do normality testing at all?
> (again, see BR's comments). Better to use robust/resistant 
> statistical procedures for estimation from the beginning, 
> though, unfortunately, this shatters the nice simple 
> mathematical framework for inference. 
> 
> -- Bert Gunter
> Genentech Non-Clinical Statistics
> South San Francisco, CA
>  
> "The business of the statistician is to catalyze the 
> scientific learning process."  - George E. P. Box
>  
>  
> 
> > Since one of the more severe and common deviations from 
> normality is 
> > "long tailed"ness (in all it's vaguety), we have been 
> recommending to 
> > QQ-plot mahalanobis distances against chi squared quantiles - even 
> > before looking at the univariate QQ plots.
> > 
> > Exactly for this reason, in R,
> > 	example(mahalanobis)
> > shows a version of how to do this!
> > 
> > Martin Maechler, ETH Zurich
> > 
> > ______________________________________________
> > R-help at stat.math.ethz.ch mailing list
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide! 
> > http://www.R-project.org/posting-guide.html
> >
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html



From edgar at cs.uprm.edu  Tue Oct 26 02:11:41 2004
From: edgar at cs.uprm.edu (Edgar Acuna)
Date: Mon, 25 Oct 2004 20:11:41 -0400 (EDT)
Subject: [R] Feature selection
In-Reply-To: <002c01c4bac6$b196fd90$1c0b308f@artney>
Message-ID: <Pine.GSO.4.33.0410252008260.8990-100000@cs.uprm.edu>

Raj,
look at academic.uprm.edu/eacuna/softw.htm, I built a library of R
functions for data preprocessing tasks including feature selection
for supervised classification.
Please send me your comments.

Edgar

On Mon, 25 Oct 2004, Rajdeep Das wrote:

> Hello,
> I want to do feature selection for classification purpose (using lda). Can someone point me to any R package or S-plus package for this? Something like SFS or SFFS method would be useful for me.
> Thanks.
> Raj
> 	[[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>



From andy_liaw at merck.com  Tue Oct 26 04:09:50 2004
From: andy_liaw at merck.com (Liaw, Andy)
Date: Mon, 25 Oct 2004 22:09:50 -0400
Subject: [R] Plotting Bivariate Normal Data
Message-ID: <3A822319EB35174CA3714066D590DCD50994E222@usrymx25.merck.com>

> From: Berton Gunter
>  
> Just a little addendum to Martin's comments below. It is well 
> known that
> using LS centers and covariances for the M-distances is 
> generally not a good
> way to do this, as these statistics, themselves, are 
> distorted by the long
> "tails" (do > 1D distributions have "tails"?) 

How about `long skirt' or some such?  That seems descriptive, at least for
2D...

Andy

> so that the problems are
> hidden (see Brian Ripley's comments on the R-Help "robust 
> regression with
> groups" thread  from last week). Hence, one should use a 
> resistant center
> (the medioid, say) and a resistant covariance matrix (e.g., 
> from cov.rob())
> to compute the M-distances.
> 
> ... But then, this begs the question: Why do normality testing at all?
> (again, see BR's comments). Better to use robust/resistant statistical
> procedures for estimation from the beginning, though, 
> unfortunately, this
> shatters the nice simple mathematical framework for inference. 
> 
> -- Bert Gunter
> Genentech Non-Clinical Statistics
> South San Francisco, CA
>  
> "The business of the statistician is to catalyze the 
> scientific learning
> process."  - George E. P. Box
>  
>  
> 
> > Since one of the more severe and common deviations from
> > normality is "long tailed"ness (in all it's vaguety), we have
> > been recommending to QQ-plot mahalanobis distances against chi
> > squared quantiles - even before looking at the univariate
> > QQ plots.
> > 
> > Exactly for this reason, in R,
> > 	example(mahalanobis)
> > shows a version of how to do this!
> > 
> > Martin Maechler, ETH Zurich
> > 
> > ______________________________________________
> > R-help at stat.math.ethz.ch mailing list
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide! 
> > http://www.R-project.org/posting-guide.html
> >
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
> 
>



From vograno at evafunds.com  Tue Oct 26 04:29:00 2004
From: vograno at evafunds.com (Vadim Ogranovich)
Date: Mon, 25 Oct 2004 19:29:00 -0700
Subject: [R] computing distribution function online
Message-ID: <C698D707214E6F4AB39AB7096C3DE5A56D90C8@phost015.EVAFUNDS.intermedia.net>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20041025/3eb7ff04/attachment.pl

From ggrothendieck at myway.com  Tue Oct 26 05:30:42 2004
From: ggrothendieck at myway.com (Gabor Grothendieck)
Date: Tue, 26 Oct 2004 03:30:42 +0000 (UTC)
Subject: [R] Scoping and nls.
References: <200410252242.i9PMgXYM001348@erdos.math.unb.ca>
Message-ID: <loom.20041026T052651-337@post.gmane.org>

Rolf Turner <rolf <at> math.unb.ca> writes:

: 
: A colleague of mine is trying to use nls() to effect an optimization,
: and is encountering a scoping problem.  I should know how to solve it
: for him but .... well, I just don't.
: 
: I also had a quick scrounge of the archives --- I know I've seen this
: topic addressed before --- but I couldn't track it down.
: 
: So here's a toy example that demonstrates the problem:
: 
: hhh <- function(y,x) {
:         g <- function(a,b,x) {1/(1+a^2 + b^2*exp(x))}
:         nls(y~g(a,b,x),data=data.frame(x=x,y=y))
: }
: set.seed(123)
: x <- runif(50,1,10)
: y <- 1/(1+16 + 36*exp(x)) + rnorm(50,0,0.1)
: hhh(y,x)
: 
: which results in an error
: 
: 	Error in get(x, envir, mode, inherits) : variable "g" was not found
: 
: So can one assign the function ``g'' somewhere where it **can**
: be found?  I.e. use something like
: 
: 	assign("g",g,envir=??????)
: 
: Or is there some other magic incantation that can be used here
: to get nls to deal with a ``g'' defined inside the function
: which call nsl()?
: 
: In the real problem the function g is a complicated gadget, taking
: different forms in different parts of its domain, so it is
: inconvenient-to-impossible to specify it explicitly in the formula in
: the call to nls().
: 


You can assign g in the global environment.  Alternately, note from
?nls that functions in the formula are looked up in the environment
of the formula so just be sure to set up your formula so that the
its environment is the one inside hhh.   

Thus any of these should work:

1. add the statement assign("g", g, .GlobalEnv) after the definition of g.
   (This has the drawback of adding variable g to the your Global Environment
   possibly clobbering any g that was already there.)

2. replace the nls statement with:
	fo <- y ~ g(a,b,x)
	nls(fo,data=data.frame(x=x,y=y),start=c(a=0,b=0))

3. replace the nls statement with:
	nls(as.formula("y~g(a,b,x)"),data=data.frame(x=x,y=y),start=c(a=0,b=0))



From ggrothendieck at myway.com  Tue Oct 26 05:42:33 2004
From: ggrothendieck at myway.com (Gabor Grothendieck)
Date: Tue, 26 Oct 2004 03:42:33 +0000 (UTC)
Subject: [R] Ref: Variable scope or function behaviour or array reassign
References: <417D5436.4030105@statistik.uni-dortmund.de>
	<220.253.19.67.1098734867.99576@my.monash.edu.au>
Message-ID: <loom.20041026T053635-367@post.gmane.org>

Kunal Shetty <kshe4 <at> student.monash.edu> writes:

: 
: Uwe Ligges
:            thank you for u prompt reply
:         my problem was in step3
:         where my function returns two  different arrays.
: Yes i did try returning an object from the array.
: but the problem became...i tired returning the two arrays in a dataframe  
such as
: 
: newXY <- data.frame("newXmean"=newx, "newYmean" =newy)
: 
: but in the calling function or loop;  i wasn't able to access each 
individual array as an array; from the
: assigned object.
: 
:     Output <- algoResult(x,y,xNA,yNA,ctr); Output
: 
:      print(Output)   ?? 
:      print(Output[1])
:      print(Output$newy)
: 
: hence was trying to assign the global arrays within the function  itself 
without bothering to return any object...
: 
:    any help on this..     
: 
: regards
: Kunal

You need to be clearer in your questions.  Reduce your problem to a 
short reproduceable example that runs and then manually provide the 
exact output that you want it to produce but it is not producing.

Reproducable means that someone can copy it from your email, paste it 
into their R session and see the output as you do.   Read the posting 
guide for more info.



From ligges at statistik.uni-dortmund.de  Tue Oct 26 08:33:26 2004
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Tue, 26 Oct 2004 08:33:26 +0200
Subject: [R] Ref: Variable scope or function behaviour or array reassign
In-Reply-To: <220.253.19.67.1098734867.99576@my.monash.edu.au>
References: <417D5436.4030105@statistik.uni-dortmund.de>
	<220.253.19.67.1098734867.99576@my.monash.edu.au>
Message-ID: <417DEFB6.1040803@statistik.uni-dortmund.de>

Kunal Shetty wrote:
> Uwe Ligges
> 
>            thank you for u prompt reply
> 
>         my problem was in step3
> 
>         where my function returns two  different arrays.
> 
> Yes i did try returning an object from the array.
> 
> but the problem became...i tired returning the two arrays in a dataframe  such as
> 
> 
> 
> newXY <- data.frame("newXmean"=newx, "newYmean" =newy)
> 


Please don't use double spacing.


You want to return a list:

foo1 <- function(.....){
	.....
	return(list(newXmean = newx, newYmean = newy))
}

and call the resulting objects as follows:

foo2 <- function(.....){
	new <- foo1(.....)
	print(new$newXmean)
	print(new$newYmean)

}


Uwe Ligges



From ligges at statistik.uni-dortmund.de  Tue Oct 26 08:39:31 2004
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Tue, 26 Oct 2004 08:39:31 +0200
Subject: [R] building a package under windows
In-Reply-To: <200410252315.21009.tobias.sing@mpi-sb.mpg.de>
References: <200410251910.27536.gilles.guillot@inapg.inra.fr>
	<200410252315.21009.tobias.sing@mpi-sb.mpg.de>
Message-ID: <417DF123.9010109@statistik.uni-dortmund.de>

Tobias Sing wrote:

> As an alternative to building the package under Windows you might go for the 
> quick-and-dirty solution suggested by Peter Dalgaard a while ago on this 
> list: simply zip the installed Linux package. The package can be installed 
> under Windows using this zip file (worked fine for me with R 1.9.1).

But only under some restrictions! DLLs won't be build (i.e. it won't 
work for packages containing C or Fortran code) and not all help files 
will be generated (e.g. compiled html).

Uwe Ligges


> Here is the original posting:
> http://tolstoy.newcastle.edu.au/R/help/04/06/1555.html
> Good luck,
>   Tobias.
> 
> On Monday 25 October 2004 19:10, Gilles GUILLOT wrote:
> 
>>Hi,
>>
>>I have a package of my own which seems to work fine under linux.
>>I want to make a compiled version for windows.
>>(I work with windows 2000 and R 2.0
>>[...]
> 
> 
> 
> __________________________________________________________________________
> Tobias Sing                         phone: +49 681 9325 315   
> Max-Planck-Institut f??r Informatik  fax  : +49 681 9325 399   
> Stuhlsatzenhausweg 85               email: tobias.sing at mpi-sb.mpg.de  
> 66123 Saarbr??cken, Germany          web  : http://www.mpi-sb.mpg.de/~tsing
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html



From ligges at statistik.uni-dortmund.de  Tue Oct 26 08:44:30 2004
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Tue, 26 Oct 2004 08:44:30 +0200
Subject: [R] computing distribution function online
In-Reply-To: <C698D707214E6F4AB39AB7096C3DE5A56D90C8@phost015.EVAFUNDS.intermedia.net>
References: <C698D707214E6F4AB39AB7096C3DE5A56D90C8@phost015.EVAFUNDS.intermedia.net>
Message-ID: <417DF24E.3010207@statistik.uni-dortmund.de>

Vadim Ogranovich wrote:

> Hi,
>  
> I am looking for a means to compute empirical distribution function for
> a very large data set and evolution of that edf with time.
>  
> Here are some specifics. Each day I have an estimate of a distribution
> function and a new sample of about 1e4 points from the distribution in
> question. I want to update my estimate to include the new observations
> (with some aging coefficient to adapt to the changes of the df w/ time).
> Is there any R (or even non-R) code that can do this? Any relevant
> references will be appreciated as well.


Hmm. I don't know of general updating algorithms (there may be some, 
though!). For, e.g., a Bernoulli and Binomial distribution stuff seems 
to be easy, but for some other distributions things become complex, so 
it might be worthwhile to specify the class of distributions you are 
going to estimate in  ...

Uwe Ligges



> Thanks,
> Vadim
>  
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html



From r-help.20.stefan817 at spamgourmet.com  Tue Oct 26 08:51:29 2004
From: r-help.20.stefan817 at spamgourmet.com (r-help.20.stefan817@spamgourmet.com)
Date: Tue, 26 Oct 2004 08:51:29 +0200
Subject: [R] Importing big plain files from ERP-System/Data Mining with R
Message-ID: <004101c4bb28$397a5160$4d73a8c0@amdxp2000>

Hi,

how can I import really big plain text data files (several GB) from an
ERP-System (SAP-Tables) to R?
The Header of these files are always similar, for example:

Tabelle:        T009
Angezeigte Felder:  7 von  7  Feststehende F??hrungsspalten: 2  Listbreite
0250
----------------------------------------------------------------------
|X|MANDT|PERIV|XKALE|XJABH|ANZBP|ANZSP|LTEXT                         |
----------------------------------------------------------------------
|X|001  |01   |X    |     |012  |02   |ABC                           |
|X|001  |V9   |     |     |012  |04   |Okt. - Sep., 4 Sonderperioden |
|X|001  |WK   |     |X    |053  |00   |Kalenderwochen                |
----------------------------------------------------------------------

(including the first 5 rows in each downloaded table, row # 4 =field names,
length of 1 row > 1023 bytes, count of fields > 256, size = several GB,
count records = several million)

What is an appropriate way to read such tables in?

Greetings
Stefan

P.S. I am a beginner with R. Until now I have used ACL (http://www.acl.com)
for data mining purposes and I'm doing now my first try with R.
Yes, I have
[X] Read R Data Import/Export
[X] Read Using R for Data Analysis
[X] Read Simple R
[X] Read Manuals
[X] Read read.table() and scan() command



From ripley at stats.ox.ac.uk  Tue Oct 26 09:23:18 2004
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Tue, 26 Oct 2004 08:23:18 +0100 (BST)
Subject: [R] Importing big plain files from ERP-System/Data Mining with R
In-Reply-To: <004101c4bb28$397a5160$4d73a8c0@amdxp2000>
Message-ID: <Pine.LNX.4.44.0410260817530.14810-100000@gannet.stats>

On Tue, 26 Oct 2004 r-help.20.stefan817 at spamgourmet.com wrote:

> how can I import really big plain text data files (several GB) from an

Unlikely unless you have a 64-bit platform.

Only starting with R 2.0.0 can some 32-bit versions of R access files >
2Gb, and to import the file into R you need enough address space in R for
the object, which is normally more than the file size.

Almost certainly not if the unmentioned platform is Windows, but you could 
access the data from a DBMS.


> ERP-System (SAP-Tables) to R?
> The Header of these files are always similar, for example:
> 
> Tabelle:        T009
> Angezeigte Felder:  7 von  7  Feststehende F??hrungsspalten: 2  Listbreite
> 0250
> ----------------------------------------------------------------------
> |X|MANDT|PERIV|XKALE|XJABH|ANZBP|ANZSP|LTEXT                         |
> ----------------------------------------------------------------------
> |X|001  |01   |X    |     |012  |02   |ABC                           |
> |X|001  |V9   |     |     |012  |04   |Okt. - Sep., 4 Sonderperioden |
> |X|001  |WK   |     |X    |053  |00   |Kalenderwochen                |
> ----------------------------------------------------------------------
> 
> (including the first 5 rows in each downloaded table, row # 4 =field names,
> length of 1 row > 1023 bytes, count of fields > 256, size = several GB,
> count records = several million)
> 
> What is an appropriate way to read such tables in?
> 
> Greetings
> Stefan
> 
> P.S. I am a beginner with R. Until now I have used ACL (http://www.acl.com)
> for data mining purposes and I'm doing now my first try with R.
> Yes, I have
> [X] Read R Data Import/Export
> [X] Read Using R for Data Analysis
> [X] Read Simple R
> [X] Read Manuals
> [X] Read read.table() and scan() command

but you have not told us your platform.

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From tom_woody at swissinfo.org  Tue Oct 26 09:28:01 2004
From: tom_woody at swissinfo.org (=?ISO-8859-15?Q?Thomas_Sch=F6nhoff?=)
Date: Tue, 26 Oct 2004 09:28:01 +0200
Subject: [R] Revision: post on Intro to R lecture
In-Reply-To: <20041025181303.6836.qmail@webmail32.rediffmail.com>
References: <20041025181303.6836.qmail@webmail32.rediffmail.com>
Message-ID: <417DFC81.5050005@swissinfo.org>

Hi,


Arin Basu schrieb:

> This follows my earlier post on webized slides on lecture presentation on introducing R. I learned that in Mozilla (Firefox) browsers, the slides did not show up. Sorry for the no show. As a reluctant windows user, I kind of carelessly clicked through Powerpoint to convert the presentation file to its html form, unwittingly leading to the mess.
> 
> See if it got corrected now (I do not have firefox yet in my computer, so no way of knowing whether it works), and please let me know. Also, made some changes and reformatted the original slides to make them other browser compatible, thanks to comments from Gabor Grothendiek, Stuart Lesk, and Sundar Dorai-Raj.
> 
> Here is the URL again:
> 
> http://www.aloofhosting.com/arinbasu/Rintroweb.htm  

I'm agree to Clints posting, much improved and now readable.


Thomas



From jgu at codan.dk  Tue Oct 26 09:30:48 2004
From: jgu at codan.dk (Jim Gustafsson)
Date: Tue, 26 Oct 2004 09:30:48 +0200
Subject: [R] help!!!
Message-ID: <OFA808036E.17CE53F1-ONC1256F39.0028EE23@codan.dk>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20041026/b09f1477/attachment.pl

From danbebber at forestecology.co.uk  Tue Oct 26 10:11:32 2004
From: danbebber at forestecology.co.uk (Dan Bebber)
Date: Tue, 26 Oct 2004 09:11:32 +0100
Subject: [R] sample variogram construction
In-Reply-To: <1098725549.417d38ad2c28c@webmail.utoronto.ca>
Message-ID: <001b01c4bb33$6783e5c0$7d2501a3@plants.ox.ac.uk>

Matt,

In this case you are plotting the variogram of the residuals of the lme
object, not of the data themselves. In your model you are assuming a linear
relationship between count and time, with different intercepts and slopes
for your different individuals. You are also assuming that the residuals
exhibit stationarity. The Variogram function *should* work, so I can only
suggest that there is something wrong with your data or code. "Mixed-Effects
Models in S and S-Plus" by Pinheiro and Bates will probably help.

Dan

Department of Plant Sciences
University of Oxford
South Parks Road
Oxford OX1 3RB
UK
Tel. 01865 275000



> -----Original Message-----
> From: matt.kowgier at utoronto.ca [mailto:matt.kowgier at utoronto.ca] 
> Sent: 25 October 2004 18:32
> To: danbebber at forestecology.co.uk; r-help at stat.math.ethz.ch
> Subject: RE: [R] sample variogram construction
> 
> 
> Okay thanks!
> No I have the following difficulty implementing a variogram 
> in the nlme package:
> 
> > cd1 <- lme(count ~ time, data=cd4,random= ~ time | id) 
> > plot(Variogram(cd1, form= ~ time | id, robust=TRUE))
> Error in as.array(X) : attempt to set an attribute on NULL
> 
> Im not sure how to fix this, and apply this function to
> a longitudinal data with unequal times?
> 
> Any help would be appreciated
> 
> 
> Quoting Dan Bebber <danbebber at forestecology.co.uk>:
> 
> > Hi Matt,
> > 
> > there are several R packages that will compute the sample variogram 
> > for you. Check out GeoR, sgeostat, nlme, spatial. There's 
> no point in 
> > recoding the whole lot yourself, unless as a learning excercise.
> > 
> > D
> > 
> > p.s. For time series autocorrelations, you could use acf in package 
> > stats.
> > 
> > Message: 9
> > Date: Mon, 25 Oct 2004 02:02:06 -0400
> > From: matt.kowgier at utoronto.ca
> > Subject: [R] sample variogram construction
> > To: r-help at stat.math.ethz.ch
> > Message-ID: <1098684126.417c96de334e1 at webmail.utoronto.ca>
> > Content-Type: text/plain; charset=US-ASCII
> > 
> > Hi
> > 
> > Im attempting to build a sample variogram for 300 obersvations of 
> > longitudinal data. So what I need to do is compute the half squared 
> > differences  between pairs of residuals (for instance if a 
> subject has 
> > 4 obersvations, this is 4 choose 2 paird differences) for each 
> > subject. Also, then I need the corresponding time 
> differences within 
> > each individual. So the end result will be a 300 by 2 matrix with 
> > columns corresponding to paired difference residuals within subject 
> > and time differences within subject. Basically im having trouble 
> > coding this kind of matrix in R, if anyone can help me out 
> or give me 
> > some tips id appreciate it.
> > 
> > Thanks.
> > Stuck in the for loop
> > student
> > 
> > 
> 
>



From vito_ricci at yahoo.com  Tue Oct 26 10:23:39 2004
From: vito_ricci at yahoo.com (Vito Ricci)
Date: Tue, 26 Oct 2004 10:23:39 +0200 (CEST)
Subject: [R] Re: Importing big plain files from ERP-System/Data Mining with R
Message-ID: <20041026082339.86872.qmail@web41210.mail.yahoo.com>

Hi,
as concern R & datamining & large databases you can
see those resources:

Diego Kuonen, Introduction au data mining avec R :
vers la reconqu??te du `knowledge discovery in
databases' par les statisticiens. Bulletin of the
Swiss Statistical Society, 40:3-7, 2001.
http://www.statoo.com/en/publications/2001.R.SSS.40/

Diego Kuonen and Reinhard Furrer, Data mining avec R
dans un monde libre. Flash Informatique Sp??cial ??t??,
pages 45-50, sep 2001.
http://sawww.epfl.ch/SIC/SA/publications/FI01/fi-sp-1/sp-1-page45.html


Brian D. Ripley, Datamining: Large Databases and
Methods, in Proceedings  of "useR! 2004 - The R User
Conference", maggio 2004
http://www.ci.tuwien.ac.at/Conferences/useR-2004/Keynotes/Ripley.pdf

Brian D. Ripley, Using Databases with R, R News,
Gennaio 2001, pagg. 18-20
http://cran.r-project.org/doc/Rnews/Rnews_2001-1.pdf

B. D. Ripley, R. M. Ripley,  Applications of R Clients
and Servers in Proceedings of the Distributed
Statistical Computing 2001 Workshop, 2001, Vienna
University of Technology.
http://www.ci.tuwien.ac.at/Conferences/DSC-2001/Proceedings/Ripley.pdf

Torsten Hothorn, David A. James, Brian D. Ripley,  R/S
Interfaces to Databases  in Proceedings of the
Distributed Statistical Computing 2001 Workshop,
2001,Vienna University of Technology.
http://www.ci.tuwien.ac.at/Conferences/DSC-2001/Proceedings/HothornJamesRipley.pdf

Lu??s Torgo, Data Mining with R. Learning by case
studies, Maggio 2003
http://www.liacc.up.pt/~ltorgo/DataMiningWithR/

Best
Vito

You wrote:

Hi,

how can I import really big plain text data files
(several GB) from an
ERP-System (SAP-Tables) to R?
The Header of these files are always similar, for
example:

Tabelle:        T009
Angezeigte Felder:  7 von  7  Feststehende
F??hrungsspalten: 2  Listbreite
0250
----------------------------------------------------------------------
|X|MANDT|PERIV|XKALE|XJABH|ANZBP|ANZSP|LTEXT          
              |
----------------------------------------------------------------------
|X|001  |01   |X    |     |012  |02   |ABC            
              |
|X|001  |V9   |     |     |012  |04   |Okt. - Sep., 4
Sonderperioden |
|X|001  |WK   |     |X    |053  |00   |Kalenderwochen 
              |
----------------------------------------------------------------------

(including the first 5 rows in each downloaded table,
row # 4 =field names,
length of 1 row > 1023 bytes, count of fields > 256,
size = several GB,
count records = several million)

What is an appropriate way to read such tables in?

Greetings
Stefan

P.S. I am a beginner with R. Until now I have used ACL
(http://www.acl.com)
for data mining purposes and I'm doing now my first
try with R.
Yes, I have
[X] Read R Data Import/Export
[X] Read Using R for Data Analysis
[X] Read Simple R
[X] Read Manuals
[X] Read read.table() and scan() command

=====
Diventare costruttori di soluzioni

"The business of the statistician is to catalyze 
the scientific learning process."  
George E. P. Box


Visitate il portale http://www.modugno.it/
e in particolare la sezione su Palese http://www.modugno.it/archivio/cat_palese.shtml



From b.stollenwerk at gmx.de  Tue Oct 26 10:31:31 2004
From: b.stollenwerk at gmx.de (=?ISO-8859-1?Q?Bj=F6rn_Stollenwerk?=)
Date: Tue, 26 Oct 2004 10:31:31 +0200
Subject: [R] read.spss Error reading system-file header
Message-ID: <417E0B63.7020400@gmx.de>

Hi Jake,

I had and still have got the same problem. There were two columns in the 
data frame which contain just missing values. When I deleted those the 
impord seemed to work, but there is still a warning message.

Warning message:
C:\test.sav: Unrecognized record type 7, subtype 13 encountered in 
system file.
 >

Jake Wegelin wrote:

 > Is there any documentation on what kind of SPSS file can and cannot be
 > read by read.spss? Alternatively, how can one modify or "clean" an SPSS
 > file to make it readable by read.spss? What properties must a *.sav file
 > before read.spss can read it?

 > The file in this example is 270KB, with 5 rows and 173 columns. I 
have no
 > trouble reading larger files with read.spss, so it's not merely a size
 > problem. I have no difficulty opening the file in SPSS. I also have no
 > trouble getting read.spss to read a dummy SPSS file with 5 rows and 173
 > columns, where each entry was randomly sampled from c(letters, LETTERS).

 > /> library("foreign") /
 > /> junk<-read.spss("indata/z2EXvideo.sav") /
 > Error in read.spss("indata/z2EXvideo.sav") :
 >         Error reading system-file header.
 > In addition: Warning message:
 > indata/z2EXvideo.sav: File layout code has unexpected value 50331648. 
Value should be 2, in
 > big-endian or little-endian format.

 > Thanks for any information

 > Jake Wegelin

/> version /



From WeiQiang.Li at seagate.com  Tue Oct 26 10:50:37 2004
From: WeiQiang.Li at seagate.com (WeiQiang.Li@seagate.com)
Date: Tue, 26 Oct 2004 16:50:37 +0800
Subject: [R] How to calculate Adjusted SS 
Message-ID: <OFF489E33F.FC9A3918-ON48256F38.00351EC0-48256F39.003088BE@seagate.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20041026/92205965/attachment.pl

From ripley at stats.ox.ac.uk  Tue Oct 26 10:57:52 2004
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Tue, 26 Oct 2004 09:57:52 +0100 (BST)
Subject: [R] help!!!
In-Reply-To: <OFA808036E.17CE53F1-ONC1256F39.0028EE23@codan.dk>
Message-ID: <Pine.LNX.4.44.0410260953280.18650-100000@gannet.stats>

On Tue, 26 Oct 2004, Jim Gustafsson wrote:

> I will compare some results in a dotplot.
> 
> The picture is very dark(grey) in the background, could I get it ligther?

library(lattice) # is needed
?trellis.device  # and see themes.

However, if you get a very dark grey, your computer system is set up
incorrectly, and you need to explore the `gamma' option (if implemented)
to the graphics device you are using to get a mid grey (or get your 
monitor calibrated).

> R-code
> dotplot(LINES~VAL, 
> groups=QQ,key=simpleKey(levels(QQ1),space="top"),xlab="Values",main="Tail-Measure" 
> )
> 
> (where LINES,VAL, QQ is vectors)


-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From nleonard at tartarus.uwa.edu.au  Tue Oct 26 11:08:18 2004
From: nleonard at tartarus.uwa.edu.au (Neil Leonard)
Date: Tue, 26 Oct 2004 17:08:18 +0800
Subject: [R] History command
Message-ID: <93B9D30E-272E-11D9-8DC2-003065D5B8EC@tartarus.uwa.edu.au>

I was wondering whether there is a command in R similar to the '!' 
command in unix.

Thanks
Neil



From ales.ziberna at guest.arnes.si  Tue Oct 26 11:21:41 2004
From: ales.ziberna at guest.arnes.si (=?iso-8859-2?Q?Ale=B9_=AEiberna?=)
Date: Tue, 26 Oct 2004 11:21:41 +0200
Subject: [R] putting legend outside ploting region
Message-ID: <004001c4bb3d$435e01d0$0609f9c2@ales>

Hello!

I have created a plot and would like to put a legend in the top left corner
(above and left of plotting region), but non of the functions can plot there
(I tried 'legend', 'text', 'points', 'lines', 'mtext', 'axes',...).

I would be very grateful if someone could help me put something in that
corrner, with any fuction.

Thanks,
Ales Ziberna



From kshe4 at student.monash.edu  Tue Oct 26 11:29:03 2004
From: kshe4 at student.monash.edu (Kunal Shetty)
Date: Tue, 26 Oct 2004 09:29:03 +0000
Subject: [R] Ref: Variable scope or function behaviour or array reassign
References: <417DEFB6.1040803@statistik.uni-dortmund.de>
Message-ID: <172.19.227.91.1098782906.43196@my.monash.edu.au>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20041026/560196bd/attachment.pl

From ligges at statistik.uni-dortmund.de  Tue Oct 26 11:43:02 2004
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Tue, 26 Oct 2004 11:43:02 +0200
Subject: [R] putting legend outside ploting region
In-Reply-To: <004001c4bb3d$435e01d0$0609f9c2@ales>
References: <004001c4bb3d$435e01d0$0609f9c2@ales>
Message-ID: <417E1C26.9080607@statistik.uni-dortmund.de>

Ales Ziberna wrote:

> Hello!
> 
> I have created a plot and would like to put a legend in the top left corner
> (above and left of plotting region), but non of the functions can plot there
> (I tried 'legend', 'text', 'points', 'lines', 'mtext', 'axes',...).
> 
> I would be very grateful if someone could help me put something in that
> corrner, with any fuction.
> 
> Thanks,
> Ales Ziberna
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html

See ?par, in particular its argument "xpd":

Form ?par:

xpd
A logical value or NA. If FALSE, all plotting is clipped to the plot 
region, if TRUE, all plotting is clipped to the figure region, and if 
NA, all plotting is clipped to the device region.

Uwe Ligges



From gilles.guillot at inapg.inra.fr  Tue Oct 26 11:37:39 2004
From: gilles.guillot at inapg.inra.fr (Gilles GUILLOT)
Date: Tue, 26 Oct 2004 11:37:39 +0200
Subject: [R] building a package under windows
Message-ID: <200410261136.43460.gilles.guillot@inapg.inra.fr>


After checking paths, it works better.
But I still have trouble, see  line -5 below : 

C:\Documents and Settings\guillot\Mes documents\package>R CMD build geneland
* checking for file 'geneland/DESCRIPTION' ... OK
* preparing 'geneland':
* cleaning src
* removing junk files
* building 'geneland_1.0.tar.gz'


C:\Documents and Settings\guillot\Mes documents\package>
C:\Documents and Settings\guillot\Mes documents\package>
C:\Documents and Settings\guillot\Mes documents\package>R CMD check geneland
* checking for working latex ...latex: OK
* using log directory 'C:/Documents and Settings/guillot/Mes 
documents/package/g
eneland.Rcheck'
* checking for file 'geneland/DESCRIPTION' ... OK
* checking if this is a source package ... OK

installing R.css in C:/DOCUME~1/guillot/MESDOC~1/package/GENELA~1.RCH


---------- Making package geneland ------------
  adding build stamp to DESCRIPTION
  making DLL ...
g77 -O2 -Wall   -c Fstat.f -o Fstat.o
g77 -O2 -Wall   -c MapPosterior.f -o MapPosterior.o
MapPosterior.f: In subroutine `mapposterior':
g77 -O2 -Wall   -c algama.f -o algama.o
g77 -O2 -Wall   -c main.f -o main.o
main.f: In subroutine `mcmc':
g77 -O2 -Wall   -c randlib-1.3.f -o randlib-1.3.o
randlib-1.3.f: In function `ignpoi':
g77 -O2 -Wall   -c sub.f -o sub.o
sub.f: In subroutine `rprioru':
sub.f: In subroutine `upddrift':
sub.f: In subroutine `upddrift2':
sub.f: In subroutine `vormove':
sub.f: In subroutine `updc':
sub.f: In subroutine `updurw':
sub.f: In subroutine `updt':
sub.f: In subroutine `bdpp':
sub.f: In subroutine `bdclass4':
sub.f: In subroutine `bdclass5':
sub.f: In subroutine `bdclass5bis':
sub.f: In subroutine `sample':
sub.f: In subroutine `bdclass6':
sub.f: In subroutine `addfreq6':
sub.f: In subroutine `bdclass7':
sub.f: In subroutine `bdclass7bis':
sub.f: In subroutine `addfreq7':
sub.f: In subroutine `addfreq7bis':
sub.f: In subroutine `remfreq7':
sub.f: In subroutine `remfreq7bis':
ar cr geneland.a Fstat.o MapPosterior.o algama.o main.o randlib-1.3.o sub.o
ranlib geneland.a
windres --include-dir c:/R/rw2000/include  -i geneland_res.rc -o 
geneland_res.o
gcc  --shared -s  -o geneland.dll geneland.def geneland.a geneland_res.o  
-Lc:/R
/rw2000/src/gnuwin32  -lg2c -lR
  ... DLL made
  installing DLL
  installing R files
  installing man source files
  installing indices
  installing help
 >>> Building/Updating help pages for package 'geneland'
     Formats: text html latex example chm
  Fstat                             text    html    latex   example
  MapPosterior                      text    html    latex   example
  PlotDrift                         text    html    latex   example
  PlotFreq                          text    html    latex   example
  PlotFreqA                         text    html    latex   example
  PlotTessellation                  text    html    latex   example
  Plotnclass                        text    html    latex   example
  Plotntile                         text    html    latex   example
  PosteriorMode                     text    html    latex   example
  mcmcFmodel                        text    html    latex   example
  rdiscr                            text    html    latex   example
  setplot                           text    html    latex   example
  simFmodel                         text    html    latex   example
make: *** No rule to make target `and'.  Stop.
*** Installation of geneland failed ***

Removing 'C:/DOCUME~1/guillot/MESDOC~1/package/GENELA~1.RCH/geneland'
 ERROR
Installation failed.



From r-help.20.stefan817 at spamgourmet.com  Tue Oct 26 14:11:31 2004
From: r-help.20.stefan817 at spamgourmet.com (r-help.20.stefan817@spamgourmet.com)
Date: Tue, 26 Oct 2004 12:11:31 GMT
Subject: [R] Importing big plain files from ERP-System/Data Mining with R
Message-ID: <200410261211.i9QCBVh19018@www5.newcon.de>


On Tue, 26 Oct 2004 r-help.20.stefan817 at spamgourmet.com wrote:

>> how can I import really big plain text data files (several GB) from an

>Unlikely unless you have a 64-bit platform.

Why? I have a 32-bit Win XP Platform running R 2.0.0. With ACL 8.21 e.g. 10 GB were no problem.

>Only starting with R 2.0.0 can some 32-bit versions of R access files >
>2Gb, and to import the file into R you need enough address space in R for
>the object, which is normally more than the file size.

Is this really so? I want to summarize the data or calculate clusters, so only the aggregated information should be in memory. Does R first import the whole file and then calculate with it? In ACL the concept is to leave the file itself on the harddisk, scanning it for each calculation and doing only the calculation in memory. (Surely not very fast, but probably the only method for big files)

>Almost certainly not if the unmentioned platform is Windows, but you could 
>access the data from a DBMS.

I can do this also, but with several limitations.

>> ERP-System (SAP-Tables) to R?
>> The Header of these files are always similar, for example:
>> 
>> Tabelle:        T009
>> Angezeigte Felder:  7 von  7  Feststehende F??hrungsspalten: 2  Listbreite
>> 0250
>> ----------------------------------------------------------------------
>> |X|MANDT|PERIV|XKALE|XJABH|ANZBP|ANZSP|LTEXT                         |
>> ----------------------------------------------------------------------
>> |X|001  |01   |X    |     |012  |02   |ABC                           |
>> |X|001  |V9   |     |     |012  |04   |Okt. - Sep., 4 Sonderperioden |
>> |X|001  |WK   |     |X    |053  |00   |Kalenderwochen                |
>> ----------------------------------------------------------------------
>> 
>> (including the first 5 rows in each downloaded table, row # 4 =field >names,
>> length of 1 row > 1023 bytes, count of fields > 256, size = several GB,
>> count records = several million)
>> 
>> What is an appropriate way to read such tables in?

Greetings
Stefan



From ripley at stats.ox.ac.uk  Tue Oct 26 12:09:17 2004
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Tue, 26 Oct 2004 11:09:17 +0100 (BST)
Subject: [R] History command
In-Reply-To: <93B9D30E-272E-11D9-8DC2-003065D5B8EC@tartarus.uwa.edu.au>
Message-ID: <Pine.LNX.4.44.0410261104080.18712-100000@gannet.stats>

On Tue, 26 Oct 2004, Neil Leonard wrote:

> I was wondering whether there is a command in R similar to the '!' 
> command in unix.

What do you think the `! command in Unix' does?  Your subject line
suggests you are thinking of the history substitution mechanism in a csh
_shell_ (copied for bash).  If that is what you mean, and you are using
Unix with readline enabled, see `man readline' on your system for what is
available (which can often be used to similar effect).  Or (to save the 
inevitable supplementary comment) use ESS if you use Emacs.

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From p.dalgaard at biostat.ku.dk  Tue Oct 26 12:21:58 2004
From: p.dalgaard at biostat.ku.dk (Peter Dalgaard)
Date: 26 Oct 2004 12:21:58 +0200
Subject: [R] building a package under windows
In-Reply-To: <200410261136.43460.gilles.guillot@inapg.inra.fr>
References: <200410261136.43460.gilles.guillot@inapg.inra.fr>
Message-ID: <x2ekjlkce1.fsf@biostat.ku.dk>

Gilles GUILLOT <gilles.guillot at inapg.inra.fr> writes:

> After checking paths, it works better.
> But I still have trouble, see  line -5 below : 
> 
> C:\Documents and Settings\guillot\Mes documents\package>R CMD build geneland
> * checking for file 'geneland/DESCRIPTION' ... OK
> * preparing 'geneland':
> * cleaning src
> * removing junk files
> * building 'geneland_1.0.tar.gz'
....
>   setplot                           text    html    latex   example
>   simFmodel                         text    html    latex   example
> make: *** No rule to make target `and'.  Stop.
> *** Installation of geneland failed ***
> 
> Removing 'C:/DOCUME~1/guillot/MESDOC~1/package/GENELA~1.RCH/geneland'
>  ERROR
> Installation failed.

I think the "and" comes from your "Documents and Settings". The
package building instructions probably have something to say about
building in directories with spaces in the name. You might try

cd C:/DOCUME~1/guillot/MESDOC~1/package/GENELA~1.RCH/geneland 

and the rebuild.


-- 
   O__  ---- Peter Dalgaard             Blegdamsvej 3  
  c/ /'_ --- Dept. of Biostatistics     2200 Cph. N   
 (*) \(*) -- University of Copenhagen   Denmark      Ph: (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)             FAX: (+45) 35327907



From rksh at soc.soton.ac.uk  Tue Oct 26 12:28:08 2004
From: rksh at soc.soton.ac.uk (Robin Hankin)
Date: Tue, 26 Oct 2004 11:28:08 +0100
Subject: [R] persp(), scatterplot3d(), "..." argument
Message-ID: <a0600200ebda3cb456654@[139.166.242.29]>


Hello list.

I very often need 3d scatterplots, and use scatterplot3D quite a lot.
I am trying to modify persp() to plot scatterplots, and make use of
the theta and phi arguments that persp() offers.  I am having some
difficulty passing the correct arguments to persp().

Here is my function so far.  Much of it is copied from the persp() manpage.



points3d <- function(x,y,z, jj.colour="black", ...){
   if(is.matrix(x)){
     z <- x[,3]
     y <- x[,2]
     x <- x[,1]
   }
     z.grid <- matrix(range(z),2,2)
     persp(range(x), range(y),	z.grid,
     col = NA,border=NA, ...) -> res


     trans3d <- function(x,y,z, pmat) {
        tr <- cbind(x,y,z,1) %*% pmat
        list(x = tr[,1]/tr[,4], y= tr[,2]/tr[,4])
      }

   points(trans3d(x,y,z,pm=res), col=jj.colour, ...)
}



With this, things like

    O <- matrix(rnorm(60),20,3)
    points3d(O,jj.colour="red",pch=16,theta=30,phi=40)

work as expected, but all extra arguments are passed to persp() _and_
lines() and this gives warnings.


QUESTION:

What is best practice to handle this sort of problem?  Should I ignore
the warnings() that this approach gives?  I can suppress them with
options(warn= -Inf), but is this a good idea?  I've read section 10.4
of AITR.


(the odd argument jj.colour is there because persp() needs to
be called with col=NA).



-- 
Robin Hankin
Uncertainty Analyst
Southampton Oceanography Centre
SO14 3ZH
tel +44(0)23-8059-7743
initialDOTsurname at soc.soton.ac.uk (edit in obvious way; spam precaution)



From andy_liaw at merck.com  Tue Oct 26 12:33:10 2004
From: andy_liaw at merck.com (Liaw, Andy)
Date: Tue, 26 Oct 2004 06:33:10 -0400
Subject: [R] building a package under windows
Message-ID: <3A822319EB35174CA3714066D590DCD50994E225@usrymx25.merck.com>

I suspect you won't have the problem if you build in a directory (aka
folder) whose path does not contain spaces...  (That is mentioned in
README.package, I believe.)

Andy

> From: Gilles GUILLOT
> 
> After checking paths, it works better.
> But I still have trouble, see  line -5 below : 
> 
> C:\Documents and Settings\guillot\Mes documents\package>R CMD 
> build geneland
> * checking for file 'geneland/DESCRIPTION' ... OK
> * preparing 'geneland':
> * cleaning src
> * removing junk files
> * building 'geneland_1.0.tar.gz'
> 
> 
> C:\Documents and Settings\guillot\Mes documents\package>
> C:\Documents and Settings\guillot\Mes documents\package>
> C:\Documents and Settings\guillot\Mes documents\package>R CMD 
> check geneland
> * checking for working latex ...latex: OK
> * using log directory 'C:/Documents and Settings/guillot/Mes 
> documents/package/g
> eneland.Rcheck'
> * checking for file 'geneland/DESCRIPTION' ... OK
> * checking if this is a source package ... OK
> 
> installing R.css in C:/DOCUME~1/guillot/MESDOC~1/package/GENELA~1.RCH
> 
> 
> ---------- Making package geneland ------------
>   adding build stamp to DESCRIPTION
>   making DLL ...
> g77 -O2 -Wall   -c Fstat.f -o Fstat.o
> g77 -O2 -Wall   -c MapPosterior.f -o MapPosterior.o
> MapPosterior.f: In subroutine `mapposterior':
> g77 -O2 -Wall   -c algama.f -o algama.o
> g77 -O2 -Wall   -c main.f -o main.o
> main.f: In subroutine `mcmc':
> g77 -O2 -Wall   -c randlib-1.3.f -o randlib-1.3.o
> randlib-1.3.f: In function `ignpoi':
> g77 -O2 -Wall   -c sub.f -o sub.o
> sub.f: In subroutine `rprioru':
> sub.f: In subroutine `upddrift':
> sub.f: In subroutine `upddrift2':
> sub.f: In subroutine `vormove':
> sub.f: In subroutine `updc':
> sub.f: In subroutine `updurw':
> sub.f: In subroutine `updt':
> sub.f: In subroutine `bdpp':
> sub.f: In subroutine `bdclass4':
> sub.f: In subroutine `bdclass5':
> sub.f: In subroutine `bdclass5bis':
> sub.f: In subroutine `sample':
> sub.f: In subroutine `bdclass6':
> sub.f: In subroutine `addfreq6':
> sub.f: In subroutine `bdclass7':
> sub.f: In subroutine `bdclass7bis':
> sub.f: In subroutine `addfreq7':
> sub.f: In subroutine `addfreq7bis':
> sub.f: In subroutine `remfreq7':
> sub.f: In subroutine `remfreq7bis':
> ar cr geneland.a Fstat.o MapPosterior.o algama.o main.o 
> randlib-1.3.o sub.o
> ranlib geneland.a
> windres --include-dir c:/R/rw2000/include  -i geneland_res.rc -o 
> geneland_res.o
> gcc  --shared -s  -o geneland.dll geneland.def geneland.a 
> geneland_res.o  
> -Lc:/R
> /rw2000/src/gnuwin32  -lg2c -lR
>   ... DLL made
>   installing DLL
>   installing R files
>   installing man source files
>   installing indices
>   installing help
>  >>> Building/Updating help pages for package 'geneland'
>      Formats: text html latex example chm
>   Fstat                             text    html    latex   example
>   MapPosterior                      text    html    latex   example
>   PlotDrift                         text    html    latex   example
>   PlotFreq                          text    html    latex   example
>   PlotFreqA                         text    html    latex   example
>   PlotTessellation                  text    html    latex   example
>   Plotnclass                        text    html    latex   example
>   Plotntile                         text    html    latex   example
>   PosteriorMode                     text    html    latex   example
>   mcmcFmodel                        text    html    latex   example
>   rdiscr                            text    html    latex   example
>   setplot                           text    html    latex   example
>   simFmodel                         text    html    latex   example
> make: *** No rule to make target `and'.  Stop.
> *** Installation of geneland failed ***
> 
> Removing 'C:/DOCUME~1/guillot/MESDOC~1/package/GENELA~1.RCH/geneland'
>  ERROR
> Installation failed.
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
> 
>



From ales.ziberna at guest.arnes.si  Tue Oct 26 12:32:47 2004
From: ales.ziberna at guest.arnes.si (=?iso-8859-2?Q?Ale=B9_=AEiberna?=)
Date: Tue, 26 Oct 2004 12:32:47 +0200
Subject: [R] putting legend outside ploting region
References: <004001c4bb3d$435e01d0$0609f9c2@ales>
	<417E1C26.9080607@statistik.uni-dortmund.de>
Message-ID: <006101c4bb47$41e14150$0609f9c2@ales>

Thank you for your answer!

I'm sorry that bother you with a question I could have found the answer to 
in the help pages, but I have missed it.

Ales

----- Original Message ----- 
From: "Uwe Ligges" <ligges at statistik.uni-dortmund.de>
To: "Ales Ziberna" <ales.ziberna at guest.arnes.si>
Cc: <r-help at stat.math.ethz.ch>
Sent: Tuesday, October 26, 2004 11:43 AM
Subject: Re: [R] putting legend outside ploting region


Ales Ziberna wrote:

> Hello!
>
> I have created a plot and would like to put a legend in the top left 
> corner
> (above and left of plotting region), but non of the functions can plot 
> there
> (I tried 'legend', 'text', 'points', 'lines', 'mtext', 'axes',...).
>
> I would be very grateful if someone could help me put something in that
> corrner, with any fuction.
>
> Thanks,
> Ales Ziberna
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html

See ?par, in particular its argument "xpd":

Form ?par:

xpd
A logical value or NA. If FALSE, all plotting is clipped to the plot
region, if TRUE, all plotting is clipped to the figure region, and if
NA, all plotting is clipped to the device region.

Uwe Ligges



From garbade at mip.paed.uni-muenchen.de  Tue Oct 26 12:37:22 2004
From: garbade at mip.paed.uni-muenchen.de (garbade@mip.paed.uni-muenchen.de)
Date: Tue, 26 Oct 2004 12:37:22 +0200 (CEST)
Subject: [R] box() and hist()
In-Reply-To: <x2breqr92a.fsf@biostat.ku.dk>
References: <50486.129.206.90.2.1098723762.squirrel@www.paed.uni-muenchen.de>
	<x2breqr92a.fsf@biostat.ku.dk>
Message-ID: <24996.129.206.90.2.1098787042.squirrel@www.paed.uni-muenchen.de>

> garbade at mip.paed.uni-muenchen.de writes:
>
>> Hi,
>> does anybody know why the following is not working:
>>
>> > hist(rnorm(200))
>> > box(bty="o")
>>
>> gives me a box without rounded corners.
>
> Because that isn't what it is supposed to do. Did you expect "7" to
> give you a slanting right edge? And which letter should represent a
> full rectangle?

Thought "o" means rectangle with rounded corners... Thanks, Sven



From ripley at stats.ox.ac.uk  Tue Oct 26 13:02:37 2004
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Tue, 26 Oct 2004 12:02:37 +0100 (BST)
Subject: [R] Importing big plain files from ERP-System/Data Mining with R
In-Reply-To: <200410261211.i9QCBVh19018@www5.newcon.de>
Message-ID: <Pine.LNX.4.44.0410261158310.1008-100000@gannet.stats>

On Tue, 26 Oct 2004 r-help.20.stefan817 at spamgourmet.com wrote:

> 
> On Tue, 26 Oct 2004 r-help.20.stefan817 at spamgourmet.com wrote:
> 
> >> how can I import really big plain text data files (several GB) from an
> 
> >Unlikely unless you have a 64-bit platform.
> 
> Why? I have a 32-bit Win XP Platform running R 2.0.0. With ACL 8.21 e.g.
> 10 GB were no problem.

For the two reasons stated in the next para!

> >Only starting with R 2.0.0 can some 32-bit versions of R access files >
> >2Gb, and to import the file into R you need enough address space in R for
> >the object, which is normally more than the file size.
> 
> Is this really so? I want to summarize the data or calculate clusters,
> so only the aggregated information should be in memory. Does R first
> import the whole file and then calculate with it? In ACL the concept is
> to leave the file itself on the harddisk, scanning it for each
> calculation and doing only the calculation in memory. (Surely not very
> fast, but probably the only method for big files)

That's the definition of `import', which is what you actually asked.
You didn't ask if R can do what ACL does, which of course it can.

> >Almost certainly not if the unmentioned platform is Windows, but you could 
> >access the data from a DBMS.
> 
> I can do this also, but with several limitations.

What limitations?

PLEASE do read carefully the posting guide, as well as the replies.
       ^^^^^^^

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From f.gherardini at pigrecodata.net  Tue Oct 26 15:46:08 2004
From: f.gherardini at pigrecodata.net (Federico Gherardini)
Date: Tue, 26 Oct 2004 15:46:08 +0200
Subject: [R] Combining columns of different length
Message-ID: <417E5520.1050304@pigrecodata.net>

Hi all,

Simple and direct question....
Is it possible to add a shorter column to a data frame or matrix in such 
a way that the missing values are replaced with NAs?
For example suppose I have

3   2
4   2
5   8

and I want to add a column

3
3

to get...

3   2   3
4   2   3
5   8   NA

Thanks

Federico



From ym at climpact.com  Tue Oct 26 14:27:37 2004
From: ym at climpact.com (Yves Magliulo)
Date: 26 Oct 2004 14:27:37 +0200
Subject: [R] Combining columns of different length
In-Reply-To: <417E5520.1050304@pigrecodata.net>
References: <417E5520.1050304@pigrecodata.net>
Message-ID: <1098793657.3863.42.camel@new-york.climpact.net>

hi,

something like :
toto is your data.frame
toto=
V1  V2	
3   2
4   2
5   8

and tata=
3
3

so to do what you want make 
cbind(toto,c(tata,rep(NA,len=(length(toto$V1)-length(tata)))))

maybe there is a easier way but it's work!

-- 
------
Yves Magliulo <ym at climpact.com>
R&D Engineer, CLIMPACT

Tel.   : +33 (0) 1 44 27 34 31
Fax.   : +33 (0) 1 44 27 49 96 
Universite Pierre et Marie Curie
Boite 101 - Tour 45 - 5eme etage - Couloir 45/46
4 place Jussieu, 75252 Paris CEDEX 05, France

Le mar 26/10/2004 ?? 15:46, Federico Gherardini a ??crit :
> Hi all,
> 
> Simple and direct question....
> Is it possible to add a shorter column to a data frame or matrix in such 
> a way that the missing values are replaced with NAs?
> For example suppose I have
> 
> 3   2
> 4   2
> 5   8
> 
> and I want to add a column
> 
> 3
> 3
> 
> to get...
> 
> 3   2   3
> 4   2   3
> 5   8   NA
> 
> Thanks
> 
> Federico
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html



From vito_ricci at yahoo.com  Tue Oct 26 14:33:07 2004
From: vito_ricci at yahoo.com (Vito Ricci)
Date: Tue, 26 Oct 2004 14:33:07 +0200 (CEST)
Subject: [R] Combining columns of different length
Message-ID: <20041026123307.97065.qmail@web41204.mail.yahoo.com>

Hi,

you can use this simple function:

add.col<-function(df, new.col) {n.row<-dim(df)[1]
           length(new.col)<-n.row
           cbind(df, new.col)
 

                        }

see this example:

> x<-cbind(c(1,2,3),c(4,5,6))
> x
     [,1] [,2]
[1,]    1    4
[2,]    2    5
[3,]    3    6
> y<-c(7,8)
> y
[1] 7 8
> add.col<-function(df, new.col) {n.row<-dim(df)[1]
+           length(new.col)<-n.row
+           cbind(df, new.col)
+ 
+                        }
> 
> new.df<-add.col(x,y)
> new.df
         new.col
[1,] 1 4       7
[2,] 2 5       8
[3,] 3 6      NA

I hope I help a little.
Best
Vito

you wrote:

Hi all,

Simple and direct question....
Is it possible to add a shorter column to a data frame
or matrix in such 
a way that the missing values are replaced with NAs?
For example suppose I have

3   2
4   2
5   8

and I want to add a column

3
3

to get...

3   2   3
4   2   3
5   8   NA

Thanks

Federico



=====
Diventare costruttori di soluzioni

"The business of the statistician is to catalyze 
the scientific learning process."  
George E. P. Box


Visitate il portale http://www.modugno.it/
e in particolare la sezione su Palese http://www.modugno.it/archivio/cat_palese.shtml



From rolf at math.unb.ca  Tue Oct 26 14:35:11 2004
From: rolf at math.unb.ca (Rolf Turner)
Date: Tue, 26 Oct 2004 09:35:11 -0300 (ADT)
Subject: [R] Combining columns of different length
Message-ID: <200410261235.i9QCZBkw025441@erdos.math.unb.ca>

In R all things are possible:

rcb <- function (...) {
# rcb <--> ``ragged cbind''
xxx <- list(...)
n <- max(unlist(lapply(xxx,function(x){ifelse(is.matrix(x),nrow(x),
                                              length(x))})))
yyy <- lapply(xxx,function(x,n){if(is.matrix(x))
                        rbind(x,matrix(NA,ncol=ncol(x),nrow=n))
                        else c(x,rep(NA,n-length(x)))},n=n)
do.call("cbind",yyy)
}


				cheers,

					Rolf Turner
					rolf at math.unb.ca

Federico Gherardini wrote:

> Simple and direct question....  Is it possible to add a shorter
> column to a data frame or matrix in such a way that the missing
> values are replaced with NAs?  For example suppose I have
> 
> 3   2
> 4   2
> 5   8
> 
> and I want to add a column
> 
> 3
> 3
> 
> to get...
> 
> 3   2   3
> 4   2   3
> 5   8   NA
> 
> Thanks
> 
> Federico



From ajayshah at mayin.org  Tue Oct 26 05:14:59 2004
From: ajayshah at mayin.org (Ajay Shah)
Date: Tue, 26 Oct 2004 08:44:59 +0530
Subject: [R] Installing packages on Debian linux
Message-ID: <7FFEE688B57D7346BC6241C55900E7300FCF2B@pollux.bfro.uni-lj.si>

I run Debian, and I do a mix of two paths: install using apt-get and
install using install.packages() under R. Both work and it's safe to
use both together. However, as has been pointed out, you tend to get
dumped with multiple copies of some packages since the two channels
lead to two different directories (/usr/local/lib/R/site-library for
install.packages() and /usr/lib/R/site-library for Debian). I haven't
yet figured out whether R will automatically use the more-recent
version if there are two versions of a library that are installed.

The Debian path is superior since it involves automatic handling of
dependencies. But I believe that's changed significantly (for the
better) in R 2.0,which hasn't yet made it into Debian::testing.

I use a generic /etc/apt/sources.list with testing. Specifically, it
reads:
  deb http://http.us.debian.org/debian testing main contrib non-free
  deb http://non-us.debian.org/debian-non-US testing/non-US main contrib non-free

So you don't need a line for cran.r-project.org in order to get the
stuff in testing. As you may know, Debian's policies involve a
two-week waiting period, and a lack of critical bugs reported, for a
package to make it into testing. So there's a certain innate lag in
what you see in testing. If you're a user of testing, it's because you
like that tradeoff between timelag and stability.

The Debian packages get updated automatically when one does 
  # apt-get update && apt-get dist-upgrade
from time to time. The stuff you get using install.packages()
doesn't. A few weeks ago I had asked Dirk how one might automate the
removal of packages that I put in, using install.packages(), which
later came out as Debian packages. I also asked how one might
automatically do an install.packages() on the stuff that is installed
on my machine but isn't yet out as a Debian package. He said this is a
bit harder than meets the eye. I haven't gone into mucking with this
yet...


On an unrelated note, here's the shell script that I use to "fetch me
everything connected with R" :

# ---------------------- snip snip snip
#!/bin/sh
apt-cache search '^r-' \
	| egrep -v -i '(biology|genetic|map|odbc|sql)' \
	| awk '/^r\-/ {print $1}' \
	| xargs apt-get install --yes
# ---------------------- snip snip snip

In order to run it, I say:

   # apt-get update && takeallR.sh

-- 
Ajay Shah                                                   Consultant
ajayshah at mayin.org                      Department of Economic Affairs
http://www.mayin.org/ajayshah           Ministry of Finance, New Delhi



From petr.pikal at precheza.cz  Tue Oct 26 14:26:22 2004
From: petr.pikal at precheza.cz (Petr Pikal)
Date: Tue, 26 Oct 2004 14:26:22 +0200
Subject: [R] Combining columns of different length
In-Reply-To: <417E5520.1050304@pigrecodata.net>
Message-ID: <417E5E8E.28214.1214B92@localhost>



On 26 Oct 2004 at 15:46, Federico Gherardini wrote:

> Hi all,
> 
> Simple and direct question....
> Is it possible to add a shorter column to a data frame or matrix in
> such a way that the missing values are replaced with NAs? For example
> suppose I have
> 
> 3   2
> 4   2
> 5   8
> 
> and I want to add a column
> 
> 3
> 3
> 
> to get...
> 
> 3   2   3
> 4   2   3
> 5   8   NA

Hi

Maybe there is another approach but this shall work

no.r <- nrow(df)
l.v <- length(your.short.vector)
difer <- no.r-l.v
cbind(df,c(your.short.vector,rep(NA,difer)))

Cheers
Petr

> 
> Thanks
> 
> Federico
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide!
> http://www.R-project.org/posting-guide.html

Petr Pikal
petr.pikal at precheza.cz



From andy_liaw at merck.com  Tue Oct 26 15:19:45 2004
From: andy_liaw at merck.com (Liaw, Andy)
Date: Tue, 26 Oct 2004 09:19:45 -0400
Subject: [R] How to calculate Adjusted SS
Message-ID: <3A822319EB35174CA3714066D590DCD50994E22A@usrymx25.merck.com>

Seems like noone has responded (at least publicly), so I'll give it a stab.

I'm not exactly sure what you mean by `adjusted SS'.  (I believe it was Bill
Venables who said something like `There's only one type of SS.')  Prof.
Fox's `car' package has the Anova() (note the capital `A') function that
computes different `types' of SS.  However, if you want to estimate variance
components, lme() in either the `nlme' or `lme4' package is probably more
suitable.  They can estimate variance components by either ML or REML.
Sounds like you're trying to use the moment estimators (aka ANOVA
estimators) of variance components, which is fairly old technology, I
believe.

HTH,
Andy

> From: WeiQiang.Li at seagate.com
> 
> Hi ALL, 
> 
>         I am trying to compute adjusted SS & estimated 
> component variance 
> in GLM with un-balanced data using R. Can anyone advise me? Thanks in 
> advance. 
> 
> Best Regards,
> WeiQiang Li
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
> 
>



From isubirana at imim.es  Tue Oct 26 16:18:12 2004
From: isubirana at imim.es (SUBIRANA CACHINERO, ISAAC)
Date: Tue, 26 Oct 2004 16:18:12 +0200
Subject: [R] GLM model vs. GAM model
Message-ID: <FAD63B0F78D0224A9AF352A4A98F1E2A588B0D@jupiter.imim.es>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20041026/20aea3c9/attachment.pl

From minhan.science at gmail.com  Tue Oct 26 16:39:21 2004
From: minhan.science at gmail.com (Min-Han Tan)
Date: Tue, 26 Oct 2004 10:39:21 -0400
Subject: [R] Fitting additional variables to model
Message-ID: <7902152a041026073951aee21a@mail.gmail.com>

Good morning,

Sorry to trouble the list. I'm working on Cox models of survival, and
am encountering a problem. I'm trying to group variables into some
kind of new staging system  By grouping, I mean : so-called
'integrated staging systems' for cancer merge categories of variables
such as tumor stage, patient status into a single range of categories
e.g.
System I = Stage I or II, Patient Status 0
System II = Stage I or II, Patient Status 1
     OR
     Stage III, Patient Status 0
So in this example, Stage I + II are grouped together, probably based
on outcome.


So in the scenario where each of the initial 2 variables A and B
involved in the model have 4 categories:

1. Is there any other way to obtain a grouping the variables by
outcome besides examining all possible 16 Kaplan Meier curves
concurrently, and seeing how they group? Would it make sense to run
pairwise survfits - but if so, what happens when more variables are
introduced into the equation? Finally, is it possible to execute this
in R? Thanks!!!

2. If there is a significant interaction between these 2 terms (A*B),
does it even make sense to ask how I can perform "grouping" of the
variables?

Thanks in advance!

Min-Han



From tlumley at u.washington.edu  Tue Oct 26 16:40:32 2004
From: tlumley at u.washington.edu (Thomas Lumley)
Date: Tue, 26 Oct 2004 07:40:32 -0700 (PDT)
Subject: [R] GLM model vs. GAM model
In-Reply-To: <FAD63B0F78D0224A9AF352A4A98F1E2A588B0D@jupiter.imim.es>
References: <FAD63B0F78D0224A9AF352A4A98F1E2A588B0D@jupiter.imim.es>
Message-ID: <Pine.A41.4.61b.0410260735500.273400@homer12.u.washington.edu>

On Tue, 26 Oct 2004, SUBIRANA CACHINERO, ISAAC wrote:

> I have a question about how to compare a GLM with a GAM model using anova
> function.

You don't say what gam() function you are using. There are at least two 
out there and they work in quite different ways.

> A GLM is performed for example:
>
> model1 <-glm(formula = exitus ~ age+gender+diabetes, family = "binomial",
> na.action = na.exclude)
>
> A second nested model could be:
>
> model2 <-glm(formula = exitus ~ age+gender, family = "binomial", na.action =
> na.exclude)
<snip>
>
> Similarly for GAM models
>
> model3 <-gam(formula = exitus ~ s(age)+gender, family = "binomial",
> na.action = na.exclude)
>
> "R" allows to compare these two models (GLM vs. GAM)
>
> anova(model2,model3, test="F")
>
> This instruction returns a p-value with no error or warning, but this test
> is based on maximum likelihood, and GAM models are not fitted with maximum
> likelihood criteria, thus I think this p-value is not correct.

Probably not.  If the number of degrees of freedom for age is small it may 
be quite a good approximation. If you are using mgcv::gam you will have 
seen a warning to this effect on the help page for anova.gam.

If you need a more accurate test you could simulate from model2 and 
compare the simulated distribution of the p-value to the value in the 
observed data.

 	-thomas



From vito.muggeo at giustizia.it  Tue Oct 26 16:47:11 2004
From: vito.muggeo at giustizia.it (Vito Muggeo)
Date: Tue, 26 Oct 2004 16:47:11 +0200
Subject: [R] vcov method for 'coxph' objects
Message-ID: <015f01c4bb6a$b01c7900$5c13070a@PROCGEN>

Dear all,
The help file for the generic function vcov states

"Classes with methods for this function include: 'lm', 'glm', 'nls', 'lme',
'gls', 'coxph' and 'survreg' (the last two in package 'survival')."

Since, I am not able to use vcov.coxph(), I am wondering whether I am
missing something (as I suspect..)

regards,
vito

library(survival)
> o<- coxph( Surv(time, status) ~ x + strata(sex), test1)  #example from
?coxph
> vcov(o)  #of course I could use o$var
Error in vcov(o) : no applicable method for "vcov"
> R.version
         _
platform i386-pc-mingw32
arch     i386
os       mingw32
system   i386, mingw32
status
major    2
minor    0.0
year     2004
month    10
day      04
language R



From amurta at ipimar.pt  Tue Oct 26 17:49:43 2004
From: amurta at ipimar.pt (Alberto Murta)
Date: Tue, 26 Oct 2004 15:49:43 +0000
Subject: [R] R for Palm OS
Message-ID: <200410261549.43870.amurta@ipimar.pt>

Dear all

Has anyone tried to compile R for the Palm OS?

-- 
                                         Alberto G. Murta
Institute for Agriculture and Fisheries Research (INIAP-IPIMAR) 
Av. Brasilia, 1449-006 Lisboa, Portugal | Phone: +351 213027120
Fax:+351 213015948 | http://ipimar-iniap.ipimar.pt/pelagicos/



From ripley at stats.ox.ac.uk  Tue Oct 26 16:55:47 2004
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Tue, 26 Oct 2004 15:55:47 +0100 (BST)
Subject: [R] GLM model vs. GAM model
In-Reply-To: <FAD63B0F78D0224A9AF352A4A98F1E2A588B0D@jupiter.imim.es>
Message-ID: <Pine.LNX.4.44.0410261548410.6414-100000@gannet.stats>

R does not contain a gam() function.

*Two* contributed packages, gam and mgcv, do.

Please do as the posting guide asks and clarify what you are talking 
about here.

Your penultimate para is not logical: the tests are _not_ based on maximum
likelihood if ML fitting is not used.  However, there are other model
comparison tests that apply to non-ML fitting.  If you mean anova.gam() in
?mgcv, do read the help page which says

WARNING:

     Unless the models have no penalized terms then these methods are
     only approximate.

but there is also such a function in package gam.


On Tue, 26 Oct 2004, SUBIRANA CACHINERO, ISAAC wrote:

> I have a question about how to compare a GLM with a GAM model using anova
> function.
> 
> A GLM is performed for example: 
> 
> model1 <-glm(formula = exitus ~ age+gender+diabetes, family = "binomial",
> na.action = na.exclude)
> 
> A second nested model could be:
> 
> model2 <-glm(formula = exitus ~ age+gender, family = "binomial", na.action =
> na.exclude)
> 
> To compare these two GLM models the instruction is: 
> 
> anova(model1,model2, test="F")
> 
> Similarly for GAM models
> 
> model3 <-gam(formula = exitus ~ s(age)+gender, family = "binomial",
> na.action = na.exclude)
> 
> "R" allows to compare these two models (GLM vs. GAM)
> 
> anova(model2,model3, test="F") 
> 
> This instruction returns a p-value with no error or warning, but this test
> is based on maximum likelihood, and GAM models are not fitted with maximum
> likelihood criteria, thus I think this p-value is not correct.
> 
> Please, I really appreciate any information about how to compare a GLM with
> a GAM model. 

PLEASE do read the posting guide before posting.

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From p.dalgaard at biostat.ku.dk  Tue Oct 26 17:34:34 2004
From: p.dalgaard at biostat.ku.dk (Peter Dalgaard)
Date: 26 Oct 2004 17:34:34 +0200
Subject: [R] vcov method for 'coxph' objects
In-Reply-To: <015f01c4bb6a$b01c7900$5c13070a@PROCGEN>
References: <015f01c4bb6a$b01c7900$5c13070a@PROCGEN>
Message-ID: <x2654xjxx1.fsf@biostat.ku.dk>

"Vito Muggeo" <vito.muggeo at giustizia.it> writes:

> Dear all,
> The help file for the generic function vcov states
> 
> "Classes with methods for this function include: 'lm', 'glm', 'nls', 'lme',
> 'gls', 'coxph' and 'survreg' (the last two in package 'survival')."
> 
> Since, I am not able to use vcov.coxph(), I am wondering whether I am
> missing something (as I suspect..)


It's a bug in the NAMESPACE file for survival:

> survival:::vcov.coxph(o)
         x
x 1.660006

or

> vcov.coxph <- survival:::vcov.coxph
> vcov(o)
         x
x 1.660006


So the only thing you're missing is example(coxph) in your example.
Without that, test1 is not found.

> > o<- coxph( Surv(time, status) ~ x + strata(sex), test1)  #example from
> ?coxph
> > vcov(o)  #of course I could use o$var
> Error in vcov(o) : no applicable method for "vcov"

It has been reported before, and I think it is fixed in the current
patch version.

-- 
   O__  ---- Peter Dalgaard             Blegdamsvej 3  
  c/ /'_ --- Dept. of Biostatistics     2200 Cph. N   
 (*) \(*) -- University of Copenhagen   Denmark      Ph: (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)             FAX: (+45) 35327907



From ripley at stats.ox.ac.uk  Tue Oct 26 17:40:13 2004
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Tue, 26 Oct 2004 16:40:13 +0100 (BST)
Subject: [R] vcov method for 'coxph' objects
In-Reply-To: <015f01c4bb6a$b01c7900$5c13070a@PROCGEN>
Message-ID: <Pine.LNX.4.44.0410261638280.6482-100000@gannet.stats>

On Tue, 26 Oct 2004, Vito Muggeo wrote:

> Dear all,
> The help file for the generic function vcov states
> 
> "Classes with methods for this function include: 'lm', 'glm', 'nls', 'lme',
> 'gls', 'coxph' and 'survreg' (the last two in package 'survival')."
> 
> Since, I am not able to use vcov.coxph(), I am wondering whether I am
> missing something (as I suspect..)

On some versions of survival it is there but not exported. In that case 
you need

vcov.coxph <- survival:::vcov.coxph

> 
> regards,
> vito
> 
> library(survival)
> > o<- coxph( Surv(time, status) ~ x + strata(sex), test1)  #example from
> ?coxph
> > vcov(o)  #of course I could use o$var
> Error in vcov(o) : no applicable method for "vcov"
> > R.version
>          _
> platform i386-pc-mingw32
> arch     i386
> os       mingw32
> system   i386, mingw32
> status
> major    2
> minor    0.0
> year     2004
> month    10
> day      04
> language R
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
> 
> 

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From Christoph.Scherber at uni-jena.de  Tue Oct 26 17:54:12 2004
From: Christoph.Scherber at uni-jena.de (Christoph Scherber)
Date: Tue, 26 Oct 2004 17:54:12 +0200
Subject: [R] integration problem
Message-ID: <417E7324.9090607@uni-jena.de>

Dear R users,

I have spectral data (say, wavelength vs. extinction coefficient) for 
which I??d like to calculate an integral (i.e. the area underneath the 
curve).

Suppose the (artificial) dataset is

lambda 	E
1 	2
2 	4
3 	5
4 	8
5 	1
6 	5
7 	4
8 	9
9 	8
10 	2



How can I calculate an integral  for these values using R?

Many thanks for any help!
Regards

Christoph



From ottorino-luca.pantani at unifi.it  Tue Oct 26 18:08:09 2004
From: ottorino-luca.pantani at unifi.it (8rino-Luca Pantani)
Date: Tue, 26 Oct 2004 18:08:09 +0200
Subject: [R] indexing within the function "aggregate"
Message-ID: <DNEELNJCLGBOLHCFLMHBGEHNCHAA.OLPantani@unifi.it>

Hi all,
I'm trying to work out the following problem, but I can't imagine how.

I have the following (much reduced & oversimplified) dataset

My.df <-
cbind.data.frame(PPM=c(15.78, 15.81, 15.87, 15.83, 15.81, 15.84,
                   15.91, 15.90, 15.83, 15.81, 15.93, 15.83,
                   15.70, 15.92, 15.76, 15.81, 15.91, 15.75,
                   15.84, 15.86, 15.82, 15.79, 15.81, 15.82,
                   15.86, 15.77, 15.86, 15.99, 15.95, 16.01,
                   15.86, 15.80, 15.87, 15.91, 15.85, 15.84,
                   15.97, 15.92, 15.87, 15.81, 15.89, 15.90,
                   15.93, 15.94, 15.91, 15.95, 15.92, 15.93),
                 expand.grid(INJ = factor(1:3),
                             VIAL = factor(1:4),
                             STD = c("EXT","INT"),
                             SEQUENCE = factor(1:2)))

representing the outcome of an HPLC analysis (PPM)
as from
2 sessions (SEQUENCE) in which
2 methods were used (with EXTernal or INTernal STanDards)
4 VIALS were analyzed by
3 INJection each

I need to calculate some values for each of the combinations
of the factors like

aggregate(My.df[1],
          by = list(SEQ=My.df$SEQUENCE, STD=My.df$STD),
          function(x){mean(x, na.rm=T)})

Up to now, nothing new to me, I can manage it, I've gone even further with

library(lattice);library(grid)
bwplot(VIAL~
       PPM|SEQUENCE*STD,
       data=My.df,
         panel= function(x,y){
         panel.bwplot(x,y)
         dieffe <- anova(lm(x~y))$Df[1]
         enne <- dieffe+1
         esse <- sqrt(anova(lm(x~y))$Mean[1])
         dieffe2 <- anova(lm(x~y))$Df[2]
         enne2 <- dieffe2+1
         esse2 <- sqrt(anova(lm(x~y))$Mean[2])
         tistud <- qt(.975,df=dieffe)*esse/sqrt(enne)
         tistud2 <- qt(.975,df=dieffe2)*esse2/sqrt(enne2)
         esse.quad.inj <- anova(lm(x~y))$Mean[2]
         esse.quad.vial <- (anova(lm(x~y))$Mean[1]-esse.quad.inj)/3
         esse.tot <- esse.quad.inj+esse.quad.vial
         perc.inj <- round(100*esse.quad.inj/esse.tot,1)
         perc.vial <-round(100*esse.quad.vial/esse.tot ,1)
         grid.text(paste("95%cl  VIAL=", round(tistud,3),
                         "  INJ=",round(tistud2,3)),
                   0.0, 0.95, just=c("left","top"))
         grid.text(paste("%var.comp inj",
                         perc.inj,"vial",
                         perc.vial),
                   0.0, 0.1, just=c("left","top"))
               })

My aim is to get a table with (some of) the values showed in the graph
above.

Since "aggregate" can manage functions that
return a single value (V&R, pag.35),
I wrote the following

My.fun.conf <-
  function(y,x){
    dieffe <-  anova(lm(y~x))$Df[1]
    enne  <-  dieffe+1
    esse  <-  sqrt(anova(lm(y~x))$Mean[1])
    limconf.95 <- qt(.975,df=dieffe)*esse/sqrt(enne)
    limconf.95}

that can be used like

tag <- My.df$SEQUENCE == "1" & My.df$STD == "INT"
My.fun.conf(My.df$PPM[tag],My.df$VIAL[tag])

but it do not works in "aggregate"
since the indexing of values is no longer done
on an intrinsecal "x" itself,
but rather on an "somewhat external y".
I hope to be clear enough to be understood.

Am I entering a wrong path?
How can I get an output like
the (cheated) table hereunder?

  SEQ STD INJ95%cl
1   1 EXT 0.035
2   2 EXT 0.031
3   1 INT 0.054
4   2 INT 0.028

Thanks to anyone who are willing to help me.


Ottorino-Luca Pantani, Universit?? di Firenze
Dip. Scienza del Suolo e Nutrizione della Pianta



From s.e.roberts at qmul.ac.uk  Tue Oct 26 18:17:46 2004
From: s.e.roberts at qmul.ac.uk (Sion Roberts)
Date: Tue, 26 Oct 2004 17:17:46 +0100
Subject: [R] 2nd Y-axis text problem
Message-ID: <417E78AA.91D31E54@qmul.ac.uk>

Dear All,
I'm having problems getting text to appear on a second y axis....because
this can only be done within a plot statement and my attempts to add the
label as text didn't work because the "locator" refers to the plot
area....

Any thoughts greatly appreciated.

Thanks in advance,
Sion

>par(mai=c(1,1,1,1.5))
>plot(time[DOY<230],Hmeas[DOY < 230],ylab=expression(paste("Fluxes,
W",m^-2)),xlab="Time of Day,
hours",ylim=c(-100,500),typ="l",lty=1,lwd=1,col=1,axes=F)

>axis(2,at=c(-100,0,100,200,300,400,500),tick=T)
>axis(1,at=c(0,6,12,18,24),tick=T) par(new=T)

>plot(time[DOY<230],Ueddy[DOY<230],xlab="",yaxt="n",ylab="",ylim=c(0,1.6),pch=1,axes=F)
>axis(4,at=c(0,0.4,0.8,1.2,1.6),tick=T) # **problem**



From sundar.dorai-raj at PDF.COM  Tue Oct 26 18:19:36 2004
From: sundar.dorai-raj at PDF.COM (Sundar Dorai-Raj)
Date: Tue, 26 Oct 2004 11:19:36 -0500
Subject: [R] integration problem
In-Reply-To: <417E7324.9090607@uni-jena.de>
References: <417E7324.9090607@uni-jena.de>
Message-ID: <417E7918.5060302@pdf.com>



Christoph Scherber wrote:
> Dear R users,
> 
> I have spectral data (say, wavelength vs. extinction coefficient) for 
> which I??d like to calculate an integral (i.e. the area underneath the 
> curve).
> 
> Suppose the (artificial) dataset is
> 
> lambda     E
> 1     2
> 2     4
> 3     5
> 4     8
> 5     1
> 6     5
> 7     4
> 8     9
> 9     8
> 10     2
> 
> 
> 
> How can I calculate an integral  for these values using R?
> 
> Many thanks for any help!
> Regards
> 
> Christoph
> 

Hi Christoph,
   You can write some code to do trapezoidal integration or use ?approx 
in the following manner:

f <- function(xnew, x, y) approx(x, y, xnew)$y
integrate(f, min(x$lambda), max(x$lambda), x = x$lambda, y = x$E)

where `x' is your data above. Using approx is perhaps overkill but it 
works. A better solution would be to use trapezoids or perhaps piecewise 
linear integration. I don't know of a package that has the latter 
approaches off the top of my head but that doesn't mean they doesn't 
exist somewhere.

--sundar



From pcampbell at econ.bbk.ac.uk  Tue Oct 26 18:41:08 2004
From: pcampbell at econ.bbk.ac.uk (Phineas Campbell)
Date: Tue, 26 Oct 2004 17:41:08 +0100
Subject: [R] Empirical P Value
Message-ID: <NGECIFANPOJAGABBAEAPOECGDJAA.pcampbell@econ.bbk.ac.uk>

I am trying to return the p value for a stat from the ECDF.  That is the
index of the first occurrence,
on an ordered vector, of a value either greater than or equal to a given
value.

Ideally I would not have to order the vector beforehand.

Currently I use:

	PValue<-function(stat, ECDF){
		###Get the length of the ECFD
     		L<-length(ECDF)
		###Loop through the ECDF until the p value is found
		for(i in 1:L){
			if(ECDF[i]>=stat){
				break
			}
		}
		###Return the 3 values that bracket the p value
		c((i-1)/L, i/L, (i+1)/L)
	}

Is there a way of doing this that avoids the explicit loop?

platform i386-pc-mingw32
arch     i386
os       mingw32
system   i386, mingw32
status
major    1
minor    9.1
year     2004
month    06
day      21
language R

Phineas Campbell



From HDoran at air.org  Tue Oct 26 18:48:15 2004
From: HDoran at air.org (Doran, Harold)
Date: Tue, 26 Oct 2004 12:48:15 -0400
Subject: [R] Splitting and saving separate dataframes
Message-ID: <88EAF3512A55DF46B06B1954AEF73F740624FA22@dc1ex2.air.org>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20041026/998d5b52/attachment.pl

From ligges at statistik.uni-dortmund.de  Tue Oct 26 19:03:07 2004
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Tue, 26 Oct 2004 19:03:07 +0200
Subject: [R] 2nd Y-axis text problem
In-Reply-To: <417E78AA.91D31E54@qmul.ac.uk>
References: <417E78AA.91D31E54@qmul.ac.uk>
Message-ID: <417E834B.2080003@statistik.uni-dortmund.de>

Sion Roberts wrote:

> Dear All,
> I'm having problems getting text to appear on a second y axis....because
> this can only be done within a plot statement and my attempts to add the
> label as text didn't work because the "locator" refers to the plot
> area....


For axis, see ?axis.
For margin text, see ?mtext.
For plotting arbitrary stuff into the margins, see ?par and its argument 
"xpd".

Uwe Ligges


> Any thoughts greatly appreciated.
> 
> Thanks in advance,
> Sion
> 
> 
>>par(mai=c(1,1,1,1.5))
>>plot(time[DOY<230],Hmeas[DOY < 230],ylab=expression(paste("Fluxes,
> 
> W",m^-2)),xlab="Time of Day,
> hours",ylim=c(-100,500),typ="l",lty=1,lwd=1,col=1,axes=F)
> 
> 
>>axis(2,at=c(-100,0,100,200,300,400,500),tick=T)
>>axis(1,at=c(0,6,12,18,24),tick=T) par(new=T)
> 
> 
>>plot(time[DOY<230],Ueddy[DOY<230],xlab="",yaxt="n",ylab="",ylim=c(0,1.6),pch=1,axes=F)
>>axis(4,at=c(0,0.4,0.8,1.2,1.6),tick=T) # **problem**
> 
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html



From HBaize at buttecounty.net  Tue Oct 26 19:17:10 2004
From: HBaize at buttecounty.net (Baize, Harold)
Date: Tue, 26 Oct 2004 10:17:10 -0700
Subject: [R] read.spss Error reading system-file header
Message-ID: <5BADEF9F1A24CD4CB7778D1CEB3769E206D4B8@bc-mail4.bci.buttecounty.net>


Bj?rn Stollenwerk <b.stollenwerk at gmx.de> wrote:

> I had and still have got the same problem. There were two columns in the 
> data frame which contain just missing values. When I deleted those the 
> impord seemed to work, but there is still a warning message.

> Warning message:
> C:\test.sav: Unrecognized record type 7, subtype 13 encountered in 
> system file.
>

This warning message has been discussed a few times. It occurs when read.spss 
reads a file from SPSS 12.0 or higher. Beginning with SPSS 12 the file format 
was changed to accommodate long variable names. It seems logical that SPSS 
added a record to their file format to hold the long variable names, while 
keeping the short (eight character) variable names to be compatible with older 
versions. The read.spss function has not been updated. It reads the short names, 
ignores the long names, and produces the warning you cite. There does not seem 
to be any error in reading the data, other than the warning and continued use of 
short variable names. I brought this up more than a year ago, just after SPSS 12 
was released. I e-mailed the author (Saikat DebRoy) of the read.spss function, 
but did not get a response, I don't know if he got the message. It would be 
nice to update the function to read the longer variable names and be rid of 
the warning.   

Harold



From ggrothendieck at myway.com  Tue Oct 26 19:13:43 2004
From: ggrothendieck at myway.com (Gabor Grothendieck)
Date: Tue, 26 Oct 2004 17:13:43 +0000 (UTC)
Subject: [R] indexing within the function "aggregate"
References: <DNEELNJCLGBOLHCFLMHBGEHNCHAA.OLPantani@unifi.it>
Message-ID: <loom.20041026T191015-785@post.gmane.org>

8rino-Luca Pantani <ottorino-luca.pantani <at> unifi.it> writes:

: 
: Hi all,
: I'm trying to work out the following problem, but I can't imagine how.
: 
: I have the following (much reduced & oversimplified) dataset
: 
: My.df <-
: cbind.data.frame(PPM=c(15.78, 15.81, 15.87, 15.83, 15.81, 15.84,
:                    15.91, 15.90, 15.83, 15.81, 15.93, 15.83,
:                    15.70, 15.92, 15.76, 15.81, 15.91, 15.75,
:                    15.84, 15.86, 15.82, 15.79, 15.81, 15.82,
:                    15.86, 15.77, 15.86, 15.99, 15.95, 16.01,
:                    15.86, 15.80, 15.87, 15.91, 15.85, 15.84,
:                    15.97, 15.92, 15.87, 15.81, 15.89, 15.90,
:                    15.93, 15.94, 15.91, 15.95, 15.92, 15.93),
:                  expand.grid(INJ = factor(1:3),
:                              VIAL = factor(1:4),
:                              STD = c("EXT","INT"),
:                              SEQUENCE = factor(1:2)))
: 
: representing the outcome of an HPLC analysis (PPM)
: as from
: 2 sessions (SEQUENCE) in which
: 2 methods were used (with EXTernal or INTernal STanDards)
: 4 VIALS were analyzed by
: 3 INJection each
: 
: I need to calculate some values for each of the combinations
: of the factors like
: 
: aggregate(My.df[1],
:           by = list(SEQ=My.df$SEQUENCE, STD=My.df$STD),
:           function(x){mean(x, na.rm=T)})
: 
: Up to now, nothing new to me, I can manage it, I've gone even further with
: 
: library(lattice);library(grid)
: bwplot(VIAL~
:        PPM|SEQUENCE*STD,
:        data=My.df,
:          panel= function(x,y){
:          panel.bwplot(x,y)
:          dieffe <- anova(lm(x~y))$Df[1]
:          enne <- dieffe+1
:          esse <- sqrt(anova(lm(x~y))$Mean[1])
:          dieffe2 <- anova(lm(x~y))$Df[2]
:          enne2 <- dieffe2+1
:          esse2 <- sqrt(anova(lm(x~y))$Mean[2])
:          tistud <- qt(.975,df=dieffe)*esse/sqrt(enne)
:          tistud2 <- qt(.975,df=dieffe2)*esse2/sqrt(enne2)
:          esse.quad.inj <- anova(lm(x~y))$Mean[2]
:          esse.quad.vial <- (anova(lm(x~y))$Mean[1]-esse.quad.inj)/3
:          esse.tot <- esse.quad.inj+esse.quad.vial
:          perc.inj <- round(100*esse.quad.inj/esse.tot,1)
:          perc.vial <-round(100*esse.quad.vial/esse.tot ,1)
:          grid.text(paste("95%cl  VIAL=", round(tistud,3),
:                          "  INJ=",round(tistud2,3)),
:                    0.0, 0.95, just=c("left","top"))
:          grid.text(paste("%var.comp inj",
:                          perc.inj,"vial",
:                          perc.vial),
:                    0.0, 0.1, just=c("left","top"))
:                })
: 
: My aim is to get a table with (some of) the values showed in the graph
: above.
: 
: Since "aggregate" can manage functions that
: return a single value (V&R, pag.35),
: I wrote the following
: 
: My.fun.conf <-
:   function(y,x){
:     dieffe <-  anova(lm(y~x))$Df[1]
:     enne  <-  dieffe+1
:     esse  <-  sqrt(anova(lm(y~x))$Mean[1])
:     limconf.95 <- qt(.975,df=dieffe)*esse/sqrt(enne)
:     limconf.95}
: 
: that can be used like
: 
: tag <- My.df$SEQUENCE == "1" & My.df$STD == "INT"
: My.fun.conf(My.df$PPM[tag],My.df$VIAL[tag])
: 
: but it do not works in "aggregate"
: since the indexing of values is no longer done
: on an intrinsecal "x" itself,
: but rather on an "somewhat external y".
: I hope to be clear enough to be understood.
: 
: Am I entering a wrong path?
: How can I get an output like
: the (cheated) table hereunder?
: 
:   SEQ STD INJ95%cl
: 1   1 EXT 0.035
: 2   2 EXT 0.031
: 3   1 INT 0.054
: 4   2 INT 0.028
: 
: Thanks to anyone who are willing to help me.

Not entirely sure what it is you are looking for but 
its likely the R `by' command that you want. 

The following does not give the answer in your post but it
does run My.fun.conf on PPM and INJ grouped by columns 4 and 5
and can be adapted to your needs:

z <- by(My.df, My.df[,4:5], function(x) My.fun.conf(x$PPM,x$INJ)) 
z # or unclass(z) or as.data.frame(unclass(z))



From ripley at stats.ox.ac.uk  Tue Oct 26 20:07:37 2004
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Tue, 26 Oct 2004 19:07:37 +0100 (BST)
Subject: [R] read.spss Error reading system-file header
In-Reply-To: <5BADEF9F1A24CD4CB7778D1CEB3769E206D4B8@bc-mail4.bci.buttecounty.net>
Message-ID: <Pine.LNX.4.44.0410261904230.6863-100000@gannet.stats>

On Tue, 26 Oct 2004, Baize, Harold wrote:

> 
> Bj?rn Stollenwerk <b.stollenwerk at gmx.de> wrote:
> 
> > I had and still have got the same problem. There were two columns in the 
> > data frame which contain just missing values. When I deleted those the 
> > impord seemed to work, but there is still a warning message.
> 
> > Warning message:
> > C:\test.sav: Unrecognized record type 7, subtype 13 encountered in 
> > system file.
>
> This warning message has been discussed a few times. It occurs when
> read.spss reads a file from SPSS 12.0 or higher. Beginning with SPSS 12
> the file format was changed to accommodate long variable names. It seems
> logical that SPSS added a record to their file format to hold the long
> variable names, while keeping the short (eight character) variable names
> to be compatible with older versions. The read.spss function has not
> been updated. It reads the short names, ignores the long names, and
> produces the warning you cite. There does not seem to be any error in
> reading the data, other than the warning and continued use of short
> variable names. I brought this up more than a year ago, just after SPSS
> 12 was released. I e-mailed the author (Saikat DebRoy) of the read.spss
> function, but did not get a response, I don't know if he got the
> message. It would be nice to update the function to read the longer
> variable names and be rid of the warning.

Please send your patch to the maintainer (R-core, not Saikat DebRoy, if
you read the DESCRIPTION file).  (If you were not offering to do this,
please don't volunteer other people to do work for you.)

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From ripley at stats.ox.ac.uk  Tue Oct 26 20:11:18 2004
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Tue, 26 Oct 2004 19:11:18 +0100 (BST)
Subject: [R] Empirical P Value
In-Reply-To: <NGECIFANPOJAGABBAEAPOECGDJAA.pcampbell@econ.bbk.ac.uk>
Message-ID: <Pine.LNX.4.44.0410261907570.6863-100000@gannet.stats>

On Tue, 26 Oct 2004, Phineas Campbell wrote:

> I am trying to return the p value for a stat from the ECDF.  That is the
> index of the first occurrence,
> on an ordered vector, of a value either greater than or equal to a given
> value.

That's called a *quantile*, probably type=1 in the Hyndman-Fan 
classification, if the `ordered vector' is the ECDF x-values.

> Ideally I would not have to order the vector beforehand.

You could use sort(partial=something), though, as quantile does.

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From lperepol at necessity.org  Tue Oct 26 20:22:37 2004
From: lperepol at necessity.org (lawrence Perepolkin)
Date: Tue, 26 Oct 2004 14:22:37 -0400
Subject: [R] Need help finding max from 3d locfit output
Message-ID: <000001c4bb88$c9b8e2e0$6a02a8c0@win1>

Hi I am running the following locfit function:
> fit <- locfit( log(base$Value) ~ base$lBrkOut +  base$lExit , alpha=0.9)
> plot(fit,type="persp" )


The above two steps create a nice 3d plot. I would like to find the xy
coordinates where the maximum z occurs. 

Does any one know of function to call that will display the xy coordinates
where max(z) occurs?

Thanks
Lawrence



From p.connolly at hortresearch.co.nz  Tue Oct 26 21:50:22 2004
From: p.connolly at hortresearch.co.nz (Patrick Connolly)
Date: Wed, 27 Oct 2004 08:50:22 +1300
Subject: [R] Intro to R: lecture presentation
In-Reply-To: <loom.20041025T174751-165@post.gmane.org>; from 
	ggrothendieck@myway.com on Mon, Oct 25, 2004 at 03:52:19PM +0000
References: <20041025143357.2740.qmail@webmail29.rediffmail.com> 
	<417D198D.3020603@swissinfo.org>
	<loom.20041025T174751-165@post.gmane.org>
Message-ID: <20041027085022.C23150@hortresearch.co.nz>

On Mon, 25-Oct-2004 at 03:52PM +0000, Gabor Grothendieck wrote:

[...]

|> The first link worked for me using Windows XP and Internet Explorer
|> 6.  Using Mozilla on the same machine it seemed to download
|> something but Mozilla was unable to render it (got same as you) so
|> its likely using IE-specific constructs.

Even if I condescend to use IE, I get only a message:

404 Error: File not found
Perhaps it moved?!



-- 
Patrick Connolly
HortResearch
Mt Albert
Auckland
New Zealand 
Ph: +64-9 815 4200 x 7188
~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~
I have the world`s largest collection of seashells. I keep it on all
the beaches of the world ... Perhaps you`ve seen it.  ---Steven Wright 
~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~



From andy_liaw at merck.com  Tue Oct 26 21:50:41 2004
From: andy_liaw at merck.com (Liaw, Andy)
Date: Tue, 26 Oct 2004 15:50:41 -0400
Subject: [R] Need help finding max from 3d locfit output
Message-ID: <3A822319EB35174CA3714066D590DCD50994E230@usrymx25.merck.com>

You can just do something like base[which.max(predict(fit)),].

One suggestion:  Use something like locfit(log(Value) ~ lBrkOut + lExit,
data=base, ...).  It's much safer that way.

HTH,
Andy

> From: lawrence Perepolkin
> 
> Hi I am running the following locfit function:
> > fit <- locfit( log(base$Value) ~ base$lBrkOut +  base$lExit 
> , alpha=0.9)
> > plot(fit,type="persp" )
> 
> 
> The above two steps create a nice 3d plot. I would like to find the xy
> coordinates where the maximum z occurs. 
> 
> Does any one know of function to call that will display the 
> xy coordinates
> where max(z) occurs?
> 
> Thanks
> Lawrence
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
> 
>



From speters at exelixis.com  Tue Oct 26 21:54:30 2004
From: speters at exelixis.com (Sandie Peters)
Date: Tue, 26 Oct 2004 12:54:30 -0700
Subject: [R] Newbie question about the use of lm and anova
Message-ID: <2B4F16E58B329B40A8775C2C07B9D579824194@EXCL01.exelixis.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20041026/87aa70f1/attachment.pl

From Ted.Harding at nessie.mcc.ac.uk  Tue Oct 26 22:21:51 2004
From: Ted.Harding at nessie.mcc.ac.uk ( (Ted Harding))
Date: Tue, 26 Oct 2004 21:21:51 +0100 (BST)
Subject: [R] Empirical P Value
In-Reply-To: <NGECIFANPOJAGABBAEAPOECGDJAA.pcampbell@econ.bbk.ac.uk>
Message-ID: <XFMail.041026201333.Ted.Harding@nessie.mcc.ac.uk>

On 26-Oct-04 Phineas Campbell wrote:
> I am trying to return the p value for a stat from the ECDF.
> That is the index of the first occurrence, on an ordered vector,
> of a value either greater than or equal to a given value.
> 
> Ideally I would not have to order the vector beforehand.
> 
> Currently I use:
> 
>       PValue<-function(stat, ECDF){
>               ###Get the length of the ECFD
>               L<-length(ECDF)
>               ###Loop through the ECDF until the p value is found
>               for(i in 1:L){
>                       if(ECDF[i]>=stat){
>                               break
>                       }
>               }
>               ###Return the 3 values that bracket the p value
>               c((i-1)/L, i/L, (i+1)/L)
>       }
> 
> Is there a way of doing this that avoids the explicit loop?

      PValue<-function(stat, ECDF){
              ###Get the length of the ECFD
              L<-length(ECDF)
              i<-min(which(ECDF>=stat))
              ###Return the 3 values that bracket the p value
              c((i-1)/L, i/L, (i+1)/L)
      }

will do what your function does, without the loop.
Best wishes,
Ted.


--------------------------------------------------------------------
E-Mail: (Ted Harding) <Ted.Harding at nessie.mcc.ac.uk>
Fax-to-email: +44 (0)870 094 0861  [NB: New number!]
Date: 26-Oct-04                                       Time: 20:13:33
------------------------------ XFMail ------------------------------



From rob at fatkat.com  Tue Oct 26 23:00:13 2004
From: rob at fatkat.com (Rob Steele)
Date: Tue, 26 Oct 2004 17:00:13 -0400
Subject: [R] Idiom for column operations?
Message-ID: <417EBADD.4030604@fatkat.com>

Is there a better way to express operations between matrices and column 
vectors than transposing the matrix twice?

This is the kind of thing I'm talking about:

m = matrix(1:20, 3, 4)
v = colSums(m)

t(t(m) / v)   ## <--  kinda ugly, ain't it?

I thought of converting the column vector to a matrix:

m / matrix(v, nrow = nrow(m), ncol = length(v), byrow = TRUE)

But that seems even worse.

Thanks!
Rob Steele



From sundar.dorai-raj at PDF.COM  Tue Oct 26 23:08:52 2004
From: sundar.dorai-raj at PDF.COM (Sundar Dorai-Raj)
Date: Tue, 26 Oct 2004 16:08:52 -0500
Subject: [R] Idiom for column operations?
In-Reply-To: <417EBADD.4030604@fatkat.com>
References: <417EBADD.4030604@fatkat.com>
Message-ID: <417EBCE4.70606@pdf.com>



Rob Steele wrote:

> Is there a better way to express operations between matrices and column 
> vectors than transposing the matrix twice?
> 
> This is the kind of thing I'm talking about:
> 
> m = matrix(1:20, 3, 4)
> v = colSums(m)
> 
> t(t(m) / v)   ## <--  kinda ugly, ain't it?
> 
> I thought of converting the column vector to a matrix:
> 
> m / matrix(v, nrow = nrow(m), ncol = length(v), byrow = TRUE)
> 
> But that seems even worse.
> 
> Thanks!
> Rob Steele
> 

Rob,

Would ?sweep be sufficient for you?

m <- matrix(1:20, 5, 4)
sweep(m, 2, colSums(m), "/")

--sundar



From martin at ist.org  Tue Oct 26 23:50:57 2004
From: martin at ist.org (Martin Keller-Ressel)
Date: Tue, 26 Oct 2004 23:50:57 +0200
Subject: [R] Error in lazyLoadDBfetch
Message-ID: <opsghyy7rzjigwsf@mail.ist.org>

Hello,

what does the following error mean??
It occured during or at the end of a lengthy (and memory-intensive) 
calculation using a routine from a shared library called via the '.C' 
function:

Error in lazyLoadDBfetch(key, datafile, compressed, envhook) :
         recursive default argument reference

I could not find any help with help.search("lazyLoadDBfetch")
Im using R 2.0.0:

> version
          _
platform i386-pc-mingw32
arch     i386
os       mingw32
system   i386, mingw32
status
major    2
minor    0.0
year     2004
month    10
day      04
language R


Any help is appreciated,

Martin Keller-Ressel


--



From f.gherardini at pigrecodata.net  Wed Oct 27 02:15:14 2004
From: f.gherardini at pigrecodata.net (Federico Gherardini)
Date: Wed, 27 Oct 2004 02:15:14 +0200
Subject: [R] Combining columns of different length
In-Reply-To: <200410261235.i9QCZBkw025441@erdos.math.unb.ca>
References: <200410261235.i9QCZBkw025441@erdos.math.unb.ca>
Message-ID: <417EE892.9090209@pigrecodata.net>

Thanks everybody for their help but in this particular case I've found a 
very easy solution. Since the matrix is preallocated I don't really have 
to add a new column: given a matrix of NAs  I can simply

m[,index][1:length(vec)] <- vec

Thanks again for your suggestions, they will be useful when I need to 
add a column  to a data frame! ;)

Cheers,

Federico



From gruppkorsband at hotmail.com  Wed Oct 27 01:45:53 2004
From: gruppkorsband at hotmail.com (John Robot)
Date: Wed, 27 Oct 2004 01:45:53 +0200
Subject: [R] Secondary y axis
Message-ID: <BAY23-F3721ju2iDL6e00017361@hotmail.com>

Hi
I had a look at the help and previous discussions but I am
still unable to solve these issues:


1
I have a set of air pressure data. I would like to
plot a and b (both about 980-1000 millibar) and c, the difference
(around 0-8 millibar). I use axis (4) to create a new axis (to the right)
but I do not know how to assign c to the new axis which currently
features the same values as the first axis. So, how do I show a, b and c
in the same plot with different scales (one for each y-axis)?


2
I would like the x-axis to be time (h): 04, 05, 06 ... 24, 01, 02 ...
(the data is stored in the same table as a and b). How do define where
to get data for the a-axis and show for example every 5th value along
the axis? When I try plot (a ~ h ... the results are messed up in the
plot.


3
Is there a way to define the color within the box in the plot?


I pasted my current code here:
-----------------
# Import

a = read.table("pres.txt", header=TRUE)
b = read.table("pres.txt", header=TRUE)
h = read.table("pres.txt", header=TRUE)

a = (a[,2])
b = (b[,3])
c = abs(a-b)
h = (h[,1])

f = seq(980, 995, 2.5)

### Plot

par (mfrow=c(1,1))    # rows, columns
par (cex.lab=1, col.axis="black", bg="lightyellow", fg="black", mar=c(4, 4, 
2.5, 2))    # cex sets font size


# Plot 1
plot (a, type="l", xlab="Time (h)", ylab="Temperature (??C)", ylim=c(980, 
995), pch=1, col="red", lwd=2, tck=-0.01)
abline (h=f, lwd=1, lty=2)

lines(b, pch=3, col="blue",  cex=2, type="l", lty=1, lwd=2) # lty controls 
line type
title("Air Pressure (mb)", font.main=1, adj=0.5, cex.main=2)


axis (4, labels=TRUE, tick=TRUE, col="black", tck=-0.01)
lines(c, pch=3, col="green", cex=2, type="l", lty=5, lwd=2)
-----------------



Best regards,

Magnus Nilsson, Sweden



From ggrothendieck at myway.com  Wed Oct 27 04:14:16 2004
From: ggrothendieck at myway.com (Gabor Grothendieck)
Date: Wed, 27 Oct 2004 02:14:16 +0000 (UTC)
Subject: [R] Combining columns of different length
References: <200410261235.i9QCZBkw025441@erdos.math.unb.ca>
	<417EE892.9090209@pigrecodata.net>
Message-ID: <loom.20041027T041259-343@post.gmane.org>

Federico Gherardini <f.gherardini <at> pigrecodata.net> writes:

: 
: Thanks everybody for their help but in this particular case I've found a 
: very easy solution. Since the matrix is preallocated I don't really have 
: to add a new column: given a matrix of NAs  I can simply
: 
: m[,index][1:length(vec)] <- vec
: 

You could simplify that to:

    m[seq(along=vec), index] <- vec



From maustin at amgen.com  Wed Oct 27 06:21:02 2004
From: maustin at amgen.com (Austin, Matt)
Date: Tue, 26 Oct 2004 21:21:02 -0700
Subject: [R] Secondary y axis
Message-ID: <E7D5AB4811D20B489622AABA9C53859104E0DA31@teal-exch.amgen.com>

You could replace your last two lines with the following.  You'll need to
play with the axes labeling to make it pretty.

scaled.c <- scale(c)
par(new=TRUE)
plot(scaled.c, ylim=c(-5, 5), xaxt="n", yaxt="n", col='green', type='l',
xlab="", ylab="")

axis (4,
      labels=round(seq(-2, 2, .25)*attr(scaled.c, 'scaled:scale')  +
attr(scaled.c, 'scaled:center'), 1),
      at=seq(-2, 2, .25), col="black", tck=-0.01, cex=.5)
par(new=FALSE)

-----Original Message-----
From: r-help-bounces at stat.math.ethz.ch
[mailto:r-help-bounces at stat.math.ethz.ch]On Behalf Of John Robot
Sent: Tuesday, October 26, 2004 16:46 PM
To: r-help at stat.math.ethz.ch
Subject: [R] Secondary y axis


Hi
I had a look at the help and previous discussions but I am
still unable to solve these issues:


1
I have a set of air pressure data. I would like to
plot a and b (both about 980-1000 millibar) and c, the difference
(around 0-8 millibar). I use axis (4) to create a new axis (to the right)
but I do not know how to assign c to the new axis which currently
features the same values as the first axis. So, how do I show a, b and c
in the same plot with different scales (one for each y-axis)?


2
I would like the x-axis to be time (h): 04, 05, 06 ... 24, 01, 02 ...
(the data is stored in the same table as a and b). How do define where
to get data for the a-axis and show for example every 5th value along
the axis? When I try plot (a ~ h ... the results are messed up in the
plot.


3
Is there a way to define the color within the box in the plot?


I pasted my current code here:
-----------------
# Import

a = read.table("pres.txt", header=TRUE)
b = read.table("pres.txt", header=TRUE)
h = read.table("pres.txt", header=TRUE)

a = (a[,2])
b = (b[,3])
c = abs(a-b)
h = (h[,1])

f = seq(980, 995, 2.5)

### Plot

par (mfrow=c(1,1))    # rows, columns
par (cex.lab=1, col.axis="black", bg="lightyellow", fg="black", mar=c(4, 4, 
2.5, 2))    # cex sets font size


# Plot 1
plot (a, type="l", xlab="Time (h)", ylab="Temperature (??C)", ylim=c(980, 
995), pch=1, col="red", lwd=2, tck=-0.01)
abline (h=f, lwd=1, lty=2)

lines(b, pch=3, col="blue",  cex=2, type="l", lty=1, lwd=2) # lty controls 
line type
title("Air Pressure (mb)", font.main=1, adj=0.5, cex.main=2)


axis (4, labels=TRUE, tick=TRUE, col="black", tck=-0.01)
lines(c, pch=3, col="green", cex=2, type="l", lty=5, lwd=2)
-----------------



Best regards,

Magnus Nilsson, Sweden

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide!
http://www.R-project.org/posting-guide.html



From ripley at stats.ox.ac.uk  Wed Oct 27 08:09:48 2004
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Wed, 27 Oct 2004 07:09:48 +0100 (BST)
Subject: [R] Idiom for column operations?
In-Reply-To: <417EBADD.4030604@fatkat.com>
Message-ID: <Pine.LNX.4.44.0410270705410.18007-100000@gannet.stats>

On Tue, 26 Oct 2004, Rob Steele wrote:

> Is there a better way to express operations between matrices and column 
> vectors than transposing the matrix twice?
> 
> This is the kind of thing I'm talking about:
> 
> m = matrix(1:20, 3, 4)

Ouch: that gives a warning as 20 > 3*4.

> v = colSums(m)
> 
> t(t(m) / v)   ## <--  kinda ugly, ain't it?
> 
> I thought of converting the column vector to a matrix:
> 
> m / matrix(v, nrow = nrow(m), ncol = length(v), byrow = TRUE)
> 
> But that seems even worse.

Use the fact that matrix/vector is done as vector/vector and matrices are 
stored down columns:

m/rep(v, each=nrow(m))

You will find that (or using other variants on rep) in lots of S/R code.

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From ripley at stats.ox.ac.uk  Wed Oct 27 08:45:20 2004
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Wed, 27 Oct 2004 07:45:20 +0100 (BST)
Subject: [R] Error in lazyLoadDBfetch
In-Reply-To: <opsghyy7rzjigwsf@mail.ist.org>
Message-ID: <Pine.LNX.4.44.0410270741490.18161-100000@gannet.stats>

On Tue, 26 Oct 2004, Martin Keller-Ressel wrote:

> Hello,
> 
> what does the following error mean??
> It occured during or at the end of a lengthy (and memory-intensive) 
> calculation using a routine from a shared library called via the '.C' 
> function:
> 
> Error in lazyLoadDBfetch(key, datafile, compressed, envhook) :
>          recursive default argument reference
> 
> I could not find any help with help.search("lazyLoadDBfetch")

No, it's an internal function.  See my article in the current R-news, 
though.

> Im using R 2.0.0:

This suggests that the C code you used has corrupted R's memory
allocation.  It should not happen, and has not been seen in really 
extensive testing of 2.0.0 prior to release.

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From ligges at statistik.uni-dortmund.de  Wed Oct 27 08:51:22 2004
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Wed, 27 Oct 2004 08:51:22 +0200
Subject: [R] box() and hist()
In-Reply-To: <24996.129.206.90.2.1098787042.squirrel@www.paed.uni-muenchen.de>
References: <50486.129.206.90.2.1098723762.squirrel@www.paed.uni-muenchen.de>	<x2breqr92a.fsf@biostat.ku.dk>
	<24996.129.206.90.2.1098787042.squirrel@www.paed.uni-muenchen.de>
Message-ID: <417F456A.8000309@statistik.uni-dortmund.de>

garbade at mip.paed.uni-muenchen.de wrote:

>>garbade at mip.paed.uni-muenchen.de writes:
>>
>>
>>>Hi,
>>>does anybody know why the following is not working:
>>>
>>>
>>>>hist(rnorm(200))
>>>>box(bty="o")
>>>
>>>gives me a box without rounded corners.
>>
>>Because that isn't what it is supposed to do. Did you expect "7" to
>>give you a slanting right edge? And which letter should represent a
>>full rectangle?
> 
> 
> Thought "o" means rectangle with rounded corners... Thanks, Sven


Right, "o" means rectangle (without the word "rounded"). How to change 
the style of line interconnects (e.g. to be "orunded" in a way) is 
descirbed in an article by Paul Murrell in the most recent issue of R News.

Uwe Ligges



> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html



From ligges at statistik.uni-dortmund.de  Wed Oct 27 09:04:44 2004
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Wed, 27 Oct 2004 09:04:44 +0200
Subject: [R] persp(), scatterplot3d(), "..." argument
In-Reply-To: <a0600200ebda3cb456654@[139.166.242.29]>
References: <a0600200ebda3cb456654@[139.166.242.29]>
Message-ID: <417F488C.7000405@statistik.uni-dortmund.de>

Robin Hankin wrote:

> 
> Hello list.
> 
> I very often need 3d scatterplots, and use scatterplot3D quite a lot.
> I am trying to modify persp() to plot scatterplots, and make use of
> the theta and phi arguments that persp() offers.  I am having some
> difficulty passing the correct arguments to persp().
> 
> Here is my function so far.  Much of it is copied from the persp() manpage.
> 
> 
> 
> points3d <- function(x,y,z, jj.colour="black", ...){
>   if(is.matrix(x)){
>     z <- x[,3]
>     y <- x[,2]
>     x <- x[,1]
>   }
>     z.grid <- matrix(range(z),2,2)
>     persp(range(x), range(y),    z.grid,
>     col = NA,border=NA, ...) -> res
> 
> 
>     trans3d <- function(x,y,z, pmat) {
>        tr <- cbind(x,y,z,1) %*% pmat
>        list(x = tr[,1]/tr[,4], y= tr[,2]/tr[,4])
>      }
> 
>   points(trans3d(x,y,z,pm=res), col=jj.colour, ...)
> }
> 
> 
> 
> With this, things like
> 
>    O <- matrix(rnorm(60),20,3)
>    points3d(O,jj.colour="red",pch=16,theta=30,phi=40)
> 
> work as expected, but all extra arguments are passed to persp() _and_
> lines() and this gives warnings.
> 
> 
> QUESTION:
> 
> What is best practice to handle this sort of problem?  Should I ignore
> the warnings() that this approach gives?  I can suppress them with
> options(warn= -Inf), but is this a good idea?  I've read section 10.4
> of AITR.

Many of us have thought about this problem.
I'd suggest to introduce frequently used arguments to your meta-function 
(such as main, xlab, ylab, ...), but ignore warnings (note: warnings, 
not errors) if less frequently arguments are passed through "...".

Alternatively, you can exclude "..." from lines() or points() and 
specify a list of arguments to be passed to that call.

Uwe Ligges



> (the odd argument jj.colour is there because persp() needs to
> be called with col=NA).



From jarioksa at sun3.oulu.fi  Wed Oct 27 09:39:50 2004
From: jarioksa at sun3.oulu.fi (Jari Oksanen)
Date: 27 Oct 2004 10:39:50 +0300
Subject: [R] persp(), scatterplot3d(), "..." argument
In-Reply-To: <417F488C.7000405@statistik.uni-dortmund.de>
References: <a0600200ebda3cb456654@[139.166.242.29]>
	<417F488C.7000405@statistik.uni-dortmund.de>
Message-ID: <1098862790.16776.29.camel@biol102145.oulu.fi>

On Wed, 2004-10-27 at 10:04, Uwe Ligges wrote:
> Robin Hankin wrote:
> 
> > 
> > Hello list.
> > 
> > I very often need 3d scatterplots, and use scatterplot3D quite a lot.
> > I am trying to modify persp() to plot scatterplots, and make use of
> > the theta and phi arguments that persp() offers.  I am having some
> > difficulty passing the correct arguments to persp().
> > 
> > Here is my function so far.  Much of it is copied from the persp() manpage.
> > work as expected, but all extra arguments are passed to persp() _and_
> > lines() and this gives warnings.
---cut---
> > 
> > 
> > QUESTION:
> > 
> > What is best practice to handle this sort of problem?  Should I ignore
> > the warnings() that this approach gives?  I can suppress them with
> > options(warn= -Inf), but is this a good idea?  I've read section 10.4
> > of AITR.
> 
> Many of us have thought about this problem.
> I'd suggest to introduce frequently used arguments to your meta-function 
> (such as main, xlab, ylab, ...), but ignore warnings (note: warnings, 
> not errors) if less frequently arguments are passed through "...".
> 
> Alternatively, you can exclude "..." from lines() or points() and 
> specify a list of arguments to be passed to that call.

This is a larger problem if 
1. one of the underlying functions does not have "..."
2. you want to relay arguments to two or more underlying functions, and
3. you don't want to list all possible arguments in your function
definition, since it is long enough already.

The solution is still there, but it is (black) magic. For instance,
'arrows' does not have "...", so you must add them with this magical
mystery string:

formals(arrows) <- c(formals(arrows), alist(... = ))

Yes, this is documented in R manuals -- I wouldn't know this otherwise.
Still, it would be nicer if 'arrows' had "..."...

cheers, jari oksanen
-- 
Jari Oksanen <jarioksa at sun3.oulu.fi>



From petr.pikal at precheza.cz  Wed Oct 27 09:47:47 2004
From: petr.pikal at precheza.cz (Petr Pikal)
Date: Wed, 27 Oct 2004 09:47:47 +0200
Subject: [R] integration problem
In-Reply-To: <417E7324.9090607@uni-jena.de>
Message-ID: <417F6EC3.21713.68A3D7@localhost>

Hi Christoph

I use this function to calculate an area under curve. It integrates 
along the whole wavelength set or you can restrict the calculation 
by setting dm and hm to some appropriate values. 

It also gives you two values. The first is total area to the x axis 
(y=0) and the second is only area below the curve and above line 
from min(x) to max(x).

integ1<-function (x,y,dm=-Inf,hm=+Inf)
{
if(dm==-Inf)dm<-min(x)
if(hm==+Inf)hm<-max(x)
vyber<-x<=hm&x>=dm
l<-length(x[vyber])
v<-diff(x[vyber])
z<-y[vyber][1:l-1]+y[vyber][2:l]
o<-z*v/2
osum<-sum(o)
o1<-
(y[x==min(x[vyber])]+y[x==max(x[vyber])])*(max(x[vyber])-
min(x[vyber]))/2
cista<-osum-o1
return(c(osum,cista))
}

I have also a function for specifying region for integration from 
plotted picture, if you are interested.

Cheers
Petr

On 26 Oct 2004 at 17:54, Christoph Scherber wrote:

> Dear R users,
> 
> I have spectral data (say, wavelength vs. extinction coefficient) for
> which I??d like to calculate an integral (i.e. the area underneath the
> curve).
> 
> Suppose the (artificial) dataset is
> 
> lambda 	E
> 1 	2
> 2 	4
> 3 	5
> 4 	8
> 5 	1
> 6 	5
> 7 	4
> 8 	9
> 9 	8
> 10 	2
> 
> 
> 
> How can I calculate an integral  for these values using R?
> 
> Many thanks for any help!
> Regards
> 
> Christoph
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide!
> http://www.R-project.org/posting-guide.html

Petr Pikal
petr.pikal at precheza.cz



From M.Mamin at intershop.de  Wed Oct 27 10:02:21 2004
From: M.Mamin at intershop.de (Marc Mamin)
Date: Wed, 27 Oct 2004 10:02:21 +0200
Subject: [R] regexp,grep: capturing more than one substring
Message-ID: <A03188C6623C0D46A703CB5AA59907F201C11C25@JENMAIL01.ad.intershop.net>


Hello,

I would like to have a function that retrieve matching strings in the same way as with java.util.regex (java 1.4.2).

Example:

f('^.*(xx?)\\.([0-9]*)$','abcxx.785')
=>
c('xx','785')

First of all: Is it possible to achiev this with grep(... perl=TRUE,value=TRUE )?


As I would call this function very often with large data, I'm reluctant to use Sjava for performance reasons.
Is this a wrong assumption that using Java directly would be slower or use more memory than to have a native R function?

Does someone already has a solution for this :)

Thanks,

Marc Mamin



From maechler at stat.math.ethz.ch  Wed Oct 27 10:02:34 2004
From: maechler at stat.math.ethz.ch (Martin Maechler)
Date: Wed, 27 Oct 2004 10:02:34 +0200
Subject: [R] integration problem
In-Reply-To: <417E7918.5060302@pdf.com>
References: <417E7324.9090607@uni-jena.de>
	<417E7918.5060302@pdf.com>
Message-ID: <16767.22042.457204.987442@gargle.gargle.HOWL>

>>>>> "Sundar" == Sundar Dorai-Raj <sundar.dorai-raj at pdf.com>
>>>>>     on Tue, 26 Oct 2004 11:19:36 -0500 writes:

    Sundar> Christoph Scherber wrote:
    >> Dear R users,
    >> 
    >> I have spectral data (say, wavelength vs. extinction coefficient) for 
    >> which I??d like to calculate an integral (i.e. the area underneath the 
    >> curve).
    >> 
    >> Suppose the (artificial) dataset is
    >> 
    >> lambda E
    >> 1     2
    >> 2     4
    >> 3     5
    >> 4     8
    >> 5     1
    >> 6     5
    >> 7     4
    >> 8     9
    >> 9     8
    >> 10    2
    >> 
    >> 
    >> 
    >> How can I calculate an integral  for these values using R?
    >> 
    >> Many thanks for any help!
    >> Regards
    >> 
    >> Christoph
    >> 

    Sundar> Hi Christoph,
    Sundar> You can write some code to do trapezoidal integration or use ?approx 
    Sundar> in the following manner:

    Sundar> f <- function(xnew, x, y) approx(x, y, xnew)$y
    Sundar> integrate(f, min(x$lambda), max(x$lambda), x = x$lambda, y = x$E)

    Sundar> where `x' is your data above. Using approx is
    Sundar> perhaps overkill but it works. A better solution
    Sundar> would be to use trapezoids or perhaps piecewise
    Sundar> linear integration. I don't know of a package that
    Sundar> has the latter approaches off the top of my head but
    Sundar> that doesn't mean they doesn't exist somewhere.

And then, it mighgt be slightly more satisfactory to use
spline() instead of approx() {smooth interpolation instead of linear}.
However, this is exactly an example where  
 approxfun() is much nicer than approx()  and
 splinefun() is much nicer than spline() :

I here give a script {output in comments}:

 x <- data.frame(lambda = 1:10, E = c(2,4,5,8,1, 5,4,9,8,2))

## Sundar's proposal
 f <- function(xnew, x, y) approx(x, y, xnew)$y
 integrate(f, min(x$lambda), max(x$lambda), x = x$lambda, y = x$E)
 ##--> 46 with absolute error < 0.0047

## Using the *fun() version

  f2 <- approxfun(x$lambda, x$E)
## or even
  f2 <- approxfun(x) # does work: 2-column data frame or matrix

  integrate(f2, min(x$lambda), max(x$lambda))
  ## same answer as above

  f3 <- splinefun(x$lambda, x$E)
  integrate(f3, min(x$lambda), max(x$lambda))
  ##--> 46.98437 with absolute error < 0.0046

## Note that you can do more than just integrate them:

plot(x, type = "b")
curve(f2(x), add = TRUE, col=2)
curve(f3(x), add = TRUE, col=3)
f4 <- splinefun(x$lambda, x$E, method ="natural")
curve(f4(x), add = TRUE, col=4)# not much difference

###-----

Further note, that only in R-patched (or R-devel),
we can also use 'splinefun(x)'
directly instead of 'splinefun(x$lambda, x$E)'.

Martin Maechler, ETH Zurich



From ligges at statistik.uni-dortmund.de  Wed Oct 27 10:11:48 2004
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Wed, 27 Oct 2004 10:11:48 +0200
Subject: [R] persp(), scatterplot3d(), "..." argument
In-Reply-To: <1098862790.16776.29.camel@biol102145.oulu.fi>
References: <a0600200ebda3cb456654@[139.166.242.29]>	
	<417F488C.7000405@statistik.uni-dortmund.de>
	<1098862790.16776.29.camel@biol102145.oulu.fi>
Message-ID: <417F5844.4040206@statistik.uni-dortmund.de>

Jari Oksanen wrote:

> On Wed, 2004-10-27 at 10:04, Uwe Ligges wrote:
> 
>>Robin Hankin wrote:
>>
>>
>>>Hello list.
>>>
>>>I very often need 3d scatterplots, and use scatterplot3D quite a lot.
>>>I am trying to modify persp() to plot scatterplots, and make use of
>>>the theta and phi arguments that persp() offers.  I am having some
>>>difficulty passing the correct arguments to persp().
>>>
>>>Here is my function so far.  Much of it is copied from the persp() manpage.
>>>work as expected, but all extra arguments are passed to persp() _and_
>>>lines() and this gives warnings.
> 
> ---cut---
> 
>>>
>>>QUESTION:
>>>
>>>What is best practice to handle this sort of problem?  Should I ignore
>>>the warnings() that this approach gives?  I can suppress them with
>>>options(warn= -Inf), but is this a good idea?  I've read section 10.4
>>>of AITR.
>>
>>Many of us have thought about this problem.
>>I'd suggest to introduce frequently used arguments to your meta-function 
>>(such as main, xlab, ylab, ...), but ignore warnings (note: warnings, 
>>not errors) if less frequently arguments are passed through "...".
>>
>>Alternatively, you can exclude "..." from lines() or points() and 
>>specify a list of arguments to be passed to that call.
> 
> 
> This is a larger problem if 
> 1. one of the underlying functions does not have "..."
> 2. you want to relay arguments to two or more underlying functions, and
> 3. you don't want to list all possible arguments in your function
> definition, since it is long enough already.
> 
> The solution is still there, but it is (black) magic. For instance,
> 'arrows' does not have "...", so you must add them with this magical
> mystery string:
> 
> formals(arrows) <- c(formals(arrows), alist(... = ))


You don't need it for simple things like:

   foo <- function(...){
       plot(1:10)
       arrows(1,1,7,7,...)
   }

foo(lwd=5) # works!




> Yes, this is documented in R manuals -- I wouldn't know this otherwise.
> Still, it would be nicer if 'arrows' had "..."...

As always, useful patches are welcome.

Uwe Ligges


> 
> cheers, jari oksanen



From vito_ricci at yahoo.com  Wed Oct 27 10:19:53 2004
From: vito_ricci at yahoo.com (Vito Ricci)
Date: Wed, 27 Oct 2004 10:19:53 +0200 (CEST)
Subject: [R] Skewness and Kurtosis
Message-ID: <20041027081953.83500.qmail@web41214.mail.yahoo.com>

Hi,

in which R-package I could find skewness and kurtosis
measures for a distribution?

I built some functions:

gamma1<-function(x)
{
m=mean(x)
n=length(x)
s=sqrt(var(x))
m3=sum((x-m)^3)/n
g1=m3/(s^3)
return(g1)
}

skewness<-function(x)
{
m=mean(x)
me=median(x)
s=sqrt(var(x))
sk=(m-me)/s
return(sk)
}

bowley<-function(x)
{
q<-as.vector(quantile(x,prob=c(.25,.50,.75)))
b=(q[3]+q[1]-2*q[2])/(q[3]-q[2])
return(b)
}

b3<-function(x)
{
m=mean(x)
me=median(x)
n=length(x)
d=sum(abs(x-me))/n
b=(m-me)/d
return(b)
}

but I'm looking for some already included in a
package.

Thanks in advance.
Best
Vito


=====
Diventare costruttori di soluzioni

"The business of the statistician is to catalyze 
the scientific learning process."  
George E. P. Box


Visitate il portale http://www.modugno.it/
e in particolare la sezione su Palese http://www.modugno.it/archivio/cat_palese.shtml



From Sebastian.Leuzinger at unibas.ch  Wed Oct 27 10:26:02 2004
From: Sebastian.Leuzinger at unibas.ch (Sebastian Leuzinger)
Date: Wed, 27 Oct 2004 10:26:02 +0200
Subject: [R] predicting x values in a polynomial regression
Message-ID: <417F5B9A.1070307@unibas.ch>

Dear R

I would like to predict x-values in a 4th order polynomial regression:

x <- c(1:10)
y <- c(2,7,19,49,89,94,97,98,92,89) # these are percentages

lm(y ~ x+I(x^2)+I(x^3)+I(x^4)-1) -> lm1

now I would like to know what the model fit (x-value) for y=50 is. This 
results in solving a 4th order quadratic equation. polyroot() does not 
really help because it only gives me the x-values for y=0. I have tried 
with nls() which sort of works, but I am sure there is a much easier 
solution to that, can anyone give me a hint?
-- 
Sebastian Leuzinger
Institute of Botany, University of Basel
Sch??nbeinstr. 6 CH-4056 Basel
Ph. 0041 (0) 61 267 3511
fax  0041 (0) 61 2673504
email: Sebastian.Leuzinger at unibas.ch <mailto:Sebastian.Leuzinger at unibas.ch>
web:  http://www.unibas.ch/botschoen/leuzinger/e.shtml 
<http://www.unibas.ch/botschoen/leuzinger/d.shtml>



From sun at cae.wisc.edu  Wed Oct 27 10:34:07 2004
From: sun at cae.wisc.edu (Sun)
Date: Wed, 27 Oct 2004 03:34:07 -0500
Subject: [R] ploting an ellipse keeps giving errors
Message-ID: <03ed01c4bbff$b9926450$23719792@star>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20041027/9bad8ea2/attachment.pl

From jarioksa at sun3.oulu.fi  Wed Oct 27 10:38:08 2004
From: jarioksa at sun3.oulu.fi (Jari Oksanen)
Date: 27 Oct 2004 11:38:08 +0300
Subject: [R] persp(), scatterplot3d(), "..." argument
In-Reply-To: <417F5844.4040206@statistik.uni-dortmund.de>
References: <a0600200ebda3cb456654@[139.166.242.29]>
	<417F488C.7000405@statistik.uni-dortmund.de>
	<1098862790.16776.29.camel@biol102145.oulu.fi>
	<417F5844.4040206@statistik.uni-dortmund.de>
Message-ID: <1098866288.16776.45.camel@biol102145.oulu.fi>

On Wed, 2004-10-27 at 11:11, Uwe Ligges wrote:
> Jari Oksanen wrote:
> 
> > On Wed, 2004-10-27 at 10:04, Uwe Ligges wrote:
> > 
> > 
> > This is a larger problem if 
> > 1. one of the underlying functions does not have "..."
> > 2. you want to relay arguments to two or more underlying functions, and
> > 3. you don't want to list all possible arguments in your function
> > definition, since it is long enough already.
> > 
> > The solution is still there, but it is (black) magic. For instance,
> > 'arrows' does not have "...", so you must add them with this magical
> > mystery string:
> > 
> > formals(arrows) <- c(formals(arrows), alist(... = ))
> 
> 
> You don't need it for simple things like:
> 
>    foo <- function(...){
>        plot(1:10)
>        arrows(1,1,7,7,...)
>    }
> 
> foo(lwd=5) # works!
> 
That's why I had point 2 above: it really would work with simpler
things. However, the following may fail:

> parrow <- 
function (x, y, ...)
{
    plot(x, y, ...)
    arrows(0, 0, x, y, ...)
    invisible()
}
> parrow(runif(10), runif(10), col="red") # works
> parrow(runif(10), runif(10), col="red", pch=16)
Error in arrows(0, 0, x, y, ...) : unused argument(s) (pch ...)

Adding formals would help.

> 
> As always, useful patches are welcome.
> 

I don't know if this counts as a "useful patch", but it is patch anyway:

diff -u2r old/arrows.R new/arrows.R
--- old/arrows.R        2004-10-27 11:32:25.000000000 +0300
+++ new/arrows.R        2004-10-27 11:32:53.000000000 +0300
@@ -1,5 +1,5 @@
 "arrows" <-
 function (x0, y0, x1, y1, length = 0.25, angle = 30, code = 2,
-    col = par("fg"), lty = NULL, lwd = par("lwd"), xpd = NULL)
+    col = par("fg"), lty = NULL, lwd = par("lwd"), xpd = NULL, ...)
 {
     .Internal(arrows(x0, y0, x1, y1, length = length, angle = angle,


cheers, jari oksanen
-- 
J.Oksanen, Oulu, Finland.
"Object-oriented programming is an exceptionally bad idea which could
only have originated in California." E. Dijkstra



From ligges at statistik.uni-dortmund.de  Wed Oct 27 10:52:04 2004
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Wed, 27 Oct 2004 10:52:04 +0200
Subject: [R] Skewness and Kurtosis
In-Reply-To: <20041027081953.83500.qmail@web41214.mail.yahoo.com>
References: <20041027081953.83500.qmail@web41214.mail.yahoo.com>
Message-ID: <417F61B4.2050302@statistik.uni-dortmund.de>

Vito Ricci wrote:

> Hi,
> 
> in which R-package I could find skewness and kurtosis
> measures for a distribution?

Both funcions are in package "e1071".

Uwe Ligges

> I built some functions:
> 
> gamma1<-function(x)
> {
> m=mean(x)
> n=length(x)
> s=sqrt(var(x))
> m3=sum((x-m)^3)/n
> g1=m3/(s^3)
> return(g1)
> }
> 
> skewness<-function(x)
> {
> m=mean(x)
> me=median(x)
> s=sqrt(var(x))
> sk=(m-me)/s
> return(sk)
> }
> 
> bowley<-function(x)
> {
> q<-as.vector(quantile(x,prob=c(.25,.50,.75)))
> b=(q[3]+q[1]-2*q[2])/(q[3]-q[2])
> return(b)
> }
> 
> b3<-function(x)
> {
> m=mean(x)
> me=median(x)
> n=length(x)
> d=sum(abs(x-me))/n
> b=(m-me)/d
> return(b)
> }
> 
> but I'm looking for some already included in a
> package.
> 
> Thanks in advance.
> Best
> Vito
> 
> 
> =====
> Diventare costruttori di soluzioni
> 
> "The business of the statistician is to catalyze 
> the scientific learning process."  
> George E. P. Box
> 
> 
> Visitate il portale http://www.modugno.it/
> e in particolare la sezione su Palese http://www.modugno.it/archivio/cat_palese.shtml
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html



From ligges at statistik.uni-dortmund.de  Wed Oct 27 10:54:34 2004
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Wed, 27 Oct 2004 10:54:34 +0200
Subject: [R] predicting x values in a polynomial regression
In-Reply-To: <417F5B9A.1070307@unibas.ch>
References: <417F5B9A.1070307@unibas.ch>
Message-ID: <417F624A.9080405@statistik.uni-dortmund.de>

Sebastian Leuzinger wrote:

> Dear R
> 
> I would like to predict x-values in a 4th order polynomial regression:
> 
> x <- c(1:10)
> y <- c(2,7,19,49,89,94,97,98,92,89) # these are percentages
> 
> lm(y ~ x+I(x^2)+I(x^3)+I(x^4)-1) -> lm1
> 
> now I would like to know what the model fit (x-value) for y=50 is. This 
> results in solving a 4th order quadratic equation. polyroot() does not 
> really help because it only gives me the x-values for y=0. I have tried 
> with nls() which sort of works, but I am sure there is a much easier 
> solution to that, can anyone give me a hint?

What about optim()?

Uwe Ligges



From ripley at stats.ox.ac.uk  Wed Oct 27 11:04:15 2004
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Wed, 27 Oct 2004 10:04:15 +0100 (BST)
Subject: [R] predicting x values in a polynomial regression
In-Reply-To: <417F5B9A.1070307@unibas.ch>
Message-ID: <Pine.LNX.4.44.0410271002160.31703-100000@gannet.stats>

On Wed, 27 Oct 2004, Sebastian Leuzinger wrote:

> I would like to predict x-values in a 4th order polynomial regression:
> 
> x <- c(1:10)
> y <- c(2,7,19,49,89,94,97,98,92,89) # these are percentages
> 
> lm(y ~ x+I(x^2)+I(x^3)+I(x^4)-1) -> lm1
> 
> now I would like to know what the model fit (x-value) for y=50 is. This 
> results in solving a 4th order quadratic equation. polyroot() does not 
> really help because it only gives me the x-values for y=0.

And what are the roots of p(x) - 50?

> I have tried with nls() which sort of works, but I am sure there is a
> much easier solution to that, can anyone give me a hint?

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From jarioksa at sun3.oulu.fi  Wed Oct 27 11:04:11 2004
From: jarioksa at sun3.oulu.fi (Jari Oksanen)
Date: 27 Oct 2004 12:04:11 +0300
Subject: [R] ploting an ellipse keeps giving errors
In-Reply-To: <03ed01c4bbff$b9926450$23719792@star>
References: <03ed01c4bbff$b9926450$23719792@star>
Message-ID: <1098867851.16776.62.camel@biol102145.oulu.fi>

On Wed, 2004-10-27 at 11:34, Sun wrote:
> library (ellipse)
> 
> shape1 = c (1, 0, 0,1)
> dim(shape1) = c(2,2)
> ellipse (center = c(0,0), shape = shape1, radius = 1)
> 
> =============================
> Error in plot.xy(xy.coords(x, y), type = type, col = col, lty = lty, ...) : 
>         plot.new has not been called yet
> 
> 
> It is really frustrating. Also what do the shape matrix, radius correspond to an ellipse function
> 
> (x-x0)^2/a + (y-y0)^2/b = 1
> 
> ? Please advise!

Sun, did you read the ?ellipse help page? I just read, but I didn't find
arguments 'center', 'shape' or 'radius' there. It could be useful to use
argument specified in the help page.  Section 'Details' of ?ellipse
explains the parametrization.

cheers, jari oksanen

-- 
Jari Oksanen <jarioksa at sun3.oulu.fi>



From petr.pikal at precheza.cz  Wed Oct 27 11:14:20 2004
From: petr.pikal at precheza.cz (Petr Pikal)
Date: Wed, 27 Oct 2004 11:14:20 +0200
Subject: [R] ploting an ellipse keeps giving errors
In-Reply-To: <03ed01c4bbff$b9926450$23719792@star>
Message-ID: <417F830C.15553.B79A61@localhost>

Hi

Did you read what ellipse does and how it shall be used?

And what about your system, R and ellipse version?

>From your example i got

> ellipse (center = c(0,0), shape = shape1, radius = 1)
Error in ellipse.default(center = c(0, 0), shape = shape1, radius = 
1) : 
        Argument "x" is missing, with no default

Ellipse is not for drawing arbitrary ellipses but has different usage.

See its help page and an example.

Cheers
Petr




On 27 Oct 2004 at 3:34, Sun wrote:

> 
> library (ellipse)
> 
> shape1 = c (1, 0, 0,1)
> dim(shape1) = c(2,2)
> ellipse (center = c(0,0), shape = shape1, radius = 1)
> 
> =============================
> Error in plot.xy(xy.coords(x, y), type = type, col = col, lty = lty,
> ...) : 
>         plot.new has not been called yet
> 
> 
> It is really frustrating. Also what do the shape matrix, radius
> correspond to an ellipse function
> 
> (x-x0)^2/a + (y-y0)^2/b = 1
> 
> ? Please advise!
> 
> Many thanks,
> 
> Sun
>  [[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide!
> http://www.R-project.org/posting-guide.html

Petr Pikal
petr.pikal at precheza.cz



From Ivy_Li at smics.com  Wed Oct 27 10:40:28 2004
From: Ivy_Li at smics.com (Ivy_Li)
Date: Wed, 27 Oct 2004 16:40:28 +0800
Subject: [R] Install Package(RODBC) warning , why?
Message-ID: <8A910F1818425847A6D18C7832D207E502AAD1@ex115.smic-sh.com>

 Hi everybody,
	Could I consult one problem?

	I want to access database with RODBC,  first download  this package from  http://www.microsoft.com/data/odbc as part of MDAC, the version is the newest version 2.7.0. But  after I have installed this Package(RODBC) and run command  libaray(RODBC), the R system report the fillowing warning information. 

	Error in eval(expr, envir, enclos) : couldn't find function "lazyLoad"
               In addition: Warning message: 
               package RODBC was built under R version 2.0.0 
               Error in library(RODBC) : package/namespace load failed

	why?
	Thanks for helping me!

Best Regards!
Ivy Li
YMS in Production & Testing
Semiconductor Manufactory International(ShangHai) Corporation
#18 ZhangJiang Road, PuDong New Area, Shanghai, China
Tel: 021-5080-2000 *11754
Email: Ivy_Li at smics.com



From sun at cae.wisc.edu  Wed Oct 27 11:08:47 2004
From: sun at cae.wisc.edu (Sun)
Date: Wed, 27 Oct 2004 04:08:47 -0500
Subject: [R] ploting an ellipse keeps giving errors
References: <03ed01c4bbff$b9926450$23719792@star>
	<417F61AB.40309@pburns.seanet.com>
Message-ID: <040b01c4bc04$91651b30$23719792@star>

Thanks a lot. I added that line.

library(ellipse)
plot(-2:2, type='n')
shape1 = c (1, 0, 0,1)
dim(shape1) = c(2,2)
center1 = c(0,0)
radius1 = 1
ellipse (center1, shape1, radius1)

But now it said:

Error in ellipse(center, shape, radius) : dim<- : dims [product 4] do not
match the length of object [200]

What is wrong?

I don't understand the shape and radius's meaning so I have no idea of what
object's lenght.

Many thanks!

Sun
----- Original Message ----- 
From: "Patrick Burns" <pburns at pburns.seanet.com>
To: "Sun" <sun at cae.wisc.edu>
Sent: Wednesday, October 27, 2004 3:51 AM
Subject: Re: [R] ploting an ellipse keeps giving errors


> The error message is telling you that it expects a plot to already be
> there.  You can do something like:
>
> plot(-2:2, type='n')
>
> Patrick Burns
>
> Burns Statistics
> patrick at burns-stat.com
> +44 (0)20 8525 0696
> http://www.burns-stat.com
> (home of S Poetry and "A Guide for the Unwilling S User")
>
> Sun wrote:
>
> >library (ellipse)
> >
> >shape1 = c (1, 0, 0,1)
> >dim(shape1) = c(2,2)
> >ellipse (center = c(0,0), shape = shape1, radius = 1)
> >
> >=============================
> >Error in plot.xy(xy.coords(x, y), type = type, col = col, lty = lty, ...)
:
> >        plot.new has not been called yet
> >
> >
> >It is really frustrating. Also what do the shape matrix, radius
correspond to an ellipse function
> >
> >(x-x0)^2/a + (y-y0)^2/b = 1
> >
> >? Please advise!
> >
> >Many thanks,
> >
> >Sun
> > [[alternative HTML version deleted]]
> >
> >______________________________________________
> >R-help at stat.math.ethz.ch mailing list
> >https://stat.ethz.ch/mailman/listinfo/r-help
> >PLEASE do read the posting guide!
http://www.R-project.org/posting-guide.html
> >
> >
> >
> >
>
>
>



From sun at cae.wisc.edu  Wed Oct 27 11:25:00 2004
From: sun at cae.wisc.edu (Sun)
Date: Wed, 27 Oct 2004 04:25:00 -0500
Subject: [R] ploting an ellipse keeps giving errors
References: <417F830C.15553.B79A61@localhost>
Message-ID: <042401c4bc06$d562bca0$23719792@star>

Thank you. I found there are two ellipses

1.
R2.0
library (car)

2.
R1.9 and R2.0
library (ellipse)

And they are different! I can't run 1.

But the 2. is kind of specialized for t-distribution confidence and so on.

I need to find a general ellipse for an ellipse equation like
(x-x0)^2/a + (y-y0)^2/b =1

. Since I used chi-square percentile not t. I am trying to obtain the large
sample 95% simultaneous confidence ellipse for two population means (say,
weight and height). The input are the two sample means and their
covariances.

Maybe I have to make my own ellipse function.

Sun
----- Original Message ----- 
From: "Petr Pikal" <petr.pikal at precheza.cz>
To: "Sun" <sun at cae.wisc.edu>
Cc: <r-help at stat.math.ethz.ch>
Sent: Wednesday, October 27, 2004 4:14 AM
Subject: Re: [R] ploting an ellipse keeps giving errors


> Hi
>
> Did you read what ellipse does and how it shall be used?
>
> And what about your system, R and ellipse version?
>
> From your example i got
>
> > ellipse (center = c(0,0), shape = shape1, radius = 1)
> Error in ellipse.default(center = c(0, 0), shape = shape1, radius =
> 1) :
>         Argument "x" is missing, with no default
>
> Ellipse is not for drawing arbitrary ellipses but has different usage.
>
> See its help page and an example.
>
> Cheers
> Petr
>
>
>
>
> On 27 Oct 2004 at 3:34, Sun wrote:
>
> >
> > library (ellipse)
> >
> > shape1 = c (1, 0, 0,1)
> > dim(shape1) = c(2,2)
> > ellipse (center = c(0,0), shape = shape1, radius = 1)
> >
> > =============================
> > Error in plot.xy(xy.coords(x, y), type = type, col = col, lty = lty,
> > ...) :
> >         plot.new has not been called yet
> >
> >
> > It is really frustrating. Also what do the shape matrix, radius
> > correspond to an ellipse function
> >
> > (x-x0)^2/a + (y-y0)^2/b = 1
> >
> > ? Please advise!
> >
> > Many thanks,
> >
> > Sun
> >  [[alternative HTML version deleted]]
> >
> > ______________________________________________
> > R-help at stat.math.ethz.ch mailing list
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide!
> > http://www.R-project.org/posting-guide.html
>
> Petr Pikal
> petr.pikal at precheza.cz
>
>
>



From jarioksa at sun3.oulu.fi  Wed Oct 27 11:33:22 2004
From: jarioksa at sun3.oulu.fi (Jari Oksanen)
Date: 27 Oct 2004 12:33:22 +0300
Subject: [R] ploting an ellipse keeps giving errors
In-Reply-To: <1098867851.16776.62.camel@biol102145.oulu.fi>
References: <03ed01c4bbff$b9926450$23719792@star>
	<1098867851.16776.62.camel@biol102145.oulu.fi>
Message-ID: <1098869602.16776.70.camel@biol102145.oulu.fi>

On Wed, 2004-10-27 at 12:04, Jari Oksanen wrote:
> On Wed, 2004-10-27 at 11:34, Sun wrote:
> > library (ellipse)

Here's your problem! See below.

> > shape1 = c (1, 0, 0,1)
> > dim(shape1) = c(2,2)
> > ellipse (center = c(0,0), shape = shape1, radius = 1)
> > 
> > =============================
> > Error in plot.xy(xy.coords(x, y), type = type, col = col, lty = lty, ...) : 
> >         plot.new has not been called yet
> > 
> > 
> > It is really frustrating. Also what do the shape matrix, radius correspond to an ellipse function
> > 
> > (x-x0)^2/a + (y-y0)^2/b = 1
> > 
> > ? Please advise!
> 
> Sun, did you read the ?ellipse help page? I just read, but I didn't find
> arguments 'center', 'shape' or 'radius' there. It could be useful to use
> argument specified in the help page.  Section 'Details' of ?ellipse
> explains the parametrization.
> 
Sun,

Actually the problem seems to be that you loaded library(ellipse), but
follow the instructions for function ellipse in library(car). Would this
help? (One additional note: ellipse::ellipse.default uses British
spelling for 'centre', but 'cent' would work both in ellipse::ellipse
and car::ellipse.)

cheers, jari oksanen
-- 
Jari Oksanen <jarioksa at sun3.oulu.fi>



From Charles.Annis at StatisticalEngineering.com  Wed Oct 27 11:50:45 2004
From: Charles.Annis at StatisticalEngineering.com (Charles Annis, P.E.)
Date: Wed, 27 Oct 2004 05:50:45 -0400
Subject: [R] ploting an ellipse keeps giving errors
In-Reply-To: <042401c4bc06$d562bca0$23719792@star>
Message-ID: <200410270950.i9R9oWNd007572@hypatia.math.ethz.ch>

I apologize to the original contributor whom I have forgotten, but here is a
function that does what you wish.  It is not mine and was contributed some
time ago.  I saved it because it is so useful.

##############################
ellipse.fn <- function(loc, cov, confidence = 0.95)
{
  A <- cov
  detA <- A[1, 1] * A[2, 2] - A[1, 2]^2
  dist <- sqrt(qchisq(confidence, 2))
  ylimit <- sqrt(A[2, 2]) * dist
  y <- seq( - ylimit, ylimit, 0.01 * ylimit)
  sqrt.discr <- sqrt(detA/A[2, 2]^2 * (A[2, 2] * dist^2 - y^2))
  sqrt.discr[c(1, length(sqrt.discr))] <- 0
  b <- loc[1] + A[1, 2]/A[2, 2] * y
  x1 <- b - sqrt.discr
  x2 <- b + sqrt.discr
  y <- loc[2] + y
  return(rbind(cbind(x1, y), cbind(rev(x2), rev(y))))
}

### example of ellipse
x <- 60 + 15 * rnorm( 200 )
y <- 50 + .8 * x + 10 * rnorm( 200 )
plot( x, y )
lines( ellipse.fn( c(mean(x),mean(y)), var(cbind(x,y)), .95 ), col=2 )
##############################

I pray that the originator will forgive my posting without attribution.



Charles Annis, P.E.
 
Charles.Annis at StatisticalEngineering.com
phone: 561-352-9699
eFax:  503-217-5849
http://www.StatisticalEngineering.com

-----Original Message-----
From: r-help-bounces at stat.math.ethz.ch
[mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Sun
Sent: Wednesday, October 27, 2004 5:25 AM
To: Petr Pikal
Cc: r-help at stat.math.ethz.ch
Subject: Re: [R] ploting an ellipse keeps giving errors

Thank you. I found there are two ellipses

1.
R2.0
library (car)

2.
R1.9 and R2.0
library (ellipse)

And they are different! I can't run 1.

But the 2. is kind of specialized for t-distribution confidence and so on.

I need to find a general ellipse for an ellipse equation like
(x-x0)^2/a + (y-y0)^2/b =1

. Since I used chi-square percentile not t. I am trying to obtain the large
sample 95% simultaneous confidence ellipse for two population means (say,
weight and height). The input are the two sample means and their
covariances.

Maybe I have to make my own ellipse function.

Sun
----- Original Message ----- 
From: "Petr Pikal" <petr.pikal at precheza.cz>
To: "Sun" <sun at cae.wisc.edu>
Cc: <r-help at stat.math.ethz.ch>
Sent: Wednesday, October 27, 2004 4:14 AM
Subject: Re: [R] ploting an ellipse keeps giving errors


> Hi
>
> Did you read what ellipse does and how it shall be used?
>
> And what about your system, R and ellipse version?
>
> From your example i got
>
> > ellipse (center = c(0,0), shape = shape1, radius = 1)
> Error in ellipse.default(center = c(0, 0), shape = shape1, radius =
> 1) :
>         Argument "x" is missing, with no default
>
> Ellipse is not for drawing arbitrary ellipses but has different usage.
>
> See its help page and an example.
>
> Cheers
> Petr
>
>
>
>
> On 27 Oct 2004 at 3:34, Sun wrote:
>
> >
> > library (ellipse)
> >
> > shape1 = c (1, 0, 0,1)
> > dim(shape1) = c(2,2)
> > ellipse (center = c(0,0), shape = shape1, radius = 1)
> >
> > =============================
> > Error in plot.xy(xy.coords(x, y), type = type, col = col, lty = lty,
> > ...) :
> >         plot.new has not been called yet
> >
> >
> > It is really frustrating. Also what do the shape matrix, radius
> > correspond to an ellipse function
> >
> > (x-x0)^2/a + (y-y0)^2/b = 1
> >
> > ? Please advise!
> >
> > Many thanks,
> >
> > Sun
> >  [[alternative HTML version deleted]]
> >
> > ______________________________________________
> > R-help at stat.math.ethz.ch mailing list
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide!
> > http://www.R-project.org/posting-guide.html
>
> Petr Pikal
> petr.pikal at precheza.cz
>
>
>

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide!
http://www.R-project.org/posting-guide.html



From ligges at statistik.uni-dortmund.de  Wed Oct 27 11:52:42 2004
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Wed, 27 Oct 2004 11:52:42 +0200
Subject: [R] Install Package(RODBC) warning , why?
In-Reply-To: <8A910F1818425847A6D18C7832D207E502AAD1@ex115.smic-sh.com>
References: <8A910F1818425847A6D18C7832D207E502AAD1@ex115.smic-sh.com>
Message-ID: <417F6FEA.4040602@statistik.uni-dortmund.de>

Ivy_Li wrote:

>  Hi everybody,
> 	Could I consult one problem?
> 
> 	I want to access database with RODBC,  first download  this package from  http://www.microsoft.com/data/odbc as part of MDAC, the version is the newest version 2.7.0. But  after I have installed this Package(RODBC) and run command  libaray(RODBC), the R system report the fillowing warning information. 
> 
> 	Error in eval(expr, envir, enclos) : couldn't find function "lazyLoad"
>                In addition: Warning message: 
>                package RODBC was built under R version 2.0.0 
>                Error in library(RODBC) : package/namespace load failed


So you have an R version < R-2.0.0, but using the RODBC package compiled
for R-2.0.0. That cannot work.
Either get a version of RODBC that fits to your version of R by
 install.packages("RODBC")
or upgrade to R-2.0.0!

Uwe Ligges



> 	why?
> 	Thanks for helping me!
> 
> Best Regards!
> Ivy Li
> YMS in Production & Testing
> Semiconductor Manufactory International(ShangHai) Corporation
> #18 ZhangJiang Road, PuDong New Area, Shanghai, China
> Tel: 021-5080-2000 *11754
> Email: Ivy_Li at smics.com
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html



From Charles.Annis at StatisticalEngineering.com  Wed Oct 27 11:53:35 2004
From: Charles.Annis at StatisticalEngineering.com (Charles Annis, P.E.)
Date: Wed, 27 Oct 2004 05:53:35 -0400
Subject: [R] ploting an ellipse keeps giving errors
In-Reply-To: <040b01c4bc04$91651b30$23719792@star>
Message-ID: <200410270953.i9R9rCQd008156@hypatia.math.ethz.ch>

Found it!

Here is the original ellipse function:

----------
From: 	Bishop, Lane[SMTP:lane.bishop at honeywell.com]
Sent: 	Monday, November 13, 2000 4:05 PM
To: 	'Jane_2_Xu at sbphrd.com'; s-news at wubios.wustl.edu
Subject: 	RE: [S] 95% ellipse region

Try this function:

ellipse <- function(loc, cov, confidence = 0.95)
{
  A <- cov
  detA <- A[1, 1] * A[2, 2] - A[1, 2]^2
  dist <- sqrt(qchisq(confidence, 2))
  ylimit <- sqrt(A[2, 2]) * dist
  y <- seq( - ylimit, ylimit, 0.01 * ylimit)
  sqrt.discr <- sqrt(detA/A[2, 2]^2 * (A[2, 2] * dist^2 - y^2))
  sqrt.discr[c(1, length(sqrt.discr))] <- 0
  b <- loc[1] + A[1, 2]/A[2, 2] * y
  x1 <- b - sqrt.discr
  x2 <- b + sqrt.discr
  y <- loc[2] + y
  return(rbind(cbind(x1, y), cbind(rev(x2), rev(y))))
}

Usage example:

x <- 60 + 15 * rnorm( 200 )
y <- 50 + .8 * x + 10 * rnorm( 200 )
plot( x, y )

lines( ellipse( c(mean(x),mean(y)), var(cbind(x,y)), .95 ),
   col=2 )

The ellipse is based on the estimated covaraince matrix, though, so it may
not cover exactly 95% of the actual data.


Charles Annis, P.E.
 
Charles.Annis at StatisticalEngineering.com
phone: 561-352-9699
eFax:  503-217-5849
http://www.StatisticalEngineering.com

-----Original Message-----
From: r-help-bounces at stat.math.ethz.ch
[mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Sun
Sent: Wednesday, October 27, 2004 5:09 AM
To: Patrick Burns
Cc: r-help at stat.math.ethz.ch
Subject: Re: [R] ploting an ellipse keeps giving errors

Thanks a lot. I added that line.

library(ellipse)
plot(-2:2, type='n')
shape1 = c (1, 0, 0,1)
dim(shape1) = c(2,2)
center1 = c(0,0)
radius1 = 1
ellipse (center1, shape1, radius1)

But now it said:

Error in ellipse(center, shape, radius) : dim<- : dims [product 4] do not
match the length of object [200]

What is wrong?

I don't understand the shape and radius's meaning so I have no idea of what
object's lenght.

Many thanks!

Sun
----- Original Message ----- 
From: "Patrick Burns" <pburns at pburns.seanet.com>
To: "Sun" <sun at cae.wisc.edu>
Sent: Wednesday, October 27, 2004 3:51 AM
Subject: Re: [R] ploting an ellipse keeps giving errors


> The error message is telling you that it expects a plot to already be
> there.  You can do something like:
>
> plot(-2:2, type='n')
>
> Patrick Burns
>
> Burns Statistics
> patrick at burns-stat.com
> +44 (0)20 8525 0696
> http://www.burns-stat.com
> (home of S Poetry and "A Guide for the Unwilling S User")
>
> Sun wrote:
>
> >library (ellipse)
> >
> >shape1 = c (1, 0, 0,1)
> >dim(shape1) = c(2,2)
> >ellipse (center = c(0,0), shape = shape1, radius = 1)
> >
> >=============================
> >Error in plot.xy(xy.coords(x, y), type = type, col = col, lty = lty, ...)
:
> >        plot.new has not been called yet
> >
> >
> >It is really frustrating. Also what do the shape matrix, radius
correspond to an ellipse function
> >
> >(x-x0)^2/a + (y-y0)^2/b = 1
> >
> >? Please advise!
> >
> >Many thanks,
> >
> >Sun
> > [[alternative HTML version deleted]]
> >
> >______________________________________________
> >R-help at stat.math.ethz.ch mailing list
> >https://stat.ethz.ch/mailman/listinfo/r-help
> >PLEASE do read the posting guide!
http://www.R-project.org/posting-guide.html
> >
> >
> >
> >
>
>
>

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide!
http://www.R-project.org/posting-guide.html



From ripley at stats.ox.ac.uk  Wed Oct 27 11:59:29 2004
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Wed, 27 Oct 2004 10:59:29 +0100 (BST)
Subject: [R] Install Package(RODBC) warning , why?
In-Reply-To: <8A910F1818425847A6D18C7832D207E502AAD1@ex115.smic-sh.com>
Message-ID: <Pine.LNX.4.44.0410271054460.31828-100000@gannet.stats>

You are trying to use a package built for R 2.0.0 under R < 2.0.0.

How you managed to do that we have no idea, as RODBC is not available on 
http://www.microsoft.com/data/odbc!  Almost certainly you got it from the 
wrong place, from CRAN/bin/windows/contrib/2.0/, so please do read
the rw-FAQ and get the correct version.

The posting guide asks you to give your R platform and version, which you 
have (again) not done.


On Wed, 27 Oct 2004, Ivy_Li wrote:

>  Hi everybody,
> 	Could I consult one problem?

You mean `yet another problem', surely?

> 	I want to access database with RODBC, first download this package
> from http://www.microsoft.com/data/odbc as part of MDAC, the version is
> the newest version 2.7.0. But after I have installed this Package(RODBC)
> and run command libaray(RODBC), the R system report the fillowing
> warning information.
> 
> 	Error in eval(expr, envir, enclos) : couldn't find function "lazyLoad"
>                In addition: Warning message: 
>                package RODBC was built under R version 2.0.0 
>                Error in library(RODBC) : package/namespace load failed
> 
> 	why?
> 	Thanks for helping me!

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From ram at global-analytics.com  Wed Oct 27 12:05:30 2004
From: ram at global-analytics.com (ram@global-analytics.com)
Date: Wed, 27 Oct 2004 03:05:30 -0700
Subject: [R] SJava loading --error!
Message-ID: <20041027100530.26327.qmail@webmail07.mesa1.secureserver.net>

Hi list,
 
     I am having a problem in SJava. while loading the library I get the
folowing error.

> library(SJava)
Error in dyn.load(x, as.logical(local), as.logical(now)) :
        unable to load shared library
"/usr/lib/R/library/SJava/libs/SJava.so":
  libRSNativeJava.so: cannot open shared object file: No such file or
directory
Error in library(SJava) : .First.lib failed for 'SJava'
>

 I have j2re1.4.2_03 and  j2sdk1.4.2_02 installed in my system. I am
using fedora core 2. I did set JAVA_HOME path.
I don't know where I went wrong. If anyone done it sucessfully can help
me in this case.
Thanks in advance.



 Regards
S.Ram Mohan



From ripley at stats.ox.ac.uk  Wed Oct 27 12:58:38 2004
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Wed, 27 Oct 2004 11:58:38 +0100 (BST)
Subject: [R] SJava loading --error!
In-Reply-To: <20041027100530.26327.qmail@webmail07.mesa1.secureserver.net>
Message-ID: <Pine.LNX.4.44.0410271155400.31938-100000@gannet.stats>

On Wed, 27 Oct 2004 ram at global-analytics.com wrote:

>      I am having a problem in SJava. while loading the library I get the
> folowing error.
> 
> > library(SJava)
> Error in dyn.load(x, as.logical(local), as.logical(now)) :
>         unable to load shared library
> "/usr/lib/R/library/SJava/libs/SJava.so":
>   libRSNativeJava.so: cannot open shared object file: No such file or
> directory
> Error in library(SJava) : .First.lib failed for 'SJava'
> >
> 
>  I have j2re1.4.2_03 and  j2sdk1.4.2_02 installed in my system. I am
> using fedora core 2. I did set JAVA_HOME path.
> I don't know where I went wrong. If anyone done it sucessfully can help
> me in this case.

Did you use one of the scripts that SJava creates to start R?  If you do

R CMD ldd /usr/lib/R/library/SJava/libs/SJava.so

you will be able to ascertain what is wrong with your library paths, and 
so fix it.

Please do read the posting guide and provide basic information like your 
version of R and SJava.  There's a considerable potential for mismatch.

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From iwhite at staffmail.ed.ac.uk  Wed Oct 27 14:52:05 2004
From: iwhite at staffmail.ed.ac.uk (I M S White)
Date: Wed, 27 Oct 2004 13:52:05 +0100 (BST)
Subject: [R] se.contrast
Message-ID: <Pine.GSO.4.58.0410271346580.10763@holyrood.ed.ac.uk>

After a one-way anova, se.contrast computes the standard error of a
contrast, but not the value of the contrast itself. Wouldn't this be
useful? Am I missing something?

======================================
I.White
ICAPB, University of Edinburgh
Ashworth Laboratories, West Mains Road
Edinburgh EH9 3JT
Fax: 0131 650 6564  Tel: 0131 650 5490
E-mail: iwhite at staffmail.ed.ac.uk



From jfox at mcmaster.ca  Wed Oct 27 15:24:44 2004
From: jfox at mcmaster.ca (John Fox)
Date: Wed, 27 Oct 2004 09:24:44 -0400
Subject: [R] ploting an ellipse keeps giving errors
In-Reply-To: <042401c4bc06$d562bca0$23719792@star>
Message-ID: <20041027132443.ZCHK1536.tomts25-srv.bellnexxia.net@JohnDesktop8300>

Dear Sun,

There are indeed (at least) two ellipse functions, and the one in the car
package has different arguments from the one in the ellipse package.

You should be able to use ellipse() in either package to do what you want;
use the t argument for the version in ellipse or the radius argument for the
version in car, setting this to the square-root of the critical chisquare or
F. (ellipse in car works by deforming a circle.) If height and weight are
from the same sample, then they won't be independent, so, e.g., the shape
argument to ellipse in car would be their sample covariance matrix. The
center argument to ellipse() in car and the centre argument to ellipse() in
ellipse should be the vector of means.

But the ellipse() function in ellipse and the data.ellipse() function in car
already does all this, so why do you feel that you need to roll your own (or
perhaps I misunderstand what you want)? 

I hope this helps.
 John

--------------------------------
John Fox
Department of Sociology
McMaster University
Hamilton, Ontario
Canada L8S 4M4
905-525-9140x23604
http://socserv.mcmaster.ca/jfox 
-------------------------------- 

> -----Original Message-----
> From: r-help-bounces at stat.math.ethz.ch 
> [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Sun
> Sent: Wednesday, October 27, 2004 4:25 AM
> To: Petr Pikal
> Cc: r-help at stat.math.ethz.ch
> Subject: Re: [R] ploting an ellipse keeps giving errors
> 
> Thank you. I found there are two ellipses
> 
> 1.
> R2.0
> library (car)
> 
> 2.
> R1.9 and R2.0
> library (ellipse)
> 
> And they are different! I can't run 1.
> 
> But the 2. is kind of specialized for t-distribution 
> confidence and so on.
> 
> I need to find a general ellipse for an ellipse equation like 
> (x-x0)^2/a + (y-y0)^2/b =1
> 
> . Since I used chi-square percentile not t. I am trying to 
> obtain the large sample 95% simultaneous confidence ellipse 
> for two population means (say, weight and height). The input 
> are the two sample means and their covariances.
> 
> Maybe I have to make my own ellipse function.
> 
> Sun
> ----- Original Message -----
> From: "Petr Pikal" <petr.pikal at precheza.cz>
> To: "Sun" <sun at cae.wisc.edu>
> Cc: <r-help at stat.math.ethz.ch>
> Sent: Wednesday, October 27, 2004 4:14 AM
> Subject: Re: [R] ploting an ellipse keeps giving errors
> 
> 
> > Hi
> >
> > Did you read what ellipse does and how it shall be used?
> >
> > And what about your system, R and ellipse version?
> >
> > From your example i got
> >
> > > ellipse (center = c(0,0), shape = shape1, radius = 1)
> > Error in ellipse.default(center = c(0, 0), shape = shape1, radius =
> > 1) :
> >         Argument "x" is missing, with no default
> >
> > Ellipse is not for drawing arbitrary ellipses but has 
> different usage.
> >
> > See its help page and an example.
> >
> > Cheers
> > Petr
> >
> >
> >
> >
> > On 27 Oct 2004 at 3:34, Sun wrote:
> >
> > >
> > > library (ellipse)
> > >
> > > shape1 = c (1, 0, 0,1)
> > > dim(shape1) = c(2,2)
> > > ellipse (center = c(0,0), shape = shape1, radius = 1)
> > >
> > > =============================
> > > Error in plot.xy(xy.coords(x, y), type = type, col = col, 
> lty = lty,
> > > ...) :
> > >         plot.new has not been called yet
> > >
> > >
> > > It is really frustrating. Also what do the shape matrix, radius
> > > correspond to an ellipse function
> > >
> > > (x-x0)^2/a + (y-y0)^2/b = 1
> > >
> > > ? Please advise!
> > >
> > > Many thanks,
> > >
> > > Sun
> > >  [[alternative HTML version deleted]]
> > >
> > > ______________________________________________
> > > R-help at stat.math.ethz.ch mailing list
> > > https://stat.ethz.ch/mailman/listinfo/r-help
> > > PLEASE do read the posting guide!
> > > http://www.R-project.org/posting-guide.html
> >
> > Petr Pikal
> > petr.pikal at precheza.cz
> >
> >
> >
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html



From henric.nilsson at statisticon.se  Wed Oct 27 15:30:12 2004
From: henric.nilsson at statisticon.se (Henric Nilsson)
Date: Wed, 27 Oct 2004 15:30:12 +0200
Subject: [R] Newbie question about the use of lm and anova
In-Reply-To: <2B4F16E58B329B40A8775C2C07B9D579824194@EXCL01.exelixis.com
 >
References: <2B4F16E58B329B40A8775C2C07B9D579824194@EXCL01.exelixis.com>
Message-ID: <6.1.2.0.0.20041027125935.04dc8cf0@10.0.10.66>

At 12:54 2004-10-26 -0700, Sandie Peters wrote:

>[...]
>Several questions:
>1.  Why do we get a different ANOVA answer when we switch the order of
>the explanatory variables , e.g. ComponentA ~ Parent + Plot vs
>ComponentA ~ Plot + Parent?
>2.  Why are the Sum Sq and F ratios calculated by this ANOVA method
>different than those that are calculated by SAS JMP?

It's in the FAQ:
http://cran.r-project.org/doc/FAQ/R-FAQ.html#Why-does-the-output-from-anova_0028_0029-depend-on-the-order-of-factors-in-the-model_003f

Also, see the `anova.lm' help page for a nice example.

If you want test based on so called Type II and III sums of squares, but 
don't want to compute them yourself (by use of `anova' and `drop1' as the 
FAQ suggests), John Fox's excellent `car' package has got an `Anova' function.

HTH,
Henric



From ripley at stats.ox.ac.uk  Wed Oct 27 15:32:14 2004
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Wed, 27 Oct 2004 14:32:14 +0100 (BST)
Subject: [R] se.contrast
In-Reply-To: <Pine.GSO.4.58.0410271346580.10763@holyrood.ed.ac.uk>
Message-ID: <Pine.LNX.4.44.0410271421490.26604-100000@gannet.stats>

On Wed, 27 Oct 2004, I M S White wrote:

> After a one-way anova, se.contrast computes the standard error of a
> contrast, but not the value of the contrast itself. Wouldn't this be
> useful? Am I missing something?

Compatibility with S, plus `does what it says on the box'.  It was written
to make some of the MASS code work under R.

If you want the contrast and its se, try estimable() in package gmodels
(in gregmisc).

se.contrast would be more useful if anyone ever wrote an aovlist method.
(But, 6 years later, I still have not, and rarely missed it.)

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From ales.ziberna at guest.arnes.si  Wed Oct 27 15:34:31 2004
From: ales.ziberna at guest.arnes.si (=?iso-8859-2?Q?Ale=B9_=AEiberna?=)
Date: Wed, 27 Oct 2004 15:34:31 +0200
Subject: [R] ploting axes and rotating strings
Message-ID: <004401c4bc29$e701c370$1909f9c2@ales>

Hello!

I have two question that rose from trying to tacle the same problem in two 
differnet ways.

What I want to do is to plot axes (only values or labels, no tick marks) in 
such a way that 'cex' can be very small, text can be perpendicular to the 
axis (as in axis(las=2) ) and the text is stil at the right position.

Let me demonstrate with a small example:
plot(0:100/100,0:100/100,axes=FALSE)
axis(side=3,at=0:100/100,labels=0:100,las=2,cex.axis=0.1)

The text on the axis should be at the same points as tick marks, but it is 
shifted right.
I tried to bypas the problem by using function 'text', which puts the text 
at the correct positions, however, it has another problem. I would like to 
rotate the text, but if I use 'str=90' (to rotate the text for 90 degrees), 
I get a warning massage:
Warning message:
parameter "str" couldn't be set in high-level plot() function

This suprises me, since if I read the help correctly ('?par'), then this 
should not happen.

So if anyone can either hepl me to rotate the text in 'text' or to correctly 
plot the text in 'axis', I would be very grateful.

Ales



From ggrothendieck at myway.com  Wed Oct 27 16:02:17 2004
From: ggrothendieck at myway.com (Gabor Grothendieck)
Date: Wed, 27 Oct 2004 14:02:17 +0000 (UTC)
Subject: [R] regexp,grep: capturing more than one substring
References: <A03188C6623C0D46A703CB5AA59907F201C11C25@JENMAIL01.ad.intershop.net>
Message-ID: <loom.20041027T155101-855@post.gmane.org>

Marc Mamin <M.Mamin <at> intershop.de> writes:

: 
: Hello,
: 
: I would like to have a function that retrieve matching strings in the same 
way as with java.util.regex (java 1.4.2).
: 
: Example:
: 
: f('^.*(xx?)\\.([0-9]*)$','abcxx.785')
: =>
: c('xx','785')
: 
: First of all: Is it possible to achiev this with grep(... 
perl=TRUE,value=TRUE )?

Actually you don't even need perl= to do that.  The
function below pastes togther a string like "\\1 \\2" 
where n determines how many of them there are.  
Then it uses gsub with the regexp in r.  Finally it is
split into individual strings.

The calculation of n, the number of backreferences, is
not foolproof so you can specify your own n if your
expression has parentheses that are not backreferences.
Also specifying n might speed it up a bit, e.g. n = 2
in the example.  The value of sep= should be a delimiter
not in your string.

s can be a vector of strings.  It returns in a list of
strings in any case, one element of the list for each
element of vector s.  If s is just a scalar string
then it will return a one element list containing
the elements as a vector.  You may wish to call it
like this f(...args...)[[1]] in that case as
shown in the example.

f <- function(r, s, n = nchar(gsub("[^(]","",r)), sep = "\10" ) {
    x <- gsub(r, paste("\\", 1:n, sep = "", collapse = sep), s)
    strsplit(x, split = sep)
}
f( '^.*(xx?)\\.([0-9]*)$', 'abcxx.785' )[[1]]



From MSchwartz at MedAnalytics.com  Wed Oct 27 16:29:24 2004
From: MSchwartz at MedAnalytics.com (Marc Schwartz)
Date: Wed, 27 Oct 2004 09:29:24 -0500
Subject: [R] ploting axes and rotating strings
In-Reply-To: <004401c4bc29$e701c370$1909f9c2@ales>
References: <004401c4bc29$e701c370$1909f9c2@ales>
Message-ID: <1098887363.6876.24.camel@localhost.localdomain>

On Wed, 2004-10-27 at 08:34, Ale?? ??iberna wrote:
> Hello!
> 
> I have two question that rose from trying to tacle the same problem in two 
> differnet ways.
> 
> What I want to do is to plot axes (only values or labels, no tick marks) in 
> such a way that 'cex' can be very small, text can be perpendicular to the 
> axis (as in axis(las=2) ) and the text is stil at the right position.
> 
> Let me demonstrate with a small example:
> plot(0:100/100,0:100/100,axes=FALSE)
> axis(side=3,at=0:100/100,labels=0:100,las=2,cex.axis=0.1)
> 
> The text on the axis should be at the same points as tick marks, but it is 
> shifted right.
> I tried to bypas the problem by using function 'text', which puts the text 
> at the correct positions, however, it has another problem. I would like to 
> rotate the text, but if I use 'str=90' (to rotate the text for 90 degrees), 
> I get a warning massage:
> Warning message:
> parameter "str" couldn't be set in high-level plot() function
> 
> This suprises me, since if I read the help correctly ('?par'), then this 
> should not happen.
> 
> So if anyone can either hepl me to rotate the text in 'text' or to correctly 
> plot the text in 'axis', I would be very grateful.
> 
> Ales

I am not able to replicate the right-shift of the text labels. It may be
a visual screen artifact of the small font size you are using. If you
increase the cex.axis to 0.25 (for example), it becomes clear that the
labels are in the proper position.

I put up a PDF containing the plot at:

www.MedAnalytics.com/Rplots.pdf

I used the following code:

pdf()
plot(0:100/100, 0:100/100, axes = FALSE)
axis(side = 3, at = 0:100/100, labels = 0:100, las = 2, cex.axis = 0.1)
axis(side = 1, at = 0:100/100, labels = 0:100, las = 2, cex.axis = 0.25)
dev.off()

I'll leave that up for a bit for review. I presume that you have the
Acrobat Reader under Windows?

Relative to the use of text() for rotating axis labels, see:

http://cran.r-project.org/doc/FAQ/R-FAQ.html#How-can-I-create-rotated-axis-labels_003f

HTH,

Marc Schwartz



From partha_bagchi at hgsi.com  Wed Oct 27 16:36:41 2004
From: partha_bagchi at hgsi.com (partha_bagchi@hgsi.com)
Date: Wed, 27 Oct 2004 10:36:41 -0400
Subject: [R] ploting axes and rotating strings
Message-ID: <OFA102CC8D.DF034C1E-ON85256F3A.00503146-85256F3A.00504387@hgsi.com>

Have you tried the adj parameter in par as in 

axis(side=3,at=0:100/100,labels=0:100,las=2,cex.axis=0.1, adj = 0.5)

?







Ales Ziberna <ales.ziberna at guest.arnes.si>
Sent by: r-help-bounces at stat.math.ethz.ch
10/27/2004 09:34 AM

 
        To:     "R-help" <r-help at stat.math.ethz.ch>
        cc: 
        Subject:        [R] ploting axes and rotating strings


Hello!

I have two question that rose from trying to tacle the same problem in two
differnet ways.

What I want to do is to plot axes (only values or labels, no tick marks) 
in
such a way that 'cex' can be very small, text can be perpendicular to the
axis (as in axis(las=2) ) and the text is stil at the right position.

Let me demonstrate with a small example:
plot(0:100/100,0:100/100,axes=FALSE)
axis(side=3,at=0:100/100,labels=0:100,las=2,cex.axis=0.1)

The text on the axis should be at the same points as tick marks, but it is
shifted right.
I tried to bypas the problem by using function 'text', which puts the text
at the correct positions, however, it has another problem. I would like to
rotate the text, but if I use 'str=90' (to rotate the text for 90 
degrees),
I get a warning massage:
Warning message:
parameter "str" couldn't be set in high-level plot() function

This suprises me, since if I read the help correctly ('?par'), then this
should not happen.

So if anyone can either hepl me to rotate the text in 'text' or to 
correctly
plot the text in 'axis', I would be very grateful.

Ales

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html



From ismith at mango-solutions.com  Wed Oct 27 17:18:14 2004
From: ismith at mango-solutions.com (Ian Smith)
Date: Wed, 27 Oct 2004 16:18:14 +0100
Subject: [R] ploting axes and rotating strings
In-Reply-To: <004401c4bc29$e701c370$1909f9c2@ales>
Message-ID: <200410271518.i9RFIJ9v026462@hypatia.math.ethz.ch>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20041027/4ffaa232/attachment.pl

From r-help.20.stefan817 at spamgourmet.com  Wed Oct 27 20:05:26 2004
From: r-help.20.stefan817 at spamgourmet.com (r-help.20.stefan817@spamgourmet.com)
Date: Wed, 27 Oct 2004 18:05:26 GMT
Subject: [R] Data-Mining with R vs. ACL
Message-ID: <200410271805.i9RI5Q519866@www5.newcon.de>


Hi,

as a Newbie with R and former user of ACL/Monarch I have a lot of questions, which I can\'t clear despite all the documentation. 

-Read Data in
How can I 
--Filter records from a file (exclude recurring headlines)
--Filter single fields from complex reports (cut information out of the report like a mask)
--Bond records consisting of several lines in plain report file
--Possibility to evaluate data files instead of importing them into R (solution for very big plain files > 2 GB)


-Analyse Data
How can I
--classify/summarize data with key field as in ACL (generate a result list with the grouped field and the result field)
--Join two tables as in SQL (outer/inner join between two tables)

I did read the posting guide carefully and a lot of documents, but can\'t find the answers - I apologize for all inconveniences if I have overlooked something. 

Greetings
Stefan

P.S. Is there somebody else who has experiences in ACL as well (Quickstart in R after a change from ACL)?



From kshe4 at student.monash.edu  Wed Oct 27 18:59:15 2004
From: kshe4 at student.monash.edu (Kunal Shetty)
Date: Wed, 27 Oct 2004 16:59:15 +0000
Subject: [R] Random number
Message-ID: <220.253.26.176.1098894590.93761@my.monash.edu.au>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20041027/4fe552d9/attachment.pl

From partha_bagchi at hgsi.com  Wed Oct 27 19:14:33 2004
From: partha_bagchi at hgsi.com (partha_bagchi@hgsi.com)
Date: Wed, 27 Oct 2004 13:14:33 -0400
Subject: [R] Random number
Message-ID: <OF63F1CBA3.A9977EFC-ON85256F3A.005EA3EF-85256F3A.005EB7CB@hgsi.com>

Let's say x is of size 100.

x <- rnorm(100)
x[sample(1:100,10)] <- NA 






Kunal Shetty <kshe4 at student.monash.edu>
Sent by: r-help-bounces at stat.math.ethz.ch
10/27/2004 12:59 PM

 
        To:     r-help at stat.math.ethz.ch
        cc: 
        Subject:        [R] Random number


Dear R- User

I created  two random number sets using rnorm function and calcuted their 
respective means. However now I would like to randomly
replace some values with NA. Thus create my own test data.
Any help or suggestions on this ?


Also wanted to confirm when multiplying two random number vectors x am y 
by matrix..is this how i do it.
A is the matrix

z <- c(x,y)    # x and y the two set of vectors

w <- A%*%Z   # each element in each vector multipled by the matrix .


regards
Kunal

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html



From sundar.dorai-raj at PDF.COM  Wed Oct 27 19:57:17 2004
From: sundar.dorai-raj at PDF.COM (Sundar Dorai-Raj)
Date: Wed, 27 Oct 2004 12:57:17 -0500
Subject: [R] Random number
In-Reply-To: <220.253.26.176.1098894590.93761@my.monash.edu.au>
References: <220.253.26.176.1098894590.93761@my.monash.edu.au>
Message-ID: <417FE17D.3040708@pdf.com>



Kunal Shetty wrote:

> Dear R- User
> 
>     I created  two random number sets using rnorm function and calcuted their respective means. However now I would like to randomly
> replace some values with NA. Thus create my own test data.
>       Any help or suggestions on this ?
> 
> 

Use ?sample:
n <- 100
x <- rnorm(n)
x[sample(n, 4)] <- NA
sum(is.na(x))
# [1] 4

>  Also wanted to confirm when multiplying two random number vectors x am y by matrix..is this how i do it.
> A is the matrix
>         
>        z <- c(x,y)    # x and y the two set of vectors
>     
>        w <- A%*%Z   # each element in each vector multipled by the matrix .
> 

(Be careful: R is case sensitive (z != Z).)

I'm not really clear what you want here. "%*%" is matrix multiplication 
and "*" is elementwise multiplication. Also using "c" makes a vector or 
length "n = length(x) + length(y)". This implies that "A" above "p x n" 
(i.e. p rows, n columns) and the result "w" would be "p x 1". Please 
provide an example of what you expect to see by the multiplication.

--sundar

> 
> regards
> Kunal
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html



From kshe4 at student.monash.edu  Wed Oct 27 20:29:50 2004
From: kshe4 at student.monash.edu (Kunal Shetty)
Date: Wed, 27 Oct 2004 18:29:50 +0000
Subject: [R] Random number
References: <417FE17D.3040708@pdf.com>
Message-ID: <220.253.2.55.1098901760.09353@my.monash.edu.au>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20041027/fb61e67a/attachment.pl

From sundar.dorai-raj at PDF.COM  Wed Oct 27 20:37:57 2004
From: sundar.dorai-raj at PDF.COM (Sundar Dorai-Raj)
Date: Wed, 27 Oct 2004 13:37:57 -0500
Subject: [R] Random number
In-Reply-To: <220.253.2.55.1098901760.09353@my.monash.edu.au>
References: <417FE17D.3040708@pdf.com>
	<220.253.2.55.1098901760.09353@my.monash.edu.au>
Message-ID: <417FEB05.5080405@pdf.com>



Kunal Shetty wrote:

> thank you Sundar,Andy, Partha for your prompt reply.
> 
>       as for the "%*%" is matrix multiplication problem sunder here is a sample of what i want...
> 
> x <- rnorm(100,17,24) # X values
> y <- rnorm(100,7,11)  # Y values
> 
> # since X and Y values are independent to get their
> # covariance need to multiply each of them with a Matrix say A
> 
> A <- matrix(c(10,25,8,40),nrow=2,ncol=2)
> 
>  further  i would like to assign both x and y vectors to one variale
> 
>  say z <- c(x,y)
> 
>   but if i do this  my matrix multiplication fail
> 
>     i.e  w <- A%*%z
>  Error in A %*% z : non-conformable arguments ??
> 
> 
>     so it tired multiply the  X and Y vector individually
> 
>  x<- (A%*%x)
>  y<- (A%*%y)
> 
>  
>  Error in A %*% z : non-conformable arguments ??
> 
> 
>   the error persist...sunder...
> 
>   or anybody who could direct me..?
> 

Of course this doesn't work. `A' is 2x2, `x' and `y' are 100x1, and `z' 
is 200x1 (I think I even mentioned this previously). I *think* you want 
something like:

z <- rbind(x, y) # now 2 x 100

w <- A %*% z # 2 x 100

If not, please provide an example of what you expect, possibly worked by 
hand.

HTH,

--sundar

> regards
> Kunal
> 
> 
> 
> 
> 
> Sundar Dorai-Raj <sundar.dorai-raj at PDF.COM> wrote:
> 
>>
>>Kunal Shetty wrote:
>>
>>
>>>Dear R- User
>>>
>>>    I created  two random number sets using rnorm function and calcuted their respective means. However now I would like to randomly
>>>replace some values with NA. Thus create my own test data.
>>>      Any help or suggestions on this ?
>>>
>>>
>>
>>Use ?sample:
>>n <- 100
>>x <- rnorm(n)
>>x[sample(n, 4)] <- NA
>>sum(is.na(x))
>># [1] 4
>>
>>
>>> Also wanted to confirm when multiplying two random number vectors x am y by matrix..is this how i do it.
>>>A is the matrix
>>>        
>>>       z <- c(x,y)    # x and y the two set of vectors
>>>    
>>>       w <- A%*%Z   # each element in each vector multipled by the matrix .
>>>
>>
>>(Be careful: R is case sensitive (z != Z).)
>>
>>I'm not really clear what you want here. "%*%" is matrix multiplication
>>and "*" is elementwise multiplication. Also using "c" makes a vector or
>>length "n = length(x) + length(y)". This implies that "A" above "p x n"
>>(i.e. p rows, n columns) and the result "w" would be "p x 1". Please 
>>provide an example of what you expect to see by the multiplication.
>>
>>--sundar
>>
>>
>>>regards
>>>Kunal
>>>
>>>______________________________________________
>>>R-help at stat.math.ethz.ch mailing list
>>>https://stat.ethz.ch/mailman/listinfo/r-help
>>>PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html



From jvega at banxico.org.mx  Wed Oct 27 20:47:34 2004
From: jvega at banxico.org.mx (=?iso-8859-1?Q?De_la_Vega_G=F3ngora_Jorge?=)
Date: Wed, 27 Oct 2004 13:47:34 -0500
Subject: [R] Warning messages in function fitdistr (library:MASS)
Message-ID: <F74A1EABDCCFFB4893FD93C96408F58AB4E613@BMCORREO.banxico.org.mx>

Why the warning messages (2:4)?

> x <- rexp(1000,0.2)
> fitdistr(x,"exponential",list(rate=1))
      rate    
  0.219824219 
 (0.006951308)
Warning messages: 
1: one-diml optimization by Nelder-Mead is unreliable: use optimize in: optim(start, mylogfn, x = x, hessian = TRUE, ...) 
2: NaNs produced in: dexp(x, 1/rate, log) 
3: NaNs produced in: dexp(x, 1/rate, log) 
4: NaNs produced in: dexp(x, 1/rate, log) 

and with respect to this function, it is curious that only for the normal family the mle is obtained in closed form. Why not in other cases like the exponential?

Using R 1.9.1 or 2.0.0 under Windows.

Jorge



From drf5n at maplepark.com  Wed Oct 27 21:16:17 2004
From: drf5n at maplepark.com (David Forrest)
Date: Wed, 27 Oct 2004 14:16:17 -0500 (CDT)
Subject: [R] Random number
In-Reply-To: <220.253.2.55.1098901760.09353@my.monash.edu.au>
References: <417FE17D.3040708@pdf.com>
	<220.253.2.55.1098901760.09353@my.monash.edu.au>
Message-ID: <Pine.LNX.4.58.0410271341200.29000@maplepark.com>

On Wed, 27 Oct 2004, Kunal Shetty wrote:

> thank you Sundar,Andy, Partha for your prompt reply.
>
>       as for the "%*%" is matrix multiplication problem sunder here is a sample of what i want...
>
> x <- rnorm(100,17,24) # X values
> y <- rnorm(100,7,11)  # Y values
>
> # since X and Y values are independent to get their
> # covariance need to multiply each of them with a Matrix say A
>
> A <- matrix(c(10,25,8,40),nrow=2,ncol=2)

If x and y are independent, then their covariance should be
zero, or at least symmetric.

Are you looking for z as a bivariate normal with covariance matrix A and
means (17,7)?

 randMV<-function(n=NA,vMean=NA,mSig=NA){
 # create n observations of multivariate normal data with a
 # mean of vMean and covariance mSig
 #
  p<-length(vMean)
  VD<-eigen(mSig)
  mSigH<-VD$vectors%*%diag(sqrt(VD$values))%*%solve(VD$vectors)

  ret<-matrix(rnorm(n=p*n),ncol=p)%*%mSigH +rep(vMean,each=n)
  ret
 }

z<-randMV(100,c(17,7),A)

colMeans(z)
cov(z)

>
>  further  i would like to assign both x and y vectors to one variale
>
>  say z <- c(x,y)
>
>   but if i do this  my matrix multiplication fail
>
>     i.e  w <- A%*%z
>  Error in A %*% z : non-conformable arguments ??
>
>
>     so it tired multiply the  X and Y vector individually
>
>  x<- (A%*%x)
>  y<- (A%*%y)
>
>
>  Error in A %*% z : non-conformable arguments ??
>
>
>   the error persist...sunder...
>
>   or anybody who could direct me..?
>
> regards
> Kunal
>
>
>
>
>
> Sundar Dorai-Raj <sundar.dorai-raj at PDF.COM> wrote:
> >
> >
> > Kunal Shetty wrote:
> >
> > > Dear R- User
> > >
> > >     I created  two random number sets using rnorm function and calcuted their respective means. However now I would like to randomly
> > > replace some values with NA. Thus create my own test data.
> > >       Any help or suggestions on this ?
> > >
> > >
> >
> > Use ?sample:
> > n <- 100
> > x <- rnorm(n)
> > x[sample(n, 4)] <- NA
> > sum(is.na(x))
> > # [1] 4
> >
> > >  Also wanted to confirm when multiplying two random number vectors x am y by matrix..is this how i do it.
> > > A is the matrix
> > >
> > >        z <- c(x,y)    # x and y the two set of vectors
> > >
> > >        w <- A%*%Z   # each element in each vector multipled by the matrix .
> > >
> >
> > (Be careful: R is case sensitive (z != Z).)
> >
> > I'm not really clear what you want here. "%*%" is matrix multiplication
> > and "*" is elementwise multiplication. Also using "c" makes a vector or
> > length "n = length(x) + length(y)". This implies that "A" above "p x n"
> > (i.e. p rows, n columns) and the result "w" would be "p x 1". Please
> > provide an example of what you expect to see by the multiplication.
> >
> > --sundar
> >
> > >
> > > regards
> > > Kunal
> > >
> > > ______________________________________________
> > > R-help at stat.math.ethz.ch mailing list
> > > https://stat.ethz.ch/mailman/listinfo/r-help
> > > PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>

-- 
 Dave Forrest
 drf at vims.edu                                    (804)684-7900w
 drf5n at maplepark.com                             (804)642-0662h
                                   http://maplepark.com/~drf5n/



From John-J.Smith at ubs.com  Wed Oct 27 21:17:19 2004
From: John-J.Smith at ubs.com (John-J.Smith@ubs.com)
Date: Wed, 27 Oct 2004 15:17:19 -0400
Subject: [R] writing lm summary to file?
Message-ID: <7D36EDA26E7C8C4299EFD3F171A88D21086E91E9@NSTMC006PEX1.ubsgs.ubsgroup.net>

Hi,

I want to write the summary from a regression.  I am doing this because I do not see a way of get the std error, tvalues from the coefficients diagnostic.  n$coef does not give this only get the intercept and slope.  I tried to use write and write.table and got error in both cases.  I jumped thru the hoops below to no avail.  Also note, this is in windows.  I used to use unix, and do not recall this problem, but only have windows at this point.  Any suggestions are greatly appreciated.

John


> n<-lm(formula = A ~ B, data = y)
> summary(n)

Call:
lm(formula = A ~ B, data = y)

Residuals:
    Min      1Q  Median      3Q     Max 
-5.0165 -0.9726 -0.4707  1.7783  3.8563 

Coefficients:
            Estimate Std. Error t value Pr(>|t|)   
(Intercept)  1.15072    0.38135   3.017  0.00506 **
B        0.18042    0.05254   3.434  0.00171 **
---
Signif. codes:  0 `***' 0.001 `**' 0.01 `*' 0.05 `.' 0.1 ` ' 1 

Residual standard error: 2.14 on 31 degrees of freedom
Multiple R-Squared: 0.2755,     Adjusted R-squared: 0.2522 
F-statistic: 11.79 on 1 and 31 DF,  p-value: 0.001711 

> help(write.table)

> write.table(summary(n),file ="C\\....")
Error in as.data.frame.default(x[[i]], optional = TRUE) : 
        can't coerce summary.lm into a data.frame
> help(write)

> write(summary(n),file ="C\\..")
Error in cat(list(...), file, sep, fill, labels, append) : 
        argument 1 not yet handled by cat

Visit our website at http://www.ubs.com

This message contains confidential information and is intend...{{dropped}}



From ripley at stats.ox.ac.uk  Wed Oct 27 21:26:39 2004
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Wed, 27 Oct 2004 20:26:39 +0100 (BST)
Subject: [R] Warning messages in function fitdistr (library:MASS)
In-Reply-To: <F74A1EABDCCFFB4893FD93C96408F58AB4E613@BMCORREO.banxico.org.mx>
Message-ID: <Pine.LNX.4.44.0410272009030.29893-100000@gannet.stats>

On Wed, 27 Oct 2004, De la Vega G??ngora Jorge wrote:

> Why the warning messages (2:4)?

The warnings are about your uninformed use of the function.  You have read
the reference (as the R posting guide asks) haven't you?

You are trying to fit a distribution with parameter > 0 by an
*unconstrained* optimizer with a deliberately poor starting value,
ignoring warning 1.

> > x <- rexp(1000,0.2)
> > fitdistr(x,"exponential",list(rate=1))
>       rate    
>   0.219824219 
>  (0.006951308)
> Warning messages: 
> 1: one-diml optimization by Nelder-Mead is unreliable: use optimize in: optim(start, mylogfn, x = x, hessian = TRUE, ...) 
> 2: NaNs produced in: dexp(x, 1/rate, log) 
> 3: NaNs produced in: dexp(x, 1/rate, log) 
> 4: NaNs produced in: dexp(x, 1/rate, log) 

fitdistr(x,"exponential",list(rate=1), lower=0.001, method="L-BFGS-B")

would be a much better way, following the examples on the help page.

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From ripley at stats.ox.ac.uk  Wed Oct 27 21:45:24 2004
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Wed, 27 Oct 2004 20:45:24 +0100 (BST)
Subject: [R] writing lm summary to file?
In-Reply-To: <7D36EDA26E7C8C4299EFD3F171A88D21086E91E9@NSTMC006PEX1.ubsgs.ubsgroup.net>
Message-ID: <Pine.LNX.4.44.0410272039550.29947-100000@gannet.stats>

On Wed, 27 Oct 2004 John-J.Smith at ubs.com wrote:

> Hi,
> 
> I want to write the summary from a regression.  I am doing this because
> I do not see a way of get the std error, tvalues from the coefficients
> diagnostic.  n$coef does not give this only get the intercept and slope.  
> I tried to use write and write.table and got error in both cases.  I
> jumped thru the hoops below to no avail.  Also note, this is in windows.  
> I used to use unix, and do not recall this problem, but only have
> windows at this point.  Any suggestions are greatly appreciated.

coef(summary(n)) is what you want, I believe.  That's a matrix.
If you want to write the printed output of the summary to a file
(not the same thing at all: see the FAQ Q8.1), use sink().

If you read the help for write() and write.table() I am surprised you 
expected them to work.  Did you try str() on what you were trying to 
write?

> 
> John
> 
> 
> > n<-lm(formula = A ~ B, data = y)
> > summary(n)
> 
> Call:
> lm(formula = A ~ B, data = y)
> 
> Residuals:
>     Min      1Q  Median      3Q     Max 
> -5.0165 -0.9726 -0.4707  1.7783  3.8563 
> 
> Coefficients:
>             Estimate Std. Error t value Pr(>|t|)   
> (Intercept)  1.15072    0.38135   3.017  0.00506 **
> B        0.18042    0.05254   3.434  0.00171 **
> ---
> Signif. codes:  0 `***' 0.001 `**' 0.01 `*' 0.05 `.' 0.1 ` ' 1 
> 
> Residual standard error: 2.14 on 31 degrees of freedom
> Multiple R-Squared: 0.2755,     Adjusted R-squared: 0.2522 
> F-statistic: 11.79 on 1 and 31 DF,  p-value: 0.001711 
> 
> > help(write.table)
> 
> > write.table(summary(n),file ="C\\....")
> Error in as.data.frame.default(x[[i]], optional = TRUE) : 
>         can't coerce summary.lm into a data.frame
> > help(write)
> 
> > write(summary(n),file ="C\\..")
> Error in cat(list(...), file, sep, fill, labels, append) : 
>         argument 1 not yet handled by cat


-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From p.murrell at auckland.ac.nz  Wed Oct 27 21:49:18 2004
From: p.murrell at auckland.ac.nz (Paul Murrell)
Date: Thu, 28 Oct 2004 08:49:18 +1300
Subject: [R] persp(), scatterplot3d(), "..." argument
References: <a0600200ebda3cb456654@[139.166.242.29]>	<417F488C.7000405@statistik.uni-dortmund.de>
	<1098862790.16776.29.camel@biol102145.oulu.fi>
Message-ID: <417FFBBE.50403@stat.auckland.ac.nz>

Hi


Jari Oksanen wrote:
> On Wed, 2004-10-27 at 10:04, Uwe Ligges wrote:
> 
>>Robin Hankin wrote:
>>
>>
>>>Hello list.
>>>
>>>I very often need 3d scatterplots, and use scatterplot3D quite a lot.
>>>I am trying to modify persp() to plot scatterplots, and make use of
>>>the theta and phi arguments that persp() offers.  I am having some
>>>difficulty passing the correct arguments to persp().
>>>
>>>Here is my function so far.  Much of it is copied from the persp() manpage.
>>>work as expected, but all extra arguments are passed to persp() _and_
>>>lines() and this gives warnings.
>>
> ---cut---
> 
>>>
>>>QUESTION:
>>>
>>>What is best practice to handle this sort of problem?  Should I ignore
>>>the warnings() that this approach gives?  I can suppress them with
>>>options(warn= -Inf), but is this a good idea?  I've read section 10.4
>>>of AITR.
>>
>>Many of us have thought about this problem.
>>I'd suggest to introduce frequently used arguments to your meta-function 
>>(such as main, xlab, ylab, ...), but ignore warnings (note: warnings, 
>>not errors) if less frequently arguments are passed through "...".
>>
>>Alternatively, you can exclude "..." from lines() or points() and 
>>specify a list of arguments to be passed to that call.
> 
> 
> This is a larger problem if 
> 1. one of the underlying functions does not have "..."
> 2. you want to relay arguments to two or more underlying functions, and
> 3. you don't want to list all possible arguments in your function
> definition, since it is long enough already.


4. you want to only relay arguments a, b, and c to one function and only 
c and d to another function.  For example, plot.default() only passes 
the col argument on to plot.xy() (not to axes or title).

5. arguments can have quite different meanings when passed to different 
functions.  e.g., col means fill colour when passed to rect(), but 
border colour when passed to box().

This all leads to argument lists like that for plot.default();  you make 
many par arguments formal arguments (with sensible defaults!) and 
explicitly control which functions they get relayed on to.

In your particular case, I think the best approach is to explicitly 
provide a bunch of arguments (with defaults) that might get passed to 
persp(), pass them to persp() and pass them plus everything else (...) 
to points().

Paul

p.s.  main, xlab, and ylab are not graphical parameters (as in par()) so 
I think they should definitely be formals of your points3d()


> The solution is still there, but it is (black) magic. For instance,
> 'arrows' does not have "...", so you must add them with this magical
> mystery string:
> 
> formals(arrows) <- c(formals(arrows), alist(... = ))
> 
> Yes, this is documented in R manuals -- I wouldn't know this otherwise.
> Still, it would be nicer if 'arrows' had "..."...
> 
> cheers, jari oksanen


-- 
Dr Paul Murrell
Department of Statistics
The University of Auckland
Private Bag 92019
Auckland
New Zealand
64 9 3737599 x85392
paul at stat.auckland.ac.nz
http://www.stat.auckland.ac.nz/~paul/



From zhliur at yahoo.com  Thu Oct 28 00:56:21 2004
From: zhliur at yahoo.com (yyan liu)
Date: Wed, 27 Oct 2004 15:56:21 -0700 (PDT)
Subject: [R] integrate a function in R 
Message-ID: <20041027225621.66464.qmail@web90107.mail.scd.yahoo.com>

Hi:
  I want to integrate the following function with
respect to "z", "u" is another argument in the
function. My program is a loop and in each loop the
argument "u" will be given a specified numeric values.
But how can I use the "integrate" function with "u" in
R?
  The function is:
zfz<-function(z,u)
{ 
  return(z*dnorm(z,u,1) )	
}

And in each loop, the integration interval is [0,u+3].
I can not just use integrate(zfz,0,u+3) because "u"
can not be specified. 
Thank you!

liu 

__________________________________________________

Download the latest ringtones, games, and more!



From sun at cae.wisc.edu  Thu Oct 28 01:00:43 2004
From: sun at cae.wisc.edu (Sun)
Date: Wed, 27 Oct 2004 18:00:43 -0500
Subject: [R] ploting an ellipse keeps giving errors
References: <20041027132443.ZCHK1536.tomts25-srv.bellnexxia.net@JohnDesktop8300>
Message-ID: <052401c4bc78$c99f1540$23719792@star>

Dear Mr. Fox:

Now I got things done using your ellipse in Package car! Just note to plot
an empty plot before calling ellipse. And input the correct radius and
covariance matrix (I input the inverse of the covariance matrix multiplied
by n as the covariance matrix, that was wrong. n should go to divide the
righ hand side and square root to get the radius and no inverse needed).

Thank you! By the way, I tried to write my own and it does not work only
giving two parallel lines.:(
Now the ellipse in car works totally well!

Sun

----- Original Message ----- 
From: "John Fox" <jfox at mcmaster.ca>
To: "'Sun'" <sun at cae.wisc.edu>
Cc: "'Petr Pikal'" <petr.pikal at precheza.cz>; <r-help at stat.math.ethz.ch>
Sent: Wednesday, October 27, 2004 8:24 AM
Subject: RE: [R] ploting an ellipse keeps giving errors


> Dear Sun,
>
> There are indeed (at least) two ellipse functions, and the one in the car
> package has different arguments from the one in the ellipse package.
>
> You should be able to use ellipse() in either package to do what you want;
> use the t argument for the version in ellipse or the radius argument for
the
> version in car, setting this to the square-root of the critical chisquare
or
> F. (ellipse in car works by deforming a circle.) If height and weight are
> from the same sample, then they won't be independent, so, e.g., the shape
> argument to ellipse in car would be their sample covariance matrix. The
> center argument to ellipse() in car and the centre argument to ellipse()
in
> ellipse should be the vector of means.
>
> But the ellipse() function in ellipse and the data.ellipse() function in
car
> already does all this, so why do you feel that you need to roll your own
(or
> perhaps I misunderstand what you want)?
>
> I hope this helps.
>  John
>
> --------------------------------
> John Fox
> Department of Sociology
> McMaster University
> Hamilton, Ontario
> Canada L8S 4M4
> 905-525-9140x23604
> http://socserv.mcmaster.ca/jfox
> -------------------------------- 
>
> > -----Original Message-----
> > From: r-help-bounces at stat.math.ethz.ch
> > [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Sun
> > Sent: Wednesday, October 27, 2004 4:25 AM
> > To: Petr Pikal
> > Cc: r-help at stat.math.ethz.ch
> > Subject: Re: [R] ploting an ellipse keeps giving errors
> >
> > Thank you. I found there are two ellipses
> >
> > 1.
> > R2.0
> > library (car)
> >
> > 2.
> > R1.9 and R2.0
> > library (ellipse)
> >
> > And they are different! I can't run 1.
> >
> > But the 2. is kind of specialized for t-distribution
> > confidence and so on.
> >
> > I need to find a general ellipse for an ellipse equation like
> > (x-x0)^2/a + (y-y0)^2/b =1
> >
> > . Since I used chi-square percentile not t. I am trying to
> > obtain the large sample 95% simultaneous confidence ellipse
> > for two population means (say, weight and height). The input
> > are the two sample means and their covariances.
> >
> > Maybe I have to make my own ellipse function.
> >
> > Sun
> > ----- Original Message -----
> > From: "Petr Pikal" <petr.pikal at precheza.cz>
> > To: "Sun" <sun at cae.wisc.edu>
> > Cc: <r-help at stat.math.ethz.ch>
> > Sent: Wednesday, October 27, 2004 4:14 AM
> > Subject: Re: [R] ploting an ellipse keeps giving errors
> >
> >
> > > Hi
> > >
> > > Did you read what ellipse does and how it shall be used?
> > >
> > > And what about your system, R and ellipse version?
> > >
> > > From your example i got
> > >
> > > > ellipse (center = c(0,0), shape = shape1, radius = 1)
> > > Error in ellipse.default(center = c(0, 0), shape = shape1, radius =
> > > 1) :
> > >         Argument "x" is missing, with no default
> > >
> > > Ellipse is not for drawing arbitrary ellipses but has
> > different usage.
> > >
> > > See its help page and an example.
> > >
> > > Cheers
> > > Petr
> > >
> > >
> > >
> > >
> > > On 27 Oct 2004 at 3:34, Sun wrote:
> > >
> > > >
> > > > library (ellipse)
> > > >
> > > > shape1 = c (1, 0, 0,1)
> > > > dim(shape1) = c(2,2)
> > > > ellipse (center = c(0,0), shape = shape1, radius = 1)
> > > >
> > > > =============================
> > > > Error in plot.xy(xy.coords(x, y), type = type, col = col,
> > lty = lty,
> > > > ...) :
> > > >         plot.new has not been called yet
> > > >
> > > >
> > > > It is really frustrating. Also what do the shape matrix, radius
> > > > correspond to an ellipse function
> > > >
> > > > (x-x0)^2/a + (y-y0)^2/b = 1
> > > >
> > > > ? Please advise!
> > > >
> > > > Many thanks,
> > > >
> > > > Sun
> > > >  [[alternative HTML version deleted]]
> > > >
> > > > ______________________________________________
> > > > R-help at stat.math.ethz.ch mailing list
> > > > https://stat.ethz.ch/mailman/listinfo/r-help
> > > > PLEASE do read the posting guide!
> > > > http://www.R-project.org/posting-guide.html
> > >
> > > Petr Pikal
> > > petr.pikal at precheza.cz
> > >
> > >
> > >
> >
> > ______________________________________________
> > R-help at stat.math.ethz.ch mailing list
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide!
> > http://www.R-project.org/posting-guide.html
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide!
http://www.R-project.org/posting-guide.html
>



From penglish at hydro.washington.edu  Thu Oct 28 01:26:06 2004
From: penglish at hydro.washington.edu (Paul English)
Date: Wed, 27 Oct 2004 16:26:06 -0700 (PDT)
Subject: [R] Problem installing RandomFields package
Message-ID: <20041027162354.L24916-100000@dynamic.hydro.washington.edu>


Hi,
	I'm running R 2.0 installed from the source port on FreeBSD 4.10
and I'm having some trouble installing the RandomFields package. Following
is the procedure and errors:

>options(CRAN="http://cran.us.r-project.org")
>install.packages("RandomFields")
<download snipped>
downloaded 227Kb

* Installing *source* package 'RandomFields' ...
** libs
c++ -I/usr/local/lib/R/include  -I/usr/local/include -mieee-fp  -fPIC  -O
-pipe -c D.H.cc -o D.H.o
c++ -I/usr/local/lib/R/include  -I/usr/local/include -mieee-fp  -fPIC  -O
-pipe -c MPP.cc -o MPP.o
c++ -I/usr/local/lib/R/include  -I/usr/local/include -mieee-fp  -fPIC  -O
-pipe -c MPPFcts.cc -o MPPFcts.o
c++ -I/usr/local/lib/R/include  -I/usr/local/include -mieee-fp  -fPIC  -O
-pipe -c RFCovFcts.cc -o RFCovFcts.o
c++ -I/usr/local/lib/R/include  -I/usr/local/include -mieee-fp  -fPIC  -O
-pipe -c RFcircembed.cc -o RFcircembed.o
c++ -I/usr/local/lib/R/include  -I/usr/local/include -mieee-fp  -fPIC  -O
-pipe -c RFdirect.cc -o RFdirect.o
c++ -I/usr/local/lib/R/include  -I/usr/local/include -mieee-fp  -fPIC  -O
-pipe -c RFempvario.cc -o RFempvario.o
c++ -I/usr/local/lib/R/include  -I/usr/local/include -mieee-fp  -fPIC  -O
-pipe -c RFgetNset.cc -o RFgetNset.o
c++ -I/usr/local/lib/R/include  -I/usr/local/include -mieee-fp  -fPIC  -O
-pipe -c RFinitNerror.cc -o RFinitNerror.o
In file included from RFinitNerror.cc:29:
/usr/include/sys/timeb.h:47: syntax error before `;'
*** Error code 1

Stop in /tmp/R.INSTALL.35020/RandomFields/src.
ERROR: compilation failed for package 'RandomFields'
** Removing '/usr/local/lib/R/library/RandomFields'
** Restoring previous '/usr/local/lib/R/library/RandomFields'

Delete downloaded files (y/N)?
The packages are in /tmp/RtmpaUoyEh/Rinstdir167eb0e7
Warning message:
Installation of package RandomFields had non-zero exit status in:
install.packages("RandomFields")



From penglish at hydro.washington.edu  Thu Oct 28 01:28:37 2004
From: penglish at hydro.washington.edu (Paul English)
Date: Wed, 27 Oct 2004 16:28:37 -0700 (PDT)
Subject: [R] Problems installing GRASS package
Message-ID: <20041027162608.X24916-100000@dynamic.hydro.washington.edu>


Hi,
	I'm running R 2.0 installed from the source port on FreeBSd 4.10
and I'm having trouble installing the GRASS package. Following is the
procedure plus errors:

> options(CRAN="http://cran.us.r-project.org")
> install.packages("GRASS")
<download snipped>
downloaded 186Kb

* Installing *source* package 'GRASS' ...
** libs
cc -I/usr/local/lib/R/include -DR_GRASS_INTERFACE -I./include
-I/usr/local/include -mieee-fp  -fPIC  -O -pipe -c R_G_init.c -o
R_G_init.o
cc -I/usr/local/lib/R/include -DR_GRASS_INTERFACE -I./include
-I/usr/local/include -mieee-fp  -fPIC  -O -pipe -c adj_cellhd.c -o
adj_cellhd.o
cc -I/usr/local/lib/R/include -DR_GRASS_INTERFACE -I./include
-I/usr/local/include -mieee-fp  -fPIC  -O -pipe -c alloc.c -o alloc.o
cc -I/usr/local/lib/R/include -DR_GRASS_INTERFACE -I./include
-I/usr/local/include -mieee-fp  -fPIC  -O -pipe -c alloc_cell.c -o
alloc_cell.o
cc -I/usr/local/lib/R/include -DR_GRASS_INTERFACE -I./include
-I/usr/local/include -mieee-fp  -fPIC  -O -pipe -c ascii_chk.c -o
ascii_chk.o
cc -I/usr/local/lib/R/include -DR_GRASS_INTERFACE -I./include
-I/usr/local/include -mieee-fp  -fPIC  -O -pipe -c auto_mask.c -o
auto_mask.o
In file included from include/G.h:3,
                 from auto_mask.c:18:
include/rpc/types.h:87: redefinition of `caddr_t'
/usr/include/sys/types.h:69: `caddr_t' previously declared here
include/rpc/types.h:89: redefinition of `u_int'
/usr/include/sys/types.h:54: `u_int' previously declared here
include/rpc/types.h:90: redefinition of `u_long'
/usr/include/sys/types.h:55: `u_long' previously declared here
include/rpc/types.h:91: redefinition of `u_short'
/usr/include/sys/types.h:53: `u_short' previously declared here
*** Error code 1

Stop in /tmp/R.INSTALL.35187/GRASS/src.
ERROR: compilation failed for package 'GRASS'
** Removing '/usr/local/lib/R/library/GRASS'
** Restoring previous '/usr/local/lib/R/library/GRASS'

Delete downloaded files (y/N)?
The packages are in /tmp/RtmpaUoyEh/Rinstdir446b9b3d
Warning message:
Installation of package GRASS had non-zero exit status in:
install.packages("GRASS")

Thanks,
Paul



From wuertz at itp.phys.ethz.ch  Thu Oct 28 01:44:10 2004
From: wuertz at itp.phys.ethz.ch (Diethelm Wuertz)
Date: Wed, 27 Oct 2004 23:44:10 +0000
Subject: [R] Skewness and Kurtosis
In-Reply-To: <20041027081953.83500.qmail@web41214.mail.yahoo.com>
References: <20041027081953.83500.qmail@web41214.mail.yahoo.com>
Message-ID: <418032CA.803@itp.phys.ethz.ch>

Hello,

fBasics has functions for skewness and kurtosis:
Comments on these functions are welcome!

Diethelm Wuertz


# **********************************************************


kurtosis =
function (x, ...)
{   # A function implemented by D. Wuertz

    # FUNCTION:
   
    UseMethod("kurtosis")
}


# 
------------------------------------------------------------------------------


kurtosis.default =
function (x, na.rm = FALSE, ...)
{   # A function implemented by D. Wuertz
 
    # Description:
    #   Returns the value of the kurtosis of a
    #   distribution function. Missing values
    #   can be handled.
   
    # FUNCTION:
   
    # Warnings:
    if (!is.numeric(x) && !is.complex(x) && !is.logical(x)) {
        warning("argument is not numeric or logical: returning NA")
        return(as.numeric(NA))}
       
    # Remove NAs:
    if (na.rm) x = x[!is.na(x)]

    # Kurtosis:
    n = length(x)
    if (is.integer(x)) x = as.numeric(x)
    kurtosis = sum((x-mean(x))^4/var(x)^2)/length(x) - 3
   
    # Return Value:
    kurtosis 
}


# 
------------------------------------------------------------------------------


kurtosis.data.frame =
function (x, ...)
{   # A function implemented by D. Wuertz

    sapply(x, kurtosis, ...)
}



# 
******************************************************************************


skewness =
function (x, ...)
{   # A function implemented by D. Wuertz

    UseMethod("skewness")
}


# 
------------------------------------------------------------------------------


skewness.default =
function (x, na.rm = FALSE, ...)
{   # A function implemented by D. Wuertz
 
    # Description:
    #   Returns the value of the skewness of a
    #   distribution function. Missing values
    #   can be handled.
   
    # FUNCTION:
   
    # Warnings:
    if (!is.numeric(x) && !is.complex(x) && !is.logical(x)) {
        warning("argument is not numeric or logical: returning NA")
        return(as.numeric(NA))}
       
    # Remove NAs:
    if (na.rm) x = x[!is.na(x)]

    # Skewness:
    n = length(x)
    if (is.integer(x)) x = as.numeric(x)
    skewness = sum((x-mean(x))^3/sqrt(var(x))^3)/length(x)
   
    # Return Value:
    skewness 
}


# 
------------------------------------------------------------------------------


skewness.data.frame =
function (x, ...)
{   # A function implemented by D. Wuertz

    sapply(x, skewness, ...)
}



Vito Ricci wrote:

>Hi,
>
>in which R-package I could find skewness and kurtosis
>measures for a distribution?
>
>I built some functions:
>
>gamma1<-function(x)
>{
>m=mean(x)
>n=length(x)
>s=sqrt(var(x))
>m3=sum((x-m)^3)/n
>g1=m3/(s^3)
>return(g1)
>}
>
>skewness<-function(x)
>{
>m=mean(x)
>me=median(x)
>s=sqrt(var(x))
>sk=(m-me)/s
>return(sk)
>}
>
>bowley<-function(x)
>{
>q<-as.vector(quantile(x,prob=c(.25,.50,.75)))
>b=(q[3]+q[1]-2*q[2])/(q[3]-q[2])
>return(b)
>}
>
>b3<-function(x)
>{
>m=mean(x)
>me=median(x)
>n=length(x)
>d=sum(abs(x-me))/n
>b=(m-me)/d
>return(b)
>}
>
>but I'm looking for some already included in a
>package.
>
>Thanks in advance.
>Best
>Vito
>
>
>=====
>Diventare costruttori di soluzioni
>
>"The business of the statistician is to catalyze 
>the scientific learning process."  
>George E. P. Box
>
>
>Visitate il portale http://www.modugno.it/
>e in particolare la sezione su Palese http://www.modugno.it/archivio/cat_palese.shtml
>
>______________________________________________
>R-help at stat.math.ethz.ch mailing list
>https://stat.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>
>  
>



From gdsr15 at yahoo.com  Thu Oct 28 03:16:58 2004
From: gdsr15 at yahoo.com (gauri)
Date: Wed, 27 Oct 2004 18:16:58 -0700 (PDT)
Subject: [R] unable to import SPSS data
Message-ID: <20041028011658.73554.qmail@web52908.mail.yahoo.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20041027/b8a601f8/attachment.pl

From WeiQiang.Li at seagate.com  Thu Oct 28 03:36:47 2004
From: WeiQiang.Li at seagate.com (WeiQiang.Li@seagate.com)
Date: Thu, 28 Oct 2004 09:36:47 +0800
Subject: [R] How to calculate Adjusted SS
In-Reply-To: <3A822319EB35174CA3714066D590DCD50994E22A@usrymx25.merck.com>
Message-ID: <OFD11D7B20.87DFE7FE-ON48256F3B.0008B6A6-48256F3B.0008D08A@seagate.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20041028/81cd15c0/attachment.pl

From hi_ono2001 at ybb.ne.jp  Thu Oct 28 04:11:32 2004
From: hi_ono2001 at ybb.ne.jp (Hisaji ONO)
Date: Thu, 28 Oct 2004 11:11:32 +0900 (JST)
Subject: [R] About lp.assign in lpSolve package
Message-ID: <20041028021132.57647.qmail@web1702.mail.yahoo.co.jp>

Hi.

 I've tried to execute &#34;example(lp.assign)&#34; in
lpSolve package.

 However, this solution seemed not to return correct
answer, all matrix elements were zeros.

 Could you give me any &#34;solution&#34; about this?


 Regards.



From Kevin.Wang at maths.anu.edu.au  Thu Oct 28 05:07:41 2004
From: Kevin.Wang at maths.anu.edu.au (Kevin Wang)
Date: Thu, 28 Oct 2004 13:07:41 +1000 (EST)
Subject: [R] gsub() on Matrix
Message-ID: <Pine.GSO.4.58.0410281306500.4083@yin>

Hi,

Suppose I've got a matrix, and the first few elements look like
  "x1 + x3 + x4 + x5 + x1:x3 + x1:x4"
  "x1 + x2 + x3 + x5 + x1:x2 + x1:x5"
  "x1 + x3 + x4 + x5 + x1:x3 + x1:x5"
and so on (have got terms from x1 ~ x14).

If I want to replace all the x1 with i7, all x2 with i14, all x3 with i13,
for example.  Is there an easy way?

I tried to put what I want to replace in a vector, like:
 repl = c("i7", "i14", "i13", "d2", "i8", "i5",
          "i6", "i3", "A", "i9", "i2",
          "i4", "i15", "i21")
and have another vector, say:
  > orig
 [1] "x1"  "x2"  "x3"  "x4"  "x5"  "x6"  "x7"  "x8"  "x9"  "x10"
[11] "x11" "x12" "x13" "x14"

Then I tried something like
  gsub(orig, repl, mat)
## mat is the name of my matrix

but it didn't work *_*.....it would replace terms like x10 with i70.

(I know it may be an easy question...but I haven't done much regular
expression)

Cheers,

Kevin

--------------------------------
Ko-Kang Kevin Wang
PhD Student
Centre for Mathematics and its Applications
Building 27, Room 1004
Mathematical Sciences Institute (MSI)
Australian National University
Canberra, ACT 0200
Australia

Homepage: http://wwwmaths.anu.edu.au/~wangk/
Ph (W): +61-2-6125-2431
Ph (H): +61-2-6125-7407
Ph (M): +61-40-451-8301



From ripley at stats.ox.ac.uk  Thu Oct 28 08:41:14 2004
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Thu, 28 Oct 2004 07:41:14 +0100 (BST)
Subject: [R] Problems installing GRASS package
In-Reply-To: <20041027162608.X24916-100000@dynamic.hydro.washington.edu>
Message-ID: <Pine.LNX.4.44.0410280728540.30877-100000@gannet.stats>

Please send such problems, with solution, to the package maintainers.

They do not occur generally and seem to be due to the system headers on 
your particular system.  Only users of `FreeBSd' will be able to 
troubleshoot, i.e. you.

In this particular case, I think the package needs to use its own headers
only if the system ones are not available (via a configure test), and
suggest you try deleting GRASS/src/include/rpc.  (Roger: the zlib.h and
zconf.h are also worrying, as they might mismatch the libz used.)

On Wed, 27 Oct 2004, Paul English wrote:

> 
> Hi,
> 	I'm running R 2.0 installed from the source port on FreeBSd 4.10

There is no R 2.0 and no `source port'.  There are sources of R 2.0.0.

> and I'm having trouble installing the GRASS package. Following is the
> procedure plus errors:
> 
> > options(CRAN="http://cran.us.r-project.org")
> > install.packages("GRASS")
> <download snipped>
> downloaded 186Kb
> 
> * Installing *source* package 'GRASS' ...
> ** libs
> cc -I/usr/local/lib/R/include -DR_GRASS_INTERFACE -I./include
> -I/usr/local/include -mieee-fp  -fPIC  -O -pipe -c R_G_init.c -o
> R_G_init.o
> cc -I/usr/local/lib/R/include -DR_GRASS_INTERFACE -I./include
> -I/usr/local/include -mieee-fp  -fPIC  -O -pipe -c adj_cellhd.c -o
> adj_cellhd.o
> cc -I/usr/local/lib/R/include -DR_GRASS_INTERFACE -I./include
> -I/usr/local/include -mieee-fp  -fPIC  -O -pipe -c alloc.c -o alloc.o
> cc -I/usr/local/lib/R/include -DR_GRASS_INTERFACE -I./include
> -I/usr/local/include -mieee-fp  -fPIC  -O -pipe -c alloc_cell.c -o
> alloc_cell.o
> cc -I/usr/local/lib/R/include -DR_GRASS_INTERFACE -I./include
> -I/usr/local/include -mieee-fp  -fPIC  -O -pipe -c ascii_chk.c -o
> ascii_chk.o
> cc -I/usr/local/lib/R/include -DR_GRASS_INTERFACE -I./include
> -I/usr/local/include -mieee-fp  -fPIC  -O -pipe -c auto_mask.c -o
> auto_mask.o
> In file included from include/G.h:3,
>                  from auto_mask.c:18:
> include/rpc/types.h:87: redefinition of `caddr_t'
> /usr/include/sys/types.h:69: `caddr_t' previously declared here
> include/rpc/types.h:89: redefinition of `u_int'
> /usr/include/sys/types.h:54: `u_int' previously declared here
> include/rpc/types.h:90: redefinition of `u_long'
> /usr/include/sys/types.h:55: `u_long' previously declared here
> include/rpc/types.h:91: redefinition of `u_short'
> /usr/include/sys/types.h:53: `u_short' previously declared here
> *** Error code 1
> 
> Stop in /tmp/R.INSTALL.35187/GRASS/src.
> ERROR: compilation failed for package 'GRASS'
> ** Removing '/usr/local/lib/R/library/GRASS'
> ** Restoring previous '/usr/local/lib/R/library/GRASS'
> 
> Delete downloaded files (y/N)?
> The packages are in /tmp/RtmpaUoyEh/Rinstdir446b9b3d
> Warning message:
> Installation of package GRASS had non-zero exit status in:
> install.packages("GRASS")
> 
> Thanks,
> Paul
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
> 
> 

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From ripley at stats.ox.ac.uk  Thu Oct 28 08:47:55 2004
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Thu, 28 Oct 2004 07:47:55 +0100 (BST)
Subject: [R] unable to import SPSS data
In-Reply-To: <20041028011658.73554.qmail@web52908.mail.yahoo.com>
Message-ID: <Pine.LNX.4.44.0410280747070.30877-100000@gannet.stats>

Please read the `R Data Import/Export' manual that ships with R.

On Wed, 27 Oct 2004, gauri wrote:

> I have been trying to import SPSS data to R for analysis and am not sure
> as to how i should go about it...The SPSS data is saved in file
> "K:\....." thx..

> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html

Please read that too.

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From dimitris.rizopoulos at med.kuleuven.ac.be  Thu Oct 28 09:39:00 2004
From: dimitris.rizopoulos at med.kuleuven.ac.be (Dimitris Rizopoulos)
Date: Thu, 28 Oct 2004 09:39:00 +0200
Subject: [R] integrate a function in R 
References: <20041027225621.66464.qmail@web90107.mail.scd.yahoo.com>
Message-ID: <006e01c4bcc1$30e599c0$0540210a@www.domain>

Hi Liu,

you should take advantage of lexical scoping, i.e.,


zfz <- function(z) z*dnorm(x=z, mean=u)
us <- seq(-1, 1, length=10)
out <- numeric(length(us))
for(i in seq(along=us)){
    u <- us[i]
    out[i] <- integrate(zfz, 0, u+3)$value
}
out


I hope it helps.

Best,
Dimitris

----
Dimitris Rizopoulos
Ph.D. Student
Biostatistical Centre
School of Public Health
Catholic University of Leuven

Address: Kapucijnenvoer 35, Leuven, Belgium
Tel: +32/16/396887
Fax: +32/16/337015
Web: http://www.med.kuleuven.ac.be/biostat/
     http://www.student.kuleuven.ac.be/~m0390867/dimitris.htm


----- Original Message ----- 
From: "yyan liu" <zhliur at yahoo.com>
To: <r-help at stat.math.ethz.ch>
Sent: Thursday, October 28, 2004 12:56 AM
Subject: [R] integrate a function in R


> Hi:
>  I want to integrate the following function with
> respect to "z", "u" is another argument in the
> function. My program is a loop and in each loop the
> argument "u" will be given a specified numeric values.
> But how can I use the "integrate" function with "u" in
> R?
>  The function is:
> zfz<-function(z,u)
> {
>  return(z*dnorm(z,u,1) )
> }
>
> And in each loop, the integration interval is [0,u+3].
> I can not just use integrate(zfz,0,u+3) because "u"
> can not be specified.
> Thank you!
>
> liu
>
> __________________________________________________
>
> Download the latest ringtones, games, and more!
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
>



From Ted.Harding at nessie.mcc.ac.uk  Thu Oct 28 09:45:09 2004
From: Ted.Harding at nessie.mcc.ac.uk ( (Ted Harding))
Date: Thu, 28 Oct 2004 08:45:09 +0100 (BST)
Subject: [R] integrate a function in R
In-Reply-To: <20041027225621.66464.qmail@web90107.mail.scd.yahoo.com>
Message-ID: <XFMail.041028084509.Ted.Harding@nessie.mcc.ac.uk>

On 27-Oct-04 yyan liu wrote:
> Hi:
>   I want to integrate the following function with
> respect to "z", "u" is another argument in the
> function. My program is a loop and in each loop the
> argument "u" will be given a specified numeric values.
> But how can I use the "integrate" function with "u" in
> R?
>   The function is:
> zfz<-function(z,u)
> { 
>   return(z*dnorm(z,u,1) )     
> }
> 
> And in each loop, the integration interval is [0,u+3].
> I can not just use integrate(zfz,0,u+3) because "u"
> can not be specified. 
> Thank you!

If you are only interested in this specific function, then there
is an analytical solution and you do not need numerical integration.
Namely (if I have understood your statement correctly) let (0,Z) be
the limits of integration (in your case Z = u+3); then the integral
is

  dnorm(u,0,1) - dnorm(Z-u,0,1) + u*pnorm(Z-u,0,1) - pnorm(-u,0,1)

which, in your case, is

  dnorm(u) - dnorm(3) + u*pnorm(3) - pnorm(-u)

(using the default values mu=0 and s=1 for dnorm and pnorm).

Hoping this helps,
Ted.


--------------------------------------------------------------------
E-Mail: (Ted Harding) <Ted.Harding at nessie.mcc.ac.uk>
Fax-to-email: +44 (0)870 094 0861  [NB: New number!]
Date: 28-Oct-04                                       Time: 08:45:09
------------------------------ XFMail ------------------------------



From ripley at stats.ox.ac.uk  Thu Oct 28 10:06:29 2004
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Thu, 28 Oct 2004 09:06:29 +0100 (BST)
Subject: [R] integrate a function in R 
In-Reply-To: <006e01c4bcc1$30e599c0$0540210a@www.domain>
Message-ID: <Pine.LNX.4.44.0410280851540.843-100000@gannet.stats>

Where is the lexical scoping in this solution?  Your one function is
defined in the global environment, and `u' just happens to be too (unlike 
C and another implementation of the S language).

Even if it were relevant, it seems to obscure what is going on.  Compare

zfz <- function(z, mu) z*dnorm(x=z, mean=mu)
for(i in seq(along=us)) {
    u <- us[i]
    out[i] <- integrate(zfz, 0, u+3, mu=u)$value
}

which is easier to follow and is less error-prone (even if you need to
ensure that `u' does not match `upper').  When I use lexical scoping to
capture variables I normally add a note in the code to remind readers.  
Otherwise someone will streamline the code to

for(i in seq(along=us)) out[i] <- integrate(zfz, 0, us[i]+3)$value


On Thu, 28 Oct 2004, Dimitris Rizopoulos wrote:

> Hi Liu,
> 
> you should take advantage of lexical scoping, i.e.,
> 
> 
> zfz <- function(z) z*dnorm(x=z, mean=u)
> us <- seq(-1, 1, length=10)
> out <- numeric(length(us))
> for(i in seq(along=us)){
>     u <- us[i]
>     out[i] <- integrate(zfz, 0, u+3)$value
> }
> out
> 
> 
> I hope it helps.
> 
> Best,
> Dimitris
> 
> ----
> Dimitris Rizopoulos
> Ph.D. Student
> Biostatistical Centre
> School of Public Health
> Catholic University of Leuven
> 
> Address: Kapucijnenvoer 35, Leuven, Belgium
> Tel: +32/16/396887
> Fax: +32/16/337015
> Web: http://www.med.kuleuven.ac.be/biostat/
>      http://www.student.kuleuven.ac.be/~m0390867/dimitris.htm
> 
> 
> ----- Original Message ----- 
> From: "yyan liu" <zhliur at yahoo.com>
> To: <r-help at stat.math.ethz.ch>
> Sent: Thursday, October 28, 2004 12:56 AM
> Subject: [R] integrate a function in R
> 
> 
> > Hi:
> >  I want to integrate the following function with
> > respect to "z", "u" is another argument in the
> > function. My program is a loop and in each loop the
> > argument "u" will be given a specified numeric values.
> > But how can I use the "integrate" function with "u" in
> > R?
> >  The function is:
> > zfz<-function(z,u)
> > {
> >  return(z*dnorm(z,u,1) )
> > }
> >
> > And in each loop, the integration interval is [0,u+3].
> > I can not just use integrate(zfz,0,u+3) because "u"
> > can not be specified.
> > Thank you!
> >
> > liu
> >
> > __________________________________________________
> >
> > Download the latest ringtones, games, and more!
> >
> > ______________________________________________
> > R-help at stat.math.ethz.ch mailing list
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide! 
> > http://www.R-project.org/posting-guide.html
> >
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
> 
> 

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From r.alberts at biol.rug.nl  Thu Oct 28 10:37:05 2004
From: r.alberts at biol.rug.nl (R.Alberts)
Date: Thu, 28 Oct 2004 10:37:05 +0200
Subject: [R] RMySQL connection
Message-ID: <web-472059@biol.rug.nl>

Hi,

I cannot connect to my mysql database from within R using 
RMySQL. What am I doing wrong?

I use linux SuSe 8.1 and R-2.0.0
RMySQL_0.5-5.tar.gz
DBI_0.1-8.tar.gz

I get the following error in R:

> library(RMySQL)
Loading required package: DBI
> m <- dbDriver("MySQL")
> m
<MySQLDriver:(1756)>
> con <- dbConnect(m, group = "mygroup")
Error in mysqlNewConnection(drv, ...) : RS-DBI driver: 
(could not connect myuser at localhost on dbname 
"mydatabase")

the .my.cnf file looks like this:

# this is a comment
[client]
user = myuser
host = localhost
password = xxxxx

[rs-dbi]
database = s-data

[mygroup]
host = localhost
database = mydatabase



From ockham at gmx.net  Thu Oct 28 10:43:23 2004
From: ockham at gmx.net (ockham@gmx.net)
Date: Thu, 28 Oct 2004 10:43:23 +0200 (MEST)
Subject: [R] read.csv(stdin(),...) with sweave
Message-ID: <9130.1098953003@www49.gmx.net>

hello, 

i would like to read in a small amount of csv data directly from a
sweave-enabled latex document. i tried 

\begin{Scode}{fig=FALSE,echo=TRUE}
timeval <- read.csv(stdin(),nrows=2)
t1,t2,t3,t4
24.23,26.79,23.47,23.97
\end{Scode}

but apparently the stdin() approach doesnt't work as I get

Writing to file test.tex
Processing code chunks ...
 1 : echo term verbatim

Error:  chunk 1
Error in parse(file, n, text, prompt) : parse error

Any help very much appreciated...

Greetings
Ockham

-- 

GMX DSL-Netzanschluss + Tarif zum supergnstigen Komplett-Preis!



From pkhomski at wiwi.uni-bielefeld.de  Thu Oct 28 10:50:29 2004
From: pkhomski at wiwi.uni-bielefeld.de (Pavel Khomski)
Date: Thu, 28 Oct 2004 10:50:29 +0200
Subject: [R] seeking for the GLME-package (Jose Pinheiro)
Message-ID: <4180B2D5.8020103@wiwi.uni-bielefeld.de>

Hello!

could you give me some advice where i can finde out/recieve/download a 
GLME package, written by Jose Pinheiro. i couldn'ti find it in the 
package lists (probably becouse it is still in a beta version).

thank you for your replay



From ales.ziberna at guest.arnes.si  Thu Oct 28 10:59:40 2004
From: ales.ziberna at guest.arnes.si (=?iso-8859-2?Q?Ale=B9_=AEiberna?=)
Date: Thu, 28 Oct 2004 10:59:40 +0200
Subject: [R] ploting axes and rotating strings
References: <20041027151823.32BB61D7954@avs3.arnes.si>
Message-ID: <00aa01c4bccc$9c4eb290$1609f9c2@ales>

RE: [R] ploting axes and rotating stringsThanks to everybody for useful 
sugestions!

Ales



From ripley at stats.ox.ac.uk  Thu Oct 28 11:13:58 2004
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Thu, 28 Oct 2004 10:13:58 +0100 (BST)
Subject: [R] seeking for the GLME-package (Jose Pinheiro)
In-Reply-To: <4180B2D5.8020103@wiwi.uni-bielefeld.de>
Message-ID: <Pine.LNX.4.44.0410281010460.1219-100000@gannet.stats>

On Thu, 28 Oct 2004, Pavel Khomski wrote:

> could you give me some advice where i can finde out/recieve/download a 
> GLME package, written by Jose Pinheiro. i couldn'ti find it in the 
> package lists (probably becouse it is still in a beta version).

AFAIK it is/was an S-PLUS package, dependent on the S-PLUS version of nlme 
(which is not the same as the R one).

See https://stat.ethz.ch/pipermail/r-help/2003-June/033763.html

which points you at lme4 (as I was going to also).

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From maechler at stat.math.ethz.ch  Thu Oct 28 11:18:34 2004
From: maechler at stat.math.ethz.ch (Martin Maechler)
Date: Thu, 28 Oct 2004 11:18:34 +0200
Subject: [R] ploting an ellipse keeps giving errors
In-Reply-To: <042401c4bc06$d562bca0$23719792@star>
References: <417F830C.15553.B79A61@localhost>
	<042401c4bc06$d562bca0$23719792@star>
Message-ID: <16768.47466.725693.301077@gargle.gargle.HOWL>

>>>>> "Sun" == Sun  <sun at cae.wisc.edu>
>>>>>     on Wed, 27 Oct 2004 04:25:00 -0500 writes:

    Sun> Thank you. I found there are two ellipses
    Sun> 1.
    Sun> R2.0
    Sun> library (car)

    Sun> 2.
    Sun> R1.9 and R2.0
    Sun> library (ellipse)

    Sun> And they are different! I can't run 1.

    Sun> But the 2. is kind of specialized for t-distribution confidence and so on.

    Sun> I need to find a general ellipse for an ellipse equation like
    Sun> (x-x0)^2/a + (y-y0)^2/b =1


    Sun> . Since I used chi-square percentile not t. I am trying to obtain the large
    Sun> sample 95% simultaneous confidence ellipse for two population means (say,
    Sun> weight and height). The input are the two sample means and their
    Sun> covariances.


    Sun> Maybe I have to make my own ellipse function.

maybe not.
There is more (than you mentioned above) available:


1) The recommended (i.e. you don't have to install it) package
  "cluster" has

  a. ellipsoidhull()   {which is not what you need directly}
    which returns an (S3) object of class 'ellipsoid'
    and there's methods for such objects:
    predict(<ellipsoid>) computes points you can draw.

    Note that
	 library(cluster)
	 help(ellipsoidhull)
    tells you how an "ellipsoid" object must look like.
    (they *are* defined in terms of  cov()-matrix , center and "radius^2")
    and also has examples.

  b. ellipsoidPoints() is the function you can really directly use:

    A version of the following example will be in the next
    version of cluster:

    library(cluster)
    library(MASS)
    ## Robust vs. L.S. covariance matrix
    set.seed(143)
    x <- rt(200, df=3)
    y <- 3*x + rt(200, df=2)
    plot(x,y, main="non-normal data (N=200)")
    X <- cbind(x,y)
    C.ls <- cov(X) ; m.ls <- colMeans(X)
    Cxy <- cov.rob(cbind(x,y))
    lines(ellipsoidPoints(C.ls,    d2 = 2, loc=m.ls),       col="green")
    lines(ellipsoidPoints(Cxy$cov, d2 = 2, loc=Cxy$center), col="red")


2) The 'sfsmisc' package has a complementary useful
   ellipsePoints() function for ellipses ``given by geometry''
   the help of which starts with

 >> Compute Radially Equispaced Points on Ellipse
 >> 
 >> Description:
 >> 
 >>      Compute points on (the boundary of) an ellipse which is given by
 >>      elementary geometric parameters.
 >> 
 >> Usage:
 >> 
 >>      ellipsePoints(a, b, alpha = 0, loc = c(0, 0), n = 201)
 >> 
 >> Arguments:
 >> 
 >>      a,b: length of half axes in (x,y) direction.
 >> 
 >>    alpha: angle (in degrees) giving the orientation of the ellipse,
 >>           i.e., the original (x,y)-axis ellipse is rotated by 'angle'.
 >> 
 >>      loc: center (LOCation) of the ellipse.
 >> 
 >>        n: number of points to generate.
 
  install.packages('sfsmisc')
  library(sfsmisc)
  example(ellipsePoints)

  has a "nice" example of ellipse drawing,
  even a movie of a rotating ellipse...

Martin Maechler, ETH Zurich



From pfd_r at yahoo.fr  Thu Oct 28 11:56:15 2004
From: pfd_r at yahoo.fr (PFD)
Date: Thu, 28 Oct 2004 11:56:15 +0200 (CEST)
Subject: [R] Demographic parameters
Message-ID: <20041028095615.59121.qmail@web25208.mail.ukl.yahoo.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20041028/035743d4/attachment.pl

From sdavis2 at mail.nih.gov  Thu Oct 28 12:03:46 2004
From: sdavis2 at mail.nih.gov (Sean Davis)
Date: Thu, 28 Oct 2004 06:03:46 -0400
Subject: [R] RMySQL connection
In-Reply-To: <web-472059@biol.rug.nl>
References: <web-472059@biol.rug.nl>
Message-ID: <A85E3040-28C8-11D9-8349-000A95D7BA10@mail.nih.gov>


On Oct 28, 2004, at 4:37 AM, R.Alberts wrote:

> Hi,
>
> I cannot connect to my mysql database from within R using RMySQL. What 
> am I doing wrong?
>
> I use linux SuSe 8.1 and R-2.0.0
> RMySQL_0.5-5.tar.gz
> DBI_0.1-8.tar.gz
>
> I get the following error in R:
>
>> library(RMySQL)
> Loading required package: DBI
>> m <- dbDriver("MySQL")
>> m
> <MySQLDriver:(1756)>
>> con <- dbConnect(m, group = "mygroup")
> Error in mysqlNewConnection(drv, ...) : RS-DBI driver: (could not 
> connect myuser at localhost on dbname "mydatabase")

On the mysql server, there is a user named myuser?  The password for 
this user is xxxxxx (or whatever)?  The database is called mydatabase?  
The host for the server is localhost (mysql is running on the same 
machine as R)?  That user/password combination has access to 
mydatabase?

Can you connect from the mysql client on the same machine with those 
settings?

Sean



From danbebber at forestecology.co.uk  Thu Oct 28 12:42:22 2004
From: danbebber at forestecology.co.uk (Dan Bebber)
Date: Thu, 28 Oct 2004 11:42:22 +0100
Subject: [R] Error with package update
In-Reply-To: <200410191018.i9JABgBl027344@hypatia.math.ethz.ch>
Message-ID: <000501c4bcda$ce935130$7d2501a3@plants.ox.ac.uk>

I received the following error when I attempted to update my packages:

updating HTML package descriptions
Warning messages: 
1: DESCRIPTION file of package  'file28862'  missing or broken
 in: packageDescription(p, lib = lib, fields = pkgFlds) 
2: number of columns of result
        not a multiple of vector length (arg 2) in: rbind(retval, c(p, lib,
desc)) 
3: DESCRIPTION file of package  'file7460'  missing or broken
 in: packageDescription(p, lib = lib, fields = pkgFlds) 
4: number of columns of result
        not a multiple of vector length (arg 2) in: rbind(retval, c(p, lib,
desc)) 
5: DESCRIPTION file of package  'file28862'  missing or broken
 in: packageDescription(i, fields = "Title", lib.loc = lib) 
6: DESCRIPTION file of package  'file7460'  missing or broken
 in: packageDescription(i, fields = "Title", lib.loc = lib) 

I am running R 2.0 on Windows XP.

Any ideas what caused this?

Many thanks,
Dan Bebber

Department of Plant Sciences
University of Oxford
South Parks Road
Oxford OX1 3RB
UK
Tel. 01865 275000



From ripley at stats.ox.ac.uk  Thu Oct 28 13:17:57 2004
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Thu, 28 Oct 2004 12:17:57 +0100 (BST)
Subject: [R] Error with package update
In-Reply-To: <000501c4bcda$ce935130$7d2501a3@plants.ox.ac.uk>
Message-ID: <Pine.LNX.4.44.0410281212300.1722-100000@gannet.stats>

The NEWS file in R-patched has

    o	Using install.packages() to install the same package to more
	than one library gave an incorrect warning message.

which looks like the same thing.  I've not actually seen this give an
error, but it could if there were more than one package being installed to
more than one library, as may be happening here.

Try using update.packages() on a specific library at a time -- and do read 
the posting guide and remember to tell us what precisely you were doing at 
the time (and that there is no version R 2.0!).


On Thu, 28 Oct 2004, Dan Bebber wrote:

> I received the following error when I attempted to update my packages:
> 
> updating HTML package descriptions
> Warning messages: 
> 1: DESCRIPTION file of package  'file28862'  missing or broken
>  in: packageDescription(p, lib = lib, fields = pkgFlds) 
> 2: number of columns of result
>         not a multiple of vector length (arg 2) in: rbind(retval, c(p, lib,
> desc)) 
> 3: DESCRIPTION file of package  'file7460'  missing or broken
>  in: packageDescription(p, lib = lib, fields = pkgFlds) 
> 4: number of columns of result
>         not a multiple of vector length (arg 2) in: rbind(retval, c(p, lib,
> desc)) 
> 5: DESCRIPTION file of package  'file28862'  missing or broken
>  in: packageDescription(i, fields = "Title", lib.loc = lib) 
> 6: DESCRIPTION file of package  'file7460'  missing or broken
>  in: packageDescription(i, fields = "Title", lib.loc = lib) 
> 
> I am running R 2.0 on Windows XP.

It is R >2.0.0< !!

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From webmaster at hcbg.com  Thu Oct 28 13:38:13 2004
From: webmaster at hcbg.com (webmaster@hcbg.com)
Date: 28 Oct 2004 11:38:13 -0000
Subject: [R] Re: Re: Your website
Message-ID: <20041028113813.4757.qmail@srv.brk01.com>

merci de votre Email, nous le traitons au plus vite
Salutations

Le webmaster



From simon at stats.gla.ac.uk  Thu Oct 28 13:40:56 2004
From: simon at stats.gla.ac.uk (Simon Wood)
Date: Thu, 28 Oct 2004 12:40:56 +0100 (BST)
Subject: [R] GLM model vs. GAM model
In-Reply-To: <FAD63B0F78D0224A9AF352A4A98F1E2A588B0D@jupiter.imim.es>
References: <FAD63B0F78D0224A9AF352A4A98F1E2A588B0D@jupiter.imim.es>
Message-ID: <Pine.LNX.4.58.0410281205430.26963@moon.stats.gla.ac.uk>

For GAMs fitted using mgcv::gam, I think that there's an argument that the 
most self consistent comparison would be based on the GCV/UBRE score for 
the models, since this has usually been used to select the degree of 
smoothness of the GAM model (you can fit the GLM using gam(), so this is 
available for both models). 

As you (and the help files) point out, the p-values produced by anova.gam 
are only approximate, when the models are fitted using penalties on the 
likelihood. There are two issues here, I think. 
 i) The calculations are conditional on the smoothing parameters, which 
   have usually been estimated.
 ii) Given the smoothing parameters, the GAMs are estimated by penalized 
   likelihood maximization, not MLE. 

(ii)is an issue when comparing two or more models using anova.gam, since 
the calculations use the MLE theory based distributional results, but with 
the model degrees of freedom set to the effective degrees of freedom of 
the model. This is not as outrageous as it sounds: the GAM can always be 
re-parameterized using the eigen-vectors of its penalty matrix. This 
parameterization is orthogonal, and has a diagonal penalty matrix with a 
elements that increase rather smoothly and steeply. This tends to mean 
that, for any given smoothing parameter, some model parameters are 
effectively zeroed while some are effectively un-penalized. If all parameters 
were in one of these two categories then using distributional theory for 
un-penalized GLMs, with model degrees of freedom set to the EDF of the GAM 
would be perfectly legitimate. Of course in reality there are almost 
always a number of parameters subject to intermediate amounts of 
penalization. The approach used in anova.gam is effectively saying 
that these are equivalent to a smaller number of unpenalized parameters, 
which can only be an approximation!

Note that single argument calls to anova.gam do not use the MLE based 
results. 

Interestingly, taking a mixed model approach (e.g. mgcv::gamm, for simple 
version) doesn't make things much better: in that case you *can* get 
unpenalized MLE's, but testing for presence or absence of smooth terms 
involves LRT's at the boundary of the parameter space, while other tests 
are often conditional on the parameters of the random effect 
distributions... 
 

> I have a question about how to compare a GLM with a GAM model using anova
> function.
> 
> A GLM is performed for example: 
> 
> model1 <-glm(formula = exitus ~ age+gender+diabetes, family = "binomial",
> na.action = na.exclude)
> 
> A second nested model could be:
> 
> model2 <-glm(formula = exitus ~ age+gender, family = "binomial", na.action =
> na.exclude)
> 
>  
> 
> To compare these two GLM models the instruction is: 
> 
> anova(model1,model2, test="F")
> 
>  
> 
> Similarly for GAM models
> 
> model3 <-gam(formula = exitus ~ s(age)+gender, family = "binomial",
> na.action = na.exclude)
> 
>  
> 
> "R" allows to compare these two models (GLM vs. GAM)
> 
> anova(model2,model3, test="F") 
> 
> This instruction returns a p-value with no error or warning, but this test
> is based on maximum likelihood, and GAM models are not fitted with maximum
> likelihood criteria, thus I think this p-value is not correct.
> 
> Please, I really appreciate any information about how to compare a GLM with
> a GAM model. 


best,
Simon
_____________________________________________________________________
> Simon Wood simon at stats.gla.ac.uk        www.stats.gla.ac.uk/~simon/
>>  Department of Statistics, University of Glasgow, Glasgow, G12 8QQ
>>>   Direct telephone: (0)141 330 4530          Fax: (0)141 330 4814



From Gerbehj at unisa.ac.za  Thu Oct 28 14:03:23 2004
From: Gerbehj at unisa.ac.za (H J Gerber)
Date: Thu, 28 Oct 2004 14:03:23 +0200
Subject: [R] Create factors in R for more than one variable
Message-ID: <s180fc36.059@alpha.unisa.ac.za>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20041028/52c99cb2/attachment.pl

From r.alberts at biol.rug.nl  Thu Oct 28 14:05:31 2004
From: r.alberts at biol.rug.nl (R.Alberts)
Date: Thu, 28 Oct 2004 14:05:31 +0200
Subject: [R] RMySQL connection
In-Reply-To: <A85E3040-28C8-11D9-8349-000A95D7BA10@mail.nih.gov>
References: <web-472059@biol.rug.nl>
	<A85E3040-28C8-11D9-8349-000A95D7BA10@mail.nih.gov>
Message-ID: <web-473863@biol.rug.nl>

The problem has been solved. I didnt install the mysql 
libraries and header files properly.

To solve it, I installed:
MySQL-client-4.1.7-0.i386.rpm
MySQL-server-4.1.7-0.i386.rpm
AND
MySQL-devel-4.1.7-0.i386.rpm  (libraries and header files)

then I installed RMySQL using:

export PKG_CPPFLAGS="-I/usr/include/mysql"
export PKG_LIBS="-L/usr/lib/mysql -lmysqlclient"

R CMD INSTALL -l /usr/local/lib/R RMySQL_0.5-5.tar.gz


thanks, Rudi.


On Thu, 28 Oct 2004 06:03:46 -0400
  Sean Davis <sdavis2 at mail.nih.gov> wrote:
> 
> On Oct 28, 2004, at 4:37 AM, R.Alberts wrote:
> 
>> Hi,
>>
>> I cannot connect to my mysql database from within R 
>>using RMySQL. What 
>> am I doing wrong?
>>
>> I use linux SuSe 8.1 and R-2.0.0
>> RMySQL_0.5-5.tar.gz
>> DBI_0.1-8.tar.gz
>>
>> I get the following error in R:
>>
>>> library(RMySQL)
>> Loading required package: DBI
>>> m <- dbDriver("MySQL")
>>> m
>> <MySQLDriver:(1756)>
>>> con <- dbConnect(m, group = "mygroup")
>> Error in mysqlNewConnection(drv, ...) : RS-DBI driver: 
>>(could not 
>> connect myuser at localhost on dbname "mydatabase")
> 
> On the mysql server, there is a user named myuser?  The 
>password for this user is xxxxxx (or whatever)?  The 
>database is called mydatabase?  The host for the server 
>is localhost (mysql is running on the same machine as R)? 
> That user/password combination has access to mydatabase?
> 
> Can you connect from the mysql client on the same 
>machine with those settings?
> 
> Sean
>



From ripley at stats.ox.ac.uk  Thu Oct 28 14:21:16 2004
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Thu, 28 Oct 2004 13:21:16 +0100 (BST)
Subject: [R] Create factors in R for more than one variable
In-Reply-To: <s180fc36.059@alpha.unisa.ac.za>
Message-ID: <Pine.LNX.4.44.0410281315380.2267-100000@gannet.stats>

On Thu, 28 Oct 2004, H J Gerber wrote:

> Hi all,
>  
> I'm using R2.0 on linux. 

As there is no such version, unlikely.

> I have fixedwidth data consisting of 50 questions on a likert scale. The raw data is coded 1 to 5. I used read.fwf() to read in the data. 
> The problem is that factors isn't created automatically, because the data is coded numerically.
> To make factors out of the 50 questions I must do:
> V1 <- factor(V1)
> I don't want to do it 50 times. Is there a quicker way without using loops?

Not really (and a loop will a matter of seconds at most), but there are 
neater ways such as

DF[] <- lapply(DF as.factor)

which will convert it for you.  Or you could get read.fwf() to do so: see
the colClasses argument to read.table, as mentioned on its help page.

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From emanuela.rossi at unimib.it  Thu Oct 28 14:32:41 2004
From: emanuela.rossi at unimib.it (Emanuela Rossi)
Date: Thu, 28 Oct 2004 14:32:41 +0200
Subject: [R] meaning of frailty estimates
Message-ID: <018701c4bcea$37aeaca0$36ec8495@Emanuela>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20041028/9c6208cc/attachment.pl

From sdavis2 at mail.nih.gov  Thu Oct 28 15:29:46 2004
From: sdavis2 at mail.nih.gov (Sean Davis)
Date: Thu, 28 Oct 2004 09:29:46 -0400
Subject: [R] Quick data-manipulation question
Message-ID: <6F705F54-28E5-11D9-8349-000A95D7BA10@mail.nih.gov>

I have a list of data frames and I want to concatenate them into a 
single data frame, basically appending all of the data frames to each 
other (they are all the same shape, in terms of columns).  I'm looking 
for a nice way to do that.  I can of course just consecutively rbind 
them to a "master" dataframe, but I have 22,000 such data frames, each 
with a few hundred rows, so this process takes a good while.  Should be 
simple, I imagine....

Here is a toy data structure.
j <- list()
for (i in letters[1:26]) {j[[i]] <- 
data.frame(rep(i,25),matrix(rnorm(250),nrow=25))}

Thanks.

Sean



From alexandersokol at ofir.dk  Thu Oct 28 15:30:29 2004
From: alexandersokol at ofir.dk (Alexander Sokol)
Date: Thu, 28 Oct 2004 15:30:29 +0200
Subject: [R] Problem with "outer" function
Message-ID: <41884295@webmail2.ofir.dk>

Hello,

I am having a problem with the "outer" function.

I am using R 1.9.1 on Windows 2000. My problem may be described as follows:

I have a function f of 3 variables and 3 vectors, aa, bb and cc. I would like 
to evaluate this function in a 3-dimensional grid of points corresponding to 
all combinations of the elements of the 3 vectors. For example, if

aa<-c(1,2)
bb<-c(3,4)
cc<-c(5,6)

then I'd like a loop which evaluates the 2^3 values f(1,3,5), f(1,3,6), 
f(1,4,5), f(1,4,6),f(2,3,5),f(2,3,6),f(2,4,5) and f(2,4,6).

The easy way to do this would be to nest 3 for-next loops, but I hope that I 
more elegant way is possible, especially because in the actual problem at hand 
I need to make such evaluations with a function of n parameters. I was hoping 
to create a 3-dimensional array consisting of 3-element vectors, and then be 
able to use a single loop to get through all the values. I expected to do this 
with

outer(outer(aa,bb,c),cc,c)

However, I get the following error when I use the "outer" function with the 
concatenate function "c" as the third argument:

>outer(aa,bb,c)
Error in outer(aa,bb,c) : dim<- : dims [product 4] do not match the length of 
object [8]

Does anyone know a way to get around this? I have consulted the R manuals 
without results, and I can't seem to find the problem described in the mail 
archives either.

Thanks,
 Alexander



From andy_liaw at merck.com  Thu Oct 28 15:47:09 2004
From: andy_liaw at merck.com (Liaw, Andy)
Date: Thu, 28 Oct 2004 09:47:09 -0400
Subject: [R] Quick data-manipulation question
Message-ID: <3A822319EB35174CA3714066D590DCD50994E242@usrymx25.merck.com>

I do not know of a better way than to do something like do.call("rbind",
listOfDataFrame).  This will take a while because rbind.data.frame needs to
check and make sure the result is a valid data.frame (e.g., rownames need to
be unique).  If all the columns are of the same type, you might find it
faster to coerce to matrices, rbind, then coerce back to data.frame.

HTH,
Andy

> From: Sean Davis
> 
> I have a list of data frames and I want to concatenate them into a 
> single data frame, basically appending all of the data frames to each 
> other (they are all the same shape, in terms of columns).  
> I'm looking 
> for a nice way to do that.  I can of course just consecutively rbind 
> them to a "master" dataframe, but I have 22,000 such data 
> frames, each 
> with a few hundred rows, so this process takes a good while.  
> Should be 
> simple, I imagine....
> 
> Here is a toy data structure.
> j <- list()
> for (i in letters[1:26]) {j[[i]] <- 
> data.frame(rep(i,25),matrix(rnorm(250),nrow=25))}
> 
> Thanks.
> 
> Sean
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
> 
>



From Matthias.Schmidt at forst.bwl.de  Thu Oct 28 15:54:34 2004
From: Matthias.Schmidt at forst.bwl.de (Schmidt.Matthias (FORST))
Date: Thu, 28 Oct 2004 15:54:34 +0200
Subject: [R] polr versus multinom
Message-ID: <855D381F618DE84FA58C035BA803FCE6B9A873@fvafr-se1.forst.bwl.de>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20041028/8300b55b/attachment.pl

From matthew_wiener at merck.com  Thu Oct 28 15:57:52 2004
From: matthew_wiener at merck.com (Wiener, Matthew)
Date: Thu, 28 Oct 2004 09:57:52 -0400
Subject: [R] Quick data-manipulation question
Message-ID: <45AAE6FD142DCB43A38C00A11FF5DF3E0222631E@uswsmx03.merck.com>

Sean --

You want do.call("rbind", your.list.of.data.frames).  do.call is helpful in
a lot of situations when you want to construct a list of arguments and then
apply a function.

Hope this helps,

Matt Wiener

-----Original Message-----
From: r-help-bounces at stat.math.ethz.ch
[mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Sean Davis
Sent: Thursday, October 28, 2004 9:30 AM
To: r-help
Subject: [R] Quick data-manipulation question


I have a list of data frames and I want to concatenate them into a 
single data frame, basically appending all of the data frames to each 
other (they are all the same shape, in terms of columns).  I'm looking 
for a nice way to do that.  I can of course just consecutively rbind 
them to a "master" dataframe, but I have 22,000 such data frames, each 
with a few hundred rows, so this process takes a good while.  Should be 
simple, I imagine....

Here is a toy data structure.
j <- list()
for (i in letters[1:26]) {j[[i]] <- 
data.frame(rep(i,25),matrix(rnorm(250),nrow=25))}

Thanks.

Sean

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide!
http://www.R-project.org/posting-guide.html



From ripley at stats.ox.ac.uk  Thu Oct 28 16:06:52 2004
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Thu, 28 Oct 2004 15:06:52 +0100 (BST)
Subject: [R] polr versus multinom
In-Reply-To: <855D381F618DE84FA58C035BA803FCE6B9A873@fvafr-se1.forst.bwl.de>
Message-ID: <Pine.LNX.4.44.0410281501340.3564-100000@gannet.stats>

Both polr and multinom are part of the support software for a book.  That
book covers your question.  Please read it, as you were asked to do.

On Thu, 28 Oct 2004, Schmidt.Matthias (FORST) wrote:

> I am searching for methods to compare regression models with an ordered
> categorical response variable (polr versus multinom).
> The pattern of predictions of both methods (using the same predictor
> variables) is quite different and the AIC is smaller for the multinom
> approach. I guess polr has more strict premises for the structure of the
> response variable, which methods can be used to test for these premises. If
> the AIC is smaller for the multinom approach is this approach more
> appropriate in every case?

> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html

  `If the function is from a package accompanying a book, e.g., the MASS 
   package, consult the book before posting.'

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From tlumley at u.washington.edu  Thu Oct 28 16:08:54 2004
From: tlumley at u.washington.edu (Thomas Lumley)
Date: Thu, 28 Oct 2004 07:08:54 -0700 (PDT)
Subject: [R] meaning of frailty estimates
In-Reply-To: <018701c4bcea$37aeaca0$36ec8495@Emanuela>
References: <018701c4bcea$37aeaca0$36ec8495@Emanuela>
Message-ID: <Pine.A41.4.61b.0410280707430.26124@homer04.u.washington.edu>

On Thu, 28 Oct 2004, Emanuela Rossi wrote:

> Hello,
>
> I'm trying to estimate a Cox's survival model with a random effect, so I 
> have added the instruction frailty.gaussian (name variable) in the 
> model.
>
> My frailty variable is a qualitative variable with four types of answer.
>
> In the resulting output there are the parameter estimates of all the 
> variables, but there are also four estimates for each type of answer of 
> the frailty variable. Which kind of estimates are they? Maybe hazard 
> ratio? But which is the reference?
>

Perhaps you could post this output so we know what you are talking about.

 	-thomas



From barrystaylor at yahoo.com  Thu Oct 28 16:14:51 2004
From: barrystaylor at yahoo.com (Barry Taylor)
Date: Thu, 28 Oct 2004 07:14:51 -0700 (PDT)
Subject: [R] matching on multiple minimums
Message-ID: <20041028141451.51553.qmail@web61003.mail.yahoo.com>

Hi,

I was wondering if someone has witnessed the following
behavior with matching on multiple minimums in a
vector:

If I manually create a vector of doubles:
test <- c(2.33,11.09,0.02,0.02.1.7,6.41)

I can then run any of the following to retrieve the
indices of the minimum values (3 and 4 in this case),
of which there are two (done purposefully):

which(test==min(test))
or
which(!is.na(match(test,min(test))))
or
match(test,min(test)) # to see the entire vector

However, my problem is as follows. I am not manually
creating a vector to test for multiple minimums but
rather doing a read.table on a file that contains the
same data with other columns, etc. I then trim off the
data from that matrix (there are several identical
ways to do this):

unlist(myMatrix[1], use.name=FALSE)
or
myMatrix[,1]

But, running any of the tests listed above only
returns the index of the last minimum. Both my manual
vector and vector culled from the matrix appear to me
to be identical (tested with is.vector, is.numeric,
is.double, etc), but it will only every return the
index of the second of the two minimums. Any help
would be truly appreciated.

Thanks
Barry



From Gerbehj at unisa.ac.za  Thu Oct 28 16:18:42 2004
From: Gerbehj at unisa.ac.za (H J Gerber)
Date: Thu, 28 Oct 2004 16:18:42 +0200
Subject: [R] Create factors in R for more than one variable
Message-ID: <s1811bf5.049@alpha.unisa.ac.za>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20041028/7b2ef764/attachment.pl

From rksh at soc.soton.ac.uk  Thu Oct 28 16:22:28 2004
From: rksh at soc.soton.ac.uk (Robin Hankin)
Date: Thu, 28 Oct 2004 15:22:28 +0100
Subject: [R] Problem with "outer" function
In-Reply-To: <41884295@webmail2.ofir.dk>
References: <41884295@webmail2.ofir.dk>
Message-ID: <a06002003bda6b07132a2@[139.166.242.29]>

Hi

you need expand.grid():


  aa <- c(1,2)
>  bb <- c(3,4)
>  cc <- c(5,6)
>  expand.grid(aa,bb,cc)
   Var1 Var2 Var3
1    1    3    5
2    2    3    5
3    1    4    5
4    2    4    5
5    1    3    6
6    2    3    6
7    1    4    6
8    2    4    6


#define a toy function (here just a max() but you can make
# this function anything you like:
>  f <- function(x){max(x[1] , x[2] ,x[3])}

#then just apply() the toy function f():

>  apply(expand.grid(aa,bb,cc),1,f)
1 2 3 4 5 6 7 8
5 5 5 5 6 6 6 6



HTH

rksh



>,
>
>I am having a problem with the "outer" function.
>
>I am using R 1.9.1 on Windows 2000. My problem may be described as follows:
>
>I have a function f of 3 variables and 3 vectors, aa, bb and cc. I would like
>to evaluate this function in a 3-dimensional grid of points corresponding to
>all combinations of the elements of the 3 vectors. For example, if
>
>aa<-c(1,2)
>bb<-c(3,4)
>cc<-c(5,6)
>
>then I'd like a loop which evaluates the 2^3 values f(1,3,5), f(1,3,6),
>f(1,4,5), f(1,4,6),f(2,3,5),f(2,3,6),f(2,4,5) and f(2,4,6).
>
>The easy way to do this would be to nest 3 for-next loops, but I hope that I
>more elegant way is possible, especially because in the actual problem at hand
>I need to make such evaluations with a function of n parameters. I was hoping
>to create a 3-dimensional array consisting of 3-element vectors, and then be
>able to use a single loop to get through all the values. I expected to do this
>with
>
>outer(outer(aa,bb,c),cc,c)
>
>However, I get the following error when I use the "outer" function with the
>concatenate function "c" as the third argument:
>
>>outer(aa,bb,c)
>Error in outer(aa,bb,c) : dim<- : dims [product 4] do not match the length of
>object [8]
>
>Does anyone know a way to get around this? I have consulted the R manuals
>without results, and I can't seem to find the problem described in the mail
>archives either.
>
>Thanks,
>  Alexander
>
>______________________________________________
>R-help at stat.math.ethz.ch mailing list
>https://stat.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html


-- 
Robin Hankin
Uncertainty Analyst
Southampton Oceanography Centre
SO14 3ZH
tel +44(0)23-8059-7743
initialDOTsurname at soc.soton.ac.uk (edit in obvious way; spam precaution)



From ripley at stats.ox.ac.uk  Thu Oct 28 16:30:28 2004
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Thu, 28 Oct 2004 15:30:28 +0100 (BST)
Subject: [R] matching on multiple minimums
In-Reply-To: <20041028141451.51553.qmail@web61003.mail.yahoo.com>
Message-ID: <Pine.LNX.4.44.0410281529100.3716-100000@gannet.stats>

Please read ?"==" for comments and a suggested solution.

On Thu, 28 Oct 2004, Barry Taylor wrote:

> Hi,
> 
> I was wondering if someone has witnessed the following
> behavior with matching on multiple minimums in a
> vector:
> 
> If I manually create a vector of doubles:
> test <- c(2.33,11.09,0.02,0.02.1.7,6.41)
> 
> I can then run any of the following to retrieve the
> indices of the minimum values (3 and 4 in this case),
> of which there are two (done purposefully):
> 
> which(test==min(test))
> or
> which(!is.na(match(test,min(test))))
> or
> match(test,min(test)) # to see the entire vector
> 
> However, my problem is as follows. I am not manually
> creating a vector to test for multiple minimums but
> rather doing a read.table on a file that contains the
> same data with other columns, etc. I then trim off the
> data from that matrix (there are several identical
> ways to do this):
> 
> unlist(myMatrix[1], use.name=FALSE)
> or
> myMatrix[,1]
> 
> But, running any of the tests listed above only
> returns the index of the last minimum. Both my manual
> vector and vector culled from the matrix appear to me
> to be identical (tested with is.vector, is.numeric,
> is.double, etc), but it will only every return the
> index of the second of the two minimums. Any help
> would be truly appreciated.

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From guan.xing at case.edu  Thu Oct 28 16:35:53 2004
From: guan.xing at case.edu (Guan Xing)
Date: Thu, 28 Oct 2004 10:35:53 -0400
Subject: [R] qustion with lars (lasso) package
Message-ID: <5d601e5d5192.5d51925d601e@cwru.edu>

Dear All,

I am using lars package written by Dr. Trevor Hastie, the version is  lars_0.9-5 downloaded from cran. When I ran the diabetes example data attached in package, I found that the beta outputs from different machines are different. The difference is only about 10^-11 to 10^-12, some friends suggested that it possibly is a machine precision problem. But I check the machine numerical characteristics with command 
" str(.Machine)", all four machines I am using give me same information. I am wondering whether someone has noticed this problem before. Could I make some restriction and get the same results on all machines

Thanks,

Guan Xing



From emanuela.rossi at unimib.it  Thu Oct 28 16:37:51 2004
From: emanuela.rossi at unimib.it (Emanuela Rossi)
Date: Thu, 28 Oct 2004 16:37:51 +0200
Subject: [R] meaning of frailty estimates
References: <018701c4bcea$37aeaca0$36ec8495@Emanuela>
	<Pine.A41.4.61b.0410280707430.26124@homer04.u.washington.edu>
Message-ID: <01cd01c4bcfb$b4262b30$36ec8495@Emanuela>

Here the output. "Specie" is the frailty variable, with four types of
answer. I'm asking what's the meaning of gauss:1, ....gauss:4

----------------------------------------------------------------------------
------------------------------

>
SURV1<-read.table("C:/LAVORO/MOBILE/survival/surv_n1.csv",header=TRUE,sep=",
")
> fit_13_2_sp<-coxph(Surv(DATA_INI1,DATA_FIN1,EVENT1)~
V1+V2+Z+G+dim+frailty.gaussian(specie)+cluster(ID),data=SURV1)
> summary(fit_13_2_sp)
Call:
coxph(formula = Surv(DATA_INI1, DATA_FIN1, EVENT1) ~ V1 + V2 +
    Z + G + dim + frailty.gaussian(specie) + cluster(ID), data = SURV1)

  n= 233450
                         coef       se(coef)   se2             Chisq  DF
p
V1                  0.04995   0.14021  0.145916    0.13  1.00  7.2e-01
V2                 -0.79656   0.20483  0.197135  15.12 1.00  1.0e-04
Z                    -0.00359  0.00067  0.000841   28.76 1.00  8.2e-08
G                     0.08186  0.00583  0.005796 196.91 1.00  0.0e+00
dim                  0.39410  0.06981  0.057294   31.87 1.00  1.6e-08
frailty.gaussian(specie)                                    181.44 2.95
0.0e+00

        exp(coef) exp(-coef) lower .95 upper .95
V1          0.951      1.051     0.723     1.252
V2          0.451      2.218     0.302     0.674
Z           0.996      1.004     0.995     0.998
G           1.085      0.921     1.073     1.098
dim         1.483      0.674     1.293     1.700
gauss:1     2.504      0.399     2.108     2.975
gauss:2     1.799      0.556     1.457     2.221
gauss:3     0.278      3.599     0.217     0.356
gauss:4     0.799      1.252     0.641     0.996

Iterations: 5 outer, 11 Newton-Raphson
     Variance of random effect= 0.975
Degrees of freedom for terms= 1 1 1 1 1 3
Rsquare= 0.003   (max possible= 0.025 )
Likelihood ratio test= 810  on 7.95 df,   p=0
Wald test            = 600  on 7.95 df,   p=0,   Robust = 355  p=0

----------------------------------------------------------------------------
-------------------------------------------


Thanks

Emanuela


----- Original Message ----- 
From: "Thomas Lumley" <tlumley at u.washington.edu>
To: "Emanuela Rossi" <emanuela.rossi at unimib.it>
Cc: <r-help at stat.math.ethz.ch>
Sent: Thursday, October 28, 2004 4:08 PM
Subject: Re: [R] meaning of frailty estimates


> On Thu, 28 Oct 2004, Emanuela Rossi wrote:
>
> > Hello,
> >
> > I'm trying to estimate a Cox's survival model with a random effect, so I
> > have added the instruction frailty.gaussian (name variable) in the
> > model.
> >
> > My frailty variable is a qualitative variable with four types of answer.
> >
> > In the resulting output there are the parameter estimates of all the
> > variables, but there are also four estimates for each type of answer of
> > the frailty variable. Which kind of estimates are they? Maybe hazard
> > ratio? But which is the reference?
> >
>
> Perhaps you could post this output so we know what you are talking about.
>
>   -thomas
>



From dinos at northwestern.edu  Thu Oct 28 16:51:03 2004
From: dinos at northwestern.edu (dinos@northwestern.edu)
Date: Thu, 28 Oct 2004 14:51:03 +0000
Subject: [R] Fw: (No subject)
Message-ID: <200410281451.i9SEp9m5018665@lulu.it.northwestern.edu>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20041028/3f16f39d/attachment.pl

From dinos at northwestern.edu  Thu Oct 28 17:02:15 2004
From: dinos at northwestern.edu (dinos@northwestern.edu)
Date: Thu, 28 Oct 2004 15:02:15 +0000
Subject: [R] plot.baysian error = only 0's may mix with negative subscripts
Message-ID: <200410281502.i9SF2TBX014947@lulu.it.northwestern.edu>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20041028/ad9517aa/attachment.pl

From ramasamy at cancer.org.uk  Thu Oct 28 17:04:44 2004
From: ramasamy at cancer.org.uk (Adaikalavan Ramasamy)
Date: Thu, 28 Oct 2004 16:04:44 +0100
Subject: [R] matching on multiple minimums
In-Reply-To: <20041028141451.51553.qmail@web61003.mail.yahoo.com>
References: <20041028141451.51553.qmail@web61003.mail.yahoo.com>
Message-ID: <1098975884.5220.11.camel@ramasamy.stats>

I cannot make sense of your question. There is no reproducible example
to help either. Please read the posting guide.

However, I think you might find using arr.ind=TRUE option in which()
when dealing with matrices to be useful. See help(which). Example :


mat <- matrix( 1:9, nc=3 ) + 100
mat[3,1] <- 101
mat
     [,1] [,2] [,3]
[1,]  101  104  107
[2,]  102  105  108
[3,]  101  106  109


which( mat == min(mat), arr.ind=T )
     row col
[1,]   1   1
[2,]   3   1


m1 <- mat[ ,1]
which( m1 == min(m1) )
[1] 1 3



On Thu, 2004-10-28 at 15:14, Barry Taylor wrote:
> Hi,
> 
> I was wondering if someone has witnessed the following
> behavior with matching on multiple minimums in a
> vector:
> 
> If I manually create a vector of doubles:
> test <- c(2.33,11.09,0.02,0.02.1.7,6.41)
> 
> I can then run any of the following to retrieve the
> indices of the minimum values (3 and 4 in this case),
> of which there are two (done purposefully):
> 
> which(test==min(test))
> or
> which(!is.na(match(test,min(test))))
> or
> match(test,min(test)) # to see the entire vector
> 
> However, my problem is as follows. I am not manually
> creating a vector to test for multiple minimums but
> rather doing a read.table on a file that contains the
> same data with other columns, etc. I then trim off the
> data from that matrix (there are several identical
> ways to do this):
> 
> unlist(myMatrix[1], use.name=FALSE)
> or
> myMatrix[,1]
> 
> But, running any of the tests listed above only
> returns the index of the last minimum. Both my manual
> vector and vector culled from the matrix appear to me
> to be identical (tested with is.vector, is.numeric,
> is.double, etc), but it will only every return the
> index of the second of the two minimums. Any help
> would be truly appreciated.
> 
> Thanks
> Barry
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>



From h.wickham at gmail.com  Thu Oct 28 17:28:58 2004
From: h.wickham at gmail.com (hadley wickham)
Date: Thu, 28 Oct 2004 10:28:58 -0500
Subject: [R] Quick data-manipulation question
In-Reply-To: <6F705F54-28E5-11D9-8349-000A95D7BA10@mail.nih.gov>
References: <6F705F54-28E5-11D9-8349-000A95D7BA10@mail.nih.gov>
Message-ID: <f8e6ff05041028082822d29397@mail.gmail.com>

How about do.call("rbind", j) ?

Hadley

On Thu, 28 Oct 2004 09:29:46 -0400, Sean Davis <sdavis2 at mail.nih.gov> wrote:
> I have a list of data frames and I want to concatenate them into a
> single data frame, basically appending all of the data frames to each
> other (they are all the same shape, in terms of columns).  I'm looking
> for a nice way to do that.  I can of course just consecutively rbind
> them to a "master" dataframe, but I have 22,000 such data frames, each
> with a few hundred rows, so this process takes a good while.  Should be
> simple, I imagine....
> 
> Here is a toy data structure.
> j <- list()
> for (i in letters[1:26]) {j[[i]] <-
> data.frame(rep(i,25),matrix(rnorm(250),nrow=25))}
> 
> Thanks.
> 
> Sean
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>



From ripley at stats.ox.ac.uk  Thu Oct 28 17:30:01 2004
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Thu, 28 Oct 2004 16:30:01 +0100 (BST)
Subject: [R] qustion with lars (lasso) package
In-Reply-To: <5d601e5d5192.5d51925d601e@cwru.edu>
Message-ID: <Pine.LNX.4.44.0410281623440.3846-100000@gannet.stats>

On Thu, 28 Oct 2004, Guan Xing wrote:

> I am using lars package written by Dr. Trevor Hastie, the version is
> lars_0.9-5 downloaded from cran. When I ran the diabetes example data
> attached in package, I found that the beta outputs from different
> machines are different. The difference is only about 10^-11 to 10^-12,
> some friends suggested that it possibly is a machine precision problem.
> But I check the machine numerical characteristics with command "
> str(.Machine)", all four machines I am using give me same information. I
> am wondering whether someone has noticed this problem before. Could I
> make some restriction and get the same results on all machines

Yes. Run exactly the same version of R (that is, the exact same files) on 
all the machines, if they have exactly the same OS and similar chips.

Otherwise you and your friends need to take a course in numerical methods.
Even different versions of the same compiler will produce different code 
ordering operations differently and some give different results.  For 
example, R built with --enable-R-shlib gives different answers to that 
built without, and R using different BLAS libraries can give quite 
different answers.

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From ripley at stats.ox.ac.uk  Thu Oct 28 17:07:26 2004
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Thu, 28 Oct 2004 16:07:26 +0100 (BST)
Subject: [R] [R-pkgs] Package foreign_0.8-0 is on CRAN
Message-ID: <Pine.LNX.4.44.0410281558340.3752-100000@gannet.stats>

There is an update for the `foreign' package now on the CRAN master and 
soon on a mirror near you.

This adds support for

DBF files (by Nicholas Lewin-Koh, Roger Bivand and myself)
Systat files written on MS-DOS/Windows (by Roger Bivand)

The reasons for announcing this update are

1) People quite often ask for DBF support.

2) There is a `maptools' update that depends on getting this one.

3) We have tested many Systat and DBF files, and even created some extreme
examples of our own.  But many applications write DBF files and inevitably
there are rogue files out there, so we would appreciate seeing examples
that either do not work as expected or crash R.

The `R Data Import/Export' manual in R-patched and R-devel has been 
revised to cover the new functionality.

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595

_______________________________________________
R-packages mailing list
R-packages at stat.math.ethz.ch
https://stat.ethz.ch/mailman/listinfo/r-packages



From tplate at acm.org  Thu Oct 28 17:55:25 2004
From: tplate at acm.org (Tony Plate)
Date: Thu, 28 Oct 2004 09:55:25 -0600
Subject: [R] gsub() on Matrix
In-Reply-To: <Pine.GSO.4.58.0410281306500.4083@yin>
References: <Pine.GSO.4.58.0410281306500.4083@yin>
Message-ID: <6.1.0.6.2.20041028094550.0b927030@mailhost.blackmesacapital.com>

Many more recent regular expression implementations have ways of indicating 
a match on a word boundary.  It's usually "\b".

Here's what you did:

 > gsub("x1", "i1", "x1 + x2 + x10 + xx1")
[1] "i1 + x2 + i10 + xi1"

The following worked for me to just change "x1" to "i1", while leaving 
alone any larger "word" that contains "x1":

 > gsub("\\bx1\\b", "i1", "x1 + x2 + x10 + xx1")
[1] "i1 + x2 + x10 + xx1"
 >

Note that the backslash must be escaped itself to get past the R lexical 
analyser, which is independent of the regexp processor.  What the regexp 
processor sees is just a single backslash.

For more on this, look for perl documentation of regular expressions.  Be 
aware that to use full perl regexps, you must supply the perl=T argument to 
gsub().  Also note that "\b" seems to be part of the most basic regular 
expression language in R; it even works with extended=F:

 > gsub("\\bx1\\b", "i1", "x1 + x2 + x10 + xx1", perl=T)
[1] "i1 + x2 + x10 + xx1"
 > gsub("\\bx1\\b", "i1", "x1 + x2 + x10 + xx1", perl=F)
[1] "i1 + x2 + x10 + xx1"
 > gsub("\\bx1\\b", "i1", "x1 + x2 + x10 + xx1", perl=F, ext=F)
[1] "i1 + x2 + x10 + xx1"
 >

(I assumed the fact that you have a matrix of strings is not relevant.)

Hope this helps,

Tony Plate

At Wednesday 09:07 PM 10/27/2004, Kevin Wang wrote:
>Hi,
>
>Suppose I've got a matrix, and the first few elements look like
>   "x1 + x3 + x4 + x5 + x1:x3 + x1:x4"
>   "x1 + x2 + x3 + x5 + x1:x2 + x1:x5"
>   "x1 + x3 + x4 + x5 + x1:x3 + x1:x5"
>and so on (have got terms from x1 ~ x14).
>
>If I want to replace all the x1 with i7, all x2 with i14, all x3 with i13,
>for example.  Is there an easy way?
>
>I tried to put what I want to replace in a vector, like:
>  repl = c("i7", "i14", "i13", "d2", "i8", "i5",
>           "i6", "i3", "A", "i9", "i2",
>           "i4", "i15", "i21")
>and have another vector, say:
>   > orig
>  [1] "x1"  "x2"  "x3"  "x4"  "x5"  "x6"  "x7"  "x8"  "x9"  "x10"
>[11] "x11" "x12" "x13" "x14"
>
>Then I tried something like
>   gsub(orig, repl, mat)
>## mat is the name of my matrix
>
>but it didn't work *_*.....it would replace terms like x10 with i70.
>
>(I know it may be an easy question...but I haven't done much regular
>expression)
>
>Cheers,
>
>Kevin
>
>--------------------------------
>Ko-Kang Kevin Wang
>PhD Student
>Centre for Mathematics and its Applications
>Building 27, Room 1004
>Mathematical Sciences Institute (MSI)
>Australian National University
>Canberra, ACT 0200
>Australia
>
>Homepage: http://wwwmaths.anu.edu.au/~wangk/
>Ph (W): +61-2-6125-2431
>Ph (H): +61-2-6125-7407
>Ph (M): +61-40-451-8301
>
>______________________________________________
>R-help at stat.math.ethz.ch mailing list
>https://stat.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html



From Jeremy.Primer at morganstanley.com  Thu Oct 28 18:48:42 2004
From: Jeremy.Primer at morganstanley.com (Primer, Jeremy (FID))
Date: Thu, 28 Oct 2004 12:48:42 -0400
Subject: [R] Auxilliary args in gls
Message-ID: <46E679AFC0427C4B90CF3342E02A538E049859@NYWEXMB31.msad.ms.com>

I am trying to fit a B-spline regression model with a corStruct using
gls. I am using bs() and specifying the knots myself. If I make the
knots data-dependent, this works but has undesirable side-effects. I
prefer to reference an auxilliary variable "knots" in my model formula.
It should not be part of the data frame, as it is a vector of a
different length. How can this be done?

The gls() code suggests it may be difficult to keep the auxillary arg
out of the model.frame but in the evaluation frame of the formula:

    glsSt <- glsStruct(corStruct = correlation, varStruct =
varFunc(weights))
    mfArgs <- list(formula = asOneFormula(formula(glsSt), model, 
        groups), data = data, na.action = na.action)
    if (!missing(subset)) {
        mfArgs[["subset"]] <- asOneSidedFormula(Call[["subset"]])[[2]]
    }
    mfArgs$drop.unused.levels <- TRUE
    dataMod <- do.call("model.frame", mfArgs)


Thanks,
Jeremy Primer 
--------------------------------------------------------
 
This is not an offer (or solicitation of an offer) to buy/sell the securities/instruments mentioned or an official confirmation.  Morgan Stanley may deal as principal in or own or act as market maker for securities/instruments mentioned or may advise the issuers.  This is not research and is not from MS Research but it may refer to a research analyst/research report.  Unless indicated, these views are the author's and may differ from those of Morgan Stanley research or others in the Firm.  We do not represent this is accurate or complete and we may not update this.  Past performance is not indicative of future returns.  For additional information, research reports and important disclosures, contact me or see https://secure.ms.com/servlet/cls.  You should not use e-mail to request, authorize or effect the purchase or sale of any security or instrument, to send transfer instructions, or to effect any other transactions.  We cannot guarantee that any such requests received via e-mail will be processed in a timely manner.  This communication is solely for the addressee(s) and may contain confidential information.  We do not waive confidentiality by mistransmission.  Contact me if you do not wish to receive these communications.  In the UK, this communication is directed in the UK to those persons who are market counterparties or intermediate customers (as defined in the UK Financial Services Authority's rules).



From mlforister at ucdavis.edu  Thu Oct 28 19:21:02 2004
From: mlforister at ucdavis.edu (Matthew Forister)
Date: Thu, 28 Oct 2004 10:21:02 -0700
Subject: [R] read.table in MacOSX
Message-ID: <p05210606bda6d7f956a6@[169.237.66.43]>



I'm running R version 1.9.0 on Mac OS 10.3.5.  I have created text 
files with data that I need to access from R.  When I use the 
read.table command, I get the warning message: "cannot open file"

These text files DO work on a Mac with a previous version of R.

I have tried pasting the data into a new text file, but I just can't 
make a file that the current version of R will open (I have tried 
this both with Word and with the native OSX text edit program).

Am I doing something wrong or is there some incompatibility here?




-- 
= = = = = = = =
Matthew L Forister
Section of Evolution and Ecology
2320 Storer Hall
University of California
One Shields Ave.
Davis, CA 95616
lab phone:(530) 752-2225
= = = = = = = =



From ligges at statistik.uni-dortmund.de  Thu Oct 28 19:43:53 2004
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Thu, 28 Oct 2004 19:43:53 +0200
Subject: [R] plot.baysian error = only 0's may mix with negative subscripts
In-Reply-To: <200410281502.i9SF2TBX014947@lulu.it.northwestern.edu>
References: <200410281502.i9SF2TBX014947@lulu.it.northwestern.edu>
Message-ID: <41812FD9.4000506@statistik.uni-dortmund.de>

dinos at northwestern.edu wrote:
> Dear R users and developers
> 
> After upgrading to Windows XP and R 1.9.1 and 2.0, I retried to execute
> plot.baysian() to a data set that I had used previously to plot with no
> problem in win2000 R1.8.  The error I get is:
> 
> Error in points(Mbar[-index], lods[-index], pch = ".") : 
>         only 0's may mix with negative subscripts

What is plot.baysian()?
Do you mean  plot.bayesian()  from package "sma"?
Probably an error has been fixed that caused your data to be plotted 
inaccurately in R < 1.9.x ... or there is a bug in sma.
But without the data it's hard to say.

So the suggestion is to ask the package maintainer of sma by providing a 
simple and easily reproducible example...

Uwe Ligges


> Thanx in advance
> Dino
> P.S.  I allready sent this message once but without a subject.  I
> apologize for the inconvenience

P.S. And with this message we do know thrice that you are:


> Konstantinos G. Liolios
> IT Software Engineer II
> Charles E. and Emma H. Morrison
> Depts.  Pathology and Microbiology-Immunology
> Northwestern University
> Ward 3-240
> 303 E. Chicago Avenue
> Chicago IL 60611
> Tel: 312-503-0224
> Fax:312-503-0281
> http://www.haldarlab.northwestern.edu

[I removed the next two information blocks who you are, since I think 
it's sufficient for most of us to have one of these blocks...]



From biocperi at yahoo.com  Thu Oct 28 19:49:26 2004
From: biocperi at yahoo.com (S Peri)
Date: Thu, 28 Oct 2004 10:49:26 -0700 (PDT)
Subject: [R] Running R on a grid engine
Message-ID: <20041028174926.10340.qmail@web50010.mail.yahoo.com>

Dear Group, 
 I am using DEAL package for modeling signal
transduction nets.  This process is deal slow on a
SunFire server with 8 gigs ram. 

we have a grid that can process much faster that one
individual server.  

However, to start the process in Grid, I have to give
a command or submit a batch process. 

Is there any way, I can run R in bach process. 

I tried the following:

R CMD | library(deal) | data <-
data.frame(read.table('file1',header=TRUE,
row.names=1))

Here I do not know know :

1. How can I point my data files to a function. 
2. creating a function (other than in R environment)


Could any one help me. 

Thank you in advance.


Cheers
Peri.



From kshe4 at student.monash.edu  Thu Oct 28 19:54:31 2004
From: kshe4 at student.monash.edu (Kunal Shetty)
Date: Thu, 28 Oct 2004 17:54:31 +0000
Subject: [R] array problem and for looping
Message-ID: <220.253.2.55.1098985889.48213@my.monash.edu.au>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20041028/dbf887a9/attachment.pl
-------------- next part --------------
An embedded and charset-unspecified text was scrubbed...
Name: random1.txt
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20041028/dbf887a9/random1.txt

From p.dalgaard at biostat.ku.dk  Thu Oct 28 20:03:44 2004
From: p.dalgaard at biostat.ku.dk (Peter Dalgaard)
Date: 28 Oct 2004 20:03:44 +0200
Subject: [R] gsub() on Matrix
In-Reply-To: <6.1.0.6.2.20041028094550.0b927030@mailhost.blackmesacapital.com>
References: <Pine.GSO.4.58.0410281306500.4083@yin>
	<6.1.0.6.2.20041028094550.0b927030@mailhost.blackmesacapital.com>
Message-ID: <x2k6ta7m9r.fsf@biostat.ku.dk>

Tony Plate <tplate at acm.org> writes:

> Many more recent regular expression implementations have ways of
> indicating a match on a word boundary.  It's usually "\b".
....

Another idea is that if what you need is something that is parseable
as a model formula RHS, then you might want to parse first and
substitute later. Something along these lines:

> e <- parse(text="x1 + x3 + x4 + x5 + x1:x3 + x1:x4")[[1]]
> repl = lapply(c("i7", "i14", "i13", "d2", "i8", "i5"),as.name)
> names(repl)<-paste("x",1:6,sep="")
> eval(substitute(substitute(e,repl),list(e=e)))
i7 + i13 + d2 + i8 + i7:i13 + i7:d2


> At Wednesday 09:07 PM 10/27/2004, Kevin Wang wrote:
> >Suppose I've got a matrix, and the first few elements look like
> >   "x1 + x3 + x4 + x5 + x1:x3 + x1:x4"
> >   "x1 + x2 + x3 + x5 + x1:x2 + x1:x5"
> >   "x1 + x3 + x4 + x5 + x1:x3 + x1:x5"
> >and so on (have got terms from x1 ~ x14).
> >
> >If I want to replace all the x1 with i7, all x2 with i14, all x3 with i13,
> >for example.  Is there an easy way?
> >
> >I tried to put what I want to replace in a vector, like:
> >  repl = c("i7", "i14", "i13", "d2", "i8", "i5",
> >           "i6", "i3", "A", "i9", "i2",
> >           "i4", "i15", "i21")
> >and have another vector, say:
> >   > orig
> >  [1] "x1"  "x2"  "x3"  "x4"  "x5"  "x6"  "x7"  "x8"  "x9"  "x10"
> >[11] "x11" "x12" "x13" "x14"
> >
> >Then I tried something like
> >   gsub(orig, repl, mat)
> >## mat is the name of my matrix
> >
> >but it didn't work *_*.....it would replace terms like x10 with i70.

-- 
   O__  ---- Peter Dalgaard             Blegdamsvej 3  
  c/ /'_ --- Dept. of Biostatistics     2200 Cph. N   
 (*) \(*) -- University of Copenhagen   Denmark      Ph: (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)             FAX: (+45) 35327907



From kshe4 at student.monash.edu  Thu Oct 28 20:16:36 2004
From: kshe4 at student.monash.edu (Kunal Shetty)
Date: Thu, 28 Oct 2004 18:16:36 +0000
Subject: [R] Loop conditioning situation
Message-ID: <220.253.2.55.1098987332.96915@my.monash.edu.au>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20041028/8c393a16/attachment.pl

From trafton at itd.nrl.navy.mil  Thu Oct 28 20:23:20 2004
From: trafton at itd.nrl.navy.mil (Greg Trafton)
Date: Thu, 28 Oct 2004 14:23:20 -0400
Subject: [R] changing coordinate systems
Message-ID: <m2k6tazopz.fsf@resume.itd.nrl.navy.mil>

Hi, All.  I am working with some eye-tracking data.  I have a graph I
generate in R with arbitrary plotting commands which I want to
display.  Then I want to plot the eyetracking data overlayed on it.

The eye-tracking data is in x-y coordinates on a 640x480 coordinate
system.  Ideally, what I'd like to do is plot my R graph (e.g.,
plot(1:10)), and then change the complete coordinate system of the
whole device to be 640x480.  commands like plot.window() don't work
because that just changes the coordinate system of the plotable area,
not the whole device or viewable image (ie, the x labels aren't
right).

Any ideas?

thanks!
greg



From dinos at northwestern.edu  Thu Oct 28 20:14:51 2004
From: dinos at northwestern.edu (Konstantinos Liolios)
Date: 28 Oct 2004 13:14:51 -0500
Subject: [R] plot.baysian error = only 0's may mix with negative subscripts
In-Reply-To: <41812FD9.4000506@statistik.uni-dortmund.de>
References: <200410281502.i9SF2TBX014947@lulu.it.northwestern.edu>
	<41812FD9.4000506@statistik.uni-dortmund.de>
Message-ID: <1098987290.17171.10.camel@bigbird>

I did make a typo and I meant plot.bayesian() from sma.  Hopefully most
people can easily guess what I meant.  It looks like NAs are illegal
from 1.9 and later.  I am surprised that stat.bayesian() still allows it
but plot has a problem with it.

Thanx
Dinos

 peopleOn Thu, 2004-10-28 at 12:43, Uwe Ligges wrote:
> dinos at northwestern.edu wrote:
> > Dear R users and developers
> > 
> > After upgrading to Windows XP and R 1.9.1 and 2.0, I retried to execute
> > plot.baysian() to a data set that I had used previously to plot with no
> > problem in win2000 R1.8.  The error I get is:
> > 
> > Error in points(Mbar[-index], lods[-index], pch = ".") : 
> >         only 0's may mix with negative subscripts
> 
> What is plot.baysian()?
> Do you mean  plot.bayesian()  from package "sma"?
> Probably an error has been fixed that caused your data to be plotted 
> inaccurately in R < 1.9.x ... or there is a bug in sma.
> But without the data it's hard to say.
> 
> So the suggestion is to ask the package maintainer of sma by providing a 
> simple and easily reproducible example...
> 
> Uwe Ligges
> 
> 
> > Thanx in advance
> > Dino
> > P.S.  I allready sent this message once but without a subject.  I
> > apologize for the inconvenience
> 
> P.S. And with this message we do know thrice that you are:
> 
> 
> > Konstantinos G. Liolios
> > IT Software Engineer II
> > Charles E. and Emma H. Morrison
> > Depts.  Pathology and Microbiology-Immunology
> > Northwestern University
> > Ward 3-240
> > 303 E. Chicago Avenue
> > Chicago IL 60611
> > Tel: 312-503-0224
> > Fax:312-503-0281
> > http://www.haldarlab.northwestern.edu
> 
> [I removed the next two information blocks who you are, since I think 
> it's sufficient for most of us to have one of these blocks...]
>



From kshe4 at student.monash.edu  Thu Oct 28 20:23:33 2004
From: kshe4 at student.monash.edu (Kunal Shetty)
Date: Thu, 28 Oct 2004 18:23:33 +0000
Subject: [R] Passing values between programs
Message-ID: <220.253.2.55.1098987332.96915@my.monash.edu.au>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20041028/8c393a16/attachment-0001.pl

From ramasamy at cancer.org.uk  Thu Oct 28 20:41:18 2004
From: ramasamy at cancer.org.uk (Adaikalavan Ramasamy)
Date: Thu, 28 Oct 2004 19:41:18 +0100
Subject: [R] Running R on a grid engine
In-Reply-To: <20041028174926.10340.qmail@web50010.mail.yahoo.com>
References: <20041028174926.10340.qmail@web50010.mail.yahoo.com>
Message-ID: <1098988878.5405.19.camel@ramasamy.stats>

See comments below.

On Thu, 2004-10-28 at 18:49, S Peri wrote:
> Dear Group, 
>  I am using DEAL package for modeling signal
> transduction nets.  This process is deal slow on a
> SunFire server with 8 gigs ram. 
> 
> we have a grid that can process much faster that one
> individual server.  
> 
> However, to start the process in Grid, I have to give
> a command or submit a batch process. 
> 
> Is there any way, I can run R in bach process. 
> 
> I tried the following:
> 
> R CMD | library(deal) | data <-
> data.frame(read.table('file1',header=TRUE,
> row.names=1))
> 

Something like this works in *NIX

  echo " print(mean(rnorm(10))) " | R --no-save

HOWEVER you will run into nightmares soon trying to backslash all the
quotes and other special characters. And try to recall a long sequence
of commands you typed in a few days ago ...


Better to put all your codes into a file, say script.R and do

 R --no-save < script.R > log_script.R  &

See help("BATCH"). 


> Here I do not know know :
> 
> 1. How can I point my data files to a function. 

Two ways :
1) Hard code the path inside the script.R
2) Take advantage of commandArgs(). 

Example. If your script.R contains 

data.path <- as.character( commandArgs()[3] )
print(data.path)
load (data.path)  # or read.delim or whatever
....

Then you can pass the path to test.rda via command line

 R --no-save < script.R /home/speri/data/test.rda > log_script.R  &

One thing to keep in mind is to pass the absolute paths and not relative
paths (e.g. ../../data/test.rda). Using relative path may not always
work.


> 2. creating a function (other than in R environment)

Huh ? You can put all your functions into a file, say functions.R, and
you can source("/path/to/functions.R") in your script.R/

> 
> Could any one help me. 
> 
> Thank you in advance.
> 
> 
> Cheers
> Peri.
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>



From ripley at stats.ox.ac.uk  Thu Oct 28 20:47:12 2004
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Thu, 28 Oct 2004 19:47:12 +0100 (BST)
Subject: [R] changing coordinate systems
In-Reply-To: <m2k6tazopz.fsf@resume.itd.nrl.navy.mil>
Message-ID: <Pine.LNX.4.44.0410281942410.18958-100000@gannet.stats>

On Thu, 28 Oct 2004, Greg Trafton wrote:

> Hi, All.  I am working with some eye-tracking data.  I have a graph I
> generate in R with arbitrary plotting commands which I want to
> display.  Then I want to plot the eyetracking data overlayed on it.
> 
> The eye-tracking data is in x-y coordinates on a 640x480 coordinate
> system.  Ideally, what I'd like to do is plot my R graph (e.g.,
> plot(1:10)), and then change the complete coordinate system of the
> whole device to be 640x480.  commands like plot.window() don't work
> because that just changes the coordinate system of the plotable area,
> not the whole device or viewable image (ie, the x labels aren't
> right).
> 
> Any ideas?

?par, especially usr (to set the plot coords) and mai (to remove the 
margins)

> plot(1:10)
> par(mai=rep(0,4))
> par(usr=c(0,640,0,480))
> rect(10, 10, 630, 470)

shows the idea

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From yshao at wadsworth.org  Thu Oct 28 20:57:51 2004
From: yshao at wadsworth.org (Yu Shao)
Date: Thu, 28 Oct 2004 14:57:51 -0400 (EDT)
Subject: [R] Weighted regresion using lm
Message-ID: <200410281857.i9SIvpC10615@csserv.wadsworth.org>

Hi:

Could anyone help me to clarify this: are the weights normalized inside lm 
function (package:stats) before applied to the error term?  For example:

>lm (cost ~ material, weights=quatity, data=receipt)

will lm normalize quatity such that sum(quatity) = 1? I traced to lm.wfit and 
then the weights get transferred into a precompiled FORTRAN module so I can't 
figure out. Thanks!



> version
         _                   
platform sparc-sun-solaris2.9
arch     sparc               
os       solaris2.9          
system   sparc, solaris2.9   
status                       
major    1                   
minor    9.0                 
year     2004                
month    04                  
day      12                  
language R                


Yu Shao
Wadsworth Center
NY Department of Health
Albany, NY 12208



From sdavis2 at mail.nih.gov  Thu Oct 28 21:03:35 2004
From: sdavis2 at mail.nih.gov (Sean Davis)
Date: Thu, 28 Oct 2004 15:03:35 -0400
Subject: [R] Table question
Message-ID: <11AE7878-2914-11D9-8349-000A95D7BA10@mail.nih.gov>

I have a table (output from table(factor1,factor2)).  I would like to 
use write.table to output that table to a file.  However, it seems that 
as.data.frame converts such a table to three columns, Var1, Var2, and 
Freq rather than converting to the data.frame with equivalent numbers 
of rows and columns.  I can use write.matrix from the MASS package, but 
then I get no rownames.  Any hints here?

Thanks
Sean



From pkleiber at honlab.nmfs.hawaii.edu  Thu Oct 28 21:26:35 2004
From: pkleiber at honlab.nmfs.hawaii.edu (Pierre Kleiber)
Date: Thu, 28 Oct 2004 09:26:35 -1000
Subject: [R] Job Opening
Message-ID: <418147EB.8010206@honlab.nmfs.hawaii.edu>

There is a job opening advertised for an R literate and C++ literate data 
analyst/computer programmer working on fish stock assessment models. 
Location is Honolulu, Hawaii.  To obtain details go to:
   http://www.rcuh.com
   Navigate to "Employment" then "Job Announcements"
   In "Reference #" enter 024545
   Click "Search", and select "PIFSC Computer Specialist-Oahu"

-- 
-----------------------------------------------------------------
Pierre Kleiber, Ph.D       Email: pkleiber at honlab.nmfs.hawaii.edu
Fishery Biologist                     Tel: 808 983-5399/737-7544
NOAA FISHERIES - Honolulu Laboratory         Fax: 808 983-2902
2570 Dole St., Honolulu, HI 96822-2396



From ripley at stats.ox.ac.uk  Thu Oct 28 21:27:18 2004
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Thu, 28 Oct 2004 20:27:18 +0100 (BST)
Subject: [R] Table question
In-Reply-To: <11AE7878-2914-11D9-8349-000A95D7BA10@mail.nih.gov>
Message-ID: <Pine.LNX.4.44.0410282026490.5572-100000@gannet.stats>

Why not use sink()?

On Thu, 28 Oct 2004, Sean Davis wrote:

> I have a table (output from table(factor1,factor2)).  I would like to 
> use write.table to output that table to a file.  However, it seems that 
> as.data.frame converts such a table to three columns, Var1, Var2, and 
> Freq rather than converting to the data.frame with equivalent numbers 
> of rows and columns.  I can use write.matrix from the MASS package, but 
> then I get no rownames.  Any hints here?

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From andy_liaw at merck.com  Thu Oct 28 21:22:28 2004
From: andy_liaw at merck.com (Liaw, Andy)
Date: Thu, 28 Oct 2004 15:22:28 -0400
Subject: [R] Table question
Message-ID: <3A822319EB35174CA3714066D590DCD50994E246@usrymx25.merck.com>

Here's one way:


> f1 <- factor(rep(1:2, e=10))
> f2 <- factor(rep(1:2, 10))
> tab <- table(f1, f2)
> str(as.data.frame(unclass(tab)))
`data.frame':   2 obs. of  2 variables:
 $ 1: int  5 5
 $ 2: int  5 5

HTH,
Andy

> From: Sean Davis
> 
> I have a table (output from table(factor1,factor2)).  I would like to 
> use write.table to output that table to a file.  However, it 
> seems that 
> as.data.frame converts such a table to three columns, Var1, Var2, and 
> Freq rather than converting to the data.frame with equivalent numbers 
> of rows and columns.  I can use write.matrix from the MASS 
> package, but 
> then I get no rownames.  Any hints here?
> 
> Thanks
> Sean
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
> 
>



From sundar.dorai-raj at pdf.com  Thu Oct 28 21:39:06 2004
From: sundar.dorai-raj at pdf.com (Sundar Dorai-Raj)
Date: Thu, 28 Oct 2004 14:39:06 -0500
Subject: [R] Weighted regresion using lm
In-Reply-To: <200410281857.i9SIvpC10615@csserv.wadsworth.org>
References: <200410281857.i9SIvpC10615@csserv.wadsworth.org>
Message-ID: <41814ADA.40504@pdf.com>



Yu Shao wrote:

> Hi:
> 
> Could anyone help me to clarify this: are the weights normalized inside lm 
> function (package:stats) before applied to the error term?  For example:
> 
> 
>>lm (cost ~ material, weights=quatity, data=receipt)
> 
> 
> will lm normalize quatity such that sum(quatity) = 1? I traced to lm.wfit and 
> then the weights get transferred into a precompiled FORTRAN module so I can't 
> figure out. Thanks!
> 


Did you look at ?lm.wfit?

        w: vector of weights (length 'n') to be used in the fitting
           process for the 'wfit' functions.  Weighted least squares is
           used with weights 'w', i.e., 'sum(w * e^2)' is minimized.

So, no, the weights are not normalized.

--sundar

> 
> 
> 
>>version
> 
>          _                   
> platform sparc-sun-solaris2.9
> arch     sparc               
> os       solaris2.9          
> system   sparc, solaris2.9   
> status                       
> major    1                   
> minor    9.0                 
> year     2004                
> month    04                  
> day      12                  
> language R                
> 
> 
> Yu Shao
> Wadsworth Center
> NY Department of Health
> Albany, NY 12208
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html



From ripley at stats.ox.ac.uk  Thu Oct 28 21:49:47 2004
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Thu, 28 Oct 2004 20:49:47 +0100 (BST)
Subject: [R] Weighted regresion using lm
In-Reply-To: <200410281857.i9SIvpC10615@csserv.wadsworth.org>
Message-ID: <Pine.LNX.4.44.0410282022550.5572-100000@gannet.stats>

On Thu, 28 Oct 2004, Yu Shao wrote:

> Could anyone help me to clarify this: are the weights normalized inside lm 
> function (package:stats) before applied to the error term?  For example:
> 
> >lm (cost ~ material, weights=quatity, data=receipt)
> 
> will lm normalize quatity such that sum(quatity) = 1? I traced to lm.wfit and 
> then the weights get transferred into a precompiled FORTRAN module so I can't 
> figure out. Thanks!

Yes you can, as you have the sources and the documentation.  (R is Open 
Source.)  You could also do a simple experiment like that below.

Weighted regression is not defined to use normalized weights, and this is
not done in R.  Note, though, that the fit and the coefficients are the
same whether you normalize or not.

y <- rnorm(100)
x <- runif(100)
w <- 1:100
w1 <- w/sum(w)
(fm <- lm(y ~x, weights=w))
(fm1 <- lm(y ~x, weights=w1))
summary(fm)
summary(fm1)

show that only the residual se and the residuals are affected.


-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From yhu at mail.nih.gov  Thu Oct 28 21:57:39 2004
From: yhu at mail.nih.gov (Hu, Ying (NIH/NCI))
Date: Thu, 28 Oct 2004 15:57:39 -0400
Subject: [R] sem : Error in solve.default(C[ind,
	ind]) : Lapack routine dgesv: system is exactly singular
Message-ID: <27C204BD76CBC142BA1AE46D62A8548E49086E@nihexchange9.nih.gov>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20041028/bc69f634/attachment.pl

From ahelyer at austin.utexas.edu  Thu Oct 28 22:11:59 2004
From: ahelyer at austin.utexas.edu (Andrew K Helyer)
Date: Thu, 28 Oct 2004 15:11:59 -0500
Subject: [R] Errors during make check
Message-ID: <A40A493F44BE7C44A55119F807CCE8851DD5DA@MAIL01.austin.utexas.edu>

On a SUN 280R running Solaris 9...

The configure and make steps completed without errors. But when I try to
perform the make check step, I get the following:


$ make check FORCE=FORCE
`Makedeps' is up to date.
running code in 'base-Ex.R' ...*** Error code 1
make: Fatal error: Command failed for target `base-Ex.Rout'
Current working directory /usr/local/R-2.0.0/tests/Examples
*** Error code 1
make: Fatal error: Command failed for target `test-Examples-Base'
Current working directory /usr/local/R-2.0.0/tests/Examples
*** Error code 1
make: Fatal error: Command failed for target `test-Examples'
Current working directory /usr/local/R-2.0.0/tests
*** Error code 1
make: Fatal error: Command failed for target `test-all-basics'
Current working directory /usr/local/R-2.0.0/tests
*** Error code 1
make: Fatal error: Command failed for target `check'


I don't know what to make of these. I've tried several options on the
configure step (static libs, pointing to tclConfig.sh and tkConfig.sh,
etc). I've reviewed the FAQs and mailing list archives. I've read and
reread the install instructions.

Can anyone give me some more clues about what I'm missing?

Regards,

--Andrew;


Andrew Helyer
ITS User Services
Systems Analyst

ahelyer at austin.utexas.edu
512-475-7795 (v)
512-606-2931 (p)



From reid_huntsinger at merck.com  Thu Oct 28 22:18:35 2004
From: reid_huntsinger at merck.com (Huntsinger, Reid)
Date: Thu, 28 Oct 2004 16:18:35 -0400
Subject: [R] array problem and for looping
Message-ID: <D9A95B4B7B20354992E165EEADA31999056A917F@uswpmx00.merck.com>

First, the condition 

  if (is.na(sample$x[i]== TRUE))

asks if sample$x[i] is equal to TRUE, and then checks whether the result of
this comparison is NA. Because the comparison returns NA when one or the
other argument is NA, this works, but note that it would work as well with
FALSE in place of TRUE. I suppose you meant to say if (is.na(sample$x[i]) ==
TRUE) which is the same as if(is.na(sample$x[i])). 

Second, your code could generate data with *both* x and y missing
simultaneously. That would produce missing results in your regression
imputation. You probably want to check for these and drop them or otherwise
handle them.

Third, you might gain some insight by running your function algoResult on
known data, and examining the results. The missing values have to come from
somewhere, but because you run the entire script which generates random data
each time you never get this check. 

Fourth, you function doesn't use its last 3 arguments, but does use the
dataframe "sample" and the vector "missing". As these are not passed as
arguments, they are searched for in the enclosing environment. Do you want
that? 

Fifth, yes, you don't need the loop. Just use subscripting for example, as
you did to insert NAs randomly, or ifelse as you used to impute by means. 

Reid Huntsinger
 
-----Original Message-----
From: r-help-bounces at stat.math.ethz.ch
[mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Kunal Shetty
Sent: Thursday, October 28, 2004 1:55 PM
To: r-help at stat.math.ethz.ch
Subject: [R] array problem and for looping


Dear R- users and Helpers



Is there some way to re initialise or clear the array elements?  Pardon me
for being vague but the problem itself quite vague. I have attached the code
along with email. 

When I run the saved r- code using source("random1.txt") , command.



The program runs fine..but at times there is an error see below # ;  but
again after the error if re-excuted it would work fine...I  probably missed
some array detail in my program any suggestions



#Error in var(parrX[1, ]) : missing observations in cov/cor



parrX[] is an array .



 2) Also pardon me for the lengthy procedural code

        but as you could see in it..i have used the for loop for finding the
positions (indexes) of the missing values and later carry out updating the
new array element values at those particular positions/

         So how can I escape the for loop in this case ?  i.e get the
missing position indexes and save another object ay vector or array ?



        And also later wanted to use matrix or vector multiplication (%*%)
for the updating statement

                        newy[i]<- u2 + covXY/varX * (sample$x[i] - u1)



         is any of the apply function good out here ?



  I really feel that I am doing something very routine and donkey work and I
am most certain that powerful R - functions could just execute the same 10
liner for loop condition to mere 4 lines ? but how...I am getting lost in
the sea of functions here...



Thank u for reading

Regards

Kunal



From ripley at stats.ox.ac.uk  Thu Oct 28 22:34:18 2004
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Thu, 28 Oct 2004 21:34:18 +0100 (BST)
Subject: [R] Errors during make check
In-Reply-To: <A40A493F44BE7C44A55119F807CCE8851DD5DA@MAIL01.austin.utexas.edu>
Message-ID: <Pine.LNX.4.44.0410282131040.10907-100000@gannet.stats>

On Thu, 28 Oct 2004, Andrew K Helyer wrote:

> On a SUN 280R running Solaris 9...

Compilers?  Optimization settings?  32- or 64-bit?

> The configure and make steps completed without errors. But when I try to
> perform the make check step, I get the following:
> 
> 
> $ make check FORCE=FORCE
> `Makedeps' is up to date.
> running code in 'base-Ex.R' ...*** Error code 1
> make: Fatal error: Command failed for target `base-Ex.Rout'
> Current working directory /usr/local/R-2.0.0/tests/Examples

Please take a look at the file that failed, probably
tests/Examples/base-Ex.Rout.fail.  If that is not there, look for a core 
dump in tests/Examples.

> I don't know what to make of these. I've tried several options on the
> configure step (static libs, pointing to tclConfig.sh and tkConfig.sh,
> etc). I've reviewed the FAQs and mailing list archives. I've read and
> reread the install instructions.
> 
> Can anyone give me some more clues about what I'm missing?

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From reid_huntsinger at merck.com  Thu Oct 28 23:06:48 2004
From: reid_huntsinger at merck.com (Huntsinger, Reid)
Date: Thu, 28 Oct 2004 17:06:48 -0400
Subject: [R] Running R on a grid engine
Message-ID: <D9A95B4B7B20354992E165EEADA31999056A9180@uswpmx00.merck.com>

Does the grid process faster because there is more than one machine or
because each machine (or at least one machine) is faster? If the former,
you're asking about splitting an R process into several processes (to take
advantage of the grid), and you will want to look at the various R packages
to facilitate that: Snow, Rmpi, Rpvm, TaskPR and perhaps others (browse
cran.rproject.org).

Reid Huntsinger

-----Original Message-----
From: r-help-bounces at stat.math.ethz.ch
[mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Adaikalavan Ramasamy
Sent: Thursday, October 28, 2004 2:41 PM
To: S Peri
Cc: R-help
Subject: Re: [R] Running R on a grid engine


See comments below.

On Thu, 2004-10-28 at 18:49, S Peri wrote:
> Dear Group, 
>  I am using DEAL package for modeling signal
> transduction nets.  This process is deal slow on a
> SunFire server with 8 gigs ram. 
> 
> we have a grid that can process much faster that one
> individual server.  
> 
> However, to start the process in Grid, I have to give
> a command or submit a batch process. 
> 
> Is there any way, I can run R in bach process. 
> 
> I tried the following:
> 
> R CMD | library(deal) | data <-
> data.frame(read.table('file1',header=TRUE,
> row.names=1))
> 

Something like this works in *NIX

  echo " print(mean(rnorm(10))) " | R --no-save

HOWEVER you will run into nightmares soon trying to backslash all the
quotes and other special characters. And try to recall a long sequence
of commands you typed in a few days ago ...


Better to put all your codes into a file, say script.R and do

 R --no-save < script.R > log_script.R  &

See help("BATCH"). 


> Here I do not know know :
> 
> 1. How can I point my data files to a function. 

Two ways :
1) Hard code the path inside the script.R
2) Take advantage of commandArgs(). 

Example. If your script.R contains 

data.path <- as.character( commandArgs()[3] )
print(data.path)
load (data.path)  # or read.delim or whatever
....

Then you can pass the path to test.rda via command line

 R --no-save < script.R /home/speri/data/test.rda > log_script.R  &

One thing to keep in mind is to pass the absolute paths and not relative
paths (e.g. ../../data/test.rda). Using relative path may not always
work.


> 2. creating a function (other than in R environment)

Huh ? You can put all your functions into a file, say functions.R, and
you can source("/path/to/functions.R") in your script.R/

> 
> Could any one help me. 
> 
> Thank you in advance.
> 
> 
> Cheers
> Peri.
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide!
http://www.R-project.org/posting-guide.html
>

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide!
http://www.R-project.org/posting-guide.html



From ahelyer at austin.utexas.edu  Thu Oct 28 23:26:19 2004
From: ahelyer at austin.utexas.edu (Andrew K Helyer)
Date: Thu, 28 Oct 2004 16:26:19 -0500
Subject: [R] Errors during make check
Message-ID: <A40A493F44BE7C44A55119F807CCE8851DD5F4@MAIL01.austin.utexas.edu>

Thank you for your help. I'm really stumped on this one.

Here is some information from the end of the configuration step...

R is now configured for sparc-sun-solaris2.9

  Source directory:          .
  Installation directory:    /usr/local

  C compiler:                gcc  -g -O2
  C++ compiler:              g++  -g -O2
  Fortran compiler:          f77  -g

  Interfaces supported:      X11, tcltk
  External libraries:        
  Additional capabilities:   PNG, JPEG
  Options enabled:           R profiling

  Recommended packages:      yes 



The ...Ex.Rout.fail file contains:


R : Copyright 2004, The R Foundation for Statistical Computing
Version 2.0.0  (2004-10-04), ISBN 3-900051-07-0

R is free software and comes with ABSOLUTELY NO WARRANTY.
You are welcome to redistribute it under certain conditions.
Type 'license()' or 'licence()' for distribution details.

R is a collaborative project with many contributors.
Type 'contributors()' for more information and
'citation()' on how to cite R or R packages in publications.

Type 'demo()' for some demos, 'help()' for on-line help, or
'help.start()' for a HTML browser interface to help.
Type 'q()' to quit R.

> ### * <HEADER>
> ###
> attach(NULL, name = "CheckExEnv")
> assign(".CheckExEnv", as.environment(2), pos = length(search())) #
base
> ## add some hooks to label plot pages for base and grid graphics
> setHook("plot.new", ".newplot.hook")
> setHook("persp", ".newplot.hook")
> setHook("grid.newpage", ".gridplot.hook")
> 
> assign("cleanEx",
+        function(env = .GlobalEnv) {
+          rm(list = ls(envir = env, all.names = TRUE), envir = env)
+            RNGkind("default", "default")
+          set.seed(1)
+          options(warn = 1)
+          assign("T", delay(stop("T used instead of TRUE")),
+                 pos = .CheckExEnv)
+          assign("F", delay(stop("F used instead of FALSE")),
+                 pos = .CheckExEnv)
+          sch <- search()
+          newitems <- sch[! sch %in% .oldSearch]
+          for(item in rev(newitems))
+                eval(substitute(detach(item), list(item=item)))
+          missitems <- .oldSearch[! .oldSearch %in% sch]
+          if(length(missitems))
+              warning("items ", paste(missitems, collapse=", "),
+                      " have been removed from the search path")
+        },
+        env = .CheckExEnv)
> assign("..nameEx", "__{must remake R-ex/*.R}__", env = .CheckExEnv) #
for now
> assign("ptime", proc.time(), env = .CheckExEnv)
> grDevices::postscript("base-Examples.ps")
> assign("par.postscript", graphics::par(no.readonly = TRUE), env =
.CheckExEnv)
> options(contrasts = c(unordered = "contr.treatment", ordered =
"contr.poly"))
> assign(".oldSearch", search(), env = .CheckExEnv)
> assign(".oldNS", loadedNamespaces(), env = .CheckExEnv)
> cleanEx(); ..nameEx <- "Arithmetic"
> 
> ### * Arithmetic
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: Arithmetic
> ### Title: Arithmetic Operators
> ### Aliases: + - * / ^ \%\% \%/\% Arithmetic
> ### Keywords: arith
> 
> ### ** Examples
> 
> x <- -1:12
> x + 1
 [1]  0  1  2  3  4  5  6  7  8  9 10 11 12 13
> 2 * x + 3
 [1]  1  3  5  7  9 11 13 15 17 19 21 23 25 27
> x %% 2 #-- is periodic
 [1] 1 0 1 0 1 0 1 0 1 0 1 0 1 0
> x %/% 5
 [1] -1  0  0  0  0  0  1  1  1  1  1  2  2  2
> 
> 
> 
> cleanEx(); ..nameEx <- "Bessel"
> 
> ### * Bessel
> 
> flush(stderr()); flush(stdout())









> -----Original Message-----
> From: Prof Brian Ripley [mailto:ripley at stats.ox.ac.uk] 
> Sent: Thursday, October 28, 2004 3:34 PM
> To: Andrew K Helyer
> Cc: r-help at stat.math.ethz.ch
> Subject: Re: [R] Errors during make check
> 
> On Thu, 28 Oct 2004, Andrew K Helyer wrote:
> 
> > On a SUN 280R running Solaris 9...
> 
> Compilers?  Optimization settings?  32- or 64-bit?
> 
> > The configure and make steps completed without errors. But 
> when I try to
> > perform the make check step, I get the following:
> > 
> > 
> > $ make check FORCE=FORCE
> > `Makedeps' is up to date.
> > running code in 'base-Ex.R' ...*** Error code 1
> > make: Fatal error: Command failed for target `base-Ex.Rout'
> > Current working directory /usr/local/R-2.0.0/tests/Examples
> 
> Please take a look at the file that failed, probably
> tests/Examples/base-Ex.Rout.fail.  If that is not there, look 
> for a core 
> dump in tests/Examples.
> 
> > I don't know what to make of these. I've tried several 
> options on the
> > configure step (static libs, pointing to tclConfig.sh and 
> tkConfig.sh,
> > etc). I've reviewed the FAQs and mailing list archives. 
> I've read and
> > reread the install instructions.
> > 
> > Can anyone give me some more clues about what I'm missing?
> 
> -- 
> Brian D. Ripley,                  ripley at stats.ox.ac.uk
> Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
> University of Oxford,             Tel:  +44 1865 272861 (self)
> 1 South Parks Road,                     +44 1865 272866 (PA)
> Oxford OX1 3TG, UK                Fax:  +44 1865 272595
> 
>



From alexpegucci at yahoo.com  Thu Oct 28 23:27:23 2004
From: alexpegucci at yahoo.com (alex pegucci)
Date: Thu, 28 Oct 2004 14:27:23 -0700 (PDT)
Subject: [R] transitivity
Message-ID: <20041028212723.55438.qmail@web14424.mail.yahoo.com>

Dear all,

Is there a function in R that checks transitivity and acyclicity of a
given nXn matrix with entries representing a decision-maker's
comparisons of n objects? Like 

0 1 0 1 1 1
0 0 0 1 0 0
1 0 0 0 1 1
0 0 1 0 0 0
0 1 0 1 0 1
0 1 0 1 0 0

1 represents xPy and 0 represents ~xPy. Is there a vectorized solution
to this? n can be quite large. 

Thanks in advance,

Alex



From wjwest at CLEMSON.EDU  Thu Oct 28 23:32:15 2004
From: wjwest at CLEMSON.EDU (Bill West)
Date: Thu, 28 Oct 2004 17:32:15 -0400
Subject: [R] Double Sided Box-Cox models?
Message-ID: <000b01c4bd35$986c7ea0$4c867f82@billwest00>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20041028/2038c4ba/attachment.pl

From ripley at stats.ox.ac.uk  Thu Oct 28 23:32:32 2004
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Thu, 28 Oct 2004 22:32:32 +0100 (BST)
Subject: [R] Errors during make check
In-Reply-To: <A40A493F44BE7C44A55119F807CCE8851DD5F4@MAIL01.austin.utexas.edu>
Message-ID: <Pine.LNX.4.44.0410282228001.13940-100000@gannet.stats>

I did ask you about compilers: this looks like the gcc 3.2.x bug
described in the R-admin manual.

  @command{gcc} 3.2.1 and 3.2.2 generate incorrect code on 32-bit Solaris
  builds with optimization, but versions 3.1, 3.2, 3.2.3 and later work
  correctly.

and that AFAIR failed at exactly that point.

On Thu, 28 Oct 2004, Andrew K Helyer wrote:

> Thank you for your help. I'm really stumped on this one.
> 
> Here is some information from the end of the configuration step...
> 
> R is now configured for sparc-sun-solaris2.9
> 
>   Source directory:          .
>   Installation directory:    /usr/local
> 
>   C compiler:                gcc  -g -O2
>   C++ compiler:              g++  -g -O2
>   Fortran compiler:          f77  -g
> 
>   Interfaces supported:      X11, tcltk
>   External libraries:        
>   Additional capabilities:   PNG, JPEG
>   Options enabled:           R profiling
> 
>   Recommended packages:      yes 
> 
> 
> 
> The ...Ex.Rout.fail file contains:
> 
> 
> R : Copyright 2004, The R Foundation for Statistical Computing
> Version 2.0.0  (2004-10-04), ISBN 3-900051-07-0
> 
> R is free software and comes with ABSOLUTELY NO WARRANTY.
> You are welcome to redistribute it under certain conditions.
> Type 'license()' or 'licence()' for distribution details.
> 
> R is a collaborative project with many contributors.
> Type 'contributors()' for more information and
> 'citation()' on how to cite R or R packages in publications.
> 
> Type 'demo()' for some demos, 'help()' for on-line help, or
> 'help.start()' for a HTML browser interface to help.
> Type 'q()' to quit R.
> 
> > ### * <HEADER>
> > ###
> > attach(NULL, name = "CheckExEnv")
> > assign(".CheckExEnv", as.environment(2), pos = length(search())) #
> base
> > ## add some hooks to label plot pages for base and grid graphics
> > setHook("plot.new", ".newplot.hook")
> > setHook("persp", ".newplot.hook")
> > setHook("grid.newpage", ".gridplot.hook")
> > 
> > assign("cleanEx",
> +        function(env = .GlobalEnv) {
> +          rm(list = ls(envir = env, all.names = TRUE), envir = env)
> +            RNGkind("default", "default")
> +          set.seed(1)
> +          options(warn = 1)
> +          assign("T", delay(stop("T used instead of TRUE")),
> +                 pos = .CheckExEnv)
> +          assign("F", delay(stop("F used instead of FALSE")),
> +                 pos = .CheckExEnv)
> +          sch <- search()
> +          newitems <- sch[! sch %in% .oldSearch]
> +          for(item in rev(newitems))
> +                eval(substitute(detach(item), list(item=item)))
> +          missitems <- .oldSearch[! .oldSearch %in% sch]
> +          if(length(missitems))
> +              warning("items ", paste(missitems, collapse=", "),
> +                      " have been removed from the search path")
> +        },
> +        env = .CheckExEnv)
> > assign("..nameEx", "__{must remake R-ex/*.R}__", env = .CheckExEnv) #
> for now
> > assign("ptime", proc.time(), env = .CheckExEnv)
> > grDevices::postscript("base-Examples.ps")
> > assign("par.postscript", graphics::par(no.readonly = TRUE), env =
> .CheckExEnv)
> > options(contrasts = c(unordered = "contr.treatment", ordered =
> "contr.poly"))
> > assign(".oldSearch", search(), env = .CheckExEnv)
> > assign(".oldNS", loadedNamespaces(), env = .CheckExEnv)
> > cleanEx(); ..nameEx <- "Arithmetic"
> > 
> > ### * Arithmetic
> > 
> > flush(stderr()); flush(stdout())
> > 
> > ### Name: Arithmetic
> > ### Title: Arithmetic Operators
> > ### Aliases: + - * / ^ \%\% \%/\% Arithmetic
> > ### Keywords: arith
> > 
> > ### ** Examples
> > 
> > x <- -1:12
> > x + 1
>  [1]  0  1  2  3  4  5  6  7  8  9 10 11 12 13
> > 2 * x + 3
>  [1]  1  3  5  7  9 11 13 15 17 19 21 23 25 27
> > x %% 2 #-- is periodic
>  [1] 1 0 1 0 1 0 1 0 1 0 1 0 1 0
> > x %/% 5
>  [1] -1  0  0  0  0  0  1  1  1  1  1  2  2  2
> > 
> > 
> > 
> > cleanEx(); ..nameEx <- "Bessel"
> > 
> > ### * Bessel
> > 
> > flush(stderr()); flush(stdout())
> 
> 
> 
> 
> 
> 
> 
> 
> 
> > -----Original Message-----
> > From: Prof Brian Ripley [mailto:ripley at stats.ox.ac.uk] 
> > Sent: Thursday, October 28, 2004 3:34 PM
> > To: Andrew K Helyer
> > Cc: r-help at stat.math.ethz.ch
> > Subject: Re: [R] Errors during make check
> > 
> > On Thu, 28 Oct 2004, Andrew K Helyer wrote:
> > 
> > > On a SUN 280R running Solaris 9...
> > 
> > Compilers?  Optimization settings?  32- or 64-bit?
> > 
> > > The configure and make steps completed without errors. But 
> > when I try to
> > > perform the make check step, I get the following:
> > > 
> > > 
> > > $ make check FORCE=FORCE
> > > `Makedeps' is up to date.
> > > running code in 'base-Ex.R' ...*** Error code 1
> > > make: Fatal error: Command failed for target `base-Ex.Rout'
> > > Current working directory /usr/local/R-2.0.0/tests/Examples
> > 
> > Please take a look at the file that failed, probably
> > tests/Examples/base-Ex.Rout.fail.  If that is not there, look 
> > for a core 
> > dump in tests/Examples.
> > 
> > > I don't know what to make of these. I've tried several 
> > options on the
> > > configure step (static libs, pointing to tclConfig.sh and 
> > tkConfig.sh,
> > > etc). I've reviewed the FAQs and mailing list archives. 
> > I've read and
> > > reread the install instructions.
> > > 
> > > Can anyone give me some more clues about what I'm missing?
> > 
> > -- 
> > Brian D. Ripley,                  ripley at stats.ox.ac.uk
> > Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
> > University of Oxford,             Tel:  +44 1865 272861 (self)
> > 1 South Parks Road,                     +44 1865 272866 (PA)
> > Oxford OX1 3TG, UK                Fax:  +44 1865 272595
> > 
> > 
> 
> 

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From ripley at stats.ox.ac.uk  Thu Oct 28 23:57:55 2004
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Thu, 28 Oct 2004 22:57:55 +0100 (BST)
Subject: [R] Double Sided Box-Cox models?
In-Reply-To: <000b01c4bd35$986c7ea0$4c867f82@billwest00>
Message-ID: <Pine.LNX.4.44.0410282250300.14073-100000@gannet.stats>

On Thu, 28 Oct 2004, Bill West wrote:

>   This is my first post to the help list; I have been using R only for a
> couple of months.  I have been able to find answers to most questions
> through the archives, but I have not seen any posts about double sided
> Box-Cox models.  Is there any way to run do this in R?

The accurate (but maybe not helpful) answer is that R is a full
programming language, so of course there is a way if you are looking for a 
well-defined method.

What exactly do you have in mind?  Google comes up with nothing useful
given your title as a search term ... and you haven't even said what these 
`models' model.

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From p.dalgaard at biostat.ku.dk  Fri Oct 29 00:02:40 2004
From: p.dalgaard at biostat.ku.dk (Peter Dalgaard)
Date: 29 Oct 2004 00:02:40 +0200
Subject: [R] Errors during make check
In-Reply-To: <Pine.LNX.4.44.0410282131040.10907-100000@gannet.stats>
References: <Pine.LNX.4.44.0410282131040.10907-100000@gannet.stats>
Message-ID: <x2fz3y7b7j.fsf@biostat.ku.dk>

Prof Brian Ripley <ripley at stats.ox.ac.uk> writes:

> On Thu, 28 Oct 2004, Andrew K Helyer wrote:
> 
> > On a SUN 280R running Solaris 9...
> 
> Compilers?  Optimization settings?  32- or 64-bit?

...and libraries. The DTU machine died a very similar death inside
libsunperf when compiled for 64bit. (extract from Makeconf -- my config.site
has some ATLAS stuff inside which I suspect didn't take so is probably
misleading. I'm not going to do a clean build now....)

BLAS_LIBS = -xlic_lib=sunperf -lsunmath
CC = cc -xarch=v9b
CFLAGS = -xO5 -xlibmil -dalign
CPICFLAGS = -KPIC
CPPFLAGS = -I/usr/local/include -I/opt/sfw/include

FFLAGS = -xO5 -xlibmil -dalign
FLIBS =  -L/usr/local/lib -lfui -lfai -lfai2 -lfsumai -lfprodai
-lfminlai -lfmaxlai -lfminvai -lfmaxvai -lfsu -lompstubs -lsunmath -lm
FPICFLAGS = -PIC
F77 = f95 -xarch=v9b
 
(If it is the same thing, base-Ex.Rout.fail dies inside example(kappa)
and example(svd) should segfault as well).
 
That segfault comes from inside libsunperf and occurs only in 64bit,
not 32bit, so I suspect a library bug. Following this up with Sun is
somewhere on my TODO list...

> > The configure and make steps completed without errors. But when I try to
> > perform the make check step, I get the following:
> > 
> > 
> > $ make check FORCE=FORCE
> > `Makedeps' is up to date.
> > running code in 'base-Ex.R' ...*** Error code 1
> > make: Fatal error: Command failed for target `base-Ex.Rout'
> > Current working directory /usr/local/R-2.0.0/tests/Examples
> 
> Please take a look at the file that failed, probably
> tests/Examples/base-Ex.Rout.fail.  If that is not there, look for a core 
> dump in tests/Examples.

-- 
   O__  ---- Peter Dalgaard             Blegdamsvej 3  
  c/ /'_ --- Dept. of Biostatistics     2200 Cph. N   
 (*) \(*) -- University of Copenhagen   Denmark      Ph: (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)             FAX: (+45) 35327907



From xxq5 at case.edu  Fri Oct 29 00:03:44 2004
From: xxq5 at case.edu (Xin Qi)
Date: Thu, 28 Oct 2004 18:03:44 -0400
Subject: [R]: a package problem
Message-ID: <5.1.1.6.0.20041028175706.02a05640@mail.cwru.edu>

Dear R- users and Helpers:

I downloaded the package from www.stat.lsa.umich.edu/~faraway/book and 
installed it from local zip file.
It looked fine. But when I input

library(faraway)

it showed " Error in library(faraway) : 'faraway' is not a valid package 
--- installed < 2.0.0?

What I used is R 2.0.0 version now.

What should I do?
Thank you very much.

Xin



From lauraholt_983 at hotmail.com  Fri Oct 29 00:13:10 2004
From: lauraholt_983 at hotmail.com (Laura Holt)
Date: Thu, 28 Oct 2004 17:13:10 -0500
Subject: [R][OT] Latex Integration question
Message-ID: <BAY12-F301tMVyoz5Bl000088c1@hotmail.com>

Hi R users!

I have a Latex Question:

When I use the following:

\begin{eqnarray*}
F(x) & = & -\lambda e^{-\lambda t} \mid_{0}^{x}
\end{eqnarray*}
the output looks ok.
But I would like the limits of integration line to be longer, if possible.  
Does anyone have any suggestions, please?

Thanks,
Laura Holt
mailto: lauraholt_983 at hotmail.com



From rpeng at jhsph.edu  Fri Oct 29 00:45:36 2004
From: rpeng at jhsph.edu (Roger D. Peng)
Date: Thu, 28 Oct 2004 18:45:36 -0400
Subject: [R]: a package problem
In-Reply-To: <5.1.1.6.0.20041028175706.02a05640@mail.cwru.edu>
References: <5.1.1.6.0.20041028175706.02a05640@mail.cwru.edu>
Message-ID: <41817690.6050703@jhsph.edu>

It seems you downloaded a precompiled binary (for Windows) that was most likely 
built for a version of R < 2.0.0.  Either the author needs to rebuild the 
package for R 2.0.0 or you need to grab the source package and build the Windows 
binary for yourself.

-roger

Xin Qi wrote:

> Dear R- users and Helpers:
> 
> I downloaded the package from www.stat.lsa.umich.edu/~faraway/book and 
> installed it from local zip file.
> It looked fine. But when I input
> 
> library(faraway)
> 
> it showed " Error in library(faraway) : 'faraway' is not a valid package 
> --- installed < 2.0.0?
> 
> What I used is R 2.0.0 version now.
> 
> What should I do?
> Thank you very much.
> 
> Xin
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
> 

-- 
Roger D. Peng
http://www.biostat.jhsph.edu/~rpeng/



From ggrothendieck at myway.com  Fri Oct 29 01:18:44 2004
From: ggrothendieck at myway.com (Gabor Grothendieck)
Date: Thu, 28 Oct 2004 23:18:44 +0000 (UTC)
Subject: [R] Table question
References: <11AE7878-2914-11D9-8349-000A95D7BA10@mail.nih.gov>
Message-ID: <loom.20041029T011617-745@post.gmane.org>

Sean Davis <sdavis2 <at> mail.nih.gov> writes:

: I have a table (output from table(factor1,factor2)).  I would like to 
: use write.table to output that table to a file.  However, it seems that 
: as.data.frame converts such a table to three columns, Var1, Var2, and 
: Freq rather than converting to the data.frame with equivalent numbers 
: of rows and columns.  I can use write.matrix from the MASS package, but 
: then I get no rownames.  Any hints here?

You could use captureOutput like this:

   data(HairEyeColor)
   capture.output(HairEyeColor, file = "myfile.txt")

See ?capture.output for more possibilities.



From ggrothendieck at myway.com  Fri Oct 29 01:29:27 2004
From: ggrothendieck at myway.com (Gabor Grothendieck)
Date: Thu, 28 Oct 2004 23:29:27 +0000 (UTC)
Subject: [R] read.csv(stdin(),...) with sweave
References: <9130.1098953003@www49.gmx.net>
Message-ID: <loom.20041029T012438-70@post.gmane.org>

 <ockham <at> gmx.net> writes:

: 
: hello, 
: 
: i would like to read in a small amount of csv data directly from a
: sweave-enabled latex document. i tried 
: 
: \begin{Scode}{fig=FALSE,echo=TRUE}
: timeval <- read.csv(stdin(),nrows=2)
: t1,t2,t3,t4
: 24.23,26.79,23.47,23.97
: \end{Scode}
: 
: but apparently the stdin() approach doesnt't work as I get
: 
: Writing to file test.tex
: Processing code chunks ...
:  1 : echo term verbatim
: 
: Error:  chunk 1
: Error in parse(file, n, text, prompt) : parse error
: 
: Any help very much appreciated...

Here a couple of solutions:

1. You could use the my.stdin function from:

   https://stat.ethz.ch/pipermail/r-help/2003-June/033622.html

like this in sweave (untested):

my.stdin <- function( tag, this.file = eval.parent(quote(file),n=3) )
  textConnection( sub(tag, "", grep(tag,readLines(this.file),value=T)) )
timeval <- read.csv(my.stdin("#x"),this.file=sweave.path.file,nrows=2)
#x t1,t2,t3,t4
#x 24.23,26.79,23.47,23.97

where sweave.path.file is the path/filename of your sweave file.


2. A second possibility is to just read it in into R, display it
using dput and copy and paste that into your sweave document.


a. Read your data in using R (note that nrows is the number
   of _data_ rows) and display it using dput:

	R> timeval <- read.csv(stdin(), nrows=1) 
	0: t1,t2,t3,t4
	1: 24.23,26.79,23.47,23.97

	R> dput(timeval)
	structure(list(t1 = 24.23, t2 = 26.79, t3 = 23.47, t4 = 23.97), 
	.Names = c("t1", "t2", "t3", "t4"), class = "data.frame", row.names 
= "1")

b. Copy the output of dput into the clipboard (i.e. select
   it with the mouse and initiate a copy ,e.g. ctrl-c on
   Windows).

c. In the editor with your sweave document enter

	   timeval <- 

   and press Paste, e.g. ctrl-V in windows.



From ihok at hotmail.com  Fri Oct 29 01:38:14 2004
From: ihok at hotmail.com (Jack Tanner)
Date: Thu, 28 Oct 2004 19:38:14 -0400
Subject: [R] reproducible rodbc crash
Message-ID: <418182E6.6070803@hotmail.com>

On Windows XP, R 2.0.0.

 > library(RODBC)
 > mydsn <- odbcDriverConnect("DSN=mydsn")
 > odbcClose(mydsn)
 > sqlQuery(mydsn, "select * from foo_t")

[boom, bam, burp, crash, wheeze]

I grant that an sqlQuery on a closed DSN is a stupid thing to do, but it 
still should't crash, right?



From gerifalte28 at hotmail.com  Fri Oct 29 01:48:54 2004
From: gerifalte28 at hotmail.com (F Z)
Date: Thu, 28 Oct 2004 23:48:54 +0000
Subject: [R] ifelse() question
Message-ID: <BAY2-F34vPEkGhAj5p100002e07@hotmail.com>

Hi

I have a data.frame with dim = 18638 (rows)     6 (cols)

names(dat)
[1] "id"      "long"    "lat"     "species" "type"    "size"

Variable "species" and "type" are factors.  Species has 5 levels "BOV" "CAP" 
"CER" "OVI" "POR"
Variable "type" has 11 levels "BRD" "CL" ... "OTHER"

I would like to replace the values on species by the values on types only if 
species is == "POR"
I tried:

x<-ifelse(dat$species %in% "POR",dat$type,dat$species)
dat[,4]<-x
but levels(x)
[1] "1"  "2"  "3"  "4"  "5"  "6"  "8"  "9"  "10" "11" "12"

So x changes the factor names by numbers.  I can not use factor() to recover 
the names since the resulting factors in x are a mixture of factors from 
species and type.

I also tried

x<-gsub(pattern = "POR",replacement= factor(dat$type),dat$species)  with 
same behavior.

Apparently I did not have my granola bar today so I can't find a solution!  
Any help is greatly appreciated

Thanks!

Francisco



From tlumley at u.washington.edu  Fri Oct 29 02:04:25 2004
From: tlumley at u.washington.edu (Thomas Lumley)
Date: Thu, 28 Oct 2004 17:04:25 -0700 (PDT)
Subject: [R] meaning of frailty estimates
In-Reply-To: <01cd01c4bcfb$b4262b30$36ec8495@Emanuela>
References: <018701c4bcea$37aeaca0$36ec8495@Emanuela>
	<Pine.A41.4.61b.0410280707430.26124@homer04.u.washington.edu>
	<01cd01c4bcfb$b4262b30$36ec8495@Emanuela>
Message-ID: <Pine.A41.4.61b.0410281702550.162042@homer12.u.washington.edu>

On Thu, 28 Oct 2004, Emanuela Rossi wrote:

> Here the output. "Specie" is the frailty variable, with four types of
> answer. I'm asking what's the meaning of gauss:1, ....gauss:4

They are hazard ratios. There is no reference group, instead the log 
hazard ratios sum to zero (hazard ratios multiply to 1).

 	-thomas

> ----------------------------------------------------------------------------
> ------------------------------
>
>>
> SURV1<-read.table("C:/LAVORO/MOBILE/survival/surv_n1.csv",header=TRUE,sep=",
> ")
>> fit_13_2_sp<-coxph(Surv(DATA_INI1,DATA_FIN1,EVENT1)~
> V1+V2+Z+G+dim+frailty.gaussian(specie)+cluster(ID),data=SURV1)
>> summary(fit_13_2_sp)
> Call:
> coxph(formula = Surv(DATA_INI1, DATA_FIN1, EVENT1) ~ V1 + V2 +
>    Z + G + dim + frailty.gaussian(specie) + cluster(ID), data = SURV1)
>
>  n= 233450
>                         coef       se(coef)   se2             Chisq  DF
> p
> V1                  0.04995   0.14021  0.145916    0.13  1.00  7.2e-01
> V2                 -0.79656   0.20483  0.197135  15.12 1.00  1.0e-04
> Z                    -0.00359  0.00067  0.000841   28.76 1.00  8.2e-08
> G                     0.08186  0.00583  0.005796 196.91 1.00  0.0e+00
> dim                  0.39410  0.06981  0.057294   31.87 1.00  1.6e-08
> frailty.gaussian(specie)                                    181.44 2.95
> 0.0e+00
>
>        exp(coef) exp(-coef) lower .95 upper .95
> V1          0.951      1.051     0.723     1.252
> V2          0.451      2.218     0.302     0.674
> Z           0.996      1.004     0.995     0.998
> G           1.085      0.921     1.073     1.098
> dim         1.483      0.674     1.293     1.700
> gauss:1     2.504      0.399     2.108     2.975
> gauss:2     1.799      0.556     1.457     2.221
> gauss:3     0.278      3.599     0.217     0.356
> gauss:4     0.799      1.252     0.641     0.996
>
> Iterations: 5 outer, 11 Newton-Raphson
>     Variance of random effect= 0.975
> Degrees of freedom for terms= 1 1 1 1 1 3
> Rsquare= 0.003   (max possible= 0.025 )
> Likelihood ratio test= 810  on 7.95 df,   p=0
> Wald test            = 600  on 7.95 df,   p=0,   Robust = 355  p=0
>
> ----------------------------------------------------------------------------
> -------------------------------------------
>
>
> Thanks
>
> Emanuela
>
>
> ----- Original Message -----
> From: "Thomas Lumley" <tlumley at u.washington.edu>
> To: "Emanuela Rossi" <emanuela.rossi at unimib.it>
> Cc: <r-help at stat.math.ethz.ch>
> Sent: Thursday, October 28, 2004 4:08 PM
> Subject: Re: [R] meaning of frailty estimates
>
>
>> On Thu, 28 Oct 2004, Emanuela Rossi wrote:
>>
>>> Hello,
>>>
>>> I'm trying to estimate a Cox's survival model with a random effect, so I
>>> have added the instruction frailty.gaussian (name variable) in the
>>> model.
>>>
>>> My frailty variable is a qualitative variable with four types of answer.
>>>
>>> In the resulting output there are the parameter estimates of all the
>>> variables, but there are also four estimates for each type of answer of
>>> the frailty variable. Which kind of estimates are they? Maybe hazard
>>> ratio? But which is the reference?
>>>
>>
>> Perhaps you could post this output so we know what you are talking about.
>>
>>   -thomas
>>
>
>

Thomas Lumley			Assoc. Professor, Biostatistics
tlumley at u.washington.edu	University of Washington, Seattle



From andy_liaw at merck.com  Fri Oct 29 02:12:53 2004
From: andy_liaw at merck.com (Liaw, Andy)
Date: Thu, 28 Oct 2004 20:12:53 -0400
Subject: [R] Double Sided Box-Cox models?
Message-ID: <3A822319EB35174CA3714066D590DCD50994E249@usrymx25.merck.com>

Perhaps you meant the transform-both-sides model studied by Carroll &
Ruppert?  I believe there used to be an S function, but have not seen one
for R.

Andy

> From: Bill West
> 
> Hello!
>   This is my first post to the help list; I have been using R 
> only for a
> couple of months.  I have been able to find answers to most questions
> through the archives, but I have not seen any posts about double sided
> Box-Cox models.  Is there any way to run do this in R?
> Thanks,
> --Bill West
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
> 
>



From PAlspach at hortresearch.co.nz  Fri Oct 29 02:33:54 2004
From: PAlspach at hortresearch.co.nz (Peter Alspach)
Date: Fri, 29 Oct 2004 13:33:54 +1300
Subject: [R] ifelse() question
Message-ID: <s18246d9.097@hra2.marc.hort.cri.nz>


Francisco

Did you try changing the factors to character, with as.character?

Also you don't really need ifelse() for this.  Something like the
following (untested) should do it:

dat[,4] <- as.character(dat[,4])
dat[,5] <- as.character(dat[,5])
dat[dat[,4]=='POR',4] <- dat[dat[,4]=='POR',5]
dat[,4] <- as.factor(dat[,4])
dat[,5] <- as.factor(dat[,5])


Peter Alspach

>>> "F Z" <gerifalte28 at hotmail.com> 29/10/04 12:48:54 >>>
Hi

I have a data.frame with dim = 18638 (rows)     6 (cols)

names(dat)
[1] "id"      "long"    "lat"     "species" "type"    "size"

Variable "species" and "type" are factors.  Species has 5 levels "BOV"
"CAP" 
"CER" "OVI" "POR"
Variable "type" has 11 levels "BRD" "CL" ... "OTHER"

I would like to replace the values on species by the values on types
only if 
species is == "POR"
I tried:

x<-ifelse(dat$species %in% "POR",dat$type,dat$species)
dat[,4]<-x
but levels(x)
[1] "1"  "2"  "3"  "4"  "5"  "6"  "8"  "9"  "10" "11" "12"

So x changes the factor names by numbers.  I can not use factor() to
recover 
the names since the resulting factors in x are a mixture of factors
from 
species and type.

I also tried

x<-gsub(pattern = "POR",replacement= factor(dat$type),dat$species) 
with 
same behavior.

Apparently I did not have my granola bar today so I can't find a
solution!  
Any help is greatly appreciated

Thanks!

Francisco

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-help 
PLEASE do read the posting guide!
http://www.R-project.org/posting-guide.html

______________________________________________________

The contents of this e-mail are privileged and/or confidenti...{{dropped}}



From romainn at hotmail.com  Fri Oct 29 03:15:46 2004
From: romainn at hotmail.com (Romain Neugebauer)
Date: Thu, 28 Oct 2004 18:15:46 -0700
Subject: [R] problem building an R package under Windows XP with calls to
	NAG C routines
Message-ID: <BAY13-F20xhJTANZwwP00036c19@hotmail.com>

Hello all,

I was able to create R packages under windows XP in the past using the 
dynamic NAG C library for windows XP (Mark 6).

Recently, I changed computers and I am now using the static NAG C library 
for windows XP (Mark 7) to create a simple R package (called "test") which 
simply returns random numbers simulated using repetitive calls to a NAG C 
routine (uniform random generator). I use R Version 2.0.0. and installed all 
the tools needed to build R packages as described in 
http://www.murdochsutherland.com/Rtools/ and readme.packages.

I was able to successfully execute the C program that is part of this 
package when compiling it using Microsoft visual studio. However, when I now 
try to create the R package with the command "Rcmd INSTALL test" I obtain 
the following:

C:\Rdevelop>Rcmd INSTALL test


---------- Making package test ------------
  adding build stamp to DESCRIPTION
  making DLL ...
making test.d from test.c
gcc   -Ic:/R/rw2000/include -Wall -O2 -I"C:\Program Files\Numerical 
Algorithms Group\CLW3207DA\include"  -c test.c -o test.o
ar cr test.a test.o
ranlib test.a
windres --include-dir c:/R/rw2000/include  -i test_res.rc -o test_res.o
gcc  --shared -s  -o test.dll test.def test.a test_res.o  
-Lc:/R/rw2000/src/gnuwin32 -L"C:\Program Files\Microsoft Visual 
Studio\VC98\Lib" -L"C:\Program Files\
Numerical Algorithms Group\CLW3207DA" -L"C:\Program Files\Numerical 
Algorithms Group\CLW3207DA\mkl\lib" -lLIBCMT -lnagcsmt-mkl -lmkl_s -lmkl_def 
-lmkl_lapack
-lADVAPI32 -lNETAPI32  -lg2c -lR
Warning: .drectve `%.*s' unrecognized
Warning: .drectve `%.*s' unrecognized
Warning: .drectve `%.*s' unrecognized
Warning: .drectve `%.*s' unrecognized
Warning: .drectve `%.*s' unrecognized
Warning: .drectve `%.*s' unrecognized
Warning: .drectve `%.*s' unrecognized
Warning: .drectve `%.*s' unrecognized
Warning: .drectve `%.*s' unrecognized
Warning: .drectve `%.*s' unrecognized
Warning: .drectve `%.*s' unrecognized
Warning: .drectve `%.*s' unrecognized
Warning: .drectve `%.*s' unrecognized
Warning: .drectve `%.*s' unrecognized
Warning: .drectve `%.*s' unrecognized
Warning: .drectve `%.*s' unrecognized
Warning: .drectve `%.*s' unrecognized
Warning: .drectve `%.*s' unrecognized
Warning: .drectve `%.*s' unrecognized
Warning: .drectve `%.*s' unrecognized
Warning: .drectve `%.*s' unrecognized
Warning: .drectve `%.*s' unrecognized
Warning: .drectve `%.*s' unrecognized
Warning: .drectve `%.*s' unrecognized
Warning: .drectve `%.*s' unrecognized
Warning: .drectve `%.*s' unrecognized
/mingw/lib/libmingw32.a(main.o)(.text+0x106):main.c: undefined reference to 
`WinMain at 16'
make[3]: *** [test.dll] Error 1
make[2]: *** [srcDynlib] Error 2
make[1]: *** [all] Error 2
make: *** [pkg-test] Error 2
*** Installation of test failed ***

Removing 'C:/R/rw2000/library/test'
Restoring previous 'C:/R/rw2000/library/test'


Interestingly enough, if I remove the calls to the NAG C routine in the C 
program AND if I remove the gcc flag "-lLIBCMT", the R package can be 
created successfully and I can use the package in R (it will simply print a 
"hello the world" line). I obtain:

C:\Rdevelop>Rcmd INSTALL test


---------- Making package test ------------
  adding build stamp to DESCRIPTION
  making DLL ...
gcc  --shared -s  -o test.dll test.def test.a test_res.o  
-Lc:/R/rw2000/src/gnuwin32 -L"C:\Program Files\Microsoft Visual 
Studio\VC98\Lib" -L"C:\Program Files\
Numerical Algorithms Group\CLW3207DA" -L"C:\Program Files\Numerical 
Algorithms Group\CLW3207DA\mkl\lib"  -lnagcsmt-mkl -lmkl_s -lmkl_def 
-lmkl_lapack  -lADVAPI
32 -lNETAPI32  -lg2c -lR
  ... DLL made
  installing DLL
  installing R files
  installing data files
  installing man source files
  installing indices
  not zipping data
  installing help
 >>> Building/Updating help pages for package 'test'
     Formats: text html latex example chm
  f                                 text    html    latex   example
  adding MD5 sums

* DONE (test)

If I add the flag "-lLIBCMT" (and the C program still does not call any NAG 
routine) I get the following:

C:\Rdevelop>Rcmd INSTALL test


---------- Making package test ------------
  adding build stamp to DESCRIPTION
  making DLL ...
making test.d from test.c
gcc   -Ic:/R/rw2000/include -Wall -O2 -I"C:\Program Files\Numerical 
Algorithms Group\CLW3207DA\include"  -c test.c -o test.o
test.c: In function `test':
test.c:7: warning: unused variable `i'
ar cr test.a test.o
ranlib test.a
gcc  --shared -s  -o test.dll test.def test.a test_res.o  
-Lc:/R/rw2000/src/gnuwin32 -L"C:\Program Files\Microsoft Visual 
Studio\VC98\Lib" -L"C:\Program Files\
Numerical Algorithms Group\CLW3207DA" -L"C:\Program Files\Numerical 
Algorithms Group\CLW3207DA\mkl\lib" -lLIBCMT -lnagcsmt-mkl -lmkl_s -lmkl_def 
-lmkl_lapack
-lADVAPI32 -lNETAPI32  -lg2c -lR
Warning: .drectve `%.*s' unrecognized
Warning: .drectve `%.*s' unrecognized
/mingw/lib/libmingw32.a(main.o)(.text+0x106):main.c: undefined reference to 
`WinMain at 16'
make[3]: *** [test.dll] Error 1
make[2]: *** [srcDynlib] Error 2
make[1]: *** [all] Error 2
make: *** [pkg-test] Error 2
*** Installation of test failed ***

Removing 'C:/R/rw2000/library/test'
Restoring previous 'C:/R/rw2000/library/test'

So it appears that the problem is not related to the calls to the NAG C 
routines but to the library "LIBCMT" which is required when using the NAG 
routines.
I contacted the NAG people who recommended to check the version of the 
binutils and gcc from Mingw. I use gcc version 3.3.1 and I update the 
binutils from
http://www.mingw.org/download.shtml using 
binutils-2.13.90-20021006-2.tar.gz.

Any help would be greatly appreciated.
Note that I am able to build the same R package with calls to NAG routines 
under linux with the static NAG C library for linux.

Thank you,

Romain



From kjetil at acelerate.com  Fri Oct 29 03:16:03 2004
From: kjetil at acelerate.com (Kjetil Brinchmann Halvorsen)
Date: Thu, 28 Oct 2004 21:16:03 -0400
Subject: [R] as.list.matrix
Message-ID: <418199D3.2050904@acelerate.com>

I found the need of converting a matrix into a list of its columns
(for use with do.call), and was surprised there was no method
as.list.matrix, it could easily be a part of as.list default

I wrote

my.as.list.matrix <- function(mat) {
       if(!is.matrix(mat))stop("Argument must be a matrix")
       n <- NCOL(mat)
       res <- vector(mode="list", length=n)
       for (i in seq(along=res))
         res[[i]] <- mat[,i]
       res
}

but still think there must be some in-built function doing this!

By the way, my use is

 with(VRS,do.call("scatter.smooth",
    c(my.as.list.matrix(na.omit(cbind(Tmed, 
resid(VRS.mod1,type="response")))),
        xlab="", ylab="")))

there shoud be an easier way for such a daily task?

Kjetil

-- 

Kjetil Halvorsen.

Peace is the most effective weapon of mass construction.
               --  Mahdi Elmandjra



From murdoch at stats.uwo.ca  Fri Oct 29 04:06:52 2004
From: murdoch at stats.uwo.ca (Duncan Murdoch)
Date: Thu, 28 Oct 2004 22:06:52 -0400
Subject: [R] problem building an R package under Windows XP with calls to
	NAG C routines
In-Reply-To: <BAY13-F20xhJTANZwwP00036c19@hotmail.com>
References: <BAY13-F20xhJTANZwwP00036c19@hotmail.com>
Message-ID: <qq83o0trlt4l0nlle3kvi5i9fs86c8k2ce@4ax.com>

The problem is that MSVC libraries are not compatible with gcc.  The
gcc linker doesn't know what to do with them, they're in a different
format than it expects.

To do static linking, you'll need to use MSVC to compile and link your
DLL in the package.  This means setting up a Makefile.win so that MSVC
gets called instead of gcc.  There's some info on this in the
README.packages file, but it's incomplete, because none of the core
developers use MSVC.   If you want to add to it, send me the patches.

Alternatively, just install the package using your "Hello, world" DLL,
then compile the real DLL using MSVC and put it into the libs
subdirectory where your package was installed.

Note that if you want to put your package on CRAN, you'll have to be
able to build it without proprietary tools.

Duncan Murdoch

On Thu, 28 Oct 2004 18:15:46 -0700, "Romain Neugebauer"
<romainn at hotmail.com> wrote:

>Hello all,
>
>I was able to create R packages under windows XP in the past using the 
>dynamic NAG C library for windows XP (Mark 6).
>
>Recently, I changed computers and I am now using the static NAG C library 
>for windows XP (Mark 7) to create a simple R package (called "test") which 
>simply returns random numbers simulated using repetitive calls to a NAG C 
>routine (uniform random generator). I use R Version 2.0.0. and installed all 
>the tools needed to build R packages as described in 
>http://www.murdochsutherland.com/Rtools/ and readme.packages.
>
>I was able to successfully execute the C program that is part of this 
>package when compiling it using Microsoft visual studio. However, when I now 
>try to create the R package with the command "Rcmd INSTALL test" I obtain 
>the following:
>
>C:\Rdevelop>Rcmd INSTALL test
>
>
>---------- Making package test ------------
>  adding build stamp to DESCRIPTION
>  making DLL ...
>making test.d from test.c
>gcc   -Ic:/R/rw2000/include -Wall -O2 -I"C:\Program Files\Numerical 
>Algorithms Group\CLW3207DA\include"  -c test.c -o test.o
>ar cr test.a test.o
>ranlib test.a
>windres --include-dir c:/R/rw2000/include  -i test_res.rc -o test_res.o
>gcc  --shared -s  -o test.dll test.def test.a test_res.o  
>-Lc:/R/rw2000/src/gnuwin32 -L"C:\Program Files\Microsoft Visual 
>Studio\VC98\Lib" -L"C:\Program Files\
>Numerical Algorithms Group\CLW3207DA" -L"C:\Program Files\Numerical 
>Algorithms Group\CLW3207DA\mkl\lib" -lLIBCMT -lnagcsmt-mkl -lmkl_s -lmkl_def 
>-lmkl_lapack
>-lADVAPI32 -lNETAPI32  -lg2c -lR
>Warning: .drectve `%.*s' unrecognized
>Warning: .drectve `%.*s' unrecognized
>Warning: .drectve `%.*s' unrecognized
>Warning: .drectve `%.*s' unrecognized
>Warning: .drectve `%.*s' unrecognized
>Warning: .drectve `%.*s' unrecognized
>Warning: .drectve `%.*s' unrecognized
>Warning: .drectve `%.*s' unrecognized
>Warning: .drectve `%.*s' unrecognized
>Warning: .drectve `%.*s' unrecognized
>Warning: .drectve `%.*s' unrecognized
>Warning: .drectve `%.*s' unrecognized
>Warning: .drectve `%.*s' unrecognized
>Warning: .drectve `%.*s' unrecognized
>Warning: .drectve `%.*s' unrecognized
>Warning: .drectve `%.*s' unrecognized
>Warning: .drectve `%.*s' unrecognized
>Warning: .drectve `%.*s' unrecognized
>Warning: .drectve `%.*s' unrecognized
>Warning: .drectve `%.*s' unrecognized
>Warning: .drectve `%.*s' unrecognized
>Warning: .drectve `%.*s' unrecognized
>Warning: .drectve `%.*s' unrecognized
>Warning: .drectve `%.*s' unrecognized
>Warning: .drectve `%.*s' unrecognized
>Warning: .drectve `%.*s' unrecognized
>/mingw/lib/libmingw32.a(main.o)(.text+0x106):main.c: undefined reference to 
>`WinMain at 16'
>make[3]: *** [test.dll] Error 1
>make[2]: *** [srcDynlib] Error 2
>make[1]: *** [all] Error 2
>make: *** [pkg-test] Error 2
>*** Installation of test failed ***
>
>Removing 'C:/R/rw2000/library/test'
>Restoring previous 'C:/R/rw2000/library/test'
>
>
>Interestingly enough, if I remove the calls to the NAG C routine in the C 
>program AND if I remove the gcc flag "-lLIBCMT", the R package can be 
>created successfully and I can use the package in R (it will simply print a 
>"hello the world" line). I obtain:
>
>C:\Rdevelop>Rcmd INSTALL test
>
>
>---------- Making package test ------------
>  adding build stamp to DESCRIPTION
>  making DLL ...
>gcc  --shared -s  -o test.dll test.def test.a test_res.o  
>-Lc:/R/rw2000/src/gnuwin32 -L"C:\Program Files\Microsoft Visual 
>Studio\VC98\Lib" -L"C:\Program Files\
>Numerical Algorithms Group\CLW3207DA" -L"C:\Program Files\Numerical 
>Algorithms Group\CLW3207DA\mkl\lib"  -lnagcsmt-mkl -lmkl_s -lmkl_def 
>-lmkl_lapack  -lADVAPI
>32 -lNETAPI32  -lg2c -lR
>  ... DLL made
>  installing DLL
>  installing R files
>  installing data files
>  installing man source files
>  installing indices
>  not zipping data
>  installing help
> >>> Building/Updating help pages for package 'test'
>     Formats: text html latex example chm
>  f                                 text    html    latex   example
>  adding MD5 sums
>
>* DONE (test)
>
>If I add the flag "-lLIBCMT" (and the C program still does not call any NAG 
>routine) I get the following:
>
>C:\Rdevelop>Rcmd INSTALL test
>
>
>---------- Making package test ------------
>  adding build stamp to DESCRIPTION
>  making DLL ...
>making test.d from test.c
>gcc   -Ic:/R/rw2000/include -Wall -O2 -I"C:\Program Files\Numerical 
>Algorithms Group\CLW3207DA\include"  -c test.c -o test.o
>test.c: In function `test':
>test.c:7: warning: unused variable `i'
>ar cr test.a test.o
>ranlib test.a
>gcc  --shared -s  -o test.dll test.def test.a test_res.o  
>-Lc:/R/rw2000/src/gnuwin32 -L"C:\Program Files\Microsoft Visual 
>Studio\VC98\Lib" -L"C:\Program Files\
>Numerical Algorithms Group\CLW3207DA" -L"C:\Program Files\Numerical 
>Algorithms Group\CLW3207DA\mkl\lib" -lLIBCMT -lnagcsmt-mkl -lmkl_s -lmkl_def 
>-lmkl_lapack
>-lADVAPI32 -lNETAPI32  -lg2c -lR
>Warning: .drectve `%.*s' unrecognized
>Warning: .drectve `%.*s' unrecognized
>/mingw/lib/libmingw32.a(main.o)(.text+0x106):main.c: undefined reference to 
>`WinMain at 16'
>make[3]: *** [test.dll] Error 1
>make[2]: *** [srcDynlib] Error 2
>make[1]: *** [all] Error 2
>make: *** [pkg-test] Error 2
>*** Installation of test failed ***
>
>Removing 'C:/R/rw2000/library/test'
>Restoring previous 'C:/R/rw2000/library/test'
>
>So it appears that the problem is not related to the calls to the NAG C 
>routines but to the library "LIBCMT" which is required when using the NAG 
>routines.
>I contacted the NAG people who recommended to check the version of the 
>binutils and gcc from Mingw. I use gcc version 3.3.1 and I update the 
>binutils from
>http://www.mingw.org/download.shtml using 
>binutils-2.13.90-20021006-2.tar.gz.
>
>Any help would be greatly appreciated.
>Note that I am able to build the same R package with calls to NAG routines 
>under linux with the static NAG C library for linux.
>
>Thank you,
>
>Romain
>
>______________________________________________
>R-help at stat.math.ethz.ch mailing list
>https://stat.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html



From PAlspach at hortresearch.co.nz  Fri Oct 29 04:08:21 2004
From: PAlspach at hortresearch.co.nz (Peter Alspach)
Date: Fri, 29 Oct 2004 15:08:21 +1300
Subject: [R] as.list.matrix
Message-ID: <s1825cf4.028@hra2.marc.hort.cri.nz>


Kjetil

Isn't a data.frame as special type of list, and thus one could use
as.data.frame?

Peter Alspach


>>> Kjetil Brinchmann Halvorsen <kjetil at acelerate.com> 29/10/04
14:16:03 >>>
I found the need of converting a matrix into a list of its columns
(for use with do.call), and was surprised there was no method
as.list.matrix, it could easily be a part of as.list default

I wrote

my.as.list.matrix <- function(mat) {
       if(!is.matrix(mat))stop("Argument must be a matrix")
       n <- NCOL(mat)
       res <- vector(mode="list", length=n)
       for (i in seq(along=res))
         res[[i]] <- mat[,i]
       res
}

but still think there must be some in-built function doing this!

By the way, my use is

 with(VRS,do.call("scatter.smooth",
    c(my.as.list.matrix(na.omit(cbind(Tmed, 
resid(VRS.mod1,type="response")))),
        xlab="", ylab="")))

there shoud be an easier way for such a daily task?

Kjetil

-- 

Kjetil Halvorsen.

Peace is the most effective weapon of mass construction.
               --  Mahdi Elmandjra

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-help 
PLEASE do read the posting guide!
http://www.R-project.org/posting-guide.html

______________________________________________________

The contents of this e-mail are privileged and/or confidenti...{{dropped}}



From pravi at gawab.com  Fri Oct 29 05:53:12 2004
From: pravi at gawab.com (Ravindra de silva)
Date: Fri, 29 Oct 2004 03:53:12 GMT
Subject: [R] ( mixture discriminant analysis)
Message-ID: <20041029035312.22159.qmail@gawab.com>


I am trying to use MDA ( mixture discriminant analysis) with
MARS method for classifying 04 groups.

My problem is i want to find out which independent variables are
most important for classifying 04 groups. How can i extract this
information through R? 

If some one can give me help its great for me

Thanks
Sena  
---------------------------------------------
Free POP3 Email from www.Gawab.com 
Sign up NOW and get your account @gawab.com!!



From ripley at stats.ox.ac.uk  Fri Oct 29 07:48:15 2004
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Fri, 29 Oct 2004 06:48:15 +0100 (BST)
Subject: [R]: a package problem
In-Reply-To: <41817690.6050703@jhsph.edu>
Message-ID: <Pine.LNX.4.44.0410290646440.14762-100000@gannet.stats>

On Thu, 28 Oct 2004, Roger D. Peng wrote:

> It seems you downloaded a precompiled binary (for Windows) that was most likely 
> built for a version of R < 2.0.0.  Either the author needs to rebuild the 
> package for R 2.0.0 or you need to grab the source package and build the Windows 
> binary for yourself.

There is a package `faraway' on CRAN, and that has a binary for Windows,
for R 2.0.0.  Looks like the same thing, so just try installing it from 
the RGui menus.

> 
> -roger
> 
> Xin Qi wrote:
> 
> > Dear R- users and Helpers:
> > 
> > I downloaded the package from www.stat.lsa.umich.edu/~faraway/book and 
> > installed it from local zip file.
> > It looked fine. But when I input
> > 
> > library(faraway)
> > 
> > it showed " Error in library(faraway) : 'faraway' is not a valid package 
> > --- installed < 2.0.0?
> > 
> > What I used is R 2.0.0 version now.
> > 
> > What should I do?
> > Thank you very much.
> > 
> > Xin
> > 
> > ______________________________________________
> > R-help at stat.math.ethz.ch mailing list
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide! 
> > http://www.R-project.org/posting-guide.html
> > 
> 
> 

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From ripley at stats.ox.ac.uk  Fri Oct 29 08:08:18 2004
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Fri, 29 Oct 2004 07:08:18 +0100 (BST)
Subject: [R] as.list.matrix
In-Reply-To: <s1825cf4.028@hra2.marc.hort.cri.nz>
Message-ID: <Pine.LNX.4.44.0410290652310.14762-100000@gannet.stats>

On Fri, 29 Oct 2004, Peter Alspach wrote:

> 
> Kjetil
> 
> Isn't a data.frame as special type of list, and thus one could use
> as.data.frame?

Yes, or

   split(mat, col(mat))

works.  So simple it hardly needs an as.list method.


> Peter Alspach
> 
> 
> >>> Kjetil Brinchmann Halvorsen <kjetil at acelerate.com> 29/10/04
> 14:16:03 >>>
> I found the need of converting a matrix into a list of its columns
> (for use with do.call), and was surprised there was no method
> as.list.matrix, it could easily be a part of as.list default
> 
> I wrote
> 
> my.as.list.matrix <- function(mat) {
>        if(!is.matrix(mat))stop("Argument must be a matrix")
>        n <- NCOL(mat)
>        res <- vector(mode="list", length=n)
>        for (i in seq(along=res))
>          res[[i]] <- mat[,i]
>        res
> }
> 
> but still think there must be some in-built function doing this!
> 
> By the way, my use is
> 
>  with(VRS,do.call("scatter.smooth",
>     c(my.as.list.matrix(na.omit(cbind(Tmed, 
> resid(VRS.mod1,type="response")))),
>         xlab="", ylab="")))
> 
> there shoud be an easier way for such a daily task?

Daily??  You created the matrix in the first place: why not create a data
frame instead - na.omit was written for data frames?  And surely do.call
is overkill for a 2-column matrix, and there seems a surplus c() in there.

m <- na.omit(cbind(VRS$Tmed, resid(VRS.mod1,type="response"))
scatter.smooth(m[,1], m[, 2], xlab="", ylab="")

is much easier to follow.

But do you need this?  The strange thing is that loess.smooth seems to 
have been written expecting missing values, but does not work with them.
It is easy to fix.


-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From ligges at statistik.uni-dortmund.de  Fri Oct 29 08:36:37 2004
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Fri, 29 Oct 2004 08:36:37 +0200
Subject: [R] plot.baysian error = only 0's may mix with negative subscripts
In-Reply-To: <1098987290.17171.10.camel@bigbird>
References: <200410281502.i9SF2TBX014947@lulu.it.northwestern.edu>	<41812FD9.4000506@statistik.uni-dortmund.de>
	<1098987290.17171.10.camel@bigbird>
Message-ID: <4181E4F5.3020005@statistik.uni-dortmund.de>

Konstantinos Liolios wrote:

> I did make a typo and I meant plot.bayesian() from sma.  Hopefully most
> people can easily guess what I meant.  

Forgive me, my brain is too small! I do not know which of the millions 
of function is in which of the > 400 CRAN packages.
It's is your homework to say which package it is in and to specify an 
example, since other people already invest quite a lot of time in order 
  to provide help. Please read the posting guide!


 > It looks like NAs are illegal
> from 1.9 and later.  I am surprised that stat.bayesian() still allows it
> but plot has a problem with it.

Maybe. The error message does not suggest it directly and you haven't 
told us anything about the data (in particular nothing about NAs). You 
have still not given an easily reproducible example.

Uwe Ligges

> Thanx
> Dinos
> 
>  peopleOn Thu, 2004-10-28 at 12:43, Uwe Ligges wrote:
> 
>>dinos at northwestern.edu wrote:
>>
>>>Dear R users and developers
>>>
>>>After upgrading to Windows XP and R 1.9.1 and 2.0, I retried to execute
>>>plot.baysian() to a data set that I had used previously to plot with no
>>>problem in win2000 R1.8.  The error I get is:
>>>
>>>Error in points(Mbar[-index], lods[-index], pch = ".") : 
>>>        only 0's may mix with negative subscripts
>>
>>What is plot.baysian()?
>>Do you mean  plot.bayesian()  from package "sma"?
>>Probably an error has been fixed that caused your data to be plotted 
>>inaccurately in R < 1.9.x ... or there is a bug in sma.
>>But without the data it's hard to say.
>>
>>So the suggestion is to ask the package maintainer of sma by providing a 
>>simple and easily reproducible example...
>>
>>Uwe Ligges
>>
>>
>>
>>>Thanx in advance
>>>Dino
>>>P.S.  I allready sent this message once but without a subject.  I
>>>apologize for the inconvenience
>>
>>P.S. And with this message we do know thrice that you are:
>>
>>
>>
>>>Konstantinos G. Liolios
>>>IT Software Engineer II
>>>Charles E. and Emma H. Morrison
>>>Depts.  Pathology and Microbiology-Immunology
>>>Northwestern University
>>>Ward 3-240
>>>303 E. Chicago Avenue
>>>Chicago IL 60611
>>>Tel: 312-503-0224
>>>Fax:312-503-0281
>>>http://www.haldarlab.northwestern.edu
>>
>>[I removed the next two information blocks who you are, since I think 
>>it's sufficient for most of us to have one of these blocks...]
>>
> 
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html



From petr.pikal at precheza.cz  Fri Oct 29 09:04:13 2004
From: petr.pikal at precheza.cz (Petr Pikal)
Date: Fri, 29 Oct 2004 09:04:13 +0200
Subject: [R] array problem and for looping
In-Reply-To: <220.253.2.55.1098985889.48213@my.monash.edu.au>
Message-ID: <4182078D.8052.5405B4@localhost>

Hi

Beter not to give a same name your values (variables) as is 
function name. R is quite clever and

sample <- rnorm(10)
sample <- sample(sample,3)

works as expected, but it is not a rule.

Cheers
Petr

On 28 Oct 2004 at 17:54, Kunal Shetty wrote:

> Dear R- users and Helpers
> 
> Is there some way to re initialise or clear the array elements? 
> Pardon me for being vague but the problem itself quite vague. I have
> attached the code along with email. When I run the saved r- code using
> source("random1.txt") , command.
> 
> The program runs fine..but at times there is an error see below # ; 
> but again after the error if re-excuted it would work fine....I  probably
> missed some array detail in my program any suggestions
> 
> #Error in var(parrX[1, ]) : missing observations in cov/cor
> 
> parrX[] is an array .
> 
>  2) Also pardon me for the lengthy procedural code
>         but as you could see in it..i have used the for loop for
>         finding the positions (indexes) of the missing values and
>         later carry out updating the new array element values at those
>         particular positions/
>          So how can I escape the for loop in this case ?  i.e get the
>          missing position indexes and save another object ay vector or
>          array ?
> 
>         And also later wanted to use matrix or vector multiplication
>         (%*%)  for the updating statement
>                         newy[i]<- u2 + covXY/varX * (sample$x[i] - u1)
> 
>          is any of the apply function good out here ?
> 
>   I really feel that I am doing something very routine and donkey work
>   and I am most certain that powerful R - functions could just execute
>   the same 10 liner for loop condition to mere 4 lines ? but how....I am
>   getting lost in the sea of functions here....
> 
> Thank u for reading
> Regards
> Kunal
> 

Petr Pikal
petr.pikal at precheza.cz



From gb at stat.umu.se  Fri Oct 29 09:16:05 2004
From: gb at stat.umu.se (Gran Brostrm)
Date: Fri, 29 Oct 2004 09:16:05 +0200
Subject: [R][OT] Latex Integration question
In-Reply-To: <BAY12-F301tMVyoz5Bl000088c1@hotmail.com>
References: <BAY12-F301tMVyoz5Bl000088c1@hotmail.com>
Message-ID: <20041029071605.GA5468@stat.umu.se>

On Thu, Oct 28, 2004 at 05:13:10PM -0500, Laura Holt wrote:
> Hi R users!
> 
> I have a Latex Question:
> 
> When I use the following:
> 
> \begin{eqnarray*}
> F(x) & = & -\lambda e^{-\lambda t} \mid_{0}^{x}
> \end{eqnarray*}
> the output looks ok.
> But I would like the limits of integration line to be longer, if possible.  
> Does anyone have any suggestions, please?

1. \usepackage{amsmath}, use equation* + split instead of eqnarray*.

2.  Look for \lvert, \lVert, \bigl, \Bigl, etc, e.g.

\begin{equation*}
\begin{split}
F(x) & =  -\lambda e^{-\lambda t} \biggl\lvert_{0}^{x}
.......
\end{split}
\end{equation*}

3. Join a latex mailing list?

G??ran
-- 
 G??ran Brostr??m                    tel: +46 90 786 5223
 Department of Statistics          fax: +46 90 786 6614
 Ume?? University                   http://www.stat.umu.se/egna/gb/
 SE-90187 Ume??, Sweden             e-mail: gb at stat.umu.se



From GICOLOMBO at cesi.it  Fri Oct 29 09:40:37 2004
From: GICOLOMBO at cesi.it (Colombo Giovanni  (FIA))
Date: Fri, 29 Oct 2004 09:40:37 +0200
Subject: [R] Interfacing R_project from program languages
Message-ID: <16B50710AA5FB44DA838F3950D50D679369231@pegasus.cesi.lan>

Hello,
I need an information: is it possible to  call the statistical R_project funtions from a C or Fortran application? In the r_exts.pdf manual I found that is possible to interface R-project with compiled C/Fortran routines, but I didn't found anything about the opposite situation.
Thanks in advance

Giovanni Colombo
CESI spa - Funzione Informatica e Automazione
Via Rubattino 54 - 20134 Milano - Italy
Tel. 0039 02 2125.5687
Fax 0039 02 2125.5520
e-mail: giovanni.colombo at cesi.it
web: www.cesi.it



From ripley at stats.ox.ac.uk  Fri Oct 29 09:51:53 2004
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Fri, 29 Oct 2004 08:51:53 +0100 (BST)
Subject: [R] Interfacing R_project from program languages
In-Reply-To: <16B50710AA5FB44DA838F3950D50D679369231@pegasus.cesi.lan>
Message-ID: <Pine.LNX.4.44.0410290849500.18898-100000@gannet.stats>

On Fri, 29 Oct 2004, Colombo Giovanni  (FIA) wrote:

> I need an information: is it possible to call the statistical R_project
> funtions from a C or Fortran application? In the r_exts.pdf manual I
> found that is possible to interface R-project with compiled C/Fortran
> routines, but I didn't found anything about the opposite situation.

Yes. See developer.r-project.org, and www.omegahat.org, and for Windows 
look at the rw-FAQ about COM facilities.

There will be a section about this in R-exts.pdf (sic) in the next 
release, as I am currently writing it.  Look also in the tests/Embedding 
directory.

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From ihok at hotmail.com  Fri Oct 29 09:58:27 2004
From: ihok at hotmail.com (Jack Tanner)
Date: Fri, 29 Oct 2004 03:58:27 -0400
Subject: [R] Re: reproducible rodbc crash
In-Reply-To: <418182E6.6070803@hotmail.com>
References: <418182E6.6070803@hotmail.com>
Message-ID: <4181F823.3000409@hotmail.com>

Naturally, the problem has already been fixed. update.packages() has an 
upgrade to RODBC 1.1-2, which does not crash.

Huge thanks to Brian Ripley.

Jack Tanner wrote:
> On Windows XP, R 2.0.0.
> 
>  > library(RODBC)
>  > mydsn <- odbcDriverConnect("DSN=mydsn")
>  > odbcClose(mydsn)
>  > sqlQuery(mydsn, "select * from foo_t")
> 
> [boom, bam, burp, crash, wheeze]
> 
> I grant that an sqlQuery on a closed DSN is a stupid thing to do, but it 
> still should't crash, right?



From sneumann at TechFak.Uni-Bielefeld.DE  Fri Oct 29 10:12:10 2004
From: sneumann at TechFak.Uni-Bielefeld.DE (Steffen Neumann)
Date: Fri, 29 Oct 2004 10:12:10 +0200 (MET DST)
Subject: [R] winDialog (equivalent) on Unix anyone ?
Message-ID: <200410290812.KAA19289@ravel.TechFak.Uni-Bielefeld.DE>

Hi,

We have a few R scripts, that have a minimalistic 
GUI Interface using winDialog() calls.

Is there any cross-plattform equivalent we can port to,
such that the code will run on both Windows and Unix ?

I thought about either tcltk or gtk Stuff.
I was astonished that I couldn't find anything 
similar that had been implemented since the 0.99 release,
or do I miss something obvius ?

Yours,
Steffen



From hb at maths.lth.se  Fri Oct 29 10:26:07 2004
From: hb at maths.lth.se (Henrik Bengtsson)
Date: Fri, 29 Oct 2004 10:26:07 +0200
Subject: [R] winDialog (equivalent) on Unix anyone ?
In-Reply-To: <200410290812.KAA19289@ravel.TechFak.Uni-Bielefeld.DE>
Message-ID: <002b01c4bd90$f06d9430$1bf1ba51@hblaptop>

Check out James Wettenhall's 'R TclTk Examples';

  http://bioinf.wehi.edu.au/~wettenhall/RTclTkExamples/

Henrik Bengtsson

> -----Original Message-----
> From: r-help-bounces at stat.math.ethz.ch 
> [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Steffen Neumann
> Sent: Friday, October 29, 2004 10:12 AM
> To: r-help at stat.math.ethz.ch
> Subject: [R] winDialog (equivalent) on Unix anyone ?
> 
> 
> Hi,
> 
> We have a few R scripts, that have a minimalistic 
> GUI Interface using winDialog() calls.
> 
> Is there any cross-plattform equivalent we can port to,
> such that the code will run on both Windows and Unix ?
> 
> I thought about either tcltk or gtk Stuff.
> I was astonished that I couldn't find anything 
> similar that had been implemented since the 0.99 release,
> or do I miss something obvius ?
> 
> Yours,
> Steffen
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list 
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
> 
>



From avril.coghlan at ucd.ie  Fri Oct 29 11:34:35 2004
From: avril.coghlan at ucd.ie (Avril Coghlan)
Date: 29 Oct 2004 10:34:35 +0100
Subject: [R] missing values in logistic regression
Message-ID: <1099042475.3777.18.camel@bioinf14>

Dear R help list,

   I am trying to do a logistic regression
where I have a categorical response variable Y
and two numerical predictors X1 and X2. There
are quite a lot of missing values for predictor X2.
eg.,

Y     X1   X2
red   0.6  0.2    *
red   0.5  0.2    *
red   0.5  NA
red   0.5  NA
green 0.2  0.1    *
green 0.1  NA
green 0.1  NA
green 0.05 0.05   *


I am wondering can I combine X1 and X2 in
a logistic regression to predict Y, using
all the data for X1, even though there are NAs in
the X2 data?

Or do I have to take only the cases for which
there is data for both X1 and X2? (marked
with *s above)

I will be very grateful for any help,

sincerely,
Avril Coghlan
University College Dublin, Ireland



From cg.pettersson at evp.slu.se  Fri Oct 29 12:00:14 2004
From: cg.pettersson at evp.slu.se (CG Pettersson)
Date: Fri, 29 Oct 2004 12:00:14 +0200
Subject: [R] Subsetting with more than one criteria
Message-ID: <200410291000.i9TA0Dkm029291@mail1.slu.se>

Hello all!

I have been using R for two years, but still have trivial problems.
For the moment, I run R1.9.1 on a W2k computer.

I work with a dataset from a variety evaluation project. Every year 25
varieties have been tested. The tested cultivars have been the same
each year in all trials, but some have changed over the three year
period. Now I want to do some calculations and plotting on subsets.
The structure looks like this:

Year	ADB	Block	Vcode	Variety	Yield	Protein			
2002	SW0024	1	20226	Denise	5843	12.8....	
2002	SW0024	1	9865	Astoria	6729	11.4	
2002	SW0024	1	9622	Barke	6121	12	
2002	SW0024	1	9604	Cecilia	5579	12.7	
2002	SW0024	1	20223	Granta	5591	11.6	
2002	SW0024	1	20222	Class	5591	11.7	
2002	SW0024	1	9922	Wiking	5744	12.5
2002	SW0024	1	20103	Vortex	5863	10.6
.
.
And so on both down and sideways.
Three years and four trials with three replicats each gives 900 lines.


How do I use several criteria to subset this?

Ast <- subset(data, Variety == "Astoria") 

works fine giving back the 36 lines where Astoria appears.
But how do I pick two (or more) varieties? AND picking one (or two)
years?

Everything I have tried results in rubbish, like:

AstBark <- subset(data, Variety == c("Astoria","Barke"))

which seems to work but gives back 19 and 18 lines of the varieties 

respectively. There exists 36 of each....

Thanks

/CG


CG Pettersson, MSci, PhD Stud.
Swedish University of Agricultural Sciences
Dep. of Ecology and Crop Production. Box 7043
SE-750 07 Uppsala



From p.dalgaard at biostat.ku.dk  Fri Oct 29 12:06:51 2004
From: p.dalgaard at biostat.ku.dk (Peter Dalgaard)
Date: 29 Oct 2004 12:06:51 +0200
Subject: [R] missing values in logistic regression
In-Reply-To: <1099042475.3777.18.camel@bioinf14>
References: <1099042475.3777.18.camel@bioinf14>
Message-ID: <x2u0sd7s90.fsf@biostat.ku.dk>

Avril Coghlan <avril.coghlan at ucd.ie> writes:

> Dear R help list,
> 
>    I am trying to do a logistic regression
> where I have a categorical response variable Y
> and two numerical predictors X1 and X2. There
> are quite a lot of missing values for predictor X2.
> eg.,
> 
> Y     X1   X2
> red   0.6  0.2    *
> red   0.5  0.2    *
> red   0.5  NA
> red   0.5  NA
> green 0.2  0.1    *
> green 0.1  NA
> green 0.1  NA
> green 0.05 0.05   *
> 
> 
> I am wondering can I combine X1 and X2 in
> a logistic regression to predict Y, using
> all the data for X1, even though there are NAs in
> the X2 data?
> 
> Or do I have to take only the cases for which
> there is data for both X1 and X2? (marked
> with *s above)
> 
> I will be very grateful for any help,

The "built-in" function (glm) for logistic regression will give you
a complete-case analysis. 

For more advanced handling of missing values, you need to look into
imputation methods. Two CRAN packages (at least) are dealing with
this, namely "mix" and "mitools". The former is support software for a
book, which you'll probably want to consult.

-- 
   O__  ---- Peter Dalgaard             Blegdamsvej 3  
  c/ /'_ --- Dept. of Biostatistics     2200 Cph. N   
 (*) \(*) -- University of Copenhagen   Denmark      Ph: (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)             FAX: (+45) 35327907



From andy_liaw at merck.com  Fri Oct 29 12:35:45 2004
From: andy_liaw at merck.com (Liaw, Andy)
Date: Fri, 29 Oct 2004 06:35:45 -0400
Subject: [R] Subsetting with more than one criteria
Message-ID: <3A822319EB35174CA3714066D590DCD50994E24B@usrymx25.merck.com>

Use %in% instead of ==, and string conditions together with &.

HTH,
Andy

> From: CG Pettersson
> 
> Hello all!
> 
> I have been using R for two years, but still have trivial problems.
> For the moment, I run R1.9.1 on a W2k computer.
> 
> I work with a dataset from a variety evaluation project. Every year 25
> varieties have been tested. The tested cultivars have been the same
> each year in all trials, but some have changed over the three year
> period. Now I want to do some calculations and plotting on subsets.
> The structure looks like this:
> 
> Year	ADB	Block	Vcode	Variety	Yield	Protein			
> 2002	SW0024	1	20226	Denise	5843	12.8....	
> 2002	SW0024	1	9865	Astoria	6729	11.4	
> 2002	SW0024	1	9622	Barke	6121	12	
> 2002	SW0024	1	9604	Cecilia	5579	12.7	
> 2002	SW0024	1	20223	Granta	5591	11.6	
> 2002	SW0024	1	20222	Class	5591	11.7	
> 2002	SW0024	1	9922	Wiking	5744	12.5
> 2002	SW0024	1	20103	Vortex	5863	10.6
> .
> .
> And so on both down and sideways.
> Three years and four trials with three replicats each gives 900 lines.
> 
> 
> How do I use several criteria to subset this?
> 
> Ast <- subset(data, Variety == "Astoria") 
> 
> works fine giving back the 36 lines where Astoria appears.
> But how do I pick two (or more) varieties? AND picking one (or two)
> years?
> 
> Everything I have tried results in rubbish, like:
> 
> AstBark <- subset(data, Variety == c("Astoria","Barke"))
> 
> which seems to work but gives back 19 and 18 lines of the varieties 
> 
> respectively. There exists 36 of each....
> 
> Thanks
> 
> /CG
> 
> 
> CG Pettersson, MSci, PhD Stud.
> Swedish University of Agricultural Sciences
> Dep. of Ecology and Crop Production. Box 7043
> SE-750 07 Uppsala
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
> 
>



From ripley at stats.ox.ac.uk  Fri Oct 29 12:40:42 2004
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Fri, 29 Oct 2004 11:40:42 +0100 (BST)
Subject: [R] Subsetting with more than one criteria
In-Reply-To: <200410291000.i9TA0Dkm029291@mail1.slu.se>
Message-ID: <Pine.LNX.4.44.0410291136590.19677-100000@gannet.stats>

On Fri, 29 Oct 2004, CG Pettersson wrote:

> I have been using R for two years, but still have trivial problems.
> For the moment, I run R1.9.1 on a W2k computer.

Time to read a book!  This sort of thing is discussed at length in Chapter
2 of MASS4 (see the FAQ), for example.  It even discusses why your example
does not work.

> I work with a dataset from a variety evaluation project. Every year 25
> varieties have been tested. The tested cultivars have been the same
> each year in all trials, but some have changed over the three year
> period. Now I want to do some calculations and plotting on subsets.
> The structure looks like this:
> 
> Year	ADB	Block	Vcode	Variety	Yield	Protein			
> 2002	SW0024	1	20226	Denise	5843	12.8....	
> 2002	SW0024	1	9865	Astoria	6729	11.4	
> 2002	SW0024	1	9622	Barke	6121	12	
> 2002	SW0024	1	9604	Cecilia	5579	12.7	
> 2002	SW0024	1	20223	Granta	5591	11.6	
> 2002	SW0024	1	20222	Class	5591	11.7	
> 2002	SW0024	1	9922	Wiking	5744	12.5
> 2002	SW0024	1	20103	Vortex	5863	10.6
> .
> .
> And so on both down and sideways.
> Three years and four trials with three replicats each gives 900 lines.
> 
> How do I use several criteria to subset this?
> 
> Ast <- subset(data, Variety == "Astoria") 
> 
> works fine giving back the 36 lines where Astoria appears.
> But how do I pick two (or more) varieties? AND picking one (or two)
> years?
> 
> Everything I have tried results in rubbish, like:
> 
> AstBark <- subset(data, Variety == c("Astoria","Barke"))

Variety %in% c("Astoria","Barke") is probably what you intended.

You can pick also on years by using & in the indexing criterion.
 
-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From ripley at stats.ox.ac.uk  Fri Oct 29 12:48:37 2004
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Fri, 29 Oct 2004 11:48:37 +0100 (BST)
Subject: [R] missing values in logistic regression
In-Reply-To: <1099042475.3777.18.camel@bioinf14>
Message-ID: <Pine.LNX.4.44.0410291140510.19677-100000@gannet.stats>

On 29 Oct 2004, Avril Coghlan wrote:

> Dear R help list,
> 
>    I am trying to do a logistic regression
> where I have a categorical response variable Y
> and two numerical predictors X1 and X2. There
> are quite a lot of missing values for predictor X2.
> eg.,
> 
> Y     X1   X2
> red   0.6  0.2    *
> red   0.5  0.2    *
> red   0.5  NA
> red   0.5  NA
> green 0.2  0.1    *
> green 0.1  NA
> green 0.1  NA
> green 0.05 0.05   *
> 
> 
> I am wondering can I combine X1 and X2 in
> a logistic regression to predict Y, using
> all the data for X1, even though there are NAs in
> the X2 data?
> 
> Or do I have to take only the cases for which
> there is data for both X1 and X2? (marked
> with *s above)

You need to either

1) Train separate models for Y | X1 and Y | X1, X2 and use the appropriate 
one.

2) Produce an imputation model for X2 | X1, and use multiple imputation.

Given that the latter look like [0, 1] scores, mix (as suggested by PD) 
is not likely to be appropriate, but e.g. a 2D kde fit may well be.


-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From Luisr at frs.fo  Fri Oct 29 12:50:43 2004
From: Luisr at frs.fo (Luis Rideau Cruz)
Date: Fri, 29 Oct 2004 11:50:43 +0100
Subject: [R] extraction of p-value
Message-ID: <s1822e99.017@ffdata.setur.fo>

R-help,

I wish to get the p-values of anova model as numeric and not as part of
the standard output (print,summary,,,)

I have tried several commands but to not vail

Thank you



From petr.pikal at precheza.cz  Fri Oct 29 13:13:07 2004
From: petr.pikal at precheza.cz (Petr Pikal)
Date: Fri, 29 Oct 2004 13:13:07 +0200
Subject: [R] Subsetting with more than one criteria
In-Reply-To: <200410291000.i9TA0Dkm029291@mail1.slu.se>
Message-ID: <418241E3.24866.137E67F@localhost>

Hi

On 29 Oct 2004 at 12:00, CG Pettersson wrote:

> Hello all!
> 
> I have been using R for two years, but still have trivial problems.
> For the moment, I run R1.9.1 on a W2k computer.
> 
> I work with a dataset from a variety evaluation project. Every year 25
> varieties have been tested. The tested cultivars have been the same
> each year in all trials, but some have changed over the three year
> period. Now I want to do some calculations and plotting on subsets.
> The structure looks like this:
> 
> Year	ADB	Block	Vcode	Variety	Yield	Protein			
> 2002	SW0024	1	20226	Denise	5843	12.8....	
> 2002	SW0024	1	9865	Astoria	6729	11.4	
> 2002	SW0024	1	9622	Barke	6121	12	
> 2002	SW0024	1	9604	Cecilia	5579	12.7	
> 2002	SW0024	1	20223	Granta	5591	11.6	
> 2002	SW0024	1	20222	Class	5591	11.7	
> 2002	SW0024	1	9922	Wiking	5744	12.5
> 2002	SW0024	1	20103	Vortex	5863	10.6
> .
> .
> And so on both down and sideways.
> Three years and four trials with three replicats each gives 900 lines.
> 
> 
> How do I use several criteria to subset this?
> 
> Ast <- subset(data, Variety == "Astoria") 
> 
> works fine giving back the 36 lines where Astoria appears.
> But how do I pick two (or more) varieties? AND picking one (or two)
> years?
> 
> Everything I have tried results in rubbish, like:
> 
> AstBark <- subset(data, Variety == c("Astoria","Barke"))

subset(data, subset = (Variety %in% c("Astoria", "Barke")))

should work. There has to be logical expression subset in subset() 
function which gives you correct answer. 

HTH
Cheers
Petr

> 
> which seems to work but gives back 19 and 18 lines of the varieties 
> 
> respectively. There exists 36 of each....
> 
> Thanks
> 
> /CG
> 
> 
> CG Pettersson, MSci, PhD Stud.
> Swedish University of Agricultural Sciences
> Dep. of Ecology and Crop Production. Box 7043
> SE-750 07 Uppsala
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide!
> http://www.R-project.org/posting-guide.html

Petr Pikal
petr.pikal at precheza.cz



From petr.pikal at precheza.cz  Fri Oct 29 13:20:14 2004
From: petr.pikal at precheza.cz (Petr Pikal)
Date: Fri, 29 Oct 2004 13:20:14 +0200
Subject: [R] extraction of p-value
In-Reply-To: <s1822e99.017@ffdata.setur.fo>
Message-ID: <4182438E.31933.13E67E6@localhost>

Hi

str(summary(fit)) will give you a whole picture how summary 
output list is structured.

summary(fit)[n] 

where n is an appropriate number will extract parts of a list. Just 
try and choose the one you need. Maybe n=4?

Cheers
Petr


On 29 Oct 2004 at 11:50, Luis Rideau Cruz wrote:

> R-help,
> 
> I wish to get the p-values of anova model as numeric and not as part
> of the standard output (print,summary,,,)
> 
> I have tried several commands but to not vail
> 
> Thank you
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide!
> http://www.R-project.org/posting-guide.html

Petr Pikal
petr.pikal at precheza.cz



From ripley at stats.ox.ac.uk  Fri Oct 29 13:21:26 2004
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Fri, 29 Oct 2004 12:21:26 +0100 (BST)
Subject: [R] extraction of p-value
In-Reply-To: <s1822e99.017@ffdata.setur.fo>
Message-ID: <Pine.LNX.4.44.0410291216540.25369-100000@gannet.stats>

On Fri, 29 Oct 2004, Luis Rideau Cruz wrote:

> I wish to get the p-values of anova model as numeric and not as part of
> the standard output (print,summary,,,)

It is part of the standard output of summary.

> I have tried several commands but to not vail

Please do as the posting guide asks and do your homework.  We have no idea 
what you tried and why it `not vail'.

If `anova model' means `aov model', see ?summary.aov.  Continuing that 
example

summary(npk.aov)[[1]]["Pr(>F)"]

something you will find discussed in the R-help archives.

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From Michael.Griffiths at lgc.co.uk  Fri Oct 29 13:35:01 2004
From: Michael.Griffiths at lgc.co.uk (Michael Griffiths)
Date: Fri, 29 Oct 2004 12:35:01 +0100
Subject: [R] Installation problems with R.classes bundle
Message-ID: <s1823915.009@tedmail2.lgc.co.uk>

Firstly hi to everyone on the list,

I am new to this list ans also R so please forgive the simplicity of my
questions over the next few months.

I have version R 1.9.1 and want to perform z-scoring for a benchmarking
procedure. I have tried to install Henrik Bengtsson's R.classes bundle
(http://www.maths.lth.se/help/R/R.classes). I have downloaded the .zip
file and placed the relevant folders in my working directory
(C:\programs\R\rw1091\library). These include R.audio, R.basic and
R.oo.

The instrauctions on the website then say to:

Verify by loading the R.oo package, e.g. library(R.oo). I have tried
this and all I keep getting os the following error message:

> library(R.oo)
Error in firstlib(which.lib.loc, package) : 
        couldn't find function "lazyLoad"
In addition: Warning message: 
package R.oo was built under R version 2.0.0 
Error in library(R.oo) : .First.lib failed
> 

Does anyone have any clues as to what I have done wrong?

Thankyou for your help.

Mike Griffiths


Michael Griffiths, Ph.D.
Chemometrician
Training, Quality and Statistics Group
LGC Limited
Queens Road
Teddington
Middlesex, TW11 0LY, UK
Tel: +44 (0)20 8943 7352
Fax: +44 (0)20 8943 2767
e-mail: michael.griffiths at lgc.co.uk
*******************************************************************
This email and any attachments are confidential. Any use, co...{{dropped}}



From veand at frisurf.no  Fri Oct 29 13:42:18 2004
From: veand at frisurf.no (Vegard Andersen)
Date: Fri, 29 Oct 2004 13:42:18 +0200
Subject: [R] How to obtain confidence intervals from a coxph object?
Message-ID: <opsgmqss0m5ukqp9@petter-smart>

Hi!

I am performing an analysis which looks like this:

cox.out <- coxph(Survobject ~ x1 + x2 + x3)
summary(cox.out)

This works fine, but I have not been able to extract any of the output  
 from "summary(cox.out)", for instance like:

summary(cox.out)$coef  or
summary(cox.out)$lower

So my question is if there is any way to extract the information given in  
summary(cox.out)?

Thanks in advance!


Best regards,
Vegard Andersen
Institute of Community Medicine
University of Tromso
Tromso, Norway


--



From kjetil at acelerate.com  Fri Oct 29 13:54:00 2004
From: kjetil at acelerate.com (Kjetil Brinchmann Halvorsen)
Date: Fri, 29 Oct 2004 07:54:00 -0400
Subject: [R] as.list.matrix
In-Reply-To: <Pine.LNX.4.44.0410290652310.14762-100000@gannet.stats>
References: <Pine.LNX.4.44.0410290652310.14762-100000@gannet.stats>
Message-ID: <41822F58.6020402@acelerate.com>

Prof Brian Ripley wrote:

>On Fri, 29 Oct 2004, Peter Alspach wrote:
>
>  
>
>>Kjetil
>>
>>Isn't a data.frame as special type of list, and thus one could use
>>as.data.frame?
>>    
>>
>
>Yes, or
>
>   split(mat, col(mat))
>
>works.  So simple it hardly needs an as.list method.
>  
>
Thanks! I thought about split(), bit did'nt know col()

>
>  
>
>>Peter Alspach
>>
>>
>>    
>>
>>>>>Kjetil Brinchmann Halvorsen <kjetil at acelerate.com> 29/10/04
>>>>>          
>>>>>
>>    
>>
>
>Daily??  You created the matrix in the first place: why not create a data
>frame instead - na.omit was written for data frames?  And surely do.call
>is overkill for a 2-column matrix, and there seems a surplus c() in there.
>
>m <- na.omit(cbind(VRS$Tmed, resid(VRS.mod1,type="response"))
>scatter.smooth(m[,1], m[, 2], xlab="", ylab="")
>
>is much easier to follow.
>
>But do you need this?  The strange thing is that loess.smooth seems to 
>have been written expecting missing values, but does not work with them.
>It is easy to fix.
>  
>
That would be nice. What made this necessary was that scatter.smooth 
did'nt accept
NA's

Kjetil

>
>  
>


-- 

Kjetil Halvorsen.

Peace is the most effective weapon of mass construction.
               --  Mahdi Elmandjra



From jgoebel at diw.de  Fri Oct 29 14:07:10 2004
From: jgoebel at diw.de (Jan Goebel)
Date: Fri, 29 Oct 2004 14:07:10 +0200
Subject: [R] Installation problems with R.classes bundle
In-Reply-To: <s1823915.009@tedmail2.lgc.co.uk>
References: <s1823915.009@tedmail2.lgc.co.uk>
Message-ID: <20041029120710.GA28245@diw138134.diw-berlin.de>

On Fri, 29 Oct 2004, Michael Griffiths wrote:

> I have version R 1.9.1 and want to perform z-scoring for a benchmarking
---snip----
> 
> > library(R.oo)
> Error in firstlib(which.lib.loc, package) : 
>         couldn't find function "lazyLoad"
> In addition: Warning message: 
> package R.oo was built under R version 2.0.0 

R 1.9.1 != R 2.0.0
you should at first update your R Istallation.

jan

> Error in library(R.oo) : .First.lib failed
> > 
> 
> Does anyone have any clues as to what I have done wrong?
> 
> Thankyou for your help.
> 
> Mike Griffiths
> 
> 
> Michael Griffiths, Ph.D.
> Chemometrician
> Training, Quality and Statistics Group
> LGC Limited
> Queens Road
> Teddington
> Middlesex, TW11 0LY, UK
> Tel: +44 (0)20 8943 7352
> Fax: +44 (0)20 8943 2767
> e-mail: michael.griffiths at lgc.co.uk
> *******************************************************************
> This email and any attachments are confidential. Any use, co...{{dropped}}
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html

-- 
+-----------------------------------------
 Jan Goebel 
 j g o e b e l @ d i w . d e

 DIW Berlin 
 German Socio-Economic Panel Study (GSOEP) 
 K??nigin-Luise-Str. 5
 D-14195 Berlin -- Germany --
 phone: 49 30 89789-377
+-----------------------------------------



From ligges at statistik.uni-dortmund.de  Fri Oct 29 14:12:44 2004
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Fri, 29 Oct 2004 14:12:44 +0200
Subject: [R] Installation problems with R.classes bundle
In-Reply-To: <s1823915.009@tedmail2.lgc.co.uk>
References: <s1823915.009@tedmail2.lgc.co.uk>
Message-ID: <418233BC.8090300@statistik.uni-dortmund.de>

Michael Griffiths wrote:

> Firstly hi to everyone on the list,
> 
> I am new to this list ans also R so please forgive the simplicity of my
> questions over the next few months.
> 
> I have version R 1.9.1 and want to perform z-scoring for a benchmarking
> procedure. I have tried to install Henrik Bengtsson's R.classes bundle
> (http://www.maths.lth.se/help/R/R.classes). I have downloaded the .zip
> file and placed the relevant folders in my working directory
> (C:\programs\R\rw1091\library). These include R.audio, R.basic and
> R.oo.
> 
> The instrauctions on the website then say to:
> 
> Verify by loading the R.oo package, e.g. library(R.oo). I have tried
> this and all I keep getting os the following error message:
> 
> 
>>library(R.oo)
> 
> Error in firstlib(which.lib.loc, package) : 
>         couldn't find function "lazyLoad"
> In addition: Warning message: 
> package R.oo was built under R version 2.0.0 
> Error in library(R.oo) : .First.lib failed
> 

The package has been compiled for R-2.0.0 and cannot be used fo R < 
2.0.0. Either compile from sources yourself or try to find an outdated 
binary.

Uwe Ligges



> Does anyone have any clues as to what I have done wrong?
> 
> Thankyou for your help.
> 
> Mike Griffiths
> 
> 
> Michael Griffiths, Ph.D.
> Chemometrician
> Training, Quality and Statistics Group
> LGC Limited
> Queens Road
> Teddington
> Middlesex, TW11 0LY, UK
> Tel: +44 (0)20 8943 7352
> Fax: +44 (0)20 8943 2767
> e-mail: michael.griffiths at lgc.co.uk
> *******************************************************************
> This email and any attachments are confidential. Any use, co...{{dropped}}
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html



From emanuela.rossi at unimib.it  Fri Oct 29 14:14:34 2004
From: emanuela.rossi at unimib.it (Emanuela Rossi)
Date: Fri, 29 Oct 2004 14:14:34 +0200
Subject: [R] meaning of frailty estimates
References: <018701c4bcea$37aeaca0$36ec8495@Emanuela>
	<Pine.A41.4.61b.0410280707430.26124@homer04.u.washington.edu>
	<01cd01c4bcfb$b4262b30$36ec8495@Emanuela>
	<Pine.A41.4.61b.0410281702550.162042@homer12.u.washington.edu>
Message-ID: <003701c4bdb0$da59d2b0$36ec8495@Emanuela>

Sorry, but I have some difficulties to properly interpret these kind of
hazard ratios without reference group.



I'm studying the risk of introduction on a particular virus in poultry
farms. My frailty variables is the specie (1, 2, 3, 4 are four types of
poultry species which, I suppose, have different risk level, because of
different management). The other variables are other covariates related to
the farms.



QUESTION:

Which is the right interpretation of the hazard ratios relative to the
species, if we can't say they are the increase in term of hazard with
respect to another category?



Thanks



Emanuela




> > Here the output. "Specie" is the frailty variable, with four types of
> > answer. I'm asking what's the meaning of gauss:1, ....gauss:4
>
> They are hazard ratios. There is no reference group, instead the log
> hazard ratios sum to zero (hazard ratios multiply to 1).
>
>   -thomas
>
>
> --------------------------------------------------------------------------
--
> > ------------------------------
> >
> >>
> >
SURV1<-read.table("C:/LAVORO/MOBILE/survival/surv_n1.csv",header=TRUE,sep=",
> > ")
> >> fit_13_2_sp<-coxph(Surv(DATA_INI1,DATA_FIN1,EVENT1)~
V1+V2+Z+G+dim+frailty.gaussian(specie)+cluster(ID),data=SURV1)
> >> summary(fit_13_2_sp)
> > Call:
> > coxph(formula = Surv(DATA_INI1, DATA_FIN1, EVENT1) ~ V1 + V2 + Z + G +
dim + frailty.gaussian(specie) + cluster(ID), data = SURV1)
> >
> >  n= 233450
> >                         coef       se(coef)   se2             Chisq  DF
> > p
> > V1                  0.04995   0.14021  0.145916    0.13  1.00  7.2e-01
> > V2                 -0.79656   0.20483  0.197135  15.12 1.00  1.0e-04
> > Z                    -0.00359  0.00067  0.000841   28.76 1.00  8.2e-08
> > G                     0.08186  0.00583  0.005796 196.91 1.00  0.0e+00
> > dim                  0.39410  0.06981  0.057294   31.87 1.00  1.6e-08
> > frailty.gaussian(specie)                                    181.44 2.95
> > 0.0e+00
> >
> >        exp(coef) exp(-coef) lower .95 upper .95
> > V1          0.951      1.051     0.723     1.252
> > V2          0.451      2.218     0.302     0.674
> > Z           0.996      1.004     0.995     0.998
> > G           1.085      0.921     1.073     1.098
> > dim         1.483      0.674     1.293     1.700
> > gauss:1     2.504      0.399     2.108     2.975
> > gauss:2     1.799      0.556     1.457     2.221
> > gauss:3     0.278      3.599     0.217     0.356
> > gauss:4     0.799      1.252     0.641     0.996
> >
> > Iterations: 5 outer, 11 Newton-Raphson
> >     Variance of random effect= 0.975
> > Degrees of freedom for terms= 1 1 1 1 1 3
> > Rsquare= 0.003   (max possible= 0.025 )
> > Likelihood ratio test= 810  on 7.95 df,   p=0
> > Wald test            = 600  on 7.95 df,   p=0,   Robust = 355  p=0
> >
>
> --------------------------------------------------------------------------
--
> > -------------------------------------------
> >
> >
> > Thanks
> >
> > Emanuela
> >
> >
> >>> Hello,
> >>>
> >>> I'm trying to estimate a Cox's survival model with a random effect, so
I
> >>> have added the instruction frailty.gaussian (name variable) in the
> >>> model.
> >>>
> >>> My frailty variable is a qualitative variable with four types of
answer.
> >>>
> >>> In the resulting output there are the parameter estimates of all the
> >>> variables, but there are also four estimates for each type of answer
of
> >>> the frailty variable. Which kind of estimates are they? Maybe hazard
> >>> ratio? But which is the reference?
> >>>
> >>
> >> Perhaps you could post this output so we know what you are talking
about.
> >>
> >>   -thomas
> >>
> >
> >
>
> Thomas Lumley Assoc. Professor, Biostatistics
> tlumley at u.washington.edu University of Washington, Seattle



From ripley at stats.ox.ac.uk  Fri Oct 29 14:28:15 2004
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Fri, 29 Oct 2004 13:28:15 +0100 (BST)
Subject: [R] Installation problems with R.classes bundle
In-Reply-To: <s1823915.009@tedmail2.lgc.co.uk>
Message-ID: <Pine.LNX.4.44.0410291320550.8858-100000@gannet.stats>

On Fri, 29 Oct 2004, Michael Griffiths wrote:

> Firstly hi to everyone on the list,
> 
> I am new to this list ans also R so please forgive the simplicity of my
> questions over the next few months.
> 
> I have version R 1.9.1 and want to perform z-scoring for a benchmarking
> procedure. I have tried to install Henrik Bengtsson's R.classes bundle
> (http://www.maths.lth.se/help/R/R.classes). I have downloaded the .zip
> file and placed the relevant folders in my working directory
> (C:\programs\R\rw1091\library). These include R.audio, R.basic and
> R.oo.
> 
> The instrauctions on the website then say to:
> 
> Verify by loading the R.oo package, e.g. library(R.oo). I have tried
> this and all I keep getting os the following error message:
> 
> > library(R.oo)
> Error in firstlib(which.lib.loc, package) : 
>         couldn't find function "lazyLoad"
> In addition: Warning message: 
> package R.oo was built under R version 2.0.0 
> Error in library(R.oo) : .First.lib failed
> > 
> 
> Does anyone have any clues as to what I have done wrong?

You appear to have downloaded the wrong copy, despite rather clear 
instructions on that site. To wit,

To install on *R v1.9.1 or earlier on Windows*, call

install.packages("R.classes", contriburl="http://www.maths.lth.se/help/R/R190")

Although it also says

Since the Windows is a binary distribution written in 100% R it can also 
installed by extracting the archive files into the library/ directory of 
your R installation, i.e. ${R_HOME}/library/ such that the directories 
library/R.oo/, library/R.graphics/ etc are created.

which is incorrect as the `100% R' is transformed in version-specific ways 
(and one does hope it has help files ...).

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From ripley at stats.ox.ac.uk  Fri Oct 29 14:29:50 2004
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Fri, 29 Oct 2004 13:29:50 +0100 (BST)
Subject: [R] How to obtain confidence intervals from a coxph object?
In-Reply-To: <opsgmqss0m5ukqp9@petter-smart>
Message-ID: <Pine.LNX.4.44.0410291328430.8858-100000@gannet.stats>

On Fri, 29 Oct 2004, Vegard Andersen wrote:

> I am performing an analysis which looks like this:
> 
> cox.out <- coxph(Survobject ~ x1 + x2 + x3)
> summary(cox.out)
> 
> This works fine, but I have not been able to extract any of the output  
>  from "summary(cox.out)", for instance like:
> 
> summary(cox.out)$coef  or
> summary(cox.out)$lower
> 
> So my question is if there is any way to extract the information given in  
> summary(cox.out)?

Yes, provided you have updated to survival_2.15 (out this week).


-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From sgrabasa at escet.urjc.es  Fri Oct 29 14:30:33 2004
From: sgrabasa at escet.urjc.es (sgrabasa@escet.urjc.es)
Date: Fri, 29 Oct 2004 14:30:33 +0200 (CEST)
Subject: [R] glmmPQL and REML
Message-ID: <1978579179sgrabasa@escet.urjc.es>


Hi, 

I am trying to use glmmPQL package for Generalized linear mixed models. 
This package works by repeated calls to lme. lme uses by default REML 
method for estimation. Then, does glmmmPQL use REML too? In contrast, 
how can I change it? 
I have tried it, writing : method="REML", but the program says: invalid 
method REML.

 If somebody can answer me....thanks, 


Sonja



From hb at maths.lth.se  Fri Oct 29 14:41:31 2004
From: hb at maths.lth.se (Henrik Bengtsson)
Date: Fri, 29 Oct 2004 14:41:31 +0200
Subject: [R] Installation problems with R.classes bundle
In-Reply-To: <s1823915.009@tedmail2.lgc.co.uk>
Message-ID: <000801c4bdb4$9e2e3d90$e502eb82@hblaptop>

Hi, I've added information on http://www.maths.lth.se/help/R/R.classes/ how
to install Windows binaries of the latest R.classes on either R v2.0.0 or R
v1.9.1. Quote:

R v2.0.0:
install.packages("R.classes", 
  contriburl="http://www.maths.lth.se/help/R")

R v1.9.1:
install.packages("R.classes",
  contriburl="http://www.maths.lth.se/help/R/R190")

Cheers

Henrik

> -----Original Message-----
> From: r-help-bounces at stat.math.ethz.ch 
> [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of 
> Michael Griffiths
> Sent: Friday, October 29, 2004 1:35 PM
> To: r-help at stat.math.ethz.ch
> Subject: [R] Installation problems with R.classes bundle
> 
> 
> Firstly hi to everyone on the list,
> 
> I am new to this list ans also R so please forgive the 
> simplicity of my questions over the next few months.
> 
> I have version R 1.9.1 and want to perform z-scoring for a 
> benchmarking procedure. I have tried to install Henrik 
> Bengtsson's R.classes bundle 
> (http://www.maths.lth.se/help/R/R.classes). I have downloaded 
> the .zip file and placed the relevant folders in my working 
> directory (C:\programs\R\rw1091\library). These include 
> R.audio, R.basic and R.oo.
> 
> The instrauctions on the website then say to:
> 
> Verify by loading the R.oo package, e.g. library(R.oo). I 
> have tried this and all I keep getting os the following error message:
> 
> > library(R.oo)
> Error in firstlib(which.lib.loc, package) : 
>         couldn't find function "lazyLoad"
> In addition: Warning message: 
> package R.oo was built under R version 2.0.0 
> Error in library(R.oo) : .First.lib failed
> > 
> 
> Does anyone have any clues as to what I have done wrong?
> 
> Thankyou for your help.
> 
> Mike Griffiths
> 
> 
> Michael Griffiths, Ph.D.
> Chemometrician
> Training, Quality and Statistics Group
> LGC Limited
> Queens Road
> Teddington
> Middlesex, TW11 0LY, UK
> Tel: +44 (0)20 8943 7352
> Fax: +44 (0)20 8943 2767
> e-mail: michael.griffiths at lgc.co.uk
> *******************************************************************
> This email and any attachments are confidential. Any use, 
> co...{{dropped}}
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list 
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
> 
>



From Ted.Harding at nessie.mcc.ac.uk  Fri Oct 29 14:45:46 2004
From: Ted.Harding at nessie.mcc.ac.uk ( (Ted Harding))
Date: Fri, 29 Oct 2004 13:45:46 +0100 (BST)
Subject: [R] missing values in logistic regression
In-Reply-To: <1099042475.3777.18.camel@bioinf14>
Message-ID: <XFMail.041029134546.Ted.Harding@nessie.mcc.ac.uk>

On 29-Oct-04 Avril Coghlan wrote:
> Dear R help list,
> 
>    I am trying to do a logistic regression
> where I have a categorical response variable Y
> and two numerical predictors X1 and X2. There
> are quite a lot of missing values for predictor X2.
> eg.,
> 
> Y     X1   X2
> red   0.6  0.2    *
> red   0.5  0.2    *
> red   0.5  NA
> red   0.5  NA
> green 0.2  0.1    *
> green 0.1  NA
> green 0.1  NA
> green 0.05 0.05   *
> 
> I am wondering can I combine X1 and X2 in
> a logistic regression to predict Y, using
> all the data for X1, even though there are NAs in
> the X2 data?
> 
> Or do I have to take only the cases for which
> there is data for both X1 and X2? (marked
> with *s above)

I don't know of any R routine directly aimed at logistic regression
with missing values as you describe.

However, if you are prepared to assume (or try to arrange by a
judiciously chosen transformation) that the distribution of (X1,X2)
is bivariate normal, with mean dependent on the value of Y but
with the same variance-covariance matrix throughout, then you
should be able to make progress along the following lines.
This ties in with Peter Dalgaard's suggestion of "mix".
I shall assume for this explanation that your Y categories take
only two values A and B (as "red" , "green"), though the method can
be directly extended to several categories in Y.

The underlying theoretical point is that a linear logistic
regression is equivalent to a Bayesian discrimination between
two normally-distributed clusters. Let the vector of means for
(X1,X2) be mA for group A, and mB for group B; and let the
covariance matrix be V. Let "x" denote (X1,X2).

Then P(A|x) = [f(x|A)*p(A)]/[f(x|A)*p(A) + f(x|B)*p(B)]

where p(A) and p(B) are the prior probabilities of a group A
or a group B item.

Now substitute

     f(x|A) = C*exp(-0.5*(x-mA)'%*%W%*%(x-mA))

and similar for f(x|B); C is the constant 1/sqrt(2*pi*det(V))^k
where k is the dimension of x, and W is the inverse of V.

Then, with a bit of algebra,

     P(A|x) = 1/(1 + exp(a + b%*%x))

(a logistic regression) where a is the scalar

     log(p(B)/p(A)) + 0.5*(mA'%*%W%*%mA - mB'%*%W%*%mB)

and b is the vector

     (mB - mA)'%*%W

Now you can come back to the "mix" package. This is for multiple
imputation of missing values in a dataset consisting of variables
of two kinds: categorical and continuous.

The joint probability model for all the variables is expressed as a
product of the multinomial distribution for the categorical variables,
with a multivariate normal distribution for the continuous variables
where it is assumed that the covariance matrix is the same for every
combination of the values of the categorical variables, while the
multivariate means may differ at different levels of the categoricals.
Hence the underlying model for the "mix" package is exactly what is
needed for the above.

The primary output from imputation runs with "mix" is a set of
completed datasets (with missing values filled in). You can then
run a logistic regression on each completed dataset, obtaining
for each dataset the estimates of the regression parameters and
their standard errors. These can then be combined using the function
"mi.inference" in the "mix" library.

You can also, however, extract the parameter values (multinomial
probabilities and multivariate means and covariance matrix) used
in a particular imputation using the function "getparam.mix" in
the "mix" library. This function needs parameters "s" (evaluated
by the preliminary processor "prelim.mix"), and "theta", evaluated
for each imputation by a "data augmentation" function such as "da.mix".
Then you can substitute these in the above formulae for a and b to get
a and b directly, without needing to do an explicit logistic regression
on the completed dataset.

Hoping this helps!
Ted.


--------------------------------------------------------------------
E-Mail: (Ted Harding) <Ted.Harding at nessie.mcc.ac.uk>
Fax-to-email: +44 (0)870 094 0861  [NB: New number!]
Date: 29-Oct-04                                       Time: 13:45:46
------------------------------ XFMail ------------------------------



From christine_mies at yahoo.de  Fri Oct 29 15:15:33 2004
From: christine_mies at yahoo.de (christine mies)
Date: Fri, 29 Oct 2004 15:15:33 +0200 (CEST)
Subject: [R] Datainput in R
Message-ID: <20041029131533.18575.qmail@web26606.mail.ukl.yahoo.com>

Sehr geehrte Damen und Herren,
 
im Rahmen einer Seminararbeit an der Universit??t
Augsburg soll ich Daten aus einer Excel-Datei in R
einlesen und damit eine Faktorenanalyse, MDS und
Procrustesanalyse durchf??hren. Nach einigen
fehlgeschlagenen Versuchen richten wir nun folgende
Fragen an Sie:

Wie kann man Daten aus einer Exceldatei in R einlesen?

Wo und wie kann man die aus Excel in R ??bertragenen
"Rohdaten" abspeichern, um Sie bei der n??chsten
Worksession wieder zur Verf??gung zu haben, ohne diese
erneut einlesen zu m??ssen? 
Vielen Dank f??r Ihre Hilfe.

Mit freundlichen Gr??ssen
 
Christine Mies



=====
Christine Mies
Gebr??der-M??nch-Stra??e 6

86153 Augsburg

Tel.: 0821-508 39 59
Mobil:0176-22341605
Homezone (Uni):0821-8990102



From ripley at stats.ox.ac.uk  Fri Oct 29 15:22:23 2004
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Fri, 29 Oct 2004 14:22:23 +0100 (BST)
Subject: [R] glmmPQL and REML
In-Reply-To: <1978579179sgrabasa@escet.urjc.es>
Message-ID: <Pine.LNX.4.44.0410291418340.9275-100000@gannet.stats>

On Fri, 29 Oct 2004 sgrabasa at escet.urjc.es wrote:

> I am trying to use glmmPQL package for Generalized linear mixed models. 
> This package works by repeated calls to lme. lme uses by default REML 
> method for estimation. Then, does glmmmPQL use REML too? In contrast, 
> how can I change it? 
> I have tried it, writing : method="REML", but the program says: invalid 
> method REML.
> 
>  If somebody can answer me....thanks, 

The docs do: it uses PQL as the name suggests, not REML and not ML.
If you don't know what that is, please read the references on the help 
page.  (The posting guide does ask you to read one of them before posting 
here.)

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From hb at maths.lth.se  Fri Oct 29 15:28:17 2004
From: hb at maths.lth.se (Henrik Bengtsson)
Date: Fri, 29 Oct 2004 15:28:17 +0200
Subject: [R] Installation problems with R.classes bundle
In-Reply-To: <Pine.LNX.4.44.0410291320550.8858-100000@gannet.stats>
Message-ID: <001001c4bdbb$2798dd00$e502eb82@hblaptop>

> -----Original Message-----
> From: r-help-bounces at stat.math.ethz.ch 
> [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Prof 
> Brian Ripley
> Sent: Friday, October 29, 2004 2:28 PM
> To: Michael Griffiths
> Cc: r-help at stat.math.ethz.ch
> Subject: Re: [R] Installation problems with R.classes bundle
> 
> 
> On Fri, 29 Oct 2004, Michael Griffiths wrote:
> 
> > Firstly hi to everyone on the list,
> > 
> > I am new to this list ans also R so please forgive the 
> simplicity of 
> > my questions over the next few months.
> > 
> > I have version R 1.9.1 and want to perform z-scoring for a 
> > benchmarking procedure. I have tried to install Henrik Bengtsson's 
> > R.classes bundle (http://www.maths.lth.se/help/R/R.classes). I have 
> > downloaded the .zip file and placed the relevant folders in 
> my working 
> > directory (C:\programs\R\rw1091\library). These include R.audio, 
> > R.basic and R.oo.
> > 
> > The instrauctions on the website then say to:
> > 
> > Verify by loading the R.oo package, e.g. library(R.oo). I 
> have tried 
> > this and all I keep getting os the following error message:
> > 
> > > library(R.oo)
> > Error in firstlib(which.lib.loc, package) : 
> >         couldn't find function "lazyLoad"
> > In addition: Warning message:
> > package R.oo was built under R version 2.0.0 
> > Error in library(R.oo) : .First.lib failed
> > > 
> > 
> > Does anyone have any clues as to what I have done wrong?
> 
> You appear to have downloaded the wrong copy, despite rather clear 
> instructions on that site. To wit,
> 
> To install on *R v1.9.1 or earlier on Windows*, call
> 
> install.packages("R.classes", 
> contriburl="http://www.maths.lth.se/help/R/R190")
> 
> Although it also says
> 
> Since the Windows is a binary distribution written in 100% R 
> it can also 
> installed by extracting the archive files into the library/ 
> directory of 
> your R installation, i.e. ${R_HOME}/library/ such that the 
> directories 
> library/R.oo/, library/R.graphics/ etc are created.
> 
> which is incorrect as the `100% R' is transformed in 
> version-specific ways 
> (and one does hope it has help files ...).

You are right. I have removed that part from the page.

Thanks for pointing it out.

/Henrik

> -- 
> Brian D. Ripley,                  ripley at stats.ox.ac.uk
> Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
> University of Oxford,             Tel:  +44 1865 272861 (self)
> 1 South Parks Road,                     +44 1865 272866 (PA)
> Oxford OX1 3TG, UK                Fax:  +44 1865 272595
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list 
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html



From ligges at statistik.uni-dortmund.de  Fri Oct 29 15:44:08 2004
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Fri, 29 Oct 2004 15:44:08 +0200
Subject: [R] Datainput in R
In-Reply-To: <20041029131533.18575.qmail@web26606.mail.ukl.yahoo.com>
References: <20041029131533.18575.qmail@web26606.mail.ukl.yahoo.com>
Message-ID: <41824928.6080108@statistik.uni-dortmund.de>

christine mies wrote:

> Sehr geehrte Damen und Herren,
>  
> im Rahmen einer Seminararbeit an der Universit??t
> Augsburg soll ich Daten aus einer Excel-Datei in R
> einlesen und damit eine Faktorenanalyse, MDS und
> Procrustesanalyse durchf??hren. Nach einigen
> fehlgeschlagenen Versuchen richten wir nun folgende
> Fragen an Sie:

Please read the manuals (in particular "R Data Import/Export"!)
Please write in english to this list and read the posting guide.


> Wie kann man Daten aus einer Exceldatei in R einlesen?
> 
> Wo und wie kann man die aus Excel in R ??bertragenen
> "Rohdaten" abspeichern, um Sie bei der n??chsten
> Worksession wieder zur Verf??gung zu haben, ohne diese
> erneut einlesen zu m??ssen? 

In the Workspace.

Also: Please ask your supervisors (which is the "Antony Unwin group", I 
suspect... - they do know answers very well.)

Uwe Ligges



> Vielen Dank f??r Ihre Hilfe.
>
> Mit freundlichen Gr??ssen
>  
> Christine Mies
> 
> 
> 
> =====
> Christine Mies
> Gebr??der-M??nch-Stra??e 6
> 
> 86153 Augsburg
> 
> Tel.: 0821-508 39 59
> Mobil:0176-22341605
> Homezone (Uni):0821-8990102
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html



From ru68y7s at myrealbox.com  Fri Oct 29 16:21:55 2004
From: ru68y7s at myrealbox.com (s viswanath)
Date: Fri, 29 Oct 2004 07:21:55 -0700
Subject: [R] lag variable addition to data frame question
Message-ID: <1099059715.6b5ff75cru68y7s@myrealbox.com>

Hi,

I was wondering if there is a more efficient way of handling the following method of creating a lagged value in a data frame without using the recursive 
'for(i in 1:n)' loop and without using as.ts

#Steps to creating a lag variable in a data frame 'my.dat.fr'
# with 275 columns, 2400 rows of numbers and factors . The #variable x is a factor of #with five different levels
the way i am creating the variable now is:

attach(my.dat.fr)
#my.dat.fr contains a variable 'x', i want to create #lagged variables #of this without using as.ts(). Is #there a more effient way of doing this than#below and #without using a recursive loop  such as 
#for( i in 1:obs)x.lag[i]= x[(i-1)]

1.here is the way i am doing the lag now
x=c(3,2,3,2,1,1,1,2,1,2,1,3...1)

obs=length(x)

x.nolag=x[2:obs]
x.lag1=x[1:(obs-1)]

my.new=cbind(x.nolag,x.lag1)

#since my data frame must line up my orginal x values to other columns I also # add the following


x.fill= cbind(0,0)
# as named the above cell lines up my factor to other factors in my data frame, #since I had chopped off the first x observation to create x above (ie x[2:obs]) #then finally

my.dat.fr=rbind(x.fill, my.new)
my.dat.fr

#Is there a easier way to create a lag variable and install in my data frame?
#thankyou in advance, Sri



From lmiceli at telos.org.br  Fri Oct 29 16:21:35 2004
From: lmiceli at telos.org.br (Leonardo L Miceli)
Date: Fri, 29 Oct 2004 11:21:35 -0300
Subject: [R] R list
Message-ID: <OF1E96B2A8.01788D12-ON03256F3C.004E6A2D-83256F3C.004F006F@telos.org.br>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20041029/13dec36d/attachment.pl

From reid_huntsinger at merck.com  Fri Oct 29 16:26:39 2004
From: reid_huntsinger at merck.com (Huntsinger, Reid)
Date: Fri, 29 Oct 2004 10:26:39 -0400
Subject: [R] transitivity
Message-ID: <D9A95B4B7B20354992E165EEADA31999056A9182@uswpmx00.merck.com>

For transitivity, if P is your incidence matrix of 0s and 1s, then P%*%P has
i,j
entry equal to the number of k with Pik = 1 and Pkj = 1. So you want to
check that when P%*%P has a nonzero (i,j) entry, then Pij = 1 too. 

Acyclicity: more generally the nth matrix power of P counts the number of
paths from i to j of length n in its ij entry. I guess you could check that
the powers all have zeros on the diagonal. (You would want to eliminate any
1s on the diagonal of P (loops in the graph) first, of course.) It's
tempting to compute the infinite sum I + P + P%*%P/2! + P%*%P%*%P/3! + ...
ie the matrix exponential. The powers of P will have zero diagonals if and
only if this sum has 1s on the diagonal. If you can diagonalize P the sum is
easy to compute because the powers diagonalize via the same similarity and
you have the usual exponential for the diagonal entries. But I see some
potential difficulty with numerical accuracy; it might be a good idea to
have a look at a book on graph theory covering analytic methods. 

Reid Huntsinger



Reid Huntsinger

-----Original Message-----
From: r-help-bounces at stat.math.ethz.ch
[mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of alex pegucci
Sent: Thursday, October 28, 2004 5:27 PM
To: r-help at stat.math.ethz.ch
Subject: [R] transitivity


Dear all,

Is there a function in R that checks transitivity and acyclicity of a
given nXn matrix with entries representing a decision-maker's
comparisons of n objects? Like 

0 1 0 1 1 1
0 0 0 1 0 0
1 0 0 0 1 1
0 0 1 0 0 0
0 1 0 1 0 1
0 1 0 1 0 0

1 represents xPy and 0 represents ~xPy. Is there a vectorized solution
to this? n can be quite large. 

Thanks in advance,

Alex

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide!
http://www.R-project.org/posting-guide.html



From Robert at sanctumfi.com  Fri Oct 29 16:44:06 2004
From: Robert at sanctumfi.com (Robert Sams)
Date: Fri, 29 Oct 2004 15:44:06 +0100
Subject: [R] R list
Message-ID: <E585EABA11227445B918BFB74C1A4D36039CA1@sanctum01.sanctumfi.com>

names() is what you want, if i understand your question correctly.

cheers,
robert
-----Original Message-----
From: Leonardo L Miceli [mailto:lmiceli at telos.org.br]
Sent: Friday, October 29, 2004 3:22 PM
To: R-help at stat.math.ethz.ch
Subject: [R] R list


Hi 

Is there any function to get the name of the components of a given list 
object?


ok.


	[[alternative HTML version deleted]]

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html



From andy_liaw at merck.com  Fri Oct 29 16:40:23 2004
From: andy_liaw at merck.com (Liaw, Andy)
Date: Fri, 29 Oct 2004 10:40:23 -0400
Subject: [R] R list
Message-ID: <3A822319EB35174CA3714066D590DCD50994E24D@usrymx25.merck.com>

Yes, names().

Andy

> From: Leonardo L Miceli
> 
> Hi 
> 
> Is there any function to get the name of the components of a 
> given list 
> object?
> 
> 
> ok.
> 
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
> 
>



From GPetris at uark.edu  Fri Oct 29 16:39:42 2004
From: GPetris at uark.edu (Giovanni Petris)
Date: Fri, 29 Oct 2004 09:39:42 -0500 (CDT)
Subject: [R] R list
In-Reply-To: <OF1E96B2A8.01788D12-ON03256F3C.004E6A2D-83256F3C.004F006F@telos.org.br>
	(message from Leonardo L Miceli on Fri, 29 Oct 2004 11:21:35 -0300)
References: <OF1E96B2A8.01788D12-ON03256F3C.004E6A2D-83256F3C.004F006F@telos.org.br>
Message-ID: <200410291439.i9TEdg8v018299@definetti.uark.edu>


What about names() ?

Giovanni

> Date: Fri, 29 Oct 2004 11:21:35 -0300
> From: Leonardo L Miceli <lmiceli at telos.org.br>
> Sender: r-help-bounces at stat.math.ethz.ch
> Cc: 
> Precedence: list
> 
> Hi 
> 
> Is there any function to get the name of the components of a given list 
> object?
> 
> 
> ok.
> 
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
> 
> 


-- 

 __________________________________________________
[                                                  ]
[ Giovanni Petris                 GPetris at uark.edu ]
[ Department of Mathematical Sciences              ]
[ University of Arkansas - Fayetteville, AR 72701  ]
[ Ph: (479) 575-6324, 575-8630 (fax)               ]
[ http://definetti.uark.edu/~gpetris/              ]
[__________________________________________________]



From apollo_w at yahoo.com  Fri Oct 29 16:48:25 2004
From: apollo_w at yahoo.com (apollo wong)
Date: Fri, 29 Oct 2004 07:48:25 -0700 (PDT)
Subject: [R] question about R on Linux Cluster
Message-ID: <20041029144825.82780.qmail@web51406.mail.yahoo.com>

Hi, I am in trying to get a new Linux Cluster. I am
thinking about putting R on it. Does anyone have any
experience with running R on a distributed Linux
Cluster? Or does R support a cluster environment at
all. Any pitfall I should really watch out for?
Thanks in advance
Apollo



From james.holtman at convergys.com  Fri Oct 29 17:00:16 2004
From: james.holtman at convergys.com (james.holtman@convergys.com)
Date: Fri, 29 Oct 2004 11:00:16 -0400
Subject: [R] Error in PDF output in R 2.0.0
Message-ID: <OF2268F018.646A883B-ON85256F3C.0051A578@nd.convergys.com>





The following script works fine in R 1.9.1.  It was creating a PDF file
with the graphs in it.  In R 2.0.0, I got the error message below.  I tried
the same script just outputting to Windows and postscript and the output
was OK.  The error message only showed up when trying to create a PDF file.


> version
         _
platform i386-pc-mingw32
arch     i386
os       mingw32
system   i386, mingw32
status
major    2
minor    0.0
year     2004
month    10
day      04
language R

## output to windows -- OK
> print(xyplot(I(usr/sys) ~ time|factor(cpu), memIn,
+     panel=function(x,y)panel.xyplot(x,y,type='l')))
> print(xyplot(csw ~ time|factor(cpu), memIn,
+     panel=function(x,y)panel.xyplot(x,y,type='l')))

## ouput to postscript file -- OK
> postscript('out.ps')
> print(xyplot(I(usr/sys) ~ time|factor(cpu), memIn,
+     panel=function(x,y)panel.xyplot(x,y,type='l')))
> print(xyplot(csw ~ time|factor(cpu), memIn,
+     panel=function(x,y)panel.xyplot(x,y,type='l')))
> dev.off()
windows
      2

## output to PDF file  --  ERRORS
> pdf('out.pdf')
> print(xyplot(I(usr/sys) ~ time|factor(cpu), memIn,
+     panel=function(x,y)panel.xyplot(x,y,type='l')))
Error in "[<-"(`*tmp*`, pos.heights[[nm]], value = numeric(0)) :
        nothing to replace with
> print(xyplot(csw ~ time|factor(cpu), memIn,
+     panel=function(x,y)panel.xyplot(x,y,type='l')))
Error in "[<-"(`*tmp*`, pos.heights[[nm]], value = numeric(0)) :
        nothing to replace with
> dev.off()
windows
      2

## traceback on error
> traceback()
3: calculateGridLayout(x, rows.per.page, cols.per.page, number.of.cond,
       panel.height, panel.width, main, sub, xlab, ylab, x.alternating,
       y.alternating, x.relation.same, y.relation.same, xaxis.rot,
       yaxis.rot, xaxis.cex, yaxis.cex, par.strip.text, legend)
2: print.trellis(xyplot(csw ~ time | factor(cpu), memIn, panel =
function(x,
       y) panel.xyplot(x, y, type = "l")))
1: print(xyplot(csw ~ time | factor(cpu), memIn, panel = function(x,
       y) panel.xyplot(x, y, type = "l")))
>
__________________________________________________________
James Holtman        "What is the problem you are trying to solve?"
Executive Technical Consultant  --  Office of Technology, Convergys
james.holtman at convergys.com
+1 (513) 723-2929
--
"NOTICE:  The information contained in this electronic mail ...{{dropped}}



From maustin at amgen.com  Fri Oct 29 17:14:29 2004
From: maustin at amgen.com (Austin, Matt)
Date: Fri, 29 Oct 2004 08:14:29 -0700
Subject: [R] lag variable addition to data frame question
Message-ID: <E7D5AB4811D20B489622AABA9C53859104E0DA54@teal-exch.amgen.com>

?diff

--Matt

-----Original Message-----
From: r-help-bounces at stat.math.ethz.ch
[mailto:r-help-bounces at stat.math.ethz.ch]On Behalf Of s viswanath
Sent: Friday, October 29, 2004 7:22 AM
To: r-help at stat.math.ethz.ch
Subject: [R] lag variable addition to data frame question


Hi,

I was wondering if there is a more efficient way of handling the following
method of creating a lagged value in a data frame without using the
recursive 
'for(i in 1:n)' loop and without using as.ts

#Steps to creating a lag variable in a data frame 'my.dat.fr'
# with 275 columns, 2400 rows of numbers and factors . The #variable x is a
factor of #with five different levels
the way i am creating the variable now is:

attach(my.dat.fr)
#my.dat.fr contains a variable 'x', i want to create #lagged variables #of
this without using as.ts(). Is #there a more effient way of doing this
than#below and #without using a recursive loop  such as 
#for( i in 1:obs)x.lag[i]= x[(i-1)]

1.here is the way i am doing the lag now
x=c(3,2,3,2,1,1,1,2,1,2,1,3...1)

obs=length(x)

x.nolag=x[2:obs]
x.lag1=x[1:(obs-1)]

my.new=cbind(x.nolag,x.lag1)

#since my data frame must line up my orginal x values to other columns I
also # add the following


x.fill= cbind(0,0)
# as named the above cell lines up my factor to other factors in my data
frame, #since I had chopped off the first x observation to create x above
(ie x[2:obs]) #then finally

my.dat.fr=rbind(x.fill, my.new)
my.dat.fr

#Is there a easier way to create a lag variable and install in my data
frame?
#thankyou in advance, Sri

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide!
http://www.R-project.org/posting-guide.html



From rpeng at jhsph.edu  Fri Oct 29 17:16:59 2004
From: rpeng at jhsph.edu (Roger D. Peng)
Date: Fri, 29 Oct 2004 11:16:59 -0400
Subject: [R] question about R on Linux Cluster
In-Reply-To: <20041029144825.82780.qmail@web51406.mail.yahoo.com>
References: <20041029144825.82780.qmail@web51406.mail.yahoo.com>
Message-ID: <41825EEB.4080607@jhsph.edu>

As far as I know, there are R interfaces to MPI and PVM.  There are 
also the `snow' and `snowFT' packages on CRAN.

-roger

apollo wong wrote:
> Hi, I am in trying to get a new Linux Cluster. I am
> thinking about putting R on it. Does anyone have any
> experience with running R on a distributed Linux
> Cluster? Or does R support a cluster environment at
> all. Any pitfall I should really watch out for?
> Thanks in advance
> Apollo
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
> 

-- 
Roger D. Peng
http://www.biostat.jhsph.edu/~rpeng/



From Bernhard.Pfaff at drkw.com  Fri Oct 29 17:42:55 2004
From: Bernhard.Pfaff at drkw.com (Pfaff, Bernhard)
Date: Fri, 29 Oct 2004 17:42:55 +0200
Subject: [R] lag variable addition to data frame question
Message-ID: <29E0BC0C716A584582941615CF9FFB0902585CB4@ibfftce107.de.ad.drkw.net>

Hello Sri,

how about embed() in stats?
Citing from the doc's detail section:

"Each row of the resulting matrix consists of sequences 'x[t]',
     'x[t-1]', ..., 'x[t-dimension+1]', where 't' is the original index
     of 'x'. If 'x' is a matrix, i.e., 'x' contains more than one
     variable, then 'x[t]' consists of the 't'th observation on each
     variable.", 

and have a look at the example's output:

embed> x <- 1:10
embed> embed(x, 3)
     [,1] [,2] [,3]
[1,]    3    2    1
[2,]    4    3    2
[3,]    5    4    3
[4,]    6    5    4
[5,]    7    6    5
[6,]    8    7    6
[7,]    9    8    7
[8,]   10    9    8

does this fit your definition of 'lag'? Please note, that the 'starting
values' are skipped, i.e. no NAs are inserted. 

HTH,
Bernhard

ps: Have a look at the function body, too.
> 
> ?diff
> 
> --Matt
> 
> -----Original Message-----
> From: r-help-bounces at stat.math.ethz.ch
> [mailto:r-help-bounces at stat.math.ethz.ch]On Behalf Of s viswanath
> Sent: Friday, October 29, 2004 7:22 AM
> To: r-help at stat.math.ethz.ch
> Subject: [R] lag variable addition to data frame question
> 
> 
> Hi,
> 
> I was wondering if there is a more efficient way of handling 
> the following
> method of creating a lagged value in a data frame without using the
> recursive 
> 'for(i in 1:n)' loop and without using as.ts
> 
> #Steps to creating a lag variable in a data frame 'my.dat.fr'
> # with 275 columns, 2400 rows of numbers and factors . The 
> #variable x is a
> factor of #with five different levels
> the way i am creating the variable now is:
> 
> attach(my.dat.fr)
> #my.dat.fr contains a variable 'x', i want to create #lagged 
> variables #of
> this without using as.ts(). Is #there a more effient way of doing this
> than#below and #without using a recursive loop  such as 
> #for( i in 1:obs)x.lag[i]= x[(i-1)]
> 
> 1.here is the way i am doing the lag now
> x=c(3,2,3,2,1,1,1,2,1,2,1,3...1)
> 
> obs=length(x)
> 
> x.nolag=x[2:obs]
> x.lag1=x[1:(obs-1)]
> 
> my.new=cbind(x.nolag,x.lag1)
> 
> #since my data frame must line up my orginal x values to 
> other columns I
> also # add the following
> 
> 
> x.fill= cbind(0,0)
> # as named the above cell lines up my factor to other factors 
> in my data
> frame, #since I had chopped off the first x observation to 
> create x above
> (ie x[2:obs]) #then finally
> 
> my.dat.fr=rbind(x.fill, my.new)
> my.dat.fr
> 
> #Is there a easier way to create a lag variable and install in my data
> frame?
> #thankyou in advance, Sri
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide!
> http://www.R-project.org/posting-guide.html
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
> 


--------------------------------------------------------------------------------
The information contained herein is confidential and is inte...{{dropped}}



From sfalcon at fhcrc.org  Fri Oct 29 18:03:23 2004
From: sfalcon at fhcrc.org (Seth Falcon)
Date: Fri, 29 Oct 2004 09:03:23 -0700
Subject: [R] question about R on Linux Cluster
In-Reply-To: <20041029144825.82780.qmail@web51406.mail.yahoo.com>
References: <20041029144825.82780.qmail@web51406.mail.yahoo.com>
Message-ID: <20041029160323.GA9546@queenbee.fhcrc.org>

I've used an openMosix cluster to load balance simultaneously running
R scripts.  For the most part this has worked as expected, but
sometimes the R processes don't migrate.

Recently, I converted one of my analyses to utilize the SNOW package
(using PVM on top of openMosix).  I've been quite pleased with the
results and plan to use SNOW for other projects in the future.

+ seth



From deepayan at stat.wisc.edu  Fri Oct 29 18:25:24 2004
From: deepayan at stat.wisc.edu (Deepayan Sarkar)
Date: Fri, 29 Oct 2004 11:25:24 -0500 (CDT)
Subject: [R] Error in PDF output in R 2.0.0
In-Reply-To: <OF2268F018.646A883B-ON85256F3C.0051A578@nd.convergys.com>
References: <OF2268F018.646A883B-ON85256F3C.0051A578@nd.convergys.com>
Message-ID: <Pine.LNX.4.58.0410291120340.26463@public02.stat.wisc.edu>

On Fri, 29 Oct 2004 james.holtman at convergys.com wrote:

>
> The following script works fine in R 1.9.1.  It was creating a PDF file
> with the graphs in it.  In R 2.0.0, I got the error message below.  I tried
> the same script just outputting to Windows and postscript and the output
> was OK.  The error message only showed up when trying to create a PDF file.

[...]

> ## output to PDF file  --  ERRORS
> > pdf('out.pdf')
> > print(xyplot(I(usr/sys) ~ time|factor(cpu), memIn,
> +     panel=function(x,y)panel.xyplot(x,y,type='l')))
> Error in "[<-"(`*tmp*`, pos.heights[[nm]], value = numeric(0)) :
>         nothing to replace with
> > print(xyplot(csw ~ time|factor(cpu), memIn,
> +     panel=function(x,y)panel.xyplot(x,y,type='l')))
> Error in "[<-"(`*tmp*`, pos.heights[[nm]], value = numeric(0)) :
>         nothing to replace with
> > dev.off()
> windows
>       2

This is not reproducible, and I didn't see an error with similar code
(with different data) in a fresh session. Are you sure this is all you
did? Didn't you have a call to trellis.par.set / lset somewhere in
between?

There is a bug involved, and it has already been reported, and a
workaround posted. I'll upload a new version of lattice which fixes this
in a couple of days.

Deepayan



From gerifalte28 at hotmail.com  Fri Oct 29 19:03:55 2004
From: gerifalte28 at hotmail.com (F Z)
Date: Fri, 29 Oct 2004 17:03:55 +0000
Subject: [R] ifelse() question
Message-ID: <BAY2-F28oHZgzDgtZKp0000a7c6@hotmail.com>

Thanks for you reply Peter.  I tried using as.character and then converting 
to factors but it did not work since it generates missing values for all the 
dat[,4]=="POR".  See:

>dat[1:5,4]
[1] BOV POR BOV POR BOV
Levels: BOV CAP CER OVI POR

test<-dat

>test[,4]<-as.character(test[,4])
>test[,5]<-as.character(test[,5])
>test[test[,4]=="POR",4]<-test[test[,4]=="POR",5]
Error in "[<-.data.frame"(`*tmp*`, test[, 4] == "POR", 4, value = c(NA,  :
        missing values are not allowed in subscripted assignments of data 
frames
>test[,4]<-as.factor(test[,4])
>test[,5]<-as.factor(test[,5])

>test[1:5,4]
[1] BOV  <NA> BOV  <NA> BOV
Levels: BOV CAP CER OVI


Any suggestions?

Thanks again!

Francisco


>From: "Peter Alspach" <PAlspach at hortresearch.co.nz>
>To: <gerifalte28 at hotmail.com>,<R-help at stat.math.ethz.ch>
>Subject: Re: [R] ifelse() question
>Date: Fri, 29 Oct 2004 13:33:54 +1300
>
>
>Francisco
>
>Did you try changing the factors to character, with as.character?
>
>Also you don't really need ifelse() for this.  Something like the
>following (untested) should do it:
>
>dat[,4] <- as.character(dat[,4])
>dat[,5] <- as.character(dat[,5])
>dat[dat[,4]=='POR',4] <- dat[dat[,4]=='POR',5]
>dat[,4] <- as.factor(dat[,4])
>dat[,5] <- as.factor(dat[,5])
>
>
>Peter Alspach
>
> >>> "F Z" <gerifalte28 at hotmail.com> 29/10/04 12:48:54 >>>
>Hi
>
>I have a data.frame with dim = 18638 (rows)     6 (cols)
>
>names(dat)
>[1] "id"      "long"    "lat"     "species" "type"    "size"
>
>Variable "species" and "type" are factors.  Species has 5 levels "BOV"
>"CAP"
>"CER" "OVI" "POR"
>Variable "type" has 11 levels "BRD" "CL" ... "OTHER"
>
>I would like to replace the values on species by the values on types
>only if
>species is == "POR"
>I tried:
>
>x<-ifelse(dat$species %in% "POR",dat$type,dat$species)
>dat[,4]<-x
>but levels(x)
>[1] "1"  "2"  "3"  "4"  "5"  "6"  "8"  "9"  "10" "11" "12"
>
>So x changes the factor names by numbers.  I can not use factor() to
>recover
>the names since the resulting factors in x are a mixture of factors
>from
>species and type.
>
>I also tried
>
>x<-gsub(pattern = "POR",replacement= factor(dat$type),dat$species)
>with
>same behavior.
>
>Apparently I did not have my granola bar today so I can't find a
>solution!
>Any help is greatly appreciated
>
>Thanks!
>
>Francisco
>
>______________________________________________
>R-help at stat.math.ethz.ch mailing list
>https://stat.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide!
>http://www.R-project.org/posting-guide.html
>
>______________________________________________________
>
>The contents of this e-mail are privileged and/or confidential to the
>named recipient and are not to be used by any other person and/or
>organisation. If you have received this e-mail in error, please notify
>the sender and delete all material pertaining to this e-mail.
>______________________________________________________



From f.gherardini at pigrecodata.net  Fri Oct 29 21:43:29 2004
From: f.gherardini at pigrecodata.net (Federico Gherardini)
Date: Fri, 29 Oct 2004 21:43:29 +0200
Subject: [R] Poisson regression
Message-ID: <41829D61.9010306@pigrecodata.net>

Dear all,

First of all sorry if this is a dumb question...
I'm trying to fit a linear model between the logarithm of two numerical 
variables (log(y) ~ log(x)). A log-log plot shows that the variance of 
log(y) is decreasing with the mean of log(x), in other words the points 
are quite dispersed for low values of log(x) and approach a straight 
line as log(x) increases. I have tried the following glm model
fit <- glm(y ~ log(x), data = tab, family = poisson)
The residuals seem very good but I have a doubt: I have used y and not 
log(y) in  the model formula because, as far as I understand, the 
poisson regression assumes a logarithmic transformation of the response. 
Is this correct? I mean is it correct to watch a y ~ log(x) poisson 
regression line on a log(y) ~ log(x) xyplot or I am confusing the two 
things?

Thank you very much

Federico



From jmoreira at fe.up.pt  Fri Oct 29 19:26:49 2004
From: jmoreira at fe.up.pt (jmoreira@fe.up.pt)
Date: Fri, 29 Oct 2004 18:26:49 +0100
Subject: [R] Giving column names to a matrix
Message-ID: <1099070809.41827d5998f09@webmail.fe.up.pt>


Heloo,

I have the following problem:

orig.data	<- NULL
Inside a loop I have instructions like:
orig.data <- rbind(orig.data, ...)
After that I do:
colnames(orig.data)<-c('Data','InicioViagem', ...)
Everything works fine.
For example, the first line of the matrix is:
> orig.data[1,]
        Data InicioViagem      ...
           1        40466      ...
The problem is: I can't refer the columns by the column names.
For example:
> orig.data[1,InicioViagem]
Error: Object "InicioViagem" not found
or
> orig.data[1,]$InicioViagem
NULL
but:
> orig.data[1,2]
[1] 40466
works!

Can someone help me?

Thanks

Joao Moreira



From ramasamy at cancer.org.uk  Fri Oct 29 19:44:06 2004
From: ramasamy at cancer.org.uk (Adaikalavan Ramasamy)
Date: Fri, 29 Oct 2004 18:44:06 +0100
Subject: [R] ifelse() question
In-Reply-To: <BAY2-F28oHZgzDgtZKp0000a7c6@hotmail.com>
References: <BAY2-F28oHZgzDgtZKp0000a7c6@hotmail.com>
Message-ID: <1099071846.3582.21.camel@ndmpc126.ihs.ox.ac.uk>

Francisco, a more reproducible example would have helped but see if the
following helps in your understanding 

# Create dataset 

df <- data.frame( type=1:5, species=LETTERS[1:5] )
df
   type species
 1    1       A
 2    2       B
 3    3       C
 4    4       D 
 5    5       E

df[ ,2] == "E"
 [1] FALSE FALSE FALSE FALSE  TRUE

# Note that you need to coerce as.character inside ifelse
attach(df)
(df[ ,2] <- ifelse( species == "E", type, as.character(species) ))
 [1] "A" "B" "C" "D" "5"
detach(df)

df
   type species
 1    1       A
 2    2       B 
 3    3       C
 4    4       D
 5    5       5


On Fri, 2004-10-29 at 18:03, F Z wrote:
> Thanks for you reply Peter.  I tried using as.character and then converting 
> to factors but it did not work since it generates missing values for all the 
> dat[,4]=="POR".  See:
> 
> >dat[1:5,4]
> [1] BOV POR BOV POR BOV
> Levels: BOV CAP CER OVI POR
> 
> test<-dat
> 
> >test[,4]<-as.character(test[,4])
> >test[,5]<-as.character(test[,5])
> >test[test[,4]=="POR",4]<-test[test[,4]=="POR",5]
> Error in "[<-.data.frame"(`*tmp*`, test[, 4] == "POR", 4, value = c(NA,  :
>         missing values are not allowed in subscripted assignments of data 
> frames
> >test[,4]<-as.factor(test[,4])
> >test[,5]<-as.factor(test[,5])
> 
> >test[1:5,4]
> [1] BOV  <NA> BOV  <NA> BOV
> Levels: BOV CAP CER OVI
> 
> 
> Any suggestions?
> 
> Thanks again!
> 
> Francisco
> 
> 
> >From: "Peter Alspach" <PAlspach at hortresearch.co.nz>
> >To: <gerifalte28 at hotmail.com>,<R-help at stat.math.ethz.ch>
> >Subject: Re: [R] ifelse() question
> >Date: Fri, 29 Oct 2004 13:33:54 +1300
> >
> >
> >Francisco
> >
> >Did you try changing the factors to character, with as.character?
> >
> >Also you don't really need ifelse() for this.  Something like the
> >following (untested) should do it:
> >
> >dat[,4] <- as.character(dat[,4])
> >dat[,5] <- as.character(dat[,5])
> >dat[dat[,4]=='POR',4] <- dat[dat[,4]=='POR',5]
> >dat[,4] <- as.factor(dat[,4])
> >dat[,5] <- as.factor(dat[,5])
> >
> >
> >Peter Alspach
> >
> > >>> "F Z" <gerifalte28 at hotmail.com> 29/10/04 12:48:54 >>>
> >Hi
> >
> >I have a data.frame with dim = 18638 (rows)     6 (cols)
> >
> >names(dat)
> >[1] "id"      "long"    "lat"     "species" "type"    "size"
> >
> >Variable "species" and "type" are factors.  Species has 5 levels "BOV"
> >"CAP"
> >"CER" "OVI" "POR"
> >Variable "type" has 11 levels "BRD" "CL" ... "OTHER"
> >
> >I would like to replace the values on species by the values on types
> >only if
> >species is == "POR"
> >I tried:
> >
> >x<-ifelse(dat$species %in% "POR",dat$type,dat$species)
> >dat[,4]<-x
> >but levels(x)
> >[1] "1"  "2"  "3"  "4"  "5"  "6"  "8"  "9"  "10" "11" "12"
> >
> >So x changes the factor names by numbers.  I can not use factor() to
> >recover
> >the names since the resulting factors in x are a mixture of factors
> >from
> >species and type.
> >
> >I also tried
> >
> >x<-gsub(pattern = "POR",replacement= factor(dat$type),dat$species)
> >with
> >same behavior.
> >
> >Apparently I did not have my granola bar today so I can't find a
> >solution!
> >Any help is greatly appreciated
> >
> >Thanks!
> >
> >Francisco
> >
> >______________________________________________
> >R-help at stat.math.ethz.ch mailing list
> >https://stat.ethz.ch/mailman/listinfo/r-help
> >PLEASE do read the posting guide!
> >http://www.R-project.org/posting-guide.html
> >
> >______________________________________________________
> >
> >The contents of this e-mail are privileged and/or confidential to the
> >named recipient and are not to be used by any other person and/or
> >organisation. If you have received this e-mail in error, please notify
> >the sender and delete all material pertaining to this e-mail.
> >______________________________________________________
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>



From sundar.dorai-raj at pdf.com  Fri Oct 29 19:42:28 2004
From: sundar.dorai-raj at pdf.com (Sundar Dorai-Raj)
Date: Fri, 29 Oct 2004 12:42:28 -0500
Subject: [R] Giving column names to a matrix
In-Reply-To: <1099070809.41827d5998f09@webmail.fe.up.pt>
References: <1099070809.41827d5998f09@webmail.fe.up.pt>
Message-ID: <41828104.7060806@pdf.com>



jmoreira at fe.up.pt wrote:

> Heloo,
> 
> I have the following problem:
> 
> orig.data	<- NULL
> Inside a loop I have instructions like:
> orig.data <- rbind(orig.data, ...)
> After that I do:
> colnames(orig.data)<-c('Data','InicioViagem', ...)
> Everything works fine.
> For example, the first line of the matrix is:
> 
>>orig.data[1,]
> 
>         Data InicioViagem      ...
>            1        40466      ...
> The problem is: I can't refer the columns by the column names.
> For example:
> 
>>orig.data[1,InicioViagem]
> 
> Error: Object "InicioViagem" not found
> or
> 
>>orig.data[1,]$InicioViagem
> 
> NULL
> but:
> 
>>orig.data[1,2]
> 
> [1] 40466
> works!
> 
> Can someone help me?
> 
> Thanks
> 
> Joao Moreira

Joao,
   The problem is that you are constructing a "matrix" and not a 
"data.frame". So you want:

orig.data[1, "InicioViagem"] # note the quotes

or

orig.data <- as.data.frame(orig.data)
orig.data$InicioViagem[1]

--sundar



From partha_bagchi at hgsi.com  Fri Oct 29 19:58:57 2004
From: partha_bagchi at hgsi.com (partha_bagchi@hgsi.com)
Date: Fri, 29 Oct 2004 13:58:57 -0400
Subject: [R] Warning message if the plot statement breaks into 2 lines
Message-ID: <OFDE6A545C.467D5E71-ON85256F3C.0060FA40-85256F3C.0062C77E@hgsi.com>

Here is a curious observation. In 

> version
         _ 
platform i386-pc-mingw32
arch     i386 
os       mingw32 
system   i386, mingw32 
status 
major    2 
minor    0.0 
year     2004 
month    10 
day      04 
language R 
>

Try the following:

> Plot(c(1:100), type = "l
+ ")
Warning message: 
plot type 'l
' truncated to first character in: plot.xy(xy, type, pch, lty, col, bg, 
cex, lwd, ...) 
>

Where as in 
> version
         _ 
platform i386-pc-mingw32
arch     i386 
os       mingw32 
system   i386, mingw32 
status 
major    1 
minor    9.1 
year     2004 
month    06 
day      21 
language R 
>
> plot(c(1:10), type = "l
Error: syntax error
>



From SLiang at wyeth.com  Fri Oct 29 20:06:03 2004
From: SLiang at wyeth.com (Sean Liang)
Date: Fri, 29 Oct 2004 14:06:03 -0400
Subject: [R] pattern search
Message-ID: <s1824e54.044@ce08a07mbk.genetics.com>

hi, I like to find a pattern within a giver sequence. There might be
multiple occurences of the pattern. I like to know the number of
occurences and the positions if possible. "grep" can tell me if a
pattern exists but can't give me the information I need. Does anyone
know any function that I can use or know how to do what I intend?.
Thanks a lot.

Sean



From caimiaow at u.washington.edu  Fri Oct 29 20:14:36 2004
From: caimiaow at u.washington.edu (Caimiao Wei)
Date: Fri, 29 Oct 2004 11:14:36 -0700
Subject: [R] fitting linear mixed model for incomplete block design 
Message-ID: <00b801c4bde3$26a15670$8800a8c0@BumgarnerLab.local>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20041029/b1776302/attachment.pl

From mikem at salter-point.com  Fri Oct 29 20:19:09 2004
From: mikem at salter-point.com (Mike Meyer)
Date: Fri, 29 Oct 2004 11:19:09 -0700
Subject: [R] Warning message if the plot statement breaks into 2 lines
In-Reply-To: <OFDE6A545C.467D5E71-ON85256F3C.0060FA40-85256F3C.0062C77E@hgsi.com>
References: <OFDE6A545C.467D5E71-ON85256F3C.0060FA40-85256F3C.0062C77E@hgsi.com>
Message-ID: <20041029111909.79486b4b.mikem@Salter-Point.com>

It has all to do with a string going over two lines and hence the line terminator is part of the string.   If you close the string on the first line then the command is just fine.

At least the newer version of the parser allows for embedded newline characters.

 --Mike

On Fri, 29 Oct 2004 13:58:57 -0400
partha_bagchi at hgsi.com wrote:

> Here is a curious observation. In 
> 
> > version
>          _ 
> platform i386-pc-mingw32
> arch     i386 
> os       mingw32 
> system   i386, mingw32 
> status 
> major    2 
> minor    0.0 
> year     2004 
> month    10 
> day      04 
> language R 
> >
> 
> Try the following:
> 
> > Plot(c(1:100), type = "l
> + ")
> Warning message: 
> plot type 'l
> ' truncated to first character in: plot.xy(xy, type, pch, lty, col, bg, 
> cex, lwd, ...) 
> >
> 
> Where as in 
> > version
>          _ 
> platform i386-pc-mingw32
> arch     i386 
> os       mingw32 
> system   i386, mingw32 
> status 
> major    1 
> minor    9.1 
> year     2004 
> month    06 
> day      21 
> language R 
> >
> > plot(c(1:10), type = "l
> Error: syntax error
> >
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
> 


-- 

Mike Meyer,  Seattle WA



From ripley at stats.ox.ac.uk  Fri Oct 29 20:20:28 2004
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Fri, 29 Oct 2004 19:20:28 +0100 (BST)
Subject: [R] Warning message if the plot statement breaks into 2 lines
In-Reply-To: <OFDE6A545C.467D5E71-ON85256F3C.0060FA40-85256F3C.0062C77E@hgsi.com>
Message-ID: <Pine.LNX.4.44.0410291918580.1413-100000@gannet.stats>

This is a consequence of a change described in NEWS to allow 
multi-line *strings*.

On Fri, 29 Oct 2004 partha_bagchi at hgsi.com wrote:

> Here is a curious observation. In 
> 
> > version
>          _ 
> platform i386-pc-mingw32
> arch     i386 
> os       mingw32 
> system   i386, mingw32 
> status 
> major    2 
> minor    0.0 
> year     2004 
> month    10 
> day      04 
> language R 
> >
> 
> Try the following:
> 
> > Plot(c(1:100), type = "l
> + ")
> Warning message: 
> plot type 'l
> ' truncated to first character in: plot.xy(xy, type, pch, lty, col, bg, 
> cex, lwd, ...) 
> >
> 
> Where as in 
> > version
>          _ 
> platform i386-pc-mingw32
> arch     i386 
> os       mingw32 
> system   i386, mingw32 
> status 
> major    1 
> minor    9.1 
> year     2004 
> month    06 
> day      21 
> language R 
> >
> > plot(c(1:10), type = "l
> Error: syntax error
> >
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
> 
> 

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From ligges at statistik.uni-dortmund.de  Fri Oct 29 20:25:26 2004
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Fri, 29 Oct 2004 20:25:26 +0200
Subject: [R] Warning message if the plot statement breaks into 2 lines
In-Reply-To: <OFDE6A545C.467D5E71-ON85256F3C.0060FA40-85256F3C.0062C77E@hgsi.com>
References: <OFDE6A545C.467D5E71-ON85256F3C.0060FA40-85256F3C.0062C77E@hgsi.com>
Message-ID: <41828B16.5080301@statistik.uni-dortmund.de>

partha_bagchi at hgsi.com wrote:

> Here is a curious observation. In 
> 
> 
>>version
> 
>          _ 
> platform i386-pc-mingw32
> arch     i386 
> os       mingw32 
> system   i386, mingw32 
> status 
> major    2 
> minor    0.0 
> year     2004 
> month    10 
> day      04 
> language R 
> 
> 
> Try the following:
> 
> 
>>Plot(c(1:100), type = "l
> 
> + ")

Type

  > "l
  + "

and see:

[1] "l\n"


The newline is truncated, as expected.
So the new feature of multiline strings is great, isn't it?

Uwe Ligges



> Warning message: 
> plot type 'l
> ' truncated to first character in: plot.xy(xy, type, pch, lty, col, bg, 
> cex, lwd, ...) 
> 
> 
> Where as in 
> 
>>version
> 
>          _ 
> platform i386-pc-mingw32
> arch     i386 
> os       mingw32 
> system   i386, mingw32 
> status 
> major    1 
> minor    9.1 
> year     2004 
> month    06 
> day      21 
> language R 
> 
>>plot(c(1:10), type = "l
> 
> Error: syntax error
> 
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html



From ligges at statistik.uni-dortmund.de  Fri Oct 29 20:27:27 2004
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Fri, 29 Oct 2004 20:27:27 +0200
Subject: [R] pattern search
In-Reply-To: <s1824e54.044@ce08a07mbk.genetics.com>
References: <s1824e54.044@ce08a07mbk.genetics.com>
Message-ID: <41828B8F.8010607@statistik.uni-dortmund.de>

Sean Liang wrote:

> hi, I like to find a pattern within a giver sequence. There might be
> multiple occurences of the pattern. I like to know the number of
> occurences and the positions if possible. "grep" can tell me if a
> pattern exists but can't give me the information I need. Does anyone
> know any function that I can use or know how to do what I intend?.

???

   gr <- grep("a", c("a", "b", "a"))

grep tells you that the positions are 1 and 3. length(gr) tells you 
there are two occurences.

Uwe Ligges


> Thanks a lot.
> 
> Sean
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html



From SLiang at wyeth.com  Fri Oct 29 20:40:52 2004
From: SLiang at wyeth.com (Sean Liang)
Date: Fri, 29 Oct 2004 14:40:52 -0400
Subject: [R] pattern search
Message-ID: <s1825683.027@ce08a07mbk.genetics.com>

Uwe:

Thanks for getting back to me. My situation is a bit more complicated.
I have a vector of sequences each of which might contain any number of a
given pattern (e.g.

>pat=c("ATCGTTTGCTAC", "GGCTAATGCATTGC");
> grep ("TGC", pat)
[1] 1 2

grep only tells me the position of first occurrence in each element
whereas the second element contains two "TGC"s. 



>>> Uwe Ligges <ligges at statistik.uni-dortmund.de> 10/29/2004 2:27:27 PM
>>>
Sean Liang wrote:

> hi, I like to find a pattern within a giver sequence. There might be
> multiple occurences of the pattern. I like to know the number of
> occurences and the positions if possible. "grep" can tell me if a
> pattern exists but can't give me the information I need. Does anyone
> know any function that I can use or know how to do what I intend?.

???

   gr <- grep("a", c("a", "b", "a"))

grep tells you that the positions are 1 and 3. length(gr) tells you 
there are two occurences.

Uwe Ligges


> Thanks a lot.
> 
> Sean
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help 
> PLEASE do read the posting guide!
http://www.R-project.org/posting-guide.html



From SLiang at wyeth.com  Fri Oct 29 20:45:56 2004
From: SLiang at wyeth.com (Sean Liang)
Date: Fri, 29 Oct 2004 14:45:56 -0400
Subject: [R] pattern search
Message-ID: <s18257ba.055@ce08a07mbk.genetics.com>

Bert:

I thought about that but was a bit concerned with the number of strings
(10^4) in my vector. Each of these strings can have any number of a
given patterns. I will give it a test to see if this is computationally
efficient. 


Thanks.

>>> Berton Gunter <gunter.berton at gene.com> 10/29/2004 2:16:53 PM >>>
Use regexp() in a loop that deletes each successive occurrence of the
pattern via substring() and keeps count.

-- Bert Gunter
Genentech Non-Clinical Statistics
South San Francisco, CA
 
"The business of the statistician is to catalyze the scientific
learning
process."  - George E. P. Box
 
 

> -----Original Message-----
> From: r-help-bounces at stat.math.ethz.ch 
> [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Sean Liang
> Sent: Friday, October 29, 2004 11:06 AM
> To: R-help at stat.math.ethz.ch 
> Subject: [R] pattern search
> 
> hi, I like to find a pattern within a giver sequence. There might be
> multiple occurences of the pattern. I like to know the number of
> occurences and the positions if possible. "grep" can tell me if a
> pattern exists but can't give me the information I need. Does anyone
> know any function that I can use or know how to do what I intend?.
> Thanks a lot.
> 
> Sean
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help 
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html 
>



From ripley at stats.ox.ac.uk  Fri Oct 29 21:05:25 2004
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Fri, 29 Oct 2004 20:05:25 +0100 (BST)
Subject: [R] pattern search
In-Reply-To: <s1825683.027@ce08a07mbk.genetics.com>
Message-ID: <Pine.LNX.4.44.0410292000390.1758-100000@gannet.stats>

On Fri, 29 Oct 2004, Sean Liang wrote:

> Thanks for getting back to me. My situation is a bit more complicated.
> I have a vector of sequences each of which might contain any number of a
> given pattern (e.g.
> 
> >pat=c("ATCGTTTGCTAC", "GGCTAATGCATTGC");
> > grep ("TGC", pat)
> [1] 1 2
> 
> grep only tells me the position of first occurrence in each element
> whereas the second element contains two "TGC"s. 

sapply(strsplit(paste(1,pat,2, sep=""), "TGC"), length)-1

and you can also figure out the positions.

You can also do it recursively with regexpr (on the same page as grep, no 
less).

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From reid_huntsinger at merck.com  Fri Oct 29 21:12:47 2004
From: reid_huntsinger at merck.com (Huntsinger, Reid)
Date: Fri, 29 Oct 2004 15:12:47 -0400
Subject: [R] question about R on Linux Cluster
Message-ID: <D9A95B4B7B20354992E165EEADA31999056A9183@uswpmx00.merck.com>

We use R on a Linux/openMosix cluster and it works very well. 

Reid Huntsinger

-----Original Message-----
From: r-help-bounces at stat.math.ethz.ch
[mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of apollo wong
Sent: Friday, October 29, 2004 10:48 AM
To: r-help at stat.math.ethz.ch
Subject: [R] question about R on Linux Cluster


Hi, I am in trying to get a new Linux Cluster. I am
thinking about putting R on it. Does anyone have any
experience with running R on a distributed Linux
Cluster? Or does R support a cluster environment at
all. Any pitfall I should really watch out for?
Thanks in advance
Apollo

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide!
http://www.R-project.org/posting-guide.html



From mondorescue at yahoo.fr  Fri Oct 29 21:19:13 2004
From: mondorescue at yahoo.fr (Brian Seok)
Date: Fri, 29 Oct 2004 21:19:13 +0200 (CEST)
Subject: [R] [rmetasim] Need help deciphering this error msg... targeted to
	those who use rmetasim...
Message-ID: <20041029191913.52574.qmail@web26106.mail.ukl.yahoo.com>

Hello,

I am trying to do some simulation using the rmetasim
package and I've run to this problem.

--beginning of error msg--
Error in "[<-"(`*tmp*`, slice[l, ], slice[l, ], value
= c(0.200000002980232,  :
        number of items to replace is not a multiple
of replacement length
--end of error msg--

Here is the script I used.

--script starts here--
## load 'rmetasim' library
library(rmetasim)

penguinland <- new.landscape.empty()
penguinland <- new.intparam.land(penguinland,h=2,s=3)
penguinland <- new.floatparam.land(penguinland,s=0)
penguinland <- new.switchparam.land(penguinland,mp=0)

## local matrices are 3x3 (or greater than 2x2)
matrixR <- matrix( c(rep(0.2, 3*3)),
ncol=3,nrow=3,byrow=TRUE )
matrixS <- matrix( c(rep(0.1, 3*3)),
ncol=3,nrow=3,byrow=TRUE )
matrixM <- matrix( c(1.0, 0.0, 1.0, 0.0, 1.0, 0.0,
1.0, 0.0, 1.0), ncol=3,nrow=3,byrow=TRUE )
penguinland <-
new.local.demo(penguinland,matrixR,matrixS,matrixM)

matrixRR <- matrix( c(rep(0,6*6)),
ncol=6,nrow=6,byrow=TRUE )
matrixSS <- matrix( c(rep(0,6*6)),
ncol=6,nrow=6,byrow=TRUE )
matrixMM <- matrix( c(rep(0,6*6)),
ncol=6,nrow=6,byrow=TRUE )
penguinland <-
new.epoch(penguinland,matrixRR,matrixSS,matrixMM)
penguinland <- new.locus(penguinland)
penguinland <- new.individuals(penguinland, c(6, 4, 4,
3, 2, 5))

## i get error here when simulate.landscape()
executed...
#+
#+ ERROR MSG:
#+
#+ Error in "[<-"(`*tmp*`, slice[l, ], slice[l, ],
value = c(0.200000002980232,  :
#+         number of items to replace is not a
multiple of replacement length
#+
penguinland <- simulate.landscape(penguinland, 4)
--script ends here--

As noted in the script, when the matrixR, matrixS,
matrixM are size greater than 2x2, the script fails to
execute at the simulate.landscape()

Here is the script with the 2x2 R,S,M matrices.

--script starts here--
## load 'rmetasim' library
library(rmetasim)

penguinland <- new.landscape.empty()
penguinland <- new.intparam.land(penguinland,h=2,s=2)
penguinland <- new.floatparam.land(penguinland,s=0)
penguinland <- new.switchparam.land(penguinland,mp=0)

## local matrices are 2x2
matrixR <- matrix( c(0.2, 0.2, 0.2, 0.2),
ncol=2,nrow=2,byrow=TRUE )
matrixS <- matrix( c(0.1, 0.1, 0.1, 0.1),
ncol=2,nrow=2,byrow=TRUE )
matrixM <- matrix( c(1.0, 0.0, 1.0, 0.0),
ncol=2,nrow=2,byrow=TRUE )
penguinland <-
new.local.demo(penguinland,matrixR,matrixS,matrixM)

matrixRR <- matrix( c(rep(0,4*4)),
ncol=4,nrow=4,byrow=TRUE )
matrixSS <- matrix( c(rep(0,4*4)),
ncol=4,nrow=4,byrow=TRUE )
matrixMM <- matrix( c(rep(0,4*4)),
ncol=4,nrow=4,byrow=TRUE )
penguinland <-
new.epoch(penguinland,matrixRR,matrixSS,matrixMM)
penguinland <- new.locus(penguinland)
penguinland <- new.individuals(penguinland, c(6, 4, 4,
3))

## when local matrices are 2x2, it works!
penguinland <- simulate.landscape(penguinland, 4)
--script ends here--

This works. I've tried various things, like change
other parts of the script, e.g. changing the
new.locus(), eventually I got an error msg similar,
instead of "        number of items to replace is not
a multiple of replacement length" but with something
about the ape package.

If someone can decipher this error msg for me, then I
maybe able to work around it or correct it or findout
what I'M doing wrong.

Any help would be greatly appreciated thanks!

Brian S.



	

	
		
Vous manquez d??espace pour stocker vos mails ?



From mondorescue at yahoo.fr  Fri Oct 29 21:23:40 2004
From: mondorescue at yahoo.fr (Brian Seok)
Date: Fri, 29 Oct 2004 21:23:40 +0200 (CEST)
Subject: [R][rmetasim] Need help deciphering this error msg... targeted to
	those who use rmetasim...
Message-ID: <20041029192340.52171.qmail@web26103.mail.ukl.yahoo.com>

Hello,

I am trying to do some simulation using the rmetasim
package and I've run to this problem.

--beginning of error msg--
Error in "[<-"(`*tmp*`, slice[l, ], slice[l, ], value
= c(0.200000002980232,  :
        number of items to replace is not a multiple
of replacement length
--end of error msg--

Here is the script I used.

--script starts here--
## load 'rmetasim' library
library(rmetasim)

penguinland <- new.landscape.empty()
penguinland <- new.intparam.land(penguinland,h=2,s=3)
penguinland <- new.floatparam.land(penguinland,s=0)
penguinland <- new.switchparam.land(penguinland,mp=0)

## local matrices are 3x3 (or greater than 2x2)
matrixR <- matrix( c(rep(0.2, 3*3)),
ncol=3,nrow=3,byrow=TRUE )
matrixS <- matrix( c(rep(0.1, 3*3)),
ncol=3,nrow=3,byrow=TRUE )
matrixM <- matrix( c(1.0, 0.0, 1.0, 0.0, 1.0, 0.0,
1.0, 0.0, 1.0), ncol=3,nrow=3,byrow=TRUE )
penguinland <-
new.local.demo(penguinland,matrixR,matrixS,matrixM)

matrixRR <- matrix( c(rep(0,6*6)),
ncol=6,nrow=6,byrow=TRUE )
matrixSS <- matrix( c(rep(0,6*6)),
ncol=6,nrow=6,byrow=TRUE )
matrixMM <- matrix( c(rep(0,6*6)),
ncol=6,nrow=6,byrow=TRUE )
penguinland <-
new.epoch(penguinland,matrixRR,matrixSS,matrixMM)
penguinland <- new.locus(penguinland)
penguinland <- new.individuals(penguinland, c(6, 4, 4,
3, 2, 5))

## i get error here when simulate.landscape()
executed...
#+
#+ ERROR MSG:
#+
#+ Error in "[<-"(`*tmp*`, slice[l, ], slice[l, ],
value = c(0.200000002980232,  :
#+         number of items to replace is not a
multiple of replacement length
#+
penguinland <- simulate.landscape(penguinland, 4)
--script ends here--

As noted in the script, when the matrixR, matrixS,
matrixM are size greater than 2x2, the script fails to
execute at the simulate.landscape()

Here is the script with the 2x2 R,S,M matrices.

--script starts here--
## load 'rmetasim' library
library(rmetasim)

penguinland <- new.landscape.empty()
penguinland <- new.intparam.land(penguinland,h=2,s=2)
penguinland <- new.floatparam.land(penguinland,s=0)
penguinland <- new.switchparam.land(penguinland,mp=0)

## local matrices are 2x2
matrixR <- matrix( c(0.2, 0.2, 0.2, 0.2),
ncol=2,nrow=2,byrow=TRUE )
matrixS <- matrix( c(0.1, 0.1, 0.1, 0.1),
ncol=2,nrow=2,byrow=TRUE )
matrixM <- matrix( c(1.0, 0.0, 1.0, 0.0),
ncol=2,nrow=2,byrow=TRUE )
penguinland <-
new.local.demo(penguinland,matrixR,matrixS,matrixM)

matrixRR <- matrix( c(rep(0,4*4)),
ncol=4,nrow=4,byrow=TRUE )
matrixSS <- matrix( c(rep(0,4*4)),
ncol=4,nrow=4,byrow=TRUE )
matrixMM <- matrix( c(rep(0,4*4)),
ncol=4,nrow=4,byrow=TRUE )
penguinland <-
new.epoch(penguinland,matrixRR,matrixSS,matrixMM)
penguinland <- new.locus(penguinland)
penguinland <- new.individuals(penguinland, c(6, 4, 4,
3))

## when local matrices are 2x2, it works!
penguinland <- simulate.landscape(penguinland, 4)
--script ends here--

This works. I've tried various things, like change
other parts of the script, e.g. changing the
new.locus(), eventually I got an error msg similar,
instead of "        number of items to replace is not
a multiple of replacement length" but with something
about the ape package. Because of this, I wasn't sure
if I should've just contact the developer or rmetasim
or not therefore is posting this issue here.

The included demo also has its R,S,M matrices size
2x2, so I've altered it to be size 3x3 and got the
same error msg.

If someone can decipher this error msg for me, then I
maybe able to work around it or correct it or findout
what I'M doing wrong.

Any help would be greatly appreciated thanks!

Brian S.



	

	
		
Vous manquez d??espace pour stocker vos mails ?



From dinos at northwestern.edu  Fri Oct 29 21:17:26 2004
From: dinos at northwestern.edu (Konstantinos Liolios)
Date: 29 Oct 2004 14:17:26 -0500
Subject: [R] plot.baysian error = only 0's may mix with negative subscripts
In-Reply-To: <4181E4F5.3020005@statistik.uni-dortmund.de>
References: <200410281502.i9SF2TBX014947@lulu.it.northwestern.edu>
	<41812FD9.4000506@statistik.uni-dortmund.de>
	<1098987290.17171.10.camel@bigbird>
	<4181E4F5.3020005@statistik.uni-dortmund.de>
Message-ID: <1099077446.17171.15.camel@bigbird>

Thanx a lot my friend, you have been helpfull enough.

Dinos


On Fri, 2004-10-29 at 01:36, Uwe Ligges wrote:
> Konstantinos Liolios wrote:
> 
> > I did make a typo and I meant plot.bayesian() from sma.  Hopefully most
> > people can easily guess what I meant.  
> 
> Forgive me, my brain is too small! I do not know which of the millions 
> of function is in which of the > 400 CRAN packages.
> It's is your homework to say which package it is in and to specify an 
> example, since other people already invest quite a lot of time in order 
>   to provide help. Please read the posting guide!
> 
> 
>  > It looks like NAs are illegal
> > from 1.9 and later.  I am surprised that stat.bayesian() still allows it
> > but plot has a problem with it.
> 
> Maybe. The error message does not suggest it directly and you haven't 
> told us anything about the data (in particular nothing about NAs). You 
> have still not given an easily reproducible example.
> 
> Uwe Ligges
> 
> > Thanx
> > Dinos
> > 
> >  peopleOn Thu, 2004-10-28 at 12:43, Uwe Ligges wrote:
> > 
> >>dinos at northwestern.edu wrote:
> >>
> >>>Dear R users and developers
> >>>
> >>>After upgrading to Windows XP and R 1.9.1 and 2.0, I retried to execute
> >>>plot.baysian() to a data set that I had used previously to plot with no
> >>>problem in win2000 R1.8.  The error I get is:
> >>>
> >>>Error in points(Mbar[-index], lods[-index], pch = ".") : 
> >>>        only 0's may mix with negative subscripts
> >>
> >>What is plot.baysian()?
> >>Do you mean  plot.bayesian()  from package "sma"?
> >>Probably an error has been fixed that caused your data to be plotted 
> >>inaccurately in R < 1.9.x ... or there is a bug in sma.
> >>But without the data it's hard to say.
> >>
> >>So the suggestion is to ask the package maintainer of sma by providing a 
> >>simple and easily reproducible example...
> >>
> >>Uwe Ligges
> >>
> >>
> >>
> >>>Thanx in advance
> >>>Dino
> >>>P.S.  I allready sent this message once but without a subject.  I
> >>>apologize for the inconvenience
> >>
> >>P.S. And with this message we do know thrice that you are:
> >>
> >>
> >>
> >>>Konstantinos G. Liolios
> >>>IT Software Engineer II
> >>>Charles E. and Emma H. Morrison
> >>>Depts.  Pathology and Microbiology-Immunology
> >>>Northwestern University
> >>>Ward 3-240
> >>>303 E. Chicago Avenue
> >>>Chicago IL 60611
> >>>Tel: 312-503-0224
> >>>Fax:312-503-0281
> >>>http://www.haldarlab.northwestern.edu
> >>
> >>[I removed the next two information blocks who you are, since I think 
> >>it's sufficient for most of us to have one of these blocks...]
> >>
> > 
> > 
> > ______________________________________________
> > R-help at stat.math.ethz.ch mailing list
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>



From bates at stat.wisc.edu  Fri Oct 29 21:47:35 2004
From: bates at stat.wisc.edu (Douglas Bates)
Date: Fri, 29 Oct 2004 14:47:35 -0500
Subject: [R] fitting linear mixed model for incomplete block design
In-Reply-To: <00b801c4bde3$26a15670$8800a8c0@BumgarnerLab.local>
References: <00b801c4bde3$26a15670$8800a8c0@BumgarnerLab.local>
Message-ID: <41829E57.8090100@stat.wisc.edu>

Caimiao Wei wrote:
> Dear R developers and users: 
>      
> I have the following data, x is the response variable, 
 >sample(individual) nested within trt, and subsample nested within nsample,
 > I want to fit trt as fixed effect, and block, nsample(trt) as
 >random effects using lme, is the following coding correct?
> 
> dat$vgrp <- getGroups(dat, form = ~ 1|trt/nsample, level = 2)
> 
> ge.lme1 <- lme(fixed=x~trt, data=dat, random=~ block + nsample|vgrp)
> 
> 
> 
>       x block trt nsample subsample 
>       -0.68984 1 1 1 1 
>       -0.2223 2 1 1 2 
>       -1.00144 3 1 2 1 
>       -2.59511 4 1 2 2 
>       2.51573 5 1 3 1 
>       -1.67577 6 1 3 2 
>       -0.31927 7 1 4 1 
>       -3.28983 8 1 4 2 
>       0.04243 9 1 5 1 
>       -0.00635 10 1 5 2 
>       -0.09925 11 1 6 1 
>       -0.73825 12 1 6 2 
>       -0.64498 13 1 7 1 
>       -2.35105 14 1 7 2 
>       1.13354 15 1 8 1 
>       -1.45652 16 1 8 2 
>       -0.05577 17 1 9 1 
>       -3.46945 18 1 9 2 
>       -0.16415 19 1 10 1 
>       -3.31346 20 1 10 2 
>       -1.67036 1 2 1 1 
>       -2.28084 2 2 1 2 
>       -0.4165 3 2 2 1 
>       -0.51967 4 2 2 2 
>       -1.20591 5 2 3 1 
>       -1.33746 6 2 3 2 
>       2.40848 7 2 4 1 
>       3.7884 8 2 4 2 
>       1.14388 9 2 5 1 
>       0.45225 10 2 5 2 
>       -0.5008 11 2 6 1 
>       -1.66627 12 2 6 2 
>       0.97082 13 2 7 1 
>       -0.14259 14 2 7 2 
>       -2.14964 15 2 8 1 
>       -1.77236 16 2 8 2 
>       -0.0305 17 2 9 1 
>       -1.23726 18 2 9 2 
>       0.43089 19 2 10 1 
>       1.07422 20 2 10 2 
> 
> 
> Thanks,
> 
> Caimiao

It looks as if there is some crossing between block and nsample(trt) so 
I think you should probably use the version of lme from the lme4 package.
 > dat$vgrp <- with(dat, nsample:trt)
 > xtabs(~block + vgrp, dat)
      vgrp
block 1:1 1:2 2:1 2:2 3:1 3:2 4:1 4:2 5:1 5:2 6:1 6:2 7:1 7:2 8:1 8:2 
9:1 9:2
    1    1   1   0   0   0   0   0   0   0   0   0   0   0   0   0   0 
  0   0
    2    1   1   0   0   0   0   0   0   0   0   0   0   0   0   0   0 
  0   0
    3    0   0   1   1   0   0   0   0   0   0   0   0   0   0   0   0 
  0   0
    4    0   0   1   1   0   0   0   0   0   0   0   0   0   0   0   0 
  0   0
    5    0   0   0   0   1   1   0   0   0   0   0   0   0   0   0   0 
  0   0
    6    0   0   0   0   1   1   0   0   0   0   0   0   0   0   0   0 
  0   0
    7    0   0   0   0   0   0   1   1   0   0   0   0   0   0   0   0 
  0   0
    8    0   0   0   0   0   0   1   1   0   0   0   0   0   0   0   0 
  0   0
    9    0   0   0   0   0   0   0   0   1   1   0   0   0   0   0   0 
  0   0
    10   0   0   0   0   0   0   0   0   1   1   0   0   0   0   0   0 
  0   0
    11   0   0   0   0   0   0   0   0   0   0   1   1   0   0   0   0 
  0   0
    12   0   0   0   0   0   0   0   0   0   0   1   1   0   0   0   0 
  0   0
    13   0   0   0   0   0   0   0   0   0   0   0   0   1   1   0   0 
  0   0
    14   0   0   0   0   0   0   0   0   0   0   0   0   1   1   0   0 
  0   0
    15   0   0   0   0   0   0   0   0   0   0   0   0   0   0   1   1 
  0   0
    16   0   0   0   0   0   0   0   0   0   0   0   0   0   0   1   1 
  0   0
    17   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0 
  1   1
    18   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0 
  1   1
    19   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0 
  0   0
    20   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0 
  0   0
      vgrp
block 10:1 10:2
    1     0    0
    2     0    0
    3     0    0
    4     0    0
    5     0    0
    6     0    0
    7     0    0
    8     0    0
    9     0    0
    10    0    0
    11    0    0
    12    0    0
    13    0    0
    14    0    0
    15    0    0
    16    0    0
    17    0    0
    18    0    0
    19    1    1
    20    1    1
 > fm1 <- lme(x ~ trt, dat, list(block = ~ 1, vgrp = ~ 1))
 > fm1
Linear mixed-effects model
Fixed: x ~ trt
  Data: dat
  log-restricted-likelihood:  -72.6419
Random effects:
  Groups   Name        Variance   Std.Dev.
  vgrp     (Intercept) 7.3585e-01 0.85781684
  block    (Intercept) 3.2395e-08 0.00017999
  Residual             1.7037e+00 1.30524021
# of obs: 40, groups: vgrp, 20; block, 20

Fixed effects:
             Estimate Std. Error DF t value Pr(>|t|)
(Intercept) -0.92005    0.39846 38 -2.3090  0.02647
trt2         0.68699    0.56350 38  1.2191  0.23030



From f.harrell at vanderbilt.edu  Fri Oct 29 21:14:17 2004
From: f.harrell at vanderbilt.edu (Frank E Harrell Jr)
Date: Fri, 29 Oct 2004 15:14:17 -0400
Subject: [R] missing values in logistic regression
In-Reply-To: <XFMail.041029134546.Ted.Harding@nessie.mcc.ac.uk>
References: <XFMail.041029134546.Ted.Harding@nessie.mcc.ac.uk>
Message-ID: <41829689.3020108@vanderbilt.edu>

(Ted Harding) wrote:
> On 29-Oct-04 Avril Coghlan wrote:
> 
>>Dear R help list,
>>
>>   I am trying to do a logistic regression
>>where I have a categorical response variable Y
>>and two numerical predictors X1 and X2. There
>>are quite a lot of missing values for predictor X2.
>>eg.,
>>
>>Y     X1   X2
>>red   0.6  0.2    *
>>red   0.5  0.2    *
>>red   0.5  NA
>>red   0.5  NA
>>green 0.2  0.1    *
>>green 0.1  NA
>>green 0.1  NA
>>green 0.05 0.05   *
>>
>>I am wondering can I combine X1 and X2 in
>>a logistic regression to predict Y, using
>>all the data for X1, even though there are NAs in
>>the X2 data?
>>
>>Or do I have to take only the cases for which
>>there is data for both X1 and X2? (marked
>>with *s above)
> 
> 
> I don't know of any R routine directly aimed at logistic regression
> with missing values as you describe.
>

The aregImpute function in the Hmisc package can handle this, using 
predictive mean matching with weighted multinomial sampling of donor 
observations' binary covariate values.

. . ..
> Ted.
> 
> 
> --------------------------------------------------------------------
> E-Mail: (Ted Harding) <Ted.Harding at nessie.mcc.ac.uk>


-- 
Frank E Harrell Jr   Professor and Chair           School of Medicine
                      Department of Biostatistics   Vanderbilt University



From John-J.Smith at ubs.com  Fri Oct 29 22:24:53 2004
From: John-J.Smith at ubs.com (John-J.Smith@ubs.com)
Date: Fri, 29 Oct 2004 16:24:53 -0400
Subject: [R] pairs(x)
Message-ID: <7D36EDA26E7C8C4299EFD3F171A88D21086E91F5@NSTMC006PEX1.ubsgs.ubsgroup.net>

Hi,

I am using pairs(x) , but would like to change the point font and color for the bottom half of "x".  I am using windows and opened a graphics page using x11(), then tried:

pairs(x)
points(x[110:114,1],x[110:114,2],col="6",pch=8)
points(x[115:119,1],x[115:119,2],col="4",pch=17)

Nothing happened.  I can program a loop and run thru the combinations of plots and use the points function to get what I need.  Hoping there is a more simple solution using pairs().

Thanks,

John

Visit our website at http://www.ubs.com

This message contains confidential information and is intend...{{dropped}}



From roebuck at odin.mdacc.tmc.edu  Fri Oct 29 22:48:40 2004
From: roebuck at odin.mdacc.tmc.edu (Paul Roebuck)
Date: Fri, 29 Oct 2004 15:48:40 -0500 (CDT)
Subject: [R] Mac OS X binary package naming convention
Message-ID: <Pine.OSF.4.58.0410291539200.71587@odin.mdacc.tmc.edu>

Did the convention change during the R 2.0.0 Cocoa update?
When I issue the build command:

$ R CMD build --binary mypkg

I expected to get 'mypkg_0.9-0.tgz' but instead got
'mypkg_0.9-0_R_powerpc-apple-darwin6.8.tar.gz'. Or should
I rename it manually?


platform	powerpc-apple-darwin6.8
arch		powerpc
os		darwin6.8
system		powerpc, darwin6.8
status
major		2
minor		0.0
year		2004
month		10
day		04
language	R

----------------------------------------------------------
SIGSIG -- signature too long (core dumped)



From joel3000 at gmail.com  Fri Oct 29 23:41:18 2004
From: joel3000 at gmail.com (Joel Bremson)
Date: Fri, 29 Oct 2004 14:41:18 -0700
Subject: [R] why should you set the mode in a vector?
Message-ID: <1253d67a041029144149bddfd9@mail.gmail.com>

Hi all,

If I write

v = vector(mode="numeric",length=10)

I'm still allowed to assign non-numerics to v.

Furthermore, R figures out what kind of vector I've got anyway
when I use the mode() function.

So what is it that assigning a mode does?

Joel



From gerifalte28 at hotmail.com  Sat Oct 30 00:08:24 2004
From: gerifalte28 at hotmail.com (F Z)
Date: Fri, 29 Oct 2004 22:08:24 +0000
Subject: [R] ifelse() question
Message-ID: <BAY2-F27ncJfKHmj9n100003af1@hotmail.com>

Many thanks to Peter, Sundar and Adaikalavan for their useful help.  As 
Adaikalavan correctly pointed out, the key was to pass as.character() as an 
argument within the ifelse() function.  Here is the soution using a subset 
of my data:

dat[1:10,]
      id      long      lat species type size
1  91029 -95.29819 46.41441     BOV  FRM   NA
2  13468 -95.09137 46.26823     POR   FP    0
3  92511 -95.29784 44.47016     BOV  FRM   NA
4  30312 -94.97496 44.41489     POR  FTF  120
5  90275 -93.73471 44.92226     BOV  FRM   NA
6  38367 -95.54004 44.02396     POR   FF   NA
7  90460 -94.34028 44.89793     BOV  FRM   NA
8  38564 -93.51084 43.65327     OVI   CL   75
9  33532 -93.06094 45.76132     POR  FTF   NA
10 11860 -95.23439 45.21244     POR   FP    0

test<-dat[1:10,]
attach(test)
test[,4]<-ifelse(species=="POR",as.character(type),as.character(species))
test
      id      long      lat species type size
1  91029 -95.29819 46.41441     BOV  FRM   NA
2  13468 -95.09137 46.26823      FP   FP    0
3  92511 -95.29784 44.47016     BOV  FRM   NA
4  30312 -94.97496 44.41489     FTF  FTF  120
5  90275 -93.73471 44.92226     BOV  FRM   NA
6  38367 -95.54004 44.02396      FF   FF   NA
7  90460 -94.34028 44.89793     BOV  FRM   NA
8  38564 -93.51084 43.65327     OVI   CL   75
9  33532 -93.06094 45.76132     FTF  FTF   NA
10 11860 -95.23439 45.21244      FP   FP    0

It worked! Now all the species =="POR" were replaced by the adjacent value 
on the column type.

Thanks again!

Francisco


>From: Adaikalavan Ramasamy <ramasamy at cancer.org.uk>
>Reply-To: ramasamy at cancer.org.uk
>To: F Z <gerifalte28 at hotmail.com>
>CC: R-help <r-help at stat.math.ethz.ch>
>Subject: Re: [R] ifelse() question
>Date: Fri, 29 Oct 2004 18:44:06 +0100
>
>Francisco, a more reproducible example would have helped but see if the
>following helps in your understanding
>
># Create dataset
>
>df <- data.frame( type=1:5, species=LETTERS[1:5] )
>df
>    type species
>  1    1       A
>  2    2       B
>  3    3       C
>  4    4       D
>  5    5       E
>
>df[ ,2] == "E"
>  [1] FALSE FALSE FALSE FALSE  TRUE
>
># Note that you need to coerce as.character inside ifelse
>attach(df)
>(df[ ,2] <- ifelse( species == "E", type, as.character(species) ))
>  [1] "A" "B" "C" "D" "5"
>detach(df)
>
>df
>    type species
>  1    1       A
>  2    2       B
>  3    3       C
>  4    4       D
>  5    5       5
>
>
>On Fri, 2004-10-29 at 18:03, F Z wrote:
> > Thanks for you reply Peter.  I tried using as.character and then 
>converting
> > to factors but it did not work since it generates missing values for all 
>the
> > dat[,4]=="POR".  See:
> >
> > >dat[1:5,4]
> > [1] BOV POR BOV POR BOV
> > Levels: BOV CAP CER OVI POR
> >
> > test<-dat
> >
> > >test[,4]<-as.character(test[,4])
> > >test[,5]<-as.character(test[,5])
> > >test[test[,4]=="POR",4]<-test[test[,4]=="POR",5]
> > Error in "[<-.data.frame"(`*tmp*`, test[, 4] == "POR", 4, value = c(NA,  
>:
> >         missing values are not allowed in subscripted assignments of 
>data
> > frames
> > >test[,4]<-as.factor(test[,4])
> > >test[,5]<-as.factor(test[,5])
> >
> > >test[1:5,4]
> > [1] BOV  <NA> BOV  <NA> BOV
> > Levels: BOV CAP CER OVI
> >
> >
> > Any suggestions?
> >
> > Thanks again!
> >
> > Francisco
> >
> >
> > >From: "Peter Alspach" <PAlspach at hortresearch.co.nz>
> > >To: <gerifalte28 at hotmail.com>,<R-help at stat.math.ethz.ch>
> > >Subject: Re: [R] ifelse() question
> > >Date: Fri, 29 Oct 2004 13:33:54 +1300
> > >
> > >
> > >Francisco
> > >
> > >Did you try changing the factors to character, with as.character?
> > >
> > >Also you don't really need ifelse() for this.  Something like the
> > >following (untested) should do it:
> > >
> > >dat[,4] <- as.character(dat[,4])
> > >dat[,5] <- as.character(dat[,5])
> > >dat[dat[,4]=='POR',4] <- dat[dat[,4]=='POR',5]
> > >dat[,4] <- as.factor(dat[,4])
> > >dat[,5] <- as.factor(dat[,5])
> > >
> > >
> > >Peter Alspach
> > >
> > > >>> "F Z" <gerifalte28 at hotmail.com> 29/10/04 12:48:54 >>>
> > >Hi
> > >
> > >I have a data.frame with dim = 18638 (rows)     6 (cols)
> > >
> > >names(dat)
> > >[1] "id"      "long"    "lat"     "species" "type"    "size"
> > >
> > >Variable "species" and "type" are factors.  Species has 5 levels "BOV"
> > >"CAP"
> > >"CER" "OVI" "POR"
> > >Variable "type" has 11 levels "BRD" "CL" ... "OTHER"
> > >
> > >I would like to replace the values on species by the values on types
> > >only if
> > >species is == "POR"
> > >I tried:
> > >
> > >x<-ifelse(dat$species %in% "POR",dat$type,dat$species)
> > >dat[,4]<-x
> > >but levels(x)
> > >[1] "1"  "2"  "3"  "4"  "5"  "6"  "8"  "9"  "10" "11" "12"
> > >
> > >So x changes the factor names by numbers.  I can not use factor() to
> > >recover
> > >the names since the resulting factors in x are a mixture of factors
> > >from
> > >species and type.
> > >
> > >I also tried
> > >
> > >x<-gsub(pattern = "POR",replacement= factor(dat$type),dat$species)
> > >with
> > >same behavior.
> > >
> > >Apparently I did not have my granola bar today so I can't find a
> > >solution!
> > >Any help is greatly appreciated
> > >
> > >Thanks!
> > >
> > >Francisco
> > >
> > >______________________________________________
> > >R-help at stat.math.ethz.ch mailing list
> > >https://stat.ethz.ch/mailman/listinfo/r-help
> > >PLEASE do read the posting guide!
> > >http://www.R-project.org/posting-guide.html
> > >
> > >______________________________________________________
> > >
> > >The contents of this e-mail are privileged and/or confidential to the
> > >named recipient and are not to be used by any other person and/or
> > >organisation. If you have received this e-mail in error, please notify
> > >the sender and delete all material pertaining to this e-mail.
> > >______________________________________________________
> >
> > ______________________________________________
> > R-help at stat.math.ethz.ch mailing list
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide! 
>http://www.R-project.org/posting-guide.html
> >
>
>______________________________________________
>R-help at stat.math.ethz.ch mailing list
>https://stat.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide! 
>http://www.R-project.org/posting-guide.html



From p.dalgaard at biostat.ku.dk  Sat Oct 30 00:22:42 2004
From: p.dalgaard at biostat.ku.dk (Peter Dalgaard)
Date: 30 Oct 2004 00:22:42 +0200
Subject: [R] why should you set the mode in a vector?
In-Reply-To: <1253d67a041029144149bddfd9@mail.gmail.com>
References: <1253d67a041029144149bddfd9@mail.gmail.com>
Message-ID: <x2zn25go5p.fsf@biostat.ku.dk>

Joel Bremson <joel3000 at gmail.com> writes:

> Hi all,
> 
> If I write
> 
> v = vector(mode="numeric",length=10)
> 
> I'm still allowed to assign non-numerics to v.

Not without changing the mode of v or the right hand side:

> v = vector(mode="numeric",length=10)
> v[3] <- T
> v
 [1] 0 0 1 0 0 0 0 0 0 0
> v[4] <- "foo"
> v
 [1] "0"   "0"   "1"   "foo" "0"   "0"   "0"   "0"   "0"   "0"

[Basically, there's a hierarchy: 

   logical < integer < double < complex < character

and coercion is guaranteed to work from "smaller" to "larger" modes,
but not the other way. In assignments, both sides are coerced to the
larger of the two modes before the actual assignment.]

 
> Furthermore, R figures out what kind of vector I've got anyway
> when I use the mode() function.
> 
> So what is it that assigning a mode does?

Well, it sets the mode so that R doesn't have to change it later....

-- 
   O__  ---- Peter Dalgaard             Blegdamsvej 3  
  c/ /'_ --- Dept. of Biostatistics     2200 Cph. N   
 (*) \(*) -- University of Copenhagen   Denmark      Ph: (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)             FAX: (+45) 35327907



From tplate at acm.org  Sat Oct 30 00:23:11 2004
From: tplate at acm.org (Tony Plate)
Date: Fri, 29 Oct 2004 16:23:11 -0600
Subject: [R] why should you set the mode in a vector?
In-Reply-To: <1253d67a041029144149bddfd9@mail.gmail.com>
References: <1253d67a041029144149bddfd9@mail.gmail.com>
Message-ID: <6.1.0.6.2.20041029160743.0be38cb8@mailhost.blackmesacapital.com>

It's useful when you need to be certain of the mode of a vector.  One such 
situation is when you are about to call a C-language function using the 
.C() interface.  As you point out, some assignments (even just to vector 
elements) can change the mode of the entire vector.  This is why it's 
important to check the mode of vectors passed to external language 
functions immediately before the call.

As to what assigning the mode does, it specifies (or changes, if necessary) 
the underlying type of storage of the vector.  In R, all the elements in a 
vector have the same storage mode.  In the example below, the storage is 
initial as double-precision floats, but after the assignment of character 
data to element 2, the vector is stored as character data (with suitably 
coerced values of the other elements).  After assignment of list data to 
element 1, the entire vector becomes a list (i.e., a vector of pointers to 
general objects).  [The terminology I'm using here is a little loose, but 
someone please correct me if it is outright wrong.]  Finally, the assigning 
of mode "numeric" to the list fails because not all elements can be 
coerced.  (And I'm not sure why the last assignment succeeds and produces 
the results it does.)

 > v <- vector(mode="numeric",length=4)
 > v[3:4] <- 3:4
 > storage.mode(v)
[1] "double"
 > v[2] <- "foo"
 > v
[1] "0"   "foo" "3"   "4"
 > storage.mode(v)
[1] "character"
 >
 > v[1] <- list(1:3)
 > v
[[1]]
[1] 1 2 3

[[2]]
[1] "foo"

[[3]]
[1] "3"

[[4]]
[1] "4"

 > mode(v) <- "numeric"
Error in as.double.default(list(as.integer(c(1, 2, 3)), "foo", "3", "4")) :
         (list) object cannot be coerced to double
 > x <- v[2:4]
 > mode(x) <- "numeric"
 > x
[1] NA NA NA
 >

-- Tony Plate

At Friday 03:41 PM 10/29/2004, Joel Bremson wrote:
>Hi all,
>
>If I write
>
>v = vector(mode="numeric",length=10)
>
>I'm still allowed to assign non-numerics to v.
>
>Furthermore, R figures out what kind of vector I've got anyway
>when I use the mode() function.
>
>So what is it that assigning a mode does?
>
>Joel
>
>______________________________________________
>R-help at stat.math.ethz.ch mailing list
>https://stat.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html



From ggrothendieck at myway.com  Sat Oct 30 02:29:27 2004
From: ggrothendieck at myway.com (Gabor Grothendieck)
Date: Sat, 30 Oct 2004 00:29:27 +0000 (UTC)
Subject: [R] pattern search
References: <s1825683.027@ce08a07mbk.genetics.com>
Message-ID: <loom.20041030T022641-337@post.gmane.org>

Sean Liang <SLiang <at> wyeth.com> writes:
> I have a vector of sequences each of which might contain any number of a
> given pattern (e.g.
> 
> >pat=c("ATCGTTTGCTAC", "GGCTAATGCATTGC");
> > grep ("TGC", pat)
> [1] 1 2
> 
> grep only tells me the position of first occurrence in each element
> whereas the second element contains two "TGC"s. 
[...]
> I like to know the number of
> occurences and the positions if possible. 

The following crates v, a list, the same length as pat, of
vectors representing pat elements split along boundaries of
TGC.  lapply then calculates the starting position of each
element selecting out those that correspond to TGC.  The
sapply at the end calculates the number of matches for each
element of pat.

pat <- c("ATCGTTTGCTAC", "GGCTAATGCATTGC")

# pat split along TGC boundaries
v <- strsplit(gsub("(TGC)", ":\\1:", pat), split = ":+")

# starting positions
lapply(v, function(x) (cumsum(nchar(x)) - nchar("TGC") + 1)[grep("TGC",x)])

# number of matches
sapply(.Last.value, length)



From unung at enciety.com  Sat Oct 30 03:53:35 2004
From: unung at enciety.com (Unung Istopo Hartanto)
Date: Sat, 30 Oct 2004 08:53:35 +0700
Subject: [R] Can R import data from firebird relational database
Message-ID: <1099101215.3244.10.camel@IT05>

Dear Users,

Can R import data or process data from firebird relational database
(firebird.sourceforge.net). Cause i still using it on linux and i'll
process my data using R.

Thanks all,

regards,

Unung



From nleonard at tartarus.uwa.edu.au  Sat Oct 30 08:41:16 2004
From: nleonard at tartarus.uwa.edu.au (Neil Leonard)
Date: Sat, 30 Oct 2004 14:41:16 +0800
Subject: [R] Recode variable as NA
Message-ID: <B35DE2C8-2A3E-11D9-8FB0-003065D5B8EC@tartarus.uwa.edu.au>

Hi,

I was wondering how I would recode a variable as being NA (or missing).


Thanks
Neil



From ripley at stats.ox.ac.uk  Sat Oct 30 09:03:19 2004
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Sat, 30 Oct 2004 08:03:19 +0100 (BST)
Subject: [R] Can R import data from firebird relational database
In-Reply-To: <1099101215.3244.10.camel@IT05>
Message-ID: <Pine.LNX.4.44.0410300753440.2839-100000@gannet.stats>

On Sat, 30 Oct 2004, Unung Istopo Hartanto wrote:

> Can R import data or process data from firebird relational database
> (firebird.sourceforge.net). Cause i still using it on linux and i'll
> process my data using R.

If it has a suitable interface.  A quick look suggests it has a Linux ODBC 
driver, so RODBC should be available.

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From ripley at stats.ox.ac.uk  Sat Oct 30 09:06:46 2004
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Sat, 30 Oct 2004 08:06:46 +0100 (BST)
Subject: [R] Recode variable as NA
In-Reply-To: <B35DE2C8-2A3E-11D9-8FB0-003065D5B8EC@tartarus.uwa.edu.au>
Message-ID: <Pine.LNX.4.44.0410300803500.2839-100000@gannet.stats>

On Sat, 30 Oct 2004, Neil Leonard wrote:

> I was wondering how I would recode a variable as being NA (or missing).
It is the values, not the variable that are NA.  For example

x <- 1:20
ind <- x%%4==0
ind
x[ind] <- NA  # or is.na(x[ind]) <- TRUE

That works for logical, integer, numeric, complex and character variables.
(Raw and list vecctors do not have missing values.)

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From nleonard at tartarus.uwa.edu.au  Sat Oct 30 10:02:46 2004
From: nleonard at tartarus.uwa.edu.au (Neil Leonard)
Date: Sat, 30 Oct 2004 16:02:46 +0800
Subject: [R] Recode variable as NA
In-Reply-To: <B998A44C8986644EA8029CFE6396A9240AAD65@exqld2-bne.qld.csiro.au>
References: <B998A44C8986644EA8029CFE6396A9240AAD65@exqld2-bne.qld.csiro.au>
Message-ID: <15BBE1AC-2A4A-11D9-8FB0-003065D5B8EC@tartarus.uwa.edu.au>

Sorry, I meant putting missing values into a variable.

Thank you for your help.

Neil

On 30/10/2004, at 3:10 PM, <Bill.Venables at csiro.au> wrote:

> Would you care to be a bit more explicit?
>
> If you mean how would you put missing values into a variable, this is
> easy
>
> blank.spots <- c(1:4, 23, 30:31)
> variable[blank.spots] <- NA
>
> -----Original Message-----
> From: r-help-bounces at stat.math.ethz.ch
> [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Neil Leonard
> Sent: Saturday, 30 October 2004 4:41 PM
> To: R-help help
> Subject: [R] Recode variable as NA
>
>
> Hi,
>
> I was wondering how I would recode a variable as being NA (or missing).
>
>
> Thanks
> Neil
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide!
> http://www.R-project.org/posting-guide.html
>



From ramasamy at cancer.org.uk  Sat Oct 30 15:14:50 2004
From: ramasamy at cancer.org.uk (Adaikalavan Ramasamy)
Date: Sat, 30 Oct 2004 14:14:50 +0100
Subject: [R] pairs(x)
In-Reply-To: <7D36EDA26E7C8C4299EFD3F171A88D21086E91F5@NSTMC006PEX1.ubsgs.ubsgroup.net>
References: <7D36EDA26E7C8C4299EFD3F171A88D21086E91F5@NSTMC006PEX1.ubsgs.ubsgroup.net>
Message-ID: <1099142090.5491.28.camel@localhost.localdomain>

The following is adapted from help(pairs) :

lpanel <- function(x, y){ points(x, y, col=2, pch=".") }
upanel <- function(x, y){ points(x, y, col=3, pch="x") }
data <- matrix( rnorm(300), nc=3 )
colnames(data) <- paste("Column", LETTERS[1:3])
pairs(data, upper.panel=upanel, lower.panel=lpanel)



On Fri, 2004-10-29 at 21:24, John-J.Smith at ubs.com wrote:
> Hi,
> 
> I am using pairs(x) , but would like to change the point font and color for the bottom half of "x".  I am using windows and opened a graphics page using x11(), then tried:
> 
> pairs(x)
> points(x[110:114,1],x[110:114,2],col="6",pch=8)
> points(x[115:119,1],x[115:119,2],col="4",pch=17)
> 
> Nothing happened.  I can program a loop and run thru the combinations of plots and use the points function to get what I need.  Hoping there is a more simple solution using pairs().
> 
> Thanks,
> 
> John
> 
> Visit our website at http://www.ubs.com
> 
> This message contains confidential information and is intend...{{dropped}}
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>



From wang at galton.uchicago.edu  Sat Oct 30 18:19:16 2004
From: wang at galton.uchicago.edu (Yong Wang)
Date: Sat, 30 Oct 2004 11:19:16 -0500 (CDT)
Subject: [R] How to plot PDF which is in the form of orthogonal polynomial 
In-Reply-To: <200410301009.i9UA8mfQ018958@hypatia.math.ethz.ch>
References: <200410301009.i9UA8mfQ018958@hypatia.math.ethz.ch>
Message-ID: <Pine.LNX.4.61.0410301109320.8296@aitken.uchicago.edu>

Dear all
using the orthogonal polymial on a set of data, I get an approximate 
density which basically is in the form: exp(-polynomial),
  as you know, the 
parameters are the converged coeeficients.
obviously, It is hard, if not impossible, to use the inverse CDF method to 
get a 
sample and then plot density. then how can I plot the approximated density 
in order to have a graphical comparision with the real data's histogram.

any hint is appreciated

thanks



From yuleih at umich.edu  Sat Oct 30 19:03:11 2004
From: yuleih at umich.edu (Yulei He)
Date: Sat, 30 Oct 2004 13:03:11 -0400 (EDT)
Subject: [R] (no subject)
Message-ID: <Pine.SOL.4.58.0410301302140.12578@timepilot.gpcc.itd.umich.edu>

Hi, there.

Does anybody know how to plot a smooth density plot for some data
simulated from certain distribution? Thanks.

Yulei


$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$
Yulei He
1586 Murfin Ave. Apt 37
Ann Arbor, MI 48105-3135
yuleih at umich.edu
734-647-0305(H)
734-763-0421(O)
734-763-0427(O)
734-764-8263(fax)
$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$



From mahbub.latif at gmail.com  Sat Oct 30 19:09:54 2004
From: mahbub.latif at gmail.com (Mahbub Latif)
Date: Sat, 30 Oct 2004 19:09:54 +0200
Subject: [R] (no subject)
In-Reply-To: <Pine.SOL.4.58.0410301302140.12578@timepilot.gpcc.itd.umich.edu>
References: <Pine.SOL.4.58.0410301302140.12578@timepilot.gpcc.itd.umich.edu>
Message-ID: <e6f3169a0410301009682532a2@mail.gmail.com>

?density


On Sat, 30 Oct 2004 13:03:11 -0400 (EDT), Yulei He <yuleih at umich.edu> wrote:
> Hi, there.
> 
> Does anybody know how to plot a smooth density plot for some data
> simulated from certain distribution? Thanks.
> 
> Yulei
> 
> $$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$
> Yulei He
> 1586 Murfin Ave. Apt 37
> Ann Arbor, MI 48105-3135
> yuleih at umich.edu
> 734-647-0305(H)
> 734-763-0421(O)
> 734-763-0427(O)
> 734-764-8263(fax)
> $$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
> 


-- 
------------------------------------------
A.H.M. Mahbub-ul Latif
PhD Student
Department of Medical Statistics, Goettingen
Germany



From wolski at molgen.mpg.de  Sun Oct 31 00:20:59 2004
From: wolski at molgen.mpg.de (Witold Eryk Wolski)
Date: Sun, 31 Oct 2004 00:20:59 +0200
Subject: [R] How to add values to an array at any position.
Message-ID: <418413CB.8020101@molgen.mpg.de>

Hi,

How to add values to an array at any position.
Asking because of the following:
e.g.

y<-c(0.1,NaN,0.2,NaN) #or data frame


x<-na.omit(y)

take some columns from x and
do some computation with functions which do not allow NaN 's.

After the computing add NaN's at positions stored in
attr(x,"na.action")
of the result vector.


/E


-- 
Dipl. bio-chem. Witold Eryk Wolski
MPI-Moleculare Genetic
Ihnestrasse 63-73 14195 Berlin
tel: 0049-30-83875219                 __("<    _
http://www.molgen.mpg.de/~wolski      \__/    'v'
http://r4proteomics.sourceforge.net    ||    /   \
mail: witek96 at users.sourceforge.net    ^^     m m
      wolski at molgen.mpg.de



From ggrothendieck at myway.com  Sun Oct 31 00:42:11 2004
From: ggrothendieck at myway.com (Gabor Grothendieck)
Date: Sat, 30 Oct 2004 22:42:11 +0000 (UTC)
Subject: [R] How to add values to an array at any position.
References: <418413CB.8020101@molgen.mpg.de>
Message-ID: <loom.20041031T004051-954@post.gmane.org>

Witold Eryk Wolski <wolski <at> molgen.mpg.de> writes:

: 
: Hi,
: 
: How to add values to an array at any position.
: Asking because of the following:
: e.g.
: 
: y<-c(0.1,NaN,0.2,NaN) #or data frame
: 
: x<-na.omit(y)
: 
: take some columns from x and
: do some computation with functions which do not allow NaN 's.
: 
: After the computing add NaN's at positions stored in
: attr(x,"na.action")
: of the result vector.

The following adds one to each element of x and reinserts
them into y:

y[-attr(x,"na.action")] <- x+1



From p.dalgaard at biostat.ku.dk  Sun Oct 31 00:56:07 2004
From: p.dalgaard at biostat.ku.dk (Peter Dalgaard)
Date: 31 Oct 2004 00:56:07 +0200
Subject: [R] How to add values to an array at any position.
In-Reply-To: <418413CB.8020101@molgen.mpg.de>
References: <418413CB.8020101@molgen.mpg.de>
Message-ID: <x27jp7n7co.fsf@biostat.ku.dk>

Witold Eryk Wolski <wolski at molgen.mpg.de> writes:

> Hi,
> 
> How to add values to an array at any position.
> Asking because of the following:
> e.g.
> 
> y<-c(0.1,NaN,0.2,NaN) #or data frame
> 
> 
> x<-na.omit(y)
> 
> take some columns from x and
> do some computation with functions which do not allow NaN 's.
> 
> After the computing add NaN's at positions stored in
> attr(x,"na.action")
> of the result vector.

You mean, something like

ix <- attr(x, "na.action")
newres <- c(res,rep(NA,length(ix))) # just to get size and mode right
newres[ix] <- NA
newres[-ix] <- res

-- 
   O__  ---- Peter Dalgaard             Blegdamsvej 3  
  c/ /'_ --- Dept. of Biostatistics     2200 Cph. N   
 (*) \(*) -- University of Copenhagen   Denmark      Ph: (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)             FAX: (+45) 35327907



From wolski at molgen.mpg.de  Sun Oct 31 01:15:16 2004
From: wolski at molgen.mpg.de (Witold Eryk Wolski)
Date: Sun, 31 Oct 2004 01:15:16 +0200
Subject: [R] How to add values to an array at any position.
In-Reply-To: <x27jp7n7co.fsf@biostat.ku.dk>
References: <418413CB.8020101@molgen.mpg.de> <x27jp7n7co.fsf@biostat.ku.dk>
Message-ID: <41842084.1060406@molgen.mpg.de>

Hi,

In the first place I was wondering if there are no function 
_insert.vector_  in base.

Thanks Peter and Gabor even for the "bad news".

E.


Peter Dalgaard wrote:

>Witold Eryk Wolski <wolski at molgen.mpg.de> writes:
>
>  
>
>>Hi,
>>
>>How to add values to an array at any position.
>>Asking because of the following:
>>e.g.
>>
>>y<-c(0.1,NaN,0.2,NaN) #or data frame
>>
>>
>>x<-na.omit(y)
>>
>>take some columns from x and
>>do some computation with functions which do not allow NaN 's.
>>
>>After the computing add NaN's at positions stored in
>>attr(x,"na.action")
>>of the result vector.
>>    
>>
>
>You mean, something like
>
>ix <- attr(x, "na.action")
>newres <- c(res,rep(NA,length(ix))) # just to get size and mode right
>newres[ix] <- NA
>newres[-ix] <- res
>
>  
>


-- 
Dipl. bio-chem. Witold Eryk Wolski
MPI-Moleculare Genetic
Ihnestrasse 63-73 14195 Berlin
tel: 0049-30-83875219                 __("<    _
http://www.molgen.mpg.de/~wolski      \__/    'v'
http://r4proteomics.sourceforge.net    ||    /   \
mail: witek96 at users.sourceforge.net    ^^     m m
      wolski at molgen.mpg.de



From gerifalte28 at hotmail.com  Sun Oct 31 02:18:47 2004
From: gerifalte28 at hotmail.com (F Z)
Date: Sun, 31 Oct 2004 00:18:47 +0000
Subject: [R] How to plot PDF which is in the form of orthogonal polynomial
Message-ID: <BAY2-F2514eBa1kg3Am0005ecf0@hotmail.com>

I am not sure that I understand very well what you want to do but you might 
want to try
density().  There are some good examples for this function under 
example(density). The result is an object with class "density" that you can 
plot directly using plot()

Francisco


PS: I reccomend you to read the posting guide 
http://lib.stat.cmu.edu/R/CRAN/ so you can get better answers

>From: Yong Wang <wang at galton.uchicago.edu>
>To: r-help-request at stat.math.ethz.ch
>CC: r-help at stat.math.ethz.ch
>Subject: [R] How to plot PDF which is in the form of orthogonal polynomial 
>Date: Sat, 30 Oct 2004 11:19:16 -0500 (CDT)
>
>Dear all
>using the orthogonal polymial on a set of data, I get an approximate 
>density which basically is in the form: exp(-polynomial),
>  as you know, the parameters are the converged coeeficients.
>obviously, It is hard, if not impossible, to use the inverse CDF method to 
>get a sample and then plot density. then how can I plot the approximated 
>density in order to have a graphical comparision with the real data's 
>histogram.
>
>any hint is appreciated
>
>thanks
>
>______________________________________________
>R-help at stat.math.ethz.ch mailing list
>https://stat.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide! 
>http://www.R-project.org/posting-guide.html



From ggrothendieck at myway.com  Sun Oct 31 02:20:10 2004
From: ggrothendieck at myway.com (Gabor Grothendieck)
Date: Sun, 31 Oct 2004 00:20:10 +0000 (UTC)
Subject: [R] How to add values to an array at any position.
References: <418413CB.8020101@molgen.mpg.de> <x27jp7n7co.fsf@biostat.ku.dk>
	<41842084.1060406@molgen.mpg.de>
Message-ID: <loom.20041031T021528-843@post.gmane.org>

Witold Eryk Wolski <wolski <at> molgen.mpg.de> writes: 
> In the first place I was wondering if there are no function 
> _insert.vector_  in base.

There is append.  e.g. insert 100:101 after position
3 of 11:15

R> append(11:15, 100:101, 3)
[1]  11  12  13 100 101  14  15



From laifang1 at yahoo.com  Sun Oct 31 08:17:58 2004
From: laifang1 at yahoo.com (fang lai)
Date: Sun, 31 Oct 2004 00:17:58 -0700 (PDT)
Subject: [R] (no subject)
Message-ID: <20041031071758.20288.qmail@web50405.mail.yahoo.com>

Dear all,
 I have several questions regarding fisher.test() in
R, and I'd highly appreciate any help with it.
 I have a group of observations, each having people's
income, and an indicator of whether selected in or out
a program. I want to test the difference between
income of people who are in and out.
 Because the distribution is far from normal, I decide
to use the fisher's exact test, using either mean or
rank as statistics.
 Question 0 is: Can I do this test using fisher.test()
in R?
If so,
 My first question is: Does fisher.test() offer an
option to choose the statistics? Actually it is not
clear from the help to me what statistics it uses.
Does it just compare the mean of people in and out of
the program?
 My second question is: when the group is large, I
always receive a warning message such as "Fisher exact
result might not be right"  when I set "hybrid=T".
When I set "hybrid=F", it does return a result of
p-value without warning message. I wonder if this
p-value is reliable or not. And, how does it get the
approximation of p-value when "hybrid=F"? Ideally, it
should randomly draw, say 1000 times, from the full
sets of permutation of assignment, and get an
approximate p-value--is this the way it works in
fisher.test( ) in R? If not, does it use another test,
or some other measure of approximation?
 My last question is: when the group is small enough,
will it calculates the exact probabilities even if
hybrid=F?
Many thanks,

Fang 

=====
Lai, Fang

PhD candidate
University of California, Berkeley
Department of Agricultural and Resource Economics
314 Giannini Hall, Berkeley, CA 94720-3310
tel: (510) 643 - 5421(O)
     (510) 847 - 9811(Cell)
fax: (510) 643 - 8911
email: lai at are.berkeley.edu
http://www.are.berkeley.edu/jobmarket/fang.html



From laifang1 at yahoo.com  Sun Oct 31 08:49:45 2004
From: laifang1 at yahoo.com (fang lai)
Date: Sun, 31 Oct 2004 00:49:45 -0700 (PDT)
Subject: [R] another question about fisher.test
Message-ID: <20041031074945.27751.qmail@web50405.mail.yahoo.com>

Dear all,
 I have another question about fisher.test:
Is there any way to save the resulting p-value in a
matrix or file after a loop as below?
 for (i in 1:42) {
# testing difference in income(pinc) between people in
# (with in1=1) and out (with in1=0) of the program for
# group i
fisher.test(m$pinc[m$group==i], m$in1[m$group==i],
workspace=40000000, hybrid=F)}

 When I run this loop, there is no results shown on
the terminal. I wonder if there is a way to store the
result for each i in a file or matrix.
Many thanks,

Fang

 

=====
Lai, Fang

PhD candidate
University of California, Berkeley
Department of Agricultural and Resource Economics
314 Giannini Hall, Berkeley, CA 94720-3310
tel: (510) 643 - 5421(O)
     (510) 847 - 9811(Cell)
fax: (510) 643 - 8911
email: lai at are.berkeley.edu
http://www.are.berkeley.edu/jobmarket/fang.html



From bob.ohara at helsinki.fi  Sun Oct 31 09:01:05 2004
From: bob.ohara at helsinki.fi (Anon.)
Date: Sun, 31 Oct 2004 10:01:05 +0200
Subject: [R] (no subject)
In-Reply-To: <20041031071758.20288.qmail@web50405.mail.yahoo.com>
References: <20041031071758.20288.qmail@web50405.mail.yahoo.com>
Message-ID: <41849BC1.8030503@helsinki.fi>

fang lai wrote:

>Dear all,
> I have several questions regarding fisher.test() in
>R, and I'd highly appreciate any help with it.
> I have a group of observations, each having people's
>income, and an indicator of whether selected in or out
>a program. I want to test the difference between
>income of people who are in and out.
> Because the distribution is far from normal, I decide
>to use the fisher's exact test, using either mean or
>rank as statistics.
> Question 0 is: Can I do this test using fisher.test()
>in R?
>  
>
The answer is "no".  I think you have mis-understood the purpose of 
Fisher's exact test: read (and understand!) the description in the help 
page.

If your data were normall distributed, then you could use a t-test 
(t.test()).  As you are not happy with the normality assumption, you 
could try an equivalent non-paramteric test, such as the Wilcoxon test 
(wilcox.test()), also known as a Mann-Whitney test.  I would recommend 
that you make sure you understand how the test works first.

Bob

-- 
Bob O'Hara
Department of Mathematics and Statistics
P.O. Box 68 (Gustaf H??llstr??min katu 2b)
FIN-00014 University of Helsinki
Finland

Telephone: +358-9-191 51479
Mobile: +358 50 599 0540
Fax:  +358-9-191 51400
WWW:  http://www.RNI.Helsinki.FI/~boh/
Journal of Negative Results - EEB: www.jnr-eeb.org



From hb at maths.lth.se  Sun Oct 31 09:47:40 2004
From: hb at maths.lth.se (Henrik Bengtsson)
Date: Sun, 31 Oct 2004 09:47:40 +0100
Subject: [R] How to add values to an array at any position.
In-Reply-To: <41842084.1060406@molgen.mpg.de>
Message-ID: <003501c4bf26$491609a0$9b0040d5@hblaptop>

> -----Original Message-----
> From: r-help-bounces at stat.math.ethz.ch 
> [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Witold 
> Eryk Wolski
> Sent: Sunday, October 31, 2004 1:15 AM
> To: Peter Dalgaard
> Cc: R Help Mailing List
> Subject: Re: [R] How to add values to an array at any position.
> 
> 
> Hi,
> 
> In the first place I was wondering if there are no function 
> _insert.vector_  in base.

I have an insert() in my R.basic package (to install, see
http://www.maths.lth.se/help/R/R.classes/);

Usage: 
insert(x, index, value=NA)

Example:
vec     <- c(1:10,13:15,19:20)
missing <- setdiff(1:20, vec)
vec2 <- insert(vec, missing, value=NA)
print(vec2)
# [1]  1  2  3  4  5  6  7  8  9 10 NA NA 13 14 15 NA NA NA 19 20

Is this what you're looking for?

Henrik


 
> Thanks Peter and Gabor even for the "bad news".
> 
> E.
> 
> 
> Peter Dalgaard wrote:
> 
> >Witold Eryk Wolski <wolski at molgen.mpg.de> writes:
> >
> >  
> >
> >>Hi,
> >>
> >>How to add values to an array at any position.
> >>Asking because of the following:
> >>e.g.
> >>
> >>y<-c(0.1,NaN,0.2,NaN) #or data frame
> >>
> >>
> >>x<-na.omit(y)
> >>
> >>take some columns from x and
> >>do some computation with functions which do not allow NaN 's.
> >>
> >>After the computing add NaN's at positions stored in
> >>attr(x,"na.action")
> >>of the result vector.
> >>    
> >>
> >
> >You mean, something like
> >
> >ix <- attr(x, "na.action")
> >newres <- c(res,rep(NA,length(ix))) # just to get size and 
> mode right 
> >newres[ix] <- NA newres[-ix] <- res
> >
> >  
> >
> 
> 
> -- 
> Dipl. bio-chem. Witold Eryk Wolski
> MPI-Moleculare Genetic
> Ihnestrasse 63-73 14195 Berlin
> tel: 0049-30-83875219                 __("<    _
> http://www.molgen.mpg.de/~wolski      \__/    'v'
> http://r4proteomics.sourceforge.net    ||    /   \
> mail: witek96 at users.sourceforge.net    ^^     m m
>       wolski at molgen.mpg.de
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list 
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
> 
>



From huber at ebi.ac.uk  Sun Oct 31 10:42:03 2004
From: huber at ebi.ac.uk (Wolfgang Huber)
Date: Sun, 31 Oct 2004 09:42:03 -0000 (GMT)
Subject: [R] Re: [Rd] (no subject)
In-Reply-To: <20041031071758.20288.qmail@web50405.mail.yahoo.com>
References: <20041031071758.20288.qmail@web50405.mail.yahoo.com>
Message-ID: <32800.81.104.215.6.1099215723.squirrel@webmail.ebi.ac.uk>

Dear Fang Lai,

you have sent your mail to both r-devel and r-help. Please do not do this,
but decide for one. Cross-posting just creates unnecessary and unpleasant
junk-mail to many people.

Furthermore, neither the r-devel nor the r-help mailing lists are intended
as replacements to taking a basic statistics course or reading the
software manuals - rather, as supplement and last resort. The answers are
provided by unpaid voluntary contributors, who appreciate that you
yourself also make at least a minimal effort before going off the mailing
list.

Best wishes
 Wolfgang

-------------------------------------
Wolfgang Huber
European Bioinformatics Institute
European Molecular Biology Laboratory
Cambridge CB10 1SD
England
Phone: +44 1223 494642
Http:  www.dkfz.de/abt0840/whuber
-------------------------------------

<quote who="fang lai">
> Dear all,
>  I have several questions regarding fisher.test() in
> R, and I'd highly appreciate any help with it.
>  I have a group of observations, each having people's
> income, and an indicator of whether selected in or out
> a program. I want to test the difference between
> income of people who are in and out.
>  Because the distribution is far from normal, I decide
> to use the fisher's exact test, using either mean or
> rank as statistics.
>  Question 0 is: Can I do this test using fisher.test()
> in R?
> If so,
>  My first question is: Does fisher.test() offer an
> option to choose the statistics? Actually it is not
> clear from the help to me what statistics it uses.
> Does it just compare the mean of people in and out of
> the program?
>  My second question is: when the group is large, I
> always receive a warning message such as "Fisher exact
> result might not be right"  when I set "hybrid=T".
> When I set "hybrid=F", it does return a result of
> p-value without warning message. I wonder if this
> p-value is reliable or not. And, how does it get the
> approximation of p-value when "hybrid=F"? Ideally, it
> should randomly draw, say 1000 times, from the full
> sets of permutation of assignment, and get an
> approximate p-value--is this the way it works in
> fisher.test( ) in R? If not, does it use another test,
> or some other measure of approximation?
>  My last question is: when the group is small enough,
> will it calculates the exact probabilities even if
> hybrid=F?
> Many thanks,
>
> Fang
>
> =====
> Lai, Fang
>
> PhD candidate
> University of California, Berkeley
> Department of Agricultural and Resource Economics
> 314 Giannini Hall, Berkeley, CA 94720-3310
> tel: (510) 643 - 5421(O)
>      (510) 847 - 9811(Cell)
> fax: (510) 643 - 8911
> email: lai at are.berkeley.edu
> http://www.are.berkeley.edu/jobmarket/fang.html
>
> ______________________________________________
> R-devel at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-devel
>



From francoisromain at free.fr  Sun Oct 31 11:20:07 2004
From: francoisromain at free.fr (=?ISO-8859-1?Q?Romain_Fran=E7ois?=)
Date: Sun, 31 Oct 2004 11:20:07 +0100
Subject: [R] another question about fisher.test
In-Reply-To: <20041031074945.27751.qmail@web50405.mail.yahoo.com>
References: <20041031074945.27751.qmail@web50405.mail.yahoo.com>
Message-ID: <4184BC57.7070103@free.fr>

Try that :

results <- rep(0,42)
for (i in 1:42) {
# testing difference in income(pinc) between people in
# (with in1=1) and out (with in1=0) of the program for
# group i
   results[i] <- fisher.test(m$pinc[m$group==i], m$in1[m$group==i],
   workspace=40000000, hybrid=F)$p.value
}
results



If you want the results to be saved in a file, take a alook at :
?sink
?write.table
Please read the posting guide, an the introduction to R, you can easily 
learn with those how to save the results of a function to a vector. And 
please post some code that can be run directly in R console.


fang lai a ??crit :

>Dear all,
> I have another question about fisher.test:
>Is there any way to save the resulting p-value in a
>matrix or file after a loop as below?
> for (i in 1:42) {
># testing difference in income(pinc) between people in
># (with in1=1) and out (with in1=0) of the program for
># group i
>fisher.test(m$pinc[m$group==i], m$in1[m$group==i],
>workspace=40000000, hybrid=F)}
>
> When I run this loop, there is no results shown on
>the terminal. I wonder if there is a way to store the
>result for each i in a file or matrix.
>Many thanks,
>
>Fang
>
> 
>
>=====
>Lai, Fang
>
>PhD candidate
>University of California, Berkeley
>Department of Agricultural and Resource Economics
>314 Giannini Hall, Berkeley, CA 94720-3310
>tel: (510) 643 - 5421(O)
>     (510) 847 - 9811(Cell)
>fax: (510) 643 - 8911
>email: lai at are.berkeley.edu
>http://www.are.berkeley.edu/jobmarket/fang.html
>
>______________________________________________
>R-help at stat.math.ethz.ch mailing list
>https://stat.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>
>
>  
>

-- 
Romain Fran??ois
25, avenue Guy Moquet
94 400 Vitry sur seine
FRANCE
_______________________
_______________________

francoisromain at free.fr
01 46 80 65 60
06 18 39 14 69



From d.stasinopoulos at londonmet.ac.uk  Sun Oct 31 12:26:49 2004
From: d.stasinopoulos at londonmet.ac.uk (Mikis Stasinopoulos)
Date: Sun, 31 Oct 2004 11:26:49 +0000
Subject: [R] Problem in building a package in R 2.0.0
Message-ID: <4184CBF9.7010200@londonmet.ac.uk>

Dear all

I am trying  to build a package in Windows.
I use the following command  (which it used to work with previous 
versions ) and I am getting the  following error


#--------------------------------------------------------------------------------------------------------------
C:\PROGRA~1\R\rw2000\bin>Rcmd build --binary --use-zip 
C:\PROGRA~1\R\rw2000\src\library\gamlss
* checking for file 
'C:\PROGRA~1\R\rw2000\src\library\gamlss/DESCRIPTION' ... OK

installing R.css in c:/TEMP/Rbuild.1520


---------- Making package gamlss ------------
  adding build stamp to DESCRIPTION
  installing R files
  installing inst files
  installing data files
  preparing package gamlss for lazy data loading
  installing man source files
  installing indices
  zipping data
  installing help
 >>> Building/Updating help pages for package 'gamlss'
     Formats: text html latex example chm
  BB                                text    html    latex   example
  BCPE                              text    html    latex   example
  BCt                               text    html    latex   example
  BI                                text    html    latex   example
......
  update.gamlss                     text    html    latex   example
  usair                             text    html    latex   example
  wp                                text    html    latex   example
  zipping help files
hhc: not found
  preparing package gamlss for lazy loading
Error in tools:::.read_description(file) :
        file '/DESCRIPTION' does not exist
Execution halted
make: *** [lazyload] Error 1
*** Installation of gamlss failed ***

Removing 'c:/TEMP/Rbuild.1520/gamlss'
 ERROR
* installation failed

* building 'gamlss_0.4-0.zip'
Can't stat gamlss: No such file or directory
        zip warning: name not matched: gamlss

zip error: Nothing to do! (try: zip -r9X 
C:/PROGRA~1/R/rw2000/bin/gamlss_0.4-0.z
ip . -i gamlss)
#--------------------------------------------------------------------------------------------- 


It looks that .read_description() can not pick the DESCRIPTION file 
properly.
Note that checking the package using  "Rcmd check 
C:\PROGRA~1\R\rw2000\src\library\gamlss"
works and that  I can install the package locally using  "make pkg-gamlss".

Thanks

Mikis Stasinopoulos



From ripley at stats.ox.ac.uk  Sun Oct 31 13:35:39 2004
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Sun, 31 Oct 2004 12:35:39 +0000 (GMT)
Subject: [R] Problem in building a package in R 2.0.0
In-Reply-To: <4184CBF9.7010200@londonmet.ac.uk>
Message-ID: <Pine.LNX.4.44.0410311221390.11993-100000@gannet.stats>

Please read what it says about installing R in a path containing spaces in
README.packages.  I don't know if this is due to the sources being in a
path with spaces in or the package, although I expect the former as I can
build packages in paths with spaces in by using the short form as
recommended.

To quote:

  BEWARE: Don't expect this to work if the path to R_HOME contains spaces.
  It may work, but we don't recommend it.

There is another error: you have WINHELP set to YES and have not installed 
HHW.

On Sun, 31 Oct 2004, Mikis Stasinopoulos wrote:

> Dear all
> 
> I am trying  to build a package in Windows.
> I use the following command  (which it used to work with previous 
> versions ) and I am getting the  following error
> 
> 
> #--------------------------------------------------------------------------------------------------------------
> C:\PROGRA~1\R\rw2000\bin>Rcmd build --binary --use-zip 
> C:\PROGRA~1\R\rw2000\src\library\gamlss
> * checking for file 
> 'C:\PROGRA~1\R\rw2000\src\library\gamlss/DESCRIPTION' ... OK
> 
> installing R.css in c:/TEMP/Rbuild.1520
> 
> 
> ---------- Making package gamlss ------------
>   adding build stamp to DESCRIPTION
>   installing R files
>   installing inst files
>   installing data files
>   preparing package gamlss for lazy data loading
>   installing man source files
>   installing indices
>   zipping data
>   installing help
>  >>> Building/Updating help pages for package 'gamlss'
>      Formats: text html latex example chm
>   BB                                text    html    latex   example
>   BCPE                              text    html    latex   example
>   BCt                               text    html    latex   example
>   BI                                text    html    latex   example
> ......
>   update.gamlss                     text    html    latex   example
>   usair                             text    html    latex   example
>   wp                                text    html    latex   example
>   zipping help files
> hhc: not found
>   preparing package gamlss for lazy loading
> Error in tools:::.read_description(file) :
>         file '/DESCRIPTION' does not exist
> Execution halted
> make: *** [lazyload] Error 1
> *** Installation of gamlss failed ***
> 
> Removing 'c:/TEMP/Rbuild.1520/gamlss'
>  ERROR
> * installation failed
> 
> * building 'gamlss_0.4-0.zip'
> Can't stat gamlss: No such file or directory
>         zip warning: name not matched: gamlss
> 
> zip error: Nothing to do! (try: zip -r9X 
> C:/PROGRA~1/R/rw2000/bin/gamlss_0.4-0.z
> ip . -i gamlss)
> #--------------------------------------------------------------------------------------------- 
> 
> 
> It looks that .read_description() can not pick the DESCRIPTION file 
> properly.
> Note that checking the package using  "Rcmd check 
> C:\PROGRA~1\R\rw2000\src\library\gamlss"
> works and that  I can install the package locally using  "make pkg-gamlss".
> 
> Thanks
> 
> Mikis Stasinopoulos
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
> 
> 

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From D.R.J.Pleydell at pgr.salford.ac.uk  Sun Oct 31 17:04:53 2004
From: D.R.J.Pleydell at pgr.salford.ac.uk (David Pleydell)
Date: Sun, 31 Oct 2004 16:04:53 +0000
Subject: [R] strange results with dmvnorm
Message-ID: <1099238693.41850d253b8cd@webmail.salford.ac.uk>

I am experiencing strange results using dmvnorm. I define a scaled distance
matrix from the coordinates bellow and then calculate a covariance matrix using
a spherical correlation function.  Then with certain combinations of
range and sill parameters dmvnorm is returning values greater than 1.  Surely
the results of dmvnorm should be in the interval 0:1 (or do I just nead a
holiday?).  In addition cov.spatial and sphercov are giving different results. 
Any clues as to what is happening here might just save my sanity.

Many Thanks
David

x <-
c(22038,50284,56009,46979,21417,10149,31305,34473,42883,55084,51097,67990,55710,20256,31972,46423,20321,70886
,18758,43727,23834,13612,70217,38389,7980,41711,19462,35191,13924,14460,52657,68895,57616,48606,53874,42444
,17307,4514,38631,20517,42091,51844,42137,57977,22168,19906,31072,56385,51056,26229,20438,34406,28462,17218
,47406,84182,29189,46444,21067,37383,24395,38658,38617,70943,65331,63586,50953,39076,51230,64931,13615,68210
,23633,2455,56004,58891,59927,60778,70023,2075,7118,47964,40564,37949,3033,28401,41224,49202,50643,57889
,48008,11062,73256,38052,64000,60843,39599,45633,70129,25122,54528,42006,54445,38568,28561,37529,43752,67653
,11958,54801,52046,52773,42120,18872,63421,72778,57048,22565,52248,12549,21301,14375,961,34307,40936,44523
,45671,2651,52247,60016,50844,24076,55843,19700,12395,30860,21322,33055,43677,33405,66787,36370,28667,31794
,51464,44090,26633,25823,34199,52506,67256,72894,44651,29108,69867,37312,24407,59456,63329,61886,30339,69144
,13877,47091,13980,24024,68432,1007,6432,13923,31904,41093,18176,22861,25122,45251,35276,8508,19127,73074
,29659,41765,27467,49725,39012,26689,8794,11480,33809,37180,71153,44432,18397,66592,49294,31554,8914,40984
,37200,43233,47493,33728,58487,50731,42288,52529,33187,28559,21471,45940,40803,24063,35777,31187,22374,42185
,37019,33101,17435,21451,64085,48624,0,72571,18484,51457,44206,49327,37643,71719,45009,47327,58397,38843
,27057,28251,34786,32818,26227,16233,51900,42102,70651,49805,33860,19510,6536,67032,27022,20490,74443,56503
,23201,8504)

y<-c(19656,24181,21863,51776,59164,40430,28033,59460,17720,28689,39794,44737,33744,60868,58488,41163,55696,28305
,39258,12051,48751,51806,31246,9147,24850,43030,62512,39028,37460,60933,56162,15139,36460,19472,62064,16044
,52145,42288,15933,52472,4508,25879,28955,39947,54684,48753,35232,43390,33528,19161,15281,8860,37159,41399
,29888,43382,31347,19873,42867,38766,46147,12663,26343,18649,15733,41152,31031,14679,49137,48205,46452,31114
,52894,22521,29526,29937,21176,12439,47782,34605,42022,21109,14249,56541,31478,15300,45095,45090,26764,34286
,14803,24848,13272,29076,34605,31612,7278,28536,454,38202,21544,13010,25735,48565,45640,5288,32648,33012
,35712,35420,21666,65959,21632,28182,28024,19770,23977,39348,19231,27179,45174,14399,30525,25703,31037,19954
,23285,47695,21027,13917,43449,34856,38771,14150,47183,41164,21451,34398,47306,15420,17779,51974,42830,51551
,61578,8379,41219,48378,7699,40958,25624,23373,7979,53196,14330,56537,58067,14237,26000,33423,39967,28781
,27549,33739,33114,41793,20995,42079,34549,41766,39550,1595,35390,42489,22246,7580,20631,29990,23001,21306
,20456,39752,43611,42768,0,35443,48950,45480,48325,10746,42252,29786,45882,45156,48526,19182,51870,37749
,18564,52335,59215,38424,27250,12685,10016,37640,10945,53196,49621,37746,15965,15008,7207,14364,27606,35459
,36399,44440,31733,25446,21065,65535,37627,26158,64824,10465,58176,36662,19769,9080,45041,16080,49648,59638
,23339,40697,47107,8536,57035,55090,44414,1321,12861,21108,32654,27068,38365,22255,31550,11789,45404,53969
,13509,36350)

Dist <- sqrt(outer(x,x, "-")^2 + outer(y,y, "-")^2)
Dist <- Dist/max(Dist)

library(spatial)
Cov1 <- sphercov(Dist, 0.8, alpha=0, se=sqrt(2))
Cov2 <- sphercov(Dist, 0.6, alpha=0, se=sqrt(0.55))
library(geoR)
Cov1b <- cov.spatial(Dist, cov.model= "spherical", cov.pars=c(2, 0.8))
Cov2b <- cov.spatial(Dist, cov.model= "spherical", cov.pars=c(0.55, 0.6))

library(mvtnorm)
dmvnorm(rep(0, nrow(Cov)), sigma=Cov1)
[1] 37949.22
dmvnorm(rep(0, nrow(Cov)), sigma=Cov1b)
[1] 2.920084e-05
dmvnorm(rep(0, nrow(Cov)), sigma=Cov2)
[1] 4.722488e+60
dmvnorm(rep(0, nrow(Cov)), sigma=Cov2b)
[1] 1.085243e+51


----------------------------------------------------------------
Concerns about content should be sent to abuse at salford.ac.uk



From murdoch at stats.uwo.ca  Sun Oct 31 17:41:39 2004
From: murdoch at stats.uwo.ca (Duncan Murdoch)
Date: Sun, 31 Oct 2004 11:41:39 -0500
Subject: [R] strange results with dmvnorm
In-Reply-To: <1099238693.41850d253b8cd@webmail.salford.ac.uk>
References: <1099238693.41850d253b8cd@webmail.salford.ac.uk>
Message-ID: <cc5ao01eg4c0gdurt4hc99tuk8q4pgv2pf@4ax.com>

On Sun, 31 Oct 2004 16:04:53 +0000, "David Pleydell"
<D.R.J.Pleydell at pgr.salford.ac.uk> wrote:

>I am experiencing strange results using dmvnorm. I define a scaled distance
>matrix from the coordinates bellow and then calculate a covariance matrix using
>a spherical correlation function.  Then with certain combinations of
>range and sill parameters dmvnorm is returning values greater than 1.  Surely
>the results of dmvnorm should be in the interval 0:1 (or do I just nead a
>holiday?).  In addition cov.spatial and sphercov are giving different results. 
>Any clues as to what is happening here might just save my sanity.

Why would you expect a density to be between 0 and 1?  

Duncan Murdoch



From p.dalgaard at biostat.ku.dk  Sun Oct 31 17:42:07 2004
From: p.dalgaard at biostat.ku.dk (Peter Dalgaard)
Date: 31 Oct 2004 17:42:07 +0100
Subject: [R] strange results with dmvnorm
In-Reply-To: <1099238693.41850d253b8cd@webmail.salford.ac.uk>
References: <1099238693.41850d253b8cd@webmail.salford.ac.uk>
Message-ID: <x2ekjelu00.fsf@biostat.ku.dk>

"David Pleydell" <D.R.J.Pleydell at pgr.salford.ac.uk> writes:
> Surely
> the results of dmvnorm should be in the interval 0:1 (or do I just nead a
> holiday?).  

You need a holiday. Consider

  curve(dnorm(x,sd=.25),from=-1,to=1)

The *integral* has to be 1 though.

-- 
   O__  ---- Peter Dalgaard             Blegdamsvej 3  
  c/ /'_ --- Dept. of Biostatistics     2200 Cph. N   
 (*) \(*) -- University of Copenhagen   Denmark      Ph: (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)             FAX: (+45) 35327907



From ggrothendieck at myway.com  Sun Oct 31 17:53:51 2004
From: ggrothendieck at myway.com (Gabor Grothendieck)
Date: Sun, 31 Oct 2004 16:53:51 +0000 (UTC)
Subject: [R] strange results with dmvnorm
References: <1099238693.41850d253b8cd@webmail.salford.ac.uk>
Message-ID: <loom.20041031T172534-506@post.gmane.org>

David Pleydell <D.R.J.Pleydell <at> pgr.salford.ac.uk> writes:

: I am experiencing strange results using dmvnorm. I define a scaled distance
: matrix from the coordinates bellow and then calculate a covariance matrix 
using
: a spherical correlation function.  Then with certain combinations of
: range and sill parameters dmvnorm is returning values greater than 1.  Surely
: the results of dmvnorm should be in the interval 0:1 (or do I just nead a
: holiday?).  In addition cov.spatial and sphercov are giving different 
results. 
: Any clues as to what is happening here might just save my sanity.

Probabilities must not exceed 1 but densities can.  In fact,
the normal density will approach a delta function as the standard
deviation approaches 0.    For example, in the one dimensional
case note how the density of the standard normal at x=0 increases 
as the standard deviation decreases from 1 to .2:

R>     dnorm(0,0,5:1/5)
[1] 0.3989423 0.4986779 0.6649038 0.9973557 1.9947114

or try plotting the densities to get a visual idea:

R> matplot( sapply(5:1/5, dnorm, x = (-50):50/10, mean = 0), type = "l")



From THOMAS.VOLSCHO at huskymail.uconn.edu  Sun Oct 31 18:41:26 2004
From: THOMAS.VOLSCHO at huskymail.uconn.edu (Thomas W Volscho)
Date: Sun, 31 Oct 2004 12:41:26 -0500
Subject: [R] Obtaining fitted model information
Message-ID: <19433419a341.19a341194334@huskymail.uconn.edu>

Dear list,
I am brand new to R  and using Dalgaard's (2002) book Introductory Statistics with R (thus, some of my terminology may be incorrect).

I am fitting regression models and I want to use Hurvich and Tsai's AICC statistic to examine my regression models.  This penalty can be expressed as: 2*npar * (n/(n-npar-1)).

While you can obtain AIC, BIC, and logLik, I want to impose the AICC penalty instead.

After fitting a model.  Is there a way of obtaining the "npar" and then assigning it to a variable?

Essentially, I want to then write a simple function to add the AICC penalty to (-2*logLik).

Thank you in advance for any help,
Tom Volscho

************************************        
Thomas W. Volscho
Graduate Student
Dept. of Sociology U-2068
University of Connecticut
Storrs, CT 06269
Phone: (860) 486-3882
http://vm.uconn.edu/~twv00001



From jfox at mcmaster.ca  Sun Oct 31 19:37:35 2004
From: jfox at mcmaster.ca (John Fox)
Date: Sun, 31 Oct 2004 13:37:35 -0500
Subject: [R] Obtaining fitted model information
In-Reply-To: <19433419a341.19a341194334@huskymail.uconn.edu>
Message-ID: <20041031183735.IENE15612.tomts16-srv.bellnexxia.net@JohnDesktop8300>

Dear Thomas,

To get the number of independent parameters in the lm object mod, you can
use mod$rank, sum(!is.na(coef(mod)), or -- if there are no linear
dependencies among the columns of the model matrix -- length(coef(mod)).

I hope this helps,
 John

--------------------------------
John Fox
Department of Sociology
McMaster University
Hamilton, Ontario
Canada L8S 4M4
905-525-9140x23604
http://socserv.mcmaster.ca/jfox 
-------------------------------- 

> -----Original Message-----
> From: r-help-bounces at stat.math.ethz.ch 
> [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Thomas 
> W Volscho
> Sent: Sunday, October 31, 2004 12:41 PM
> To: r-help at stat.math.ethz.ch
> Subject: [R] Obtaining fitted model information
> 
> Dear list,
> I am brand new to R  and using Dalgaard's (2002) book 
> Introductory Statistics with R (thus, some of my terminology 
> may be incorrect).
> 
> I am fitting regression models and I want to use Hurvich and 
> Tsai's AICC statistic to examine my regression models.  This 
> penalty can be expressed as: 2*npar * (n/(n-npar-1)).
> 
> While you can obtain AIC, BIC, and logLik, I want to impose 
> the AICC penalty instead.
> 
> After fitting a model.  Is there a way of obtaining the 
> "npar" and then assigning it to a variable?
> 
> Essentially, I want to then write a simple function to add 
> the AICC penalty to (-2*logLik).
> 
> Thank you in advance for any help,
> Tom Volscho
> 
> ************************************        
> Thomas W. Volscho
> Graduate Student
> Dept. of Sociology U-2068
> University of Connecticut
> Storrs, CT 06269
> Phone: (860) 486-3882
> http://vm.uconn.edu/~twv00001
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html



From renaud.lancelot at cirad.fr  Sun Oct 31 20:06:57 2004
From: renaud.lancelot at cirad.fr (Renaud Lancelot)
Date: Sun, 31 Oct 2004 21:06:57 +0200
Subject: [R] Obtaining fitted model information
In-Reply-To: <20041031183735.IENE15612.tomts16-srv.bellnexxia.net@JohnDesktop8300>
References: <20041031183735.IENE15612.tomts16-srv.bellnexxia.net@JohnDesktop8300>
Message-ID: <418537D1.3000106@cirad.fr>

With models estimated with lm, the number of parameters is obtained 
adding 1 to the rank of the fitted model (to account for the residuals 
variance). The number of parameters is found in logLik objects:

 > # example from ?lm
 > ctl <- c(4.17,5.58,5.18,6.11,4.50,4.61,5.17,4.53,5.33,5.14)
 > trt <- c(4.81,4.17,4.41,3.59,5.87,3.83,6.03,4.89,4.32,4.69)
 > group <- gl(2,10,20, labels=c("Ctl","Trt"))
 > weight <- c(ctl, trt)
 > lm.D9 <- lm(weight ~ group)
 >
 > # rank of the model
 > lm.D9$rank
[1] 2
 >
 > # loglik
 > logLik(lm.D9)
`log Lik.' -20.08824 (df=3)
 >
 > # number of parameters in the model
 > attr(logLik(lm.D9), "df")
[1] 3
 >
 > # AIC
 > AIC(lm.D9)
[1] 46.17648
 >
 > c(- 2 * logLik(lm.D9) + 2 * attr(logLik(lm.D9), "df"))
[1] 46.17648
 >
 > # AICc = AIC + 2 * k * (k + 1)/(n - k - 1)
 >
 > AICc_lm <- function(x){
+   n <- length(resid(x))
+   k <- attr(logLik(lm.D9), "df")
+   AIC(x) + 2 * k * (k + 1) / (n - k - 1)
+   }
 >
 > AICc_lm(lm.D9)
[1] 47.67648

Best regards,

Renaud


John Fox a ??crit :

> Dear Thomas,
> 
> To get the number of independent parameters in the lm object mod, you can
> use mod$rank, sum(!is.na(coef(mod)), or -- if there are no linear
> dependencies among the columns of the model matrix -- length(coef(mod)).
> 
> I hope this helps,
>  John
> 
> --------------------------------
> John Fox
> Department of Sociology
> McMaster University
> Hamilton, Ontario
> Canada L8S 4M4
> 905-525-9140x23604
> http://socserv.mcmaster.ca/jfox 
> -------------------------------- 
> 
> 
>>-----Original Message-----
>>From: r-help-bounces at stat.math.ethz.ch 
>>[mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Thomas 
>>W Volscho
>>Sent: Sunday, October 31, 2004 12:41 PM
>>To: r-help at stat.math.ethz.ch
>>Subject: [R] Obtaining fitted model information
>>
>>Dear list,
>>I am brand new to R  and using Dalgaard's (2002) book 
>>Introductory Statistics with R (thus, some of my terminology 
>>may be incorrect).
>>
>>I am fitting regression models and I want to use Hurvich and 
>>Tsai's AICC statistic to examine my regression models.  This 
>>penalty can be expressed as: 2*npar * (n/(n-npar-1)).
>>
>>While you can obtain AIC, BIC, and logLik, I want to impose 
>>the AICC penalty instead.
>>
>>After fitting a model.  Is there a way of obtaining the 
>>"npar" and then assigning it to a variable?
>>
>>Essentially, I want to then write a simple function to add 
>>the AICC penalty to (-2*logLik).
>>
>>Thank you in advance for any help,
>>Tom Volscho
>>
>>************************************        
>>Thomas W. Volscho
>>Graduate Student
>>Dept. of Sociology U-2068
>>University of Connecticut
>>Storrs, CT 06269
>>Phone: (860) 486-3882
>>http://vm.uconn.edu/~twv00001
>>
>>______________________________________________
>>R-help at stat.math.ethz.ch mailing list
>>https://stat.ethz.ch/mailman/listinfo/r-help
>>PLEASE do read the posting guide! 
>>http://www.R-project.org/posting-guide.html
> 
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
> 


-- 
Dr Renaud Lancelot, v??t??rinaire
C/0 Ambassade de France - SCAC
BP 834 Antananarivo 101 - Madagascar

e-mail: renaud.lancelot at cirad.fr
tel.:   +261 32 40 165 53 (cell)
         +261 20 22 665 36 ext. 225 (work)
         +261 20 22 494 37 (home)



From renaud.lancelot at cirad.fr  Sun Oct 31 20:16:46 2004
From: renaud.lancelot at cirad.fr (Renaud Lancelot)
Date: Sun, 31 Oct 2004 21:16:46 +0200
Subject: [R] Obtaining fitted model information
In-Reply-To: <20041031183735.IENE15612.tomts16-srv.bellnexxia.net@JohnDesktop8300>
References: <20041031183735.IENE15612.tomts16-srv.bellnexxia.net@JohnDesktop8300>
Message-ID: <41853A1E.2000204@cirad.fr>

Excuse me for the unfortunate cut and paste:

AICc_lm <- function(x){
   n <- length(resid(x))
   k <- attr(logLik(x), "df")    #### error in the previous message
   AIC(x) + 2 * k * (k + 1) / (n - k - 1)
   }

Best,

Renaud

John Fox a ??crit :

> Dear Thomas,
> 
> To get the number of independent parameters in the lm object mod, you can
> use mod$rank, sum(!is.na(coef(mod)), or -- if there are no linear
> dependencies among the columns of the model matrix -- length(coef(mod)).
> 
> I hope this helps,
>  John
[snip]
-- 
Dr Renaud Lancelot, v??t??rinaire
C/0 Ambassade de France - SCAC
BP 834 Antananarivo 101 - Madagascar

e-mail: renaud.lancelot at cirad.fr
tel.:   +261 32 40 165 53 (cell)
         +261 20 22 665 36 ext. 225 (work)
         +261 20 22 494 37 (home)



From ripley at stats.ox.ac.uk  Sun Oct 31 20:34:16 2004
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Sun, 31 Oct 2004 19:34:16 +0000 (GMT)
Subject: [R] Obtaining fitted model information
In-Reply-To: <418537D1.3000106@cirad.fr>
Message-ID: <Pine.LNX.4.44.0410311928500.12635-100000@gannet.stats>

The harder task is actually to get `n', not `npar'.

length(resid(x)) may or may not include missing values, depending on the 
na.action used, and will include observations with weight zero.
However, logLik's "lm" method returns an attribute "nobs" that is a better 
choice.

On Sun, 31 Oct 2004, Renaud Lancelot wrote:

> With models estimated with lm, the number of parameters is obtained 
> adding 1 to the rank of the fitted model (to account for the residuals 
> variance). The number of parameters is found in logLik objects:
> 
>  > # example from ?lm
>  > ctl <- c(4.17,5.58,5.18,6.11,4.50,4.61,5.17,4.53,5.33,5.14)
>  > trt <- c(4.81,4.17,4.41,3.59,5.87,3.83,6.03,4.89,4.32,4.69)
>  > group <- gl(2,10,20, labels=c("Ctl","Trt"))
>  > weight <- c(ctl, trt)
>  > lm.D9 <- lm(weight ~ group)
>  >
>  > # rank of the model
>  > lm.D9$rank
> [1] 2
>  >
>  > # loglik
>  > logLik(lm.D9)
> `log Lik.' -20.08824 (df=3)
>  >
>  > # number of parameters in the model
>  > attr(logLik(lm.D9), "df")
> [1] 3
>  >
>  > # AIC
>  > AIC(lm.D9)
> [1] 46.17648
>  >
>  > c(- 2 * logLik(lm.D9) + 2 * attr(logLik(lm.D9), "df"))
> [1] 46.17648
>  >
>  > # AICc = AIC + 2 * k * (k + 1)/(n - k - 1)
>  >
>  > AICc_lm <- function(x){
> +   n <- length(resid(x))
> +   k <- attr(logLik(lm.D9), "df")
> +   AIC(x) + 2 * k * (k + 1) / (n - k - 1)
> +   }
>  >
>  > AICc_lm(lm.D9)
> [1] 47.67648
> 
> Best regards,
> 
> Renaud
> 
> 
> John Fox a ??crit :
> 
> > Dear Thomas,
> > 
> > To get the number of independent parameters in the lm object mod, you can
> > use mod$rank, sum(!is.na(coef(mod)), or -- if there are no linear
> > dependencies among the columns of the model matrix -- length(coef(mod)).
> > 
> > I hope this helps,
> >  John
> >>-----Original Message-----
> >>From: r-help-bounces at stat.math.ethz.ch 
> >>[mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Thomas 
> >>W Volscho
> >>Sent: Sunday, October 31, 2004 12:41 PM
> >>To: r-help at stat.math.ethz.ch
> >>Subject: [R] Obtaining fitted model information
> >>
> >>Dear list,
> >>I am brand new to R  and using Dalgaard's (2002) book 
> >>Introductory Statistics with R (thus, some of my terminology 
> >>may be incorrect).
> >>
> >>I am fitting regression models and I want to use Hurvich and 
> >>Tsai's AICC statistic to examine my regression models.  This 
> >>penalty can be expressed as: 2*npar * (n/(n-npar-1)).
> >>
> >>While you can obtain AIC, BIC, and logLik, I want to impose 
> >>the AICC penalty instead.
> >>
> >>After fitting a model.  Is there a way of obtaining the 
> >>"npar" and then assigning it to a variable?
> >>
> >>Essentially, I want to then write a simple function to add 
> >>the AICC penalty to (-2*logLik).
> >>
> >>Thank you in advance for any help,
> >>Tom Volscho

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From renaud.lancelot at cirad.fr  Sun Oct 31 20:43:19 2004
From: renaud.lancelot at cirad.fr (Renaud Lancelot)
Date: Sun, 31 Oct 2004 21:43:19 +0200
Subject: [R] Obtaining fitted model information
In-Reply-To: <Pine.LNX.4.44.0410311928500.12635-100000@gannet.stats>
References: <Pine.LNX.4.44.0410311928500.12635-100000@gannet.stats>
Message-ID: <41854057.6020301@cirad.fr>

Prof Brian Ripley a ??crit :
> The harder task is actually to get `n', not `npar'.
> 
> length(resid(x)) may or may not include missing values, depending on the 
> na.action used, and will include observations with weight zero.
> However, logLik's "lm" method returns an attribute "nobs" that is a better 
> choice.
> 
[snip]

Thanks for pointing this.

Renaud


-- 
Dr Renaud Lancelot, v??t??rinaire
C/0 Ambassade de France - SCAC
BP 834 Antananarivo 101 - Madagascar

e-mail: renaud.lancelot at cirad.fr
tel.:   +261 32 40 165 53 (cell)
         +261 20 22 665 36 ext. 225 (work)
         +261 20 22 494 37 (home)



From ripley at stats.ox.ac.uk  Sun Oct 31 20:48:39 2004
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Sun, 31 Oct 2004 19:48:39 +0000 (GMT)
Subject: [R] Obtaining fitted model information
In-Reply-To: <Pine.LNX.4.44.0410311928500.12635-100000@gannet.stats>
Message-ID: <Pine.LNX.4.44.0410311941390.12635-100000@gannet.stats>

On Sun, 31 Oct 2004, Prof Brian Ripley wrote:

> The harder task is actually to get `n', not `npar'.
> 
> length(resid(x)) may or may not include missing values, depending on the 
> na.action used, and will include observations with weight zero.
> However, logLik's "lm" method returns an attribute "nobs" that is a better 
> choice.

But only better, as it looks like it has fallen into the first trap.
AFAICS, if 'fit' is an lm fit, then

n = df.residual(fit) + fit$rank


Quick check:

x <- rnorm(10); x[2] <- NA
y <- rnorm(10); y[1] <- NA
w <- c(rep(1,9), 0)
fit <- lm(y ~x, weights = w, na.action=na.exclude)
df.residual(fit) + fit$rank  # 7, correct
attributes(logLik(fit)) # nall 9 nobs 9 df 3
# but AIC fails
length(resid(fit)) # 10

fit <- lm(y ~x, weights = w, na.action=na.omit)
df.residual(fit) + fit$rank  # 7, correct
attributes(logLik(fit)) # nall 7 nobs 7 df 3
length(resid(fit)) # 8




> 
> On Sun, 31 Oct 2004, Renaud Lancelot wrote:
> 
> > With models estimated with lm, the number of parameters is obtained 
> > adding 1 to the rank of the fitted model (to account for the residuals 
> > variance). The number of parameters is found in logLik objects:
> > 
> >  > # example from ?lm
> >  > ctl <- c(4.17,5.58,5.18,6.11,4.50,4.61,5.17,4.53,5.33,5.14)
> >  > trt <- c(4.81,4.17,4.41,3.59,5.87,3.83,6.03,4.89,4.32,4.69)
> >  > group <- gl(2,10,20, labels=c("Ctl","Trt"))
> >  > weight <- c(ctl, trt)
> >  > lm.D9 <- lm(weight ~ group)
> >  >
> >  > # rank of the model
> >  > lm.D9$rank
> > [1] 2
> >  >
> >  > # loglik
> >  > logLik(lm.D9)
> > `log Lik.' -20.08824 (df=3)
> >  >
> >  > # number of parameters in the model
> >  > attr(logLik(lm.D9), "df")
> > [1] 3
> >  >
> >  > # AIC
> >  > AIC(lm.D9)
> > [1] 46.17648
> >  >
> >  > c(- 2 * logLik(lm.D9) + 2 * attr(logLik(lm.D9), "df"))
> > [1] 46.17648
> >  >
> >  > # AICc = AIC + 2 * k * (k + 1)/(n - k - 1)
> >  >
> >  > AICc_lm <- function(x){
> > +   n <- length(resid(x))
> > +   k <- attr(logLik(lm.D9), "df")
> > +   AIC(x) + 2 * k * (k + 1) / (n - k - 1)
> > +   }
> >  >
> >  > AICc_lm(lm.D9)
> > [1] 47.67648
> > 
> > Best regards,
> > 
> > Renaud
> > 
> > 
> > John Fox a ??crit :
> > 
> > > Dear Thomas,
> > > 
> > > To get the number of independent parameters in the lm object mod, you can
> > > use mod$rank, sum(!is.na(coef(mod)), or -- if there are no linear
> > > dependencies among the columns of the model matrix -- length(coef(mod)).
> > > 
> > > I hope this helps,
> > >  John
> > >>-----Original Message-----
> > >>From: r-help-bounces at stat.math.ethz.ch 
> > >>[mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Thomas 
> > >>W Volscho
> > >>Sent: Sunday, October 31, 2004 12:41 PM
> > >>To: r-help at stat.math.ethz.ch
> > >>Subject: [R] Obtaining fitted model information
> > >>
> > >>Dear list,
> > >>I am brand new to R  and using Dalgaard's (2002) book 
> > >>Introductory Statistics with R (thus, some of my terminology 
> > >>may be incorrect).
> > >>
> > >>I am fitting regression models and I want to use Hurvich and 
> > >>Tsai's AICC statistic to examine my regression models.  This 
> > >>penalty can be expressed as: 2*npar * (n/(n-npar-1)).
> > >>
> > >>While you can obtain AIC, BIC, and logLik, I want to impose 
> > >>the AICC penalty instead.
> > >>
> > >>After fitting a model.  Is there a way of obtaining the 
> > >>"npar" and then assigning it to a variable?
> > >>
> > >>Essentially, I want to then write a simple function to add 
> > >>the AICC penalty to (-2*logLik).
> > >>
> > >>Thank you in advance for any help,
> > >>Tom Volscho
> 
> 

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From rbest1 at binghamton.edu  Sun Oct 31 22:15:04 2004
From: rbest1 at binghamton.edu (rbest1@binghamton.edu)
Date: Sun, 31 Oct 2004 16:15:04 -0500 (EST)
Subject: [R] Error Message: MCMCpack and coda
Message-ID: <2651.67.23.176.48.1099257304.squirrel@smail.binghamton.edu>

Hello All,

I'm trying to run a one-dimenional irt model using the packages MCMC and
coda on a rather large set of roll-call voting data with many missing
observations.  Here's a sample of the code:
Post10<-
MCMCirt1d (Italy10, burnin = 1000, mcmc=50000, thin=100, verbose=TRUE,
theta.constraints = list(V549=1, V443=-1))

The MCMCirt1d command seems to work fine, but when I try to summarize the
output I get the following error message(s):

summary(Post10)
Error: NA/NaN/Inf in foreign function call (arg 1)
In addition: Warning message:
Step size truncated due to divergence

My understanding is that this has something to do with the missing data,
though I don't know how to address this issue.  Any help would be greatly
appreciated.

Thank you,
Robin Best



From ripley at stats.ox.ac.uk  Sun Oct 31 23:11:27 2004
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Sun, 31 Oct 2004 22:11:27 +0000 (GMT)
Subject: [R] Error Message: MCMCpack and coda
In-Reply-To: <2651.67.23.176.48.1099257304.squirrel@smail.binghamton.edu>
Message-ID: <Pine.LNX.4.44.0410312207420.12975-100000@gannet.stats>

On Sun, 31 Oct 2004 rbest1 at binghamton.edu wrote:

> Hello All,
> 
> I'm trying to run a one-dimenional irt model using the packages MCMC and
> coda on a rather large set of roll-call voting data with many missing
> observations.  Here's a sample of the code:
> Post10<-
> MCMCirt1d (Italy10, burnin = 1000, mcmc=50000, thin=100, verbose=TRUE,
> theta.constraints = list(V549=1, V443=-1))
> 
> The MCMCirt1d command seems to work fine, but when I try to summarize the
> output I get the following error message(s):
> 
> summary(Post10)
> Error: NA/NaN/Inf in foreign function call (arg 1)
> In addition: Warning message:
> Step size truncated due to divergence
> 
> My understanding is that this has something to do with the missing data,
> though I don't know how to address this issue.  Any help would be greatly
> appreciated.

Your understanding is not correct:  NaN and Inf result from numeric 
overflow, divide by zero and similar.

When you get an error message, try traceback() to see where it came from.
This came from a `foreign', that is .C or .Fortran, call, passing in a NaN 
or Inf or NA, and probably one of the first two given the warning on 
divergence.  But without a tracebacl() we don't know which call.

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



