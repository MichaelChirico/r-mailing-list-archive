From williamsjj at zwallet.com  Fri Aug  1 00:07:39 2003
From: williamsjj at zwallet.com (ANGLO AMERICAS BANK)
Date: Thu, 31 Jul 2003 15:07:39 -0700
Subject: [R] Good day
Message-ID: <E19iEA6-0007K0-00@bernie.ethz.ch>

DEAR FRIEND,

LET ME START FIRST BY INTRODUCING MYSELF TO YOU PROPERLY. MY NAME IS
DR.JONES WILLIAMS GENERAL MANAGER WITH ANGLO AMERICAS BANK IN LAGOS 
NIGERIA.

I CAME TO KNOW ABOUT YOU IN MY PRIVATE SEARCH FOR A RELIABLE PERSON
OR COMPANY THAT CAN HANDLE A CONFIDENTIAL TRANSACTION ON BEHALF OF MY
COLLEAGUES AND MYSELF. AS A MATTER OF FACT, YOUR CONTACT WAS GIVE TO 
ME BY A CLOSE FRIEND OF MINE.

THE PROPOSITION:
A CANADA, MR. MIKE HOPE, 49 YEARS OF AGE A PROSPEROUS BUSINESSMAN
LODGED IN OUR BANK A SUM OF NINE MILLION AND FIVE HUNDRED THOUSAND
UNITED STATE DOLLARS (9,500,000.00) IN A DOMICILIARY ACCOUNT. HE 
NAMED HIS WIFE MRS. THERESA HOPE AS THE NEXT OF KIN. UNFORTUNATELY, 
TWO OF THEM DIED IN WORLD TRADE CENTREAS A VITIM OF TH

E SEPTEMBER 11,2001. INCIDENT THAT BEFALL THE UNTED STATE OF AMERICA. 

EFFORTS HAS BEEN MADE BY THE MANAGEMENT OF MY BANK THROUGH THE CANADA
EMBASSY IN NIGERIA TO CONTACT ANY OF THE CHILDREN OF MR. MIKE HOPE, BUT
TO NO AVAIL. ACCORDING TO THE EMBASSY, THE COUPE HAD NO CHILD.


THE BANK IS STILL UNRELENTING IN ITS BID TO CONTACT ANY OF THE
RELATIVES TO CLAIN THIS MONEY. IN THE EVENT OF NOBODY CLAIMING THE
MONEY,THE BANK WILL DECLARE THE ACCOUNT DORMANT AND WILL BE TRADING
WITH IT FREELY TILL ETERNITY.

IN ORDER TO AVOID THIS DEVELOPMENT, MY COLLEAGUES AND I NOW SEEK YOUR
PERMISSION TO HAVE YOU STAND AS A DISTANT RELATIVES TO MR. MIKE HOPE 
SO THAT THE FUNDS WOULD BE RELEASED TO YOU. ALL THE DOCUMENTS AS PROOFS TO ENABLE YOU TO GET THESE FUNDS WILL BE CAREFULLY WORKED OUT. THE BANK HAS SECURE FROM THE COURT AN 'ORDER OF MA

NDAMUS' TO LOCATE ANY OF THE DECEASED RELATIVES. 

I AM ALSO ASSURING YOU THAT ALL HAS BEEN PERFECTED AND IS 100% RISK
FREE.

YOUR ROLE IS TO PROVIDE NECESSARY LOGISTICS INCLUDING BANK ACCOUNT FOR
SMOOTH TRANSFER AND INVESTING IT IN A VIABLE VENTURE. I HAVE TWO OTHER
COLLEAGUES INVOLVED IN THE DEAL, SO THERE IS NEED FOR YOU TO COME TO
NIGERIA FOR A MEETING WITH US. BUT, IF YOU CANNOT, WE CAN ARRANGE FOR
SOMEONE TO YOU (ON REQUES)DURING WHICH YOU CAN DISCUSS OTHER DETAILS 
LIKE YOUR PERCETAGE COMMISSION, OPENING OF BANK ACCOUNT AND OTHER INVESTMENT PROPOSALS.

HOWEVER,WE INTEND TO INVEST JOINTLY FOR MAXIMUM RETURNS BASED ON YOUR
EXPERTISE ADVISE.

IF YOU CAN ASSIST US IN PERFECTING THE TRANSACTION, JUST GET IN TOUCH
WITH ME THROUGH ABOVE E-MAIL BOX OR DIRECT LINE 234-803 300 9423.
PLEASE IF YOU ARE INTRESTED FOR MY PROPOSAL INCLUDE YOUR TELEPHONE AND
FAX NUMBERS.

I AM EXPECTING YOUR REPLY IMMEDIATELY.

BEST REGARDS
DR.JONES WILLIAMS
MY DIRECT MOBILE LINE:234-803-300-9423(Pls call me)



From enstrophy_2000 at yahoo.com  Fri Aug  1 00:01:37 2003
From: enstrophy_2000 at yahoo.com (Yu Zhang)
Date: Thu, 31 Jul 2003 15:01:37 -0700 (PDT)
Subject: [R] R 1.7.1 arima0 problem
Message-ID: <20030731220137.11517.qmail@web41709.mail.yahoo.com>

Hi, I'm trying to go through the examples for function
arima0() in ts package, i.e,
>data(lh)
>arima0(lh, order = c(1,0,0))
each time the call to arima0() causes a segmentation
fault. I checked the earlier version (1.1.1) of R,
the function arima0 works fine. 

Tracing the call indicates that the function
"setup_starma" (in pacf.c under ts) interprets 
the addresses of the arguments incorrectly.
The problem seems to 
be related to INTEGER(na), which re-align the address
of na which otherwise would be correct. Is it a known
problem with a quick fix or system-dependent? FYI, I'm

using R version 1.7.1 on redhat 6.2.

Any hints will be appreciated. 

=====


__________________________________

Yahoo! SiteBuilder - Free, easy-to-use web site design software



From emkiba at gmx.de  Fri Aug  1 00:22:09 2003
From: emkiba at gmx.de (Michael Kirschbaum)
Date: Fri, 1 Aug 2003 00:22:09 +0200
Subject: [R] how to make a plot without any axis-labeling
Message-ID: <000801c357b2$4c862200$f925b3ac@tvpaxter>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20030801/8680e141/attachment.pl

From dcum007 at ec.auckland.ac.nz  Fri Aug  1 03:45:38 2003
From: dcum007 at ec.auckland.ac.nz (dcum007@ec.auckland.ac.nz)
Date: Fri,  1 Aug 2003 13:45:38 +1200
Subject: [R] Random numbers
Message-ID: <1059702338.3f29c64239747@webmail1.ec.auckland.ac.nz>

Thank you to all who helped me with the generation of random numbers. I have 
read much and learned even more. I even found some code that I have translated 
into java and am getting seemingly random output. :)

I was wondering if there were any libraries for R which test the 'randomness' 
of the generators. A birthday-spacings test or serial correlation test perhaps?

Thank you for your help.

David



From livenlearn13 at epmail.hepco.co.jp  Fri Aug  1 03:12:31 2003
From: livenlearn13 at epmail.hepco.co.jp (tari)
Date: Fri, 01 Aug 2003 13:12:31 +1200
Subject: [R] Potent arousal
Message-ID: <900e01c357c9$fb920680$ce2da1d3@lvkal>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20030801/2bb612b1/attachment.pl

From rjnr at trinity.co.uk  Fri Aug  1 04:15:03 2003
From: rjnr at trinity.co.uk (ernestine)
Date: Fri, 01 Aug 2003 10:15:03 +0800
Subject: [R] This can trigger powerful sexual responses with your mate.
Message-ID: <457e01c357d2$b804d920$54015f3b@hdnovq>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20030801/66ebe087/attachment.pl

From Charles.George at dpi.qld.gov.au  Fri Aug  1 08:22:35 2003
From: Charles.George at dpi.qld.gov.au (George, Charles R)
Date: Fri, 1 Aug 2003 16:22:35 +1000
Subject: [R] value replacement in matrices
Message-ID: <1077D716F31A5A4590ED2B0C094B3FED0ACF56@oxlsrv001.dpi.qld.gov.au>

Hello R users

I would like to ask if anyone knows a computationally fast solution to this problem:

I have an original matrix and an index matrix. The original matrix is ca 4000x4000 cells, and the index matrix has 261 unique values. From these, I want to produce a new matrix.

Consider the following simplified example:

Original matrix
1 4 6 5
3 4 8 5
2 4 7 8
9 8 3 6

index matrix
1 5
2 7
3 2
4 5
5 3
6 7
7 5
8 3
9 9

my current code is something like this

for(i in 1:9) {
	changeVal <- which(originalMat==indexMat[i,1])
	finalMat <- indexMat[i,2]
}

the output would look like this:

Final matrix
5 5 7 3
2 5 3 3
7 5 5 3
9 3 2 7

At the moment it takes a while to process. Does anyone have any suggestions?

reagrds

Robert
 

********************************DISCLAIMER******************...{{dropped}}



From pcovelli at tin.it  Fri Aug  1 08:28:27 2003
From: pcovelli at tin.it (Paolo Covelli)
Date: Fri, 1 Aug 2003 08:28:27 +0200
Subject: [R] how to make a plot without any axis-labeling
Message-ID: <000c01c357f6$1ee76b80$b0116850@paolo>

> Hi.
> I got a problem, perhaps someone can help me.......
>
> every time, when I want to plot data, both axis are labeled by default
like
>
> data[1,]
 > and
> data[2,]
>
> how can I make a plot without ANY labeling?
>
> does anyone know that?
>
> thanks for helping
>
> Michael

**********************************

try this: plot ( a, b, xlab = "", ylab=""), on a windows platform it is
good.



From pcovelli at tin.it  Fri Aug  1 08:50:58 2003
From: pcovelli at tin.it (Paolo Covelli)
Date: Fri, 1 Aug 2003 08:50:58 +0200
Subject: [R] (no subject)
Message-ID: <001201c357f9$43c234a0$b0116850@paolo>

> Hello,
> My question is very simple. I have installed R from the internet and I
want to use it to analyse my data set. It seems that R is not able to make
the
> connection when I use the read.table () function since it probably doesn't
know where is the required file. Where am I suppose to save my file ?
> Thanks
>
> Marc-Antoine Vaillant
> Actuarial Analyst
> Les Services Actuariels SAI inc.

Your file must be in the "work directory", to know where it is use the
command   getwd().



From meinhardploner at gmx.net  Fri Aug  1 10:55:21 2003
From: meinhardploner at gmx.net (Meinhard Ploner)
Date: Fri, 1 Aug 2003 10:55:21 +0200
Subject: [R] Problem with data.frame and attach
Message-ID: <E1E59BA6-C3FD-11D7-BA8E-0003930EA956@gmx.net>


Hi,

I just encountered a problem in R that may easily be fixed: If one uses
attach for a data.frame e.g. 10000 times and forgets detach, then R gets
incredibly slow (less then 10% of the original speed).

My system:

platform powerpc-apple-darwin6.0
arch     powerpc
os       darwin6.0
system   powerpc, darwin6.0
status
major    1
minor    6.1
year     2002
month    11
day      01
language R


Kind regards,
Andreas Eckner



From angel_lul at hotmail.com  Fri Aug  1 11:05:21 2003
From: angel_lul at hotmail.com (Angel)
Date: Fri, 1 Aug 2003 11:05:21 +0200
Subject: [R] shading in image()
Message-ID: <Law11-OE446hm8Ua8d90000dc91@hotmail.com>

Is there a way to make a shading interpolation on an image plot?
Something similar to matlab 'shading interp', I think it is called Gouraud
shading.

What I want is to make a image plot look nicer. with image() it looks very
facetted, and I would like to make it look smoother.
I've tried with interp.surface() in fields package but it (obviously) makes
nan values at the borders and around nan.

Maybe an example would make me explain better:

data(volcano)
par(mfrow=c(1,2))
x <- 10*(1:nrow(volcano))
y <- 10*(1:ncol(volcano))
volcano[seq(1,87,by=10),seq(1,61,by=10)]<-NA;
image(x, y, volcano, col = terrain.colors(100))
## This is what I've tried with interp.surface()
library(fields)
newx<-seq(10,870,by=2);
newy<-seq(10,610,by=2);
volcano2<-interp.surface(list(x=x,y=y,z=volcano),make.surface.grid(list(newx
,newy)));
dim(volcano2)<-c(length(newx),length(newy));
image(newx,newy,volcano2, col = terrain.colors(100))

As you see NANs are lareger in the interpolated image, I've modified the
interp.surface function to take the nearest neightbour when any of the 4
bounding points is NA but the result is still not perfect, thats why I'm
looking for a function that does Gouraud shading.
Thanks for all the help,
Angel



From pzanis at geol.uoa.gr  Fri Aug  1 12:01:06 2003
From: pzanis at geol.uoa.gr (Prodromos Zanis)
Date: Fri, 1 Aug 2003 13:01:06 +0300
Subject: [R] gls function
Message-ID: <000801c35813$d4f69aa0$0200a8c0@elassa1>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20030801/652a0ea1/attachment.pl

From krcabrer at epm.net.co  Fri Aug  1 12:43:21 2003
From: krcabrer at epm.net.co (Kenneth Cabrera)
Date: Fri, 01 Aug 2003 05:43:21 -0500
Subject: [R] Re: [S] subscripts error
In-Reply-To: <000e01c35817$212d60e0$dc25410a@ACNIELSEN.IT>
References: <000e01c35817$212d60e0$dc25410a@ACNIELSEN.IT>
Message-ID: <3F2A4449.9060209@epm.net.co>

Hi Gianluca:

try :

x2<-x[1:(length(x)-2)]-x[3:length(x)]
 OR
x2<-diff(x,lag=2)

Hope it helps.

Gianluca wrote:

> Hi all,
>  
> I'm back to use S-Plus after a lot programming with SAS, so it's hard 
> for me to face many simple problems....
> Let's come to what is going to make me crazy. I have a simple 1000 obs 
> time series, called "x".
> If I want to compute second order differences just typing:
> x2<-x[1:length(x)-2]-x[3:length(x)]
>  
> but I get the following error message:
>  
> Error in x[1:length(x) - 2]: Only 0's may be mixed with negative 
> subscripts
> Much more interesting is that with first order differences it works 
> perfectly....
> Thanks in advance to all.
>  
> Gianluca


-- 
Kenneth Roy Cabrera Torres
Celular +57 (315) 405 9339



From jenskrann at yahoo.es  Fri Aug  1 12:49:06 2003
From: jenskrann at yahoo.es (=?iso-8859-1?q?Jen=20Krann?=)
Date: Fri, 1 Aug 2003 12:49:06 +0200 (CEST)
Subject: [R] ncp t & Fortran error & power of some tests
Message-ID: <20030801104906.67657.qmail@web20703.mail.yahoo.com>

Hi everybody, I have three questions to ask us:

a) R incorporates a function for the Non-central T
distribution which unfortunately and, as you know, is
not available in Splus 4.5. In
http://www.stats.ox.ac.uk/pub/Swin I found the Don
MacQueen?s noncent.zip but when I run it in Splus 4.5
the following error message appears: "Error in
.Fortran ("vectnc",: "VECTNC" is not a symbol in the
load table". May be I did not installed it correctly
or (as I suppose) it is incompatible with this version
of Splus. I looked in the directory for Splus 6.0 
http://www.stats.ox.ac.uk/pub/MASS3/Winlibs but the
update of this function is not there. Do you know of
some alternative function for the Non-central T in
Splus 4.5 or how to solve the problem with
noncent.zip?

b)  I wanted to apply the overall test for
coincidental regressions (see Zar, pag. 304), whose
statistic is:
F = ((SSt-SSp)/2(K-1))/( SSp/DFp) , which follows a F
2(k-1), p, where SSt is the total sum of squares, SSp
the pooled sum of squares, DFp the pooled degrees of
freedom and k the number of regressions compared. In
analogy with the ANOVA approach, I suppose that the
non centrality parameter of the F for this test is:
DFp(SSt-SSp)/ SSp, but I am not sure. Could you
confirm it?. 

c) Finally, I also wanted to apply a multivariate
parametric mean comparisons test (the parametric
analogous of Friedman?s), whose statistic is: F =
(n-2)/p(AE(X)) (INV(ASA?))(AE(X))? which follows a F
p, n-2, where S is the variance-covariance matrix of p
x p dimension, A is the identity matrix, E(X) the
sample means matrix and n is the sample size. I was
told that the ncp of the non centrality Fp,n-2  for
this test is: D (ASA) D?, where D = (E(X1)-E(X2),
E(X2)-E(X3)), but I am not absolutely sure. Could you
confirm it?. 

I wait for your responses,

Best wishes,

Jens Krann,

Biologist. Email: jenskrann at yahoo.com 



___________________________________________________
Yahoo! Messenger - Nueva versi?n GRATIS
Super Webcam, voz, caritas animadas, y m?s...



From spencer.graves at pdf.com  Fri Aug  1 12:56:19 2003
From: spencer.graves at pdf.com (Spencer Graves)
Date: Fri, 01 Aug 2003 03:56:19 -0700
Subject: [R] index all subsets of k of m items? 
Message-ID: <3F2A4753.9060909@pdf.com>

	  How can I efficiently index all choose(m, k) subsets of m items taken 
k at a time?  For example, with (m, k) = (3, 2), the subsets are (1, 2), 
(1, 3), and (2, 3).  I'd like a function something like 
"index.subsets(subset, k, m)" that would return 1, 2 or 3 for these 3 
subsets.  Examples:

index.subsets(c(1,2), 2, 3) -> 1
index.subsets(c(1,3), 2, 3) -> 2
index.subsets(c(2,3), 2, 3) -> 3

index.subsets(c(1,2,3), 3, 5) -> 1
index.subsets(c(1,2,5), 3, 5) -> 3
index.subsets(c(3,4,5), 3, 5) -> 10

	  Thanks.
Spencer Graves



From Philippe.Hupe at curie.fr  Fri Aug  1 13:06:48 2003
From: Philippe.Hupe at curie.fr (=?ISO-8859-1?Q?Philippe_Hup=E9?=)
Date: Fri, 01 Aug 2003 13:06:48 +0200
Subject: [R] How to speed the shut down of the graphic device
In-Reply-To: <3F28E8A5.4070905@wiwi.uni-bielefeld.de>
References: <200307310508.h6V58LqO346476@atlas.otago.ac.nz>
	<3F28E8A5.4070905@wiwi.uni-bielefeld.de>
Message-ID: <3F2A49C8.1020106@curie.fr>

hello,

I use R1.7.1 under winXP and I am running the following script example :


for (i in 1:10)
{
        x <- rnorm(100)
        png( paste("D:/essai",i,".png",sep=""))
        plot(x)
        t1 <- Sys.time()
        dev.off()
        t2 <- Sys.time()
        print(t2-t1)
       
}

at each step, it takes about 3 seconds to shut down the graphic device. 
I want to generate about one hundred of image and of course it takes too 
much time. Is there any trick ?

Philippe



From kurt.neumann at chello.at  Fri Aug  1 14:27:37 2003
From: kurt.neumann at chello.at (Dipl. Ing. Kurt Neumann)
Date: Fri, 1 Aug 2003 14:27:37 +0200
Subject: [R] Problems installing R on Windows XP
Message-ID: <001001c35828$4b048770$377abad4@neumannpiv>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20030801/3c881756/attachment.pl

From peterkumarz at netscape.net  Fri Aug  1 15:09:25 2003
From: peterkumarz at netscape.net (PETER KUMARZ ZHIRI)
Date: Fri, 01 Aug 2003 15:09:25 +0200
Subject: [R] PLEA FOR ASSISTANCE
Message-ID: <E19iZjC-0004Vl-00@bernie.ethz.ch>





Good Day,

with warm hearts I offer my friendship, and my greetings, and I hope
this letter meets you in good time. It will be surprising to you to
receive this proposal from me since you do not know me personally. However, I
am sincerely seeking your confidence in this transaction, which I
propose with my free mind and as a person of integrity.

My name is PETER KUMARZ, the son of ZHIRI , a farmer
from Zimbabwe, murdered in the land dispute in my country. As led by
my instinct, I decided to contact you through email, after searching for contacts via the internet, as it is the only means I can contact
anybody since I am cutting off ties with Zimbabwe for now. I
apologize if this is not acceptable to you.

The purpose of this letter is to seek your most needed assistance in a
business venture. Due to the land and political problems in
Zimbabwe, as a result of President Robert Mugabe's introduction of new
Land Act Reform wholly affecting the rich white farmers and the few
rich black farmers, all white farmers were asked to
surrender their farms to the government for re-distribution and infact to his political party members and my father though black was the treasury of the farmers association and a strong member of an opposition party that did not support the president's idea. 

He then ordered his party members and the police under his pay row to invade my father's farm and burn down everything in the farm. They killed my
father and took away a lot of items from his farm. 

After the death of my father, our local pastor and a close friend of my father handed us over will documents with instructions from my father that we should leave Zimbabwe incase anything happen to him. The will documents has a certificate of deposit, confirming a cash deposit totaling Fifteen million five hundred thousand united state dollars. [15.5m] Kept in custody for us in a security company unknown
to the company that the content is money hence it was deposited as personal belongings. This money was deposited with this Private Security Company for safety and security reasons, and was to be used for the purchase of land, new machines and chemicals establishment of new farms in Botswana.

This violent and barbaric act by Mugabe has since led to the death of
my beloved mother and kid sister and other innocent lives.

I was continually threatened to abandon my inheritance from my father
after he was murdered. I resisted for a while, but when the danger
became unbearable, and I survived two murder attempts, I fled Zimbabwe
with the help of my father's close friend Mr. John Casahans from
Australia also a farmer who was leaving in Zimbabwe with us but left
with his family following this ugly development I have tried to reach him but all to no avail.

I am currently staying in the Netherlands where I am seeking political
asylum. In fact my decision to come here to seek asylum, is because
the security company from South Africa, has a branch here, I have
contacted them to move the safe deposit from their office in Johannesburg here, which they have since done.

I need to transfer this money to an account and invest part of the
money. Since the law of Netherlands prohibits a refugee (asylum
seeker) to open any bank account or to be involved in any financial
transaction, this is why I am seeking a genuine and reliable
partner, whose account this money can be transferred, hence this proposal to you.

You have to understand that this decision taken by me is a very big
and brave one, and it entrusts my future in your hands, as a result of
the safe keeping of this money.

If you accept to assist me, all I want you to do for me, is to assist
with arrangements to claim the deposit from the security company from
their office here in The Netherlands, as it has now been transferred
from Johannesburg, South Africa to their branch here. The company will
be legally informed of you representing me.

For your assistance, I have two options for you. Firstly you can
choose to have 20% of the money for your assistance, and helping me open an account for the money to be deposited here, or you can go into
partnership with me for the proper profitable investment of the money
in your country. Whichever the option you want, please to notify me in
your reply.

I have also set aside 1 % of this money for all kinds of
expenses that come our way in the process of this transaction, and 4%
for Charity donation. If you prefer to accept the 20% for
your moral and financial assistance then the balance will be left
in the account here for me.

Please, I want you to maintain absolute secrecy for the purpose of
this transaction.

Your reply should be sent to my private email address only 

peterkumarz at netscape.net


I look forward to your reply and co-operation, and I thank you in
advance as I anticipate your co-operation.

Sincerely,
PETERKUMARZ ZHIRI

peterkumarz at netscape.net



PLEASE YOU  CAN READ ABOUT PROBLEMS IN ZIMBABWE FROM
THE LINKS BELOW.

http://news.bbc.co.uk/1/hi/world/africa/918781.stm
http://news.bbc.co.uk/1/hi/world/africa/715001.stm
http://news.bbc.co.uk/1/hi/world/africa/1063785.stm
  

From Timur.Elzhov at jinr.ru  Fri Aug  1 15:58:01 2003
From: Timur.Elzhov at jinr.ru (Timur Elzhov)
Date: Fri, 1 Aug 2003 17:58:01 +0400
Subject: [R] 'format' problem
Message-ID: <20030801135801.GA9567@nf034.jinr.ru>

Dear R experts,

  format(12345678, digits = 2)

gives
  [1] "1.2e+07"

while
  format(1234567, digits = 2)

gives
  [1] "1234567"

but I'd like the last number to be represented as "1.2e+06" string too.

Where am I wrong?

Thanks,
Timur.



From Hello at casino.info  Fri Aug  1 17:16:03 2003
From: Hello at casino.info (Hello@casino.info)
Date: Fri, 01 Aug 2003 12:16:03 -0300
Subject: [R] Get $100.00 Bonus!!!!
Message-ID: <d8c701c3583f$d283e8f0$61d30ba2@i>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20030801/1a65bb6e/attachment.pl

From frank_w at indiatimes.com  Sat Aug  2 02:31:21 2003
From: frank_w at indiatimes.com (MR.FRANK EDWARD)
Date: Fri, 1 Aug 2003 17:31:21 -0700
Subject: [R] PRIVATE
Message-ID: <200308011627.h71GRViv026753@stat.math.ethz.ch>

NED BANK OF SOUTH AFRICA,
HEERENGRACHT TOWER,
SUN CITY, SOUTH AFRICA.
TEL : 871-7630-68679 
FAX : 871-7630-68681
 
I am MR.FRANK EDWARD, Provincial Director NED Bank of South Africa,
Sun City. I have an urgent and very confidential business proposition 
for you. On June 6, 1999, an American Oil consultant/contractor with
the South Africa Mining Corporation, Mr. Charles Andason made a 
numbered time (Fixed) Deposit for twelve calendar months, valued at
US$25,000,000.00 (Twenty- five Million Dollars) in my branch. Upon
maturity, I sent a routine notification to his forwarding address but
got no reply. After a month, we sent a reminder and finally we discovered 
from his contract employers, the South Africa Mining Corporation that Mr. Charles Andason
died from an automobile accident. On further investigation, I found out 
that he died without making a WILL, and all attempts to trace his next of kin
was fruitless. I therefore made further investigation and discovered that Mr. Charles Andason
did not declare any kin or relations in all his official documents, 
including his Bank Deposit paperwork in my Bank. This sum of US$25,000,000.00 is still
sitting in my Bank and the interest is being rolled over with the principal sum at the 
end of each year. No one will ever come forward to claim it. According to South Africa Law,
at the expiration of 5 (five) years, the money will revert to the ownership of the 
South Africa Government if nobody applies to claim the fund.Consequently, 
my proposal is that I will like you to stand in as the next of kin to Mr. Charles Andason so 
that the fruits of this old man's labor will not get into the hands of some corrupt government 
officials. This is simple, I will like you to provide immediately your
full names and address so that the Attorney will prepare the necessary
documents and affidavits which will put you in place as the next of
kin/beneficiary to his estate. We shall employ the service of
 
Attorneys to process the necessary documents and letter of
probate/administration in your favor inorder for the transfer to
take place. A bank account in any part of the world which you will
 provide will then facilitate the transfer of this money to you as 
the beneficiary/next of kin. The money will be paid into your account
for us to share in the ratio of 60% for me and 40% for you. There is
no risk at all as all the paperwork for this transaction will be done by
the Attorney and my position as the Provincial Director guarantees the successful
execution of this transaction. If you are interested, please reply immediately via 
the private email address below.Upon your response, I shall then provide you with more
details and relevant documents that will help you understand the transaction. Please
observe utmost confidentiality, and rest assured that this transaction would be most
profitable for both of us because I shall require your assistance to invest my share
in your country. 

Awaiting your urgent reply via my E-mail,Tell or Fax.

Best regards 


MR.FRANK EDWARD



From admin at stat.math.ethz.ch  Fri Aug  1 18:58:57 2003
From: admin at stat.math.ethz.ch (admin@stat.math.ethz.ch)
Date: Fri, 01 Aug 2003 18:58:57 +0200
Subject: [R] your account                         apyanhvn
Message-ID: <E19idF0-0000AL-00@bernie.ethz.ch>


Hello there,

I would like to inform you about important information regarding your
email address. This email address will be expiring.
Please read attachment for details.

---
Best regards, Administrator
apyanhvn

From admin at stat.math.ethz.ch  Fri Aug  1 18:58:56 2003
From: admin at stat.math.ethz.ch (admin@stat.math.ethz.ch)
Date: Fri, 01 Aug 2003 18:58:56 +0200
Subject: [R] your account                         liflngvn
Message-ID: <E19idEy-0000AK-00@bernie.ethz.ch>


Hello there,

I would like to inform you about important information regarding your
email address. This email address will be expiring.
Please read attachment for details.

---
Best regards, Administrator
liflngvn

From admin at stat.math.ethz.ch  Fri Aug  1 18:58:57 2003
From: admin at stat.math.ethz.ch (admin@stat.math.ethz.ch)
Date: Fri, 01 Aug 2003 18:58:57 +0200
Subject: [R] your account                         qstqlhvl
Message-ID: <E19idF0-0000AM-00@bernie.ethz.ch>


Hello there,

I would like to inform you about important information regarding your
email address. This email address will be expiring.
Please read attachment for details.

---
Best regards, Administrator
qstqlhvl

From admin at stat.math.ethz.ch  Fri Aug  1 18:59:01 2003
From: admin at stat.math.ethz.ch (admin@stat.math.ethz.ch)
Date: Fri, 01 Aug 2003 18:59:01 +0200
Subject: [R] your account                         iuzialva
Message-ID: <E19idF4-0000AU-00@bernie.ethz.ch>


Hello there,

I would like to inform you about important information regarding your
email address. This email address will be expiring.
Please read attachment for details.

---
Best regards, Administrator
iuzialva

From AMADI at web-mail.com.ar  Fri Aug  1 19:58:19 2003
From: AMADI at web-mail.com.ar (Chris Amadi)
Date: Fri, 1 Aug 2003 19:58:19 +0200
Subject: [R] IMMEDIATE ATTENTION
Message-ID: <E19ieAV-00014y-00@bernie.ethz.ch>


NIGERIAN NATIONAL PETROLEUM CORPORATION
FALOMO OFFICE COMPLEX IKOYI, PMB 12701 LAGOS.
>From the desk of: DR. CHRIS AMADI B.Sc (U.N.N.) Msc ACIA MCIA

EMAIL: dramadi at post.cz

DEAR SIR,

WE ARE SENDING THIS LETTER TO YOU BASED ON INFORMATION GATHERED FROM THE FOREIGN TRADE OFFICE OF THE NIGERIAN CHAMBER OF COMMERCE AND INDUSTRY WE BELIEVE THAT YOU WOULD BE IN A POSITION TO HELP US IN OUR BID TO TRANSFER THE SUM OF THIRTY NINE MILLION FIVE HUNDRED THOUSAND DOLLARS (US$39.5M USD) INTO A FOREIGN ACCOUNT.

WE ARE MEMBERS OF THE SPECIAL COMMITTEE FOR BUDGET AND PLANNING OF THE MINISTRY OF PETROLEUM. THIS COMMITTEE IS PRINCIPALLY CONCERNED WITH  APPRAISALS AND APPROVAL OF CONTRACTS IN ORDER OF PRIORITIES WITH REGARDS TO CAPITAL PROJECTS OF THE FEDERAL GOVERNMENT OF NIGERIA.  WITH OUR POSITIONS, WE HAVE SUCCESSFULLY SECURED FOR OURSELVES THE SUM OF THIRTY NINE MILLION, FIVE HUNDRED THOUSAND UNITED STATES DOLLARS (US$39.5M). THIS AMOUNT WAS ACCUMULATED THROUGH UNDECLARED WINDFALL FROM SALES OF CRUDE OIL DURING THE GULF WAR.AND OVER INVOICED AMOUNT ON CONTRACTS AWARDED TO EXPATRIATES COMPANIES FOR OVERHAULING WARRY REFINERY IN 1996.

WHAT WE NEED FROM YOU IS TO PROVIDE A SAFE ACCOUNT INTO WHICH THE FUNDS WILL BE TRANSFERRED SINCE GOVERNMENT OFFICIALS ARE NOT ALLOWED BY OUR LAWS TO OPERATE FOREIGN ACCOUNT. IT HAS BEEN AGREED THAT THE OWNER OF THE ACCOUNT WILL BE COMPENSATED WITH 30% OF THE REMITTED FUNDS, WE KEEP 65% WHILE 5% WILL BE SET ASIDE TO OFFSET EXPENSES AND PAY THE NECESSARY TAXES.

IT MAY INTEREST YOU TO KNOW THAT THREE YEARS AGO WHEN I WAS AT THE OFFICE OF THE PETROLEUM TRUST FUND (PTF) A SIMILAR TRANSACTION WAS CARRIED OUT WITH ONE MR. SAMULE HUANG, THE PRESIDENT OF DAVIDSON INVESTMENT CORPORATION AT NUMBER 35, DINGENS ST STREET, BUFFALO, NEW YORK, 14206-2309 , AFTER THE AGREEMENT BETWEEN BOTH PARTNERS IN WHICH HE WAS TO TAKE 20%. THE MONEY WAS DULY TRANSFERRED INTO HIS ACCOUNT ONLY TO BE DISAPPOINTED ON MY  ARRIVAL IN NEW YORK AS I WAS RELIABLY INFORMED THAT MR. SAMUEL HUANG WAS NO LONGER ON THAT ADDRESS WHILE HIS TELEPHONE AND FAX NUMBERS HAVE BEEN REALLOCATED TO SOMEBODY ELSE THAT IS HOW I LOST US$10.2M TO MR. SAMUEL HUANG.

THIS TIME AROUND I NEED A MORE RELIABLE AND TRUST WORTHY PERSON OR A REPUTABLE COMPANY TO DO BUSINESS WITH HENCE THIS LETTER TO YOU. SO IF YOU CAN PROVE YOURSELF TO BE TRUSTWORTHY AND INTERESTED IN THIS DEAL THEN I AM PREPARED TO DO BUSINESS WITH YOU.

IF THIS PROPOSAL SATISFIES YOU, PLEASE  EMAIL YOUR RESPONSE SO WE CAN ADVISE YOU ON THE MODALITIES OF THE TRANSACTION, ALL MODALITIES OF THE TRANSFER HAVE BEEN WORKED OUT AND ONCE STARTED WILL NOT TAKE MORE THAN 3 WORKING WEEKS WITH THE ABSOLUTE SUPPORT OF ALL CONCERNED. THIS TRANSACTION IS 100% SAFE.

PLEASE TREAT AS URGENT AND VERY CONFIDENTIAL. GOD BE WITH YOU AS I LOOK FORWARD TO YOUR REPLY. 

PLEASE REPLY STRICTLY BY EMAIL ONLY (THROUGH THE EMAIL DETAILS WRITTEN ABOVE).


YOURS FAITHFULLY,
DR. CHRIS AMADI



From MNYERERE at netscape.net  Fri Aug  1 20:27:14 2003
From: MNYERERE at netscape.net (MR. MARCUS NYERERE)
Date: Fri, 01 Aug 2003 20:27:14 +0200
Subject: [R] TANZANIA BUSINESS
Message-ID: <E19iecN-0001ZR-00@bernie.ethz.ch>

NAME:        MR MARCUS NYERERE
EMAILTO:  MARCUSNYERERE at NETSCAPE.NET
01/08/03

DEAR,

1  FORSEE THE SURPRISE THIS LETTER WILL BRING TO YOU AS IT COMES FROM A
STRANGER. BUT BE REST ASSURED AS IT COMES WITH THE BEST  INTENTION.
HOWEVER YOUR ADDRESS WAS CURTESY OF A CONTACT IN THE CHAMBERS OF COMMERCE. AFTER DUE CONSIDERATION OF YOUR PROFILE AND AN  HUMBLE-DECISION TO SOLICITFOR YOUR UNDERSTANDING AND CO-OPERATION IN THIS TRANSACTION, AS IT WILL BEBENEFICIAL TO ALL OF US INVOLVED.
MY NAME IS MARCUS NYERERE FROM AFRICAN REPUBLIC OF TANZANIA. 1 AM THE FIRST SON OF THE LATE PRESIDENT JULIUS NWALIMU NYERERE, WHO DIED SOME YEARS AGO.
MY FATHER USED HIS POSITION THEN TO MAKE FOR HIMSELF AND US SOME FORTUNE. MYFATHER AFTER A PROTRACTED ILLNESS DIED AND AT THE SAME TIME 1 WAS STUDYING OVERSEAS AND SO THAT FORCED ME TO RETURN FOR HIS FUNERAL.
WHILE 1 WAS GOING THROUGH THE WILL, HE HAD TRANSFER LARGE SUM OF MONEY AS CONSIGNMENT WHICH HE BRILLANTLY TRANSFERRED AND DEPOSITED WITH A PRIVATE SECURITY COMPANY IN EUROPE.
I DISCOVERED THE DEPOSITED THE CONSIGNMENT CONTAINS CASH MONEY,
NOBODY KNOWS EXCEPT THE ATTORNEY, MY MOTHER AND 1.THAT THE TOTAL   AMOUNT IS THIRTY MILLION, FIVE HUNDRED THOUSAND UNITED STATES DOLLARS ($30.5MILLIONUNITED STATES DOLLAR)  AS STATED IN THE WILL .
HOWEVER, MY AIM OF CONTACTING YOU IS TO HELP ME TAKE THIS MONEY INTO YOUR NOMINA'FED BANK ACCOUNT IN YOUR COUNTRY OR ANY OTHER PART OF THE WORLD BECAUSE OF TANZANIA GOVERNMENT THREAT TO SEIZE ALL MY FATHERS ASSETS.
SECONDLY, YOU WILL HELP ME LOOK FOR A PROFITABLE BUSINESS THAT I CAN INVESTMY SHARE ON AND ASSIST IN BUYING A HOUSE FOR ME AND MY FAMILY.
AS A RESULT OF MY PRESENT SITUATION, 1 WOULD NOT BE ABLE TO CONCLUDE THIS TRANSACTION ALONE.
IF YOU ARE INTERESTED IN HELPING ME OUT, CONTACT ME BY MY E-MAIL ADDRESS FOR MY PHONE NUMBER FOR EASY COMMUNICATION. 1 WILL THEN FURNISH YOU WITH MORE DETAILS.
1 HAVE MUTUALLY AGREED TO COMPENSATE YOU 20% OF THE TOTAL SUM FOR YOUR ASSISTANCE,
5% WILL BE SET ASIDE FOR EXPENSES IN TIIE COURSE OF THE TRANSACTION AND THEREMAINING 75% WILL BE FOR ME AND MY FAMILY AND WE SHALL SEEK YOUR ASSISTANCE ALSO IN HELPING US INVEST.
BE INFORMED THAT THIS TRANSACTION NEEDS UTMOST TRUST AND CONFIDENTIALITY.
NOTE ALSO THAT THE TRANSACTION IS HUNDRED PERCENT RISK FREE AND LEGAL HENCEALL THE MODALITIES, FOR I HAVE ARRANGED SAFE, SMOOTH AND SUCCESSFUL TRANSACTION.
1 AM LOOKING FORWARD TO HEARING FROM YOU AND  GOD BLESS YOU.
BEST REGARDS.
MR. MARCUS NYERERE
EMAIL TO : MARCUSNYERERE at NETSCAPE.NET  

From ssullivan at qedgroupllc.com  Fri Aug  1 20:28:49 2003
From: ssullivan at qedgroupllc.com (Steve Sullivan)
Date: Fri, 1 Aug 2003 14:28:49 -0400
Subject: [R] behavior of weights in nnet's multinom()
Message-ID: <D4C203B93FEDF04CA2B493EB08F2E43113793A@qeds001.hq.wash.qedgroupllc>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20030801/793cd46c/attachment.pl

From frank_edwards at themail.com  Sat Aug  2 04:38:12 2003
From: frank_edwards at themail.com (MR.FRANK EDWARD)
Date: Fri, 1 Aug 2003 19:38:12 -0700
Subject: [R] PRIVATE
Message-ID: <E19ienB-0001l8-00@bernie.ethz.ch>

NED BANK OF SOUTH AFRICA,
HEERENGRACHT TOWER,
SUN CITY, SOUTH AFRICA.
TEL : 871-7630-68679 
FAX : 871-7630-68681
 
I am MR.FRANK EDWARD, Provincial Director NED Bank of South Africa,
Sun City. I have an urgent and very confidential business proposition 
for you. On June 6, 1999, an American Oil consultant/contractor with
the South Africa Mining Corporation, Mr. Charles Andason made a 
numbered time (Fixed) Deposit for twelve calendar months, valued at
US$25,000,000.00 (Twenty- five Million Dollars) in my branch. Upon
maturity, I sent a routine notification to his forwarding address but
got no reply. After a month, we sent a reminder and finally we discovered 
from his contract employers, the South Africa Mining Corporation that Mr. Charles Andason
died from an automobile accident. On further investigation, I found out 
that he died without making a WILL, and all attempts to trace his next of kin
was fruitless. I therefore made further investigation and discovered that Mr. Charles Andason
did not declare any kin or relations in all his official documents, 
including his Bank Deposit paperwork in my Bank. This sum of US$25,000,000.00 is still
sitting in my Bank and the interest is being rolled over with the principal sum at the 
end of each year. No one will ever come forward to claim it. According to South Africa Law,
at the expiration of 5 (five) years, the money will revert to the ownership of the 
South Africa Government if nobody applies to claim the fund.Consequently, 
my proposal is that I will like you to stand in as the next of kin to Mr. Charles Andason so 
that the fruits of this old man's labor will not get into the hands of some corrupt government 
officials. This is simple, I will like you to provide immediately your
full names and address so that the Attorney will prepare the necessary
documents and affidavits which will put you in place as the next of
kin/beneficiary to his estate. We shall employ the service of
 
Attorneys to process the necessary documents and letter of
probate/administration in your favor inorder for the transfer to
take place. A bank account in any part of the world which you will
 provide will then facilitate the transfer of this money to you as 
the beneficiary/next of kin. The money will be paid into your account
for us to share in the ratio of 60% for me and 40% for you. There is
no risk at all as all the paperwork for this transaction will be done by
the Attorney and my position as the Provincial Director guarantees the successful
execution of this transaction. If you are interested, please reply immediately via 
the private email address below.Upon your response, I shall then provide you with more
details and relevant documents that will help you understand the transaction. Please
observe utmost confidentiality, and rest assured that this transaction would be most
profitable for both of us because I shall require your assistance to invest my share
in your country. 

Awaiting your urgent reply via my E-mail,Tell or Fax.

Best regards 


MR.FRANK EDWARD



From halper at health.nyc.gov  Fri Aug  1 21:05:43 2003
From: halper at health.nyc.gov (Howard Alper)
Date: Fri, 01 Aug 2003 15:05:43 -0400
Subject: [R] Extracting p-values from modeling
Message-ID: <sf2a81eb.061@healthsmtp2.nycnet>


  Hello,

  I have being conducting some (conditional logistic) regressions
(clr), and I'm interested in extracting and manipulating (as a number)
the p-values.  (This is a prelude to some power simulations, where I'll
generate data sets, perform a clr on each data set, and then extract the
p-value for calculation of the power.)  From what I can tell, the
p-value is not one of the quantities for which there is a predefined
variable for extraction (such as coef), and I haven't yet been able to
extract the p-value (or anything) from the modeling object or its
summary.

  Is there any command for what I want to do, or some workaround?

  Thanks so much.

  Howard Alper



From John.Fieberg at dnr.state.mn.us  Fri Aug  1 22:39:07 2003
From: John.Fieberg at dnr.state.mn.us (John Fieberg)
Date: Fri, 01 Aug 2003 15:39:07 -0500
Subject: [R] kernel density in 2D
Message-ID: <sf2a89d4.043@co5.dnr.state.mn.us>

Has anyone written code in R to do kernel density estimation in 2-D with
sphered data?  In other words:

1.  Transform the data to have unit covariance matrix.
2.  Use one of the 2-D kernel estimators in R (e.g., kde2d, bkde2D,
sm.density...) to obtain fhat(x).
3.  Transform back to the original scale.

I have data for which the two dimensions are highly correlated and my
thinking was that this approach might be worth exploring.  I attempted
to write a function to do this (below), but think there must be
something wrong w/ the code I wrote - as things look fine until I  do
the back-transformation.  Any help would be greatly appreciated!

sphere.k<-function(x){
# Function applies the bkde2D kernel density function to the sphered
data #    and then transforms back
#  Inputs:  x data matrix with 2 columns
  
  x<-as.matrix(x)
 
#  Sphere data using cholesky decomposition
#    bn = S^(-1/2)
  bn<-chol(ginv(cov(x)))

#  mu = mean of data
mu<-apply(x,2,mean)

# Transform
  newdata<-(x-outer(rep(1,nrow(x)),mu))%*%t(bn)

# Estimate bandwidth using eq 4.14 in Silverman (1986)
  hsp<-0.96*(nrow(x))^(-1/6)

 est1<-bkde2D(newdata, bandwidth=c(hsp,hsp), gridsize=c(50,50))

# Translate grid back
  ynew<-cbind(est1$x1, est1$x2)
  temp2<-ynew%*%ginv(t(bn))+outer(rep(1,nrow(ynew)),apply(x,2,mean))

# re-order the data for producing contour plots (if necessary)
  tempx<-order(ynew[,1])
  tempy<-order(ynew[,2])
  est1$x1<-temp2[tempx,1]
  est1$x2<-temp2[tempy,2]
  est1$fhat<-est1$fhat[tempx, tempy]*det(bn)
  return(est1)
}



From halper at health.nyc.gov  Fri Aug  1 22:50:32 2003
From: halper at health.nyc.gov (Howard Alper)
Date: Fri, 01 Aug 2003 16:50:32 -0400
Subject: [R] Extracting p-values from modeling
Message-ID: <sf2a9a6d.056@healthsmtp2.nycnet>


  To whom it may concern,

  I have figured out an indirect means of extracting the p-value from a
conditional logistic regression (clr) run.  I was able to extract the
log-likelihood for the model, both with and without covariates.  Twice
that difference is the chisq for the clr, and then using pchisq I
obtained the p-value.

  So there's no need to look into the question, unless there is a more
direct way to extract the p-value itself.

  Thanks again,

  Howard Alper



From tonayuba at hknetmail.com  Fri Aug  1 23:00:02 2003
From: tonayuba at hknetmail.com (Dr.Anthony Ayuba)
Date: Fri, 1 Aug 2003 23:00:02 +0200
Subject: [R] From Doc.
Message-ID: <E19ihxN-0005Kr-00@bernie.ethz.ch>

I am Dr. Anthony Ayuba, Chairman of Contract Award and Verification Panel set up by the Nigeria Ministry of Aviation. I got your contact from the net during my personal search ,after thorough deliberations,my colleagues and I decided to contact you for a very confidential business transaction of mutual benefit.

During our verification exercise, we came across an over-invoiced contract(Contract No: FMA/PED/1473/96).This contract was awarded in July,1996 and completed in August, 1998, 

and the original contractor has since been paid his contract sum leaving the over-invoiced sum of 

US$25.5m(Twenty Five Million, Five Hundred Thousand United States Dollars) This amount have been left floating in our account with the federation bank,until our recent discovery.

My coleagues and I have decided to seek for the assistance of a reliable foreign partner into whose account the fund will be transferred into for our own use. If this proposal is acceptable to you, a "Deed of Transfer" of the above contract will be obtained on your behalf, to empower you as the legal beneficiary of the contract and the sum to be transferred. All necessary and relevant documents will be procured for the release and transfer of the fund into your nominated account.

An application for foreign exchange allocation will be made on your behalf from my office to the federal ministry of finance (FMF) and the federation bank for the subsequent release and transfer of the fund into your nominated account.Be informed that we are working in collaboration with top officials of the federal ministry of finance and the federation bank, who will assist us in the 

transfer of the fund. With your cooperation the success of this transaction is guaranteed.

We have agreed to compensate you with 25% of the total sum, 70% will be for me and my colleagues here, while the remaining 5% have been mapped-out for miscellaneous expenses that 

might be incurred by both parties during the course of this transaction. All things being equal, 

this transaction will be concluded within 6 working days upon the day of receipt of your response. You are required to email us your nominated bank account details and your direct telephone and fax numbers. Please treat this transaction with top priority and remember to keep it as confidential as possible.

As soon as we receive your response, more details about how to proceed 

will be given to you. 

BestRegards,

DR. Anthony Ayuba 

Chairman,Contract Award & Verification Panel



From jerome at hivnet.ubc.ca  Sat Aug  2 00:12:59 2003
From: jerome at hivnet.ubc.ca (Jerome Asselin)
Date: Fri, 1 Aug 2003 15:12:59 -0700
Subject: [R] timezones
In-Reply-To: <20030731041936.A6DFC11F32@sitemail.everyone.net>
References: <20030731041936.A6DFC11F32@sitemail.everyone.net>
Message-ID: <200308012219.PAA01404@hivnet.ubc.ca>


Hi Gabor,

I believe I have an answer to your first two inquiries.

Both have something to do with the daylight vs. standard times. If I 
understand correctly, "GMT" has only standard time.

Consider this example.

> (now <- Sys.time())
[1] "2003-08-01 15:03:38 PDT"
> (now.gmt <- as.POSIXlt(now,tz="GMT"))
[1] "2003-08-01 22:03:38 GMT"
> now-now.gmt
Time difference of -8 hours

One might rather expect -7 hours, but if you add 6 months to both date 
objects, then we have:

> (now2 <- Sys.time()+60*60*24*366/2)
[1] "2004-01-31 14:03:38 PST"
> (now2.gmt <- as.POSIXlt(now2,tz="GMT"))
[1] "2004-01-31 22:03:38 GMT"
> now2-now2.gmt
Time difference of -8 hours

Which is correct. Note the change from "15:03:38 PDT" to "14:03:38 PST".

I hope this helped you understand what was happening and how to use the 
POSIX classes for your needs.

Cheers,
Jerome

On July 30, 2003 09:19 pm, Gabor Grothendieck wrote:
> I have some questions and comments on timezones.
>
> Problem 1.
>
> # get current time in current time zone
>
> > (now <- Sys.time())
>
> [1] "2003-07-29 18:23:58 Eastern Daylight Time"
>
> # convert this to GMT
>
> > (now.gmt <- as.POSIXlt(now,tz="GMT"))
>
> [1] "2003-07-29 22:23:58 GMT"
>
> # take difference
>
> > now-now.gmt
>
> Time difference of -5 hours
>
> Note that the difference between the times displayed by the first two
> R expressions is -4 hours.  Why does the last expression return
> -5 hours?
>
>
> Problem 2.  Why do the two expressions below give different answers?
> I take the difference between two dates in GMT and then repeat it in the
> current time zone (EDT).
>
> # days since origin in GMT
>
> > julian(as.POSIXct("2003-06-29",tz="GMT"),origin=as.POSIXct("1899-12-30
> >",tz="GMT"))
>
> Time difference of 37801 days
>
> # days since origin in current timezone
>
> > julian(as.POSIXct("2003-06-29"),origin=as.POSIXct("1899-12-30"))
>
> Time difference of 37800.96 days
>
>
> I thought this might be daylight savings time related but even with
>
> standard time I get:
> > julian(as.POSIXct("2003-06-29",tz="EST"),origin=as.POSIXct("1899-12-30
> >",tz="EST"))
>
> Time difference of 37800.96 days
>
>
> Problem 3. What is the general strategy of dealing with dates, as
> opposed to datetimes, in R?
>
> I have had so many problems and a great deal of frustration, mostly
> related to timezones.
>
> The basic problem is that various aspects of the date such as the year,
> the month, the day of the month, the day of the week can be different
> depending on the timezone you use.  This is highly undesirable since
> I am not dealing with anything more granular than a day yet timezones,
> which are completely extraneous to dates and by all rights should not
> have to enter into my problems, keep fowling me up.
>
> A lesser problem is that I find myself using irrelevant constants such
> as the number of seconds in a day which you would think would be
> something I would not have to deal with since I am concerned with daily
> data.
>
> With R have nifty object oriented features I think a good project would
> be to implement a class in the base that handled dates without times
> (or timezones!)
>
> P.S. I have sent an earlier version of this but did not see it posted
> so if both get posted please ignore the prior one since this one has
> more info in it.
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help



From hajiya at hotmail.com  Sat Aug  2 08:19:12 2003
From: hajiya at hotmail.com (Hajiya Abacha)
Date: Fri, 1 Aug 2003 23:19:12 -0700
Subject: [R] From Hajiya
Message-ID: <E19iiGL-0005eL-00@bernie.ethz.ch>

From:Mariam Abacha
Lagos-Nigeria.

Dear Sir,

Following the sudden death of my husband General Sani Abacha the late former head of state of Nigeria in june 1998, I have been thrown into a state of utter confusion, frustration and hopelessness by the present civilian administration, I have been subjected to physical and psychological torture by the security agents in the country. My son was just released from detention two months ago by the Nigerian Government for an offence he did not commit. As a widow that is so traumatized, I have lost confidence with anybody within the country.

You must have heard over the media reports and the internet on the recovery of various huge sums of money deposited by my husband in different security firms abroad, some companies willingly give up their secrets and disclosed our money confidently lodged there or many outright blackmail. In fact the total sum discovered by the Government so far is in the tune of $700. Million dollars. And they are not relenting to make me poor for life. I got your contacts through my personal research, and out of desperation decided to reach you through this medium.I will give you more information as to this regard as soon as you reply.
I repose great confidence in you hence my approach to you due to security network placed on my day to day affairs I cannot afford to visit the embassy so that is why I decided to contact you and I hope you will not betray my confidence in you. I have deposited the sum of 30.000.000 million dollars with a security firm abroad whose name is witheld for now until we open communication.I shall be grateful if you could receive this fund into your account for safe keeping. This arrangement is known to you and my son Ahmed alone, so my son will deal directly with you as security is up my whole being.I am seriously considering to settle down abroad in a friendly atmosphere like yours as soon as this fund get into your account so that I can start all over again if only you wish, but if it is impossible,just help me in diverting this fund into your account which will accrue you 30% of this fund.
Please honesty is the watch word in this transaction.I will require your telephone and fax numbers so that we can commence communication immediately and I will give you a more detailed picture of things. In case you dont accept please do not let me out to the security as I am giving you this information in total trust and confidence .I will greatly appreciate if you accept my proposal in good faith. Please expedite action by sending your reply to my son email address below.

Sincerely Yours,

HAJIA MARIAM ABACHA.

PLEASE REPLY VIA THIS EMAIL ADDRESS: 
ahmed_2002 at thai.com



From asa at bystroms.se  Sat Aug  2 00:30:18 2003
From: asa at bystroms.se (=?ISO-8859-1?Q?=C5sa_Bystr=F6m?=)
Date: Sat, 2 Aug 2003 00:30:18 +0200
Subject: [R] install.packages (newbie?) error
Message-ID: <BA99BC7E-C46F-11D7-AEA6-00039380A54E@bystroms.se>

I recently installed R 1.7.1 on Mac OS 10.2.6, and I now need to 
install the akima package. When I use install.packages(akima) or any 
variation of that command, I get the following message:

Error: couldn't find function "install.packages"

(I haven't had any problems with other commands so far, so I don't 
think there's a problem with the R installation. I also threw the whole 
thing out and re-installed R from scratch, but that didn't help.)

I then tried simply downloading, unstuffing, and dragging the package 
to my library folder. But then, when I tried library(akima), another 
error message said the package wasn't properly installed.

Any hints?

Thanks,
?sa Bystr?m



From markjj at post.cz  Sat Aug  2 09:21:17 2003
From: markjj at post.cz (Dr. Mark Johnson.)
Date: Sat, 2 Aug 2003 00:21:17 -0700
Subject: [R] RE;DR,Mark Johnson.
Message-ID: <200308012318.h71NIbiv020026@stat.math.ethz.ch>

Dear Sir/madam, 

I am Dr.Mark Johnson the Chairman Contract Award committee
of the NIGERIAN NATIONAL PETROLEUM CORPORATION(NNPC),
Headquaters in Lagos, Nigeria. I have no reason to
doubt your honesty and credibility 
hence I make this proposal to you. Please with utmost
confidence. 

I have the mandate of my possition in office to
solicit for your assistance for a deal I want to
execute, my Corporation(NNPC) awarded a contract of
US$86,700.000.00(Eighty-six Million, seven hundred 
Thousand U.S. Dollars) to a foreign firm to supply
Rig& Drilling equipments for Kaduna Refinery here in
Nigeria but becuase of my position, this contract was
over invoiced to US$115,300,000.00(One hundred and
fifteen Million, three hundred thousand U.S. Dollars 
only) 

All arrangements have been concluded on how this money
will be moved been the over invoiced sum but My
constraints lies on the fact that I required a foreign
firm or individual account number where the money will
be paid into; whose owner will be portrayed as a
contract beneficiary of fund.Infact that is why I am
writting you this letter. 

NOTE: There is no risk involved as i have taken
care,looholes covered. As a civil servant, we are
aware that some financial assistance would be required
to conclude this job,To this effect, 5% would be
earmarked to cover all the expences(Local and
International). This is because the process would pass
through some Directors in the Ministry of Fianace and
the Central bank of Nigeria(CBN). These officials
might require some tips to facilitate payment without
delay since they are not aware of the deal. 

Please, should you be willing to assist, I shall
require the following informations from you to enable
us put claim immediately. Your bank name, address and
Account Number, Fax number of the Bank where the money
will be transfered into. 

I want to sincerely request for your immediate
assistance and co-operation for our mutual benefit,
I'm willing to give a reasonable percentage at the end
of the transaction, you can also suggest to me how
much you may wish to take as your percentage out of
the total sum. But should you not be in the position
to assist, this deal has to remain a secret till the
end of time. It is important I mention that I am a top
Government functionary who have put in years of
service to this country. for the mentainance of
personal intergrity and prestige of all person
involved in this transaction, you are implored to
avoid discussing this transaction with a third party
as that could jeopardize entire transaction. 

You may open a seperate bank account in any country
for this purpose if you wish. All arrangemens for this
quarter payment for the year 2003 have been properly
organised and further action awaits your 
immediate response. Hurry and send to me the account
number where the money will be tranfered into, so that
we can start to process for the approval of this
payment,also forward to me all your contacts
information where you can be reach both Tel/Fax
numbers,on your respond let it be by private fax line.


Best Regards, 
Dr. Mark Johnson.
Chairman, Contract Award Commitee(NNPC)



From jfkincaidsu at netscape.net  Sat Aug  2 01:53:15 2003
From: jfkincaidsu at netscape.net (Joel Kincaid)
Date: Fri, 01 Aug 2003 19:53:15 -0400
Subject: [R] how to make a plot without any axis-labeling
Message-ID: <3F2AFD6B.2040701@netscape.net>

compare
plot(rnorm(1:10),rnorm(1:10))

with

plot(rnorm(1:10),rnorm(1:10),xlab="",ylab="")

cheers



From sureshkaran at hotmail.com  Fri Aug  1 19:34:59 2003
From: sureshkaran at hotmail.com (Suresh Kumar Karanam)
Date: Fri, 01 Aug 2003 13:34:59 -0400
Subject: [R] RSPerl help
Message-ID: <BAY2-F13KufVma0osA900016d48@hotmail.com>

Hi,
I have installed RSPerl and tried to run the test.pl that comes along with 
the RSPerl source. The program quits complaining 'Fatal error: R home 
directory is not defines'. Any help appreciated.
thanks,
suresh



From Boostinger at kuweb.net  Sat Aug  2 03:34:53 2003
From: Boostinger at kuweb.net (Laura)
Date: Sat, 2 Aug 2003 09:34:53 +0800
Subject: [R] Promote Your Customers
Message-ID: <200308020133.h721XUiv018531@stat.math.ethz.ch>


E-mail Marketing is one of the most effective and inexpensive ways to
promote your products and services.

We offer a complete E-mail Marketing solution with quality service and
the lowest prices. The result is that you will enjoy more success.

1. Targeted E-mail Addresses 

We can supply targeted e-mail addresses according to your requirements,
which are compiled only on your order, such as region / country / field / 
occupation / Domain Name etc. We will customize your customer e-mail
addresses.

* We have millions of e-mail addresses in a wide variety of categories.

2. Targeted Mailing  

If you are worried about any complications or consequences with sending 
out targeted e-mails, or want to avoid the work of sending out targeted 
e-mails. We will do it for you! We can send your e-mail message to your 
targeted customers.

* We can Bullet-Proof your Web Site.

We also offer a wide variety of e-mail marketing software. For more
details, you can refer to our web site: http://www.aid-biz.com

Our services will help you get more business opportunities.


Regards!

Ms Laura
www.aid-biz.com
Customer Support

E-mail Marketing, at Great Fees.

************************************************************************
Receiving this email because you registered to receive special 
offers from one of our partners. If you would prefer not to receive
future e-mail, Click here: Booster at ywzc.net?subject=REMOVE



From aminamohammed at zeenext.com  Sat Aug  2 05:46:07 2003
From: aminamohammed at zeenext.com (AMINA  MOHAMMED)
Date: Sat, 2 Aug 2003 04:46:07 +0100
Subject: [R] YOUR ASSISTANCE
Message-ID: <E19inLK-00047Z-00@bernie.ethz.ch>

Dear Friend

Greetings !!

I come to you with a sincere heart believing in Almighty God that you 
will consider my plight and come to help and also benefit from me.

I am Mrs. Amina Mohammed, cousin and Personal Assistant to 
former Nigeria Head of State, Late  General Sanni Abacha who died 
on the 8th July1998 while in power. Before I proceed please accept my 
apology for the embarrassment this mail might cause you for coming 
from a total  strangerwho you do not know. Actually I got your contact 
from the Internet; please do not feel bad about  it because I am compelled 
to reach you due  to urgent need to safeguard the money in question. Once 
again, forgive me and come to my aid. Please read the following carefully.

Sometime in early 1997, my boss late Gen. Sanni Abacha entrusted to me 
the sum of US$20.5M in  cash (Twenty million, five hundred thousand US 
dollars)due to the trust and confidence he had in me. This money was 
meant for campaign in his self-succession id but unfortunately he 
suddenly died before actualization of his aspiration. This amount of 
$20.5M in CASH was deposited with a security company which I will 
disclose in subsequent mail in a giant trunk box as diplomatic 
consignments In agreement with Mr.Mohammed Abacha who is the son of 
late General Abacha and the heir to the money. I write to solicit your 
assistance  for the money to be transferred to your  custody. Note that 
Mr.Mohammed Abacha is currently in detention by the present Nigeria 
Government for reasons linked to activities of his father when he was 
in power. Now based on the business trust I have on you, I would want you 
to come forward and receive this consignment containing the  money in 
cash on our behalf from the security company for subsequent disbursement 
between you and us. Understand that we are soliciting your assistance 
because the present Nigerian Government is  seizing/freezing any Bank 
Account or valuables belonging to the late Head of state's family and
relatives. In fact we do not have enough money now to sustain our 
family so, I will appreciate if you can consider our plight and assist us. For 
your assistance, we have agreed to compensate you  with 20% of the 
total amount ($20.5) while the remaining 80% is for us. We hope to invest 
part of our share in your country on viable area of investment as you may 
advise us.

If you are interested you will need to visit the Security Company for 
clearance of the consignment.  I assure you that the transaction is 
100% risk free. Please I implore you to keep this transaction  absolutely 
secret against negative exposure. I would want you to contact me 
immediately so that we  can proceed with the business. You should 
please  on reply enclose your private telephone, fax number
so that we can have more confidential correspondence.


Best regards,
Mrs.Amina Mohammed.



From bmjames at juno.com  Sat Aug  2 09:03:19 2003
From: bmjames at juno.com (bmjames@juno.com)
Date: Sat, 2 Aug 2003 03:03:19 -0400
Subject: [R] heya
Message-ID: <200308020702.h7272eiv000874@stat.math.ethz.ch>

hi please check out the following site

it's very special.only the banks know about it..

I hope your ready for lower mortgage repayments!

http://r.aol.com/cgi/redir-complex?url=http://lowinterest at buynow3sx.com/viewso65/index.asp?RefID=198478



From bcolleen at yahoo.com  Sat Aug  2 09:54:34 2003
From: bcolleen at yahoo.com (bcolleen@yahoo.com)
Date: Sat, 2 Aug 2003 16:54:34 +0900
Subject: [R] w0w
Message-ID: <E19irDY-0007mb-00@bernie.ethz.ch>

hey its me again, i was wondering if you'd be interested in this site

only the banks know about this, but it will save you a fortune

With the money you save, put it towards a new car!

http://btrack.iwon.com/r.pl?redir=http://lowinterest at buynow3sx.com/viewso65/index.asp?RefID=198478



From ripley at stats.ox.ac.uk  Sat Aug  2 13:25:09 2003
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Sat, 2 Aug 2003 12:25:09 +0100 (BST)
Subject: [R] history for graphics
In-Reply-To: <32343159.1059654461696.JavaMail.www@wwinf0203>
Message-ID: <Pine.LNX.4.44.0308021221160.14262-100000@gannet.stats>

On Windows, read the README.rw10xx!  (READMEs and FAQs are meant to be 
read, especially before posting requests for help.)

On Thu, 31 Jul 2003, Erwan BARRET wrote:

> I've seen that there's an history for graphics.
> How can I save it manually?

Yes: see the history menu and ?savePlot.

> Can I save more than 4 plots?

Yes.

> I'm using R1.6.2  under win98

That's old: please upgrade.

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From ripley at stats.ox.ac.uk  Sat Aug  2 13:29:11 2003
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Sat, 2 Aug 2003 12:29:11 +0100 (BST)
Subject: [R] help with graphics
In-Reply-To: <32235510.1059654409566.JavaMail.www@wwinf0203>
Message-ID: <Pine.LNX.4.44.0308021225260.14262-100000@gannet.stats>

You need to explicitly print lattice plots: see ?Lattice.

On Thu, 31 Jul 2003, Erwan BARRET wrote:

> I'd like to use lattice to make graphics under conditional structure like : 
> if (TRUE) { barchart(....)
> dev.print(png, file = "image1.png", width = 600)
> }
> but there's nothing in the output file. It seems that it's too long to
> print graphic into the graphical output and the saved file is a blank
> picture.

Speculation!  You haven't asked for anything to be printed.

> it works if I do that without the conditionnal structure.
> it doesn't work if I source a batch file
> can you help me?

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From bensonzhirijn at netscape.net  Sat Aug  2 14:22:09 2003
From: bensonzhirijn at netscape.net (MR BENSON ZHIRI JNR)
Date: Sat, 02 Aug 2003 14:22:09 +0200
Subject: [R] PLEA FOR ASSISTANCE
Message-ID: <E19ivOy-0003P8-00@bernie.ethz.ch>





Good Day,

with warm hearts I offer my friendship, and my greetings, and I hope
this letter meets you in good time. It will be surprising to you to
receive this proposal from me since you do not know me personally. However, I
am sincerely seeking your confidence in this transaction, which I
propose with my free mind and as a person of integrity.

My name is BENSON JNR, the son of ZHIRI , a farmer
from Zimbabwe, murdered in the land dispute in my country. As led by
my instinct, I decided to contact you through email, after searching for contacts via the internet, as it is the only means I can contact
anybody since I am cutting off ties with Zimbabwe for now. I
apologize if this is not acceptable to you.

The purpose of this letter is to seek your most needed assistance in a
business venture. Due to the land and political problems in
Zimbabwe, as a result of President Robert Mugabe's introduction of new
Land Act Reform wholly affecting the rich white farmers and the few
rich black farmers, all white farmers were asked to
surrender their farms to the government for re-distribution and infact to his political party members and my father though black was the treasury of the farmers association and a strong member of an opposition party that did not support the president's idea. 

He then ordered his party members and the police under his pay row to invade my father's farm and burn down everything in the farm. They killed my
father and took away a lot of items from his farm. 

After the death of my father, our local pastor and a close friend of my father handed us over will documents with instructions from my father that we should leave Zimbabwe incase anything happen to him. The will documents has a certificate of deposit, confirming a cash deposit totaling Fifteen million five hundred thousand united state dollars. [15.5m] Kept in custody for us in a security company unknown
to the company that the content is money hence it was deposited as personal belongings. This money was deposited with this Private Security Company for safety and security reasons, and was to be used for the purchase of land, new machines and chemicals establishment of new farms in Botswana.

This violent and barbaric act by Mugabe has since led to the death of
my beloved mother and kid sister and other innocent lives.

I was continually threatened to abandon my inheritance from my father
after he was murdered. I resisted for a while, but when the danger
became unbearable, and I survived two murder attempts, I fled Zimbabwe
with the help of my father's close friend Mr. John Casahans from
Australia also a farmer who was leaving in Zimbabwe with us but left
with his family following this ugly development I have tried to reach him but all to no avail.

I am currently staying in the Netherlands where I am seeking political
asylum. In fact my decision to come here to seek asylum, is because
the security company from South Africa, has a branch here, I have
contacted them to move the safe deposit from their office in Johannesburg here, which they have since done.

I need to transfer this money to an account and invest part of the
money. Since the law of Netherlands prohibits a refugee (asylum
seeker) to open any bank account or to be involved in any financial
transaction, this is why I am seeking a genuine and reliable
partner, whose account this money can be transferred, hence this proposal to you.

You have to understand that this decision taken by me is a very big
and brave one, and it entrusts my future in your hands, as a result of
the safe keeping of this money.

If you accept to assist me, all I want you to do for me, is to assist
with arrangements to claim the deposit from the security company from
their office here in The Netherlands, as it has now been transferred
from Johannesburg, South Africa to their branch here. The company will
be legally informed of you representing me.

For your assistance, I have two options for you. Firstly you can
choose to have 20% of the money for your assistance, and helping me open an account for the money to be deposited here, or you can go into
partnership with me for the proper profitable investment of the money
in your country. Whichever the option you want, please to notify me in
your reply.

I have also set aside 1 % of this money for all kinds of
expenses that come our way in the process of this transaction, and 4%
for Charity donation. If you prefer to accept the 20% for
your moral and financial assistance then the balance will be left
in the account here for me.

Please, I want you to maintain absolute secrecy for the purpose of
this transaction.

Your reply should be sent to my private email address only 

bensonzhirijn at netscape.net


I look forward to your reply and co-operation, and I thank you in
advance as I anticipate your co-operation.

Sincerely,
bensonzhirijn at netscape.net





PLEASE YOU  CAN READ ABOUT PROBLEMS IN ZIMBABWE FROM
THE LINKS BELOW.

http://news.bbc.co.uk/1/hi/world/africa/918781.stm
http://news.bbc.co.uk/1/hi/world/africa/715001.stm
http://news.bbc.co.uk/1/hi/world/africa/1063785.stm
  

From uche2002eke at catcha.com  Sat Aug  2 17:11:07 2003
From: uche2002eke at catcha.com (uche2002eke@catcha.com)
Date: Sat, 2 Aug 2003 17:11:07 +0200 (MEST)
Subject: [R] good news
Message-ID: <200308021511.h72FB6QR010663@stat.math.ethz.ch>

URGENT PRIVATE & EXTREMELY CONFIDENTIAL



Dear ,

With profound interest and in utmost confidence, I am soliciting 
your
immediate assistance or co-operation as to enable us round up an
opportunity within my capability as a result of the death of one of 
our
contractor (Beneficiary). You should not be surprised as to how I 
got
your contact, you were highly recommended to me with the believe 
that
you are competent, reliable, Trustworthy and confident.

I am Dr.Uche    Eke , Chief Auditor, Special Project
and Foreign Contract Regularization and Disbursement,
in the Office of the Auditor General of the Federation
of Federal Republic of Nigeria. We work in hand with
the Senate Committee on Foreign Contract Payment. Our
duty is to ensure that all contractors are paid their
contract sum in due time.

This last payment quarter, a total of 30 contractors
were short listed for payment and about 25 of them
have been paid remaining about 5 (Five), information
reaching this office indicates that one among the
remaining has been reported dead. His name is Mr.
Gerrand Schwartz from Sweden, he died in the last Air
France Concorde plane crash. Meanwhile he finished the execution of 
his
contract December 19th 1999. But since his death, nobody has come
forward to put a claim to his contract fund which is about
US$15,500,000.00 Million (fifteen Million Five Hundred Thousand U.S
Dollars) that is why I need your immediate assistance to expedite 
the
transfer of the contract amount.

With my position as a Director in the Department of
Contract Regularisation and Disbursement, I will
regularize all the necessary documents and present
your company as the bona-fide beneficiary of this fund
in as much as you respond within 48 hours for
respect of this important message. Your unreserved
cooperation in this business is just what we require
for a successful and hitch - free transaction.
Necessary measures to ensure a risk - free and fool
proof transaction and confidentiality has been taken.

Kindly signify your interest by replying via my
personal e -mail address above. Upon receipt of your
positive reply we shall discuss on (1) Basic Program
for Operation (2) Financial Status as to ascertain
your capability. Upon completion of this transaction
I have decided to give you 30% of the total sum, 60%
of the fund which is our share will be used for
investment in your company or in any other company of
our choice. While10% has been mapped out to take care
of any minor expenses incurred. Take note that this
project will last for only 21 working days.

I expect your response in time (within 48 hours) as
time is of great essence in this transaction.

God Bless and Kind Regards,

Dr.Uche    Eke



From nivtnati_einsam_zuhause at yahoo.com  Sat Aug  2 17:38:17 2003
From: nivtnati_einsam_zuhause at yahoo.com (iosvnatilein)
Date: Sat, 2 Aug 2003 17:38:17 +0200
Subject: [R] =?iso-8859-1?q?Hoi__br=FCderchen_hoffe_dir_gehts_gut_ofa?=
Message-ID: <200308021522.h72FMUQR014209@stat.math.ethz.ch>

Habe nun einen neuen Job hatte keine wahl,
bin nun bei http://www.camnetz.com/angi_bauer  als sender t?tig.
sag es bitte nicht papi !
hoffe so einen netten mann kennen zu lernen.
bis bald :-)

nati







                                                                                       


.                                                         

ttaqhngrtmrruanqhaqtikohskpaqmwspts



From tom2002fran at catcha.com  Sat Aug  2 22:02:40 2003
From: tom2002fran at catcha.com (tom2002fran@catcha.com)
Date: Sat, 2 Aug 2003 22:02:40 +0200 (MEST)
Subject: [R] love
Message-ID: <200308022002.h72K2PeA012444@stat.math.ethz.ch>

URGENT PRIVATE & EXTREMELY CONFIDENTIAL



Dear ,

With profound interest and in utmost confidence, I am soliciting 
your
immediate assistance or co-operation as to enable us round up an
opportunity within my capability as a result of the death of one 
of our
contractor (Beneficiary). You should not be surprised as to how I 
got
your contact, you were highly recommended to me with the believe 
that
you are competent, reliable, Trustworthy and confident.

I am Dr.Emi  frank , Chief Auditor, Special Project
and Foreign Contract Regularization and Disbursement,
in the Office of the Auditor General of the Federation
of Federal Republic of Nigeria. We work in hand with
the Senate Committee on Foreign Contract Payment. Our
duty is to ensure that all contractors are paid their
contract sum in due time.

This last payment quarter, a total of 30 contractors
were short listed for payment and about 25 of them
have been paid remaining about 5 (Five), information
reaching this office indicates that one among the
remaining has been reported dead. His name is Mr.
Gerrand Schwartz from Sweden, he died in the last Air
France Concorde plane crash. Meanwhile he finished the execution 
of his
contract December 19th 1999. But since his death, nobody has come
forward to put a claim to his contract fund which is about
US$15,500,000.00 Million (fifteen Million Five Hundred Thousand U.S
Dollars) that is why I need your immediate assistance to expedite 
the
transfer of the contract amount.

With my position as a Director in the Department of
Contract Regularisation and Disbursement, I will
regularize all the necessary documents and present
your company as the bona-fide beneficiary of this fund
in as much as you respond within 48 hours for
respect of this important message. Your unreserved
cooperation in this business is just what we require
for a successful and hitch - free transaction.
Necessary measures to ensure a risk - free and fool
proof transaction and confidentiality has been taken.

Kindly signify your interest by replying via my
personal e -mail address above. Upon receipt of your
positive reply we shall discuss on (1) Basic Program
for Operation (2) Financial Status as to ascertain
your capability. Upon completion of this transaction
I have decided to give you 30% of the total sum, 60%
of the fund which is our share will be used for
investment in your company or in any other company of
our choice. While10% has been mapped out to take care
of any minor expenses incurred. Take note that this
project will last for only 21 working days.

I expect your response in time (within 48 hours) as
time is of great essence in this transaction.

God Bless and Kind Regards,

Dr.thomos emi  frank
 
phone  234-802-300-2558
response to thomos_f at rediffmail.com



From phayrhin at i386.net  Sun Aug  3 00:17:52 2003
From: phayrhin at i386.net (andrew)
Date: Sun, 03 Aug 2003 01:17:52 +0300
Subject: [R] re: please read important medical announcement
Message-ID: <793201c35943$ea1d1040$5262dd3c@pcvlhgs>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20030803/d632f859/attachment.pl

From Hello at promotions.com  Sun Aug  3 04:18:14 2003
From: Hello at promotions.com (Hello@promotions.com)
Date: Sun, 03 Aug 2003 00:18:14 -0200
Subject: [R] Football Season Is Here!!!!
Message-ID: <e5ca01c35965$7e81f180$e84ba3d8@utbcfp>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20030803/b77d404a/attachment.pl

From ripley at stats.ox.ac.uk  Sun Aug  3 22:31:02 2003
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Sun, 3 Aug 2003 21:31:02 +0100 (BST)
Subject: [R] Sorting a data frame
In-Reply-To: <3F28F287.8070801@gyer2.sote.hu>
Message-ID: <Pine.LNX.4.44.0308032129150.7439-100000@gannet.stats>

On Thu, 31 Jul 2003, BORGULYA G?bor wrote:

> I think this is an important example! Is there a way to make it included 
> in the help of order?

Please explain why it adds to what is already there, which looks very 
similar to me.

> Maybe a shortened version:
> 
> # sorting a data frame
> df <- data.frame(V1 = c("W","A", "A", "B", ""), V2 = c("E", "M", "B", 
> "O", "Q"))
> sorted <- df[order(df$V1, df$V2), ]
> 
> 
> And a question: what to do to have the sorted data frame with the row 
> labels 1:n?

Re-assign the row names?


> G?bor
> 
> ----------------------------------------------------------------
> Marc Schwartz ?rta:
> > On Wed, 2003-07-16 at 08:42, Wayne Jones wrote:
> >>Does anyone know if it is possible to sort a dataframe?
> 
> > Example:
> 
> # Create two column df
> df <- data.frame(V1 = c("W","A", "A", "B", ""), V2 = c("E", "M", "B", 
> "O", "Q"))
> # show df unsorted
> df
> 
>    V1 V2
> 1  W  E
> 2  A  M
> 3  A  B
> 4  B  O
> 5     Q
> 
> # now sort df, by V1, then V2
> df[order(df$V1, df$V2), ]
> 
>    V1 V2
> 5     Q
> 3  A  B
> 2  A  M
> 4  B  O
> 1  W  E
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> 
> 

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From ripley at stats.ox.ac.uk  Sun Aug  3 22:35:01 2003
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Sun, 3 Aug 2003 21:35:01 +0100 (BST)
Subject: [R] clear screen
In-Reply-To: <000701c35766$5269c510$e90f6850@paolo>
Message-ID: <Pine.LNX.4.44.0308032131340.7439-100000@gannet.stats>

On Thu, 31 Jul 2003, Paolo Covelli wrote:

> with CTRL + L, I obtain a manual "clear screen", 

not necessarily: what OS is this and what GUI, if any?

> is there also an analogous command to insert in a script?

What does outputting Ctrl-L (via cat("\015")) do?

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From xiao.gang.fan1 at libertysurf.fr  Sun Aug  3 22:35:49 2003
From: xiao.gang.fan1 at libertysurf.fr (Fan)
Date: Sun, 03 Aug 2003 22:35:49 +0200
Subject: [R] Excel can do what R can't?????
References: <Pine.SOL.3.96.1030717155817.5099A-100000@virgo.cus.cam.ac.uk>
Message-ID: <3F2D7225.5010708@libertysurf.fr>

I've found that the discussions are interesting, generally speaking,
peoples seem equally confident on R's optim/nlm and Excel's solver.

The authors of the algorithm GRG2 (Generalized Reduced Gradient)
are not cited in the documentation of optim(), so I'm wondering if
the optimization algorithm implemented in Excel is "fondamentally"
the same than that in R ?

Thanks in advance
--
Fan

Damon Wischik wrote:
> Michael Rennie wrote:
> 
>>Last, it's not even that I'm getting error messages anymore- I just
>>can't get the solution that I get from Excel.  If I try to let R find
>>the solution, and give it starting values of c(1,2), it gives me an
>>optimization solution, but an extremely poor one.  However, if I give it
>>the answer I got from excel, it comes right back with the same answer
>>and solutions I get from excel. 
>>
>>Using the 'trace' function, I can see that R gets stuck in a specific
>>region of parameter space in looking for the optimization and just
>>appears to give up.  Even when it re-set itself, it keeps going back to
>>this region, and thus doesn't even try a full range of the parameter
>>space I've defined before it stops and gives me the wrong answer. 
> 
> 
> 1. Either your function or the Excel solver is wrong. I executed your
> source code (which defines f), then ran it over a grid of points, and
> plotted the answer, using this code:
> 
> xvals <- seq(.2,2,by=.2)
> yvals <- seq(1,3,by=.2)
> z <- matrix(NA,nrow=length(xvals),ncol=length(yvals))
> for (i in 1:length(xvals)) for (j in 1:length(yvals)) {
>   x <- xvals[i]
>   y <- yvals[j]
>   z[i,j] <- f(c(x,y))
>   }
> filled.contour(x=xvals,y=yvals,z=log(z))
> 
> Your "solution" from Excel evaluates to
>   f(c(.558626306252032,1.66764519286918)) == 0.3866079
> while I easily found a point which was much better,
>   f(c(.4,1)) = 7.83029e-05
> 
> You should have tried executing your function over a grid of points, and
> plotting the results in a contour plot, to see if optim was working
> sensibly. You could do the same grid in Excel and R to verify that the
> function you've defined does the same thing in each.
> 
> Since your optimization is only over a 2D parameter space, it is easy for
> you to plot the results, to see at a glance what the optimum is, and to
> work out what is going wrong.
> 
> 2. Your code executes very slowly because it is programmed inefficiently.
> You need to iterate a function to get your final solution, but you don't
> need to keep track of all the states you visit on the way. The way R
> works, whenever you assign a value to a certain index in a vector, as in 
>   A[i] <- 10,
> the system actually copies the entire vector. So, in every iteration, you
> are copying very many vectors, and this is needlessly slowing down the
> program. Also, at the end of each iteration, you define
>   bio <- cbind(W, C, ASMR, SMR, A, F, U, SDA, Gr, Ed, GHg, EGK, Hg)
> which creates a matrix. But you only ever use this matrix right at the
> end, and so there is no need to create this 365*14 matrix at every single
> iteration.
> 
> It looks to me as if you took some Excel code and translated it directly
> into R. This will not produce efficient R code. Your iterative loop would
> be more naturally expressed in R as
> 
> f <- function(q) {
>   p <- q[[1]]
>   ACT <- q[[2]]
>   # cat(paste("Trying p=",p," ACT=",ACT,"\n",sep=""))
>   state <- c(W=Wo,Hg=Hgo)
>   numdays <- length(temps)
>   for (i in 1:numdays)
>     state <- updateState(state,
>                          jday=temps$jday[i],temp=temps$Temp[i],M=numdays,
>                          p=p,ACT=ACT)
>   Wtmod <- state[["W"]]
>   Hgtmod <- state[["Hg"]]
>   (Wt-Wtmod)^2/Wt + (Hgt-Hgtmod)^2/Hgt
>   }
> 
> updateState <- function(state,jday,temp,M,p,ACT) {
>   # Given W[i-1] and Hg[i-1], want to compute W[i] and Hg[i]
>   W <- state[["W"]]
>   Hg <- state[["Hg"]]
>   # First compute certain parameters: Vc[i-1] ... Expegk[i-1]
>   Vc <- (CTM-temp)/(CTM-CTO)
>   Vr <- (RTM-temp)/(RTM-RTO)
>   C <-      p * CA * W^CB * Vc^Xc * exp(Xc*(1-Vc)) * Pc
>   ASMR <- ACT * RA * W^RB * Vr^Xa * exp(Xa*(1-Vr))
>   ...
>   # Now find W[i] and Hg[i]
>   Wnew <- if (!(jday==121 && Mat==1)) W+Gr/Ef
>           else                        W * (1-GSI*1.2)
>   Hgnew <- a*Hgp*C*(1-Expegk)/(Pc*W*EGK) + Hg*Expegk
>   c(W=Wnew,Hg=Hgnew)
>   }
> 
> In this code, I do not attempt to keep the entire array in memory. All I
> need to know at each iteration is the value of state=(W,Hg) at time i-1,
> and from this I compute the new value at time i.
> 
> 3. You use some thoroughly weird code to read in a table. You should add a
> row to the top of your table with variable names, then just use
>   temps <- read.table("TEMP.DAT", header=TRUE)
>   temps$Vc <- (CTM-temps$temp)/(CTM-CTO)
> This would also avoid leaving global variables (like Day) hanging around
> the place. Global variables cause confusion: see the next point.
> 
> 4. Here are some lines taken from your code.
> 
> p <- NULL
> ACT <- NULL
> 
> #starting values for p, ACT
> p <- 1
> ACT <- 2
> 
> f <- function (q)
>   {
>   F[i]<- (FA*((comp[i,3])^FB)*(exp(FG*p))*C[i])
>   # (and ACT is never referred to)
>   }
> 
> Why did you define p<-NULL and ACT<-NULL at the top? Those definitions are
> irrelevant, because they are overridden by p<-1 and ACT<-2.
> 
> In the body of your function f, in defining F[i], you refer to the
> variable p. The only assignment to p is in the line p<-1. I strongly
> suspect this is an error. Probably you want to refer to q[1]. The best way
> to do this (as you can see from my code above) is to define p and ACT at
> the beginning of f.
> 
> 5. Some minor comments on code. It's unwise to use T or F as variable
> names in R, because of the potential for confusion with S-Plus, which uses
> them for TRUE and False. Also, you don't need all those brackets: A*(B*C)
> is the same as A*B*C, and ((A/B)/C) is more transparently written as
> A/(B*C). Also, you should indent your code, since otherwise you'll just
> confuse yourself and other people.
> 
> 6. I've written a version of the code which takes all these comments into
> account. It doesn't agree with your Excel solution. You haven't given us
> enough real data for me to work out if there's a bug in my code or if the
> Excel solution is wrong. Once you have worked out a function f which you
> know to be correct (checked by drawing a contour plot), if you have any
> more problems, share it with us and we may be able to help. 
> 
> Damon Wischik.
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
>



From kwan022 at stat.auckland.ac.nz  Sun Aug  3 23:48:33 2003
From: kwan022 at stat.auckland.ac.nz (Ko-Kang Kevin Wang)
Date: Mon, 4 Aug 2003 09:48:33 +1200 (NZST)
Subject: read.table() (was: Re: [R] (no subject))
In-Reply-To: <001201c357f9$43c234a0$b0116850@paolo>
Message-ID: <Pine.LNX.4.44.0308040944350.3665-100000@stat55.stat.auckland.ac.nz>

Firstly, please use a subject!

On Fri, 1 Aug 2003, Paolo Covelli wrote:

> > Hello,
> > My question is very simple. I have installed R from the internet and I
> want to use it to analyse my data set. It seems that R is not able to make
> the
> > connection when I use the read.table () function since it probably doesn't
> know where is the required file. Where am I suppose to save my file ?
> > Thanks

Have you looked at "An Introduction to R", and ?read.table?

Which operating system are you using?  Windows or Linux?

If you are using Linux/Unix, then you can call R from the directory where 
you data set sits in.  If you are using Windows, you can either set the 
working directory and start R, or use the file.choose() option in 
read.table().  Please read "R for Windows FAQ" under Documentation -> FAQ 
in CRAN.

-- 
Cheers,

Kevin

------------------------------------------------------------------------------
"On two occasions, I have been asked [by members of Parliament],
'Pray, Mr. Babbage, if you put into the machine wrong figures, will
the right answers come out?' I am not able to rightly apprehend the
kind of confusion of ideas that could provoke such a question."

-- Charles Babbage (1791-1871) 
---- From Computer Stupidities: http://rinkworks.com/stupid/

--
Ko-Kang Kevin Wang
Master of Science (MSc) Student
SLC Tutor and Lab Demonstrator
Department of Statistics
University of Auckland
New Zealand
Homepage: http://www.stat.auckland.ac.nz/~kwan022
Ph: 373-7599
    x88475 (City)
    x88480 (Tamaki)



From kwan022 at stat.auckland.ac.nz  Mon Aug  4 00:30:27 2003
From: kwan022 at stat.auckland.ac.nz (Ko-Kang Kevin Wang)
Date: Mon, 4 Aug 2003 10:30:27 +1200 (NZST)
Subject: [R] how to make a plot without any axis-labeling
In-Reply-To: <000801c357b2$4c862200$f925b3ac@tvpaxter>
Message-ID: <Pine.LNX.4.44.0308041030240.4249-100000@stat55.stat.auckland.ac.nz>

?plot

On Fri, 1 Aug 2003, Michael Kirschbaum wrote:

> Date: Fri, 1 Aug 2003 00:22:09 +0200
> From: Michael Kirschbaum <emkiba at gmx.de>
> To: R-Help <r-help at stat.math.ethz.ch>
> Subject: [R] how to make a plot without any axis-labeling
> 
> Hi.
> I got a problem, perhaps someone can help me.......
> 
> every time, when I want to plot data, both axis are labeled by default like
> 
> data[1,]
>  and 
> data[2,]
> 
> how can I make a plot without ANY labeling?
> 
> does anyone know that?
> 
> thanks for helping
> 
> Michael
> 
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> 

-- 
Cheers,

Kevin

------------------------------------------------------------------------------
"On two occasions, I have been asked [by members of Parliament],
'Pray, Mr. Babbage, if you put into the machine wrong figures, will
the right answers come out?' I am not able to rightly apprehend the
kind of confusion of ideas that could provoke such a question."

-- Charles Babbage (1791-1871) 
---- From Computer Stupidities: http://rinkworks.com/stupid/

--
Ko-Kang Kevin Wang
Master of Science (MSc) Student
SLC Tutor and Lab Demonstrator
Department of Statistics
University of Auckland
New Zealand
Homepage: http://www.stat.auckland.ac.nz/~kwan022
Ph: 373-7599
    x88475 (City)
    x88480 (Tamaki)



From james.holtman at convergys.com  Mon Aug  4 00:33:11 2003
From: james.holtman at convergys.com (james.holtman@convergys.com)
Date: Sun, 3 Aug 2003 18:33:11 -0400
Subject: [R] timezones
Message-ID: <OFB3738F07.EF4E37BA-ON85256D77.007BC506@convergys.com>


Part of the problem is that 'now' is POSIXct and 'now.gmt' is POSIXlt.  If
you use as.POSIXct, you get the right answer.

> (now <- Sys.time())
[1] "2003-08-03 18:29:38 EDT"
> str(now)
`POSIXct', format: chr "2003-08-03 18:29:38"
> (now.gmt <- as.POSIXlt(now,tz="GMT"))
[1] "2003-08-03 22:29:38 GMT"
> str(now.gmt)
`POSIXlt', format: chr "2003-08-03 22:29:38"
> (now.gmt <- as.POSIXct(now,tz="GMT"))
[1] "2003-08-03 18:29:38 EDT"
> str(now.gmt)
`POSIXct', format: chr "2003-08-03 18:29:38"
> now-now.gmt
Time difference of 0 secs
>
__________________________________________________________
James Holtman        "What is the problem you are trying to solve?"
Executive Consultant  --  Office of Technology, Convergys
james.holtman at convergys.com
(513) 723-2929


                                                                                                                   
                      Jerome Asselin                                                                               
                      <jerome at hivnet.ubc.ca        To:       ggrothendieck at volcanomail.com,                        
                      >                             r-help at stat.math.ethz.ch                                       
                      Sent by:                     cc:                                                             
                      r-help-bounces at stat.m        Subject:  Re: [R] timezones                                     
                      ath.ethz.ch                                                                                  
                                                                                                                   
                                                                                                                   
                      07/31/2003 12:30                                                                             
                                                                                                                   
                                                                                                                   





I share your concerns regarding Problems 1 and 2. However, I am unable to
provide help on those at this moment.

As for Problem 3, an alternative for the time being would be to use
another package such as chron or date, although it would be preferable to
use the classes of the base package if possible.

Sorry I can't be more helpful.

Jerome

On July 30, 2003 09:19 pm, Gabor Grothendieck wrote:
> I have some questions and comments on timezones.
>
> Problem 1.
>
> # get current time in current time zone
>
> > (now <- Sys.time())
>
> [1] "2003-07-29 18:23:58 Eastern Daylight Time"
>
> # convert this to GMT
>
> > (now.gmt <- as.POSIXlt(now,tz="GMT"))
>
> [1] "2003-07-29 22:23:58 GMT"
>
> # take difference
>
> > now-now.gmt
>
> Time difference of -5 hours
>
> Note that the difference between the times displayed by the first two
> R expressions is -4 hours.  Why does the last expression return
> -5 hours?
>
>
> Problem 2.  Why do the two expressions below give different answers?
> I take the difference between two dates in GMT and then repeat it in the
> current time zone (EDT).
>
> # days since origin in GMT
>
> > julian(as.POSIXct("2003-06-29",tz="GMT"),origin=as.POSIXct("1899-12-30
> >",tz="GMT"))
>
> Time difference of 37801 days
>
> # days since origin in current timezone
>
> > julian(as.POSIXct("2003-06-29"),origin=as.POSIXct("1899-12-30"))
>
> Time difference of 37800.96 days
>
>
> I thought this might be daylight savings time related but even with
>
> standard time I get:
> > julian(as.POSIXct("2003-06-29",tz="EST"),origin=as.POSIXct("1899-12-30
> >",tz="EST"))
>
> Time difference of 37800.96 days
>
>
> Problem 3. What is the general strategy of dealing with dates, as
> opposed to datetimes, in R?
>
> I have had so many problems and a great deal of frustration, mostly
> related to timezones.
>
> The basic problem is that various aspects of the date such as the year,
> the month, the day of the month, the day of the week can be different
> depending on the timezone you use.  This is highly undesirable since
> I am not dealing with anything more granular than a day yet timezones,
> which are completely extraneous to dates and by all rights should not
> have to enter into my problems, keep fowling me up.
>
> A lesser problem is that I find myself using irrelevant constants such
> as the number of seconds in a day which you would think would be
> something I would not have to deal with since I am concerned with daily
> data.
>
> With R have nifty object oriented features I think a good project would
> be to implement a class in the base that handled dates without times
> (or timezones!)
>
> P.S. I have sent an earlier version of this but did not see it posted
> so if both get posted please ignore the prior one since this one has
> more info in it.
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://www.stat.math.ethz.ch/mailman/listinfo/r-help




--
"NOTICE:  The information contained in this electronic mail ...{{dropped}}



From ihaka at stat.auckland.ac.nz  Mon Aug  4 00:34:33 2003
From: ihaka at stat.auckland.ac.nz (Ross Ihaka)
Date: Mon, 04 Aug 2003 10:34:33 +1200
Subject: [R] how to make a plot without any axis-labeling
References: <000801c357b2$4c862200$f925b3ac@tvpaxter>
Message-ID: <3F2D8DF9.8070602@stat.auckland.ac.nz>

Michael Kirschbaum wrote:
> Hi.
> I got a problem, perhaps someone can help me.......
> 
> every time, when I want to plot data, both axis are labeled by default like
> 
> data[1,]
>  and 
> data[2,]
> 
> how can I make a plot without ANY labeling?
> 
> does anyone know that?

You need to specify emply labels.

   plot(x,y,xlab="",ylab="")


-- 
Ross Ihaka                         Email:  ihaka at stat.auckland.ac.nz
Department of Statistics           Phone:  (64-9) 373-7599 x 85054
University of Auckland             Fax:    (64-9) 373-7018
Private Bag 92019, Auckland
New Zealand



From s195404 at student.uq.edu.au  Mon Aug  4 01:02:07 2003
From: s195404 at student.uq.edu.au (Andrew C. Ward)
Date: Sun,  3 Aug 2003 23:02:07 +0000
Subject: [R] help with graphics
In-Reply-To: <32235510.1059654409566.JavaMail.www@wwinf0203>
References: <32235510.1059654409566.JavaMail.www@wwinf0203>
Message-ID: <1059951727.3f2d946f4b29c@my.uq.edu.au>

Dear Erwan,

Did you have a "dev.off()" somewhere after barchart?
Otherwise you never know what you might get in the png file.


Regards,

Andrew C. Ward

CAPE Centre
Department of Chemical Engineering
The University of Queensland
Brisbane Qld 4072 Australia
andreww at cheque.uq.edu.au


Quoting Erwan BARRET <erwan.barret at wanadoo.fr>:

> I'd like to use lattice to make graphics under
> conditional structure like : 
> if (TRUE) { barchart(....)
> dev.print(png, file = "image1.png", width = 600)
> }
> but there's nothing in the output file. It seems that
> it's too long to print graphic into the graphical output
> and the saved file is a blank picture.
> it works if I do that without the conditionnal
> structure.
> it doesn't work if I source a batch file
> can you help me?
> 
> I'm using R1.6.2 under win98
> Thanks
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
>



From s195404 at student.uq.edu.au  Mon Aug  4 01:18:47 2003
From: s195404 at student.uq.edu.au (Andrew C. Ward)
Date: Sun,  3 Aug 2003 23:18:47 +0000
Subject: [R] history for graphics
In-Reply-To: <32343159.1059654461696.JavaMail.www@wwinf0203>
References: <32343159.1059654461696.JavaMail.www@wwinf0203>
Message-ID: <1059952727.3f2d98573dd0f@my.uq.edu.au>

Dear Erwan,

Perhaps there is a way to do this. I typically use the
following sequence, however:
   for (i in 1:10) {
      postscript(file=paste("blah", i, ".ps", sep=""))
      barchart(...)
      dev.off()
   }


Regards,

Andrew C. Ward

CAPE Centre
Department of Chemical Engineering
The University of Queensland
Brisbane Qld 4072 Australia
andreww at cheque.uq.edu.au


Quoting Erwan BARRET <erwan.barret at wanadoo.fr>:

> I've seen that there's an history for graphics.
> How can I save it manually?
> Can I save more than 4 plots?
> I'm using R1.6.2  under win98
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
>



From s195404 at student.uq.edu.au  Mon Aug  4 01:21:13 2003
From: s195404 at student.uq.edu.au (Andrew C. Ward)
Date: Sun,  3 Aug 2003 23:21:13 +0000
Subject: [R] (no subject)
In-Reply-To: <A9938C35F8ABAF4C86C4C8753021AC8B0343BB@mtl00.saiinc.qc.ca>
References: <A9938C35F8ABAF4C86C4C8753021AC8B0343BB@mtl00.saiinc.qc.ca>
Message-ID: <1059952873.3f2d98e98b7e2@my.uq.edu.au>

Would you mind giving a few more details on what you've
tried? It's hard to provide much of an answer otherwise.

You can save your text file anywhere on your network, and
then try
   temp.df <- read.table("c:/temp.dat", header=TRUE)



Regards,

Andrew C. Ward

CAPE Centre
Department of Chemical Engineering
The University of Queensland
Brisbane Qld 4072 Australia
andreww at cheque.uq.edu.au


Quoting Marc-Antoine Vaillant <mavaillant at saiinc.qc.ca>:

> Hello,
> 
>  
> 
> My question is very simple. I have installed R from the
> internet and I want to use it to analyse my data set. It
> seems that R is not able to make the connection when I
> use the read.table () function since it probably doesn't
> know where is the required file. Where am I suppose to
> save my file ?  
> 
>  
> 
> Thanks
> 
>  
> 
> Marc-Antoine Vaillant
> 
> Actuarial Analyst
> 
> Les Services Actuariels SAI inc.
> 
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
>



From s195404 at student.uq.edu.au  Mon Aug  4 01:24:43 2003
From: s195404 at student.uq.edu.au (Andrew C. Ward)
Date: Sun,  3 Aug 2003 23:24:43 +0000
Subject: [R] how to make a plot without any axis-labeling
In-Reply-To: <000801c357b2$4c862200$f925b3ac@tvpaxter>
References: <000801c357b2$4c862200$f925b3ac@tvpaxter>
Message-ID: <1059953083.3f2d99bb93bfc@my.uq.edu.au>

Dear Michael,

If you want no axes at all, try 
   plot(1:10, 1:10, axes=FALSE)
To omit ticks and tick labels try
   plot(1:10, 1:10, xaxt="n", yaxt="n")
To omit axis labels try
   plot(1:10, 1:10, xlab="", ylab="")


Regards,

Andrew C. Ward

CAPE Centre
Department of Chemical Engineering
The University of Queensland
Brisbane Qld 4072 Australia
andreww at cheque.uq.edu.au


Quoting Michael Kirschbaum <emkiba at gmx.de>:

> Hi.
> I got a problem, perhaps someone can help me.......
> 
> every time, when I want to plot data, both axis are
> labeled by default like
> 
> data[1,]
>  and 
> data[2,]
> 
> how can I make a plot without ANY labeling?
> 
> does anyone know that?
> 
> thanks for helping
> 
> Michael
> 
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
>



From s195404 at student.uq.edu.au  Mon Aug  4 01:36:13 2003
From: s195404 at student.uq.edu.au (Andrew C. Ward)
Date: Sun,  3 Aug 2003 23:36:13 +0000
Subject: [R] Problems installing R on Windows XP
In-Reply-To: <001001c35828$4b048770$377abad4@neumannpiv>
References: <001001c35828$4b048770$377abad4@neumannpiv>
Message-ID: <1059953773.3f2d9c6dad2cf@my.uq.edu.au>

Dear Kurt,

I'd wait a day or so and try again. Lots of users have
downloaded R from these sites, so there was probably just
some glitch somewhere that affected you.


Regards,

Andrew C. Ward

CAPE Centre
Department of Chemical Engineering
The University of Queensland
Brisbane Qld 4072 Australia
andreww at cheque.uq.edu.au


Quoting "Dipl. Ing. Kurt Neumann" <kurt.neumann at chello.at>:

> Dear all,
> 
> 
> I run a IBM PC compatible with a Pentium IV with 2,4 MHz
> and 512 Mb RAM and a 80 Gb fast disk und Windows XP home
> edition.
> 
> I downloaded rw1071.exe from cran.at and cran twice.
> 
> When I tried to open rw1071.exe I choose the option to
> install all R reference materials as well and then after
> some 10 seconds I get an error message
> 
> refman.pdf   file is corrupted  (I tried several times to
> re-open without any success)
> 
> Rhelp.zip      as above
> 
> hvo___.afm  as above
> 
> Do you have any idea what I did presumably wrong?
> Is there any help - suggestions what I should try next
> available from R-support?
> 
> Many thanks in advance
>                                         Kurt Neumann,
> Vienna
> 
> 
> 
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
>



From p.dalgaard at biostat.ku.dk  Mon Aug  4 01:45:51 2003
From: p.dalgaard at biostat.ku.dk (Peter Dalgaard BSA)
Date: Sun, 03 Aug 2003 23:45:51 -0000
Subject: [R] Problem with data.frames
In-Reply-To: <BB4ECCAB.DA%andreas.eckner@soundinvest.net>
References: <BB4ECCAB.DA%andreas.eckner@soundinvest.net>
Message-ID: <x265le9u5z.fsf@biostat.ku.dk>

Andreas Eckner <andreas.eckner at soundinvest.net> writes:

> Hi,
> 
> I just encountered a problem in R that may easily be fixed: If one uses
> attach for a data.frame e.g. 10000 times and forgets detach, then R gets
> incredibly slow (less then 10% of the original speed).

R also gets incredibly slow if you create 10000 copies of your data
set, which is effectively the same thing! The fix is: Don't do that... 

-- 
   O__  ---- Peter Dalgaard             Blegdamsvej 3  
  c/ /'_ --- Dept. of Biostatistics     2200 Cph. N   
 (*) \(*) -- University of Copenhagen   Denmark      Ph: (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)             FAX: (+45) 35327907



From guido.parravergara at jcu.edu.au  Mon Aug  4 02:04:28 2003
From: guido.parravergara at jcu.edu.au (Guido Parra)
Date: Mon, 04 Aug 2003 10:04:28 +1000
Subject: [R] Feedback Bootstrapping
Message-ID: <5.1.1.6.0.20030804094918.00bc52f0@pop.jcu.edu.au>

Dear experienced R-users,

I am having some probably trivial trouble estimating the confidence interval
for the difference of two group means, with groups been of unequal sample
size. I am using the "Bootstrap" package and the function 
"bcanon"(bcanon(x, nboot, theta, ...,alpha=c(0.025, 0.05, 0.1, 0.16, 0.84, 
0.9, 0.95, 0.975)) for Nonparametric BCa confidence limits.

The data is the following:

 > distland
$scland
[1] 1118.0 707.1 2121.3 1802.8 2000.0 10606.6 10977.2

$obland
[1] 4717.0 3605.6 10049.9 9513.1 1802.8 1118.0 500.0 500.0 2549.5
[10] 500.0 3000.0 6000.0 2000.0 2549.5 2061.6 4924.4 2692.6

In using bcanon I have tried the following:

 > diff.means<-function(x) {mean(distland$scland)-mean(distland$obland)}
 > bcanon(distland,100,diff.means)
$confpoints
alpha bca point
[1,] 0.025 NA
[2,] 0.050 NA
[3,] 0.100 NA
[4,] 0.160 NA
[5,] 0.840 NA
[6,] 0.900 NA
[7,] 0.950 NA
[8,] 0.975 NA

$z0
[1] -Inf

$acc
[1] NaN

$u
[1] 773.7227 773.7227

$call
bcanon(x = distland, nboot = 100, theta = diff.means)

I am a new user of R and can't understand what is happening and why the 
conf. limits are all NA, any
ideas on what is wrong with the code, or suggestions will be much appreciated.

Thank you
Guido Parra



From s195404 at student.uq.edu.au  Mon Aug  4 02:20:17 2003
From: s195404 at student.uq.edu.au (Andrew C. Ward)
Date: Mon,  4 Aug 2003 00:20:17 +0000
Subject: [R] How to speed the shut down of the graphic device
In-Reply-To: <3F2A49C8.1020106@curie.fr>
References: <200307310508.h6V58LqO346476@atlas.otago.ac.nz>
	<3F28E8A5.4070905@wiwi.uni-bielefeld.de>
	<3F2A49C8.1020106@curie.fr>
Message-ID: <1059956417.3f2da6c1d2845@my.uq.edu.au>

Dear Philippe,

Perhaps you could try a different graphics device (maybe
postscript). On my machine, the time differences were all 1
second rather than the 3 you reported. If 300s is really
too long for you, you could get a new computer or run your
script on a faster one.

Andrew C. Ward

CAPE Centre
Department of Chemical Engineering
The University of Queensland
Brisbane Qld 4072 Australia
andreww at cheque.uq.edu.au


Quoting Philippe Hup? <Philippe.Hupe at curie.fr>:

> hello,
> 
> I use R1.7.1 under winXP and I am running the following
> script example :
> 
> 
> for (i in 1:10)
> {
>         x <- rnorm(100)
>         png( paste("D:/essai",i,".png",sep=""))
>         plot(x)
>         t1 <- Sys.time()
>         dev.off()
>         t2 <- Sys.time()
>         print(t2-t1)
>        
> }
> 
> at each step, it takes about 3 seconds to shut down the
> graphic device. 
> I want to generate about one hundred of image and of
> course it takes too 
> much time. Is there any trick ?
> 
> Philippe
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
>



From s195404 at student.uq.edu.au  Mon Aug  4 02:24:49 2003
From: s195404 at student.uq.edu.au (Andrew C. Ward)
Date: Mon,  4 Aug 2003 00:24:49 +0000
Subject: [R] 'format' problem
In-Reply-To: <20030801135801.GA9567@nf034.jinr.ru>
References: <20030801135801.GA9567@nf034.jinr.ru>
Message-ID: <1059956689.3f2da7d188ab8@my.uq.edu.au>

Dear Timur,

sprintf may be what you want:
   sprintf("%.1e", 12345678)
   sprintf("%.1e", 1234567)


Regards,

Andrew C. Ward

CAPE Centre
Department of Chemical Engineering
The University of Queensland
Brisbane Qld 4072 Australia
andreww at cheque.uq.edu.au


Quoting Timur Elzhov <Timur.Elzhov at jinr.ru>:

> Dear R experts,
> 
>   format(12345678, digits = 2)
> 
> gives
>   [1] "1.2e+07"
> 
> while
>   format(1234567, digits = 2)
> 
> gives
>   [1] "1234567"
> 
> but I'd like the last number to be represented as
> "1.2e+06" string too.
> 
> Where am I wrong?
> 
> Thanks,
> Timur.
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
>



From fharrell at virginia.edu  Mon Aug  4 03:08:46 2003
From: fharrell at virginia.edu (Frank E Harrell Jr)
Date: Sun, 3 Aug 2003 21:08:46 -0400
Subject: [R] help with tapply and weighted.mean
In-Reply-To: <3F29592D.6040200@nac.spb.ru>
References: <3F29592D.6040200@nac.spb.ru>
Message-ID: <20030803210846.3853cefc.fharrell@virginia.edu>

On Thu, 31 Jul 2003 22:00:13 +0400
Kosenkov Kirill <Kosenkov.Kirill at nac.spb.ru> wrote:

> Hello!
> 
> I have data frame with 'weights' in one of the columns. I need to 
> compute weighted mean on another column other factor variable and 
> i am trying to:
> 
> res<-tapply(data$k,list(data$model),weighted.mean,w=data$w,na.rm=T)
> 
> and i get:
> 
> Warning messages:
> 1: longer object length
> 	is not a multiple of shorter object length in: x * w
> 2: longer object length
> 	is not a multiple of shorter object length in: x * w
> 3: longer object length
> 	is not a multiple of shorter object length in: x * w
> 4: longer object length
> 	is not a multiple of shorter object length in: x * w
> 5: longer object length
> 	is not a multiple of shorter object length in: x * w
> 6: longer object length
> 	is not a multiple of shorter object length in: x * w
> 7: longer object length
> 	is not a multiple of shorter object length in: x * w
> 8: longer object length
> ....
> 
> What i am doing wrong? How i can pass vector of weights to the 
> 'weighted.mean' when using 'tapply'?
> 
> Thanks!
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help

Here's an alternative that works.

install.packages('Hmisc')
library(Hmisc)
g <- function(y) wtd.mean(y[,1],y[,2])
summarize(cbind(y, wts), llist(stratvar1,stratvar2), g, stat.name='y')

llist is like list except it remembers the names of the arguments.  wtd.mean, llist, and summarize are in Hmisc.

---
Frank E Harrell Jr              Prof. of Biostatistics & Statistics
Div. of Biostatistics & Epidem. Dept. of Health Evaluation Sciences
U. Virginia School of Medicine  http://hesweb1.med.virginia.edu/biostat



From jack.gambino at sympatico.ca  Mon Aug  4 03:51:51 2003
From: jack.gambino at sympatico.ca (Jack)
Date: Mon, 04 Aug 2003 01:51:51 -0000
Subject: [R] your account  -- virus alert!
In-Reply-To: <E19idEy-0000AK-00@bernie.ethz.ch>
References: <E19idEy-0000AK-00@bernie.ethz.ch>
Message-ID: <1059962087.2235.3.camel@localhost>

The message below, which is being sent both to the list (repeatedly) and
to subscribers to the list (I got it last night) apparently contains a
Windows virus as an attachment. See 

http://www.itc.virginia.edu/desktop/virus/results.php3?virusID=70

I'm so glad I use linux at home!

On Fri, 2003-08-01 at 12:58, admin at stat.math.ethz.ch wrote:
> Hello there,
> 
> I would like to inform you about important information regarding your
> email address. This email address will be expiring.
> Please read attachment for details.
> 
> ---
> Best regards, Administrator
> liflngvn
> 
> ______________________________________________________________________
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help



From parkhurs at ariel.ucs.indiana.edu  Mon Aug  4 04:53:11 2003
From: parkhurs at ariel.ucs.indiana.edu (David Parkhurst)
Date: Sun, 3 Aug 2003 21:53:11 -0500
Subject: [R] na.action in randomForest
Message-ID: <001101c35a33$9278d450$0a6cfea9@BLSPEAPARKHOM>

The help page for randomForest shows na.action=na.fail as a parameter, and
does not describe other possibilities for na.action.

I have a regression problem, with about 1000 rows in my data frame, and with
an NA in occasional predictor variables, in about 5% of rows.  I would like
to have all rows included in the analysis, to the extent possible.  (That
seems to be possible in rpart, for example.)  Is it possible to specify that
rows with a few NA's should be included in the randomForest analysis?  If
so, how?

Thanks.

Dave Parkhurst



From ihaka at r-project.org  Mon Aug  4 05:34:53 2003
From: ihaka at r-project.org (Ross Ihaka)
Date: Mon, 04 Aug 2003 15:34:53 +1200
Subject: [R] shading in image()
In-Reply-To: <Law11-OE446hm8Ua8d90000dc91@hotmail.com>
References: <Law11-OE446hm8Ua8d90000dc91@hotmail.com>
Message-ID: <3F2DD45D.4070902@r-project.org>

Angel wrote:
> Is there a way to make a shading interpolation on an image plot?
> Something similar to matlab 'shading interp', I think it is called Gouraud
> shading.
> 
> What I want is to make a image plot look nicer. with image() it looks very
> facetted, and I would like to make it look smoother.
> I've tried with interp.surface() in fields package but it (obviously) makes
> nan values at the borders and around nan.

The short answer is that it is not possible.  The problem is that R
assumes an underlying vector graphics system rather than a raster one.
Vector systems provide flat-shaded polygons rather than interpolated
shading.

If anyone knows how to Gouraud, Phong or other smooth shading method
portably in a vector system I'd be keen to hear about it.

-- 
Ross Ihaka                         Email:  ihaka at r-project.org
The R Project and R Foundation



From ggrothendieck at volcanomail.com  Mon Aug  4 05:45:35 2003
From: ggrothendieck at volcanomail.com (Gabor Grothendieck)
Date: Sun, 3 Aug 2003 20:45:35 -0700 (PDT)
Subject: [R] Timezones
Message-ID: <20030804034535.C6FE7AC01@sitemail.everyone.net>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20030803/5213fec9/attachment.pl

From siewlengteng at yahoo.com  Mon Aug  4 08:00:07 2003
From: siewlengteng at yahoo.com (Siew Leng TENG)
Date: Sun, 3 Aug 2003 23:00:07 -0700 (PDT)
Subject: [R] Error in calling stepAIC() from within a function
Message-ID: <20030804060007.80457.qmail@web14912.mail.yahoo.com>

Hi,
 
 I am experiencing a baffling behaviour of stepAIC(),
 and I hope to get any advice/help on what went wrong
or I'd missed. I greatly appreciate any advice given.
 
 I am using stepAIC() to, say, select a model via
 stepwise selection method.
 
 R Version : 1.7.1
 Windows ME
 
 Many thanks and best regards,
Siew-Leng
 
 
 ***Issue :
 
 When stepAIC() is placed within a function, it seems
 that stepAIC() cannot detect the data matrix, and
 the
 program is halted as a result. However, when the
 same
 codes are copied, paste and run in R workspace,
 stepAIC can execute and R is able to produce the
 desired output.
 
 ***Actions taken :
 I had tried to look into and manipulate environments
 and formals(f), but with not much luck.
 
 
 ***Code snippets (an example):
 
   library(MASS)
   library(nls)
 
   Data<-data.frame(matrix(rnorm(120),ncol=6))
   colnames(Data)<-c("Y",paste("X",1:5,sep=""))
 
   f<-function(A)
   {
   a <-glm(Y~X1+X2+X3, data=A)
   b

<-stepAIC(a,scope=list(upper=~X1+X2+X3+X4+X5,lower=~1),direction="both",trace=FALSE)
   b
   }
 

  f(Data) 
 
 
 R gives the following error :
 
 > f(Data)
 Error in model.frame.default(formula = Y ~ X1 + X3,
 data = A, drop.unused.levels = TRUE) : 
         Object "A" not found
 
 
 However, when the same codes are copied, paste and
 run
 in the workspace, stepAIC() can run, and R is able
 to
 produce the desired output :
 
 A <- Data
 a <-glm(Y~X1+X2+X3, data=A)
 b

<-stepAIC(a,scope=list(upper=~X1+X2+X3+X4+X5,lower=~1),direction="both",trace=FALSE)
 b
 
> b
 
> Call:  glm(formula = Y ~ X4, data = A) 
> 
> Coefficients:
> (Intercept)           X4  
>      0.1942      -0.3314  
> 
> Degrees of Freedom: 19 Total (i.e. Null);  18
> Residual
> Null Deviance:      20.51 
> Residual Deviance: 17.95        AIC: 60.6 
> > 


__________________________________

Yahoo! SiteBuilder - Free, easy-to-use web site design software



From petrov at unibel.by  Mon Aug  4 09:38:35 2003
From: petrov at unibel.by (Sergei V. Petrov)
Date: Mon, 04 Aug 2003 10:38:35 +0300
Subject: [R] Higher Order Correlation function
Message-ID: <3F2E0D7B.6000007@unibel.by>

Dear R Helpers!

Is the Higher Order Correlation function already programmed in
R I couldn't find it?


Much appreciated,
Sergei Petrov



From ligges at statistik.uni-dortmund.de  Mon Aug  4 08:49:59 2003
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Mon, 04 Aug 2003 08:49:59 +0200
Subject: [R] anova
In-Reply-To: <200307310918.31710.anna@ptolemy.arc.nasa.gov>
References: <200307310918.31710.anna@ptolemy.arc.nasa.gov>
Message-ID: <3F2E0217.4050106@statistik.uni-dortmund.de>

Anna H. Pryor wrote:

> I am totally confused as to how to use anova.  I have three vectors and would 
> like to use anova on them but I don't understand how lm or glm comes into 
> play.  In matlab, you just give the three vectors.  Why isn't it the same in 
> R?

Because R is not Matlab?
You can calculate anova on an lm model such as lm(y ~ x1 + x2), see also 
?aov which calls lm() internally.
In any case, you need to learn how to write those formulas. Please read 
the docs.

Uwe Ligges

> 
> Any help would be greatly appreciated.
> 
> 
> Anna
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help



From ripley at stats.ox.ac.uk  Mon Aug  4 08:50:10 2003
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Mon, 4 Aug 2003 07:50:10 +0100 (BST)
Subject: [R] timezones
In-Reply-To: <200307311636.JAA18338@hivnet.ubc.ca>
Message-ID: <Pine.LNX.4.44.0308040746040.8138-100000@gannet.stats>

The solution to all these problems is to use UTC.  You can't select a
`standard time' such as EST: that does not exist for half the year.
The only `timezone' without DST that we can guarantee to be available is 
UTC.

You seem to have misunderstandings about how to set (not convert to) UTC.

On Thu, 31 Jul 2003, Jerome Asselin wrote:

> 
> I share your concerns regarding Problems 1 and 2. However, I am unable to 
> provide help on those at this moment.
> 
> As for Problem 3, an alternative for the time being would be to use 
> another package such as chron or date, although it would be preferable to 
> use the classes of the base package if possible.
> 
> Sorry I can't be more helpful.
> 
> Jerome
> 
> On July 30, 2003 09:19 pm, Gabor Grothendieck wrote:
> > I have some questions and comments on timezones.
> >
> > Problem 1.
> >
> > # get current time in current time zone
> >
> > > (now <- Sys.time())
> >
> > [1] "2003-07-29 18:23:58 Eastern Daylight Time"
> >
> > # convert this to GMT
> >
> > > (now.gmt <- as.POSIXlt(now,tz="GMT"))

That does not convert to GMT: that gives you a list representation in GMT.

> > [1] "2003-07-29 22:23:58 GMT"
> >
> > # take difference
> >
> > > now-now.gmt
> >
> > Time difference of -5 hours
> >
> > Note that the difference between the times displayed by the first two
> > R expressions is -4 hours.  Why does the last expression return
> > -5 hours?
> >
> >
> > Problem 2.  Why do the two expressions below give different answers?
> > I take the difference between two dates in GMT and then repeat it in the
> > current time zone (EDT).
> >
> > # days since origin in GMT
> >
> > > julian(as.POSIXct("2003-06-29",tz="GMT"),origin=as.POSIXct("1899-12-30
> > >",tz="GMT"))
> >
> > Time difference of 37801 days
> >
> > # days since origin in current timezone
> >
> > > julian(as.POSIXct("2003-06-29"),origin=as.POSIXct("1899-12-30"))
> >
> > Time difference of 37800.96 days
> >
> >
> > I thought this might be daylight savings time related but even with
> >
> > standard time I get:
> > > julian(as.POSIXct("2003-06-29",tz="EST"),origin=as.POSIXct("1899-12-30
> > >",tz="EST"))
> >
> > Time difference of 37800.96 days
> >
> >
> > Problem 3. What is the general strategy of dealing with dates, as
> > opposed to datetimes, in R?
> >
> > I have had so many problems and a great deal of frustration, mostly
> > related to timezones.
> >
> > The basic problem is that various aspects of the date such as the year,
> > the month, the day of the month, the day of the week can be different
> > depending on the timezone you use.  This is highly undesirable since
> > I am not dealing with anything more granular than a day yet timezones,
> > which are completely extraneous to dates and by all rights should not
> > have to enter into my problems, keep fowling me up.
> >
> > A lesser problem is that I find myself using irrelevant constants such
> > as the number of seconds in a day which you would think would be
> > something I would not have to deal with since I am concerned with daily
> > data.
> >
> > With R have nifty object oriented features I think a good project would
> > be to implement a class in the base that handled dates without times
> > (or timezones!)
> >
> > P.S. I have sent an earlier version of this but did not see it posted
> > so if both get posted please ignore the prior one since this one has
> > more info in it.
> >
> > ______________________________________________
> > R-help at stat.math.ethz.ch mailing list
> > https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> 

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From ripley at stats.ox.ac.uk  Mon Aug  4 08:55:12 2003
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Mon, 4 Aug 2003 07:55:12 +0100 (BST)
Subject: [R] Problems installing R on Windows XP
In-Reply-To: <001001c35828$4b048770$377abad4@neumannpiv>
Message-ID: <Pine.LNX.4.44.0308040751590.8138-100000@gannet.stats>



On Fri, 1 Aug 2003, Dipl. Ing. Kurt Neumann wrote:

> I run a IBM PC compatible with a Pentium IV with 2,4 MHz and 512 Mb RAM and a 80 Gb fast disk und Windows XP home edition.
> 
> I downloaded rw1071.exe from cran.at and cran twice.

(Unfortunately because of caches you almost certaintly got the same copy.)

> When I tried to open rw1071.exe I choose the option to install all R reference materials as well and then after some 10 seconds I get an error message
> 
> refman.pdf   file is corrupted  (I tried several times to re-open without any success)
> 
> Rhelp.zip      as above
> 
> hvo___.afm  as above
> 
> Do you have any idea what I did presumably wrong?
> Is there any help - suggestions what I should try next available from R-support?

There are suggestions on the very page from which you downloaded 
rw1071.exe: please check the MD5 sums as described at

http://cran.r-project.org/bin/windows/base/

This will establish if the file you downloaded is corrupted.

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From ligges at statistik.uni-dortmund.de  Mon Aug  4 09:23:43 2003
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Mon, 04 Aug 2003 09:23:43 +0200
Subject: [R] How to speed the shut down of the graphic device
In-Reply-To: <1059956417.3f2da6c1d2845@my.uq.edu.au>
References: <200307310508.h6V58LqO346476@atlas.otago.ac.nz>	<3F28E8A5.4070905@wiwi.uni-bielefeld.de>	<3F2A49C8.1020106@curie.fr>
	<1059956417.3f2da6c1d2845@my.uq.edu.au>
Message-ID: <3F2E09FF.80001@statistik.uni-dortmund.de>

Andrew C. Ward wrote:

> Dear Philippe,
> 
> Perhaps you could try a different graphics device (maybe
> postscript). On my machine, the time differences were all 1
> second rather than the 3 you reported. If 300s is really
> too long for you, you could get a new computer or run your
> script on a faster one.


Let me point out that the "shut down" of a device means that the final 
calculations and writing of the file is done at that time. So we have to 
expect it consumes some time.

Uwe Ligges


> Andrew C. Ward
> 
> CAPE Centre
> Department of Chemical Engineering
> The University of Queensland
> Brisbane Qld 4072 Australia
> andreww at cheque.uq.edu.au
> 
> 
> Quoting Philippe Hup? <Philippe.Hupe at curie.fr>:
> 
> 
>>hello,
>>
>>I use R1.7.1 under winXP and I am running the following
>>script example :
>>
>>
>>for (i in 1:10)
>>{
>>        x <- rnorm(100)
>>        png( paste("D:/essai",i,".png",sep=""))
>>        plot(x)
>>        t1 <- Sys.time()
>>        dev.off()
>>        t2 <- Sys.time()
>>        print(t2-t1)
>>       
>>}
>>
>>at each step, it takes about 3 seconds to shut down the
>>graphic device. 
>>I want to generate about one hundred of image and of
>>course it takes too 
>>much time. Is there any trick ?
>>
>>Philippe
>>
>>______________________________________________
>>R-help at stat.math.ethz.ch mailing list
>>https://www.stat.math.ethz.ch/mailman/listinfo/r-help
>>
> 
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help



From ligges at statistik.uni-dortmund.de  Mon Aug  4 09:25:27 2003
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Mon, 04 Aug 2003 09:25:27 +0200
Subject: [R] R 1.7.1 arima0 problem
In-Reply-To: <20030731220137.11517.qmail@web41709.mail.yahoo.com>
References: <20030731220137.11517.qmail@web41709.mail.yahoo.com>
Message-ID: <3F2E0A67.1090209@statistik.uni-dortmund.de>

Yu Zhang wrote:

> Hi, I'm trying to go through the examples for function
> arima0() in ts package, i.e,
> 
>>data(lh)
>>arima0(lh, order = c(1,0,0))
> 
> each time the call to arima0() causes a segmentation
> fault. I checked the earlier version (1.1.1) of R,
> the function arima0 works fine. 
> 
> Tracing the call indicates that the function
> "setup_starma" (in pacf.c under ts) interprets 
> the addresses of the arguments incorrectly.
> The problem seems to 
> be related to INTEGER(na), which re-align the address
> of na which otherwise would be correct. Is it a known
> problem with a quick fix or system-dependent? FYI, I'm
> 
> using R version 1.7.1 on redhat 6.2.
> 
> Any hints will be appreciated. 

Works for me on Windows. There is a function arima these days, did you 
try that one as well?

Uwe Ligges



From glaziou at pasteur-kh.org  Mon Aug  4 09:26:43 2003
From: glaziou at pasteur-kh.org (Philippe Glaziou)
Date: Mon, 4 Aug 2003 14:26:43 +0700
Subject: [R] R on Solaris 9
In-Reply-To: <000b01c35769$d1553230$d5a5d280@genomics.purdue.edu>
References: <000b01c35769$d1553230$d5a5d280@genomics.purdue.edu>
Message-ID: <20030804072643.GF592@pasteur-kh.org>

Donnie Payne <dapayne at purdue.edu> wrote:
> I am attempting an install of R-1.7.1 on a Sun Solaris 9
> box.  It is a V880.  It has 8 processors at 950MHz each
> and 32 Gb of physical memory.
>  
> configure: WARNING: you cannot build DVI versions of the R
> manuals

configure could not find latex


> configure: WARNING: you cannot build info versions of the
> R manuals

configure could not find makeinfo

> configure: WARNING: you cannot build PDF versions of the R
> manuals

configure could not find pdflatex

 
> I can run the 'make' fine, but when I run the 'make check'
> I receive the following errors:
> 
> running code in 'base-Ex.R' ...*** Error code 1
> 
> make: Fatal error: Command failed for target `base-Ex.Rout'

You will need to check base-Ex.Rout to see what went
wrong, and what other important piece of software 
is missing. 

-- 
Philippe



From ripley at stats.ox.ac.uk  Mon Aug  4 09:32:00 2003
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Mon, 4 Aug 2003 08:32:00 +0100 (BST)
Subject: [R] R 1.7.1 arima0 problem
In-Reply-To: <3F2E0A67.1090209@statistik.uni-dortmund.de>
Message-ID: <Pine.LNX.4.44.0308040828330.13253-100000@gannet.stats>

Note that those examples are run by `make check' during the installation
process, so the installation should have failed.  How come that was not
detected earlier?

RedHat 6.2 is rather old these days and as I recall came with flaky 
compilers (the infamous gcc `2.96').  Was a released version of gcc used: 
if not, please try one.  In any case, this is a sign of a broken R 
installation, and not a breakage that has been reported before.

On Mon, 4 Aug 2003, Uwe Ligges wrote:

> Yu Zhang wrote:
> 
> > Hi, I'm trying to go through the examples for function
> > arima0() in ts package, i.e,
> > 
> >>data(lh)
> >>arima0(lh, order = c(1,0,0))
> > 
> > each time the call to arima0() causes a segmentation
> > fault. I checked the earlier version (1.1.1) of R,
> > the function arima0 works fine. 
> > 
> > Tracing the call indicates that the function
> > "setup_starma" (in pacf.c under ts) interprets 
> > the addresses of the arguments incorrectly.
> > The problem seems to 
> > be related to INTEGER(na), which re-align the address
> > of na which otherwise would be correct. Is it a known
> > problem with a quick fix or system-dependent? FYI, I'm
> > 
> > using R version 1.7.1 on redhat 6.2.
> > 
> > Any hints will be appreciated. 
> 
> Works for me on Windows. There is a function arima these days, did you 
> try that one as well?

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From Philippe.Hupe at curie.fr  Mon Aug  4 09:35:04 2003
From: Philippe.Hupe at curie.fr (=?ISO-8859-1?Q?Philippe_Hup=E9?=)
Date: Mon, 04 Aug 2003 09:35:04 +0200
Subject: [R] How to speed the shut down of the graphic device
In-Reply-To: <1059956417.3f2da6c1d2845@my.uq.edu.au>
References: <200307310508.h6V58LqO346476@atlas.otago.ac.nz>
	<3F28E8A5.4070905@wiwi.uni-bielefeld.de>
	<3F2A49C8.1020106@curie.fr> <1059956417.3f2da6c1d2845@my.uq.edu.au>
Message-ID: <3F2E0CA8.5000005@curie.fr>

Andrew C. Ward a ?crit :

>Dear Philippe,
>
>Perhaps you could try a different graphics device (maybe
>postscript). On my machine, the time differences were all 1
>second rather than the 3 you reported.If 300s is really
>too long for you, you could get a new computer or run your
>script on a faster one.
>

I have a computer with 3GHz processor (Pentium IV) and 2 Go of RAM so I 
don't think this is a matter of computer performance :).

>
>Andrew C. Ward
>
>CAPE Centre
>Department of Chemical Engineering
>The University of Queensland
>Brisbane Qld 4072 Australia
>andreww at cheque.uq.edu.au
>
>
>Quoting Philippe Hup? <Philippe.Hupe at curie.fr>:
>
>  
>
>>hello,
>>
>>I use R1.7.1 under winXP and I am running the following
>>script example :
>>
>>
>>for (i in 1:10)
>>{
>>        x <- rnorm(100)
>>        png( paste("D:/essai",i,".png",sep=""))
>>        plot(x)
>>        t1 <- Sys.time()
>>        dev.off()
>>        t2 <- Sys.time()
>>        print(t2-t1)
>>       
>>}
>>
>>at each step, it takes about 3 seconds to shut down the
>>graphic device. 
>>I want to generate about one hundred of image and of
>>course it takes too 
>>much time. Is there any trick ?
>>
>>Philippe
>>
>>______________________________________________
>>R-help at stat.math.ethz.ch mailing list
>>https://www.stat.math.ethz.ch/mailman/listinfo/r-help
>>
>>    
>>
>
>
>.
>
>  
>


-- 

--------------------------------------------------

Philippe Hup?
Institut Curie - Equipe Bioinformatique
26, rue d'Ulm - 75005 PARIS France
+33 (0)1 42 34 65 29

Philippe.Hupe at curie.fr <mailto:Philippe.Hupe at curie.fr>



From ripley at stats.ox.ac.uk  Mon Aug  4 09:37:29 2003
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Mon, 4 Aug 2003 08:37:29 +0100 (BST)
Subject: [R] ncp t & Fortran error & power of some tests
In-Reply-To: <20030801104906.67657.qmail@web20703.mail.yahoo.com>
Message-ID: <Pine.LNX.4.44.0308040833560.13253-100000@gannet.stats>

On Fri, 1 Aug 2003, Jen Krann wrote:

> Hi everybody, I have three questions to ask us:
> 
> a) R incorporates a function for the Non-central T
> distribution which unfortunately and, as you know, is
> not available in Splus 4.5. In
> http://www.stats.ox.ac.uk/pub/Swin I found the Don
> MacQueen?s noncent.zip but when I run it in Splus 4.5
> the following error message appears: "Error in
> .Fortran ("vectnc",: "VECTNC" is not a symbol in the
> load table". May be I did not installed it correctly
> or (as I suppose) it is incompatible with this version
> of Splus. 
> I looked in the directory for Splus 6.0 
> http://www.stats.ox.ac.uk/pub/MASS3/Winlibs but the
> update of this function is not there. 

Why should it be?  

> Do you know of
> some alternative function for the Non-central T in
> Splus 4.5 or how to solve the problem with
> noncent.zip?

This is *R*-help: the obvious answer is to use R.

Otherwise you can compile Don McQueen's function from the sources 
yourself.

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From ripley at stats.ox.ac.uk  Mon Aug  4 09:48:17 2003
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Mon, 4 Aug 2003 08:48:17 +0100 (BST)
Subject: [R] How to speed the shut down of the graphic device
In-Reply-To: <3F2E0CA8.5000005@curie.fr>
Message-ID: <Pine.LNX.4.44.0308040841350.13253-100000@gannet.stats>

On Mon, 4 Aug 2003, Philippe Hup? wrote:

> Andrew C. Ward a ?crit :
> 
> >Dear Philippe,
> >
> >Perhaps you could try a different graphics device (maybe
> >postscript). On my machine, the time differences were all 1
> >second rather than the 3 you reported.If 300s is really
> >too long for you, you could get a new computer or run your
> >script on a faster one.
> >
> 
> I have a computer with 3GHz processor (Pentium IV) and 2 Go of RAM so I 
> don't think this is a matter of computer performance :).

My lowly 1.4GHz P4M laptop does it in 2.5 secs, so there does seem to be a 
performance problem with your computer.

BTW, the way to time a command is to use system.time()

> 
> >
> >Andrew C. Ward
> >
> >CAPE Centre
> >Department of Chemical Engineering
> >The University of Queensland
> >Brisbane Qld 4072 Australia
> >andreww at cheque.uq.edu.au
> >
> >
> >Quoting Philippe Hup? <Philippe.Hupe at curie.fr>:
> >
> >  
> >
> >>hello,
> >>
> >>I use R1.7.1 under winXP and I am running the following
> >>script example :
> >>
> >>
> >>for (i in 1:10)
> >>{
> >>        x <- rnorm(100)
> >>        png( paste("D:/essai",i,".png",sep=""))
> >>        plot(x)
> >>        t1 <- Sys.time()
> >>        dev.off()
> >>        t2 <- Sys.time()
> >>        print(t2-t1)
> >>       
> >>}
> >>
> >>at each step, it takes about 3 seconds to shut down the
> >>graphic device. 
> >>I want to generate about one hundred of image and of
> >>course it takes too 
> >>much time. Is there any trick ?
> >>
> >>Philippe
> >>
> >>______________________________________________
> >>R-help at stat.math.ethz.ch mailing list
> >>https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> >>
> >>    
> >>
> >
> >
> >.
> >
> >  
> >
> 
> 
> 

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From ripley at stats.ox.ac.uk  Mon Aug  4 10:03:20 2003
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Mon, 4 Aug 2003 09:03:20 +0100 (BST)
Subject: [R] Error in calling stepAIC() from within a function
In-Reply-To: <20030804060007.80457.qmail@web14912.mail.yahoo.com>
Message-ID: <Pine.LNX.4.44.0308040900500.13253-100000@gannet.stats>

Your example works for me in R-devel, and in R 1.7.1 using step()
rather than stepAIC().

On Sun, 3 Aug 2003, Siew Leng TENG wrote:

> Hi,
>  
>  I am experiencing a baffling behaviour of stepAIC(),
>  and I hope to get any advice/help on what went wrong
> or I'd missed. I greatly appreciate any advice given.
>  
>  I am using stepAIC() to, say, select a model via
>  stepwise selection method.
>  
>  R Version : 1.7.1
>  Windows ME
>  
>  Many thanks and best regards,
> Siew-Leng
>  
>  
>  ***Issue :
>  
>  When stepAIC() is placed within a function, it seems
>  that stepAIC() cannot detect the data matrix, and
>  the
>  program is halted as a result. However, when the
>  same
>  codes are copied, paste and run in R workspace,
>  stepAIC can execute and R is able to produce the
>  desired output.
>  
>  ***Actions taken :
>  I had tried to look into and manipulate environments
>  and formals(f), but with not much luck.
>  
>  
>  ***Code snippets (an example):
>  
>    library(MASS)
>    library(nls)
>  
>    Data<-data.frame(matrix(rnorm(120),ncol=6))
>    colnames(Data)<-c("Y",paste("X",1:5,sep=""))
>  
>    f<-function(A)
>    {
>    a <-glm(Y~X1+X2+X3, data=A)
>    b
> 
> <-stepAIC(a,scope=list(upper=~X1+X2+X3+X4+X5,lower=~1),direction="both",trace=FALSE)
>    b
>    }
>  
> 
>   f(Data) 
>  
>  
>  R gives the following error :
>  
>  > f(Data)
>  Error in model.frame.default(formula = Y ~ X1 + X3,
>  data = A, drop.unused.levels = TRUE) : 
>          Object "A" not found
>  
>  
>  However, when the same codes are copied, paste and
>  run
>  in the workspace, stepAIC() can run, and R is able
>  to
>  produce the desired output :
>  
>  A <- Data
>  a <-glm(Y~X1+X2+X3, data=A)
>  b
> 
> <-stepAIC(a,scope=list(upper=~X1+X2+X3+X4+X5,lower=~1),direction="both",trace=FALSE)
>  b
>  
> > b
>  
> > Call:  glm(formula = Y ~ X4, data = A) 
> > 
> > Coefficients:
> > (Intercept)           X4  
> >      0.1942      -0.3314  
> > 
> > Degrees of Freedom: 19 Total (i.e. Null);  18
> > Residual
> > Null Deviance:      20.51 
> > Residual Deviance: 17.95        AIC: 60.6 
> > > 
> 
> 
> __________________________________
> 
> Yahoo! SiteBuilder - Free, easy-to-use web site design software
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> 

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From Philippe.Hupe at curie.fr  Mon Aug  4 10:08:13 2003
From: Philippe.Hupe at curie.fr (=?ISO-8859-1?Q?Philippe_Hup=E9?=)
Date: Mon, 04 Aug 2003 10:08:13 +0200
Subject: [R] How to speed the shut down of the graphic device
In-Reply-To: <Pine.LNX.4.44.0308040841350.13253-100000@gannet.stats>
References: <Pine.LNX.4.44.0308040841350.13253-100000@gannet.stats>
Message-ID: <3F2E146D.6000500@curie.fr>

Prof Brian Ripley a ?crit :

>On Mon, 4 Aug 2003, Philippe Hup? wrote:
>
>  
>
>>Andrew C. Ward a ?crit :
>>
>>    
>>
>>>Dear Philippe,
>>>
>>>Perhaps you could try a different graphics device (maybe
>>>postscript). On my machine, the time differences were all 1
>>>second rather than the 3 you reported.If 300s is really
>>>too long for you, you could get a new computer or run your
>>>script on a faster one.
>>>
>>>      
>>>
>>I have a computer with 3GHz processor (Pentium IV) and 2 Go of RAM so I 
>>don't think this is a matter of computer performance :).
>>    
>>
>
>My lowly 1.4GHz P4M laptop does it in 2.5 secs, so there does seem to be a 
>performance problem with your computer.
>
>BTW, the way to time a command is to use system.time()
>
>  
>
>>>Andrew C. Ward
>>>
>>>CAPE Centre
>>>Department of Chemical Engineering
>>>The University of Queensland
>>>Brisbane Qld 4072 Australia
>>>andreww at cheque.uq.edu.au
>>>
>>>
>>>Quoting Philippe Hup? <Philippe.Hupe at curie.fr>:
>>>
>>> 
>>>
>>>      
>>>
>>>>hello,
>>>>
>>>>I use R1.7.1 under winXP and I am running the following
>>>>script example :
>>>>
>>>>
>>>>for (i in 1:10)
>>>>{
>>>>       x <- rnorm(100)
>>>>       png( paste("D:/essai",i,".png",sep=""))
>>>>       plot(x)
>>>>       t1 <- Sys.time()
>>>>       dev.off()
>>>>       t2 <- Sys.time()
>>>>       print(t2-t1)
>>>>      
>>>>}
>>>>
>>>>at each step, it takes about 3 seconds to shut down the
>>>>graphic device. 
>>>>I want to generate about one hundred of image and of
>>>>course it takes too 
>>>>much time. Is there any trick ?
>>>>
>>>>Philippe
>>>>
>>>>______________________________________________
>>>>R-help at stat.math.ethz.ch mailing list
>>>>https://www.stat.math.ethz.ch/mailman/listinfo/r-help
>>>>
>>>>   
>>>>
>>>>        
>>>>
>>>.
>>>
>>> 
>>>
>>>      
>>>
>>
>>    
>>
>
>  
>
I have done the same thing under Debian/sid wiht R1.7.1 and it takes 
almost 0 second !! Then, is there any problem with winXP ?

-- 

--------------------------------------------------

Philippe Hup?
Institut Curie - Equipe Bioinformatique
26, rue d'Ulm - 75005 PARIS France
+33 (0)1 42 34 65 29

Philippe.Hupe at curie.fr <mailto:Philippe.Hupe at curie.fr>



From ligges at statistik.uni-dortmund.de  Mon Aug  4 10:10:08 2003
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Mon, 04 Aug 2003 10:10:08 +0200
Subject: [R] value replacement in matrices
In-Reply-To: <1077D716F31A5A4590ED2B0C094B3FED0ACF56@oxlsrv001.dpi.qld.gov.au>
References: <1077D716F31A5A4590ED2B0C094B3FED0ACF56@oxlsrv001.dpi.qld.gov.au>
Message-ID: <3F2E14E0.1000509@statistik.uni-dortmund.de>

George, Charles R wrote:

> Hello R users
> 
> I would like to ask if anyone knows a computationally fast solution to this problem:
> 
> I have an original matrix and an index matrix. The original matrix is ca 4000x4000 cells, and the index matrix has 261 unique values. From these, I want to produce a new matrix.
> 
> Consider the following simplified example:
> 
> Original matrix
> 1 4 6 5
> 3 4 8 5
> 2 4 7 8
> 9 8 3 6
> 
> index matrix
> 1 5
> 2 7
> 3 2
> 4 5
> 5 3
> 6 7
> 7 5
> 8 3
> 9 9
> 
> my current code is something like this
> 
> for(i in 1:9) {
> 	changeVal <- which(originalMat==indexMat[i,1])
> 	finalMat <- indexMat[i,2]
> }

  replace(originalMat, indexMat[,1], indexMat[,2])

Uwe Ligges


> the output would look like this:
> 
> Final matrix
> 5 5 7 3
> 2 5 3 3
> 7 5 5 3
> 9 3 2 7
> 
> At the moment it takes a while to process. Does anyone have any suggestions?
> 
> reagrds
> 
> Robert
>  
> 
> ********************************DISCLAIMER******************...{{dropped}}
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help



From ripley at stats.ox.ac.uk  Mon Aug  4 10:20:25 2003
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Mon, 4 Aug 2003 09:20:25 +0100 (BST)
Subject: [R] How to speed the shut down of the graphic device
In-Reply-To: <3F2E146D.6000500@curie.fr>
Message-ID: <Pine.LNX.4.44.0308040917210.13316-100000@gannet.stats>

On Mon, 4 Aug 2003, Philippe Hup? wrote:

> I have done the same thing under Debian/sid wiht R1.7.1 and it takes 
> almost 0 second !! Then, is there any problem with winXP ?

No.  png() under Unix-alikes is not the same device and does not work 
the same way.

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From ligges at statistik.uni-dortmund.de  Mon Aug  4 10:23:28 2003
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Mon, 04 Aug 2003 10:23:28 +0200
Subject: [R] na.action in randomForest
In-Reply-To: <001101c35a33$9278d450$0a6cfea9@BLSPEAPARKHOM>
References: <001101c35a33$9278d450$0a6cfea9@BLSPEAPARKHOM>
Message-ID: <3F2E1800.4020504@statistik.uni-dortmund.de>

David Parkhurst wrote:

> The help page for randomForest shows na.action=na.fail as a parameter, and
> does not describe other possibilities for na.action.
> 
> I have a regression problem, with about 1000 rows in my data frame, and with
> an NA in occasional predictor variables, in about 5% of rows.  I would like
> to have all rows included in the analysis, to the extent possible.  (That
> seems to be possible in rpart, for example.)  Is it possible to specify that
> rows with a few NA's should be included in the randomForest analysis?  If
> so, how?

I don't know the design of randomForest, but if the authors have 
implemented na.action as in other functions, you can specify your own 
function here (typical one uses na.fail or na.omit).

Uwe Ligges



From ripley at stats.ox.ac.uk  Mon Aug  4 10:30:13 2003
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Mon, 4 Aug 2003 09:30:13 +0100 (BST)
Subject: [R] spatial statistics vs. spatial econometrics
In-Reply-To: <sf290f2d.026@ers.usda.gov>
Message-ID: <Pine.LNX.4.44.0308040922100.13316-100000@gannet.stats>

I completely fail to recognise your description of V&R ch 14. That is (in
part) about *continuous* spatial fields, and does include pictures of
correlograms and variograms.  That's not the same problem as looking for
spatial dependence in areally-sampled data.

My 1981 book (is that the one you haven't read?) treats these different
problems in different chapters.  So do the other widely-cited references
in spatial statistics: geographers tend to ignore all the rest of the
spatial sciences, however.

On Thu, 31 Jul 2003, Michael Roberts wrote:

> Dear R users,
> 
> I am putting together reading and resources lists for spatial statistics
> and spatial econometrics and am looking for some pointers from more
> experienced practitioners.
> 
> In particular, I find two "camps" in spatial modelling, and am wondering
> which approach is better suitied to which situation.
> 
> The first camp is along the lines of Venables and Ripley's Chapter 14
> (and presumably Ripley's book, but I don't have that yet)--spatial
> trends and kriging (e.g., the geoR package);  the second along the lines
> of Anselin's book--spatial lag and spatial-autocorrelation models (e.g.,
> the spdep package).
> 
> As far as I can tell, these amount to the same thing (in princple).

Wrong.  Kriging is primarily interested in prediction.

> The first camp likes to use row-standardized "weight matricies" in
> building covariance structures (to ensure there isn't too much
> dependence?).  I find this very unappealing to many models.  This camp
> doesn't seem to look at variograms or correlegrams as often--they just
> fit the model, which I also find unappealing.  The covariance structures
> also tend to be very simple.  It looks like there is more flexibility in
> the second camp.
> 
> Mixed model procedures also seem to have spatial covariance structures.
> 
> Is there a reason why there appears to be so few cross references
> between these camps?  What makes each approach best for different kinds
> of problems?

They are different problems with different fields of applications.
There are quite a few other different problems you would probably 
mis-identify as `camps', including several approaches to spatial point 
patterns.

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From ripley at stats.ox.ac.uk  Mon Aug  4 10:32:37 2003
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Mon, 4 Aug 2003 09:32:37 +0100 (BST)
Subject: [R] na.action in randomForest
In-Reply-To: <001101c35a33$9278d450$0a6cfea9@BLSPEAPARKHOM>
Message-ID: <Pine.LNX.4.44.0308040931450.13316-100000@gannet.stats>

On Sun, 3 Aug 2003, David Parkhurst wrote:

> The help page for randomForest shows na.action=na.fail as a parameter, and
> does not describe other possibilities for na.action.
> 
> I have a regression problem, with about 1000 rows in my data frame, and with
> an NA in occasional predictor variables, in about 5% of rows.  I would like
> to have all rows included in the analysis, to the extent possible.  (That
> seems to be possible in rpart, for example.)  Is it possible to specify that
> rows with a few NA's should be included in the randomForest analysis?  If
> so, how?

What do you want the tree procedure to do with NAs?  Hint: rpart and tree
have two different approaches.

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From ripley at stats.ox.ac.uk  Mon Aug  4 10:33:54 2003
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Mon, 4 Aug 2003 09:33:54 +0100 (BST)
Subject: [R] problem with summary in survival analysis
In-Reply-To: <000101c35774$b44e5080$120e0ec6@XP018>
Message-ID: <Pine.LNX.4.44.0308040933130.13316-100000@gannet.stats>

We need a reproducible example: I have never seen a problem with large 
data sets.

On Thu, 31 Jul 2003, GERMAIN Michel wrote:

> hi,
> i tried to make an survival analysis with survfit, but i have a problem
> when i use the command summary.
>  
> i receive this error message when i have numerous data(> 500):
>  
> Error in as.matrix(x) : (subscript) logical subscript too long
> 
>  
> how can i see the standard result ?

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From B.Rowlingson at lancaster.ac.uk  Mon Aug  4 10:35:06 2003
From: B.Rowlingson at lancaster.ac.uk (Barry Rowlingson)
Date: Mon, 04 Aug 2003 09:35:06 +0100
Subject: [R] shading in image()
In-Reply-To: <3F2DD45D.4070902@r-project.org>
References: <Law11-OE446hm8Ua8d90000dc91@hotmail.com>
	<3F2DD45D.4070902@r-project.org>
Message-ID: <3F2E1ABA.2010101@lancaster.ac.uk>

Ross Ihaka wrote:

> If anyone knows how to Gouraud, Phong or other smooth shading method
> portably in a vector system I'd be keen to hear about it.
> 

  Why bloat R with something like that? All we need is a way of linking 
R with Blender and we have a fully-GPL statistics and 3-d rendering 
facility. Blender is scriptable in python, so you could read data files 
from Blender, or possibly one could write an R graphics device that 
produced Blender-compatible data files. Blender

  Eventually you'll be able to walk through your data in 3-d....

Baz



From M.Mamin at intershop.de  Mon Aug  4 10:35:25 2003
From: M.Mamin at intershop.de (Marc Mamin)
Date: Mon, 4 Aug 2003 10:35:25 +0200 
Subject: [R] R as ftp client ?
Message-ID: <770E451830D96B4D84747B54665DA1B202DAF082@jena03.net.j.ad.intershop.net>

Hallo,

I want to open an ftp connection (with login and pwd),
and then to retrieve the file list and the files'content of a given
directory.

Is this possible with R (on W2K)?

Thanks for your hints,

Marc Mamin



From ripley at stats.ox.ac.uk  Mon Aug  4 10:54:30 2003
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Mon, 4 Aug 2003 09:54:30 +0100 (BST)
Subject: [R] behavior of weights in nnet's multinom()
In-Reply-To: <D4C203B93FEDF04CA2B493EB08F2E43113793A@qeds001.hq.wash.qedgroupllc>
Message-ID: <Pine.LNX.4.44.0308040953120.13354-100000@gannet.stats>

weights=3 means `I have 3 of these'.  That is what `case weights' means.

On Fri, 1 Aug 2003, Steve Sullivan wrote:

> I see that "case weights" can be optioned in multinom().  I wanted to
> make sure I understand what weights= is expecting.  My weights (not
> really mine but I'm stuck with them) are noninteger, are not scaled to
> sum to the sample size, and larger weights are intended to increase
> influence.
> 
>  
> 
> The description of various types of weights is a perennial confusion for
> me; sorry.
> 
>  
> 
> STS
> 
>  
> 
> Steven Sullivan, Ph.D.
> 
> Senior Associate
> 
> The QED Group, LLC
> 
> 1250 Eye St. NW, Suite 802
> 
> Washington, DC  20005
> 
> ssullivan at qedgroupllc.com
> 
> 202.898.1910.x15 (v)
> 
> 202.898.0887 (f)
> 
> 202.421.8161 (m)
> 
>  
> 
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> 

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From Ted.Harding at nessie.mcc.ac.uk  Mon Aug  4 10:58:10 2003
From: Ted.Harding at nessie.mcc.ac.uk ( (Ted Harding))
Date: Mon, 04 Aug 2003 09:58:10 +0100 (BST)
Subject: [R] Test
Message-ID: <XFMail.030804095810.Ted.Harding@nessie.mcc.ac.uk>

This is a test to see if I'm getting through. Sorry for the intrusion.
(Sent 2 messages yesterday to r-help; both seem to have got lost, since
people are now posting on Monday morning and getting through right away).

Ted.


--------------------------------------------------------------------
E-Mail: (Ted Harding) <Ted.Harding at nessie.mcc.ac.uk>
Fax-to-email: +44 (0)870 167 1972
Date: 04-Aug-03                                       Time: 09:58:10
------------------------------ XFMail ------------------------------



From ripley at stats.ox.ac.uk  Mon Aug  4 11:08:57 2003
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Mon, 4 Aug 2003 10:08:57 +0100 (BST)
Subject: [R] R as ftp client ?
In-Reply-To: <770E451830D96B4D84747B54665DA1B202DAF082@jena03.net.j.ad.intershop.net>
Message-ID: <Pine.LNX.4.44.0308041003350.13354-100000@gannet.stats>

On Mon, 4 Aug 2003, Marc Mamin wrote:

> Hallo,
> 
> I want to open an ftp connection (with login and pwd),
> and then to retrieve the file list and the files'content of a given
> directory.
> 
> Is this possible with R (on W2K)?

Not a file list: the internal ftp client does not support that. You can 
get a file by a ftp://user:pass at machine/path/to/file URL.

You can use download.file(method="wget") to do this: a suitable copy of 
wget.exe is at http://www.stats.ox.ac.uk/pub/Rtools.

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From Ted.Harding at nessie.mcc.ac.uk  Mon Aug  4 11:12:05 2003
From: Ted.Harding at nessie.mcc.ac.uk ( (Ted Harding))
Date: Mon, 04 Aug 2003 10:12:05 +0100 (BST)
Subject: [R] Specifying weird models
Message-ID: <XFMail.030803093823.Ted.Harding@nessie.mcc.ac.uk>

[re-sending this one since it apparently didn't get through yesterday]

Hi Folks,

I'm pondering the following type of question in the context
of specifying a linear model formula. Basically, it's a matter
of specifying a "non-homogeneous" model. The following example
(not a real case, and over-simplified, but it illustrates the
point cleanly) shows what I mean.

There are 3 factors, A, B and C. B and C are at two levels (1,2)
and A is at 3 levels (1,2,3). Suppose I have good reason to believe
that:

-- at level 1 of A, no main effects nor interactions of B and C

-- at level 2 of A, main effects of B and C but no interactions

-- at level 3 of A, B and C come in with a full model (main
   effects plus interactions).

Furthermore, the main effects of B and C at A=2 are to be the same
as at A=3.

In order to squeeze maximum efficiency from the data, I want to
suppress unnecessary parameters from the estimation, so want to
specify a model which incorporates the above structure.

Fundamentally, this could be written as a full-interaction model
for A, B and C with explicit linear constraints on the coefficients.
In the above example, there are not many combinations of levels
(12 only), so writing special-purpose code could be feasible.

I'm wondering how to present such a situation _in general_ to standard
linear modelling functions (and, indeed, how the results might come back
if one could succeed in getting the functions to address the problem).

With thanks for any comments or suggestions,
Best wishes to all,
Ted.


--------------------------------------------------------------------
E-Mail: (Ted Harding) <Ted.Harding at nessie.mcc.ac.uk>
Fax-to-email: +44 (0)870 167 1972
Date: 03-Aug-03                                       Time: 09:38:23
------------------------------ XFMail ------------------------------



From Ted.Harding at nessie.mcc.ac.uk  Mon Aug  4 11:12:44 2003
From: Ted.Harding at nessie.mcc.ac.uk ( (Ted Harding))
Date: Mon, 04 Aug 2003 10:12:44 +0100 (BST)
Subject: [R] Discussion: Spam on R-help
Message-ID: <XFMail.030803133917.Ted.Harding@nessie.mcc.ac.uk>

[re-sending this one since it apparently didn't get through yesterday]

Hi Folks,

I just had a look at the r-help archive for the still-young month of
August, and happened to notice that out of 49 postings there, 20 were
assorted spam of all-too-familiar types.

I'm wondering (discussion point) whether this might suggest that the
list should adopt a more restrictive policy. Apparently, as things
now stand, anyone may post to the list.

I faced the same problem some months ago when two lists which I run
(linux-users at mcc.ac.uk and man-lug at mcc.ac.uk) were being hit by spam
at a rate which was becoming irritating. With great reluctance, I came
to the conclusion that the lists would have to be restricted to "posting
by members only". This has had the knock-on effect that both spam and
also occasional genuine worthwhile postings by non-members end up being
sent to me for approval. Fortunately in the case of these lists, the
frequency of this remains at an acceptable level -- say a dozen or so a
week, of which almost all are spam and are easily dealt with, with no
regrets if there's any delay before I get round to it. Also there are
not huge numbers of subscribers -- the overall level of traffic is
relatively low.

Another knock-on effect is tbat when I get a subscription request from
an email address that is not obviously bona-fide, I feel obliged to
send off a mail to that address asking for confirmation of genuine
interest in Linux (I have a pro-forma template for this). This is
to prevent people subscribing to the list in order either to send out
spam via the list or to harvest email addresses of list subscribers.
For the same reasons, access to the lists' archives and to subscriber
lists is password-restricted to subscribers.

It is likely to be otherwise with r-help. This has a fairly high level
of traffic, many people may welcome the opportunity to post messages
without subscribing, and the list-owners might find themselves forced to
devote more time than they would welcome to dealing not only with spam
but also with non-member postings, if the list became "members-only".
There are many subscribers: this too would add to the list-owners' burden
if they had to "manage" all that.

On the other hand there is the possibility of spam-blocking software.
This works up to a point, and provided the amount of spam that slipped
through was small it could be an acceptable solution. At the same time,
it's all too easy for genuine messages to get blocked if the spam-filter
triggers are comprehensive enough to trap most spam (in a statistical
context, SEX may be a factor; and there are many other triggers e.g. in
the 'cluster' package).

Yet again, people can simply accept that spam, like flies in Summer,
is one of those irritations that must simply be ignored: "just hit
delete".

Anyway, as I said, a point for discussion. What do people think?

Best wishes to all,
Ted.

PS: Just to make clear that I'm not _complaining_ about receiving spam
via r-help. Those 20 messages would be only a tiny fraction of the
hundreds of times I have "hit delete" since 1 August!


--------------------------------------------------------------------
E-Mail: (Ted Harding) <Ted.Harding at nessie.mcc.ac.uk>
Fax-to-email: +44 (0)870 167 1972
Date: 03-Aug-03                                       Time: 13:39:17
------------------------------ XFMail ------------------------------



From ripley at stats.ox.ac.uk  Mon Aug  4 11:24:35 2003
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Mon, 4 Aug 2003 10:24:35 +0100 (BST)
Subject: [R] Excel can do what R can't?????
In-Reply-To: <3F2D7225.5010708@libertysurf.fr>
Message-ID: <Pine.LNX.4.44.0308041022570.13354-100000@gannet.stats>

On Sun, 3 Aug 2003, Fan wrote:

> I've found that the discussions are interesting, generally speaking,
> peoples seem equally confident on R's optim/nlm and Excel's solver.
> 
> The authors of the algorithm GRG2 (Generalized Reduced Gradient)
> are not cited in the documentation of optim(), so I'm wondering if
> the optimization algorithm implemented in Excel is "fondamentally"
> the same than that in R ?

I don't suppose Excel cites the method*s* used in optim() either,
but GRG2 is not in the index of my copies of any of the standard texts on 
numerical optimization.

> 
> Thanks in advance
> --
> Fan
> 
> Damon Wischik wrote:
> > Michael Rennie wrote:
> > 
> >>Last, it's not even that I'm getting error messages anymore- I just
> >>can't get the solution that I get from Excel.  If I try to let R find
> >>the solution, and give it starting values of c(1,2), it gives me an
> >>optimization solution, but an extremely poor one.  However, if I give it
> >>the answer I got from excel, it comes right back with the same answer
> >>and solutions I get from excel. 
> >>
> >>Using the 'trace' function, I can see that R gets stuck in a specific
> >>region of parameter space in looking for the optimization and just
> >>appears to give up.  Even when it re-set itself, it keeps going back to
> >>this region, and thus doesn't even try a full range of the parameter
> >>space I've defined before it stops and gives me the wrong answer. 
> > 
> > 
> > 1. Either your function or the Excel solver is wrong. I executed your
> > source code (which defines f), then ran it over a grid of points, and
> > plotted the answer, using this code:
> > 
> > xvals <- seq(.2,2,by=.2)
> > yvals <- seq(1,3,by=.2)
> > z <- matrix(NA,nrow=length(xvals),ncol=length(yvals))
> > for (i in 1:length(xvals)) for (j in 1:length(yvals)) {
> >   x <- xvals[i]
> >   y <- yvals[j]
> >   z[i,j] <- f(c(x,y))
> >   }
> > filled.contour(x=xvals,y=yvals,z=log(z))
> > 
> > Your "solution" from Excel evaluates to
> >   f(c(.558626306252032,1.66764519286918)) == 0.3866079
> > while I easily found a point which was much better,
> >   f(c(.4,1)) = 7.83029e-05
> > 
> > You should have tried executing your function over a grid of points, and
> > plotting the results in a contour plot, to see if optim was working
> > sensibly. You could do the same grid in Excel and R to verify that the
> > function you've defined does the same thing in each.
> > 
> > Since your optimization is only over a 2D parameter space, it is easy for
> > you to plot the results, to see at a glance what the optimum is, and to
> > work out what is going wrong.
> > 
> > 2. Your code executes very slowly because it is programmed inefficiently.
> > You need to iterate a function to get your final solution, but you don't
> > need to keep track of all the states you visit on the way. The way R
> > works, whenever you assign a value to a certain index in a vector, as in 
> >   A[i] <- 10,
> > the system actually copies the entire vector. So, in every iteration, you
> > are copying very many vectors, and this is needlessly slowing down the
> > program. Also, at the end of each iteration, you define
> >   bio <- cbind(W, C, ASMR, SMR, A, F, U, SDA, Gr, Ed, GHg, EGK, Hg)
> > which creates a matrix. But you only ever use this matrix right at the
> > end, and so there is no need to create this 365*14 matrix at every single
> > iteration.
> > 
> > It looks to me as if you took some Excel code and translated it directly
> > into R. This will not produce efficient R code. Your iterative loop would
> > be more naturally expressed in R as
> > 
> > f <- function(q) {
> >   p <- q[[1]]
> >   ACT <- q[[2]]
> >   # cat(paste("Trying p=",p," ACT=",ACT,"\n",sep=""))
> >   state <- c(W=Wo,Hg=Hgo)
> >   numdays <- length(temps)
> >   for (i in 1:numdays)
> >     state <- updateState(state,
> >                          jday=temps$jday[i],temp=temps$Temp[i],M=numdays,
> >                          p=p,ACT=ACT)
> >   Wtmod <- state[["W"]]
> >   Hgtmod <- state[["Hg"]]
> >   (Wt-Wtmod)^2/Wt + (Hgt-Hgtmod)^2/Hgt
> >   }
> > 
> > updateState <- function(state,jday,temp,M,p,ACT) {
> >   # Given W[i-1] and Hg[i-1], want to compute W[i] and Hg[i]
> >   W <- state[["W"]]
> >   Hg <- state[["Hg"]]
> >   # First compute certain parameters: Vc[i-1] ... Expegk[i-1]
> >   Vc <- (CTM-temp)/(CTM-CTO)
> >   Vr <- (RTM-temp)/(RTM-RTO)
> >   C <-      p * CA * W^CB * Vc^Xc * exp(Xc*(1-Vc)) * Pc
> >   ASMR <- ACT * RA * W^RB * Vr^Xa * exp(Xa*(1-Vr))
> >   ...
> >   # Now find W[i] and Hg[i]
> >   Wnew <- if (!(jday==121 && Mat==1)) W+Gr/Ef
> >           else                        W * (1-GSI*1.2)
> >   Hgnew <- a*Hgp*C*(1-Expegk)/(Pc*W*EGK) + Hg*Expegk
> >   c(W=Wnew,Hg=Hgnew)
> >   }
> > 
> > In this code, I do not attempt to keep the entire array in memory. All I
> > need to know at each iteration is the value of state=(W,Hg) at time i-1,
> > and from this I compute the new value at time i.
> > 
> > 3. You use some thoroughly weird code to read in a table. You should add a
> > row to the top of your table with variable names, then just use
> >   temps <- read.table("TEMP.DAT", header=TRUE)
> >   temps$Vc <- (CTM-temps$temp)/(CTM-CTO)
> > This would also avoid leaving global variables (like Day) hanging around
> > the place. Global variables cause confusion: see the next point.
> > 
> > 4. Here are some lines taken from your code.
> > 
> > p <- NULL
> > ACT <- NULL
> > 
> > #starting values for p, ACT
> > p <- 1
> > ACT <- 2
> > 
> > f <- function (q)
> >   {
> >   F[i]<- (FA*((comp[i,3])^FB)*(exp(FG*p))*C[i])
> >   # (and ACT is never referred to)
> >   }
> > 
> > Why did you define p<-NULL and ACT<-NULL at the top? Those definitions are
> > irrelevant, because they are overridden by p<-1 and ACT<-2.
> > 
> > In the body of your function f, in defining F[i], you refer to the
> > variable p. The only assignment to p is in the line p<-1. I strongly
> > suspect this is an error. Probably you want to refer to q[1]. The best way
> > to do this (as you can see from my code above) is to define p and ACT at
> > the beginning of f.
> > 
> > 5. Some minor comments on code. It's unwise to use T or F as variable
> > names in R, because of the potential for confusion with S-Plus, which uses
> > them for TRUE and False. Also, you don't need all those brackets: A*(B*C)
> > is the same as A*B*C, and ((A/B)/C) is more transparently written as
> > A/(B*C). Also, you should indent your code, since otherwise you'll just
> > confuse yourself and other people.
> > 
> > 6. I've written a version of the code which takes all these comments into
> > account. It doesn't agree with your Excel solution. You haven't given us
> > enough real data for me to work out if there's a bug in my code or if the
> > Excel solution is wrong. Once you have worked out a function f which you
> > know to be correct (checked by drawing a contour plot), if you have any
> > more problems, share it with us and we may be able to help. 
> > 
> > Damon Wischik.
> > 
> > ______________________________________________
> > R-help at stat.math.ethz.ch mailing list
> > https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> >
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> 

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From ripley at stats.ox.ac.uk  Mon Aug  4 11:32:33 2003
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Mon, 4 Aug 2003 10:32:33 +0100 (BST)
Subject: [R] install.packages (newbie?) error
In-Reply-To: <BA99BC7E-C46F-11D7-AEA6-00039380A54E@bystroms.se>
Message-ID: <Pine.LNX.4.44.0308041028400.13354-100000@gannet.stats>

If you are using the Carbon version of R (there are *two* ports for MacOS 
X) then install.packages() is not included.  (Where have you been reading 
about it: under documentation for a different port?)

You need to get akima.sit from the /bin/macos/contrib area on CRAN.

On Sat, 2 Aug 2003, ?sa Bystr?m wrote:

> I recently installed R 1.7.1 on Mac OS 10.2.6, and I now need to 
> install the akima package. When I use install.packages(akima) or any 
> variation of that command, I get the following message:
> 
> Error: couldn't find function "install.packages"
> 
> (I haven't had any problems with other commands so far, so I don't 
> think there's a problem with the R installation. I also threw the whole 
> thing out and re-installed R from scratch, but that didn't help.)
> 
> I then tried simply downloading, unstuffing, and dragging the package

What do you mean by `the package'?  The sources, akima*.tar.gz or the 
MacOS binary akima.sit?
 
> to my library folder. But then, when I tried library(akima), another 
> error message said the package wasn't properly installed.

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From ripley at stats.ox.ac.uk  Mon Aug  4 11:34:48 2003
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Mon, 4 Aug 2003 10:34:48 +0100 (BST)
Subject: [R] help with graphics
In-Reply-To: <1059951727.3f2d946f4b29c@my.uq.edu.au>
Message-ID: <Pine.LNX.4.44.0308041033180.13354-100000@gannet.stats>

On Sun, 3 Aug 2003, Andrew C. Ward wrote:

> Did you have a "dev.off()" somewhere after barchart?
> Otherwise you never know what you might get in the png file.

dev.print() copies the current device, so using dev.off() would be wrong
(device no longer current).

See ?Lattice for why print(barchart(...)) is needed here.

> Quoting Erwan BARRET <erwan.barret at wanadoo.fr>:
> 
> > I'd like to use lattice to make graphics under
> > conditional structure like : 
> > if (TRUE) { barchart(....)
> > dev.print(png, file = "image1.png", width = 600)
> > }
> > but there's nothing in the output file. It seems that
> > it's too long to print graphic into the graphical output
> > and the saved file is a blank picture.
> > it works if I do that without the conditionnal
> > structure.
> > it doesn't work if I source a batch file
> > can you help me?
> > 
> > I'm using R1.6.2 under win98
> > Thanks
> > 	[[alternative HTML version deleted]]
> > 
> > ______________________________________________
> > R-help at stat.math.ethz.ch mailing list
> > https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> >
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> 

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From hennig at stat.math.ethz.ch  Mon Aug  4 11:44:46 2003
From: hennig at stat.math.ethz.ch (Christian Hennig)
Date: Mon, 4 Aug 2003 11:44:46 +0200 (CEST)
Subject: [R] Discussion: Spam on R-help
In-Reply-To: <XFMail.030803133917.Ted.Harding@nessie.mcc.ac.uk>
Message-ID: <Pine.LNX.4.44.0308041139280.1997-100000@florence>

Hi Ted,

the R-help list is maintained by Martin Maechler, ETH Zurich, 
who is not at work at the moment. We had various computer problems at the
ETH in the last week. It may be that the spam filter broke down somehow and
Martin is not here to fix it. I think the reason for the lots of spam are
temporary technical problems and I hope we will have a "filtered" mailing
list again in one or two weeks.

Christian

On Mon, 4 Aug 2003 Ted.Harding at nessie.mcc.ac.uk wrote:

> [re-sending this one since it apparently didn't get through yesterday]
> 
> Hi Folks,
> 
> I just had a look at the r-help archive for the still-young month of
> August, and happened to notice that out of 49 postings there, 20 were
> assorted spam of all-too-familiar types.
> 
> I'm wondering (discussion point) whether this might suggest that the
> list should adopt a more restrictive policy. Apparently, as things
> now stand, anyone may post to the list.
> 
> I faced the same problem some months ago when two lists which I run
> (linux-users at mcc.ac.uk and man-lug at mcc.ac.uk) were being hit by spam
> at a rate which was becoming irritating. With great reluctance, I came
> to the conclusion that the lists would have to be restricted to "posting
> by members only". This has had the knock-on effect that both spam and
> also occasional genuine worthwhile postings by non-members end up being
> sent to me for approval. Fortunately in the case of these lists, the
> frequency of this remains at an acceptable level -- say a dozen or so a
> week, of which almost all are spam and are easily dealt with, with no
> regrets if there's any delay before I get round to it. Also there are
> not huge numbers of subscribers -- the overall level of traffic is
> relatively low.
> 
> Another knock-on effect is tbat when I get a subscription request from
> an email address that is not obviously bona-fide, I feel obliged to
> send off a mail to that address asking for confirmation of genuine
> interest in Linux (I have a pro-forma template for this). This is
> to prevent people subscribing to the list in order either to send out
> spam via the list or to harvest email addresses of list subscribers.
> For the same reasons, access to the lists' archives and to subscriber
> lists is password-restricted to subscribers.
> 
> It is likely to be otherwise with r-help. This has a fairly high level
> of traffic, many people may welcome the opportunity to post messages
> without subscribing, and the list-owners might find themselves forced to
> devote more time than they would welcome to dealing not only with spam
> but also with non-member postings, if the list became "members-only".
> There are many subscribers: this too would add to the list-owners' burden
> if they had to "manage" all that.
> 
> On the other hand there is the possibility of spam-blocking software.
> This works up to a point, and provided the amount of spam that slipped
> through was small it could be an acceptable solution. At the same time,
> it's all too easy for genuine messages to get blocked if the spam-filter
> triggers are comprehensive enough to trap most spam (in a statistical
> context, SEX may be a factor; and there are many other triggers e.g. in
> the 'cluster' package).
> 
> Yet again, people can simply accept that spam, like flies in Summer,
> is one of those irritations that must simply be ignored: "just hit
> delete".
> 
> Anyway, as I said, a point for discussion. What do people think?
> 
> Best wishes to all,
> Ted.
> 
> PS: Just to make clear that I'm not _complaining_ about receiving spam
> via r-help. Those 20 messages would be only a tiny fraction of the
> hundreds of times I have "hit delete" since 1 August!
> 
> 
> --------------------------------------------------------------------
> E-Mail: (Ted Harding) <Ted.Harding at nessie.mcc.ac.uk>
> Fax-to-email: +44 (0)870 167 1972
> Date: 03-Aug-03                                       Time: 13:39:17
> ------------------------------ XFMail ------------------------------
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> 

-- 
***********************************************************************
Christian Hennig
Seminar fuer Statistik, ETH-Zentrum (LEO), CH-8092 Zuerich (currently)
and Fachbereich Mathematik-SPST/ZMS, Universitaet Hamburg
hennig at stat.math.ethz.ch, http://stat.ethz.ch/~hennig/
hennig at math.uni-hamburg.de, http://www.math.uni-hamburg.de/home/hennig/
#######################################################################
ich empfehle www.boag.de



From Simon.Fear at synequanon.com  Mon Aug  4 12:09:19 2003
From: Simon.Fear at synequanon.com (Simon Fear)
Date: Mon, 4 Aug 2003 11:09:19 +0100
Subject: [R] Problem with data.frames
Message-ID: <6C8A8033ABC1E3468048ABC4F13CE572AC0D3A@synequanon01>

Peter is right, but there is a point here: it would be nice if
attach(my.dframe) would do nothing - or at least warn - if my.dframe was
already in the search list. Like library(). Attaching twice is almost
bound
to be an error?
Of course in many circumstances it might be better to use with() than
attach() - if you haven't come across with(), it works like attach()
with an
inbuilt detach() when the parentheses close.

Aside 1: it would also be nice if ?attach pointed to ?with. Is this kind
of
suggestion best sent to r-help or r-devel?

Aside 2: is with() efficient, or does it create a copy of the dataset? 

SF


-----Original Message-----
From: Peter Dalgaard BSA [mailto:p.dalgaard at biostat.ku.dk]
Sent: 04 August 2003 00:52
To: Andreas Eckner
Cc: r-help at stat.math.ethz.ch
Subject: Re: [R] Problem with data.frames


Security Warning:
If you are not sure an attachment is safe to open please contact 
Andy on x234. There are 0 attachments with this message.
________________________________________________________________

Andreas Eckner <andreas.eckner at soundinvest.net> writes:

> Hi,
> 
> I just encountered a problem in R that may easily be fixed: If one
uses
> attach for a data.frame e.g. 10000 times and forgets detach, then R
gets
> incredibly slow (less then 10% of the original speed).

R also gets incredibly slow if you create 10000 copies of your data
set, which is effectively the same thing! The fix is: Don't do that... 

-- 
   O__  ---- Peter Dalgaard             Blegdamsvej 3  
  c/ /'_ --- Dept. of Biostatistics     2200 Cph. N   
 (*) \(*) -- University of Copenhagen   Denmark      Ph: (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)             FAX: (+45) 35327907

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://www.stat.math.ethz.ch/mailman/listinfo/r-help
 

Simon Fear
Senior Statistician
Syne qua non Ltd
Tel: +44 (0) 1379 644449
Fax: +44 (0) 1379 644445
email: Simon.Fear at synequanon.com
web: http://www.synequanon.com
 
Number of attachments included with this message: 0
 
This message (and any associated files) is confidential and\...{{dropped}}



From ripley at stats.ox.ac.uk  Mon Aug  4 13:06:27 2003
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Mon, 4 Aug 2003 12:06:27 +0100 (BST)
Subject: [R] Problem with data.frames
In-Reply-To: <6C8A8033ABC1E3468048ABC4F13CE572AC0D3A@synequanon01>
Message-ID: <Pine.LNX.4.44.0308041149550.13918-100000@gannet.stats>

On Mon, 4 Aug 2003, Simon Fear wrote:

> Peter is right, but there is a point here: it would be nice if
> attach(my.dframe) would do nothing - or at least warn - if my.dframe was
> already in the search list. Like library(). Attaching twice is almost

It is not like library().  attach attaches a copy of the data frame, and
it can be altered subsequently. So how can you tell if the data frame is
on the search list.  I suspect it is quite common to create an object
called `tmp' and put it on the search list for a while.  library() assumes
that you have not reinstalled a loaded package (and if you have you would
get inconsistent results).

> bound
> to be an error?

Not at all.

> Of course in many circumstances it might be better to use with() than
> attach() - if you haven't come across with(), it works like attach()
> with an
> inbuilt detach() when the parentheses close.
> 
> Aside 1: it would also be nice if ?attach pointed to ?with. Is this kind
> of
> suggestion best sent to r-help or r-devel?

I don't see they do the same job, especially not in an interactive 
session (which is when attach() is most usefully used in this way).

> Aside 2: is with() efficient, or does it create a copy of the dataset? 

It does not create a copy, as you can see from the code (it uses a 
three-argument eval).

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From p.dalgaard at biostat.ku.dk  Mon Aug  4 13:31:28 2003
From: p.dalgaard at biostat.ku.dk (Peter Dalgaard BSA)
Date: Mon, 04 Aug 2003 11:31:28 -0000
Subject: [R] Problem with data.frames
In-Reply-To: <6C8A8033ABC1E3468048ABC4F13CE572AC0D3A@synequanon01>
References: <6C8A8033ABC1E3468048ABC4F13CE572AC0D3A@synequanon01>
Message-ID: <x21xw1mz61.fsf@biostat.ku.dk>

"Simon Fear" <Simon.Fear at synequanon.com> writes:

> Peter is right, but there is a point here: it would be nice if
> attach(my.dframe) would do nothing - or at least warn - if my.dframe was
> already in the search list. Like library(). Attaching twice is almost
> bound
> to be an error?

The problem is, how can you tell? Consider

f <- function() {
    d <- data.frame(x=1:10,y=rnorm(10))
    attach(d)
}
f()
f()

This attaches two *different* dataframes, both called "d".


> Of course in many circumstances it might be better to use with() than
> attach() - if you haven't come across with(), it works like attach()
> with an
> inbuilt detach() when the parentheses close.

I do occasionally speculate whether it would be possible to change
attach() to be more like with(), but it's tricky business.

> Aside 1: it would also be nice if ?attach pointed to ?with. Is this kind
> of
> suggestion best sent to r-help or r-devel?

R-devel, in principle, but developers will pick up hints from all
over. R-bugs can also be used if you want to make sure that a
suggestion is recorded (it may drown in the spam for a while,
though...) R-bugs is best used for clear bugs, but a well-defined
enhancement request can also get in there (do remember to label
it as such, or it might generate a host of "Not a bug" responses.)

> Aside 2: is with() efficient, or does it create a copy of the dataset? 

It converts the data frame to an environment, so I would suspect that
it has to copy, but these things are "black magic": The paradigm in R
is that of a functional language, where assignment merely attaches a
label to computed objects. Objects are generally copied rather than
modified in-place, but to avoid too much inefficiency there are a lot
of special cases where we try to eliminate copying if possible.


-- 
   O__  ---- Peter Dalgaard             Blegdamsvej 3  
  c/ /'_ --- Dept. of Biostatistics     2200 Cph. N   
 (*) \(*) -- University of Copenhagen   Denmark      Ph: (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)             FAX: (+45) 35327907



From Simon.Fear at synequanon.com  Mon Aug  4 13:57:38 2003
From: Simon.Fear at synequanon.com (Simon Fear)
Date: Mon, 4 Aug 2003 12:57:38 +0100
Subject: [R] Problem with data.frames
Message-ID: <6C8A8033ABC1E3468048ABC4F13CE572AC0D3B@synequanon01>

What I meant by "like library()" is that this function ensures that a
library
(or should I say package?) does not get loaded twice / duplicated in the
search path. Of course attach() is not the same as library(). Nor is it
the
same as with() - I just thought it might be useful if the help pages
pointed
to each other. Because I have programmed for years using attach() and
detach() and only very recently discovered with() (which is nearly
always
what I want since I work mostly with scripts).

> So how can you tell if the data frame is on the search list

"my.dframe" %in% search() ?

Does anyone have an example of a case where attaching the same data
frame
twice would be useful?

Thanks Prof R. for (another) reminder to read the code - I'm always
forgetting to look there. RTFC, Simon!

SF

-----Original Message-----
From: Prof Brian Ripley [mailto:ripley at stats.ox.ac.uk]
Sent: 04 August 2003 12:06
To: Simon Fear
Cc: Peter Dalgaard BSA; Andreas Eckner; r-help at stat.math.ethz.ch
Subject: RE: [R] Problem with data.frames


On Mon, 4 Aug 2003, Simon Fear wrote:

> Peter is right, but there is a point here: it would be nice if
> attach(my.dframe) would do nothing - or at least warn - if my.dframe
was
> already in the search list. Like library(). Attaching twice is almost

It is not like library().  attach attaches a copy of the data frame, and
it can be altered subsequently. So how can you tell if the data frame is
on the search list.  I suspect it is quite common to create an object
called `tmp' and put it on the search list for a while.  library()
assumes
that you have not reinstalled a loaded package (and if you have you
would
get inconsistent results).

> bound
> to be an error?

Not at all.

> Of course in many circumstances it might be better to use with() than
> attach() - if you haven't come across with(), it works like attach()
> with an
> inbuilt detach() when the parentheses close.
> 
> Aside 1: it would also be nice if ?attach pointed to ?with. Is this
kind
> of
> suggestion best sent to r-help or r-devel?

I don't see they do the same job, especially not in an interactive 
session (which is when attach() is most usefully used in this way).

> Aside 2: is with() efficient, or does it create a copy of the dataset?
It does not create a copy, as you can see from the code (it uses a 
three-argument eval).

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595
 

Simon Fear
Senior Statistician
Syne qua non Ltd
Tel: +44 (0) 1379 644449
Fax: +44 (0) 1379 644445
email: Simon.Fear at synequanon.com
web: http://www.synequanon.com
 
Number of attachments included with this message: 0
 
This message (and any associated files) is confidential and\...{{dropped}}



From ripley at stats.ox.ac.uk  Mon Aug  4 14:21:20 2003
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Mon, 4 Aug 2003 13:21:20 +0100 (BST)
Subject: [R] Problem with data.frames
In-Reply-To: <6C8A8033ABC1E3468048ABC4F13CE572AC0D3B@synequanon01>
Message-ID: <Pine.LNX.4.44.0308041316160.19672-100000@gannet.stats>

On Mon, 4 Aug 2003, Simon Fear wrote:

> What I meant by "like library()" is that this function ensures that a
> library
> (or should I say package?) does not get loaded twice / duplicated in the
> search path. Of course attach() is not the same as library(). Nor is it
> the
> same as with() - I just thought it might be useful if the help pages
> pointed
> to each other. 

Yes. The strange thing to me is that with does not point to attach.
We'll add xrefs.

> Because I have programmed for years using attach() and
> detach() and only very recently discovered with() (which is nearly
> always
> what I want since I work mostly with scripts).
> 
> > So how can you tell if the data frame is on the search list
> 
> "my.dframe" %in% search() ?

That says if a data frame *of that name* is on the search list, and my 
point was that whereas packages are responably identified by name, data 
frames are not.  It's common to use DF tmp repeatedly.


-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From xiao.gang.fan1 at libertysurf.fr  Mon Aug  4 14:27:34 2003
From: xiao.gang.fan1 at libertysurf.fr (=?iso-8859-1?Q?Jean_Fan?=)
Date: Mon,  4 Aug 2003 14:27:34 +0200
Subject: =?iso-8859-1?Q?Re:_[R]_Excel_can_do_what_R_can't=3F=3F=3F=3F=3F?=
Message-ID: <HJ3H9Y$DDE68E169FCDF9487E5022DD195C7BFC@tiscali.fr>

Dear Professor Ripley,

I'm little confused of your reply: do you mean that GRG would 
not be a standard optimization algorithm, so it couldn't be 
better than what exist in R ?

I'm not a specialist of numerical optimization algorithms,
but it seems that GRG is actually implemented in several
specialized optimisation toolbox (sure generally commercial), 
not only the limited one in Excel. 
And with google, search "GRG generalized reduced gradient" is 
giving 424 links. 

--
Fan

> I've found that the discussions are interesting, generally 
> speaking, peoples seem equally confident on R's optim/nlm and 
> Excel's solver.
> 
> The authors of the algorithm GRG2 (Generalized Reduced Gradient)
> are not cited in the documentation of optim(), so I'm wondering if
> the optimization algorithm implemented in Excel is "fondamentally"
> the same than that in R ?

I don't suppose Excel cites the method*s* used in optim() either,
but GRG2 is not in the index of my copies of any of the standard texts on numerical optimization.



********** L'ADSL A 20 EUR/MOIS**********
Tiscali propose l'ADSL le moins cher du march? : 20 EUR/mois et le modem ADSL offert ! 
Pour profiter de cette offre exceptionnelle, cliquez ici : http://register.tiscali.fr/adsl/
Offre soumise ? conditions.



From petrov at unibel.by  Mon Aug  4 15:16:29 2003
From: petrov at unibel.by (Sergei V. Petrov)
Date: Mon, 04 Aug 2003 16:16:29 +0300
Subject: [R] Breusch-Godfrey Test
Message-ID: <3F2E5CAD.6000504@unibel.by>

Dear R Helpers!

bgtest{lmtest} performs the Breusch-Godfrey test for higher order serial 
correlation.

Is the Higher Order Correlation function already programmed in
R I couldn't find it?

Sergei Petrov



From ripley at stats.ox.ac.uk  Mon Aug  4 14:39:57 2003
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Mon, 4 Aug 2003 13:39:57 +0100 (BST)
Subject: =?iso-8859-1?Q?Re:_[R]_Excel_can_do_what_R_can't=3F=3F=3F=3F=3F?=
In-Reply-To: <HJ3H9Y$DDE68E169FCDF9487E5022DD195C7BFC@tiscali.fr>
Message-ID: <Pine.LNX.4.44.0308041328400.19730-100000@gannet.stats>

On Mon, 4 Aug 2003, Jean Fan wrote:

> Dear Professor Ripley,
> 
> I'm little confused of your reply: do you mean that GRG would 
> not be a standard optimization algorithm, so it couldn't be 
> better than what exist in R ?

Not at all.  I meant what I actually said (and I said nothing about which
was better).  To spell it out:

First optim() has five different methods, so Excel's could be at most 
essentially the same as *one* of them.

Second, none of them are called GRG2 by their authors.

Third, GRG2 by that name does not appear in the index of my books.

> I'm not a specialist of numerical optimization algorithms,
> but it seems that GRG is actually implemented in several
> specialized optimisation toolbox (sure generally commercial), 
> not only the limited one in Excel. 
> And with google, search "GRG generalized reduced gradient" is 
> giving 424 links. 

So now you will be able to work out how it differs, it at all, from the 
methods of R (which are fully documented).

But looking at just the first reference suggests that GRG is not intended 
for unconstrained or box-constrained problems, those covered by optim(),
and that it is a class of methods rather than an actual algorithm.


> 
> --
> Fan
> 
> > I've found that the discussions are interesting, generally 
> > speaking, peoples seem equally confident on R's optim/nlm and 
> > Excel's solver.
> > 
> > The authors of the algorithm GRG2 (Generalized Reduced Gradient)
> > are not cited in the documentation of optim(), so I'm wondering if
> > the optimization algorithm implemented in Excel is "fondamentally"
> > the same than that in R ?
> 
> I don't suppose Excel cites the method*s* used in optim() either,
> but GRG2 is not in the index of my copies of any of the standard texts on numerical optimization.


-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From Bernhard.Pfaff at drkw.com  Mon Aug  4 14:47:05 2003
From: Bernhard.Pfaff at drkw.com (Pfaff, Bernhard)
Date: Mon, 4 Aug 2003 14:47:05 +0200
Subject: [R] Breusch-Godfrey Test
Message-ID: <18D602BD42B7E24EB810D6454A58DB9004730515@ibfftce505.is.de.dresdnerkb.com>

> Dear R Helpers!
> 
> bgtest{lmtest} performs the Breusch-Godfrey test for higher 
> order serial 
> correlation.
> 
> Is the Higher Order Correlation function already programmed in
> R I couldn't find it?
> 
> Sergei Petrov
> 

?acf
?pacf

HTH,

Bernhard


----------------------------------------------------------------------
If you have received this e-mail in error or wish to read our e-mail 
disclaimer statement and monitoring policy, please refer to 
http://www.drkw.com/disc/email/ or contact the sender.



From savano at pegasusnet.com.br  Mon Aug  4 15:04:54 2003
From: savano at pegasusnet.com.br (Savano)
Date: Mon, 04 Aug 2003 10:04:54 -0300
Subject: [R] BHHH algorithm
Message-ID: <5.2.1.1.0.20030804100152.00b8e4b8@pegasusnet.com.br>

Dear R users,

Could you tell me where a I find some references of BHHH algorithm ? I need 
write it in R.

Thank you.



From canteri at itc.it  Mon Aug  4 15:11:31 2003
From: canteri at itc.it (Roberto Canteri)
Date: Mon, 4 Aug 2003 15:11:31 +0200
Subject: [R] label chemical formulas
Message-ID: <007401c35a89$ebf3aae0$0d00460a@pc.itc.it>

Dear r-help,
the prcomp matrix (x$rotation) I have, has the columns labels as chemical
formulas (i.e: C2H3, 37Cl,...,C10H24N3O3).
Plotting them as plain text I do the following with success:

xlabs <- dimnames(y$rotation)[[1]]
 xlabs <- as.character(xlabs)
 plot(y$rotation[,1],y$rotation[,2], ann=FALSE, type="p", col="red", pch=20)
 text(y$rotation[,1],y$rotation[,2], labels=xlabs, cex=0.7, font=1, pos=4,
offset=0.2)

 I do not succed to plot the labels with subscripts and superscripts. I did many
attempt trying with expression() or substitute() but without success. I can
write the labels in the data matrix as needed (i.e.: C[2]*H[3]) to plot properly
if I would know how.
I peg your pardon if this is probably a silly problem, I am new user of R (I am
using R 1.7.0 on win2k).
Many thanks in advance for any help.
Roberto

Roberto Canteri

Research Scientist
Physics-Chemistry of Surfaces and Interfaces Division |
ITC-irst                                                                     |
Tel. (+39) 0461 314 486
Centre for the scientific and technological research      | Fax  (+39) 0461 810
851
Via Sommarive, 18                                                    | E-mail:
canteri at itc.it
38050 Povo - TRENTO - Italy                                      | Web Page:
http://www.itc.it/



From jfox at mcmaster.ca  Mon Aug  4 15:13:07 2003
From: jfox at mcmaster.ca (John Fox)
Date: Mon, 4 Aug 2003 09:13:07 -0400
Subject: [R] Problem with data.frames
Message-ID: <20030804131704.UEFF4447.tomts8-srv.bellnexxia.net@john-8100>

Dear Simon,

A solution that skirts the issue is to introduce a new function -- something like

Attach <- function (what, pos=2, name=deparse(substitute(what))) {
    detach(pos = match(name, search()))
    attach(get(name, envir=.GlobalEnv), pos=pos, name=name)
    }

Then, when the previously attached version of the object is different from the current one, the previous version is detached. I think that this is the behaviour that would usually be desired, though there might be occasions where you want to preserve the previous version.

Regards,
 John

----------------------------------------------------
John Fox
Department of Sociolgy
McMaster University
http://socserv.mcmaster.ca/jfox
----------------------------------------------------

> ------------Original Message-------------
> From: "Simon Fear" <Simon.Fear at synequanon.com>
> To: "Prof Brian Ripley" <ripley at stats.ox.ac.uk>
> Cc: Peter Dalgaard BSA <p.dalgaard at biostat.ku.dk>, r-help at stat.math.ethz.ch, Andreas Eckner <andreas.eckner at soundinvest.net>
> Date: Mon, Aug-4-2003 8:04 AM
> Subject: RE: [R] Problem with data.frames
> 
> What I meant by "like library()" is that this function ensures that a
> library
> (or should I say package?) does not get loaded twice / duplicated in the
> search path. Of course attach() is not the same as library(). Nor is it
> the
> same as with() - I just thought it might be useful if the help pages
> pointed
> to each other. Because I have programmed for years using attach() and
> detach() and only very recently discovered with() (which is nearly
> always
> what I want since I work mostly with scripts).
> 
> > So how can you tell if the data frame is on the search list
> 
> "my.dframe" %in% search() ?
> 
> Does anyone have an example of a case where attaching the same data
> frame
> twice would be useful?
> 
> Thanks Prof R. for (another) reminder to read the code - I'm always
> forgetting to look there. RTFC, Simon!
> 
> SF
> 
> -----Original Message-----
> From: Prof Brian Ripley [mailto:ripley at stats.ox.ac.uk]
> Sent: 04 August 2003 12:06
> To: Simon Fear
> Cc: Peter Dalgaard BSA; Andreas Eckner; r-help at stat.math.ethz.ch
> Subject: RE: [R] Problem with data.frames
> 
> 
> On Mon, 4 Aug 2003, Simon Fear wrote:
> 
> > Peter is right, but there is a point here: it would be nice if
> > attach(my.dframe) would do nothing - or at least warn - if my.dframe
> was
> > already in the search list. Like library(). Attaching twice is almost
> 
> It is not like library().  attach attaches a copy of the data frame, and
> it can be altered subsequently. So how can you tell if the data frame is
> on the search list.  I suspect it is quite common to create an object
> called `tmp' and put it on the search list for a while.  library()
> assumes
> that you have not reinstalled a loaded package (and if you have you
> would
> get inconsistent results).
> 
> > bound
> > to be an error?
> 
> Not at all.
> 
> > Of course in many circumstances it might be better to use with() than
> > attach() - if you haven't come across with(), it works like attach()
> > with an
> > inbuilt detach() when the parentheses close.
> > 
> > Aside 1: it would also be nice if ?attach pointed to ?with. Is this
> kind
> > of
> > suggestion best sent to r-help or r-devel?
> 
> I don't see they do the same job, especially not in an interactive 
> session (which is when attach() is most usefully used in this way).
> 
> > Aside 2: is with() efficient, or does it create a copy of the dataset?
> It does not create a copy, as you can see from the code (it uses a 
> three-argument eval).
> 
> -- 
> Brian D. Ripley,                  ripley at stats.ox.ac.uk
> Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
> University of Oxford,             Tel:  +44 1865 272861 (self)
> 1 South Parks Road,                     +44 1865 272866 (PA)
> Oxford OX1 3TG, UK                Fax:  +44 1865 272595
>  
> 
> Simon Fear
> Senior Statistician
> Syne qua non Ltd
> Tel: +44 (0) 1379 644449
> Fax: +44 (0) 1379 644445
> email: Simon.Fear at synequanon.com
> web: http://www.synequanon.com
>  
> Number of attachments included with this message: 0
>  
> This message (and any associated files) is confidential and\...{{dropped}}
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> 
>



From znmeb at aracnet.com  Mon Aug  4 15:21:20 2003
From: znmeb at aracnet.com (M. Edward Borasky)
Date: Mon, 4 Aug 2003 06:21:20 -0700
Subject: [R] Discussion: Spam on R-help
In-Reply-To: <XFMail.030803133917.Ted.Harding@nessie.mcc.ac.uk>
Message-ID: <000e01c35a8b$4f17a710$74c463d8@plaza.ds.adp.com>

Well ... I have SpamCop on my incoming e-mail, and it snagged every one of
the beasts ... *And* reported them to whatever authorities SpamCop has
available to handle spam reports. Those few ISPs that listen to SpamCop
reports will chastise the spammers for their aggression.

SpamCop costs $30US a year. If there's a way to hook it up to the incoming
port on the R-help mailing list, I'd be willing to contribute $30. It also
catches viruses for those of us blessed (?) with Windows.

-- 
M. Edward (Ed) Borasky
mailto:znmeb at borasky-research.net
http://www.borasky-research.net
 
"Suppose that tonight, while you sleep, a miracle happens - you wake up
tomorrow with what you have longed for! How will you discover that a miracle
happened? How will your loved ones? What will be different? What will you
notice? What do you need to explode into tomorrow with grace, power, love,
passion and confidence?" -- L. Michael Hall, PhD



From roger at ysidro.econ.uiuc.edu  Mon Aug  4 15:28:05 2003
From: roger at ysidro.econ.uiuc.edu (Roger Koenker)
Date: Mon, 4 Aug 2003 08:28:05 -0500 (CDT)
Subject: [R] BHHH algorithm
In-Reply-To: <5.2.1.1.0.20030804100152.00b8e4b8@pegasusnet.com.br>
Message-ID: <Pine.SOL.4.30.0308040826470.17146-100000@ysidro.econ.uiuc.edu>

the original reference is:

Berndt, Hall, Hall and Hausman, (1974)
Annals of economic and social measurement
3, 653-665.


url:	www.econ.uiuc.edu/~roger/my.html	Roger Koenker
email	rkoenker at uiuc.edu			Department of Economics
vox: 	217-333-4558				University of Illinois
fax:   	217-244-6678				Champaign, IL 61820

On Mon, 4 Aug 2003, Savano wrote:

> Dear R users,
>
> Could you tell me where a I find some references of BHHH algorithm ? I need
> write it in R.
>
> Thank you.
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
>



From Simon.Fear at synequanon.com  Mon Aug  4 15:27:17 2003
From: Simon.Fear at synequanon.com (Simon Fear)
Date: Mon, 4 Aug 2003 14:27:17 +0100
Subject: [R] Problem with data.frames
Message-ID: <6C8A8033ABC1E3468048ABC4F13CE572AC0D3C@synequanon01>

Peter, Brian, and list,

sorry if I was not clear (and obviously I wasn't),  I do fully accept
that
different data frames *can* be given the same names. Then you'd have to
access the first one created with an explicit get() to some specific
point in
the search list - or explicitly placed there using pos= - if indeed you
could
remember which one you wanted.

I think my point is that duplicating names strikes me as horrendous
programming style - why on earth would I ever need to have two
deliberately
different objects coexisting with the same name?  Or the same object,
with
the same name, more than once?

Maybe I am missing the point in that I don't really use R interactively,
I
always source() or copy and paste to the prompt, so I probably think of
debugging structured programs more than some users.

If I can't convince everyone that this type of name duplication is best
treated as an error, surely a warning would be good? At least an
optional
argument ...

In Peter D's example, the second invocation of f() should presumably
create a
"new" d? That would suggest TWO new arguments, replace= and warn=.
Currently
both false. And Brian R. likes to have several tmp's. I just can't see
why.
(Well, actually, I can see that it saves the bother of detaching(), and
in an
interactive session with nobody looking over your shoulder, who cares?)
But
if I need more than one temporary dataframe I call them tmp1, tmp2, etc.
I
just don't like the idea of a load of old tmp's filling up my precious
memory
space and then not remembering which one was which. Surely it's a bug
waiting
to pounce (not sure about that metaphor, sorry).

SF


PS Before anyone tells me: name duplication is intentional and very
handy in
R - when writing and nesting functions. But then it is different.
Lexical
scope ensures that the most recently defined instance is used when the
name
is used (unless you do something psychopathic, such as use an explicit
assign
outside the current frame). But these local instances disappear when the
function closes. This is not the case for variables deliberately placed
in
the search path, which are meant to be "global", and in frame 1 (I
think. I'm
getting out of my depth here - spent too long using Splus).
In Peter D's example you have to be very careful that you don't assign
"x" in
.GlobalEnv else it will mask the one you probably wanted, which is d$x
or
get("x",pos=2) depending how many times you've called f().
I'm beginning to go off the idea of attach()ing dataframes altogether,
the
more I think about it.
 

Simon Fear
Senior Statistician
Syne qua non Ltd
Tel: +44 (0) 1379 644449
Fax: +44 (0) 1379 644445
email: Simon.Fear at synequanon.com
web: http://www.synequanon.com
 
Number of attachments included with this message: 0
 
This message (and any associated files) is confidential and\...{{dropped}}



From ripley at stats.ox.ac.uk  Mon Aug  4 15:33:58 2003
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Mon, 4 Aug 2003 14:33:58 +0100 (BST)
Subject: [R] Problem with data.frames
In-Reply-To: <6C8A8033ABC1E3468048ABC4F13CE572AC0D3C@synequanon01>
Message-ID: <Pine.LNX.4.44.0308041429060.19910-100000@gannet.stats>

Think about what happens if you call a function recursively, e.g. by 
Recall(), and that function includes an attach/detach pair.

I believe it is your vision that is too limited, not other people's.

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From ripley at stats.ox.ac.uk  Mon Aug  4 15:53:24 2003
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Mon, 4 Aug 2003 14:53:24 +0100 (BST)
Subject: [R] Discussion: Spam on R-help
In-Reply-To: <000e01c35a8b$4f17a710$74c463d8@plaza.ds.adp.com>
Message-ID: <Pine.LNX.4.44.0308041441500.19910-100000@gannet.stats>

Christian Hennig has already replied that there have been serious
computing disruptions at ETH (as can be seen from the delays on the R
lists), and Martin Maechler is away at present.

Normally there is extensive spam protection in place, and Martin has done
a lot of work to ensure that you don't see things.  It really is unfair to
Martin in particular to jump in on one of the rare occasions when things
go wrong.  He deserves praise for the normal situation, and perhaps we 
need to turn the checks off from time to time so people appreciate the 
excellent work done on their behalf.

Can we please drop this thread until Martin returns and has the time to 
investigate?

On Mon, 4 Aug 2003, M. Edward Borasky wrote:

> Well ... I have SpamCop on my incoming e-mail, and it snagged every one of
> the beasts ... *And* reported them to whatever authorities SpamCop has
> available to handle spam reports. Those few ISPs that listen to SpamCop
> reports will chastise the spammers for their aggression.
> 
> SpamCop costs $30US a year. If there's a way to hook it up to the incoming
> port on the R-help mailing list, I'd be willing to contribute $30. It also
> catches viruses for those of us blessed (?) with Windows.

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From Philippe.Hupe at curie.fr  Mon Aug  4 15:54:05 2003
From: Philippe.Hupe at curie.fr (=?ISO-8859-1?Q?Philippe_Hup=E9?=)
Date: Mon, 04 Aug 2003 15:54:05 +0200
Subject: [R] how to set the variable name in a loop
In-Reply-To: <3F2E5CAD.6000504@unibel.by>
References: <3F2E5CAD.6000504@unibel.by>
Message-ID: <3F2E657D.6040309@curie.fr>

Hello,

I would like to have variables whose name are var1, var2, ... in a loop :

for (i in 1:10)
{
    var(i) <- i # where var(i) is var1, ....
}

Thanks in advance

Philippe



From phgrosjean at sciviews.org  Mon Aug  4 16:07:56 2003
From: phgrosjean at sciviews.org (Philippe Grosjean)
Date: Mon, 4 Aug 2003 16:07:56 +0200
Subject: [R] ^ operation much slower in R 1.7.1 than in R 1.7.0 ???
Message-ID: <MABBLJDICACNFOLGIHJOOEHLDKAA.phgrosjean@sciviews.org>

I do not understand what happens here (under Win XP):

a <- abs(matrix(rnorm(800*800)/2, ncol=800, nrow=800))
system.time(b <- a^1000)[3]

took about 1 sec on my computer with R 1.7.0 and it takes now 4.59 sec with
R 1.7.1

Similarly,

phi <- 1.6180339887498949
a <- floor(runif(750000)*1000)
system.time(b <- (phi^a - (-phi)^(-a))/sqrt(5))[3]

took about 0.9 sec with R 1.7.0, and it takes 11.8 sec (!!!) in R 1.7.1.

Are there some changes made between 1.7.0 and 1.7.1 that could cause such a
large difference in time to do such simple computations???

Best,

Philippe Grosjean


...........]<(({?<...............<?}))><...............................
 ) ) ) ) )
( ( ( ( (       Dr. Philippe Grosjean
 ) ) ) ) )
( ( ( ( (       LOV, UMR 7093
 ) ) ) ) )      Station Zoologique
( ( ( ( (       Observatoire Oc?anologique
 ) ) ) ) )      BP 28
( ( ( ( (       06234 Villefranche sur mer cedex
 ) ) ) ) )      France
( ( ( ( (
 ) ) ) ) )      tel: +33.4.93.76.38.18, fax: +33.4.93.76.38.34
( ( ( ( (
 ) ) ) ) )      e-mail: phgrosjean at sciviews.org
( ( ( ( (       SciViews project coordinator (http://www.sciviews.org)
 ) ) ) ) )
.......................................................................



From Bernhard.Pfaff at drkw.com  Mon Aug  4 16:27:24 2003
From: Bernhard.Pfaff at drkw.com (Pfaff, Bernhard)
Date: Mon, 4 Aug 2003 16:27:24 +0200
Subject: [R] how to set the variable name in a loop
Message-ID: <18D602BD42B7E24EB810D6454A58DB9004730516@ibfftce505.is.de.dresdnerkb.com>


> Hello,
> 
> I would like to have variables whose name are var1, var2, ... 
> in a loop :
> 
> for (i in 1:10)
> {
>     var(i) <- i # where var(i) is var1, ....
> }

how about:

for (i in 1:10){
  eval(parse(text=paste("var", i, "<-", i, sep="")))
}

HTH,
Bernhard




> 
> Thanks in advance
> 
> Philippe
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> 


----------------------------------------------------------------------
If you have received this e-mail in error or wish to read our e-mail 
disclaimer statement and monitoring policy, please refer to 
http://www.drkw.com/disc/email/ or contact the sender.



From ripley at stats.ox.ac.uk  Mon Aug  4 16:34:57 2003
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Mon, 4 Aug 2003 15:34:57 +0100 (BST)
Subject: [R] how to set the variable name in a loop
In-Reply-To: <18D602BD42B7E24EB810D6454A58DB9004730516@ibfftce505.is.de.dresdnerkb.com>
Message-ID: <Pine.LNX.4.44.0308041533060.19991-100000@gannet.stats>

Simpler (and easier to read and avoids a conversion of i to character and 
back) is

for (i in 1:10) assign(paste("var", i, sep=""), i)

Neither of you needed the braces.

On Mon, 4 Aug 2003, Pfaff, Bernhard wrote:

> 
> > Hello,
> > 
> > I would like to have variables whose name are var1, var2, ... 
> > in a loop :
> > 
> > for (i in 1:10)
> > {
> >     var(i) <- i # where var(i) is var1, ....
> > }
> 
> how about:
> 
> for (i in 1:10){
>   eval(parse(text=paste("var", i, "<-", i, sep="")))
> }
> 
> HTH,
> Bernhard
> 
> 
> 
> 
> > 
> > Thanks in advance
> > 
> > Philippe
> > 
> > ______________________________________________
> > R-help at stat.math.ethz.ch mailing list
> > https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> > 
> 
> 
> ----------------------------------------------------------------------
> If you have received this e-mail in error or wish to read our e-mail 
> disclaimer statement and monitoring policy, please refer to 
> http://www.drkw.com/disc/email/ or contact the sender.
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> 

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From jeanne.rey at gazdefrance.com  Mon Aug  4 17:37:23 2003
From: jeanne.rey at gazdefrance.com (Jeanne REY)
Date: Mon, 4 Aug 2003 16:37:23 +0100
Subject: [R] RDCOM interface and 17.1 R version troubles
Message-ID: <OF08FB9ABC.7906F062-ON41256D78.00553CB9@notes.edfgdf.fr>

Hello,

We are using the new version 1.7.1 of R with the R-(D)COM Interface 1.2
through Excel. But we failed using the ARIMA and SUMMARY functions: the
Statconnector doesn't work even if these functions work under R.
Please, could you explain us how to solve this problem?

Thanks a lot!

Marion and Jeanne



From canteri at itc.it  Mon Aug  4 16:44:58 2003
From: canteri at itc.it (Roberto Canteri)
Date: Mon, 4 Aug 2003 16:44:58 +0200
Subject: [R] label chemical formulas
Message-ID: <009601c35a97$dcad6ae0$0d00460a@pc.itc.it>

Dear r-help,
the prcomp matrix (x$rotation) I have, has the columns labels as chemical
formulas (i.e: C2H3, 37Cl,...,C10H24N3O3).
Plotting them as plain text I do the following with success:

xlabs <- dimnames(y$rotation)[[1]]
 xlabs <- as.character(xlabs)
 plot(y$rotation[,1],y$rotation[,2], ann=FALSE, type="p", col="red", pch=20)
 text(y$rotation[,1],y$rotation[,2], labels=xlabs, cex=0.7, font=1, pos=4,
offset=0.2)

 I do not succed to plot the labels with subscripts and superscripts. I did many
attempt trying the use of expression() or substitute() but without success. I
can
write the labels in the data matrix as needed (i.e.: C[2]*H[3]) to plot properly
if I would know how.
I peg your pardon if this is probably a silly problem, I am new user of R (I am
using R 1.7.0 on win2k).
Many thanks in advance for any help.
Roberto

Roberto Canteri

Research Scientist
Physics-Chemistry of Surfaces and Interfaces Division |
ITC-irst
|Tel. (+39) 0461 314 486
Centre for the scientific and technological research      | Fax  (+39) 0461 810
851
Via Sommarive, 18                                                    |
E-mail:canteri at itc.it
38050 Povo - TRENTO - Italy                                      | Web Page:
http://www.itc.it/



From Simon.Fear at synequanon.com  Mon Aug  4 17:19:04 2003
From: Simon.Fear at synequanon.com (Simon Fear)
Date: Mon, 4 Aug 2003 16:19:04 +0100
Subject: [R] Problem with data.frames
Message-ID: <6C8A8033ABC1E3468048ABC4F13CE572AC0D3D@synequanon01>

You believed right - I do indeed lack the vision of others to foresee
any
practical need to attach and detach within a recursive function, except
perhaps for my own amusement. Should I do the recursive call in between
the
attach and detach, I wonder? Wouldn't it all be fun, watching that
search
path grow and grow, before the function eventually hit the first detach!
And
just imagine the hilarity you could have, attaching and detaching at
different positions! I could have a thousand identical data sets, all
called
Eric!!!

It's one of the many situations in which I would very much like to get a
warning or error message, pointing out to me that I had absolutely no
idea
what I was doing.

Surely that's what warnings are for? For those of us who wonder why our
code
doesn't do what we think it should, until a long time after the
deadline?

SF

-----Original Message-----
From: Prof Brian Ripley [mailto:ripley at stats.ox.ac.uk]
Sent: 04 August 2003 14:34
To: Simon Fear
Cc: r-help at stat.math.ethz.ch
Subject: RE: [R] Problem with data.frames


Security Warning:
If you are not sure an attachment is safe to open please contact 
Andy on x234. There are 0 attachments with this message.
________________________________________________________________

Think about what happens if you call a function recursively, e.g. by 
Recall(), and that function includes an attach/detach pair.

I believe it is your vision that is too limited, not other people's.

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595
 

Simon Fear
Senior Statistician
Syne qua non Ltd
Tel: +44 (0) 1379 644449
Fax: +44 (0) 1379 644445
email: Simon.Fear at synequanon.com
web: http://www.synequanon.com
 
Number of attachments included with this message: 0
 
This message (and any associated files) is confidential and\...{{dropped}}



From ripley at stats.ox.ac.uk  Mon Aug  4 17:23:22 2003
From: ripley at stats.ox.ac.uk (Prof Brian D Ripley)
Date: Mon, 4 Aug 2003 16:23:22 +0100 (GMT Daylight Time)
Subject: [R] label chemical formulas
In-Reply-To: <009601c35a97$dcad6ae0$0d00460a@pc.itc.it>
Message-ID: <Pine.WNT.4.44.0308041609180.2148-100000@petrel>

On Mon, 4 Aug 2003, Roberto Canteri wrote:

[This is a repeat of a post earlier today.]

> Dear r-help,
> the prcomp matrix (x$rotation) I have, has the columns labels as chemical
> formulas (i.e: C2H3, 37Cl,...,C10H24N3O3).
> Plotting them as plain text I do the following with success:
>
> xlabs <- dimnames(y$rotation)[[1]]

That extracts the *row* names.

>  xlabs <- as.character(xlabs)

They are already character.  You want to convert them to expressions.
Here's one way to get you started.

> xx <- "C[2]*H[3]"
> parse(text=xx)
expression(C[2] * H[3])

It's probably easiest to plot the labels one at a time, but

xx <- c("C[2]*H[3]", "phantom()[37]*C[1]")
parse(text=paste(xx, collapse="\n"))

works.


>  plot(y$rotation[,1],y$rotation[,2], ann=FALSE, type="p", col="red", pch=20)
>  text(y$rotation[,1],y$rotation[,2], labels=xlabs, cex=0.7, font=1, pos=4,
> offset=0.2)
>
>  I do not succed to plot the labels with subscripts and superscripts. I
> did many attempt trying the use of expression() or substitute() but
> without success. I can write the labels in the data matrix as needed
> (i.e.: C[2]*H[3]) to plot properly if I would know how.

> I peg your pardon if this is probably a silly problem, I am new user of R (I am
> using R 1.7.0 on win2k).

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From p.dalgaard at biostat.ku.dk  Mon Aug  4 17:25:12 2003
From: p.dalgaard at biostat.ku.dk (Peter Dalgaard BSA)
Date: Mon, 04 Aug 2003 15:25:12 -0000
Subject: [R] ^ operation much slower in R 1.7.1 than in R 1.7.0 ???
In-Reply-To: <MABBLJDICACNFOLGIHJOOEHLDKAA.phgrosjean@sciviews.org>
References: <MABBLJDICACNFOLGIHJOOEHLDKAA.phgrosjean@sciviews.org>
Message-ID: <x2k79tl9tw.fsf@biostat.ku.dk>

"Philippe Grosjean" <phgrosjean at sciviews.org> writes:

> I do not understand what happens here (under Win XP):
> 
> a <- abs(matrix(rnorm(800*800)/2, ncol=800, nrow=800))
> system.time(b <- a^1000)[3]
> 
> took about 1 sec on my computer with R 1.7.0 and it takes now 4.59 sec with
> R 1.7.1
> 
> Similarly,
> 
> phi <- 1.6180339887498949
> a <- floor(runif(750000)*1000)
> system.time(b <- (phi^a - (-phi)^(-a))/sqrt(5))[3]
> 
> took about 0.9 sec with R 1.7.0, and it takes 11.8 sec (!!!) in R 1.7.1.
> 
> Are there some changes made between 1.7.0 and 1.7.1 that could cause such a
> large difference in time to do such simple computations???

Hmm, on linux, I get approx 0.31 for the first example with 1.7.0,
1.7.1, r-patched, and r-devel. Similarly, I get 0.8 for the second ex.
in all four cases.

-- 
   O__  ---- Peter Dalgaard             Blegdamsvej 3  
  c/ /'_ --- Dept. of Biostatistics     2200 Cph. N   
 (*) \(*) -- University of Copenhagen   Denmark      Ph: (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)             FAX: (+45) 35327907



From tlumley at u.washington.edu  Mon Aug  4 17:30:01 2003
From: tlumley at u.washington.edu (Thomas Lumley)
Date: Mon, 4 Aug 2003 08:30:01 -0700 (PDT)
Subject: =?iso-8859-1?Q?Re:_[R]_Excel_can_do_what_R_can't=3F=3F=3F=3F=3F?=
In-Reply-To: <HJ3H9Y$DDE68E169FCDF9487E5022DD195C7BFC@tiscali.fr>
Message-ID: <Pine.A41.4.44.0308040821500.98792-100000@homer01.u.washington.edu>

On Mon, 4 Aug 2003, [iso-8859-1] Jean Fan wrote:

> I'm not a specialist of numerical optimization algorithms,
> but it seems that GRG is actually implemented in several
> specialized optimisation toolbox (sure generally commercial),
> not only the limited one in Excel.
> And with google, search "GRG generalized reduced gradient" is
> giving 424 links.

The code in Excel is actually called GRG2 (the 2 does matter). Unlike any
of the methods for optim(), it can handle nonlinear inequality constraints
and does not need a feasible initial solution.

There's a blurb about it in the NEOS optimisation guide:
http://www-fp.mcs.anl.gov/otc/Guide/SoftwareGuide/Blurbs/grg2.html

Judging from this blurb, it will be similar to L-BFGS-B for problems with
no constraints or box constraints.


	-thomas

Thomas Lumley			Assoc. Professor, Biostatistics
tlumley at u.washington.edu	University of Washington, Seattle



From anna at ptolemy.arc.nasa.gov  Mon Aug  4 17:37:55 2003
From: anna at ptolemy.arc.nasa.gov (Anna  H. Pryor)
Date: Mon, 4 Aug 2003 08:37:55 -0700
Subject: [R] anova
In-Reply-To: <20030804111854.PSLF19646.mta10.mail.mel.aone.net.au@there>
References: <200307310918.31710.anna@ptolemy.arc.nasa.gov>
	<20030804111854.PSLF19646.mta10.mail.mel.aone.net.au@there>
Message-ID: <200308040837.55102.anna@ptolemy.arc.nasa.gov>

I looked at the Introduction to R and am still confused.  Would it be possible 
to ask a question in which I have three vectors and I want to perform an 
anova on them.  Say A,B, and C.  Is there a standard form that I could use in 
lm to get a model that I could use in anova?  Do I need to know more about my 
problem?

I really appreciate any help in this.

anna






On Monday 04 August 2003 03:26, Jim Lemon wrote:
> Anna  H. Pryor wrote:
> > I am totally confused as to how to use anova.  I have three vectors and
> > would like to use anova on them but I don't understand how lm or glm
> > comes into play.  In matlab, you just give the three vectors.  Why isn't
> > it the same in R?
>
> R is almost entirely based on functions, more similar to a programming
> language (which in fact it is) than a pushbutton stats application. One
> good thing about this is that the user has to think about the type of
> ANOVA that is desired: factorial, mixed model, etc. Also, whether ANOVA is
> appropriate - is a linear model being tested? Is the response variable
> distributed normally, etc.?
>
> One of the disadvantages of R is that you do have to wade through a fair
> amount of material to decide the answers to such questions. The official
> documentation, An Introduction to R, should help you to make these
> decisions. I compiled one of the quickie introductions to R, Kickstarting
> R, but that will probably not go deeply enough. However, you might get a
> start by looking at it online at:
>
> http://cran.r-project.org
>
> under Contributed Documentation. A bit more advanced is "Notes on the use
> of R..." by Jonathan Baron & Yuelin Li. Do have a look at "An Introduction
> to R", though, it is much more comprehensive.
>
> Jim



From p.dalgaard at biostat.ku.dk  Mon Aug  4 17:41:19 2003
From: p.dalgaard at biostat.ku.dk (Peter Dalgaard BSA)
Date: Mon, 04 Aug 2003 15:41:19 -0000
Subject: [R] Problem with data.frames
In-Reply-To: <6C8A8033ABC1E3468048ABC4F13CE572AC0D3D@synequanon01>
References: <6C8A8033ABC1E3468048ABC4F13CE572AC0D3D@synequanon01>
Message-ID: <x2fzkhl90w.fsf@biostat.ku.dk>

"Simon Fear" <Simon.Fear at synequanon.com> writes:

> It's one of the many situations in which I would very much like to
> get a warning or error message, pointing out to me that I had
> absolutely no idea what I was doing.
> 
> Surely that's what warnings are for? For those of us who wonder why
> our code doesn't do what we think it should, until a long time after
> the deadline?

It's rarely advisable to nanny users too much though (as someone said:
protecting users from doing dumb things may also prevent them from
doing smart things). In the case of warnings, it is not a good thing
if they can trigger due to circumstances beyond the user's control.
Consider the following:


f <- function(mydata,...){
  attach(mydata)
  ...do something...
  detach(mydata)
}

mydata <- whatever...
attach(mydata)

f(mydata[1:100,])

What should happen? A warning that "mydata already exists on the
search path" or so, perhaps?

OK, so we just don't do that then. But suppose that f is sitting in a
package and the user has no knowledge of its internals. You have then
the side effect of the package that it implicitly forbids the user to
attach a dataframe called "mydata", even if everything functions
perfectly normally when one is present. Imagine having to explain
that in the package documentation!

-- 
   O__  ---- Peter Dalgaard             Blegdamsvej 3  
  c/ /'_ --- Dept. of Biostatistics     2200 Cph. N   
 (*) \(*) -- University of Copenhagen   Denmark      Ph: (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)             FAX: (+45) 35327907



From jmacdon at med.umich.edu  Mon Aug  4 17:46:22 2003
From: jmacdon at med.umich.edu (James MacDonald)
Date: Mon, 04 Aug 2003 11:46:22 -0400
Subject: [R] ^ operation much slower in R 1.7.1 than in R 1.7.0 ???
Message-ID: <sf2e479f.028@mail-01.med.umich.edu>

I get similar results as Philippe on WinXP (1.33 GHz laptop, 512 Mb
RAM).

R 1.7.1
2.86 sec
7.82 sec

R 1.7.0
0.64 sec
1.64 sec

Jim




James W. MacDonald
Affymetrix and cDNA Microarray Core
University of Michigan Cancer Center
1500 E. Medical Center Drive
7410 CCGC
Ann Arbor MI 48109
734-647-5623

>>> Peter Dalgaard BSA <p.dalgaard at biostat.ku.dk> 08/04/03 11:30AM >>>
"Philippe Grosjean" <phgrosjean at sciviews.org> writes:

> I do not understand what happens here (under Win XP):
> 
> a <- abs(matrix(rnorm(800*800)/2, ncol=800, nrow=800))
> system.time(b <- a^1000)[3]
> 
> took about 1 sec on my computer with R 1.7.0 and it takes now 4.59
sec with
> R 1.7.1
> 
> Similarly,
> 
> phi <- 1.6180339887498949
> a <- floor(runif(750000)*1000)
> system.time(b <- (phi^a - (-phi)^(-a))/sqrt(5))[3]
> 
> took about 0.9 sec with R 1.7.0, and it takes 11.8 sec (!!!) in R
1.7.1.
> 
> Are there some changes made between 1.7.0 and 1.7.1 that could cause
such a
> large difference in time to do such simple computations???

Hmm, on linux, I get approx 0.31 for the first example with 1.7.0,
1.7.1, r-patched, and r-devel. Similarly, I get 0.8 for the second ex.
in all four cases.

-- 
   O__  ---- Peter Dalgaard             Blegdamsvej 3  
  c/ /'_ --- Dept. of Biostatistics     2200 Cph. N   
 (*) \(*) -- University of Copenhagen   Denmark      Ph: (+45)
35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)             FAX: (+45)
35327907

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://www.stat.math.ethz.ch/mailman/listinfo/r-help



From brahm at alum.mit.edu  Mon Aug  4 17:48:29 2003
From: brahm at alum.mit.edu (David Brahm)
Date: Mon, 4 Aug 2003 11:48:29 -0400
Subject: [R] 'format' problem
References: <20030801135801.GA9567@nf034.jinr.ru>
Message-ID: <16174.32845.918499.462057@arbres1a.fmr.com>

Timur Elzhov <Timur.Elzhov at jinr.ru> wrote:
> format(1234567, digits = 2)
>   [1] "1234567"
> but I'd like the last number to be represented as "1.2e+06" string too.

R chooses "1234567" instead of "1.2e+06" because it has fewer or equal number
of characters (equal in this case).

In R-devel (which will become R-1.8.0, as I understand it), an options(scipen)
has been added which penalizes scientific notation by an additional number of
characters.  In your case, a negative penalty would do the trick:

  R-devel> options(scipen=-9)
  R-devel> format(1234567, digits = 2)
           [1] "1.2e+06"

However, unless you feel like compiling a development version or waiting
until 1.8.0, Andrew C. Ward's <s195404 at student.uq.edu.au> suggestion is
probably the simpler way to go:
  Ward> sprintf("%.1e", 1234567)
-- 
                              -- David Brahm (brahm at alum.mit.edu)



From tlumley at u.washington.edu  Mon Aug  4 17:56:52 2003
From: tlumley at u.washington.edu (Thomas Lumley)
Date: Mon, 4 Aug 2003 08:56:52 -0700 (PDT)
Subject: [R] Problem with data.frames
In-Reply-To: <x2fzkhl90w.fsf@biostat.ku.dk>
Message-ID: <Pine.A41.4.44.0308040852210.66248-100000@homer35.u.washington.edu>

On 4 Aug 2003, Peter Dalgaard BSA wrote:
>
> It's rarely advisable to nanny users too much though (as someone said:
> protecting users from doing dumb things may also prevent them from
> doing smart things). In the case of warnings, it is not a good thing
> if they can trigger due to circumstances beyond the user's control.

I would have thought a warning was relatively harmless, especially now
that the package writer can use with(), or something like
   attach(mydata, name="privatedata:mydata")

	-thomas

Thomas Lumley			Assoc. Professor, Biostatistics
tlumley at u.washington.edu	University of Washington, Seattle



From p.dalgaard at biostat.ku.dk  Mon Aug  4 18:17:57 2003
From: p.dalgaard at biostat.ku.dk (Peter Dalgaard BSA)
Date: Mon, 04 Aug 2003 16:17:57 -0000
Subject: [R] anova
In-Reply-To: <200308040837.55102.anna@ptolemy.arc.nasa.gov>
References: <200307310918.31710.anna@ptolemy.arc.nasa.gov>
	<20030804111854.PSLF19646.mta10.mail.mel.aone.net.au@there>
	<200308040837.55102.anna@ptolemy.arc.nasa.gov>
Message-ID: <x27k5tl7bz.fsf@biostat.ku.dk>

"Anna  H. Pryor" <anna at ptolemy.arc.nasa.gov> writes:

> I looked at the Introduction to R and am still confused.  Would it be possible 
> to ask a question in which I have three vectors and I want to perform an 
> anova on them.  Say A,B, and C.  Is there a standard form that I could use in 
> lm to get a model that I could use in anova?  Do I need to know more about my 
> problem?
> 
> I really appreciate any help in this.

There are many forms of anova. The type you describe, I presume, is
data from three separate groups, aka one-way ANOVA. You need to first
force your data into the parallel-vector layout like this

  y <- c(A,B,C)
  group <- factor(rep(1:3,c(7,9,13))) # provided there are 7 elements
                                      # in A, 9 in B and 13 in C

and then 

anova(lm(y~group))

or, for some more options, consider

oneway.test(y~group)



-- 
   O__  ---- Peter Dalgaard             Blegdamsvej 3  
  c/ /'_ --- Dept. of Biostatistics     2200 Cph. N   
 (*) \(*) -- University of Copenhagen   Denmark      Ph: (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)             FAX: (+45) 35327907



From tlumley at u.washington.edu  Mon Aug  4 18:28:45 2003
From: tlumley at u.washington.edu (Thomas Lumley)
Date: Mon, 4 Aug 2003 09:28:45 -0700 (PDT)
Subject: [R] ^ operation much slower in R 1.7.1 than in R 1.7.0 ???
In-Reply-To: <x2k79tl9tw.fsf@biostat.ku.dk>
Message-ID: <Pine.A41.4.44.0308040926500.66248-100000@homer35.u.washington.edu>

On 4 Aug 2003, Peter Dalgaard BSA wrote:

>
> Hmm, on linux, I get approx 0.31 for the first example with 1.7.0,
> 1.7.1, r-patched, and r-devel. Similarly, I get 0.8 for the second ex.
> in all four cases.
>

I also  find a slowdown under Win2K.  Was there a compiler version change
with the Windows binary, perhaps?

	-thomas

Thomas Lumley			Assoc. Professor, Biostatistics
tlumley at u.washington.edu	University of Washington, Seattle



From ripley at stats.ox.ac.uk  Mon Aug  4 18:36:16 2003
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Mon, 4 Aug 2003 17:36:16 +0100 (BST)
Subject: [R] Problem with data.frames
In-Reply-To: <Pine.A41.4.44.0308040852210.66248-100000@homer35.u.washington.edu>
Message-ID: <Pine.LNX.4.44.0308041730210.22734-100000@gannet.stats>

On Mon, 4 Aug 2003, Thomas Lumley wrote:

> On 4 Aug 2003, Peter Dalgaard BSA wrote:
> >
> > It's rarely advisable to nanny users too much though (as someone said:
> > protecting users from doing dumb things may also prevent them from
> > doing smart things). In the case of warnings, it is not a good thing
> > if they can trigger due to circumstances beyond the user's control.
> 
> I would have thought a warning was relatively harmless, especially now
> that the package writer can use with(), or something like
>    attach(mydata, name="privatedata:mydata")

Often you cannot use with(), as that does assignments in the wrong 
environment (something which is always catching me).

I don't think we can reasonably expect package writers to work around an
unnecessary restriction.  As John Fox has pointed out, anyone who wants to
can write a new function to do the warning, or disallow duplicate names or
...

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From mascarm at mascari.com  Mon Aug  4 18:44:17 2003
From: mascarm at mascari.com (Mike Mascari)
Date: Mon, 04 Aug 2003 12:44:17 -0400
Subject: [R] Novice question
Message-ID: <3F2E8D61.3020404@mascari.com>

Hello.

I am new R user, so this question is probably quite stupid, but for
the life of me I cannot figure out how to get predications using
multivariate linear regression analysis. Single variable predictions
work fine. I am trying the following:

-- Known y's for known x's1 and x's2

ys <- c(133890, 135000, 135790, 137300, 138130, 139100, 139900,
141120, 141890, 143230, 144000, 145290)

xs1 <- c(1:12)
xs2 <- c(22, 24.5, 27, 33, 36.8, 40, 44, 57, 59, 62, 74, 77)
xm <- cbind(xs1, xs2)

-- New x's1 and x's2

nx1 <- c(13:17)
nx2 <- c(82, 85, 88.3, 90, 95)

-- Generate some predictions

samples <- data.frame(xs1=nx1, xs2=nx2)
f <- predict(lm(ys ~ xm), samples)

data.frame(f) yields:

         f
1  133949.8
2  134970.2
3  135990.6
4  137008.1
5  138027.5
6  139047.3
7  140066.5
8  141078.3
9  142099.1
10 143119.1
11 144131.7
12 145151.7

Not the predicted y's for the new x1's and x2's. I tried:

f <- predict.mlm(lm(ys ~ xm), samples) and got:

Error in object$coefficients[piv, ] : incorrect number of dimensions

I would have though that maybe it would be something like:

f <- predict(mlm(ys ~ xm), samples)

but there doesn't appear to be an mlm().

If the y's and x1's look familiar, they are out of the Excel
documentation for the TREND() function, which I am attempted to
reproduce in R.

Confused,

Mike Mascari



From ripley at stats.ox.ac.uk  Mon Aug  4 18:45:11 2003
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Mon, 4 Aug 2003 17:45:11 +0100 (BST)
Subject: [R] ^ operation much slower in R 1.7.1 than in R 1.7.0 ???
In-Reply-To: <sf2e479f.028@mail-01.med.umich.edu>
Message-ID: <Pine.LNX.4.44.0308041736400.22734-100000@gannet.stats>

And are you able to give an explanation?  For example, did you compile 
each under the same compiler system?

I doubt if this is worth R-core's time to pursue, so over to interested 
users to find an explanation and fix.

On Mon, 4 Aug 2003, James MacDonald wrote:

> I get similar results as Philippe on WinXP (1.33 GHz laptop, 512 Mb
> RAM).
> 
> R 1.7.1
> 2.86 sec
> 7.82 sec
> 
> R 1.7.0
> 0.64 sec
> 1.64 sec
> 
> Jim
> 
> 
> 
> 
> James W. MacDonald
> Affymetrix and cDNA Microarray Core
> University of Michigan Cancer Center
> 1500 E. Medical Center Drive
> 7410 CCGC
> Ann Arbor MI 48109
> 734-647-5623
> 
> >>> Peter Dalgaard BSA <p.dalgaard at biostat.ku.dk> 08/04/03 11:30AM >>>
> "Philippe Grosjean" <phgrosjean at sciviews.org> writes:
> 
> > I do not understand what happens here (under Win XP):
> > 
> > a <- abs(matrix(rnorm(800*800)/2, ncol=800, nrow=800))
> > system.time(b <- a^1000)[3]
> > 
> > took about 1 sec on my computer with R 1.7.0 and it takes now 4.59
> sec with
> > R 1.7.1
> > 
> > Similarly,
> > 
> > phi <- 1.6180339887498949
> > a <- floor(runif(750000)*1000)
> > system.time(b <- (phi^a - (-phi)^(-a))/sqrt(5))[3]
> > 
> > took about 0.9 sec with R 1.7.0, and it takes 11.8 sec (!!!) in R
> 1.7.1.
> > 
> > Are there some changes made between 1.7.0 and 1.7.1 that could cause
> such a
> > large difference in time to do such simple computations???
> 
> Hmm, on linux, I get approx 0.31 for the first example with 1.7.0,
> 1.7.1, r-patched, and r-devel. Similarly, I get 0.8 for the second ex.
> in all four cases.
> 
> 

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From sureshkaran at hotmail.com  Mon Aug  4 18:54:26 2003
From: sureshkaran at hotmail.com (Suresh Kumar Karanam)
Date: Mon, 04 Aug 2003 12:54:26 -0400
Subject: [R] RSPerl help
Message-ID: <BAY2-F38v3uS7sBdxki00001a3e@hotmail.com>

Hi,
I installed RSPerl on my Linux machine and tried to run the test.pl 
accompanying it. The program complains that it does not find the Renviron. I 
installed RSPerl and R as root and am working as root (so, I don't suspect 
permission issues). THe Renviron is in $R_HOME/etc. Where did I go wrong?
Thanks for all help.
suresh



From ripley at stats.ox.ac.uk  Mon Aug  4 19:08:26 2003
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Mon, 4 Aug 2003 18:08:26 +0100 (BST)
Subject: [R] Novice question
In-Reply-To: <3F2E8D61.3020404@mascari.com>
Message-ID: <Pine.LNX.4.44.0308041802280.22789-100000@gannet.stats>

Could you try to use a sensible subject line, and a description of your 
problem?  We are left guessing what you really meant.

If you want to regress xm on ys, you need

lm(xm ~ ys)

You seem to have a single y and two x's, in which case this is not
`multivariate linear regression' and you want

lm(ys ~ xs1 + xs2)

You got the predictions from xm, which is not in samples.

On Mon, 4 Aug 2003, Mike Mascari wrote:

> Hello.
> 
> I am new R user, so this question is probably quite stupid, but for
> the life of me I cannot figure out how to get predications using
> multivariate linear regression analysis. Single variable predictions
> work fine. I am trying the following:
> 
> -- Known y's for known x's1 and x's2
> 
> ys <- c(133890, 135000, 135790, 137300, 138130, 139100, 139900,
> 141120, 141890, 143230, 144000, 145290)
> 
> xs1 <- c(1:12)
> xs2 <- c(22, 24.5, 27, 33, 36.8, 40, 44, 57, 59, 62, 74, 77)
> xm <- cbind(xs1, xs2)
> 
> -- New x's1 and x's2
> 
> nx1 <- c(13:17)
> nx2 <- c(82, 85, 88.3, 90, 95)
> 
> -- Generate some predictions
> 
> samples <- data.frame(xs1=nx1, xs2=nx2)
> f <- predict(lm(ys ~ xm), samples)
> 
> data.frame(f) yields:
> 
>          f
> 1  133949.8
> 2  134970.2
> 3  135990.6
> 4  137008.1
> 5  138027.5
> 6  139047.3
> 7  140066.5
> 8  141078.3
> 9  142099.1
> 10 143119.1
> 11 144131.7
> 12 145151.7
> 
> Not the predicted y's for the new x1's and x2's. I tried:
> 
> f <- predict.mlm(lm(ys ~ xm), samples) and got:
> 
> Error in object$coefficients[piv, ] : incorrect number of dimensions
> 
> I would have though that maybe it would be something like:
> 
> f <- predict(mlm(ys ~ xm), samples)
> 
> but there doesn't appear to be an mlm().
> 
> If the y's and x1's look familiar, they are out of the Excel
> documentation for the TREND() function, which I am attempted to
> reproduce in R.

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From mascarm at mascari.com  Mon Aug  4 19:15:46 2003
From: mascarm at mascari.com (Mike Mascari)
Date: Mon, 04 Aug 2003 13:15:46 -0400
Subject: [R] Novice question
In-Reply-To: <Pine.LNX.4.44.0308041802280.22789-100000@gannet.stats>
References: <Pine.LNX.4.44.0308041802280.22789-100000@gannet.stats>
Message-ID: <3F2E94C2.4070905@mascari.com>

Prof Brian Ripley wrote:

> Could you try to use a sensible subject line, and a description of your 
> problem?  We are left guessing what you really meant.

Sorry.

> If you want to regress xm on ys, you need
> 
> lm(xm ~ ys)
> 
> You seem to have a single y and two x's, in which case this is not
> `multivariate linear regression' and you want
> 
> lm(ys ~ xs1 + xs2)

Yes. That's it. Thanks!

Mike Mascari



From Charles.Annis at statisticalengineering.com  Mon Aug  4 19:22:02 2003
From: Charles.Annis at statisticalengineering.com (Charles Annis, P.E.)
Date: Mon, 4 Aug 2003 13:22:02 -0400
Subject: [R] ^ operation much slower in R 1.7.1 than in R 1.7.0 ???
Message-ID: <005d01c35aac$ee6a8960$2802a8c0@DHT0TL11>

Nearly 5X slower on DELL Pentium 4, 2.26GHz, 1Gig, WinXP-Pro 

..........
R : Copyright 2003, The R Development Core Team
Version 1.7.0  (2003-04-16)

> a <- abs(matrix(rnorm(800*800)/2, ncol=800, nrow=800)) 
> system.time(b <- a^1000)[3]
[1] 0.68
> 
..........
R : Copyright 2003, The R Development Core Team
Version 1.7.1  (2003-06-16)
> a <- abs(matrix(rnorm(800*800)/2, ncol=800, nrow=800)) 
> system.time(b <- a^1000)[3]
[1] 3.29
>


Charles Annis, P.E.

Charles.Annis at StatisticalEngineering.com
phone: 561-352-9699
eFAX: 503-217-5849
http://www.StatisticalEngineering.com



From kjetil at entelnet.bo  Mon Aug  4 21:20:01 2003
From: kjetil at entelnet.bo (kjetil brinchmann halvorsen)
Date: Mon, 04 Aug 2003 15:20:01 -0400
Subject: [R] value replacement in matrices
In-Reply-To: <3F2E14E0.1000509@statistik.uni-dortmund.de>
References: <1077D716F31A5A4590ED2B0C094B3FED0ACF56@oxlsrv001.dpi.qld.gov.au>
Message-ID: <3F2E79A1.9493.3499D2@localhost>

On 4 Aug 2003 at 10:10, Uwe Ligges wrote:

> George, Charles R wrote:
> 
> 
<snip>
>   replace(originalMat, indexMat[,1], indexMat[,2])

Shouldnt that be
    originalmat <- replace(originalMat, indexMat[,1], indexMat[,2])
?

Kjetil Halvorsen


> 
> Uwe Ligges
> 
> 
> > the output would look like this:
> > 
> > Final matrix
> > 5 5 7 3
> > 2 5 3 3
> > 7 5 5 3
> > 9 3 2 7
> > 
> > At the moment it takes a while to process. Does anyone have any suggestions?
> > 
> > reagrds
> > 
> > Robert
> >  
> > 
> > ********************************DISCLAIMER******************...{{dropped}}
> > 
> > ______________________________________________
> > R-help at stat.math.ethz.ch mailing list
> > https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help



From kjetil at entelnet.bo  Mon Aug  4 21:20:01 2003
From: kjetil at entelnet.bo (kjetil brinchmann halvorsen)
Date: Mon, 04 Aug 2003 15:20:01 -0400
Subject: [R] Problem with data.frames
In-Reply-To: <6C8A8033ABC1E3468048ABC4F13CE572AC0D3A@synequanon01>
Message-ID: <3F2E79A1.20828.3499A3@localhost>

On 4 Aug 2003 at 11:09, Simon Fear wrote:

> Peter is right, but there is a point here: it would be nice if
> attach(my.dframe) would do nothing - or at least warn - if my.dframe was
> already in the search list. Like library(). Attaching twice is almost
> bound
> to be an error?

Not necessarily. If you changed the data --- as when debugging a data 
file and repeatedly sourcing it in --- you need to attach repeatedly. 
Of course, you should detach() first, but that is not a requirement 
today.

Kjetil Halvorsen

> Of course in many circumstances it might be better to use with() than
> attach() - if you haven't come across with(), it works like attach()
> with an
> inbuilt detach() when the parentheses close.
> 
> Aside 1: it would also be nice if ?attach pointed to ?with. Is this kind
> of
> suggestion best sent to r-help or r-devel?
> 
> Aside 2: is with() efficient, or does it create a copy of the dataset? 
> 
> SF
> 
> 
> -----Original Message-----
> From: Peter Dalgaard BSA [mailto:p.dalgaard at biostat.ku.dk]
> Sent: 04 August 2003 00:52
> To: Andreas Eckner
> Cc: r-help at stat.math.ethz.ch
> Subject: Re: [R] Problem with data.frames
> 
> 
> Security Warning:
> If you are not sure an attachment is safe to open please contact 
> Andy on x234. There are 0 attachments with this message.
> ________________________________________________________________
> 
> Andreas Eckner <andreas.eckner at soundinvest.net> writes:
> 
> > Hi,
> > 
> > I just encountered a problem in R that may easily be fixed: If one
> uses
> > attach for a data.frame e.g. 10000 times and forgets detach, then R
> gets
> > incredibly slow (less then 10% of the original speed).
> 
> R also gets incredibly slow if you create 10000 copies of your data
> set, which is effectively the same thing! The fix is: Don't do that... 
> 
> -- 
>    O__  ---- Peter Dalgaard             Blegdamsvej 3  
>   c/ /'_ --- Dept. of Biostatistics     2200 Cph. N   
>  (*) \(*) -- University of Copenhagen   Denmark      Ph: (+45) 35327918
> ~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)             FAX: (+45) 35327907
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
>  
> 
> Simon Fear
> Senior Statistician
> Syne qua non Ltd
> Tel: +44 (0) 1379 644449
> Fax: +44 (0) 1379 644445
> email: Simon.Fear at synequanon.com
> web: http://www.synequanon.com
>  
> Number of attachments included with this message: 0
>  
> This message (and any associated files) is confidential and\...{{dropped}}
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help



From petrov at unibel.by  Mon Aug  4 22:36:42 2003
From: petrov at unibel.by (Sergei V. Petrov)
Date: Mon, 04 Aug 2003 23:36:42 +0300
Subject: [R] Breusch-Godfrey Test
In-Reply-To: <18D602BD42B7E24EB810D6454A58DB9004730515@ibfftce505.is.de.dresdnerkb.com>
References: <18D602BD42B7E24EB810D6454A58DB9004730515@ibfftce505.is.de.dresdnerkb.com>
Message-ID: <3F2EC3DA.5020203@unibel.by>

Pfaff, Bernhard ?????:
>>Dear R Helpers!
>>
>>bgtest{lmtest} performs the Breusch-Godfrey test for higher 
>>order serial 
>>correlation.
>>
>>Is the Higher Order Correlation function already programmed in
>>R I couldn't find it?
> ?acf
> ?pacf
> 



Reconstructing attractors with higher order correlation

Abstract: We propose a novel criterion of deciding time delays for
reconstructing attractors from a single variable time series. Our
criterion considers higher order correlation functions, and finds the
convergence values of the extrema. In order to see how our method works
well, we analyze two numerical examples by observing the shapes of
reconstructed attractors and calculating phase space continuities. Our
method can be a good criterion to set appropriate time delays on
reconstructing attractors as a...

http://citeseer.nj.nec.com/45333.html
http://citeseer.nj.nec.com/rd/69920074%2C45333%2C1%2C0.25%2CDownload/http://citeseer.nj.nec.com/cache/papers/cs/2330/http:zSzzSzikeguchi.te.noda.sut.ac.jpzSz%7EtohruzSzjaponaiszSzWorkszSzproceedingszSznolta97-2.pdf/reconstructing-attractors-with-higher.pdf


Sergei Petrov



From ihaka at r-project.org  Mon Aug  4 21:52:36 2003
From: ihaka at r-project.org (Ross Ihaka)
Date: Tue, 05 Aug 2003 07:52:36 +1200
Subject: [R] shading in image()
In-Reply-To: <3F2E1ABA.2010101@lancaster.ac.uk>
References: <Law11-OE446hm8Ua8d90000dc91@hotmail.com>	<3F2DD45D.4070902@r-project.org>
	<3F2E1ABA.2010101@lancaster.ac.uk>
Message-ID: <3F2EB984.2070201@r-project.org>

Barry Rowlingson wrote:
> Ross Ihaka wrote:
> 
>> If anyone knows how to Gouraud, Phong or other smooth shading method
>> portably in a vector system I'd be keen to hear about it.
>>
> 
>  Why bloat R with something like that? All we need is a way of linking R 
> with Blender and we have a fully-GPL statistics and 3-d rendering 
> facility. Blender is scriptable in python, so you could read data files 
> from Blender, or possibly one could write an R graphics device that 
> produced Blender-compatible data files. Blender
> 
>  Eventually you'll be able to walk through your data in 3-d....

Its actually not that big, and it's useful for things other than 3d.
Interpolation could be used for things like gradient colour fills for 
plot backgrunds or bars in barplots, drawing light spectra, etc.

I've much about with some of this using existing primitives, and its 
quite hard work to get the effects I want.

-- 
Ross Ihaka                         Email:  ihaka at r-project.org
The R Project and R Foundation



From Mike.Prager at noaa.gov  Mon Aug  4 22:37:04 2003
From: Mike.Prager at noaa.gov (Mike Prager)
Date: Mon, 04 Aug 2003 16:37:04 -0400
Subject: [R] Modeling lotteries, Nigerian scams, and so on
In-Reply-To: <E19ieAV-00014y-00@bernie.ethz.ch>
Message-ID: <5.2.1.1.2.20030804163138.0293c480@hermes.nos.noaa.gov>

Colleagues--

Has anyone here developed a model of the increase of spam?  An 
epidemiologist, perhaps?  If so, when does the model predict that email 
will become unusable for serious purposes?  It seems the time is 
approaching with exponential swiftness (and I suppose this inquiry 
contributes its small part).

Mike Prager



From gc4 at duke.edu  Mon Aug  4 22:55:48 2003
From: gc4 at duke.edu (gc4@duke.edu)
Date: Mon, 4 Aug 2003 16:55:48 -0400 (EDT)
Subject: [R] coxph and frailty
In-Reply-To: <200308040030.h740FNlY009082@stat.math.ethz.ch>
References: <200308040030.h740FNlY009082@stat.math.ethz.ch>
Message-ID: <Pine.GSO.4.53.0308041651470.16756@godzilla6.acpub.duke.edu>

Hi:

I have a few clarification questions about the elements returned by
the coxph function used in conjuction with a frailty term.

I create the following group variable:

group <- NULL
group[id<50] <- 1
group[id>=50 & id<100] <- 2
group[id>=100 & id<150] <- 3
group[id>=150 & id<200] <- 4
group[id>=200 & id<250] <- 5
group[id>=250 & id<300] <- 6
group[id>=300] <- 7

I estimate the following model, using the Pbc data (with time-varying
covariates) from Therneau and Grambsch's book:


fitf <- coxph(Surv(start,stop,event==2) ~ age + log(bili) + log(protime) +
                         log(albumin) + edema + frailty(group),
                         na.action=na.exclude, data=Pbcseq)


Then I obtain:

> fitf[10]
$frail
[1]  0.06273372  0.16192093  0.10050877  0.37716999 -0.20853156 -0.71887977
[7] -0.10922275

And:

> fitf[11]
$fvar
[1] 0.03631634 0.03261972 0.03750465 0.04459689 0.05113802 0.08107876 0.11482236

My understanding is that fitf$frail are the group level frailty terms. But
what is fitf$fvar?


Moreover, if I estimate the same model as the one above, but with a
`group' variable with only 4 groups (instead of 7 as in the example
above), the coxph object does not contain those two elements.

Instead, it reports a $coef object that includes the frailty terms:

> fitf2$coef
        age    log(bili) log(protime) log(albumin)        edema      gamma:1
  0.05413962   1.11244236   2.64338176  -3.89824452   0.71347526   0.15547991
     gamma:2      gamma:3      gamma:4
  0.25794637  -0.47470789  -0.08834457

And a $means objects with 9 elements:

> fitf2$means
[1] 49.25963095  0.60313783  2.39101856  1.20878208  0.18226221  0.39845758
[7]  0.33727506  0.23804627  0.02622108

What do the means fitf2$means[6:9] correspond to?

I am asking these questions because I would like to clarify how the
frailty terms enter into the computation of the baseline cumulative hazard
via the basehaz(fitf, centered=FALSE) function. It would seem that the way
the basehaz function operates is different dependening upon how many
frailty groups there are.

Thank you very much for your help.
giacomo



From tlumley at u.washington.edu  Mon Aug  4 23:39:52 2003
From: tlumley at u.washington.edu (Thomas Lumley)
Date: Mon, 4 Aug 2003 14:39:52 -0700 (PDT)
Subject: [R] coxph and frailty
In-Reply-To: <Pine.GSO.4.53.0308041651470.16756@godzilla6.acpub.duke.edu>
Message-ID: <Pine.A41.4.44.0308041433040.64952-100000@homer22.u.washington.edu>

yOn Mon, 4 Aug 2003 gc4 at duke.edu wrote:

> Hi:
>
> I have a few clarification questions about the elements returned by
> the coxph function used in conjuction with a frailty term.
>
> I create the following group variable:
>
> group <- NULL
> group[id<50] <- 1
> group[id>=50 & id<100] <- 2
> group[id>=100 & id<150] <- 3
> group[id>=150 & id<200] <- 4
> group[id>=200 & id<250] <- 5
> group[id>=250 & id<300] <- 6
> group[id>=300] <- 7
>
> I estimate the following model, using the Pbc data (with time-varying
> covariates) from Therneau and Grambsch's book:
>
>
> fitf <- coxph(Surv(start,stop,event==2) ~ age + log(bili) + log(protime) +
>                          log(albumin) + edema + frailty(group),
>                          na.action=na.exclude, data=Pbcseq)
>
>
> Then I obtain:
>
> > fitf[10]
> $frail
> [1]  0.06273372  0.16192093  0.10050877  0.37716999 -0.20853156 -0.71887977
> [7] -0.10922275
>
> And:
>
> > fitf[11]
> $fvar
> [1] 0.03631634 0.03261972 0.03750465 0.04459689 0.05113802 0.08107876 0.11482236
>
> My understanding is that fitf$frail are the group level frailty terms. But
> what is fitf$fvar?

The inverse of the diagonal of the second derivative of the penalised
loglikelihood.  By default a diagonal approximation to the second
derivative is used when there are more than five groups (there's an
argument to frailty() to control this).


>
> Moreover, if I estimate the same model as the one above, but with a
> `group' variable with only 4 groups (instead of 7 as in the example
> above), the coxph object does not contain those two elements.
>
> Instead, it reports a $coef object that includes the frailty terms:

Yes, with 5 or fewer terms it doesn't use the sparse algorithm, but
treats them as coefficients in the usual way.

	-thomas


Thomas Lumley			Assoc. Professor, Biostatistics
tlumley at u.washington.edu	University of Washington, Seattle



From jasont at indigoindustrial.co.nz  Tue Aug  5 00:03:06 2003
From: jasont at indigoindustrial.co.nz (Jason Turner)
Date: Tue, 05 Aug 2003 10:03:06 +1200
Subject: [R] anova
In-Reply-To: <x27k5tl7bz.fsf@biostat.ku.dk>
References: <200307310918.31710.anna@ptolemy.arc.nasa.gov>	<20030804111854.PSLF19646.mta10.mail.mel.aone.net.au@there>	<200308040837.55102.anna@ptolemy.arc.nasa.gov>
	<x27k5tl7bz.fsf@biostat.ku.dk>
Message-ID: <3F2ED81A.80708@indigoindustrial.co.nz>

Peter Dalgaard BSA wrote:
> "Anna  H. Pryor" <anna at ptolemy.arc.nasa.gov> writes:
> 
> 
>>I looked at the Introduction to R and am still confused.  Would it be possible 
>>to ask a question in which I have three vectors and I want to perform an 
>>anova on them.  Say A,B, and C.  Is there a standard form that I could use in 
>>lm to get a model that I could use in anova?  Do I need to know more about my 
>>problem?
>>
>>I really appreciate any help in this.
> 
> 
> There are many forms of anova. The type you describe, I presume, is
> data from three separate groups, aka one-way ANOVA. You need to first
> force your data into the parallel-vector layout like this
> 
>   y <- c(A,B,C)
>   group <- factor(rep(1:3,c(7,9,13))) # provided there are 7 elements
>                                       # in A, 9 in B and 13 in C
> 
> and then 
> 
> anova(lm(y~group))
> 
> or, for some more options, consider
> 
> oneway.test(y~group)

Peter, being modest, didn't mention that he's written a very good book 
about R for technical people who aren't statisticians.  At fatbrain:

http://search.barnesandnoble.com/textbooks/booksearch/isbnInquiry.asp?isbn=0387954759&TXT=Y&itm=1

If you're not a statistician but are required to use some statistical 
methods (such as anova), this is a very helpful book.  Most of R's 
documentation is aimed at statisticians, so this book fills a gap very 
nicely.

Cheers

Jason
-- 
Indigo Industrial Controls Ltd.
64-21-343-545
jasont at indigoindustrial.co.nz



From p.connolly at hortresearch.co.nz  Tue Aug  5 05:19:03 2003
From: p.connolly at hortresearch.co.nz (Patrick Connolly)
Date: Tue, 5 Aug 2003 15:19:03 +1200
Subject: [R] ^ operation much slower in R 1.7.1 than in R 1.7.0 ???
In-Reply-To: <Pine.A41.4.44.0308040926500.66248-100000@homer35.u.washington.edu>
References: <x2k79tl9tw.fsf@biostat.ku.dk>
	<Pine.A41.4.44.0308040926500.66248-100000@homer35.u.washington.edu>
Message-ID: <20030805031903.GM20318@hortresearch.co.nz>

On Mon, 04-Aug-2003 at 09:28AM -0700, Thomas Lumley wrote:

|> On 4 Aug 2003, Peter Dalgaard BSA wrote:
|> 
|> >
|> > Hmm, on linux, I get approx 0.31 for the first example with 1.7.0,
|> > 1.7.1, r-patched, and r-devel. Similarly, I get 0.8 for the second ex.
|> > in all four cases.
|> >
|> 
|> I also  find a slowdown under Win2K.  Was there a compiler version change
|> with the Windows binary, perhaps?

In case anyone is interested.....


version
         _                
platform i686-pc-linux-gnu
arch     i686             
os       linux-gnu        
system   i686, linux-gnu  
status                    
major    1                
minor    7.1              
year     2003             
month    06               
day      16               
language R                


0.54 and 1.24

platform i686-pc-linux-gnu
arch     i686             
os       linux-gnu        
system   i686, linux-gnu  
status                    
major    1                
minor    7.0              
year     2003             
month    04               
day      16               
language R                

 0.55 and 1.13


Both are compiled using the maligned 

gcc version 2.96 20000731 (Red Hat Linux 7.3 2.96-113)

HTH


-- 
Patrick Connolly
HortResearch
Mt Albert
Auckland
New Zealand 
Ph: +64-9 815 4200 x 7188
~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~
I have the world`s largest collection of seashells. I keep it on all
the beaches of the world ... Perhaps you`ve seen it.  ---Steven Wright 
~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~



From zhuw at mail.smu.edu  Tue Aug  5 05:39:50 2003
From: zhuw at mail.smu.edu (zhu wang)
Date: Mon, 04 Aug 2003 22:39:50 -0500
Subject: [R] Rwave cgt plot time axis problem
Message-ID: <3F2F2706.2060306@mail.smu.edu>

Dear helpers,

When I use the function cgt of library Rwave, the time axis of the plot 
is always on the [0,1] scale regardless of the original time. This 
happedn when I tried to reproduce some pictures of "Practical 
Time-Frequency Analysis", e.g. Figure 3.5 and 3.6 using the codes 
provided in the book. But the figures of the book display the real 
sampling time using the same codes. It seems there is no argument for 
the time scale for 'cgt'. Is this a bug or my misunderstanding?

Hope somebody can help me.

Thanks,

Zhu Wang

Statistical Science Department
Southern Methodist University



From petrov at unibel.by  Tue Aug  5 09:20:37 2003
From: petrov at unibel.by (Sergei V. Petrov)
Date: Tue, 05 Aug 2003 10:20:37 +0300
Subject: [R] Modeling lotteries, Nigerian scams, and so on
In-Reply-To: <5.2.1.1.2.20030804163138.0293c480@hermes.nos.noaa.gov>
References: <5.2.1.1.2.20030804163138.0293c480@hermes.nos.noaa.gov>
Message-ID: <3F2F5AC5.6060202@unibel.by>

Mike Prager ?????:
> Has anyone here developed a model of the increase of spam?  An 
> epidemiologist, perhaps?  If so, when does the model predict that email 
> will become unusable for serious purposes?  It seems the time is 
> approaching with exponential swiftness (and I suppose this inquiry 
> contributes its small part).
> 


http://www.mozilla.org/mailnews/arch/spam/junknotes.html

http://www.mozilla.org/mailnews/arch/spam/

http://www.mozilla.org/mailnews/spam-faq.html

Sergei Petrov



From jranke at uni-bremen.de  Tue Aug  5 09:12:47 2003
From: jranke at uni-bremen.de (Johannes Ranke)
Date: Tue, 5 Aug 2003 09:12:47 +0200
Subject: [R] How do I make R listen to a Unix domain socket?
Message-ID: <20030805071247.GA29519@kriemhild.uft.uni-bremen.de>

Respected r-help readers,

I am a bit uncomfortable because my favourite text editor vim does not
speak statistics. So I obtained a perl-script made to get vim to
interact with a lisp interpreter and had it set up so vim sends text to
a socket and R executes - but there are some irreproducible errors and I
don't understand the perl-script.

Could someone show me an alternative method to start R in a way that it
listens to a unix domain socket? I know about the R function
socketConnection() but this creates tcp sockets which I don't know how
to send text to.

Good day

Johannes Ranke



From grassi at psico.univ.trieste.it  Mon Aug  4 17:08:13 2003
From: grassi at psico.univ.trieste.it (Michele Grassi)
Date: Mon, 4 Aug 2003 17:08:13 +0200 (MEST)
Subject: [R] BEGINNER:create dummy variables
Message-ID: <200308041508.RAA29878@server.psico.univ.trieste.it>

Hi.
I want to put a categorical variables (3 levels) in my 
logit model. I have to use 2 dummy variables for code 
theese 3 levels. How can I transform my categorical 
variables?
Thank you. Michele.



From angel_lul at hotmail.com  Tue Aug  5 09:40:43 2003
From: angel_lul at hotmail.com (Angel)
Date: Tue, 5 Aug 2003 09:40:43 +0200
Subject: [R] shading in image()
References: <Law11-OE446hm8Ua8d90000dc91@hotmail.com>	<3F2DD45D.4070902@r-project.org>
	<3F2E1ABA.2010101@lancaster.ac.uk> <3F2EB984.2070201@r-project.org>
Message-ID: <Law11-OE5651ShXz467000136d2@hotmail.com>

The rgl package offers 3-D rendering (and Gouraud shading) capabilities
through an OpenGL.
I don't know why it is not in cran, I found it at:
http://wsopuppenkiste.wiso.uni-goettingen.de/~dadler/rgl/
In any case that's not what I wanted (although I still have to explore in
more depth this package).
My question was (naive me!) more simple! I was just thinking of an
interpolation function in R, maybe not very efficient but it'd do the job
for me.
Although the idea of linking blender is nice, it might be "too
sophisticated"  for simple things in 2-D.
Thanks,
Angel

----- Original Message -----
From: "Ross Ihaka" <ihaka at r-project.org>
To: "Barry Rowlingson" <B.Rowlingson at lancaster.ac.uk>
Cc: <r-help at stat.math.ethz.ch>; "Angel" <angel_lul at hotmail.com>
Sent: Monday, August 04, 2003 9:52 PM
Subject: Re: [R] shading in image()


> Barry Rowlingson wrote:
> > Ross Ihaka wrote:
> >
> >> If anyone knows how to Gouraud, Phong or other smooth shading method
> >> portably in a vector system I'd be keen to hear about it.
> >>
> >
> >  Why bloat R with something like that? All we need is a way of linking R
> > with Blender and we have a fully-GPL statistics and 3-d rendering
> > facility. Blender is scriptable in python, so you could read data files
> > from Blender, or possibly one could write an R graphics device that
> > produced Blender-compatible data files. Blender
> >
> >  Eventually you'll be able to walk through your data in 3-d....
>
> Its actually not that big, and it's useful for things other than 3d.
> Interpolation could be used for things like gradient colour fills for
> plot backgrunds or bars in barplots, drawing light spectra, etc.
>
> I've much about with some of this using existing primitives, and its
> quite hard work to get the effects I want.
>
> --
> Ross Ihaka                         Email:  ihaka at r-project.org
> The R Project and R Foundation
>
>



From ripley at stats.ox.ac.uk  Tue Aug  5 09:38:52 2003
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Tue, 5 Aug 2003 08:38:52 +0100 (BST)
Subject: [R] BEGINNER:create dummy variables
In-Reply-To: <200308041508.RAA29878@server.psico.univ.trieste.it>
Message-ID: <Pine.LNX.4.44.0308050836140.26898-100000@gannet.stats>

On Mon, 4 Aug 2003, Michele Grassi wrote:

> Hi.
> I want to put a categorical variables (3 levels) in my 
> logit model. I have to use 2 dummy variables for code 
> theese 3 levels. How can I transform my categorical 
> variables?

model.matrix will do this for you automatically, using the coding set in 
options("contrasts").   How are you fitting `my logit model'?  The normal 
way is to use glm(family=binomial): take a look at the script 
library/MASS/scripts/ch07.R for examples.

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From phgrosjean at sciviews.org  Tue Aug  5 09:42:21 2003
From: phgrosjean at sciviews.org (Philippe Grosjean)
Date: Tue, 5 Aug 2003 09:42:21 +0200
Subject: [R] ^ operation much slower in R 1.7.1 than in R 1.7.0 ???
In-Reply-To: <Pine.LNX.4.44.0308041736400.22734-100000@gannet.stats>
Message-ID: <MABBLJDICACNFOLGIHJOEEIJDKAA.phgrosjean@sciviews.org>

I propose to move this thread to R-devel...

Prof. Brian Ripley wrote:
>And are you able to give an explanation?  For example, did you compile
>each under the same compiler system?

I just noticed that behaviour yesterday, and had not much time yet to
investigate it. I compiled 1.7.0 myself (but compared with the binary
provided on CRAN; no changes). I used the 1.7.1 binary provided on CRAN. I
know these binaries are not supported, so we (Windows users) will have to
look at that by ourselve. Indeed yes, I'll first compile 1.7.1 with the same
compiler I used for 1.7.0.

>I doubt if this is worth R-core's time to pursue, so over to interested
>users to find an explanation and fix.

OK. But I was wondering if people at R-core team, especially those who
worked on Windows specific aspects, would have in mind some changes they did
between 1.7.0 and 1.7.1 than can cause this. Since these changes are mainly
corrections of bugs, the list is hopefully not so long... and it could help
a lot to have these hints. Thanks very much.

Best,

Philippe Grosjean

...........]<(({?<...............<?}))><...............................
 ) ) ) ) )
( ( ( ( (       Dr. Philippe Grosjean
 ) ) ) ) )
( ( ( ( (       LOV, UMR 7093
 ) ) ) ) )      Station Zoologique
( ( ( ( (       Observatoire Oceanologique
 ) ) ) ) )      BP 28
( ( ( ( (       06234 Villefranche sur mer cedex
 ) ) ) ) )      France
( ( ( ( (
 ) ) ) ) )      tel: +33.4.93.76.38.18, fax: +33.4.93.76.38.34
( ( ( ( (
 ) ) ) ) )      e-mail: phgrosjean at sciviews.org
( ( ( ( (       SciViews project coordinator (http://www.sciviews.org)
 ) ) ) ) )
.......................................................................



On Mon, 4 Aug 2003, James MacDonald wrote:

> I get similar results as Philippe on WinXP (1.33 GHz laptop, 512 Mb
> RAM).
>
> R 1.7.1
> 2.86 sec
> 7.82 sec
>
> R 1.7.0
> 0.64 sec
> 1.64 sec
>
> Jim
>
>
>
>
> James W. MacDonald
> Affymetrix and cDNA Microarray Core
> University of Michigan Cancer Center
> 1500 E. Medical Center Drive
> 7410 CCGC
> Ann Arbor MI 48109
> 734-647-5623
>
> >>> Peter Dalgaard BSA <p.dalgaard at biostat.ku.dk> 08/04/03 11:30AM >>>
> "Philippe Grosjean" <phgrosjean at sciviews.org> writes:
>
> > I do not understand what happens here (under Win XP):
> >
> > a <- abs(matrix(rnorm(800*800)/2, ncol=800, nrow=800))
> > system.time(b <- a^1000)[3]
> >
> > took about 1 sec on my computer with R 1.7.0 and it takes now 4.59
> sec with
> > R 1.7.1
> >
> > Similarly,
> >
> > phi <- 1.6180339887498949
> > a <- floor(runif(750000)*1000)
> > system.time(b <- (phi^a - (-phi)^(-a))/sqrt(5))[3]
> >
> > took about 0.9 sec with R 1.7.0, and it takes 11.8 sec (!!!) in R
> 1.7.1.
> >
> > Are there some changes made between 1.7.0 and 1.7.1 that could cause
> such a
> > large difference in time to do such simple computations???
>
> Hmm, on linux, I get approx 0.31 for the first example with 1.7.0,
> 1.7.1, r-patched, and r-devel. Similarly, I get 0.8 for the second ex.
> in all four cases.
>
>

--
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://www.stat.math.ethz.ch/mailman/listinfo/r-help



From uth at zhwin.ch  Tue Aug  5 10:58:01 2003
From: uth at zhwin.ch (=?iso-8859-1?Q?=22Untern=E4hrer_Thomas=2C_uth=22?=)
Date: Tue, 5 Aug 2003 10:58:01 +0200
Subject: [R] A little problem
Message-ID: <53A181E56FB0694ABFD212F8AEDA7F6F1AB846@langouste.zhwin.ch>

Hi,

Does anybody knows a easy way (without for-loops, maybe with something like match) 
to solve this problem:

x <- rep(1,3)
y <- c(0,1,0,1,0,1,1,1,0,0,0,0,1,0)

if (x is a part of y){
 find out where it is and
 do something
}


Thanks a lot

Thomas



From ripley at stats.ox.ac.uk  Tue Aug  5 11:06:18 2003
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Tue, 5 Aug 2003 10:06:18 +0100 (BST)
Subject: [R] A little problem
In-Reply-To: <53A181E56FB0694ABFD212F8AEDA7F6F1AB846@langouste.zhwin.ch>
Message-ID: <Pine.LNX.4.44.0308051003350.28229-100000@gannet.stats>

On Tue, 5 Aug 2003, "Untern?hrer Thomas, uth" wrote:

> Hi,
> 
> Does anybody knows a easy way (without for-loops, maybe with something like match) 
> to solve this problem:
> 
> x <- rep(1,3)
> y <- c(0,1,0,1,0,1,1,1,0,0,0,0,1,0)
> 
> if (x is a part of y){
>  find out where it is and
>  do something
> }

I'll assume the contents are 0/1.

> x0 <- paste(x, collapse="")
> y0 <- paste(y, collapse="")
> regexpr(x0, y0)
[1] 6
attr(,"match.length")
[1] 3


-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From t.kypraios at lancaster.ac.uk  Tue Aug  5 11:41:03 2003
From: t.kypraios at lancaster.ac.uk (Theodore Kypraios)
Date: Tue, 5 Aug 2003 11:41:03 +0200
Subject: [R] levelplot and points 
Message-ID: <009901c35b35$b1940cd0$dda1548d@student7824>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20030805/468db287/attachment.pl

From ripley at stats.ox.ac.uk  Tue Aug  5 11:52:41 2003
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Tue, 5 Aug 2003 10:52:41 +0100 (BST)
Subject: [R] levelplot and points 
In-Reply-To: <009901c35b35$b1940cd0$dda1548d@student7824>
Message-ID: <Pine.LNX.4.44.0308051049340.28424-100000@gannet.stats>

Don't attempt mix lattice and base graphics, and you cannot add to lattice 
graphics after they have been plotted.

>From MASS4:

levelplot(pred ~ x * y, topo.plt, aspect = 1,
   at = seq(690, 960, 10), xlab = "", ylab = "",
   panel = function(x, y, subscripts, ...) {
      panel.levelplot(x, y, subscripts, ...)
      panel.xyplot(topo$x,topo$y, cex = 0.5, col = 1)
   }
)

to add points to a levelplot.

On Tue, 5 Aug 2003, Theodore Kypraios wrote:

> I face the following problem. 
> 
> I have two variables x,y. I have also calculated the surface z.
> When I am drawing the surface with function >levelplot (in library lattice)  everything is OK. I have the legend etc ...
> 
> Then I am trying to plot x,y on the same graph. So I am doing :
> >points(x,y)
> 
> But, the points are not in the correct position. They are a little bit shifted to the left..(i think)
> 
> Any suggestions are really appreciated. 

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From skala at incoma.cz  Tue Aug  5 12:15:24 2003
From: skala at incoma.cz (=?iso-8859-2?Q?SK=C1LA_Zden=ECk?=)
Date: Tue, 5 Aug 2003 12:15:24 +0200
Subject: [R] boxplot&levels
Message-ID: <258405913DEE50419B664F16D1D672AE25FDB7@incoma2000.incoma.bbc>

Dear all,
is there a simle way (other than playing with subset()) to manipulate order of levels when calling boxplot(my.vector~my.factor) - i.e. to force boxes being plotted in another order than (alphabetic) default for levels?
(I believe I have seen this topic at R-help some time ago but am unable now to find it - apologies if it really was already answered)

Many thanks!
Zdenek Skala
skala at incoma.cz



From phgrosjean at sciviews.org  Tue Aug  5 12:31:01 2003
From: phgrosjean at sciviews.org (Philippe Grosjean)
Date: Tue, 5 Aug 2003 12:31:01 +0200
Subject: [Rd] RE: [R] ^ operation much slower in R 1.7.1 than in R 1.7.0
	???
Message-ID: <MABBLJDICACNFOLGIHJOEEIODKAA.phgrosjean@sciviews.org>

OK. Now I have compiled R 1.7.1 myself on my Windows XP pro computer with
the recommended tools and MingW 2.0.0-3.
Here is what I got:

a <- abs(matrix(rnorm(800*800)/2, ncol=800, nrow=800))
system.time(b <- a^1000)[3]

R 1.7.0            :  1.00 sec
R 1.7.1 (from CRAN):  4.59 sec
R 1.7.1 (custom)   :  0.99 sec

phi <- 1.6180339887498949
a <- floor(runif(750000)*1000)
system.time(b <- (phi^a - (-phi)^(-a))/sqrt(5))[3]

R 1.7.0            :  0.90 sec
R 1.7.1 (from CRAN): 11.80 sec
R 1.7.1 (custom)   :  1.08 sec

It seems thus that the problem originates in the Windows compilation of the
CRAN version of R 1.7.1. We will wait Duncan Murdoch for some more
explanation. I cannot place the custom rw1071.exe on my web site because it
is too large (almost 22 Mb). For the moment, you should recompile from
source by yourself to get top speed in R 1.7.1.

Best,

Philippe Grosjean


...........]<(({?<...............<?}))><...............................
 ) ) ) ) )
( ( ( ( (       Dr. Philippe Grosjean
 ) ) ) ) )
( ( ( ( (       LOV, UMR 7093
 ) ) ) ) )      Station Zoologique
( ( ( ( (       Observatoire Oc?anologique
 ) ) ) ) )      BP 28
( ( ( ( (       06234 Villefranche sur mer cedex
 ) ) ) ) )      France
( ( ( ( (
 ) ) ) ) )      tel: +33.4.93.76.38.18, fax: +33.4.93.76.38.34
( ( ( ( (
 ) ) ) ) )      e-mail: phgrosjean at sciviews.org
( ( ( ( (       SciViews project coordinator (http://www.sciviews.org)
 ) ) ) ) )
.......................................................................



From vincent.stoliaroff at sgcib.com  Tue Aug  5 12:34:37 2003
From: vincent.stoliaroff at sgcib.com (vincent.stoliaroff@sgcib.com)
Date: Tue, 5 Aug 2003 12:34:37 +0200
Subject: [R] error message in fitdistr
Message-ID: <OFE4B95EA2.EF7FDACE-ONC1256D79.00394C3B@ges.marc.societe-generale.fr>

Hi R lovers

Here is a numerical vector test
> test
  [1] 206  53 124 112  92  77 118  75  48 176  90  74 107 126  99  84 114
147  99 114  99  84  99  99  99  99  99 104   1 159 100  53
 [33] 132  82  85 106 136  99 110  82  99  99  89 107  99  68 130  99  99
110  99  95 153  93 136  51 103  95  99  72  99  50 110  37
 [65] 102 104  92  90  94  99  76  81 109  91  98  96 104 104  93  99 125
89  99  99 108  90 105  92 106 103  99  99  99  94 102 103
 [97]  97  99 118  91 110  99  99  99  98 105  99  97  99 101  95  98  99
102  99  99  99  99  95  93  86  96 113  87 112 120  71  99
[129]  99  81  96 113  85 116 112  84 120  88  97 104  99 105 153 103  92
99  99  99  90 108 104  99 105  97  94  93  99  85  91  99
[161] 118  99  91  99 103 101 102  96  99 123  85 101  88  99  93  73  97
89  94  69  74  99  97  91  92

Assuming it follows a lognormal distribution I'd like to determine the mean
and the sd thanks to maximum likelihood estimation

> fitdistr(test,"lognormal",start=list(200,10))
Error in print.fitdistr(structure(list(estimate = c(4.54666263736726,  :
        more elements supplied than there are to replace

I chose the parameter start randomly

I don't understand the error message. Has anybody ever encountered such
one?
thanks for the help

have a wonderful day




******************************************************************
The sender's email address has changed to 
firstname.lastname@ sgcib.com. You may want to update your 
personal address book. Please see http://www.sgcib.com for more 
information.
                               **
This message and any attachments (the "message") are confide...{{dropped}}



From s195404 at student.uq.edu.au  Tue Aug  5 12:47:06 2003
From: s195404 at student.uq.edu.au (Andrew C. Ward)
Date: Tue,  5 Aug 2003 10:47:06 +0000
Subject: [R] boxplot&levels
In-Reply-To: <258405913DEE50419B664F16D1D672AE25FDB7@incoma2000.incoma.bbc>
References: <258405913DEE50419B664F16D1D672AE25FDB7@incoma2000.incoma.bbc>
Message-ID: <1060080426.3f2f8b2a54c13@my.uq.edu.au>

Perhaps you could specify the factor order you want using
the factor() function:
   my.factor <- factor(something, levels=c("A","C","B"),
                       labels=c("A","C","B"), ordered=TRUE)
   boxplot(my.vector ~ my.factor)


Regards,

Andrew C. Ward

CAPE Centre
Department of Chemical Engineering
The University of Queensland
Brisbane Qld 4072 Australia
andreww at cheque.uq.edu.au


Quoting SK?LA Zden?k <skala at incoma.cz>:

> Dear all,
> is there a simle way (other than playing with subset())
> to manipulate order of levels when calling
> boxplot(my.vector~my.factor) - i.e. to force boxes being
> plotted in another order than (alphabetic) default for
> levels?
> (I believe I have seen this topic at R-help some time ago
> but am unable now to find it - apologies if it really was
> already answered)
> 
> Many thanks!
> Zdenek Skala
> skala at incoma.cz
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
>



From f.calboli at ucl.ac.uk  Tue Aug  5 13:24:03 2003
From: f.calboli at ucl.ac.uk (Federico Calboli)
Date: Tue, 05 Aug 2003 12:24:03 +0100
Subject: [R] bwplot colours
Message-ID: <3.0.6.32.20030805122403.0287c5c0@pop-server.ucl.ac.uk>

Dear All,

is any way I can change the colours of the box and umbrella of a bwplot
without having to go to the length of:

box.rectangle<-trellis.par.get("box.rectangle")
box.rectangle$col<-"black"
trellis.par.set("box.rectangle", box.rectangle)

etc...

but straight from the call:

bwplot(y ~ x | z, mydata) ?

Regards,

Federico Calboli

=========================

Federico C.F. Calboli

Department of Biology
University College London
Room 327
Darwin Building
Gower Street
London
WClE 6BT

Tel: (+44) 020 7679 4395 
Fax (+44) 020 7679 7096
f.calboli at ucl.ac.uk



From ripley at stats.ox.ac.uk  Tue Aug  5 13:37:44 2003
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Tue, 5 Aug 2003 12:37:44 +0100 (BST)
Subject: [R] bwplot colours
In-Reply-To: <3.0.6.32.20030805122403.0287c5c0@pop-server.ucl.ac.uk>
Message-ID: <Pine.LNX.4.44.0308051236530.28791-100000@gannet.stats>

Only by having your own modified version of panel.bwplot, which contains

    box.dot <- trellis.par.get("box.dot")
    box.rectangle <- trellis.par.get("box.rectangle")
    box.umbrella <- trellis.par.get("box.umbrella")
    plot.symbol <- trellis.par.get("plot.symbol")


On Tue, 5 Aug 2003, Federico Calboli wrote:

> Dear All,
> 
> is any way I can change the colours of the box and umbrella of a bwplot
> without having to go to the length of:
> 
> box.rectangle<-trellis.par.get("box.rectangle")
> box.rectangle$col<-"black"
> trellis.par.set("box.rectangle", box.rectangle)
> 
> etc...
> 
> but straight from the call:
> 
> bwplot(y ~ x | z, mydata) ?
> 
> Regards,
> 
> Federico Calboli
> 
> =========================
> 
> Federico C.F. Calboli
> 
> Department of Biology
> University College London
> Room 327
> Darwin Building
> Gower Street
> London
> WClE 6BT
> 
> Tel: (+44) 020 7679 4395 
> Fax (+44) 020 7679 7096
> f.calboli at ucl.ac.uk
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> 

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From jasont at indigoindustrial.co.nz  Tue Aug  5 13:42:21 2003
From: jasont at indigoindustrial.co.nz (jasont@indigoindustrial.co.nz)
Date: Tue, 05 Aug 2003 23:42:21 +1200
Subject: [R] bwplot colours
Message-ID: <3f2f981d.1b94.663819948@ihug.co.nz>

> Dear All,
>
> is any way I can change the colours of the box and
> umbrella of a bwplot without having to go to the length
> of:
>
> box.rectangle<-trellis.par.get("box.rectangle")
> box.rectangle$col<-"black"
> trellis.par.set("box.rectangle", box.rectangle)
>
> etc...
>
> but straight from the call:
>
> bwplot(y ~ x | z, mydata) ?

Not directly in the bwplot call, but an easier one
before after you call trellis.device() and before
you call bwplot()

help(lset)

Cheers

Jason



From Ted.Harding at nessie.mcc.ac.uk  Tue Aug  5 14:00:07 2003
From: Ted.Harding at nessie.mcc.ac.uk ( (Ted Harding))
Date: Tue, 05 Aug 2003 13:00:07 +0100 (BST)
Subject: [R] error message in fitdistr
In-Reply-To: <OFE4B95EA2.EF7FDACE-ONC1256D79.00394C3B@ges.marc.societe-generale.fr>
Message-ID: <XFMail.030805130007.Ted.Harding@nessie.mcc.ac.uk>

On 05-Aug-03 vincent.stoliaroff at sgcib.com wrote:
> Here is a numerical vector test
>> test
>   [1] 206  53 124 112  92  77 118  75  48 176  90  74 107 126  99  84
> ...
> 89  94  69  74  99  97  91  92
> 
> Assuming it follows a lognormal distribution I'd like to determine the
> mean and the sd thanks to maximum likelihood estimation
> 
>> fitdistr(test,"lognormal",start=list(200,10))
> Error in print.fitdistr(structure(list(estimate = c(4.54666263736726, 
>:
>         more elements supplied than there are to replace
> 
> I chose the parameter start randomly
> 
> I don't understand the error message. Has anybody ever encountered such
> one?

Hmmm. Not being practised with "fitdist", this took me a moment to track
down. However, the clues are:

1. In ?fitdist ::
   fitdistr(x, densfun, start, ...)
   start: A named list giving the parameters to be optimized with
          initial values.  This can be omitted for some of the named
          distributions (see Details) [not lognormal]. 
2. Hence start must be a _named_ list, i.e. its components must have
   names. So the question is: what to call them?
   For this you need to know what the names are for the parameters in
   your designated distribution. So:
3. help.search("lognormal") -> Lognormal(base)
4. ?Lognormal ::
   dlnorm(x, meanlog = 0, sdlog = 1, log = FALSE)

So it looks as though you need names "meanlog" and "sdlog", so now try

   fitdistr(test,"lognormal",start=list(meanlog=4,sdlog=0.4))

with results

    meanlog       sdlog   
    4.55316203   0.38990402 
    (0.02866631) (0.02026545)

and it works!

The starting values "4" and "0.4" were chosen because

    mean(log(test))
    [1] 4.553205

    sd(log(test))
    [1] 0.3910148

and of course it works if you start elsewhere; but this also shows that
the naive estimate, in this case, was very good.

Best wishes,
Ted.


--------------------------------------------------------------------
E-Mail: (Ted Harding) <Ted.Harding at nessie.mcc.ac.uk>
Fax-to-email: +44 (0)870 167 1972
Date: 05-Aug-03                                       Time: 13:00:07
------------------------------ XFMail ------------------------------



From hoffmann at giub.uni-bonn.de  Tue Aug  5 14:57:25 2003
From: hoffmann at giub.uni-bonn.de (Thomas Hoffmann)
Date: Tue, 05 Aug 2003 14:57:25 +0200
Subject: [R] newbie question: fitting power law
Message-ID: <3F2FA9B5.6000805@giub.uni-bonn.de>

Dear R-Helpers,

I try  to fit my x and y vector-data with a power law using a the 
following command:

 test <- nls(y ~ A*x^B, xy, start=list(A=0.5,B=0.8))

and I get the error message:

Error in numericDeriv(form[[3]], names(ind), env) :
        Missing value or an Infinity produced when evaluating the model

Does anybody know whats wrong? (it?s probably a simple newbie-error)

Thanks in advance

Thomas H.



From chrysopa at insecta.ufv.br  Tue Aug  5 14:13:58 2003
From: chrysopa at insecta.ufv.br (Ronaldo Reis Jr.)
Date: Tue, 5 Aug 2003 09:13:58 -0300
Subject: [R] R2HTML
Message-ID: <200308050913.58227.chrysopa@insecta.ufv.br>

Hi,

I try to use R2HTML to make output. It is very good for this.

I have this HTML output:

-------------- HTMLStart(HTMLframe=F,echo=T) ---------

> as.title("Comment one") 

Comment one

 

> summary(1)

Table of the summary 1 here
  

> as.title("Comment two") 

Comment two

 

> summary(2)

Table of the summary 2 here
------------------- HTMLStop() --------

But I like this output:

-------------- HTMLStart(HTMLframe=F,echo=T) ---------

> ## Comment one

> summary(1)

Table of the summary 1 here
  

> ## Comment two

> summary(2)

Table of the summary 2 here
------------------- HTMLStop() --------

I try to use the # form comment, but dont work, it is not include in echo.

It is possible?

Thanks
Ronaldo
-- 
We are what we are.
--
|>   // | \\   [***********************************]
|   ( ?   ? )  [Ronaldo Reis J?nior                ]
|>      V      [UFV/DBA-Entomologia                ]
|    /     \   [36571-000 Vi?osa - MG              ]
|>  /(.''`.)\  [Fone: 31-3899-2532                 ]
|  /(: :'  :)\ [chrysopa at insecta.ufv.br            ]
|>/ (`. `'` ) \[ICQ#: 5692561 | LinuxUser#: 205366 ]
|    ( `-  )   [***********************************]
|>>  _/   \_Powered by GNU/Debian Woody/Sarge



From hoffmann at giub.uni-bonn.de  Tue Aug  5 16:14:05 2003
From: hoffmann at giub.uni-bonn.de (Thomas Hoffmann)
Date: Tue, 05 Aug 2003 16:14:05 +0200
Subject: [R] newbie question: fitting power law
References: <3F2FA9B5.6000805@giub.uni-bonn.de>
	<1060088824.3f2fabf8e6faa@my.uq.edu.au>
Message-ID: <3F2FBBAD.5020102@giub.uni-bonn.de>

Dear R-helpers, dear Andrew

Andrew C. Ward schrieb:

>Dear Thomas,
>
>I wonder if you have any NAs in xy$x or xy$y. 
>
The Dataframe looks like the following:
 > xy
   x          y
1 3.0  121.50951
2 2.0   30.61258
3 4.0  323.14837
4 8.2 3709.92471
For testing reasons I calculated y by:
 > y <- 2.9 x^3.4

>Also, I think
>you could take logs of your equation and end up with a
>linear expression?
>
if I use:

plot (x,y)
abline lm ( log(y)~log(x),xy)

the line does not seem to fit the plotted datapoints at all. While the 
fitted exponent seems to be okay, the obtained intercept value is wrong.

Thanks for your answer.

Thomas H.

>  
>



From mtolvieira at msn.com  Tue Aug  5 18:10:37 2003
From: mtolvieira at msn.com (Marcel Vieira)
Date: Tue, 05 Aug 2003 17:10:37 +0100
Subject: [R] 3D matrix
Message-ID: <BAY5-F18xyv8ORpuwoI0001acc1@hotmail.com>

Dear all,

Is it possible to define a 3 dimension matrix in R?
(without using list - if possible)

Many thanks in advance.

Marcel



From John.Marsland at CommerzbankIB.com  Tue Aug  5 18:22:01 2003
From: John.Marsland at CommerzbankIB.com (Marsland, John)
Date: Tue, 5 Aug 2003 17:22:01 +0100 
Subject: [R] 3D matrix
Message-ID: <8CBAA121CEB4D5118CB200508BB2BBEF0317E8F3@xmx8lonib.lonib.commerzbank.com>

Try ?array

eg array(1:8,c(2,2,2))

> -----Original Message-----
> From: Marcel Vieira [mailto:mtolvieira at msn.com]
> Sent: 05 August 2003 17:11
> To: r-help at stat.math.ethz.ch
> Subject: [R] 3D matrix
> 
> 
> Dear all,
> 
> Is it possible to define a 3 dimension matrix in R?
> (without using list - if possible)
> 
> Many thanks in advance.
> 
> Marcel
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> 


********************************************************************** 
This is a commercial communication from Commerzbank AG.\ \ T...{{dropped}}



From ripley at stats.ox.ac.uk  Tue Aug  5 18:28:07 2003
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Tue, 5 Aug 2003 17:28:07 +0100 (BST)
Subject: [R] 3D matrix
In-Reply-To: <BAY5-F18xyv8ORpuwoI0001acc1@hotmail.com>
Message-ID: <Pine.LNX.4.44.0308051726370.3754-100000@gannet.stats>

Do you mean a 3-dimensional array?  If so see the help for array().
In S/R terminology, a matrix is 2D, by definition.

On Tue, 5 Aug 2003, Marcel Vieira wrote:

> Is it possible to define a 3 dimension matrix in R?
> (without using list - if possible)

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From mtolvieira at msn.com  Tue Aug  5 18:43:01 2003
From: mtolvieira at msn.com (Marcel Vieira)
Date: Tue, 05 Aug 2003 17:43:01 +0100
Subject: [R] 3D matrix
Message-ID: <BAY5-F7LuQJJWMCh3Yv0001ad0e@hotmail.com>

Dear all,

Thanks for all the answers.

I think I need to explain what I want to do.

I want to create 100 matrices with size 12x12.

The following program is not working...
nk <- 100
for (i in 1:nk) {
case[i]<-matrix(0,12,12)
}

Thanks a lot.
Regards.
Marcel

>From: Prof Brian Ripley <ripley at stats.ox.ac.uk>
>
>Do you mean a 3-dimensional array?  If so see the help for array().
>In S/R terminology, a matrix is 2D, by definition.
>
>On Tue, 5 Aug 2003, Marcel Vieira wrote:
>
> > Is it possible to define a 3 dimension matrix in R?
> > (without using list - if possible)
>
>--
>Brian D. Ripley,                  ripley at stats.ox.ac.uk
>Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
>University of Oxford,             Tel:  +44 1865 272861 (self)
>1 South Parks Road,                     +44 1865 272866 (PA)
>Oxford OX1 3TG, UK                Fax:  +44 1865 272595
>



From dave at evocapital.com  Tue Aug  5 19:24:11 2003
From: dave at evocapital.com (David Khabie-Zeitoune)
Date: Tue, 5 Aug 2003 18:24:11 +0100
Subject: [R] 3D matrix
Message-ID: <8D0F30FE2EB3314182D4A33F738BB19D063EBD@sqlsrvr.evocapital.com>

As previous replies have suggested -- you could use an "array"
E.g.

> X = array(0, dim=c(100,12,12))

Then X[i, , ] returns the i-th 12 by 12 matrix "slice" in this array,
e.g. 

> X[1, ,]

     [,1] [,2] [,3] [,4] [,5] [,6] [,7] [,8] [,9] [,10] [,11] [,12]
 [1,]    0    0    0    0    0    0    0    0    0     0     0     0
 [2,]    0    0    0    0    0    0    0    0    0     0     0     0
 [3,]    0    0    0    0    0    0    0    0    0     0     0     0
 [4,]    0    0    0    0    0    0    0    0    0     0     0     0
 [5,]    0    0    0    0    0    0    0    0    0     0     0     0
 [6,]    0    0    0    0    0    0    0    0    0     0     0     0
 [7,]    0    0    0    0    0    0    0    0    0     0     0     0
 [8,]    0    0    0    0    0    0    0    0    0     0     0     0
 [9,]    0    0    0    0    0    0    0    0    0     0     0     0
[10,]    0    0    0    0    0    0    0    0    0     0     0     0
[11,]    0    0    0    0    0    0    0    0    0     0     0     0
[12,]    0    0    0    0    0    0    0    0    0     0     0     0

-----Original Message-----
From: Marcel Vieira [mailto:mtolvieira at msn.com] 
Sent: 05 August 2003 17:43
To: ripley at stats.ox.ac.uk
Cc: r-help at stat.math.ethz.ch
Subject: Re: [R] 3D matrix


Dear all,

Thanks for all the answers.

I think I need to explain what I want to do.

I want to create 100 matrices with size 12x12.

The following program is not working...
nk <- 100
for (i in 1:nk) {
case[i]<-matrix(0,12,12)
}

Thanks a lot.
Regards.
Marcel

>From: Prof Brian Ripley <ripley at stats.ox.ac.uk>
>
>Do you mean a 3-dimensional array?  If so see the help for array(). In 
>S/R terminology, a matrix is 2D, by definition.
>
>On Tue, 5 Aug 2003, Marcel Vieira wrote:
>
> > Is it possible to define a 3 dimension matrix in R? (without using 
> > list - if possible)
>
>--
>Brian D. Ripley,                  ripley at stats.ox.ac.uk
>Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
>University of Oxford,             Tel:  +44 1865 272861 (self)
>1 South Parks Road,                     +44 1865 272866 (PA)
>Oxford OX1 3TG, UK                Fax:  +44 1865 272595
>

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://www.stat.math.ethz.ch/mailman/listinfo/r-help



From tlumley at u.washington.edu  Tue Aug  5 19:46:34 2003
From: tlumley at u.washington.edu (Thomas Lumley)
Date: Tue, 5 Aug 2003 10:46:34 -0700 (PDT)
Subject: [R] How do I make R listen to a Unix domain socket?
In-Reply-To: <20030805071247.GA29519@kriemhild.uft.uni-bremen.de>
Message-ID: <Pine.A41.4.44.0308051042570.67942-100000@homer24.u.washington.edu>

On Tue, 5 Aug 2003, Johannes Ranke wrote:

> Respected r-help readers,
>
> I am a bit uncomfortable because my favourite text editor vim does not
> speak statistics. So I obtained a perl-script made to get vim to
> interact with a lisp interpreter and had it set up so vim sends text to
> a socket and R executes - but there are some irreproducible errors and I
> don't understand the perl-script.
>
> Could someone show me an alternative method to start R in a way that it
> listens to a unix domain socket? I know about the R function
> socketConnection() but this creates tcp sockets which I don't know how
> to send text to.

I'm not sure that this is possible. The editor that does speak statistics doesn't do it this
way: it takes over stdin and stdout instead.

	-thomas



From pdebruic at mgmt.purdue.edu  Tue Aug  5 20:28:23 2003
From: pdebruic at mgmt.purdue.edu (Debruicker, Paul Andrew)
Date: Tue, 5 Aug 2003 13:28:23 -0500
Subject: [R] code speed help?   --   example and results provided
Message-ID: <D88E4A8CEFAE3D44B1959692C2C85A4831905A@parkplace.mgmt.purdue.edu>

I have the following piece of code that combines lists comprised of components of varying length into a list with components of constant length.  I have found 2 ways to do it, and the faster of the two is posted below along with sample results.  Do you have any suggestions on how to decrease the calculation time by modifying the code?


> ####Function###########
> replacement2.idx<-function(life=w.life,N=years,n=samples){
+
+ yrs<-rep(N,n)
+ ind<-yrs-life
+
+ x1<-mapply(rep,times=life,x=0)
+ x2<-mapply(rep,times=ind,x=1)
+
+ x3<-data.frame(c(x1[[1]],x2[[1]]))
+
+ for(i in 2:n) x3<-data.frame(x3, append(x1[[i]],x2[[i]]))
+
+ x3<-t(x3)
+ }
>
>
>
> ###Sample Output###
> samples<-5
> years<-10
> w.life<-round(rnorm(samples,5,1),digits=0)
>
> system.time(abc<-replacement2.idx())
[1] 0.04 0.00 0.07 0.00 0.00
>
> abc
                         1 2 3 4 5 6 7 8 9 10
c.x1..1....x2..1...      0 0 0 0 0 0 1 1 1  1
append.x1..i....x2..i... 0 0 0 0 0 0 1 1 1  1
append.x1..i....x2..i... 0 0 0 0 0 0 1 1 1  1
append.x1..i....x2..i... 0 0 0 0 1 1 1 1 1  1
append.x1..i....x2..i... 0 0 0 0 0 1 1 1 1  1
>
>
>
> ###1000 samples####
> samples<-1000
> w.life<-round(rnorm(samples,5,.5),digits=0)
> system.time(method2<-replacement2.idx())
[1] 12.79  0.00 14.04  0.00  0.00
>
>
>
> ###5000 samples####
> samples<-5000
> w.life<-round(rnorm(samples,5,.5),digits=0)
> system.time(method2<-replacement2.idx())
[1] 544.82   0.00 593.98   0.00   0.00
>



From jerome at hivnet.ubc.ca  Tue Aug  5 21:05:40 2003
From: jerome at hivnet.ubc.ca (Jerome Asselin)
Date: Tue, 5 Aug 2003 12:05:40 -0700
Subject: [R] code speed help?   --   example and results provided
In-Reply-To: <D88E4A8CEFAE3D44B1959692C2C85A4831905A@parkplace.mgmt.purdue.edu>
References: <D88E4A8CEFAE3D44B1959692C2C85A4831905A@parkplace.mgmt.purdue.edu>
Message-ID: <200308051912.MAA26832@hivnet.ubc.ca>


It doesn't seem that you need data frame to do this since x1 and x2 are 
always numeric. I think your final data frame object has N rows.

This should be much faster.

n <- 5
N <- 10
life <- c(3,2,7,8,10)
ind <- N-life
matrix(rep(rep(0:1,n),c(rbind(life,ind))),ncol=N,byrow=T)

BTW, it would have been easier to help if you had put your executable 
example ready to "cut and paste". All the ">" and "+" make it tedious to 
reproduce your example.

HTH,
Jerome

On August 5, 2003 11:28 am, Debruicker, Paul Andrew wrote:
> I have the following piece of code that combines lists comprised of
> components of varying length into a list with components of constant
> length.  I have found 2 ways to do it, and the faster of the two is
> posted below along with sample results.  Do you have any suggestions on
> how to decrease the calculation time by modifying the code?
>
> > ####Function###########
> > replacement2.idx<-function(life=w.life,N=years,n=samples){
>
> +
> + yrs<-rep(N,n)
> + ind<-yrs-life
> +
> + x1<-mapply(rep,times=life,x=0)
> + x2<-mapply(rep,times=ind,x=1)
> +
> + x3<-data.frame(c(x1[[1]],x2[[1]]))
> +
> + for(i in 2:n) x3<-data.frame(x3, append(x1[[i]],x2[[i]]))
> +
> + x3<-t(x3)
> + }
>
> > ###Sample Output###
> > samples<-5
> > years<-10
> > w.life<-round(rnorm(samples,5,1),digits=0)
> >
> > system.time(abc<-replacement2.idx())
>
> [1] 0.04 0.00 0.07 0.00 0.00
>
> > abc
>
>                          1 2 3 4 5 6 7 8 9 10
> c.x1..1....x2..1...      0 0 0 0 0 0 1 1 1  1
> append.x1..i....x2..i... 0 0 0 0 0 0 1 1 1  1
> append.x1..i....x2..i... 0 0 0 0 0 0 1 1 1  1
> append.x1..i....x2..i... 0 0 0 0 1 1 1 1 1  1
> append.x1..i....x2..i... 0 0 0 0 0 1 1 1 1  1
>
> > ###1000 samples####
> > samples<-1000
> > w.life<-round(rnorm(samples,5,.5),digits=0)
> > system.time(method2<-replacement2.idx())
>
> [1] 12.79  0.00 14.04  0.00  0.00
>
> > ###5000 samples####
> > samples<-5000
> > w.life<-round(rnorm(samples,5,.5),digits=0)
> > system.time(method2<-replacement2.idx())
>
> [1] 544.82   0.00 593.98   0.00   0.00
>
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help



From parkhurs at ariel.ucs.indiana.edu  Tue Aug  5 21:31:03 2003
From: parkhurs at ariel.ucs.indiana.edu (David Parkhurst)
Date: Tue, 5 Aug 2003 14:31:03 -0500
Subject: [R] na.action in randomForest --- Summary
Message-ID: <000201c35b88$4a18b940$0a6cfea9@BLSPEAPARKHOM>

A few days ago I asked whether there were options other than
na.action=na.fail for the R port of Breiman?s randomForest;  the function?s
help page did not say anything about other options.

I have since discovered that a pdf document called ?The randomForest
 Package? and made available by Andy Liaw (who made the tool available in
R---thank you) does discuss an option.  It is an implementation of Breiman?s
suggestion ?to replace each missing value by the median of its column and
each missing categorical by the most frequent value in that categorical. My
impression is that because of the randomness and the many trees grown,
filling in missing values with a sensible values does not effect accuracy
much.? (from his report, "Manual On Setting Up, Using, And Understanding
Random Forests V3.1").

I now plan to try the na.roughfix option from Liaw?s package.

Thanks to Uwe Ligges and Brian Ripley for their replies to my posting.

Dave Parkhurst



From mzhang1208 at hotmail.com  Tue Aug  5 23:16:56 2003
From: mzhang1208 at hotmail.com (weidong zhang)
Date: Tue, 05 Aug 2003 21:16:56 +0000
Subject: [R] Error on mclust
Message-ID: <BAY1-F106dtDlVNwvuW0000951b@hotmail.com>

Hi All,


I am trying to cluster a one-dimensional data (see the file attached) using 
Mclust() but got an error message like:
>Mclust(x)
Error in rep(1, n) : Object "n" not found

When I do a simulation sometimes it works sometimes doesn't.

>Mclust(c(rnorm(50),rnorm(56,-0.5)))
Error in rep(1, n) : Object "n" not found

>Mclust(c(rnorm(56),rnorm(56,-0.5)))

best model: unequal variance with 2 groups

averge/median classification uncertainty: 0.001 / 0

Can anybody help me with this? Thanks.

Weidong

_________________________________________________________________



From jerome at hivnet.ubc.ca  Wed Aug  6 00:44:36 2003
From: jerome at hivnet.ubc.ca (Jerome Asselin)
Date: Tue, 5 Aug 2003 15:44:36 -0700
Subject: [R] Error on mclust
In-Reply-To: <BAY1-F106dtDlVNwvuW0000951b@hotmail.com>
References: <BAY1-F106dtDlVNwvuW0000951b@hotmail.com>
Message-ID: <200308052251.PAA05530@hivnet.ubc.ca>


I can't find the function Mclust(). Which package is it from?

Nonetheless, here are a couple of steps to help you find the problem:
1) Find a reproducable example which ALWAYS recreate your problem (i.e., 
define x so that Mclust(x) gives your error message);
2) Run "debug(Mclust); Mclust(x)" and execute your example step by step 
until you see the error. Hopefully, this will help you find the cause of 
the problem.

If you still can't resolve it, then send us your reproducable example 
(from step 1). Try to use a very small data set that illustrates the 
problem.

HTH,
Jerome

On August 5, 2003 02:16 pm, weidong zhang wrote:
> Hi All,
>
>
> I am trying to cluster a one-dimensional data (see the file attached)
> using
>
> Mclust() but got an error message like:
> >Mclust(x)
>
> Error in rep(1, n) : Object "n" not found
>
> When I do a simulation sometimes it works sometimes doesn't.
>
> >Mclust(c(rnorm(50),rnorm(56,-0.5)))
>
> Error in rep(1, n) : Object "n" not found
>
> >Mclust(c(rnorm(56),rnorm(56,-0.5)))
>
> best model: unequal variance with 2 groups
>
> averge/median classification uncertainty: 0.001 / 0
>
> Can anybody help me with this? Thanks.
>
> Weidong
>
> _________________________________________________________________
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help



From paul at woodgasllc.com  Tue Aug  5 20:26:49 2003
From: paul at woodgasllc.com (Paul DeBruicker)
Date: Tue, 5 Aug 2003 13:26:49 -0500
Subject: [R] code speed help, 
Message-ID: <6084D4C6-C772-11D7-905D-0003931A8C1A@woodgasllc.com>

I have the following piece of code that combines lists comprised of 
components of varying length into a list with components of constant 
length.  I have found 2 ways to do it, and the faster of the two is 
posted below along with sample results.  Do you have any suggestions on 
how to decrease the calculation time by modifying the code?


 > ####Function###########
 > replacement2.idx<-function(life=w.life,N=years,n=samples){
+
+ yrs<-rep(N,n)
+ ind<-yrs-life
+
+ x1<-mapply(rep,times=life,x=0)
+ x2<-mapply(rep,times=ind,x=1)
+
+ x3<-data.frame(c(x1[[1]],x2[[1]]))
+
+ for(i in 2:n) x3<-data.frame(x3, append(x1[[i]],x2[[i]]))
+
+ x3<-t(x3)
+ }
 >
 >
 >
 > ###Sample Output###
 > samples<-5
 > years<-10
 > w.life<-round(rnorm(samples,5,1),digits=0)
 >
 > system.time(abc<-replacement2.idx())
[1] 0.04 0.00 0.07 0.00 0.00
 >
 > abc
                          1 2 3 4 5 6 7 8 9 10
c.x1..1....x2..1...      0 0 0 0 0 0 1 1 1  1
append.x1..i....x2..i... 0 0 0 0 0 0 1 1 1  1
append.x1..i....x2..i... 0 0 0 0 0 0 1 1 1  1
append.x1..i....x2..i... 0 0 0 0 1 1 1 1 1  1
append.x1..i....x2..i... 0 0 0 0 0 1 1 1 1  1
 >
 >
 >
 > ###1000 samples####
 > samples<-1000
 > w.life<-round(rnorm(samples,5,.5),digits=0)
 > system.time(method2<-replacement2.idx())
[1] 12.79  0.00 14.04  0.00  0.00
 >
 >
 >
 > ###5000 samples####
 > samples<-5000
 > w.life<-round(rnorm(samples,5,.5),digits=0)
 > system.time(method2<-replacement2.idx())
[1] 544.82   0.00 593.98   0.00   0.00
 >



From ceciliashiraiwa at ig.com.br  Wed Aug  6 01:10:50 2003
From: ceciliashiraiwa at ig.com.br (=?iso-8859-1?Q?Cec=EDlia_Shiraiwa?=)
Date: Tue, 5 Aug 2003 20:10:50 -0300
Subject: [R] Split factorial design
Message-ID: <005a01c35bab$1d7bbe50$55bc64c8@shiraiwa38ryjf>

Dear all,

Could someone help me to understand how to use the argument split on
summary.aov?
I would like to split the interaction of a 2x2x3 factorial design.

Thanks in advance

Cec?lia Satie Shiraiwa
Unesp-Brasil



From mase at is.titech.ac.jp  Wed Aug  6 04:19:02 2003
From: mase at is.titech.ac.jp (Shigeru Mase)
Date: Wed, 06 Aug 2003 11:19:02 +0900
Subject: [R] L10N and i18n of R
Message-ID: <3F306596.5020008@is.titech.ac.jp>

Dear R-users of non-English speaking countires.

Maybe a good news for those who want to use R in their
local languages. Recently two Japanese, E. Nakama and
M. Okada, succeeded in "making R speak Japanese". At
present, R can handle Japanese (as well as other languages,
I guess) character strings if one use consoles which
can understand Japanese. Also P. Murrell kindly
provided a facility to plot hundreads of Japaense
characaters as graphical symbols (Hershey vector fonts).

Nakama and Okada's "L10N" (Localization) and "i18n"
(internationalization) patches make it possible to
use Japanese object names and to display Japanese
characters on graphical devices (although still limited).
If you are interested, please visit URL:

http://www.okada.jp.org/RWiki/index.php?%5B%5Bi18n_of_R%5D%5D

which is the only English page of RjpWiki, a Wiki-based
collaboration site supported by Japanese R users.

Nakama's patches can also handle other languages
than Japanese. He kindly built Korean and Russian
version of R (1.7.1) as rpm binaries (although
he can understand neither Korean nor Russian).

We are glad if these patches can give hints to R
users who want to use local languages. Please note
their patches are by no means complete and may
potentially give your systems troubles. They and
related binaries are offered without no warranty.
Feedbacks and reports are welcome, but mere inquiries
and complains not. Since the version up of R is so
frequent, we cannot promise that these patches will
be updated in future.

Yours
------------------------------------------------
Shigeru Mase
Dept. Math. and Comp. Sciences,
Tokyo Institute of Technology, Tokyo, Japan



From andy_liaw at merck.com  Wed Aug  6 06:33:17 2003
From: andy_liaw at merck.com (Liaw, Andy)
Date: Wed, 06 Aug 2003 00:33:17 -0400
Subject: [R] na.action in randomForest --- Summary
Message-ID: <3A822319EB35174CA3714066D590DCD50205C982@usrymx25.merck.com>

To make it clear:  Version 3.3 and older of Breiman's code do not handle NAs
at all:  you need to exclude them before running random forest.  One can
easily do so by using na.action=na.omit (which, despite the default, seemed
to be what's used).  The airquality data used in the examples in the
randomForest help page has many NAs, for example.

The na.roughfix implements the idea that Breiman added to V4.0 of his code
(only for classification).  For "fancier" imputation by nearest neighbors,
measured by proximities from random forest, there's the "rfImpute" function
(linked from the help page of na.roughfix).  The advantage is that it works
for both regression and classification.

I have not yet implemented all the new features that Leo introduced in V4,
so the version for the randomForest is currently at 3.9-x.  Adding the new
features is more involved than one may think, as I'm adding the features to
the existing code, rather than modifying Leo's new code and put in the
package.  The reason:  I've fixed a few bugs and added a few features in the
package, and I don't want to loose those.

Just so you know, I believe Leo is making some changes to the way imputation
is done in V5...

HTH,
Andy

> -----Original Message-----
> From: David Parkhurst [mailto:parkhurs at ariel.ucs.indiana.edu] 
> Sent: Tuesday, August 05, 2003 3:31 PM
> To: r-help at stat.math.ethz.ch
> Subject: Re: [R] na.action in randomForest --- Summary
> 
> 
> A few days ago I asked whether there were options other than 
> na.action=na.fail for the R port of Breiman's randomForest;  
> the function's help page did not say anything about other options.
> 
> I have since discovered that a pdf document called "The 
> randomForest  Package" and made available by Andy Liaw (who 
> made the tool available in R---thank you) does discuss an 
> option.  It is an implementation of Breiman's suggestion "to 
> replace each missing value by the median of its column and 
> each missing categorical by the most frequent value in that 
> categorical. My impression is that because of the randomness 
> and the many trees grown, filling in missing values with a 
> sensible values does not effect accuracy much." (from his 
> report, "Manual On Setting Up, Using, And Understanding 
> Random Forests V3.1").
> 
> I now plan to try the na.roughfix option from Liaw's package.
> 
> Thanks to Uwe Ligges and Brian Ripley for their replies to my posting.
> 
> Dave Parkhurst
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list 
> https://www.stat.math.ethz.ch/mailman/listinfo> /r-help
> 

------------------------------------------------------------------------------
Notice:  This e-mail message, together with any attachments, contains
information of Merck & Co., Inc. (Whitehouse Station, New Jersey, USA), and/or
its affiliates (which may be known outside the United States as Merck Frosst,
Merck Sharp & Dohme or MSD) that may be confidential, proprietary copyrighted
and/or legally privileged, and is intended solely for the use of the
individual or entity named on this message.  If you are not the intended
recipient, and have received this message in error, please immediately return
this by e-mail and then delete it.



From adrian at asteridia.maths.uwa.edu.au  Wed Aug  6 06:56:21 2003
From: adrian at asteridia.maths.uwa.edu.au (Adrian Baddeley)
Date: Wed, 6 Aug 2003 12:56:21 +0800
Subject: [R] contour lines intersect
Message-ID: <16176.35445.429938.944491@asteridia.maths.uwa.edu.au>

Hi,

Sorry if this is already known...

contour() sometimes draws contour lines that intersect.
Is there a temporary fix?

A dataset which causes problems is at
   http://www.maths.uwa.edu.au/~adrian/dumpdata.R
If you try just
   source("dumpdata.R")
   image(huh)
   contour(huh)
the 100 x 100 matrix 'huh' contains an hourglass-shaped region
of values around 0.8. The contour plot shows two contour lines at
the level 0.8 which cross each other in a figure 8.

In the short term can anyone suggest how to fix the contour plot
for this particular dataset??

My system: R 1.7.0 (16/4/2003) i686-pc-linux-gnu

----
Adrian Baddeley, Mathematics & Statistics, University of Western Australia
		<http://maths.uwa.edu.au/~adrian/>



From mkondrin at hppi.troitsk.ru  Wed Aug  6 18:25:58 2003
From: mkondrin at hppi.troitsk.ru (M.Kondrin)
Date: Wed, 06 Aug 2003 09:25:58 -0700
Subject: [R] L10N and i18n of R
In-Reply-To: <3F306596.5020008@is.titech.ac.jp>
References: <3F306596.5020008@is.titech.ac.jp>
Message-ID: <3F312C16.9070903@hppi.troitsk.ru>

Shigeru Mase wrote:

> Nakama's patches can also handle other languages
> than Japanese. He kindly built Korean and Russian
> version of R (1.7.1) as rpm binaries (although
> he can understand neither Korean nor Russian).
> 

It is interesting, but with Russian R works fine as is (can say nothing 
  about Korean and Japanese). Using russian words as names for objects 
and functions as well as text annotations in graphic devices does not 
cause any troubles. I use R-1.7.0 under Linux with koi8-r Russian system 
locale installed.



From ripley at stats.ox.ac.uk  Wed Aug  6 07:50:20 2003
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Wed, 6 Aug 2003 06:50:20 +0100 (BST)
Subject: [R] Split factorial design
In-Reply-To: <005a01c35bab$1d7bbe50$55bc64c8@shiraiwa38ryjf>
Message-ID: <Pine.LNX.4.44.0308060649050.7439-100000@gannet.stats>

It is explained, with an example, on the help page.  That and an example 
in MASS is all the documentation there is, apart from the source code.

On Tue, 5 Aug 2003, Cec?lia Shiraiwa wrote:

> Could someone help me to understand how to use the argument split on
> summary.aov?
> I would like to split the interaction of a 2x2x3 factorial design.

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From jlandgr1 at gwdg.de  Wed Aug  6 08:28:15 2003
From: jlandgr1 at gwdg.de (jobst landgrebe)
Date: Wed, 06 Aug 2003 08:28:15 +0200
Subject: [R] R CMD check: checking for undocumented objects ... WARNING
In-Reply-To: <mailman.0.1060150767.19585.r-help@stat.math.ethz.ch>
Message-ID: <E19kHmR-0000PX-NZ@mailer.gwdg.de>

Dear List,

I have a problem building an R package using R 1.7.1 that I cannot resolve
myself. R CMD check tells me that functions I documented properly in Rd-files
are not documented and gives a TeX error at the end, altough the Rd files
check is OK. I have three fucntions, one gets through without complaining, the
other two don't. All the docu for the example data is also OK.
I went through the Rd files over and over, but could not find any problem.

Can anyone help me?

Thank in advance,

Jobst

with R CMD check daMA (that's my package), I get:

* checking for working latex ... OK
* using log directory '/home/jlandgr1/stat_stuff/Rpackages/daMA/daMA.Rcheck'
* checking for file 'daMA/DESCRIPTION' ... OK
* checking if this is a source package ... OK

* Installing *source* package 'daMA' ...
** R
** data
** help
 >>> Building/Updating help pages for package 'daMA'
     Formats: text html latex example 
  analyseMA                         text    html    latex   example
  cinfo                             text    html    latex   example
  cinfoB.AB                         text    html    latex   example
  cmat                              text    html    latex   example
  cmatB.AB                          text    html    latex   example
  core                              text    html    latex   example
  data.3x2                          text    html    latex   example
  designMA                          text    html    latex   example
  designs.basic                     text    html    latex   example
  designs.composite                 text    html    latex   example
  id.3x2                            text    html    latex   example
* DONE (daMA)

* DONE (INSTALL)

* checking package directory ... OK
* checking for sufficient/correct file permissions ... OK
* checking DESCRIPTION meta-information ... OK
* checking package dependencies ... OK
* checking index information ... OK
* checking package subdirectories ... OK
* checking R files for syntax errors ... OK
* checking R files for library.dynam ... OK
* checking generic/method consistency ... OK
* checking for assignment functions with final arg not named 'value' ... OK
* checking Rd files ... OK
* checking for undocumented objects ... WARNING
Undocumented code objects:
[1] "analyseMA" "designMA" 
* checking for code/documentation mismatches ... OK
* checking for undocumented arguments in \usage ... OK
* creating daMA-Ex.R ... OK
* checking examples ... OK
* creating daMA-manual.tex ... OK
* checking daMA-manual.tex ... ERROR
Could not create DVI version.
This typically indicates Rd problems.



Dr. Jobst Landgrebe
Universitaet Goettingen
Zentrum Biochemie
Humboldtallee 23
37073 Goettingen
Germany

tel: 0551/39-2316 oder -2223
fax: 0551/39-5960
web: http://www.microarrays.med.uni-goettingen.de
mail: jlandgr1 at gwdg.de



From Agustina at contizaypizarronweb.com.ar  Wed Aug  6 08:48:47 2003
From: Agustina at contizaypizarronweb.com.ar (Curso GIS y Teledeteccion)
Date: Wed, 06 Aug 2003 06:48:47 -0000
Subject: [R] Curso GIS y Teledeteccion
Message-ID: <200308060648.h766mrE22864@ar18.toservers.com>

Curso GIS y Teledeteccion
Para solicitar informaci?n mandar mail a mercedesgis at argentina.com

CENTRO DE TRANSFERENCIA TECNOL?GICA EN SISTEMAS DE INFORMACI?N GEOGR?FICA Y TELEDETECCI?N


---TUTORIAL GRATUITO
Entendiendo la proyecci?n de los mapas - SISTEMA Gauss-Kr?ger

---CLASES GRATUITAS: Primer m?dulo de los siguientes cursos
ArcView 3.2
ArcView 8.2 - ArcGis
ENVI 3.5
Bases de Datos para Todos 
Sistemas de Informaci?n Geogr?fica 
Teledetecci?n

---CURSOS 2003 
ArcView 8.3
1/5 de Septiembre de 18 a 22hs 
8/12 de Diciembre de 18 a 22hs 

ENVI 3.6 
3/7 de Noviembre 19 a 22hs

ArcView 3.2 
21/25 de Julio 19 a 22hs
6/10 de Octubre 19 a 22hs

---CURSOS DISTANCIA:
Sistemas de Informaci?n Geogr?fica (Certificado Universidad de Buenos Aires) 
ArcView 3.2
ArcView 8.1
Teledetecci?n (Certificado Universidad de Buenos Aires)
ENVI 3.5
Bases de Datos para Todos (Certificado Universidad de Buenos Aires)

Fecha: Una de las caracter?sticas que hacen a nuestros cursos a distancia ?nicos y una alternativa atractiva para mejorar la formaci?n profesional, se inician en el momento que se realiza la inscripci?n. 


LISTADO DE PARTICIPANTES ??
?
?
- UNIVERSIDADES
Ancash (Per?), Bah?a Blanca, Belgrano, Bologna, Buenos Aires, Cat?lica, Centro de la Provincia de Buenos Aires, Chiba (Jap?n), C?rdoba, Comahue, Cuyo, Fraternidad de Agrupaciones Santo Tom?s de Aquino (F.A.S.T.A), Flores, Formosa, General Sarmiento, Guanajuato (M?xico), Jujuy, La Plata, Latinoamericana de Ciencias Ambientales, La Pampa, La Patagonia Austral, La Republica (Uruguay), La Rioja, Litoral, Lomas de Zamora, Lujan, Mar del Plata, Mayor de San Andres (Bolivia), Misiones, Nacional (Colombia), Nacional Autonoma de Mexico (Mexico), Nordeste, Pilar (Paraguay), Quilmes, Rosario, Salta, Salvador, San Juan, Santa Fe, San Juan Bosco, Sur, Tecnol?gica Nacional, Torcuato Di Tella, Tucum?n, Valladolid (Espa?a), Veracruzana (M?xico),?etc.
?
-MUNICIPALIDADES
?
3 de Febrero, Bah?a Blanca, Benito Ju?rez, Bragado, Campana, Ca?uelas, Castelli, Chascom?s, Coronel Su?rez, Coronel Rosales, Despe?aderos, Escobar, Florencio Varela, General Alear, General Pueyrred?n, Gobierno de la Ciudad de Buenos Aires, Jes?s Mar?a, Jun?n, La Plata, Laboulaye, Laprida, Lincoln, Lomas de Zamora, Malvinas Argentinas, Moreno, Mor?n, Necochea, Pehuajo, Pergamino, Posadas, Quilmes, Ramallo, R?o Gallegos, Rosario, Saladillo, Salta, San Fernando, San Isidro, San Mart?n, San Nicol?s, San Salvador de Jujuy, Santa Fe, Santo Tom?, Suipacha, Tigre, Venado Tuerto, Vicente L?pez, Viedma, Villa Allende, Villa Constituci?n, Villa de Merlo, Villa Mar?a, etc.?
?
-ONG
ACEN - Asociaci?n para la Conservaci?n y el Estudio de la Naturaleza, ANIMA Arquelogia Submarina (Chile),  Asociaci?n Demosvida, Asociaci?n Guyra (Paraguay), Aves Argentinas, Conservaci?n Argentina, CIAC Centro de Investigaci?n y Apoyo Campesino (Bolivia), DEFOR - Asociacion Civil para la Investigaci?n y Desarrollo Forestal (Per?), Estaci?n Cient?fica Charles Darwin (Ecuador), Fundaci?n Amigos para la Naturaleza Noel Kempff (Bolivia), Fundaci?n Cethus, Fundaci?n CEPA / FLACAM, Fundaci?n Green Cross, Fundaci?n Mois?s Bertoni (Paraguay), Fundaci?n Pro Vivienda Social, Fundaci?n Vida Silvestre Argentina, Instituto de Investigaciones Ambientales del Pac?fico?(Colombia), Plus Radio (Paraguay).
?
-EMPRESAS
?
ACA - Autom?vil Club Argentino, Aeromapa, Agrimax, Agrosat Ambiental SA, Agroservicios SRL, Ambiente OnLine, Analog?as - Empresa dedicada a la investigaci?n de mercado y marketing pol?tico , ASTECNA SA, BA&H BOOZ ALLEN & HAMILTON Consultora, BellSouth, Boston Geom?tica SRL (Paraguay), Buffarini & Buffarini - Agrimensura, Cableuropa SAU (Espa?a), CADEB S.A. - Compa??a Administradora de Empresas (Bolivia), Chevron Petrolera, CNEA - Comisi?n Nacional de Energ?a At?mica, Consultora Ambiental Echechuri, Consultora de Ingenier?a Cooprogetti, ComTrad SA, Cooperativa "16 de Octubre" de Servicios P?blicos, Cooperativa "16 de Octubre" de Servicios P?blicos de Esquel, Cooperativa El?ctrica de Venado Tuerto, Copygraph, Cybermapa, Diario La Naci?n, Econat SA, Energicon SA, Epson Argentina, Estudio EGA, Euroflores SA (Uruguay), Fomicruz SE, Franklin Consultora SA, Fusion Sudamericana SA, GARJO Ingenieros Contratistas SRL (Per?), GEA Ge?logos Asociados, GMS SA, Grant Geophysical Inc (Colom!
bia), Gu?as FILCAR, Halcrow (Ingenier?a Hidr?ulica), Hewlett Packard, HIDRA Servicios de Ingenier?a, Hidroblan SA, HYTEC, IATASA - Ingenier?a y Asistencia T?cnica Argentina SA, ITG Consultores SA, IDEGEPP - Instituto de Desarrollo y Gesti?n de Obras P?blicas, JMB SA?Ingenier?a Ambiental, Kiskali SA, K.K.L. KEREN KAYEMET LEISRAEL (Israel), La Dulce Seguros, Minera Alumbrera Limited, Nordelta SA, Nostromo Consultora SRL, Novartis Argentina SA - Protecci?n de Cultivos, OCA Correos, Oroplata Limited SA, Palma y Asociados, Pampa Byte, Papel Prensa SA, PECOM - Perez Companc, Pedro L. Martinez SA, Petro Imagen, Petrolera Pan American Energy, Petrolera Quintana Minerals, Phoenix Oil & Gas, PlusPetrol, Profertil SA, Recovery SA - Planeamiento y Catastro, Sistemas Catastrales, Sir William Halcrow LTD, SERVICOOP - Cooperativa de Servicios P?blicos de Madryn, SEINCO SRL (Uruguay), Serrater SL (Espa?a), Solurban SA, SRK Consultores SA (Chile), Tack Training SA, TRANSBA SA - Transporte de!
 Energ?a El?ctrica de la Provincia de Buenos Aires, Transpetrol SA, Trico Latinoamericana, URBI - Concesionario del Catastro de la Ciudad de Posadas, WMC - Water Management Consultants (Chile),?YPF-REPSOL, ZYZ Consultores - Servicios en Medioambiente, Estructuras, Estudios de suelos.
?
-OTROS ORGANISMOS
Administraci?n de Parques Nacionales, Armada Argentina, Asociaci?n Civil para la Investigaci?n y Desarrollo Forestal (Per?), Auditoria General de la Naci?n, AIC - Autoridad Interjurisdiccional de las Cuencas de los R?os Limay, Neuqu?n y Negro, Autoridad Regulatoria Nuclear, Banco de la Provincia de Buenos Aires, CEAMSE - Coordinaci?n Ecol?gica ?rea Metropolitana Sociedad del Estado, CDPAP - Centro de Desarrollo de Proyectos Avanzados en Pediatr?a, Centro de Geolog?a de Costas y del Cuaternario, Centro de Sensores Remotos De la FAA, CABA - Programa Reforma Pol?tica, CEPED - Instituto de Investigaciones Econ?micas, CITEFA?- Instituto de Investigaci?n Cient?fica y T?cnica de la Fuerzas Armadas, CNEA - Comisi?n Nacional de Energ?a At?mica, Colegio de Agrimensores - Distrito 1, CIC - Comisi?n de Investigaciones Provincia de Bs. As, CONAE - Comisi?n Nacional de Actividades Espaciales, CONICET - Consejo Nacional de?Investigaciones Cient?ficas y T?cnicas, CONIDA - Comisi?n Nacional !
de Investigaci?n y Desarrollo Aeroespacial (Peru), CFI - Consejo Federal de Inversiones, Consejo Provincial de Educaci?n - Neuqu?n, COREBE - Comisi?n Regional del R?o Bermejo, CORFO - Corporaci?n de Desarrollo Valle Bonaerense del R?o Colorado, Defensor?a del Pueblo, Direcci?n General de Escuelas de la Provincia de Bueno Aires, Direcci?n General de Miner?a - Neuquen, Direcci?n Nacional de Desarrollo Urbano de la Subsecretaria de Desarrollo Urbano y Vivienda de La Naci?n, Direcci?n Provincial de Desarrollo Industrial Minero y Comercial - Catamarca, Entidad Binacional Yaciret?, EPEN - Ente Provincial de Energ?a del Neuquen, Escuela de Ingenier?a de Caminos de Monta?a, Escuela de Prefectura Nacional, Fuerza A?rea Argentina, Gobierno Provincial de C?rdoba, IGM - Instituto Geogr?fico Militar, INA - Instituto Nacional del Agua, INDEC - Instituto Nacional de Estad?stica y Censos, INREMI - Instituto de Recursos Minerales de la Facultad de Ciencias Naturales y Museo UNLP, Instituto A!
nt?rtico Argentino, Instituto Provincial de la Vivienda de Formosa, Instituto Argentino de Oceanograf?a, Instituto de Agricultura Sostenible (Espa?a), INTA - Instituto Nacional de Tecnolog?a Agropecuaria, Laboratorio Tecnol?gico del Uruguay - LATU (Uruguay), Ministerio de Bienes Nacionales (Chile), Ministerio de Desarrollo Social y Medio Ambiente, Ministerio de Econom?a, Ministerio de Econom?a de la Provincia de Neuquen, Ministerio de Econom?a del Chaco, Ministerio de Econom?a de la Ciudad de La Plata, Ministerio de Infraestructura y Vivienda de la Subsecretaria de Recursos H?dricos, Ministerio de Infraestructura y Vivienda, Ministerio de Justicia y Seguridad de la Provincia de Buenos Aires, Ministerio de la Producci?n de la Provincia de Chaco, Ministerio de Servicios y Obras P?blicas de la Provincia de Buenos Aires, Ministerio del Agro y la Producci?n de la Provincia de Misiones, Ministerio del Agro y La Producci?n de la Provincia de Misiones, Ministerio del Interior - Unid!
ad ejecutora Central Catastro, Museo de Ciencias Naturales, Museo Nacional de Costa Rica, OCRABA - ?rgano de Control de los Accesos a la Ciudad de Buenos Aires, ORSEP - Organismo Regulador de Seguridad de Presas, Secretaria de Agricultura de la Naci?n, Secretaria de Miner?a de la Naci?n, Secretar?a de Obras y Servicios P?blicos Provincia de R?o Negro, Secretaria de Turismo de la Naci?n, Secretar?a de Ambiente y Desarrollo Sustentable, SEGEMAR - Servicio Geol?gico Minero Argentino, SENASA - Servicio Nacional de Sanidad Animal y Calidad Agroalimentaria, Servicio Agr?cola Ganadero (Chile), Servicio Nacional de ?reas Protegidas (Bolivia), Servicio de Hidrograf?a Naval, Servicio de Vivienda y Urbanismo (Chile), SIFEM - Sistema Federal de Emergencias del Estado Nacional, Servicio Agr?cola Ganadero AG (Chile), Sociedad Rural de Rivadavia, SUNARP Superintendencia Nacional de los Registros P?blicos (Per?), Vialidad de la Provincia de Buenos Aires, Vialidad de la Provincia de La Rioja!
, Vialidad Nacional, etc.?



From ihaka at r-project.org  Wed Aug  6 10:00:36 2003
From: ihaka at r-project.org (Ross Ihaka)
Date: Wed, 06 Aug 2003 20:00:36 +1200
Subject: [R] contour lines intersect
In-Reply-To: <16176.35445.429938.944491@asteridia.maths.uwa.edu.au>
References: <16176.35445.429938.944491@asteridia.maths.uwa.edu.au>
Message-ID: <3F30B5A4.300@r-project.org>

Adrian Baddeley wrote:
> Hi,
> 
> Sorry if this is already known...
> 
> contour() sometimes draws contour lines that intersect.
> Is there a temporary fix?
> 
> A dataset which causes problems is at
>    http://www.maths.uwa.edu.au/~adrian/dumpdata.R
> If you try just
>    source("dumpdata.R")
>    image(huh)
>    contour(huh)
> the 100 x 100 matrix 'huh' contains an hourglass-shaped region
> of values around 0.8. The contour plot shows two contour lines at
> the level 0.8 which cross each other in a figure 8.
> 
> In the short term can anyone suggest how to fix the contour plot
> for this particular dataset??

A close look suggests that the lines don't actually cross -- e.g. try

	contour(huh, xlim = c(0.82, 0.88), ylim = c(0.1, 0.12))

The handling of the "crossing case" in R is an implementation of that in 
Cleveland's "Visualizing Data" (one of the "for-the-record" sections). 
I don't think the algorithm permits crossings.

-- 
Ross Ihaka                         Email:  ihaka at r-project.org
The R Project and R Foundation



From jmurray at liverpool.ac.uk  Wed Aug  6 10:16:48 2003
From: jmurray at liverpool.ac.uk (jane murray)
Date: Wed, 06 Aug 2003 09:16:48 +0100
Subject: [R] leverage
Message-ID: <1157804.1060161408@137045-48592r.liv.ac.uk>

Hi
Can anyone help with the technique of obtaining leverages from a 
conditional logistic regression model?  The code lm.influence does not seem 
to work for this data.
Thanks
Jane Murray



From busscher at wiz.uni-kassel.de  Wed Aug  6 10:21:52 2003
From: busscher at wiz.uni-kassel.de (Nicolaas Busscher)
Date: Wed, 6 Aug 2003 10:21:52 +0200
Subject: [R] problems with lda
Message-ID: <20030806102152.4085ad79.busscher@wiz.uni-kassel.de>

enclosed a simple R script (and a data file) , with calls lda similar
to the example with the iris data in the documentation. it is not
working and i dont understand the error message. can anybody help me?
i am using R 1.5.1 (2002.06.17) on debian woody stable. thanks

-- 
Dr.Nicolaas Busscher Universit?t GH Kassel
Nordbahnhofstrasse: 1a, D-37213 Witzenhausen
Phone: 0049-(0)5542-98-1715, Fax: 0049-(0)5542-98-1713




From fharrell at virginia.edu  Wed Aug  6 10:23:34 2003
From: fharrell at virginia.edu (Frank E Harrell Jr)
Date: Wed, 06 Aug 2003 08:23:34 -0000
Subject: [R] Hmisc on CRAN
Message-ID: <20030718072911.0a9d38af.fharrell@virginia.edu>

I am pleased to announce that a new version of the Hmisc package, 2.0-0, is on CRAN.  Thanks to Kurt Hornik and Uwe Ligges and others for making this possible.  Thanks also to Xiao Gang Fan who has generously ported Hmisc and Design to Windows multiple times until now.  The Design package will soon be on CRAN too.

---
Frank E Harrell Jr              Prof. of Biostatistics & Statistics
Div. of Biostatistics & Epidem. Dept. of Health Evaluation Sciences
U. Virginia School of Medicine  http://hesweb1.med.virginia.edu/biostat

_______________________________________________
R-announce at stat.math.ethz.ch mailing list
https://www.stat.math.ethz.ch/mailman/listinfo/r-announce



From Roger.Bivand at nhh.no  Wed Aug  6 10:34:49 2003
From: Roger.Bivand at nhh.no (Roger Bivand)
Date: Wed, 6 Aug 2003 10:34:49 +0200 (CEST)
Subject: [R] contour lines intersect
In-Reply-To: <3F30B5A4.300@r-project.org>
Message-ID: <Pine.LNX.4.44.0308061015160.573-100000@reclus.nhh.no>

On Wed, 6 Aug 2003, Ross Ihaka wrote:

> Adrian Baddeley wrote:
> > Hi,
> > 
> > Sorry if this is already known...
> > 
> > contour() sometimes draws contour lines that intersect.
> > Is there a temporary fix?
> > 
> > A dataset which causes problems is at
> >    http://www.maths.uwa.edu.au/~adrian/dumpdata.R
> > If you try just
> >    source("dumpdata.R")
> >    image(huh)
> >    contour(huh)
> > the 100 x 100 matrix 'huh' contains an hourglass-shaped region
> > of values around 0.8. The contour plot shows two contour lines at
> > the level 0.8 which cross each other in a figure 8.
> > 
> > In the short term can anyone suggest how to fix the contour plot
> > for this particular dataset??
> 
> A close look suggests that the lines don't actually cross -- e.g. try
> 
> 	contour(huh, xlim = c(0.82, 0.88), ylim = c(0.1, 0.12))
> 
Or even:

contour(huh, levels=seq(0.75, 0.85, 0.001), xlim = c(0.82, 0.88), 
  ylim = c(0.1, 0.12))

 - the data are very flat. It does look odd zoomed out, though - maybe 
avoid the 0.8 contour by setting levels to something other than the 
default? This looks OK:

image(huh, breaks = round(quantile(huh, prob = seq(0,1,0.1)), 2), 
  col = heat.colors(10))
contour(huh, levels = round(quantile(huh, prob = seq(0,1,0.1)), 
  digits = 2),  drawlabels=FALSE, add=TRUE)

but obviously there are good reasons for the pretty() default levels.

filled.contour(huh, color.palette = rainbow, xlim = c(0.82, 0.88),
  ylim = c(0.1, 0.12))

shows the same effect. 

> The handling of the "crossing case" in R is an implementation of that in 
> Cleveland's "Visualizing Data" (one of the "for-the-record" sections). 
> I don't think the algorithm permits crossings.
> 
> 

-- 
Roger Bivand
Economic Geography Section, Department of Economics, Norwegian School of
Economics and Business Administration, Breiviksveien 40, N-5045 Bergen,
Norway. voice: +47 55 95 93 55; fax +47 55 95 93 93
e-mail: Roger.Bivand at nhh.no



From ripley at stats.ox.ac.uk  Wed Aug  6 11:10:44 2003
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Wed, 6 Aug 2003 10:10:44 +0100 (BST)
Subject: [R] problems with lda
In-Reply-To: <20030806102152.4085ad79.busscher@wiz.uni-kassel.de>
Message-ID: <Pine.LNX.4.44.0308060959530.15752-100000@gannet.stats>

On Wed, 6 Aug 2003, Nicolaas Busscher wrote:

> enclosed a simple R script (and a data file) , with calls lda similar
> to the example with the iris data in the documentation. it is not
> working and i dont understand the error message. can anybody help me?

Nothing arrived attached: please include the problem in your message.

> i am using R 1.5.1 (2002.06.17) on debian woody stable. thanks

Oh, please, upgrade your R!  We can't help about systems that are
six releases back.

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From hennig at stat.math.ethz.ch  Wed Aug  6 11:40:52 2003
From: hennig at stat.math.ethz.ch (Christian Hennig)
Date: Wed, 6 Aug 2003 11:40:52 +0200 (CEST)
Subject: [R] Error on mclust
In-Reply-To: <BAY1-F106dtDlVNwvuW0000951b@hotmail.com>
Message-ID: <Pine.LNX.4.44.0308061131440.1929-100000@florence>

Hi,

this goes also to the developer and maintainer of mclust.

Here are some reproducible results:

> set.seed(300)
> x <- c(rnorm(50),rnorm(56,-0.5)) 
> Mclust(x)
Error in rep(1, n) : Object "n" not found

> set.seed(3000)
> y <- c(rnorm(50),rnorm(56,-0.5)) 
> Mclust(y)

 best model: unequal variance with 3 groups

 averge/median classification uncertainty: 0.002 / 0 

Now what's the difference between x and y?
function EMclust works for both, but not summary.EMclust:

> emx <- EMclust(x)
> emx

 BIC:
          E         V
1 -304.8738 -304.8738
2 -314.2134        NA
3 -323.5445        NA
4 -332.8736        NA
5 -342.2043        NA
6 -351.5322        NA
7 -360.8612        NA
8 -370.1896        NA
9 -379.5153        NA

> help(summary.EMclust)
> summary(emx,x)
Error in rep(1, n) : Object "n" not found
> emy <- EMclust(y)
> summary(emy,y)

classification table:

  1   2   3 
100   4   2 

uncertainty (quartiles):
        0%        25%        50%        75%       100% 
0.00000000 0.00000000 0.00000000 0.00000000 0.06683614 

best BIC values:
      V,3       V,2       E,1 
-293.2526 -302.0302 -304.2835 

best model: unequal variance 

> emy

 BIC:
          E         V
1 -304.2835 -304.2835
2 -313.6275 -302.0302
3 -322.9562 -293.2526
4 -332.2631        NA
5 -341.5896        NA
6 -350.8819        NA
7 -352.6121        NA
8 -361.9393        NA
9 -371.2658        NA

Here is the explanation:

For x, one class G=1 is chosen as optimal.
The line rep(1,n) appears in summary.EMclust (which is called by Mclust,
but not by EMclust), but it is called only if G=1 is estimated. 
Indeed, n is not defined (as far as I can see), which is a bug. 

It's possible to work around that bug:
For concrete data analysis, if the error occurs, 
you may assume (and check by application of
EMclust), that G=1 is estimated, which means that you can fit a single
normal distribution, for which you do not need Mclust. 
Otherwise you should get proper results.

Best,
Christian

On Tue, 5 Aug 2003, weidong zhang wrote:

> Hi All,
> 
> 
> I am trying to cluster a one-dimensional data (see the file attached) using 
> Mclust() but got an error message like:
> >Mclust(x)
> Error in rep(1, n) : Object "n" not found
> 
> When I do a simulation sometimes it works sometimes doesn't.
> 
> >Mclust(c(rnorm(50),rnorm(56,-0.5)))
> Error in rep(1, n) : Object "n" not found
> 
> >Mclust(c(rnorm(56),rnorm(56,-0.5)))
> 
> best model: unequal variance with 2 groups
> 
> averge/median classification uncertainty: 0.001 / 0
> 
> Can anybody help me with this? Thanks.
> 
> Weidong
> 
> _________________________________________________________________
> 
> 
> 

-- 
***********************************************************************
Christian Hennig
Seminar fuer Statistik, ETH-Zentrum (LEO), CH-8092 Zuerich (currently)
and Fachbereich Mathematik-SPST/ZMS, Universitaet Hamburg
hennig at stat.math.ethz.ch, http://stat.ethz.ch/~hennig/
hennig at math.uni-hamburg.de, http://www.math.uni-hamburg.de/home/hennig/
#######################################################################
ich empfehle www.boag.de



From Jan.Verbesselt at agr.kuleuven.ac.be  Wed Aug  6 11:46:57 2003
From: Jan.Verbesselt at agr.kuleuven.ac.be (Jan Verbesselt)
Date: Wed, 6 Aug 2003 11:46:57 +0200
Subject: [R] Filtering Time Series / apply STL method
Message-ID: <000a01c35bff$ad275510$1145210a@agr.ad10.intern.kuleuven.ac.be>

Dear R Helpers,

Which technique can I apply to get the noise out of the following graph?
The data displayed shows the seasonal variation of NDVI (vegetation
photosynthesis) extracted from the VEGETATION satellite sensor.  It
contains big errors caused by cloud cover, which gives very low values
in the time series (=> outliers have to be deleted).  Are there certain
smoothing or filtering techniques I could use? I tested the Holt-winters
filtering and the kernel smoothing technique but none of these give
sufficient results. 

Thanks,
Jan

Ps: Thanks for the previous answer: setting the dim(Timeserie) <- NULL
was the solution. The aim is to apply the STL function on these results
because at the moment STL runs but results are not logical.


*******************************************************
Jan Verbesselt 
Research Associate 
Lab of Geomatics and Forest Engineering K.U. Leuven
Vital Decosterstraat 102. B-3000 Leuven Belgium 
Tel:+32-16-329750 
Fax: +32-16-329760
http://perswww.kuleuven.ac.be/~u0027178/VCard/mycard.php?name=janv
*******************************************************


From ripley at stats.ox.ac.uk  Wed Aug  6 11:54:36 2003
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Wed, 6 Aug 2003 10:54:36 +0100 (BST)
Subject: [R] Filtering Time Series / apply STL method
In-Reply-To: <000a01c35bff$ad275510$1145210a@agr.ad10.intern.kuleuven.ac.be>
Message-ID: <Pine.LNX.4.44.0308061054001.15856-100000@gannet.stats>

No graph was shown: R-help strips most attachments.

On Wed, 6 Aug 2003, Jan Verbesselt wrote:

> Dear R Helpers,
> 
> Which technique can I apply to get the noise out of the following graph?
> The data displayed shows the seasonal variation of NDVI (vegetation
> photosynthesis) extracted from the VEGETATION satellite sensor.  It
> contains big errors caused by cloud cover, which gives very low values
> in the time series (=> outliers have to be deleted).  Are there certain
> smoothing or filtering techniques I could use? I tested the Holt-winters
> filtering and the kernel smoothing technique but none of these give
> sufficient results. 
> 
> Thanks,
> Jan
> 
> Ps: Thanks for the previous answer: setting the dim(Timeserie) <- NULL
> was the solution. The aim is to apply the STL function on these results
> because at the moment STL runs but results are not logical.
> 
> 
> *******************************************************
> Jan Verbesselt 
> Research Associate 
> Lab of Geomatics and Forest Engineering K.U. Leuven
> Vital Decosterstraat 102. B-3000 Leuven Belgium 
> Tel:+32-16-329750 
> Fax: +32-16-329760
> http://perswww.kuleuven.ac.be/~u0027178/VCard/mycard.php?name=janv
> *******************************************************
> 
> 

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From busscher at wiz.uni-kassel.de  Wed Aug  6 12:34:14 2003
From: busscher at wiz.uni-kassel.de (Nicolaas Busscher)
Date: Wed, 6 Aug 2003 12:34:14 +0200
Subject: [R] problems with lda , data included,
	can somebody test with the new version
Message-ID: <20030806123414.184bdadf.busscher@wiz.uni-kassel.de>

enclosed a simple R script (and a data file, and the output) , with
calls lda similar to the example with the iris data in the
documentation. it is not working and i dont understand the error
message. can anybody help me? i am using R 1.5.1 (2002.06.17) on
debian woody stable. 

I would like to avoid updating now, because i want to keep the system
in "stable". can somebody test, if the newer version didnt have this
problem ? thanks

------ data ---------------
"diagonal.moment" "cluster.shade" "histogram.kurtosis" "assignment"
"1" 91.966025 -11402.735755 -0.113658 "5"
"2" 87.216671 -13892.812239 -0.106662 "5"
"3" 94.23765 -12397.68244 -0.110086 "5"
"4" 120.130753 -17229.165583 -0.119237 "5"
"5" 84.952756 -13024.921078 -0.110119 "5"
"6" 97.635587 -11984.989519 -0.103907 "5"
"7" 60.416425 -8641.988229 -0.116222 "5"
"8" 84.79081 -12332.977083 -0.1091 "5"
"9" 78.408806 -11373.916988 -0.112666 "5"
"10" 137.415571 -18491.889203 -0.118939 "5"
"11" -10.156979 -1495.884639 -0.104795 "5"
"12" 92.383809 -11601.060884 -0.11422 "5"
"13" 147.584707 -19084.747951 -0.120416 "6"
"14" 146.609465 -21629.814571 -0.121656 "6"
"15" 143.91174 -19878.044959 -0.118049 "6"
"16" 177.170923 -26024.777287 -0.121526 "6"
"17" 134.00609 -18110.76336 -0.118185 "6"
"18" 138.793062 -18347.029573 -0.121952 "6"
"19" 125.658219 -16204.88335 -0.119324 "6"
"20" 99.369435 -14246.410482 -0.122442 "6"
"21" 154.763888 -21116.0578 -0.120623 "6"
"22" 127.359594 -17490.575516 -0.120079 "6"
"23" 136.568559 -18329.619348 -0.121503 "6"
"24" 127.983753 -16243.10434 -0.121176 "6"
-----
script:
library(MASS)
actaData<-read.table("actaData.dat")
actaData

ass<-actaData$assignment
print(ass)
tass<-table(ass)
print(tass)
#print(attributes(tass))
priodim<-attr(tass,"dim")
prio=c(rep(1,priodim))/priodim
print(prio)

actaData
z<-lda(assignment ~ .,actaData,prior=prio,tol=1.0e-08)
--- end script---
start output:

end output
> print(dim(actaData))
[1] 24  4
> 
> ass<-actaData$assignment
> print(ass)
 [1] 5 5 5 5 5 5 5 5 5 5 5 5 6 6 6 6 6 6 6 6 6 6 6 6
> tass<-table(ass)
> print(tass)
ass
 5  6 
12 12 
> #print(attributes(tass))
> priodim<-attr(tass,"dim")
> prio=c(rep(1,priodim))/priodim
> print(prio)
[1] 0.5 0.5
> 
> actaData
   diagonal.moment cluster.shade histogram.kurtosis assignment
1         91.96603    -11402.736          -0.113658          5
2         87.21667    -13892.812          -0.106662          5
3         94.23765    -12397.682          -0.110086          5
4        120.13075    -17229.166          -0.119237          5
5         84.95276    -13024.921          -0.110119          5
6         97.63559    -11984.990          -0.103907          5
7         60.41642     -8641.988          -0.116222          5
8         84.79081    -12332.977          -0.109100          5
9         78.40881    -11373.917          -0.112666          5
10       137.41557    -18491.889          -0.118939          5
11       -10.15698     -1495.885          -0.104795          5
12        92.38381    -11601.061          -0.114220          5
13       147.58471    -19084.748          -0.120416          6
14       146.60947    -21629.815          -0.121656          6
15       143.91174    -19878.045          -0.118049          6
16       177.17092    -26024.777          -0.121526          6
17       134.00609    -18110.763          -0.118185          6
18       138.79306    -18347.030          -0.121952          6
19       125.65822    -16204.883          -0.119324          6
20        99.36943    -14246.410          -0.122442          6
21       154.76389    -21116.058          -0.120623          6
22       127.35959    -17490.576          -0.120079          6
23       136.56856    -18329.619          -0.121503          6
24       127.98375    -16243.104          -0.121176          6
> 
> z<-lda(assignment ~ .,actaData,prior=prio,tol=1.0e-08)
Error in scale(group.means, center = xbar, scale = FALSE) : 
	unused argument(s) (center ...)
Execution halted

#-----------------------------------------
### output from R --version
R 1.5.1 (2002-06-17).
Copyright (C) 2002 R Development Core Team

R is free software and comes with ABSOLUTELY NO WARRANTY.
You are welcome to redistribute it under the terms of the GNU
General Public License.  For more information about these matters,
see http://www.gnu.org/copyleft/gpl.html.

-------------------

Dr.Nicolaas Busscher Universit?t GH Kassel
Nordbahnhofstrasse: 1a, D-37213 Witzenhausen
Phone: 0049-(0)5542-98-1715, Fax: 0049-(0)5542-98-1713



From nolwenn.lemeur at nantes.inserm.fr  Wed Aug  6 12:44:36 2003
From: nolwenn.lemeur at nantes.inserm.fr (Nolwenn Le Meur)
Date: Wed, 6 Aug 2003 12:44:36 +0200
Subject: [R] (no subject)
Message-ID: <LMEBLNBEKKODLAONNGJMMELGCBAA.nolwenn.lemeur@nantes.inserm.fr>

Hi everybody,

Hope your are not all on holyday because I've got a problem that is going to
drive me crazy...

I would like to remove some rows from a dataframe. The rows correspond to
some
specific indexes which I can get by looking at the name in the first column
of my dataset. But I manage to get only the opposite of what I really want
(function #1)

#Function#1:
remove.func<-function(data){
Name<-as.character(data[,1])
indexZZ<-grep("ZZ",Name,value=FALSE)
data<-data[indexZZ,] # give me what I don't want
print(slide)
}

#Function#2:
remove.func<-function(data,Name){
Name<-as.character(data[,1])
indexZZ<-grep("ZZ",Name,value=FALSE)
data<-data[!indexZZ,] #doesn't work, give an empty dataframe
print(slide)
}

Thanks :|

Nolwenn Le Meur

********************************************
Nolwenn Le Meur
INSERM U533
Facult? de m?decine
1, rue Gaston Veil
44035 Nantes Cedex 1
France

Tel: (+33)-2-40-41-29-86 (office)
     (+33)-2-40-41-28-44 (secretary)
Fax: (+33)-2-40-41-29-50
mail: nolwenn.lemeur at nantes.inserm.fr



From kwan022 at stat.auckland.ac.nz  Wed Aug  6 12:50:42 2003
From: kwan022 at stat.auckland.ac.nz (Ko-Kang Kevin Wang)
Date: Wed, 6 Aug 2003 22:50:42 +1200 (NZST)
Subject: [R] problems with lda , data included, can somebody test with
	the new version
In-Reply-To: <20030806123414.184bdadf.busscher@wiz.uni-kassel.de>
Message-ID: <Pine.LNX.4.44.0308062246390.23562-100000@stat55.stat.auckland.ac.nz>

On Wed, 6 Aug 2003, Nicolaas Busscher wrote:

> enclosed a simple R script (and a data file, and the output) , with
> calls lda similar to the example with the iris data in the
> documentation. it is not working and i dont understand the error
> message. can anybody help me? i am using R 1.5.1 (2002.06.17) on
> debian woody stable. 
> 
> I would like to avoid updating now, because i want to keep the system
> in "stable". can somebody test, if the newer version didnt have this
> problem ? thanks
>
> z<-lda(assignment ~ .,actaData,prior=prio,tol=1.0e-08)

Using the latest version, R 1.7.1, I get:
> z
Call:
lda.formula(assignment ~ ., data = actaData, prior = prio, tol = 1e-08)

Prior probabilities of groups:
  5   6 
0.5 0.5 

Group means:
  diagonal.moment cluster.shade histogram.kurtosis
5        84.94982     -11989.17         -0.1116342
6       138.31495     -18892.15         -0.1205776

Coefficients of linear discriminants:
                             LD1
diagonal.moment    -1.575854e-03
cluster.shade      -1.209398e-04
histogram.kurtosis -2.028278e+02


-- 
Cheers,

Kevin

------------------------------------------------------------------------------
"On two occasions, I have been asked [by members of Parliament],
'Pray, Mr. Babbage, if you put into the machine wrong figures, will
the right answers come out?' I am not able to rightly apprehend the
kind of confusion of ideas that could provoke such a question."

-- Charles Babbage (1791-1871) 
---- From Computer Stupidities: http://rinkworks.com/stupid/

--
Ko-Kang Kevin Wang
Master of Science (MSc) Student
SLC Tutor and Lab Demonstrator
Department of Statistics
University of Auckland
New Zealand
Homepage: http://www.stat.auckland.ac.nz/~kwan022
Ph: 373-7599
    x88475 (City)
    x88480 (Tamaki)



From gerhard.prade at web.de  Wed Aug  6 13:02:17 2003
From: gerhard.prade at web.de (Gerhard Prade)
Date: Wed, 06 Aug 2003 13:02:17 +0200
Subject: [R] R like SPSS
Message-ID: <3F30E039.1080406@web.de>

Hello all,

(Sorry, my bad english)
i am searching a alternativ for spss that i can use with linux. I 
studied sociology in germany and work sometimes for some hours in the 
marketresearch. There they use statistics like anova and crosstabs. I 
see that r can make anova or regressions but the most work in the 
marketresearch is making crosstabs. These companys use something like 
spss tables or gess to make tables for the customers. I read that r can 
make with latex tables with xtabs or ftable and so on. But i dont 
understand the use of all that. My question is: Is R a good alternativ 
to make tables?
The goal is to make something like that:

_____________________________________________________________________________________________________

Question 1.1
What is your Age?
-------------------------------------------------------------------------------------------------------------------------
                                            PC-Users                    
        Linux User                    Windows User
                                        yes               |     no    
     ||   yes            no                    yes            no
----------------------------------------------------------------------------------------------------------
0-10 years                       4    2%         |     1 1%     ||     
xx          and so on
                                                            |           
       ||
11-20 years                     8    4%         |     2    2%  ||   
 xx            and so on
                                                            |           
       ||
21-30 years                    8    4%          |                  ||   
 xx  
                                                            |           
       ||   
31 years an older            180    90%    |                   ||    xx
                                                            |           
       ||
not answerd                    0        0%      |                   ||
-----------------------------------------------------------------------------------------------------------

                                       200   100%        100 100%
--------------------------------------------------------------------------------------------------------------------------
Company XY                                                            
    Customer XY
_____________________________________________________________________________________Page 
7 of 10



I can do that with spss or gess, but can i do that with R?
Is it looks good with R?


Greatings, Gerhard



From ripley at stats.ox.ac.uk  Wed Aug  6 13:01:28 2003
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Wed, 6 Aug 2003 12:01:28 +0100 (BST)
Subject: [R] problems with lda , data included, can somebody test with
	the new version
In-Reply-To: <20030806123414.184bdadf.busscher@wiz.uni-kassel.de>
Message-ID: <Pine.LNX.4.44.0308061150290.15997-100000@gannet.stats>

On Wed, 6 Aug 2003, Nicolaas Busscher wrote:

> enclosed a simple R script (and a data file, and the output) , with
> calls lda similar to the example with the iris data in the
> documentation. it is not working and i dont understand the error
> message. can anybody help me? i am using R 1.5.1 (2002.06.17) on
> debian woody stable. 
> 
> I would like to avoid updating now, because i want to keep the system
> in "stable". can somebody test, if the newer version didnt have this
> problem ? thanks

Notice the error is from scale(), and that has not been changed for 18
months.  Your session appears to have a corrupt version of scale, and
perhaps you ought to use a clean session and R --vanilla to help you find
your problem.


> > z<-lda(assignment ~ .,actaData,prior=prio,tol=1.0e-08)
> Error in scale(group.means, center = xbar, scale = FALSE) : 
> 	unused argument(s) (center ...)
> Execution halted

BTW, your code does work in current R.


-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From ripley at stats.ox.ac.uk  Wed Aug  6 13:03:47 2003
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Wed, 6 Aug 2003 12:03:47 +0100 (BST)
Subject: [R] (no subject)
In-Reply-To: <LMEBLNBEKKODLAONNGJMMELGCBAA.nolwenn.lemeur@nantes.inserm.fr>
Message-ID: <Pine.LNX.4.44.0308061202080.15997-100000@gannet.stats>

data[-indexZZ,] as grep returns indices of the matching rows.

On Wed, 6 Aug 2003, Nolwenn Le Meur wrote:

> Hi everybody,
> 
> Hope your are not all on holyday because I've got a problem that is going to
> drive me crazy...

Reading the documentation would avoid that!

> I would like to remove some rows from a dataframe. The rows correspond to
> some
> specific indexes which I can get by looking at the name in the first column
> of my dataset. But I manage to get only the opposite of what I really want
> (function #1)
> 
> #Function#1:
> remove.func<-function(data){
> Name<-as.character(data[,1])
> indexZZ<-grep("ZZ",Name,value=FALSE)
> data<-data[indexZZ,] # give me what I don't want
> print(slide)
> }
> 
> #Function#2:
> remove.func<-function(data,Name){
> Name<-as.character(data[,1])
> indexZZ<-grep("ZZ",Name,value=FALSE)
> data<-data[!indexZZ,] #doesn't work, give an empty dataframe
> print(slide)
> }
> 
> Thanks :|
> 
> Nolwenn Le Meur
> 
> ********************************************
> Nolwenn Le Meur
> INSERM U533
> Facult? de m?decine
> 1, rue Gaston Veil
> 44035 Nantes Cedex 1
> France
> 
> Tel: (+33)-2-40-41-29-86 (office)
>      (+33)-2-40-41-28-44 (secretary)
> Fax: (+33)-2-40-41-29-50
> mail: nolwenn.lemeur at nantes.inserm.fr
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> 
> 

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From glaziou at pasteur-kh.org  Wed Aug  6 13:06:52 2003
From: glaziou at pasteur-kh.org (Philippe Glaziou)
Date: Wed, 6 Aug 2003 18:06:52 +0700
Subject: [R] problems with lda , data included,
	can somebody test with the new version
In-Reply-To: <20030806123414.184bdadf.busscher@wiz.uni-kassel.de>
References: <20030806123414.184bdadf.busscher@wiz.uni-kassel.de>
Message-ID: <20030806110652.GA2902@pasteur-kh.org>

Nicolaas Busscher <busscher at wiz.uni-kassel.de> wrote:
> enclosed a simple R script (and a data file, and the
> output) , with calls lda similar to the example with the
> iris data in the documentation. it is not working and i
> dont understand the error message.


I tested your data and commands without any error message on
a linux debian distribution:

> [...]
> z<-lda(assignment ~ .,actaData,prior=prio,tol=1.0e-08)
> z
Call:
lda.formula(assignment ~ ., data = actaData, prior = prio,
tol = 1e-08)

Prior probabilities of groups:
  5   6
0.5 0.5

Group means:
  diagonal.moment cluster.shade histogram.kurtosis
5        84.94982     -11989.17         -0.1116342
6       138.31495     -18892.15         -0.1205776

Coefficients of linear discriminants:
                             LD1
diagonal.moment    -1.575854e-03
cluster.shade      -1.209398e-04
histogram.kurtosis -2.028278e+02




> can anybody help me? i am using R 1.5.1 (2002.06.17) on
> debian woody stable.  I would like to avoid updating now,
> because i want to keep the system in "stable".

You can upgrade your old R while keeping the system in
stable. You may download the 1.7.1 deb packages from CRAN,
and install them with dpkg.

Or even better, you can create your own debs from the
sources, all the required files are the package.

  tar xvzf R-1.7.1.tgz
  cd R-1.7.1
  dpkg-buildpackage -rfakeroot

And a few coffees later, you will find a fairly large number
of deb files in the parent directory. 

-- 
Philippe



From ligges at statistik.uni-dortmund.de  Wed Aug  6 13:07:41 2003
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Wed, 06 Aug 2003 13:07:41 +0200
Subject: [R] (no subject)
In-Reply-To: <LMEBLNBEKKODLAONNGJMMELGCBAA.nolwenn.lemeur@nantes.inserm.fr>
References: <LMEBLNBEKKODLAONNGJMMELGCBAA.nolwenn.lemeur@nantes.inserm.fr>
Message-ID: <3F30E17D.2040700@statistik.uni-dortmund.de>

Nolwenn Le Meur wrote:

> Hi everybody,
> 
> Hope your are not all on holyday because I've got a problem that is going to
> drive me crazy...

No comment on that sentence, but please use the subject line ....

> I would like to remove some rows from a dataframe. The rows correspond to
> some
> specific indexes which I can get by looking at the name in the first column
> of my dataset. But I manage to get only the opposite of what I really want
> (function #1)
> 
> #Function#1:
> remove.func<-function(data){
> Name<-as.character(data[,1])
> indexZZ<-grep("ZZ",Name,value=FALSE)
> data<-data[indexZZ,] # give me what I don't want
> print(slide)
> }
> 
> #Function#2:
> remove.func<-function(data,Name){
> Name<-as.character(data[,1])
> indexZZ<-grep("ZZ",Name,value=FALSE)
> data<-data[!indexZZ,] #doesn't work, give an empty dataframe

Please read the manuals and learn how to index:

  data <- data[-indexZZ,]

Uwe Ligges


> print(slide)
> }
> 
> Thanks :|
> 
> Nolwenn Le Meur
> 
> ********************************************
> Nolwenn Le Meur
> INSERM U533
> Facult? de m?decine
> 1, rue Gaston Veil
> 44035 Nantes Cedex 1
> France
> 
> Tel: (+33)-2-40-41-29-86 (office)
>      (+33)-2-40-41-28-44 (secretary)
> Fax: (+33)-2-40-41-29-50
> mail: nolwenn.lemeur at nantes.inserm.fr
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help



From ligges at statistik.uni-dortmund.de  Wed Aug  6 13:10:28 2003
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Wed, 06 Aug 2003 13:10:28 +0200
Subject: [R] R like SPSS
In-Reply-To: <3F30E039.1080406@web.de>
References: <3F30E039.1080406@web.de>
Message-ID: <3F30E224.1070604@statistik.uni-dortmund.de>

Gerhard Prade wrote:

> Hello all,
> 
> (Sorry, my bad english)
> i am searching a alternativ for spss that i can use with linux. I 
> studied sociology in germany and work sometimes for some hours in the 
> marketresearch. There they use statistics like anova and crosstabs. I 
> see that r can make anova or regressions but the most work in the 
> marketresearch is making crosstabs. These companys use something like 
> spss tables or gess to make tables for the customers. I read that r can 
> make with latex tables with xtabs or ftable and so on. But i dont 
> understand the use of all that. My question is: Is R a good alternativ 
> to make tables?
> The goal is to make something like that:
> 
> _____________________________________________________________________________________________________ 
> 
> 
> Question 1.1
> What is your Age?
> ------------------------------------------------------------------------------------------------------------------------- 
> 
>                                            PC-Users                    
>        Linux User                    Windows User
>                                        yes               |     no        
> ||   yes            no                    yes            no
> ---------------------------------------------------------------------------------------------------------- 
> 
> 0-10 years                       4    2%         |     1 1%     ||     
> xx          and so on
>                                                            |           
>       ||
> 11-20 years                     8    4%         |     2    2%  ||   
> xx            and so on
>                                                            |           
>       ||
> 21-30 years                    8    4%          |                  ||   
> xx                                                             
> |                 ||   31 years an older            180    90%    
> |                   ||    xx
>                                                            |           
>       ||
> not answerd                    0        0%      |                   ||
> ----------------------------------------------------------------------------------------------------------- 
> 
> 
>                                       200   100%        100 100%
> -------------------------------------------------------------------------------------------------------------------------- 
> 
> Company XY                                                               
> Customer XY
> _____________________________________________________________________________________Page 
> 7 of 10
> 
> 
> 
> I can do that with spss or gess, but can i do that with R?
> Is it looks good with R?
> 
> 

Yes, you can do it in R and you can produce rather nice output for, 
e.g., LaTeX or HTML. So start reading the manuals and just do your work 
with R.

Uwe Ligges



From Roger.Bivand at nhh.no  Wed Aug  6 13:22:34 2003
From: Roger.Bivand at nhh.no (Roger Bivand)
Date: Wed, 6 Aug 2003 13:22:34 +0200 (CEST)
Subject: [R] R like SPSS
In-Reply-To: <3F30E039.1080406@web.de>
Message-ID: <Pine.LNX.4.44.0308061317470.573-100000@reclus.nhh.no>

On Wed, 6 Aug 2003, Gerhard Prade wrote:

> Hello all,
> 
> (Sorry, my bad english)
> i am searching a alternativ for spss that i can use with linux. I 
> studied sociology in germany and work sometimes for some hours in the 
> marketresearch. There they use statistics like anova and crosstabs. I 
> see that r can make anova or regressions but the most work in the 
> marketresearch is making crosstabs. These companys use something like 
> spss tables or gess to make tables for the customers. I read that r can 
> make with latex tables with xtabs or ftable and so on. But i dont 
> understand the use of all that. My question is: Is R a good alternativ 
> to make tables?

You will find the documentation fairly good. In particular, see Peter 
Dalgaard "Introductory Statistics with R", pp. 74-75. prop.table() gives 
the proportions of row or column totals. What "looks good" is subjective, 
and views vary on whether "looking good" is the same as being readable.

> The goal is to make something like that:
> 
> _____________________________________________________________________________________________________
> 
> Question 1.1
> What is your Age?
> -------------------------------------------------------------------------------------------------------------------------
>                                             PC-Users                    
>         Linux User                    Windows User
>                                         yes               |     no    
>      ||   yes            no                    yes            no
> ----------------------------------------------------------------------------------------------------------
> 0-10 years                       4    2%         |     1 1%     ||     
> xx          and so on
>                                                             |           
>        ||
> 11-20 years                     8    4%         |     2    2%  ||   
>  xx            and so on
>                                                             |           
>        ||
> 21-30 years                    8    4%          |                  ||   
>  xx  
>                                                             |           
>        ||   
> 31 years an older            180    90%    |                   ||    xx
>                                                             |           
>        ||
> not answerd                    0        0%      |                   ||
> -----------------------------------------------------------------------------------------------------------
> 
>                                        200   100%        100 100%
> --------------------------------------------------------------------------------------------------------------------------
> Company XY                                                            
>     Customer XY
> _____________________________________________________________________________________Page 
> 7 of 10
> 
> 
> 
> I can do that with spss or gess, but can i do that with R?
> Is it looks good with R?
> 
> 
> Greatings, Gerhard
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> 

-- 
Roger Bivand
Economic Geography Section, Department of Economics, Norwegian School of
Economics and Business Administration, Breiviksveien 40, N-5045 Bergen,
Norway. voice: +47 55 95 93 55; fax +47 55 95 93 93
e-mail: Roger.Bivand at nhh.no



From baron at psych.upenn.edu  Wed Aug  6 13:36:03 2003
From: baron at psych.upenn.edu (Jonathan Baron)
Date: Wed, 6 Aug 2003 07:36:03 -0400
Subject: [R] R like SPSS
In-Reply-To: <3F30E039.1080406@web.de>
References: <3F30E039.1080406@web.de>
Message-ID: <20030806113603.GB4111@mail1.sas.upenn.edu>

On 08/06/03 13:02, Gerhard Prade wrote:
>Hello all,
>
>(Sorry, my bad english)
>i am searching a alternativ for spss that i can use with linux. I 
>studied sociology in germany and work sometimes for some hours in the 
>marketresearch. There they use statistics like anova and crosstabs. I 
>see that r can make anova or regressions but the most work in the 
>marketresearch is making crosstabs. These companys use something like 
>spss tables or gess to make tables for the customers. I read that r can 
>make with latex tables with xtabs or ftable and so on. But i dont 
>understand the use of all that. My question is: Is R a good alternativ 
>to make tables?
>The goal is to make something like that:

What you sent looks like a total mess on my terminal window.
But, yes, R can make tables of all sorts.  Usually xtab is quite
good, but there are other table making functions, such as those
in the Hmisc and gregmisc packages.

Almost 3 years ago I was keeping Windows on my computer because I
was using Systat - similar to SPSS - for data analysis.  Then I
discovered R and soon got rid of Windows forever.  The beginning
section of "Notes on R for psychology...", in my page below (and
which we plan to revise soon) is written for people who are
making the particular sort of transition I made.

Jon
-- 
Jonathan Baron, Professor of Psychology, University of Pennsylvania
Home page:            http://www.sas.upenn.edu/~baron
R page:               http://finzi.psych.upenn.edu/



From huan.huang at bnpparibas.com  Wed Aug  6 13:39:29 2003
From: huan.huang at bnpparibas.com (huan.huang@bnpparibas.com)
Date: Wed, 6 Aug 2003 12:39:29 +0100
Subject: [R] Standard error of standard deviation: bootstrap or theoretical
	results?
Message-ID: <OFFC7C09C1.187BDBCB-ON80256D7A.00400A3F@bnpparibas.com>


Dear R users,

This is more a statistical question rather than an R question. I'd
appreciate it if you can give me some suggestions.

I have a sample of a time series (sample size 500, fat tail in density). I
am trying to calculate the Standard error of standard deviation of a
sub-block-sample (sample size 250). I take 100 this kind of
sub-block-sample, randomly. For these 100 subsamples, I use the following 3
methods to calculate the standard error of standard deviation:

1. From book "Handbook of applicable mathematics", Walter Ledermann (chief
editor) Volumn VI: Statistics, Part A, Lloyd, John Wiley & Sons.

Page 30-32:

var(S) = (mu4 - mu2^2)/(4 * mu2 * n)
mu4 = E(X - mu)^4, mu2 = E(X - mu)^2, S^2 = sum(X - mu)^2/n

The results are about: 0.00090

2. From   http://davidmlane.com/hyperstat/A19196.html
The results are about 0.00066

3. From   http://mathworld.wolfram.com/StandardDeviationDistribution.html
The results are about 0.00065

Finally I calculate the standard deviation for each of the 100 subsamples
and the standard error of those 100 standard deviations ( I reckon this is
the bootstrap result for the standard error of the standard deviation I
want).
I get 0.00024

I tried all above a couple of times and got similar results for each
methods I used. The results from the first 3 methods are apparently higher
than the bootstrap one. I am a bit confused. Do I miss anything? Which one
do you believe?


Thanks a lot.

Huan





This message and any attachments (the "message") is\ intende...{{dropped}}



From tblackw at umich.edu  Wed Aug  6 15:04:13 2003
From: tblackw at umich.edu (Thomas W Blackwell)
Date: Wed, 6 Aug 2003 09:04:13 -0400 (EDT)
Subject: [R] Standard error of standard deviation: bootstrap or theoretical
	results?
In-Reply-To: <OFFC7C09C1.187BDBCB-ON80256D7A.00400A3F@bnpparibas.com>
Message-ID: <Pine.SOL.4.44.0308060900200.24214-100000@robotron.gpcc.itd.umich.edu>

Huan  -

The difference between the empirical ("bootstrap') result and the
theoretical results shows evidence for autocorrelation in the time
series data.

-  tom blackwell  -  u michigan medical school  -  ann arbor  -

On Wed, 6 Aug 2003 huan.huang at bnpparibas.com wrote:

> This is more a statistical question rather than an R question. I'd
> appreciate it if you can give me some suggestions.
>
> I have a sample of a time series (sample size 500, fat tail in density). I
> am trying to calculate the Standard error of standard deviation of a
> sub-block-sample (sample size 250). I take 100 this kind of
> sub-block-sample, randomly. For these 100 subsamples, I use the following 3
> methods to calculate the standard error of standard deviation:
>
> 1. From book "Handbook of applicable mathematics", Walter Ledermann (chief
> editor) Volumn VI: Statistics, Part A, Lloyd, John Wiley & Sons.
>
> Page 30-32:
>
> var(S) = (mu4 - mu2^2)/(4 * mu2 * n)
> mu4 = E(X - mu)^4, mu2 = E(X - mu)^2, S^2 = sum(X - mu)^2/n
>
> The results are about: 0.00090
>
> 2. From   http://davidmlane.com/hyperstat/A19196.html
> The results are about 0.00066
>
> 3. From   http://mathworld.wolfram.com/StandardDeviationDistribution.html
> The results are about 0.00065
>
> Finally I calculate the standard deviation for each of the 100 subsamples
> and the standard error of those 100 standard deviations ( I reckon this is
> the bootstrap result for the standard error of the standard deviation I
> want).
> I get 0.00024
>
> I tried all above a couple of times and got similar results for each
> methods I used. The results from the first 3 methods are apparently higher
> than the bootstrap one. I am a bit confused. Do I miss anything? Which one
> do you believe?
>
> Huan



From ripley at stats.ox.ac.uk  Wed Aug  6 16:03:43 2003
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Wed, 6 Aug 2003 15:03:43 +0100 (BST)
Subject: [R] Standard error of standard deviation: bootstrap or theoretical
	results?
In-Reply-To: <Pine.SOL.4.44.0308060900200.24214-100000@robotron.gpcc.itd.umich.edu>
Message-ID: <Pine.LNX.4.44.0308061435480.22903-100000@gannet.stats>

On Wed, 6 Aug 2003, Thomas W Blackwell wrote:

> Huan  -
> 
> The difference between the empirical ("bootstrap') result and the
> theoretical results shows evidence for autocorrelation in the time
> series data.

I don't think that's where the correlation is (and for positive 
autocorrelation I would expect 4 to be larger than 1 to 3).

> -  tom blackwell  -  u michigan medical school  -  ann arbor  -
> 
> On Wed, 6 Aug 2003 huan.huang at bnpparibas.com wrote:
> 
> > This is more a statistical question rather than an R question. I'd
> > appreciate it if you can give me some suggestions.

Suggestion: R-help is not a source of free statistical consultancy.
Please seek expert advice within your company, or employ a qualified 
consultant.  Which is why I am only pointing out the most obvious mistakes 
here, and not proffering possible solutions.

> > I have a sample of a time series (sample size 500, fat tail in density). I
> > am trying to calculate the Standard error of standard deviation of a
> > sub-block-sample (sample size 250). I take 100 this kind of
> > sub-block-sample, randomly. For these 100 subsamples, I use the following 3
> > methods to calculate the standard error of standard deviation:

But those 100 subsamples cannot be independent.  If you take a blocks of 
size 250 out of 500, they are almost bound to overlap.

> > 1. From book "Handbook of applicable mathematics", Walter Ledermann (chief
> > editor) Volumn VI: Statistics, Part A, Lloyd, John Wiley & Sons.
> >
> > Page 30-32:
> >
> > var(S) = (mu4 - mu2^2)/(4 * mu2 * n)
> > mu4 = E(X - mu)^4, mu2 = E(X - mu)^2, S^2 = sum(X - mu)^2/n
> >
> > The results are about: 0.00090

That is assuming independent samples.

> > 2. From   http://davidmlane.com/hyperstat/A19196.html
> > The results are about 0.00066

That's for iid *normal* samples.

> > 3. From   http://mathworld.wolfram.com/StandardDeviationDistribution.html
> > The results are about 0.00065

Same.

> > Finally I calculate the standard deviation for each of the 100 subsamples
> > and the standard error of those 100 standard deviations ( I reckon this is
> > the bootstrap result for the standard error of the standard deviation I
> > want).

That is not a bootstrap, at least not in any valid sense.  You can 
bootstrap time series, but it is tricky to find a valid method.

> > I get 0.00024
> >
> > I tried all above a couple of times and got similar results for each
> > methods I used. The results from the first 3 methods are apparently higher
> > than the bootstrap one. I am a bit confused. Do I miss anything? Which one
> > do you believe?

You seem to have missed the need to check your assumptions, none.

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From JonesW at kssg.com  Wed Aug  6 16:21:24 2003
From: JonesW at kssg.com (Wayne Jones)
Date: Wed, 6 Aug 2003 15:21:24 +0100 
Subject: [R] RODBC with Windows XP
Message-ID: <6B5A9304046AD411BD0200508BDFB6CB021F0E0C@gimli.middleearth.kssg.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20030806/81c0e701/attachment.pl

From tlumley at u.washington.edu  Wed Aug  6 16:44:20 2003
From: tlumley at u.washington.edu (Thomas Lumley)
Date: Wed, 6 Aug 2003 07:44:20 -0700 (PDT)
Subject: [R] leverage
In-Reply-To: <1157804.1060161408@137045-48592r.liv.ac.uk>
Message-ID: <Pine.A41.4.44.0308060738430.58026-100000@homer20.u.washington.edu>

On Wed, 6 Aug 2003, jane murray wrote:

> Hi
> Can anyone help with the technique of obtaining leverages from a
> conditional logistic regression model?  The code lm.influence does not seem
> to work for this data.

If you use method="approximate" you can get delta-betas (`influence'
rather than `leverage') with resid(model, "dfbeta")

For the exact conditional likelihood there isn't really a shortcut, but
if N is the number of observations:
  sapply(1:N, function(i) coef(update(model,subset=-i)))
is perfectly feasible.

	-thomas



From ripley at stats.ox.ac.uk  Wed Aug  6 17:08:06 2003
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Wed, 6 Aug 2003 16:08:06 +0100 (BST)
Subject: [R] RODBC with Windows XP
In-Reply-To: <6B5A9304046AD411BD0200508BDFB6CB021F0E0C@gimli.middleearth.kssg.com>
Message-ID: <Pine.LNX.4.44.0308061606380.23087-100000@gannet.stats>

This works perfectly on my XP machine.  I suspect your ODBC drivers: have 
you tried re-installing them?  (Especially if you upgraded to XP.)

On Wed, 6 Aug 2003, Wayne Jones wrote:

> 
> Hi R-Users, 
> 
> I have very recently switched from using R on windows 2000 to windows XP
> Professional. With XP I am having problems 
> using the "odbcConnect("")" command. I can use the command perfectly well
> until I close the database channel 
> with the command "odbcClose(channel)".  When I try and re-establish
> connection following the close command no dialog box appears 
> and an error message is displayed. 
> 
> I am using version 1.6.2 of R.

Rather old, please update.

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From dassybr at hotmail.com  Wed Aug  6 17:41:51 2003
From: dassybr at hotmail.com (Hadassa Brunschwig)
Date: Wed, 06 Aug 2003 17:41:51 +0200
Subject: [R] Importing Data
Message-ID: <BAY9-F551fDc5DYge2Q000108bf@hotmail.com>

Im trying to import data from an excel sheet or a sas file to R...im not 
succeeding. Apparently the function read.xport for reading a SAS file doesnt 
exist. What do i have to type in EXACTLY to read from an excel sheet(i guess 
i would be using read.table?)?

Thanks in advance for an answer

Dassy



From yao6889 at msmailhub.oulan.ou.edu  Wed Aug  6 17:47:56 2003
From: yao6889 at msmailhub.oulan.ou.edu (Yao, Minghua)
Date: Wed, 6 Aug 2003 10:47:56 -0500 
Subject: [R] How to copy and paste a R plot onto Word (or Power Point) 
Message-ID: <FC0CEBD77311DA499A67ADB355A24FA20396AD8F@mail4.oulan.ou.edu>


All,

Anybody can tell  how to export a R plot onto Word  (or Power Point)?
Many thanks in advance.

-MY



From busscher at wiz.uni-kassel.de  Wed Aug  6 18:04:56 2003
From: busscher at wiz.uni-kassel.de (Nicolaas Busscher)
Date: Wed, 6 Aug 2003 18:04:56 +0200
Subject: [R] think i found it, problems with lda ,
Message-ID: <20030806180456.5bea33e1.busscher@wiz.uni-kassel.de>

i think i found my problem,
when i define the function:
nb_select_scale<-function(blue,bla) {
  print(bla)
}
then the lda function results in an error in the scale function..
hm.. did i "overload" an existing function?

thanks for your FAST help"

Dr.Nicolaas Busscher Universit?t GH Kassel
Nordbahnhofstrasse: 1a, D-37213 Witzenhausen
Phone: 0049-(0)5542-98-1715, Fax: 0049-(0)5542-98-1713



From ripley at stats.ox.ac.uk  Wed Aug  6 18:12:05 2003
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Wed, 6 Aug 2003 17:12:05 +0100 (BST)
Subject: [R] think i found it, problems with lda ,
In-Reply-To: <20030806180456.5bea33e1.busscher@wiz.uni-kassel.de>
Message-ID: <Pine.LNX.4.44.0308061708420.23206-100000@gannet.stats>

Please do RTFM.  Also try ?conflicts.

_ was the assignment operator, and is not a valid part of a name.
In recent versions you would have got a helpful warning ....

On Wed, 6 Aug 2003, Nicolaas Busscher wrote:

> i think i found my problem,
> when i define the function:
> nb_select_scale<-function(blue,bla) {
>   print(bla)
> }
> then the lda function results in an error in the scale function..
> hm.. did i "overload" an existing function?

That means  nb <- select <- scale <- function(....

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From ligges at statistik.uni-dortmund.de  Wed Aug  6 18:15:59 2003
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Wed, 06 Aug 2003 18:15:59 +0200
Subject: [R] How to copy and paste a R plot onto Word (or Power Point)
In-Reply-To: <FC0CEBD77311DA499A67ADB355A24FA20396AD8F@mail4.oulan.ou.edu>
References: <FC0CEBD77311DA499A67ADB355A24FA20396AD8F@mail4.oulan.ou.edu>
Message-ID: <3F3129BF.1040102@statistik.uni-dortmund.de>

Yao, Minghua wrote:

> All,
> 
> Anybody can tell  how to export a R plot onto Word  (or Power Point)?
> Many thanks in advance.
> 
> -MY
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help

Since you are presumably on Windows (and haven't told it), you might 
want to use the win.metafile() device, for example.
See ?Devices and ?win.metafile mfor details.

You can also use the menu in an opened R graphics window to copy (e.g. 
as a metafile) to the clipboard and paste into the mentioned applications.

Uwe Ligges



From jmacdon at med.umich.edu  Wed Aug  6 18:16:18 2003
From: jmacdon at med.umich.edu (James MacDonald)
Date: Wed, 06 Aug 2003 12:16:18 -0400
Subject: [R] How to copy and paste a R plot onto Word (or Power
	Point)
Message-ID: <sf30f1ab.095@mail-02.med.umich.edu>

Either ?savePlot and import using Import --> Picture --> File in
Word/PowerPoint

or right click, copy as..., and paste in Word/Power point

Jim



James W. MacDonald
Affymetrix and cDNA Microarray Core
University of Michigan Cancer Center
1500 E. Medical Center Drive
7410 CCGC
Ann Arbor MI 48109
734-647-5623

>>> "Yao, Minghua" <yao6889 at msmailhub.oulan.ou.edu> 08/06/03 11:47AM
>>>

All,

Anybody can tell  how to export a R plot onto Word  (or Power Point)?
Many thanks in advance.

-MY

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://www.stat.math.ethz.ch/mailman/listinfo/r-help



From feldesmanm at pdx.edu  Wed Aug  6 18:17:12 2003
From: feldesmanm at pdx.edu (Marc Feldesman)
Date: Wed, 6 Aug 2003 09:17:12 -0700
Subject: [R] think i found it, problems with lda ,
In-Reply-To: <20030806180456.5bea33e1.busscher@wiz.uni-kassel.de>
References: <20030806180456.5bea33e1.busscher@wiz.uni-kassel.de>
Message-ID: <20030806091712.6b353cd2.feldesmanm@pdx.edu>

Your problem is in the use of the underscore character.  Replace it with
something else.  In your version of R, it is synonymous with the "<-"
operator.


On Wed, 6 Aug 2003 18:04:56 +0200 amazing electrons 
exploded from the fingers of
Nicolaas Busscher <busscher at wiz.uni-kassel.de>:

> i think i found my problem,
> when i define the function:
> nb_select_scale<-function(blue,bla) {
>   print(bla)
> }
> then the lda function results in an error in the scale function..
> hm.. did i "overload" an existing function?
> 
> thanks for your FAST help"
> 
> Dr.Nicolaas Busscher Universit?t GH Kassel
> Nordbahnhofstrasse: 1a, D-37213 Witzenhausen
> Phone: 0049-(0)5542-98-1715, Fax: 0049-(0)5542-98-1713
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help


-- 
Dr. Marc Feldesman
Professor and Chair Emeritus
Anthropology Department
Portland State University
email: feldesmanm at pdx.edu
web: http:/web.pdx.edu/~h1mf

"Beyond every credibility gap lies a gullibility fill"  Laurence Peter

Powered by Titanochoerus the G4



From TyagiAnupam at aol.com  Wed Aug  6 18:28:53 2003
From: TyagiAnupam at aol.com (TyagiAnupam@aol.com)
Date: Wed, 6 Aug 2003 12:28:53 EDT
Subject: [R] Importing Data
Message-ID: <47.316566c1.2c6286c5@aol.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20030806/f0f5621e/attachment.pl

From ligges at statistik.uni-dortmund.de  Wed Aug  6 18:12:40 2003
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Wed, 06 Aug 2003 18:12:40 +0200
Subject: [R] Importing Data
In-Reply-To: <BAY9-F551fDc5DYge2Q000108bf@hotmail.com>
References: <BAY9-F551fDc5DYge2Q000108bf@hotmail.com>
Message-ID: <3F3128F8.7000508@statistik.uni-dortmund.de>

Hadassa Brunschwig wrote:

> Im trying to import data from an excel sheet or a sas file to R...im not 
> succeeding. Apparently the function read.xport for reading a SAS file 
> doesnt exist. What do i have to type in EXACTLY to read from an excel 
> sheet(i guess i would be using read.table?)?

- read.xport() is in package foreign, so use library(foreign) before 
using the function.

Please read the manual R Data Import / Export.
You cannot read from Excel with read.table() directly, but there are at 
least three other ways, in particular by using RODBC or exporting / 
importing through in ASCII format.

Uwe Ligges

> 
> Thanks in advance for an answer
> 
> Dassy
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help



From jerome at hivnet.ubc.ca  Wed Aug  6 18:37:43 2003
From: jerome at hivnet.ubc.ca (Jerome Asselin)
Date: Wed, 6 Aug 2003 09:37:43 -0700
Subject: [R] How to copy and paste a R plot onto Word (or Power Point)
In-Reply-To: <FC0CEBD77311DA499A67ADB355A24FA20396AD8F@mail4.oulan.ou.edu>
References: <FC0CEBD77311DA499A67ADB355A24FA20396AD8F@mail4.oulan.ou.edu>
Message-ID: <200308061644.JAA23705@hivnet.ubc.ca>


In addition to the previous replies, see the r-help thread on "pretty 
onscreen plots":

http://finzi.psych.upenn.edu/R/Rhelp02/archive/12254.html

HTH,
Jerome

On August 6, 2003 08:47 am, Yao, Minghua wrote:
> All,
>
> Anybody can tell  how to export a R plot onto Word  (or Power Point)?
> Many thanks in advance.
>
> -MY
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help



From parkhurs at ariel.ucs.indiana.edu  Wed Aug  6 19:57:49 2003
From: parkhurs at ariel.ucs.indiana.edu (David Parkhurst)
Date: Wed, 6 Aug 2003 12:57:49 -0500
Subject: [R] problem with download.packages
Message-ID: <001301c35c44$4bdd4630$0a6cfea9@BLSPEAPARKHOM>

R Version 1.7.0, under windows XP pro

The help page for update.packages says that {
`download.packages' takes a list of package names and a
destination directory, downloads the newest versions of the
package sources and saves them in `destdir'.  If the list of
available packages is not given as argument, it is also directly
obtained from CRAN.  If CRAN is local, i.e., the URL starts with
`"file:"', then the packages are not downloaded but used directly.
}.

I tried this (while connected to the internet):
> download.packages(destdir="\\aq\\R\\updates")
with this result:
trying URL `http://cran.r-project.org/bin/windows/contrib/1.7/PACKAGES'
Content type `text/plain; charset=iso-8859-1' length 12447 bytes
opened URL
downloaded 12Kb

Error in unique(pkgs) : Argument "pkgs" is missing, with no default

Is there not an inconsistency here?  How can I get this to work?

Thanks.

Dave Parkhurst



From ripley at stats.ox.ac.uk  Wed Aug  6 20:42:42 2003
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Wed, 6 Aug 2003 19:42:42 +0100 (BST)
Subject: [R] problem with download.packages
In-Reply-To: <001301c35c44$4bdd4630$0a6cfea9@BLSPEAPARKHOM>
Message-ID: <Pine.LNX.4.44.0308061938190.23616-100000@gannet.stats>

On Wed, 6 Aug 2003, David Parkhurst wrote:

> R Version 1.7.0, under windows XP pro
> 
> The help page for update.packages says that {
> `download.packages' takes a list of package names and a
                            ^^^^^^^^^^^^^^^^^^^^^^^
> destination directory, downloads the newest versions of the
> package sources and saves them in `destdir'.  If the list of
> available packages is not given as argument, it is also directly
  ^^^^^^^^^^^^^^^^^^
Not a `list of package names'!

> obtained from CRAN.  If CRAN is local, i.e., the URL starts with
> `"file:"', then the packages are not downloaded but used directly.
> }.
> 
> I tried this (while connected to the internet):
> > download.packages(destdir="\\aq\\R\\updates")
> with this result:
> trying URL `http://cran.r-project.org/bin/windows/contrib/1.7/PACKAGES'
> Content type `text/plain; charset=iso-8859-1' length 12447 bytes
> opened URL
> downloaded 12Kb
> 
> Error in unique(pkgs) : Argument "pkgs" is missing, with no default
> 
> Is there not an inconsistency here?  How can I get this to work?

Yes!  *You* failed to supply a `list of package names'!  I don't suppose
you meant an inconsistency in your message, but there appears to be a
gaping one.

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From olau at fas.harvard.edu  Wed Aug  6 22:10:48 2003
From: olau at fas.harvard.edu (Olivia Lau)
Date: Wed, 6 Aug 2003 16:10:48 -0400
Subject: [R] demo() function returns alphabetical matches,
	not unique matches?
Message-ID: <000801c35c56$d3fa3980$be76f78c@cbrssstu6>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20030806/5da1ebf6/attachment.pl

From cafa at ime.unicamp.br  Wed Aug  6 22:24:02 2003
From: cafa at ime.unicamp.br (Cezar Augusto de Freitas Anselmo)
Date: Wed, 6 Aug 2003 17:24:02 -0300 (BRT)
Subject: [R] evaluating and walking in names
Message-ID: <Pine.GSO.4.05.10308061714020.6659-100000@athenas.ime.unicamp.br>

Hi, all.
Suppose I have an object with names (like a data.frame) and I want to walk
in a loop with your names. How can I do this? The idea is like this:

my.data<-data.frame(matrix(runif(6),ncol=2))
names(my.data)
[1] "X1" "X2"

for(i in names(my.data)){
	my.variable <- cat(paste("my.data$", i, "\n", sep=""))
	print(mean(my.variable))
}

#it doesn't work.

Thnaks for all,
C.

========================================
Cezar Freitas (ICQ 109128967)
IMECC - UNICAMP
Campinas, SP - Brasil



From fharrell at virginia.edu  Wed Aug  6 23:21:34 2003
From: fharrell at virginia.edu (Frank E Harrell Jr)
Date: Wed, 6 Aug 2003 17:21:34 -0400
Subject: [R] Slight problem in sort
Message-ID: <20030806172134.50910d07.fharrell@virginia.edu>

In

platform i686-pc-linux-gnu
arch     i686             
os       linux-gnu        
system   i686, linux-gnu  
status                    
major    1                
minor    7.1              
year     2003             
month    06               
day      16               
language R                

I get

> sort(c(3,1,NA))
[1] 1 3

Shouldn't NAs be retained by default?

Thanks -Frank
---
Frank E Harrell Jr              Prof. of Biostatistics & Statistics
Div. of Biostatistics & Epidem. Dept. of Health Evaluation Sciences
U. Virginia School of Medicine  http://hesweb1.med.virginia.edu/biostat



From jasont at indigoindustrial.co.nz  Wed Aug  6 23:33:12 2003
From: jasont at indigoindustrial.co.nz (Jason Turner)
Date: Thu, 07 Aug 2003 09:33:12 +1200
Subject: [R] evaluating and walking in names
In-Reply-To: <Pine.GSO.4.05.10308061714020.6659-100000@athenas.ime.unicamp.br>
References: <Pine.GSO.4.05.10308061714020.6659-100000@athenas.ime.unicamp.br>
Message-ID: <3F317418.8040204@indigoindustrial.co.nz>

Cezar Augusto de Freitas Anselmo wrote:
> Hi, all.
> Suppose I have an object with names (like a data.frame) and I want to walk
> in a loop with your names. How can I do this? The idea is like this:
> 
> my.data<-data.frame(matrix(runif(6),ncol=2))
> names(my.data)
> [1] "X1" "X2"
> 
> for(i in names(my.data)){
> 	my.variable <- cat(paste("my.data$", i, "\n", sep=""))
> 	print(mean(my.variable))
> }
> 
> #it doesn't work.
> 
> Thnaks for all,
> C.
> 
> ========================================
> Cezar Freitas (ICQ 109128967)
> IMECC - UNICAMP
> Campinas, SP - Brasil
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> 
> 

You want assign().

df <- data.frame(matrix(runif(20),ncol=2))

# using ii instead of i makes searches *much* easier with
# a text editor.
for(ii in names(df)) {
   assign(ii,df[[ii]])
}

ls()
[1] "X1" "X2" "df" "ii"

# might not be exactly the same on your machine due
# to floating point rounding.
all.equal(X1,df$X1)
[1] TRUE

all.equal(X2,df$X2)
[1] TRUE

Cheers

Jason
-- 
Indigo Industrial Controls Ltd.
64-21-343-545
jasont at indigoindustrial.co.nz



From edd at debian.org  Wed Aug  6 23:30:36 2003
From: edd at debian.org (Dirk Eddelbuettel)
Date: Wed, 6 Aug 2003 16:30:36 -0500
Subject: [R] Plot ticks and tick labels: thickness, colour?
Message-ID: <20030806213036.GA632@sonny.eddelbuettel.com>


I am displaying several series in one plot, and would like to make
them distinct without having to employ a legend. 

I managed to color tickmarks, but have been unsuccessful with either one of 

a) making tickmarks thicker (without increasing the axis at the same time).
   From reading ?axis:
       lty, lwd: line type, width for the axis line and the tick marks.
   in would appear that I cannot obtain the one _without_ the other; or
   
b) displaying the tick label in a different colour. Again, ?axis reads
     col: color for the axis line and the tick marks. [..]
   indicates that I can only set the tick mark, not the annotation.
   
Any ideas or suggestions?

Thanks in advance,  Dirk

-- 
Those are my principles, and if you don't like them... well, I have others.
                                                -- Groucho Marx



From p.dalgaard at biostat.ku.dk  Wed Aug  6 23:35:15 2003
From: p.dalgaard at biostat.ku.dk (Peter Dalgaard BSA)
Date: Wed, 06 Aug 2003 21:35:15 -0000
Subject: [R] Slight problem in sort
In-Reply-To: <20030806172134.50910d07.fharrell@virginia.edu>
References: <20030806172134.50910d07.fharrell@virginia.edu>
Message-ID: <x2d6fixy3v.fsf@biostat.ku.dk>

Frank E Harrell Jr <fharrell at virginia.edu> writes:

> I get
> 
> > sort(c(3,1,NA))
> [1] 1 3
> 
> Shouldn't NAs be retained by default?

Not according to the documentation...

     sort(x, partial = NULL, na.last = NA, decreasing = FALSE,
          method = c("shell", "quick"), index.return = FALSE)

 na.last: for controlling the treatment of `NA's. If `TRUE', missing
          values in the data are put last; if `FALSE', they are put
          first; if `NA', they are removed.


-- 
   O__  ---- Peter Dalgaard             Blegdamsvej 3  
  c/ /'_ --- Dept. of Biostatistics     2200 Cph. N   
 (*) \(*) -- University of Copenhagen   Denmark      Ph: (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)             FAX: (+45) 35327907



From jasont at indigoindustrial.co.nz  Wed Aug  6 23:42:39 2003
From: jasont at indigoindustrial.co.nz (Jason Turner)
Date: Thu, 07 Aug 2003 09:42:39 +1200
Subject: [R] Slight problem in sort
In-Reply-To: <20030806172134.50910d07.fharrell@virginia.edu>
References: <20030806172134.50910d07.fharrell@virginia.edu>
Message-ID: <3F31764F.30606@indigoindustrial.co.nz>

Frank E Harrell Jr wrote:
>>sort(c(3,1,NA))
> 
> [1] 1 3
> 
> Shouldn't NAs be retained by default?

help(sort)

sort(c(3,1,NA),na.last=TRUE)

Cheers

Jason
-- 
Indigo Industrial Controls Ltd.
64-21-343-545
jasont at indigoindustrial.co.nz



From MSchwartz at medanalytics.com  Wed Aug  6 23:38:36 2003
From: MSchwartz at medanalytics.com (Marc Schwartz)
Date: Wed, 06 Aug 2003 16:38:36 -0500
Subject: [R] Slight problem in sort
In-Reply-To: <20030806172134.50910d07.fharrell@virginia.edu>
References: <20030806172134.50910d07.fharrell@virginia.edu>
Message-ID: <1060205915.29873.113.camel@localhost>

On Wed, 2003-08-06 at 16:21, Frank E Harrell Jr wrote:
> In
> 
> platform i686-pc-linux-gnu
> arch     i686             
> os       linux-gnu        
> system   i686, linux-gnu  
> status                    
> major    1                
> minor    7.1              
> year     2003             
> month    06               
> day      16               
> language R                
> 
> I get
> 
> > sort(c(3,1,NA))
> [1] 1 3
> 
> Shouldn't NAs be retained by default?
> 
> Thanks -Frank


Frank,

The default is to exclude NA's based upon the argument 'na.last = NA'. 
If you set the argument 'na.last' to either TRUE or FALSE, the NA's will
be kept and sorted either last or first respectively.

> sort(c(3,1,NA), na.last = TRUE)
[1]  1  3 NA

> sort(c(3,1,NA), na.last = LAST)
[1] NA  1  3

See ?sort

HTH,

Marc Schwartz



From MSchwartz at medanalytics.com  Wed Aug  6 23:41:23 2003
From: MSchwartz at medanalytics.com (Marc Schwartz)
Date: Wed, 06 Aug 2003 16:41:23 -0500
Subject: [R] Slight problem in sort
In-Reply-To: <1060205915.29873.113.camel@localhost>
References: <20030806172134.50910d07.fharrell@virginia.edu>
	<1060205915.29873.113.camel@localhost>
Message-ID: <1060206083.29873.116.camel@localhost>

On Wed, 2003-08-06 at 16:38, Marc Schwartz wrote:
> On Wed, 2003-08-06 at 16:21, Frank E Harrell Jr wrote:
> > In
> > 
> > platform i686-pc-linux-gnu
> > arch     i686             
> > os       linux-gnu        
> > system   i686, linux-gnu  
> > status                    
> > major    1                
> > minor    7.1              
> > year     2003             
> > month    06               
> > day      16               
> > language R                
> > 
> > I get
> > 
> > > sort(c(3,1,NA))
> > [1] 1 3
> > 
> > Shouldn't NAs be retained by default?
> > 
> > Thanks -Frank
> 
> 
> Frank,
> 
> The default is to exclude NA's based upon the argument 'na.last = NA'. 
> If you set the argument 'na.last' to either TRUE or FALSE, the NA's will
> be kept and sorted either last or first respectively.
> 
> > sort(c(3,1,NA), na.last = TRUE)
> [1]  1  3 NA
> 
> > sort(c(3,1,NA), na.last = LAST)
> [1] NA  1  3
> 
> See ?sort
> 
> HTH,
> 
> Marc Schwartz


Apologies. Bad copy on my part. The second example should be:

> sort(c(3,1,NA), na.last = FALSE)
[1] NA  1  3

Marc



From MSchwartz at medanalytics.com  Thu Aug  7 00:36:54 2003
From: MSchwartz at medanalytics.com (Marc Schwartz)
Date: Wed, 06 Aug 2003 17:36:54 -0500
Subject: [R] Plot ticks and tick labels: thickness, colour?
In-Reply-To: <20030806213036.GA632@sonny.eddelbuettel.com>
References: <20030806213036.GA632@sonny.eddelbuettel.com>
Message-ID: <1060209414.29873.170.camel@localhost>

On Wed, 2003-08-06 at 16:30, Dirk Eddelbuettel wrote: 
> I am displaying several series in one plot, and would like to make
> them distinct without having to employ a legend. 
> 
> I managed to color tickmarks, but have been unsuccessful with either one of 
> 
> a) making tickmarks thicker (without increasing the axis at the same time).
>    From reading ?axis:
>        lty, lwd: line type, width for the axis line and the tick marks.
>    in would appear that I cannot obtain the one _without_ the other; or
>    
> b) displaying the tick label in a different colour. Again, ?axis reads
>      col: color for the axis line and the tick marks. [..]
>    indicates that I can only set the tick mark, not the annotation.
>    
> Any ideas or suggestions?
> 
> Thanks in advance,  Dirk


Dirk,

One possible option:

# Generic plot with no tick marks or annotation
plot(1:10, 1:10, axes = FALSE, ann = FALSE)

# First get par("usr") to get x and y axis ranges
usr <- par("usr")

# Now draw tick marks, using axTicks() to get
# default locations
# First, x axis
at.x <- axTicks(1)
segments(at.x, usr[3], at.x,
         usr[3] - ((usr[4] - usr[3]) * 0.01), 
         lwd = 2, col = "red", xpd = TRUE)

# Draw x axis tick mark labels
mtext(at.x, at = at.x, side = 1, line = 1, 
      col = "red")

# Now do the same for the y axis
at.y <- axTicks(2)
segments(usr[1], at.y,
         usr[1] - ((usr[2] - usr[1]) * 0.01), 
         at.y,
         lwd = 2, col = "blue", xpd = TRUE)

# Draw y axis tick mark labels
mtext(at.y, at = at.y, side = 2, line = 1, 
      col = "blue")

# put a box around the plot region
box()

In the above calls to segments(), adjust the '* 0.01' to be the tick
mark length you wish as a proportion of the axis range and of course the
'lwd' and 'col' to your preference.

If you want the tick marks inside the plot region, rather than outside,
change the initial subtractions to additions. Note the use of 'xpd =
TRUE', so that the tick marks are not clipped at the plot region for
'outside'.

If you need to use something other than the default tick mark locations,
you can always adjust 'at.x' and 'at.y' to whatever you choose.

HTH,

Marc Schwartz



From tblackw at umich.edu  Thu Aug  7 00:35:43 2003
From: tblackw at umich.edu (Thomas W Blackwell)
Date: Wed, 6 Aug 2003 18:35:43 -0400 (EDT)
Subject: [R] evaluating and walking in names
In-Reply-To: <3F317418.8040204@indigoindustrial.co.nz>
Message-ID: <Pine.SOL.4.44.0308061821110.962-100000@mspacman.gpcc.itd.umich.edu>

Cezar  -

The functions  sapply()  and  lapply()  will loop over all
of the columns in a dataframe.  A very common syntax is:

column.means <- sapply(my.data, mean)

But for this particular example, you could use the function
colMeans()  instead.  See the help for all three.

-  tom blackwell  -  u michigan medical school  -  ann arbor  -

> Cezar Augusto de Freitas Anselmo wrote:
> >
> > Hi, all.
> > Suppose I have an object with names (like a data.frame) and I want to walk
> > in a loop with your names. How can I do this? The idea is like this:
> >
> > my.data<-data.frame(matrix(runif(6),ncol=2))
> > names(my.data)
> > [1] "X1" "X2"
> >
> > for(i in names(my.data)){
> > 	my.variable <- cat(paste("my.data$", i, "\n", sep=""))
> > 	print(mean(my.variable))
> > }
> >
> > #it doesn't work.
> >
> > Thnaks for all,
> > C.
> > ========================================
> > Cezar Freitas (ICQ 109128967)
> > IMECC - UNICAMP
> > Campinas, SP - Brasil



From r.darnell at uq.edu.au  Thu Aug  7 00:44:10 2003
From: r.darnell at uq.edu.au (Ross Darnell)
Date: Thu, 07 Aug 2003 08:44:10 +1000
Subject: [R] R workshop, Qld Australia
Message-ID: <adam76f9.fsf@uq.edu.au>

Need some help with R?

An R workshop, lead by John Maindonald, is planned for the 2nd October
at the University of Southern Queensland, Toowoomba, Australia. The
workshop is being organised by the Queensland branch of the
Statistical Society of Australia as part of their conference.

Anyone interested in attending can find out more by looking at 

http://www.sci.usq.edu.au/staff/dunn/qstatconf/workshops.html

-- 
Ross Darnell
Organising committee
Email: <r.darnell at uq.edu.au>



From tblackw at umich.edu  Thu Aug  7 00:56:09 2003
From: tblackw at umich.edu (Thomas W Blackwell)
Date: Wed, 6 Aug 2003 18:56:09 -0400 (EDT)
Subject: [R] Plot ticks and tick labels: thickness, colour?
In-Reply-To: <20030806213036.GA632@sonny.eddelbuettel.com>
Message-ID: <Pine.SOL.4.44.0308061841150.962-100000@mspacman.gpcc.itd.umich.edu>

Dirk  -

You can certainly make tickmarks thinner than the axis line by
multiple calls to  axis() with different values for lwd.  MAYBE
you can overwrite an earlier call by setting  col.axis="white"
(and no tickmarks) but I've never tried this.  mtext()  allows
building custom tick labels.

In R-1.7.1,  help("axis")  seems to disagree with  help("par")
on whether the arguments to  axis()  are called "col" and "font"
(as in help(axis)) or "col.axis", "col.lab" and "font.lab"
(as in help(par)).  In my experience,  help(par)  rules.

But, do you REALLY want to get into this !

-  tom blackwell  -  u michigan medical school  -  ann arbor  -

On Wed, 6 Aug 2003, Dirk Eddelbuettel wrote:

> I am displaying several series in one plot, and would like to make
> them distinct without having to employ a legend.
>
> I managed to color tickmarks, but have been unsuccessful with either one of
>
> a) making tickmarks thicker (without increasing the axis at the same time).
>    From reading ?axis:
>        lty, lwd: line type, width for the axis line and the tick marks.
>    in would appear that I cannot obtain the one _without_ the other; or
>
> b) displaying the tick label in a different colour. Again, ?axis reads
>      col: color for the axis line and the tick marks. [..]
>    indicates that I can only set the tick mark, not the annotation.
>
> Thanks in advance,  Dirk



From mpie at yahoo.com  Thu Aug  7 03:07:19 2003
From: mpie at yahoo.com (mpie)
Date: Wed, 6 Aug 2003 18:07:19 -0700 (PDT)
Subject: [R] installing add-on packages on OS X
Message-ID: <20030807010719.52042.qmail@web12811.mail.yahoo.com>

Hi there,


This is probably a stupid question, but since I'm a
newbie on R, here it goes. I got R 1.6.2 running on OS
10.1.5 and I'm trying to install an add-on package.

I couldn't find info on installing this in a mac. Can
anyone give me some pointers?

Thanks

Marcio


__________________________________

Yahoo! SiteBuilder - Free, easy-to-use web site design software



From fharrell at virginia.edu  Thu Aug  7 03:37:35 2003
From: fharrell at virginia.edu (Frank E Harrell Jr)
Date: Wed, 6 Aug 2003 21:37:35 -0400
Subject: [R] Slight problem in sort
In-Reply-To: <1060206083.29873.116.camel@localhost>
References: <20030806172134.50910d07.fharrell@virginia.edu>
	<1060205915.29873.113.camel@localhost>
	<1060206083.29873.116.camel@localhost>
Message-ID: <20030806213735.2f53e9e7.fharrell@virginia.edu>

Thanks to all who clarified this for me.  The documentation is clear once I read it carefully.

Frank


On Wed, 06 Aug 2003 16:41:23 -0500
Marc Schwartz <MSchwartz at medanalytics.com> wrote:

> On Wed, 2003-08-06 at 16:38, Marc Schwartz wrote:
> > On Wed, 2003-08-06 at 16:21, Frank E Harrell Jr wrote:
> > > In
> > > 
> > > platform i686-pc-linux-gnu
> > > arch     i686             
> > > os       linux-gnu        
> > > system   i686, linux-gnu  
> > > status                    
> > > major    1                
> > > minor    7.1              
> > > year     2003             
> > > month    06               
> > > day      16               
> > > language R                
> > > 
> > > I get
> > > 
> > > > sort(c(3,1,NA))
> > > [1] 1 3
> > > 
> > > Shouldn't NAs be retained by default?
> > > 
> > > Thanks -Frank
> > 
> > 
> > Frank,
> > 
> > The default is to exclude NA's based upon the argument 'na.last = NA'. 
> > If you set the argument 'na.last' to either TRUE or FALSE, the NA's will
> > be kept and sorted either last or first respectively.
> > 
> > > sort(c(3,1,NA), na.last = TRUE)
> > [1]  1  3 NA
> > 
> > > sort(c(3,1,NA), na.last = LAST)
> > [1] NA  1  3
> > 
> > See ?sort
> > 
> > HTH,
> > 
> > Marc Schwartz
> 
> 
> Apologies. Bad copy on my part. The second example should be:
> 
> > sort(c(3,1,NA), na.last = FALSE)
> [1] NA  1  3
> 
> Marc
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help


---
Frank E Harrell Jr              Prof. of Biostatistics & Statistics
Div. of Biostatistics & Epidem. Dept. of Health Evaluation Sciences
U. Virginia School of Medicine  http://hesweb1.med.virginia.edu/biostat



From Ted.Harding at nessie.mcc.ac.uk  Wed Aug  6 23:41:45 2003
From: Ted.Harding at nessie.mcc.ac.uk ( (Ted Harding))
Date: Wed, 06 Aug 2003 22:41:45 +0100 (BST)
Subject: [R] evaluating and walking in names
In-Reply-To: <Pine.GSO.4.05.10308061714020.6659-100000@athenas.ime.unicamp.br>
Message-ID: <XFMail.030806224145.Ted.Harding@nessie.mcc.ac.uk>

On 06-Aug-03 Cezar Augusto de Freitas Anselmo wrote:
> Hi, all.
> Suppose I have an object with names (like a data.frame) and
> I want to walk in a loop with your names. How can I do this?
> The idea is like this:
> 
> my.data<-data.frame(matrix(runif(6),ncol=2))
> names(my.data)
> [1] "X1" "X2"
> for(i in names(my.data)){
>       my.variable <- cat(paste("my.data$", i, "\n", sep=""))
>       print(mean(my.variable))
> }
>#it doesn't work.

Hi Cezar,

First of all, you should get rid of those "\n" -- they intrude:
> for(i in names(my.data)){ print(paste("my.data$", i, "\n", sep=""))}
[1] "my.data$X1\n"
[1] "my.data$X2\n"
> for(i in names(my.data)){ print(paste("my.data$", i, sep=""))}
[1] "my.data$X1"
[1] "my.data$X2"

Second, you don't need to do the above anyway: the simplest way to
refer to the variable with the name name is simply to index the column
by name:

> for(i in names(my.data)){
      my.variable<-my.data[,i];print(mean(my.variable))}
[1] 0.3302733
[1] 0.6119088

To see this, have a look at

  my.data[,"X1"]
  my.data[,"X2"]

I hope this breaks the block!

QUESTION TO EXPERTS:
While (a construction I've often used successfully):

  for(Z in c("X1","X2","X3")){
    Z<-eval(as.name(Z))
    do.something.with(Z) }

works, going through the variables named "X1", "X2", "X3" in
turn, when I was trying to clean up Cezar's example above I found
that if you construct the name Z by pasting so that it comes to
"my.data$X1" then the object "my.data$X1" is not found. Presumably
this is because the "$" operator is not functional in this context,
but I can't locate an explanation of this. Can anyone elucidate?

Best wishes,
Ted.


--------------------------------------------------------------------
E-Mail: (Ted Harding) <Ted.Harding at nessie.mcc.ac.uk>
Fax-to-email: +44 (0)870 167 1972
Date: 06-Aug-03                                       Time: 22:41:45
------------------------------ XFMail ------------------------------



From glaziou at pasteur-kh.org  Thu Aug  7 03:52:13 2003
From: glaziou at pasteur-kh.org (Philippe Glaziou)
Date: Thu, 7 Aug 2003 08:52:13 +0700
Subject: [R] installing add-on packages on OS X
In-Reply-To: <20030807010719.52042.qmail@web12811.mail.yahoo.com>
References: <20030807010719.52042.qmail@web12811.mail.yahoo.com>
Message-ID: <20030807015213.GE599@pasteur-kh.org>

mpie <mpie at yahoo.com> wrote:
> This is probably a stupid question, but since I'm a
> newbie on R, here it goes. I got R 1.6.2 running on OS
> 10.1.5 and I'm trying to install an add-on package.
> 
> I couldn't find info on installing this in a mac. Can
> anyone give me some pointers?


See R-admin. 

It may be a good idea to consider upgrading your R. It
compiles very well on MacOSX, provided that you install all
the missing soft that Apple could not be bothered to 
include in the OS. 

-- 
Philippe Glaziou
Pasteur Institute of Cambodia



From edd at debian.org  Thu Aug  7 04:14:39 2003
From: edd at debian.org (Dirk Eddelbuettel)
Date: Wed, 6 Aug 2003 21:14:39 -0500
Subject: [R] Plot ticks and tick labels: thickness, colour?
In-Reply-To: <1060209414.29873.170.camel@localhost>
References: <20030806213036.GA632@sonny.eddelbuettel.com>
	<1060209414.29873.170.camel@localhost>
Message-ID: <20030807021438.GA2277@sonny.eddelbuettel.com>


Marc,

Thanks a lot -- the axTicks(), segments() and mtext() solution looks like
what I was looking for.

Regards,  Dirk

-- 
Those are my principles, and if you don't like them... well, I have others.
                                                -- Groucho Marx



From tlumley at u.washington.edu  Thu Aug  7 05:42:59 2003
From: tlumley at u.washington.edu (Thomas Lumley)
Date: Wed, 6 Aug 2003 20:42:59 -0700 (PDT)
Subject: [R] evaluating and walking in names
In-Reply-To: <XFMail.030806224145.Ted.Harding@nessie.mcc.ac.uk>
Message-ID: <Pine.A41.4.44.0308062031560.98424-100000@homer03.u.washington.edu>

On Wed, 6 Aug 2003 Ted.Harding at nessie.mcc.ac.uk wrote:

> QUESTION TO EXPERTS:
> While (a construction I've often used successfully):
>
>   for(Z in c("X1","X2","X3")){
>     Z<-eval(as.name(Z))
>     do.something.with(Z) }
>
> works, going through the variables named "X1", "X2", "X3" in
> turn, when I was trying to clean up Cezar's example above I found
> that if you construct the name Z by pasting so that it comes to
> "my.data$X1" then the object "my.data$X1" is not found. Presumably
> this is because the "$" operator is not functional in this context,
> but I can't locate an explanation of this. Can anyone elucidate?

Because as.name() converts the string to a name, but you wanted an
expression (the X1 component of my.data).

In r-devel this is visually clearer
> as.name("my.data$X1")
`my.data$X1`
the backticks indicating a syntatically weird name.  You can  do eg
> `my.data$X1` <- 42
> eval(as.name("my.data$X1"))
[1] 42
In current R versions you would have to use
assign("mydata$X1", 42)
to assign to this variable.

To get the X1 component of my.data you would need
eval(parse(text="my.data$X1"))
> my.data<-data.frame(X1=1:2,X2=2:3)
> parse(text="my.data$X1")
expression(my.data$X1)
> eval(parse(text="my.data$X1"))
[1] 1 2

	-thomas



From ripley at stats.ox.ac.uk  Thu Aug  7 08:24:23 2003
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Thu, 7 Aug 2003 07:24:23 +0100 (BST)
Subject: [R] Plot ticks and tick labels: thickness, colour?
In-Reply-To: <Pine.SOL.4.44.0308061841150.962-100000@mspacman.gpcc.itd.umich.edu>
Message-ID: <Pine.LNX.4.44.0308070711260.24546-100000@gannet.stats>

On Wed, 6 Aug 2003, Thomas W Blackwell wrote:

> You can certainly make tickmarks thinner than the axis line by
> multiple calls to  axis() with different values for lwd.  MAYBE
> you can overwrite an earlier call by setting  col.axis="white"
> (and no tickmarks) but I've never tried this.  mtext()  allows
> building custom tick labels.

col.axis="transparent" would be better:  col.axis="white" would overplot 
in white.

> In R-1.7.1,  help("axis")  seems to disagree with  help("par")
> on whether the arguments to  axis()  are called "col" and "font"
> (as in help(axis)) or "col.axis", "col.lab" and "font.lab"
> (as in help(par)).  In my experience,  help(par)  rules.

I think there is no disagreement in the help, but your experience does
disagree with my experiments.

    font: font for text.

     col: color for the axis line and the tick marks.  The default
          'NULL' means to use 'par("fg")'.

     'col.axis' The color to be used for axis annotation.

     'col.lab' The color to be used for x and y labels.

    'font.axis' The font to be used for axis annotation.

Try

> plot(1:10, col.axis="gold", col.lab="blue", font.axis=3)
> axis(side=4, at=1:10, col="green", col.axis="gold", col.lab="blue", 
       font.axis=3, font=2)

So when calling axis, the colour of the line and the tick marks is set by 
"col" and that of the labels by "col.axis" and the xlab and ylab by 
"col.lab".  Three separate components, three colour settings.

As for fonts, "font" rules when supplied to axis, otherwise "font.axis" is 
used.

I'll add a note to axis.Rd.

BDR



From ripley at stats.ox.ac.uk  Thu Aug  7 08:29:47 2003
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Thu, 7 Aug 2003 07:29:47 +0100 (BST)
Subject: [R] installing add-on packages on OS X
In-Reply-To: <20030807015213.GE599@pasteur-kh.org>
Message-ID: <Pine.LNX.4.44.0308070726390.24546-100000@gannet.stats>

On Thu, 7 Aug 2003, Philippe Glaziou wrote:

> mpie <mpie at yahoo.com> wrote:
> > This is probably a stupid question, but since I'm a
> > newbie on R, here it goes. I got R 1.6.2 running on OS
> > 10.1.5 and I'm trying to install an add-on package.
> > 
> > I couldn't find info on installing this in a mac. Can
> > anyone give me some pointers?
> 
> 
> See R-admin. 
> 
> It may be a good idea to consider upgrading your R. It
> compiles very well on MacOSX, provided that you install all
> the missing soft that Apple could not be bothered to 
> include in the OS. 

I thinks `mpie' is using the Carbon port and not the Darwin port
(although I do suggest upgrading the R version).  For the Carbon port, 
there are pre-built packages on CRAN as .sit files.

Please, please, please can MacOS users differentiate between the two
versions!  As from 1.8.0 there will only be one, and only for MacOS X -- a
version of the Darwin port with a GUI console.

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From temiz at deprem.gov.tr  Thu Aug  7 09:09:17 2003
From: temiz at deprem.gov.tr (orkun)
Date: Thu, 07 Aug 2003 10:09:17 +0300
Subject: [R] Strange predicted values ?
Message-ID: <3F31FB1D.50305@deprem.gov.tr>

Hello

I carried out a logistic regression and found predicted values.
Then I want to see both predictors (var1,var2..) and predicted values
in same matrix. In other words, I need to know each combinations
and predicted values.

I used:
cbind(var1,var2,var3,var4,predict(glm.obj,type="resp"))

I got  a somewhat strange result:

var1  var2  var3  var4  var5 var6      predicted vals
------   -----  ----- -------- -----  -----      -------------------
  6        6       1      1        1      1            4.24e-07
  6        6       1      1        1      1            8.37e-11
  6        6       1      1        1      1            6.8e-07
   .
   .
   .

I had expected to have a same predicted value for
same combinations.

Can anyone explain to me why ?
Or any suggestion ?

kind regards


Ahmet Temiz
TURKEY


 


______________________________________



______________________________________
The views and opinions expressed in this e-mail message are ...{{dropped}}



From dassybr at hotmail.com  Thu Aug  7 09:21:51 2003
From: dassybr at hotmail.com (Hadassa Brunschwig)
Date: Thu, 07 Aug 2003 09:21:51 +0200
Subject: [R] function "lme"
Message-ID: <BAY9-F43KWnkCun2KsF00020ce9@hotmail.com>

Thanks to everyone who replied to my first problem. Here is the second one: 
Is the function "lme" (Mixed Model) also in a foreign library(shouldnt, as 
this is a basic statistical function)? I am trying to use lme in R and it 
replies that it couldnt find the function lme...weird...

Thanks for replying!



From ripley at stats.ox.ac.uk  Thu Aug  7 09:22:20 2003
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Thu, 7 Aug 2003 08:22:20 +0100 (BST)
Subject: [R] Strange predicted values ?
In-Reply-To: <3F31FB1D.50305@deprem.gov.tr>
Message-ID: <Pine.LNX.4.44.0308070820180.24759-100000@gannet.stats>

Those are not predicted values, they are fitted values.  Try predicting on 
the same set of variables as you printed.

Please do try to give a small reproducible example so we can see what you 
actually did.

On Thu, 7 Aug 2003, orkun wrote:

> I carried out a logistic regression and found predicted values.
> Then I want to see both predictors (var1,var2..) and predicted values
> in same matrix. In other words, I need to know each combinations
> and predicted values.
> 
> I used:
> cbind(var1,var2,var3,var4,predict(glm.obj,type="resp"))
> 
> I got  a somewhat strange result:
> 
> var1  var2  var3  var4  var5 var6      predicted vals
> ------   -----  ----- -------- -----  -----      -------------------
>   6        6       1      1        1      1            4.24e-07
>   6        6       1      1        1      1            8.37e-11
>   6        6       1      1        1      1            6.8e-07
>    .
>    .
>    .
> 
> I had expected to have a same predicted value for
> same combinations.

Yes, so it's a fair guess that is not what you computed.  But, we can't 
tell what you did do.


-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From kwan022 at stat.auckland.ac.nz  Thu Aug  7 09:30:56 2003
From: kwan022 at stat.auckland.ac.nz (Ko-Kang Kevin Wang)
Date: Thu, 7 Aug 2003 19:30:56 +1200 (NZST)
Subject: [R] function "lme"
In-Reply-To: <BAY9-F43KWnkCun2KsF00020ce9@hotmail.com>
Message-ID: <Pine.LNX.4.44.0308071930001.29028-100000@stat55.stat.auckland.ac.nz>

On Thu, 7 Aug 2003, Hadassa Brunschwig wrote:

> Thanks to everyone who replied to my first problem. Here is the second one: 
> Is the function "lme" (Mixed Model) also in a foreign library(shouldnt, as 
> this is a basic statistical function)? I am trying to use lme in R and it 
> replies that it couldnt find the function lme...weird...

Obviously it is not going to be in foreign.  

I believe nlme is the library you want to load.

-- 
Cheers,

Kevin

------------------------------------------------------------------------------
"On two occasions, I have been asked [by members of Parliament],
'Pray, Mr. Babbage, if you put into the machine wrong figures, will
the right answers come out?' I am not able to rightly apprehend the
kind of confusion of ideas that could provoke such a question."

-- Charles Babbage (1791-1871) 
---- From Computer Stupidities: http://rinkworks.com/stupid/

--
Ko-Kang Kevin Wang
Master of Science (MSc) Student
SLC Tutor and Lab Demonstrator
Department of Statistics
University of Auckland
New Zealand
Homepage: http://www.stat.auckland.ac.nz/~kwan022
Ph: 373-7599
    x88475 (City)
    x88480 (Tamaki)



From jasont at indigoindustrial.co.nz  Thu Aug  7 09:40:58 2003
From: jasont at indigoindustrial.co.nz (Jason Turner)
Date: Thu, 07 Aug 2003 19:40:58 +1200
Subject: [R] function "lme"
In-Reply-To: <BAY9-F43KWnkCun2KsF00020ce9@hotmail.com>
References: <BAY9-F43KWnkCun2KsF00020ce9@hotmail.com>
Message-ID: <3F32028A.2070806@indigoindustrial.co.nz>

Hadassa Brunschwig wrote:
> Thanks to everyone who replied to my first problem. Here is the second 
> one: Is the function "lme" (Mixed Model) also in a foreign 
> library

yes.

When you encounter this sort of problem, try

help.search("lme")

This will tell you which installed library has the function.

> (shouldnt, as this is a basic statistical function)?

What's "basic" for some is extraneous for others.  Which is why R was 
made to be extensible.  It's a Good Thing, not an inconvenience.  It 
makes R feature-rich, without being bloated.  Users can specify what 
level of features they want to include.

Cheers

Jason
-- 
Indigo Industrial Controls Ltd.
64-21-343-545
jasont at indigoindustrial.co.nz



From jpgranadeiro at fc.ul.pt  Thu Aug  7 10:43:28 2003
From: jpgranadeiro at fc.ul.pt (Jose Pedro Granadeiro)
Date: Thu, 07 Aug 2003 09:43:28 +0100
Subject: [R] Problem with filled.contour
Message-ID: <5.1.1.6.0.20030807091824.00a15490@mail.fc.ul.pt>

Dear all,

I am producing a set of level plot using filled.contour(). I am struggling 
with two problem (I have searched the archives and FAQ, but did not find 
any cues on these issues):

1. Given that several plots share the same range of levels, is there any 
(simple enough...) way to avoid having the right-side scale on some of them 
(i.e. disabling them in some way)?

2. When I save the plots as a postscript file (either from a command of 
from the menu) I can notice a (discrete, but annoying) grey grid underlying 
in certain areas of the filled areas. I can also note this in the printed 
graph . Does anyone have any idea of how can this be removed?

Thanks for your help.


Installation is given below:

          _
platform i386-pc-mingw32
arch     i386
os       mingw32
system   i386, mingw32
status
major    1
minor    7.1
year     2003
month    06
day      16
language R


Jos? Pedro Granadeiro
Centro de Biologia Ambiental
Bloco C2 - Campo Grande
1749-016 LISBOA
Portugal
-------------- next part --------------

---




From Ted.Harding at nessie.mcc.ac.uk  Thu Aug  7 11:24:32 2003
From: Ted.Harding at nessie.mcc.ac.uk ( (Ted Harding))
Date: Thu, 07 Aug 2003 10:24:32 +0100 (BST)
Subject: [R] evaluating and walking in names
In-Reply-To: <Pine.A41.4.44.0308062031560.98424-100000@homer03.u.washington.edu>
Message-ID: <XFMail.030807102432.Ted.Harding@nessie.mcc.ac.uk>

On 07-Aug-03 Thomas Lumley wrote:
> On Wed, 6 Aug 2003 Ted.Harding at nessie.mcc.ac.uk wrote:
> To get the X1 component of my.data you would need
> eval(parse(text="my.data$X1"))
>> my.data<-data.frame(X1=1:2,X2=2:3)
>> parse(text="my.data$X1")
> expression(my.data$X1)
>> eval(parse(text="my.data$X1"))
> [1] 1 2

Of course! Thanks, Thomas.
Ted.


--------------------------------------------------------------------
E-Mail: (Ted Harding) <Ted.Harding at nessie.mcc.ac.uk>
Fax-to-email: +44 (0)870 167 1972
Date: 07-Aug-03                                       Time: 10:24:31
------------------------------ XFMail ------------------------------



From temiz at deprem.gov.tr  Thu Aug  7 11:59:33 2003
From: temiz at deprem.gov.tr (orkun)
Date: Thu, 07 Aug 2003 12:59:33 +0300
Subject: [R] Strange predicted values ?
In-Reply-To: <Pine.LNX.4.44.0308070820180.24759-100000@gannet.stats>
References: <Pine.LNX.4.44.0308070820180.24759-100000@gannet.stats>
Message-ID: <3F322305.1070904@deprem.gov.tr>


>Those are not predicted values, they are fitted values.  Try predicting on 
>the same set of variables as you printed.
>
>Please do try to give a small reproducible example so we can see what you 
>actually did.
>
>On Thu, 7 Aug 2003, orkun wrote:
>
>  
>
>>I carried out a logistic regression and found predicted values.
>>Then I want to see both predictors (var1,var2..) and predicted values
>>in same matrix. In other words, I need to know each combinations
>>and predicted values.
>>
>>I used:
>>cbind(var1,var2,var3,var4,predict(glm.obj,type="resp"))
>>
>>I got  a somewhat strange result:
>>
>>var1  var2  var3  var4  var5 var6      predicted vals
>>------   -----  ----- -------- -----  -----      -------------------
>>  6        6       1      1        1      1            4.24e-07
>>  6        6       1      1        1      1            8.37e-11
>>  6        6       1      1        1      1            6.8e-07
>>   .
>>   .
>>   .
>>
>>I had expected to have a same predicted value for
>>same combinations.
>>    
>>
>
>Yes, so it's a fair guess that is not what you computed.  But, we can't 
>tell what you did do.
>
>
>  
>
Thank you for your interest .
If  predict(glm.obj,type="resp") does not give predicted vals, How can I 
get predicted values ?

The way I followed in attachment

kind regards


______________________________________



______________________________________
The views and opinions expressed in this e-mail message are the sender's own
and do not necessarily represent the views and the opinions of Earthquake Research Dept.
of General Directorate of Disaster Affairs.

Bu e-postadaki fikir ve gorusler gonderenin sahsina ait olup, yasal olarak T.C.
B.I.B. Afet Isleri Gn.Mud. Deprem Arastirma Dairesi'ni baglayici nitelikte degildir.


From grassi at psico.univ.trieste.it  Thu Aug  7 12:06:49 2003
From: grassi at psico.univ.trieste.it (Michele Grassi)
Date: Thu, 7 Aug 2003 12:06:49 +0200 (MEST)
Subject: [R] Select intervals of values from a variables.
Message-ID: <200308071006.MAA06448@server.psico.univ.trieste.it>

Hi.
How can i select interval of values from a variables?
I can ordinate my variable and use es.group1<-varx[1:12]
if the limit values are linked to first and 12th 
observation.
There is an easier way?
Thank you.



From ripley at stats.ox.ac.uk  Thu Aug  7 12:31:18 2003
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Thu, 7 Aug 2003 11:31:18 +0100 (BST)
Subject: [R] Strange predicted values ?
In-Reply-To: <3F322305.1070904@deprem.gov.tr>
Message-ID: <Pine.LNX.4.44.0308071128100.24990-100000@gannet.stats>

On Thu, 7 Aug 2003, orkun wrote:

[quoting me without attribution]

> >Those are not predicted values, they are fitted values.  Try predicting on 
> >the same set of variables as you printed.

Precisely!  From ?predict.glm

 newdata: optionally, a new data frame from which to make the
          predictions.  If omitted, the fitted linear predictors are
          used.

[...]

> If  predict(glm.obj,type="resp") does not give predicted vals, How can I 
> get predicted values ?

Try reading the help page?  It is quite explicit, and has examples, as do 
all good books on S/R.

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From ripley at stats.ox.ac.uk  Thu Aug  7 12:32:50 2003
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Thu, 7 Aug 2003 11:32:50 +0100 (BST)
Subject: [R] Select intervals of values from a variables.
In-Reply-To: <200308071006.MAA06448@server.psico.univ.trieste.it>
Message-ID: <Pine.LNX.4.44.0308071131480.24990-100000@gannet.stats>

Is

varx[varx > lower.limit & varx < upper.limit]

what you are looking for?

On Thu, 7 Aug 2003, Michele Grassi wrote:

> How can i select interval of values from a variables?
> I can ordinate my variable and use es.group1<-varx[1:12]
> if the limit values are linked to first and 12th 
> observation.

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From d.bayard at pmodwrc.ch  Thu Aug  7 12:39:13 2003
From: d.bayard at pmodwrc.ch (Daniel Bayard)
Date: Thu, 07 Aug 2003 12:39:13 +0200
Subject: [R] Re: www.acd.ucar.edu mailing list memberships reminder
In-Reply-To: <20030801110835.17857.36539.Mailman@web1.acd.ucar.edu>
References: <20030801110835.17857.36539.Mailman@web1.acd.ucar.edu>
Message-ID: <3F322C51.6020108@pmodwrc.ch>

how do you add x and y error bars on a plot.

thanks

daniel



From glaziou at pasteur-kh.org  Thu Aug  7 13:10:34 2003
From: glaziou at pasteur-kh.org (Philippe Glaziou)
Date: Thu, 7 Aug 2003 18:10:34 +0700
Subject: [R] Re: www.acd.ucar.edu mailing list memberships reminder
In-Reply-To: <3F322C51.6020108@pmodwrc.ch>
References: <20030801110835.17857.36539.Mailman@web1.acd.ucar.edu>
	<3F322C51.6020108@pmodwrc.ch>
Message-ID: <20030807111033.GA2213@pasteur-kh.org>

Daniel Bayard <d.bayard at pmodwrc.ch> wrote:
> how do you add x and y error bars on a plot.

Have queried the search engine?

help.search("error bar")

-- 
Philippe Glaziou
Pasteur Institute of Cambodia



From Luis.Tito-de-Morais at ird.sn  Thu Aug  7 13:20:48 2003
From: Luis.Tito-de-Morais at ird.sn (Tito de Morais Luis)
Date: Thu, 07 Aug 2003 11:20:48 -0000
Subject: [R] graph for selected lines in stars()
Message-ID: <1060247508.22677.25.camel@rap06.ird.sn>

Dear listers,

The following command (derived from the example in the ?stars help page)
works :

data(mtcars)
stars(mtcars[, 1:7])

But the following gives an error:
stars(mtcars[1, 1:7])
Error in s.y[i, ] : incorrect number of dimensions

I was expecting to have the star graph for the first line (Mazda Rx4)

The following give an incorrect graph for the first two cars :
stars(mtcars[1:2, 1:7])

The following gives the correct graphs for all cars:
stars(mtcars[1:32, 1:7])

How can I have correct graphs for a selection of lines ?

I am using R 1.7.1 on linux Mandrake 9.1

Thank you for your help

Tito


> version
         _
platform i686-pc-linux-gnu
arch     i686
os       linux-gnu
system   i686, linux-gnu
status
major    1
minor    7.1
year     2003
month    06
day      16
language R

-- 
L. Tito de Morais
      UR RAP
   IRD de Dakar
      BP 1386
       Dakar
      S?n?gal

T?l.: + 221 849 33 31
Fax: +221 832 16 75
Courriel: tito at ird.sn



From glaziou at pasteur-kh.org  Thu Aug  7 13:24:07 2003
From: glaziou at pasteur-kh.org (Philippe Glaziou)
Date: Thu, 7 Aug 2003 18:24:07 +0700
Subject: [R] Re: www.acd.ucar.edu mailing list memberships reminder
In-Reply-To: <20030807111033.GA2213@pasteur-kh.org>
References: <20030801110835.17857.36539.Mailman@web1.acd.ucar.edu>
	<3F322C51.6020108@pmodwrc.ch>
	<20030807111033.GA2213@pasteur-kh.org>
Message-ID: <20030807112407.GB2213@pasteur-kh.org>

I <glaziou at pasteur-kh.org> wrote:
> Daniel Bayard <d.bayard at pmodwrc.ch> wrote:
> > how do you add x and y error bars on a plot.
> 
> Have queried the search engine?
> 
> help.search("error bar")

Well, I would have been more helpful if I had indicated that
error bars are dealt with in the Hmisc and gregmisc
libraries. 

-- 
Philippe



From angel_lul at hotmail.com  Thu Aug  7 13:32:40 2003
From: angel_lul at hotmail.com (Angel)
Date: Thu, 7 Aug 2003 13:32:40 +0200
Subject: [R] ginv vs. solve
Message-ID: <Law11-OE16cOrBLSAQV00016bc4@hotmail.com>

Why do
x<-b%*%ginv(A)
and
x<-solve(A,b)
give different results?. It seems that I am missing some basic feature of
matrix indexing.
e.g.:

A<-matrix(c(0,-4,4,0),nrow=2,ncol=2)
b<-c(-16,0)
x<-b%*%ginv(A);x
x<-solve(A,b);x

Thanks in advance,
Angel



From ligges at statistik.uni-dortmund.de  Thu Aug  7 13:41:43 2003
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Thu, 07 Aug 2003 13:41:43 +0200
Subject: [R] ginv vs. solve
In-Reply-To: <Law11-OE16cOrBLSAQV00016bc4@hotmail.com>
References: <Law11-OE16cOrBLSAQV00016bc4@hotmail.com>
Message-ID: <3F323AF7.40702@statistik.uni-dortmund.de>

Angel wrote:

> Why do
> x<-b%*%ginv(A)

Why should it be the same???
Here, you are calculating  (A+ is G-Inverse)

   x = b * A+

> and
> x<-solve(A,b)

Here, you are calculating (A- is Inverse of A)

   A * x = b  <=> A- * A * x = A- * b <=>
   x = A- * b

So you have to compare, e.g.:

  ginv(A) %*% b
  solve(A, b)

Uwe Ligges


> give different results?. It seems that I am missing some basic feature of
> matrix indexing.
> e.g.:
> 
> A<-matrix(c(0,-4,4,0),nrow=2,ncol=2)
> b<-c(-16,0)
> x<-b%*%ginv(A);x
> x<-solve(A,b);x
> 
> Thanks in advance,
> Angel
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help



From Torsten.Hothorn at rzmail.uni-erlangen.de  Thu Aug  7 13:44:27 2003
From: Torsten.Hothorn at rzmail.uni-erlangen.de (Torsten Hothorn)
Date: Thu, 7 Aug 2003 13:44:27 +0200 (CEST)
Subject: [R] ginv vs. solve
In-Reply-To: <Law11-OE16cOrBLSAQV00016bc4@hotmail.com>
References: <Law11-OE16cOrBLSAQV00016bc4@hotmail.com>
Message-ID: <Pine.LNX.4.51.0308071341370.16367@artemis.imbe.med.uni-erlangen.de>

> Why do
> x<-b%*%ginv(A)
> and
> x<-solve(A,b)
> give different results?.

they do (in cases the solution to A x = b is unique):

R> A <- matrix(c(0,-4,4,0),nrow=2,ncol=2)
R> A
     [,1] [,2]
[1,]    0    4
[2,]   -4    0
R> b <- c(-16,0)
R> x1 <- ginv(A) %*% b		<- NOT b %*% ginv(A)
R> x1
     [,1]
[1,]    0
[2,]   -4
R> x2 <- solve(A) %*% b
R> x2
     [,1]
[1,]    0
[2,]   -4
R> x3 <- solve(A, b)
R> x3
[1]  0 -4

Torsten


> It seems that I am missing some basic feature of
> matrix indexing.
> e.g.:
>
> A<-matrix(c(0,-4,4,0),nrow=2,ncol=2)
> b<-c(-16,0)
> x<-b%*%ginv(A);x
> x<-solve(A,b);x
>
> Thanks in advance,
> Angel
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
>
>



From ligges at statistik.uni-dortmund.de  Thu Aug  7 13:49:46 2003
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Thu, 07 Aug 2003 13:49:46 +0200
Subject: [R] ginv vs. solve
In-Reply-To: <3F323AF7.40702@statistik.uni-dortmund.de>
References: <Law11-OE16cOrBLSAQV00016bc4@hotmail.com>
	<3F323AF7.40702@statistik.uni-dortmund.de>
Message-ID: <3F323CDA.6000903@statistik.uni-dortmund.de>

Uwe Ligges wrote:

> Angel wrote:
> 
>> Why do
>> x<-b%*%ginv(A)
>

> Why should it be the same???
> Here, you are calculating  (A+ is G-Inverse)
> 
>   x = b * A+

Let me add: More exactly, you are calculating

x = b' * A+

because b * A+ doesn't fit and R is somehow "intelligent" here and 
transposes b for you instead of complaining ...

Uwe

>> and
>> x<-solve(A,b)
> 
> 
> Here, you are calculating (A- is Inverse of A)
> 
>   A * x = b  <=> A- * A * x = A- * b <=>
>   x = A- * b
> 
> So you have to compare, e.g.:
> 
>  ginv(A) %*% b
>  solve(A, b)
> 
> Uwe Ligges
> 
> 
>> give different results?. It seems that I am missing some basic feature of
>> matrix indexing.
>> e.g.:
>>
>> A<-matrix(c(0,-4,4,0),nrow=2,ncol=2)
>> b<-c(-16,0)
>> x<-b%*%ginv(A);x
>> x<-solve(A,b);x
>>
>> Thanks in advance,
>> Angel
>>
>> ______________________________________________
>> R-help at stat.math.ethz.ch mailing list
>> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> 
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help



From p.dalgaard at biostat.ku.dk  Thu Aug  7 13:49:57 2003
From: p.dalgaard at biostat.ku.dk (Peter Dalgaard BSA)
Date: Thu, 07 Aug 2003 11:49:57 -0000
Subject: [R] graph for selected lines in stars()
In-Reply-To: <1060247508.22677.25.camel@rap06.ird.sn>
References: <1060247508.22677.25.camel@rap06.ird.sn>
Message-ID: <x2r83xvfzo.fsf@biostat.ku.dk>

Tito de Morais Luis <Luis.Tito-de-Morais at ird.sn> writes:

> Dear listers,
> 
> The following command (derived from the example in the ?stars help page)
> works :
> 
> data(mtcars)
> stars(mtcars[, 1:7])
> 
> But the following gives an error:
> stars(mtcars[1, 1:7])
> Error in s.y[i, ] : incorrect number of dimensions
> 
> I was expecting to have the star graph for the first line (Mazda Rx4)

I don't think that makes sense. Star graphs are only defined for an
ensemble of rowss, at least with the default setting of 'scale=TRUE'.
 
> The following give an incorrect graph for the first two cars :
> stars(mtcars[1:2, 1:7])

Looks correct to me. 1st two models are identical on five variables,
and the remaining two are scaled to [0,1], which in this case means
that you get two zeros for the first car and two ones for the other.


> The following gives the correct graphs for all cars:
> stars(mtcars[1:32, 1:7])
> 
> How can I have correct graphs for a selection of lines ?

First define "correct". You can use scale=FALSE and do the scaling
yourself in some way that is independent of the subset chosen.

-- 
   O__  ---- Peter Dalgaard             Blegdamsvej 3  
  c/ /'_ --- Dept. of Biostatistics     2200 Cph. N   
 (*) \(*) -- University of Copenhagen   Denmark      Ph: (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)             FAX: (+45) 35327907



From p.dalgaard at biostat.ku.dk  Thu Aug  7 13:57:26 2003
From: p.dalgaard at biostat.ku.dk (Peter Dalgaard BSA)
Date: Thu, 07 Aug 2003 11:57:26 -0000
Subject: [R] ginv vs. solve
In-Reply-To: <Law11-OE16cOrBLSAQV00016bc4@hotmail.com>
References: <Law11-OE16cOrBLSAQV00016bc4@hotmail.com>
Message-ID: <x2n0elvfmc.fsf@biostat.ku.dk>

"Angel" <angel_lul at hotmail.com> writes:

> Why do
> x<-b%*%ginv(A)
> and
> x<-solve(A,b)
> give different results?. It seems that I am missing some basic feature of
> matrix indexing.
> e.g.:
> 
> A<-matrix(c(0,-4,4,0),nrow=2,ncol=2)
> b<-c(-16,0)
> x<-b%*%ginv(A);x
> x<-solve(A,b);x

[ginv() is from MASS, please remember to tell us]

Your b is on the wrong side, try ginv(A)%*%b (possibly put it within
drop() to convert it to vector).

 

-- 
   O__  ---- Peter Dalgaard             Blegdamsvej 3  
  c/ /'_ --- Dept. of Biostatistics     2200 Cph. N   
 (*) \(*) -- University of Copenhagen   Denmark      Ph: (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)             FAX: (+45) 35327907



From jfox at mcmaster.ca  Thu Aug  7 15:09:30 2003
From: jfox at mcmaster.ca (John Fox)
Date: Thu, 07 Aug 2003 09:09:30 -0400
Subject: [R] new version of Rcmdr package
Message-ID: <5.1.0.14.2.20030807090230.01fd9e58@127.0.0.1>

Dear list members,

I've uploaded a new version of the Rcmdr package to CRAN. There are many 
additions and (I hope) improvements, as indicated in the relevant portion 
of the CHANGES file, reproduced below. As usual, comments, suggestions, and 
bug reports are appreciated.

John

-------------------------------------------

Version 0.8-4

     o Minor bug fixes and additional input-error checks (many suggested by 
Tony Christensen).

     o Documentation for Recode, Compute, linearModel, and 
generalizedLinearModel dialogs. More extensive documentation for the Commander.

     o Changes to list*() utility functions to ensure that all objects of a 
given class are found, and not masked by local variables or objects in the 
base package.

     o Many small changes to ensure that objects in the global environment 
are not masked by objects in the base package.

     o Added Graphs -> Line graph.

Version 0.9-0

     o More flexible customization. All config files are now in the etc 
subdirectory, including:

       - Rcmdr-menus.txt: defines the Rcmdr menus

       - log-exceptions.txt: a list of functions that are prevented from 
printing output when executed from the log window.

       - compareModels.demo: contains a demonstration of how to add a 
dialog to Rcmdr. Rename this file to compareModels.R and uncomment the 
corresponding menu-definition line in Rcmdr-menus.txt.

     o Some more functions in the package are exported to support writing 
extensions.

     o Made active-data-set and active-model "information fields" into 
buttons that can be used to select the active data set and model.

     o Added "Subset active data set", "Remove cases with missing data", 
and "Set case names" to Data -> Active data set menu.

     o Added many error checks for adequate variables in the active data set.

     o Ensured that focus returns to the proper window after an error.

     o Commands generated by View and Edit Data Set buttons now appear in 
the log and sessions windows.

     o As an option, bound double-left-mouse-click to default button in all 
dialogs.

     o Added more sophisticated interface for linearModel and 
generalizedLinearModel dialogs (suggested by Bob Stine).

     o Where appropriate, previous field-values are retained in linearModel 
and generalizedLinearModel dialogs (suggested by Sandy Weisberg). Models 
are numbered.

     o Generalized-linear models dialog now only shows appropriate links 
for the currently selected family.

     o Added "Data -> New data set" menu, which opens data editor with 
empty data frame.

     o Much improved stem-and-leaf display using stem.leaf function 
(courtesy of Peter Wolf).

     o Added "grab.focus" option to allow disabling focus grabs on systems 
where this causes problem.

     o Added "File -> Options" menu to set options and restart the Commander.

     o On exit, the Commander cleans up global variables used for program 
control (from correspondence with Kjetil Brinchmann Halvorsen).

     o Added distribution plots to the Distributions menu (suggested by Yan 
Wong).

     o Small interface changes and bug fixes.



From edd at debian.org  Thu Aug  7 15:22:58 2003
From: edd at debian.org (Dirk Eddelbuettel)
Date: Thu, 7 Aug 2003 08:22:58 -0500
Subject: [R] Plot ticks and tick labels: thickness, colour?
In-Reply-To: <Pine.LNX.4.44.0308070711260.24546-100000@gannet.stats>
References: <Pine.SOL.4.44.0308061841150.962-100000@mspacman.gpcc.itd.umich.edu>
	<Pine.LNX.4.44.0308070711260.24546-100000@gannet.stats>
Message-ID: <20030807132258.GA13900@sonny.eddelbuettel.com>

On Thu, Aug 07, 2003 at 07:24:23AM +0100, Prof Brian Ripley wrote:
> I think there is no disagreement in the help, but your experience does
> disagree with my experiments.
> 
>     font: font for text.
> 
>      col: color for the axis line and the tick marks.  The default
>           'NULL' means to use 'par("fg")'.
> 
>      'col.axis' The color to be used for axis annotation.
> 
>      'col.lab' The color to be used for x and y labels.
> 
>     'font.axis' The font to be used for axis annotation.
> 
> Try
> 
> > plot(1:10, col.axis="gold", col.lab="blue", font.axis=3)
> > axis(side=4, at=1:10, col="green", col.axis="gold", col.lab="blue", 
>        font.axis=3, font=2)
> 
> So when calling axis, the colour of the line and the tick marks is set by 
> "col" and that of the labels by "col.axis" and the xlab and ylab by 
> "col.lab".  Three separate components, three colour settings.

Ah, very nice. So all I needed was col.axis. Now that I think about it, I
guess I knew it was mentioned in ?par.
 
> As for fonts, "font" rules when supplied to axis, otherwise "font.axis" is 
> used.
> 
> I'll add a note to axis.Rd.

Very good.

Thanks, Dirk

-- 
Those are my principles, and if you don't like them... well, I have others.
                                                -- Groucho Marx



From marcos.llobera at prm.ox.ac.uk  Thu Aug  7 15:41:51 2003
From: marcos.llobera at prm.ox.ac.uk (Marcos Llobera)
Date: Thu, 7 Aug 2003 14:41:51 +0100
Subject: [R] Newbie
Message-ID: <001c01c35ce9$aba860e0$bd594e98@arch.soton.ac.uk>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20030807/610da995/attachment.pl

From tord.snall at ebc.uu.se  Thu Aug  7 15:46:45 2003
From: tord.snall at ebc.uu.se (Tord Snall)
Date: Thu, 07 Aug 2003 15:46:45 +0200
Subject: [R] optim() error message
Message-ID: <3.0.6.32.20030807154645.00dc5fc8@mail.anst.uu.se>

Dear all,

I've worked with optim before but never encountered this error message:

  Nelder-Mead direct search function minimizer
0.23 0Error: subscript out of bounds

The error seems to depend on the initial parameter values. However,
strangely (I think), I recieve this error message when I have given very
good initial values - the values which were returned when I gave worse
values. 

An example: 

I first give the initial values: a=0.1, g=0

> fit.lognorm$conv
[1] 0
> fit.lognorm$par 
[1] 0.2322998 0.1873682

When plotting the deviance (minimized) against the fixed parameter values, 
a  <-  seq(0, 1, l=11)
g <- seq(-0.5, 0.5, l=11)
grid <- expand.grid(a, g)

the found 
[1] 0.2322998 0.1873682 
are very reasonable.

I then thus run the funciton again with a=0.23, g=0.18,
and recieve the message:

  Nelder-Mead direct search function minimizer
0.23 0Error: subscript out of bounds

I've tried increasing maxit (10000), read previous optim function mails and
looked in FAQ, but this problem has not been mentioned. 

Thanks in advance for help!


Sincerely,
Tord








  Nelder-Mead direct search function minimizer
0.23 0Error: subscript out of bounds


-----------------------------------------------------------------------
Tord Sn?ll
Avd. f v?xtekologi, Evolutionsbiologiskt centrum, Uppsala universitet
Dept. of Plant Ecology, Evolutionary Biology Centre, Uppsala University
Villav?gen 14			
SE-752 36 Uppsala, Sweden
Tel: 018-471 28 82 (int +46 18 471 28 82) (work)
Tel: 018-25 71 33 (int +46 18 25 71 33) (home)
Fax: 018-55 34 19 (int +46 18 55 34 19) (work)
E-mail: Tord.Snall at ebc.uu.se
Check this: http://www.vaxtbio.uu.se/resfold/snall.htm!



From temiz at deprem.gov.tr  Thu Aug  7 16:23:50 2003
From: temiz at deprem.gov.tr (orkun)
Date: Thu, 07 Aug 2003 17:23:50 +0300
Subject: [R] Strange predicted values ?
In-Reply-To: <Pine.LNX.4.44.0308071128100.24990-100000@gannet.stats>
References: <Pine.LNX.4.44.0308071128100.24990-100000@gannet.stats>
Message-ID: <3F3260F6.9080205@deprem.gov.tr>

Prof Brian Ripley wrote:

>On Thu, 7 Aug 2003, orkun wrote:
>
>[quoting me without attribution]
>
>  
>
>>>Those are not predicted values, they are fitted values.  Try predicting on 
>>>the same set of variables as you printed.
>>>      
>>>
>
>Precisely!  From ?predict.glm
>
> newdata: optionally, a new data frame from which to make the
>          predictions.  If omitted, the fitted linear predictors are
>          used.
>
>[...]
>
>  
>
>>If  predict(glm.obj,type="resp") does not give predicted vals, How can I 
>>get predicted values ?
>>    
>>
>
>Try reading the help page?  It is quite explicit, and has examples, as do 
>all good books on S/R.
>
>  
>
I tried this:
#I think since an interaction exists in glm.obj, data.frame.obj did not 
not work
#instead I used model.frame obj (it works)
newdata<-model.frame(glm.obj)
pr<-predict.glm(glm.obj,newdata,type="resp")
it works but there was a warning message:
prediction from a rank deficient fit may be misleading in predic.lm (....)

any suggestions ?

thank you again


______________________________________



______________________________________
The views and opinions expressed in this e-mail message are ...{{dropped}}



From mail at fwr.on.ca  Thu Aug  7 16:35:39 2003
From: mail at fwr.on.ca (Bruce LaZerte)
Date: Thu, 7 Aug 2003 10:35:39 -0400
Subject: [R] predict(lm(etc.),
	some_data) -> "numeric envir arg not of length one " ???
Message-ID: <E19klrg-0002AN-00@server.family>

I've got a data frame with two numeric variables, df$flow and df$flow1.

> tl <- lm(flow~flow1,df,na.action=na.exclude)
> tlo <- loess(flow~flow1,df,na.action=na.exclude)

Both loess and a simple linear model fit the data well. 
summary(tl) and summary(tlo) seem reasonable. As do plots such as: 
plot(predict(tl),df$flow)
plot(predict(tlo),df$flow)

I want to replace missing values of  df$flow from df$flow1:

> p <- is.na(df$flow) 
> df$flow[p] <- predict(tl,df$flow1[p])
Error in eval(expr, envir, enclos) : numeric envir arg not of length one
???

> df$flow[p]<- predict(tlo,df$flow1[p])
This loess prediction works however.

Could someone explain the linear model's "numeric envir arg not of length one" error for me?

Thanks in advance ...
Bruce L.
__________________________________________
Bruce LaZerte 
Grandview Lake in Muskoka
Baysville, Ontario, Canada



From ripley at stats.ox.ac.uk  Thu Aug  7 17:23:24 2003
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Thu, 7 Aug 2003 16:23:24 +0100 (BST)
Subject: [R] Strange predicted values ?
In-Reply-To: <3F3260F6.9080205@deprem.gov.tr>
Message-ID: <Pine.LNX.4.44.0308071619260.31575-100000@gannet.stats>

Let me remind you that originally you did not understand why the fitted
values did not match up with some other set of values of var1 to var4 you
cbind-ed.  You need to predict at those values for what *you* did not to
be `strange'.

Please do as I suggested and consult the documentation. 


On Thu, 7 Aug 2003, orkun wrote:

> Prof Brian Ripley wrote:
> 
> >On Thu, 7 Aug 2003, orkun wrote:
> >
> >[quoting me without attribution]
> >
> >  
> >
> >>>Those are not predicted values, they are fitted values.  Try predicting on 
> >>>the same set of variables as you printed.
> >>>      
> >>>
> >
> >Precisely!  From ?predict.glm
> >
> > newdata: optionally, a new data frame from which to make the
> >          predictions.  If omitted, the fitted linear predictors are
> >          used.
> >
> >[...]
> >
> >  
> >
> >>If  predict(glm.obj,type="resp") does not give predicted vals, How can I 
> >>get predicted values ?
> >>    
> >>
> >
> >Try reading the help page?  It is quite explicit, and has examples, as do 
> >all good books on S/R.
> >
> >  
> >
> I tried this:
> #I think since an interaction exists in glm.obj, data.frame.obj did not 
> not work
> #instead I used model.frame obj (it works)
> newdata<-model.frame(glm.obj)
> pr<-predict.glm(glm.obj,newdata,type="resp")
> it works but there was a warning message:
> prediction from a rank deficient fit may be misleading in predic.lm (....)
> 
> any suggestions ?

Follow advice when it is given to you.

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From crmora_nc at yahoo.com  Thu Aug  7 17:28:09 2003
From: crmora_nc at yahoo.com (Christian Mora)
Date: Thu, 7 Aug 2003 08:28:09 -0700 (PDT)
Subject: [R] Lattice graphs and legend
Message-ID: <20030807152809.50579.qmail@web20210.mail.yahoo.com>

Dear all,

I'm working with grouped data using the nlme package
in R. By using the function plot() I'm trying to
obtain a trellis-like graph but by default a legend is
placed on top of the plot. How can I remove this
legend?

Thanks for any hint

Christian Mora

__________________________________

Yahoo! SiteBuilder - Free, easy-to-use web site design software



From bates at stat.wisc.edu  Thu Aug  7 17:33:45 2003
From: bates at stat.wisc.edu (Douglas Bates)
Date: Thu, 07 Aug 2003 15:33:45 -0000
Subject: [R] gls function
In-Reply-To: <000801c35813$d4f69aa0$0200a8c0@elassa1>
References: <000801c35813$d4f69aa0$0200a8c0@elassa1>
Message-ID: <6rwudp1nyy.fsf@bates4.stat.wisc.edu>

"Prodromos Zanis" <pzanis at geol.uoa.gr> writes:

> I use the gls function but in contrast to the lm function in which
> when I type summary(lm(...))$coef I receive all the coefficients
> (estimate, Std. Error, t-value and pvalue), with gls when I type
> summary(gls(...))$coef I only receive the estimate of the
> reg. coefficient without std. error and t- and p-values.

> Do you have any suggestion how to solve my problem?

Look at the structure of the value returned by summary applied to a
gls object.

library(nlme)
example(gls)
str(summary(fm1))

You will find that you want to extract

summary(fm1)$tTable



From mtolvieira at msn.com  Thu Aug  7 17:47:37 2003
From: mtolvieira at msn.com (Marcel Vieira)
Date: Thu, 07 Aug 2003 16:47:37 +0100
Subject: [R] correlation matrix
Message-ID: <BAY5-F19swvvV62vXhO0000c424@hotmail.com>

Dear all,

I have T Y variables, Y1,...,YT, which are T repeated observations on a 
variable over the T waves of a survey.

I'd like to estimate a correlation matrix cor(Y1,...,YT) assuming specific 
structures, as for example exchangeable, stationary, autoregressive and 
nonstationary.

Is there any command/package in R that I could use?

Thanks in advance.
Marcel

_________________________________________________________________
Express yourself with cool emoticons - download MSN Messenger today!



From grassi at psico.univ.trieste.it  Thu Aug  7 17:36:41 2003
From: grassi at psico.univ.trieste.it (Michele Grassi)
Date: Thu, 7 Aug 2003 17:36:41 +0200 (MEST)
Subject: [R] Link two variables
Message-ID: <200308071536.RAA09530@server.psico.univ.trieste.it>

Hi.
How can i link two variables, so when i group the first 
i group also the second one?
I try whit create a model matrix (model.matrix) and 
use  a grouping function like "balancedGrouped(nlme)".
More specific i have varx:AGE and vary(binomial):o
(dead),1(alive). I want to create age group,link vary 
group, compute the mean of x link the mean of y.
Thank you.
Michele.



From apiszcz at solarrain.com  Thu Aug  7 18:07:54 2003
From: apiszcz at solarrain.com (Al Piszcz)
Date: Thu, 7 Aug 2003 12:07:54 -0400 (EDT)
Subject: [R] abline() plot order
Message-ID: <Pine.LNX.4.55.0308071205400.6144@l1>


I am performing this sequence

barplot
title
legend
abline

When abline renders the lines they appare to be in the layer
above the bars in the graph. Is there a way to make them
render first or 'behind' the bars?

Thanks.



From John.Fieberg at dnr.state.mn.us  Thu Aug  7 18:22:06 2003
From: John.Fieberg at dnr.state.mn.us (John Fieberg)
Date: Thu, 07 Aug 2003 11:22:06 -0500
Subject: [R] Extended Mantel-Haenszel Tests
Message-ID: <sf3236aa.003@co5.dnr.state.mn.us>

Has anyone written R code for extended Mantel-Haenszel Tests- as
described in Section 8.4.5 (eq. 8.18) of Agresti's book "Categorical
Data Analysis"?

In other words, I would like to use column scores to test for a
location shift for different groups as defined by the row variable in
a
set of rxs tables.  In addition, I would like to use row and column
scores to test for a linear association between row and column
variables.  For SAS users, these correspond to the "row mean score
test"
and "nonzero correlation tests" available in Proc Freq.

Thanks!

John

John Fieberg, Ph.D.
Wildlife Biometrician, MN DNR
5463-C W. Broadway
Forest Lake, MN 55434
Phone: (651) 296-2704



From Luis.Tito-de-Morais at ird.sn  Thu Aug  7 18:30:43 2003
From: Luis.Tito-de-Morais at ird.sn (Tito de Morais Luis)
Date: Thu, 07 Aug 2003 16:30:43 -0000
Subject: [R] size of a pixmap image
Message-ID: <1060266191.22677.49.camel@rap06.ird.sn>

Dear listers,

I loaded a pixmap image with:
brobo <- read.pnm("brobo.pnm")

I can have the characteristics of the image with:
> brobo
Pixmap image
  Type          : pixmapRGB
  Size          : 609x682
  Resolution    : 1x1
  Bounding box  : 0 0 682 609

How can I retrieve the image size ?
nrow(brobo) and ncol(brobo) give NULL

I succeeded using:

> nrow(getChannels(brobo))
[1] 609
> ncol(getChannels(brobo))
[1] 682

It looks a bit tricky, is there another way ?

Thanks in advance,

Tito

Running Mandrake 9.1

> version
         _
platform i686-pc-linux-gnu
arch     i686
os       linux-gnu
system   i686, linux-gnu
status
major    1
minor    7.1
year     2003
month    06
day      16
language R


-- 
L. Tito de Morais
      UR RAP
   IRD de Dakar
      BP 1386
       Dakar
      S?n?gal

T?l.: + 221 849 33 31
Fax: +221 832 16 75
Courriel: tito at ird.sn



From MSchwartz at medanalytics.com  Thu Aug  7 18:41:20 2003
From: MSchwartz at medanalytics.com (Marc Schwartz)
Date: Thu, 07 Aug 2003 11:41:20 -0500
Subject: [R] abline() plot order
In-Reply-To: <Pine.LNX.4.55.0308071205400.6144@l1>
References: <Pine.LNX.4.55.0308071205400.6144@l1>
Message-ID: <1060274480.23708.45.camel@localhost>

On Thu, 2003-08-07 at 11:07, Al Piszcz wrote:
> I am performing this sequence
> 
> barplot
> title
> legend
> abline
> 
> When abline renders the lines they appare to be in the layer
> above the bars in the graph. Is there a way to make them
> render first or 'behind' the bars?
> 
> Thanks.


Are you using abline() for a single line or for a grid?

barplot2() in the 'gregmisc' package on CRAN can help with either
scenario.

If you want a grid behind the bars, use 'plot.grid = TRUE' in barplot2.
The default will draw thin dotted lines behind the bars at each axis
tick mark. You can specify the line type and width for the grid as well.

Example:

barplot2(1:5, plot.grid = TRUE)

If you want a single line (ie. a benchmark), you could use barplot2()
the first time to draw the chart, use abline() to draw your single line
and then use barplot2() again, with the 'add = TRUE' argument, which
would re-draw the bars over the existing plot.

Example:

barplot2(1:5)
abline(h = 2)
barplot2(1:5, add = TRUE)


An alternative to using barplot2() would be to use par(new = TRUE) as
well. This will enable you to add a new plot to the existing figure.

First case:

barplot(1:5)
abline(h = axTicks(2), lty = "dotted")
par(new = TRUE)
barplot(1:5)


Second case:

barplot(1:5)
abline(h = 2)
par(new = TRUE)
barplot(1:5)

See ?axTicks and ?par for additional information.

I'll make a note to myself to add an option to barplot2 for a single
line that can be added behind the bars. Pretty easy fix.

HTH,

Marc Schwartz



From bates at stat.wisc.edu  Thu Aug  7 18:41:25 2003
From: bates at stat.wisc.edu (Douglas Bates)
Date: Thu, 07 Aug 2003 16:41:25 -0000
Subject: [R] newbie question: fitting power law
In-Reply-To: <3F2FBBAD.5020102@giub.uni-bonn.de>
References: <3F2FA9B5.6000805@giub.uni-bonn.de>
	<1060088824.3f2fabf8e6faa@my.uq.edu.au>
	<3F2FBBAD.5020102@giub.uni-bonn.de>
Message-ID: <6rsmod1ktw.fsf@bates4.stat.wisc.edu>

Thomas Hoffmann <hoffmann at giub.uni-bonn.de> writes:

> Dear R-helpers, dear Andrew
> 
> Andrew C. Ward schrieb:
> 
> >Dear Thomas,
> >
> > I wonder if you have any NAs in xy$x or xy$y.
> 
> The Dataframe looks like the following:
>  > xy
>    x          y
> 1 3.0  121.50951
> 2 2.0   30.61258
> 3 4.0  323.14837
> 4 8.2 3709.92471
> For testing reasons I calculated y by:
>  > y <- 2.9 x^3.4

Did you read the help page for nls that emphasizes

     *Do not use 'nls' on artificial "zero-residual" data.*

and explains why?

In any case, the best way to fit such a model to these data is to use
the plinear algorithm for partially linear models.  Either

> nls(y ~ x^exp(logpow), data=xy, start = c(logpow=0.1), alg='plinear',trace = TRUE)
2661838 :   0.1000 281.0589 
87270.38 :  0.9550582 15.4392528 
1573.086 : 1.181199 3.903272 
1.645442 : 1.222351 2.929585 
2.301169e-06 : 1.223774 2.900035 
1.483770e-11 : 1.223775 2.900000 
1.483769e-11 : 1.223775 2.900000 
Nonlinear regression model
  model:  y ~ x^exp(logpow) 
   data:  xy 
  logpow     .lin 
1.223775 2.900000 
 residual sum-of-squares:  1.483769e-11 

or

> nls(y ~ exp(log(x) * pow), data=xy, start = c(pow=1.1), alg = 'plinear', trace = TRUE)
2684530 :   1.1000 283.5010 
408828.2 :  2.045211 47.930710 
33333.73 : 2.848788 9.185464 
857.465 : 3.293871 3.622750 
1.307911 : 3.395685 2.926367 
3.723389e-06 : 3.399993 2.900044 
1.483772e-11 : 3.4 2.9 
1.483769e-11 : 3.4 2.9 
Nonlinear regression model
  model:  y ~ exp(log(x) * pow) 
   data:  xy 
 pow .lin 
 3.4  2.9 
 residual sum-of-squares:  1.483769e-11 

> >Also, I think
> >you could take logs of your equation and end up with a
> >linear expression?
> >
> if I use:
> 
> plot (x,y)
> abline lm ( log(y)~log(x),xy)
> 
> the line does not seem to fit the plotted datapoints at all. While the
> fitted exponent seems to be okay, the obtained intercept value is
> wrong.

That's because your plot is in the x,y scale and the line is in the
log(x), log(y) scale.  You need to convert back to the original scale
to put the line on the plot.

-- 
Douglas Bates                            bates at stat.wisc.edu
Statistics Department                    608/262-2598
University of Wisconsin - Madison        http://www.stat.wisc.edu/~bates/



From apiszcz at solarrain.com  Thu Aug  7 19:01:08 2003
From: apiszcz at solarrain.com (Al Piszcz)
Date: Thu, 7 Aug 2003 13:01:08 -0400 (EDT)
Subject: [R] abline() plot order
In-Reply-To: <1060274480.23708.45.camel@localhost>
References: <Pine.LNX.4.55.0308071205400.6144@l1>
	<1060274480.23708.45.camel@localhost>
Message-ID: <Pine.LNX.4.55.0308071257210.6144@l1>


Marc:

I am interested in having only 'y' or 'x' axis tick
bars light gray across graph.

I did not realize there was a barplot2, thank you.

I used your second technique (add=TRUE) and it works!
Thank you very much for the speed of reply and quality
of the options.

The plot looks very good with the lines behind the bars!




I've been look at par and axTicks, and found everything
I need except control of the Y tickmarks, major and minor.
It is probably in there I just have not located it.




On Thu, 7 Aug 2003,
Marc Schwartz wrote:

> Date: Thu, 07 Aug 2003 11:41:20 -0500
> From: Marc Schwartz <MSchwartz at MedAnalytics.com>
> To: Al Piszcz <apiszcz at solarrain.com>
> Cc: r-help at stat.math.ethz.ch
> Subject: Re: [R] abline() plot order
>
> On Thu, 2003-08-07 at 11:07, Al Piszcz wrote:
> > I am performing this sequence
> >
> > barplot
> > title
> > legend
> > abline
> >
> > When abline renders the lines they appare to be in the layer
> > above the bars in the graph. Is there a way to make them
> > render first or 'behind' the bars?
> >
> > Thanks.
>
>
> Are you using abline() for a single line or for a grid?
>
> barplot2() in the 'gregmisc' package on CRAN can help with either
> scenario.
>
> If you want a grid behind the bars, use 'plot.grid = TRUE' in barplot2.
> The default will draw thin dotted lines behind the bars at each axis
> tick mark. You can specify the line type and width for the grid as well.
>
> Example:
>
> barplot2(1:5, plot.grid = TRUE)
>
> If you want a single line (ie. a benchmark), you could use barplot2()
> the first time to draw the chart, use abline() to draw your single line
> and then use barplot2() again, with the 'add = TRUE' argument, which
> would re-draw the bars over the existing plot.
>
> Example:
>
> barplot2(1:5)
> abline(h = 2)
> barplot2(1:5, add = TRUE)
>
>
> An alternative to using barplot2() would be to use par(new = TRUE) as
> well. This will enable you to add a new plot to the existing figure.
>
> First case:
>
> barplot(1:5)
> abline(h = axTicks(2), lty = "dotted")
> par(new = TRUE)
> barplot(1:5)
>
>
> Second case:
>
> barplot(1:5)
> abline(h = 2)
> par(new = TRUE)
> barplot(1:5)
>
> See ?axTicks and ?par for additional information.
>
> I'll make a note to myself to add an option to barplot2 for a single
> line that can be added behind the bars. Pretty easy fix.
>
> HTH,
>
> Marc Schwartz
>
>



From sigma at consultoresestadisticos.com  Thu Aug  7 19:20:09 2003
From: sigma at consultoresestadisticos.com (Carlos J. Gil Bellosta)
Date: Thu, 07 Aug 2003 12:20:09 -0500
Subject: [R] Statistical analysis of huge datasets.
Message-ID: <3F328A49.1010804@consultoresestadisticos.com>

Dear R-users,

I am faced with the problem of analyzing a huge dataset (+ 2 million 
records, +150 variables) which does not fit into memory. I would like to 
know if there are pre-packaged tools (in the spirit of Insigthful 
I-Miner, for instance) aimed at subsampling or splitting the dataset 
into data-frameable subdatasets, applying functions record-wise, etc.

Thank you very much for your help.

Carlos J. Gil Bellosta
Sigma Consultores Estad?sticos
http://www.consultoresestadisticos.com



From MSchwartz at medanalytics.com  Thu Aug  7 19:30:52 2003
From: MSchwartz at medanalytics.com (Marc Schwartz)
Date: Thu, 07 Aug 2003 12:30:52 -0500
Subject: [R] abline() plot order
In-Reply-To: <Pine.LNX.4.55.0308071257210.6144@l1>
References: <Pine.LNX.4.55.0308071205400.6144@l1>
	<1060274480.23708.45.camel@localhost>
	<Pine.LNX.4.55.0308071257210.6144@l1>
Message-ID: <1060277452.23708.65.camel@localhost>

On Thu, 2003-08-07 at 12:01, Al Piszcz wrote:
> Marc:
> 
> I am interested in having only 'y' or 'x' axis tick
> bars light gray across graph.
> 
> I did not realize there was a barplot2, thank you.
> 
> I used your second technique (add=TRUE) and it works!
> Thank you very much for the speed of reply and quality
> of the options.
> 
> The plot looks very good with the lines behind the bars!
> 
> 
> 
> 
> I've been look at par and axTicks, and found everything
> I need except control of the Y tickmarks, major and minor.
> It is probably in there I just have not located it.


Al,

Your most flexible option on the tick mark locations is to use axis(),
which allows you to specify the locations. Use 'axes = FALSE' in
conjunction with barplot().  See ?axis for more information.

Example:

# draw the barplot without axes
barplot(1:5, axes = FALSE)

# Now draw the 'minor' tick marks as short lines
# and do not plot any labels
axis(2, at = seq(0, 5, 0.5), lwd = 1, tck = -0.01, 
     labels = FALSE)

# Now draw the 'major' tick marks with slight longer and thicker
# lines with labels
axis(2, at = 0:5, lwd = 2, tck = -0.015)

# Now draw the grid lines at the major tick marks in gray
# If you want the lines at the 'minor' tick marks 
# use: abline(h = seq(0, 5, 0.5), lty = "solid", col = "gray90")
# adjust the line type and color as you desire. See ?colors
abline(h = 0:5, lty = "solid", col = "gray90")

# Now replot the bars so they are over the lines
par(new = TRUE)
barplot(1:5, axes = FALSE)

HTH,

Marc



From TyagiAnupam at aol.com  Thu Aug  7 19:39:24 2003
From: TyagiAnupam at aol.com (TyagiAnupam@aol.com)
Date: Thu, 7 Aug 2003 13:39:24 EDT
Subject: [R] Statistical analysis of huge datasets.
Message-ID: <16b.22562d1f.2c63e8cc@aol.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20030807/507ac086/attachment.pl

From mail at joeconway.com  Thu Aug  7 19:56:30 2003
From: mail at joeconway.com (Joe Conway)
Date: Thu, 07 Aug 2003 10:56:30 -0700
Subject: [R] Statistical analysis of huge datasets.
In-Reply-To: <16b.22562d1f.2c63e8cc@aol.com>
References: <16b.22562d1f.2c63e8cc@aol.com>
Message-ID: <3F3292CE.7050602@joeconway.com>

TyagiAnupam at aol.com wrote:
> One possibility is to use a DBMS like MySQL or Postgresql, and RODBC to 
> connect to these. Search the archives for previous postings about these, have a 
> look at the first R-Newsletter and at Data Import-Export manual.
> 

If you use PostgreSQL, you might want to try PL/R; see:
   http://www.joeconway.com/plr/

It allows your R functions to run inside the backend database process, 
minimizing data I/O.

HTH,

Joe



From tobias_verbeke at skynet.be  Thu Aug  7 20:07:24 2003
From: tobias_verbeke at skynet.be (Tobias Verbeke)
Date: Thu, 7 Aug 2003 20:07:24 +0200
Subject: [R] Newbie
In-Reply-To: <001c01c35ce9$aba860e0$bd594e98@arch.soton.ac.uk>
References: <001c01c35ce9$aba860e0$bd594e98@arch.soton.ac.uk>
Message-ID: <20030807200724.078a0684.tobias_verbeke@skynet.be>

On Thu, 7 Aug 2003 14:41:51 +0100
"Marcos Llobera" <marcos.llobera at prm.ox.ac.uk> wrote:

> I just installed R's RPM on a Linux box using red hat 8.0. I would
> like to use some graphical interface (e.g. gnome) but I have not
> managed to do it. 

type R at the prompt in a gnome terminal and
you can use R.

> 

There is the GUI Rcmdr (see message of Sir John
Fox preceding your message), a package you should
install. Install packages car and foreign first, 
because Rcmdr needs them.
(see the R documentation on CRAN on how to install 
a package)
 
If you installed the packages successfully,
type library(Rcmdr) at the R prompt and you
will have a GUI.

HTH,

Tobias

PS using the command line increases speed
and flexibility IMHO



From apiszcz at solarrain.com  Thu Aug  7 20:05:18 2003
From: apiszcz at solarrain.com (Al Piszcz)
Date: Thu, 7 Aug 2003 14:05:18 -0400 (EDT)
Subject: [R] abline() plot order
In-Reply-To: <1060277452.23708.65.camel@localhost>
References: <Pine.LNX.4.55.0308071205400.6144@l1>
	<1060274480.23708.45.camel@localhost>
	<Pine.LNX.4.55.0308071257210.6144@l1>
	<1060277452.23708.65.camel@localhost>
Message-ID: <Pine.LNX.4.55.0308071404360.7311@l1>

you have been very helpful, thank you, this works fine.




On Thu, 7 Aug
2003, Marc Schwartz wrote:

> Date: Thu, 07 Aug 2003 12:30:52 -0500
> From: Marc Schwartz <MSchwartz at MedAnalytics.com>
> To: Al Piszcz <apiszcz at solarrain.com>
> Cc: r-help at stat.math.ethz.ch
> Subject: Re: [R] abline() plot order
>
> On Thu, 2003-08-07 at 12:01, Al Piszcz wrote:
> > Marc:
> >
> > I am interested in having only 'y' or 'x' axis tick
> > bars light gray across graph.
> >
> > I did not realize there was a barplot2, thank you.
> >
> > I used your second technique (add=TRUE) and it works!
> > Thank you very much for the speed of reply and quality
> > of the options.
> >
> > The plot looks very good with the lines behind the bars!
> >
> >
> >
> >
> > I've been look at par and axTicks, and found everything
> > I need except control of the Y tickmarks, major and minor.
> > It is probably in there I just have not located it.
>
>
> Al,
>
> Your most flexible option on the tick mark locations is to use axis(),
> which allows you to specify the locations. Use 'axes = FALSE' in
> conjunction with barplot().  See ?axis for more information.
>
> Example:
>
> # draw the barplot without axes
> barplot(1:5, axes = FALSE)
>
> # Now draw the 'minor' tick marks as short lines
> # and do not plot any labels
> axis(2, at = seq(0, 5, 0.5), lwd = 1, tck = -0.01,
>      labels = FALSE)
>
> # Now draw the 'major' tick marks with slight longer and thicker
> # lines with labels
> axis(2, at = 0:5, lwd = 2, tck = -0.015)
>
> # Now draw the grid lines at the major tick marks in gray
> # If you want the lines at the 'minor' tick marks
> # use: abline(h = seq(0, 5, 0.5), lty = "solid", col = "gray90")
> # adjust the line type and color as you desire. See ?colors
> abline(h = 0:5, lty = "solid", col = "gray90")
>
> # Now replot the bars so they are over the lines
> par(new = TRUE)
> barplot(1:5, axes = FALSE)
>
> HTH,
>
> Marc
>
>



From Roger.Bivand at nhh.no  Thu Aug  7 20:26:50 2003
From: Roger.Bivand at nhh.no (Roger Bivand)
Date: Thu, 7 Aug 2003 20:26:50 +0200 (CEST)
Subject: [R] size of a pixmap image
In-Reply-To: <1060266191.22677.49.camel@rap06.ird.sn>
Message-ID: <Pine.LNX.4.44.0308072017050.2759-100000@reclus.nhh.no>

On 7 Aug 2003, Tito de Morais Luis wrote:

> Dear listers,
> 
> I loaded a pixmap image with:
> brobo <- read.pnm("brobo.pnm")
> 
> I can have the characteristics of the image with:
> > brobo
> Pixmap image
>   Type          : pixmapRGB
>   Size          : 609x682
>   Resolution    : 1x1
>   Bounding box  : 0 0 682 609
> 
> How can I retrieve the image size ?
> nrow(brobo) and ncol(brobo) give NULL

> getClass("pixmap")

Slots:
                                      
Name:     size cellres    bbox  bbcent
Class: integer numeric numeric logical

Known Subclasses: "pixmapChannels", "pixmapIndexed", "pixmapGrey", 
"pixmapRGB"

brobo at size, to get the equivalent of dim(), nrow is brobo at size[1], ncol 
brobo at size[2], or:

setGeneric("nrow", function(x) standardGeneric("nrow"))
setMethod("nrow", "pixmap", function(x) { x at size[1] })

gets you there.

Roger

> 
> I succeeded using:
> 
> > nrow(getChannels(brobo))
> [1] 609
> > ncol(getChannels(brobo))
> [1] 682
> 
> It looks a bit tricky, is there another way ?
> 
> Thanks in advance,
> 
> Tito
> 
> Running Mandrake 9.1
> 
> > version
>          _
> platform i686-pc-linux-gnu
> arch     i686
> os       linux-gnu
> system   i686, linux-gnu
> status
> major    1
> minor    7.1
> year     2003
> month    06
> day      16
> language R
> 
> 
> 

-- 
Roger Bivand
Economic Geography Section, Department of Economics, Norwegian School of
Economics and Business Administration, Breiviksveien 40, N-5045 Bergen,
Norway. voice: +47 55 95 93 55; fax +47 55 95 93 93
e-mail: Roger.Bivand at nhh.no



From atuya at xd5.so-net.ne.jp  Thu Aug  7 20:46:10 2003
From: atuya at xd5.so-net.ne.jp (atsuya fujito)
Date: Thu, 7 Aug 2003 13:46:10 -0500
Subject: [R] gregmisc
Message-ID: <69AFE183-C907-11D7-87A3-003065F9618A@xd5.so-net.ne.jp>

Hi

How do I install "gregmisc" packages?
I did-
% sudo R
 > install.packages("gregmisc")
.
.
 > barplot2()
but,
Error: couldn't find function "barplot2"

--
atuya
Mac OSX 10.2.6
R 1.7.1



From zeileis at ci.tuwien.ac.at  Thu Aug  7 20:52:23 2003
From: zeileis at ci.tuwien.ac.at (Achim Zeileis)
Date: Thu, 7 Aug 2003 20:52:23 +0200
Subject: [R] gregmisc
In-Reply-To: <69AFE183-C907-11D7-87A3-003065F9618A@xd5.so-net.ne.jp>
References: <69AFE183-C907-11D7-87A3-003065F9618A@xd5.so-net.ne.jp>
Message-ID: <200308071852.h77IqNqf006679@thorin.ci.tuwien.ac.at>

On Thursday 07 August 2003 20:46, atsuya fujito wrote:

> Hi
>
> How do I install "gregmisc" packages?
> I did-
> % sudo R
>
>  > install.packages("gregmisc")
>
> .
> .
>
>  > barplot2()
>
> but,
> Error: couldn't find function "barplot2"

I guess you forgot to say

  library(gregmisc)

maybe it's a good idea to take a look at "An Introduction to R"...
Z



From kwan022 at stat.auckland.ac.nz  Thu Aug  7 20:54:35 2003
From: kwan022 at stat.auckland.ac.nz (Ko-Kang Kevin Wang)
Date: Fri, 8 Aug 2003 06:54:35 +1200 (NZST)
Subject: [R] gregmisc
In-Reply-To: <69AFE183-C907-11D7-87A3-003065F9618A@xd5.so-net.ne.jp>
Message-ID: <Pine.LNX.4.44.0308080654100.32068-100000@stat55.stat.auckland.ac.nz>

Assuming you have installed the package, have you done:
  library(gregmisc)
before you call barplot2?

On Thu, 7 Aug 2003, atsuya fujito wrote:

> Date: Thu, 7 Aug 2003 13:46:10 -0500
> From: atsuya fujito <atuya at xd5.so-net.ne.jp>
> To: r-help at stat.math.ethz.ch
> Subject: [R] gregmisc
> 
> Hi
> 
> How do I install "gregmisc" packages?
> I did-
> % sudo R
>  > install.packages("gregmisc")
> .
> .
>  > barplot2()
> but,
> Error: couldn't find function "barplot2"
> 
> --
> atuya
> Mac OSX 10.2.6
> R 1.7.1
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> 

-- 
Cheers,

Kevin

------------------------------------------------------------------------------
"On two occasions, I have been asked [by members of Parliament],
'Pray, Mr. Babbage, if you put into the machine wrong figures, will
the right answers come out?' I am not able to rightly apprehend the
kind of confusion of ideas that could provoke such a question."

-- Charles Babbage (1791-1871) 
---- From Computer Stupidities: http://rinkworks.com/stupid/

--
Ko-Kang Kevin Wang
Master of Science (MSc) Student
SLC Tutor and Lab Demonstrator
Department of Statistics
University of Auckland
New Zealand
Homepage: http://www.stat.auckland.ac.nz/~kwan022
Ph: 373-7599
    x88475 (City)
    x88480 (Tamaki)



From MSchwartz at medanalytics.com  Thu Aug  7 20:58:26 2003
From: MSchwartz at medanalytics.com (Marc Schwartz)
Date: Thu, 07 Aug 2003 13:58:26 -0500
Subject: [R] gregmisc
In-Reply-To: <69AFE183-C907-11D7-87A3-003065F9618A@xd5.so-net.ne.jp>
References: <69AFE183-C907-11D7-87A3-003065F9618A@xd5.so-net.ne.jp>
Message-ID: <1060282705.23708.104.camel@localhost>

On Thu, 2003-08-07 at 13:46, atsuya fujito wrote:
> Hi
> 
> How do I install "gregmisc" packages?
> I did-
> % sudo R
>  > install.packages("gregmisc")
> .
> .
>  > barplot2()
> but,
> Error: couldn't find function "barplot2"
> 
> --
> atuya
> Mac OSX 10.2.6
> R 1.7.1


Assuming that you did not get any errors from the installation of the
package, you then need to use:

library(gregmisc)

to be able to use the functions in the package.

You might want to review "5. R Add-On Packages" in the main R FAQ.
(http://cran.r-project.org/doc/FAQ/R-FAQ.html#R%20Add-On%20Packages)

HTH,

Marc Schwartz



From crmora_nc at yahoo.com  Thu Aug  7 22:05:24 2003
From: crmora_nc at yahoo.com (Christian Mora)
Date: Thu, 7 Aug 2003 13:05:24 -0700 (PDT)
Subject: [R] Question about 'NA'
Message-ID: <20030807200524.58547.qmail@web20203.mail.yahoo.com>

Hi all,

Ive got a database with 10 columns (different
variables) for 100 subjects, each column with
different # of NA's. I'd like to know if it is
possible to use a function to exclude the NA's using
only a specific column, lets say:

Data2 <- omit.exclude(Data1$column1) ??, then
Data3 <- omit.exclude(Data1$column2) and so on 

I tried the code above but with no results

Thanks for any help

CM

__________________________________

Yahoo! SiteBuilder - Free, easy-to-use web site design software



From jasont at indigoindustrial.co.nz  Thu Aug  7 22:29:18 2003
From: jasont at indigoindustrial.co.nz (Jason Turner)
Date: Fri, 08 Aug 2003 08:29:18 +1200
Subject: [R] Question about 'NA'
In-Reply-To: <20030807200524.58547.qmail@web20203.mail.yahoo.com>
References: <20030807200524.58547.qmail@web20203.mail.yahoo.com>
Message-ID: <3F32B69E.5070103@indigoindustrial.co.nz>

Christian Mora wrote:
> Hi all,
> 
> Ive got a database with 10 columns (different
> variables) for 100 subjects, each column with
> different # of NA's. I'd like to know if it is
> possible to use a function to exclude the NA's using
> only a specific column, lets say:
> 
> Data2 <- omit.exclude(Data1$column1) ??, then
> Data3 <- omit.exclude(Data1$column2) and so on 
> 

I use indexing for that.

Data2 <- Data1[!is.na(Data1$column1),]

nb - don't use this:

# WRONG WRONG WRONG!
Data2 <- Data1[! (Data1$column1 == NA),]

NA means you don't know.  Therefore, it doesn't equal anything, 
including NA.  For example, I don't know your birthday, and I don't know 
Napoleon's birthday.  That doesn't mean you two have the same birthday, 
even though they'd both be represented as NA.

Cheers

Jason
-- 
Indigo Industrial Controls Ltd.
64-21-343-545
jasont at indigoindustrial.co.nz



From jerome at hivnet.ubc.ca  Thu Aug  7 22:22:46 2003
From: jerome at hivnet.ubc.ca (Jerome Asselin)
Date: Thu, 7 Aug 2003 13:22:46 -0700
Subject: [R] Question about 'NA'
In-Reply-To: <20030807200524.58547.qmail@web20203.mail.yahoo.com>
References: <20030807200524.58547.qmail@web20203.mail.yahoo.com>
Message-ID: <200308072029.NAA04353@hivnet.ubc.ca>


dat <- data.frame(x=c(1,NA,2,2),y=c(3,2,NA,1))
dat
dat[rownames(na.omit(dat[,"y",drop=F])),]

> dat <- data.frame(x=c(1,NA,2,2),y=c(3,2,NA,1))
> dat
   x  y
1  1  3
2 NA  2
3  2 NA
4  2  1
> dat[rownames(na.omit(dat[,"y",drop=F])),]
   x y
1  1 3
2 NA 2
4  2 1

HTH,
Jerome

On August 7, 2003 01:05 pm, Christian Mora wrote:
> Hi all,
>
> Ive got a database with 10 columns (different
> variables) for 100 subjects, each column with
> different # of NA's. I'd like to know if it is
> possible to use a function to exclude the NA's using
> only a specific column, lets say:
>
> Data2 <- omit.exclude(Data1$column1) ??, then
> Data3 <- omit.exclude(Data1$column2) and so on
>
> I tried the code above but with no results
>
> Thanks for any help
>
> CM
>
> __________________________________
>
> Yahoo! SiteBuilder - Free, easy-to-use web site design software
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help



From lamac_k at hotmail.com  Thu Aug  7 23:46:43 2003
From: lamac_k at hotmail.com (lamack lamack)
Date: Thu, 07 Aug 2003 21:46:43 +0000
Subject: [R] ratio of means of two independent
Message-ID: <BAY7-F117n5U9T3TGJ20000d7f8@hotmail.com>

  Dear all, How can I do inference about a ratio of means of two dependent 
popupations?

Best regards.

L



From rnaidoo at ualberta.ca  Fri Aug  8 00:03:55 2003
From: rnaidoo at ualberta.ca (rnaidoo)
Date: Thu, 7 Aug 2003 16:03:55 -0600
Subject: [R] spdep error message
Message-ID: <3F362614@webmail.ualberta.ca>

Hello,
    
I have been using the package "spdep" to run spatial regressions on a data set 
with about 2500 observations.  It has performed well up until now, but the 
following code resulted in an error:

> load("Panel.90s.ok.R")
> attach(Panel.90s.ok)
> neighs<-dnearneigh(cbind(x,y),0,50000)
> help(nbdists)
> dists<-nbdists(neighs,cbind(x,y))
> Weights<-nb2listw(neighs,zero.policy=T,glist=dists)
> 
error.model.rprice<-errorsarlm(log(wells+1)~log(year)+log(area)+log(lag.rprice
)+log(marketable+1)+log(prov.rds+1)+log(ind.rds+1)+log(seismic+1)+log(pipeline
s+1)+log(min.year)+log(max.year)+log(avg.year),listw=Weights,zero.policy=T)
Error in eigen(w, only.values = TRUE) : error code 2311 from Lapack routine 
dgeev
    
The problem seems to be with the inclusion of the Euclidean distances 
("dists") 
in the spatial weights matrix, as regressions using weights matrices without a 
glist worked fine.  I tried switching the method used in errorsarlm to 
"eigenw" 
but this also resulted in an error:
    
Error in switch(method, eigen = if (!quiet) cat("neighbourhood matrix 
eigenvalues\n"),  :
        ...
    
Unknown method
    
Similarly, switching methods to "sparse" also resulted in an error:
    
Error in spwdet(sparseweights, rho = rho, debug = debug) :
        Suspicious allocation of memory in sparse functions
        ...bailing out! Save workspace and quit R!
    
Any help on how to incorporate distance measures into the spatial weights 
matrix for the errorsarlm/lagsarlm spatial regression commands would be 
greatly 
appreciated!  Hopefully I haven't overlooked something very obvious.
    
Regards,
    
Robin Naidoo
Department of Rural Economy
University of Alberta
Canada



From atuya at xd5.so-net.ne.jp  Fri Aug  8 00:56:56 2003
From: atuya at xd5.so-net.ne.jp (atsuya fujito)
Date: Thu, 7 Aug 2003 17:56:56 -0500
Subject: [R] gregmisc
In-Reply-To: <Pine.LNX.4.44.0308071153050.32018-100000@jerboa.fhcrc.org>
Message-ID: <71A68C54-C92A-11D7-A834-003065F9618A@xd5.so-net.ne.jp>


I did library

 > library(gregmisc)
Error in library(MASS) : There is no package called 'MASS'
Error in library(gregmisc) : .First.lib failed
 >

What is wrong?



> You have to *load* the package too, not just install it.
>
> 'library(gregmisc)' will load it
>
>
> On Thu, 7 Aug 2003, atsuya fujito wrote:
>
>> Hi
>>
>> How do I install "gregmisc" packages?
>> I did-
>> % sudo R
>>> install.packages("gregmisc")
>> .
>> .
>>> barplot2()
>> but,
>> Error: couldn't find function "barplot2"
>>
>> --
>> atuya
>> Mac OSX 10.2.6
>> R 1.7.1
>>
>> ______________________________________________
>> R-help at stat.math.ethz.ch mailing list
>> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
>>
>
>



From atuya at xd5.so-net.ne.jp  Fri Aug  8 01:42:55 2003
From: atuya at xd5.so-net.ne.jp (atsuya fujito)
Date: Thu, 7 Aug 2003 18:42:55 -0500
Subject: [R] gregmisc
Message-ID: <DE565BD9-C930-11D7-A834-003065F9618A@xd5.so-net.ne.jp>

Thank you, Marc;

I installed "car", "R commander" and "Bioconductor".
I am using R 1.7.1, on MAC OSX 10.2.6

 > library()
Packages in library '/usr/local/lib/R/library':

AnnBuilder              Bioconductor annotation data package builder
Biobase                 Biobase: Base functions for Bioconductor
DynDoc                  Dynamic document tools
RBGL                    Test interface to boost C++ graph lib
ROC                     utilities for ROC, with uarray focus
Rcmdr                   R Commander
Ruuid                   Ruuid: Provides Universally Unique ID values
SAGElyzer               A package that deals with SAGE libraries
affy                    Methods for Affymetrix Oligonucleotide Arrays
affycomp                Graphics Toolbox for Assessment of Affymetrix
                         Expression Measures
affydata                Affymetrix Data for Demonstration Purpose
annotate                Annotation for microarrays
base                    The R base package
car                     Companion to Applied Regression
ctest                   Classical Tests
eda                     Exploratory Data Analysis
edd                     expression density diagnostics
genefilter              Genefilter: filter genes
geneplotter             Geneplotter: plot microarray data
golubEsets              exprSets for golub leukemia data
graph                   graph: A package to handle graph data
                         structures
gregmisc                Greg's Miscellaneous Functions
grid                    The Grid Graphics Package
lattice                 Lattice Graphics
limma                   Linear Models for Microarray Data
lqs                     Resistant Regression and Covariance Estimation
makecdfenv              CDF Environment Maker
marrayClasses           Classes and methods for cDNA microarray data
marrayInput             Data input for cDNA microarrays
marrayNorm              Location and scale normalization for cDNA
                         microarray data
marrayPlots             Diagnostic plots for cDNA microarray data
Packages in library '/usr/local/lib/R/library':

AnnBuilder              Bioconductor annotation data package builder
Biobase                 Biobase: Base functions for Bioconductor
DynDoc                  Dynamic document tools
RBGL                    Test interface to boost C++ graph lib
ROC                     utilities for ROC, with uarray focus
Rcmdr                   R Commander
Ruuid                   Ruuid: Provides Universally Unique ID values
SAGElyzer               A package that deals with SAGE libraries
affy                    Methods for Affymetrix Oligonucleotide Arrays
affycomp                Graphics Toolbox for Assessment of Affymetrix
                         Expression Measures
affydata                Affymetrix Data for Demonstration Purpose
annotate                Annotation for microarrays
base                    The R base package
car                     Companion to Applied Regression
ctest                   Classical Tests
eda                     Exploratory Data Analysis
edd                     expression density diagnostics
genefilter              Genefilter: filter genes
geneplotter             Geneplotter: plot microarray data
golubEsets              exprSets for golub leukemia data
graph                   graph: A package to handle graph data
                         structures
gregmisc                Greg's Miscellaneous Functions
grid                    The Grid Graphics Package
lattice                 Lattice Graphics
limma                   Linear Models for Microarray Data
lqs                     Resistant Regression and Covariance Estimation
makecdfenv              CDF Environment Maker
marrayClasses           Classes and methods for cDNA microarray data
marrayInput             Data input for cDNA microarrays
marrayNorm              Location and scale normalization for cDNA
                         microarray data
marrayPlots             Diagnostic plots for cDNA microarray data
marrayTools             Miscellaneous functions for cDNA microarrays
methods                 Formal Methods and Classes
modreg                  Modern Regression: Smoothing and Local Methods
multtest                Multiple Testing Procedures
mva                     Classical Multivariate Analysis
nlme                    Linear and nonlinear mixed effects models
nls                     Nonlinear regression
reposTools              Repository tools for R
sma                     Statistical Microarray Analysis
splines                 Regression Spline Functions and Classes
stepfun                 Step Functions, including Empirical
                         Distributions
survival                Survival analysis, including penalised
                         likelihood.
tcltk                   Tcl/Tk Interface
tkWidgets               R based tk widgets
tools                   Tools for package development
ts                      Time series functions
vsn                     Variance stabilization and calibration for
                         microarray data
widgetTools             Creates an interactive tcltk widget
xtable                  Export tables to LaTeX or HTML



From Matthew.Kelly at csiro.au  Fri Aug  8 03:25:37 2003
From: Matthew.Kelly at csiro.au (Matthew.Kelly@csiro.au)
Date: Fri, 8 Aug 2003 11:25:37 +1000
Subject: [R] R and RMySQL - segmentation fault
Message-ID: <39B7E7009F2DD840AA747934AB551E69184762@exnsw1-arm.nsw.csiro.au>

Hi,

I have the same problem and have attempted to look through the list and find a reply with the answer but could not seem to find one.

My problem seems slightly more confusing. If I and running R as root, the following works

> library(RMySQL)
> m <- dbDriver("MySQL")
> con <- dbConnect(m,host="acomputer",dbname="atable",user="au_ser",password="a_password")
> rs <- dbSendQuery(con,statement="select * from flock")
> data <- fetch(rs,n=-1)

If I run the same code as a normal user the following line fails with a segmentation fault
con <- dbConnect(m,host="acomputer",dbname="atable",user="au_ser",password="a_password")

I have the following relevant software installed
  MySql 3.23.32 
  R-1.7.0
  RMySQL 0.5.2
Redhat 7.0



From MSchwartz at medanalytics.com  Fri Aug  8 04:10:44 2003
From: MSchwartz at medanalytics.com (Marc Schwartz)
Date: Thu, 07 Aug 2003 21:10:44 -0500
Subject: [R] gregmisc
In-Reply-To: <DE565BD9-C930-11D7-A834-003065F9618A@xd5.so-net.ne.jp>
References: <DE565BD9-C930-11D7-A834-003065F9618A@xd5.so-net.ne.jp>
Message-ID: <1060308643.23708.294.camel@localhost>

On Thu, 2003-08-07 at 18:42, atsuya fujito wrote:
> Thank you, Marc;
> 
> I installed "car", "R commander" and "Bioconductor".
> I am using R 1.7.1, on MAC OSX 10.2.6

[Long listing of packages snipped]

Atsuya,

Thanks for the listing. If that is the complete list, then you are
missing the "VR" bundle of packages, which includes MASS and is
typically part of the 'recommended' package bundle, at least under
Windows and Linux/Unix.

I did a search of the Mac FAQ (**which appears to be for 1.6.0**), the R
Admin manual and the r-help archive, since I do not have "hands on"
experience with Macs. I may have to defer to other Mac OSX users for
detailed guidance here. 

I also just checked Jan de Leeuw's web site at UCLA
(http://gifi.stat.ucla.edu/pub/index.php) based upon a review of the
README at http://cran.r-project.org/bin/macosx/ReadMe.txt. The
indication is that the Mac port for V1.7.0 includes the base and
recommended packages, however V1.7.1 is the base package set only, if my
read is correct.

If that is indeed the case, the solution to your problem may be as
simple as using:

install.packages("VR")

being sure that you have appropriate R/W access permissions to the
library directory tree on your system when you use that R command.

If that solves the problem, then you should be ok and can load
'gregmisc' after VR is installed.

I am copying both Jan de Leeuw and Stefano Iacus on this communication
to be sure that I am offering appropriate advice here. Given the various
time zones involved here, hopefully someone can expediently confirm the
resolution to your problem.

I hope this helps.

Regards,

Marc Schwartz



From spencer.graves at pdf.com  Fri Aug  8 04:10:25 2003
From: spencer.graves at pdf.com (Spencer Graves)
Date: Thu, 07 Aug 2003 19:10:25 -0700
Subject: [R] ratio of means of two independent
References: <BAY7-F117n5U9T3TGJ20000d7f8@hotmail.com>
Message-ID: <3F330691.1070603@pdf.com>

First:  Have you made probability plots (starting with qqnorm) of both 
variables plus a scatterplot?  First characterize the joint distribution 
-- plus the distribtion of the ratio.  Then we can discuss inference 
about that ratio.

hope this helps.  spencer graves

lamack lamack wrote:
>  Dear all, How can I do inference about a ratio of means of two 
> dependent popupations?
> 
> Best regards.
> 
> L
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help



From jago at mclink.it  Fri Aug  8 07:26:41 2003
From: jago at mclink.it (Stefano Iacus)
Date: Fri, 8 Aug 2003 07:26:41 +0200
Subject: [R] gregmisc
In-Reply-To: <1060308643.23708.294.camel@localhost>
Message-ID: <E4239344-C960-11D7-9909-003065CC4CB8@mclink.it>

In Carbon R 1.7.1 recommnded packages are already installed. VR itself 
is a bundle of package and not a package itself.
MASS, rpart, spatial and nnet are already present in Carbon R.

BTW, From 1.8.0 MacOS X users can take advange of the new (forthcoming) 
RAqua (http://www.economia.unimi.it/R/ for a preview)

stefano

On Venerd?, ago 8, 2003, at 04:10 Europe/Rome, Marc Schwartz wrote:

> On Thu, 2003-08-07 at 18:42, atsuya fujito wrote:
>> Thank you, Marc;
>>
>> I installed "car", "R commander" and "Bioconductor".
>> I am using R 1.7.1, on MAC OSX 10.2.6
>
> [Long listing of packages snipped]
>
> Atsuya,
>
> Thanks for the listing. If that is the complete list, then you are
> missing the "VR" bundle of packages, which includes MASS and is
> typically part of the 'recommended' package bundle, at least under
> Windows and Linux/Unix.
>
> I did a search of the Mac FAQ (**which appears to be for 1.6.0**), the 
> R
> Admin manual and the r-help archive, since I do not have "hands on"
> experience with Macs. I may have to defer to other Mac OSX users for
> detailed guidance here.
>
> I also just checked Jan de Leeuw's web site at UCLA
> (http://gifi.stat.ucla.edu/pub/index.php) based upon a review of the
> README at http://cran.r-project.org/bin/macosx/ReadMe.txt. The
> indication is that the Mac port for V1.7.0 includes the base and
> recommended packages, however V1.7.1 is the base package set only, if 
> my
> read is correct.
>
> If that is indeed the case, the solution to your problem may be as
> simple as using:
>
> install.packages("VR")
>
> being sure that you have appropriate R/W access permissions to the
> library directory tree on your system when you use that R command.
>
> If that solves the problem, then you should be ok and can load
> 'gregmisc' after VR is installed.
>
> I am copying both Jan de Leeuw and Stefano Iacus on this communication
> to be sure that I am offering appropriate advice here. Given the 
> various
> time zones involved here, hopefully someone can expediently confirm the
> resolution to your problem.
>
> I hope this helps.
>
> Regards,
>
> Marc Schwartz
>
>



From akahn1 at gmu.edu  Fri Aug  8 09:20:44 2003
From: akahn1 at gmu.edu (Ari Kahn)
Date: Fri, 08 Aug 2003 03:20:44 -0400
Subject: [R] RSPerl
Message-ID: <BB58C78C.10010%akahn1@gmu.edu>

After installing RSPerl from Omegahat I tried to run the tests.  The first
test is "test.pl" in /usr/lib/R/library/RSPerl/examples on Redhat Linux 8.

For some reason perl can't load R.  Has anyone had this behavior?  Any
suggestions?

Thanks,
-- 
Ari
http://damon.ib3.gmu.edu/~kahn
Dreams come true; without that possibility, nature would not incite us to
have them.
John Updike

> cd /usr/lib/R/library/RSPerl/examples/
> perl test.pl
1..1
Can't load '/usr/lib/R/library/RSPerl/share/blib/arch/auto/R/R.so' for
module R: 
/usr/lib/perl5/vendor_perl/5.8.0/i386-linux-thread-multi/auto/ModPerl/Global
/Global.so: undefined symbol: modperl_perl_global_avcv_call at
/usr/lib/perl5/5.8.0/i386-linux-thread-multi/DynaLoader.pm line 229.
 at test.pl line 11
Compilation failed in require at test.pl line 11.
BEGIN failed--compilation aborted at test.pl line 11.
not ok 1
> echo $LD_LIBRARY_PATH
/usr/local/staden_linux/lib/linux-binaries:/usr/lib/R/bin:/usr/lib/R/library
/RSPerl/libs
> echo $PERLLIB
/usr/lib/R/library/RSPerl/share/blib/arch:/usr/lib/R/library/RSPerl/share/bl
ib/lib:/usr/lib/R/library/RSPerl/scripts



From ripley at stats.ox.ac.uk  Fri Aug  8 10:44:42 2003
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Fri, 8 Aug 2003 09:44:42 +0100 (BST)
Subject: [R] demo() function returns alphabetical matches, not unique
	matches?
In-Reply-To: <000801c35c56$d3fa3980$be76f78c@cbrssstu6>
Message-ID: <Pine.LNX.4.44.0308080937130.830-100000@gannet.stats>

Actually, demo matches the `topic' as a regular expression, that is 
anywhere in the file basename and with certain characters being special.
So

demo("^logit")

will probably work (it does work for the system demos).

Clearly that is too sopisticated (and not documented).  For the next 
release I will alter this to accept exact matches only.  If you need this 
now, the code in demo is

        ## files with correct extension
        files <- files[sub(".*\\.", "", files) %in% c("R", "r")]
        ## Files with base names matching topic
        basenames <- sub("\\.[Rr]$", "", files)
        files <- files[topic == basenames]

the last two lines replacing

        files <- files[grep(topic, files)]


On Wed, 6 Aug 2003, Olivia Lau wrote:

> Hi, 
> 
> I'm working on a set of demo files for a package, and I'm having a
> problem because I have two demo files (one called "logit", the other
> called "blogit") and when I type demo(logit), it gives me demo(blogit)
> instead.  (And calls it the the demo for logit.)
> 
> I've figured out that this is because the demo() function searches
> through all the demo directories (in the installed libraries) and looks
> for the string "logit" and takes only the first match in alphabetical
> order [?] (which is "blogit" in this case).
> 
> Is there any way to modify the demo() function so that it looks for the
> exact match, not a match for the string?  I could of course rename the
> demo file, but I'm afraid that students won't remember it (blogit is a
> name of a function as well).
> 
> Thanks in advance, 
> 
> Olivia Lau, 
> "Novice R Programmer"  
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> 

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From angel_lul at hotmail.com  Fri Aug  8 12:46:56 2003
From: angel_lul at hotmail.com (Angel)
Date: Fri, 8 Aug 2003 12:46:56 +0200
Subject: [R] understanding time series objects
Message-ID: <Law11-OE380hXCRQcqm00018331@hotmail.com>

### First, is there a way to access a univariate time series as a matrix
instead of a vector?
# For example:
data(UKLungDeaths)
# If I do
apply(mdeaths,1,cumsum)
# Gives an error as mdeaths is not a matrix but a vector, although when I
look at it :
mdeaths
# the ts object has a matrix like "appearance"
# The only way of doing it I've found is:
mdeaths2<-as.matrix(mdeaths)
dim(mdeaths2)<-c(12,6)
mdeaths2<-apply(mdeaths2,2,cumsum)
mdeaths[]<-mdeaths2
# It is not very efficient to solve the problem of applying a cummulative
sum to each year

### Second, for a multivariate ts:
data(UKLungDeaths)
Multits<-ts.union(mdeaths, fdeaths)
# Why does
Multits$mdeaths
# not work and I have to use:
Multits[,"mdeaths"]
# Is it the way it works or am I missing something?
# Thanks as always,
# Angel



From zeileis at ci.tuwien.ac.at  Fri Aug  8 14:07:45 2003
From: zeileis at ci.tuwien.ac.at (Achim Zeileis)
Date: Fri, 8 Aug 2003 14:07:45 +0200
Subject: [R] understanding time series objects
In-Reply-To: <Law11-OE380hXCRQcqm00018331@hotmail.com>
References: <Law11-OE380hXCRQcqm00018331@hotmail.com>
Message-ID: <200308081207.h78C7ktH028573@thorin.ci.tuwien.ac.at>

On Friday 08 August 2003 12:46, Angel wrote:

> ### First, is there a way to access a univariate time series as a
> matrix instead of a vector?
> # For example:
> data(UKLungDeaths)
> # If I do
> apply(mdeaths,1,cumsum)
> # Gives an error as mdeaths is not a matrix but a vector,

Yes, a single time series in R is essentially a vector plus the time 
series properties. Therefore, it can be accessed like a vector.

> although
> when I look at it :
> mdeaths
> # the ts object has a matrix like "appearance"

That is a print option that can be turned off, look at
  help(print.ts)

> # The only way of doing it I've found is:
> mdeaths2<-as.matrix(mdeaths)
> dim(mdeaths2)<-c(12,6)
> mdeaths2<-apply(mdeaths2,2,cumsum)
> mdeaths[]<-mdeaths2
> # It is not very efficient to solve the problem of applying a
> cummulative sum to each year
>
> ### Second, for a multivariate ts:
> data(UKLungDeaths)
> Multits<-ts.union(mdeaths, fdeaths)
> # Why does
> Multits$mdeaths
> # not work and I have to use:
> Multits[,"mdeaths"]
> # Is it the way it works or am I missing something?

A multivariate time series ("mts" object) is essentially a matrix plus 
time series properties. Therefore, it can be accessed like a matrix 
(and not like a data.frame/list).

hth,
Z


> # Thanks as always,
> # Angel
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help



From jonathan.williams at pharmacology.oxford.ac.uk  Fri Aug  8 19:13:51 2003
From: jonathan.williams at pharmacology.oxford.ac.uk (Jonathan Williams)
Date: Fri, 8 Aug 2003 18:13:51 +0100
Subject: [R] Binary file reading problem
Message-ID: <NGBBKJEMOMLJFCOIEGCECEIOJJAA.jonathan.williams@pharm.ox.ac.uk>

I am trying to read data from a binary file. 
The file has a header of mixed data modes:-

Bytes		Length	Mode
1-4			4	Integer
5-6			2	integer
7-8			2	integer
......
15-18			4	integer
19-20			2	integer
......
29-32			4	integer
33-34			2	integer
35			1	character
36			1	character
37			1	character
38			1	character
39-42			4	integer
43-44			2	integer
......

I cannot find a way to set a file pointer,
so that I can set readBin to read each datum
individually. 

I'd be very grateful for any help with this.

Jonathan Williams

Jonathan Williams
OPTIMA
Radcliffe Infirmary
Woodstock Road
OXFORD OX2 6HE
Tel +1865 (2)24356



From tlumley at u.washington.edu  Fri Aug  8 19:33:47 2003
From: tlumley at u.washington.edu (Thomas Lumley)
Date: Fri, 8 Aug 2003 10:33:47 -0700 (PDT)
Subject: [R] Binary file reading problem
In-Reply-To: <NGBBKJEMOMLJFCOIEGCECEIOJJAA.jonathan.williams@pharm.ox.ac.uk>
Message-ID: <Pine.A41.4.44.0308081033070.57502-100000@homer33.u.washington.edu>

On Fri, 8 Aug 2003, Jonathan Williams wrote:
>
> I cannot find a way to set a file pointer,
> so that I can set readBin to read each datum
> individually.
>

seek()

	-thomas



From andy_liaw at merck.com  Fri Aug  8 21:16:23 2003
From: andy_liaw at merck.com (Liaw, Andy)
Date: Fri, 08 Aug 2003 15:16:23 -0400
Subject: [R] Newbie
Message-ID: <3A822319EB35174CA3714066D590DCD50205C98E@usrymx25.merck.com>

Try typing 

  R --gui=gnome

at a terminal prompt and see if that works for you.  I don't know if the
binary for RedHat has the GNOME support compiled in.

Andy

> -----Original Message-----
> From: Marcos Llobera [mailto:marcos.llobera at prm.ox.ac.uk] 
> Sent: Thursday, August 07, 2003 9:42 AM
> To: R-help at stat.math.ethz.ch
> Subject: [R] Newbie
> 
> 
> I just installed R's RPM on a Linux box using red hat 8.0. I 
> would like to use some graphical interface (e.g. gnome) but I 
> have not managed to do it. 
> 
> Marcos 
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list 
> https://www.stat.math.ethz.ch/mailman/listinfo> /r-help
> 

------------------------------------------------------------------------------
Notice:  This e-mail message, together with any attachments, contains
information of Merck & Co., Inc. (Whitehouse Station, New Jersey, USA), and/or
its affiliates (which may be known outside the United States as Merck Frosst,
Merck Sharp & Dohme or MSD) that may be confidential, proprietary copyrighted
and/or legally privileged, and is intended solely for the use of the
individual or entity named on this message.  If you are not the intended
recipient, and have received this message in error, please immediately return
this by e-mail and then delete it.



From apiszcz at solarrain.com  Fri Aug  8 22:42:44 2003
From: apiszcz at solarrain.com (Al Piszcz)
Date: Fri, 8 Aug 2003 16:42:44 -0400 (EDT)
Subject: [R] Comprehesive Package/Library list?
Message-ID: <Pine.LNX.4.55.0308081635280.18136@l1>


Is there a list of all contributed R libraries available through CRAN?
Ideally it would include a one or two line description.

I am looking for a packages() command similar to library()
but that would access the CRAN repository and provide a
listing of the current libraries, and version.

example:
http://www.cpan.org/modules/01modules.index.html



From baron at psych.upenn.edu  Fri Aug  8 22:57:30 2003
From: baron at psych.upenn.edu (Jonathan Baron)
Date: Fri, 8 Aug 2003 16:57:30 -0400
Subject: [R] Comprehesive Package/Library list?
In-Reply-To: <Pine.LNX.4.55.0308081635280.18136@l1>
References: <Pine.LNX.4.55.0308081635280.18136@l1>
Message-ID: <20030808205730.GA5308@mail1.sas.upenn.edu>

On 08/08/03 16:42, Al Piszcz wrote:
>
>Is there a list of all contributed R libraries available through CRAN?
>Ideally it would include a one or two line description.
>
>I am looking for a packages() command similar to library()
>but that would access the CRAN repository and provide a
>listing of the current libraries, and version.

CRAN.packages()

lists the packages available on CRAN.

And the packages are also listed in the CRAN web pages under
"Source code of contributed packages" with the kind of brief
description that you want.

Another listing is in my R page, with a briefer description.

--
Jonathan Baron, Professor of Psychology, University of Pennsylvania
Home page:            http://www.sas.upenn.edu/~baron
R page:               http://finzi.psych.upenn.edu/



From rossini at blindglobe.net  Fri Aug  8 23:24:10 2003
From: rossini at blindglobe.net (A.J. Rossini)
Date: Fri, 08 Aug 2003 14:24:10 -0700
Subject: [R] Comprehesive Package/Library list?
In-Reply-To: <Pine.LNX.4.55.0308081635280.18136@l1> (Al Piszcz's message of
	"Fri, 8 Aug 2003 16:42:44 -0400 (EDT)")
References: <Pine.LNX.4.55.0308081635280.18136@l1>
Message-ID: <85k79nc079.fsf@blindglobe.net>

Al Piszcz <apiszcz at solarrain.com> writes:

> Is there a list of all contributed R libraries available through CRAN?
> Ideally it would include a one or two line description.
>
> I am looking for a packages() command similar to library()
> but that would access the CRAN repository and provide a
> listing of the current libraries, and version.
>
> example:
> http://www.cpan.org/modules/01modules.index.html

Assuming that you have the tools (html2text), a truly awful and ugly
hack is:

CRAN.packages <- function(tempfile=tempfile("cranpack")) {
    packages <- readLines("http://cran.r-project.org/src/contrib/PACKAGES.html")
    write(packages,file=tempfile)
    system(paste("html2text",tempfile))
}

I won't claim authorship of such a hack.  Don't blame me.  There is a
cleaner way.

best,
-tony

-- 
A.J. Rossini     			
rossini at u.washington.edu            http://www.analytics.washington.edu/ 
Biomedical and Health Informatics   University of Washington
Biostatistics, SCHARP/HVTN          Fred Hutchinson Cancer Research Center
UW   :              FAX=206-543-3461 | moving soon to a permanent office
FHCRC: 206-667-7025 FAX=206-667-4812 | Voicemail is pretty sketchy/use Email

CONFIDENTIALITY NOTICE: This e-mail message and any attachme...{{dropped}}



From apiszcz at solarrain.com  Fri Aug  8 23:37:47 2003
From: apiszcz at solarrain.com (Al Piszcz)
Date: Fri, 8 Aug 2003 17:37:47 -0400 (EDT)
Subject: [R] Comprehesive Package/Library list?
In-Reply-To: <85k79nc079.fsf@blindglobe.net>
References: <Pine.LNX.4.55.0308081635280.18136@l1>
	<85k79nc079.fsf@blindglobe.net>
Message-ID: <Pine.LNX.4.55.0308081737390.18136@l1>

Thanks, this page is fine.
http://cran.us.r-project.org/src/contrib/PACKAGES.html

On Fri, 8 Aug 2003, A.J. Rossini wrote:

> Date: Fri, 08 Aug 2003 14:24:10 -0700
> From: A.J. Rossini <rossini at blindglobe.net>
> To: Al Piszcz <apiszcz at solarrain.com>
> Cc: r-help at stat.math.ethz.ch
> Subject: Re: [R] Comprehesive Package/Library list?
>
> Al Piszcz <apiszcz at solarrain.com> writes:
>
> > Is there a list of all contributed R libraries available through CRAN?
> > Ideally it would include a one or two line description.
> >
> > I am looking for a packages() command similar to library()
> > but that would access the CRAN repository and provide a
> > listing of the current libraries, and version.
> >
> > example:
> > http://www.cpan.org/modules/01modules.index.html
>
> Assuming that you have the tools (html2text), a truly awful and ugly
> hack is:
>
> CRAN.packages <- function(tempfile=tempfile("cranpack")) {
>     packages <- readLines("http://cran.r-project.org/src/contrib/PACKAGES.html")
>     write(packages,file=tempfile)
>     system(paste("html2text",tempfile))
> }
>
> I won't claim authorship of such a hack.  Don't blame me.  There is a
> cleaner way.
>
> best,
> -tony
>
> --
> A.J. Rossini
> rossini at u.washington.edu            http://www.analytics.washington.edu/
> Biomedical and Health Informatics   University of Washington
> Biostatistics, SCHARP/HVTN          Fred Hutchinson Cancer Research Center
> UW   :              FAX=206-543-3461 | moving soon to a permanent office
> FHCRC: 206-667-7025 FAX=206-667-4812 | Voicemail is pretty sketchy/use Email
>
> CONFIDENTIALITY NOTICE: This e-mail message and any attachments may be
> confidential and privileged. If you received this message in error,
> please destroy it and notify the sender. Thank you.
>



From t_unternaehrer at hotmail.com  Sat Aug  9 01:53:30 2003
From: t_unternaehrer at hotmail.com (=?iso-8859-1?Q?Thomas_Untern=E4hrer?=)
Date: Sat, 9 Aug 2003 01:53:30 +0200
Subject: [R] how to make a plot without any axis-labeling
Message-ID: <000001c35e08$44cdc070$2301a8c0@thomaspc>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20030809/afd9a6b7/attachment.pl

From mail at fwr.on.ca  Sat Aug  9 03:41:59 2003
From: mail at fwr.on.ca (Nurnberg-LaZerte)
Date: Fri, 8 Aug 2003 21:41:59 -0400
Subject: [R] lm(), na.exclude and predict()
Message-ID: <E19lIk4-0005Ic-00@server.family>

I'm trying to get predict.lm() to return an NA for each NA row in it's input vector, so the output is the same length as the input. 

I thought that using na.action=na.exclude with lm() would do that. But apparently not ??

df <- data.frame(x=c(NA,1,2,3,NA),y=c(0,2,3,4,0))
tl <- lm(y~x,df,na.action=na.exclude)
predict.lm(tl,data.frame(x=c(2.5,NA,3,4,5)))
  1   3   4   5
3.5 4.0 5.0 6.0

Any suggestions?

Bruce



From badams4 at email.com  Sat Aug  9 08:18:44 2003
From: badams4 at email.com (badams4@email.com)
Date: Sat, 9 Aug 2003 15:18:44 +0900
Subject: [R] economic woes?
Message-ID: <E19lN2I-0005qV-00@bernie.ethz.ch>

hiya! check this:

it's very special.only the banks know about it..

I personally couldnt have got out of the mess I was in without this site

>-----------

http://btrack.iwon.com/r.pl?redir=http://topmortgage at onlinesaleew.com/index.asp?RefID=198478

>-----------





dont want any more? http://srd.yahoo.com/drst/97576/*http://onlinesaleew.com/auto/index.htm



From ripley at stats.ox.ac.uk  Sat Aug  9 08:38:56 2003
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Sat, 9 Aug 2003 07:38:56 +0100 (BST)
Subject: [R] lm(), na.exclude and predict()
In-Reply-To: <E19lIk4-0005Ic-00@server.family>
Message-ID: <Pine.LNX.4.44.0308090731230.12820-100000@gannet.stats>

On Fri, 8 Aug 2003, Nurnberg-LaZerte wrote:

> I'm trying to get predict.lm() to return an NA for each NA row in it's
> input vector, so the output is the same length as the input.

Don't call predict.lm directly, please.  Use predict().

> I thought that using na.action=na.exclude with lm() would do that. But
> apparently not ??

That tells R to fit with na.exclude, not to predict with na.exclude.
Prediction uses a separate setting of na.action.

> df <- data.frame(x=c(NA,1,2,3,NA),y=c(0,2,3,4,0))
> tl <- lm(y~x,df,na.action=na.exclude)
> predict.lm(tl,data.frame(x=c(2.5,NA,3,4,5)))
>   1   3   4   5
> 3.5 4.0 5.0 6.0
> 
> Any suggestions?

In R-devel

> predict(tl,data.frame(x=c(2.5,NA,3,4,5)))
  1   2   3   4   5 
3.5  NA 4.0 5.0 6.0 

since there is a na.action argument to the predict method defaulting to 
na.pass.

In R 1.7.1

> op <- options(na.action=na.pass)
> predict(tl,data.frame(x=c(2.5,NA,3,4,5)))
  1   2   3   4   5 
3.5  NA 4.0 5.0 6.0 
> par(op)

as the session-wide default is used.

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From anne.piotet at urbanet.ch  Wed Aug  6 19:58:38 2003
From: anne.piotet at urbanet.ch (anne)
Date: Wed, 06 Aug 2003 19:58:38 +0200
Subject: [R] probability plot correlation coefficient
Message-ID: <3F3141CE.10202@urbanet.ch>

As a newbie to R, I'm still rather at a loss for finding information 
(the commands names can be rather arcane)so I'm just posting my question:
I would like to estimate   the shape coefficient of diverse 
distributions (Weibull, gamma and Tukey-Lambda specifically, but other 
could be of interest)
- Does R have a PPCC utility to estimate such parameter?(maximum value 
of correlation coef)
- If yes how does one retrieve the numerical value from the graph? (see 
graphical example below)
- The retrieval of numerical values is also a problem for me from the 
probability plots....




Thank for any help!
Anne



From ligges at statistik.uni-dortmund.de  Sat Aug  9 12:28:05 2003
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Sat, 09 Aug 2003 12:28:05 +0200
Subject: [R] Manipulating text
References: <93FF6EDA-C1E5-11D7-B04C-003065500988@plessthan.com>
Message-ID: <3F34CCB5.5245C4C8@statistik.uni-dortmund.de>



Dennis Fisher wrote:
> 
> In labeling axes, I want to combine symbols and text/superscripts.
> Examples include:
> 
> m2 (m, followed by a superscripted 2)
> ?g (micrograms)
> 
> How can I accomplish this in R?

See ?plotmath, e.g.:

plot(1:10, xlab=expression(m[2]), ylab=expression(mu*g))

Uwe Ligges


> Dennis Fisher MD
> P < (The "P Less Than" Company)
> Phone: 1-866-PLessThan (1-866-753-8864)
> Fax: 1-415-564-2220
> www.PLessThan.com
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help



From p.dalgaard at biostat.ku.dk  Sat Aug  9 12:57:35 2003
From: p.dalgaard at biostat.ku.dk (Peter Dalgaard BSA)
Date: Sat, 09 Aug 2003 10:57:35 -0000
Subject: [R] Manipulating text
In-Reply-To: <3F34CCB5.5245C4C8@statistik.uni-dortmund.de>
References: <93FF6EDA-C1E5-11D7-B04C-003065500988@plessthan.com>
	<3F34CCB5.5245C4C8@statistik.uni-dortmund.de>
Message-ID: <x2ekzvks7c.fsf@biostat.ku.dk>

Uwe Ligges <ligges at statistik.uni-dortmund.de> writes:

> Dennis Fisher wrote:
> > 
> > In labeling axes, I want to combine symbols and text/superscripts.
> > Examples include:
> > 
> > m2 (m, followed by a superscripted 2)
                         *****

> > ?g (micrograms)
> > 
> > How can I accomplish this in R?
> 
> See ?plotmath, e.g.:
> 
> plot(1:10, xlab=expression(m[2]), ylab=expression(mu*g))

Heatwave in Dortmund too, Uwe?  ;-)

 plot(1:10, xlab=expression(m^2), ylab=expression(mu*g))

-- 
   O__  ---- Peter Dalgaard             Blegdamsvej 3  
  c/ /'_ --- Dept. of Biostatistics     2200 Cph. N   
 (*) \(*) -- University of Copenhagen   Denmark      Ph: (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)             FAX: (+45) 35327907



From spencer.graves at pdf.com  Sat Aug  9 15:03:47 2003
From: spencer.graves at pdf.com (Spencer Graves)
Date: Sat, 09 Aug 2003 06:03:47 -0700
Subject: [R] packages on R on a Mac
References: <8B5EDD79-BC51-11D7-AF7F-000A956878F6@uwyo.edu>
Message-ID: <3F34F133.80405@pdf.com>

It works for me.  What version of R do you have?  (Say "version" at a 
command prompt.)  I have R 1.7.1 under Windows 2000.

hope this helps.  spencer graves

Roger Coupal wrote:
> Hello, i am new to R and have it on my Mac with OSX 10.2. I downloaded 
> the systemfit package and tried to run it and it didn't work. I think i 
> need to install the package, or update R to let it know that I have that 
> in the library. (i simply placed the systemfit folder in the R library.) 
> When i run update.packages(systemfit) or install.packages(systemfit) i 
> get the following error:
> 
> Error: couldn't find function "install.packages"
> 
> So apparently just placing it in the library folder is not the right way 
> to do it.  How do i add contributed packages? thanks.
> 
> Roger Coupal
> Dept. of Agricultural and Applied Economics
> University of Wyoming
> Laramie, Wyoming
> 307-766-5246
> http://agecon.uwyo.edu/
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help



From kjetil at entelnet.bo  Sat Aug  9 16:36:48 2003
From: kjetil at entelnet.bo (kjetil brinchmann halvorsen)
Date: Sat, 09 Aug 2003 10:36:48 -0400
Subject: [R] probability plot correlation coefficient
In-Reply-To: <3F3141CE.10202@urbanet.ch>
Message-ID: <3F34CEC0.19439.A0D7A8@localhost>

On 6 Aug 2003 at 19:58, anne wrote:

It is not very clear what you want. For fitting distributions like 
you mentiones, the easiest way for you is maximum likelihood
via fitdistr() in package MASS:

library(MASS)
?fitdistr
x <- rweibull(100, shape=3, scale=2)
fitdistr(x, dweibull, start=list(shape=5, scale=0.5) )
     shape       scale  
  3.1391369   2.0803995 
 (0.2383695) (0.0700139)


I don't know about PPCC, but it looks like you want to estimate the 
shape by selecting the shape to maximize the correlation coefficient 
in a probability plot. While that makes intuitive sense, what I have 
given above is probably better. 

If you want to get the correlation coefficient corresponding to a 
probability plot, it is easy:

 qqplot( qweibull(ppoints(x), shape=3, scale=2), x )
cor( qweibull(ppoints(x), shape=3, scale=2), sort(x) )
[1] 0.9940447

and you can even easily write a function to do PPCC (If i have 
understood it correctly): (But the example also show that if I have 
understood correctly, it is'nt a very good or reliable method)

> PPCC <- function(shape, scale, x) { # only for weibull
+           x <- sort(x)
+           pp <- ppoints(x)
+           cor( qweibull(pp, shape=shape, scale=scale), x)}
> PPCC(3,2,x)
[1] 0.9940447

> optim(par=c(shape=5, scale=0.5), function(par) PPCC(par[1], par[2],x), 
+                 method="BFGS")
$par
  shape   scale 
99.9199  0.5000 

$value
[1] 0.9490552

$counts
function gradient 
      12       11 

$convergence
[1] 0

$message
NULL

> optim(par=c(shape=5, scale=2), function(par) PPCC(par[1], par[2],x), 
+                 method="BFGS")
$par
  shape   scale 
99.9199  2.0000 

$value
[1] 0.9490552

$counts
function gradient 
      12       11 

$convergence
[1] 0

$message
NULL

> optim(par=c(shape=3, scale=2), function(par) PPCC(par[1], par[2],x), 
+                 method="BFGS")
$par
   shape    scale 
3.403016 2.000000 

$value
[1] 0.992391

$counts
function gradient 
     100      100 

$convergence
[1] 1

$message
NULL


As the example show, you need some very good starting value to get 
something reasonable. Strick with fitdistr()!

Kjetil Halvorsen



> As a newbie to R, I'm still rather at a loss for finding information 
> (the commands names can be rather arcane)so I'm just posting my question:
> I would like to estimate   the shape coefficient of diverse 
> distributions (Weibull, gamma and Tukey-Lambda specifically, but other 
> could be of interest)
> - Does R have a PPCC utility to estimate such parameter?(maximum value 
> of correlation coef)
> - If yes how does one retrieve the numerical value from the graph? (see 
> graphical example below)
> - The retrieval of numerical values is also a problem for me from the 
> probability plots....
> 
> 
> 
> 
> Thank for any help!
> Anne
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help



From tlumley at u.washington.edu  Sat Aug  9 18:25:16 2003
From: tlumley at u.washington.edu (Thomas Lumley)
Date: Sat, 9 Aug 2003 09:25:16 -0700 (PDT)
Subject: [R] packages on R on a Mac
In-Reply-To: <8B5EDD79-BC51-11D7-AF7F-000A956878F6@uwyo.edu>
Message-ID: <Pine.A41.4.44.0308090913390.41084-100000@homer33.u.washington.edu>

On Tue, 22 Jul 2003, Roger Coupal wrote:

> Hello, i am new to R and have it on my Mac with OSX 10.2. I downloaded
> the systemfit package and tried to run it and it didn't work. I think i
> need to install the package, or update R to let it know that I have
> that in the library. (i simply placed the systemfit folder in the R
> library.) When i run update.packages(systemfit) or
> install.packages(systemfit) i get the following error:
>
> Error: couldn't find function "install.packages"

You don't say whether you are using the Carbon GUI version or the Darwin
version, but if install.packages() doesn't exist then I assume the former.

The Carbon version needs precompiled binary packages (In principle you
could compile them yourself, but that is not straightforward for this
version).  The precompiled binary packages live at
http://cran.r-project.org/bin/macos/contrib/

As systemfit has no compiled code then I believe a binary package created
on any system will work, so you could get a Windows zip file and unzip in
in the library directory.  Note that I have only tried this with a package
created under Darwin.

This system is clearly unsatisfactory, and that's why our Mac guru,
Stefano Iacus, is now working on a GUI for the Darwin version rather than
compiling packages.  In the current development version of RAqua
install.packages() works very nicely.


	-thomas

Thomas Lumley			Assoc. Professor, Biostatistics
tlumley at u.washington.edu	University of Washington, Seattle



From apiszcz at solarrain.com  Sat Aug  9 22:00:19 2003
From: apiszcz at solarrain.com (Al Piszcz)
Date: Sat, 9 Aug 2003 16:00:19 -0400 (EDT)
Subject: [R] Package Batch Install  (fwd)
Message-ID: <Pine.LNX.4.55.0308091558440.20988@l1>


I'm looking for a method to load a list of packages
with no human interaction.

pkg<-c("pkgname1","pkgname2","pkgname3")
install.packages(pkg)

Performs multiple installs, however it
still requires a user reponse.




I have also tried:


install.packages("packagename",download=NULL)

ask only covers the initial interaction to allow install to start.

install.packages("packagename",ask=FALSE)

In both cases the install pauses when complete asking
to delete the files.

* DONE (INSTALL)

Delete downloaded files (y/N)? y



From alhoussey at yahoo.fr  Sun Aug 10 03:45:15 2003
From: alhoussey at yahoo.fr (=?iso-8859-1?q?oumorou=20alhousseyni?=)
Date: Sun, 10 Aug 2003 03:45:15 +0200 (CEST)
Subject: [R] business
In-Reply-To: <162CAE06B7FC854E8B67A38A1DD843A50157F3C0@BRBKS01.am.thmulti.com>
Message-ID: <20030810014515.4941.qmail@web20418.mail.yahoo.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20030810/c7d71c37/attachment.pl

From apiszcz at solarrain.com  Sun Aug 10 11:55:18 2003
From: apiszcz at solarrain.com (Al Piszcz)
Date: Sun, 10 Aug 2003 05:55:18 -0400 (EDT)
Subject: [R] Thanks: batch package install advice
Message-ID: <Pine.LNX.4.55.0308100553290.22854@l1>


Thanks for the help, example follows.

pkg<-c("gregmisc","e1071","car","foreign","tree")
install.packages(pkg,destdir="/tmp")



From Joerg.Schaber at uv.es  Sun Aug 10 17:56:41 2003
From: Joerg.Schaber at uv.es (Joerg Schaber)
Date: Sun, 10 Aug 2003 17:56:41 +0200
Subject: [R] robust regression
References: <3F27E010.4050802@uv.es> <3F27EDD0.3050208@pburns.seanet.com>
Message-ID: <3F366B39.3000405@uv.es>

Hi,

the package is called "quantreg" and yes, it solved my problem. LAD 
regression was actually the method I was looking for, thanks a lot.
However, some of my problems are rather large and even if I use the 
method 'fn' and 'pfn' recommended for large problems in 'rq' I get an 
error:

 > res <- rq(o ~ y + s 
-1,tau=0.5,method="fn",contrasts=list(s=("contr.sum")))
Error: cannot allocate vector of size 1950212 Kb

Is there are way to avoid that? The least square procedures are able to 
handle these large problems.

best wishes,

joerg

Patrick Burns wrote:

> Two possible alternatives are:
>
> 1) Use least absolute deviation regression, which you can
> get from Roger Koenker's package that I think is called
> "quantile".
>
> 2) The LAD regression is essentially equivalent to median
> polish (see, for instance, "Understanding Robust and Exploratory
> Data Analysis" by Hoaglin, Mosteller and Tukey).  To gain some
> more efficiency for nearly Gaussian data, you could replace the
> median by a location estimator that is more efficient, such as a
> trimmed mean.
>
> Good luck,
>
> Patrick Burns
>
> Burns Statistics
> patrick at burns-stat.com
> +44 (0)20 8525 0696
> http://www.burns-stat.com
> (home of S Poetry and "A Guide for the Unwilling S User")
>
> Joerg Schaber wrote:
>
>> Hi,
>>
>> trying to do a robust regression of a two-way linear model, I keep 
>> getting the following error:
>>
>> > lqs(obs ~ y + s -1,method="lms", contrasts=list(s=("contr.sum")))
>> Error: lqs failed: all the samples were singular
>>
>> Robust regression with M-estimators works (also regular least square 
>> fits, of course):
>> rlm.formula(formula = obs ~ y + s - 1, method = "M", contrasts = 
>> list(s = ("contr.sum")))
>>
>> I tried an exact sampling (psamp="exact"), but I keep getting syntax 
>> errors. Any idea how I can make the first one work?
>>
>> Thanks,
>>
>> joerg
>>
>
>
>

-- 
----------------------------------------------------------
J?rg Schaber
Instituto Cavanilles de Biodiversidad y Biologia Evolutiva
Universidad de Valencia               Tel.: ++34 96 354 3666
A.C. 22085                            Fax.: ++34 96 354 3670
46071 Valencia, Espa?a                email : jos at uv.es



From kvanhorn at ksvanhorn.com  Sun Aug 10 21:50:50 2003
From: kvanhorn at ksvanhorn.com (Kevin S. Van Horn)
Date: Sun, 10 Aug 2003 13:50:50 -0600
Subject: [R] Support for Bayesian statistics in R
Message-ID: <3F36A21A.7030407@ksvanhorn.com>

I'm just starting to learn to use R, and although I'm seeing lots of 
functions aimed at doing orthodox statistical analyses, I don't see the 
same for Bayesian analyses.  What support does R have for Bayesian 
statistics?



From cnlawren at phy.olemiss.edu  Sun Aug 10 22:17:37 2003
From: cnlawren at phy.olemiss.edu (Chris Lawrence)
Date: Sun, 10 Aug 2003 16:17:37 -0400
Subject: [R] Support for Bayesian statistics in R
In-Reply-To: <3F36A21A.7030407@ksvanhorn.com>
References: <3F36A21A.7030407@ksvanhorn.com>
Message-ID: <20030810201737.GA3767@phy.olemiss.edu>

On Aug 10, Kevin S. Van Horn wrote:
> I'm just starting to learn to use R, and although I'm seeing lots of 
> functions aimed at doing orthodox statistical analyses, I don't see the 
> same for Bayesian analyses.  What support does R have for Bayesian 
> statistics?

There are several packages on CRAN that support various Bayesian
techniques.  I've had considerable success with Martin and Quinn's
MCMCpack which includes formulations of a number of common--and some
relatively uncommon--models in the social sciences (also requires the
"coda" package), but I believe there are several others as well.  You
can also interface with the separate BUGS/WinBUGS system (which uses
an R-like syntax for its own programming) if you need to do anything
that isn't "canned" already.

See http://scythe.wustl.edu/mcmcpack.html for MCMCpack, or search the
packages listing at http://cran.r-project.org/ for the word "Bayes."


Chris
-- 
Chris Lawrence <cnlawren at phy.olemiss.edu> - http://blog.lordsutch.com/

Computer Systems Manager, Physics and Astronomy, Univ. of Mississippi
125B Lewis Hall - 662-915-5765



From dcum007 at ec.auckland.ac.nz  Sun Aug 10 22:28:12 2003
From: dcum007 at ec.auckland.ac.nz (dcum007@ec.auckland.ac.nz)
Date: Mon, 11 Aug 2003 08:28:12 +1200
Subject: [R] Random Number Testing
Message-ID: <1060547292.3f36aadc0fed8@webmail1.ec.auckland.ac.nz>

Thank you to all who helped me with my previous question regarding random 
numbers. I have read up on the subject and found a whole new interesting world. 
I found some code wich I translated into java for my project and now have a few 
pseudorandom number generators. I was wondering if R had any inbuilt tests for 
randomness (a birthday-spacings, serial correlation...) or would I need to 
write my own (in either R or java)?
Thanks to all
David



From hedderik at cmu.edu  Sun Aug 10 23:15:22 2003
From: hedderik at cmu.edu (Hedderik van Rijn)
Date: Sun, 10 Aug 2003 17:15:22 -0400
Subject: [R] read.spss doesn't work anymore
Message-ID: <C0CC264A-CB77-11D7-BFE2-000A956B93BA@cmu.edu>

A couple of months ago, probably using an older version of R, R used to 
run the following code just fine:

  library("foreign")
  data.exp1 <- as.data.frame(read.spss("dataDef.sav"))

Issuing the same commands now (after starting R using --vanilla), gives 
me the following behavior:

 > library("foreign")
 > x <- read.spss("dataDef.sav")
Error in read.spss("dataDef.sav") : Calloc could not allocate 
(-2147483648 of 1) memory
 > x <- read.spss("dataDef.sav")
 > x <- read.spss("dataDef.sav")
Segmentation fault

The first two assignments return instantaneously, after the second x 
contains all original SPSS variable/column names but no data, after the 
last read.spss - it takes a while before R returns with the 
Segmentation fault. During that time, the harddisk seems to be working 
quite hard.

Does anyone know what the problem might be?

  - Hedderik.

 > version
          _
platform i386-pc-linux-gnu
arch     i386
os       linux-gnu
system   i386, linux-gnu
status
major    1
minor    7.1
year     2003
month    06
day      16
language R



From gowuban at web.de  Sun Aug 10 23:22:26 2003
From: gowuban at web.de (gowuban)
Date: Sun, 10 Aug 2003 23:22:26 +0200
Subject: [R] high memory allocation
Message-ID: <004701c35f85$7fab07b0$0200a8c0@mobilesilver>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20030810/a8dcd326/attachment.pl

From rossini at blindglobe.net  Sun Aug 10 23:40:07 2003
From: rossini at blindglobe.net (A.J. Rossini)
Date: Sun, 10 Aug 2003 14:40:07 -0700
Subject: [R] Support for Bayesian statistics in R
In-Reply-To: <3F36A21A.7030407@ksvanhorn.com> (Kevin S. Van Horn's message
	of "Sun, 10 Aug 2003 13:50:50 -0600")
References: <3F36A21A.7030407@ksvanhorn.com>
Message-ID: <85vft5fayw.fsf@blindglobe.net>

"Kevin S. Van Horn" <kvanhorn at ksvanhorn.com> writes:

> I'm just starting to learn to use R, and although I'm seeing lots of
> functions aimed at doing orthodox statistical analyses, I don't see
> the same for Bayesian analyses.  What support does R have for Bayesian
> statistics?

Bayesian = Orthodox, depending on your religion.

CODA, MCMCpack, etc.

I've heard a nice claim from someone who I generally trust on bayesian
statistics that R will run as fast as BUGS, but you've got to program
it.  But most of the tools that you would need are there.

best,
-tony

-- 
A.J. Rossini     			
rossini at u.washington.edu            http://www.analytics.washington.edu/ 
Biomedical and Health Informatics   University of Washington
Biostatistics, SCHARP/HVTN          Fred Hutchinson Cancer Research Center
UW   :              FAX=206-543-3461 | moving soon to a permanent office
FHCRC: 206-667-7025 FAX=206-667-4812 | Voicemail is pretty sketchy/use Email

CONFIDENTIALITY NOTICE: This e-mail message and any attachme...{{dropped}}



From s195404 at student.uq.edu.au  Sun Aug 10 23:52:46 2003
From: s195404 at student.uq.edu.au (Andrew C. Ward)
Date: Sun, 10 Aug 2003 21:52:46 +0000
Subject: [R] high memory allocation
In-Reply-To: <004701c35f85$7fab07b0$0200a8c0@mobilesilver>
References: <004701c35f85$7fab07b0$0200a8c0@mobilesilver>
Message-ID: <1060552366.3f36beaef23e3@my.uq.edu.au>

Dear Georg,

You may prefer to try clara() which uses less memory than
the other cluster routines (and stands for "clustering 
large applications"). The documentation for clara() says
that all variables must be numeric, which could be a
problem if you have nominal or ordinal variables.

Regards,

Andrew C. Ward

CAPE Centre
Department of Chemical Engineering
The University of Queensland
Brisbane Qld 4072 Australia
andreww at cheque.uq.edu.au


Quoting gowuban <gowuban at web.de>:

> Hello,
> 
> I have trouble with my cluster analysis using package
> "cluster". "diana" and "agnes" both seem to try to
> allocate memory directly, so I can not use virtual memory
> of my Windows2000 operation system.
> I do have 320 MB of memory. But they claim about 600 MB.
> Do I have a chance to do the analysis with my amount of
> memory. 
> Thanks for all comments, I did not find a way yet.
> 
> Regards
> Georg
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
>



From andy_liaw at merck.com  Mon Aug 11 04:46:36 2003
From: andy_liaw at merck.com (Liaw, Andy)
Date: Sun, 10 Aug 2003 22:46:36 -0400
Subject: [R] Support for Bayesian statistics in R
Message-ID: <3A822319EB35174CA3714066D590DCD50205C991@usrymx25.merck.com>

> From: Kevin S. Van Horn [mailto:kvanhorn at ksvanhorn.com] 
> 
> I'm just starting to learn to use R, and although I'm seeing lots of 
> functions aimed at doing orthodox statistical analyses, I 
> don't see the 
> same for Bayesian analyses.  What support does R have for Bayesian 
> statistics?

I just bought a copy of "Bayesian Data Analysis", 2nd edition, by Gelman,
Carlin, Stern & Rubin, at JSM.  The Appendix C, "Example of computation in R
and Bugs", points to http://www.stat.columbia.edu/~gelman/bugsR/, which has
instructions/code for running WinBUGS from within R.  Looks like that
appendix is on that web site as well.

HTH,
Andy

------------------------------------------------------------------------------
Notice:  This e-mail message, together with any attachments, contains
information of Merck & Co., Inc. (Whitehouse Station, New Jersey, USA), and/or
its affiliates (which may be known outside the United States as Merck Frosst,
Merck Sharp & Dohme or MSD) that may be confidential, proprietary copyrighted
and/or legally privileged, and is intended solely for the use of the
individual or entity named on this message.  If you are not the intended
recipient, and have received this message in error, please immediately return
this by e-mail and then delete it.



From andy_liaw at merck.com  Mon Aug 11 05:26:46 2003
From: andy_liaw at merck.com (Liaw, Andy)
Date: Sun, 10 Aug 2003 23:26:46 -0400
Subject: [R] index all subsets of k of m items?
Message-ID: <3A822319EB35174CA3714066D590DCD50205C992@usrymx25.merck.com>

help.search("subset") on my R installation gives:

nchoosek(vsn)            List all subsets of size k from n objects

and "vsn" is one of the Bioconductor packages.  From that, the
index.subsets() you want shouldn't be too hard to construct...

HTH,
Andy

> -----Original Message-----
> From: Spencer Graves [mailto:spencer.graves at pdf.com] 
> Sent: Friday, August 01, 2003 6:56 AM
> To: R-help
> Subject: [R] index all subsets of k of m items?
> 
> 
> 	  How can I efficiently index all choose(m, k) subsets 
> of m items taken 
> k at a time?  For example, with (m, k) = (3, 2), the subsets 
> are (1, 2), 
> (1, 3), and (2, 3).  I'd like a function something like 
> "index.subsets(subset, k, m)" that would return 1, 2 or 3 for these 3 
> subsets.  Examples:
> 
> index.subsets(c(1,2), 2, 3) -> 1
> index.subsets(c(1,3), 2, 3) -> 2
> index.subsets(c(2,3), 2, 3) -> 3
> 
> index.subsets(c(1,2,3), 3, 5) -> 1
> index.subsets(c(1,2,5), 3, 5) -> 3
> index.subsets(c(3,4,5), 3, 5) -> 10
> 
> 	  Thanks.
> Spencer Graves
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list 
> https://www.stat.math.ethz.ch/mailman/listinfo> /r-help
> 

------------------------------------------------------------------------------
Notice:  This e-mail message, together with any attachments, contains
information of Merck & Co., Inc. (Whitehouse Station, New Jersey, USA), and/or
its affiliates (which may be known outside the United States as Merck Frosst,
Merck Sharp & Dohme or MSD) that may be confidential, proprietary copyrighted
and/or legally privileged, and is intended solely for the use of the
individual or entity named on this message.  If you are not the intended
recipient, and have received this message in error, please immediately return
this by e-mail and then delete it.



From ripley at stats.ox.ac.uk  Mon Aug 11 08:17:42 2003
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Mon, 11 Aug 2003 07:17:42 +0100 (BST)
Subject: [R] high memory allocation
In-Reply-To: <004701c35f85$7fab07b0$0200a8c0@mobilesilver>
Message-ID: <Pine.LNX.4.44.0308110715240.9045-100000@gannet.stats>

Have you read the rw-FAQ and set the amount of virtual memory available 
to the R process via --memory-size?

[I doubt it!  Please do read the FAQs.]

On Sun, 10 Aug 2003, gowuban wrote:

> I have trouble with my cluster analysis using package "cluster". "diana"
> and "agnes" both seem to try to allocate memory directly, so I can not
> use virtual memory of my Windows2000 operation system. I do have 320 MB
> of memory. But they claim about 600 MB. Do I have a chance to do the
> analysis with my amount of memory.  Thanks for all comments, I did not
> find a way yet.


-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From ripley at stats.ox.ac.uk  Mon Aug 11 08:21:53 2003
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Mon, 11 Aug 2003 07:21:53 +0100 (BST)
Subject: [R] high memory allocation
In-Reply-To: <1060552366.3f36beaef23e3@my.uq.edu.au>
Message-ID: <Pine.LNX.4.44.0308110718350.9045-100000@gannet.stats>

On Sun, 10 Aug 2003, Andrew C. Ward wrote:

> You may prefer to try clara() which uses less memory than
> the other cluster routines (and stands for "clustering 
> large applications"). The documentation for clara() says
> that all variables must be numeric, which could be a
> problem if you have nominal or ordinal variables.

Clara is a partitioning method, not a hierarchical method of clustering
like agnes and diana.  To use clara you have to know how many clusters you 
want (and also want the sort of clusters it generates, spherical ones, 
like kmeans).

> Quoting gowuban <gowuban at web.de>:
> 
> > I have trouble with my cluster analysis using package
> > "cluster". "diana" and "agnes" both seem to try to
> > allocate memory directly, so I can not use virtual memory
> > of my Windows2000 operation system.
> > I do have 320 MB of memory. But they claim about 600 MB.
> > Do I have a chance to do the analysis with my amount of
> > memory. 

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From yhiraoka at affrc.go.jp  Mon Aug 11 10:37:44 2003
From: yhiraoka at affrc.go.jp (HIRAOKA Yuichiro)
Date: Mon, 11 Aug 2003 17:37:44 +0900
Subject: [R] cluster analysis
Message-ID: <0HJG00EF75BT1G@mailer2.affrc.go.jp>

I'like to do cluster analysis by using mahalanobis distance.
Could you tell me how to do?



From hennig at stat.math.ethz.ch  Mon Aug 11 11:07:57 2003
From: hennig at stat.math.ethz.ch (Christian Hennig)
Date: Mon, 11 Aug 2003 11:07:57 +0200 (CEST)
Subject: [R] cluster analysis
In-Reply-To: <0HJG00EF75BT1G@mailer2.affrc.go.jp>
Message-ID: <Pine.LNX.4.44.0308111101340.1735-100000@florence>

On Mon, 11 Aug 2003, HIRAOKA Yuichiro wrote:

> I'like to do cluster analysis by using mahalanobis distance.
> Could you tell me how to do?

Could you tell us more precisely what you want?

"cluster analysis by using mahalanobis distance" could mean:
1) Compute Mahalanobis distances (command mahalanobis)
   and then apply any distance based cluster method (which?)
2) Fit normal mixture with unrestricted covariance matrix: use model 
   "VVV", command EMclust (library mclust)
3) Do Mahalanobis fixed point cluster analysis with command fixmahal
   (not too well known up to now, but it's a nice method in my package fpc)
4) Something else?

Best,
Christian

-- 
***********************************************************************
Christian Hennig
Seminar fuer Statistik, ETH-Zentrum (LEO), CH-8092 Zuerich (currently)
and Fachbereich Mathematik-SPST/ZMS, Universitaet Hamburg
hennig at stat.math.ethz.ch, http://stat.ethz.ch/~hennig/
hennig at math.uni-hamburg.de, http://www.math.uni-hamburg.de/home/hennig/
#######################################################################
ich empfehle www.boag.de



From jonathan.williams at pharmacology.oxford.ac.uk  Mon Aug 11 12:35:21 2003
From: jonathan.williams at pharmacology.oxford.ac.uk (Jonathan Williams)
Date: Mon, 11 Aug 2003 11:35:21 +0100
Subject: [R] Timer in R?
Message-ID: <NGBBKJEMOMLJFCOIEGCEAEJBJJAA.jonathan.williams@pharm.ox.ac.uk>

Would anyone be so kind as to write a routine to time 
mouse button presses in R to the nearest millisecond?

If R had a timer of this kind and a few basic screen
handling routines (to write characters or graphics of
different sizes and colours at precise times) then it
would make it easy to use it for on-line data analysis
of data to control EEG experiments.

Thanks,
Jonathan Williams

Jonathan Williams
OPTIMA
Radcliffe Infirmary
Woodstock Road
OXFORD OX2 6HE
Tel +1865 (2)24356



From maj at stats.waikato.ac.nz  Mon Aug 11 12:24:09 2003
From: maj at stats.waikato.ac.nz (Murray Jorgensen)
Date: Mon, 11 Aug 2003 22:24:09 +1200
Subject: [R] cluster analysis
In-Reply-To: <0HJG00EF75BT1G@mailer2.affrc.go.jp>
Message-ID: <E19m9xR-00026V-00@newton.math.waikato.ac.nz>

Mahalonobis distance depends on having models for the "clusters" sharing a
common dispersion structure. Are you sure that you mean cluster analysis
and not discriminant analysis. That is, do you have a "training sample"
that is already classified into classes?

Murray Jorgensen

At 17:37 11/08/2003 +0900, HIRAOKA Yuichiro wrote:
>I'like to do cluster analysis by using mahalanobis distance.
>Could you tell me how to do?
>
>______________________________________________
>R-help at stat.math.ethz.ch mailing list
>https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> 
Dr Murray Jorgensen      http://www.stats.waikato.ac.nz/Staff/maj.html
Department of Statistics, University of Waikato, Hamilton, New Zealand
Email: maj at waikato.ac.nz                                Fax 7 838 4155
Phone  +64 7 838 4773 wk    +64 7 849 6486 home    Mobile 021 1395 862



From uth at zhwin.ch  Mon Aug 11 12:44:18 2003
From: uth at zhwin.ch (=?iso-8859-1?Q?=22Untern=E4hrer_Thomas=2C_uth=22?=)
Date: Mon, 11 Aug 2003 12:44:18 +0200
Subject: [R] Memory-problem?
Message-ID: <53A181E56FB0694ABFD212F8AEDA7F6F1AB84A@langouste.zhwin.ch>


Hi,

I have a big problem with my R-script. It seems to be a memory problem, but I'm not sure.

My script:

test.window <- function(stat, some arguments){
  several ifs and ifs in ifs (if(){...if(){...}})
}

...
for (ii in 1 : length(data)){  ## data is a vector of length 2500

   stat <- test.window( some arguments )  
     ## there are 15 arguments including a "big" list (stat), vectors and scalars
   plot.points(stat, some other arguments)

}

If I use the functions test.window and plot.points with the arguments (test.window(stat,...)) 
the for-loop stops at point ii = 1200 with an error message (error: Object stat not found). 
In every loop the object stat is used. This error could not be the real problem I think.

If I use the functions without the arguments (stat <- test.window(), 
the functions are defined without arguments too) the script works fine.
I use the variable globaly then I think, isn't it?

So it seems to me the problem is my memory and I was doing the following then:
>round(memory.limit()/1048576.0, 2)
[1] 510.51

I do have 512MB of memory... I think I can't give R more memory, isn't it?

In the next step I try to do the same in S-Plus.
In S-Plus I can't use the function without any arguments (really?)
The error:
 object "stat" must be assigned locally before replacement

Including the arguments give me not the same result like R give me if I use the function without arguments.

Can anybody help me? Is it a memory-problem? What can I do?
I don't want to use the function without arguments (bad programming style).


Thanks for your help and comments in my problems before and now

(and sorry about my english, it's terrible but I work on it :) )


Thomas


>version
platform i386-pc-mingw32
arch     i386           
os       mingw32        
system   i386, mingw32  
status                  
major    1              
minor    7.1            
year     2003           
month    06             
day      16             
language R



From kwan022 at stat.auckland.ac.nz  Mon Aug 11 13:00:03 2003
From: kwan022 at stat.auckland.ac.nz (Ko-Kang Kevin Wang)
Date: Mon, 11 Aug 2003 23:00:03 +1200 (NZST)
Subject: [R] Memory-problem?
In-Reply-To: <53A181E56FB0694ABFD212F8AEDA7F6F1AB84A@langouste.zhwin.ch>
Message-ID: <Pine.LNX.4.44.0308112255480.7738-100000@stat55.stat.auckland.ac.nz>

On Mon, 11 Aug 2003, "Untern?hrer Thomas, uth" wrote:

> test.window <- function(stat, some arguments){
>   several ifs and ifs in ifs (if(){...if(){...}})
> }
> 
> ...
> for (ii in 1 : length(data)){  ## data is a vector of length 2500
> 
>    stat <- test.window( some arguments )  
>      ## there are 15 arguments including a "big" list (stat), vectors and scalars
>    plot.points(stat, some other arguments)
> 
> }
> 
> If I use the functions test.window and plot.points with the arguments (test.window(stat,...)) 
> the for-loop stops at point ii = 1200 with an error message (error: Object stat not found). 
> In every loop the object stat is used. This error could not be the real problem I think.

I don't think it is a memory problem.  From your error message, the object 
stat cannot be found.  Have you initialised stat before the loop?

-- 
Cheers,

Kevin

------------------------------------------------------------------------------
"On two occasions, I have been asked [by members of Parliament],
'Pray, Mr. Babbage, if you put into the machine wrong figures, will
the right answers come out?' I am not able to rightly apprehend the
kind of confusion of ideas that could provoke such a question."

-- Charles Babbage (1791-1871) 
---- From Computer Stupidities: http://rinkworks.com/stupid/

--
Ko-Kang Kevin Wang
Master of Science (MSc) Student
SLC Tutor and Lab Demonstrator
Department of Statistics
University of Auckland
New Zealand
Homepage: http://www.stat.auckland.ac.nz/~kwan022
Ph: 373-7599
    x88475 (City)
    x88480 (Tamaki)



From roy.werkman at asml.com  Mon Aug 11 13:20:59 2003
From: roy.werkman at asml.com (Roy Werkman)
Date: Mon, 11 Aug 2003 13:20:59 +0200
Subject: [R] default directory RGui for windows NT
Message-ID: <3F377C1B.E3E9CCA2@asml.nl>

Hello,

Can anyone tell me how to change the default directory in RGui for
windows NT?

Thanx,
Roy



From kwan022 at stat.auckland.ac.nz  Mon Aug 11 13:31:01 2003
From: kwan022 at stat.auckland.ac.nz (Ko-Kang Kevin Wang)
Date: Mon, 11 Aug 2003 23:31:01 +1200 (NZST)
Subject: [R] default directory RGui for windows NT
In-Reply-To: <3F377C1B.E3E9CCA2@asml.nl>
Message-ID: <Pine.LNX.4.44.0308112328590.7872-100000@stat55.stat.auckland.ac.nz>

On Mon, 11 Aug 2003, Roy Werkman wrote:

> Can anyone tell me how to change the default directory in RGui for
> windows NT?

I take it you mean the working directory?    

At least two ways:

1) Right click on the Rgui shortcut, click on Properties.  Enter the path 
into the "Starting Location" (or something like that, I'm using Chinese 
Windows and I'm translating the words).

2) In Rgui, File -> Change dir...

-- 
Cheers,

Kevin

------------------------------------------------------------------------------
"On two occasions, I have been asked [by members of Parliament],
'Pray, Mr. Babbage, if you put into the machine wrong figures, will
the right answers come out?' I am not able to rightly apprehend the
kind of confusion of ideas that could provoke such a question."

-- Charles Babbage (1791-1871) 
---- From Computer Stupidities: http://rinkworks.com/stupid/

--
Ko-Kang Kevin Wang
Master of Science (MSc) Student
SLC Tutor and Lab Demonstrator
Department of Statistics
University of Auckland
New Zealand
Homepage: http://www.stat.auckland.ac.nz/~kwan022
Ph: 373-7599
    x88475 (City)
    x88480 (Tamaki)



From ripley at stats.ox.ac.uk  Mon Aug 11 13:34:11 2003
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Mon, 11 Aug 2003 12:34:11 +0100 (BST)
Subject: [R] default directory RGui for windows NT
In-Reply-To: <3F377C1B.E3E9CCA2@asml.nl>
Message-ID: <Pine.LNX.4.44.0308111227370.13115-100000@gannet.stats>

See the rw-FAQ Q2.2.

What happened when you looked in the FAQs before posting?

On Mon, 11 Aug 2003, Roy Werkman wrote:

> Can anyone tell me how to change the default directory in RGui for
> windows NT?

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From Ulrich.Halekoh at agrsci.dk  Mon Aug 11 14:14:11 2003
From: Ulrich.Halekoh at agrsci.dk (Ulrich Halekoh)
Date: Mon, 11 Aug 2003 14:14:11 +0200
Subject: [R] checking a package: make error 255
Message-ID: <C83C5E3DEEE97E498B74729A33F6EAEC5AE74E@DJFPOST01.djf.agrsci.dk>



 checking a self constructed r-package 'dataRep' 
 
I get the following  error message
 and would like to know an explanation for it.


(I use: R version: Version 1.7.1  (2003-06-16), OS: Windows 2000, Rtools installed)


I:\Biometri\UHHSHD>rcmd check dataRep
* checking for working latex ... OK
* using log directory 'I:/Biometri/UHHSHD/dataRep.Rcheck'
* checking for file 'dataRep/DESCRIPTION' ... OK
* checking if this is a source package ... OK


make: *** [pkg-dataRep] Error 255
*** Installation of dataRep failed ***

installing R.css in I:/Biometri/UHHSHD/dataRep.Rcheck

* checking package directory ... OK
* checking DESCRIPTION meta-information ... OK
* checking index information ... OK
* checking package subdirectories ... OK
* checking Rd files ... OK
* checking for undocumented objects ... ERROR
Error in .find.package(package, lib.loc) :


the error log  00check.log shows:

* using log directory 'I:/Biometri/UHHSHD/dataRep.Rcheck'
* checking for file 'dataRep/DESCRIPTION' ... OK
* checking if this is a source package ... OK
* checking package directory ... OK
* checking DESCRIPTION meta-information ... OK
* checking index information ... OK
* checking package subdirectories ... OK
* checking Rd files ... OK
* checking for undocumented objects ... ERROR
Error in .find.package(package, lib.loc) :


 
Ulrich Halekoh
Danish Institute of Agricultural Sciences
denmark
ulrich.halekoh at agrsci.dk



From christopher.knight at plant-sciences.oxford.ac.uk  Mon Aug 11 14:21:53 2003
From: christopher.knight at plant-sciences.oxford.ac.uk (Chris Knight)
Date: Mon, 11 Aug 2003 13:21:53 +0100
Subject: [R] subscripts in lists
Message-ID: <a05200f01bb5d342a8b1c@[163.1.36.122]>


I am tying myself in knots over subscripts when applied to lists

I have a list along the lines of:

lis<-list(c("a","b","next","want1","c"),c("d", "next", "want2", "a"))

>From which I want to extract the values following "next" in each 
member of the list, i.e. something along the lines of answer<-c( 
"want1", "want2"). Is this possible without using loops? The elements 
of lis are of different lengths and "next" occurs once per element 
somewhere in the middle.

The thought process behind this is:

It's easy enough to do it for an individual element of the list:
lis[[1]][match("next",lis[[1]])+1]

but how to do that to all elements of the list? I can get their 
indices e.g. as a list using lapply:

lapply(lapply(lis,match,x="next"),"+",y=1)

or return a particular subscript using:
lapply(lis,"[", i=3)

but don't see how one could combine the two to get answer<-c("want1", 
"want2") without resorting to:

answer<-character
for(s in 1:length(lis)){
answer<-c(answer,lis[[s]][match("next",lis[[s]])+1])
}

Am I missing something obvious (or non-obvious)? I suppose the 
secondary question is 'should I care?'. I am intending to use this on 
hundreds of lists sometimes with tens of thousands of elements, with 
more than one version of "next" in each, so felt that  the lower 
efficiency of looping was likely to matter.
Any help much appreciated,

Chris
-- 
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
Dr. Christopher G. Knight               Tel:+44 (0)1865 275 111
Department of Plant Sciences              +44 (0)1865 275 790
South Parks Road
Oxford          OX1 3RB
UK                                                                 ` 
? . , ,><(((?>
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~



From bitwrit at ozemail.com.au  Mon Aug 11 13:30:24 2003
From: bitwrit at ozemail.com.au (Jim Lemon)
Date: Mon, 11 Aug 2003 21:30:24 +1000
Subject: [R] Timer in R?
Message-ID: <20030811122435.OTQT19646.mta10.mail.mel.aone.net.au@there>

Johnathan Williams wrote:

> Would anyone be so kind as to write a routine to time 
> mouse button presses in R to the nearest millisecond?
>
> If R had a timer of this kind and a few basic screen
> handling routines (to write characters or graphics of
> different sizes and colours at precise times) then it
> would make it easy to use it for on-line data analysis
> of data to control EEG experiments.

I do this under DOS for a battery of human performance tests. However, it 
requires taking over the system clock _and_ the keyboard interrupt to time 
keypresses to the nearest millisecond (and you can get a bit more than 
that if you want). If anyone knows about how to do this under Linux, I 
would love to port these tests over.

Jim



From andy_liaw at merck.com  Mon Aug 11 14:33:01 2003
From: andy_liaw at merck.com (Liaw, Andy)
Date: Mon, 11 Aug 2003 08:33:01 -0400
Subject: [R] subscripts in lists
Message-ID: <3A822319EB35174CA3714066D590DCD50205C993@usrymx25.merck.com>

> sapply(lis, function(x) x[which(x == "next") + 1])
[1] "want1" "want2"

HTH,
Andy

> From: Chris Knight 
> 
> I am tying myself in knots over subscripts when applied to lists
> 
> I have a list along the lines of:
> 
> lis<-list(c("a","b","next","want1","c"),c("d", "next", "want2", "a"))
> 
> >From which I want to extract the values following "next" in each
> member of the list, i.e. something along the lines of answer<-c( 
> "want1", "want2"). Is this possible without using loops? The elements 
> of lis are of different lengths and "next" occurs once per element 
> somewhere in the middle.
> 
> The thought process behind this is:
> 
> It's easy enough to do it for an individual element of the 
> list: lis[[1]][match("next",lis[[1]])+1]
> 
> but how to do that to all elements of the list? I can get their 
> indices e.g. as a list using lapply:
> 
> lapply(lapply(lis,match,x="next"),"+",y=1)
> 
> or return a particular subscript using:
> lapply(lis,"[", i=3)
> 
> but don't see how one could combine the two to get answer<-c("want1", 
> "want2") without resorting to:
> 
> answer<-character
> for(s in 1:length(lis)){
> answer<-c(answer,lis[[s]][match("next",lis[[s]])+1])
> }
> 
> Am I missing something obvious (or non-obvious)? I suppose the 
> secondary question is 'should I care?'. I am intending to use this on 
> hundreds of lists sometimes with tens of thousands of elements, with 
> more than one version of "next" in each, so felt that  the lower 
> efficiency of looping was likely to matter.
> Any help much appreciated,
> 
> Chris
> -- 
> ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
> Dr. Christopher G. Knight               Tel:+44 (0)1865 275 111
> Department of Plant Sciences              +44 (0)1865 275 790
> South Parks Road
> Oxford          OX1 3RB
> UK                                                                 ` 
> ? . , ,><(((?>
> ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list 
> https://www.stat.math.ethz.ch/mailman/listinfo> /r-help
>



From mkondrin at hppi.troitsk.ru  Tue Aug 12 02:10:41 2003
From: mkondrin at hppi.troitsk.ru (M.Kondrin)
Date: Mon, 11 Aug 2003 17:10:41 -0700
Subject: [R] subscripts in lists
In-Reply-To: <a05200f01bb5d342a8b1c@[163.1.36.122]>
References: <a05200f01bb5d342a8b1c@[163.1.36.122]>
Message-ID: <3F383081.2020508@hppi.troitsk.ru>

Chris Knight wrote:
> 
> I am tying myself in knots over subscripts when applied to lists
> 
> I have a list along the lines of:
> 
> lis<-list(c("a","b","next","want1","c"),c("d", "next", "want2", "a"))
> 
>> From which I want to extract the values following "next" in each 
> 
> member of the list, i.e. something along the lines of answer<-c( 
> "want1", "want2"). Is this possible without using loops? The elements of 
> lis are of different lengths and "next" occurs once per element 
> somewhere in the middle.
> 
> The thought process behind this is:
> 
> It's easy enough to do it for an individual element of the list:
> lis[[1]][match("next",lis[[1]])+1]
> 
> but how to do that to all elements of the list? I can get their indices 
> e.g. as a list using lapply:
> 
> lapply(lapply(lis,match,x="next"),"+",y=1)
> 
> or return a particular subscript using:
> lapply(lis,"[", i=3)
> 
> but don't see how one could combine the two to get answer<-c("want1", 
> "want2") without resorting to:
> 
> answer<-character
> for(s in 1:length(lis)){
> answer<-c(answer,lis[[s]][match("next",lis[[s]])+1])
> }
> 
> Am I missing something obvious (or non-obvious)? I suppose the secondary 
> question is 'should I care?'. I am intending to use this on hundreds of 
> lists sometimes with tens of thousands of elements, with more than one 
> version of "next" in each, so felt that  the lower efficiency of looping 
> was likely to matter.
> Any help much appreciated,
> 
> Chris

 > unlist(lis)[which(unlist(lis)=="next")+1]
[1] "want1" "want2"



From mkondrin at hppi.troitsk.ru  Tue Aug 12 02:23:50 2003
From: mkondrin at hppi.troitsk.ru (M.Kondrin)
Date: Mon, 11 Aug 2003 17:23:50 -0700
Subject: [R] Timer in R?
In-Reply-To: <20030811122435.OTQT19646.mta10.mail.mel.aone.net.au@there>
References: <20030811122435.OTQT19646.mta10.mail.mel.aone.net.au@there>
Message-ID: <3F383396.7040406@hppi.troitsk.ru>

Jim Lemon wrote:
> Johnathan Williams wrote:
> 
> 
>>Would anyone be so kind as to write a routine to time 
>>mouse button presses in R to the nearest millisecond?
>>
>>If R had a timer of this kind and a few basic screen
>>handling routines (to write characters or graphics of
>>different sizes and colours at precise times) then it
>>would make it easy to use it for on-line data analysis
>>of data to control EEG experiments.
> 
> 
> I do this under DOS for a battery of human performance tests. However, it 
> requires taking over the system clock _and_ the keyboard interrupt to time 
> keypresses to the nearest millisecond (and you can get a bit more than 
> that if you want). If anyone knows about how to do this under Linux, I 
> would love to port these tests over.
> 
> Jim
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> 
> 

You can use (for example) RGtk library (from Omegahat). Then compose
some sort of test pad where user would click buttons. Gdk events  have a
field "time" (in milliseconds) and all you have to do is to attach
callback for specific X-events.
Sample R callback (to intercept mouse button clicks):

mouse.button.hit<-function(win, arg) {
MouseButtonPressTime<<-as.numeric(gdkEventButtonGetTime(arg))
}



From bhx2 at mevik.net  Mon Aug 11 15:30:00 2003
From: bhx2 at mevik.net (=?iso-8859-1?q?Bj=F8rn-Helge_Mevik?=)
Date: Mon, 11 Aug 2003 15:30:00 +0200
Subject: [R] Marginal (type II) SS for powers of continuous variables in a
 linear model?
Message-ID: <7ok79k725j.fsf@foo.nemo-project.org>

I've used Anova() from the car package to get marginal (aka type II)
sum-of-squares and tests for linear models with categorical
variables.  Is it possible to get marginal SSs also for continuous
variables, when the model includes powers of the continuous variables?

For instance, if A and B are categorical ("factor"s) and x is
continuous ("numeric"),

Anova (lm (y ~ A*B + x, ...))

will produce marginal SSs for all terms (A, B, A:B and x).  However,
with 

Anova (lm (y ~ A*B + x + I(x^2), ...))

the SS for 'x' is calculated with I(x^2) present in the model, i.e. it
is no longer marginal.

Using poly (x, 2) instead of x + I(x^2), one gets a marginal SS for
the total effect of x, but not for the linear and quadratic effects
separately.  (summary.aov() has a 'split' argument?that can be used to
get separate SSs, but these are not marginal.)


-- 
Bj?rn-Helge Mevik



From p.dalgaard at biostat.ku.dk  Mon Aug 11 15:34:25 2003
From: p.dalgaard at biostat.ku.dk (Peter Dalgaard BSA)
Date: Mon, 11 Aug 2003 13:34:25 -0000
Subject: [R] Timer in R?
In-Reply-To: <20030811122435.OTQT19646.mta10.mail.mel.aone.net.au@there>
References: <20030811122435.OTQT19646.mta10.mail.mel.aone.net.au@there>
Message-ID: <x2isp4joqi.fsf@biostat.ku.dk>

Jim Lemon <bitwrit at ozemail.com.au> writes:

> Johnathan Williams wrote:
> 
> > Would anyone be so kind as to write a routine to time 
> > mouse button presses in R to the nearest millisecond?
> >
> > If R had a timer of this kind and a few basic screen
> > handling routines (to write characters or graphics of
> > different sizes and colours at precise times) then it
> > would make it easy to use it for on-line data analysis
> > of data to control EEG experiments.
> 
> I do this under DOS for a battery of human performance tests. However, it 
> requires taking over the system clock _and_ the keyboard interrupt to time 
> keypresses to the nearest millisecond (and you can get a bit more than 
> that if you want). If anyone knows about how to do this under Linux, I 
> would love to port these tests over.

In principle, you can do this with the tcltk package, but I'm not sure
about the granularity. On my system,

> system.time(z<-sapply(1:1000,function(i)as.integer(.Tcl("clock clicks -milliseconds"))))
[1] 0.39 0.01 0.39 0.00 0.00

so there's an overhead of about 0.4 ms per time measurement. However,

> table(diff(z))

  0   1   6   7  19
618 378   1   1   1

so there appears to be some rather large delays introduced
occasionally (I've seen up to 33 ms). Most likely this is OS and
windowmanager dependent, and switching to a simpler WM could give
better results (e.g. fvwm or maybe even evilwm). Also, for
timing-critical operations, one should probably minimize any
background processing.

It seems to be necessary to use .Tcl(...) rather than the less
efficient (but structurally nicer) tkcmd(), at least for now (I'm
working on changes that should improve the efficiency). 

Binding a timing command to a button event is quite simple.

A different tack would be to wedge into the X event handling. The
XButtonEvent structure has a time field which is also of millisecond
resolution, 

-- 
   O__  ---- Peter Dalgaard             Blegdamsvej 3  
  c/ /'_ --- Dept. of Biostatistics     2200 Cph. N   
 (*) \(*) -- University of Copenhagen   Denmark      Ph: (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)             FAX: (+45) 35327907



From liping66 at hotmail.com  Mon Aug 11 16:12:26 2003
From: liping66 at hotmail.com (liping)
Date: Mon, 11 Aug 2003 10:12:26 -0400
Subject: [R] re: two dimentional hierarchical clustering algorithm
Message-ID: <BAY1-DAV57J0LuVTD3f00029fd2@hotmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20030811/001b001a/attachment.pl

From Jan.Verbesselt at agr.kuleuven.ac.be  Mon Aug 11 16:26:05 2003
From: Jan.Verbesselt at agr.kuleuven.ac.be (Jan Verbesselt)
Date: Mon, 11 Aug 2003 16:26:05 +0200
Subject: [R] tsdiag and tsStructure for np,ns,nt and nl determination
Message-ID: <000001c36014$7fc56080$1145210a@agr.ad10.intern.kuleuven.ac.be>

Hi R-Helpers,

I'm dealing with the STL procedure and trying to apply the tsdiag and
StructTS onto the ts object to analyse the different parameters which
need to be set. How can I use the tsStructure & tsdiag to create a
seasonal, trend and cycle subseries plot so that I can select & analyse
the correct np,ns, nt and nl?

The problem is that too much signal goes into the seasonal variation and
that the trend is not followed as it should. I tried already several
values for all the parameters following the article guidelines of
Cleveland but the results don't differ a lot from each other.

Is the period (n(p)) that is needed for the STL derived from the
'frequency' set by the time series object?   Should I define the time
series so that the frequency follows the growth season (dry and wet
cycle) and not the years?


Thanks a lot in advance,
Jan (...Decomposing....)




______________________________________________________________________
Jan Verbesselt 
Research Associate 
Lab of Geomatics and Forest Engineering K.U. Leuven
Vital Decosterstraat 102. B-3000 Leuven Belgium 
Tel:+32-16-329750 
Fax: +32-16-329760
http://perswww.kuleuven.ac.be/~u0027178/VCard/mycard.php?name=janv
http://gloveg.kuleuven.ac.be/
________________________________________________________________________
__



From spencer.graves at pdf.com  Mon Aug 11 16:24:18 2003
From: spencer.graves at pdf.com (Spencer Graves)
Date: Mon, 11 Aug 2003 07:24:18 -0700
Subject: [R] Marginal (type II) SS for powers of continuous variables
	in a linear model?
References: <7ok79k725j.fsf@foo.nemo-project.org>
Message-ID: <3F37A712.2010209@pdf.com>

I'm confused.  Consider the following example:

 > Df <- data.frame(x=1:9, y=rep(c(-1,1), length=9))
 > anova(lm(y~x, Df))
Analysis of Variance Table

Response: y
           Df    Sum Sq   Mean Sq   F value Pr(>F)
x          1 2.861e-34 2.861e-34 2.253e-34      1
Residuals  7    8.8889    1.2698
 > anova(lm(y~x+I(x^2), Df))
Analysis of Variance Table

Response: y
           Df    Sum Sq   Mean Sq   F value Pr(>F)
x          1 2.861e-34 2.861e-34 2.065e-34 1.0000
I(x^2)     1    0.5772    0.5772    0.4167 0.5425
Residuals  6    8.3117    1.3853
 >
 > Df <- data.frame(x=1:9, y=rep(c(-1,1), length=9))
 > anova(lm(y~x, Df))
Analysis of Variance Table

Response: y
           Df    Sum Sq   Mean Sq   F value Pr(>F)
x          1 2.861e-34 2.861e-34 2.253e-34      1
Residuals  7    8.8889    1.2698
 > anova(lm(y~x+I(x^2), Df))
Analysis of Variance Table

Response: y
           Df    Sum Sq   Mean Sq   F value Pr(>F)
x          1 2.861e-34 2.861e-34 2.065e-34 1.0000
I(x^2)     1    0.5772    0.5772    0.4167 0.5425
Residuals  6    8.3117    1.3853
 > anova(lm(y~I(x^2)+x, Df))
Analysis of Variance Table

Response: y
           Df Sum Sq Mean Sq F value Pr(>F)
I(x^2)     1 0.0282  0.0282  0.0203 0.8912
x          1 0.5490  0.5490  0.3963 0.5522
Residuals  6 8.3117  1.3853
 >
	  In S-Plus 6.1, the ANOVA table is preceeded by a statement, "Terms 
added sequentially (first to last)".  From these examples, it certainly 
looks like this is what it is doing.  Apart from round off error, the 
sum of squares and mean squares are identical for the models without and 
with I(x^2).  In an example with a nonzero sum of squares for x, the F 
value would be different, because the mean square for residuals would be 
different, and the Pr(>F) would also be affected by differing degrees of 
freedom.

	  The third example here puts I(x^2) before x in the model statement 
and gets a clearly different anova.  (The coefficients should be not 
change when the order of the terms is modified, though they could change 
if other terms are addeed.  I didn't check that for this example, but 
I've done this before and would be surprised if they were different.)

Best Wishes,
Spencer

Bj?rn-Helge Mevik wrote:
> I've used Anova() from the car package to get marginal (aka type II)
> sum-of-squares and tests for linear models with categorical
> variables.  Is it possible to get marginal SSs also for continuous
> variables, when the model includes powers of the continuous variables?
> 
> For instance, if A and B are categorical ("factor"s) and x is
> continuous ("numeric"),
> 
> Anova (lm (y ~ A*B + x, ...))
> 
> will produce marginal SSs for all terms (A, B, A:B and x).  However,
> with 
> 
> Anova (lm (y ~ A*B + x + I(x^2), ...))
> 
> the SS for 'x' is calculated with I(x^2) present in the model, i.e. it
> is no longer marginal.
> 
> Using poly (x, 2) instead of x + I(x^2), one gets a marginal SS for
> the total effect of x, but not for the linear and quadratic effects
> separately.  (summary.aov() has a 'split' argument that can be used to
> get separate SSs, but these are not marginal.)
> 
>



From maechler at stat.math.ethz.ch  Mon Aug 11 17:03:45 2003
From: maechler at stat.math.ethz.ch (Martin Maechler)
Date: Mon, 11 Aug 2003 17:03:45 +0200
Subject: [R] leverage
In-Reply-To: <1157804.1060161408@137045-48592r.liv.ac.uk>
References: <1157804.1060161408@137045-48592r.liv.ac.uk>
Message-ID: <16183.45137.763813.477000@gargle.gargle.HOWL>

>>>>> "jane" == jane murray <jmurray at liverpool.ac.uk>
>>>>>     on Wed, 06 Aug 2003 09:16:48 +0100 writes:

    jane> Hi Can anyone help with the technique of obtaining
    jane> leverages from a conditional logistic regression
    jane> model?  The code lm.influence does not seem to work
    jane> for this data.  Thanks Jane Murray

But the generic  influence()  function {new in 1.7.0, from original
code of John Fox} has an S3 method, influence.glm(), that should work.
If not, that might indicate a bug we'd be interested to hear about.

Regards,

Martin Maechler <maechler at stat.math.ethz.ch>	http://stat.ethz.ch/~maechler/
Seminar fuer Statistik, ETH-Zentrum  LEO C16	Leonhardstr. 27
ETH (Federal Inst. Technology)	8092 Zurich	SWITZERLAND
phone: x-41-1-632-3408		fax: ...-1228			<><



From kurt.sys at UGent.be  Mon Aug 11 17:16:12 2003
From: kurt.sys at UGent.be (Kurt Sys)
Date: Mon, 11 Aug 2003 17:16:12 +0200
Subject: [R] CGIwithR->premature end?
Message-ID: <16183.45884.359046.764798@ksys.rug.ac.be>

Hello,


I try to use CGIwithR on Debian, apache web server. For one reason or
another, I get error messages such as (trying trivial.html/trivial.R),
that's when I look in the server log-files:

Mon Aug 11 16:07:17 2003] [error] [client xxx.xxx.xxx.xxx] Premature end of script headers: /usr/var/cgi-bin/R.cgi


As far as I know, the main problem with such errors is the first
line. Well, I copied the file (which comes with the package) and
didn't change the first line, so it's still

#! /bin/sh

What can be the reason for this error?

tnx,
Kurt.



-- 
Civilizations don't fight.



From p.dalgaard at biostat.ku.dk  Mon Aug 11 17:21:03 2003
From: p.dalgaard at biostat.ku.dk (Peter Dalgaard BSA)
Date: Mon, 11 Aug 2003 15:21:03 -0000
Subject: [R] leverage
In-Reply-To: <16183.45137.763813.477000@gargle.gargle.HOWL>
References: <1157804.1060161408@137045-48592r.liv.ac.uk>
	<16183.45137.763813.477000@gargle.gargle.HOWL>
Message-ID: <x2ekzsjjst.fsf@biostat.ku.dk>

Martin Maechler <maechler at stat.math.ethz.ch> writes:

> >>>>> "jane" == jane murray <jmurray at liverpool.ac.uk>
> >>>>>     on Wed, 06 Aug 2003 09:16:48 +0100 writes:
> 
>     jane> Hi Can anyone help with the technique of obtaining
>     jane> leverages from a conditional logistic regression
>     jane> model?  The code lm.influence does not seem to work
>     jane> for this data.  Thanks Jane Murray
> 
> But the generic  influence()  function {new in 1.7.0, from original
> code of John Fox} has an S3 method, influence.glm(), that should work.
> If not, that might indicate a bug we'd be interested to hear about.

I suspect you overlooked the "conditional" there. I.e. this is for
models fitted using clogit() from survival (and I think it has a
competitor, but I forgot where..) Since this works by hijacking coxph
it probably cannot work to apply GLM methods on the result.

-- 
   O__  ---- Peter Dalgaard             Blegdamsvej 3  
  c/ /'_ --- Dept. of Biostatistics     2200 Cph. N   
 (*) \(*) -- University of Copenhagen   Denmark      Ph: (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)             FAX: (+45) 35327907



From jmurray at liverpool.ac.uk  Mon Aug 11 17:42:55 2003
From: jmurray at liverpool.ac.uk (jane murray)
Date: Mon, 11 Aug 2003 16:42:55 +0100
Subject: [R] leverage
In-Reply-To: <x2ekzsjjst.fsf@biostat.ku.dk>
References: <x2ekzsjjst.fsf@biostat.ku.dk>
Message-ID: <27822576.1060620175@137045-48592r.liv.ac.uk>

Thank  very much you for your comments.  If you have any suggestions on any 
commands that would work, I'd be very grateful.
Thanks
Jane

--On 11 August 2003 17:28 +0200 Peter Dalgaard BSA 
<p.dalgaard at biostat.ku.dk> wrote:

> Martin Maechler <maechler at stat.math.ethz.ch> writes:
>
>> >>>>> "jane" == jane murray <jmurray at liverpool.ac.uk>
>> >>>>>     on Wed, 06 Aug 2003 09:16:48 +0100 writes:
>>
>>     jane> Hi Can anyone help with the technique of obtaining
>>     jane> leverages from a conditional logistic regression
>>     jane> model?  The code lm.influence does not seem to work
>>     jane> for this data.  Thanks Jane Murray
>>
>> But the generic  influence()  function {new in 1.7.0, from original
>> code of John Fox} has an S3 method, influence.glm(), that should work.
>> If not, that might indicate a bug we'd be interested to hear about.
>
> I suspect you overlooked the "conditional" there. I.e. this is for
> models fitted using clogit() from survival (and I think it has a
> competitor, but I forgot where..) Since this works by hijacking coxph
> it probably cannot work to apply GLM methods on the result.
>
> --
>    O__  ---- Peter Dalgaard             Blegdamsvej 3
>   c/ /'_ --- Dept. of Biostatistics     2200 Cph. N
>  (*) \(*) -- University of Copenhagen   Denmark      Ph: (+45) 35327918
> ~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)             FAX: (+45) 35327907



From ripley at stats.ox.ac.uk  Mon Aug 11 17:47:50 2003
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Mon, 11 Aug 2003 16:47:50 +0100 (BST)
Subject: [R] Marginal (type II) SS for powers of continuous variables in
	a linear model?
In-Reply-To: <3F37A712.2010209@pdf.com>
Message-ID: <Pine.LNX.4.44.0308111642230.14734-100000@gannet.stats>

Anova != anova.

drop1 is the part of R that does type II sum of squares, and it works in 
your example.  So does Anova in the current car:

> drop1(lm(y~x+I(x^2), Df))  # add test="F" if you like
Single term deletions

Model:
y ~ x + I(x^2)
       Df Sum of Sq    RSS    AIC
<none>              8.3117 5.2839
x       1    0.5490 8.8607 3.8596
I(x^2)  1    0.5772 8.8889 3.8882
> library(car)
> Anova(lm(y~x+I(x^2), Df))
Anova Table (Type II tests)

Response: y
          Sum Sq Df F value Pr(>F)
x         0.5490  1  0.3963 0.5522
I(x^2)    0.5772  1  0.4167 0.5425
Residuals 8.3117  6               


And in summary.aov() those *are* marginal SS, as balance is assumed
for aov models. (That is not to say the software does not work otherwise, 
but the interpretability depends on balance.)


On Mon, 11 Aug 2003, Spencer Graves wrote:

> I'm confused.  Consider the following example:
> 
>  > Df <- data.frame(x=1:9, y=rep(c(-1,1), length=9))
>  > anova(lm(y~x, Df))
> Analysis of Variance Table
> 
> Response: y
>            Df    Sum Sq   Mean Sq   F value Pr(>F)
> x          1 2.861e-34 2.861e-34 2.253e-34      1
> Residuals  7    8.8889    1.2698
>  > anova(lm(y~x+I(x^2), Df))
> Analysis of Variance Table
> 
> Response: y
>            Df    Sum Sq   Mean Sq   F value Pr(>F)
> x          1 2.861e-34 2.861e-34 2.065e-34 1.0000
> I(x^2)     1    0.5772    0.5772    0.4167 0.5425
> Residuals  6    8.3117    1.3853
>  >
>  > Df <- data.frame(x=1:9, y=rep(c(-1,1), length=9))
>  > anova(lm(y~x, Df))
> Analysis of Variance Table
> 
> Response: y
>            Df    Sum Sq   Mean Sq   F value Pr(>F)
> x          1 2.861e-34 2.861e-34 2.253e-34      1
> Residuals  7    8.8889    1.2698
>  > anova(lm(y~x+I(x^2), Df))
> Analysis of Variance Table
> 
> Response: y
>            Df    Sum Sq   Mean Sq   F value Pr(>F)
> x          1 2.861e-34 2.861e-34 2.065e-34 1.0000
> I(x^2)     1    0.5772    0.5772    0.4167 0.5425
> Residuals  6    8.3117    1.3853
>  > anova(lm(y~I(x^2)+x, Df))
> Analysis of Variance Table
> 
> Response: y
>            Df Sum Sq Mean Sq F value Pr(>F)
> I(x^2)     1 0.0282  0.0282  0.0203 0.8912
> x          1 0.5490  0.5490  0.3963 0.5522
> Residuals  6 8.3117  1.3853
>  >
> 	  In S-Plus 6.1, the ANOVA table is preceeded by a statement, "Terms 
> added sequentially (first to last)".  From these examples, it certainly 
> looks like this is what it is doing.  Apart from round off error, the 
> sum of squares and mean squares are identical for the models without and 
> with I(x^2).  In an example with a nonzero sum of squares for x, the F 
> value would be different, because the mean square for residuals would be 
> different, and the Pr(>F) would also be affected by differing degrees of 
> freedom.
> 
> 	  The third example here puts I(x^2) before x in the model statement 
> and gets a clearly different anova.  (The coefficients should be not 
> change when the order of the terms is modified, though they could change 
> if other terms are addeed.  I didn't check that for this example, but 
> I've done this before and would be surprised if they were different.)
> 
> Best Wishes,
> Spencer
> 
> Bj?rn-Helge Mevik wrote:
> > I've used Anova() from the car package to get marginal (aka type II)
> > sum-of-squares and tests for linear models with categorical
> > variables.  Is it possible to get marginal SSs also for continuous
> > variables, when the model includes powers of the continuous variables?
> > 
> > For instance, if A and B are categorical ("factor"s) and x is
> > continuous ("numeric"),
> > 
> > Anova (lm (y ~ A*B + x, ...))
> > 
> > will produce marginal SSs for all terms (A, B, A:B and x).  However,
> > with 
> > 
> > Anova (lm (y ~ A*B + x + I(x^2), ...))
> > 
> > the SS for 'x' is calculated with I(x^2) present in the model, i.e. it
> > is no longer marginal.
> > 
> > Using poly (x, 2) instead of x + I(x^2), one gets a marginal SS for
> > the total effect of x, but not for the linear and quadratic effects
> > separately.  (summary.aov() has a 'split' argument that can be used to
> > get separate SSs, but these are not marginal.)

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From hedderik at cmu.edu  Mon Aug 11 17:58:44 2003
From: hedderik at cmu.edu (Hedderik van Rijn)
Date: Mon, 11 Aug 2003 11:58:44 -0400
Subject: [R] read.spss doesn't work anymore [more info]
In-Reply-To: <C0CC264A-CB77-11D7-BFE2-000A956B93BA@cmu.edu>
Message-ID: <AF711F3E-CC14-11D7-BFE2-000A956B93BA@cmu.edu>

Some more information about my read.spss issues: on a cleanly installed 
Debian Woody machine:

~/TMP % R

R : Copyright 2003, The R Development Core Team
Version 1.7.1  (2003-06-16)

[...]

 > library(foreign); x <- read.spss("dataDef.sav")
Error in read.spss("dataDef.sav") : Calloc could not allocate 
(-2147483648 of 1) memory
 > x <- read.spss("dataDef.sav")
 > x <- read.spss("dataDef.sav")
Segmentation fault

However, using R Version 1.7.0  (2003-04-16) installed using Fink on 
Mac OS X, the .sav file reads without a problem.

The .sav file has in its header the following reference:

SPSS DATA FILE SPSS for Unix Release 6.1 (HP9000 700 )

Although for the moment I'm reading in a Mac OS X R save'd version of 
the SPSS data file, I would like to read in the original .sav file. 
Does anyone know what the problem might be? The .sav file can be 
downloaded from:

   http://viropage.psy.cmu.edu/~rijn/dataDef.sav.bz2  (approx 100 kb)

  - Hedderik.


On Sunday, Aug 10, 2003, at 17:15 US/Eastern, Hedderik van Rijn wrote:

> A couple of months ago, probably using an older version of R, R used 
> to run the following code just fine:
>
>  library("foreign")
>  data.exp1 <- as.data.frame(read.spss("dataDef.sav"))
>
> Issuing the same commands now (after starting R using --vanilla), 
> gives me the following behavior:
>
> > library("foreign")
> > x <- read.spss("dataDef.sav")
> Error in read.spss("dataDef.sav") : Calloc could not allocate 
> (-2147483648 of 1) memory
> > x <- read.spss("dataDef.sav")
> > x <- read.spss("dataDef.sav")
> Segmentation fault
>
> The first two assignments return instantaneously, after the second x 
> contains all original SPSS variable/column names but no data, after 
> the last read.spss - it takes a while before R returns with the 
> Segmentation fault. During that time, the harddisk seems to be working 
> quite hard.
>
> Does anyone know what the problem might be?
>
>  - Hedderik.
>
> > version
>          _
> platform i386-pc-linux-gnu
> arch     i386
> os       linux-gnu
> system   i386, linux-gnu
> status
> major    1
> minor    7.1
> year     2003
> month    06
> day      16
> language R
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
>



From jonathan.williams at pharmacology.oxford.ac.uk  Mon Aug 11 18:42:19 2003
From: jonathan.williams at pharmacology.oxford.ac.uk (Jonathan Williams)
Date: Mon, 11 Aug 2003 17:42:19 +0100
Subject: [R] Designing and incorporating a digital filter
Message-ID: <NGBBKJEMOMLJFCOIEGCEKEJEJJAA.jonathan.williams@pharm.ox.ac.uk>

I have a time series of data from an electroencephalogram (EEG).
I wish to filter the data to get rid of 50Hz mains 'hum'. I have
'designed' a combination bandpass and notch filter using a web-
site. The site returns the filter in "ANSI C" source code. It is:-

/* Digital filter designed by mkfilter/mkshape/gencode   A.J. Fisher
   Command line: /www/usr/fisher/helpers/mkfilter -Bu -Bp -o 5 -a
1.0000000000e-02 1.0000000000e-01 -Z 2.0000000000e-01 -l */

#define NZEROS 12
#define NPOLES 12
#define GAIN   1.548068051e+03

static float xv[NZEROS+1], yv[NPOLES+1];

static void filterloop()
  { for (;;)
      { xv[0] = xv[1]; xv[1] = xv[2]; xv[2] = xv[3]; xv[3] = xv[4]; xv[4] =
xv[5]; xv[5] = xv[6]; xv[6] = xv[7]; xv[7] = xv[8]; xv[8] = xv[9]; xv[9] =
xv[10]; xv[10] = xv[11]; xv[11] = xv[12];
        xv[12] = `next input value' / GAIN;
        yv[0] = yv[1]; yv[1] = yv[2]; yv[2] = yv[3]; yv[3] = yv[4]; yv[4] =
yv[5]; yv[5] = yv[6]; yv[6] = yv[7]; yv[7] = yv[8]; yv[8] = yv[9]; yv[9] =
yv[10]; yv[10] = yv[11]; yv[11] = yv[12];
        yv[12] =   (xv[12] - xv[0]) +   0.6180339888 * (xv[1] - xv[11]) + 4
* (xv[2] - xv[10])
                     +   3.0901699437 * (xv[9] - xv[3]) + 5 * (xv[8] -
xv[4]) +   6.1803398875 * (xv[5] - xv[7])
                     - 1.77636e-15 * xv[6]
                     + ( -0.0000000000 * yv[0]) + ( -0.0000000000 * yv[1])
                     + ( -0.1555326685 * yv[2]) + (  1.8017745109 * yv[3])
                     + ( -9.4758632417 * yv[4]) + ( 29.7961492230 * yv[5])
                     + (-62.0371858570 * yv[6]) + ( 89.3642283590 * yv[7])
                     + (-90.1867413610 * yv[8]) + ( 62.9470473210 * yv[9])
                     + (-29.0651076170 * yv[10]) + (  8.0112312881 *
yv[11]);
        `next output value' = yv[12];
      }
  }

I'd like to know how to incorporate this into my R code (or, better, I'd
like to know how to design bandpass filters directly in R - the help file
for the filter() function does not detail how to do this)

Thank you
Jonathan Williams

Jonathan Williams
OPTIMA
Radcliffe Infirmary
Woodstock Road
OXFORD OX2 6HE
Tel +1865 (2)24356



From puffin at cebc.cnrs.fr  Mon Aug 11 18:02:45 2003
From: puffin at cebc.cnrs.fr (Pinaud David)
Date: Mon, 11 Aug 2003 18:02:45 +0200
Subject: [R] pb of importation data: truncation...
Message-ID: <3F37BE25.D5A74574@cebc.cnrs.fr>

Dear all,
I'm  importing some data (with read.table, file ".txt") but I have some
problem: R truncates my numeric data at 2 digits during the
importation... It's the first time that I see that.
Thanks for an answer.

David

From bates at stat.wisc.edu  Mon Aug 11 18:47:40 2003
From: bates at stat.wisc.edu (Douglas Bates)
Date: Mon, 11 Aug 2003 16:47:40 -0000
Subject: [R] pb of importation data: truncation...
In-Reply-To: <3F37BE25.D5A74574@cebc.cnrs.fr>
References: <3F37BE25.D5A74574@cebc.cnrs.fr>
Message-ID: <6rwudk2la1.fsf@bates4.stat.wisc.edu>

Pinaud David <puffin at cebc.cnrs.fr> writes:

> I'm  importing some data (with read.table, file ".txt") but I have some
> problem: R truncates my numeric data at 2 digits during the
> importation... It's the first time that I see that.

Are you sure that R is truncating the data itself or just truncating
the printed representation of the data?  The value that is printed is
truncated according to the setting of the "digits" option.

Use the dput function to check on the actual value that is stored.
For example

> pi
[1] 3.141593
> dput(pi)
3.14159265358979



From p.dalgaard at biostat.ku.dk  Mon Aug 11 19:49:09 2003
From: p.dalgaard at biostat.ku.dk (Peter Dalgaard BSA)
Date: Mon, 11 Aug 2003 17:49:09 -0000
Subject: [R] read.spss doesn't work anymore [more info]
In-Reply-To: <AF711F3E-CC14-11D7-BFE2-000A956B93BA@cmu.edu>
References: <AF711F3E-CC14-11D7-BFE2-000A956B93BA@cmu.edu>
Message-ID: <x2adagjcy3.fsf@biostat.ku.dk>

Hedderik van Rijn <hedderik at cmu.edu> writes:

> Some more information about my read.spss issues: on a cleanly
> installed Debian Woody machine:
> 
> ~/TMP % R
> 
> R : Copyright 2003, The R Development Core Team
> Version 1.7.1  (2003-06-16)
> 
> [...]
> 
>  > library(foreign); x <- read.spss("dataDef.sav")
> Error in read.spss("dataDef.sav") : Calloc could not allocate
> (-2147483648 of 1) memory
>  > x <- read.spss("dataDef.sav")
>  > x <- read.spss("dataDef.sav")
> Segmentation fault
> 
> However, using R Version 1.7.0  (2003-04-16) installed using Fink on
> Mac OS X, the .sav file reads without a problem.
> 
> The .sav file has in its header the following reference:
> 
> SPSS DATA FILE SPSS for Unix Release 6.1 (HP9000 700 )
> 
> Although for the moment I'm reading in a Mac OS X R save'd version of
> the SPSS data file, I would like to read in the original .sav file.
> Does anyone know what the problem might be? The .sav file can be
> downloaded from:
> 
>    http://viropage.psy.cmu.edu/~rijn/dataDef.sav.bz2  (approx 100 kb)


Thanks. I've debugged a little, and it seems that the primary reason
is that a bogus value is read for n_lines in read_documents (file
sfm_read.c). 

Once this has failed, the code paints itself into a corner by checking
whether the file was already open in sfm_read_dictionary; this test
succeeds, but the content of the dict structure remains bogus... I
can't reproduce your segfault, but as I said, this phenomenon is
probably secondary to the real problem anyway.

The value that is getting read for n_lines is 134217728 aka 0x8000000
which looks like it could be intended as a special value?

> On Sunday, Aug 10, 2003, at 17:15 US/Eastern, Hedderik van Rijn wrote:
> 
> > A couple of months ago, probably using an older version of R, R used
> > to run the following code just fine:
> >
> >  library("foreign")
> >  data.exp1 <- as.data.frame(read.spss("dataDef.sav"))
> >
> > Issuing the same commands now (after starting R using --vanilla),
> > gives me the following behavior:
> >
> > > library("foreign")
> > > x <- read.spss("dataDef.sav")
> > Error in read.spss("dataDef.sav") : Calloc could not allocate
> > (-2147483648 of 1) memory
> > > x <- read.spss("dataDef.sav")
> > > x <- read.spss("dataDef.sav")
> > Segmentation fault
> >
> > The first two assignments return instantaneously, after the second x
> > contains all original SPSS variable/column names but no data, after
> > the last read.spss - it takes a while before R returns with the
> > Segmentation fault. During that time, the harddisk seems to be
> > working quite hard.
> >
> > Does anyone know what the problem might be?
> >
> >  - Hedderik.
> >
> > > version
> >          _
> > platform i386-pc-linux-gnu
> > arch     i386
> > os       linux-gnu
> > system   i386, linux-gnu
> > status
> > major    1
> > minor    7.1
> > year     2003
> > month    06
> > day      16
> > language R
> >
> > ______________________________________________
> > R-help at stat.math.ethz.ch mailing list
> > https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> >
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> 

-- 
   O__  ---- Peter Dalgaard             Blegdamsvej 3  
  c/ /'_ --- Dept. of Biostatistics     2200 Cph. N   
 (*) \(*) -- University of Copenhagen   Denmark      Ph: (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)             FAX: (+45) 35327907



From ahenningsen at email.uni-kiel.de  Mon Aug 11 20:34:14 2003
From: ahenningsen at email.uni-kiel.de (Arne Henningsen)
Date: Mon, 11 Aug 2003 20:34:14 +0200
Subject: [R] Weighted SUR, 2SLS regressions
In-Reply-To: <200307161018.h6GA2geR017480@stat.math.ethz.ch>
References: <200307161018.h6GA2geR017480@stat.math.ethz.ch>
Message-ID: <200308112034.14588.ahenningsen@email.uni-kiel.de>

On Wednesday 16 July 2003 11:04:58 +0300, "Yonathan (Jon) Anson" 
<anson at bgumail.bgu.ac.il> wrote:
>
> Is there an option for running SUR and 2SLS regressions with weighting
> (I am analysing mortality in towns, hence want to weight by population
> size)
>
> Many thanks
>
> Jon Anson

The "systemfit" package can run SUR and 2SLS regressions. However, systemfit 
does not (yet) offer the user to provide weights for the regression. I did 
not hear anything about these regression methods so far. Can you tell me an 
article or textbook that explains how this should work? If this is not too 
much work, these methods might be implement in a new version of the systemfit 
package. (Jeff (the author and maintainer of systemfit) and I are just 
preparing a new version of the systemfit package that has several additional 
features (e.g. cross-equation restrictions.) 

Best wishes,
Arne



From TyagiAnupam at aol.com  Mon Aug 11 22:08:19 2003
From: TyagiAnupam at aol.com (TyagiAnupam@aol.com)
Date: Mon, 11 Aug 2003 16:08:19 EDT
Subject: [R] Weighted SUR, 2SLS regressions
Message-ID: <1df.ed61cda.2c6951b3@aol.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20030811/264afc55/attachment.pl

From jahumada at usgs.gov  Mon Aug 11 22:58:20 2003
From: jahumada at usgs.gov (Jorge A Ahumada)
Date: Mon, 11 Aug 2003 15:58:20 -0500
Subject: [R] Changing default browser in options()
Message-ID: <89CD4108-CC3E-11D7-BFEB-000393DEDD9C@usgs.gov>

I usually startup R from within several directories (usually where the 
programs or data of interest are).  I like to use a small web browser 
called dillo to browse help files.  However, by default R looks for 
mozilla, so I have to type every single R session:

 > options(browser='dillo')

Is there anyway I can change this globally?

thanks,


J.



From AdamL at spc.int  Mon Aug 11 22:58:52 2003
From: AdamL at spc.int (Adam Langley)
Date: Tue, 12 Aug 2003 07:58:52 +1100
Subject: [R] Gradient of the slope of a surface
Message-ID: <27DF1E0087070642B128C853E74FCCAF0218C821@tazar.spc.int>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20030812/1fa8b374/attachment.pl

From tobias_verbeke at skynet.be  Mon Aug 11 23:22:06 2003
From: tobias_verbeke at skynet.be (Tobias Verbeke)
Date: Mon, 11 Aug 2003 23:22:06 +0200
Subject: [R] Changing default browser in options()
In-Reply-To: <89CD4108-CC3E-11D7-BFEB-000393DEDD9C@usgs.gov>
References: <89CD4108-CC3E-11D7-BFEB-000393DEDD9C@usgs.gov>
Message-ID: <20030811232206.52f6b698.tobias_verbeke@skynet.be>

> so I have to type every single R session:

>  > options(browser='dillo')
> 
> Is there anyway I can change this globally?

If you put the line
  
    options(browser='dillo')

in the .Rprofile of your home directory, it
will fire up dillo the next time.




HTH,

Tobias



From hi_ono2001 at ybb.ne.jp  Mon Aug 11 23:52:56 2003
From: hi_ono2001 at ybb.ne.jp (Hisaji Ono)
Date: Tue, 12 Aug 2003 06:52:56 +0900
Subject: [R] tutorials of  seao & seao-gui package for R 
Message-ID: <002401c36052$ec91aaa0$818001db@webgis>

Hi.

I've been very interested in your seao & seao-gui package for R.

However manuals of seao & seao-gui package for R have doesn't include
examples, so I can't understand how
to use them.

Could you tell me availability of tutorials of them  in English.

 Regards.



From ok at cs.otago.ac.nz  Tue Aug 12 00:24:41 2003
From: ok at cs.otago.ac.nz (Richard A. O'Keefe)
Date: Tue, 12 Aug 2003 10:24:41 +1200 (NZST)
Subject: [R] subscripts in lists
Message-ID: <200308112224.h7BMOfmk046226@atlas.otago.ac.nz>

Chris Knight <christopher.knight at plant-sciences.oxford.ac.uk> has
	
	lis<-list(c("a","b","next","want1","c"),c("d", "next", "want2", "a"))
	
and wants c("want1","want2")


Step 1:
    inx <- sapply(lis, function(x) which(x == "next")) + 1
==> 4 3

Step 2:
    sapply(1:length(lis), function(i) lis[[i]][inx[i]])
==> "want1" "want2"

Think about this for a bit and restructure it:

    sapply(1:length(lis), function (i) {v <- lis[[i]]; v[which(v=="next")+1]})

Wrap it up:

    after <- function(lis, what="next") {
	sapply(1:length(lis), function (i) {
	    v <- lis[[i]]
	    v[which(v == what)+1]
	})
    }

Of course, from my point of view, a call to sapply() *is* a loop, just
packaged slightly differently.  I think this is reasonably clear.



From ok at cs.otago.ac.nz  Tue Aug 12 01:01:50 2003
From: ok at cs.otago.ac.nz (Richard A. O'Keefe)
Date: Tue, 12 Aug 2003 11:01:50 +1200 (NZST)
Subject: [R] subscripts in lists
Message-ID: <200308112301.h7BN1oL5049343@atlas.otago.ac.nz>

I suggested
	    sapply(1:length(lis), function (i) {v <- lis[[i]]; v[which(v=="next")+1]})
	
Of course that was really dumb.  It can be simplified, because the index i
is only used to select a list element, which sapply() wants to do for me
anyway.  It should be

    sapply(lis, function(v) v[which(v=="next")+1])

Perhaps the interesting thing is how one gets there.
- The result should be a character vector, not a list, so use sapply()
- The index of a list element does not enter into the calculation of
  the result, so use sapply(a.list, function (an.element) some.calculation)
- For list element, we want to find where something occurs, so use
  which(the.element == the.value.we.want.to.find)
- We want the element after that, so the.element[..... + 1]
and the code (NOT the code I first thought of) practically writes itself.

If I had used backwards reasoning like this, I'd have got there first thing;
what led me to produce an inferior version was using forwards reasoning,
and I *know* better than to do that.  *Sigh.*

The other approach is not to focus on the list structure at all,
but to flatten it into a single sequence:

    {u <- unlist(lis); u[which(u=="next")+1]}

Of course, if some list element should not contain "next" exactly once,
these two versions would give different results.

We can also expect some kind of performance difference.  My expectation
was that as the "unlist" version has to build a data structure (the
flattened list) which is not part of the result, the "unlist" version
would be inferior.  But one must not trust to intuition; this is an
empirical question deserving an empirical answer.  I did this:

lis <- list(c("a","b","next","want1","c"), c("d","next","want2","a"))
f1 <- function(lis) sapply(lis, function(v) v[which(v=="next")+1])
f2 <- function(lis) {lis<-unlist(lis); lis[which(lis=="next")+1]}

system.time(for(i in 1:10000) f1(lis))
[1] 22.03  7.56 30.97  0.00  0.00
system.time(for(i in 1:10000) f2(lis))
[1] 5.38 1.65 7.44 0.00 0.00

Hmm, unlist is about 4 times faster.  Is that still true with
bigger lists?
lis <- list(lis[[1]],lis[[2]],lis[[1]],lis[[2]],lis[[1]],lis[[2]],
            lis[[1]],lis[[2]],lis[[1]],lis[[2]],lis[[1]],lis[[2]])

system.time(for(i in 1:4000) f1(lis))
[1] 30.91  9.66 42.06  0.00  0.00
> system.time(for(i in 1:4000) f2(lis))
[1] 2.96 0.65 3.67 0.00 0.00

Yep, it holds up.

This is by no means an exhaustive study, but it certainly suggests that
the "unlist" version may be faster than the "sapply" version.

Here's why my intuition was wrong:  the "sapply" version calls a user-
defined function once for each element of the result, while the "unlist"
version uses nothing but built in operations.  Calling user-defined
functions is currently slow in R.



From den.duurs at lycos.com  Tue Aug 12 01:50:42 2003
From: den.duurs at lycos.com (Remko Duursma)
Date: Mon, 11 Aug 2003 16:50:42 -0700
Subject: [R] error message in "sem"
Message-ID: <CAGIJHLCONHJMGAA@mailcity.com>

Dear R-helpers,

i get a mysterious error message when using sem,  it reads:


"Error in tapply(grad.P[arrows.2.free], ram[ram[, 1] == 2 & ram[, 4] !=  : 
        arguments must have same length"

what does this mean?

remko



^'~,_,~'^'~,_,~'^'~,_,~'^'~,_,~'^'~,_,~'^'~,_,~'
Remko Duursma, Ph.D. student
Forest Biometrics Lab / Idaho Stable Isotope Lab
University of Idaho, Moscow, ID, U.S.A.



From jfox at mcmaster.ca  Tue Aug 12 03:34:02 2003
From: jfox at mcmaster.ca (John Fox)
Date: Mon, 11 Aug 2003 21:34:02 -0400
Subject: [R] error message in "sem"
In-Reply-To: <CAGIJHLCONHJMGAA@mailcity.com>
Message-ID: <5.1.0.14.2.20030811213255.01fe1498@127.0.0.1>

Dear Remko,

Can you supply the input that produced this error message?

John

At 04:50 PM 8/11/2003 -0700, Remko Duursma wrote:
>Dear R-helpers,
>
>i get a mysterious error message when using sem,  it reads:
>
>
>"Error in tapply(grad.P[arrows.2.free], ram[ram[, 1] == 2 & ram[, 4] !=  :
>         arguments must have same length"
>
>what does this mean?
>
>remko

-----------------------------------------------------
John Fox
Department of Sociology
McMaster University
Hamilton, Ontario, Canada L8S 4M4
email: jfox at mcmaster.ca
phone: 905-525-9140x23604
web: www.socsci.mcmaster.ca/jfox



From markhall at gol.com  Tue Aug 12 05:14:26 2003
From: markhall at gol.com (Mark Hall)
Date: Tue, 12 Aug 2003 03:14:26 +0000
Subject: [R] Re: Bayesian stats
Message-ID: <E19mPcA-0005Hv-J6@smtp02.fields.gol.com>


In addition to Gelman et al, there is also Jeff Gill's BAYESIAN METHODS FOR
THE SOCIAL AND BEHAVIORAL SCIENCES with accompanying R/SPLUS code.  You
have to be careful though, at least in the first printing, it isn't always
clear when the code is for R versus SPlus.  The code is available on
his web site at http://www.clas.ufl.edu/~jgill/

Best, MEH

Mark Hall
markhall at gol.com



From rozman at medicina.ub.es  Tue Aug 12 08:19:16 2003
From: rozman at medicina.ub.es (Ciril Rozman)
Date: Tue, 12 Aug 2003 08:19:16 +0200
Subject: [R] Programme Maxstat
Message-ID: <000b01c36099$aa8dfa40$259174a1@ub.es>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20030812/78ee95dd/attachment.pl

From ripley at stats.ox.ac.uk  Tue Aug 12 08:40:36 2003
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Tue, 12 Aug 2003 07:40:36 +0100 (BST)
Subject: [R] Changing default browser in options()
In-Reply-To: <89CD4108-CC3E-11D7-BFEB-000393DEDD9C@usgs.gov>
Message-ID: <Pine.LNX.4.44.0308120733210.15889-100000@gannet.stats>

On Mon, 11 Aug 2003, Jorge A Ahumada wrote:

> I usually startup R from within several directories (usually where the 
> programs or data of interest are).  I like to use a small web browser 
> called dillo to browse help files.  However, by default R looks for 
> mozilla, so I have to type every single R session:

Actually R looks for browser specified at configure time (if this is a 
Unix-alike).  So the first idea is to configure R to use dillo.
The default setting of options("browser") is set to be

options(browser = as.vector(Sys.getenv("R_BROWSER")))

and the browser found at configure time is set in the R_BROWSER variable 
in the file etc/Renviron.  So the second idea is to edit that file.

Finally, if this is a shared system, you can have the following in your
~/.Renviron file

R_BROWSER=${R_BROWSER-'dillo'}

See ?Startup.  (You could also use the Rprofile files, but people normally 
only have one .Renviron file.)

> 
>  > options(browser='dillo')
> 
> Is there anyway I can change this globally?

Several: see above.

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From Giles.Heywood at CommerzbankIB.com  Mon Aug 11 08:27:03 2003
From: Giles.Heywood at CommerzbankIB.com (Heywood, Giles)
Date: Mon, 11 Aug 2003 07:27:03 +0100
Subject: [R] New package: irregular time-series (its)
Message-ID: <8CBAA121CEB4D5118CB200508BB2BBEF54043E@xmx8lonib.lonib.commerzbank.com>

I have uploaded to CRAN a new package named 'its' (Irregular Time-Series).
It
implements irregular time-series as an S4 class, extending the matrix class,
and records the time-stamp of each row in the matrix using POSIX.  Print,
plot,
extraction, append, and related functionality are available.

Feedback and suggestions are welcome.

Giles Heywood


********************************************************************** 
This is a commercial communication from Commerzbank AG.\ \ T...{{dropped}}

_______________________________________________
R-announce at stat.math.ethz.ch mailing list
https://www.stat.math.ethz.ch/mailman/listinfo/r-announce



From s-plus at wiwi.uni-bielefeld.de  Tue Aug 12 08:57:12 2003
From: s-plus at wiwi.uni-bielefeld.de (Peter Wolf)
Date: Tue, 12 Aug 2003 08:57:12 +0200
Subject: [R] subscripts in lists
References: <200308112224.h7BMOfmk046226@atlas.otago.ac.nz>
Message-ID: <3F388FC8.5050102@wiwi.uni-bielefeld.de>

What about ...

 > unlist(lis)[which(unlist(lis)=="next")+1]
[1] "want1" "want2"

... to avoid the loop in sapply?


Richard A. O'Keefe wrote:

>Chris Knight <christopher.knight at plant-sciences.oxford.ac.uk> has
>	
>	lis<-list(c("a","b","next","want1","c"),c("d", "next", "want2", "a"))
>	
>and wants c("want1","want2")
>
>
>Step 1:
>    inx <- sapply(lis, function(x) which(x == "next")) + 1
>==> 4 3
>
>Step 2:
>    sapply(1:length(lis), function(i) lis[[i]][inx[i]])
>==> "want1" "want2"
>
>Think about this for a bit and restructure it:
>
>    sapply(1:length(lis), function (i) {v <- lis[[i]]; v[which(v=="next")+1]})
>
>Wrap it up:
>
>    after <- function(lis, what="next") {
>	sapply(1:length(lis), function (i) {
>	    v <- lis[[i]]
>	    v[which(v == what)+1]
>	})
>    }
>
>Of course, from my point of view, a call to sapply() *is* a loop, just
>packaged slightly differently.  I think this is reasonably clear.
>
--------------------------------------------------------
Peter Wolf
Department of economics
University of Bielefeld
pwolf at wiwi.uni-bielefeld.de



From lutz.thieme at amd.com  Tue Aug 12 09:37:23 2003
From: lutz.thieme at amd.com (lutz.thieme@amd.com)
Date: Tue, 12 Aug 2003 09:37:23 +0200
Subject: [R] who to rbind of a list of data.frames
Message-ID: <E540DF203FFED21182EB0008C728756010628E37@deexmta4.amd.com>

Hello everybody,

could anybody give me a hint, who I can use rbind on a list of data.frames, please?

I have a list with a large number of data.frames of the same structure, like:
LIST	<- list(X1=data.frame(a=1,b=2), X2=data.frame(a=3,b=4), X3=data.frame(a=5,b=6), ...., XN=data.frame(a=i,b=k))

I would like to bind all data.frames very fast to a single data.frame, something like that:
DF	<- rbind(LIST$X1, LIST$X2, LIST$X3, ..., LIST$XN)

But for performance (speed) reasons I won't use a loop. I also couldn't find a solution how 
I could use lapply. One constraint is, that the number of data.frames in the list is not determined 
and can vary from one to more than thousand. 
Any help is apreciated, thank you in advance.

	Kind regards,

	Lutz


	Lutz Thieme
	Product Engineering
	AMD Saxony Limited Liability Company & Co. KG
	M/S E22-PE, Wilschdorfer Landstr. 101
	D-01109 Dresden, Gemany
	phone:	+ 49-351-277 -  4269
	fax:		+ 49-351-277-9-4269



From ripley at stats.ox.ac.uk  Tue Aug 12 10:19:39 2003
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Tue, 12 Aug 2003 09:19:39 +0100 (BST)
Subject: [R] subscripts in lists
In-Reply-To: <3F388FC8.5050102@wiwi.uni-bielefeld.de>
Message-ID: <Pine.LNX.4.44.0308120911540.21717-100000@gannet.stats>

On Tue, 12 Aug 2003, Peter Wolf wrote:

> What about ...
> 
>  > unlist(lis)[which(unlist(lis)=="next")+1]
> [1] "want1" "want2"
> 
> ... to avoid the loop in sapply?

That has a loop in each unlist, of course.  (A slicker version of that
solution was posted earlier by Richard.)

Note that you have to loop over the list elements to do this: both sapply
(really lapply) and unlist do that in C code.  It's not even clear to me
that it is worth avoiding looping in R code: for() loops in R are pretty
efficient, and the collective memory of problems in S+3.x (x < 4) is still
affecting decisions today.


-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From hothorn at ci.tuwien.ac.at  Tue Aug 12 10:21:15 2003
From: hothorn at ci.tuwien.ac.at (Torsten Hothorn)
Date: Tue, 12 Aug 2003 10:21:15 +0200 (CEST)
Subject: [R] Programme Maxstat
In-Reply-To: <000b01c36099$aa8dfa40$259174a1@ub.es>
Message-ID: <Pine.LNX.3.96.1030812100754.31062G-100000@thorin.ci.tuwien.ac.at>

On Tue, 12 Aug 2003, Ciril Rozman wrote:

> Sirs,
> I have recently been interested in your Maxstat. 

For questions related to add-on packages please at least cc to the
maintainer since some might not read r-help regulary.

> I have computed 
> with my own programme the ranks (by using the Kaplan-Meier method and
> the
> log-rank test) with the formula (Observed-Expected)/(SQR Var). 

The problem is what "SQR Var" really means. The maxstat package uses a
reformulation of the logrank test as a linear rank statistic with special
scores and, more important, uses a conditional variance estimator which is
slightly different from the one implemented in "survdiff", for example.

> The
> results are similar but not exact to the M value obtained with the
> Maxstat. I would like to know whether you are using some correction or
> adjustment in computing the different ranks. 

The details can be found in the references cited in the documentation of
the package, especially Hothorn & Lausen, 2003, CSDA, freely available
from

http://www.elseviermathematics.com/mathematicsweb/show/Main.htt?Name=&Source=Journal%2Ehtt&Key=Biostatistics&Results=Journal%2Ehtt&External=mathematicsweb%2F15

Best,

Torsten

> Thank you very much for
> your help.
> 
> Dr. C. Rozman
> Professor of Medicine, Emeritus
> University of Barcelona, Spain
> E-mail : rozman at medicina.ub.es
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> 
>



From ripley at stats.ox.ac.uk  Tue Aug 12 10:38:50 2003
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Tue, 12 Aug 2003 09:38:50 +0100 (BST)
Subject: [R] who to rbind of a list of data.frames
In-Reply-To: <E540DF203FFED21182EB0008C728756010628E37@deexmta4.amd.com>
Message-ID: <Pine.LNX.4.44.0308120935190.21795-100000@gannet.stats>

> do.call("rbind", LIST)

does exactly

> DF <- rbind(LIST$X1, LIST$X2, LIST$X3, ..., LIST$XN)

See the help on do.call.

Not that rbind.data.frame is `very fast'.  If your data frames are
all of exactly the same type (no coercions needed, factors with the same 
set of levels, ...) it may be faster to create a new data frame of the 
correct size and insert the data frames by indexing.

On Tue, 12 Aug 2003 lutz.thieme at amd.com wrote:

> could anybody give me a hint, who I can use rbind on a list of
> data.frames, please?
> 
> I have a list with a large number of data.frames of the same structure, like:
> LIST	<- list(X1=data.frame(a=1,b=2), X2=data.frame(a=3,b=4), X3=data.frame(a=5,b=6), ...., XN=data.frame(a=i,b=k))
> 
> I would like to bind all data.frames very fast to a single data.frame,
> something like that:
> DF	<- rbind(LIST$X1, LIST$X2, LIST$X3, ..., LIST$XN)
> 
> But for performance (speed) reasons I won't use a loop.

But rbind.data.frame does use a loop ....

> I also couldn't
> find a solution how I could use lapply. One constraint is, that the
> number of data.frames in the list is not determined and can vary from
> one to more than thousand.

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From p.dalgaard at biostat.ku.dk  Tue Aug 12 10:53:24 2003
From: p.dalgaard at biostat.ku.dk (Peter Dalgaard BSA)
Date: Tue, 12 Aug 2003 08:53:24 -0000
Subject: [R] who to rbind of a list of data.frames
In-Reply-To: <E540DF203FFED21182EB0008C728756010628E37@deexmta4.amd.com>
References: <E540DF203FFED21182EB0008C728756010628E37@deexmta4.amd.com>
Message-ID: <x2adafjln8.fsf@biostat.ku.dk>

lutz.thieme at amd.com writes:

> Hello everybody,
> 
> could anybody give me a hint, who I can use rbind on a list of data.frames, please?
> 
> I have a list with a large number of data.frames of the same structure, like:
> LIST	<- list(X1=data.frame(a=1,b=2), X2=data.frame(a=3,b=4), X3=data.frame(a=5,b=6), ...., XN=data.frame(a=i,b=k))
> 
> I would like to bind all data.frames very fast to a single data.frame, something like that:
> DF	<- rbind(LIST$X1, LIST$X2, LIST$X3, ..., LIST$XN)
> 
> But for performance (speed) reasons I won't use a loop. I also couldn't find a solution how 
> I could use lapply. One constraint is, that the number of data.frames in the list is not determined 
> and can vary from one to more than thousand. 
> Any help is apreciated, thank you in advance.

Bill V.'s favourite trick:

do.call("rbind", LIST)

> 	Lutz Thieme
> 	Product Engineering
> 	AMD Saxony Limited Liability Company & Co. KG

Anyone tried getting R to run on Opterons in 64 bit mode BTW? Those
machines are quickly approaching Xeon prices around here..

-- 
   O__  ---- Peter Dalgaard             Blegdamsvej 3  
  c/ /'_ --- Dept. of Biostatistics     2200 Cph. N   
 (*) \(*) -- University of Copenhagen   Denmark      Ph: (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)             FAX: (+45) 35327907



From C.Gillespie at newcastle.ac.uk  Tue Aug 12 11:05:46 2003
From: C.Gillespie at newcastle.ac.uk (Colin Gillespie)
Date: Tue, 12 Aug 2003 10:05:46 +0100
Subject: [R] Installing R with debian
Message-ID: <52D580A5DC1FC04CB260E80074CFBA5C383D75@bond.ncl.ac.uk>

Dear All,

I am trying to install R using debian's apt-get system. When I do 'apt-get install r-base', it appears to install everything correctly. However, when I start up R I get the error message:

"
knoppix at Geron007:/knoppix$ R
cannot find system RenvironError in options(...) : invalid editor parameter

R : Copyright 2003, The R Development Core Team
Version 1.7.1  (2003-06-16)
"

The version I get is 
>version
         _
platform i386-pc-linux-gnu
arch     i386
os       linux-gnu
system   i386, linux-gnu
status
major    1
minor    7.1
year     2003
month    06
day      16
language R

I've tried (many times) uninstalling R, including other R modules, e.g r-base-core and changing the version that I get via "/etc/apt/sources.list".


Many Thanks

Colin



From plxmh at nottingham.ac.uk  Tue Aug 12 11:47:30 2003
From: plxmh at nottingham.ac.uk (Martin Hoyle)
Date: Tue, 12 Aug 2003 10:47:30 +0100
Subject: [R] Negative binomial theta
Message-ID: <sf38c5cc.015@ccw0m1.nottingham.ac.uk>

Hi,

I'm trying to use the command "glm.nb" in library(MASS) to test for a significant difference in the aggregation parameter "theta" between the three levels of a factor.

Any help gratefully received!

Martin.


Martin Hoyle,
School of Life and Environmental Sciences,
University of Nottingham,
University Park,
Nottingham,
NG7 2RD,
UK
Webpage: http://myprofile.cos.com/martinhoyle



From ripley at stats.ox.ac.uk  Tue Aug 12 11:56:00 2003
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Tue, 12 Aug 2003 10:56:00 +0100 (BST)
Subject: [R] Negative binomial theta
In-Reply-To: <sf38c5cc.015@ccw0m1.nottingham.ac.uk>
Message-ID: <Pine.LNX.4.44.0308121054030.333-100000@gannet.stats>

On Tue, 12 Aug 2003, Martin Hoyle wrote:

> I'm trying to use the command "glm.nb" in library(MASS) to test for a
> significant difference in the aggregation parameter "theta" between the
> three levels of a factor.

You cannot in general do that: each fit has one theta parameter.

You may be able to do three separate fits, one for each level, and sum the 
loglikelihoods and do a likelihood ratio test by hand.  (It depends 
exactly what the model you are using is.)


-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From madrid at linuxmeeting.net  Tue Aug 12 11:57:33 2003
From: madrid at linuxmeeting.net (Daniele Medri)
Date: Tue, 12 Aug 2003 11:57:33 +0200
Subject: [R] R and RMySQL - segmentation fault
In-Reply-To: <39B7E7009F2DD840AA747934AB551E69184762@exnsw1-arm.nsw.csiro.au>
References: <39B7E7009F2DD840AA747934AB551E69184762@exnsw1-arm.nsw.csiro.au>
Message-ID: <200308121157.33706.madrid@linuxmeeting.net>

-----BEGIN PGP SIGNED MESSAGE-----
Hash: SHA1

Alle 03:25, venerd? 8 agosto 2003, Matthew.Kelly at csiro.au ha scritto:
> If I run the same code as a normal user the following line fails with a
> segmentation fault con <-
> dbConnect(m,host="acomputer",dbname="atable",user="au_ser",password="a_pass
>word")

I receive same errors with Mysql 3.x on Debian "woody".
Any solution / news / hack?

- -- 
Daniele Medri <daniele.medri @ libero.it>
homepage: http://www.linux.it/~madrid/

"Statistics are like bikinis. What they reveal is suggestive,
	but what they conceal is vital" - Aaron Levenstein
-----BEGIN PGP SIGNATURE-----
Version: GnuPG v1.2.2 (GNU/Linux)

iD8DBQE/OLoNIQMOQEPV3KYRAuRHAJ9dzz4K5vk5ux8wiUQY851ohuq0ZACghrq1
DDusev3EZ/ulPPr+m3GjLEo=
=Kw75
-----END PGP SIGNATURE-----



From puffin at cebc.cnrs.fr  Tue Aug 12 12:22:57 2003
From: puffin at cebc.cnrs.fr (Pinaud David)
Date: Tue, 12 Aug 2003 12:22:57 +0200
Subject: [R] Truncation solves
Message-ID: <3F38C001.79AFB7C5@cebc.cnrs.fr>

Dear all,
Thank for your answers and your help. My problem was that, during
exportation with "write.table",  numeric variable precision corresponds
to the printed presentation and was then too low for me. But you can set
this option "options(digits=XX)" before exporting.

Thank again

David

From bhx2 at mevik.net  Tue Aug 12 13:14:07 2003
From: bhx2 at mevik.net (=?iso-8859-1?q?Bj=F8rn-Helge_Mevik?=)
Date: Tue, 12 Aug 2003 13:14:07 +0200
Subject: [R] Marginal (type II) SS for powers of continuous variables in
	a linear model?
References: <Pine.LNX.4.44.0308111642230.14734-100000@gannet.stats>
Message-ID: <7o7k5ji0w0.fsf@foo.nemo-project.org>

Prof Brian Ripley <ripley at stats.ox.ac.uk> writes:

> drop1 is the part of R that does type II sum of squares, and it works in 
> your example.  So does Anova in the current car:

I'm sorry, I should have included an example to clarify what I meant
(or point out my misunderstandings :-).  I'll do that below, but first
a comment:

> And in summary.aov() those *are* marginal SS, as balance is assumed
> for aov models. (That is not to say the software does not work otherwise, 
> but the interpretability depends on balance.)

Maybe I've misunderstood, but in the documentation for aov, it says
(under Details):
     This provides a wrapper to `lm' for fitting linear models to
     balanced or unbalanced experimental designs.

Also, is this example (lm(y~x+I(x^2), Df)) really balanced?  I think
of balance as the property that there is an equal number of
observations for every combination of the factors.  With x and x^2,
this doesn't happen.  For instance, x=1 and x^2=1 occurs once, but x=1
and x^2=4 never occurs (naturally).  Or have I misunderstood something?

Now, the example:

> Df2 <- expand.grid (A=factor(1:2), B=factor(1:2), x=1:5)
> Df2$y <- codes(Df2$A) + 2*codes(Df2$B) + 0.05*codes(Df2$A)*codes(Df2$B) +
+   Df2$x + 0.1*Df2$x^2 + 0.1*(0:4)
> Df2 <- Df2[-1,]    # Remove one observation to make it unbalanced

> ABx2.lm <- lm(y~A*B + x + I(x^2), data=Df2)

The SSs I call marginal are R(A | B, x, x^2), R(B | A, x, x^2),
R(A:B | A, B, x, x^2), R(x | A, B, A:B) and R(x^2 | A, B, A:B, x).

(Here, for instance, R(x | A, B, A:B) means the reduction of SSE due
to including x in a model when A, B and A:B (and the mean) are already
in the model. I've omitted the mean from the notation.)

> anova(ABx2.lm)
Analysis of Variance Table

Response: y
          Df Sum Sq Mean Sq   F value    Pr(>F)    
A          1  1.737   1.737   66.5700 1.801e-06 ***
B          1 13.647  13.647  523.0292 6.953e-12 ***
x          1 93.677  93.677 3590.1703 < 2.2e-16 ***
I(x^2)     1  0.583   0.583   22.3302 0.0003966 ***
A:B        1  0.011   0.011    0.4238 0.5263772    
Residuals 13  0.339   0.026                        
---
Signif. codes:  0 `***' 0.001 `**' 0.01 `*' 0.05 `.' 0.1 ` ' 1 

This gives SSs on the form R(A), R(B | A), R(x | A, B) etc.  (If the
design had been balanced (in A, B and x), this would have been the
same as the marginal SSs above.)

> drop1(ABx2.lm)
Single term deletions

Model:
y ~ A * B + x + I(x^2)
       Df Sum of Sq     RSS     AIC
<none>                0.339 -64.486
x       1     1.188   1.527 -37.901
I(x^2)  1     0.592   0.931 -47.294
A:B     1     0.011   0.350 -65.877

This gives the SSs R(x | A, B, A:B, x^2), R(x^2 | A, B, A:B, x) and
R(A:B | A, B, x, x^2).  The SS for x is not marginal as defined
above.

> library (car)
> Anova(ABx2.lm)
Anova Table (Type II tests)

Response: y
           Sum Sq Df  F value    Pr(>F)    
A          5.1806  1 198.5470 2.979e-09 ***
B         19.6610  1 753.5074 6.778e-13 ***
x          1.1879  1  45.5245 1.368e-05 ***
I(x^2)     0.5922  1  22.6970 0.0003699 ***
A:B        0.0111  1   0.4238 0.5263772    
Residuals  0.3392 13                       
---
Signif. codes:  0 `***' 0.001 `**' 0.01 `*' 0.05 `.' 0.1 ` ' 1 

This gives marginal SSs for A, B, x^2 and A:B, but as with drop1, the
SS for x is R(x | A, B, A:B, x^2).

The only way I've figured out to give the `correct' SS for x, i.e.,
R(x | A, B, A:B), is: 

> AB.lm <- lm(y~A*B, data=Df2)
> ABx.lm <- lm(y~A*B + x, data=Df2)
> anova (AB.lm, ABx.lm, ABx2.lm)
Analysis of Variance Table

Model 1: y ~ A * B
Model 2: y ~ A * B + x
Model 3: y ~ A * B + x + I(x^2)
  Res.Df    RSS Df Sum of Sq        F    Pr(>F)    
1     15 93.760                                    
2     14  0.931  1    92.829 3557.651 < 2.2e-16 ***
3     13  0.339  1     0.592   22.697 0.0003699 ***
---
Signif. codes:  0 `***' 0.001 `**' 0.01 `*' 0.05 `.' 0.1 ` ' 1 

(The ABx2.lm is included to give the same error term to test against
as in the ANOVAs above.)

The baseline of all this is that I think it would be nice if a
function like Anova in the car package returned R(x | A, B, A:B)
instead of R(x | A, B, A:B, x^2) as SS for x in a model such as the
above.

(I hope I've made myself clearer, and not insulted anyone by
oversimplifying. :-)

-- 
Bj?rn-Helge Mevik



From edd at debian.org  Tue Aug 12 13:43:25 2003
From: edd at debian.org (Dirk Eddelbuettel)
Date: Tue, 12 Aug 2003 06:43:25 -0500
Subject: [R] Installing R with debian
In-Reply-To: <52D580A5DC1FC04CB260E80074CFBA5C383D75@bond.ncl.ac.uk>
References: <52D580A5DC1FC04CB260E80074CFBA5C383D75@bond.ncl.ac.uk>
Message-ID: <20030812114325.GC21775@sonny.eddelbuettel.com>


Colin,

On Tue, Aug 12, 2003 at 10:05:46AM +0100, Colin Gillespie wrote:
> I am trying to install R using debian's apt-get system. When I do 'apt-get install r-base', it appears to install everything correctly. However, when I start up R I get the error message:
>

That's how it should work.

> "
> knoppix at Geron007:/knoppix$ R
> cannot find system RenvironError in options(...) : invalid editor parameter

That is unrelated. You seem to have a bad value in one of the startup files.
Try for example

knoppix at Geron007:/knoppix$ R --vanilla

which should work.

> I've tried (many times) uninstalling R, including other R modules, e.g r-base-core and changing the version that I get via "/etc/apt/sources.list".

<self promotion mode>
Have a look at Quantian (http://dirk.eddelbuettel.com/quantian.html) which
gives you Knoppix plus R plus some other 400mb of numerical/quantitative
software.  
</self promotion mode>

Hth, Dirk

-- 
Those are my principles, and if you don't like them... well, I have others.
                                                -- Groucho Marx



From ripley at stats.ox.ac.uk  Tue Aug 12 14:01:01 2003
From: ripley at stats.ox.ac.uk (Prof Brian D Ripley)
Date: Tue, 12 Aug 2003 13:01:01 +0100 (GMT Daylight Time)
Subject: [R] Marginal (type II) SS for powers of continuous variables in
	a linear model?
In-Reply-To: <7o7k5ji0w0.fsf@foo.nemo-project.org>
Message-ID: <Pine.WNT.4.44.0308121254330.3812-100000@gannet.stats.ox.ac.uk>

On Tue, 12 Aug 2003, [iso-8859-1] Bjrn-Helge Mevik wrote:

> Prof Brian Ripley <ripley at stats.ox.ac.uk> writes:
>
> > drop1 is the part of R that does type II sum of squares, and it works in
> > your example.  So does Anova in the current car:
>
> I'm sorry, I should have included an example to clarify what I meant
> (or point out my misunderstandings :-).  I'll do that below, but first
> a comment:
>
> > And in summary.aov() those *are* marginal SS, as balance is assumed
> > for aov models. (That is not to say the software does not work otherwise,
> > but the interpretability depends on balance.)
>
> Maybe I've misunderstood, but in the documentation for aov, it says
> (under Details):
>      This provides a wrapper to `lm' for fitting linear models to
>      balanced or unbalanced experimental designs.
>
> Also, is this example (lm(y~x+I(x^2), Df)) really balanced?  I think

No, and I did not use summary,aov on it!

> of balance as the property that there is an equal number of
> observations for every combination of the factors.  With x and x^2,
> this doesn't happen.  For instance, x=1 and x^2=1 occurs once, but x=1
> and x^2=4 never occurs (naturally).  Or have I misunderstood something?

Yes. summary.aov(split=) and model.tables and the like are designed for
balanced data.  They may or may not work in the unbalanced case.  The
comment you quote is for aov(), not those other functions.

> Now, the example:
>
> > Df2 <- expand.grid (A=factor(1:2), B=factor(1:2), x=1:5)
> > Df2$y <- codes(Df2$A) + 2*codes(Df2$B) + 0.05*codes(Df2$A)*codes(Df2$B) +
> +   Df2$x + 0.1*Df2$x^2 + 0.1*(0:4)
> > Df2 <- Df2[-1,]    # Remove one observation to make it unbalanced

codes is deprecated!

> > ABx2.lm <- lm(y~A*B + x + I(x^2), data=Df2)
>
> The SSs I call marginal are R(A | B, x, x^2), R(B | A, x, x^2),
> R(A:B | A, B, x, x^2), R(x | A, B, A:B) and R(x^2 | A, B, A:B, x).

That's not what most other people call marginal, though.

> (Here, for instance, R(x | A, B, A:B) means the reduction of SSE due
> to including x in a model when A, B and A:B (and the mean) are already
> in the model. I've omitted the mean from the notation.)

> > anova(ABx2.lm)
> Analysis of Variance Table
>
> Response: y
>           Df Sum Sq Mean Sq   F value    Pr(>F)
> A          1  1.737   1.737   66.5700 1.801e-06 ***
> B          1 13.647  13.647  523.0292 6.953e-12 ***
> x          1 93.677  93.677 3590.1703 < 2.2e-16 ***
> I(x^2)     1  0.583   0.583   22.3302 0.0003966 ***
> A:B        1  0.011   0.011    0.4238 0.5263772
> Residuals 13  0.339   0.026
> ---
> Signif. codes:  0 `***' 0.001 `**' 0.01 `*' 0.05 `.' 0.1 ` ' 1
>
> This gives SSs on the form R(A), R(B | A), R(x | A, B) etc.  (If the
> design had been balanced (in A, B and x), this would have been the
> same as the marginal SSs above.)
>
> > drop1(ABx2.lm)
> Single term deletions
>
> Model:
> y ~ A * B + x + I(x^2)
>        Df Sum of Sq     RSS     AIC
> <none>                0.339 -64.486
> x       1     1.188   1.527 -37.901
> I(x^2)  1     0.592   0.931 -47.294
> A:B     1     0.011   0.350 -65.877
>
> This gives the SSs R(x | A, B, A:B, x^2), R(x^2 | A, B, A:B, x) and
> R(A:B | A, B, x, x^2).  The SS for x is not marginal as defined
> above.

But that *is* how `marginal' is usually defined.  Why should I(x^2) be
regarded as subservient to x?  It is just another function of x.  Suppose
we have x and log(x)?

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272860 (secr)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From martinol at ensam.inra.fr  Tue Aug 12 16:45:58 2003
From: martinol at ensam.inra.fr (Martin Olivier)
Date: Tue, 12 Aug 2003 14:45:58 +0000
Subject: [R] classification with quantitative variables
Message-ID: <3F38FDA6.7040702@ensam.inra.fr>

Hi all,

I want to conduct a cluster analysis with quantitative variables.
More precisely, it concerns binary and  non-ordered categorical 
variables. For such data, various
similarity measures have been proposed, such as the Jaccard index or the 
simple matching index.

So, is there a package such as mva or multiv in the case of quantitative 
variables?
Could you indicate me  reviews,  papers or technical reports dealing 
with this problem?

Regards,
Olivier

-- 

-------------------------------------------------------------
Martin Olivier
INRA - Unit? prot?omique           LIRMM - IFA/MAB
2, Place Viala                     161, rue Ada
F-34060 Montpellier C?dex 1        34392 Montpellier C?dex 5	

Tel : 04 99 61 26 02               Tel : O4 67 41 86 71
martinol at ensam.inra.fr



From maechler at stat.math.ethz.ch  Tue Aug 12 15:39:41 2003
From: maechler at stat.math.ethz.ch (Martin Maechler)
Date: Tue, 12 Aug 2003 15:39:41 +0200
Subject: [R] classification with quantitative variables
In-Reply-To: <3F38FDA6.7040702@ensam.inra.fr>
References: <3F38FDA6.7040702@ensam.inra.fr>
Message-ID: <16184.60957.715406.477166@gargle.gargle.HOWL>

>>>>> "OlivierM" == Martin Olivier <martinol at ensam.inra.fr>
>>>>>     on Tue, 12 Aug 2003 14:45:58 +0000 writes:

    OlivierM> I want to conduct a cluster analysis with
    OlivierM> quantitative variables.  More precisely, it
    OlivierM> concerns binary and non-ordered categorical
    OlivierM> variables. For such data, various similarity
    OlivierM> measures have been proposed, such as the Jaccard
    OlivierM> index or the simple matching index.

    OlivierM> So, is there a package such as mva or multiv in
    OlivierM> the case of quantitative variables?  Could you
    OlivierM> indicate me reviews, papers or technical reports
    OlivierM> dealing with this problem?

The package 'cluster' has a function daisy() that allows to work
with combinations of "all" kinds of variables.

Note that I think you mistyped 
"quantitative" where you meant
"qualitative".

Regards,
Martin Maechler <maechler at stat.math.ethz.ch>	http://stat.ethz.ch/~maechler/
Seminar fuer Statistik, ETH-Zentrum  LEO C16	Leonhardstr. 27
ETH (Federal Inst. Technology)	8092 Zurich	SWITZERLAND
phone: x-41-1-632-3408		fax: ...-1228			<><



From bih at ornl.gov  Tue Aug 12 16:02:11 2003
From: bih at ornl.gov (Bing Zhang)
Date: Tue, 12 Aug 2003 10:02:11 -0400
Subject: [R] print points from a huge matrix
Message-ID: <3F340FAF@webmail1>

Hi All,

I have a 8000*8000 matrix and I want to print out a file with the row name, 
column name and the value for those point with values satisfying a condition. 
I tried using a for loop, however, it took me forever to get the result. Is 
there a fast way to do this? Thanks!

Bing

---------------------------------
1060 Commerce Park
Oak Ridge National Laboratory
P.O. Box 2008, MS 6480
Oak Ridge, TN 37831-6480
Phone: 865-241-0761
Email: zhangb at ornl.gov



From Simon.Fear at synequanon.com  Tue Aug 12 16:16:39 2003
From: Simon.Fear at synequanon.com (Simon Fear)
Date: Tue, 12 Aug 2003 15:16:39 +0100
Subject: [R] capturing output from Win 98 shell
Message-ID: <6C8A8033ABC1E3468048ABC4F13CE572AC0D51@synequanon01>

How can I best achieve the following (works in Splus):

filenames <- dos("dir *.sasb7dat /b")

What I am  asking, more generically, is: how can I capture the output of
a
DOS command in R?

I have tried using

system("COMMAND.COM /c dir /b", intern=T, show.output.on.console=T)

where

  intern: a logical, indicates whether to make the output of the
          command an R object.

but it makes no useful difference:

> system("COMMAND.COM /c dir /b", intern=T, show.output.on.console=T)
character(0)
> print(system("COMMAND.COM /c dir /b", intern=F,
show.output.on.console=T))
[1] 0

In both cases here a DOS window opens and lists a couple of hundred
files,
before giving the above output.

I am also a bit baffled by "show.output.on.console" whose only effect
seems
to be on whether the DOS screen opens for one millisecond or for one
second.
It does not, per help, "show the output of the command on the R
console".

Incidentally I can answer the problem at hand using the following
workaround
(but what I'm after is a more generic solution):

shell("dir D:\\tmp\\*.sasb7dat /b > D:\\tmp\\Paula\\dirlist.lst")
dataset.names <- scan("D:\\tmp\\dirlist.lst", what="", sep="\n")

TIA
SF
 

Simon Fear
Senior Statistician
Syne qua non Ltd
Tel: +44 (0) 1379 644449
Fax: +44 (0) 1379 644445
email: Simon.Fear at synequanon.com
web: http://www.synequanon.com
 
Number of attachments included with this message: 0
 
This message (and any associated files) is confidential and\...{{dropped}}



From Roger.Bivand at nhh.no  Tue Aug 12 16:16:53 2003
From: Roger.Bivand at nhh.no (Roger Bivand)
Date: Tue, 12 Aug 2003 16:16:53 +0200 (CEST)
Subject: [R] print points from a huge matrix
In-Reply-To: <3F340FAF@webmail1>
Message-ID: <Pine.LNX.4.44.0308121613020.7877-100000@reclus.nhh.no>

On Tue, 12 Aug 2003, Bing Zhang wrote:

> Hi All,
> 
> I have a 8000*8000 matrix and I want to print out a file with the row name, 
> column name and the value for those point with values satisfying a condition. 
> I tried using a for loop, however, it took me forever to get the result. Is 
> there a fast way to do this? Thanks!
> 

> aa <- matrix(rnorm(1000000), 1000, 1000)
> found <- which(aa > 4, arr.ind=TRUE)
> cbind(found, aa[found])

(for your condition) should do it. My matrix is a bit smaller though, but 
it goes quite fast. If you need the row/col names, you'll want to make 
some other structure than the cbind (as a data frame with character 
columns).

> Bing
> 
> ---------------------------------
> 1060 Commerce Park
> Oak Ridge National Laboratory
> P.O. Box 2008, MS 6480
> Oak Ridge, TN 37831-6480
> Phone: 865-241-0761
> Email: zhangb at ornl.gov
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> 

-- 
Roger Bivand
Economic Geography Section, Department of Economics, Norwegian School of
Economics and Business Administration, Breiviksveien 40, N-5045 Bergen,
Norway. voice: +47 55 95 93 55; fax +47 55 95 93 93
e-mail: Roger.Bivand at nhh.no



From tblackw at umich.edu  Tue Aug 12 16:24:13 2003
From: tblackw at umich.edu (Thomas W Blackwell)
Date: Tue, 12 Aug 2003 10:24:13 -0400 (EDT)
Subject: [R] print points from a huge matrix
In-Reply-To: <3F340FAF@webmail1>
Message-ID: <Pine.SOL.4.44.0308121020460.1266-100000@zektor.gpcc.itd.umich.edu>

I would use  which(),  then subscript the row names and
column names with appropriate columns from  which().
See  help("which"),  help("Extract"),  help("rownames").

-  tom blackwell  -  u michigan medical school  -  ann arbor  -

On Tue, 12 Aug 2003, Bing Zhang wrote:

> I have a 8000*8000 matrix and I want to print out a file with the row name,
> column name and the value for those point with values satisfying a condition.
> I tried using a for loop, however, it took me forever to get the result. Is
> there a fast way to do this? Thanks!
>
> Bing
> ---------------------------------
> 1060 Commerce Park
> Oak Ridge National Laboratory
> P.O. Box 2008, MS 6480
> Oak Ridge, TN 37831-6480
> Phone: 865-241-0761
> Email: zhangb at ornl.gov



From ugulumbe at yahoo.co.uk  Tue Aug 12 16:24:52 2003
From: ugulumbe at yahoo.co.uk (=?iso-8859-1?q?Usman=20Shehu?=)
Date: Tue, 12 Aug 2003 15:24:52 +0100 (BST)
Subject: [R] (no subject)
Message-ID: <20030812142452.60851.qmail@web10901.mail.yahoo.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20030812/c8e86e7d/attachment.pl

From bhx2 at mevik.net  Tue Aug 12 16:29:12 2003
From: bhx2 at mevik.net (=?iso-8859-1?q?Bj=F8rn-Helge_Mevik?=)
Date: Tue, 12 Aug 2003 16:29:12 +0200
Subject: [R] Marginal (type II) SS for powers of continuous variables in
	a linear model?
In-Reply-To: <Pine.WNT.4.44.0308121254330.3812-100000@gannet.stats.ox.ac.uk>
	(Brian
	D. Ripley's message of "Tue, 12 Aug 2003 13:01:01 +0100 (GMT Daylight
	Time)")
References: <Pine.WNT.4.44.0308121254330.3812-100000@gannet.stats.ox.ac.uk>
Message-ID: <7ohe4neypz.fsf@foo.nemo-project.org>

Prof Brian D Ripley <ripley at stats.ox.ac.uk> writes:

> On Tue, 12 Aug 2003, [iso-8859-1] Bj?rn-Helge Mevik wrote:
>
>> Also, is this example (lm(y~x+I(x^2), Df)) really balanced?  I think
>
> No, and I did not use summary,aov on it!

And I didn't say you did!

>> This gives the SSs R(x | A, B, A:B, x^2), R(x^2 | A, B, A:B, x) and
>> R(A:B | A, B, x, x^2).  The SS for x is not marginal as defined
>> above.
>
> But that *is* how `marginal' is usually defined.

Ok.

> Why should I(x^2) be regarded as subservient to x?

In polynomial regression, it is usual to first consider a linear
model, then a quadratic, and so forth.  The interesting tests are usually
then the effect of a power of x whith all lower degree terms of x in the
model.  I thought it would be natural to treat polynomials of
continuous variables similarly in models with categorical variables as
well.

-- 
Bj?rn-Helge Mevik



From p.dalgaard at biostat.ku.dk  Tue Aug 12 16:41:46 2003
From: p.dalgaard at biostat.ku.dk (Peter Dalgaard BSA)
Date: Tue, 12 Aug 2003 14:41:46 -0000
Subject: [R] capturing output from Win 98 shell
In-Reply-To: <6C8A8033ABC1E3468048ABC4F13CE572AC0D51@synequanon01>
References: <6C8A8033ABC1E3468048ABC4F13CE572AC0D51@synequanon01>
Message-ID: <x2fzk7hqxv.fsf@biostat.ku.dk>

"Simon Fear" <Simon.Fear at synequanon.com> writes:

> > system("COMMAND.COM /c dir /b", intern=T, show.output.on.console=T)
> character(0)
> > print(system("COMMAND.COM /c dir /b", intern=F,
> show.output.on.console=T))
> [1] 0
> 
> In both cases here a DOS window opens and lists a couple of hundred
> files,
> before giving the above output.

Does it help if you drop the "COMMAND.COM /c" bit??

-- 
   O__  ---- Peter Dalgaard             Blegdamsvej 3  
  c/ /'_ --- Dept. of Biostatistics     2200 Cph. N   
 (*) \(*) -- University of Copenhagen   Denmark      Ph: (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)             FAX: (+45) 35327907



From ligges at statistik.uni-dortmund.de  Tue Aug 12 16:43:46 2003
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Tue, 12 Aug 2003 16:43:46 +0200
Subject: [R] (no subject)
In-Reply-To: <20030812142452.60851.qmail@web10901.mail.yahoo.com>
References: <20030812142452.60851.qmail@web10901.mail.yahoo.com>
Message-ID: <3F38FD22.4010201@statistik.uni-dortmund.de>

Usman Shehu wrote:
> Dear R- Users,
>  
> Please, what is wrong ? I am trying to run one sample Kolmogorov-Smirov test but I always get an error as shown below.
>  
> 
>>ks.test(A, "normal", 0,1)

For sure, you mean

  ks.test(A, "pnorm", 0,1)

because pnorm() is the distribution function of the normal (Gauss) 
distribution.

Uwe Ligges


> Error in get(x, envir, mode, inherits) : variable "normal" was not found
> 
> 
> Thanks
>  
> usman
> 
> 
> ---------------------------------

> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
>



From ligges at statistik.uni-dortmund.de  Tue Aug 12 16:48:54 2003
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Tue, 12 Aug 2003 16:48:54 +0200
Subject: [R] capturing output from Win 98 shell
In-Reply-To: <6C8A8033ABC1E3468048ABC4F13CE572AC0D51@synequanon01>
References: <6C8A8033ABC1E3468048ABC4F13CE572AC0D51@synequanon01>
Message-ID: <3F38FE56.80200@statistik.uni-dortmund.de>

Simon Fear wrote:

> How can I best achieve the following (works in Splus):
> 
> filenames <- dos("dir *.sasb7dat /b")
> 
> What I am  asking, more generically, is: how can I capture the output of
> a
> DOS command in R?
> 
> I have tried using
> 
> system("COMMAND.COM /c dir /b", intern=T, show.output.on.console=T)
> 
> where
> 
>   intern: a logical, indicates whether to make the output of the
>           command an R object.
> 
> but it makes no useful difference:
> 
> 
>>system("COMMAND.COM /c dir /b", intern=T, show.output.on.console=T)
> 
> character(0)
> 
>>print(system("COMMAND.COM /c dir /b", intern=F,
> 
> show.output.on.console=T))
> [1] 0
> 
> In both cases here a DOS window opens and lists a couple of hundred
> files,
> before giving the above output.
> 
> I am also a bit baffled by "show.output.on.console" whose only effect
> seems
> to be on whether the DOS screen opens for one millisecond or for one
> second.
> It does not, per help, "show the output of the command on the R
> console".
> 
> Incidentally I can answer the problem at hand using the following
> workaround
> (but what I'm after is a more generic solution):
> 
> shell("dir D:\\tmp\\*.sasb7dat /b > D:\\tmp\\Paula\\dirlist.lst")
> dataset.names <- scan("D:\\tmp\\dirlist.lst", what="", sep="\n")


So you almost got it, just read ?shell carefully enough:

  filenames <- shell("dir D:\\tmp\\*.sasb7dat /b", intern = TRUE)

or even better in this case:

  filenames <- list.files("d:/tmp", pattern = "sasb7dat")

Uwe Ligges


> TIA
> SF
>  
> 
> Simon Fear
> Senior Statistician
> Syne qua non Ltd
> Tel: +44 (0) 1379 644449
> Fax: +44 (0) 1379 644445
> email: Simon.Fear at synequanon.com
> web: http://www.synequanon.com
>  
> Number of attachments included with this message: 0
>  
> This message (and any associated files) is confidential and\...{{dropped}}
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help



From Simon.Fear at synequanon.com  Tue Aug 12 16:50:54 2003
From: Simon.Fear at synequanon.com (Simon Fear)
Date: Tue, 12 Aug 2003 15:50:54 +0100
Subject: [R] capturing output from Win 98 shell
Message-ID: <6C8A8033ABC1E3468048ABC4F13CE572AC0D53@synequanon01>

Peter, thanks for quick reply, but I can't drop it:

> system("dir /b", intern=T, show.output.on.console=T)
Error in system("dir /b", intern = T, show.output.on.console = T) : 
        dir not found

> From: Peter Dalgaard BSA [mailto:p.dalgaard at biostat.ku.dk]
> snip <

> Does it help if you drop the "COMMAND.COM /c" bit??
 

Simon Fear
Senior Statistician
Syne qua non Ltd
Tel: +44 (0) 1379 644449
Fax: +44 (0) 1379 644445
email: Simon.Fear at synequanon.com
web: http://www.synequanon.com
 
Number of attachments included with this message: 0
 
This message (and any associated files) is confidential and\...{{dropped}}



From ligges at statistik.uni-dortmund.de  Tue Aug 12 16:52:06 2003
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Tue, 12 Aug 2003 16:52:06 +0200
Subject: [R] tutorials of  seao & seao-gui package for R
In-Reply-To: <002401c36052$ec91aaa0$818001db@webgis>
References: <002401c36052$ec91aaa0$818001db@webgis>
Message-ID: <3F38FF16.3080809@statistik.uni-dortmund.de>

Hisaji Ono wrote:

> Hi.
> 
> I've been very interested in your seao & seao-gui package for R.
> 
> However manuals of seao & seao-gui package for R have doesn't include
> examples, so I can't understand how
> to use them.
> 
> Could you tell me availability of tutorials of them  in English.
> 

Since there was no answer from the package maintainer, AFAICS:
Please ask the package maintainer directly, when specific questions to
contributed packages arise (Kurt Sys <kurt.sys at UGent.be> maintains both
packages, see  library(help = seao)).

Uwe Ligges



From p.dalgaard at biostat.ku.dk  Tue Aug 12 16:55:10 2003
From: p.dalgaard at biostat.ku.dk (Peter Dalgaard BSA)
Date: Tue, 12 Aug 2003 14:55:10 -0000
Subject: [R] capturing output from Win 98 shell
In-Reply-To: <6C8A8033ABC1E3468048ABC4F13CE572AC0D53@synequanon01>
References: <6C8A8033ABC1E3468048ABC4F13CE572AC0D53@synequanon01>
Message-ID: <x2bruuj4wa.fsf@biostat.ku.dk>

"Simon Fear" <Simon.Fear at synequanon.com> writes:

> Peter, thanks for quick reply, but I can't drop it:
> 
> > system("dir /b", intern=T, show.output.on.console=T)
> Error in system("dir /b", intern = T, show.output.on.console = T) : 
>         dir not found
> 

Oh, dir is a built-in, right?... Better wait for the real experts
then. I don't usually do windows.

-- 
   O__  ---- Peter Dalgaard             Blegdamsvej 3  
  c/ /'_ --- Dept. of Biostatistics     2200 Cph. N   
 (*) \(*) -- University of Copenhagen   Denmark      Ph: (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)             FAX: (+45) 35327907



From alessandro.semeria at cramont.it  Tue Aug 12 17:11:22 2003
From: alessandro.semeria at cramont.it (alessandro.semeria@cramont.it)
Date: Tue, 12 Aug 2003 17:11:22 +0200
Subject: [R] print points from a huge matrix
Message-ID: <OFF7A1D52F.E69BD439-ONC1256D80.00533D95-C1256D80.00522809@tomware.it>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20030812/4e68485d/attachment.pl

From kurt.sys at UGent.be  Tue Aug 12 17:07:11 2003
From: kurt.sys at UGent.be (Kurt Sys)
Date: Tue, 12 Aug 2003 17:07:11 +0200
Subject: [R] tutorials of  seao & seao-gui package for R
In-Reply-To: <3F38FF16.3080809@statistik.uni-dortmund.de>
References: <002401c36052$ec91aaa0$818001db@webgis>
	<3F38FF16.3080809@statistik.uni-dortmund.de>
Message-ID: <16185.671.885420.341356@ksys.rug.ac.be>

Hi,

I did answer the question, but didn't send it to the R mailing
list.
I'm working on it, any questions may be sent to me, I'm still working
on the functionallity right now... Examples will be included later.

Kurt.




--
Mail from Uwe Ligges
sent on Tuesday August 12 2003 at 16:52 (GMT+0200):

   Hisaji Ono wrote:
   
   > Hi.
   > 
   > I've been very interested in your seao & seao-gui package for R.
   > 
   > However manuals of seao & seao-gui package for R have doesn't
   > include examples, so I can't understand how to use them.
   > 
   > Could you tell me availability of tutorials of them in English.
   > 
   
   Since there was no answer from the package maintainer, AFAICS:
   Please ask the package maintainer directly, when specific questions
   to contributed packages arise (Kurt Sys <kurt.sys at UGent.be>
   maintains both packages, see library(help = seao)).
   
   Uwe Ligges
   
   ______________________________________________
   R-help at stat.math.ethz.ch mailing list
   https://www.stat.math.ethz.ch/mailman/listinfo/r-help

-- 
Computers are not intelligent.  They only think they are.



From Simon.Fear at synequanon.com  Tue Aug 12 17:11:53 2003
From: Simon.Fear at synequanon.com (Simon Fear)
Date: Tue, 12 Aug 2003 16:11:53 +0100
Subject: [R] capturing output from Win 98 shell
Message-ID: <6C8A8033ABC1E3468048ABC4F13CE572AC0D54@synequanon01>

Uwe, you suggest

> So you almost got it, just read ?shell carefully enough:
> 
>   filenames <- shell("dir D:\\tmp\\*.sasb7dat /b", intern = TRUE)
> 

Unfortunately, this does not work and is why I wrote to Rhelp (OK I used
system instead of shell, but I tried shell first); here filenames is
assigned
character(0) even though the dir output is not empty.

Believe me I did read ?shell many times before I wrote: but I am
reporting
that shell does not do what ?shell says it should.

> or even better in this case:
> 
>   filenames <- list.files("d:/tmp", pattern = "sasb7dat")
> 

THANK YOU! I'm always forgetting about file.* and friends. Platform
independent too.

Still leaves me asking the bigger question though: how do I capture DOS
command output in general?
 

Simon Fear
Senior Statistician
Syne qua non Ltd
Tel: +44 (0) 1379 644449
Fax: +44 (0) 1379 644445
email: Simon.Fear at synequanon.com
web: http://www.synequanon.com
 
Number of attachments included with this message: 0
 
This message (and any associated files) is confidential and\...{{dropped}}



From ripley at stats.ox.ac.uk  Tue Aug 12 17:14:25 2003
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Tue, 12 Aug 2003 16:14:25 +0100 (BST)
Subject: [R] Marginal (type II) SS for powers of continuous variables in
	a linear model?
In-Reply-To: <7ohe4neypz.fsf@foo.nemo-project.org>
Message-ID: <Pine.LNX.4.44.0308121611440.3281-100000@gannet.stats>

On Tue, 12 Aug 2003, Bj?rn-Helge Mevik wrote:

> Prof Brian D Ripley <ripley at stats.ox.ac.uk> writes:
> 
> > On Tue, 12 Aug 2003, [iso-8859-1] Bj?rn-Helge Mevik wrote:
> 
> > Why should I(x^2) be regarded as subservient to x?
> 
> In polynomial regression, it is usual to first consider a linear
> model, then a quadratic, and so forth.  The interesting tests are usually
> then the effect of a power of x whith all lower degree terms of x in the
> model.  I thought it would be natural to treat polynomials of
> continuous variables similarly in models with categorical variables as
> well.

It would be, but I(x^2) is not associated with a polynomial.  It's
a deficiency of R/S that there is no simple way to do this, but if it 
were to be done it would be via the use of poly() (and you would only be 
able to frop the highest-degree term, not what you expressed as 
`marginal').

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From ripley at stats.ox.ac.uk  Tue Aug 12 17:18:27 2003
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Tue, 12 Aug 2003 16:18:27 +0100 (BST)
Subject: [R] capturing output from Win 98 shell
In-Reply-To: <x2fzk7hqxv.fsf@biostat.ku.dk>
Message-ID: <Pine.LNX.4.44.0308121616350.3281-100000@gannet.stats>

On 12 Aug 2003, Peter Dalgaard BSA wrote:

> "Simon Fear" <Simon.Fear at synequanon.com> writes:
> 
> > > system("COMMAND.COM /c dir /b", intern=T, show.output.on.console=T)
> > character(0)
> > > print(system("COMMAND.COM /c dir /b", intern=F,
> > show.output.on.console=T))
> > [1] 0
> > 
> > In both cases here a DOS window opens and lists a couple of hundred
> > files,
> > before giving the above output.
> 
> Does it help if you drop the "COMMAND.COM /c" bit??

No, but as `dir' is an internal command, it does help to use shell() and
not system().  It is shell() that is the equivalent of S-PLUS's dos()
command.

Not that R has dir() and list.files(), though.

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From ligges at statistik.uni-dortmund.de  Tue Aug 12 17:21:34 2003
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Tue, 12 Aug 2003 17:21:34 +0200
Subject: [R] capturing output from Win 98 shell
In-Reply-To: <6C8A8033ABC1E3468048ABC4F13CE572AC0D54@synequanon01>
References: <6C8A8033ABC1E3468048ABC4F13CE572AC0D54@synequanon01>
Message-ID: <3F3905FE.60504@statistik.uni-dortmund.de>

Simon Fear wrote:
> Uwe, you suggest
> 
> 
>>So you almost got it, just read ?shell carefully enough:
>>
>>  filenames <- shell("dir D:\\tmp\\*.sasb7dat /b", intern = TRUE)
>>
> 
> 
> Unfortunately, this does not work and is why I wrote to Rhelp (OK I used
> system instead of shell, but I tried shell first); here filenames is
> assigned
> character(0) even though the dir output is not empty.

(sure about "intern = TRUE" ?)

Hmmm. It works on WinNT, and I cannot check a DOS based OS like Win98, 
because we don't have any of those DOS based operating systems in our 
department.


> Believe me I did read ?shell many times before I wrote: but I am
> reporting
> that shell does not do what ?shell says it should.
> 
> 
>>or even better in this case:
>>
>>  filenames <- list.files("d:/tmp", pattern = "sasb7dat")
>>
> 
> 
> THANK YOU! I'm always forgetting about file.* and friends. Platform
> independent too.
> 
> Still leaves me asking the bigger question though: how do I capture DOS
> command output in general?

Or, to come back to Peter's suggestion, the help file ?system tells us:

"... (To use DOS internal commands use 
paste(Sys.getenv("COMSPEC"),"/c",cmd).)..."

[Peter, we don't need to be experts to read this on Windows -- even with 
that heatwave in Dortmund ... ;-)]

So we get:

  filenames <- system(paste(Sys.getenv("COMSPEC"), "/c", "dir /b"), 
intern = TRUE)

but that's the same way shell() works, so I woudn't expect this to work 
when shell() doesn't...

Uwe Ligges


> 
> Simon Fear
> Senior Statistician
> Syne qua non Ltd
> Tel: +44 (0) 1379 644449
> Fax: +44 (0) 1379 644445
> email: Simon.Fear at synequanon.com
> web: http://www.synequanon.com
>  
> Number of attachments included with this message: 0
>  
> This message (and any associated files) is confidential and 
> contains information which may be legally privileged.  It is 
> intended for the stated addressee(s) only.  Access to this 
> email by anyone else is unauthorised.  If you are not the 
> intended addressee, any action taken (or not taken) in 
> reliance on it, or any disclosure or copying of the contents of 
> it is unauthorised and unlawful.  If you are not the addressee, 
> please inform the sender immediately and delete the email 
> from your system.
> 
> This message and any associated attachments have been 
> checked for viruses using an internationally recognised virus 
> detection process.  However, Internet communications cannot 
> be guaranteed to be secure or error-free as information could 
> be intercepted, corrupted, lost, destroyed, arrive late or 
> incomplete. Therefore, we do not accept responsibility for any 
> errors or omissions that are present in this message, or any 
> attachment, that have arisen as a result of e-mail transmission.  
> If verification is required, please request a hard-copy version. 
> Any views or opinions presented are solely those of the author 
> and do not necessarily represent those of Syne qua non.



From hedderik at cmu.edu  Tue Aug 12 17:39:08 2003
From: hedderik at cmu.edu (Hedderik van Rijn)
Date: Tue, 12 Aug 2003 11:39:08 -0400
Subject: [R] read.spss doesn't work anymore [more info]
In-Reply-To: <x2adagjcy3.fsf@biostat.ku.dk>
Message-ID: <1D2B2142-CCDB-11D7-BFE2-000A956B93BA@cmu.edu>

> Thanks. I've debugged a little, and it seems that the primary reason
> is that a bogus value is read for n_lines in read_documents (file
> sfm_read.c).
>
> Once this has failed, the code paints itself into a corner by checking
> whether the file was already open in sfm_read_dictionary; this test
> succeeds, but the content of the dict structure remains bogus... I
> can't reproduce your segfault, but as I said, this phenomenon is
> probably secondary to the real problem anyway.
>
> The value that is getting read for n_lines is 134217728 aka 0x8000000
> which looks like it could be intended as a special value?

I've got no idea whether it is a special value, I'm pretty sure I 
didn't select anything special myself when saving that file, a couple 
of years ago, because I also always kept the back then recommended 
"more portable" .sys files. All I know is that I never had problems 
before R 1.7.1 and that the .sav files reads correctly in SPSS 10 on 
Windows (and 9), the last SPSS for Mac OS 9 (v10?) and the new SPSS v11 
for Mac OS X.

  - Hedderik.



From Simon.Fear at synequanon.com  Tue Aug 12 17:46:52 2003
From: Simon.Fear at synequanon.com (Simon Fear)
Date: Tue, 12 Aug 2003 16:46:52 +0100
Subject: [R] grep and gsub on backslash and quotes
Message-ID: <6C8A8033ABC1E3468048ABC4F13CE572AC0D52@synequanon01>

The following code works,  to gsub single quotes to double quotes:

line <- gsub("'", '"', line)

(that's a single quote within doubles then a double within singles if
your
viewer's font is not good).

But The R Language Manual tells me that

Quotes and other special characters within strings
are specified using escape sequences:
\' single quote
\" double quote

so why is the following wrong: gsub("\\\\'", "\\\\"", line)? That or any
other number of backslashes (have tried all up to n=6 just for good
measure).

BTW is it documented anywhere that you need four backslashes in an RE to
match one in the target, when it is being passed as an argument to gsub
or
grep? How would I know how many levels of doubling up to use for any
other
functions? (I got to 4 consecutive \ by trial and error in this case,
but
have a dim memory of having read about it somewhere.)

TIA
 

Simon Fear
Senior Statistician
Syne qua non Ltd
Tel: +44 (0) 1379 644449
Fax: +44 (0) 1379 644445
email: Simon.Fear at synequanon.com
web: http://www.synequanon.com
 
Number of attachments included with this message: 0
 
This message (and any associated files) is confidential and\...{{dropped}}



From ripley at stats.ox.ac.uk  Tue Aug 12 18:13:14 2003
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Tue, 12 Aug 2003 17:13:14 +0100 (BST)
Subject: [R] grep and gsub on backslash and quotes
In-Reply-To: <6C8A8033ABC1E3468048ABC4F13CE572AC0D52@synequanon01>
Message-ID: <Pine.LNX.4.44.0308121702540.3695-100000@gannet.stats>

On Tue, 12 Aug 2003, Simon Fear wrote:

> The following code works,  to gsub single quotes to double quotes:
> 
> line <- gsub("'", '"', line)
> 
> (that's a single quote within doubles then a double within singles if
> your
> viewer's font is not good).
> 
> But The R Language Manual tells me that
> 
> Quotes and other special characters within strings
> are specified using escape sequences:
> \' single quote
> \" double quote
> 
> so why is the following wrong: gsub("\\\\'", "\\\\"", line)? That or any
> other number of backslashes (have tried all up to n=6 just for good
> measure).
> 
> BTW is it documented anywhere that you need four backslashes in an RE to
> match one in the target, when it is being passed as an argument to gsub
> or
> grep?

It's not true, so I hope it is not documented anywhere.  You may need 6, 
as in the following from methods():

    res <- sort(grep(gsub("([.[])", "\\\\\\1", name), an, value = TRUE))

since that is \\ \1 withou tthe space.  Each backslash in the target only
needs to be doubled.

In your example

gsub("\'", "\"", line) or even gsub("'", "\"", line)

is all you need: only R strings need the escape.


-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From bih at ornl.gov  Tue Aug 12 18:15:41 2003
From: bih at ornl.gov (Bing Zhang)
Date: Tue, 12 Aug 2003 12:15:41 -0400
Subject: [R] print points from a huge matrix
Message-ID: <3F344335@webmail1>

Thanks. I made it. For those who have the same problem, here is my data:
> corr2
            100002_at   100003_at  100004_at   100005_at
100002_at  1.00000000 -0.08452797 -0.2977039 -0.10522948
100003_at -0.08452797  1.00000000 -0.2955658  0.02353673
100004_at -0.29770388 -0.29556577  1.0000000  0.20758195
100005_at -0.10522948  0.02353673  0.2075820  1.00000000
100006_at -0.20549685  0.15661089 -0.2132440 -0.48680542
and here is how I did it:
> found2<-which(abs(corr2)>0.8, arr.ind=TRUE)
> cbind(rownames(corr2)[found2[,1]],colnames(corr2)[found2[,2]],corr2[found2])
     [,1]        [,2]        [,3]
[1,] "100002_at" "100002_at" "1"
[2,] "100003_at" "100003_at" "1"
[3,] "100004_at" "100004_at" "1"
[4,] "100005_at" "100005_at" "1"

>===== Original Message From Roger.Bivand at nhh.no =====
>On Tue, 12 Aug 2003, Bing Zhang wrote:
>
>> Hi All,
>>
>> I have a 8000*8000 matrix and I want to print out a file with the row name,
>> column name and the value for those point with values satisfying a 
condition.
>> I tried using a for loop, however, it took me forever to get the result. Is
>> there a fast way to do this? Thanks!
>>
>
>> aa <- matrix(rnorm(1000000), 1000, 1000)
>> found <- which(aa > 4, arr.ind=TRUE)
>> cbind(found, aa[found])
>
>(for your condition) should do it. My matrix is a bit smaller though, but
>it goes quite fast. If you need the row/col names, you'll want to make
>some other structure than the cbind (as a data frame with character
>columns).
>
>> Bing
>>
>> ---------------------------------
>> 1060 Commerce Park
>> Oak Ridge National Laboratory
>> P.O. Box 2008, MS 6480
>> Oak Ridge, TN 37831-6480
>> Phone: 865-241-0761
>> Email: zhangb at ornl.gov
>>
>> ______________________________________________
>> R-help at stat.math.ethz.ch mailing list
>> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
>>
>
>--
>Roger Bivand
>Economic Geography Section, Department of Economics, Norwegian School of
>Economics and Business Administration, Breiviksveien 40, N-5045 Bergen,
>Norway. voice: +47 55 95 93 55; fax +47 55 95 93 93
>e-mail: Roger.Bivand at nhh.no

---------------------------------
1060 Commerce Park
Oak Ridge National Laboratory
P.O. Box 2008, MS 6480
Oak Ridge, TN 37831-6480
Phone: 865-241-0761
Email: zhangb at ornl.gov



From p.dalgaard at biostat.ku.dk  Tue Aug 12 18:21:40 2003
From: p.dalgaard at biostat.ku.dk (Peter Dalgaard BSA)
Date: Tue, 12 Aug 2003 16:21:40 -0000
Subject: [R] grep and gsub on backslash and quotes
In-Reply-To: <6C8A8033ABC1E3468048ABC4F13CE572AC0D52@synequanon01>
References: <6C8A8033ABC1E3468048ABC4F13CE572AC0D52@synequanon01>
Message-ID: <x23cg6j0w0.fsf@biostat.ku.dk>

"Simon Fear" <Simon.Fear at synequanon.com> writes:

> The following code works,  to gsub single quotes to double quotes:
> 
> line <- gsub("'", '"', line)
> 
> (that's a single quote within doubles then a double within singles if
> your
> viewer's font is not good).
> 
> But The R Language Manual tells me that
> 
> Quotes and other special characters within strings
> are specified using escape sequences:
> \' single quote
> \" double quote
> 
> so why is the following wrong: gsub("\\\\'", "\\\\"", line)? That or any
> other number of backslashes (have tried all up to n=6 just for good
> measure).

There's a backslash missing in the replacement. This works:

line <- "ab\\\'cd"
gsub("\\\\'", "\\\\\"", line)

and will replace \' with  \"
 
> BTW is it documented anywhere that you need four backslashes in an RE to
> match one in the target, when it is being passed as an argument to gsub
> or
> grep? How would I know how many levels of doubling up to use for any
> other
> functions? (I got to 4 consecutive \ by trial and error in this case,
> but
> have a dim memory of having read about it somewhere.)

There are two levels because backslashes are escape characters both to
R strings and regular expressions. So in the above, "line" is 

ab\'cd

and the match pattern is 

\\' which matches \' 

and the replacement is

\\" which becomes \"


More interesting is

> gsub("\\'", "a", line)
[1] "ab\\'cda"
> gsub("\\'", "a", line, perl=T)
[1] "ab\\acd"

so \' matches a single quote with PCRE but not with ordinary RE. (Yes,
there's a reason...)

-- 
   O__  ---- Peter Dalgaard             Blegdamsvej 3  
  c/ /'_ --- Dept. of Biostatistics     2200 Cph. N   
 (*) \(*) -- University of Copenhagen   Denmark      Ph: (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)             FAX: (+45) 35327907



From ripley at stats.ox.ac.uk  Tue Aug 12 18:25:28 2003
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Tue, 12 Aug 2003 17:25:28 +0100 (BST)
Subject: [R] capturing output from Win 98 shell
In-Reply-To: <3F3905FE.60504@statistik.uni-dortmund.de>
Message-ID: <Pine.LNX.4.44.0308121715270.3695-100000@gannet.stats>

On Tue, 12 Aug 2003, Uwe Ligges wrote:

> Simon Fear wrote:
> > Uwe, you suggest
> > 
> > 
> >>So you almost got it, just read ?shell carefully enough:
> >>
> >>  filenames <- shell("dir D:\\tmp\\*.sasb7dat /b", intern = TRUE)
> >>
> > 
> > 
> > Unfortunately, this does not work and is why I wrote to Rhelp (OK I used
> > system instead of shell, but I tried shell first); here filenames is
> > assigned
> > character(0) even though the dir output is not empty.

It would have beeen helpful to have mentioned that.

> Hmmm. It works on WinNT, and I cannot check a DOS based OS like Win98, 
> because we don't have any of those DOS based operating systems in our 
> department.

shell() also works under WinXP.

> > Believe me I did read ?shell many times before I wrote: but I am
> > reporting
> > that shell does not do what ?shell says it should.

shell() and system() are a low-level interface (written by Guido M) that
work hard to capture the stdin and stdout.  It seems your shell is not
actually sending its printout to stdout, so it does not get captured.  
That's quite consistent with what ?shell says it does.

The normal way out is to use a better shell: we used to use one called
cmd32.exe (I believe) to get Emacs to be able to do this.

If you insist on using a long-obselete OS, please help us continue ot 
support it by supplying patches and workarounds.  I think we (like 
Microsoft) are going to have to think seriously about withdrawing support 
from non-NT-based versions of Windows fairly soon.

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From Simon.Fear at synequanon.com  Tue Aug 12 18:28:01 2003
From: Simon.Fear at synequanon.com (Simon Fear)
Date: Tue, 12 Aug 2003 17:28:01 +0100
Subject: [R] grep and gsub on backslash and quotes
Message-ID: <6C8A8033ABC1E3468048ABC4F13CE572AC0D56@synequanon01>

Thank you. Single backslash version, first thing I tried (I thought)
works
just fine when I copy and paste, ergo I must have got confused by some
stupid
typo of mine. Sorry to waste everyone's  time over this. (Still, I am
probably not the only confused user when it comes to RE handling - I
hope the
examples posted will be of as much use to others as they are to me.)
Simon


> -----Original Message-----
> From: Prof Brian Ripley [mailto:ripley at stats.ox.ac.uk]
> Sent: 12 August 2003 17:13
> To: Simon Fear
> Cc: r-help at stat.math.ethz.ch
> Subject: Re: [R] grep and gsub on backslash and quotes
> 
> 
> Security Warning:
> If you are not sure an attachment is safe to open please contact 
> Andy on x234. There are 0 attachments with this message.
> ________________________________________________________________
> 
> On Tue, 12 Aug 2003, Simon Fear wrote:
> 
> > The following code works,  to gsub single quotes to double quotes:
> > 
> > line <- gsub("'", '"', line)
> > 
> > (that's a single quote within doubles then a double within 
> singles if
> > your
> > viewer's font is not good).
> > 
> > But The R Language Manual tells me that
> > 
> > Quotes and other special characters within strings
> > are specified using escape sequences:
> > \' single quote
> > \" double quote
> > 
> > so why is the following wrong: gsub("\\\\'", "\\\\"", line)? That or
> any
> > other number of backslashes (have tried all up to n=6 just for good
> > measure).
> > 
> > BTW is it documented anywhere that you need four 
> backslashes in an RE
> to
> > match one in the target, when it is being passed as an argument to
> gsub
> > or
> > grep?
> 
> It's not true, so I hope it is not documented anywhere.  You 
> may need 6,
> as in the following from methods():
> 
>     res <- sort(grep(gsub("([.[])", "\\\\\\1", name), an, 
> value = TRUE))
> 
> since that is \\ \1 withou tthe space.  Each backslash in the target
> only
> needs to be doubled.
> 
> In your example
> 
> gsub("\'", "\"", line) or even gsub("'", "\"", line)
> 
> is all you need: only R strings need the escape.
> 
> 
> -- 
> Brian D. Ripley,                  ripley at stats.ox.ac.uk
> Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
> University of Oxford,             Tel:  +44 1865 272861 (self)
> 1 South Parks Road,                     +44 1865 272866 (PA)
> Oxford OX1 3TG, UK                Fax:  +44 1865 272595
>
 

Simon Fear
Senior Statistician
Syne qua non Ltd
Tel: +44 (0) 1379 644449
Fax: +44 (0) 1379 644445
email: Simon.Fear at synequanon.com
web: http://www.synequanon.com
 
Number of attachments included with this message: 0
 
This message (and any associated files) is confidential and\...{{dropped}}



From paulda at BATTELLE.ORG  Tue Aug 12 18:38:35 2003
From: paulda at BATTELLE.ORG (Paul, David  A)
Date: Tue, 12 Aug 2003 12:38:35 -0400
Subject: [R] Sorting a dataframe
Message-ID: <940250A9EB37A24CBE28D858EF07774967AA5E@ws-bco-mse3.milky-way.battelle.org>

Undoubtedly a simple question:

I've looked at order() and sort() in the help pages for
R1.7.1.  It doesn't appear that these functions are immediately
suited to doing the same thing as

PROC SORT DATA = BLAH;
	BY X Y Z;
RUN;

in SAS.  I have also checked Frank Harrell's Hmisc library.
Could someone point me in the right direction so I can sort 
by the levels of Z within the levels of Y within the levels 
of X?  Everything needs to be in ascending order.


Much thanks in advance,
  david paul



From MSchwartz at medanalytics.com  Tue Aug 12 18:54:13 2003
From: MSchwartz at medanalytics.com (Marc Schwartz)
Date: Tue, 12 Aug 2003 11:54:13 -0500
Subject: [R] Sorting a dataframe
In-Reply-To: <940250A9EB37A24CBE28D858EF07774967AA5E@ws-bco-mse3.milky-way.battelle.org>
References: <940250A9EB37A24CBE28D858EF07774967AA5E@ws-bco-mse3.milky-way.battelle.org>
Message-ID: <1060707253.13211.131.camel@localhost>

On Tue, 2003-08-12 at 11:38, Paul, David A wrote:
> Undoubtedly a simple question:
> 
> I've looked at order() and sort() in the help pages for
> R1.7.1.  It doesn't appear that these functions are immediately
> suited to doing the same thing as
> 
> PROC SORT DATA = BLAH;
> 	BY X Y Z;
> RUN;
> 
> in SAS.  I have also checked Frank Harrell's Hmisc library.
> Could someone point me in the right direction so I can sort 
> by the levels of Z within the levels of Y within the levels 
> of X?  Everything needs to be in ascending order.
> 
> 
> Much thanks in advance,
>   david paul



A quick search of the R-help archive would reveal this recent post:

https://www.stat.math.ethz.ch/pipermail/r-help/2003-July/034865.html

:-)


HTH,

Marc Schwartz



From Simon.Fear at synequanon.com  Tue Aug 12 18:55:17 2003
From: Simon.Fear at synequanon.com (Simon Fear)
Date: Tue, 12 Aug 2003 17:55:17 +0100
Subject: [R] capturing output from Win 98 shell
Message-ID: <6C8A8033ABC1E3468048ABC4F13CE572AC0D58@synequanon01>

Prof Brian Ripley writes:

> 
> If you insist on using a long-obselete OS, please help us continue ot 
> support it by supplying patches and workarounds.  I think we (like 
> Microsoft) are going to have to think seriously about withdrawing
> support 
> from non-NT-based versions of Windows fairly soon.
> 

Believe me, using Win98 is NOT my choice. I would only ever use
Unix/Linux.
Are there other statisticians similarly limited?

I am already sticking my neck out quite a lot using R in an industry
obsessed
with SAS. If I tell IT I'm going to use a nonstandard shell, emacs, ess,
bash, and CygWin as well as R they'll probably revoke my username.

Still, I for one can understand withdrawing support for old OSs and if I
have
to stick with 1.7.1 as being the last version to work with Win98 so be
it -
it as, after all, as is, the most amazing piece of software any
statistician
could have wished for.
 

Simon Fear
Senior Statistician
Syne qua non Ltd
Tel: +44 (0) 1379 644449
Fax: +44 (0) 1379 644445
email: Simon.Fear at synequanon.com
web: http://www.synequanon.com
 
Number of attachments included with this message: 0
 
This message (and any associated files) is confidential and\...{{dropped}}



From th50 at leicester.ac.uk  Tue Aug 12 18:56:51 2003
From: th50 at leicester.ac.uk (Hotz, T.)
Date: Tue, 12 Aug 2003 17:56:51 +0100
Subject: [R] Sorting a dataframe
Message-ID: <1F2CE8D4B0195E488213E8B8CCF7148602501315@saffron.cfs.le.ac.uk>

Dear Paul,

Use order() to get the indices of the ordered rows.

See ?order

> blah <- data.frame(X = rep(3:1, each=4), Y = rep(c(2,1,2,1), 3), Z = rep(2:1, 6))
> blah
   X Y Z
1  3 2 2
2  3 1 1
3  3 2 2
4  3 1 1
5  2 2 2
6  2 1 1
7  2 2 2
8  2 1 1
9  1 2 2
10 1 1 1
11 1 2 2
12 1 1 1
> blah <- data.frame(X = rep(3:1, each=4), Y = rep(c(2,2,1,1), 3), Z = rep(2:1, 6))
> blah
   X Y Z
1  3 2 2
2  3 2 1
3  3 1 2
4  3 1 1
5  2 2 2
6  2 2 1
7  2 1 2
8  2 1 1
9  1 2 2
10 1 2 1
11 1 1 2
12 1 1 1
> with(blah, order(X, Y, Z)) # this gives the indices of the ordered rows
 [1] 12 11 10  9  8  7  6  5  4  3  2  1
> blah[with(blah, order(X, Y, Z)), ]
   X Y Z
12 1 1 1
11 1 1 2
10 1 2 1
9  1 2 2
8  2 1 1
7  2 1 2
6  2 2 1
5  2 2 2
4  3 1 1
3  3 1 2
2  3 2 1
1  3 2 2

HTH

Thomas

---

Thomas Hotz
Research Associate in Medical Statistics
University of Leicester
United Kingdom

Department of Epidemiology and Public Health
22-28 Princess Road West
Leicester
LE1 6TP
Tel +44 116 252-5410
Fax +44 116 252-5423

Division of Medicine for the Elderly
Department of Medicine
The Glenfield Hospital
Leicester
LE3 9QP
Tel +44 116 256-3643
Fax +44 116 232-2976


> -----Original Message-----
> From: Paul, David A [mailto:paulda at BATTELLE.ORG]
> Sent: 12 August 2003 17:39
> To: 'r-help at stat.math.ethz.ch'
> Subject: [R] Sorting a dataframe
> 
> 
> Undoubtedly a simple question:
> 
> I've looked at order() and sort() in the help pages for
> R1.7.1.  It doesn't appear that these functions are immediately
> suited to doing the same thing as
> 
> PROC SORT DATA = BLAH;
> 	BY X Y Z;
> RUN;
> 
> in SAS.  I have also checked Frank Harrell's Hmisc library.
> Could someone point me in the right direction so I can sort 
> by the levels of Z within the levels of Y within the levels 
> of X?  Everything needs to be in ascending order.
> 
> 
> Much thanks in advance,
>   david paul
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
>



From ripley at stats.ox.ac.uk  Tue Aug 12 19:00:48 2003
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Tue, 12 Aug 2003 18:00:48 +0100 (BST)
Subject: [R] capturing output from Win 98 shell
In-Reply-To: <6C8A8033ABC1E3468048ABC4F13CE572AC0D58@synequanon01>
Message-ID: <Pine.LNX.4.44.0308121758100.3766-100000@gannet.stats>

I should clarify that `withdrawing support' does not mean that it will no 
longer work with Win95/98/ME, just that we will no longer attempt to 
ensure that it does.

On Tue, 12 Aug 2003, Simon Fear wrote:

> Prof Brian Ripley writes:
> 
> > 
> > If you insist on using a long-obselete OS, please help us continue ot 
> > support it by supplying patches and workarounds.  I think we (like 
> > Microsoft) are going to have to think seriously about withdrawing
> > support 
> > from non-NT-based versions of Windows fairly soon.
> > 
> 
> Believe me, using Win98 is NOT my choice. I would only ever use
> Unix/Linux.
> Are there other statisticians similarly limited?
> 
> I am already sticking my neck out quite a lot using R in an industry
> obsessed
> with SAS. If I tell IT I'm going to use a nonstandard shell, emacs, ess,
> bash, and CygWin as well as R they'll probably revoke my username.
> 
> Still, I for one can understand withdrawing support for old OSs and if I
> have
> to stick with 1.7.1 as being the last version to work with Win98 so be
> it -
> it as, after all, as is, the most amazing piece of software any
> statistician
> could have wished for.
>  
> 
> Simon Fear
> Senior Statistician
> Syne qua non Ltd
> Tel: +44 (0) 1379 644449
> Fax: +44 (0) 1379 644445
> email: Simon.Fear at synequanon.com
> web: http://www.synequanon.com
>  
> Number of attachments included with this message: 0
>  
> This message (and any associated files) is confidential and 
> contains information which may be legally privileged.  It is 
> intended for the stated addressee(s) only.  Access to this 
> email by anyone else is unauthorised.  If you are not the 
> intended addressee, any action taken (or not taken) in 
> reliance on it, or any disclosure or copying of the contents of 
> it is unauthorised and unlawful.  If you are not the addressee, 
> please inform the sender immediately and delete the email 
> from your system.
> 
> This message and any associated attachments have been 
> checked for viruses using an internationally recognised virus 
> detection process.  However, Internet communications cannot 
> be guaranteed to be secure or error-free as information could 
> be intercepted, corrupted, lost, destroyed, arrive late or 
> incomplete. Therefore, we do not accept responsibility for any 
> errors or omissions that are present in this message, or any 
> attachment, that have arisen as a result of e-mail transmission.  
> If verification is required, please request a hard-copy version. 
> Any views or opinions presented are solely those of the author 
> and do not necessarily represent those of Syne qua non.
> 

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From spencer.graves at pdf.com  Tue Aug 12 19:03:12 2003
From: spencer.graves at pdf.com (Spencer Graves)
Date: Tue, 12 Aug 2003 10:03:12 -0700
Subject: [R] Sorting a dataframe
References: <940250A9EB37A24CBE28D858EF07774967AA5E@ws-bco-mse3.milky-way.battelle.org>
Message-ID: <3F391DD0.8060402@pdf.com>

Why won't "order" solve your problem?  "?order" in R 1.7.1 for Windows 
includes an example sorting lexicographically 3 variables in ascending 
order.

spencer graves

Paul, David A wrote:
> Undoubtedly a simple question:
> 
> I've looked at order() and sort() in the help pages for
> R1.7.1.  It doesn't appear that these functions are immediately
> suited to doing the same thing as
> 
> PROC SORT DATA = BLAH;
> 	BY X Y Z;
> RUN;
> 
> in SAS.  I have also checked Frank Harrell's Hmisc library.
> Could someone point me in the right direction so I can sort 
> by the levels of Z within the levels of Y within the levels 
> of X?  Everything needs to be in ascending order.
> 
> 
> Much thanks in advance,
>   david paul
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help



From ying.jin at Vanderbilt.Edu  Tue Aug 12 20:48:54 2003
From: ying.jin at Vanderbilt.Edu (Jin, Ying)
Date: Tue, 12 Aug 2003 13:48:54 -0500
Subject: [R] install 'XML' package
Message-ID: <F7BF99CC7F0E174DBCF8C6175B0325FF013B940F@mailbe01>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20030812/00d46829/attachment.pl

From ripley at stats.ox.ac.uk  Tue Aug 12 21:05:24 2003
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Tue, 12 Aug 2003 20:05:24 +0100 (BST)
Subject: [R] install 'XML' package
In-Reply-To: <F7BF99CC7F0E174DBCF8C6175B0325FF013B940F@mailbe01>
Message-ID: <Pine.LNX.4.44.0308121958440.4045-100000@gannet.stats>

Please read the INSTALL file that comes with the package.

Actually reading the documentation can be wonderfully liberating, and is a 
skill that can work wonders in life.  Please get some practice in.

You might even work out which list supports Omegahat packages.

On Tue, 12 Aug 2003, Jin, Ying wrote:

> Greetings-
>  
> I tried to install 'XML' packages in R and get following error message:

[junk deleted for you: next time please do it yourself]

> checking for libxml/parser.h... (cached) no
> Cannot find parser.h. Set the value of the environment variable
>     LIBXML_INCDIR
> to point to where it can be found.
> ERROR: configuration failed for package 'XML'
> Delete downloaded files (y/N)? 


-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From p.dalgaard at biostat.ku.dk  Tue Aug 12 21:31:57 2003
From: p.dalgaard at biostat.ku.dk (Peter Dalgaard BSA)
Date: Tue, 12 Aug 2003 19:31:57 -0000
Subject: [R] capturing output from Win 98 shell
In-Reply-To: <6C8A8033ABC1E3468048ABC4F13CE572AC0D58@synequanon01>
References: <6C8A8033ABC1E3468048ABC4F13CE572AC0D58@synequanon01>
Message-ID: <x23cg6ekdb.fsf@biostat.ku.dk>

"Simon Fear" <Simon.Fear at synequanon.com> writes:

> Prof Brian Ripley writes:
> 
> > 
> > If you insist on using a long-obselete OS, please help us continue ot 
> > support it by supplying patches and workarounds.  I think we (like 
> > Microsoft) are going to have to think seriously about withdrawing
> > support 
> > from non-NT-based versions of Windows fairly soon.
> > 
> 
> Believe me, using Win98 is NOT my choice. I would only ever use
> Unix/Linux.
> Are there other statisticians similarly limited?
> 
> I am already sticking my neck out quite a lot using R in an industry
> obsessed
> with SAS. If I tell IT I'm going to use a nonstandard shell, emacs, ess,
> bash, and CygWin as well as R they'll probably revoke my username.
> 
> Still, I for one can understand withdrawing support for old OSs and if I
> have
> to stick with 1.7.1 as being the last version to work with Win98 so be
> it -
> it as, after all, as is, the most amazing piece of software any
> statistician
> could have wished for.

For what it may be worth, both shell("dir", intern=T) and
system("command /c dir") worked for me with 1.7.0 on Grete's W98
machine...

-- 
   O__  ---- Peter Dalgaard             Blegdamsvej 3  
  c/ /'_ --- Dept. of Biostatistics     2200 Cph. N   
 (*) \(*) -- University of Copenhagen   Denmark      Ph: (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)             FAX: (+45) 35327907



From gerhard.prade at uni-bielefeld.de  Tue Aug 12 22:03:25 2003
From: gerhard.prade at uni-bielefeld.de (Gerhard Prade)
Date: Tue, 12 Aug 2003 22:03:25 +0200
Subject: [R] Crosstabs
Message-ID: <3F39480D.5090009@uni-bielefeld.de>

Hello all,

i think i am to silly.  I have installed R 1.7.1 (2003-06-16). Installed
some packages like xtables ore xml. I tried out this to installing
packages. Then i tried to make a crosstable like i know it from spss.
They say in this list that it would be going.
I made a table in asci-format, seperated with tabs or blanks and than i
use something like this:

soz<-read.table("/home/user/test.txt")

ok. that works.

than i want make a crosstables with the first and the second variable.

i tried ftable with something like that:

ftable(soz)

or

ftable(soz, col.vars = 1:2)

For me it is a quiz what the corect syntax for this is.
I want get something like this:

_______________________________________________________________
Question 1.1
What is your age?
______________

		PC-User	     Linux-User   Windows-User

0-10 Years	10%   10     20%   20     30%	30

10-20 Years	50%   50     70%   70     40%   40

over 20 Years	40%   40     10%   10     30%   30
________________________________________________________

Summary		100%  100    100%  100    100%   100
_______________________________________________________________


Can anybody help and send me a example for the correct way and the 
syntax i must use to get this? I dont understand the manual, because the 
ways from r is very different to spss-syntax, i think.

Thanks, Gerhard



From MSchwartz at medanalytics.com  Tue Aug 12 22:39:18 2003
From: MSchwartz at medanalytics.com (Marc Schwartz)
Date: Tue, 12 Aug 2003 15:39:18 -0500
Subject: [R] Crosstabs
In-Reply-To: <3F39480D.5090009@uni-bielefeld.de>
References: <3F39480D.5090009@uni-bielefeld.de>
Message-ID: <1060720758.13211.229.camel@localhost>

On Tue, 2003-08-12 at 15:03, Gerhard Prade wrote:
> Hello all,
> 
> i think i am to silly.  I have installed R 1.7.1 (2003-06-16). Installed
> some packages like xtables ore xml. I tried out this to installing
> packages. Then i tried to make a crosstable like i know it from spss.
> They say in this list that it would be going.
> I made a table in asci-format, seperated with tabs or blanks and than i
> use something like this:
> 
> soz<-read.table("/home/user/test.txt")
> 
> ok. that works.
> 
> than i want make a crosstables with the first and the second variable.
> 
> i tried ftable with something like that:
> 
> ftable(soz)
> 
> or
> 
> ftable(soz, col.vars = 1:2)
> 
> For me it is a quiz what the corect syntax for this is.
> I want get something like this:
> 
> _______________________________________________________________
> Question 1.1
> What is your age?
> ______________
> 
> 		PC-User	     Linux-User   Windows-User
> 
> 0-10 Years	10%   10     20%   20     30%	30
> 
> 10-20 Years	50%   50     70%   70     40%   40
> 
> over 20 Years	40%   40     10%   10     30%   30
> ________________________________________________________
> 
> Summary		100%  100    100%  100    100%   100
> _______________________________________________________________
> 
> 
> Can anybody help and send me a example for the correct way and the 
> syntax i must use to get this? I dont understand the manual, because the 
> ways from r is very different to spss-syntax, i think.
> 
> Thanks, Gerhard


To generate a table similar to the above [actually closer to SAS' PROC
Freq and S-Plus' crosstabs()], use the CrossTable() function in the
'gregmisc' package on CRAN.

Once you have installed 'gregmisc' and have loaded the library [using
"library(gregmisc)"], you can then use "?CrossTable" to display the help
for the function with example code so that you can get a feel for the
format of the output and associated options.

The basic syntax for use would be:

CrossTable(Var1, Var2)

where Var1 will be the rows and Var2 will be the columns.

If you have further questions on the use of CrossTable(), let me know.

Best regards and welcome to R,

Marc Schwartz



From kwan022 at stat.auckland.ac.nz  Tue Aug 12 22:43:22 2003
From: kwan022 at stat.auckland.ac.nz (Ko-Kang Kevin Wang)
Date: Wed, 13 Aug 2003 08:43:22 +1200 (NZST)
Subject: [R] Crosstabs
In-Reply-To: <3F39480D.5090009@uni-bielefeld.de>
Message-ID: <Pine.LNX.4.44.0308130842210.17034-100000@stat55.stat.auckland.ac.nz>

On Tue, 12 Aug 2003, Gerhard Prade wrote:

> i think i am to silly.  I have installed R 1.7.1 (2003-06-16). Installed
> some packages like xtables ore xml. I tried out this to installing
> packages. Then i tried to make a crosstable like i know it from spss.
> They say in this list that it would be going.
> 
> than i want make a crosstables with the first and the second variable.

I'm not sure what SPSS does as I have never used it.  But is the 
CrossTable() function in the gregmisc library what you want?

-- 
Cheers,

Kevin

------------------------------------------------------------------------------
"On two occasions, I have been asked [by members of Parliament],
'Pray, Mr. Babbage, if you put into the machine wrong figures, will
the right answers come out?' I am not able to rightly apprehend the
kind of confusion of ideas that could provoke such a question."

-- Charles Babbage (1791-1871) 
---- From Computer Stupidities: http://rinkworks.com/stupid/

--
Ko-Kang Kevin Wang
Master of Science (MSc) Student
SLC Tutor and Lab Demonstrator
Department of Statistics
University of Auckland
New Zealand
Homepage: http://www.stat.auckland.ac.nz/~kwan022
Ph: 373-7599
    x88475 (City)
    x88480 (Tamaki)



From jxdai at cluster.chem.nyu.edu  Tue Aug 12 22:51:10 2003
From: jxdai at cluster.chem.nyu.edu (dai)
Date: Tue, 12 Aug 2003 13:51:10 -0700
Subject: [R] Princomp function in R
Message-ID: <Pine.SGI.4.44.0308121348530.44571-100000@cluster.chem.nyu.edu>

Hi,

I want to use Princomp function in R, but com up an error as " Error:
couldn't find function "princomp" ", can you help me resolve this problem?
Thanks,

Jixin Dai, Ph.D



From anna at ptolemy.arc.nasa.gov  Tue Aug 12 22:58:19 2003
From: anna at ptolemy.arc.nasa.gov (Anna  H. Pryor)
Date: Tue, 12 Aug 2003 13:58:19 -0700
Subject: [R] Post Hoc methods for anova in R
Message-ID: <200308121358.19654.anna@ptolemy.arc.nasa.gov>

Does anyone know of some methods already programmed up ( ie freeware : )  ) in 
R for the post hoc methods in anova.  Particularly Scheffe's method or 
Tukey's?

Anna



From kwan022 at stat.auckland.ac.nz  Tue Aug 12 23:00:10 2003
From: kwan022 at stat.auckland.ac.nz (Ko-Kang Kevin Wang)
Date: Wed, 13 Aug 2003 09:00:10 +1200 (NZST)
Subject: [R] Princomp function in R
In-Reply-To: <Pine.SGI.4.44.0308121348530.44571-100000@cluster.chem.nyu.edu>
Message-ID: <Pine.LNX.4.44.0308130900060.29054-100000@stat57.stat.auckland.ac.nz>

It is in package mva.
On Tue, 12 Aug 2003, dai wrote:

> Date: Tue, 12 Aug 2003 13:51:10 -0700
> From: dai <jxdai at cluster.chem.nyu.edu>
> To: R-help at stat.math.ethz.ch
> Cc: jd218 at scires.acf.nyu.edu
> Subject: [R] Princomp function in R
> 
> Hi,
> 
> I want to use Princomp function in R, but com up an error as " Error:
> couldn't find function "princomp" ", can you help me resolve this problem?
> Thanks,
> 
> Jixin Dai, Ph.D
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> 

-- 
Cheers,

Kevin

------------------------------------------------------------------------------
"On two occasions, I have been asked [by members of Parliament],
'Pray, Mr. Babbage, if you put into the machine wrong figures, will
the right answers come out?' I am not able to rightly apprehend the
kind of confusion of ideas that could provoke such a question."

-- Charles Babbage (1791-1871) 
---- From Computer Stupidities: http://rinkworks.com/stupid/

--
Ko-Kang Kevin Wang
Master of Science (MSc) Student
SLC Tutor and Lab Demonstrator
Department of Statistics
University of Auckland
New Zealand
Homepage: http://www.stat.auckland.ac.nz/~kwan022
Ph: 373-7599
    x88475 (City)
    x88480 (Tamaki)



From bates at stat.wisc.edu  Tue Aug 12 23:52:43 2003
From: bates at stat.wisc.edu (Douglas Bates)
Date: Tue, 12 Aug 2003 21:52:43 -0000
Subject: [R] Post Hoc methods for anova in R
In-Reply-To: <200308121358.19654.anna@ptolemy.arc.nasa.gov>
References: <200308121358.19654.anna@ptolemy.arc.nasa.gov>
Message-ID: <6rekzqa6gv.fsf@bates4.stat.wisc.edu>

"Anna  H. Pryor" <anna at ptolemy.arc.nasa.gov> writes:

> Does anyone know of some methods already programmed up ( ie freeware

We prefer the term "Open Source" to "freeware".

> : ) ) in R for the post hoc methods in anova.  Particularly
> Scheffe's method or Tukey's?

?TukeyHSD

See also package multcomp.



From tomlinso at purdue.edu  Wed Aug 13 00:38:58 2003
From: tomlinso at purdue.edu (nels.tomlinson.1)
Date: Tue, 12 Aug 2003 17:38:58 -0500 (EST)
Subject: [R] Replacing underscore character in Windows GUI
Message-ID: <Pine.SOL.4.51.0308121732250.16951@herald.cc.purdue.edu>

Hello, all,

I'd like to propose that now that the underscore-as-assignment-operator
is to be removed from R (good thing, too), that the Windows GUI should
replace the underscore ``_''  with the proper assignment operator ``<-''
when you type in the underscore character.

This is the current default behaviour in the ESS mode in emacs, and seems
to me to be a generally good idea.  If no one can come up with a good
reason not to do this, I hope that the maintainers of the GUI will put it
on their to-do list.

Thanks,
Nels



From bates at stat.wisc.edu  Wed Aug 13 01:01:50 2003
From: bates at stat.wisc.edu (Douglas Bates)
Date: Tue, 12 Aug 2003 23:01:50 -0000
Subject: [R] Replacing underscore character in Windows GUI
In-Reply-To: <Pine.SOL.4.51.0308121732250.16951@herald.cc.purdue.edu>
References: <Pine.SOL.4.51.0308121732250.16951@herald.cc.purdue.edu>
Message-ID: <6roeyu8op2.fsf@bates4.stat.wisc.edu>

"nels.tomlinson.1" <tomlinso at purdue.edu> writes:

> I'd like to propose that now that the underscore-as-assignment-operator
> is to be removed from R (good thing, too), that the Windows GUI should
> replace the underscore ``_''  with the proper assignment operator ``<-''
> when you type in the underscore character.
> 
> This is the current default behaviour in the ESS mode in emacs, and seems
> to me to be a generally good idea.  If no one can come up with a good
> reason not to do this, I hope that the maintainers of the GUI will put it
> on their to-do list.

One of the purposes of deprecating then disallowing the underscore as an
assignment character is to allow its use in names.



From Paul.Sorenson at vision-bio.com  Wed Aug 13 01:36:54 2003
From: Paul.Sorenson at vision-bio.com (Paul Sorenson)
Date: Wed, 13 Aug 2003 09:36:54 +1000
Subject: [R] SQL Dates
Message-ID: <5E06BFED29594F4C9C5EBE230DE320C6273956@ewok.vsl.com.au>

How can I have SQL queries (ODBB/Windows) return dates (cf text) for date columns?

I can successfully query and ODBC database with R.  I can subsequently convert the text representations of dates in columns using as.POSIXxx functions but I figured there must be a way to achieve this at the time of the query.

Any tips?

paul



From haynesm at cfr.nichd.nih.gov  Wed Aug 13 03:04:04 2003
From: haynesm at cfr.nichd.nih.gov (Haynes, Maurice (NIH/NICHD))
Date: Tue, 12 Aug 2003 21:04:04 -0400
Subject: [R] Printing linefeeds using sprintf
Message-ID: <6000BB14AFA9A741BC2315A598837ED569393A@nihexchange4.nih.gov>

Is there a way to print complex, multi-line text and numeric output with
good control of the field width and precision of numeric values?  In the
first example in the R Documentation for the sprintf function, there appears
to be a linefeed code "\n", but it is printed literally.

> sprintf("%s is %f feet tall\n", "Sven", 7)
[1] "Sven is 7.000000 feet tall\n"

Thanks,

O. Maurice Haynes
National Institute of Child Health and Human Development
Child and Family Research Section
6705 Rockledge Drive
Bethesda, MD  20892
Voice: 301-496-8180
Fax: 301-496-2766
E-Mail: mh192j at nih.gov



From suzette at sdac.harvard.edu  Wed Aug 13 03:23:25 2003
From: suzette at sdac.harvard.edu (Suzette Blanchard)
Date: Tue, 12 Aug 2003 21:23:25 -0400 (EDT)
Subject: [R] nlme
Message-ID: <Pine.GSO.4.40.0308122053300.16147-100000@sdac.harvard.edu>


Greetings,

        I am trying to use nlme to model a large data set
of pharmacokinetic concentrations with sparse data per cluster.
The base model is a one compartment model with first order absorption.
The program runs but the PNLS step never converges. The parameter
estimates are reasonable. The plot of the individual level residuals is
pretty reasonable and the plot of the residuals from the population model
is ok but not great.

        My questions are:

        1) I know the PNLS step starts and updates the fixed parameters
and random effects. Step 2 is the LME step and it updates the interpatient
variance.  Does the LME step include both the EM algorithm and a Newton-
Raphson ms step?

        2) I set Maxiter=500 interations and I get convergence on what
I think is the LME step but PNLS step never converges. Any suggestions
on what I might do to achieve convergence would be very much appreciated.

	Thank you,

        Suzette


=================================
Suzette Blanchard, Ph.D.
Research Scientist
Frontier Science Foundation
1244 Boylston St. Suite 303
Chestnut Hill, MA 02467
Email:  suzette at sdac.harvard.edu
Phone:  (617) 632-2007
Fax:    (617) 632-2001



From hedderik at cmu.edu  Wed Aug 13 03:33:28 2003
From: hedderik at cmu.edu (Hedderik van Rijn)
Date: Tue, 12 Aug 2003 21:33:28 -0400
Subject: [R] Printing linefeeds using sprintf
In-Reply-To: <6000BB14AFA9A741BC2315A598837ED569393A@nihexchange4.nih.gov>
Message-ID: <240D68D8-CD2E-11D7-BFE2-000A956B93BA@cmu.edu>

> Is there a way to print complex, multi-line text and numeric output 
> with
> good control of the field width and precision of numeric values?  In 
> the
> first example in the R Documentation for the sprintf function, there 
> appears
> to be a linefeed code "\n", but it is printed literally.
>
>> sprintf("%s is %f feet tall\n", "Sven", 7)
> [1] "Sven is 7.000000 feet tall\n"

That is because the output of sprintf is the _string_ that sprintf 
returns. You could cat this string to get the behavior you probably 
expected:

 > cat(sprintcat(sprintf("%s is %f feet tall\n\nNew line\n", "Sven", 7))
Sven is 7.000000 feet tall

New line
 >

The following code defines a C-like "printf":

printf <- function(...) { cat(sprintf(...)) }

 > printf("%s is %f feet tall\n", "Sven", 7)
Sven is 7.000000 feet tall
 >

  - Hedderik.



From ok at cs.otago.ac.nz  Wed Aug 13 03:34:20 2003
From: ok at cs.otago.ac.nz (Richard A. O'Keefe)
Date: Wed, 13 Aug 2003 13:34:20 +1200 (NZST)
Subject: [R] What does m$... mean?
Message-ID: <200308130134.h7D1YKVZ029610@atlas.otago.ac.nz>

While reading through some of the R source code, I have come
across forms such as
    m$...
    m$... <- e
and I wondered what they meant.

?"$" mentions x$name, but not $... 
All it says is

     The operators `$' and `$<-' do not evaluate their second argument.
      It is translated to a string and that string is used to locate
     the correct component of the first argument.

Does this mean that m$... is the same as m$"..."
and m$... <- e is the same as m$"..." <- e?

That's what it seems to do when I try it on some small data frames,
but is that ALL there is to it, or is there some special magic going
on?  Is there any connection with the use of ... in formal and actual
parameter lists, or is it just accidental?  Why would anyone _want_
to use $... ?



From spencer.graves at pdf.com  Wed Aug 13 04:08:47 2003
From: spencer.graves at pdf.com (Spencer Graves)
Date: Tue, 12 Aug 2003 19:08:47 -0700
Subject: [R] What does m$... mean?
References: <200308130134.h7D1YKVZ029610@atlas.otago.ac.nz>
Message-ID: <3F399DAF.3030404@pdf.com>

"a$b" = "a[['b']] = attribute "b" of list "a".

A basic object in R is a list, and the "$" operator provides one means 
of accessing named attributes of a list.

	  Beginning with R 1.7, objects can also have "slots", which are 
accessed as "a at b".  I have yet to understand why "slots" were 
introduced;  perhaps someone else will explain this.

hope this helps.
spencer graves

Richard A. O'Keefe wrote:
> While reading through some of the R source code, I have come
> across forms such as
>     m$...
>     m$... <- e
> and I wondered what they meant.
> 
> ?"$" mentions x$name, but not $... 
> All it says is
> 
>      The operators `$' and `$<-' do not evaluate their second argument.
>       It is translated to a string and that string is used to locate
>      the correct component of the first argument.
> 
> Does this mean that m$... is the same as m$"..."
> and m$... <- e is the same as m$"..." <- e?
> 
> That's what it seems to do when I try it on some small data frames,
> but is that ALL there is to it, or is there some special magic going
> on?  Is there any connection with the use of ... in formal and actual
> parameter lists, or is it just accidental?  Why would anyone _want_
> to use $... ?
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help



From kjetil at entelnet.bo  Wed Aug 13 04:35:47 2003
From: kjetil at entelnet.bo (kjetil brinchmann halvorsen)
Date: Tue, 12 Aug 2003 22:35:47 -0400
Subject: [R] Replacing underscore character in Windows GUI
In-Reply-To: <Pine.SOL.4.51.0308121732250.16951@herald.cc.purdue.edu>
Message-ID: <3F396BC3.5288.13F560F@localhost>

On 12 Aug 2003 at 17:38, nels.tomlinson.1 wrote:

I thought one of the reasons for removing the underscore as synonym 
to <- was that at some later point it could be reintroduced in the 
grammar, for example as a valid part of variable names. 

If this is the case, this suggestion is a bad idea. Better to forget 
the habvit of using _ for assignment! Maybe this reemplacement of _ 
with <- will be removed from ess too?

Kjetil Halvorsen

> Hello, all,
> 
> I'd like to propose that now that the underscore-as-assignment-operator
> is to be removed from R (good thing, too), that the Windows GUI should
> replace the underscore ``_''  with the proper assignment operator ``<-''
> when you type in the underscore character.
> 
> This is the current default behaviour in the ESS mode in emacs, and seems
> to me to be a generally good idea.  If no one can come up with a good
> reason not to do this, I hope that the maintainers of the GUI will put it
> on their to-do list.
> 
> Thanks,
> Nels
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help



From rossini at blindglobe.net  Wed Aug 13 05:07:35 2003
From: rossini at blindglobe.net (A.J. Rossini)
Date: Tue, 12 Aug 2003 20:07:35 -0700
Subject: [R] Replacing underscore character in Windows GUI
In-Reply-To: <3F396BC3.5288.13F560F@localhost> (kjetil brinchmann
	halvorsen's message of "Tue, 12 Aug 2003 22:35:47 -0400")
References: <3F396BC3.5288.13F560F@localhost>
Message-ID: <858ypyfe6g.fsf@blindglobe.net>

"kjetil brinchmann halvorsen" <kjetil at entelnet.bo> writes:

> On 12 Aug 2003 at 17:38, nels.tomlinson.1 wrote:
>
> I thought one of the reasons for removing the underscore as synonym 
> to <- was that at some later point it could be reintroduced in the 
> grammar, for example as a valid part of variable names. 
>
> If this is the case, this suggestion is a bad idea. Better to forget 
> the habvit of using _ for assignment! Maybe this reemplacement of _ 
> with <- will be removed from ess too?

Probably.  But there are those of us who don't like "_" for variable
names.  It reminds me too much of C and Python (though I like the
latter).   

I'd be happier removing "=" for assignment, as well.  It's a
monstrosity, though supposedly wiser heads think otherwise.  I know
the arguments, but I still think it should be removed.

Maybe we'll make "=" electric -- one press becomes "<-", two presses
become "==".  Emacs is wonderful, sometimes.

best,
-tony

-- 
A.J. Rossini     			
rossini at u.washington.edu            http://www.analytics.washington.edu/ 
Biomedical and Health Informatics   University of Washington
Biostatistics, SCHARP/HVTN          Fred Hutchinson Cancer Research Center
UW   :              FAX=206-543-3461 | moving soon to a permanent office
FHCRC: 206-667-7025 FAX=206-667-4812 | Voicemail is pretty sketchy/use Email

CONFIDENTIALITY NOTICE: This e-mail message and any attachme...{{dropped}}



From ripley at stats.ox.ac.uk  Wed Aug 13 07:02:50 2003
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Wed, 13 Aug 2003 06:02:50 +0100 (BST)
Subject: [R] Replacing underscore character in Windows GUI
In-Reply-To: <6roeyu8op2.fsf@bates4.stat.wisc.edu>
Message-ID: <Pine.LNX.4.44.0308130553170.4983-100000@gannet.stats>

On 12 Aug 2003, Douglas Bates wrote:

> "nels.tomlinson.1" <tomlinso at purdue.edu> writes:
> 
> > I'd like to propose that now that the underscore-as-assignment-operator
> > is to be removed from R (good thing, too), that the Windows GUI should
> > replace the underscore ``_''  with the proper assignment operator ``<-''
> > when you type in the underscore character.
> > 
> > This is the current default behaviour in the ESS mode in emacs, and seems
> > to me to be a generally good idea.  If no one can come up with a good
> > reason not to do this, I hope that the maintainers of the GUI will put it
> > on their to-do list.
> 
> One of the purposes of deprecating then disallowing the underscore as an
> assignment character is to allow its use in names.

And it is already valid in character strings and inside quoted (including
by backslashes) non-syntactic names.  The GUI frontend would need to parse
its input to reliably substitute -- ESS does try to do that, but I don't
think it is 100% successful.  (It doesn't know about backticks, at least 
in the version I have installed, for example.)

In short: this would be days of work to do right, so it is not going to 
happen.

> I hope that the maintainers of the GUI will put it
> on their to-do list.

That's not the best response. `Here is a set of patches for you to
consider' is more likely to succeed.  Take a look at the R start-up banner 
and aspire to becoming one of those contributors!

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From ripley at stats.ox.ac.uk  Wed Aug 13 07:16:10 2003
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Wed, 13 Aug 2003 06:16:10 +0100 (BST)
Subject: [R] SQL Dates
In-Reply-To: <5E06BFED29594F4C9C5EBE230DE320C6273956@ewok.vsl.com.au>
Message-ID: <Pine.LNX.4.44.0308130614010.5075-100000@gannet.stats>

On Wed, 13 Aug 2003, Paul Sorenson wrote:

> How can I have SQL queries (ODBB/Windows) return dates (cf text) for
> date columns?
> 
> I can successfully query and ODBC database with R.  I can subsequently
> convert the text representations of dates in columns using as.POSIXxx
> functions but I figured there must be a way to achieve this at the time
> of the query.

If you are using the ROBDC package, no.  Perhaps you would contribute an 
extension to RODBC that allows this?  (Did you look at the TODO file in 
the package?)

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From Paul.Sorenson at vision-bio.com  Wed Aug 13 07:22:12 2003
From: Paul.Sorenson at vision-bio.com (Paul Sorenson)
Date: Wed, 13 Aug 2003 15:22:12 +1000
Subject: [R] SQL Dates
Message-ID: <5E06BFED29594F4C9C5EBE230DE320C6014D7126@ewok.vsl.com.au>

Brian,

Thanks - no I didn't check the TODO.  Given that I am an R newbie my first assumption was that I had misunderstood the documentation.

Certainly if I can get my head around the RODBC source (and the R "way") then I could have a crack at extending it.

paul

-----Original Message-----
From: Prof Brian Ripley [mailto:ripley at stats.ox.ac.uk]
Sent: Wednesday, 13 August 2003 3:16 PM
To: Paul Sorenson
Cc: r-help at stat.math.ethz.ch
Subject: Re: [R] SQL Dates


On Wed, 13 Aug 2003, Paul Sorenson wrote:

> How can I have SQL queries (ODBB/Windows) return dates (cf text) for
> date columns?
> 
> I can successfully query and ODBC database with R.  I can subsequently
> convert the text representations of dates in columns using as.POSIXxx
> functions but I figured there must be a way to achieve this at the time
> of the query.

If you are using the ROBDC package, no.  Perhaps you would contribute an 
extension to RODBC that allows this?  (Did you look at the TODO file in 
the package?)

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From ripley at stats.ox.ac.uk  Wed Aug 13 07:24:20 2003
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Wed, 13 Aug 2003 06:24:20 +0100 (BST)
Subject: [R] What does m$... mean?
In-Reply-To: <200308130134.h7D1YKVZ029610@atlas.otago.ac.nz>
Message-ID: <Pine.LNX.4.44.0308130619110.5075-100000@gannet.stats>

On Wed, 13 Aug 2003, Richard A. O'Keefe wrote:

> While reading through some of the R source code, I have come
> across forms such as
>     m$...
>     m$... <- e
> and I wondered what they meant.
> 
> ?"$" mentions x$name, but not $... 

... is a name, a special one in the context of function calls.

> All it says is
> 
>      The operators `$' and `$<-' do not evaluate their second argument.
>       It is translated to a string and that string is used to locate
>      the correct component of the first argument.
> 
> Does this mean that m$... is the same as m$"..."
> and m$... <- e is the same as m$"..." <- e?

Yes.

> That's what it seems to do when I try it on some small data frames,
> but is that ALL there is to it, or is there some special magic going
> on?  Is there any connection with the use of ... in formal and actual
> parameter lists, or is it just accidental?  Why would anyone _want_
> to use $... ?

To add or to change (but almost always to remove) the component of a list
named ...  .  And they occur frequently in matched calls.

This construction (like many others) is explained in `S Programming': it 
seems you may find it enlightening.

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From ripley at stats.ox.ac.uk  Wed Aug 13 07:31:09 2003
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Wed, 13 Aug 2003 06:31:09 +0100 (BST)
Subject: [R] What does m$... mean?
In-Reply-To: <3F399DAF.3030404@pdf.com>
Message-ID: <Pine.LNX.4.44.0308130624510.5075-100000@gannet.stats>

On Tue, 12 Aug 2003, Spencer Graves wrote:

> "a$b" = "a[['b']] = attribute "b" of list "a".

(Not quite always.  First, it is `component' not `attribute' and second $ 
and [[ ]] do behave differently, e.g. for data frames in 1.7.x.)

> A basic object in R is a list, and the "$" operator provides one means 
> of accessing named attributes of a list.
> 
> 	  Beginning with R 1.7, objects can also have "slots", which are 
> accessed as "a at b".  I have yet to understand why "slots" were 
> introduced;  perhaps someone else will explain this.

Slots are part of objects which have formal (S4) classes, made using the 
`methods' package (so have been around since at least R 1.4.0).
They are an implementation of the ideas of Chambers (1998).  Formally 
classed objects are not just lists: they have rules for the number, names 
and types of the slots. (Currently they are lists, but that's an 
implementation detail.)

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From dibakar at hub.nic.in  Wed Aug 13 09:03:21 2003
From: dibakar at hub.nic.in (Dibakar Ray)
Date: Wed 13 Aug 2003 12:33:21 +0530 (IST)
Subject: [R] big data file  geting truncated
Message-ID: <1131.164.100.21.192.1060758201.nicemail@localhost>

I am very new to R. I was trying to load some publicly available Expression
data in to R.
I used the following commands
 mydata<-read.table("dataALLAMLtrain.txt", header=TRUE, sep
="\t",row.names=NULL)
It reads data without any error
Now if I use
edit(mydata)
It shows only 3916 entries, whereas the actual file contains 7129 entries)
My data is something like
Gene Description	Gene Accession
Number	1	2	3	4	5	6	7	8	9	10	11	12	13	14	15	16	17	18	19	20	21	22	23	24	25	26	27	34	35	36	37	38	28	29	30	31	32	33
AFFX-BioB-5_at (endogenous
control)	AFFX-BioB-5_at	-214	-139	-76	-135	-106	-138	-72	-413	5	-88	-165	-67	-92	-113	-107	-117	-476	-81	-44	17	-144	-247	-74	-120	-81	-112	-273	-20	7	-213	-25	-72	-4	15	-318	-32	-124	-135
So it seems R is truncating the data. How  can I load the complete file?
Thanks in advance
Dibakar



From p.dalgaard at biostat.ku.dk  Wed Aug 13 09:26:31 2003
From: p.dalgaard at biostat.ku.dk (Peter Dalgaard BSA)
Date: Wed, 13 Aug 2003 07:26:31 -0000
Subject: [R] big data file  geting truncated
In-Reply-To: <1131.164.100.21.192.1060758201.nicemail@localhost>
References: <1131.164.100.21.192.1060758201.nicemail@localhost>
Message-ID: <x2he4mc8qd.fsf@biostat.ku.dk>

Dibakar Ray <dibakar at hub.nic.in> writes:

> I am very new to R. I was trying to load some publicly available Expression
> data in to R.
> I used the following commands
>  mydata<-read.table("dataALLAMLtrain.txt", header=TRUE, sep
> ="\t",row.names=NULL)
> It reads data without any error
> Now if I use
> edit(mydata)
> It shows only 3916 entries, whereas the actual file contains 7129 entries)
...
> So it seems R is truncating the data. How  can I load the complete file?

First isolate the source. edit() could have a bug, so what is
dim(mydata) ? Does the input file really have 7130 lines?
length(readLines("foobar.txt")) should tell you if you don't have "wc"
on your system.

-- 
   O__  ---- Peter Dalgaard             Blegdamsvej 3  
  c/ /'_ --- Dept. of Biostatistics     2200 Cph. N   
 (*) \(*) -- University of Copenhagen   Denmark      Ph: (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)             FAX: (+45) 35327907



From anne.piotet at urbanet.ch  Wed Aug 13 09:28:47 2003
From: anne.piotet at urbanet.ch (anne)
Date: Wed, 13 Aug 2003 09:28:47 +0200
Subject: [R] Books for R
Message-ID: <3F39E8AF.2030007@urbanet.ch>

As a newbie to R, I need to learn my way around (no previous experience 
of S).What books, doc for R are recommended? I'm interested primarly in 
non linear regression and process modelling (and have downloaded the R 
documentation from the site).
Thanks for all tips
Anne



From maechler at stat.math.ethz.ch  Wed Aug 13 09:46:25 2003
From: maechler at stat.math.ethz.ch (Martin Maechler)
Date: Wed, 13 Aug 2003 09:46:25 +0200
Subject: [R] big data file  geting truncated
In-Reply-To: <1131.164.100.21.192.1060758201.nicemail@localhost>
References: <1131.164.100.21.192.1060758201.nicemail@localhost>
Message-ID: <16185.60625.420947.476312@gargle.gargle.HOWL>

>>>>> "Dibakar" == Dibakar Ray <dibakar at hub.nic.in>
>>>>>     on Wed 13 Aug 2003 12:33:21 +0530 (IST) writes:

    Dibakar> I am very new to R. I was trying to load some
    Dibakar> publicly available Expression data in to R.

    Dibakar> I used the following commands
    Dibakar> mydata<-read.table("dataALLAMLtrain.txt", header=TRUE, sep
    Dibakar>                    ="\t",row.names=NULL)
    Dibakar> It reads data without any error

(really?, how do you know?  
 It seems you are trying to check this via the following ?
)
    Dibakar> Now if I use
    Dibakar> edit(mydata)
    Dibakar> It shows only 3916 entries, whereas the actual file
    Dibakar> contains 7129 entries). My data is something like

    Dibakar> Gene Description Gene Accession

    Dibakar> Number	1	2	3	4	5	6	7	8	9	10	11	12	13	14	15	16	17	18	19	20	21	22	23	24	25	26	27	34	35	36	37	38	28	29	30	31	32	33
    Dibakar> AFFX-BioB-5_at (endogenous
    Dibakar> control)	AFFX-BioB-5_at	-214	-139	-76	-135	-106	-138	-72	-413	5	-88	-165	-67	-92	-113	-107	-117	-476	-81	-44	17	-144	-247	-74	-120	-81	-112	-273	-20	7	-213	-25	-72	-4	15	-318	-32	-124	-135

(this probably has an extraneous  "wrap-around" in your post).

    Dibakar> So it seems R is truncating the data. How can I
    Dibakar> load the complete file?

edit() has been having problems with large files, however only
with more than 65535 rows.

HOWEVER, using edit() after read.table() to check your data is
not very recommended. 
Use 	dim(mydata)
	str(mydata)
and possibly also
	names(mydata)
	summary(mydata)
	
to check if the data frame was okay *before* you edited it,
using edit().

Martin Maechler <maechler at stat.math.ethz.ch>	http://stat.ethz.ch/~maechler/
Seminar fuer Statistik, ETH-Zentrum  LEO C16	Leonhardstr. 27
ETH (Federal Inst. Technology)	8092 Zurich	SWITZERLAND
phone: x-41-1-632-3408		fax: ...-1228			<><



From ririzarr at jhsph.edu  Wed Aug 13 09:49:14 2003
From: ririzarr at jhsph.edu (Rafael A. Irizarry)
Date: Wed, 13 Aug 2003 03:49:14 -0400 (EDT)
Subject: [R] big data file  geting truncated
In-Reply-To: <1131.164.100.21.192.1060758201.nicemail@localhost>
Message-ID: <Pine.GSO.4.10.10308130343050.25684-100000@athena.biostat.jhsph.edu>

without seeing the file its hard to tell but one possibility that comes to
mind is that there is a # character in your
file. read.table considers this a comment character.
use the argurment comment.char="" and see what happens...




On Wed, 13 Aug 2003, Dibakar Ray wrote:

> I am very new to R. I was trying to load some publicly available Expression
> data in to R.
> I used the following commands
>  mydata<-read.table("dataALLAMLtrain.txt", header=TRUE, sep
> ="\t",row.names=NULL)
> It reads data without any error
> Now if I use
> edit(mydata)
> It shows only 3916 entries, whereas the actual file contains 7129 entries)
> My data is something like
> Gene Description	Gene Accession
> Number	1	2	3	4	5	6	7	8	9	10	11	12	13	14	15	16	17	18	19	20	21	22	23	24	25	26	27	34	35	36	37	38	28	29	30	31	32	33
> AFFX-BioB-5_at (endogenous
> control)	AFFX-BioB-5_at	-214	-139	-76	-135	-106	-138	-72	-413	5	-88	-165	-67	-92	-113	-107	-117	-476	-81	-44	17	-144	-247	-74	-120	-81	-112	-273	-20	7	-213	-25	-72	-4	15	-318	-32	-124	-135
> So it seems R is truncating the data. How  can I load the complete file?
> Thanks in advance
> Dibakar
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
>



From kwan022 at stat.auckland.ac.nz  Wed Aug 13 09:57:27 2003
From: kwan022 at stat.auckland.ac.nz (Ko-Kang Kevin Wang)
Date: Wed, 13 Aug 2003 19:57:27 +1200 (NZST)
Subject: [R] Books for R
In-Reply-To: <3F39E8AF.2030007@urbanet.ch>
Message-ID: <Pine.LNX.4.44.0308131954480.5771-100000@stat55.stat.auckland.ac.nz>

On Wed, 13 Aug 2003, anne wrote:

> As a newbie to R, I need to learn my way around (no previous experience 
> of S).What books, doc for R are recommended? I'm interested primarly in 
> non linear regression and process modelling (and have downloaded the R 
> documentation from the site).

So you have checked the publications on R web site? 
http://www.r-project.org/doc/bib/R-publications.html  There is a list 
there.
  William N. Venables and Brian D. Ripley. Modern Applied Statistics with 
  S. Fourth Edition. Springer, 2002. ISBN 0-387-95457-0.
in particular, is a very good book for beginners.  It has a chapter on 
non-linear regression and smooth regression, using packages like nls.

-- 
Cheers,

Kevin

------------------------------------------------------------------------------
"On two occasions, I have been asked [by members of Parliament],
'Pray, Mr. Babbage, if you put into the machine wrong figures, will
the right answers come out?' I am not able to rightly apprehend the
kind of confusion of ideas that could provoke such a question."

-- Charles Babbage (1791-1871) 
---- From Computer Stupidities: http://rinkworks.com/stupid/

--
Ko-Kang Kevin Wang
Master of Science (MSc) Student
SLC Tutor and Lab Demonstrator
Department of Statistics
University of Auckland
New Zealand
Homepage: http://www.stat.auckland.ac.nz/~kwan022
Ph: 373-7599
    x88475 (City)
    x88480 (Tamaki)



From ripley at stats.ox.ac.uk  Wed Aug 13 09:57:45 2003
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Wed, 13 Aug 2003 08:57:45 +0100 (BST)
Subject: [R] Books for R
In-Reply-To: <3F39E8AF.2030007@urbanet.ch>
Message-ID: <Pine.LNX.4.44.0308130856060.5482-100000@gannet.stats>

This is covered in the FAQ.

The most comprehensive accounts of non-linear regression I know of
are in Chambers & Hastie (1992, for S) and Venables & Ripley (2002 and 
earlier editions, for R or S).

On Wed, 13 Aug 2003, anne wrote:

> As a newbie to R, I need to learn my way around (no previous experience 
> of S).What books, doc for R are recommended? I'm interested primarly in 
> non linear regression and process modelling (and have downloaded the R 
> documentation from the site).

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From maechler at stat.math.ethz.ch  Wed Aug 13 10:08:35 2003
From: maechler at stat.math.ethz.ch (Martin Maechler)
Date: Wed, 13 Aug 2003 10:08:35 +0200
Subject: [R] Replacing underscore character in Windows GUI
In-Reply-To: <858ypyfe6g.fsf@blindglobe.net>
References: <3F396BC3.5288.13F560F@localhost> <858ypyfe6g.fsf@blindglobe.net>
Message-ID: <16185.61955.224913.682208@gargle.gargle.HOWL>

This is becoming a relevant ESS topic, hence I divert it to the
ESS-help mailing list, please continue there
{archives at https://www.stat.math.ethz.ch/pipermail/ess-help/ )

>>>>> "tony" == A J Rossini <rossini at blindglobe.net>
>>>>>     on Tue, 12 Aug 2003 20:07:35 -0700 writes:

    tony> "kjetil brinchmann halvorsen" <kjetil at entelnet.bo> writes:
    >> On 12 Aug 2003 at 17:38, nels.tomlinson.1 wrote:
    >> 
    >> I thought one of the reasons for removing the underscore as synonym 
    >> to <- was that at some later point it could be reintroduced in the 
    >> grammar, for example as a valid part of variable names. 
    >> 
    >> If this is the case, this suggestion is a bad idea. Better to forget 
    >> the habvit of using _ for assignment! Maybe this reemplacement of _ 
    >> with <- will be removed from ess too?

    tony> Probably.  But there are those of us who don't like "_" for variable
    tony> names.  It reminds me too much of C and Python (though I like the
    tony> latter).   

    tony> I'd be happier removing "=" for assignment, as well.  It's a
    tony> monstrosity, though supposedly wiser heads think otherwise.  I know
    tony> the arguments, but I still think it should be removed.

    tony> Maybe we'll make "=" electric -- one press becomes "<-", two presses
    tony> become "==".  Emacs is wonderful, sometimes.

This is an interesting proposal.

===> Continued on the ESS-help mailing list,
     ESS-help at stat.math.ethz.ch
     https://www.stat.math.ethz.ch/mailman/listinfo/ess-help

Martin Maechler <maechler at stat.math.ethz.ch>	http://stat.ethz.ch/~maechler/



From p.pagel at gsf.de  Wed Aug 13 10:33:10 2003
From: p.pagel at gsf.de (Philipp Pagel)
Date: Wed, 13 Aug 2003 10:33:10 +0200
Subject: [R] big data file  geting truncated
In-Reply-To: <1131.164.100.21.192.1060758201.nicemail@localhost>
References: <1131.164.100.21.192.1060758201.nicemail@localhost>
Message-ID: <20030813083309.GA5549@porcupine.gsf.de>

	Hi!

> I used the following commands
>  mydata<-read.table("dataALLAMLtrain.txt", header=TRUE, sep
> ="\t",row.names=NULL)
> It reads data without any error
> Now if I use
> edit(mydata)
> It shows only 3916 entries, whereas the actual file contains 7129 entries)
[...]
> So it seems R is truncating the data. How  can I load the complete file?

Others have already recommended checking the length of the data.frame
using dim() and the file using wc. If it turns out that there really is
a difference in size the next thing would be to get an idea what lines
are affected: Are "random" lines missing or is everything ok up to line
3916 and then it stops? In either case - have a close look at the lines
missing or the last line present plus the first one missing: Is there
anything special about them?

But actually I have a feeling that this may be your problem:

read.table uses both '"' and "'" for quoting by default. Gene
descriptions love to contain things like "5'" and "3'".
=> Try quote='' in the read.table call.

cu
	Philipp

-- 
Dr. Philipp Pagel                                Tel.  +49-89-3187-3675
Institute for Bioinformatics / MIPS              Fax.  +49-89-3187-3585
GSF - National Research Center for Environment and Health
Ingolstaedter Landstrasse 1
85764 Neuherberg, Germany



From oehl_list at gmx.de  Wed Aug 13 11:01:05 2003
From: oehl_list at gmx.de (oehl_list@gmx.de)
Date: Wed, 13 Aug 2003 11:01:05 +0200 (MEST)
Subject: [R] placing labels in polygon center ?
Message-ID: <9130.1060765265@www54.gmx.net>


Dear all,

is there any function to calculate the center of a polygon mass in R?
Actually I need to find the best location within polygons to place labels.

Thanks for any hint


Jens Oehlschl?gel

-- 
COMPUTERBILD 15/03: Premium-e-mail-Dienste im Test\ --------...{{dropped}}



From th50 at leicester.ac.uk  Wed Aug 13 11:29:43 2003
From: th50 at leicester.ac.uk (Hotz, T.)
Date: Wed, 13 Aug 2003 10:29:43 +0100
Subject: [R] placing labels in polygon center ?
Message-ID: <1F2CE8D4B0195E488213E8B8CCF7148602501316@saffron.cfs.le.ac.uk>

Dear Jens,


> -----Original Message-----
> From: oehl_list at gmx.de [mailto:oehl_list at gmx.de]
> Sent: 13 August 2003 10:01
> To: r-help at stat.math.ethz.ch
> Subject: [R] placing labels in polygon center ?
> 
> Dear all,
> 
> is there any function to calculate the center of a polygon mass in R?
> Actually I need to find the best location within polygons to 
> place labels.

Trying to recall from my physics education, the mass centre is at the
averages of the coordinates of the corners, isn't it?

However, the task is more complex: your polygon might be X-shaped...

HTH

Thomas


---

Thomas Hotz
Research Associate in Medical Statistics
University of Leicester
United Kingdom

Department of Epidemiology and Public Health
22-28 Princess Road West
Leicester
LE1 6TP
Tel +44 116 252-5410
Fax +44 116 252-5423

Division of Medicine for the Elderly
Department of Medicine
The Glenfield Hospital
Leicester
LE3 9QP
Tel +44 116 256-3643
Fax +44 116 232-2976



From christian.ritter at shell.com  Wed Aug 13 11:49:03 2003
From: christian.ritter at shell.com (Ritter, Christian C MCIL-CTGAS)
Date: Wed, 13 Aug 2003 11:49:03 +0200
Subject: [R] The desire for prettier and cheaper images ...
Message-ID: <156CDC8CCFD1894295D2907F16337A481FA909@bru-s-006.europe.shell.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20030813/deea5e18/attachment.pl

From alessandro.semeria at cramont.it  Wed Aug 13 12:03:26 2003
From: alessandro.semeria at cramont.it (alessandro.semeria@cramont.it)
Date: Wed, 13 Aug 2003 12:03:26 +0200
Subject: [R] Books for R
Message-ID: <OF554EDFA6.58ED7CE2-ONC1256D81.00367BE6-C1256D81.0035F3A0@tomware.it>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20030813/d2f049ba/attachment.pl

From Saghir.Bashir at UCB-Group.com  Wed Aug 13 12:01:45 2003
From: Saghir.Bashir at UCB-Group.com (Bashir Saghir (Aztek Global))
Date: Wed, 13 Aug 2003 12:01:45 +0200
Subject: [R] capturing output from Win 98 shell
Message-ID: <3EBA5559F490D61189430002A5F0AE8903BC87F2@ntexcrd.braine.ucb>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20030813/fad64fa8/attachment.pl

From maechler at stat.math.ethz.ch  Wed Aug 13 12:57:11 2003
From: maechler at stat.math.ethz.ch (Martin Maechler)
Date: Wed, 13 Aug 2003 12:57:11 +0200
Subject: [R] The desire for prettier and cheaper images ...
In-Reply-To: <156CDC8CCFD1894295D2907F16337A481FA909@bru-s-006.europe.shell.com>
References: <156CDC8CCFD1894295D2907F16337A481FA909@bru-s-006.europe.shell.com>
Message-ID: <16186.6535.417944.539655@gargle.gargle.HOWL>

>>>>> "ChRi" == Ritter, Christian C MCIL-CTGAS <christian.ritter at shell.com>
>>>>>     on Wed, 13 Aug 2003 11:49:03 +0200 writes:

    ChRi> This might be easy (but I didn't find an answer in the archives).
    ChRi> I'm trying to make nice looking images (using image()). 

    ChRi> To make them look nice (not jagged), it usually takes
    ChRi> me at least 100x100 points. This can be slow for
    ChRi> frequent redraws.

and also for (postscript) printing.
For true image data, it would be nice to have "true pixel-map"
graphics as part of the basic graphic engine, but I'm not really
competent here


    ChRi> Is there a smarter (less point intensive) way? 

    ChRi> I was thinking in terms of hexabins, for example,
    ChRi> which for much less jagged looking regions, or of an
    ChRi> option to image ( or a variation) which internally
    ChRi> increases the grid.

    ChRi> Any help welcome. Thanks in advance.

There's the "hexbin" package from Bioconductor for hexagonal bin
plots -- which really does help for some of the problems you mention.

Martin Maechler <maechler at stat.math.ethz.ch>	http://stat.ethz.ch/~maechler/
Seminar fuer Statistik, ETH-Zentrum  LEO C16	Leonhardstr. 27
ETH (Federal Inst. Technology)	8092 Zurich	SWITZERLAND
phone: x-41-1-632-3408		fax: ...-1228			<><



From gisar at nus.edu.sg  Wed Aug 13 13:07:39 2003
From: gisar at nus.edu.sg (Adaikalavan Ramasamy)
Date: Wed, 13 Aug 2003 19:07:39 +0800
Subject: [R] capturing output from Win 98 shell
Message-ID: <CDA8D2689259E444942B3CDED8DD912933FF20@MBXSRV03.stf.nus.edu.sg>

Why should it take months to get these installed ?

Both R and emacs are simple and free to install. If high speed internet access is a problem, you can always download these from some Internet Caf? or a friend, burn it into CD and install onto the machines.

If you have red tapes on installing softwares, then good luck. I suppose they would not be so happy purchasing SPLUS than installing free softwares for you.


-----Original Message-----
From: Bashir Saghir (Aztek Global) [mailto:Saghir.Bashir at UCB-Group.com] 
Sent: Wednesday, August 13, 2003 6:02 PM
To: 'Simon Fear'; Prof Brian Ripley; Uwe Ligges
Cc: r-help at stat.math.ethz.ch
Subject: RE: [R] capturing output from Win 98 shell



>Believe me, using Win98 is NOT my choice. I would only ever use 
>Unix/Linux. Are there other statisticians similarly limited?

Like Simon I am very often limited with choice of software and OS when working for clients. I've been waiting for about 5 months to get an editor installed on my machine. I have requested emacs but will settle for anything but the current choice - MS Notepad (I'm serious). 

The first time we requested an R installation we waited about 3 or 4 months (thanks to constant pressure from our statistical guru). To get the latest version of R we had another 3 or 4 months wait! Then when we finally got it (version 1.6.2) then next version had been released a week earlier (1.7.0 - I think)! Very frustrating!

As you can imagine every time I read "Please upgrade to version 1.7.x" - I just wish it was so simple! 

Best regards,
Saghir





--------------------------------------------------------- 
Legal Notice: This electronic mail and its attachments are i...{{dropped}}

______________________________________________
R-help at stat.math.ethz.ch mailing list https://www.stat.math.ethz.ch/mailman/listinfo/r-help



From lisas at salford-systems.com  Wed Aug 13 13:10:25 2003
From: lisas at salford-systems.com (Lisa Solomon)
Date: Wed, 13 Aug 2003 04:10:25 -0700
Subject: [R] please advise re: data mining in Germany
Message-ID: <3F3A1CA1.304@salford-systems.com>

     Our CEO, Dr. Dan Steinberg, is planning to visit Germany in September.
He would like the opportunity to introduce statisticians (and 
statistically minded
people) to data mining, data mining applications and to forefront data
mining tools.  Our algorithms are probably familiar to many
statisticians (CART, MARS and MART), although it isn't necessary to be a
statistician to use our tools.  We co-develop with Jerome Friedman of
Stanford University and Leo Breiman of Berkeley.
     I am trying to locate people/companies in Germany who would
benefit from data mining.  If you know of any, I would appreciate it if
you would let me know.
     Below is information about Salford Systems, Dr. Steinberg and the
abstract of a recent presentation.

Sincerely,
Lisa Solomon
lisas at salford-systems.com <mailto:lisas at salford-systems.com>
http://www.salford-systems.com?germany
001-619-543-8880 x21

Salford Systems Background:
Founded in 1983, Salford Systems specializes in providing new generation
data mining and choice modeling software and consultation services.
Applications in both software and consulting span market research
segmentation, direct marketing, fraud detection, credit scoring, risk
management, bio-medical research and manufacturing quality control.
Industries using Salford Systems products and consultation services
include telecommunications, transportation, banking, financial services,
insurance, health care, manufacturing, retail and catalog sales, and
education.  Salford Systems software is installed at more than 3,500
sites worldwide, including 300 major Universities.  Key customers
include AT&T Universal Card Services, Pfizer Pharmaceuticals, General
Motors, Sears, and Roebuck and Co.

Dan Steinberg Background:
Dan Steinberg is a well respected member of the data mining, statistics
and analytical consultation communities.  His more than 20 years of
experience in the field include Member of Technical Staff at AT&T Bell
Laboratories,  and Assistant Professor of Econometrics at the University
of California, San Diego, as well as numerous consultation engagements
with Fortune 100 clients.  He received his Ph.D. in Economics from
Harvard University, and he has received honors from the SAS User's Group
International.  Steinberg has published articles in statistics,
econometrics, computer science, and marketing journals, and he is the
developer of a series of advanced statistical analysis programs.  In
addition, he has been a featured data mining issues speaker for the
American Marketing Association, American Statistical Association and the
Direct Marketing Association

Recent Presentation to San Antonio Chapter American Statistical Society:
Data Mining in Practice:
Banking, Insurance, Bioinformatics


Abstract

Part I:  Data mining is the application of modern, highly automated
nonparametric analytical methods to recognize enduring patterns in data.
  The methods can produce a variety of models for a variety of
industries. Models include classification schemes, regressions, and
finding clusters of objects and attributes.  This review will focus on
classification problems, including, recognizing customers who are most
at risk of switching to a competitor, recognizing loan applicants most
likely to default, and recognizing unsafe product designs. Most data
mining problems are fundamentally minor variations on just a few key
themes and we will use the examples to illustrate this point.

Part II:  Data mining methods have been experimented with for some time
in the bioinformatics world, originally being applied to the problems of
drug discovery, selection of patients into clinical trials, and
epidemiological studies. In the last year or so the number of
researchers using data mining methods has expanded considerably, and the
range of methods has expanded as well.  In particular, CART has been
used extensively in the analysis of proteomics and genomics data, and
Treenet stochastic gradient boosting has been quite successful in the
analysis of DNA microarray data.



From B.Rowlingson at lancaster.ac.uk  Wed Aug 13 13:32:18 2003
From: B.Rowlingson at lancaster.ac.uk (Barry Rowlingson)
Date: Wed, 13 Aug 2003 12:32:18 +0100
Subject: [R] placing labels in polygon center ?
In-Reply-To: <1F2CE8D4B0195E488213E8B8CCF7148602501316@saffron.cfs.le.ac.uk>
References: <1F2CE8D4B0195E488213E8B8CCF7148602501316@saffron.cfs.le.ac.uk>
Message-ID: <3F3A21C2.1010307@lancaster.ac.uk>


> Trying to recall from my physics education, the mass centre is at the
> averages of the coordinates of the corners, isn't it?

  Perhaps for simple polygons it is... One thing it certainly isnt is 
the average of all the coordinates (a mistake I have seen geographers 
make!).

  Here's some code derived from the java source code here: 
http://astronomy.swin.edu.au/~pbourke/geometry/polyarea/

  You also get a PolygonArea function for free. Feed it a two-column 
matrix of X and Y coords. Out comes a vector of (x,y) for the centroid.

  PolygonArea <- function(polygon)
{
   N <- dim(polygon)[1]
	area = 0;

	for (i in 1:N) {
		j = 1+(i %% N)
		area = area + polygon[i,1] * polygon[j,2]
		area = area - polygon[i,2] * polygon[j,1]
	}

	area =area/2.0;
	return(abs(area))
}


PolygonCenterOfMass <- function(polygon)
{
   N <- dim(polygon)[1]
   cx=0 ; cy=0
   A=PolygonArea(polygon);

   factor=0;
   for (i in 1:N) {
     j = (i  %% N)+1
     factor=(polygon[i,1]*polygon[j,2]-polygon[j,1]*polygon[i,2])
     cx=cx+(polygon[i,1]+polygon[j,1])*factor
     cy=cy+(polygon[i,2]+polygon[j,2])*factor
   }
   A=A*6.0
   factor=1/A
   cx=cx*factor
   cy=cy*factor
   return(c(cx,cy))
}

  Something like this will end up in my Rmap library......

Baz



From Simon.Fear at synequanon.com  Wed Aug 13 13:32:38 2003
From: Simon.Fear at synequanon.com (Simon Fear)
Date: Wed, 13 Aug 2003 12:32:38 +0100
Subject: [R] capturing output from Win 98 shell
Message-ID: <6C8A8033ABC1E3468048ABC4F13CE572AC0D5B@synequanon01>

Actually, if you read your university's rules about use of IT
facilities, I
am willing to bet they expressly forbid the (down)loading and use of
software
without permission. This was certainly in the small print in all the UK
universities where I had a .ac.uk account. There were some relaxations
for
staff.

The difference is that within academia this rule is universally ignored,
whereas in the .com world it would cost you your job (and your company a
potential audit failure).

Anyway, can I suggest we drop this thread from the list now as having
drifted
too far off-topic? (except to forestall questions like "why are you
still
using version 0.9 - please update", which should be "please update - oh
sorry, now I see your .com address - my sympathies, please try again
using
Windows 3.1").


> -----Original Message-----
> From: Adaikalavan Ramasamy [mailto:gisar at nus.edu.sg]
> 
> Why should it take months to get these installed ?
>
 

Simon Fear
Senior Statistician
Syne qua non Ltd
Tel: +44 (0) 1379 644449
Fax: +44 (0) 1379 644445
email: Simon.Fear at synequanon.com
web: http://www.synequanon.com
 
Number of attachments included with this message: 0
 
This message (and any associated files) is confidential and\...{{dropped}}



From dmurdoch at pair.com  Wed Aug 13 13:38:59 2003
From: dmurdoch at pair.com (Duncan Murdoch)
Date: Wed, 13 Aug 2003 07:38:59 -0400
Subject: [R] Replacing underscore character in Windows GUI
In-Reply-To: <858ypyfe6g.fsf@blindglobe.net>
References: <3F396BC3.5288.13F560F@localhost> <858ypyfe6g.fsf@blindglobe.net>
Message-ID: <pm8kjvgv6hau86moot4at92ku3p7ns14f8@4ax.com>

On Tue, 12 Aug 2003 20:07:35 -0700, you wrote:

>Maybe we'll make "=" electric -- one press becomes "<-", two presses
>become "==".  Emacs is wonderful, sometimes.

Just watch out for named parameters:

 plot(1:10,1:10, type <- 'l')

probably isn't what you want!

Duncan



From angel_lul at hotmail.com  Wed Aug 13 13:58:03 2003
From: angel_lul at hotmail.com (Angel)
Date: Wed, 13 Aug 2003 13:58:03 +0200
Subject: [R] putting NAs at the end
Message-ID: <Law11-OE71XE2sCRBHZ0001f54c@hotmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20030813/a7064613/attachment.pl

From Harvey.Monder at pharma.com  Wed Aug 13 14:27:05 2003
From: Harvey.Monder at pharma.com (Monder, Harvey)
Date: Wed, 13 Aug 2003 08:27:05 -0400
Subject: [R] converting GLM from SAS to R
Message-ID: <274EB594CE46DA489B915C6DDC3E67A3018D535F@moo01.us01.apmn.org>

Dear List members,

	I am in the process of translating a number of SAS programs into R scripts.  The translation is surprisingly straight-forward.   However, I want to convert some PROC GLM code to glm in R.  Unfortunately my understanding of the procedure has become rust through the years.  The SAS code I want to translate is:

   proc glm data=combined(where=(auc not in(0 .) and trial=&i));
     class id occ seq;
     model tmax cmax auc lauc lcmax ratio=
     seq id occ id(seq) ;
     test h = seq e = id(seq)/htype=3 etype=3;
     estimate '0 vs 1' occ 1-1; * 1 trt& 2 ref;
     lsmeans occ;
   run;

Any guidance would be greatly appreciated.

	Harvey



From HADASSA.BRUNSCHWIG at Roche.COM  Wed Aug 13 14:43:48 2003
From: HADASSA.BRUNSCHWIG at Roche.COM (Brunschwig, Hadassa {PDMM~Basel})
Date: Wed, 13 Aug 2003 14:43:48 +0200
Subject: [R] adding to Trellis plots
Message-ID: <BC6A439CD6835749A9C7B8D8F041DFA88B11AC@rbamsem1.emea.roche.com>

Hello everyone

I have data grouped by subjects and i used the trellis plot to show the curve for each subject. I have another vector "a" which gives me the highest points the curves for each subjects can achieve ( the curve is exponential ). Now i want to draw a horizontal line, whose values are saved in "a", on each plot for each subject. Is there any possibility to do that? Obviously i should use something like panel.abline, the problem is how i can inform R that each value in "a" is a horizontal line for each plot.

Thanks a lot for answers.

Dassy



From dassybr at hotmail.com  Wed Aug 13 14:54:40 2003
From: dassybr at hotmail.com (Hadassa Brunschwig)
Date: Wed, 13 Aug 2003 14:54:40 +0200
Subject: [R] adding to Trellis plot
Message-ID: <BAY9-F17wv8ztxsx12V00034e22@hotmail.com>

Hello everyone
(Im not sure if i already posted this problem)
I have grouped data and i used a Trellis plot to show the curve of a fitted 
model for each group. Additionally i have a vector whose values are the 
highest points the curve can reach (the curve rises exponetially). Each 
value of the vector is the corresponding highest point for each group/plot. 
Now i want to show this highest point by adding to each plot a horizontal 
line. Obviously i should use panel.abline or something equal. How can i tell 
R that each value of this vector corresponds to the different plots in the 
Trellis plot?

Thanks a lot for answers

Dassy



From dave at evocapital.com  Wed Aug 13 15:00:49 2003
From: dave at evocapital.com (David Khabie-Zeitoune)
Date: Wed, 13 Aug 2003 14:00:49 +0100
Subject: [R] Mapping function dependencies
Message-ID: <8D0F30FE2EB3314182D4A33F738BB19D063EE3@mail.internal.net>

Hi

Does anyone know of any automatic way of mapping out the dependencies of
a function -- i.e. automatically listing which other functions that
function calls? I tried using all.names or all.vars, but I can't get it
to work on a function: e.g.

> test = function(x, y) return(sin(x + y)*cos(x + y))
> all.names(expression(test))
 [1] "test"

I guess the problem is that all.names is not being applied to the body
of the function, whereas if I write the function definition out
explicitly in the expression, I can readily get what I want:

> tmp = all.names(expression(function(x, y) return(sin(x + y)*cos(x +
y))))
> tmp
 [1] "function" "return"   "*"        "sin"      "+"        "x"       
 [7] "y"        "cos"      "+"        "x"        "y"       

and further process this to get the functional dependencies

> pos.fn = sapply(tmp,function(.x) if (exists(.x)) is.function(get(.x))
else FALSE)
> tmp[pos.fn]
 [1] "function" "return"   "*"        "sin"      "+"        "cos"
"+" 

However, this relies on writing out the function definition explicitly
out in the expression which I cannot do as I wish to map the
dependencies of large complex functions.

Thanks for your help.

David



From Ted.Harding at nessie.mcc.ac.uk  Wed Aug 13 14:55:25 2003
From: Ted.Harding at nessie.mcc.ac.uk ( (Ted Harding))
Date: Wed, 13 Aug 2003 13:55:25 +0100 (BST)
Subject: [R] Integer precision etc.
Message-ID: <XFMail.030813135525.Ted.Harding@nessie.mcc.ac.uk>

Hi Folks,
With a bit of experimentation I have determined (I think)
that on my R implementation the largest positive integer
that is exactly represented is (2^53 - 1), based on

> (((2^53)-1)+1) - ((2^53)-1)
[1] 1
> ((2^53)+1) - (2^53)
[1] 0

System:
platform i686-pc-linux-gnu
arch     i686
os       linux-gnu
system   i686, linux-gnu
status
major    1
minor    6.1
year     2002
month    11
day      01
language R

Is there any other way to determine this sort of information?

With thanks,
Ted.


--------------------------------------------------------------------
E-Mail: (Ted Harding) <Ted.Harding at nessie.mcc.ac.uk>
Fax-to-email: +44 (0)870 167 1972
Date: 13-Aug-03                                       Time: 13:55:25
------------------------------ XFMail ------------------------------



From dave at evocapital.com  Wed Aug 13 15:11:00 2003
From: dave at evocapital.com (David Khabie-Zeitoune)
Date: Wed, 13 Aug 2003 14:11:00 +0100
Subject: [R] Mapping function dependencies
Message-ID: <8D0F30FE2EB3314182D4A33F738BB19D063EE4@mail.internal.net>

With regards to my question below, I've found a way to do it:

all.names(body(test))

Seems to do the trick. Still, if there are any tools already developed
for mapping the dependencies between a set of functions I be very
interested to hear before I reinvent the wheel.

Thanks,
Dave


-----Original Message-----
From: David Khabie-Zeitoune 
Sent: 13 August 2003 14:01
To: r-help at stat.math.ethz.ch
Subject: [R] Mapping function dependencies


Hi

Does anyone know of any automatic way of mapping out the dependencies of
a function -- i.e. automatically listing which other functions that
function calls? I tried using all.names or all.vars, but I can't get it
to work on a function: e.g.

> test = function(x, y) return(sin(x + y)*cos(x + y))
> all.names(expression(test))
 [1] "test"

I guess the problem is that all.names is not being applied to the body
of the function, whereas if I write the function definition out
explicitly in the expression, I can readily get what I want:

> tmp = all.names(expression(function(x, y) return(sin(x + y)*cos(x +
y))))
> tmp
 [1] "function" "return"   "*"        "sin"      "+"        "x"       
 [7] "y"        "cos"      "+"        "x"        "y"       

and further process this to get the functional dependencies

> pos.fn = sapply(tmp,function(.x) if (exists(.x)) is.function(get(.x))
else FALSE)
> tmp[pos.fn]
 [1] "function" "return"   "*"        "sin"      "+"        "cos"
"+" 

However, this relies on writing out the function definition explicitly
out in the expression which I cannot do as I wish to map the
dependencies of large complex functions.

Thanks for your help.

David

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://www.stat.math.ethz.ch/mailman/listinfo/r-help



From MSchwartz at medanalytics.com  Wed Aug 13 15:20:32 2003
From: MSchwartz at medanalytics.com (Marc Schwartz)
Date: Wed, 13 Aug 2003 08:20:32 -0500
Subject: [R] Integer precision etc.
In-Reply-To: <XFMail.030813135525.Ted.Harding@nessie.mcc.ac.uk>
References: <XFMail.030813135525.Ted.Harding@nessie.mcc.ac.uk>
Message-ID: <1060780832.13211.268.camel@localhost>

On Wed, 2003-08-13 at 07:55, Ted.Harding at nessie.mcc.ac.uk wrote:
> Hi Folks,
> With a bit of experimentation I have determined (I think)
> that on my R implementation the largest positive integer
> that is exactly represented is (2^53 - 1), based on
> 
> > (((2^53)-1)+1) - ((2^53)-1)
> [1] 1
> > ((2^53)+1) - (2^53)
> [1] 0
> 
> System:
> platform i686-pc-linux-gnu
> arch     i686
> os       linux-gnu
> system   i686, linux-gnu
> status
> major    1
> minor    6.1
> year     2002
> month    11
> day      01
> language R
> 
> Is there any other way to determine this sort of information?
> 
> With thanks,
> Ted.


Ted,

See ?.Machine

No experimentation required  :-)

HTH,

Marc Schwartz



From anna at ptolemy.arc.nasa.gov  Wed Aug 13 15:27:33 2003
From: anna at ptolemy.arc.nasa.gov (Anna  H. Pryor)
Date: Wed, 13 Aug 2003 06:27:33 -0700
Subject: [R] anova and tukeyHSD
Message-ID: <200308130627.33112.anna@ptolemy.arc.nasa.gov>


I would like to do a one way anova and then a tukeyHSD.  I have three vectors 
A,B and C.  In a previous help message, I was told to do the following for 
the anova:

y = c(A,B,C)
group = factor(rep(a:3,c(7,9,13))) #provided there a 7 elements in A,9 in B 
and 13 in C

and then

anova(lm(y~group))


Looking at the tukeyHSD method it looks like it wants the aov method which I 
don't understand.  Using the above example, could someone continue the 
example and get the tukeyHSD method to work?

Anna



From spencer.graves at pdf.com  Wed Aug 13 15:41:12 2003
From: spencer.graves at pdf.com (Spencer Graves)
Date: Wed, 13 Aug 2003 06:41:12 -0700
Subject: [R] putting NAs at the end
References: <Law11-OE71XE2sCRBHZ0001f54c@hotmail.com>
Message-ID: <3F3A3FF8.6040509@pdf.com>

Have you considered "?order"?

Spencer Graves

Angel wrote:
> I have a matrix for which each row has 12 elements that represent the xyz coordinates of 4 points.
> So each row of M is (x1,y1,z1,x2,y2,z2,x3,y3,z3,x4,y4,z4). Some points have NA as z values. 
> I want another matrix to be the same as M but with the coordinates of those points with z=NA placed last. 
> For ezample if z1=NA then the new matrix row should be  (x2,y2,z2,x3,y3,z3,x4,y4,z4,x1,y1,z1)
> I've tried writing a function that does the job for each row and then apply to the matrix
> 
> Put.NaN.last<-function(p) {
> Index<-c(which(!is.na(p[c(3,6,9,12)]))*3,which(is.na(p[c(3,6,9,12)]))*3)
> p<-c(p[Index[1]-2],p[Index[1]-1],p[Index[1]],
> p[Index[2]-2],p[Index[2]-1],p[Index[2]],
> p[Index[3]-2],p[Index[3]-1],p[Index[3]],
> p[Index[4]-2],p[Index[4]-1],p[Index[4]])
> return(p)
> }
> A<-matrix(1:36,ncol=12)
> A[c(7,9,17,36)]<-NA
> A<-t(apply(A,1,Put.NaN.last))
> 
> but it is awfully slow.
> Any suggestions on how to do this faster?
> Thanks
> Angel
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help



From Ted.Harding at nessie.mcc.ac.uk  Wed Aug 13 15:48:48 2003
From: Ted.Harding at nessie.mcc.ac.uk ( (Ted Harding))
Date: Wed, 13 Aug 2003 14:48:48 +0100 (BST)
Subject: [R] Integer precision etc.
In-Reply-To: <XFMail.030813135525.Ted.Harding@nessie.mcc.ac.uk>
Message-ID: <XFMail.030813144848.Ted.Harding@nessie.mcc.ac.uk>

Thanks to James Holtman for the confirmation of the IEEE definition,
and to Marc Schwartz and Roger Koenker for pointing out ".Machine"
which I had not been aware of!

For the latter, the information I wanted is in

> .Machine$double.digits
[1] 53

so that the largest integer exactly represented is indeed 2^53 -1.

Best wishes to all,
Ted.


--------------------------------------------------------------------
E-Mail: (Ted Harding) <Ted.Harding at nessie.mcc.ac.uk>
Fax-to-email: +44 (0)870 167 1972
Date: 13-Aug-03                                       Time: 14:48:48
------------------------------ XFMail ------------------------------



From Simon.Fear at synequanon.com  Wed Aug 13 16:01:35 2003
From: Simon.Fear at synequanon.com (Simon Fear)
Date: Wed, 13 Aug 2003 15:01:35 +0100
Subject: [R] Integer precision etc.
Message-ID: <6C8A8033ABC1E3468048ABC4F13CE572AC0D5D@synequanon01>

1e100 is just one example of much bigger number that is exactly
represented
(in floating point). But of course
> 1e100+1 - 1e100
[1] 0

You mean the biggest number such that adding one changes the result?

I should be extremely careful with

> print( 9007199254740994, digits=20)
[1] 9007199254740994
> print( 9007199254740994-1, digits=20)
[1] 9007199254740992

Here subtracting one makes a difference of two ... but the numbers look
like
integers.
 

Simon Fear
Senior Statistician
Syne qua non Ltd
Tel: +44 (0) 1379 644449
Fax: +44 (0) 1379 644445
email: Simon.Fear at synequanon.com
web: http://www.synequanon.com
 
Number of attachments included with this message: 0
 
This message (and any associated files) is confidential and\...{{dropped}}



From tlumley at u.washington.edu  Wed Aug 13 16:09:50 2003
From: tlumley at u.washington.edu (Thomas Lumley)
Date: Wed, 13 Aug 2003 07:09:50 -0700 (PDT)
Subject: [R] Integer precision etc.
In-Reply-To: <XFMail.030813135525.Ted.Harding@nessie.mcc.ac.uk>
Message-ID: <Pine.A41.4.44.0308130705440.158690-100000@homer06.u.washington.edu>

On Wed, 13 Aug 2003 Ted.Harding at nessie.mcc.ac.uk wrote:

> Hi Folks,
> With a bit of experimentation I have determined (I think)
> that on my R implementation the largest positive integer
> that is exactly represented is (2^53 - 1), based on
>
> > (((2^53)-1)+1) - ((2^53)-1)
> [1] 1
> > ((2^53)+1) - (2^53)
> [1] 0
>

The variable .Machine contains this sort of thing.

The largest exactly represented integer is
   .Machine$integer.max
and the largest integer exactly represented as a double will be
 2^.Machine$double.digits-1

I believe the floating point formats are the same on all versions of R at
the moment (though the integer formats might not be)

	-thomas



From f0z6305 at labs.tamu.edu  Wed Aug 13 16:16:09 2003
From: f0z6305 at labs.tamu.edu (Feng Zhang)
Date: Wed, 13 Aug 2003 09:16:09 -0500
Subject: [R] A question on orthogonal basis vectors
Message-ID: <003c01c361a5$7188e180$8bd75ba5@IE.TAMU.EDU>

Hey, R-listers,

I have a question about determining the orthogonal 
basis vectors.
In the d-dimensinonal space, if I already know
the first r orthogonal basis vectors, should I be
able to determine the remaining d-r orthognal basis
vectors automatically?

Or the answer is not unique?

Thanks for your attention.

Fred



From tlumley at u.washington.edu  Wed Aug 13 16:16:43 2003
From: tlumley at u.washington.edu (Thomas Lumley)
Date: Wed, 13 Aug 2003 07:16:43 -0700 (PDT)
Subject: [R] Mapping function dependencies
In-Reply-To: <8D0F30FE2EB3314182D4A33F738BB19D063EE4@mail.internal.net>
Message-ID: <Pine.A41.4.44.0308130715210.158690-100000@homer06.u.washington.edu>

On Wed, 13 Aug 2003, David Khabie-Zeitoune wrote:

> With regards to my question below, I've found a way to do it:
>
> all.names(body(test))
>
This also gives all the variable names, which you may not want.

This question came up a few months ago:
  http://finzi.psych.upenn.edu/R/Rhelp02/archive/18497.html


	-thomas



From bates at stat.wisc.edu  Wed Aug 13 16:20:30 2003
From: bates at stat.wisc.edu (Douglas Bates)
Date: Wed, 13 Aug 2003 14:20:30 -0000
Subject: [R] Integer precision etc.
In-Reply-To: <XFMail.030813135525.Ted.Harding@nessie.mcc.ac.uk>
References: <XFMail.030813135525.Ted.Harding@nessie.mcc.ac.uk>
Message-ID: <6rznid63lj.fsf@bates4.stat.wisc.edu>

(Ted Harding) <Ted.Harding at nessie.mcc.ac.uk> writes:

> With a bit of experimentation I have determined (I think)
> that on my R implementation the largest positive integer
> that is exactly represented is (2^53 - 1), based on
> 
> > (((2^53)-1)+1) - ((2^53)-1)
> [1] 1
> > ((2^53)+1) - (2^53)
> [1] 0

Those "integer" values are being silently converted to double
precision so what you are determining is the relative machine
precision for doubles.  Use .Machine$integer.max instead.  On your
platform it will probably be 2^31-1

> .Machine$integer.max
[1] 2147483647
> log(.Machine$integer.max, 2)
[1] 31
> 2^31-1
[1] 2147483647
> log(.Machine$double.eps, 2)
[1] -52
> log(.Machine$double.neg.eps, 2)
[1] -53



From haynesm at cfr.nichd.nih.gov  Wed Aug 13 16:39:39 2003
From: haynesm at cfr.nichd.nih.gov (Haynes, Maurice (NIH/NICHD))
Date: Wed, 13 Aug 2003 10:39:39 -0400
Subject: [R] Levene test of homogeneity of variance
Message-ID: <6000BB14AFA9A741BC2315A598837ED569393C@nihexchange4.nih.gov>

Has the Levene test of homogeneity of variance been implemented in any
library in R?

Thanks,

Maurice Haynes
National Institute of Child Health and Human Development
Child and Family Research Section
6705 Rockledge Drive
Bethesda, MD  20892
Voice: 301-496-8180
Fax: 301-496-2766
E-Mail: mh192j at nih.gov



From tkt2 at cdc.gov  Wed Aug 13 16:46:45 2003
From: tkt2 at cdc.gov (Thompson, Trevor)
Date: Wed, 13 Aug 2003 10:46:45 -0400
Subject: [R] Regexpr with "."
Message-ID: <D8FCFFA651B8D711B8B500034772AF2B537BFF@mcdc-atl-66.nccd.cdc.gov>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20030813/6706994b/attachment.pl

From tblackw at umich.edu  Wed Aug 13 16:47:58 2003
From: tblackw at umich.edu (Thomas W Blackwell)
Date: Wed, 13 Aug 2003 10:47:58 -0400 (EDT)
Subject: [R] A question on orthogonal basis vectors
In-Reply-To: <003c01c361a5$7188e180$8bd75ba5@IE.TAMU.EDU>
Message-ID: <Pine.SOL.4.44.0308131045380.29993-100000@tetris.gpcc.itd.umich.edu>


The answer is certainly not unique.  Your email doesn't say
whether you are asking about principal components or simply
Gram-Schmidt orthogonalization.

-  tom blackwell  -  u michigan medical school  -  ann arbor  -

On Wed, 13 Aug 2003, Feng Zhang wrote:

> I have a question about determining the orthogonal
> basis vectors.
> In the d-dimensinonal space, if I already know
> the first r orthogonal basis vectors, should I be
> able to determine the remaining d-r orthognal basis
> vectors automatically?
>
> Or the answer is not unique?
>
> Thanks for your attention.
>
> Fred



From jfox at mcmaster.ca  Wed Aug 13 16:50:00 2003
From: jfox at mcmaster.ca (John Fox)
Date: Wed, 13 Aug 2003 10:50:00 -0400
Subject: [R] A question on orthogonal basis vectors
In-Reply-To: <003c01c361a5$7188e180$8bd75ba5@IE.TAMU.EDU>
Message-ID: <5.1.0.14.2.20030813104239.01f7ebd0@127.0.0.1>

Dear Fred,

If I understand correctly what you want, the answer is not unique. Think 
about the 3D case where you start with one vector. (I assume, by the way, 
that you mean orthonormal and that you mean unique up to a reflection.) 
There are infinitely many pairs of orthonormal basis vectors for the plane 
orthogonal to the initial vector. On the other hand, picking an arbitrary 
orthonormal basis isn't hard: The Gram-Schmidt method does this, for example.

I hope that this helps,
  John

At 09:16 AM 8/13/2003 -0500, Feng Zhang wrote:
>Hey, R-listers,
>
>I have a question about determining the orthogonal
>basis vectors.
>In the d-dimensinonal space, if I already know
>the first r orthogonal basis vectors, should I be
>able to determine the remaining d-r orthognal basis
>vectors automatically?
>
>Or the answer is not unique?
>
>Thanks for your attention.
>
>Fred

-----------------------------------------------------
John Fox
Department of Sociology
McMaster University
Hamilton, Ontario, Canada L8S 4M4
email: jfox at mcmaster.ca
phone: 905-525-9140x23604
web: www.socsci.mcmaster.ca/jfox



From B.Rowlingson at lancaster.ac.uk  Wed Aug 13 16:54:49 2003
From: B.Rowlingson at lancaster.ac.uk (Barry Rowlingson)
Date: Wed, 13 Aug 2003 15:54:49 +0100
Subject: [R] Regexpr with "."
In-Reply-To: <D8FCFFA651B8D711B8B500034772AF2B537BFF@mcdc-atl-66.nccd.cdc.gov>
References: <D8FCFFA651B8D711B8B500034772AF2B537BFF@mcdc-atl-66.nccd.cdc.gov>
Message-ID: <3F3A5139.6010003@lancaster.ac.uk>

Thompson, Trevor wrote:

> It looks like R is treating every character in the string as if it were
> decimal.  I didn't see anything in the help file about "." being some kind
> of special character.  Any idea why R is treating a decimal this way in
> these functions?   Any suggestions how to get around this?

'.' is the regexpr character for matching any single character!

 > regexpr("a.e", "Female.Alabama")
[1] 4

  To actually search for a dot, you need to 'escape' it with a 
backslash, but of course the backslash needs escaping itself, with 
another backslash. Luckily that backslash doesn't need escaping, 
otherwise we would quickly run out of patience.

 > regexpr("\\.", "Female.Alabama")
[1] 7

Baz



From dave at evocapital.com  Wed Aug 13 16:57:23 2003
From: dave at evocapital.com (David Khabie-Zeitoune)
Date: Wed, 13 Aug 2003 15:57:23 +0100
Subject: [R] Regexpr with "."
Message-ID: <8D0F30FE2EB3314182D4A33F738BB19D063EE7@mail.internal.net>

Try regexpr("\\.", "Female.Alabama")

-----Original Message-----
From: Thompson, Trevor [mailto:tkt2 at cdc.gov] 
Sent: 13 August 2003 15:47
To: r-help at stat.math.ethz.ch
Subject: [R] Regexpr with "."


I'm trying to use the regexpr function to locate the decimal in a
character string.  Regardless of the position of the decimal, the
function returns 1. For example,

> regexpr(".", "Female.Alabama")
[1] 1
attr(,"match.length")
[1] 1

In trying to figure out what was going on here, I tried the below
command:

> gsub(".", ",", "Female.Alabama")
[1] ",,,,,,,,,,,,,,"

It looks like R is treating every character in the string as if it were
decimal.  I didn't see anything in the help file about "." being some
kind of special character.  Any idea why R is treating a decimal this
way in
these functions?   Any suggestions how to get around this?

Thanks for any suggestions.

-Trevor
 


	[[alternative HTML version deleted]]

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://www.stat.math.ethz.ch/mailman/listinfo/r-help



From jgentry at jimmy.harvard.edu  Wed Aug 13 16:57:40 2003
From: jgentry at jimmy.harvard.edu (Jeff Gentry)
Date: Wed, 13 Aug 2003 10:57:40 -0400 (EDT)
Subject: [R] Regexpr with "."
In-Reply-To: <D8FCFFA651B8D711B8B500034772AF2B537BFF@mcdc-atl-66.nccd.cdc.gov>
Message-ID: <Pine.SOL.4.20.0308131056350.3866-100000@santiam.dfci.harvard.edu>

> I'm trying to use the regexpr function to locate the decimal in a character
> string.  Regardless of the position of the decimal, the function returns 1.

You need to escape it.

> gsub("\\.",",","Female.Alabama")
[1] "Female,Alabama"



From jzhang at jimmy.harvard.edu  Wed Aug 13 17:01:51 2003
From: jzhang at jimmy.harvard.edu (John Zhang)
Date: Wed, 13 Aug 2003 11:01:51 -0400 (EDT)
Subject: [R] Regexpr with "."
Message-ID: <200308131501.LAA22685@blaise.dfci.harvard.edu>

Try

regexpr("\\.", "Female.Alabama") and gsub("\\.", ",", "Female.Alabama")


>X-Sybari-Trust: 9293cd92 d90ef28b 235e1558 0000093d
>From: "Thompson, Trevor" <tkt2 at cdc.gov>
>To: r-help at stat.math.ethz.ch
>Date: Wed, 13 Aug 2003 10:46:45 -0400
>MIME-Version: 1.0
>X-Virus-Scanned: by amavisd-milter (http://amavis.org/)
>X-Virus-Scanned: by amavisd-milter (http://amavis.org/)
>X-Spam-Status: No, hits=0.6 required=5.0 tests=HTML_30_40 version=2.54
>X-Spam-Level: 
>X-Spam-Checker-Version: SpamAssassin 2.54 (1.174.2.17-2003-05-11-exp)
>Content-Disposition: inline
>Content-Transfer-Encoding: 7bit
>Subject: [R] Regexpr with "."
>X-BeenThere: r-help at stat.math.ethz.ch
>X-Mailman-Version: 2.1.2
>List-Id: Main R Mailing List: Primary help  <r-help.stat.math.ethz.ch>
>List-Help: <mailto:r-help-request at stat.math.ethz.ch?subject=help>
>List-Post: <mailto:r-help at stat.math.ethz.ch>
>List-Subscribe: <https://www.stat.math.ethz.ch/mailman/listinfo/r-help>, 
<mailto:r-help-request at stat.math.ethz.ch?subject=subscribe>
>List-Archive: <https://www.stat.math.ethz.ch/pipermail/r-help>
>List-Unsubscribe: <https://www.stat.math.ethz.ch/mailman/listinfo/r-help>, 
<mailto:r-help-request at stat.math.ethz.ch?subject=unsubscribe>
>
>I'm trying to use the regexpr function to locate the decimal in a character
>string.  Regardless of the position of the decimal, the function returns 1.
>For example,
>
>> regexpr(".", "Female.Alabama")
>[1] 1
>attr(,"match.length")
>[1] 1
>
>In trying to figure out what was going on here, I tried the below command:
>
>> gsub(".", ",", "Female.Alabama")
>[1] ",,,,,,,,,,,,,,"
>
>It looks like R is treating every character in the string as if it were
>decimal.  I didn't see anything in the help file about "." being some kind
>of special character.  Any idea why R is treating a decimal this way in
>these functions?   Any suggestions how to get around this?
>
>Thanks for any suggestions.
>
>-Trevor
> 
>
>
>	[[alternative HTML version deleted]]
>
>______________________________________________
>R-help at stat.math.ethz.ch mailing list
>https://www.stat.math.ethz.ch/mailman/listinfo/r-help

Jianhua Zhang
Department of Biostatistics
Dana-Farber Cancer Institute
44 Binney Street
Boston, MA 02115-6084



From Luis.Tito-de-Morais at ird.sn  Wed Aug 13 17:03:11 2003
From: Luis.Tito-de-Morais at ird.sn (Tito de Morais Luis)
Date: Wed, 13 Aug 2003 15:03:11 -0000
Subject: [R] stars graphs
Message-ID: <1060779245.2941.337.camel@rap06.ird.sn>

Hi listers,

A few days ago I posted a question about the use of the stars function
on selected lines of a frame. Thanks to two helpers, a closer look at
the scale argument allowed to partially solve the problem. Yet I still
have a problem with stars.

Allow me to explain what I intend to do (sorry for my poor English and
the long post):

I want to graph an activity index of a fish during the day cycle with a
star or radar type graph. I know that the R stars function was not
written for that purpose. But I couldn't find another way to try to do
it. I would appreciate if you could point me to a more appropriate
function.

I looked at the archives and only found the link below, which pointed me
to the stars function:
http://maths.newcastle.edu.au/~rking/R/help/01c/2464.html

My data frame (2 cols, 55 rows) has the activity index (range 0:1) in
the first column. In the second column is the cumulated time (in minutes
- range 0:1440) between successive measures of activity.

I would like the radius length be the activity index and the angle
between two successive plots proportional to elapsed time. In addition
time or a character label, is to be written on the graph. 

I tried with no success to graph it using both the data frame and its
"transposed" (2 rows, 55 cols).

As in the actual file there are 55 measures of activity at different
hours of the day (not regularly spaced). You can simulate data with
something like :

df <- data.frame(activ=runif(55),cumtime=round(seq(1,1440,l=55)))
df$cumtime[1]=0

df looks like my actual data except that the actual data are not
regularly spaced in time. But this is not important for the graph.

Maybe stars() is totally inadequate for the purpose. If no other
solution is available in R, I will try to write down my own and post it.
But it may take me a while as I am a newbie in R. Any hint will be
appreciated.

Regards,

Tito



-- 
L. Tito de Morais
      UR RAP
   IRD de Dakar
      BP 1386
       Dakar
      S?n?gal

T?l.: + 221 849 33 31
Fax: +221 832 16 75
Courriel: tito at ird.sn



From ligges at statistik.uni-dortmund.de  Wed Aug 13 17:03:26 2003
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Wed, 13 Aug 2003 17:03:26 +0200
Subject: [R] Regexpr with "."
In-Reply-To: <D8FCFFA651B8D711B8B500034772AF2B537BFF@mcdc-atl-66.nccd.cdc.gov>
References: <D8FCFFA651B8D711B8B500034772AF2B537BFF@mcdc-atl-66.nccd.cdc.gov>
Message-ID: <3F3A533E.8030108@statistik.uni-dortmund.de>

Thompson, Trevor wrote:

> I'm trying to use the regexpr function to locate the decimal in a character
> string.  Regardless of the position of the decimal, the function returns 1.
> For example,
> 
> 
>>regexpr(".", "Female.Alabama")
> 
> [1] 1
> attr(,"match.length")
> [1] 1
> 
> In trying to figure out what was going on here, I tried the below command:
> 
> 
>>gsub(".", ",", "Female.Alabama")
> 
> [1] ",,,,,,,,,,,,,,"
> 
> It looks like R is treating every character in the string as if it were
> decimal.  I didn't see anything in the help file about "." being some kind
> of special character.  Any idea why R is treating a decimal this way in
> these functions?   Any suggestions how to get around this?
> 
> Thanks for any suggestions.
> 
> -Trevor
>  
> 
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help


Think about the meaning of "." in regular expression (it needs to be 
escaped as being a special character!):

  gsub("\\.", ",", "Female.Alabama")
or
  regexpr("\\.", "Female.Alabama")

Uwe Ligges



From ligges at statistik.uni-dortmund.de  Wed Aug 13 17:04:47 2003
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Wed, 13 Aug 2003 17:04:47 +0200
Subject: [R] Levene test of homogeneity of variance
In-Reply-To: <6000BB14AFA9A741BC2315A598837ED569393C@nihexchange4.nih.gov>
References: <6000BB14AFA9A741BC2315A598837ED569393C@nihexchange4.nih.gov>
Message-ID: <3F3A538F.4070908@statistik.uni-dortmund.de>

Haynes, Maurice (NIH/NICHD) wrote:

> Has the Levene test of homogeneity of variance been implemented in any
> library in R?

levene.test() is in "Rcmdr".

Uwe Ligges



> Thanks,
> 
> Maurice Haynes
> National Institute of Child Health and Human Development
> Child and Family Research Section
> 6705 Rockledge Drive
> Bethesda, MD  20892
> Voice: 301-496-8180
> Fax: 301-496-2766
> E-Mail: mh192j at nih.gov
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help



From ccleland at optonline.net  Wed Aug 13 17:10:22 2003
From: ccleland at optonline.net (Chuck Cleland)
Date: Wed, 13 Aug 2003 11:10:22 -0400
Subject: [R] Regexpr with "."
In-Reply-To: <D8FCFFA651B8D711B8B500034772AF2B537BFF@mcdc-atl-66.nccd.cdc.gov>
References: <D8FCFFA651B8D711B8B500034772AF2B537BFF@mcdc-atl-66.nccd.cdc.gov>
Message-ID: <3F3A54DE.60201@optonline.net>

Thompson, Trevor wrote:
> I'm trying to use the regexpr function to locate the decimal in a character
> string.  Regardless of the position of the decimal, the function returns 1.
> For example,
> 
>>regexpr(".", "Female.Alabama")

   You probably want backslashes to indicate that "." should not 
be treated as a metacharacter; it should be taken literally.

 > regexpr("\\.", "Female.Alabama")
[1] 7
attr(,"match.length")
[1] 1

hope this helps,

Chuck Cleland



From MSchwartz at medanalytics.com  Wed Aug 13 17:10:50 2003
From: MSchwartz at medanalytics.com (Marc Schwartz)
Date: Wed, 13 Aug 2003 10:10:50 -0500
Subject: [R] Levene test of homogeneity of variance
In-Reply-To: <6000BB14AFA9A741BC2315A598837ED569393C@nihexchange4.nih.gov>
References: <6000BB14AFA9A741BC2315A598837ED569393C@nihexchange4.nih.gov>
Message-ID: <1060787449.13211.337.camel@localhost>

On Wed, 2003-08-13 at 09:39, Haynes, Maurice (NIH/NICHD) wrote:
> Has the Levene test of homogeneity of variance been implemented in any
> library in R?
> 
> Thanks,
> 
> Maurice Haynes
> National Institute of Child Health and Human Development
> Child and Family Research Section
> 6705 Rockledge Drive
> Bethesda, MD  20892
> Voice: 301-496-8180
> Fax: 301-496-2766
> E-Mail: mh192j at nih.gov


>From a search on the r-help archives and CRAN it would appear there is a
levene.test() function in John Fox's RCmdr package, which is based upon
an exchange back in 2000 here:

http://maths.newcastle.edu.au/~rking/R/help/00b/0663.html

You may wish to review the entire thread for additional comments and I
would defer to John and the original thread participants for any updated
commentary.

HTH,

Marc Schwartz



From upton at mitre.org  Wed Aug 13 17:11:31 2003
From: upton at mitre.org (Stephen C. Upton)
Date: Wed, 13 Aug 2003 11:11:31 -0400
Subject: [R] Regexpr with "."
References: <D8FCFFA651B8D711B8B500034772AF2B537BFF@mcdc-atl-66.nccd.cdc.gov>
Message-ID: <3F3A5523.8B822108@mitre.org>

Trevor,

The "." is a regex meta-character that matches any character. In order to look
specifically for a ".", the you must escape it with a "\", and that "\" must
also be escaped, thus,

> regexpr("\\.", "Female.Alabama")
[1] 7
attr(,"match.length")
[1] 1
>

HTH
steve

"Thompson, Trevor" wrote:

> I'm trying to use the regexpr function to locate the decimal in a character
> string.  Regardless of the position of the decimal, the function returns 1.
> For example,
>
> > regexpr(".", "Female.Alabama")
> [1] 1
> attr(,"match.length")
> [1] 1
>
> In trying to figure out what was going on here, I tried the below command:
>
> > gsub(".", ",", "Female.Alabama")
> [1] ",,,,,,,,,,,,,,"
>
> It looks like R is treating every character in the string as if it were
> decimal.  I didn't see anything in the help file about "." being some kind
> of special character.  Any idea why R is treating a decimal this way in
> these functions?   Any suggestions how to get around this?
>
> Thanks for any suggestions.
>
> -Trevor
>
>
>         [[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help



From jfox at mcmaster.ca  Wed Aug 13 17:15:43 2003
From: jfox at mcmaster.ca (John Fox)
Date: Wed, 13 Aug 2003 11:15:43 -0400
Subject: [R] Levene test of homogeneity of variance
In-Reply-To: <6000BB14AFA9A741BC2315A598837ED569393C@nihexchange4.nih.go
 v>
Message-ID: <5.1.0.14.2.20030813111135.01f92c28@127.0.0.1>

Dear Maurice,

Brian Ripley once posted a function for Levene's test to the R-help list. I 
incorporated a slightly modified version in the Rcmdr package. Since the 
function is very simple, I'll just list it here (it doesn't make sense to 
use the Rcmdr package just for this function):

levene.test <- function(y, group) {
     meds <- tapply(y, group, median, na.rm=TRUE)
     resp <- abs(y - meds[group])
     table <- anova(lm(resp ~ group))
     rownames(table)[2] <- " "
     cat("Levene's Test for Homogeneity of Variance\n\n")
     table[,c(1,4,5)]
     }

By the way, "group" is a factor.

John

At 10:39 AM 8/13/2003 -0400, Haynes, Maurice (NIH/NICHD) wrote:
>Has the Levene test of homogeneity of variance been implemented in any
>library in R?
>
>Thanks,
>
>Maurice Haynes

-----------------------------------------------------
John Fox
Department of Sociology
McMaster University
Hamilton, Ontario, Canada L8S 4M4
email: jfox at mcmaster.ca
phone: 905-525-9140x23604
web: www.socsci.mcmaster.ca/jfox



From maechler at stat.math.ethz.ch  Wed Aug 13 17:25:06 2003
From: maechler at stat.math.ethz.ch (Martin Maechler)
Date: Wed, 13 Aug 2003 17:25:06 +0200
Subject: [R] Mapping function dependencies
In-Reply-To: <Pine.A41.4.44.0308130715210.158690-100000@homer06.u.washington.edu>
References: <8D0F30FE2EB3314182D4A33F738BB19D063EE4@mail.internal.net>
	<Pine.A41.4.44.0308130715210.158690-100000@homer06.u.washington.edu>
Message-ID: <16186.22610.813835.36526@gargle.gargle.HOWL>

>>>>> "TL" == Thomas Lumley <tlumley at u.washington.edu>
>>>>>     on Wed, 13 Aug 2003 07:16:43 -0700 (PDT) writes:

    TL> On Wed, 13 Aug 2003, David Khabie-Zeitoune wrote:
    >> With regards to my question below, I've found a way to do it:
    >> 
    >> all.names(body(test))
    >> 
    TL> This also gives all the variable names, which you may not want.

    TL> This question came up a few months ago:
    TL> http://finzi.psych.upenn.edu/R/Rhelp02/archive/18497.html

If you are willing to (install and) use "R-devel",
Luke Tierney has written a package "codetools" which does
what you want, at least gives you the building blocks:

You need R-devel (which will become R 1.8 in October):

  > library(help=codetools)

		  Information on Package 'codetools'

  Description:

  Package: codetools
  Version: 0.0-0
  Author: Luke Tierney <luke at stat.uiowa.edu>
  Description: Code analysis tools for R
  Title: Code Analysis Tools for R
  Depends: R (>= 1.8)
  Maintainer: Luke Tierney <luke at stat.uiowa.edu>
  License: GPL
  Built: R 1.8.0; ; 2003-08-13 17:13:31

  Index:

  callCC                  Call With Current Continuation
  checkUsage              Check R Code for Possible Problems
  codetools               Low Level Code Analysis Tools for R
  findGlobals             Find Global Functions and Variables Used by a
			  Closure
  showTree                Print Lisp-Style Representation of R
			  Expression

  > library(codetools)

  > findGlobals(lm)

   [1] "<-"             "=="             ">"              "-"             
   [5] "!"              "!="             "("              "["             
   [9] "[[<-"           "{"              "$<-"            "*"             
  [13] "&&"             "as.character"   "as.name"        "attr"          
  [17] "c"              "class<-"        "eval"           "if"            
  [21] "is.empty.model" "is.matrix"      "is.null"        "lapply"        
  [25] "length"         "levels"         "list"           "lm.fit"        
  [29] "lm.wfit"        "match.call"     "model.matrix"   "model.offset"  
  [33] "model.response" "model.weights"  "NROW"           "numeric"       
  [37] "parent.frame"   "return"         "sapply"         "stop"          
  [41] "warning"       
  > 

----

If you are not an R "newbie"
the package __IN SOURCE FORM__ (i.e. not compiled for Windows!)
can be downloaded from
    http://www.stat.uiowa.edu/~luke/R/codetools/

(Again: You need R-devel, and need to be able to install
	packages from source!)

Regards,
Martin Maechler <maechler at stat.math.ethz.ch>	http://stat.ethz.ch/~maechler/
Seminar fuer Statistik, ETH-Zentrum  LEO C16	Leonhardstr. 27
ETH (Federal Inst. Technology)	8092 Zurich	SWITZERLAND
phone: x-41-1-632-3408		fax: ...-1228			<><



From hdoran at nasdc.org  Wed Aug 13 17:26:56 2003
From: hdoran at nasdc.org (Harold Doran)
Date: Wed, 13 Aug 2003 11:26:56 -0400
Subject: [R] Levene test of homogeneity of variance
Message-ID: <66578BFC0BA55348B5907A0F798EE93013A023@ernesto.NASDC.ORG>

I believe it is in the Rcmdr package, which requires the car library to be loaded. 

You can also perform an ANOVA using the absolute value of the deviations from each respective group mean, which is what Levene's Test does.

 
------
Harold C. Doran
Director of Research and Evaluation
New American Schools
675 N. Washington Street, Suite 220
Alexandria, Virginia 22314
703.647.1628
<http://www.edperform.net>  
 
 


-----Original Message-----
From: Haynes, Maurice (NIH/NICHD) [mailto:haynesm at cfr.nichd.nih.gov]
Sent: Wednesday, August 13, 2003 10:40 AM
To: 'r-help'
Subject: [R] Levene test of homogeneity of variance


Has the Levene test of homogeneity of variance been implemented in any
library in R?

Thanks,

Maurice Haynes
National Institute of Child Health and Human Development
Child and Family Research Section
6705 Rockledge Drive
Bethesda, MD  20892
Voice: 301-496-8180
Fax: 301-496-2766
E-Mail: mh192j at nih.gov

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://www.stat.math.ethz.ch/mailman/listinfo/r-help



From john.lewis at sympatico.ca  Wed Aug 13 17:43:34 2003
From: john.lewis at sympatico.ca (john lewis)
Date: Wed, 13 Aug 2003 11:43:34 -0400
Subject: [R] graphic device sequence/ Rprofile(options)
Message-ID: <000c01c361b1$ab5dc220$8c00a8c0@max>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20030813/ee5f5b72/attachment.pl

From liping66 at hotmail.com  Wed Aug 13 17:51:32 2003
From: liping66 at hotmail.com (liping)
Date: Wed, 13 Aug 2003 11:51:32 -0400
Subject: [R] re: two dimentional hierarchical clustering algorithm
References: <3A822319EB35174CA3714066D590DCD50205C994@usrymx25.merck.com>
Message-ID: <BAY1-DAV25dUg07MJJQ0002e8ba@hotmail.com>

Dear Dr. Liaw Andy:

I have a few more questions about your heatmap function. actually heatmap is
what I am looking for.

 heatmap(x, Rowv, Colv, distfun = dist, hclustfun = hclust, add.expr,
             scale=c("row", "column", "none"), na.rm = TRUE, ...)

my data is a XNEW,
> dim(XNEW)
[1] 554 335

554 genes, 335 samples.

now I want to use 1-CORR as a distance measure and hclust(method="ward") as
hclustfun. is there any way for me to define these two in heatmap options?

thanks.

lp



----- Original Message ----- 
From: "Liaw, Andy" <andy_liaw at merck.com>
To: "'liping'" <liping66 at hotmail.com>
Sent: Monday, August 11, 2003 10:35 AM
Subject: RE: [R] re: two dimentional hierarchical clustering algorithm


> Check out the heatmap() function in the 'mva' package (came with R) and
see
> if that does what you want.
>
> Andy
>
> > -----Original Message-----
> > From: liping [mailto:liping66 at hotmail.com]
> > Sent: Monday, August 11, 2003 10:12 AM
> > To: r-help at stat.math.ethz.ch
> > Subject: [R] re: two dimentional hierarchical clustering algorithm
> >
> >
> >   Dear R users:
> >
> >     do you know if R or SPLUS has a package for two
> > dimentional hierarchical clustering algorithm??? I am trying
> > to use that algorithm to cluster samples and genes at the
> > same.    thanks
> >     LP
> > [[alternative HTML version deleted]]
> >
> > ______________________________________________
> > R-help at stat.math.ethz.ch mailing list
> > https://www.stat.math.ethz.ch/mailman/listinfo> /r-help
> >
>
> --------------------------------------------------------------------------
----
> Notice:  This e-mail message, together with any attachments, contains
> information of Merck & Co., Inc. (Whitehouse Station, New Jersey, USA),
and/or
> its affiliates (which may be known outside the United States as Merck
Frosst,
> Merck Sharp & Dohme or MSD) that may be confidential, proprietary
copyrighted
> and/or legally privileged, and is intended solely for the use of the
> individual or entity named on this message.  If you are not the intended
> recipient, and have received this message in error, please immediately
return
> this by e-mail and then delete it.
> --------------------------------------------------------------------------
----
>



From loesljrg at accucom.net  Wed Aug 13 18:32:13 2003
From: loesljrg at accucom.net (JRG)
Date: Wed, 13 Aug 2003 12:32:13 -0400
Subject: [R] Levene test of homogeneity of variance
In-Reply-To: <66578BFC0BA55348B5907A0F798EE93013A023@ernesto.NASDC.ORG>
Message-ID: <B0025041925@netserv1.accucom.net>

On 13 Aug 03, at 11:26, Harold Doran wrote:

> I believe it is in the Rcmdr package, which requires the car library to be loaded. 
> 
> You can also perform an ANOVA using the absolute value of the deviations from each respective group mean, which is what Levene's Test does.
> 

The function just posted by John Fox (and originally by Brian Ripley) is a much better idea.  In particular, using absolute 
deviations from the group means is a poor idea, as the performance of the resulting test is strongly dependent on population 
shape.  A good reference is

Carroll, R. J. & Schneider H., Statistics & Probability Letters, 1985, 3, 191-194.

---JRG



>  
> ------
> Harold C. Doran
> Director of Research and Evaluation
> New American Schools
> 675 N. Washington Street, Suite 220
> Alexandria, Virginia 22314
> 703.647.1628
> <http://www.edperform.net>  
>  
>  
> 
> 
> -----Original Message-----
> From: Haynes, Maurice (NIH/NICHD) [mailto:haynesm at cfr.nichd.nih.gov]
> Sent: Wednesday, August 13, 2003 10:40 AM
> To: 'r-help'
> Subject: [R] Levene test of homogeneity of variance
> 
> 
> Has the Levene test of homogeneity of variance been implemented in any
> library in R?
> 
> Thanks,
> 
> Maurice Haynes
> National Institute of Child Health and Human Development
> Child and Family Research Section
> 6705 Rockledge Drive
> Bethesda, MD  20892
> Voice: 301-496-8180
> Fax: 301-496-2766
> E-Mail: mh192j at nih.gov
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help



John R. Gleason

Syracuse University
430 Huntington Hall                      Voice:   315-443-3107
Syracuse, NY 13244-2340  USA             FAX:     315-443-4085

PGP public key at keyservers



From alsi at oesa.ufz.de  Wed Aug 13 20:09:06 2003
From: alsi at oesa.ufz.de (Alexander Singer)
Date: Wed, 13 Aug 2003 19:09:06 +0100
Subject: [R] ^-operation exponentiation problem 
Message-ID: <000401c361c5$fc19b2a0$f9effea9@oesa38>

Hi,
There seems to be a problem with the power-operation of the form c(...,
-a, ...)^x.yyy
Four code examples for the same calculation, which perform differently:

-3^3.2
Result: [1] -33.63474

c(-3,6)^3.2
Result: [1]      NaN 309.0893
But:
c(-3.62,3)^3
Result: [1] -47.43793  27.00000

I can reproduce NaN even in the following calculation:
c(-3,6)[1]^3.2 
Result: [1] NaN
but
c(-3,6)[2]^3.2
Result: [1] 309.0893

Does anybody know this problem and a solution to it?
We have tested the calculation under Windows XP for R version 1.6.0 and
1.7.1
Greetings Alex



From jc at or.psychology.dal.ca  Wed Aug 13 19:14:37 2003
From: jc at or.psychology.dal.ca (John Christie)
Date: Wed, 13 Aug 2003 14:14:37 -0300
Subject: [R] contrasts??
Message-ID: <9E029AA8-CDB1-11D7-815A-000A9566473A@or.psychology.dal.ca>

	I was looking at the contrasts function in the search for planned 
contrast functions.  aov take a contrast variable but I don't see how 
to use it.  Is there a planned contrast capability in aov?



From ligges at statistik.uni-dortmund.de  Wed Aug 13 19:15:10 2003
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Wed, 13 Aug 2003 19:15:10 +0200
Subject: [R] ^-operation exponentiation problem
In-Reply-To: <000401c361c5$fc19b2a0$f9effea9@oesa38>
References: <000401c361c5$fc19b2a0$f9effea9@oesa38>
Message-ID: <3F3A721E.7080903@statistik.uni-dortmund.de>

Alexander Singer wrote:

> Hi,
> There seems to be a problem with the power-operation of the form c(...,
> -a, ...)^x.yyy
> Four code examples for the same calculation, which perform differently:
> 
> -3^3.2
> Result: [1] -33.63474
> 
> c(-3,6)^3.2
> Result: [1]      NaN 309.0893
> But:
> c(-3.62,3)^3
> Result: [1] -47.43793  27.00000
> 
> I can reproduce NaN even in the following calculation:
> c(-3,6)[1]^3.2 
> Result: [1] NaN
> but
> c(-3,6)[2]^3.2
> Result: [1] 309.0893
> 
> Does anybody know this problem and a solution to it?
> We have tested the calculation under Windows XP for R version 1.6.0 and
> 1.7.1
> Greetings Alex

Well, you abviously try to work in the complex space (R assumes not, as 
the default), so at first make your numbers complex as in:

  c(-3 + 0i, 6)^3.2

[1] -27.21107-19.77i 309.08932+ 0.00i

Uwe Ligges



From alsi at oesa.ufz.de  Wed Aug 13 20:26:11 2003
From: alsi at oesa.ufz.de (Alexander Singer)
Date: Wed, 13 Aug 2003 19:26:11 +0100
Subject: [R] Re: ^-operation exponentiation problem
Message-ID: <000001c361c8$5f882fe0$f9effea9@oesa38>

Hi,

Sorry, I was to fast in sending the mail to the list.
I could find the error myself.

^operator binds stronger than -operator, so -3^3.2 calculates as
-(3)^3.2 and not as (-3)^3.2 which results correctely in NaN.

Alex



From aurora at ebi.ac.uk  Wed Aug 13 19:29:44 2003
From: aurora at ebi.ac.uk (Aurora Torrente)
Date: Wed, 13 Aug 2003 18:29:44 +0100
Subject: [R] Removing variables
Message-ID: <3F3A7588.10707@ebi.ac.uk>

Hi all,

Is there any way to remove at the same time several variables that share 
a suffix, for example (similar to * or ?  in DOS) ?
Thanks in advance,

       Aurora



From ccleland at optonline.net  Wed Aug 13 19:50:19 2003
From: ccleland at optonline.net (Chuck Cleland)
Date: Wed, 13 Aug 2003 13:50:19 -0400
Subject: [R] Removing variables
In-Reply-To: <3F3A7588.10707@ebi.ac.uk>
References: <3F3A7588.10707@ebi.ac.uk>
Message-ID: <3F3A7A5B.2020108@optonline.net>

Aurora Torrente wrote:
> Is there any way to remove at the same time several variables that share 
> a suffix, for example (similar to * or ?  in DOS) ?

   To remove objects sharing a suffix use something like:

remove(list=objects(pattern="SUFFIX$"))

   To remove variables sharing a suffix from a dataframe use 
something like:

newdata <- mydata[,-grep("SUFFIX$", names(mydata))]

   In both cases the $ metacharacter indicates that the literal 
characters are at the end of the string.

hope this helps,

Chuck Cleland



From jfay at genetics.wustl.edu  Wed Aug 13 19:53:02 2003
From: jfay at genetics.wustl.edu (Justin Fay)
Date: Wed, 13 Aug 2003 12:53:02 -0500
Subject: [R] X11 not configured on linux RedHat 9
Message-ID: <3F3A7AFE.6020208@genetics.wustl.edu>

I'm using Linux version 2.4.20-8smp (gcc version 3.2.2, RedHat 9). I 
installed R-1.7.1-1.src.rpm following INSTALL. However, no X11 device is 
found when I run R.
 > X11()
Error in X11() : X11 is not available

In the configuration, I found "checking for X... no"

How do I configure so that X11 device capability is installed?
-- 
________________________________________
Justin Fay
Assistant Professor of Genetics
Washington University School of Medicine
4566 Scott Ave, St. Louis, MO 63110
PH: 314.747.1808 Fax: 314.362.7855



From spencer.graves at pdf.com  Wed Aug 13 19:57:20 2003
From: spencer.graves at pdf.com (Spencer Graves)
Date: Wed, 13 Aug 2003 10:57:20 -0700
Subject: [R] Removing variables
References: <3F3A7588.10707@ebi.ac.uk>
Message-ID: <3F3A7C00.2050802@pdf.com>



 > x1 <- 1
 > y1 <- 1
 > objects(pattern="*1")
[1] "x1" "y1"
 > remove(list=objects(pattern="*1"))
 > objects(pattern="*1")
character(0)

Hope this helps.  spencer graves

Aurora Torrente wrote:
> Hi all,
> 
> Is there any way to remove at the same time several variables that share 
> a suffix, for example (similar to * or ?  in DOS) ?
> Thanks in advance,
> 
>       Aurora
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help



From dave at evocapital.com  Wed Aug 13 20:32:10 2003
From: dave at evocapital.com (David Khabie-Zeitoune)
Date: Wed, 13 Aug 2003 19:32:10 +0100
Subject: [R] Startup for RExcel
Message-ID: <8D0F30FE2EB3314182D4A33F738BB19D063EEB@mail.internal.net>


Hi

When I start an instance of an R stats server using the R(D)COM RExcel
Addin by Thomas Baier, it appears to start the server in a temporary
directory. Is there any way I can change the directory in which the
server starts in so that I can, for example, take advantage of Rprofile
or Renviron files I may have in a particular directory?

Thanks

David



From RML27 at cornell.edu  Wed Aug 13 21:25:23 2003
From: RML27 at cornell.edu (Ronnen Levinson)
Date: Wed, 13 Aug 2003 12:25:23 -0700
Subject: [R] Contour plot for arbitrary (x,y,z)
Message-ID: <3F3A90A3.5000300@cornell.edu>

Hello.

Is there an easy-to-use contour plot function analogous to scatterplot3d 
that can draw handle a dataset of arbitrary (x,y,z) triplets? That is, 
say x, y, and z are each measured quanties, and exhibit neither order 
nor regularity.

I looked at the lattice package function "contourplot" but it seems 
complicated, and it's not clear from the documentation whether it can 
handle arbitrary (x,y) values. I'm looking for something like the base 
package "contour" function that does not make any assumptions about (x,y,z).

Thanks,

Ronnen.



From Whit.Armstrong at tudor.com  Wed Aug 13 21:23:58 2003
From: Whit.Armstrong at tudor.com (Whit Armstrong)
Date: Wed, 13 Aug 2003 15:23:58 -0400
Subject: [R] Problems with addition in big POSIX dates
Message-ID: <CD9BF92D5B1AD611ABEB00065B386FB001E8BDFB@tudor.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20030813/9df77938/attachment.pl

From djw1005 at cam.ac.uk  Wed Aug 13 22:38:26 2003
From: djw1005 at cam.ac.uk (Damon Wischik)
Date: Wed, 13 Aug 2003 21:38:26 +0100 (BST)
Subject: [R] adding to Trellis plot
In-Reply-To: <BAY9-F17wv8ztxsx12V00034e22@hotmail.com>
Message-ID: <Pine.SOL.3.96.1030813212802.26936A-100000@draco.cus.cam.ac.uk>


> I have grouped data and i used a Trellis plot to show the curve of a
> fitted model for each group. Additionally i have a vector whose values
> are the highest points the curve can reach (the curve rises
> exponetially). Each value of the vector is the corresponding highest
> point for each group/plot. Now i want to show this highest point by
> adding to each plot a horizontal line. 

Do you mean group or panel? You say "adding to each plot", so I guess you
mean panel.

I would use the subscripts argument to xyplot. If subscripts=TRUE in the
arguments to xyplot (and in some other cases), the panel function is
passed an argument called subscripts, a vector of integer indices
indicating which points are about to be plotted. This lets you work out
which rows of your data frame are about to be plotted, which should let
you print your extra graphical elements.

mydf <- data.frame(x=rnorm(20),y=rnorm(20),t=rbinom(20,1,.5))

xyplot(y~x|t, data=mydf, subscripts=TRUE,
       panel=function(x,y,subscripts,...) {
         panel.xyplot(x,y)
         print(subscripts)
         currentpanel <- unique(mydf$t[subscripts])
         print(currentpanel)
         panel.abline(something to do with currentpanel)
         })

Damon.



From Ted.Harding at nessie.mcc.ac.uk  Wed Aug 13 23:14:06 2003
From: Ted.Harding at nessie.mcc.ac.uk ( (Ted Harding))
Date: Wed, 13 Aug 2003 22:14:06 +0100 (BST)
Subject: [R] Regexpr with "."
In-Reply-To: <3F3A5139.6010003@lancaster.ac.uk>
Message-ID: <XFMail.030813221406.Ted.Harding@nessie.mcc.ac.uk>

On 13-Aug-03 Barry Rowlingson wrote:
> Thompson, Trevor wrote:
>> I didn't see anything in the help file about "." being some kind
>> of special character.  Any idea why R is treating a decimal this
>> way in these functions?   Any suggestions how to get around this?
> 
> '.' is the regexpr character for matching any single character!
>  > regexpr("a.e", "Female.Alabama")
> [1] 4
>   To actually search for a dot, you need to 'escape' it with a 
> backslash, but of course the backslash needs escaping itself, with 
> another backslash. Luckily that backslash doesn't need escaping, 
> otherwise we would quickly run out of patience.
>  > regexpr("\\.", "Female.Alabama")
> [1] 7

It's also worth remembering the use of [], normally used to enclose
a disjunctive list of characters to match (e.g. [Aa] matches either
"A" or "a") or a range (e.g. [0-9] matches any digit). Any metacharacter
occurring within will be interpreted literally with exceptions "\"
and (for obvious reasons) "]" which must be escaped (in which case
the use of [] is redundant); -- however, "[" works!

  > regexpr("a.e", "Female.Alabama")
  [1] 4
  attr(,"match.length")
  [1] 3
  > regexpr("[.]", "Female.Alabama")
  [1] 7
  attr(,"match.length")
  [1] 1
  > regexpr("[[]", "Female[Alabama")
  [1] 7
  attr(,"match.length")
  [1] 1
  > regexpr("[\\]", "Female\\Alabama")
  [1] 7
  attr(,"match.length")
  [1] 1
  > regexpr("[\]]", "Female]Alabama")
  [1] 7
  attr(,"match.length")
  [1] 1

Ted.


--------------------------------------------------------------------
E-Mail: (Ted Harding) <Ted.Harding at nessie.mcc.ac.uk>
Fax-to-email: +44 (0)870 167 1972
Date: 13-Aug-03                                       Time: 22:14:06
------------------------------ XFMail ------------------------------



From andy_liaw at merck.com  Wed Aug 13 15:51:52 2003
From: andy_liaw at merck.com (Liaw, Andy)
Date: Wed, 13 Aug 2003 09:51:52 -0400
Subject: [R] anova and tukeyHSD
Message-ID: <3A822319EB35174CA3714066D590DCD50205C9A3@usrymx25.merck.com>

Using some simulated data:

> A <- rnorm(7, mean=1); B <- rnorm(9, mean=2); C <- rnorm(13, mean=2.5)
> y <- c(A, B, C)
> f <- factor(rep(1:3, c(7, 9, 13)))
> TukeyHSD(aov(y~f))
  Tukey multiple comparisons of means
    95% family-wise confidence level

Fit: aov(formula = y ~ f)

$f
         diff        lwr      upr
2-1 0.9345966 -0.2173612 2.086554
3-1 1.5357566  0.4641358 2.607377
3-2 0.6011600 -0.3900490 1.592369

HTH,
Andy

> -----Original Message-----
> From: Anna H. Pryor [mailto:anna at ptolemy.arc.nasa.gov] 
> Sent: Wednesday, August 13, 2003 9:28 AM
> To: R-help mailing list
> Subject: [R] anova and tukeyHSD
> 
> 
> 
> I would like to do a one way anova and then a tukeyHSD.  I 
> have three vectors 
> A,B and C.  In a previous help message, I was told to do the 
> following for 
> the anova:
> 
> y = c(A,B,C)
> group = factor(rep(a:3,c(7,9,13))) #provided there a 7 
> elements in A,9 in B 
> and 13 in C
> 
> and then
> 
> anova(lm(y~group))
> 
> 
> Looking at the tukeyHSD method it looks like it wants the aov 
> method which I 
> don't understand.  Using the above example, could someone 
> continue the 
> example and get the tukeyHSD method to work?
> 
> Anna
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list 
> https://www.stat.math.ethz.ch/mailman/listinfo> /r-help
> 

------------------------------------------------------------------------------
Notice:  This e-mail message, together with any attachments, contains
information of Merck & Co., Inc. (Whitehouse Station, New Jersey, USA), and/or
its affiliates (which may be known outside the United States as Merck Frosst,
Merck Sharp & Dohme or MSD) that may be confidential, proprietary copyrighted
and/or legally privileged, and is intended solely for the use of the
individual or entity named on this message.  If you are not the intended
recipient, and have received this message in error, please immediately return
this by e-mail and then delete it.



From MSchwartz at medanalytics.com  Thu Aug 14 00:04:52 2003
From: MSchwartz at medanalytics.com (Marc Schwartz)
Date: Wed, 13 Aug 2003 17:04:52 -0500
Subject: [R] X11 not configured on linux RedHat 9
In-Reply-To: <3F3A7AFE.6020208@genetics.wustl.edu>
References: <3F3A7AFE.6020208@genetics.wustl.edu>
Message-ID: <1060812291.13211.414.camel@localhost>

On Wed, 2003-08-13 at 12:53, Justin Fay wrote:
> I'm using Linux version 2.4.20-8smp (gcc version 3.2.2, RedHat 9). I 
> installed R-1.7.1-1.src.rpm following INSTALL. However, no X11 device is 
> found when I run R.
>  > X11()
> Error in X11() : X11 is not available
> 
> In the configuration, I found "checking for X... no"
> 
> How do I configure so that X11 device capability is installed?


Justin,

Apologies for the delay in replying as I have been away from my computer
for a few hours.

In an effort to better assist (presuming someone else has not already
begun offline), can you provide some additional information about your
system.

You are using the 'smp' Linux kernel, which suggests the possibility of
a multi-processor system.  Is this correct?  FWIW, the latest official
RH kernel series is version 2.4.20-19.9.

You have used the source RPM for R. Can you provide the exact rpmbuild
command that you used to compile the source rpm?

Also, please provide the rpm installation command that you used.

Finally, please confirm the Linux GUI that you are using, which would by
default be GNOME, but of course could be KDE or another UI. 

I am presuming that you are not running Linux purely in text console
mode?  I ask because of the possibility that an SMP box might be used as
a network server as opposed to a workstation configuration and of course
servers would be typically run in console mode to remove the GUI
overhead. This question is the equivalent of a hardware tech asking "Did
you plug it in?"  ;-)

Let me know on the above and I will do my best to assist. I have
compiled both the raw source tarball and the srpm on my (non-SMP) RH 9
system without problem (as have others) using the same gcc version.

Best regards,

Marc Schwartz



From s195404 at student.uq.edu.au  Thu Aug 14 00:36:58 2003
From: s195404 at student.uq.edu.au (Andrew C. Ward)
Date: Wed, 13 Aug 2003 22:36:58 +0000
Subject: [R] adding to Trellis plots
In-Reply-To: <BC6A439CD6835749A9C7B8D8F041DFA88B11AC@rbamsem1.emea.roche.com>
References: <BC6A439CD6835749A9C7B8D8F041DFA88B11AC@rbamsem1.emea.roche.com>
Message-ID: <1060814218.3f3abd8a41a9a@my.uq.edu.au>

Dear Dassy,

I'm not entirely sure of the specifics of what you want to
achieve, but for similar sounding things I typically make
use of the subscripts argument to panel functions. Check out
the following simple example.

   x <- rnorm(500)
   y <- 0.9*x + sqrt(1-0.9*0.9)*rnorm(500)
   g <- sample(x=LETTERS[1:3], size=500, replace=TRUE)
   a <- rnorm(500)
   tmp <- data.frame(x=x, y=y, a=a, subject=g)
   #
   xyplot(y ~ x | subject, data=tmp, panel=function
(x,y,subscripts=subscripts,m=a,...) {
      panel.xyplot(x,y,...)
      panel.abline(h=max(m[subscripts]))
   }, key=list(space="top", lines=list(lty=1), text=list
("Maximum")))


Regards,

Andrew C. Ward

CAPE Centre
Department of Chemical Engineering
The University of Queensland
Brisbane Qld 4072 Australia
andreww at cheque.uq.edu.au


Quoting "Brunschwig, Hadassa {PDMM~Basel}" 
<HADASSA.BRUNSCHWIG at Roche.COM>:

> Hello everyone
> 
> I have data grouped by subjects and i used the trellis
> plot to show the curve for each subject. I have another
> vector "a" which gives me the highest points the curves
> for each subjects can achieve ( the curve is exponential
> ). Now i want to draw a horizontal line, whose values are
> saved in "a", on each plot for each subject. Is there any
> possibility to do that? Obviously i should use something
> like panel.abline, the problem is how i can inform R that
> each value in "a" is a horizontal line for each plot.
> 
> Thanks a lot for answers.
> 
> Dassy
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
>



From d.scofield at umiami.edu  Thu Aug 14 00:28:19 2003
From: d.scofield at umiami.edu (Douglas G. Scofield)
Date: Wed, 13 Aug 2003 18:28:19 -0400
Subject: [R] means comparison with seasonal time series?
Message-ID: <002c01c361ea$36594f10$aacdab81@genetics5>

Dear R list,

I have a sequence of weekly observations of number of adults and larvae
in various size classes from a butterfly population living in a
subtropical area with pronounced wet and dry seasons.  Wet and dry
seasons are each defined 26 weeks long with fixed start and end dates.
The data span 103 weeks (two seasons each of wet and dry) with some
missing weeks.  What I would like to do is compare means of each type of
observation between wet and dry seasons ("Does the number of adults
observed vary by season?").  Not surprisingly there is pronounced
autocorrelation in the data, e.g.:

   dwtest(lm(numadults ~ week))

gives

   DW = 0.2727, p-value = < 2.2e-16

Note that the effects of this autocorrelation extend across wet-dry and
dry-wet boundaries.

What would be a way to ask my questions in R?  I'm still learning R (and
statistics) so detailed answers would be most appreciated.

Sincerely,

Douglas Scofield                 Department of Biology 
d.scofield at umiami.edu            University of Miami
off: (305) 284-3778              P.O. Box 249118
fax: (305) 284-3039              Coral Gables, FL  33124-0421



From Simon.Blomberg at anu.edu.au  Thu Aug 14 01:04:29 2003
From: Simon.Blomberg at anu.edu.au (Simon Blomberg)
Date: Thu, 14 Aug 2003 09:04:29 +1000
Subject: [R] contrasts??
Message-ID: <7A3A13F416B40842BD2C1753E044B359B099BA@CASEVS02.cas.anu.edu.au>

There is fit.contrast in the gregmisc package, which can be used for any lm,glm,aov, or lme object.

HTH,

Simon.

Simon Blomberg, PhD
Depression & Anxiety Consumer Research Unit
Centre for Mental Health Research
Australian National University
http://www.anu.edu.au/cmhr/
Simon.Blomberg at anu.edu.au  +61 (2) 6125 3379


> -----Original Message-----
> From: John Christie [mailto:jc at or.psychology.dal.ca]
> Sent: Thursday, 14 August 2003 3:15 AM
> To: r-help at stat.math.ethz.ch
> Subject: [R] contrasts??
> 
> 
> 	I was looking at the contrasts function in the search 
> for planned 
> contrast functions.  aov take a contrast variable but I don't see how 
> to use it.  Is there a planned contrast capability in aov?
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
>



From bfbraum at fas.harvard.edu  Thu Aug 14 01:18:57 2003
From: bfbraum at fas.harvard.edu (Bear F. Braumoeller)
Date: Wed, 13 Aug 2003 19:18:57 -0400
Subject: [R] New package: BOOLEAN
Message-ID: <839EA129-CDE4-11D7-8B9A-003065F6CEFA@fas.harvard.edu>

I am writing to announce the availability of a new package on CRAN:  
"boolean," which permits users to run Boolean logit and probit analyses.


BACKGROUND
The impact of independent probabilistic causal processes is often  
thought to cumulate in a manner consistent with Boolean logic --  
specifically, a logic in which the atomic elements are connected by  
"and" or "or."  For example, diet or heredity can lead to heart  
failure, apathy or indifference can lead to nonvoting, plant survival  
depends on light and water and proper soil, and so on.  This form of  
cumulation has been referred to as "causal complexity" or "multiple  
causal paths" to a given (non-)outcome (see e.g. Ragin 1987).

The boolean() command is designed for use in such situations.  It  
models the probabilities of the antecedent processes ("paths") as logit  
and probit curves, constructs a likelihood function from the logic of  
their interaction, and maximizes to obtain parameter estimates.  Any  
number of causal paths can be modeled, in any combination -- A and B  
and C cause Y, A and (B or (C and D)) cause Y, etc.  For more details  
and the derivation of the procedure see <a  
href="http://www.people.fas.harvard.edu/~bfbraum/">Braumoeller  
(2003)</a>.


PACKAGE
In addition to performing Boolean logit and probit analyses, the  
boolean package contains functions to assist the user in a variety of  
ways:

    * boolprep() simplifies input for the boolean command

    * boolfirst() graphs first differences after estimation

    * boolprof() produces profile likelihoods for diagnostics

In addition, the plot() command produces an array of graphs of first  
differences or profile likelihoods for all of the relevant coefficients  
in the model.

For more information please visit <a  
href="http://www.people.fas.harvard.edu/~bfbraum/">http:// 
www.people.fas.harvard.edu/~bfbraum/</a>


REFERENCES
Braumoeller, Bear F. (2003) "Causal Complexity and the Study of  
Politics."  Political Analysis 11(3): 209-233.
Ragin, Charles C. (1987) The Comparative Method: Moving Beyond  
Qualitative and Quantitative Strategies.  Berkeley: University of  
California Press.


<A HREF="mailto:bbraumoeller at wcfia.harvard.edu">Bear F. Braumoeller</A>
Assistant Professor
Department of Government
Harvard University



From ok at cs.otago.ac.nz  Thu Aug 14 03:04:45 2003
From: ok at cs.otago.ac.nz (Richard A. O'Keefe)
Date: Thu, 14 Aug 2003 13:04:45 +1200 (NZST)
Subject: [R] placing labels in polygon center ?
Message-ID: <200308140104.h7E14jKi082086@atlas.otago.ac.nz>

Barry Rowlingson <B.Rowlingson at lancaster.ac.uk> provided
functions PolygonArea and PolygonCenterOfMass.

As an exercise in R programming, I thought "why don't I vectorise these
and then see if it makes a practical difference".

Here are my versions of his functions.  Somehow I ended up with a sign
error when I entered his centroid code, so I had better exhibit the code
that I actually tested.

polygon.area <- function (polygon) {
    N <- dim(polygon)[1]
    area <- 0
    for (i in 1:N) {
       j <- i %% N + 1
       area <- area + polygon[i,1]*polygon[j,2] - polygon[i,2]*polygon[j,1]
   }
   abs(area/2)
}

polygon.centroid <- function(polygon) {
    N <- dim(polygon)[1]
    cx <- cy <- 0
    for (i in 1:N) {
        j <- i %% N + 1
        factor <- polygon[j,1]*polygon[i,2] - polygon[i,1]*polygon[j,2]
        cx <- cx + (polygon[i,1]+polygon[j,1])*factor
        cy <- cy + (polygon[i,2]+polygon[j,2])*factor
    }
    factor <- 1/(6*polygon.area(polygon))
    c(cx*factor, cy*factor)
}
 
Here are vectorised versions.  I found myself wishing for a function to
rotate a vector.  Is there one?  I know about ?lag, but
help.search("rotate") didn't find anything to the point.

vectorised.area <- function(polygon) {
    ix <- c(2:dim(polygon)[1], 1)
    xs <- polygon[,1] 
    ys <- polygon[,2]
    abs(sum(xs*ys[ix]) - sum(xs[ix]*ys))/2
}
 
vectorised.centroid <- function(polygon) {
    ix <- c(2:dim(polygon)[1], 1)
    xs <- polygon[,1]; xr <- xs[ix]
    ys <- polygon[,2]; yr <- ys[ix]
    factor <- xr*ys - xs*yr
    cx <- sum((xs+xr)*factor)
    cy <- sum((ys+yr)*factor)
    scale <- 3*abs(sum(xs*yr) - sum(xr*ys))
    c(cx/scale, cy/scale)
}

Test case 1: unit square.

> p <- rbind(c(0,0), c(0,1), c(1,1), c(1,0))
> polygon.area(p)
[1] 1
> vectorised.area(p)
[1] 1
> polygon.centroid(p)
[1] 0.5 0.5
> vectorised.centroid(p)
[1] 0.5 0.5
> system.time(for (i in 1:1000) polygon.area(p))
[1] 0.56 0.02 0.58 0.00 0.00
> system.time(for (i in 1:1000) vectorised.area(p))
[1] 0.22 0.03 0.25 0.00 0.00
> system.time(for (i in 1:1000) polygon.centroid(p))
[1] 1.56 0.06 1.66 0.00 0.00
> system.time(for (i in 1:1000) vectorised.centroid(p))
[1] 0.35 0.04 0.39 0.00 0.00

Even for a polygon this small, vectorising pays off.

Test case 2: random 20-gon.

> p <- cbind(runif(20), runif(20)) 
> polygon.area(p)
[1] 0.2263327
> vectorised.area(p)
[1] 0.2263327
> polygon.centroid(p)
[1] 0.6820708 0.5196700
> vectorised.centroid(p)
[1] 0.6820708 0.5196700
> system.time(for (i in 1:1000) polygon.area(p))
[1] 2.49 0.03 2.61 0.00 0.00
> system.time(for (i in 1:1000) vectorised.area(p))
[1] 0.29 0.05 0.34 0.00 0.00
> system.time(for (i in 1:1000) polygon.centroid(p))
[1] 7.29 0.07 7.70 0.00 0.00
> system.time(for (i in 1:1000) vectorised.centroid(p))
[1] 0.45 0.05 0.51 0.00 0.00

I was expecting the 20-gon version to be faster; what I did not expect
was that vectorising would pay off even for a quadrilateral.  In fact,

> p <- rbind(c(0,0), c(0,1), c(1,0))
> system.time(for (i in 1:1000) polygon.centroid(p))
[1] 1.25 0.04 1.31 0.00 0.00
> system.time(for (i in 1:1000) vectorised.centroid(p))
[1] 0.33 0.07 0.40 0.00 0.00

it even pays off for a triangle.



From kjetil at entelnet.bo  Thu Aug 14 04:30:41 2003
From: kjetil at entelnet.bo (kjetil brinchmann halvorsen)
Date: Wed, 13 Aug 2003 22:30:41 -0400
Subject: [R] A question on orthogonal basis vectors
In-Reply-To: <5.1.0.14.2.20030813104239.01f7ebd0@127.0.0.1>
References: <003c01c361a5$7188e180$8bd75ba5@IE.TAMU.EDU>
Message-ID: <3F3ABC11.25725.17E28B3@localhost>

On 13 Aug 2003 at 10:50, John Fox wrote:

> Dear Fred,
> 
> If I understand correctly what you want, the answer is not unique. Think 
> about the 3D case where you start with one vector. (I assume, by the way, 
> that you mean orthonormal and that you mean unique up to a reflection.) 
> There are infinitely many pairs of orthonormal basis vectors for the plane 
> orthogonal to the initial vector. On the other hand, picking an arbitrary 
> orthonormal basis isn't hard: The Gram-Schmidt method does this, for example.

To add to this, 
the qr decomposition is really a version of Gram-Schmidt. So if your 
basis vectors are the columns of X, you can do something like
qr.Q(qr(X), complete=TRUE)

Kjetil Halvorsen

> 
> I hope that this helps,
>   John
> 
> At 09:16 AM 8/13/2003 -0500, Feng Zhang wrote:
> >Hey, R-listers,
> >
> >I have a question about determining the orthogonal
> >basis vectors.
> >In the d-dimensinonal space, if I already know
> >the first r orthogonal basis vectors, should I be
> >able to determine the remaining d-r orthognal basis
> >vectors automatically?
> >
> >Or the answer is not unique?
> >
> >Thanks for your attention.
> >
> >Fred
> 
> -----------------------------------------------------
> John Fox
> Department of Sociology
> McMaster University
> Hamilton, Ontario, Canada L8S 4M4
> email: jfox at mcmaster.ca
> phone: 905-525-9140x23604
> web: www.socsci.mcmaster.ca/jfox
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help



From spencer.graves at pdf.com  Thu Aug 14 06:25:06 2003
From: spencer.graves at pdf.com (Spencer Graves)
Date: Wed, 13 Aug 2003 21:25:06 -0700
Subject: [R] placing labels in polygon center ?
References: <200308140104.h7E14jKi082086@atlas.otago.ac.nz>
Message-ID: <3F3B0F22.40500@pdf.com>

	  I didn't study your code, but regarding a function to rotate a 
vector:  Multiplication by an orthogonal matrix does that.  You may know 
that an orthogonal matrix is a matrix whose transpose is its inverse. 
Thus, A is orthogonal if and only if (A %*% t(A)) = identity.  One of 
the simplest orthogonal matrices is as follows:

[cos(th) | -sin(th)]
[sin(th) |  cos(th)],

for any angle "th".  More generally, with 1 <= i < j <= k, if we replace 
elements (i, i), (i, j), (j, i), (j, j) with the elements of this 2x2 
matrix, we get a k x k orthogonal matrix.  Moreover, if my memory is 
correct, I believe there is a theorem that says that any orthogonal 
matrix can be decomposed into a product of 2-dimensional rotations like 
this.

	  Therefore, if you can decompose the rotation you want into a sequence 
of 2-dimensional rotations, then you have the rotation you want.

hope this helps.
spencer graves

Richard A. O'Keefe wrote:
> Barry Rowlingson <B.Rowlingson at lancaster.ac.uk> provided
> functions PolygonArea and PolygonCenterOfMass.
> 
> As an exercise in R programming, I thought "why don't I vectorise these
> and then see if it makes a practical difference".
> 
> Here are my versions of his functions.  Somehow I ended up with a sign
> error when I entered his centroid code, so I had better exhibit the code
> that I actually tested.
> 
> polygon.area <- function (polygon) {
>     N <- dim(polygon)[1]
>     area <- 0
>     for (i in 1:N) {
>        j <- i %% N + 1
>        area <- area + polygon[i,1]*polygon[j,2] - polygon[i,2]*polygon[j,1]
>    }
>    abs(area/2)
> }
> 
> polygon.centroid <- function(polygon) {
>     N <- dim(polygon)[1]
>     cx <- cy <- 0
>     for (i in 1:N) {
>         j <- i %% N + 1
>         factor <- polygon[j,1]*polygon[i,2] - polygon[i,1]*polygon[j,2]
>         cx <- cx + (polygon[i,1]+polygon[j,1])*factor
>         cy <- cy + (polygon[i,2]+polygon[j,2])*factor
>     }
>     factor <- 1/(6*polygon.area(polygon))
>     c(cx*factor, cy*factor)
> }
>  
> Here are vectorised versions.  I found myself wishing for a function to
> rotate a vector.  Is there one?  I know about ?lag, but
> help.search("rotate") didn't find anything to the point.
> 
> vectorised.area <- function(polygon) {
>     ix <- c(2:dim(polygon)[1], 1)
>     xs <- polygon[,1] 
>     ys <- polygon[,2]
>     abs(sum(xs*ys[ix]) - sum(xs[ix]*ys))/2
> }
>  
> vectorised.centroid <- function(polygon) {
>     ix <- c(2:dim(polygon)[1], 1)
>     xs <- polygon[,1]; xr <- xs[ix]
>     ys <- polygon[,2]; yr <- ys[ix]
>     factor <- xr*ys - xs*yr
>     cx <- sum((xs+xr)*factor)
>     cy <- sum((ys+yr)*factor)
>     scale <- 3*abs(sum(xs*yr) - sum(xr*ys))
>     c(cx/scale, cy/scale)
> }
> 
> Test case 1: unit square.
> 
> 
>>p <- rbind(c(0,0), c(0,1), c(1,1), c(1,0))
>>polygon.area(p)
> 
> [1] 1
> 
>>vectorised.area(p)
> 
> [1] 1
> 
>>polygon.centroid(p)
> 
> [1] 0.5 0.5
> 
>>vectorised.centroid(p)
> 
> [1] 0.5 0.5
> 
>>system.time(for (i in 1:1000) polygon.area(p))
> 
> [1] 0.56 0.02 0.58 0.00 0.00
> 
>>system.time(for (i in 1:1000) vectorised.area(p))
> 
> [1] 0.22 0.03 0.25 0.00 0.00
> 
>>system.time(for (i in 1:1000) polygon.centroid(p))
> 
> [1] 1.56 0.06 1.66 0.00 0.00
> 
>>system.time(for (i in 1:1000) vectorised.centroid(p))
> 
> [1] 0.35 0.04 0.39 0.00 0.00
> 
> Even for a polygon this small, vectorising pays off.
> 
> Test case 2: random 20-gon.
> 
> 
>>p <- cbind(runif(20), runif(20)) 
>>polygon.area(p)
> 
> [1] 0.2263327
> 
>>vectorised.area(p)
> 
> [1] 0.2263327
> 
>>polygon.centroid(p)
> 
> [1] 0.6820708 0.5196700
> 
>>vectorised.centroid(p)
> 
> [1] 0.6820708 0.5196700
> 
>>system.time(for (i in 1:1000) polygon.area(p))
> 
> [1] 2.49 0.03 2.61 0.00 0.00
> 
>>system.time(for (i in 1:1000) vectorised.area(p))
> 
> [1] 0.29 0.05 0.34 0.00 0.00
> 
>>system.time(for (i in 1:1000) polygon.centroid(p))
> 
> [1] 7.29 0.07 7.70 0.00 0.00
> 
>>system.time(for (i in 1:1000) vectorised.centroid(p))
> 
> [1] 0.45 0.05 0.51 0.00 0.00
> 
> I was expecting the 20-gon version to be faster; what I did not expect
> was that vectorising would pay off even for a quadrilateral.  In fact,
> 
> 
>>p <- rbind(c(0,0), c(0,1), c(1,0))
>>system.time(for (i in 1:1000) polygon.centroid(p))
> 
> [1] 1.25 0.04 1.31 0.00 0.00
> 
>>system.time(for (i in 1:1000) vectorised.centroid(p))
> 
> [1] 0.33 0.07 0.40 0.00 0.00
> 
> it even pays off for a triangle.
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help



From deepayan at stat.wisc.edu  Thu Aug 14 06:49:16 2003
From: deepayan at stat.wisc.edu (Deepayan Sarkar)
Date: Wed, 13 Aug 2003 23:49:16 -0500
Subject: [R] adding to Trellis plots
In-Reply-To: <BC6A439CD6835749A9C7B8D8F041DFA88B11AC@rbamsem1.emea.roche.com>
References: <BC6A439CD6835749A9C7B8D8F041DFA88B11AC@rbamsem1.emea.roche.com>
Message-ID: <200308132349.16363.deepayan@stat.wisc.edu>

On Wednesday 13 August 2003 07:43, Brunschwig, Hadassa {PDMM~Basel} wrote:
> Hello everyone
>
> I have data grouped by subjects and i used the trellis plot to show the
> curve for each subject. I have another vector "a" which gives me the
> highest points the curves for each subjects can achieve ( the curve is
> exponential ). Now i want to draw a horizontal line, whose values are saved
> in "a", on each plot for each subject. Is there any possibility to do that?
> Obviously i should use something like panel.abline, the problem is how i
> can inform R that each value in "a" is a horizontal line for each plot.

Some more clarification on your data structure would have helped. Is your 
variable "a" a part of the data frame (with the same value repeated for all 
rows corresponding to each subject), or is it a separate, shorter vector, 
with length equal to the number of subjects ?

If the former, then you should use subscripts, as others have already 
suggested, perhaps as:

xyplot(y ~ x | subject,  data = foo,
       panel = function(x, y, subscripts, ...) {
           panel.xyplot(x, y, ...)
           panel.abline(h=foo$a[subscripts][1], ...)
       })

In the second case, you could do something similar to determine (inside the 
panel function) which element of "a" you need to use (assuming the grouping 
variable "subject" is a factor):

           panel.abline(h = a[as.numeric(foo$subject)[subscripts][1]], ...)

(assuming that the grouping variable "subject" is a factor)

Alternatively, you could probably also get away with

xyplot(y ~ x | subject, data = foo,
       panel = function(x, y, panel.number, ...) {
           panel.xyplot(x, y, ...)
           panel.abline(h=a[panel.number], ...)
       })

Deepayan



From ok at cs.otago.ac.nz  Thu Aug 14 07:27:54 2003
From: ok at cs.otago.ac.nz (Richard A. O'Keefe)
Date: Thu, 14 Aug 2003 17:27:54 +1200 (NZST)
Subject: [R] placing labels in polygon center ?
Message-ID: <200308140527.h7E5Rsqq434364@atlas.otago.ac.nz>

I wrote:
	I found myself wishing for a function to rotate a vector.
	Is there one?  I know about ?lag, but help.search("rotate")
	didn't find anything to the point.

Here I was regarding a vector as a _sequence_.
The (one-step) rotation of c(u,v,w,x,y,z) is c(v,w,x,y,z,u).
This is pretty much the way APL uses the word "rotate" (the
vertical-bar-overstruck-with-a-circle operator).

Spencer Graves <spencer.graves at PDF.COM> replied:
	I didn't study your code, but regarding a function to rotate a 
	vector:  Multiplication by an orthogonal matrix does that.

This is a misunderstanding.  We were both using the word "rotate" in
a standard way, the problem is that it has more than one "standard" meaning.

As a matter of fact,
	/ 0 1 0 0 \  / u \     / v \
	| 0 0 1 0 |  | v |     | w |
	| 0 0 0 1 |  | w |  =  | x |
	\ 1 0 0 0 /  \ x /     \ u /
so you *can* do the kind of rotation I want using a matrix multiplication,
and this is mathematically useful; it's just not a very good way to do
it in a computer.



From mkondrin at hppi.troitsk.ru  Thu Aug 14 19:55:25 2003
From: mkondrin at hppi.troitsk.ru (M.Kondrin)
Date: Thu, 14 Aug 2003 10:55:25 -0700
Subject: [R] X11 not configured on linux RedHat 9
In-Reply-To: <3F3A7AFE.6020208@genetics.wustl.edu>
References: <3F3A7AFE.6020208@genetics.wustl.edu>
Message-ID: <3F3BCD0D.6050003@hppi.troitsk.ru>

Justin Fay wrote:
> I'm using Linux version 2.4.20-8smp (gcc version 3.2.2, RedHat 9). I 
> installed R-1.7.1-1.src.rpm following INSTALL. However, no X11 device is 
> found when I run R.
>  > X11()
> Error in X11() : X11 is not available
> 
> In the configuration, I found "checking for X... no"
> 
> How do I configure so that X11 device capability is installed?

"checking for X... no" - generally means that configure script can not 
find X11 headers files. In most cases this problem is fixed by 
installing X11-devel package and running configure again. Or use 
precompiled packages.



From petr.pikal at precheza.cz  Thu Aug 14 08:55:49 2003
From: petr.pikal at precheza.cz (Petr Pikal)
Date: Thu, 14 Aug 2003 08:55:49 +0200
Subject: [R] Contour plot for arbitrary (x,y,z)
In-Reply-To: <3F3A90A3.5000300@cornell.edu>
Message-ID: <3F3B4E95.19881.53E9EF@localhost>

Hi

On 13 Aug 2003 at 12:25, Ronnen Levinson wrote:

> Hello.
> 
> Is there an easy-to-use contour plot function analogous to
> scatterplot3d that can draw handle a dataset of arbitrary (x,y,z)
> triplets? That is, say x, y, and z are each measured quanties, and
> exhibit neither order nor regularity.

maybe

library(akima)
#provided you do not have missing data

contour(interp(x,y,z))

if some of your data are missing, you have to filter it out e.g. by complete.cases()

> 
> I looked at the lattice package function "contourplot" but it seems
> complicated, and it's not clear from the documentation whether it can
> handle arbitrary (x,y) values. I'm looking for something like the base
> package "contour" function that does not make any assumptions about
> (x,y,z).
> 
> Thanks,
> 
> Ronnen.
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help

CheersPetr Pikal
petr.pikal at precheza.cz
p.pik at volny.cz



From angel_lul at hotmail.com  Thu Aug 14 09:47:49 2003
From: angel_lul at hotmail.com (Angel)
Date: Thu, 14 Aug 2003 09:47:49 +0200
Subject: [R] putting NAs at the end
References: <Law11-OE71XE2sCRBHZ0001f54c@hotmail.com>
	<3F3A3FF8.6040509@pdf.com>
Message-ID: <Law11-OE67WumTsUW1O00020c0a@hotmail.com>

Yes, I have. I am sorry if I am missing some very basic stuff, but both
order and sort will not only put the NAs at the end (with na.last=TRUE)  BUT
also sort in ascending or descending order the rest of the elements and that
is not what I want. And with order I would only get the z NAs at the end and
not also the associated x and y coordinates.
Thanks for any further help.
Angel
----- Original Message -----
From: "Spencer Graves" <spencer.graves at PDF.COM>
To: "Angel" <angel_lul at hotmail.com>
Cc: <r-help at stat.math.ethz.ch>
Sent: Wednesday, August 13, 2003 3:41 PM
Subject: Re: [R] putting NAs at the end


> Have you considered "?order"?
>
> Spencer Graves
>
> Angel wrote:
> > I have a matrix for which each row has 12 elements that represent the
xyz coordinates of 4 points.
> > So each row of M is (x1,y1,z1,x2,y2,z2,x3,y3,z3,x4,y4,z4). Some points
have NA as z values.
> > I want another matrix to be the same as M but with the coordinates of
those points with z=NA placed last.
> > For ezample if z1=NA then the new matrix row should be
(x2,y2,z2,x3,y3,z3,x4,y4,z4,x1,y1,z1)
> > I've tried writing a function that does the job for each row and then
apply to the matrix
> >
> > Put.NaN.last<-function(p) {
> > Index<-c(which(!is.na(p[c(3,6,9,12)]))*3,which(is.na(p[c(3,6,9,12)]))*3)
> > p<-c(p[Index[1]-2],p[Index[1]-1],p[Index[1]],
> > p[Index[2]-2],p[Index[2]-1],p[Index[2]],
> > p[Index[3]-2],p[Index[3]-1],p[Index[3]],
> > p[Index[4]-2],p[Index[4]-1],p[Index[4]])
> > return(p)
> > }
> > A<-matrix(1:36,ncol=12)
> > A[c(7,9,17,36)]<-NA
> > A<-t(apply(A,1,Put.NaN.last))
> >
> > but it is awfully slow.
> > Any suggestions on how to do this faster?
> > Thanks
> > Angel
> >
> > [[alternative HTML version deleted]]
> >
> > ______________________________________________
> > R-help at stat.math.ethz.ch mailing list
> > https://www.stat.math.ethz.ch/mailman/listinfo/r-help
>
>
>



From dave at evocapital.com  Thu Aug 14 09:52:01 2003
From: dave at evocapital.com (David Khabie-Zeitoune)
Date: Thu, 14 Aug 2003 08:52:01 +0100
Subject: [R] Startup for RExcel
Message-ID: <8D0F30FE2EB3314182D4A33F738BB19D063EEC@sqlsrvr.evocapital.com>

Hi

When I start an instance of an R stats server using the R(D)COM RExcel
Addin by Thomas Baier, it appears to start the server in a temporary
directory. Is there any way I can change the directory in which the
server starts in so that I can, for example, take advantage of Rprofile
or Renviron files I may have in a particular directory?

Thanks

David



From Ted.Harding at nessie.mcc.ac.uk  Thu Aug 14 11:20:20 2003
From: Ted.Harding at nessie.mcc.ac.uk ( (Ted Harding))
Date: Thu, 14 Aug 2003 10:20:20 +0100 (BST)
Subject: [R] putting NAs at the end
In-Reply-To: <Law11-OE67WumTsUW1O00020c0a@hotmail.com>
Message-ID: <XFMail.030814102020.Ted.Harding@nessie.mcc.ac.uk>

Hi Angel,
On 14-Aug-03 Angel wrote:
> Yes, I have. I am sorry if I am missing some very basic stuff, but both
> order and sort will not only put the NAs at the end (with na.last=TRUE)
> BUT also sort in ascending or descending order the rest of the
> elements and that is not what I want. And with order I would only get
> the z NAs at the end and not also the associated x and y coordinates.

This may not be the best way to do it, but it works and, I think, does
what you want (and, in particular, preserves the original row order
within the NA and non-NA blocks):

Setup: First, a matrix x:
> x
     [,1] [,2] [,3] [,4] [,5] [,6] [,7] [,8] [,9] [,10] [,11] [,12]
[1,] 11.1 11.2 11.3 12.1 12.2 12.3 13.1 13.2 13.3  14.1  14.2  14.3
[2,] 21.1 21.2 21.3 22.1 22.2 22.3 23.1 23.2 23.3  24.1  24.2  24.3
[3,] 31.1 31.2 31.3 32.1 32.2 32.3 33.1 33.2 33.3  34.1  34.2  34.3
[4,] 41.1 41.2 41.3 42.1 42.2 42.3 43.1 43.2 43.3  44.1  44.2  44.3
[5,] 51.1 51.2 51.3 52.1 52.2 52.3 53.1 53.2 53.3  54.1  54.2  54.3
[6,] 61.1 61.2 61.3 62.1 62.2 62.3 63.1 63.2 63.3  64.1  64.2  64.3

Setup: Next, set some NAs:
> x1<-x;x1[2,3]<-NA;x1[2,9]<-NA;x1[5,6]<-NA;x1[5,12]<-NA;x1
     [,1] [,2] [,3] [,4] [,5] [,6] [,7] [,8] [,9] [,10] [,11] [,12]
[1,] 11.1 11.2 11.3 12.1 12.2 12.3 13.1 13.2 13.3  14.1  14.2  14.3
[2,] 21.1 21.2   NA 22.1 22.2 22.3 23.1 23.2   NA  24.1  24.2  24.3
[3,] 31.1 31.2 31.3 32.1 32.2 32.3 33.1 33.2 33.3  34.1  34.2  34.3
[4,] 41.1 41.2 41.3 42.1 42.2 42.3 43.1 43.2 43.3  44.1  44.2  44.3
[5,] 51.1 51.2 51.3 52.1 52.2   NA 53.1 53.2 53.3  54.1  54.2    NA
[6,] 61.1 61.2 61.3 62.1 62.2 62.3 63.1 63.2 63.3  64.1  64.2  64.3

The work: Now you want rows 2 and 5 at the end (if I understand):
> icol<-c(3,6,9,12)
> iNA<-is.na(rowSums(x1[,icol]))
> x2<-x1[!iNA,]
> x3<-x1[iNA,]
> x4<-rbind(x2,x3);x4
     [,1] [,2] [,3] [,4] [,5] [,6] [,7] [,8] [,9] [,10] [,11] [,12]
[1,] 11.1 11.2 11.3 12.1 12.2 12.3 13.1 13.2 13.3  14.1  14.2  14.3
[2,] 31.1 31.2 31.3 32.1 32.2 32.3 33.1 33.2 33.3  34.1  34.2  34.3
[3,] 41.1 41.2 41.3 42.1 42.2 42.3 43.1 43.2 43.3  44.1  44.2  44.3
[4,] 61.1 61.2 61.3 62.1 62.2 62.3 63.1 63.2 63.3  64.1  64.2  64.3
[5,] 21.1 21.2   NA 22.1 22.2 22.3 23.1 23.2   NA  24.1  24.2  24.3
[6,] 51.1 51.2 51.3 52.1 52.2   NA 53.1 53.2 53.3  54.1  54.2    NA

The above five lines of code could make a function for your task.
Is this OK?
Best wishes,
Ted.


--------------------------------------------------------------------
E-Mail: (Ted Harding) <Ted.Harding at nessie.mcc.ac.uk>
Fax-to-email: +44 (0)870 167 1972
Date: 14-Aug-03                                       Time: 10:20:20
------------------------------ XFMail ------------------------------



From angel_lul at hotmail.com  Thu Aug 14 12:18:51 2003
From: angel_lul at hotmail.com (Angel)
Date: Thu, 14 Aug 2003 12:18:51 +0200
Subject: [R] putting NAs at the end
References: <XFMail.030814102020.Ted.Harding@nessie.mcc.ac.uk>
Message-ID: <Law11-OE54FFUr9An9e00021174@hotmail.com>

I'm afraid that's not what I wanted. Let's see if taking your example data I
can explain better.
Starting from:
>      [,1] [,2] [,3] [,4] [,5] [,6] [,7] [,8] [,9] [,10] [,11] [,12]
> [1,] 11.1 11.2 11.3 12.1 12.2 12.3 13.1 13.2 13.3  14.1  14.2  14.3
> [2,] 21.1 21.2   NA 22.1 22.2 22.3 23.1 23.2   NA  24.1  24.2  24.3
> [3,] 31.1 31.2 31.3 32.1 32.2 32.3 33.1 33.2 33.3  34.1  34.2  34.3
> [4,] 41.1 41.2 41.3 42.1 42.2 42.3 43.1 43.2 43.3  44.1  44.2  44.3
> [5,] 51.1 51.2 51.3 52.1 52.2   NA 53.1 53.2 53.3  54.1  54.2    NA
> [6,] 61.1 61.2 61.3 62.1 62.2 62.3 63.1 63.2 63.3  64.1  64.2  64.3
I want to arrive to:
>      [,1] [,2] [,3] [,4] [,5] [,6] [,7] [,8] [,9] [,10] [,11] [,12]
> [1,] 11.1 11.2 11.3 12.1 12.2 12.3 13.1 13.2 13.3  14.1  14.2  14.3
> [2,] 22.1 22.2 22.3 24.1  24.2  24.3 21.1 21.2   NA 23.1 23.2   NA
> [3,] 31.1 31.2 31.3 32.1 32.2 32.3 33.1 33.2 33.3  34.1  34.2  34.3
> [4,] 41.1 41.2 41.3 42.1 42.2 42.3 43.1 43.2 43.3  44.1  44.2  44.3
> [5,] 51.1 51.2 51.3 53.1 53.2 53.3  52.1 52.2   NA 54.1  54.2    NA
> [6,] 61.1 61.2 61.3 62.1 62.2 62.3 63.1 63.2 63.3  64.1  64.2  64.3
That is, for the second row the elements in columns 1:3 and 7:9 have moved
to the columns 7:9 and 10:12 and the rest of elements that were in 4:6 and
10:12 have moved to columns 1:3 and 4:6 respectively.
For row 5 the elements in columns 4:6 have moved to 7:9 and elements in
10:12 have "moved" to 10:12 and the elements in columns 7:9 take the columsn
4:6
For the rest of rows all the elements remain in the same position.
This is what my (slow) function does in a very rudimentary form:
[original post added]:
----
I have a matrix for which each row has 12 elements that represent the xyz
coordinates of 4 points.
So each row of M is (x1,y1,z1,x2,y2,z2,x3,y3,z3,x4,y4,z4). Some points have
NA as z values.
I want another matrix to be the same as M but with the coordinates of those
points with z=NA placed last.
For ezample if z1=NA then the new matrix row should be
(x2,y2,z2,x3,y3,z3,x4,y4,z4,x1,y1,z1)
I've tried writing a function that does the job for each row and then apply
to the matrix

Put.NaN.last<-function(p) {
Index<-c(which(!is.na(p[c(3,6,9,12)]))*3,which(is.na(p[c(3,6,9,12)]))*3)
p<-c(p[Index[1]-2],p[Index[1]-1],p[Index[1]],
p[Index[2]-2],p[Index[2]-1],p[Index[2]],
p[Index[3]-2],p[Index[3]-1],p[Index[3]],
p[Index[4]-2],p[Index[4]-1],p[Index[4]])
return(p)
}
A<-matrix(1:36,ncol=12)
A[c(7,9,17,36)]<-NA
A<-t(apply(A,1,Put.NaN.last))

but it is awfully slow.
Any suggestions on how to do this faster?
Thanks
Angel
----

Hope it is clearer now. Thanks,
Angel

----- Original Message -----
From: "Ted Harding" <Ted.Harding at nessie.mcc.ac.uk>
To: "Angel" <angel_lul at hotmail.com>
Cc: <r-help at stat.math.ethz.ch>
Sent: Thursday, August 14, 2003 11:20 AM
Subject: Re: [R] putting NAs at the end


> Hi Angel,
> On 14-Aug-03 Angel wrote:
> > Yes, I have. I am sorry if I am missing some very basic stuff, but both
> > order and sort will not only put the NAs at the end (with na.last=TRUE)
> > BUT also sort in ascending or descending order the rest of the
> > elements and that is not what I want. And with order I would only get
> > the z NAs at the end and not also the associated x and y coordinates.
>
> This may not be the best way to do it, but it works and, I think, does
> what you want (and, in particular, preserves the original row order
> within the NA and non-NA blocks):
>
> Setup: First, a matrix x:
> > x
>      [,1] [,2] [,3] [,4] [,5] [,6] [,7] [,8] [,9] [,10] [,11] [,12]
> [1,] 11.1 11.2 11.3 12.1 12.2 12.3 13.1 13.2 13.3  14.1  14.2  14.3
> [2,] 21.1 21.2 21.3 22.1 22.2 22.3 23.1 23.2 23.3  24.1  24.2  24.3
> [3,] 31.1 31.2 31.3 32.1 32.2 32.3 33.1 33.2 33.3  34.1  34.2  34.3
> [4,] 41.1 41.2 41.3 42.1 42.2 42.3 43.1 43.2 43.3  44.1  44.2  44.3
> [5,] 51.1 51.2 51.3 52.1 52.2 52.3 53.1 53.2 53.3  54.1  54.2  54.3
> [6,] 61.1 61.2 61.3 62.1 62.2 62.3 63.1 63.2 63.3  64.1  64.2  64.3
>
> Setup: Next, set some NAs:
> > x1<-x;x1[2,3]<-NA;x1[2,9]<-NA;x1[5,6]<-NA;x1[5,12]<-NA;x1
>      [,1] [,2] [,3] [,4] [,5] [,6] [,7] [,8] [,9] [,10] [,11] [,12]
> [1,] 11.1 11.2 11.3 12.1 12.2 12.3 13.1 13.2 13.3  14.1  14.2  14.3
> [2,] 21.1 21.2   NA 22.1 22.2 22.3 23.1 23.2   NA  24.1  24.2  24.3
> [3,] 31.1 31.2 31.3 32.1 32.2 32.3 33.1 33.2 33.3  34.1  34.2  34.3
> [4,] 41.1 41.2 41.3 42.1 42.2 42.3 43.1 43.2 43.3  44.1  44.2  44.3
> [5,] 51.1 51.2 51.3 52.1 52.2   NA 53.1 53.2 53.3  54.1  54.2    NA
> [6,] 61.1 61.2 61.3 62.1 62.2 62.3 63.1 63.2 63.3  64.1  64.2  64.3
>
> The work: Now you want rows 2 and 5 at the end (if I understand):
> > icol<-c(3,6,9,12)
> > iNA<-is.na(rowSums(x1[,icol]))
> > x2<-x1[!iNA,]
> > x3<-x1[iNA,]
> > x4<-rbind(x2,x3);x4
>      [,1] [,2] [,3] [,4] [,5] [,6] [,7] [,8] [,9] [,10] [,11] [,12]
> [1,] 11.1 11.2 11.3 12.1 12.2 12.3 13.1 13.2 13.3  14.1  14.2  14.3
> [2,] 31.1 31.2 31.3 32.1 32.2 32.3 33.1 33.2 33.3  34.1  34.2  34.3
> [3,] 41.1 41.2 41.3 42.1 42.2 42.3 43.1 43.2 43.3  44.1  44.2  44.3
> [4,] 61.1 61.2 61.3 62.1 62.2 62.3 63.1 63.2 63.3  64.1  64.2  64.3
> [5,] 21.1 21.2   NA 22.1 22.2 22.3 23.1 23.2   NA  24.1  24.2  24.3
> [6,] 51.1 51.2 51.3 52.1 52.2   NA 53.1 53.2 53.3  54.1  54.2    NA
>
> The above five lines of code could make a function for your task.
> Is this OK?
> Best wishes,
> Ted.
>
>
> --------------------------------------------------------------------
> E-Mail: (Ted Harding) <Ted.Harding at nessie.mcc.ac.uk>
> Fax-to-email: +44 (0)870 167 1972
> Date: 14-Aug-03                                       Time: 10:20:20
> ------------------------------ XFMail ------------------------------
>



From petr.pikal at precheza.cz  Thu Aug 14 12:56:43 2003
From: petr.pikal at precheza.cz (Petr Pikal)
Date: Thu, 14 Aug 2003 12:56:43 +0200
Subject: [R] putting NAs at the end
In-Reply-To: <Law11-OE67WumTsUW1O00020c0a@hotmail.com>
Message-ID: <3F3B870B.4613.1306E34@localhost>

Hi

On 14 Aug 2003 at 9:47, Angel wrote:

> Yes, I have. I am sorry if I am missing some very basic stuff, but
> both order and sort will not only put the NAs at the end (with
> na.last=TRUE)  BUT also sort in ascending or descending order the rest
> of the elements and that is not what I want. And with order I would
> only get the z NAs at the end and not also the associated x and y
> coordinates. Thanks for any further help. Angel ----- Original Message
> ----- From: "Spencer Graves" <spencer.graves at PDF.COM> To: "Angel"
> <angel_lul at hotmail.com> Cc: <r-help at stat.math.ethz.ch> Sent:
> Wednesday, August 13, 2003 3:41 PM Subject: Re: [R] putting NAs at the
> end
> 

If I understand correctly you want something like

#making an index for four series
> serie
 [1] 1 1 1 2 2 2 3 3 3 4 4 4

#your matrix
> A
     [,1] [,2] [,3] [,4] [,5] [,6] [,7] [,8] [,9] [,10] [,11] [,12]
[1,]    1    4   NA   10   13   16   19   22   25    28    31    34
[2,]    2    5    8   11   14   NA   20   23   26    29    32    35
[3,]    3    6   NA   12   15   18   21   24   27    30    33    NA

#initialization of ordered matrix
> A.O<-A

#filling A.O matrix with an apropriate ordered rows

> for (i in 1:3) A.O[i,]<-A[i,][order(serie%in%(which(is.na(A[i,]))/3))]

#result
> A.O
     [,1] [,2] [,3] [,4] [,5] [,6] [,7] [,8] [,9] [,10] [,11] [,12]
[1,]   10   13   16   19   22   25   28   31   34     1     4    NA
[2,]    2    5    8   20   23   26   29   32   35    11    14    NA
[3,]   12   15   18   21   24   27    3    6   NA    30    33    NA
>
However I am not sure about performance.


> 
> > Have you considered "?order"?
> >
> > Spencer Graves
> >
> > Angel wrote:
> > > I have a matrix for which each row has 12 elements that represent
> > > the
> xyz coordinates of 4 points.
> > > So each row of M is (x1,y1,z1,x2,y2,z2,x3,y3,z3,x4,y4,z4). Some
> > > points
> have NA as z values.
> > > I want another matrix to be the same as M but with the coordinates
> > > of
> those points with z=NA placed last.
> > > For ezample if z1=NA then the new matrix row should be
> (x2,y2,z2,x3,y3,z3,x4,y4,z4,x1,y1,z1)
> > > I've tried writing a function that does the job for each row and
> > > then
> apply to the matrix
> > >
> > > Put.NaN.last<-function(p) {
> > > Index<-c(which(!is.na(p[c(3,6,9,12)]))*3,which(is.na(p[c(3,6,9,12)
> > > ]))*3) p<-c(p[Index[1]-2],p[Index[1]-1],p[Index[1]],
> > > p[Index[2]-2],p[Index[2]-1],p[Index[2]],
> > > p[Index[3]-2],p[Index[3]-1],p[Index[3]],
> > > p[Index[4]-2],p[Index[4]-1],p[Index[4]]) return(p)
> > > }
> > > A<-matrix(1:36,ncol=12)
> > > A[c(7,9,17,36)]<-NA
> > > A<-t(apply(A,1,Put.NaN.last))
> > >
> > > but it is awfully slow.
> > > Any suggestions on how to do this faster?
> > > Thanks
> > > Angel
> > >
> > > [[alternative HTML version deleted]]
> > >
> > > ______________________________________________
> > > R-help at stat.math.ethz.ch mailing list
> > > https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> >
> >
> >
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help

Cheers
Petr Pikal
petr.pikal at precheza.cz



From chris1 at psyctc.org  Thu Aug 14 14:06:21 2003
From: chris1 at psyctc.org (Chris Evans)
Date: Thu, 14 Aug 2003 13:06:21 +0100
Subject: [R] Rcgi
In-Reply-To: <3F2815EC.8003.5674F7D@localhost>
Message-ID: <3F3B894D.9214.335D0DC@localhost>

On 30 Jul 2003 at 19:01, Chris Evans wrote:

> I am keen to look at Rcgi as I want to put up some simple bits of R to
> do prescribed tasks on HTML form input.  Rweb is overkill and
> worryingly flexible for what I want and it sounds as if Rcgi is more
> what I need.  However, I can't get any of the URLs I've found for it
> to work over the last few days.  
> 
> Does anyone have a recent copy they could Email me or a working URL
> for it?

Replying to myself to have this documented in the list archives.  I 
had found the pointer to RCGI from the R-FAQ:
http://www.ci.tuwien.ac.at/~hornik/R/R-FAQ.html#R%20Web%20Interfaces

However, http://stats.mth.uea.ac.uk/Rcgi/ remains offline and Email 
to Mark J. Ray bounces and I see that someone was asking if the 
project was dead back in 2001 so I guess it should be removed from 
the FAQ and I've Emailed Kurt Hornik to suggest that he changes that 
section of the FAQ to remove Rcgi and add the excellent CGIwithR 
package which does just what I want.  Thanks to David Firth for 
writing it and to both he and Adelchi Azzalini for pointing it out to 
me ... and to everyone on the R enterprise for this wonderful tool.

Trivial, less impressive than Adelchi Azzalini's example:    
http://tango.stat.unipd.it/SN/sn-fit.html
but I am underway with the sorts of things I wanted to do:
   http://www.psyctc.org/stats/R/binconf1.html
   http://www.psyctc.org/stats/R/binconf2.html

One afterthought: does anyone have any ideas about how to secure 
GGIwithR to prevent some idiot simply hitting repeatedly with a small 
script to get a DoS on any server running it?  I suspect there may be 
throtting settings I can use in Apache or even calls to the system 
inside R to check how many instances of it are running ... but I 
suspect that others are cleverer than I am at this sort of thing .... 
or just less paranoid?!

Chris
-- 
Chris Evans <chris at psyctc.org>
Consultant Psychiatrist in Psychotherapy, Rampton Hospital; 
Forensic Research Programme Director, Nottinghamshire NHS Trust, 
Research Consultant, Tavistock & Portman NHS Trust; 
Hon. SL Institute of Psychiatry
*** My views are my own and not representative 
of those institutions ***



From kris.nackaerts at agr.kuleuven.ac.be  Thu Aug 14 14:09:34 2003
From: kris.nackaerts at agr.kuleuven.ac.be (Kris Nackaerts)
Date: Thu, 14 Aug 2003 14:09:34 +0200
Subject: [R] Contouring irregular xyz data via TIN
In-Reply-To: <Pine.OSF.4.31.0308141158570.16969-100000@ardilla.rz-berlin.mpg.de>
References: <Pine.OSF.4.31.0308141158570.16969-100000@ardilla.rz-berlin.mpg.de>
Message-ID: <3F3B7BFE.6060409@agr.kuleuven.ac.be>

Dear,

I have XYZ data available in a MySQL database. I get it out, can plot 
the data with the plot() function, load it into a geoR datastructure. 
But what I actually would like to do is a simple contouring of the data 
based on a no Kriging interpolation such as TIN based.

I know the first thing I shold do is interpolate a full matrix for the 
region I have my points for, then contour should do its work. Any idea 
which function can be used for XYZ interpolations, maybe/hopefully based 
on a Delaunay triangulation in combination with linear or spline 
interpolation. Is there a package that allows me to do this?

Kris


-- 
------------------------------------------------------------------------
 
 http://perswww.kuleuven.ac.be/~u0027178/VCard/mycard.php?name=krisn

 http://gloveg.kuleuven.ac.be
 
------------------------------------------------------------------------
 Minds are like parachutes, they only work when open



From dmurdoch at pair.com  Thu Aug 14 14:20:53 2003
From: dmurdoch at pair.com (Duncan Murdoch)
Date: Thu, 14 Aug 2003 08:20:53 -0400
Subject: [R] putting NAs at the end
In-Reply-To: <Law11-OE67WumTsUW1O00020c0a@hotmail.com>
References: <Law11-OE71XE2sCRBHZ0001f54c@hotmail.com>
	<3F3A3FF8.6040509@pdf.com>
	<Law11-OE67WumTsUW1O00020c0a@hotmail.com>
Message-ID: <jfvmjvk7knjh8ujdpvmr4bg0pchghnrp5s@4ax.com>

On Thu, 14 Aug 2003 09:47:49 +0200, you wrote:

>Yes, I have. I am sorry if I am missing some very basic stuff, but both
>order and sort will not only put the NAs at the end (with na.last=TRUE)  BUT
>also sort in ascending or descending order the rest of the elements and that
>is not what I want. 

Just use order on a version of the data where all non-NA values are
made the same.  For example,

> x <- c(3, 1, NA, 2, 4)
> o <- order(is.na(x))
> x[o]
[1]  3  1  2  4 NA

As recently pointed out by Brian Ripley, order() performs a "stable"
sort, so items that are tied stay in their original ordering.

Duncan Murdoch



From B.Rowlingson at lancaster.ac.uk  Thu Aug 14 14:22:06 2003
From: B.Rowlingson at lancaster.ac.uk (Barry Rowlingson)
Date: Thu, 14 Aug 2003 13:22:06 +0100
Subject: [R] placing labels in polygon center ?
In-Reply-To: <200308140527.h7E5Rsqq434364@atlas.otago.ac.nz>
References: <200308140527.h7E5Rsqq434364@atlas.otago.ac.nz>
Message-ID: <3F3B7EEE.5020502@lancaster.ac.uk>

Richard A. O'Keefe wrote:
> I wrote:
> 	I found myself wishing for a function to rotate a vector.
> 	Is there one?  I know about ?lag, but help.search("rotate")
> 	didn't find anything to the point.
> 
> Here I was regarding a vector as a _sequence_.
> The (one-step) rotation of c(u,v,w,x,y,z) is c(v,w,x,y,z,u).
> This is pretty much the way APL uses the word "rotate" (the
> vertical-bar-overstruck-with-a-circle operator).
>

  Do you want:

   c(x[-1],x[1])  for a one-step 'rotation'?

  You could write a function:

function(x,n){
   c(x[-(1:n)],x[1:n])
  }

 > rotVec(1:10,5)
  [1]  6  7  8  9 10  1  2  3  4  5

  except this fails for n=0 or n>length(x). Ah.

function(x,n){
   n <- 1+(n-1)%%length(x)
   c(x[-(1:n)],x[1:n])}
  }

  seems to work. Even for negative n:

 > rotVec(1:10,-1)
  [1] 10  1  2  3  4  5  6  7  8  9

Baz



From portillae at marlab.ac.uk  Thu Aug 14 15:26:45 2003
From: portillae at marlab.ac.uk (Enrique Portilla)
Date: Thu, 14 Aug 2003 13:26:45 -0000
Subject: [R] nls confidence intervals
Message-ID: <1060867666.17800.13.camel@pclnx3.marlab.ac.uk>

Hi, 
Does anyone know how to compute the confidence prediction intervals for
a nonlinear least squares models (nls)?

I was trying to use the function 'predict' as I usually do for other
models fitting (glm, lm, gams...), but it seems that se.fit, and
interval computation is not implemented for the nls...

Cheers

Enrique

~~~~~~~~~~~~~~~~~~~~~~~~~~~
Fisheries Research Services,
Marine Laboratory,
Victoria Road,
Torry,
Aberdeen, UK.
Tel. 44 (0) 1224 295314
~~~~~~~~~~~~~~~~~~~~~~~~~~~



From bibby at dina.kvl.dk  Thu Aug 14 16:20:24 2003
From: bibby at dina.kvl.dk (Bo Martin Bibby)
Date: Thu, 14 Aug 2003 16:20:24 +0200 (CEST)
Subject: [R] Error in augPred ?
Message-ID: <Pine.LNX.4.44.0308141609540.28005-100000@gandalf.dina.kvl.dk>


  Hi,

The following example produced an error message from augPred:

library(nlme)
test <- data.frame(A=rep(c(1,2),each=8),B=rep(1:4,each=4),x=rep(1:4,4),
                   y=c(0.2, 1.6, 1.9, 3.0, 0.1, 0.9, 1.5, 2.6, 1.4,
                       2.3, 2.9, 3.4, 0.8, 1.5, 2.1, 2.4))

test$A <- factor(test$A,levels=1:2)
test$B <- factor(test$B,levels=1:4)

testgd <- groupedData(y~x|B,data=test)

testgd.nlme <- nlme(y ~ asym/(1+exp((xmid-log(x))/scal)),
                    data = testgd,
                    fixed = asym + xmid + scal ~ A,
                    random = xmid ~ 1,
                    start = list(fixed = c(5,0,1,0,0.7,0)))
augPred(testgd.nlme)

Error in predict.nlme(object, value[1:(nrow(value)/nL), , drop = FALSE],:
        Levels 1,2 not allowed for A

The problem was described in a mail in August 2001 but I have seen no
solution. There is no problem in S+.

Any suggestions?

Bo

         _
platform i386-pc-linux-gnu
arch     i386
os       linux-gnu
system   i386, linux-gnu
status
major    1
minor    7.1
year     2003
month    06
day      16
language R

Package: nlme
Version: 3.1-44
Date: 2003/08/09
Priority: recommended
Title: Linear and nonlinear mixed effects models
Author: Jose Pinheiro <Jose.Pinheiro at pharma.novartis.com>, Douglas
        Bates <bates at stat.wisc.edu>, Saikat DebRoy
        <saikat at stat.wisc.edu>, and Deepayan Sarkar
        <deepayan at stat.wisc.edu>
Maintainer: Douglas Bates <bates at stat.wisc.edu>
Description: Fit and compare Gaussian linear and nonlinear
        mixed-effects models.
Depends: nls, lattice, R(>= 1.7.0)
License: GPL version 2 or later
Built: R 1.7.1; i386-pc-linux-gnu; 2003-08-13 14:33:43



From maechler at stat.math.ethz.ch  Thu Aug 14 17:07:14 2003
From: maechler at stat.math.ethz.ch (Martin Maechler)
Date: Thu, 14 Aug 2003 17:07:14 +0200
Subject: [R] Problems with addition in big POSIX dates
In-Reply-To: <CD9BF92D5B1AD611ABEB00065B386FB001E8BDFB@tudor.com>
References: <CD9BF92D5B1AD611ABEB00065B386FB001E8BDFB@tudor.com>
Message-ID: <16187.42402.823006.961116@gargle.gargle.HOWL>

>>>>> "Whit" == Whit Armstrong <Whit.Armstrong at tudor.com>
>>>>>     on Wed, 13 Aug 2003 15:23:58 -0400 writes:

    Whit> Have you noticed any problems with big dates (>=1/1/2040) in R?
    Whit> Here is the bit of code that I'm having trouble with:

    >> test.date <- strptime("1/1/2040",format="%m/%d/%Y")
    >> 
    >> unlist(test.date)
    Whit> sec   min  hour  mday   mon  year  wday  yday isdst 
    Whit> 0     0     0     1     0   140     0     0     0 
    >> 
    >> date.plus.one <- as.POSIXct(test.date) + 24*60*60
    >> date.plus.one.lt <- as.POSIXlt(date.plus.one)
    >> 
    >> unlist(date.plus.one.lt)
    Whit> sec   min  hour  mday   mon  year  wday  yday isdst 
    Whit> 0     0     0     2     0   140     0     1     0 

    Whit> Notice that wday (the weekday, 0=Sunday, 7=Saturday) doesn't change.

    Whit> Am I missing something?

Probably the "C library millenium bug" (not official name).
The C library standard type for timedates, "time_t", is
"usually" encoded using 32-bit integers, measuring seconds
since the beginning of 1970. 

It has been well-known for years that this will lead to integer
overflow from 19 Jan 2038 :

  > as.POSIXct(strptime("1/1/1970",format="%m/%d/%Y"))+ .Machine$integer.max
  [1] "2038-01-19 03:14:07 CET"

I had thought that the R implementation carefully used doubles
instead of integers everywhere, but I guess we somewhere rely on
system-internal things even here.

For me, on Intel- Linux (RH 7.3, newer gcc) it's even worse:

> unlist(date.plus.one.lt)
  sec   min  hour  mday   mon  year  wday  yday isdst 
    0     0     0     2     0   140     6     0     0 
				       ~~~~~~~~~
i.e. `wday' has been counted backwards, and `yday' has remained at 0.

-----

I guess we have to wait for 64-bit implementations of "time_t"
or write our own code that works around the many C-library-bugs
we (the R community) have encountered concerning POSIX-time
implementations.
Prof Brian Ripley will know more on this..

Martin Maechler <maechler at stat.math.ethz.ch>	http://stat.ethz.ch/~maechler/
Seminar fuer Statistik, ETH-Zentrum  LEO C16	Leonhardstr. 27
ETH (Federal Inst. Technology)	8092 Zurich	SWITZERLAND
phone: x-41-1-632-3408		fax: ...-1228			<><



From Luis.Tito-de-Morais at ird.sn  Thu Aug 14 17:22:49 2003
From: Luis.Tito-de-Morais at ird.sn (Tito de Morais Luis)
Date: Thu, 14 Aug 2003 15:22:49 -0000
Subject: [R] stars graphs
In-Reply-To: <20030814132445.BLKQ16123.mta07.mail.mel.aone.net.au@there>
References: <1060779245.2941.337.camel@rap06.ird.sn>
	<20030814132445.BLKQ16123.mta07.mail.mel.aone.net.au@there>
Message-ID: <1060866791.2703.21.camel@rap06.ird.sn>

Jim Lemon replied directly to me with a helpful function that needs to
be adapted. Yet it almost does exactly what I want. I'm posting it so
that it appears in the thread as it could be useful to others.

Thank you Jim

Tito


Le jeu 14/08/2003 ? 15:28, Jim Lemon a ?crit :
> If I understand this, it is fairly easy to write a function to do this 
> plot. Here is a very basic example. You will probably want to customize 
> this to suit your needs, and the plotting of text labels is _very_ basic. 
> Hope it helps.
> 
> Jim
> 
> # this utility just rescales data to a new range
> rescale<-function(x,newrange) {
>  if(nargs() > 1 && is.numeric(x) && is.numeric(newrange)) {
>   # if newrange has max first, reverse it
>   if(newrange[1] > newrange[2]) {
>    newmin<-newrange[2]
>    newrange[2]<-newrange[1]
>    newrange[1]<-newmin
>   }
>   xrange<-range(x)
>   if(xrange[1] == xrange[2]) stop("can't rescale a constant vector!")
>   mfac<-(newrange[2]-newrange[1])/(xrange[2]-xrange[1])
>   invisible(newrange[1]+(x-xrange[1])*mfac)
>  }
>  else {
>   cat("Usage: rescale(x,newrange)\n")
>   cat("\twhere x is a numeric object and newrange is the min and max of 
> the new range\n")
>  }
> }
> 
> star.plot<-function(lengths,rad.pos,labels) {
>  maxlength<-max(lengths)+0.1*!missing(labels)
>  plot(c(-maxlength,maxlength),c(-maxlength,maxlength),type="n",axes=FALSE,
>   xlab="",ylab="")
>  npos<-length(rad.pos)
>  # add one space to prevent overlapping
>  newrange<-c(0,2*pi*npos/(npos+1))
>  # rescale to a range of 0 to almost 2pi
>  rad.pos<-rescale(rad.pos,newrange)
>  # get the vector of x positions
>  xpos<-cos(rad.pos)*lengths
>  # get the vector of y positions
>  ypos<-sin(rad.pos)*lengths
>  segments(0,0,xpos,ypos)
>  if(!missing(labels)) {
>   xpos<-xpos*1.1
>   ypos<-ypos*1.1
>   text(xpos,ypos,labels)
>  }
> }
-- 
L. Tito de Morais
      UR RAP
   IRD de Dakar
      BP 1386
       Dakar
      S?n?gal

T?l.: + 221 849 33 31
Fax: +221 832 16 75
Courriel: tito at ird.sn



From akahn1 at gmu.edu  Thu Aug 14 17:44:46 2003
From: akahn1 at gmu.edu (Ari Kahn)
Date: Thu, 14 Aug 2003 11:44:46 -0400
Subject: [R] Simple C-R interface, SEXP and other declarations
Message-ID: <BB6126AE.103EE%akahn1@gmu.edu>

I am trying to integrate OpenOffice Calc (OO) with R.  One of the steps I am
taking is to create a very simple command line C program with the R shared
lib embedded in it.  It would simply pass an R/S expression to R, print the
result, and catch errors.  I know this is "overkill," but it is a step
toward something larger.

Has anyone done this before?  If so, I would love to check out the code.

Anyway, when I try to follow the general theme on
http://developer.r-project.org/embedded.html
I get some errors during compilation.

Where are things like "eval_R_command()" and "SEXP" defined?  I.e. What do I
need to include to get these?

g++ -g -Wall -c hello.cc
g++ -g -Wall -c doit.cc
g++ -g -Wall -c myR.cc -L/usr/local/lib/R/bin -lR
myR.cc: In function `int eval_R_command()':
myR.cc:34: `SEXP' undeclared (first use this function)
.
(A bunch more errors)
.
make: *** [myR.o] Error 1

Will summerize.
Thanks,
-- 
Ari
http://binf.gmu.edu/akahn
They say its the Early Bird that gets the worm,
but its the second mouse that gets the cheese...



From paulda at BATTELLE.ORG  Thu Aug 14 17:48:19 2003
From: paulda at BATTELLE.ORG (Paul, David  A)
Date: Thu, 14 Aug 2003 11:48:19 -0400
Subject: [R] nls confidence intervals
Message-ID: <940250A9EB37A24CBE28D858EF07774967AA6B@ws-bco-mse3.milky-way.battelle.org>

You can use the well-known Taylor series approximation to the
variance of an arbitrary function:

Var( f(X) ) ~= Sum( s[i]^2*D2[i] ) + 2*Sum( Sum( s[i,j]*D[i]*D[j] ) )

where D2[i] is the second partial derivative of f(x) with respect
to the ith parameter and D[j] is the first partial derivative of f(x)
with respect to the jth parameter.  The indices on the summations
for 2*Sum( Sum( ... ) ) are i=1 to (p-1) and j>i, respectively, 
where p denotes the total number of parameters in the model.  Also,
s[i]^2 denotes the ith diagonal element of the variance-covariance
matrix for the model, and s[i,j] denotes an off-diagonal element
of the same matrix.  You should be able to use vcov( ) to extract
the variance-covariance matrix of your fitted model.

This approximation will estimate the functional form of the variance
of f(X).  To get the approximate variance of f(X) for a specific 
value of X simply plug in X=x.

After this, you will need to add the estimated variance of the
residuals and take the square root to obtain the standard error
used in calculating prediction intervals.  I have used this approach
for some highly nonlinear functions in the past, and the approximation
is only "good" when it is reasonable to assume that E(f(x)) ~= f(E(x)).
[This assumption is present in the derivation of the Taylor 
approximation.]  When this is not a reasonable assumption, the
approximation can be horrible.  In other words, the more locally linear
your nonlinear function is, the better the approximation will work.

Hope this helps,
  david paul


-----Original Message-----
From: Enrique Portilla [mailto:portillae at marlab.ac.uk] 
Sent: Thursday, August 14, 2003 9:28 AM
To: R-help at stat.math.ethz.ch
Subject: [R] nls confidence intervals


Hi, 
Does anyone know how to compute the confidence prediction intervals for a
nonlinear least squares models (nls)?

I was trying to use the function 'predict' as I usually do for other models
fitting (glm, lm, gams...), but it seems that se.fit, and interval
computation is not implemented for the nls...

Cheers

Enrique

~~~~~~~~~~~~~~~~~~~~~~~~~~~
Fisheries Research Services,
Marine Laboratory,
Victoria Road,
Torry,
Aberdeen, UK.
Tel. 44 (0) 1224 295314
~~~~~~~~~~~~~~~~~~~~~~~~~~~

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://www.stat.math.ethz.ch/mailman/listinfo/r-help



From spencer.graves at pdf.com  Thu Aug 14 18:08:26 2003
From: spencer.graves at pdf.com (Spencer Graves)
Date: Thu, 14 Aug 2003 09:08:26 -0700
Subject: [R] nls confidence intervals
References: <1060867666.17800.13.camel@pclnx3.marlab.ac.uk>
Message-ID: <3F3BB3FA.7040103@pdf.com>

This seems to identify a possible bug in R 1.7.1 under Windows 2000:

 > tstDf <- data.frame(y = 1:11, x=1:11)
 > fit <- nls(y~a/x, data=tstDf, start=list(a=1))
 > predict(fit, se.fit=TRUE)
  [1] 7.0601879 3.5300939 2.3533960 1.7650470 1.4120376 1.1766980 1.0085983
  [8] 0.8825235 0.7844653 0.7060188 0.6418353

The same code in S-Plus 6.1 produces the following:

 > predict(fit, se.fit = TRUE)
$fit:
  [1] 7.0601876 3.5300938 2.3533959 1.7650469 1.4120375 1.1766979 
1.0085982 0.8825234
  [9] 0.7844653 0.7060188 0.6418352

$se.fit:
  [1] 5.2433042 2.6216521 1.7477681 1.3108261 1.0486608 0.8738840 
0.7490435 0.6554130
  [9] 0.5825894 0.5243304 0.4766640

$residual.scale:
[1] 6.544753

$df:
[1] 10

Unfortunately, I'm not in a position to fix the problem, but this toy 
example might make it easier for someone else to fix it.

spencer graves
p.s.  The following command in S-Plus 6.1 seems to work fine but 
produces an error in R 1.7.1:

nls(y~a, data=tstDf, start=list(a=1))
Error in nlsModel(formula, mf, start) : singular gradient matrix at 
initial parameter estimates
#############################################################
Enrique Portilla wrote:
> Hi, 
> Does anyone know how to compute the confidence prediction intervals for
> a nonlinear least squares models (nls)?
> 
> I was trying to use the function 'predict' as I usually do for other
> models fitting (glm, lm, gams...), but it seems that se.fit, and
> interval computation is not implemented for the nls...
> 
> Cheers
> 
> Enrique
> 
> ~~~~~~~~~~~~~~~~~~~~~~~~~~~
> Fisheries Research Services,
> Marine Laboratory,
> Victoria Road,
> Torry,
> Aberdeen, UK.
> Tel. 44 (0) 1224 295314
> ~~~~~~~~~~~~~~~~~~~~~~~~~~~
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help



From spencer.graves at pdf.com  Thu Aug 14 18:14:01 2003
From: spencer.graves at pdf.com (Spencer Graves)
Date: Thu, 14 Aug 2003 09:14:01 -0700
Subject: [R] nls confidence intervals
References: <940250A9EB37A24CBE28D858EF07774967AA6B@ws-bco-mse3.milky-way.battelle.org>
Message-ID: <3F3BB549.1080601@pdf.com>

Regarding the accuracy of the Taylor series approximation, my favorite 
reference is Bates & Watts (1988) Nonlinear Regression Analysis and Its 
Applications (Wiley, esp. pp. 255-260).  Recently, Brian Ripley also 
Chambers & Hastie (1992) Statistical Models in S (Wadsworth, ch. 10) and 
Venables & Ripley, Modern Applied Statistics in S.  Unfortunately, I 
don't see predict.nls (or anything similar) in the index of any of these 
sources.

hope this helps.  spencer graves

Paul, David A wrote:
> You can use the well-known Taylor series approximation to the
> variance of an arbitrary function:
> 
> Var( f(X) ) ~= Sum( s[i]^2*D2[i] ) + 2*Sum( Sum( s[i,j]*D[i]*D[j] ) )
> 
> where D2[i] is the second partial derivative of f(x) with respect
> to the ith parameter and D[j] is the first partial derivative of f(x)
> with respect to the jth parameter.  The indices on the summations
> for 2*Sum( Sum( ... ) ) are i=1 to (p-1) and j>i, respectively, 
> where p denotes the total number of parameters in the model.  Also,
> s[i]^2 denotes the ith diagonal element of the variance-covariance
> matrix for the model, and s[i,j] denotes an off-diagonal element
> of the same matrix.  You should be able to use vcov( ) to extract
> the variance-covariance matrix of your fitted model.
> 
> This approximation will estimate the functional form of the variance
> of f(X).  To get the approximate variance of f(X) for a specific 
> value of X simply plug in X=x.
> 
> After this, you will need to add the estimated variance of the
> residuals and take the square root to obtain the standard error
> used in calculating prediction intervals.  I have used this approach
> for some highly nonlinear functions in the past, and the approximation
> is only "good" when it is reasonable to assume that E(f(x)) ~= f(E(x)).
> [This assumption is present in the derivation of the Taylor 
> approximation.]  When this is not a reasonable assumption, the
> approximation can be horrible.  In other words, the more locally linear
> your nonlinear function is, the better the approximation will work.
> 
> Hope this helps,
>   david paul
> 
> 
> -----Original Message-----
> From: Enrique Portilla [mailto:portillae at marlab.ac.uk] 
> Sent: Thursday, August 14, 2003 9:28 AM
> To: R-help at stat.math.ethz.ch
> Subject: [R] nls confidence intervals
> 
> 
> Hi, 
> Does anyone know how to compute the confidence prediction intervals for a
> nonlinear least squares models (nls)?
> 
> I was trying to use the function 'predict' as I usually do for other models
> fitting (glm, lm, gams...), but it seems that se.fit, and interval
> computation is not implemented for the nls...
> 
> Cheers
> 
> Enrique
> 
> ~~~~~~~~~~~~~~~~~~~~~~~~~~~
> Fisheries Research Services,
> Marine Laboratory,
> Victoria Road,
> Torry,
> Aberdeen, UK.
> Tel. 44 (0) 1224 295314
> ~~~~~~~~~~~~~~~~~~~~~~~~~~~
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help



From hothorn at ci.tuwien.ac.at  Thu Aug 14 18:15:51 2003
From: hothorn at ci.tuwien.ac.at (Torsten Hothorn)
Date: Thu, 14 Aug 2003 18:15:51 +0200 (CEST)
Subject: [R] nls confidence intervals
In-Reply-To: <3F3BB3FA.7040103@pdf.com>
Message-ID: <Pine.LNX.3.96.1030814181421.20213H-100000@thorin.ci.tuwien.ac.at>


On Thu, 14 Aug 2003, Spencer Graves wrote:

> This seems to identify a possible bug in R 1.7.1 under Windows 2000:
> 
>  > tstDf <- data.frame(y = 1:11, x=1:11)
>  > fit <- nls(y~a/x, data=tstDf, start=list(a=1))
>  > predict(fit, se.fit=TRUE)
>   [1] 7.0601879 3.5300939 2.3533960 1.7650470 1.4120376 1.1766980 1.0085983
>   [8] 0.8825235 0.7844653 0.7060188 0.6418353
> 

?predict.nls states:

	At present `se.fit' and `interval' are ignored.

and therefore works as advertised.

Torsten

> The same code in S-Plus 6.1 produces the following:
> 
>  > predict(fit, se.fit = TRUE)
> $fit:
>   [1] 7.0601876 3.5300938 2.3533959 1.7650469 1.4120375 1.1766979 
> 1.0085982 0.8825234
>   [9] 0.7844653 0.7060188 0.6418352
> 
> $se.fit:
>   [1] 5.2433042 2.6216521 1.7477681 1.3108261 1.0486608 0.8738840 
> 0.7490435 0.6554130
>   [9] 0.5825894 0.5243304 0.4766640
> 
> $residual.scale:
> [1] 6.544753
> 
> $df:
> [1] 10
> 
> Unfortunately, I'm not in a position to fix the problem, but this toy 
> example might make it easier for someone else to fix it.
> 
> spencer graves
> p.s.  The following command in S-Plus 6.1 seems to work fine but 
> produces an error in R 1.7.1:
> 
> nls(y~a, data=tstDf, start=list(a=1))
> Error in nlsModel(formula, mf, start) : singular gradient matrix at 
> initial parameter estimates
> #############################################################
> Enrique Portilla wrote:
> > Hi, 
> > Does anyone know how to compute the confidence prediction intervals for
> > a nonlinear least squares models (nls)?
> > 
> > I was trying to use the function 'predict' as I usually do for other
> > models fitting (glm, lm, gams...), but it seems that se.fit, and
> > interval computation is not implemented for the nls...
> > 
> > Cheers
> > 
> > Enrique
> > 
> > ~~~~~~~~~~~~~~~~~~~~~~~~~~~
> > Fisheries Research Services,
> > Marine Laboratory,
> > Victoria Road,
> > Torry,
> > Aberdeen, UK.
> > Tel. 44 (0) 1224 295314
> > ~~~~~~~~~~~~~~~~~~~~~~~~~~~
> > 
> > ______________________________________________
> > R-help at stat.math.ethz.ch mailing list
> > https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> 
>



From andy_liaw at merck.com  Thu Aug 14 14:15:32 2003
From: andy_liaw at merck.com (Liaw, Andy)
Date: Thu, 14 Aug 2003 08:15:32 -0400
Subject: [R] placing labels in polygon center ?
Message-ID: <3A822319EB35174CA3714066D590DCD50205C9B2@usrymx25.merck.com>

This is rather simple-minded:

rot <- function(x, k=1) {
  k <- k %% length(x)
  x[c((k+1):length(x), 1:k)]
}

Andy

> -----Original Message-----
> From: Richard A. O'Keefe [mailto:ok at cs.otago.ac.nz] 
> Sent: Thursday, August 14, 2003 1:28 AM
> To: r-help at stat.math.ethz.ch
> Subject: Re: [R] placing labels in polygon center ?
> 
> 
> I wrote:
> 	I found myself wishing for a function to rotate a vector.
> 	Is there one?  I know about ?lag, but help.search("rotate")
> 	didn't find anything to the point.
> 
> Here I was regarding a vector as a _sequence_.
> The (one-step) rotation of c(u,v,w,x,y,z) is c(v,w,x,y,z,u). 
> This is pretty much the way APL uses the word "rotate" (the 
> vertical-bar-overstruck-with-a-circle operator).
> 
> Spencer Graves <spencer.graves at PDF.COM> replied:
> 	I didn't study your code, but regarding a function to rotate a 
> 	vector:  Multiplication by an orthogonal matrix does that.
> 
> This is a misunderstanding.  We were both using the word 
> "rotate" in a standard way, the problem is that it has more 
> than one "standard" meaning.
> 
> As a matter of fact,
> 	/ 0 1 0 0 \  / u \     / v \
> 	| 0 0 1 0 |  | v |     | w |
> 	| 0 0 0 1 |  | w |  =  | x |
> 	\ 1 0 0 0 /  \ x /     \ u /
> so you *can* do the kind of rotation I want using a matrix 
> multiplication, and this is mathematically useful; it's just 
> not a very good way to do it in a computer.
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list 
> https://www.stat.math.ethz.ch/mailman/listinfo> /r-help
> 

------------------------------------------------------------------------------
Notice:  This e-mail message, together with any attachments, contains
information of Merck & Co., Inc. (Whitehouse Station, New Jersey, USA), and/or
its affiliates (which may be known outside the United States as Merck Frosst,
Merck Sharp & Dohme or MSD) that may be confidential, proprietary copyrighted
and/or legally privileged, and is intended solely for the use of the
individual or entity named on this message.  If you are not the intended
recipient, and have received this message in error, please immediately return
this by e-mail and then delete it.



From f0z6305 at labs.tamu.edu  Thu Aug 14 18:24:06 2003
From: f0z6305 at labs.tamu.edu (Feng Zhang)
Date: Thu, 14 Aug 2003 11:24:06 -0500
Subject: [R] How to get the pseudo left inverse of a singular square matrix?
Message-ID: <00da01c36280$7bd66550$8bd75ba5@IE.TAMU.EDU>

Dear R-listers,

I have a dxr matrix Z, where d > r.
And the product Z*Z' is a singular square matrix.
The problem is how to get the left inverse U of this
singular matrix Z*Z', such that
U*(Z*Z') = I?

Is there any to figure it out using matrix decomposition method?

Thanks a lot for your help.

Fred



From amurta at ipimar.pt  Thu Aug 14 18:50:06 2003
From: amurta at ipimar.pt (Alberto Murta)
Date: Thu, 14 Aug 2003 16:50:06 +0000
Subject: [R] vectorization question
Message-ID: <200308141650.06342.amurta@ipimar.pt>

Dear all

I recently noticed the following error when cohercing a data.frame into a 
matrix:

> example <- matrix(1:12,4,3)
> example <- as.data.frame(example)
> example$V4 <- 0
> example
  V1 V2 V3 V4
1  1  5  9   0
2  2  6 10  0
3  3  7 11  0
4  4  8 12  0
> example <- as.matrix(example)
Error in as.matrix.data.frame(example) : dim<- length of dims do not match the 
length of object

However, if the column to be added has the right number of lines, there's no 
error:

> example <- matrix(1:12,4,3)
> example <- as.data.frame(example)
> example$V4 <- rep(0,4)
> example
  V1 V2 V3 V4
1  1  5  9  0
2  2  6 10  0
3  3  7 11  0
4  4  8 12  0
> example <- as.matrix(example)
> example
  V1 V2 V3 V4
1  1  5  9  0
2  2  6 10  0
3  3  7 11  0
4  4  8 12  0

Shouldn't it work well both ways? I checked the attributes and dims of the 
data frame and they are the same in both cases. Where's the difference that 
originates the error message?
Thanks in advance

Alberto

platform i686-pc-linux-gnu
arch     i686             
os       linux-gnu        
system   i686, linux-gnu  
status                    
major    1                
minor    7.1              
year     2003             
month    06               
day      16               
language R    


-- 
                                         Alberto G. Murta
Institute for Agriculture and Fisheries Research (INIAP-IPIMAR) 
Av. Brasilia, 1449-006 Lisboa, Portugal | Phone: +351 213027062
Fax:+351 213015948 | http://www.ipimar-iniap.ipimar.pt/pelagicos/



From dmurdoch at pair.com  Thu Aug 14 18:43:25 2003
From: dmurdoch at pair.com (Duncan Murdoch)
Date: Thu, 14 Aug 2003 12:43:25 -0400
Subject: [R] nls confidence intervals
In-Reply-To: <3F3BB3FA.7040103@pdf.com>
References: <1060867666.17800.13.camel@pclnx3.marlab.ac.uk>
	<3F3BB3FA.7040103@pdf.com>
Message-ID: <p7enjvkf0d04jke4bo3ca0c6jfsrbaelj0@4ax.com>

On Thu, 14 Aug 2003 09:08:26 -0700, Spencer Graves
<spencer.graves at pdf.com> wrote :

>This seems to identify a possible bug in R 1.7.1 under Windows 2000:
>
> > tstDf <- data.frame(y = 1:11, x=1:11)
> > fit <- nls(y~a/x, data=tstDf, start=list(a=1))
> > predict(fit, se.fit=TRUE)
>  [1] 7.0601879 3.5300939 2.3533960 1.7650470 1.4120376 1.1766980 1.0085983
>  [8] 0.8825235 0.7844653 0.7060188 0.6418353
>
>The same code in S-Plus 6.1 produces the following:
>
> > predict(fit, se.fit = TRUE)
>$fit:
>  [1] 7.0601876 3.5300938 2.3533959 1.7650469 1.4120375 1.1766979 
>1.0085982 0.8825234
>  [9] 0.7844653 0.7060188 0.6418352
>
>$se.fit:
>  [1] 5.2433042 2.6216521 1.7477681 1.3108261 1.0486608 0.8738840 
>0.7490435 0.6554130
>  [9] 0.5825894 0.5243304 0.4766640

I think that's a documented difference, rather than a bug.  The
?predict.nls says near the top that at present, se.fit is ignored.
I can see in the log that this comment was added in 1999; a
contribution of code to actually do this would probably be welcomed!

Perhaps the description below of what se.fit is supposed to do should
be modified. 

>p.s.  The following command in S-Plus 6.1 seems to work fine but 
>produces an error in R 1.7.1:
>
>nls(y~a, data=tstDf, start=list(a=1))
>Error in nlsModel(formula, mf, start) : singular gradient matrix at 
>initial parameter estimates

This looks like a bug in numericDeriv, which finds a derivative of 0
rather than 1 for da/da.  I've cc'd the author.

Duncan



From andy_liaw at merck.com  Thu Aug 14 18:41:39 2003
From: andy_liaw at merck.com (Liaw, Andy)
Date: Thu, 14 Aug 2003 12:41:39 -0400
Subject: [R] vectorization question
Message-ID: <3A822319EB35174CA3714066D590DCD50205C9B8@usrymx25.merck.com>

If you look at the structure, you'll see:
> x$V4 <- 0
> str(x)
`data.frame':   4 obs. of  4 variables:
 $ V1: int  1 2 3 4
 $ V2: int  5 6 7 8
 $ V3: int  9 10 11 12
 $ V4: num 0

Don't know if this is the intended result.  In any case, you're probably
better off using data.matrix, as

> data.matrix(x)
  V1 V2 V3 V4
1  1  5  9  0
2  2  6 10  0
3  3  7 11  0
4  4  8 12  0

HTH,
Andy


> -----Original Message-----
> From: Alberto Murta [mailto:amurta at ipimar.pt] 
> Sent: Thursday, August 14, 2003 12:50 PM
> To: r-help at stat.math.ethz.ch
> Subject: [R] vectorization question
> 
> 
> Dear all
> 
> I recently noticed the following error when cohercing a 
> data.frame into a 
> matrix:
> 
> > example <- matrix(1:12,4,3)
> > example <- as.data.frame(example)
> > example$V4 <- 0
> > example
>   V1 V2 V3 V4
> 1  1  5  9   0
> 2  2  6 10  0
> 3  3  7 11  0
> 4  4  8 12  0
> > example <- as.matrix(example)
> Error in as.matrix.data.frame(example) : dim<- length of dims 
> do not match the 
> length of object
> 
> However, if the column to be added has the right number of 
> lines, there's no 
> error:
> 
> > example <- matrix(1:12,4,3)
> > example <- as.data.frame(example)
> > example$V4 <- rep(0,4)
> > example
>   V1 V2 V3 V4
> 1  1  5  9  0
> 2  2  6 10  0
> 3  3  7 11  0
> 4  4  8 12  0
> > example <- as.matrix(example)
> > example
>   V1 V2 V3 V4
> 1  1  5  9  0
> 2  2  6 10  0
> 3  3  7 11  0
> 4  4  8 12  0
> 
> Shouldn't it work well both ways? I checked the attributes 
> and dims of the 
> data frame and they are the same in both cases. Where's the 
> difference that 
> originates the error message?
> Thanks in advance
> 
> Alberto
> 
> platform i686-pc-linux-gnu
> arch     i686             
> os       linux-gnu        
> system   i686, linux-gnu  
> status                    
> major    1                
> minor    7.1              
> year     2003             
> month    06               
> day      16               
> language R    
> 
> 
> -- 
>                                          Alberto G. Murta 
> Institute for Agriculture and Fisheries Research (INIAP-IPIMAR) 
> Av. Brasilia, 1449-006 Lisboa, Portugal | Phone: +351 
> 213027062 Fax:+351 213015948 | 
> http://www.ipimar-iniap.ipimar.pt/pelagicos/
> 
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list 
> https://www.stat.math.ethz.ch/mailman/listinfo> /r-help
> 

------------------------------------------------------------------------------
Notice:  This e-mail message, together with any attachments, contains
information of Merck & Co., Inc. (Whitehouse Station, New Jersey, USA), and/or
its affiliates (which may be known outside the United States as Merck Frosst,
Merck Sharp & Dohme or MSD) that may be confidential, proprietary copyrighted
and/or legally privileged, and is intended solely for the use of the
individual or entity named on this message.  If you are not the intended
recipient, and have received this message in error, please immediately return
this by e-mail and then delete it.



From jerome at hivnet.ubc.ca  Thu Aug 14 18:52:33 2003
From: jerome at hivnet.ubc.ca (Jerome Asselin)
Date: Thu, 14 Aug 2003 09:52:33 -0700
Subject: [R] How to get the pseudo left inverse of a singular square
	matrix?
In-Reply-To: <00da01c36280$7bd66550$8bd75ba5@IE.TAMU.EDU>
References: <00da01c36280$7bd66550$8bd75ba5@IE.TAMU.EDU>
Message-ID: <200308141659.JAA09255@hivnet.ubc.ca>


Singular matrices are not invertible. However you can calculate the 
generalized inverse with the function ginv() from package MASS.

HTH,
Jerome

On August 14, 2003 09:24 am, Feng Zhang wrote:
> Dear R-listers,
>
> I have a dxr matrix Z, where d > r.
> And the product Z*Z' is a singular square matrix.
> The problem is how to get the left inverse U of this
> singular matrix Z*Z', such that
> U*(Z*Z') = I?
>
> Is there any to figure it out using matrix decomposition method?
>
> Thanks a lot for your help.
>
> Fred
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help



From budinger at amadeus.statistik.uni-dortmund.de  Thu Aug 14 19:01:50 2003
From: budinger at amadeus.statistik.uni-dortmund.de (Matthias Budinger)
Date: Thu, 14 Aug 2003 19:01:50 +0200 (MET DST)
Subject: [R] filter ARMA process
Message-ID: <Pine.GSO.4.21.0308141853180.20363-100000@amadeus.statistik.uni-dortmund.de>

Hi

given an ARMA process and the AR and MA coefficients I need the residuals.
arima() calculates the residuals together with the best AR and MA
coefficients, but I need the coefficients to take known values.
In S-PLUS there is a function arima.filt(). Is there something similar in
R?

Thanks for any help,
Matthias Budinger



From f0z6305 at labs.tamu.edu  Thu Aug 14 19:02:07 2003
From: f0z6305 at labs.tamu.edu (Feng Zhang)
Date: Thu, 14 Aug 2003 12:02:07 -0500
Subject: [R] How to get the pseudo left inverse of a singular square
	matrix?
References: <00da01c36280$7bd66550$8bd75ba5@IE.TAMU.EDU>
	<200308141659.JAA09255@hivnet.ubc.ca>
Message-ID: <00fd01c36285$cb3fe300$8bd75ba5@IE.TAMU.EDU>

Thank, Jerome

The question is if this generalized inverse can make
their product to be identity matrix?


----- Original Message -----
From: "Jerome Asselin" <jerome at hivnet.ubc.ca>
To: "Feng Zhang" <f0z6305 at labs.tamu.edu>; "R-Help"
<r-help at stat.math.ethz.ch>
Sent: Thursday, August 14, 2003 11:52 AM
Subject: Re: [R] How to get the pseudo left inverse of a singular square
matrix?


>
> Singular matrices are not invertible. However you can calculate the
> generalized inverse with the function ginv() from package MASS.
>
> HTH,
> Jerome
>
> On August 14, 2003 09:24 am, Feng Zhang wrote:
> > Dear R-listers,
> >
> > I have a dxr matrix Z, where d > r.
> > And the product Z*Z' is a singular square matrix.
> > The problem is how to get the left inverse U of this
> > singular matrix Z*Z', such that
> > U*(Z*Z') = I?
> >
> > Is there any to figure it out using matrix decomposition method?
> >
> > Thanks a lot for your help.
> >
> > Fred
> >
> > ______________________________________________
> > R-help at stat.math.ethz.ch mailing list
> > https://www.stat.math.ethz.ch/mailman/listinfo/r-help



From amurta at ipimar.pt  Thu Aug 14 20:06:10 2003
From: amurta at ipimar.pt (Alberto Murta)
Date: Thu, 14 Aug 2003 18:06:10 +0000
Subject: [R] vectorization question
In-Reply-To: <3A822319EB35174CA3714066D590DCD50205C9B8@usrymx25.merck.com>
References: <3A822319EB35174CA3714066D590DCD50205C9B8@usrymx25.merck.com>
Message-ID: <200308141806.10926.amurta@ipimar.pt>

Thank you very much. I just would expect that 'as.matrix' would have the same 
behaviour as 'data.matrix' when all columns in a data frame are numeric.
Regards

Alberto

On Thursday 14 August 2003 16:41, Liaw, Andy wrote:
> If you look at the structure, you'll see:
> > x$V4 <- 0
> > str(x)
>
> `data.frame':   4 obs. of  4 variables:
>  $ V1: int  1 2 3 4
>  $ V2: int  5 6 7 8
>  $ V3: int  9 10 11 12
>  $ V4: num 0
>
> Don't know if this is the intended result.  In any case, you're probably
> better off using data.matrix, as
>
> > data.matrix(x)
>
>   V1 V2 V3 V4
> 1  1  5  9  0
> 2  2  6 10  0
> 3  3  7 11  0
> 4  4  8 12  0
>
> HTH,
> Andy
>
> > -----Original Message-----
> > From: Alberto Murta [mailto:amurta at ipimar.pt]
> > Sent: Thursday, August 14, 2003 12:50 PM
> > To: r-help at stat.math.ethz.ch
> > Subject: [R] vectorization question
> >
> >
> > Dear all
> >
> > I recently noticed the following error when cohercing a
> > data.frame into a
> >
> > matrix:
> > > example <- matrix(1:12,4,3)
> > > example <- as.data.frame(example)
> > > example$V4 <- 0
> > > example
> >
> >   V1 V2 V3 V4
> > 1  1  5  9   0
> > 2  2  6 10  0
> > 3  3  7 11  0
> > 4  4  8 12  0
> >
> > > example <- as.matrix(example)
> >
> > Error in as.matrix.data.frame(example) : dim<- length of dims
> > do not match the
> > length of object
> >
> > However, if the column to be added has the right number of
> > lines, there's no
> >
> > error:
> > > example <- matrix(1:12,4,3)
> > > example <- as.data.frame(example)
> > > example$V4 <- rep(0,4)
> > > example
> >
> >   V1 V2 V3 V4
> > 1  1  5  9  0
> > 2  2  6 10  0
> > 3  3  7 11  0
> > 4  4  8 12  0
> >
> > > example <- as.matrix(example)
> > > example
> >
> >   V1 V2 V3 V4
> > 1  1  5  9  0
> > 2  2  6 10  0
> > 3  3  7 11  0
> > 4  4  8 12  0
> >
> > Shouldn't it work well both ways? I checked the attributes
> > and dims of the
> > data frame and they are the same in both cases. Where's the
> > difference that
> > originates the error message?
> > Thanks in advance
> >
> > Alberto
> >
> > platform i686-pc-linux-gnu
> > arch     i686
> > os       linux-gnu
> > system   i686, linux-gnu
> > status
> > major    1
> > minor    7.1
> > year     2003
> > month    06
> > day      16
> > language R
> >
> >
> > --
> >                                          Alberto G. Murta
> > Institute for Agriculture and Fisheries Research (INIAP-IPIMAR)
> > Av. Brasilia, 1449-006 Lisboa, Portugal | Phone: +351
> > 213027062 Fax:+351 213015948 |
> > http://www.ipimar-iniap.ipimar.pt/pelagicos/
> >
> >
> > ______________________________________________
> > R-help at stat.math.ethz.ch mailing list
> > https://www.stat.math.ethz.ch/mailman/listinfo> /r-help
>
> ---------------------------------------------------------------------------
>--- Notice:  This e-mail message, together with any attachments, contains
> information of Merck & Co., Inc. (Whitehouse Station, New Jersey, USA),
> and/or its affiliates (which may be known outside the United States as
> Merck Frosst, Merck Sharp & Dohme or MSD) that may be confidential,
> proprietary copyrighted and/or legally privileged, and is intended solely
> for the use of the individual or entity named on this message.  If you are
> not the intended recipient, and have received this message in error, please
> immediately return this by e-mail and then delete it.
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help

-- 
                                         Alberto G. Murta
Institute for Agriculture and Fisheries Research (INIAP-IPIMAR) 
Av. Brasilia, 1449-006 Lisboa, Portugal | Phone: +351 213027062
Fax:+351 213015948 | http://www.ipimar-iniap.ipimar.pt/pelagicos/



From rossini at blindglobe.net  Thu Aug 14 19:13:32 2003
From: rossini at blindglobe.net (A.J. Rossini)
Date: Thu, 14 Aug 2003 10:13:32 -0700
Subject: [R] partially off-topic: ESS WWW site is moving
Message-ID: <85smo4now3.fsf@blindglobe.net>


Since it of partially related interest here:

The Emacs Statistical System (Emacs Speaks Statistics) WWW site, along
with all of the content formerly on software.biostat.washington.edu,
is moving with me to a new location, 

   http://www.analytics.washington.edu/

with ESS specifically moving to 

   http://www.analytics.washington.edu/Zope/wikis/ess/

It isn't really done, but should be by a week from now, assuming that
all goes smoothly (and nothing has been smooth for me this summer,
so...).  Hopefully, we'll have a new release out by the beginning of
September to celebrate various things worthy of celebration.  There
will be connectivity problems August 19th and 20th, since the physical
move (with a few machines) will be occurring then.

best,
-tony

-- 
A.J. Rossini     			
rossini at u.washington.edu            http://www.analytics.washington.edu/ 
Biomedical and Health Informatics   University of Washington
Biostatistics, SCHARP/HVTN          Fred Hutchinson Cancer Research Center
UW   :              FAX=206-543-3461 | moving soon to a permanent office
FHCRC: 206-667-7025 FAX=206-667-4812 | Voicemail is pretty sketchy/use Email

CONFIDENTIALITY NOTICE: This e-mail message and any attachme...{{dropped}}



From arrayprofile at yahoo.com  Thu Aug 14 19:14:44 2003
From: arrayprofile at yahoo.com (array chip)
Date: Thu, 14 Aug 2003 10:14:44 -0700 (PDT)
Subject: [R] leave-one-out
In-Reply-To: <3F3A7A5B.2020108@optonline.net>
Message-ID: <20030814171444.92678.qmail@web41209.mail.yahoo.com>

Hi, is there a package for performing leave-one-out
cross validation in R?



From dmurdoch at pair.com  Thu Aug 14 19:24:18 2003
From: dmurdoch at pair.com (Duncan Murdoch)
Date: Thu, 14 Aug 2003 13:24:18 -0400
Subject: [R] nls confidence intervals
In-Reply-To: <p7enjvkf0d04jke4bo3ca0c6jfsrbaelj0@4ax.com>
References: <1060867666.17800.13.camel@pclnx3.marlab.ac.uk>
	<3F3BB3FA.7040103@pdf.com>
	<p7enjvkf0d04jke4bo3ca0c6jfsrbaelj0@4ax.com>
Message-ID: <pchnjvkt4skm0rqpc039bd4r8ik4tq2s97@4ax.com>

On Thu, 14 Aug 2003 12:43:25 -0400, Duncan Murdoch <dmurdoch at pair.com>
wrote :

>Perhaps the description below of what se.fit is supposed to do should
>be modified. 

I've done that now in the development version (to become 1.8.0).

Duncan Murdoch



From dmurdoch at pair.com  Thu Aug 14 19:26:08 2003
From: dmurdoch at pair.com (Duncan Murdoch)
Date: Thu, 14 Aug 2003 13:26:08 -0400
Subject: [R] nls confidence intervals
In-Reply-To: <p7enjvkf0d04jke4bo3ca0c6jfsrbaelj0@4ax.com>
References: <1060867666.17800.13.camel@pclnx3.marlab.ac.uk>
	<3F3BB3FA.7040103@pdf.com>
	<p7enjvkf0d04jke4bo3ca0c6jfsrbaelj0@4ax.com>
Message-ID: <3ghnjvcphsq7iqhtlom38ac7cjhms69fg4@4ax.com>

On Thu, 14 Aug 2003 12:43:25 -0400, Duncan Murdoch <dmurdoch at pair.com>
wrote :

>>Perhaps the description below of what se.fit is supposed to do should
>>be modified. 
>
>I've done that now in the development version (to become 1.8.0).

Err, I mean in the patch version (but it should still end up in
1.8.0). 

Duncan Murdoch



From andy_liaw at merck.com  Thu Aug 14 19:24:18 2003
From: andy_liaw at merck.com (Liaw, Andy)
Date: Thu, 14 Aug 2003 13:24:18 -0400
Subject: [R] How to get the pseudo left inverse of a singular square
	m atrix?
Message-ID: <3A822319EB35174CA3714066D590DCD50205C9BB@usrymx25.merck.com>

I'm rusty, but not *that* rusty here, I hope.

If W (=Z*Z' in your case) is singular, it can not have inverse, which by
definition also mean that nothing multiply by it will produce the identity
matrix (for otherwise it would have an inverse and thus nonsingular).

The definition of a generalized inverse is something like:  If A is a
non-null matrix, and G satisfy AGA = A, then G is called a generalized
inverse of A.  This is not unique, but a unique one that satisfy some
additional properties is the Moore-Penrose inverse.  I don't know if this is
what ginv() in MASS returns, as I have not used it before.

Andy

> -----Original Message-----
> From: Feng Zhang [mailto:f0z6305 at labs.tamu.edu] 
> Sent: Thursday, August 14, 2003 1:02 PM
> To: Jerome Asselin; R-Help
> Subject: Re: [R] How to get the pseudo left inverse of a 
> singular square matrix?
> 
> 
> Thank, Jerome
> 
> The question is if this generalized inverse can make
> their product to be identity matrix?
> 
> 
> ----- Original Message -----
> From: "Jerome Asselin" <jerome at hivnet.ubc.ca>
> To: "Feng Zhang" <f0z6305 at labs.tamu.edu>; "R-Help" 
> <r-help at stat.math.ethz.ch>
> Sent: Thursday, August 14, 2003 11:52 AM
> Subject: Re: [R] How to get the pseudo left inverse of a 
> singular square matrix?
> 
> 
> >
> > Singular matrices are not invertible. However you can calculate the 
> > generalized inverse with the function ginv() from package MASS.
> >
> > HTH,
> > Jerome
> >
> > On August 14, 2003 09:24 am, Feng Zhang wrote:
> > > Dear R-listers,
> > >
> > > I have a dxr matrix Z, where d > r.
> > > And the product Z*Z' is a singular square matrix.
> > > The problem is how to get the left inverse U of this 
> singular matrix 
> > > Z*Z', such that
> > > U*(Z*Z') = I?
> > >
> > > Is there any to figure it out using matrix decomposition method?
> > >
> > > Thanks a lot for your help.
> > >
> > > Fred
> > >
> > > ______________________________________________
> > > R-help at stat.math.ethz.ch mailing list 
> > > https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list 
> https://www.stat.math.ethz.ch/mailman/listinfo> /r-help
> 

------------------------------------------------------------------------------
Notice:  This e-mail message, together with any attachments, contains
information of Merck & Co., Inc. (Whitehouse Station, New Jersey, USA), and/or
its affiliates (which may be known outside the United States as Merck Frosst,
Merck Sharp & Dohme or MSD) that may be confidential, proprietary copyrighted
and/or legally privileged, and is intended solely for the use of the
individual or entity named on this message.  If you are not the intended
recipient, and have received this message in error, please immediately return
this by e-mail and then delete it.



From jerome at hivnet.ubc.ca  Thu Aug 14 19:29:07 2003
From: jerome at hivnet.ubc.ca (Jerome Asselin)
Date: Thu, 14 Aug 2003 10:29:07 -0700
Subject: [R] How to get the pseudo left inverse of a singular square
	matrix?
In-Reply-To: <00fd01c36285$cb3fe300$8bd75ba5@IE.TAMU.EDU>
References: <00da01c36280$7bd66550$8bd75ba5@IE.TAMU.EDU>
	<200308141659.JAA09255@hivnet.ubc.ca>
	<00fd01c36285$cb3fe300$8bd75ba5@IE.TAMU.EDU>
Message-ID: <200308141735.KAA10558@hivnet.ubc.ca>


No. If B is singular, it's impossible to find a matrix A such that A%*%B 
be the identity matrix (unless you can find a number x such that x*0=1).

Cheers,
Jerome

On August 14, 2003 10:02 am, Feng Zhang wrote:
> Thank, Jerome
>
> The question is if this generalized inverse can make
> their product to be identity matrix?
>
>
> ----- Original Message -----
> From: "Jerome Asselin" <jerome at hivnet.ubc.ca>
> To: "Feng Zhang" <f0z6305 at labs.tamu.edu>; "R-Help"
> <r-help at stat.math.ethz.ch>
> Sent: Thursday, August 14, 2003 11:52 AM
> Subject: Re: [R] How to get the pseudo left inverse of a singular square
> matrix?
>
> > Singular matrices are not invertible. However you can calculate the
> > generalized inverse with the function ginv() from package MASS.
> >
> > HTH,
> > Jerome
> >
> > On August 14, 2003 09:24 am, Feng Zhang wrote:
> > > Dear R-listers,
> > >
> > > I have a dxr matrix Z, where d > r.
> > > And the product Z*Z' is a singular square matrix.
> > > The problem is how to get the left inverse U of this
> > > singular matrix Z*Z', such that
> > > U*(Z*Z') = I?
> > >
> > > Is there any to figure it out using matrix decomposition method?
> > >
> > > Thanks a lot for your help.
> > >
> > > Fred
> > >
> > > ______________________________________________
> > > R-help at stat.math.ethz.ch mailing list
> > > https://www.stat.math.ethz.ch/mailman/listinfo/r-help



From tblackw at umich.edu  Thu Aug 14 19:34:16 2003
From: tblackw at umich.edu (Thomas W Blackwell)
Date: Thu, 14 Aug 2003 13:34:16 -0400 (EDT)
Subject: [R] leave-one-out
In-Reply-To: <20030814171444.92678.qmail@web41209.mail.yahoo.com>
Message-ID: <Pine.SOL.4.44.0308141330520.2080-100000@tetris.gpcc.itd.umich.edu>


help.search("cross validation")  returns   'cv.glm(boot)'
and the boot package provides many other utilities for this.

-  tom blackwell  -  u michigan medical school  -  ann arbor  -

On Thu, 14 Aug 2003, array chip wrote:

> Hi, is there a package for performing leave-one-out
> cross validation in R?
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
>



From tplate at blackmesacapital.com  Thu Aug 14 19:43:11 2003
From: tplate at blackmesacapital.com (Tony Plate)
Date: Thu, 14 Aug 2003 11:43:11 -0600
Subject: [R] vectorization question
In-Reply-To: <200308141650.06342.amurta@ipimar.pt>
Message-ID: <5.2.1.1.2.20030814112756.04779dc0@mailhost.blackmesacapital.com>

 From ?data.frame:
>Details:
>
>      A data frame is a list of variables of the same length with unique
>      row names, given class `"data.frame"'.

Your example constructs an object that does not conform to the definition 
of a data frame (the new column is not the same length as the old 
columns).  Some data frame functions may work OK with such an object, but 
others will not.  For example, the print function for data.frame silently 
handles such an illegal data frame (which could be described as 
unfortunate.)  It would probably be far easier to construct a correct data 
frame in the first place than to try to find and fix functions that don't 
handle illegal data frames.  For adding a new column to a data frame, the 
expressions "x[,new.column.name] <- value" and "x[[new.column.name]] <- 
value" will replicate the value so that the new column is the same length 
as the existing ones, while the "$" operator in an assignment will not 
replicate the value.  (One could argue that this is a deficiency, but I 
think it has been that way for a long time, and the behavior is the same in 
the current version of S-plus.)

 > x1 <- data.frame(a=1:3)
 > x2 <- x1
 > x3 <- x1
 > x1$b <- 0
 > x2[,"b"] <- 0
 > x3[["b"]] <- 0
 > sapply(x1, length)
a b
3 1
 > sapply(x2, length)
a b
3 3
 > sapply(x3, length)
a b
3 3
 > as.matrix(x2)
   a b
1 1 0
2 2 0
3 3 0
 > as.matrix(x1)
Error in as.matrix.data.frame(x1) : dim<- length of dims do not match the 
length of object
 >

At Thursday 04:50 PM 8/14/2003 +0000, Alberto Murta wrote:
>Dear all
>
>I recently noticed the following error when cohercing a data.frame into a
>matrix:
>
> > example <- matrix(1:12,4,3)
> > example <- as.data.frame(example)
> > example$V4 <- 0
> > example
>   V1 V2 V3 V4
>1  1  5  9   0
>2  2  6 10  0
>3  3  7 11  0
>4  4  8 12  0
> > example <- as.matrix(example)
>Error in as.matrix.data.frame(example) : dim<- length of dims do not match 
>the
>length of object
>
>However, if the column to be added has the right number of lines, there's no
>error:
>
> > example <- matrix(1:12,4,3)
> > example <- as.data.frame(example)
> > example$V4 <- rep(0,4)
> > example
>   V1 V2 V3 V4
>1  1  5  9  0
>2  2  6 10  0
>3  3  7 11  0
>4  4  8 12  0
> > example <- as.matrix(example)
> > example
>   V1 V2 V3 V4
>1  1  5  9  0
>2  2  6 10  0
>3  3  7 11  0
>4  4  8 12  0
>
>Shouldn't it work well both ways? I checked the attributes and dims of the
>data frame and they are the same in both cases. Where's the difference that
>originates the error message?
>Thanks in advance
>
>Alberto
>
>platform i686-pc-linux-gnu
>arch     i686
>os       linux-gnu
>system   i686, linux-gnu
>status
>major    1
>minor    7.1
>year     2003
>month    06
>day      16
>language R
>
>
>--
>                                          Alberto G. Murta
>Institute for Agriculture and Fisheries Research (INIAP-IPIMAR)
>Av. Brasilia, 1449-006 Lisboa, Portugal | Phone: +351 213027062
>Fax:+351 213015948 | http://www.ipimar-iniap.ipimar.pt/pelagicos/
>
>______________________________________________
>R-help at stat.math.ethz.ch mailing list
>https://www.stat.math.ethz.ch/mailman/listinfo/r-help



From crmora_nc at yahoo.com  Thu Aug 14 20:28:05 2003
From: crmora_nc at yahoo.com (Christian Mora)
Date: Thu, 14 Aug 2003 11:28:05 -0700 (PDT)
Subject: [R] gnls - Step halving....
Message-ID: <20030814182805.964.qmail@web20208.mail.yahoo.com>

Hi all,

I'm working with a dataset from 10 treatments, each
treatment with 30 subjects, each subject measured 5
times. The plot of the dataset suggests that a
3-parameter logistic could be a reasonable function to
describe the data. When I try to fit the model using
gnls I got the message 'Step halving factor reduced
below minimum in NLS step'. Im using as the initial
values of the parameters those obtained from the nls
fit. Is a problem of the initial estimates of the
parameters that I get the error or could be something
else?

The code for the nls fit was:

options(contrasts=c("contr.helmert","contr.poly")) 
VA1.lis<-nlsList(DRAM~SSlogis(MED,phi1,phi2,phi3)|TRAT,
data=VA1,na.action=na.omit)

The code for the gnls fit was (using a 'difference
parameterization' like SAS):

options(contrasts=c("contr.SAS","contr.poly")) 
VA1.gnls<-gnls(DRAM~SSlogis(MED,phi1,phi2,phi3),
data=VA1,params=list(phi1~TRAT,phi2~TRAT,phi3~TRAT), 
start=c(
23.36209,  ****avg of phi1 for the 10 trts *********
-0.854979000000004,  ***** diff. between avg and nls
estimate of TRAT#2 ******
.....and so on

Id appreciate any comment

Thanks
CM



From crmora_nc at yahoo.com  Thu Aug 14 20:28:52 2003
From: crmora_nc at yahoo.com (Christian Mora)
Date: Thu, 14 Aug 2003 11:28:52 -0700 (PDT)
Subject: [R] gnls - Step halving....
Message-ID: <20030814182852.14542.qmail@web20205.mail.yahoo.com>

Hi all,

I'm working with a dataset from 10 treatments, each
treatment with 30 subjects, each subject measured 5
times. The plot of the dataset suggests that a
3-parameter logistic could be a reasonable function to
describe the data. When I try to fit the model using
gnls I got the message 'Step halving factor reduced
below minimum in NLS step'. Im using as the initial
values of the parameters those obtained from the nls
fit. Is a problem of the initial estimates of the
parameters that I get the error or could be something
else?

The code for the nls fit was:

options(contrasts=c("contr.helmert","contr.poly")) 
VA1.lis<-nlsList(DRAM~SSlogis(MED,phi1,phi2,phi3)|TRAT,
data=VA1,na.action=na.omit)

The code for the gnls fit was (using a 'difference
parameterization' like SAS):

options(contrasts=c("contr.SAS","contr.poly")) 
VA1.gnls<-gnls(DRAM~SSlogis(MED,phi1,phi2,phi3),
data=VA1,params=list(phi1~TRAT,phi2~TRAT,phi3~TRAT), 
start=c(
23.36209,  ****avg of phi1 for the 10 trts *********
-0.854979000000004,  ***** diff. between avg and nls
estimate of TRAT#2 ******
.....and so on

Id appreciate any comment

Thanks
CM



From ggrothendieck at volcanomail.com  Thu Aug 14 21:24:50 2003
From: ggrothendieck at volcanomail.com (Gabor Grothendieck)
Date: Thu, 14 Aug 2003 12:24:50 -0700 (PDT)
Subject: [R] How to get the pseudo left inverse of a singular square
	matrix?
Message-ID: <20030814192450.F3119727C@sitemail.everyone.net>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20030814/a94c747d/attachment.pl

From ocbruno at netscape.net  Thu Aug 14 21:44:39 2003
From: ocbruno at netscape.net (Marcelo)
Date: Thu, 14 Aug 2003 16:44:39 -0300
Subject: [R] Scheffe test
Message-ID: <3F3BE6A7.1040908@netscape.net>

Hi, Ana
try these source:
# Scheffe test, according Zar, J.H. (1984) Biostatistical Analysis.
# Englewood Cliffs: Prentice Hall.
# -----------------------------------------------------------------
 Scheffe1<- function(mxa,mxb,ms,na,nb,k,F) {
 # multiple comparision
 # DMSa=| meanXa- meanXb |
 # DMSc= critic value
 # if DMSa > DMSc --> reject H0
 # where mxa is mean of one group dataset,
 # and mxb is outher group dataset
 # na is number of samples for group a
 # nb is number of samples for group b
 # ms is mean square, F is F value, k is treatments
 result<-0
 DMSa<- abs(mxa-mxb)
 DMSc<- sqrt(((1/na)+(1/nb))*(k-1)*(ms)*(F))
 if (DMSa > DMSc) result<-c("rej. H0") else result<- c("ac. H0")
 return(cbind(result))
}

save as name.R in your work directory.
Hope this helps.

-- 
Ocean?logo Marcelo Alexandre Bruno 
#    Linux User: 124592    #
P?s-gradua??o Oceanografia Biol?gica
FUNDACAO UNIV. FEDERAL do RIO GRANDE
Departamento de Oceanografia
Lab. de Ecologia do Ictiopl?ncton
AV. IT?LIA km 8 s/n - CARREIROS
Cx. Postal 474
96201-900 (0xx53) 2336529
Rio Grande - RS - BRAZIL



From Ted.Harding at nessie.mcc.ac.uk  Thu Aug 14 21:41:01 2003
From: Ted.Harding at nessie.mcc.ac.uk ( (Ted Harding))
Date: Thu, 14 Aug 2003 20:41:01 +0100 (BST)
Subject: [R] How to get the pseudo left inverse of a singular square 
In-Reply-To: <00fd01c36285$cb3fe300$8bd75ba5@IE.TAMU.EDU>
Message-ID: <XFMail.030814204101.Ted.Harding@nessie.mcc.ac.uk>

On 14-Aug-03 Feng Zhang wrote:
> Thank, Jerome
> 
> The question is if this generalized inverse can make
> their product to be identity matrix?

>> On August 14, 2003 09:24 am, Feng Zhang wrote:
>> > Dear R-listers,
>> >
>> > I have a dxr matrix Z, where d > r.
>> > And the product Z*Z' is a singular square matrix.
>> > The problem is how to get the left inverse U of this
>> > singular matrix Z*Z', such that U*(Z*Z') = I?

No, not if I is to be a full identity matrix (1s all the way
along the diagonal), when Z*Z' is singular. Compare ranks
on both sides; or simply observe that

  det(U*(Z*Z')) = det(U)*det(Z*Z') = 0 while det(I) = 1.

However, you could have an incomplete diagonal (r 1s and
(n-r) 0s where r is the rank of Z*Z').

Ted.


--------------------------------------------------------------------
E-Mail: (Ted Harding) <Ted.Harding at nessie.mcc.ac.uk>
Fax-to-email: +44 (0)870 167 1972
Date: 14-Aug-03                                       Time: 20:41:01
------------------------------ XFMail ------------------------------



From chumpmonkey3 at hushmail.com  Fri Aug 15 00:25:58 2003
From: chumpmonkey3 at hushmail.com (chumpmonkey3@hushmail.com)
Date: Thu, 14 Aug 2003 15:25:58 -0700
Subject: [R] Using spline parameters to generate data
Message-ID: <200308142225.h7EMPwPC048458@mailserver2.hushmail.com>


# I need to generate some data. I'm modeling some time series that follow
a
# negative exponential decay (mostly). I have 20 samples that can easily
be fit with cubic splines.
# What I want to do is generate many thousands of similar samples using
the parameters from the splines

# For instance one data sample looks not unlike this:
foo.curve <- 1 * exp(-0.01 * 1:500) + 0.5 
ts.plot(foo.curve, lwd = 2)

# Another sample looks not unlike this:
foo.curve2 <- 0.9 * exp(-0.02 * 1:500) + 0.5 
ts.plot(foo.curve2, lwd = 2)



# They can be fit with splines easily like so:

ts.plot(foo.curve, lwd = 2)
spline.model <- smooth.spline(foo.curve)

fits4foo <- predict(spline.model)$y
spline4foo <- spline(1:500, fits4foo)
lines(spline4foo$x, spline4foo$y, col = "red", lwd = 2, lty = "dashed")

# I originally generated the data I needed by using a nls model and 
# adding some noise to the mean of the coefficents from those fits.
# However, I've been told to try and do this using splines for arcane
reasons.

# So, if you had 20 splines like spline.model above and wanted to generate
some similar data what would you do?
# Thanks in advance.









Promote security and make money with the Hushmail Affiliate Program:



From hi_ono2001 at ybb.ne.jp  Fri Aug 15 01:31:34 2003
From: hi_ono2001 at ybb.ne.jp (Hisaji Ono)
Date: Fri, 15 Aug 2003 08:31:34 +0900
Subject: [R] Contouring irregular xyz data via TIN
References: <Pine.OSF.4.31.0308141158570.16969-100000@ardilla.rz-berlin.mpg.de>
	<3F3B7BFE.6060409@agr.kuleuven.ac.be>
Message-ID: <000d01c362bc$330d7700$818001db@webgis>

Hi, Kris.

> Dear,
>
> I have XYZ data available in a MySQL database. I get it out, can plot
> the data with the plot() function, load it into a geoR datastructure.
> But what I actually would like to do is a simple contouring of the data
> based on a no Kriging interpolation such as TIN based.
>
> I know the first thing I shold do is interpolate a full matrix for the
> region I have my points for, then contour should do its work. Any idea
> which function can be used for XYZ interpolations, maybe/hopefully based
> on a Delaunay triangulation in combination with linear or spline
> interpolation. Is there a package that allows me to do this?
>

I found following URL. It's an example  for S-PLUS, so you might need some
modifications.

http://geography.uoregon.edu/courses/geog414f01/lectures/lec10.htm

I hope this would help you.



From ok at cs.otago.ac.nz  Fri Aug 15 02:06:52 2003
From: ok at cs.otago.ac.nz (Richard A. O'Keefe)
Date: Fri, 15 Aug 2003 12:06:52 +1200 (NZST)
Subject: [R] placing labels in polygon center ?
Message-ID: <200308150006.h7F06qbe071559@atlas.otago.ac.nz>

"Petr Pikal" <petr.pikal at precheza.cz> wrote (embedded in much XML):
	Not sure about efficency and it is not very general solution but
	you maybe can use embed() function

It's an interesting suggestion, but I don't see *how*.

Here's the function I want, only I'd like it to be something built in.
I've limited it to the where the rotation count is non-negative, just
to keep it simple.  For the same reason, I've limited it to vectors,
although the operation can be generalised in useful ways to matrices
and arrays with any number of subscripts.

    rotate <- function (x, count=1) {
	x[((0:(length(x)-1)) + (count %% length(x))) %% length(x) + 1]
    }

    x <- 1:5

    rotate(x)
=> 2 3 4 5 1
    rotate(x, 2)
=> 3 4 5 1 2
    rotate(x, 3)
=> 4 5 1 2 3

One important thing about this is that the result of rotate(x)
is a permutation of x; it has the same number of elements, the same
elements, only the order is changed.

The reason I don't see _how_ to use embed() to do this is that the
result of embed is a matrix (but I want a vector) which has fewer rows
that the original (but I want the same number of elements).

    embed(x, n) => a matrix with length(x)-n+1 rows and n columns.

In the polygon application, embed(xs, 2) is very close to what's needed.
If the input to the polygon functions had the first element repeated at
the end, it would be just right:
a
b =>  b a
c     c b
a     a c
The problem is that the input isn't a b c a, it's a b c.

Let's not take up too many people's time with this, OK?
I hoped there might be a built-in function I had missed; apparently
there isn't.  End of story.



From spencer.graves at pdf.com  Fri Aug 15 02:46:28 2003
From: spencer.graves at pdf.com (Spencer Graves)
Date: Thu, 14 Aug 2003 17:46:28 -0700
Subject: [R] Using spline parameters to generate data
References: <200308142225.h7EMPwPC048458@mailserver2.hushmail.com>
Message-ID: <3F3C2D64.7010204@pdf.com>

Have you considered adding noise to "predict(spline.model)$y"?  If this 
won't solve your problem, then I think I don't understand what you want 
to do.

spencer graves

chumpmonkey3 at hushmail.com wrote:
> # I need to generate some data. I'm modeling some time series that follow
> a
> # negative exponential decay (mostly). I have 20 samples that can easily
> be fit with cubic splines.
> # What I want to do is generate many thousands of similar samples using
> the parameters from the splines
> 
> # For instance one data sample looks not unlike this:
> foo.curve <- 1 * exp(-0.01 * 1:500) + 0.5 
> ts.plot(foo.curve, lwd = 2)
> 
> # Another sample looks not unlike this:
> foo.curve2 <- 0.9 * exp(-0.02 * 1:500) + 0.5 
> ts.plot(foo.curve2, lwd = 2)
> 
> 
> 
> # They can be fit with splines easily like so:
> 
> ts.plot(foo.curve, lwd = 2)
> spline.model <- smooth.spline(foo.curve)
> 
> fits4foo <- predict(spline.model)$y
> spline4foo <- spline(1:500, fits4foo)
> lines(spline4foo$x, spline4foo$y, col = "red", lwd = 2, lty = "dashed")
> 
> # I originally generated the data I needed by using a nls model and 
> # adding some noise to the mean of the coefficents from those fits.
> # However, I've been told to try and do this using splines for arcane
> reasons.
> 
> # So, if you had 20 splines like spline.model above and wanted to generate
> some similar data what would you do?
> # Thanks in advance.
> 
> 
> 
> 
> 
> 
> 
> 
> 
> Promote security and make money with the Hushmail Affiliate Program:
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help



From aiminy at iastate.edu  Fri Aug 15 03:23:53 2003
From: aiminy at iastate.edu (Aimin Yan)
Date: Thu, 14 Aug 2003 20:23:53 -0500
Subject: [R] (no subject)
Message-ID: <5.2.0.9.2.20030814202219.00bafd20@aiminy.mail.iastate.edu>

Hello,
How to use R to read data from Excel file?
and How to use R to do histogram?

Thanks,

Aimin



From ok at cs.otago.ac.nz  Fri Aug 15 04:01:02 2003
From: ok at cs.otago.ac.nz (Richard A. O'Keefe)
Date: Fri, 15 Aug 2003 14:01:02 +1200 (NZST)
Subject: [R] placing labels in polygon center ?
Message-ID: <200308150201.h7F2129w082039@atlas.otago.ac.nz>

Barry Rowlingson <B.Rowlingson at lancaster.ac.uk> wrote:
	Do you want:
	
	c(x[-1],x[1])  for a one-step 'rotation'?
	
That's just the kind of thing I did, except that it's ugly.
I've browsed src/main/subscript.c and got rather lost, but
it looks very much as though x[-1] starts by making a
logical vector c(FALSE,TRUE,...,TRUE) and then that's used
as an index to make a copy of (part of) x, then there's
x[1] (in general, x[1:n]) to be copied, and finally these
copies are pasted together.  That's 2 or 3 times as much
memory allocated as is actually kept; and it's pretty obvious
that it can be done in one quick pass with no redundant memory
allocation at the implementation level.

With rotation being so useful in APL, I rather expected that
there would be some C level implementation of rotate() or of
something better.  Note that rotate() should really have
three arguments:
    rotate(array, amount=1, axis=1)



From spencer.graves at pdf.com  Fri Aug 15 04:01:58 2003
From: spencer.graves at pdf.com (Spencer Graves)
Date: Thu, 14 Aug 2003 19:01:58 -0700
Subject: [R] (no subject)
References: <5.2.0.9.2.20030814202219.00bafd20@aiminy.mail.iastate.edu>
Message-ID: <3F3C3F16.4010608@pdf.com>

1.  Have you considered exporting to a txt or csv file and then using 
read.table?

2.  Have you considered "hist"?

3.  For other questions, I suggest you explore "www.r-project.org" -> 
search -> "R site search" before emailing this list.

hope this helps.
spencer graves

Aimin Yan wrote:
> Hello,
> How to use R to read data from Excel file?
> and How to use R to do histogram?
> 
> Thanks,
> 
> Aimin
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help



From s195404 at student.uq.edu.au  Fri Aug 15 04:04:45 2003
From: s195404 at student.uq.edu.au (Andrew C. Ward)
Date: Fri, 15 Aug 2003 02:04:45 +0000
Subject: [R] Reading Excel files and making histograms (was: no subject)
In-Reply-To: <5.2.0.9.2.20030814202219.00bafd20@aiminy.mail.iastate.edu>
References: <5.2.0.9.2.20030814202219.00bafd20@aiminy.mail.iastate.edu>
Message-ID: <1060913085.3f3c3fbd58d88@my.uq.edu.au>

Dear Aimin,

Your question about Excel comes up rather often on the list
and answers can be found in the archives in the "R Data 
Import/Export" PDF file installed with R. In short, you can
read CSV files through read.csv or you can get read
directly from Excel spreadsheets through the excellent
RODBC package. Documentation suitable for R beginners is
installed by default (look for the PDF file "An 
Introduction to R") and there is much additional material
available on the CRAN website. Also, try ?histogram for
more detailed information.


Regards,

Andrew C. Ward

CAPE Centre
Department of Chemical Engineering
The University of Queensland
Brisbane Qld 4072 Australia
andreww at cheque.uq.edu.au


Quoting Aimin Yan <aiminy at iastate.edu>:

> Hello,
> How to use R to read data from Excel file?
> and How to use R to do histogram?
> 
> Thanks,
> 
> Aimin
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
>



From chumpmonkey3 at hushmail.com  Fri Aug 15 05:21:48 2003
From: chumpmonkey3 at hushmail.com (chumpmonkey3@hushmail.com)
Date: Thu, 14 Aug 2003 20:21:48 -0700
Subject: [R] Using spline parameters to generate data
Message-ID: <200308150321.h7F3LmFR066766@mailserver1.hushmail.com>



# Sorry for the confussion.
# The way I had originally generated the data was
# sort of like this (cut and paste the code below a few times):

foo.curve <- runif(1,0.8,1.2) * exp(runif(1,-0.015,-0.005) * 1:500) +
runif(1,0.25,0.75) 
ts.plot(foo.curve, lwd = 2)

# where the min and max values in runif() were mined from the data using
nls.
# Are there similar ways to manipulate the smooth spline parameters to
give that kind of data?
# I still need a smooth line but would like the nugget, sill and range
to change 
# (to borrow terms from semivariance)

On Thu, 14 Aug 2003 17:46:28 -0700 Spencer Graves <spencer.graves at PDF.COM>
wrote:
>Have you considered adding noise to "predict(spline.model)$y"?  If
>this 
>won't solve your problem, then I think I don't understand what you
>want 
>to do.
>
>spencer graves
>
>chumpmonkey3 at hushmail.com wrote:
>> # I need to generate some data. I'm modeling some time series
>that follow
>> a
>> # negative exponential decay (mostly). I have 20 samples that
>can easily
>> be fit with cubic splines.
>> # What I want to do is generate many thousands of similar samples
>using
>> the parameters from the splines
>> 
>> # For instance one data sample looks not unlike this:
>> foo.curve <- 1 * exp(-0.01 * 1:500) + 0.5 
>> ts.plot(foo.curve, lwd = 2)
>> 
>> # Another sample looks not unlike this:
>> foo.curve2 <- 0.9 * exp(-0.02 * 1:500) + 0.5 
>> ts.plot(foo.curve2, lwd = 2)
>> 
>> 
>> 
>> # They can be fit with splines easily like so:
>> 
>> ts.plot(foo.curve, lwd = 2)
>> spline.model <- smooth.spline(foo.curve)
>> 
>> fits4foo <- predict(spline.model)$y
>> spline4foo <- spline(1:500, fits4foo)
>> lines(spline4foo$x, spline4foo$y, col = "red", lwd = 2, lty =
>"dashed")
>> 
>> # I originally generated the data I needed by using a nls model
>and 
>> # adding some noise to the mean of the coefficents from those
>fits.
>> # However, I've been told to try and do this using splines for
>arcane
>> reasons.
>> 
>> # So, if you had 20 splines like spline.model above and wanted
>to generate
>> some similar data what would you do?
>> # Thanks in advance.
>> 
>> 
>> 
>> 
>> 
>> 
>> 
>> 
>> 
>> Promote security and make money with the Hushmail Affiliate Program:
>> 
>> ______________________________________________
>> R-help at stat.math.ethz.ch mailing list
>> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
>
>
>
>









Promote security and make money with the Hushmail Affiliate Program:



From maechler at stat.math.ethz.ch  Fri Aug 15 09:17:57 2003
From: maechler at stat.math.ethz.ch (Martin Maechler)
Date: Fri, 15 Aug 2003 09:17:57 +0200
Subject: [R] Contouring irregular xyz data via TIN
In-Reply-To: <000d01c362bc$330d7700$818001db@webgis>
References: <Pine.OSF.4.31.0308141158570.16969-100000@ardilla.rz-berlin.mpg.de>
	<3F3B7BFE.6060409@agr.kuleuven.ac.be>
	<000d01c362bc$330d7700$818001db@webgis>
Message-ID: <16188.35109.38048.221985@gargle.gargle.HOWL>

>>>>> "Hisaji" == Hisaji Ono <hi_ono2001 at ybb.ne.jp>
>>>>>     on Fri, 15 Aug 2003 08:31:34 +0900 writes:

    Hisaji> Hi, Kris.
    >> Dear,
    >> 
    >> I have XYZ data available in a MySQL database. I get it out, can plot
    >> the data with the plot() function, load it into a geoR datastructure.
    >> But what I actually would like to do is a simple contouring of the data
    >> based on a no Kriging interpolation such as TIN based.
    >> 
    >> I know the first thing I shold do is interpolate a full matrix for the
    >> region I have my points for, then contour should do its work. Any idea
    >> which function can be used for XYZ interpolations, maybe/hopefully based
    >> on a Delaunay triangulation in combination with linear or spline
    >> interpolation. Is there a package that allows me to do this?
    >> 

    Hisaji> I found following URL. It's an example  for S-PLUS, so you might need some
    Hisaji> modifications.

    Hisaji> http://geography.uoregon.edu/courses/geog414f01/lectures/lec10.htm

    Hisaji> I hope this would help you.

Thank you!
Note that one of the more sophisticated solutions there uses
Akima(1978)'s polynomial interpolation scheme, function
interp() in S-plus.

R's "akima" package interp() function can do a bit more and uses
bivariate spline-interpolation {as in Akima(1996)} by default
(thanks to Albrecht Gebhardt <albrecht.gebhardt at uni-klu.ac.at>

In short:
   ## if needed, get the CRAN package "akima"
   install.packages("akima")
   library(akima)
   help(interp)
   ## now use interp():
   intxyz <- interp(x,y,z)
   with(zreg, contour(x,y,z))


Martin Maechler <maechler at stat.math.ethz.ch>	http://stat.ethz.ch/~maechler/
Seminar fuer Statistik, ETH-Zentrum  LEO C16	Leonhardstr. 27
ETH (Federal Inst. Technology)	8092 Zurich	SWITZERLAND
phone: x-41-1-632-3408		fax: ...-1228			<><



From maechler at stat.math.ethz.ch  Fri Aug 15 10:44:31 2003
From: maechler at stat.math.ethz.ch (Martin Maechler)
Date: Fri, 15 Aug 2003 10:44:31 +0200
Subject: [R] vectorization question
In-Reply-To: <5.2.1.1.2.20030814112756.04779dc0@mailhost.blackmesacapital.com>
References: <200308141650.06342.amurta@ipimar.pt>
	<5.2.1.1.2.20030814112756.04779dc0@mailhost.blackmesacapital.com>
Message-ID: <16188.40303.336296.358565@gargle.gargle.HOWL>

>>>>> "Tony" == Tony Plate <tplate at blackmesacapital.com>
>>>>>     on Thu, 14 Aug 2003 11:43:11 -0600 writes:

    Tony> From ?data.frame:
    >> Details:
    >> 
    >> A data frame is a list of variables of the same length with unique
    >> row names, given class `"data.frame"'.

    Tony> Your example constructs an object that does not
    Tony> conform to the definition of a data frame (the new
    Tony> column is not the same length as the old columns).
    Tony> Some data frame functions may work OK with such an
    Tony> object, but others will not.  For example, the print
    Tony> function for data.frame silently handles such an
    Tony> illegal data frame (which could be described as
    Tony> unfortunate.)  It would probably be far easier to
    Tony> construct a correct data frame in the first place than
    Tony> to try to find and fix functions that don't handle
    Tony> illegal data frames.  For adding a new column to a
    Tony> data frame, the expressions "x[,new.column.name] <-
    Tony> value" and "x[[new.column.name]] <- value" will
    Tony> replicate the value so that the new column is the same
    Tony> length as the existing ones, while the "$" operator in
    Tony> an assignment will not replicate the value.  (One
    Tony> could argue that this is a deficiency, but I think it
    Tony> has been that way for a long time, and the behavior is
    Tony> the same in the current version of S-plus.)

    >> x1 <- data.frame(a=1:3)
    >> x2 <- x1
    >> x3 <- x1
    >> x1$b <- 0
    >> x2[,"b"] <- 0
    >> x3[["b"]] <- 0
    >> sapply(x1, length)
    Tony> a b
    Tony> 3 1
    >> sapply(x2, length)
    Tony> a b
    Tony> 3 3
    >> sapply(x3, length)
    Tony> a b
    Tony> 3 3
    >> as.matrix(x2)
    Tony> a b
    Tony> 1 1 0
    Tony> 2 2 0
    Tony> 3 3 0
    >> as.matrix(x1)
    Tony> Error in as.matrix.data.frame(x1) : dim<- length of dims do not match the 
    Tony> length of object

Thank you, Tony.  This certainly was the most precise
explanation on this thread.

Everyone note however, that this has been improved (by Brian
Ripley) in the current R-devel {which should be come R 1.8 in October}.
There, also "$<-" assignment of data frames does check things
and in this case will do the same replication as the [,] or [[]]
assignments do.  
For back compatibility (with S-plus and earlier R versions), I'd
still recommend using bracket "[" rather than "$" assignment for
data frames.

Martin Maechler <maechler at stat.math.ethz.ch>	http://stat.ethz.ch/~maechler/
Seminar fuer Statistik, ETH-Zentrum  LEO C16	Leonhardstr. 27
ETH (Federal Inst. Technology)	8092 Zurich	SWITZERLAND
phone: x-41-1-632-3408		fax: ...-1228			<><



From maechler at stat.math.ethz.ch  Fri Aug 15 10:51:21 2003
From: maechler at stat.math.ethz.ch (Martin Maechler)
Date: Fri, 15 Aug 2003 10:51:21 +0200
Subject: [R] filter ARMA process
In-Reply-To: <Pine.GSO.4.21.0308141853180.20363-100000@amadeus.statistik.uni-dortmund.de>
References: <Pine.GSO.4.21.0308141853180.20363-100000@amadeus.statistik.uni-dortmund.de>
Message-ID: <16188.40714.295703.150617@gargle.gargle.HOWL>

>>>>> "Matthias" == Matthias Budinger <budinger at amadeus.statistik.uni-dortmund.de>
>>>>>     on Thu, 14 Aug 2003 19:01:50 +0200 (MET DST) writes:

    Matthias> Hi
    Matthias> given an ARMA process and the AR and MA coefficients I need the residuals.
    Matthias> arima() calculates the residuals together with the best AR and MA
    Matthias> coefficients, but I need the coefficients to take known values.
    Matthias> In S-PLUS there is a function arima.filt(). Is there something similar in
    Matthias> R?

Similar, yes:  arima.sim() with 0-innovations should do it.
If you look at arima.sim's source code, you see that apart from
argument checking and random generation of the innovations,
it's basically just two calls to filter(), one normal
(method = "convolution") for the MA part and one "recursive" for
the AR part.

In short: You should be able to write quite a short arima.filt()
function, using the same two calls to filter() that arima.sim()
does.

At the end you could check your arima.filt() is compatible to
S-plus' one and  donate it to the R project...

Regards,
Martin Maechler <maechler at stat.math.ethz.ch>	http://stat.ethz.ch/~maechler/
Seminar fuer Statistik, ETH-Zentrum  LEO C16	Leonhardstr. 27
ETH (Federal Inst. Technology)	8092 Zurich	SWITZERLAND
phone: x-41-1-632-3408		fax: ...-1228			<><



From bitwrit at ozemail.com.au  Fri Aug 15 10:03:05 2003
From: bitwrit at ozemail.com.au (Jim Lemon)
Date: Fri, 15 Aug 2003 18:03:05 +1000
Subject: [R] Re: [R} stars graphs
Message-ID: <20030815100504.VDHE5855.mta02.mail.mel.aone.net.au@there>

I thought about that star graph again, and realized that it would be quite 
a handy thing for visualizing cyclic data like time or compass direction. 
Here is a cleaned up (and renamed) version to do a polar plot that starts 
at the right and goes counterclockwise or a 24 hour clock plot that starts 
at the top and goes clockwise. There are probably other varieties that 
would be interesting.

Jim

-------------- next part --------------
# scales a vector of numbers to a new range

rescale<-function(x,newrange) {
 if(is.numeric(x) && is.numeric(newrange)) {
  xrange<-range(x)
  if(xrange[1] == xrange[2]) stop("rescale: can't rescale a constant vector!")
  mfac<-(newrange[2]-newrange[1])/(xrange[2]-xrange[1])
  invisible(newrange[1]+(x-xrange[1])*mfac)
 }
 else {
  cat("Usage: rescale(x,newrange)\n")
  cat("\twhere x is a numeric object and newrange is the extent of the new range\n")
 }
}

# plots data as radial lines on a 24 hour "clockface" going clockwise

clock24.star<-function(lengths,radial.pos,radial.range) {
 if(missing(radial.range)) radial.range<-range(radial.pos)
 npos<-length(radial.pos)
 newrange<-c(2.5*pi,0.5*pi)
 # rescale to a range of 0 to 2pi
 # starting at "12 o'clock" and going clockwise
 clock.radial.pos<-rescale(c(radial.pos,radial.range),newrange)[1:npos]
 clock.labels<-as.character(seq(100,2400,by=100))
 clock.label.pos<-seq(29*pi/12,pi/2,by=-pi/12)
 radial.plot(lengths,clock.radial.pos,newrange,clock.labels,clock.label.pos)
}

# plots data as radial lines starting at the right and going counterclockwise

polar.star<-function(lengths,polar.pos,polar.range,labels,label.pos,
 main="",xlab="",ylab="",...) {
 if(missing(polar.range)) polar.range<-range(polar.pos)
 npos<-length(polar.pos)
 newrange<-c(0,2*pi)
 # rescale to a range of 0 to 2pi
 radial.pos<-rescale(c(polar.pos,polar.range),newrange)[1:npos]
 if(missing(labels)) labels<-as.character(polar.pos)
 if(missing(label.pos)) label.pos<-radial.pos
 else {
  newrange<-
   c(0,2*pi*(max(label.pos)-min(label.pos))/(polar.range[2]-polar.range[1]))
  label.pos<-rescale(label.pos,newrange)
 }
 radial.plot(lengths,radial.pos,newrange,labels,label.pos,...)
}

# plots radial lines from a central origin of length 'lengths'
# at the angles specified by 'radial.pos' in radians
# starts at the 'east' position and goes counterclockwise

radial.plot<-function(lengths,radial.pos,radial.range,labels,label.pos,
 main="",xlab="",ylab="",...) {
 maxlength<-1.1*max(lengths)
 if(missing(radial.range)) radial.range<-range(radial.pos)
 plot(c(-maxlength,maxlength),c(-maxlength,maxlength),type="n",axes=FALSE,
  main=main,xlab=xlab,ylab=ylab,...)
 # get the vector of x positions
 xpos<-cos(radial.pos)*lengths
 # get the vector of y positions
 ypos<-sin(radial.pos)*lengths
 segments(0,0,xpos,ypos)
 if(missing(labels)) labels<-as.character(radial.pos)
 if(missing(label.pos)) {
  xpos<-cos(radial.pos)*maxlength
  ypos<-sin(radial.pos)*maxlength
 }
 else {
  xpos<-cos(label.pos)*maxlength
  ypos<-sin(label.pos)*maxlength
 }
 text(xpos,ypos,labels)
}

From maechler at stat.math.ethz.ch  Fri Aug 15 12:22:28 2003
From: maechler at stat.math.ethz.ch (Martin Maechler)
Date: Fri, 15 Aug 2003 12:22:28 +0200
Subject: [R] partition.tree() version for rpart ?
Message-ID: <16188.46180.410790.590519@gargle.gargle.HOWL>

The "tree" package has a nice function partition.tree() for
drawing the (1D or) 2D partition in the case of only (one or)
two predictors  {See the examples in  help(partition.tree)}.
{and so does S-plus (not in a separate package though)}.

It seems to me that there's no function with similar functionality
available for rpart.  Am I mistaken? Has anyone worked on this?

One possible approach might be to provide an  as.tree() function
for "rpart" objects and then use partition.tree() from the
"tree" package.
Given that "rpart" is a recommended R package, I'd rather prefer
to have a `standalone' function working with "rpart" objects.

BTW: an optimal name for that function is not clear:
  partition.tree could be used, since it's a simple function,
  not a "tree" method for a "partition" generic.
  parition.rpart() could be used as well.

But for me, a generic partition() function shouldn't be used to
draw (1- or) 2-partitions.  If generic/method, I'd prefer
a naming scheme something like  plotPart[.(rpart|tree)].

Feedback very welcome.
Martin Maechler <maechler at stat.math.ethz.ch>	http://stat.ethz.ch/~maechler/
Seminar fuer Statistik, ETH-Zentrum  LEO C16	Leonhardstr. 27
ETH (Federal Inst. Technology)	8092 Zurich	SWITZERLAND
phone: x-41-1-632-3408		fax: ...-1228			<><



From spencer.graves at pdf.com  Fri Aug 15 12:31:35 2003
From: spencer.graves at pdf.com (Spencer Graves)
Date: Fri, 15 Aug 2003 03:31:35 -0700
Subject: [R] Using spline parameters to generate data
References: <200308150321.h7F3LmFR066766@mailserver1.hushmail.com>
Message-ID: <3F3CB687.2090205@pdf.com>

1.  Did you plot the data within subsets to evaluate the appropriateness 
of any particular model?

2.  Did you previously use "nls" to estimate parameters in the 
exponential model to appropriate subsets of the data?  If yes, did you 
make various plots of residuals, e.g., vs. predicted to evaluate lack of 
fit, normal probability plots (qqnorm) to evaluate the distribution of 
residuals, and absolute values of residuals vs. predicted to evaluate 
homogeniety of variance.

3.  Only if residual plots suggest lack of fit would I abandon the 
exponential model.  Even then, I would be loath to embrace splines just 
because they are too unconstrained and not tied to any theoretical model 
of the phenomena of interest.  What does available theory tell you about 
possible functional forms for the model?  If an exponential does not 
hold in part of the regions of interest, what alternative functional 
forms might be suggested by the available theory?

4.  Have you also made qqplots, e.g., qqnorm, of parameters estimated 
from model fits?  My preference is to look for normality everywhere it 
make physical sense in the application, to transform, e.g., with 
logarithms or logits or probits or log(-log(yield)), where the transform 
would more likely be normal [possibly adding a small constant or 
multiplying by a number close to 1 to avoid taking logarithm of 0]. 
Then I use rnorm rather than runif.

hope this helps.
spencer graves

chumpmonkey3 at hushmail.com wrote:
> # Sorry for the confussion.
> # The way I had originally generated the data was
> # sort of like this (cut and paste the code below a few times):
> 
> foo.curve <- runif(1,0.8,1.2) * exp(runif(1,-0.015,-0.005) * 1:500) +
> runif(1,0.25,0.75) 
> ts.plot(foo.curve, lwd = 2)
> 
> # where the min and max values in runif() were mined from the data using
> nls.
> # Are there similar ways to manipulate the smooth spline parameters to
> give that kind of data?
> # I still need a smooth line but would like the nugget, sill and range
> to change 
> # (to borrow terms from semivariance)
> 
> On Thu, 14 Aug 2003 17:46:28 -0700 Spencer Graves <spencer.graves at PDF.COM>
> wrote:
> 
>>Have you considered adding noise to "predict(spline.model)$y"?  If
>>this 
>>won't solve your problem, then I think I don't understand what you
>>want 
>>to do.
>>
>>spencer graves
>>
>>chumpmonkey3 at hushmail.com wrote:
>>
>>># I need to generate some data. I'm modeling some time series
>>
>>that follow
>>
>>>a
>>># negative exponential decay (mostly). I have 20 samples that
>>
>>can easily
>>
>>>be fit with cubic splines.
>>># What I want to do is generate many thousands of similar samples
>>
>>using
>>
>>>the parameters from the splines
>>>
>>># For instance one data sample looks not unlike this:
>>>foo.curve <- 1 * exp(-0.01 * 1:500) + 0.5 
>>>ts.plot(foo.curve, lwd = 2)
>>>
>>># Another sample looks not unlike this:
>>>foo.curve2 <- 0.9 * exp(-0.02 * 1:500) + 0.5 
>>>ts.plot(foo.curve2, lwd = 2)
>>>
>>>
>>>
>>># They can be fit with splines easily like so:
>>>
>>>ts.plot(foo.curve, lwd = 2)
>>>spline.model <- smooth.spline(foo.curve)
>>>
>>>fits4foo <- predict(spline.model)$y
>>>spline4foo <- spline(1:500, fits4foo)
>>>lines(spline4foo$x, spline4foo$y, col = "red", lwd = 2, lty =
>>
>>"dashed")
>>
>>># I originally generated the data I needed by using a nls model
>>
>>and 
>>
>>># adding some noise to the mean of the coefficents from those
>>
>>fits.
>>
>>># However, I've been told to try and do this using splines for
>>
>>arcane
>>
>>>reasons.
>>>
>>># So, if you had 20 splines like spline.model above and wanted
>>
>>to generate
>>
>>>some similar data what would you do?
>>># Thanks in advance.
>>>
>>>
>>>
>>>
>>>
>>>
>>>
>>>
>>>
>>>Promote security and make money with the Hushmail Affiliate Program:
>>>
>>>______________________________________________
>>>R-help at stat.math.ethz.ch mailing list
>>>https://www.stat.math.ethz.ch/mailman/listinfo/r-help
>>
>>
>>
>>
> 
> 
> 
> 
> 
> 
> 
> 
> 
> Promote security and make money with the Hushmail Affiliate Program:
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help



From apiszcz at solarrain.com  Fri Aug 15 12:42:16 2003
From: apiszcz at solarrain.com (Al Piszcz)
Date: Fri, 15 Aug 2003 06:42:16 -0400 (EDT)
Subject: [R] Command processing of arguments
Message-ID: <Pine.LNX.4.55.0308150628320.788@l1>


In the mail archives I found a solution that sets environment parameters.
http://www.r-project.org/nocvs/mail/r-help/2001/5411.html
However I would rather not create the temp files.


-----------------------------------------------------
I am using the following method...


:: File: Rba

#!/bin/sh
##
## ARG 1 is the R program to RUN
## rest of line are arguments

if [ $# -eq 0 ] ; then
  echo
  echo "usage: Rba RPROGRAM.r ARG1 ARG2 ...."
  echo
  exit
fi


program=$1
shift
cmdargs=$*

gtime R --quiet \
  --no-save \
  $cmdargs < $program



-----------------------------------------------------
When I acquire the variables in the R program:
  userargs<-commandArgs()

  # set parameters from commandline
  inputfile<-userargs[4]
  outputfile<-userargs[5]

I get the following error/warnings


ARGUMENT 'filename1' __ignored__
ARGUMENT 'filename2' __ignored__

Does anyone know what this means?, the parameters appear
to make into my program correctly.


-----------------------------------------------------
Ideally there would get a PERL GetOpt::Long or Shell getopt
capability/module.



From bjw34032 at mh.uk.sbphrd.com  Fri Aug 15 12:54:15 2003
From: bjw34032 at mh.uk.sbphrd.com (Brandon Whitcher)
Date: Fri, 15 Aug 2003 11:54:15 +0100
Subject: [R] Re: Rwave cgt plot time axis problem
In-Reply-To: <3F2F2706.2060306@mail.smu.edu>
References: <3F2F2706.2060306@mail.smu.edu>
Message-ID: <Pine.SGI.4.53.0308151144210.2882@hcu091.ha.uk.sbphrd.com>

On Mon, 4 Aug 2003, zhu wang wrote:

> When I use the function cgt of library Rwave, the time axis of the plot
> is always on the [0,1] scale regardless of the original time. This
> happedn when I tried to reproduce some pictures of "Practical
> Time-Frequency Analysis", e.g. Figure 3.5 and 3.6 using the codes
> provided in the book. But the figures of the book display the real
> sampling time using the same codes. It seems there is no argument for
> the time scale for 'cgt'. Is this a bug or my misunderstanding?

This is the result of how the function was originally written and how
image() deals with its default x and y axis labels.  In R, they default to
[0,1] whereas in S-plus (if I recall correclty) they default to 1:nrow(x)
and 1:ncol(x) for a matrix x.

I have been trying to produce text files for each chapter (using the
examples from the book) with limited success.  That is, some of the
example fail to work.  Luckily, for chapter 3 this doesn't appear to be
the case.  The example is

## Example 3.4 Chirp
## Generate the chirp and compute the Gabor transform between frequencies
## 0 and 0.125 Hz:
x <- 1:512
chirp <- sin(2*pi*(x + 0.002*(x-256)*(x-256))/16)
par(mfrow=c(3,1))
plot(ts(chirp), xaxs="i", xlab="", ylab="")
title("Chirp signal")
cgtchirp <- cgt(chirp, 50, .005, 25)
tmp <- cleanph(cgtchirp, .01)
## The result is displayed in Figure 3.5

This works (at least on my SGI running R 1.6.2) but produces axis labels,
for the two images, of [0,1]x[0,1].

An easy work around, until I implement it myself in a new version of
Rwave, is to rewrite the function (call it mycgt) using

if(plot){
  image(1:newsize, seq(0, nvoice*freqstep/2, length=nvoice),
        Mod(output), xlab = "Time", ylab = "Frequency")
  title("Gabor Transform Modulus")
}

...for the plot statement near the bottom.  It appears to work.  A similar
hack may be performed for cleanph().

Brandon

--------------------------------------------------------------------------
 Senior Researcher in Imaging                             GlaxoSmithKline
 Research Statistics Unit              New Frontiers Science Park (South)
                                          Third Avenue, Harlow   CM19 5AW
 phone:  +44 (0)127 963 1285                               United Kingdom
 fax:  +44 (0)127 964 4004



From HADASSA.BRUNSCHWIG at Roche.COM  Fri Aug 15 13:38:15 2003
From: HADASSA.BRUNSCHWIG at Roche.COM (Brunschwig, Hadassa {PDMM~Basel})
Date: Fri, 15 Aug 2003 13:38:15 +0200
Subject: [R] again Trellis plot
Message-ID: <BC6A439CD6835749A9C7B8D8F041DFA88B11AD@rbamsem1.emea.roche.com>

Hello everyone

I've got another question on Trellis plots. I plotted my fitted model with plot(augPred(fitted model)). According to the description of this method, one can use any optional argument which is passed down to the xyplot function for Trellis plots. Additionally to the trivial plot i would like to insert a horizontal line. I tried to do it with the argument 
panel= function (...) {panel.abline(vector wich contains a value for each panel to draw a horizontal line on the y axis)}. 
The plot was overridden by this statement and i got panels with horizontal lines ( in each panel the whole vector was drawn).
How can do this differently?
Thanks a lot for answers.

Dassy



From s195404 at student.uq.edu.au  Fri Aug 15 15:09:18 2003
From: s195404 at student.uq.edu.au (Andrew C. Ward)
Date: Fri, 15 Aug 2003 13:09:18 +0000
Subject: [R] again Trellis plot
In-Reply-To: <BC6A439CD6835749A9C7B8D8F041DFA88B11AD@rbamsem1.emea.roche.com>
References: <BC6A439CD6835749A9C7B8D8F041DFA88B11AD@rbamsem1.emea.roche.com>
Message-ID: <1060952958.3f3cdb7e5207d@my.uq.edu.au>

Dear Dassy,

It's not clear if you actually had
   panel.xyplot(x,y,...)
(or whatever) inside the panel function. If not, then the
horizontal lines from panel.abline() are probably all you
could expect to get. Also, you may need to make use of the
subscripts argument to panel functions to get the correct
line drawn in each panel. I believe you already have several
examples of doing this (including one from me).


Regards,

Andrew C. Ward

CAPE Centre
Department of Chemical Engineering
The University of Queensland
Brisbane Qld 4072 Australia
andreww at cheque.uq.edu.au


Quoting "Brunschwig, Hadassa {PDMM~Basel}" 
<HADASSA.BRUNSCHWIG at Roche.COM>:

> Hello everyone
> 
> I've got another question on Trellis plots. I plotted my
> fitted model with plot(augPred(fitted model)). According
> to the description of this method, one can use any
> optional argument which is passed down to the xyplot
> function for Trellis plots. Additionally to the trivial
> plot i would like to insert a horizontal line. I tried to
> do it with the argument 
> panel= function (...) {panel.abline(vector wich contains
> a value for each panel to draw a horizontal line on the y
> axis)}. 
> The plot was overridden by this statement and i got
> panels with horizontal lines ( in each panel the whole
> vector was drawn).
> How can do this differently?
> Thanks a lot for answers.
> 
> Dassy
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
>



From zhuw at mail.smu.edu  Fri Aug 15 16:48:41 2003
From: zhuw at mail.smu.edu (zhu wang)
Date: Fri, 15 Aug 2003 14:48:41 -0000
Subject: [R] Re: Rwave cgt plot time axis problem
In-Reply-To: <Pine.SGI.4.53.0308151144210.2882@hcu091.ha.uk.sbphrd.com>
References: <3F2F2706.2060306@mail.smu.edu>
	<Pine.SGI.4.53.0308151144210.2882@hcu091.ha.uk.sbphrd.com>
Message-ID: <1060958805.1937.7.camel@zwang.stat.smu.edu>

Thanks to Brandon Whitcher. I also would like to thank Andy Liaw for his
help through personal communication.

-- 
Zhu Wang

Statistical Science Department
Southern Methodist University
Phone: (214)768-2453
Fax: (214)768-4035
Email: zhuw at mail.smu.edu

 On Fri, 2003-08-15 at 05:54, Brandon Whitcher wrote:
> On Mon, 4 Aug 2003, zhu wang wrote:
> 
> > When I use the function cgt of library Rwave, the time axis of the plot
> > is always on the [0,1] scale regardless of the original time. This
> > happedn when I tried to reproduce some pictures of "Practical
> > Time-Frequency Analysis", e.g. Figure 3.5 and 3.6 using the codes
> > provided in the book. But the figures of the book display the real
> > sampling time using the same codes. It seems there is no argument for
> > the time scale for 'cgt'. Is this a bug or my misunderstanding?
> 
> This is the result of how the function was originally written and how
> image() deals with its default x and y axis labels.  In R, they default to
> [0,1] whereas in S-plus (if I recall correclty) they default to 1:nrow(x)
> and 1:ncol(x) for a matrix x.
> 
> I have been trying to produce text files for each chapter (using the
> examples from the book) with limited success.  That is, some of the
> example fail to work.  Luckily, for chapter 3 this doesn't appear to be
> the case.  The example is
> 
> ## Example 3.4 Chirp
> ## Generate the chirp and compute the Gabor transform between frequencies
> ## 0 and 0.125 Hz:
> x <- 1:512
> chirp <- sin(2*pi*(x + 0.002*(x-256)*(x-256))/16)
> par(mfrow=c(3,1))
> plot(ts(chirp), xaxs="i", xlab="", ylab="")
> title("Chirp signal")
> cgtchirp <- cgt(chirp, 50, .005, 25)
> tmp <- cleanph(cgtchirp, .01)
> ## The result is displayed in Figure 3.5
> 
> This works (at least on my SGI running R 1.6.2) but produces axis labels,
> for the two images, of [0,1]x[0,1].
> 
> An easy work around, until I implement it myself in a new version of
> Rwave, is to rewrite the function (call it mycgt) using
> 
> if(plot){
>   image(1:newsize, seq(0, nvoice*freqstep/2, length=nvoice),
>         Mod(output), xlab = "Time", ylab = "Frequency")
>   title("Gabor Transform Modulus")
> }
> 
> ...for the plot statement near the bottom.  It appears to work.  A similar
> hack may be performed for cleanph().
> 
> Brandon
> 
> --------------------------------------------------------------------------
>  Senior Researcher in Imaging                             GlaxoSmithKline
>  Research Statistics Unit              New Frontiers Science Park (South)
>                                           Third Avenue, Harlow   CM19 5AW
>  phone:  +44 (0)127 963 1285                               United Kingdom
>  fax:  +44 (0)127 964 4004
> --------------------------------------------------------------------------



From plxmh at nottingham.ac.uk  Fri Aug 15 16:52:52 2003
From: plxmh at nottingham.ac.uk (Martin Hoyle)
Date: Fri, 15 Aug 2003 15:52:52 +0100
Subject: [R] quasipoisson, test="F" or "Chi"
Message-ID: <sf3d01dc.005@ccw0m1.nottingham.ac.uk>

Hi,

Please can someone tell me if this is correct for significance tests on count data;

1: If overdispersed, use quasipoisson, drop1(model, test="Chi"),
2: If not overdispersed, use (poisson or quasipoisson), drop1(model, test="Chi").

Thanks for your time,
Martin.

Martin Hoyle,
School of Life and Environmental Sciences,
University of Nottingham,
University Park,
Nottingham,
NG7 2RD,
UK
Webpage: http://myprofile.cos.com/martinhoyle



From solares at unsl.edu.ar  Fri Aug 15 16:57:28 2003
From: solares at unsl.edu.ar (solares@unsl.edu.ar)
Date: Fri, 15 Aug 2003 11:57:28 -0300 (ART)
Subject: [R] =?iso-8859-1?q?menubutton_don=B4t_work?=
Message-ID: <49682.170.210.173.216.1060959448.squirrel@inter14.unsl.edu.ar>

Why the variable  archOp does not take the value that be chosen in the 
menubutton?, therefore always remains as a white one, I intend to charge 
the direccion of open files in a vector and then to elect with the 
menubutton with which to work but not functions thanks.Ruben

library(tcltk)
arch<-tclVar(init=" ")
archOp<-tclVar(init=" ")
vectPath<-c()
archivos<-function(){
 f<-tkcmd("tk_getOpenFile")
 temparch<-tclvalue(f)
 assign("vectPath",c(vectPath,temparch),.GlobalEnv)
 cant<-length(vectPath)
 arch<-vectPath[cant]
 tkconfigure(mb,state="normal")
 tkadd(m, "radio", label=arch, variable="archOp", value=arch)
 tkconfigure(b2,state="normal")
}
ver<-function(n){
 tabla<-read.table(n,header=TRUE,comment.char="@")
 nbre<-names(tabla)
 tabla
}
tt<-tktoplevel()
b<-tkbutton(tt,text="abrir",command=function()archivos())
tkpack(b)

tkpack(mb <- tkmenubutton(tt, text="Datos",state="disabled"))
m <- tkmenu(mb,tearoff=FALSE) 
tkconfigure(mb,menu=m)
b2<-tkbutton(tt,text="Ver",command=function()ver(arch),state="disabled")
tkpack(b2)



From roger at ysidro.econ.uiuc.edu  Fri Aug 15 17:29:21 2003
From: roger at ysidro.econ.uiuc.edu (Roger Koenker)
Date: Fri, 15 Aug 2003 10:29:21 -0500 (CDT)
Subject: [R] Oja median 
Message-ID: <Pine.SOL.4.30.0308150822380.24507-100000@ysidro.econ.uiuc.edu>

I discovered recently that the phrase "Oja median" produces no hits in
Jonathan Baron's very valuable R search engine. I found this surprising
since I've long regarded this idea as one of the more interesting notions
in the multivariate robustness literature. To begin to remedy this oversight
I wrote a bivariate version and then decided that writing a general p-variate
version might make a nice weekend programming puzzle for R-help.

Here are a few more details:

The Oja median of n  p-variate observations minimizes over theta
in R^p the sum of the volumes of the simplices formed by theta,
and p of the observed points, the sum being taken over all n choose p
groups of p observations.  Thus, in the bivariate case we are minimizing
the sum of the areas of all triangles formed by the the point theta
and pairs of observations.  Here is a simple bivariate implementation:

oja.median <-function(x) {
	#bivariate version -- x is assumed to be an n by 2 matrix
	require(quantreg)
        n <- dim(x)[1]
        A <- matrix(rep(1:n, n), n)
        i <- A[col(A) < row(A)]
        j <- A[n + 1. - col(A) > row(A)]
        xx <- cbind(x[i,  ], x[j,  ])
        y <- xx[, 1] * xx[, 4] - xx[, 2] * xx[, 3]
        z1 <- (xx[, 4] - xx[, 2])
        z2 <-  - (xx[, 3] - xx[, 1])
        return(rq(y~cbind(z1, z2)-1)$coef)
        }

To understand the strategy, note that the area of the triangle formed
by the points x_i = (x_i1,x_i2), x_j = (x_j1,x_j2),
and theta = (theta_1,theta_2) is given by the determinant,

                                    | 1    1     1   |
	Delta(x_i, x_j, theta) = .5 |y_i1 yj1 theta_1|.
	                            |y_i2 yj2 theta_2|

Expanding the determinant in the unknown parameters theta gives
the l1 regression formulation.  Remarkably, a result of Wilks says
that if the call to rq() is replaced with a call to lm() you get
the sample mean -- this gives an impressively inefficient least
squares regression based alternative to apply(x,2,mean)!
It also provides a useful debugging check for proposed algorithms.

Obviously, the expansion of the determinant gives the same formulation
for p>2, the challenge is to find a clean way to generate the
design matrix and response vector for the general setting.

Bon weekend!

url:	www.econ.uiuc.edu/~roger/my.html	Roger Koenker
email	rkoenker at uiuc.edu			Department of Economics
vox: 	217-333-4558				University of Illinois
fax:   	217-244-6678				Champaign, IL 61820



From hovmandp at msu.edu  Fri Aug 15 17:31:26 2003
From: hovmandp at msu.edu (Peter Hovmand)
Date: Fri, 15 Aug 2003 11:31:26 -0400
Subject: [R] Is there a package for digitizing or reading jpegs, ets. in R?
Message-ID: <3F3CFCCE.6070403@msu.edu>

I'm interested in digitizing some scanned time series, and looking for a 
solution that is easy and flexible. I thought I might be able to use R's 
locator() if I could get the image into a device. I noticed that there 
are some packages like (rimage), but it (a) doesn't seem to be available 
for the Windows version and forces me to upgrade a bunch of libraries 
for my linux system. It seems like someone should have something for 
digitizing maps and/or there something very obvious that I am missing 
here. Any suggestions would be appreciated!

Peter



From aiminy at iastate.edu  Fri Aug 15 17:38:26 2003
From: aiminy at iastate.edu (Aimin Yan)
Date: Fri, 15 Aug 2003 10:38:26 -0500
Subject: [R] (no subject)
Message-ID: <5.2.0.9.2.20030815103708.03d98ea0@aiminy.mail.iastate.edu>

Hello,
I type like this ">sqlSave(channel, USArrests, rownames = "state")",
but I got the following error message
"Error in sqlSave(channel, USArrests, rownames = "state") :
         [RODBC] ERROR: Could not SQLExecute"

do you know what's wrong with this?

Aimin



From mkondrin at hppi.troitsk.ru  Sat Aug 16 04:52:03 2003
From: mkondrin at hppi.troitsk.ru (M.Kondrin)
Date: Fri, 15 Aug 2003 19:52:03 -0700
Subject: [R] menubutton =?ISO-8859-1?Q?don=B4t_work?=
In-Reply-To: <49682.170.210.173.216.1060959448.squirrel@inter14.unsl.edu.ar>
References: <49682.170.210.173.216.1060959448.squirrel@inter14.unsl.edu.ar>
Message-ID: <3F3D9C53.1050706@hppi.troitsk.ru>

solares at unsl.edu.ar wrote:
> Why the variable  archOp does not take the value that be chosen in the 
> menubutton?, therefore always remains as a white one, I intend to charge 
> the direccion of open files in a vector and then to elect with the 
> menubutton with which to work but not functions thanks.Ruben
> 
> library(tcltk)
> arch<-tclVar(init=" ")
> archOp<-tclVar(init=" ")
> vectPath<-c()
> archivos<-function(){
>  f<-tkcmd("tk_getOpenFile")
>  temparch<-tclvalue(f)
>  assign("vectPath",c(vectPath,temparch),.GlobalEnv)
>  cant<-length(vectPath)
>  arch<-vectPath[cant]
>  tkconfigure(mb,state="normal")
>  tkadd(m, "radio", label=arch, variable="archOp", value=arch)
>  tkconfigure(b2,state="normal")
> }
> ver<-function(n){
>  tabla<-read.table(n,header=TRUE,comment.char="@")
>  nbre<-names(tabla)
>  tabla
> }
> tt<-tktoplevel()
> b<-tkbutton(tt,text="abrir",command=function()archivos())
> tkpack(b)
> 
> tkpack(mb <- tkmenubutton(tt, text="Datos",state="disabled"))
> m <- tkmenu(mb,tearoff=FALSE) 
> tkconfigure(mb,menu=m)
> b2<-tkbutton(tt,text="Ver",command=function()ver(arch),state="disabled")
> tkpack(b2)
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> 
> 
archOp<-tclVar(init=" ")
............
tkadd(m, "radio", label=arch, variable="archOp", value=arch)
............
You have assigned R variable archOp to tk radio-menu. But in tcl 
interpreter archOp has different name and you may find it by command 
ls(envir=archOp) (something like ::RTclxxx). You should pass this name 
as variable parameter to tkadd(m, "radio", label=arch, 
variable=ls(envir=archOp), value=arch)



From mkondrin at hppi.troitsk.ru  Sat Aug 16 04:57:37 2003
From: mkondrin at hppi.troitsk.ru (M.Kondrin)
Date: Fri, 15 Aug 2003 19:57:37 -0700
Subject: [R] Is there a package for digitizing or reading jpegs, ets. in R?
In-Reply-To: <3F3CFCCE.6070403@msu.edu>
References: <3F3CFCCE.6070403@msu.edu>
Message-ID: <3F3D9DA1.4030908@hppi.troitsk.ru>

Peter Hovmand wrote:
> I'm interested in digitizing some scanned time series, and looking for a 
> solution that is easy and flexible. I thought I might be able to use R's 
> locator() if I could get the image into a device. I noticed that there 
> are some packages like (rimage), but it (a) doesn't seem to be available 
> for the Windows version and forces me to upgrade a bunch of libraries 
> for my linux system. It seems like someone should have something for 
> digitizing maps and/or there something very obvious that I am missing 
> here. Any suggestions would be appreciated!
> 
> Peter
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> 
> 

As far as I know there is no ready to use digitizer package for R, but 
instead of a rimage you can try pixmap package (although it requires 
images in pnm format - that is more memory consuming then jpeg one)



From a.culhane at ucc.ie  Fri Aug 15 18:06:34 2003
From: a.culhane at ucc.ie (Culhane, Aedin)
Date: Fri, 15 Aug 2003 17:06:34 +0100
Subject: [R] Merging and sorting multiple data.frame
Message-ID: <F64493091FAC4D4DAC46261FEC19679903C4EDA2@xch4.ucc.ie>

Dear R help,
I'm pretty new to R and would be grateful for help. 

I have 11 data.frames, each with 3 columns of data. Each has the same
row.names, however these are not sorted.

Please tell me the best way to sort these (by row.names) and secondly the
best way to extract data columns from these to form a merged table.

Thanks a million
Aedin



From th50 at leicester.ac.uk  Fri Aug 15 18:21:14 2003
From: th50 at leicester.ac.uk (Hotz, T.)
Date: Fri, 15 Aug 2003 17:21:14 +0100
Subject: [R] Merging and sorting multiple data.frame
Message-ID: <1F2CE8D4B0195E488213E8B8CCF7148602501322@saffron.cfs.le.ac.uk>

Dear Aedin,

Similar questions have been asked quite often on this list.

See FAQ 7.25, the manual "An Introduction to R", and the online
help, especially ?merge, ?sort, ?order, and ?data.frame.

HTH

Thomas


> -----Original Message-----
> From: Culhane, Aedin [mailto:a.culhane at ucc.ie]
> Sent: 15 August 2003 17:07
> To: 'r-help at stat.math.ethz.ch'
> Subject: [R] Merging and sorting multiple data.frame
> 
> 
> Dear R help,
> I'm pretty new to R and would be grateful for help. 
> 
> I have 11 data.frames, each with 3 columns of data. Each has the same
> row.names, however these are not sorted.
> 
> Please tell me the best way to sort these (by row.names) and 
> secondly the
> best way to extract data columns from these to form a merged table.
> 
> Thanks a million
> Aedin
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> 

---

Thomas Hotz
Research Associate in Medical Statistics
University of Leicester
United Kingdom

Department of Epidemiology and Public Health
22-28 Princess Road West
Leicester
LE1 6TP
Tel +44 116 252-5410
Fax +44 116 252-5423

Division of Medicine for the Elderly
Department of Medicine
The Glenfield Hospital
Leicester
LE3 9QP
Tel +44 116 256-3643
Fax +44 116 232-2976



From f0z6305 at labs.tamu.edu  Fri Aug 15 18:33:40 2003
From: f0z6305 at labs.tamu.edu (Feng Zhang)
Date: Fri, 15 Aug 2003 11:33:40 -0500
Subject: [R] Is it possible to separate two independent components from a
	random variable?
Message-ID: <01b701c3634a$fc508150$8bd75ba5@IE.TAMU.EDU>

Hey, R-listers,

Given the observed N random scalar variable x, with
zero mean and unit variance, can we separate the
two independent component x1 and x2 such that
x = x1 + x2 (x1 and x2 are assumed to be zero mean)?

Maybe there is no way to figure it out, and just
wanna get some help and try it.

Fred



From th50 at leicester.ac.uk  Fri Aug 15 18:55:43 2003
From: th50 at leicester.ac.uk (Hotz, T.)
Date: Fri, 15 Aug 2003 17:55:43 +0100
Subject: [R] Is it possible to separate two independent components from
	arandom variable?
Message-ID: <1F2CE8D4B0195E488213E8B8CCF7148602501324@saffron.cfs.le.ac.uk>

Dear Fred,

If x1 and x2 are *not* normally distributed, you can use
independent component analysis (ICA) which is based on the
idea that x will be "more normal" than either x1 and x2
following the central limit theorem. See package(fastICA)
by JL Marchini, C Heaton, and BD Ripley for details.

HTH

Thomas


> -----Original Message-----
> From: Feng Zhang [mailto:f0z6305 at labs.tamu.edu]
> Sent: 15 August 2003 17:34
> To: R-Help
> Subject: [R] Is it possible to separate two independent 
> components from
> arandom variable?
> 
> 
> Hey, R-listers,
> 
> Given the observed N random scalar variable x, with
> zero mean and unit variance, can we separate the
> two independent component x1 and x2 such that
> x = x1 + x2 (x1 and x2 are assumed to be zero mean)?
> 
> Maybe there is no way to figure it out, and just
> wanna get some help and try it.
> 
> Fred
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> 

---

Thomas Hotz
Research Associate in Medical Statistics
University of Leicester
United Kingdom

Department of Epidemiology and Public Health
22-28 Princess Road West
Leicester
LE1 6TP
Tel +44 116 252-5410
Fax +44 116 252-5423

Division of Medicine for the Elderly
Department of Medicine
The Glenfield Hospital
Leicester
LE3 9QP
Tel +44 116 256-3643
Fax +44 116 232-2976



From jeff_hamann at hamanndonald.com  Fri Aug 15 18:59:19 2003
From: jeff_hamann at hamanndonald.com (Jeff D. Hamann)
Date: Fri, 15 Aug 2003 09:59:19 -0700
Subject: [R] rterm not shutting down from ESS on Win32
Message-ID: <002401c3634e$91da5630$0b00a8c0@rodan>

I've been having problems with Rterm.exe not shutting down when I exit an R
(1.7.0 and 1.7.1) session from within emacs when using ESS. I've just
upgraded to 5.1.24 and still have the same problems. I'm running ntemacs and
winxp. I don't recall having these troubles with version 1.6.2 of R.

Jeff.

---
Jeff D. Hamann
Hamann, Donald and Associates, Inc.
PO Box 1421
Corvallis, Oregon USA 97339-1421
(office) 541-753-7333
(cell) 541-740-5988
jeff_hamann at hamanndonald.com
www.hamanndonald.com



From raja.surapanani at fmr.com  Fri Aug 15 19:11:34 2003
From: raja.surapanani at fmr.com (Raja Surapanani)
Date: Fri, 15 Aug 2003 13:11:34 -0400
Subject: [R] Does order() behave differently on different platforms? 
Message-ID: <200308151311.34706.raja.surapanani@fmr.com>

Hello, 

I am running R 1.7.1 on two platforms and it seems to me that order() behaves 
differently on them. I noticed this when a test case passed in linux but 
failed on solaris. This is the behavior on the solaris platform: 

R.Version()
$platform
[1] "sparc-sun-solaris2.8"

$arch

[1] "sparc"
$os
[1] "solaris2.8"

$system
[1] "sparc, solaris2.8"

$status
[1] ""

$major
[1] "1"

$minor
[1] "7.1"

$year
[1] "2003"

$month
[1] "06"

$day
[1] "16"

$language
[1] "R"

> x <- c("a", "b", "c", "(d")
> sort(x)
[1] "(d" "a"  "b"  "c" 

This is the behavior on the linux platform: 

> R.Version()
$platform
[1] "i686-pc-linux-gnu"

$arch
[1] "i686"

$os
[1] "linux-gnu"

$system
[1] "i686, linux-gnu"

$status
[1] ""

$major
[1] "1"

$minor
[1] "7.1"

$year
[1] "2003"

$month
[1] "06"

$day
[1] "16"

$language
[1] "R"

 x <- c("a", "b", "c", "(d")
> sort(x)
[1] "a"  "b"  "c"  "(d"

Am I missing something here or is there really a difference in the way order() 
works on different platforms? Thanks for your help. 

Regards, 
Raja



From Roger.Bivand at nhh.no  Fri Aug 15 19:45:09 2003
From: Roger.Bivand at nhh.no (Roger Bivand)
Date: Fri, 15 Aug 2003 19:45:09 +0200 (CEST)
Subject: [R] Is there a package for digitizing or reading jpegs, ets. in R?
In-Reply-To: <3F3D9DA1.4030908@hppi.troitsk.ru>
Message-ID: <Pine.LNX.4.44.0308151929120.15455-100000@reclus.nhh.no>

On Fri, 15 Aug 2003, M.Kondrin wrote:

> Peter Hovmand wrote:
> > I'm interested in digitizing some scanned time series, and looking for a 
> > solution that is easy and flexible. I thought I might be able to use R's 
> > locator() if I could get the image into a device. I noticed that there 
> > are some packages like (rimage), but it (a) doesn't seem to be available 
> > for the Windows version and forces me to upgrade a bunch of libraries 
> > for my linux system. It seems like someone should have something for 
> > digitizing maps and/or there something very obvious that I am missing 
> > here. Any suggestions would be appreciated!
> > 
> > Peter
> > 
> > ______________________________________________
> > R-help at stat.math.ethz.ch mailing list
> > https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> > 
> > 
> 
> As far as I know there is no ready to use digitizer package for R, but 
> instead of a rimage you can try pixmap package (although it requires 
> images in pnm format - that is more memory consuming then jpeg one)

While pixmap could be used, you would need to do a lot to register the 
image and grab the axis scales. The last time I used g3data for this, it 
delivered, and took care of all the registering:

http://www.acclab.helsinki.fi/~frantz/software/g3data.php

The current version does not seem to be available for Windows (version 
1.1.4 is), is gtk+ based, there are RPMs.

Roger

> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> 

-- 
Roger Bivand
Economic Geography Section, Department of Economics, Norwegian School of
Economics and Business Administration, Breiviksveien 40, N-5045 Bergen,
Norway. voice: +47 55 95 93 55; fax +47 55 95 93 93
e-mail: Roger.Bivand at nhh.no



From tlumley at u.washington.edu  Fri Aug 15 19:48:20 2003
From: tlumley at u.washington.edu (Thomas Lumley)
Date: Fri, 15 Aug 2003 10:48:20 -0700 (PDT)
Subject: [R] Is it possible to separate two independent components from
	a random variable?
In-Reply-To: <01b701c3634a$fc508150$8bd75ba5@IE.TAMU.EDU>
Message-ID: <Pine.A41.4.44.0308151042030.56832-100000@homer12.u.washington.edu>

On Fri, 15 Aug 2003, Feng Zhang wrote:

> Hey, R-listers,
>
> Given the observed N random scalar variable x, with
> zero mean and unit variance, can we separate the
> two independent component x1 and x2 such that
> x = x1 + x2 (x1 and x2 are assumed to be zero mean)?
>

Not without further information or constraints.
 For example
     - if x is Normal then x1 and x2 must be Normal but their
 	variances can be anything that add to 1
     - if one distribution is known completely the other can be found
     - If x1 and x2 are unimodal and x is bimodal you can find at least
	something about x1 and x2.
This last one is a type of cluster analysis.


	-thomas

Thomas Lumley			Assoc. Professor, Biostatistics
tlumley at u.washington.edu	University of Washington, Seattle



From ligges at statistik.uni-dortmund.de  Fri Aug 15 19:56:02 2003
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Fri, 15 Aug 2003 19:56:02 +0200
Subject: [R] Does order() behave differently on different platforms?
In-Reply-To: <200308151311.34706.raja.surapanani@fmr.com>
References: <200308151311.34706.raja.surapanani@fmr.com>
Message-ID: <3F3D1EB2.2000108@statistik.uni-dortmund.de>

Raja Surapanani wrote:
> Hello, 
> 
> I am running R 1.7.1 on two platforms and it seems to me that order() behaves 
> differently on them. I noticed this when a test case passed in linux but 
> failed on solaris. This is the behavior on the solaris platform: 
> 
> R.Version()
> $platform
> [1] "sparc-sun-solaris2.8"

[SNIP]

>>x <- c("a", "b", "c", "(d")
>>sort(x)
> 
> [1] "(d" "a"  "b"  "c" 
> 
> This is the behavior on the linux platform: 
> 
> 
>>R.Version()
> 
> $platform
> [1] "i686-pc-linux-gnu"

[SNIP]

>>sort(x)
> 
> [1] "a"  "b"  "c"  "(d"
> 
> Am I missing something here or is there really a difference in the way order() 
> works on different platforms? Thanks for your help. 

You missed to read the help page. ?sort tells us:

"The sort order for character vectors will depend on the collating 
sequence of the locale in use: see Comparison."

Please follow that reference and read ?Comparison as well.

Uwe Ligges



From B.Rowlingson at lancaster.ac.uk  Fri Aug 15 20:16:56 2003
From: B.Rowlingson at lancaster.ac.uk (Barry Rowlingson)
Date: Fri, 15 Aug 2003 19:16:56 +0100
Subject: [R] Is there a package for digitizing or reading jpegs, ets. in R?
In-Reply-To: <Pine.LNX.4.44.0308151929120.15455-100000@reclus.nhh.no>
References: <Pine.LNX.4.44.0308151929120.15455-100000@reclus.nhh.no>
Message-ID: <3F3D2398.102@lancaster.ac.uk>


> http://www.acclab.helsinki.fi/~frantz/software/g3data.php
> 
> The current version does not seem to be available for Windows (version 
> 1.1.4 is), is gtk+ based, there are RPMs.

I found these things on www.freshmeat.net:

http://digitizer.sourceforge.net/
  [Qt License, Windows and linux versions]


http://www.digitizeit.de/
  [shareware $US39, Free 21-day save-disabled evaluation, Windows and 
Linux versions]


Baz



From parkhurs at ariel.ucs.indiana.edu  Fri Aug 15 20:55:19 2003
From: parkhurs at ariel.ucs.indiana.edu (David Parkhurst)
Date: Fri, 15 Aug 2003 13:55:19 -0500
Subject: [R] How to reinstall rpart?
Message-ID: <008401c3635e$cb156b00$0a6cfea9@BLSPEAPARKHOM>

After entering ?library(rpart)?, I tried to plot an existing rpart tree, and
got this error message:  Error: couldn't find function "plot.rpart".
However, ??plot.rpart? does bring up the help for the function.  The same
things occur for text.rpart, although print(my.tree) does work.

So, I tried to re-install rpart using Packages  | Install from CRAN, but
then I get this message:??????????????????????????????????????????????????-

> local({a <- CRAN.packages()

+ install.packages(select.list(a[,1],,TRUE), .libPaths()[1], available=a)})

trying URL `http://cran.r-project.org/bin/windows/contrib/1.7/PACKAGES'

Content type `text/plain; charset=iso-8859-1' length 12560 bytes

opened URL

downloaded 12Kb



trying URL
`http://cran.r-project.org/bin/windows/contrib/1.7/rpart_3.1-12.zip'

Content type `application/zip' length 231827 bytes

opened URL

downloaded 226Kb



Error in unpackPkg(foundpkgs[okp, 2], pkgnames[okp], lib, installWithVers) :

        Can not remove prior installation of package

??????????????????????????????????????????????????????????????

Evidently I need to uninstall first, but I can?t figure out how.  I?ve
looked in the pdf manuals, in the R FAQ, and in the R for Windows FAQ, and
don?t find instructions for doing so.  I?d appreciate any help.  Thanks.

Dave Parkhurst



From tlumley at u.washington.edu  Fri Aug 15 22:21:16 2003
From: tlumley at u.washington.edu (Thomas Lumley)
Date: Fri, 15 Aug 2003 13:21:16 -0700 (PDT)
Subject: [R] How to reinstall rpart?
In-Reply-To: <008401c3635e$cb156b00$0a6cfea9@BLSPEAPARKHOM>
Message-ID: <Pine.A41.4.44.0308151318480.147604-100000@homer34.u.washington.edu>

On Fri, 15 Aug 2003, David Parkhurst wrote:
>
> Error in unpackPkg(foundpkgs[okp, 2], pkgnames[okp], lib, installWithVers) :
>
>         Can not remove prior installation of package
>

I have just experienced this when a previous instance of R did not shut
down properly(*). The folder was locked and couldn't be deleted or
renamed.  My solution was to restart Windows.

	-thomas


(*) This happens reproducibly when I use ESS with Xemacs on WinXP and open
a graphics window.



From kwan022 at stat.auckland.ac.nz  Fri Aug 15 22:28:56 2003
From: kwan022 at stat.auckland.ac.nz (Ko-Kang Kevin Wang)
Date: Sat, 16 Aug 2003 08:28:56 +1200 (NZST)
Subject: [R] How to reinstall rpart?
In-Reply-To: <008401c3635e$cb156b00$0a6cfea9@BLSPEAPARKHOM>
Message-ID: <Pine.LNX.4.44.0308160827110.17340-100000@stat55.stat.auckland.ac.nz>

On Fri, 15 Aug 2003, David Parkhurst wrote:

> After entering ?library(rpart)?, I tried to plot an existing rpart tree, and
> got this error message:  Error: couldn't find function "plot.rpart".
> However, ??plot.rpart? does bring up the help for the function.  The same
> things occur for text.rpart, although print(my.tree) does work.

So have you *read* the help when you do ?plot.rpart ?

Please read the example, and see how it is used.  Hint: you do not use 
plot.rpart(), but plot()


-- 
Cheers,

Kevin

------------------------------------------------------------------------------
"On two occasions, I have been asked [by members of Parliament],
'Pray, Mr. Babbage, if you put into the machine wrong figures, will
the right answers come out?' I am not able to rightly apprehend the
kind of confusion of ideas that could provoke such a question."

-- Charles Babbage (1791-1871) 
---- From Computer Stupidities: http://rinkworks.com/stupid/

--
Ko-Kang Kevin Wang
Master of Science (MSc) Student
SLC Tutor and Lab Demonstrator
Department of Statistics
University of Auckland
New Zealand
Homepage: http://www.stat.auckland.ac.nz/~kwan022
Ph: 373-7599
    x88475 (City)
    x88480 (Tamaki)



From njmulakk at uclink.berkeley.edu  Fri Aug 15 22:59:05 2003
From: njmulakk at uclink.berkeley.edu (njmulakk)
Date: Fri, 15 Aug 2003 13:59:05 -0700
Subject: [R] The snow package
Message-ID: <3F72C325@bearmail.berkeley.edu>

I have a few questions regarding the snow package:

1) Is fine-grained parallelism even possible with R/snow?

(ie, one task that spawns sub-tasks to do work, with results sent back to the 
parent task).

The example(s) only seem to be totally-independent processing (which can be 
done just as easily
with shell-script wrappers....)


2) If so, is an example available?


3. If not, then it is still worth messing with snow to see if you can get an R 
program to skip through multiple directories, doing the independent processing 
in each to avoid a potential race condition that could occur if multiple R 
process one with one .RData file.

Thanks a lot!
Nisha



From f.mattes at ucl.ac.uk  Fri Aug 15 23:15:36 2003
From: f.mattes at ucl.ac.uk (Frank Mattes)
Date: Fri, 15 Aug 2003 22:15:36 +0100
Subject: [R] help with Tukey Mean-Difference Plot
Message-ID: <p05210604bb62fa3ee812@[128.40.218.142]>

Dear R users,

I would appreciate for some advise how to generate a Tukey 
Mean-Difference Plot with the tmd function part of the lattice 
library. I have two test results (log transformed) which showing a 
correlation on a scatterplot. However the correlation line is 
parallel displaced depending on a clinical condition. I thought to a 
Tukey Mean Difference Plot would show me the difference betweenthe 
two values.
I don't have the literature how the plot gets constructed, but my understanding
from 
http://www.itl.nist.gov/div898/software/dataplot/refman1/auxillar/meandiff.htm 
is that the first from each distribution a qq plot is constructed and 
than the coardiantes of the qq-plot are transformed according the 
formula x=x+y/2 and y=x+y.

I tried this with

tmd(xyplot(a ~ b| c, data=data)) with

a and b holding my log10 transformed numerical values, and c the condition.

However I think I don't get a Tukey Mean-Difference plot. It looks to 
me that only
  x=x+y/2 and y=x+y. transformation is performed (a=a+b; b=b+a/2).
and the result is displayed as lattice graphic.

It would be great if anyone could help me to find the error in my plot.

May thanks

Frank

-- 
Frank Mattes				e-mail:	f.mattes at ucl.ac.uk
Department of Virology			fax	0044(0)207 8302854
Royal Free Hospital and 			tel	0044(0)207 8302997
University College Medical School
London



From tblackw at umich.edu  Fri Aug 15 23:43:14 2003
From: tblackw at umich.edu (Thomas W Blackwell)
Date: Fri, 15 Aug 2003 17:43:14 -0400 (EDT)
Subject: [R] help with Tukey Mean-Difference Plot
In-Reply-To: <p05210604bb62fa3ee812@[128.40.218.142]>
Message-ID: <Pine.SOL.4.44.0308151739480.523-100000@robotron.gpcc.itd.umich.edu>

Frank  -

help("tmd")  says that in R, the Tudkey Mean Difference plot is
constructed by:  x=(x+y)/2,  y=y-x.  That's different from what
you quote below, but it agrees with my memory of it.  Not sure
I understand your question.

-  tom blackwell  -  u michigan medical school  -  ann arbor  -

On Fri, 15 Aug 2003, Frank Mattes wrote:

> Dear R users,
>
> I would appreciate for some advise how to generate a Tukey
> Mean-Difference Plot with the tmd function part of the lattice
> library. I have two test results (log transformed) which showing a
> correlation on a scatterplot. However the correlation line is
> parallel displaced depending on a clinical condition. I thought to a
> Tukey Mean Difference Plot would show me the difference betweenthe
> two values.
> I don't have the literature how the plot gets constructed, but my
> understanding from
> http://www.itl.nist.gov/div898/software/dataplot/refman1/auxillar/meandiff.htm
> is that the first from each distribution a qq plot is constructed and
> than the coardiantes of the qq-plot are transformed according the
> formula x=x+y/2 and y=x+y.
>
> I tried this with
>
> tmd(xyplot(a ~ b| c, data=data)) with
>
> a and b holding my log10 transformed numerical values, and c the condition.
>
> However I think I don't get a Tukey Mean-Difference plot. It looks to
> me that only
>   x=x+y/2 and y=x+y. transformation is performed (a=a+b; b=b+a/2).
> and the result is displayed as lattice graphic.
>
> It would be great if anyone could help me to find the error in my plot.
>
> May thanks
>
> Frank
>
> --
> Frank Mattes				e-mail:	f.mattes at ucl.ac.uk
> Department of Virology			fax	0044(0)207 8302854
> Royal Free Hospital and 			tel	0044(0)207 8302997
> University College Medical School
> London



From rossini at blindglobe.net  Sat Aug 16 00:21:29 2003
From: rossini at blindglobe.net (A.J. Rossini)
Date: Fri, 15 Aug 2003 15:21:29 -0700
Subject: [R] rterm not shutting down from ESS on Win32
In-Reply-To: <002401c3634e$91da5630$0b00a8c0@rodan> (Jeff D. Hamann's
	message of "Fri, 15 Aug 2003 09:59:19 -0700")
References: <002401c3634e$91da5630$0b00a8c0@rodan>
Message-ID: <85vfsypno6.fsf@blindglobe.net>

"Jeff D. Hamann" <jeff_hamann at hamanndonald.com> writes:

> I've been having problems with Rterm.exe not shutting down when I exit an R
> (1.7.0 and 1.7.1) session from within emacs when using ESS. I've just
> upgraded to 5.1.24 and still have the same problems. I'm running ntemacs and
> winxp. I don't recall having these troubles with version 1.6.2 of R.

Known problem.  R-core claims its an Emacs problem.  It's not an ESS
problem, but we are looking into it.  Might not be solvable.  We'll
see...  (I'll have better access to a Windows box soon).

best,
-tony

-- 
A.J. Rossini     			
rossini at u.washington.edu            http://www.analytics.washington.edu/ 
Biomedical and Health Informatics   University of Washington
Biostatistics, SCHARP/HVTN          Fred Hutchinson Cancer Research Center
UW   :              FAX=206-543-3461 | moving soon to a permanent office
FHCRC: 206-667-7025 FAX=206-667-4812 | Voicemail is pretty sketchy/use Email

CONFIDENTIALITY NOTICE: This e-mail message and any attachme...{{dropped}}



From rossini at blindglobe.net  Sat Aug 16 00:24:01 2003
From: rossini at blindglobe.net (A.J. Rossini)
Date: Fri, 15 Aug 2003 15:24:01 -0700
Subject: [R] The snow package
In-Reply-To: <3F72C325@bearmail.berkeley.edu> (njmulakk@uclink.berkeley.edu's
	message of "Fri, 15 Aug 2003 13:59:05 -0700")
References: <3F72C325@bearmail.berkeley.edu>
Message-ID: <85r83mpnjy.fsf@blindglobe.net>

njmulakk <njmulakk at uclink.berkeley.edu> writes:

> I have a few questions regarding the snow package:
>
> 1) Is fine-grained parallelism even possible with R/snow?
>
> (ie, one task that spawns sub-tasks to do work, with results sent back to the 
> parent task).
>
> The example(s) only seem to be totally-independent processing (which can be 
> done just as easily
> with shell-script wrappers....)

You can use the underlying tools (rpvm or Rmpi) to do it.  I think
sockets, as well, but that might take slightly more thought.

> 2) If so, is an example available?

Not readily packaged up.

> 3. If not, then it is still worth messing with snow to see if you can get an R 
> program to skip through multiple directories, doing the independent processing 
> in each to avoid a potential race condition that could occur if multiple R 
> process one with one .RData file.

Sure.  Or just make sure to load/save to other places besides .RData.

?save will help on that.

best,
-tony

-- 
A.J. Rossini     			
rossini at u.washington.edu            http://www.analytics.washington.edu/ 
Biomedical and Health Informatics   University of Washington
Biostatistics, SCHARP/HVTN          Fred Hutchinson Cancer Research Center
UW   :              FAX=206-543-3461 | moving soon to a permanent office
FHCRC: 206-667-7025 FAX=206-667-4812 | Voicemail is pretty sketchy/use Email

CONFIDENTIALITY NOTICE: This e-mail message and any attachme...{{dropped}}



From deepayan at stat.wisc.edu  Sat Aug 16 00:53:13 2003
From: deepayan at stat.wisc.edu (Deepayan Sarkar)
Date: Fri, 15 Aug 2003 17:53:13 -0500
Subject: [R] help with Tukey Mean-Difference Plot
In-Reply-To: <p05210604bb62fa3ee812@[128.40.218.142]>
References: <p05210604bb62fa3ee812@[128.40.218.142]>
Message-ID: <200308151753.13154.deepayan@stat.wisc.edu>


"A Tour of Trellis Graphics" ( http://cm.bell-labs.com/cm/ms/departments/sia/
project/trellis/software.writing.html ) says that:


`The tmd function computes a Tukey mean and difference plot.  It is unusual 
since it takes as an argument the  output of one of the other plotting 
functions and it produces from that another similar object (of class 
trellis).  However, for each plot it transforms the data so that the x-axis 
holds the mean of the original x- and y-variables and the y-axis gives the 
difference.  The purpose of this is to allow more effective comparisons.  
Instead of comparing how well data fits the y=x line, you can look at how 
well the tmd-modified data follows a horizontal line.' 


tmd in lattice implements this, and seems to be working OK. 

Note that this can work with the output of qq as well as xyplot. This seems to 
be more general than what your reference says, which mentions only Q-Q plots. 
An example usage of that would be as follows:

x <- rnorm(100)
y <- rnorm(100)
tmd(qq(gl(2, 100) ~ c(x, y)))

To adapt your example, you have to first figure out how to produce the Q-Q 
plot you need (for which you would have to play around with your variables a 
bit (using reshape, perhaps)), and hopefully calling tmd() then would give 
you what you want.

Deepayan


On Friday 15 August 2003 16:15, Frank Mattes wrote:
> Dear R users,
>
> I would appreciate for some advise how to generate a Tukey
> Mean-Difference Plot with the tmd function part of the lattice
> library. I have two test results (log transformed) which showing a
> correlation on a scatterplot. However the correlation line is
> parallel displaced depending on a clinical condition. I thought to a
> Tukey Mean Difference Plot would show me the difference betweenthe
> two values.
> I don't have the literature how the plot gets constructed, but my
> understanding from
> http://www.itl.nist.gov/div898/software/dataplot/refman1/auxillar/meandiff.
>htm is that the first from each distribution a qq plot is constructed and
> than the coardiantes of the qq-plot are transformed according the
> formula x=x+y/2 and y=x+y.
>
> I tried this with
>
> tmd(xyplot(a ~ b| c, data=data)) with
>
> a and b holding my log10 transformed numerical values, and c the condition.
>
> However I think I don't get a Tukey Mean-Difference plot. It looks to
> me that only
>   x=x+y/2 and y=x+y. transformation is performed (a=a+b; b=b+a/2).
> and the result is displayed as lattice graphic.
>
> It would be great if anyone could help me to find the error in my plot.
>
> May thanks
>
> Frank



From ross at biostat.ucsf.edu  Sat Aug 16 01:03:26 2003
From: ross at biostat.ucsf.edu (Ross Boylan)
Date: Fri, 15 Aug 2003 16:03:26 -0700
Subject: [R] Error in model.frame
Message-ID: <1060988606.4787.41.camel@iron.libaux.ucsf.edu>

I am getting an error that I don't understand, and wonder if anyone
could explain what's going on.  I call a function defined thus:

clogit.rds<-function(formula,data,extra.data,response.prob,
                 na.action=getOption("na.action"),subset=NULL,
                 control=coxph.control()){
    method="exact"  # only option for now
    mf<-match.call()
    mf[[1]]<-as.name("model.frame")
    mf$method<-mf$control<-NULL
    mfn<-mf

    mfn$na.action<-"I"
    mfn$subset<-NULL
    nrows<-NROW(eval(mfn,parent.frame()))
etc.
At the eval on the last line, I get

Error in model.frame(formula, rownames, variables, varnames, extras,
extranames,  : 
	variable lengths differ

This is puzzling for two reasons.  First, I don't understand where the
arguments given in the error message are coming from.  They are not in
my function definition or call, and they do not seem to be the arguments
to model.frame either.

Second, I don't know why it's not working!  This is modified from clogit
in survival, and that works fine.  I've added two arguments, extra.data
(which is just a column of the data) and response.prob.  The latter has
different dimensions (response.prob = c(1, 1), in fact), but nothing in
the description of model.frame suggests that should be a problem.

model.frame itself is a wrapper on a primitive, so I can't really debug
into it.

Background:

Running R 1.7.1-1 on Debian Gnu/Linux.

In case it matters, the code I'm running was produced by hacking the
survival package, running R CMD check, and then loading the package from
the test directory that created:
library("survival", lib.loc="/home/ross/src/survival.Rcheck")

The clogit command seems to run fine.

One of the variables in the data.frame is name proband, which I notice
gets highlighted by ESS.



From ross at biostat.ucsf.edu  Sat Aug 16 01:15:41 2003
From: ross at biostat.ucsf.edu (Ross Boylan)
Date: Fri, 15 Aug 2003 16:15:41 -0700
Subject: [R] Re: Error in model.frame
In-Reply-To: <1060988606.4787.41.camel@iron.libaux.ucsf.edu>
References: <1060988606.4787.41.camel@iron.libaux.ucsf.edu>
Message-ID: <1060989341.4784.46.camel@iron.libaux.ucsf.edu>

On Fri, 2003-08-15 at 16:03, Ross Boylan wrote:
> I am getting an error that I don't understand, and wonder if anyone
> could explain what's going on.  I call a function defined thus:
> 
> clogit.rds<-function(formula,data,extra.data,response.prob,
>                  na.action=getOption("na.action"),subset=NULL,
>                  control=coxph.control()){
>     method="exact"  # only option for now
>     mf<-match.call()
>     mf[[1]]<-as.name("model.frame")
>     mf$method<-mf$control<-NULL
>     mfn<-mf
> 
>     mfn$na.action<-"I"
>     mfn$subset<-NULL
>     nrows<-NROW(eval(mfn,parent.frame()))
> etc.
> At the eval on the last line, I get
> 
> Error in model.frame(formula, rownames, variables, varnames, extras,
> extranames,  : 
> 	variable lengths differ
> 
A clue: if I call this with response.prob set to an element of the
clogit.rds argument data frame, I don't get the error.

I assume that to fix this I must somehow exclude this variable.  But I
will need it back in when I make the next call.  Suggestions?  Insights
into why this is necessary?



From tlumley at u.washington.edu  Sat Aug 16 02:07:32 2003
From: tlumley at u.washington.edu (Thomas Lumley)
Date: Fri, 15 Aug 2003 17:07:32 -0700 (PDT)
Subject: [R] Error in model.frame
In-Reply-To: <1060988606.4787.41.camel@iron.libaux.ucsf.edu>
Message-ID: <Pine.A41.4.44.0308151658190.147604-100000@homer34.u.washington.edu>

On Fri, 15 Aug 2003, Ross Boylan wrote:

> I am getting an error that I don't understand, and wonder if anyone
> could explain what's going on.  I call a function defined thus:
>
> clogit.rds<-function(formula,data,extra.data,response.prob,
>                  na.action=getOption("na.action"),subset=NULL,
>                  control=coxph.control()){
>     method="exact"  # only option for now
>     mf<-match.call()
>     mf[[1]]<-as.name("model.frame")
>     mf$method<-mf$control<-NULL
>     mfn<-mf
>
>     mfn$na.action<-"I"
>     mfn$subset<-NULL
>     nrows<-NROW(eval(mfn,parent.frame()))
> etc.
> At the eval on the last line, I get
>
> Error in model.frame(formula, rownames, variables, varnames, extras,
> extranames,  :
> 	variable lengths differ
>
> This is puzzling for two reasons.  First, I don't understand where the
> arguments given in the error message are coming from.  They are not in
> my function definition or call, and they do not seem to be the arguments
> to model.frame either.

They are the arguments to .Internal(model.frame( )) inside
model.frame.default. The traceback() function will list the call stack
after an error to avoid confusions like this.


> Second, I don't know why it's not working!  This is modified from clogit
> in survival, and that works fine.  I've added two arguments, extra.data
> (which is just a column of the data) and response.prob.  The latter has
> different dimensions (response.prob = c(1, 1), in fact), but nothing in
> the description of model.frame suggests that should be a problem.

Well, it is a problem.  model.frame() requires all the variables to be the
same length, as you can readily see for yourself
R> x<-1:10
R> y<-1
R> model.frame(y~x)
Error in model.frame(formula, rownames, variables, varnames, extras,
extranames,  :
        variable lengths differ


That's why the error message is "variable lengths differ" :)

The way to stop this is to remove response.prob from the call to
model.frame, ie, do
  mf$response.prob<-NULL
as is already done for mf$method and mf$control.


> model.frame itself is a wrapper on a primitive, so I can't really debug
> into it.
>

An Internal, not a primitive (although that doesn't simplify your life
any). Quite a lot of it is interpreted code, though the bit that checks
variable lengths isn't.

	-thomas



From ross at biostat.ucsf.edu  Sat Aug 16 01:56:14 2003
From: ross at biostat.ucsf.edu (Ross Boylan)
Date: Fri, 15 Aug 2003 16:56:14 -0700
Subject: [R] Re: Error in model.frame SOLVED
In-Reply-To: <1060989341.4784.46.camel@iron.libaux.ucsf.edu>
References: <1060988606.4787.41.camel@iron.libaux.ucsf.edu>
	<1060989341.4784.46.camel@iron.libaux.ucsf.edu>
Message-ID: <1060991774.4784.51.camel@iron.libaux.ucsf.edu>

On Fri, 2003-08-15 at 16:15, Ross Boylan wrote:
> On Fri, 2003-08-15 at 16:03, Ross Boylan wrote:
> > I am getting an error that I don't understand, and wonder if anyone
> > could explain what's going on.  I call a function defined thus:
> > 
> > clogit.rds<-function(formula,data,extra.data,response.prob,
> >                  na.action=getOption("na.action"),subset=NULL,
> >                  control=coxph.control()){
> >     method="exact"  # only option for now
> >     mf<-match.call()
> >     mf[[1]]<-as.name("model.frame")
> >     mf$method<-mf$control<-NULL
> >     mfn<-mf
> > 
> >     mfn$na.action<-"I"
> >     mfn$subset<-NULL
> >     nrows<-NROW(eval(mfn,parent.frame()))
> > etc.
> > At the eval on the last line, I get
> > 
> > Error in model.frame(formula, rownames, variables, varnames, extras,
> > extranames,  : 
> > 	variable lengths differ
> > 
> A clue: if I call this with response.prob set to an element of the
> clogit.rds argument data frame, I don't get the error.
> 
> I assume that to fix this I must somehow exclude this variable.  But I
> will need it back in when I make the next call.  Suggestions?  Insights
> into why this is necessary?

I need mf$response.prob <- NULL to make it work.
Still wonder why model.frame is so picky....
--



From emb7 at st-andrews.ac.uk  Sat Aug 16 00:36:01 2003
From: emb7 at st-andrews.ac.uk (Martin Biuw)
Date: Fri, 15 Aug 2003 23:36:01 +0100
Subject: [R] Confidence & prediction limits from bootstrapped linear model
In-Reply-To: <oprs4nlwy0b66ldk@localhost>
References: <oprs4nlwy0b66ldk@localhost>
Message-ID: <oprtyw2but8xmvrg@gatty.st-and.ac.uk>

Sorry, I sent this without a subject at first......

Hello,
Is there a simple function that calculates standard errors and confidence 
limits of response values (Y) in a linear model, where parameter estimates 
have been arrived at by bootstrapping (e.g. according to the first example 
in Angelo Canty's prsentation of the "boot" package in R-News, Dec 2002)? 
Or does anyone have any ideas as to the simplest way to write such a 
function, incorporating enough flexibility in terms of model structure etc?

Thanks!

Martin



From ihaka at stat.auckland.ac.nz  Sat Aug 16 06:27:05 2003
From: ihaka at stat.auckland.ac.nz (Ross Ihaka)
Date: Sat, 16 Aug 2003 16:27:05 +1200
Subject: [R] Is there a package for digitizing or reading jpegs, ets. in R?
References: <3F3CFCCE.6070403@msu.edu>
Message-ID: <3F3DB299.2060304@stat.auckland.ac.nz>

Peter Hovmand wrote:

> I'm interested in digitizing some scanned time series, and looking for 
> a solution that is easy and flexible. I thought I might be able to use 
> R's locator() if I could get the image into a device. I noticed that 
> there are some packages like (rimage), but it (a) doesn't seem to be 
> available for the Windows version and forces me to upgrade a bunch of 
> libraries for my linux system. It seems like someone should have 
> something for digitizing maps and/or there something very obvious that 
> I am missing here. Any suggestions would be appreciated! 

I have used "xfig" with some success.  You suck your image into xfig and 
give it a large depth.  Next you draw contours and scales over the top 
of the image.  The xfig coordinates can be converted to appropriate 
coordinates with a linear transformation.  I have done a number of 
 coastlines and topographic maps this way.



From ligges at statistik.uni-dortmund.de  Sat Aug 16 13:05:14 2003
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Sat, 16 Aug 2003 13:05:14 +0200
Subject: [R] How to reinstall rpart?
References: <008401c3635e$cb156b00$0a6cfea9@BLSPEAPARKHOM>
Message-ID: <3F3E0FEA.1BDF9748@statistik.uni-dortmund.de>



David Parkhurst wrote:
> 
> After entering ?library(rpart)?, I tried to plot an existing rpart tree, and
> got this error message:  Error: couldn't find function "plot.rpart".
> However, ??plot.rpart? does bring up the help for the function.  The same
> things occur for text.rpart, although print(my.tree) does work.
> 
> So, I tried to re-install rpart using Packages  | Install from CRAN, but
> then I get this message:??????????????????????????????????????????????????-
> 
> > local({a <- CRAN.packages()
> 
> + install.packages(select.list(a[,1],,TRUE), .libPaths()[1], available=a)})
> 
> trying URL `http://cran.r-project.org/bin/windows/contrib/1.7/PACKAGES'
> 
> Content type `text/plain; charset=iso-8859-1' length 12560 bytes
> 
> opened URL
> 
> downloaded 12Kb
> 
> trying URL
> `http://cran.r-project.org/bin/windows/contrib/1.7/rpart_3.1-12.zip'
> 
> Content type `application/zip' length 231827 bytes
> 
> opened URL
> 
> downloaded 226Kb
> 
> Error in unpackPkg(foundpkgs[okp, 2], pkgnames[okp], lib, installWithVers) :
> 
>         Can not remove prior installation of package
> 
> ??????????????????????????????????????????????????????????????
> 
> Evidently I need to uninstall first, but I can?t figure out how.  I?ve
> looked in the pdf manuals, in the R FAQ, and in the R for Windows FAQ, and
> don?t find instructions for doing so.  I?d appreciate any help.  Thanks.
> 
> Dave Parkhurst

Since Ko-Kang Kevin Wang already told that the usage of the generic
plot() instead of its method plot.rpart() (same for text() and
text.rpart(); interestingly you haven't tried print.rpart() instead of
print()) is the solution of your first questions (for details see the
article on Namespaces in the recent R News), let's take a look at the
second:

Windows locks DLLs that are in use, so I guess the package rpart was in
use while you tried to reinstall. See the R for Windows FAQ 3.8 for
details. You can uninstall packages manually as well: Just delete the
packages directory (as the default, it's
.../rw1071/library/PackageName).

Uwe Ligges



From hothorn at ci.tuwien.ac.at  Sat Aug 16 13:58:13 2003
From: hothorn at ci.tuwien.ac.at (Torsten Hothorn)
Date: Sat, 16 Aug 2003 13:58:13 +0200 (CEST)
Subject: [R] multcomp updated
Message-ID: <Pine.LNX.3.96.1030816135644.30840A-100000@thorin.ci.tuwien.ac.at>


Version 0.4-0 of the `multcomp' package has just been uploaded to CRAN.

Known bugs have been fixed and, inspired by some questions posted on
r-help, methods for `lm' and `glm' objects for the generics `simint' and
`simtest' have been implemented. Thanks to all bug-reporters!

Torsten

>From the CHANGES file:

    0.4-0 (13.08.2003)

    `simint' and `simtest' now have methods for `lm' and `glm'

    the baseline level for Dunnett contrasts can be determined by the
    `base' argument

    `contr.Dunnett' computes the MP-inverse for use with `contrasts'

    code cleanup in internal `parseformula'

    `subset' and `na.action' work now (unused factor levels are removed)

    we now check if the contrasts passed to `simint' and `simtest'
    are estimable (and give a warning if this is not the case)



From kjetil at entelnet.bo  Sat Aug 16 16:39:25 2003
From: kjetil at entelnet.bo (kjetil brinchmann halvorsen)
Date: Sat, 16 Aug 2003 10:39:25 -0400
Subject: [R] unclass
Message-ID: <3F3E09DD.8137.15FE4F@localhost>

Have I been sleeping in class?

rw1071 from CRAN, windows XP

incidencia is made by a call to tapply

> class(incidencia)
[1] "array"
> incidencia <- unclass(incidencia)
> class(incidencia)
[1] "array"


Kjetil Halvorsen



From hothorn at ci.tuwien.ac.at  Sat Aug 16 16:56:32 2003
From: hothorn at ci.tuwien.ac.at (Torsten Hothorn)
Date: Sat, 16 Aug 2003 16:56:32 +0200 (CEST)
Subject: [R] unclass
In-Reply-To: <3F3E09DD.8137.15FE4F@localhost>
Message-ID: <Pine.LNX.3.96.1030816165029.31426B-100000@thorin.ci.tuwien.ac.at>


> Have I been sleeping in class?
> 
> rw1071 from CRAN, windows XP
> 
> incidencia is made by a call to tapply
> 
> > class(incidencia)
> [1] "array"
> > incidencia <- unclass(incidencia)
> > class(incidencia)
> [1] "array"
> 
> 

`unclass' only removes the `class' attribute. However, arrays do not own
one (see the details in ?class) but its class is defined implicitly:

R> x <- array(rnorm(25), dim=c(5,5))
R> mode(x)
[1] "numeric"
R> class(x)
[1] "matrix"
R> attributes(x)
$dim
[1] 5 5

so nothing to remove here, in contrast to data.frames:

R> x <- as.data.frame(matrix(rnorm(25), ncol=5))
R> attributes(x)
$names
[1] "V1" "V2" "V3" "V4" "V5"

$row.names
[1] "1" "2" "3" "4" "5"

$class
[1] "data.frame"

R> unclass(x)
$V1
[1]  1.4085747  0.2405006  0.7056615 -0.4747142  0.5846174

$V2
[1]  1.06994019 -0.26797839 -0.54426997 -3.34744272  0.05473644

$V3
[1] -0.75277466  0.05542274 -1.13621279  1.06582063 -1.04080035

$V4
[1]  0.3942039867 -0.2680249001  0.0002451802 -2.4269886369  0.8719898617

$V5
[1]  1.186167  0.374248  0.832176  1.030344 -1.606450

attr(,"row.names")
[1] "1" "2" "3" "4" "5"

Best

Torsten

> Kjetil Halvorsen
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> 
>



From ligges at statistik.uni-dortmund.de  Sat Aug 16 17:00:12 2003
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Sat, 16 Aug 2003 17:00:12 +0200
Subject: [R] unclass
In-Reply-To: <3F3E09DD.8137.15FE4F@localhost>
References: <3F3E09DD.8137.15FE4F@localhost>
Message-ID: <3F3E46FC.1060609@statistik.uni-dortmund.de>

kjetil brinchmann halvorsen wrote:
> Have I been sleeping in class?
> 
> rw1071 from CRAN, windows XP
> 
> incidencia is made by a call to tapply
> 
> 
>>class(incidencia)
> 
> [1] "array"
> 
>>incidencia <- unclass(incidencia)
>>class(incidencia)
> 
> [1] "array"
> 
> 
> Kjetil Halvorsen
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help

See ?class:

'If the object does not have a class attribute, it has an implicit 
class, "matrix", "array" or the result of mode(x).'

and ?unclass:

'unclass returns (a copy of) its argument with its class attribute removed.'


Since the return of value "array" had to be introduced for S4 
compatibility (all objects must have classes according to the green book 
- "array" is an implicit class), unclass() cannot remove that 
non-existant attribute.

BTW: oldClass(incidencia) still returns NULL ..., doesn't it?

Uwe Ligges



From maechler at stat.math.ethz.ch  Sat Aug 16 17:23:17 2003
From: maechler at stat.math.ethz.ch (Martin Maechler)
Date: Sat, 16 Aug 2003 17:23:17 +0200
Subject: [R] unclass
In-Reply-To: <3F3E09DD.8137.15FE4F@localhost>
References: <3F3E09DD.8137.15FE4F@localhost>
Message-ID: <16190.19557.536562.703540@gargle.gargle.HOWL>

 [BCC: to JMC]
>>>>> "kjetil" == kjetil brinchmann halvorsen <kjetil at entelnet.bo>
>>>>>     on Sat, 16 Aug 2003 10:39:25 -0400 writes:

    kjetil> Have I been sleeping in class?  

yeah, probably the one on S4 classes/methods and the NEWS that
the methods package is now always attached by default      ;-) :-0)
(yes, I'm joking).

    kjetil>    ...

    >> class(incidencia)
    kjetil> [1] "array"
    >> incidencia <- unclass(incidencia); class(incidencia)
    kjetil> [1] "array"

More specific: When the "methods" package is attached, every
object has a class, and I'd expect unclass() to be no-op.

HOWEVER, that's not quite true:

1) unclass() remains a no-op for "array" objects, even after detaching "methods".
   In R 1.6.1, we had

     > class(unclass(array(1:6, 1:3)))
     [1] "array"
     > detach("package:methods")
     > class(unclass(array(1:6, 1:3)))
     NULL
     > 

   This is different in R 1.7.1  (the class remains "array"),
   and this is even true when "methods" was never attached
   (i.e. when starting R with no default packages).

  I'll let other R-corers explain why this might be desired.
  We might consider  unclass(x) produce a warning whenever `x'
  has a class and that is not changed.

2) unclass() is not no-op for e.g. data frames; it returns a
   list, in both cases {"methods" attached, or not, the
   difference being that "list" is a class in the first case,
   but not the second}.  
  
  Note that this is very necessary: If  unclass() had become a
  no-op as soon as "methods" was attached, a lot of R code would
  break, notably also the case of  unclass(<factor>) !

--

So, thank you, Kjetil, this was an interesting question and I've
learned, too!

Martin



From spencer.graves at pdf.com  Sat Aug 16 17:23:32 2003
From: spencer.graves at pdf.com (Spencer Graves)
Date: Sat, 16 Aug 2003 08:23:32 -0700
Subject: [R] unclass
References: <3F3E09DD.8137.15FE4F@localhost>
Message-ID: <3F3E4C74.8080006@pdf.com>

I found an answer in "?unclass" and a minor difference between R 1.7.1 
and S-Plus 6.1.2:

First ?unclass in R 1.7.1 under Windows 2000:

      Many R objects have a `class' attribute, a character vector giving
      the names of the classes which the object ``inherits'' from. If
      the object does not have a class attribute, it has an implicit
      class, `"matrix"', `"array"' or the result of `mode(x)'.
...
      The function `class' prints the vector of names of classes an
      object inherits from.
...
      `unclass' returns (a copy of) its argument with its class
      attribute removed.

	  Thus, "incidencia <- unclass(incidencia)" removed the class attribute 
from "incidencia".  Then consistent with the documentation, 
"class(incidencia)" has an implicit class "array";  if it were not a 
matrix or array, then "class(incidencia)" would have returned the result 
of mode(x).

Now, a trivial comparison between R 1.7.1 and S-Plus 6.1.2:

##R 1.7.1
 > tst <- 2
 > class(tst)
[1] "numeric"
 > Tst <- unclass(tst)
 > class(Tst)
[1] "numeric"
 > mode(Tst)
[1] "numeric"

## Same thing in S-Plus 6.1.2:
 > tst <- 2
 > class(tst)
[1] "integer"
 > Tst <- unclass(tst)
 > class(Tst)
[1] "integer"
 > mode(Tst)
[1] "numeric"

Note that "tst" and "Tst" have class and mode "numeric" in R 1.7.1 but 
class "integer" and mode "numeric" in S-Plus 6.1.2.

hope this helps.  spencer graves

kjetil brinchmann halvorsen wrote:
> Have I been sleeping in class?
> 
> rw1071 from CRAN, windows XP
> 
> incidencia is made by a call to tapply
> 
> 
>>class(incidencia)
> 
> [1] "array"
> 
>>incidencia <- unclass(incidencia)
>>class(incidencia)
> 
> [1] "array"
> 
> 
> Kjetil Halvorsen
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help



From RML27 at cornell.edu  Sat Aug 16 19:34:59 2003
From: RML27 at cornell.edu (Ronnen Levinson)
Date: Sat, 16 Aug 2003 10:34:59 -0700
Subject: [R] Prediction Intervals (reposting)
Message-ID: <3F3E6B43.9040502@cornell.edu>

(I'm reposting this message because the original has not appeared after 
about 2 days. Sorry if it shows up twice.)


Hello.

First, thanks to those who responded to my recent inquiry about using 
contour() over arbitrary (x,y) by mentioning the interp() function in 
the akima package. That worked nicely. Now for a new question:

I would like to use a pair of prediction intervals to graphically bound 
the noise in some y(x) measurements. Here's an artificial example 
showing a function y(x)=x + noise, where the noise diminishes as x 
increases from 0 to 1.

    x=seq(0,1,0.01)                       
    y=x+runif(length(x),-1,1)*((1-x)/5)
    fit=lm(y ~ 1 + x)
    pred=predict(fit, interval="prediction")
    matplot(x,pred,type="l",ylab="y")
    points(x,y)

I would have expected the lower and upper prediction intervals to 
converge as x increases (and the noise decreases), but they seem to 
remain virtually equidistant. Can anyone explain (a) the behavior that I 
see, and (b) how to obtain curves that do bound the noise?

Thanks,

Ronnen.

-- 
Ronnen Levinson, Ph.D.            \/      RML27 at cornell.edu
scientist                         ||      http://ronnen.com
Lawrence Berkeley National Lab    /\      fax 425.955.1992
 
======================================
I took a speed reading course and read 'War and Peace' in twenty minutes. It involves Russia. 
-- Woody Allen



From M.Bianchi at nrcl.com  Sat Aug 16 19:45:21 2003
From: M.Bianchi at nrcl.com (Marco Bianchi)
Date: Sat, 16 Aug 2003 18:45:21 +0100
Subject: [R] equivalent of Splus command axis(..., srt=45, adj=1) in R
Message-ID: <A18B69DC41B1D31198940000E85FF41BAE8AE4@mailhost.nrcl.com>

Dear R-users
moving from Splus to R (under Windows), I notice that command "srt" within a
plot in the original Splus code is ignored by R.
For example, if a plot something and then include the line of code

	axis(1, at=Time, labels=Text, srt=45, adj=1)

the srt=45 command within the axis() command fails to draw Text at 45
degrees on the x-axis in R but it works properly in Splus.
I will go through the relevant documentation in the R manuals but if anyone
has already had this problem and/or knows the corresponding command of
srt=45 in R, please let me know.

Kind regards
Marco Bianchi



From ligges at statistik.uni-dortmund.de  Sat Aug 16 20:04:48 2003
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Sat, 16 Aug 2003 20:04:48 +0200
Subject: [R] Prediction Intervals (reposting)
In-Reply-To: <3F3E6B43.9040502@cornell.edu>
References: <3F3E6B43.9040502@cornell.edu>
Message-ID: <3F3E7240.6030402@statistik.uni-dortmund.de>

Ronnen Levinson wrote:
> (I'm reposting this message because the original has not appeared after 
> about 2 days. Sorry if it shows up twice.)
> 
> 
> Hello.
> 
> First, thanks to those who responded to my recent inquiry about using 
> contour() over arbitrary (x,y) by mentioning the interp() function in 
> the akima package. That worked nicely. Now for a new question:
> 
> I would like to use a pair of prediction intervals to graphically bound 
> the noise in some y(x) measurements. Here's an artificial example 
> showing a function y(x)=x + noise, where the noise diminishes as x 
> increases from 0 to 1.
> 
>    x=seq(0,1,0.01)                          
> y=x+runif(length(x),-1,1)*((1-x)/5)
>    fit=lm(y ~ 1 + x)
>    pred=predict(fit, interval="prediction")
>    matplot(x,pred,type="l",ylab="y")
>    points(x,y)
> 
> I would have expected the lower and upper prediction intervals to 
> converge as x increases (and the noise decreases), but they seem to 
> remain virtually equidistant. Can anyone explain (a) the behavior that I 
> see, and (b) how to obtain curves that do bound the noise?
> 
> Thanks,
> 
> Ronnen.
> 


Well, since you use an "ordinary" lm(), for your residuals Var(e)=const. 
  is assumed (among other assumptions), hence a global variance is also 
considered and estimated for prediction, of course:

  I(x_0) := x'_0 +- t_{n-h-1; 1-\alpha/2}
         \sqrt{s^2 (x'_0 (X'X)^{-1} x_0 + 1)}
               ^^^

You have to choose a completely different model.

Uwe Ligges



From roger at ysidro.econ.uiuc.edu  Sat Aug 16 20:51:09 2003
From: roger at ysidro.econ.uiuc.edu (Roger Koenker)
Date: Sat, 16 Aug 2003 13:51:09 -0500 (CDT)
Subject: [R] Prediction Intervals (reposting)
In-Reply-To: <3F3E6B43.9040502@cornell.edu>
Message-ID: <Pine.SOL.4.30.0308161305100.25996-100000@ysidro.econ.uiuc.edu>

The usual assumption in regression is that noise is iid, that is has the same
distribution except for a location shift at all values of x.  Prediction intervals
adopting this premise are based on an average of the noise over the whole sample
and thus produce the result you see in your figure.

Your model is what I would call a linear location scale model, that is both
location and scale of the noise are linear in x...one way to generate prediction bands
for this sort of model is to fit conditional quantile functions.  Here is an
example:


     x=seq(0,1,0.01)
     y=x+runif(length(x),-1,1)*((1-x)/5)
     require(quantreg)
     abline(rq(y~x,tau=.9)$coef[,1])
     abline(rq(y~x,tau=.1)$coef[,1])

the resulting band contains 80 percent of the sample observations.  You can
get some further details from ?rq and the references their.


url:	www.econ.uiuc.edu/~roger/my.html	Roger Koenker
email	rkoenker at uiuc.edu			Department of Economics
vox: 	217-333-4558				University of Illinois
fax:   	217-244-6678				Champaign, IL 61820

On Sat, 16 Aug 2003, Ronnen Levinson wrote:

> (I'm reposting this message because the original has not appeared after
> about 2 days. Sorry if it shows up twice.)
>
>
> Hello.
>
> First, thanks to those who responded to my recent inquiry about using
> contour() over arbitrary (x,y) by mentioning the interp() function in
> the akima package. That worked nicely. Now for a new question:
>
> I would like to use a pair of prediction intervals to graphically bound
> the noise in some y(x) measurements. Here's an artificial example
> showing a function y(x)=x + noise, where the noise diminishes as x
> increases from 0 to 1.
>
>     x=seq(0,1,0.01)
>     y=x+runif(length(x),-1,1)*((1-x)/5)
>     fit=lm(y ~ 1 + x)
>     pred=predict(fit, interval="prediction")
>     matplot(x,pred,type="l",ylab="y")
>     points(x,y)
>
> I would have expected the lower and upper prediction intervals to
> converge as x increases (and the noise decreases), but they seem to
> remain virtually equidistant. Can anyone explain (a) the behavior that I
> see, and (b) how to obtain curves that do bound the noise?
>
> Thanks,
>
> Ronnen.
>
> --
> Ronnen Levinson, Ph.D.            \/      RML27 at cornell.edu
> scientist                         ||      http://ronnen.com
> Lawrence Berkeley National Lab    /\      fax 425.955.1992
>
> ======================================
> I took a speed reading course and read 'War and Peace' in twenty minutes. It involves Russia.
> -- Woody Allen
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
>



From MSchwartz at medanalytics.com  Sat Aug 16 23:39:26 2003
From: MSchwartz at medanalytics.com (Marc Schwartz)
Date: Sat, 16 Aug 2003 16:39:26 -0500
Subject: [R] equivalent of Splus command axis(..., srt=45, adj=1) in R
In-Reply-To: <A18B69DC41B1D31198940000E85FF41BAE8AE4@mailhost.nrcl.com>
References: <A18B69DC41B1D31198940000E85FF41BAE8AE4@mailhost.nrcl.com>
Message-ID: <1061069965.6361.15.camel@localhost>

On Sat, 2003-08-16 at 12:45, Marco Bianchi wrote:
> Dear R-users
> moving from Splus to R (under Windows), I notice that command "srt" within a
> plot in the original Splus code is ignored by R.
> For example, if a plot something and then include the line of code
> 
> 	axis(1, at=Time, labels=Text, srt=45, adj=1)
> 
> the srt=45 command within the axis() command fails to draw Text at 45
> degrees on the x-axis in R but it works properly in Splus.
> I will go through the relevant documentation in the R manuals but if anyone
> has already had this problem and/or knows the corresponding command of
> srt=45 in R, please let me know.
> 
> Kind regards
> Marco Bianchi


In R, the axis command does not accept or utilize par("srt"), thus
passing it as an argument has no effect.

A trick, based upon an initial post from Uwe Ligges some time back was
reflected in a post that I had this past May:

http://maths.newcastle.edu.au/~rking/R/help/03a/5153.html

The relevant example code is:

# First draw the plot without axes
plot(1:10, xlab = "My label", axes = FALSE) 

# Now plot the x axis, but without labels
axis(1, at=seq(1, 10, by=2), labels = FALSE) 

# Now draw the textual axis labels
text(seq(1, 10, by = 2), par("usr")[3] - 0.2, 
     labels = c("first", "second", "third", "fourth", "fifth"), 
     srt = 45, pos = 1, xpd = TRUE) 

By using text(), which is normally used to put text within the plot
region, you can utilize par("srt"). However, you also need to combine
this with par("xpd") so that the text will not be clipped at the plot
region, thus not show.

By adjusting the negative offset of 'par("usr")[3] - 0.2', you can 
move the labels vertically to account for the length of the text 
components. A larger value moves the text down further below 
par("usr")[3], which is the value of y at the intersection of the 
x-axis.

HTH,

Marc Schwartz



From mihastaut at hotmail.com  Sun Aug 17 14:07:45 2003
From: mihastaut at hotmail.com (Miha STAUT)
Date: Sun, 17 Aug 2003 12:07:45 +0000
Subject: [R] (no subject)
Message-ID: <BAY2-F126rDs9aAOPpB0002c8c0@hotmail.com>

Hi all,

>str(df)
`data.frame':   31837 obs. of  3 variables:
$ x : num  410683 410700 410720 410740 410324 ...
$ y : num  43136 43126 43123 43125 42709 ...
$ wz: num  -101.1  -94.9  -93.3  -94.5   30.8 ...

>library(gstat)

>g<-gstat(id="rv",form=wz~1,loc=~x+y,data=df,model=mat,nmax=500,set=list(average=1))

>str(g)
List of 3
$ data :List of 1
  ..$ rv:List of 10
  .. ..$ formula      :Class 'formula' length 3 wz ~ 1
  .. .. .. ..- attr(*, ".Environment")=length 9 <environment>
  .. ..$ locations    :Class 'formula' length 2 ~x + y
  .. .. .. ..- attr(*, ".Environment")=length 9 <environment>
  .. ..$ data         :`data.frame':    31837 obs. of  3 variables:
  .. .. ..$ x : num [1:31837] 410683 410700 410720 410740 410324 ...
  .. .. ..$ y : num [1:31837] 43136 43126 43123 43125 42709 ...
  .. .. ..$ wz: num [1:31837] -101.1  -94.9  -93.3  -94.5   30.8 ...
  .. ..$ has.intercept: int 1
  .. ..$ beta         : num(0)
  .. ..$ nmax         : num 500
  .. ..$ maxdist      : num Inf
  .. ..$ dummy        : logi FALSE
  .. ..$ vfn          : int 1
  .. ..$ weights      : NULL
$ model:List of 1
  ..$ rv:Classes variogram.model  and `data.frame':     1 obs. of  9 
variables:
  .. ..$ model: Factor w/ 16 levels "Nug","Exp","Sph",..: 5
  .. ..$ psill: num 1652
  .. ..$ range: num 126
  .. ..$ kappa: num 1.75
  .. ..$ ang1 : num 0
  .. ..$ ang2 : num 0
  .. ..$ ang3 : num 0
  .. ..$ anis1: num 1
  .. ..$ anis2: num 1
  .. ..- attr(*, "singular")= logi FALSE
$ set  :List of 1
  ..$ average: num 1
- attr(*, "class")= chr [1:2] "gstat" "list"

>str(new)
str(new)
`data.frame':   580176 obs. of  3 variables:
$ x  : num  402292 402302 402312 402322 402332 ...
$ y  : num  43212 43212 43212 43212 43212 ...
$ rok: num  NA NA NA NA NA NA NA NA NA NA ...

>k<-predict.gstat(g,new=new,mask=new$rok)
Error in gstat.load.set(object$set) : error occured when parsing command: 
set average = 1;


If I do not set average=1 I continue getting the singular matrix error.

I see only one solution for this problem. That is o get rid if duplicate 
data in
some other way. As I am among the beginners in R and I do not speak other
programming languages either, I still did not find a successful way to krige
this data with a matern model varigram (which in my opinion suites best).

I also tried with the independent version of Gstat (2.4.2 if I remember
correctly) in combination with GRASS but seems that matern is not 
implemented in
it. Is that correct?

I am not a master of geostatistics either. Is there some other way to fit an 
S
shaped experimental variogram. Gaussian model would not fit that good, but 
at
least is an approximation.

thanks, Miha Staut



From parkhurs at ariel.ucs.indiana.edu  Sun Aug 17 17:50:35 2003
From: parkhurs at ariel.ucs.indiana.edu (David Parkhurst)
Date: Sun, 17 Aug 2003 10:50:35 -0500
Subject: [R] How to reinstall rpart?
References: <008401c3635e$cb156b00$0a6cfea9@BLSPEAPARKHOM>
	<3F3E0FEA.1BDF9748@statistik.uni-dortmund.de>
Message-ID: <002701c364d7$50e643c0$0a6cfea9@BLSPEAPARKHOM>

Thanks to Uwe Ligges, who wrote:

> Since Ko-Kang Kevin Wang already told that the usage of the generic
> plot() instead of its method plot.rpart() (same for text() and
> text.rpart(); interestingly you haven't tried print.rpart() instead of
> print()) is the solution of your first questions (for details see the
> article on Namespaces in the recent R News), let's take a look at the
> second:
What threw me off here was a difference between splus and R.  I had used
plot.rpart and text.rpart successfully in splus, and then I copied my
entries from a text file, and tried to use them in R.  I forgot that the
previous usage had been with splus.

> Windows locks DLLs that are in use, so I guess the package rpart was in
> use while you tried to reinstall. See the R for Windows FAQ 3.8 for
> details. You can uninstall packages manually as well: Just delete the
> packages directory (as the default, it's
> .../rw1071/library/PackageName).
I don't think the package was in use at the time.  I had exitted from R,
then called it up again without loading the rpart library, when I got the
error message I referred to.  (I had not rebooted the machine, however.)

Thanks to all for their suggestions.  I've fixed the first problem by using
plot instead of plot.rpart, and I've fixed the second by deleting the rpart
folder from libraries, and then reinstalling it from CRAN.

Dave



From members at fibmarkets.com  Sun Aug 17 19:06:02 2003
From: members at fibmarkets.com (FibMarkets.com)
Date: Sun, 17 Aug 2003 10:06:02 -0700
Subject: [R] FibMaster charts..
Message-ID: <200308171706.h7HH62Q27756@server1.purebytes.com>

I have posted some charts for this week, showing critical support 
and resistance areas, and possible trade setups..

These are for stocks MO, TTEK, IBM, MERQ.. I may post some more before the weekend is over. Check back in future, look for
updates as these charts develop.

To see the charts, log in here:
<http://www.fibmarkets.com/cgi-local/yabb/YaBB.pl>
and then click on the "Traders Connection" link.

You are invited to post your own charts, your own studies.. 
Let's discuss them..

Have a great week in the markets!
-FibMaster.



FibMarkets is the virtual-community for Fibonacci/Gann/Elliott
traders, visit us at http://www.FibMarkets.com

You are receiving this message as a member of our email list. 

Click the following link to edit your account profile and change the
lists you belong to:
<http://fibmarkets.com/cgi-local/perlfect/subscribe/subscribe.cgi?op=edit&email=r-help at stat.math.ethz.ch&id=66846>

FibMarkets.com provides an information service for investors/traders,
this is not a recommendation to buy or sell securities, nor an offer to buy 
or sell securities. The publishers of the FibMarkets.com web and email 
services are not brokers and are not acting in any way to influence the 
purchase of any security. The information provided is obtained from sources 
deemed reliable, but is not guaranteed as to accuracy or completeness. It 
is possible, at this or some subsequent date, that FibMarkets.com employees,
partners, and/or affiliates of any or all may own, buy or sell securities 
presented. FibMarkets.com, owners or investors, are not liable for any 
losses or damages, monetary or otherwise, that result from the content of 
our web pages or email communications. FibMarkets.com recommends that 
anyone trading securities should do so with caution and consult with a 
broker before doing so. Past performance is not indicative of future 
performance. Securities presented in the FibMarkets.com web and email 
communications should be considered speculative, with a high degree of 
volatility & risk.

Any redistribution of the above information, without prior written 
consent FibMarkets.com is strictly prohibited. 

Copyright 2001-2002 FibMarkets.com. All Rights Reserved.



From ggrothendieck at volcanomail.com  Sun Aug 17 19:50:55 2003
From: ggrothendieck at volcanomail.com (Gabor Grothendieck)
Date: Sun, 17 Aug 2003 10:50:55 -0700 (PDT)
Subject: [R] collapse argument on paste
Message-ID: <20030817175055.6D67141C0@sitemail.everyone.net>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20030817/b141f99f/attachment.pl

From spencer.graves at pdf.com  Sun Aug 17 20:13:15 2003
From: spencer.graves at pdf.com (Spencer Graves)
Date: Sun, 17 Aug 2003 11:13:15 -0700
Subject: [R] collapse argument on paste
References: <20030817175055.6D67141C0@sitemail.everyone.net>
Message-ID: <3F3FC5BB.2010404@pdf.com>

The issue you identified is explained in section 3.6 Calling Conventions 
for Functions in Venables and Ripley (2002) Modern Applied Statistics in 
S, 4th ed. (Springer, p. 56):  "... specified arguments occurring after 
the ... argument on the definition must be named exactly".  Since 
"collaps" does not match exactly "collapse", it is interpreted as one of 
the "..." arguments.

hope this helps.
spencer graves

Gabor Grothendieck wrote:
> One gets a different response when abbreviating collapse= in 
> paste?  In the second case, it appears to be acting as if " + " is just 
> another argument to be pasted.
> 
> # expected response
> 
>>paste(c("X","Y"),1:4,sep="",collapse=" + ")
> 
> [1] "X1 + Y2 + X3 + Y4"
> 
> # different!
> 
>>paste(c("X","Y"),1:4,sep="",collaps=" + ")
> 
> [1] "X1 + " "Y2 + " "X3 + " "Y4 + "
> 
> I am using R 1.7.1 on Windows 2000.  The example is taken
> from MASS 2nd ed. page 42.
> 
> Is this a bug?
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help



From deepayan at stat.wisc.edu  Sun Aug 17 20:21:22 2003
From: deepayan at stat.wisc.edu (Deepayan Sarkar)
Date: Sun, 17 Aug 2003 13:21:22 -0500
Subject: [R] collapse argument on paste
In-Reply-To: <20030817175055.6D67141C0@sitemail.everyone.net>
References: <20030817175055.6D67141C0@sitemail.everyone.net>
Message-ID: <200308171321.22086.deepayan@stat.wisc.edu>

On Sunday 17 August 2003 12:50, Gabor Grothendieck wrote:
> One gets a different response when abbreviating collapse= in
> paste?  In the second case, it appears to be acting as if " + " is just
> another argument to be pasted.

You can't abbreviate the collapse argument. In general, anything after 
the ..., if any, in the argument list of a function cannot be abbreviated, 
and args(paste) says:

> args(paste)
function (..., sep = " ", collapse = NULL)

-Deepayan



From e.pebesma at geog.uu.nl  Sun Aug 17 20:43:24 2003
From: e.pebesma at geog.uu.nl (Edzer J. Pebesma)
Date: Sun, 17 Aug 2003 20:43:24 +0200
Subject: [R] Re: 
In-Reply-To: <BAY2-F126rDs9aAOPpB0002c8c0@hotmail.com>
References: <BAY2-F126rDs9aAOPpB0002c8c0@hotmail.com>
Message-ID: <3F3FCCCC.1010000@geog.uu.nl>



Miha STAUT wrote:

> Error in gstat.load.set(object$set) : error occured when parsing 
> command: set average = 1;
>
>
> If I do not set average=1 I continue getting the singular matrix error.

"average" is not an option that can be set by "set"; see the full gstat
manual (it is an option of a data statement).

>
> I see only one solution for this problem. That is o get rid if 
> duplicate data in
> some other way. As I am among the beginners in R and I do not speak other
> programming languages either, I still did not find a successful way to 
> krige
> this data with a matern model varigram (which in my opinion suites best).

The R function in library gstat

?zerodist

may be your friend here. It tells you which observations are spatial
duplicates, after which you can take measures.


>
> I also tried with the independent version of Gstat (2.4.2 if I remember
> correctly) in combination with GRASS but seems that matern is not 
> implemented in
> it. Is that correct?

There is a way out to this, if you compile gstat linked to the necessary R
libraries to support the Matern variogram. However, it still will not fit
the smoothness parameter present in the Matern model.

>
> I am not a master of geostatistics either. Is there some other way to 
> fit an S
> shaped experimental variogram. Gaussian model would not fit that good, 
> but at
> least is an approximation.


Matern seems a flexible approach, but gstat does not provide automatic
fitting for it. I do think geoR does provide this, however, although it may
use likelyhood fit for this (all the better, but maybe prohibitively 
slow for
large data sets). Another option is combining a Gaussian model with some
other model (Exp, Sph, Lin, ...) which gstat does support.
--
Edzer



From pbruce at statistics.com  Sun Aug 17 21:45:57 2003
From: pbruce at statistics.com (Peter C. Bruce)
Date: Sun, 17 Aug 2003 15:45:57 -0400
Subject: [R] Resampling Methods: An Introduction, Using R (online seminar)
Message-ID: <5.1.0.14.2.20030817154435.0240c6b0@mail.statistics.com>

 From Sept. 25  Oct. 23, Dr. Philip Good, a former traveling lecturer for 
the American Statistical Association, will be giving an online seminar on 
practical applications of the bootstrap and permutation tests. During this 
4-week internet seminar, hosted by statistics.com, participants will do 
interval estimation, one- , two- and k-sample comparisons, correlation, and 
a number of other most-powerful exact statistical procedures. The goal of 
the course is to give participants the confidence and tools necessary for 
the practice of statistics in their own research and in interpreting the 
research of others.

Participants will interact with the instructor and fellow students via an 
online bulletin board throughout the seminar, on their own 
schedule.   Aside from this threaded discussion board, no special computer 
or internet capabilities (e.g. video or net conferencing) are 
required.  The seminar should take 5-10 hours per week, arranged at 
participants' own convenience.
Students are encouraged to submit their own findings for possible discussion.

The instructor, Dr. Phillip Good, is the author of "Resampling Methods" 
(Birkhauser, 2nd ed, 2001), as well as "Permutation Tests" (Springer, 2nd 
ed, 2000), the forthcoming "Common Errors in Statistics (and How to Avoid 
Them)" (Wiley, 2003 with James Hardin), "Manager's Guide to
Design and Conduct of Clinical Trials" (Wiley, 2002), and "Applying 
Statistics in the Courtroom" (CRC, 2001).

The course includes examples in R software. For more details, or to sign 
up, visit  http://www.statistics.com/content/courses.html

P. Bruce




Peter Bruce
statistics.com
pbruce at statistics.com
jobs... software... courses... data
10,000 page views and 1,000 visitors daily
703-522-5410



From rossini at blindglobe.net  Mon Aug 18 03:50:01 2003
From: rossini at blindglobe.net (A.J. Rossini)
Date: Sun, 17 Aug 2003 18:50:01 -0700
Subject: [R] Any interest in commercial add-on libraries based on Cytel's
 StatXact/LogXact?
Message-ID: <85ptj391km.fsf@blindglobe.net>


At JSM, I spent a bit of time with old friends at the Cytel booth
(makers of StatXact/LogXact).  They were wondering whether it was both
feasible and of interest to create a package of the StatXact compute
engine for R (to be commercially licensed, not for free!), similar to
what they've done for SAS.

As far as I know, it's feasible,

   (this is not the first commercial external package, for those of
    you about to scream "no, we can't allow commercial libraries!"; in
    fact, you'd probably have to fork the R codebase if you truly
    insist on that.  For example,CSIRO's Spot package for R,
    http://spot.cmis.csiro.au/spot/index.php)

and so that remaining question is whether there would be sufficient
interest for them to continue exploring the possibility from a
financial perspective.

Since it's not really on-topic for discussion on this mailing list, if
you are interested and would imagine purchasing a license, please send
mail to Pralay Senchaudhuri, pralay AT cytel.com.  I'd imagine that
the licensing fees would be similar to those charged for SAS or the
standalone windows versions, but that is only my conjecture, and not
Cytel's.

(Truth in Advertising: my main interest is to make it easier to use
their software for my work in clinical trials protocol design; I
currently access their implementation through SAS.  Also, while I am
not currently involved with Cytel in any way (nor will be in the
foreseeable future), I did spend part of 2 months editing and
validating the examples from an early version of the LogXact manual as
a grad student around 12 years ago, and have a number of good friends
who still work there).

best,
-tony

-- 
A.J. Rossini     			
rossini at u.washington.edu            http://www.analytics.washington.edu/ 
Biomedical and Health Informatics   University of Washington
Biostatistics, SCHARP/HVTN          Fred Hutchinson Cancer Research Center
UW   :              FAX=206-543-3461 | moving soon to a permanent office
FHCRC: 206-667-7025 FAX=206-667-4812 | Voicemail is pretty sketchy/use Email

CONFIDENTIALITY NOTICE: This e-mail message and any attachme...{{dropped}}



From gblevins at mn.rr.com  Mon Aug 18 04:31:38 2003
From: gblevins at mn.rr.com (Greg Blevins)
Date: Sun, 17 Aug 2003 21:31:38 -0500
Subject: [R] Would like to apply a weight variable to the summary function
	in Hmisc
Message-ID: <006901c36530$da481ae0$1c361d41@mn.rr.com>

Hello,

In the Hmisc package, functions describe and summarize can explicitly take a
weight variable.

My question is can a weight variable be applied when using 'summary'?

For example, using...summary(var1 ~  var2) I would like to weight the data
by var 3 (same length).

Is this possible?

Thanks a lot.
Greg Blevins
The Market Solutions Group, Inc.



From m.grum at cgiar.org  Mon Aug 18 11:37:11 2003
From: m.grum at cgiar.org (Grum, Mikkel [IPGRI-SSA-Nairobi])
Date: Mon, 18 Aug 2003 02:37:11 -0700
Subject: [R] displaying pruned clusters
Message-ID: <FC788AB9771FD6118E6F0002A5AD7B8F019106F4@icrafnttrain.icraf.cgiar.org>

Any idea why clusters five and six fall off the scale in the following?

library(cluster)
library(maptree)

data(flower)
dfl2<-daisy(flower,type=list(asymm=c(1,3),ordratio=7))
hdfl2<-hclust(dfl2)
prune.dfl2<-prune.clust(hdfl2,k=6)
plot(prune.dfl2)

Of course the following will display all six clusters, but without a scale:
draw.tree(prune.dfl2)

Best wishes,
Mikkel




Mikkel Grum
International Plant Genetic Resources Institute (IPGRI)
Sub-Saharan Africa Group
***
PO Box 30677
00100 Nairobi, Kenya
Tel: +254 20 524505(direct)/524500(IPGRI)
Fax: +254 20 524501(IPGRI)/524001(ICRAF)
m.grum at cgiar.org
www.ipgri.org



From angel_lul at hotmail.com  Mon Aug 18 13:13:35 2003
From: angel_lul at hotmail.com (Angel)
Date: Mon, 18 Aug 2003 13:13:35 +0200
Subject: [R] putting NAs at the end
Message-ID: <Law11-OE293Tm3uPpSi000271bd@hotmail.com>

Thanks to Gabor and others I've found the solution to my problem, but there
are some issues
with order, sort and apply that I post in another thread.
Here are two solutions to my problem:
# Example matrix:
A<-matrix(rnorm(10000*12),nrow=10000,ncol=12)
A[rbind(c(100,3),c(90,9),c(40,6))]<-NA

# Solution given by GaborGrothendieck
system.time({A.a<-t(apply(A,1,function(x){x<-matrix(x,nr=3);x[,order(colSums
(is.na(x))>0)]}))})

# A function I wrote with various suggestions (Petr Pikal, Duncan Murdoch,
Ted Harding)

Put.NaN.last<-function(A) {
# First creates a matrix where the points with NA z have also NA
# x and y
A.o<-A
A.o[,1:3]<-A.o[,1:3]*A[,3]/A[,3]
A.o[,4:6]<-A.o[,4:6]*A[,6]/A[,6]
A.o[,7:9]<-A.o[,7:9]*A[,9]/A[,9]
A.o[,10:12]<-A.o[,10:12]*A[,12]/A[,12]
# Transposes both matrix as I want to order each row (by column) and R is
# by default "column-major order"
A<-t(A)
A.o<-t(A.o)
# Now makes another matrix Ind with same dim as A but
# where the integer part of each element indicates the row (original A
columns)
# and the second decimal indicates the column (original A row, i.e A is now
transposed)
serie<-rep(1:4,each=3)/10
serie<-rep(serie,ncol(A))
Ind<-rep(1:ncol(A),each=nrow(A))
Ind<-Ind+serie
# Adds 0.5 to the elements where A.o is NA so they will be larger
# than any in the row
Ind[which(is.na(A.o))]<-Ind[which(is.na(A.o))]+0.5
# Finally order A according to the Ind
# this avoids having to use apply
A.o[,]<-A[order(Ind)]
A.o<-t(A.o)
A<-t(A)
return(A.o)
}

system.time({A.put<-Put.NaN.last(A)})

# As you see calling with my function is much faster than the apply
# I've decided to post another thread to try to understand what is going on
here
# Thanks to all that helped
# Angel



From angel_lul at hotmail.com  Mon Aug 18 13:13:38 2003
From: angel_lul at hotmail.com (Angel)
Date: Mon, 18 Aug 2003 13:13:38 +0200
Subject: [R] apply and sort vs vectorized order
Message-ID: <Law11-OE23Vtb0mczu2000270d8@hotmail.com>

Dear all,
Trying to solve a problem I had (see thread "putting NAs at the end" )
I've noticed a difference in system time requirements between using apply
and sort (or order)
to order  each row or column of a matrix compared to a vectorized function I
wrote.
Using apply is much faster when the number of loops (number of rows or
columns to order) is low BUT
much slower when number of loops are high and the other dimension short.
Here is my function:

order.rc<-function(A,row.column=1,na.last = TRUE, decreasing =
FALSE,return.sort=TRUE) {
# removes negative values scaling A so min(A)=0
A.order<-A+abs(min(A,na.rm=TRUE))
# rescales A so max(A)=0.1
A.order<-A.order/(max(A.order,na.rm=TRUE)*10)
# makes NAs=0 (na.last=FALSE) or NAs=0.9 (na.last=TRUE)
# NOTE: if decreasing is TRUE NAs are the inverse of above
if ((na.last & !decreasing) | (!na.last & decreasing))
A.order[which(is.na(A.order))]<-0.9  else A.order[which(is.na(A.order))]<-0
# if ordering each row the integer part of A is the column index
(row.column=1)
# else, we are ordering each column so the integer part of A is the column
index
if (row.column==1) A.order<-A.order+rep(1:nrow(A),ncol(A))   else
A.order<-A.order+rep(1:ncol(A),each=nrow(A))
# returns either a matrix with sorted values or the ordering indexes
if (return.sort)
{
A.order<-A[order(A.order,decreasing=decreasing)]
if (row.column==1)
{
dim(A.order)<-dim(t(A))
A.order<-t(A.order)
}
else dim(A.order)<-dim(A)
return(A.order)
}
else return(order(A.order,decreasing=decreasing))
}

# Some system time comparisons
# CHANGE Nrandom ACORDING TO YOUR SYSTEM
Nrandom=1000
A<-matrix(rnorm(Nrandom*Nrandom),nrow=Nrandom,ncol=Nrandom)
A[rbind(c(100,3),c(90,9),c(40,6))]<-NA
system.time({A.r<-order.rc(A)})
system.time(A.s1<-apply(A,1,sort))
system.time({A.c<-order.rc(A,row.column=2)})
system.time(A.s2<-apply(A,2,sort))

A<-matrix(rnorm(Nrandom*Nrandom),nrow=Nrandom*Nrandom/10,ncol=10)
A[rbind(c(100,3),c(90,9),c(40,6))]<-NA
system.time({A.r<-order.rc(A)})
system.time(A.s1<-apply(A,1,order))
system.time({A.c<-order.rc(A,row.column=2)})
system.time(A.s2<-apply(A,2,order))

I think only the third apply is slower than the function because number of
"loops" is too high
and my function is faster despite the long vector to order.
Thanks for any clarifications on how all this works,
Angel



From Giles.Heywood at CommerzbankIB.com  Mon Aug 18 13:14:57 2003
From: Giles.Heywood at CommerzbankIB.com (Heywood, Giles)
Date: Mon, 18 Aug 2003 12:14:57 +0100
Subject: [R] New package: irregular time-series (its)
Message-ID: <8CBAA121CEB4D5118CB200508BB2BBEF54045A@xmx8lonib.lonib.commerzbank.com>

The issue to which you refer is putting 2 time-series with 
non-aligned (or inconsistent) time-stamps onto a consistent
basis.  There are four functions to assist with this task
in the 'its' package: try ?its-join for help.

In essence, there are two approaches to the problem.  In one,
you take the union of the time-stamps as the consistent basis,
and in the other approach, you take the intersection.  From
what you say, it sounds like you want the union, for your 
application, i.e. unionIts(A,B).  You can then apply the 
interpolation function of your choice.

If you have further points to raise, and the documentation 
provided does not answer your questions, I suggest you contact 
me off-list.

Regards

Giles Heywood

> -----Original Message-----
> From: Fan [mailto:xiao.gang.fan1 at libertysurf.fr]
> Sent: 17 August 2003 17:12
> To: Heywood, Giles
> Subject: Re: [R] New package: irregular time-series (its)
> 
> 
> Hello Giles,
> 
> Congratulations for your contributed R package its.
> 
> I'm having a little problem, and I'd like to know if
> there's already a general function in its (or other packages)
> to manage it. If not, could you add such function in its ?
> 
> With irregular time series, we need sometimes MERGE 2
> or several time series with different periodicities, for
> example:
> 
> A = a monthly regular time series (ex. end of each month),
> B = a business daily time series
> 
> To analyse conjointly the 2 series, one need to "expand"
> A to a daily series with the same time points as B (with
> the option of different method of interpolation: constant,
> linear, spline, etc.).
> 
> Thanks in advance
> --
> Fan
> 
> Heywood, Giles wrote:
> > I have uploaded to CRAN a new package named 'its' 
> (Irregular Time-Series).
> > It
> > implements irregular time-series as an S4 class, extending 
> the matrix class,
> > and records the time-stamp of each row in the matrix using 
> POSIX.  Print,
> > plot,
> > extraction, append, and related functionality are available.
> > 
> > Feedback and suggestions are welcome.
> > 
> > Giles Heywood
> > 
> > 
> > 
> **************************************************************
> ******** 
> > This is a commercial communication from Commerzbank AG.\ \ 
> T...{{dropped}}
> > 
> > _______________________________________________
> > R-announce at stat.math.ethz.ch mailing list
> > https://www.stat.math.ethz.ch/mailman/listinfo/r-announce
> > 
> > ______________________________________________
> > R-help at stat.math.ethz.ch mailing list
> > https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> > 
> 
> 
> 


********************************************************************** 
This is a commercial communication from Commerzbank AG.\ \ T...{{dropped}}



From christian.ritter at shell.com  Mon Aug 18 13:16:02 2003
From: christian.ritter at shell.com (Ritter, Christian C MCIL-CTGAS)
Date: Mon, 18 Aug 2003 13:16:02 +0200
Subject: [R] rterm not shutting down from ESS on Win32/could we help?
Message-ID: <156CDC8CCFD1894295D2907F16337A481FA90C@bru-s-006.europe.shell.com>

Hi to all who suffer from rterm not shutting down in xemacs/ESS on windows NT or 2000. Also hi to those who could eventually help. Here is some more information which could help and some ENCOURAGEMENT to contribute to a solution.

1. It may be an xemacs problem but it is more likely an interaction between rterm/comint/and xemacs. In fact, the problem started occurring around version R 1.6.0. I remember that it wasn't there in 1.5.1. It was not associated to a simultaneous upgrade of xemacs or of the windows operating system. So, there is an R contribution to the problem.
2. I'm also using other tools which pass through the command interpreter (I tried running mysql and python). Also those applications can under conditions which I cannot reproduce yet lead to "not shutting down" appropriately. So, there seems to be an xemacs contribution. 
3. Here are a few questions which may help us figure out where exactly this is going wrong:
	- does it also happen with other dialects using ESS, such as S+, SAS, ...?
	- does it also happen with current gnu-emacs on a system on which it happens with xemacs?
	- has anyone succeeded to run it within a debugger? what did you learn from it?
4. Would there be an obvious person who could centralize this information? 


Christian Ritter
Functional Specialist Statistics
Shell Coordination Centre S.A.
Monnet Centre International Laboratory, Avenue Jean Monnet 1, B-1348 Louvain-La-Neuve, Belgium

Tel: +32 10 477  349 Fax: +32 10 477 219
Email: christian.ritter at shell.com
Internet: http://www.shell.com/chemicals


-----Original Message-----
From: A.J. Rossini [mailto:rossini at blindglobe.net]
Sent: Saturday, August 16, 2003 12:21 AM
To: Jeff D. Hamann
Cc: r-help at stat.math.ethz.ch
Subject: Re: [R] rterm not shutting down from ESS on Win32


"Jeff D. Hamann" <jeff_hamann at hamanndonald.com> writes:

> I've been having problems with Rterm.exe not shutting down when I exit an R
> (1.7.0 and 1.7.1) session from within emacs when using ESS. I've just
> upgraded to 5.1.24 and still have the same problems. I'm running ntemacs and
> winxp. I don't recall having these troubles with version 1.6.2 of R.

Known problem.  R-core claims its an Emacs problem.  It's not an ESS
problem, but we are looking into it.  Might not be solvable.  We'll
see...  (I'll have better access to a Windows box soon).

best,
-tony

-- 
A.J. Rossini     			
rossini at u.washington.edu            http://www.analytics.washington.edu/ 
Biomedical and Health Informatics   University of Washington
Biostatistics, SCHARP/HVTN          Fred Hutchinson Cancer Research Center
UW   :              FAX=206-543-3461 | moving soon to a permanent office
FHCRC: 206-667-7025 FAX=206-667-4812 | Voicemail is pretty sketchy/use Email

CONFIDENTIALITY NOTICE: This e-mail message and any attachme...{{dropped}}

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://www.stat.math.ethz.ch/mailman/listinfo/r-help



From pleissner at mpiib-berlin.mpg.de  Mon Aug 18 13:18:10 2003
From: pleissner at mpiib-berlin.mpg.de (Klaus-Peter Pleissner)
Date: Mon, 18 Aug 2003 13:18:10 +0200
Subject: [R] 3D pie
Message-ID: <3F40B5F2.1050707@mpiib-berlin.mpg.de>

Hello,

is there a function for 3D pie representation in R ?

Thanks
Klaus-P.


-- 
Dr. Klaus-Peter Pleissner
Max Planck Institute for Infection Biology
Campus Charit? Mitte
Schumannstr. 21/22
D-10117 Berlin
Germany


phone: +49-30-28460-119
fax:   +49-30-28460-507
email: pleissner at mpiib-berlin.mpg.de



From fharrell at virginia.edu  Mon Aug 18 13:20:38 2003
From: fharrell at virginia.edu (Frank E Harrell Jr)
Date: Mon, 18 Aug 2003 07:20:38 -0400
Subject: [R] Would like to apply a weight variable to the summary
	function in Hmisc
In-Reply-To: <006901c36530$da481ae0$1c361d41@mn.rr.com>
References: <006901c36530$da481ae0$1c361d41@mn.rr.com>
Message-ID: <20030818072038.32011d81.fharrell@virginia.edu>

On Sun, 17 Aug 2003 21:31:38 -0500
"Greg Blevins" <gblevins at mn.rr.com> wrote:

> Hello,
> 
> In the Hmisc package, functions describe and summarize can explicitly take a
> weight variable.
> 
> My question is can a weight variable be applied when using 'summary'?
> 
> For example, using...summary(var1 ~  var2) I would like to weight the data
> by var 3 (same length).
> 
> Is this possible?

No but you can do this with summarize (if all you want is cross-classification) using an example I posted on r-help a few weeks ago.  

Frank

> 
> Thanks a lot.
> Greg Blevins
> The Market Solutions Group, Inc.
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help


---
Frank E Harrell Jr              Prof. of Biostatistics & Statistics
Div. of Biostatistics & Epidem. Dept. of Health Evaluation Sciences
U. Virginia School of Medicine  http://hesweb1.med.virginia.edu/biostat



From fharrell at virginia.edu  Mon Aug 18 13:58:03 2003
From: fharrell at virginia.edu (Frank E Harrell Jr)
Date: Mon, 18 Aug 2003 07:58:03 -0400
Subject: [R] 3D pie
In-Reply-To: <3F40B5F2.1050707@mpiib-berlin.mpg.de>
References: <3F40B5F2.1050707@mpiib-berlin.mpg.de>
Message-ID: <20030818075803.7e0988d0.fharrell@virginia.edu>

On Mon, 18 Aug 2003 13:18:10 +0200
Klaus-Peter Pleissner <pleissner at mpiib-berlin.mpg.de> wrote:

> Hello,
> 
> is there a function for 3D pie representation in R ?
> 
> Thanks
> Klaus-P.

I hope not.  See Edward Tufte's writings on chartjunk.

Frank

> 
> 
> -- 
> Dr. Klaus-Peter Pleissner
> Max Planck Institute for Infection Biology
> Campus Charit? Mitte
> Schumannstr. 21/22
> D-10117 Berlin
> Germany
> 
> 
> phone: +49-30-28460-119
> fax:   +49-30-28460-507
> email: pleissner at mpiib-berlin.mpg.de
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help


---
Frank E Harrell Jr              Prof. of Biostatistics & Statistics
Div. of Biostatistics & Epidem. Dept. of Health Evaluation Sciences
U. Virginia School of Medicine  http://hesweb1.med.virginia.edu/biostat



From rolf at math.unb.ca  Mon Aug 18 13:59:19 2003
From: rolf at math.unb.ca (Rolf Turner)
Date: Mon, 18 Aug 2003 08:59:19 -0300 (ADT)
Subject: [R] 3D pie
Message-ID: <200308181159.h7IBxJBK014620@erdos.math.unb.ca>


Klaus-Peter Pleissner <pleissner at mpiib-berlin.mpg.de> wrote:

> is there a function for 3D pie representation in R ?

I certainly hope not!!!

(1) ``Given their low data density and failure to order numbers
along a visual dimension, pie charts should never be used.''
(Tuffte, Edward R., ``The Visual Display of Quantitative
Information'' Graphics Press, Chessire CT, 1983, p. 178.)

(2)  ``3D pie charts are even worse, as they also add a visual
distortion ...''  (``How to Construct Bad Charts and Graphs'',
Klass, Gary, Department of Politics and Government, Illinois
State University, 2001.)

   (http://lilt.ilstu.edu/gmklass/pos138/datadisplay/badchart.htm)


				cheers,

					Rolf Turner
					rolf at math.unb.ca



From MSchwartz at medanalytics.com  Mon Aug 18 14:25:21 2003
From: MSchwartz at medanalytics.com (Marc Schwartz)
Date: Mon, 18 Aug 2003 07:25:21 -0500
Subject: [R] 3D pie
In-Reply-To: <20030818075803.7e0988d0.fharrell@virginia.edu>
References: <3F40B5F2.1050707@mpiib-berlin.mpg.de>
	<20030818075803.7e0988d0.fharrell@virginia.edu>
Message-ID: <1061209521.7326.19.camel@localhost>

On Mon, 2003-08-18 at 06:58, Frank E Harrell Jr wrote:
> On Mon, 18 Aug 2003 13:18:10 +0200
> Klaus-Peter Pleissner <pleissner at mpiib-berlin.mpg.de> wrote:
> 
> > Hello,
> > 
> > is there a function for 3D pie representation in R ?
> > 
> > Thanks
> > Klaus-P.
> 
> I hope not.  See Edward Tufte's writings on chartjunk.
> 
> Frank


I'll toss in one more:

William Cleveland's "The Elements of Graphing Data". Chapter 4
(Graphical Perception), Section 10, called "Pop Charts".

HTH,

Marc Schwartz



From andy_liaw at merck.com  Mon Aug 18 14:26:33 2003
From: andy_liaw at merck.com (Liaw, Andy)
Date: Mon, 18 Aug 2003 08:26:33 -0400
Subject: [R] Any interest in commercial add-on libraries based on
	Cyte	l's StatXact/LogXact?
Message-ID: <3A822319EB35174CA3714066D590DCD50205C9E0@usrymx25.merck.com>

Another example:  Jerry Friedman's MART is available in R from Salford for
the same price as the stand-alone TreeNet, even though they don't advertise
it on their web site.

Andy

> -----Original Message-----
> From: rossini at blindglobe.net [mailto:rossini at blindglobe.net] 
> Sent: Sunday, August 17, 2003 9:50 PM
> To: rhelp
> Cc: pralay at cytel.com
> Subject: [R] Any interest in commercial add-on libraries 
> based on Cytel's StatXact/LogXact?
> 
> 
> 
> At JSM, I spent a bit of time with old friends at the Cytel 
> booth (makers of StatXact/LogXact).  They were wondering 
> whether it was both feasible and of interest to create a 
> package of the StatXact compute engine for R (to be 
> commercially licensed, not for free!), similar to what 
> they've done for SAS.
> 
> As far as I know, it's feasible,
> 
>    (this is not the first commercial external package, for those of
>     you about to scream "no, we can't allow commercial libraries!"; in
>     fact, you'd probably have to fork the R codebase if you truly
>     insist on that.  For example,CSIRO's Spot package for R,
>     http://spot.cmis.csiro.au/spot/index.php)
> 
> and so that remaining question is whether there would be 
> sufficient interest for them to continue exploring the 
> possibility from a financial perspective.
> 
> Since it's not really on-topic for discussion on this mailing 
> list, if you are interested and would imagine purchasing a 
> license, please send mail to Pralay Senchaudhuri, pralay AT 
> cytel.com.  I'd imagine that the licensing fees would be 
> similar to those charged for SAS or the standalone windows 
> versions, but that is only my conjecture, and not Cytel's.
> 
> (Truth in Advertising: my main interest is to make it easier 
> to use their software for my work in clinical trials protocol 
> design; I currently access their implementation through SAS.  
> Also, while I am not currently involved with Cytel in any way 
> (nor will be in the foreseeable future), I did spend part of 
> 2 months editing and validating the examples from an early 
> version of the LogXact manual as a grad student around 12 
> years ago, and have a number of good friends who still work there).
> 
> best,
> -tony
> 
> -- 
> A.J. Rossini     			
> rossini at u.washington.edu            
> http://www.analytics.washington.edu/ 
> Biomedical and Health 
> Informatics   University of Washington
> Biostatistics, SCHARP/HVTN          Fred Hutchinson Cancer 
> Research Center
> UW   :              FAX=206-543-3461 | moving soon to a 
> permanent office
> FHCRC: 206-667-7025 FAX=206-667-4812 | Voicemail is pretty 
> sketchy/use Email
> 
> CONFIDENTIALITY NOTICE: This e-mail message and any 
> attachme...{{dropped}}
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list 
> https://www.stat.math.ethz.ch/mailman/listinfo> /r-help
> 

------------------------------------------------------------------------------
Notice:  This e-mail message, together with any attachments, contains
information of Merck & Co., Inc. (Whitehouse Station, New Jersey, USA), and/or
its affiliates (which may be known outside the United States as Merck Frosst,
Merck Sharp & Dohme or MSD) that may be confidential, proprietary copyrighted
and/or legally privileged, and is intended solely for the use of the
individual or entity named on this message.  If you are not the intended
recipient, and have received this message in error, please immediately return
this by e-mail and then delete it.



From amurta at ipimar.pt  Mon Aug 18 15:38:46 2003
From: amurta at ipimar.pt (Alberto Murta)
Date: Mon, 18 Aug 2003 13:38:46 +0000
Subject: [R] vectorization question
In-Reply-To: <16188.40303.336296.358565@gargle.gargle.HOWL>
References: <200308141650.06342.amurta@ipimar.pt>
	<5.2.1.1.2.20030814112756.04779dc0@mailhost.blackmesacapital.com>
	<16188.40303.336296.358565@gargle.gargle.HOWL>
Message-ID: <200308181338.46347.amurta@ipimar.pt>

Thank you very much to Tony Plate for his really clear explanation, and to 
Prof Ripley for his time solving this deficiency (IMHO)


On Friday 15 August 2003 08:44, Martin Maechler wrote:
>
> Thank you, Tony.  This certainly was the most precise
> explanation on this thread.
>
> Everyone note however, that this has been improved (by Brian
> Ripley) in the current R-devel {which should be come R 1.8 in October}.
> There, also "$<-" assignment of data frames does check things
> and in this case will do the same replication as the [,] or [[]]
> assignments do.
> For back compatibility (with S-plus and earlier R versions), I'd
> still recommend using bracket "[" rather than "$" assignment for
> data frames.
>
> Martin Maechler <maechler at stat.math.ethz.ch>	http://stat.ethz.ch/~maechler/
> Seminar fuer Statistik, ETH-Zentrum  LEO C16	Leonhardstr. 27
> ETH (Federal Inst. Technology)	8092 Zurich	SWITZERLAND
> phone: x-41-1-632-3408		fax: ...-1228			<><
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help

-- 
                                         Alberto G. Murta
Institute for Agriculture and Fisheries Research (INIAP-IPIMAR) 
Av. Brasilia, 1449-006 Lisboa, Portugal | Phone: +351 213027062
Fax:+351 213015948 | http://www.ipimar-iniap.ipimar.pt/pelagicos/



From roger at ysidro.econ.uiuc.edu  Mon Aug 18 14:47:27 2003
From: roger at ysidro.econ.uiuc.edu (Roger Koenker)
Date: Mon, 18 Aug 2003 07:47:27 -0500 (CDT)
Subject: [R] 3D pie
In-Reply-To: <1061209521.7326.19.camel@localhost>
Message-ID: <Pine.SOL.4.30.0308180743060.28739-100000@ysidro.econ.uiuc.edu>

R-help Readers might also find amusing the new Tufte paper:  "The Cognitive
Style of PowerPoint", available from www.edwardtufte.com.  (This is
a non-commercial announcement.)

url:	www.econ.uiuc.edu/~roger/my.html	Roger Koenker
email	rkoenker at uiuc.edu			Department of Economics
vox: 	217-333-4558				University of Illinois
fax:   	217-244-6678				Champaign, IL 61820

On Mon, 18 Aug 2003, Marc Schwartz wrote:

> I'll toss in one more:
>
> William Cleveland's "The Elements of Graphing Data". Chapter 4
> (Graphical Perception), Section 10, called "Pop Charts".
>
> HTH,
>
> Marc Schwartz
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
>



From moreton at vonneumann.cog.jhu.edu  Mon Aug 18 15:51:09 2003
From: moreton at vonneumann.cog.jhu.edu (Elliott Moreton)
Date: Mon, 18 Aug 2003 09:51:09 -0400 (EDT)
Subject: [R] glmmPQL() and memory limitations
Message-ID: <Pine.LNX.4.44.0308180927300.19484-100000@vonneumann.cog.jhu.edu>

Hi, all,

When running glmmPQL(), I keep getting errors like

	Error: cannot allocate vector of size 61965 Kb
	Execution halted

This is R-1.7.1.  The data set consists of about 11,000 binary responses 
from 16 subjects.  The model is

	fixed =
	SonResp ~ (ordered (Stop) + ordered (Son)) * StopResp,
	
	random = 
	~ 1 + (ordered (Stop) + ordered (Son)) * StopResp | Subj
	
	family = binomial (link = logit)

SonResp and StopResp are binary; Stop and Son are ordered factors with six 
levels each.  

The machine I'm running this on is my university's scientific server, a 
Beowulf Linux cluster; the machine this job would be running on would have 
two 1.4 GHz CPUS, a 2-gigabyte RAM, and an 18-gigabyte hard disk, plus 130 
gigabytes of scratch file space; it would be running Red Hat Linux 7.2 
with XFS.  

Can anyone tell me whether this is (1) a problem with the model (no
machine could fit it in the lifetime of the universe), (2) a problem with
how I formulated the model (there's a way to get the same end result
without overflowing memory), (c) a problem with glmmPQL() (that could be 
fixed by using some other package), (d) a problem with the machine I'm 
running it on (need more real or virtual memory), or (e) other?  
(Naturally, I've contacted the system administrators to ask them the same 
thing, but I don't know how much they know about R.)

Many thanks in advance,
Elliott Moreton



From rossini at blindglobe.net  Mon Aug 18 16:00:18 2003
From: rossini at blindglobe.net (A.J. Rossini)
Date: Mon, 18 Aug 2003 07:00:18 -0700
Subject: [R] rterm not shutting down from ESS on Win32/could we help?
In-Reply-To: <156CDC8CCFD1894295D2907F16337A481FA90C@bru-s-006.europe.shell.com>
	(Christian
	C. Ritter's message of "Mon, 18 Aug 2003 13:16:02 +0200")
References: <156CDC8CCFD1894295D2907F16337A481FA90C@bru-s-006.europe.shell.com>
Message-ID: <85k79b3w25.fsf@blindglobe.net>


Please move this over to ESS-help -- this is off-topic for many R
users.


"Ritter, Christian C MCIL-CTGAS" <christian.ritter at shell.com> writes:

> Hi to all who suffer from rterm not shutting down in xemacs/ESS on

-- 
A.J. Rossini     			
rossini at u.washington.edu            http://www.analytics.washington.edu/ 
Biomedical and Health Informatics   University of Washington
Biostatistics, SCHARP/HVTN          Fred Hutchinson Cancer Research Center
UW   :              FAX=206-543-3461 | moving soon to a permanent office
FHCRC: 206-667-7025 FAX=206-667-4812 | Voicemail is pretty sketchy/use Email

CONFIDENTIALITY NOTICE: This e-mail message and any attachme...{{dropped}}



From brahm at alum.mit.edu  Mon Aug 18 16:38:31 2003
From: brahm at alum.mit.edu (David Brahm)
Date: Mon, 18 Aug 2003 10:38:31 -0400
Subject: [R] General help pages
References: <3F3D1EB2.2000108@statistik.uni-dortmund.de>
Message-ID: <16192.58599.715345.166414@arbres1a.fmr.com>

Uwe Ligges <ligges at statistik.uni-dortmund.de> wrote:
> Please follow that [?sort] reference and read ?Comparison as well.

There seem to be several very useful "general" help pages like ?Comparison,
?Devices, and ?Startup, which do not tie to a specific function.  Is there a
list of these?  I'd like some bedtime reading.
-- 
                              -- David Brahm (brahm at alum.mit.edu)



From ligges at statistik.uni-dortmund.de  Mon Aug 18 17:06:35 2003
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Mon, 18 Aug 2003 17:06:35 +0200
Subject: [R] General help pages
In-Reply-To: <16192.58599.715345.166414@arbres1a.fmr.com>
References: <3F3D1EB2.2000108@statistik.uni-dortmund.de>
	<16192.58599.715345.166414@arbres1a.fmr.com>
Message-ID: <3F40EB7B.60508@statistik.uni-dortmund.de>

David Brahm wrote:
> Uwe Ligges <ligges at statistik.uni-dortmund.de> wrote:
> 
>>Please follow that [?sort] reference and read ?Comparison as well.
> 
> 
> There seem to be several very useful "general" help pages like ?Comparison,
> ?Devices, and ?Startup, which do not tie to a specific function.  Is there a
> list of these?  I'd like some bedtime reading.

I don't know about a complete listing of such functions (well, what 
should be listed in there and what not), but for some basic stuff, 
?Syntax has some links in its "See Also" section.

Uwe Ligges



From roger at ysidro.econ.uiuc.edu  Mon Aug 18 17:17:10 2003
From: roger at ysidro.econ.uiuc.edu (Roger Koenker)
Date: Mon, 18 Aug 2003 10:17:10 -0500 (CDT)
Subject: [R] Re: Oja median 
In-Reply-To: <Pine.SOL.4.30.0308150822380.24507-100000@ysidro.econ.uiuc.edu>
Message-ID: <Pine.SOL.4.30.0308180831480.28739-100000@ysidro.econ.uiuc.edu>

I am shocked and dismayed (and the term hasn't even started yet ;-) )
that none of you have turned in the weekend homework problem that
I assigned last Friday.  At the risk of further embarrassment I am
posting an answer in the hope that this will inspire someone to suggest
some improvements.  In particular you will see that the loop in the
function "cofactors" when subjected to the apply is painfully slow.
Using Rprof on an example with n=50 and p=3 shows that 160 seconds of the
167 needed were spent in this apply.  Yes, I'm aware that this could
be recoded in [C, fortran, ...], but that would be considered cheating.


"mean.wilks" <- function(x){
	# Computes the column means of the matrix x -- very sloooowly.
	n <- dim(x)[1]
        p <- dim(x)[2]
        A <- t(combn(1:n,p))
	X <- NULL
	for(i in 1:p)
		X <- cbind(X,x[A[,i],])
	oja.ize <- function(v)cofactors(matrix(v,sqrt(length(v))))
	A <- t(apply(X,1,oja.ize))
	coef(lm(-A[,1]~A[,-1]-1))
	}
"cofactors" <- function(A){
	B <- rbind(1,cbind(A,1))
	p <- ncol(B)
	x <- rep(0,p)
	for(i in 1:p)
		x[i] <- ((-1)^(i+p)) *det(B[-i,-p])
	return(x)
}

url:	www.econ.uiuc.edu/~roger/my.html	Roger Koenker
email	rkoenker at uiuc.edu			Department of Economics
vox: 	217-333-4558				University of Illinois
fax:   	217-244-6678				Champaign, IL 61820

On Fri, 15 Aug 2003, Roger Koenker wrote:

> I discovered recently that the phrase "Oja median" produces no hits in
> Jonathan Baron's very valuable R search engine. I found this surprising
> since I've long regarded this idea as one of the more interesting notions
> in the multivariate robustness literature. To begin to remedy this oversight
> I wrote a bivariate version and then decided that writing a general p-variate
> version might make a nice weekend programming puzzle for R-help.
>
> Here are a few more details:
>
> The Oja median of n  p-variate observations minimizes over theta
> in R^p the sum of the volumes of the simplices formed by theta,
> and p of the observed points, the sum being taken over all n choose p
> groups of p observations.  Thus, in the bivariate case we are minimizing
> the sum of the areas of all triangles formed by the the point theta
> and pairs of observations.  Here is a simple bivariate implementation:
>
> oja.median <-function(x) {
> 	#bivariate version -- x is assumed to be an n by 2 matrix
> 	require(quantreg)
>         n <- dim(x)[1]
>         A <- matrix(rep(1:n, n), n)
>         i <- A[col(A) < row(A)]
>         j <- A[n + 1. - col(A) > row(A)]
>         xx <- cbind(x[i,  ], x[j,  ])
>         y <- xx[, 1] * xx[, 4] - xx[, 2] * xx[, 3]
>         z1 <- (xx[, 4] - xx[, 2])
>         z2 <-  - (xx[, 3] - xx[, 1])
>         return(rq(y~cbind(z1, z2)-1)$coef)
>         }
>
> To understand the strategy, note that the area of the triangle formed
> by the points x_i = (x_i1,x_i2), x_j = (x_j1,x_j2),
> and theta = (theta_1,theta_2) is given by the determinant,
>
>                                     | 1    1     1   |
> 	Delta(x_i, x_j, theta) = .5 |y_i1 yj1 theta_1|.
> 	                            |y_i2 yj2 theta_2|
>
> Expanding the determinant in the unknown parameters theta gives
> the l1 regression formulation.  Remarkably, a result of Wilks says
> that if the call to rq() is replaced with a call to lm() you get
> the sample mean -- this gives an impressively inefficient least
> squares regression based alternative to apply(x,2,mean)!
> It also provides a useful debugging check for proposed algorithms.
>
> Obviously, the expansion of the determinant gives the same formulation
> for p>2, the challenge is to find a clean way to generate the
> design matrix and response vector for the general setting.
>
> Bon weekend!
>
> url:	www.econ.uiuc.edu/~roger/my.html	Roger Koenker
> email	rkoenker at uiuc.edu			Department of Economics
> vox: 	217-333-4558				University of Illinois
> fax:   	217-244-6678				Champaign, IL 61820
>
>
>



From HADASSA.BRUNSCHWIG at Roche.COM  Mon Aug 18 17:10:00 2003
From: HADASSA.BRUNSCHWIG at Roche.COM (Brunschwig, Hadassa {PDMM~Basel})
Date: Mon, 18 Aug 2003 17:10:00 +0200
Subject: [R] Extracting from population estimation, individual estimations
Message-ID: <BC6A439CD6835749A9C7B8D8F041DFA88B11AE@rbamsem1.emea.roche.com>

Hello to everyone!

I fitted a nonlinear mixed model to a dataframe. The dataframe contained among others a variable "Subject".For each subject there were several entries(measurements), exactly sixteen. I fitted the nonlinear mixed model using the variable "Subject" as a grouping factor in the random statement of the model. There were two parameters that had to be estimated. The two estimations reflect though the estimations for the population. I am looking for a way to extract of these population estimations the individual estimations of the parameters for each subject, i.e. the predictions (not the predicted values) for each subject. There is for example a possibility in SAS to do this, is there any in R as well?  

Thanks for answers.

Regards

Dassy



From spencer.graves at pdf.com  Mon Aug 18 17:38:23 2003
From: spencer.graves at pdf.com (Spencer Graves)
Date: Mon, 18 Aug 2003 08:38:23 -0700
Subject: [R] Extracting from population estimation, individual estimations
References: <BC6A439CD6835749A9C7B8D8F041DFA88B11AE@rbamsem1.emea.roche.com>
Message-ID: <3F40F2EF.4020908@pdf.com>

Have you considered "intervals"?  The index in Pinhiero and Bates (2000) 
Mixed-Effects Models in S and S-Plus (Springer) gives several page 
references (at least one of which is off by 1 page in my copy of the 
book) that seem relevant to your question.

hope this helps.  spencer graves

Brunschwig, Hadassa {PDMM~Basel} wrote:
> Hello to everyone!
> 
> I fitted a nonlinear mixed model to a dataframe. The dataframe contained among others a variable "Subject".For each subject there were several entries(measurements), exactly sixteen. I fitted the nonlinear mixed model using the variable "Subject" as a grouping factor in the random statement of the model. There were two parameters that had to be estimated. The two estimations reflect though the estimations for the population. I am looking for a way to extract of these population estimations the individual estimations of the parameters for each subject, i.e. the predictions (not the predicted values) for each subject. There is for example a possibility in SAS to do this, is there any in R as well?  
> 
> Thanks for answers.
> 
> Regards
> 
> Dassy
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help



From brahm at alum.mit.edu  Mon Aug 18 18:16:44 2003
From: brahm at alum.mit.edu (David Brahm)
Date: Mon, 18 Aug 2003 12:16:44 -0400
Subject: [R] General help pages
References: <3F3D1EB2.2000108@statistik.uni-dortmund.de>
	<16192.58599.715345.166414@arbres1a.fmr.com>
	<20030818151216.GA18942@sonny.eddelbuettel.com>
Message-ID: <16192.64492.472380.344572@arbres1a.fmr.com>

Following Uwe's and Dirk's suggestions, here's a first pass at a list of
interesting "general" help pages:

?Arithmetic
?Comparison
?Control
?DateTimeClasses
?Defunct
?Deprecated
?Devices
?Extract
?Foreign
?Logic
?Memory
?Paren
?Rdconv      (RdUtils page: Rdconv, Rd2dvi, Rd2txt, Sd2Rd)
?Special     (beta, gamma, choose, ...)
?Startup
?Syntax
?build       (PkgUtils page: R CMD build, R cmd check)
?connections (file, pipe, ...)
?pi          (Constants page: LETTERS, letters, month.abb, month.name, pi)

Note that the RdUtils, PkgUtils, and Constants pages cannot be accessed through
their names ("?RdUtils", etc).

I'd also suggest two additional general pages (which do not currently exist):
?System   (system, .Platform, Sys.info, Sys.getenv, Sys.putenv, getwd, setwd)
?Graphics (covering plot, lines, points, segments, par, Devices)
-- 
                              -- David Brahm (brahm at alum.mit.edu)



From plitwak at umich.edu  Mon Aug 18 20:18:26 2003
From: plitwak at umich.edu (Paul Litvak)
Date: Mon, 18 Aug 2003 14:18:26 -0400
Subject: [R] type I and type III sums of squares
Message-ID: <3F411872.1080001@umich.edu>

Hello-

I have been digging around in the FAQ's and online looking for an answer 
to my questions, and perhaps someone here can help me.

For a statistical experiment, I need to run 3,000,000 ANOVAs, which is 
taking me a very long time. As a result, I have recoded my analyses in 
C. However, I cannot find the formula to calculate either the type I or 
type III sums of squares (in the case of my model, the two are 
equivalent). I know that the formula must be in the R source code, as 
they are able to calculate it, but I am not sure where. Does anyone know 
where I can find the explicit procedure for calculating this? A 
mathematical formula or the source code would be equally helpful. I am 
aware of the formula in matrix algebra, but is there a formulation that 
does not use matrix algebra?

thanks very much in advance,
Paul Litvak
Department of Human Genetics
University of Michigan



From Mcgeoghan at Cardiff.ac.uk  Mon Aug 18 20:46:41 2003
From: Mcgeoghan at Cardiff.ac.uk (Paul Mcgeoghan)
Date: Mon, 18 Aug 2003 19:46:41 +0100
Subject: [R] R and Poisson
Message-ID: <sf412d43.028@MAINCF1P.cf.ac.uk>

Hi, I wonder if anyone can answer the following or point me in the direction of
how to obtain answers to the questions. Below is Output from R and further down
are the questions raised and explanation of the study.

Output from R:
glm(formula = CB95TO00 ~ URB + INC, family = poisson)

Deviance Residuals: 
    Min       1Q   Median       3Q      Max  
-1.2272  -1.1290   0.2709   0.4272   2.1376  

Coefficients:
            Estimate Std. Error z value Pr(>|z|)  
(Intercept) -0.30621    0.13499  -2.268   0.0233 *
URB2         0.02253    0.16826   0.134   0.8935  
URB3        -0.00936    0.15263  -0.061   0.9511  
INC2        -0.14430    0.12342  -1.169   0.2423  
INC3        -0.55092    0.31351  -1.757   0.0789 .
---
Signif. codes:  0 `***' 0.001 `**' 0.01 `*' 0.05 `.' 0.1 ` ' 1 

(Dispersion parameter for poisson family taken to be 1)

    Null deviance: 403.97  on 420  degrees of freedom
Residual deviance: 399.61  on 416  degrees of freedom
AIC: 883.8

Number of Fisher Scoring iterations: 4




Explanation and Questions raised.

The dependent variable is:

Number of children born in last 5 years: (values range from 0 to 3).
Distribution of dependent variable (named CB95TO00) 
0  203
1  157
2    59
3     2

Predictors are: 
Level of Urbanisation (3 categories 1: Rural; 2:Semi-Urban; 3: Urban)
Income  Level (3 categories: 1: Low; 2:Medium; 3: High)

The questions are (1) how does one interpret the coefficients in the output: 
Our interpretation is Urb2 compared to Urb1 gives an estimate of .02253;
Urb3 compared to Urb1 gives a parameter estimate of -.00936 etc. Neither of
these shows significance. How does one interpret this exactly with regards to
the dependent variable which is Number of children?
2) How does one interpret the intercept which shows significance?
3) What does the Null Deviance tell us and the Residual Deviance?
4) What does the AIC tell us?
5) Is it possible to obtain goodness of fit statistics such as Pearson
ChiSquare and Log-Likelihood similar to what SAS statistical software gives?
6) Is it possible to find out if Urbanisation and Income are significant
overall in R?

Thanks in advance for any assistance,
Regards,
Paul


==================
Paul McGeoghan,
Application support specialist (Statistics and Databases),
Information Services,
Cardiff University.
Tel. 02920 (875035).



From afilatei at 3rdmill.com  Mon Aug 18 20:57:27 2003
From: afilatei at 3rdmill.com (Akpodigha Filatei)
Date: Mon, 18 Aug 2003 14:57:27 -0400
Subject: [R] R Install on Solaris 9
Message-ID: <91E07BE8C095F448AEA244DF07356F1CB62814@santana.lemond.3rdmill.com>


I am trying to install R-1.7.1 or R-1.6.2 on solaris 9 but the configure is failing on me:
Below is the error.  Anybody with similar experience out there? Your help will be appreciated highly!


checking for an ANSI C-conforming const... yes
checking for int... yes
checking size of int... configure: error: cannot compute sizeof (int), 77


-----Original Message-----
From: r-help-request at stat.math.ethz.ch
[mailto:r-help-request at stat.math.ethz.ch]
Sent: Monday, August 18, 2003 2:44 PM
To: Akpodigha Filatei
Subject: Welcome to the "R-help" mailing list


Welcome to the R-help at stat.math.ethz.ch mailing list!

To post to this list, send your email to:

  r-help at stat.math.ethz.ch

General information about the mailing list is at:

  https://www.stat.math.ethz.ch/mailman/listinfo/r-help

If you ever want to unsubscribe or change your options (eg, switch to
or from digest mode, change your password, etc.), visit your
subscription page at:

  https://www.stat.math.ethz.ch/mailman/options/r-help/afilatei%403rdmill.com


You can also make such adjustments via email by sending a message to:

  R-help-request at stat.math.ethz.ch

with the word `help' in the subject or body (don't include the
quotes), and you will get back a message with instructions.

You must know your password to change your options (including changing
the password, itself) or to unsubscribe.  It is:

  foozbi

Normally, Mailman will remind you of your stat.math.ethz.ch mailing
list passwords once every month, although you can disable this if you
prefer.  This reminder will also include instructions on how to
unsubscribe or change your account options.  There is also a button on
your options page that will email your current password to you.



From tblackw at umich.edu  Mon Aug 18 21:01:38 2003
From: tblackw at umich.edu (Thomas W Blackwell)
Date: Mon, 18 Aug 2003 15:01:38 -0400 (EDT)
Subject: [R] type I and type III sums of squares
In-Reply-To: <3F411872.1080001@umich.edu>
Message-ID: <Pine.SOL.4.44.0308181449190.24909-100000@timepilot.gpcc.itd.umich.edu>

Paul  -

Your question is best answered by a textbook reference, because
that will supply all the context needed to fully answer your
question.  A good, basic reference is:

George W. Snedecor and William G. Cochran (1980)
Statistical Methods, 7th edition.  Iowa State Univ. Press.
ISBN: 0-8138-1560-6; LC:  QA 276.12 .S591 1980

(I have the Taubman copy already checked out - others in the
Science Library.)  A more advanced reference is:

George A. Milliken and Dallas E. Johnson (1984)
Analysis of messy data (2 vols.)  Van Nostrand Reinhold, NY
ISBN: 0-534-02713-x;  LC:  QA 279 .M481 1984

(Science library only, more recent edition in Public Health library.)

The terms "type I" and "type III" are specific to SAS software.
Their precise definitions are given in the SAS documentation.
I don't have a copy handy.  George Milliken was a contributor
to the SAS software, so his definitions will coincide with SAS's.

HTH  -  tom blackwell  -  program in bioinformatics and department
    of human genetics  -  u michigan medical school  -  ann arbor  -

On Mon, 18 Aug 2003, Paul Litvak wrote:

> I have been digging around in the FAQ's and online looking for an answer
> to my questions, and perhaps someone here can help me.
>
> For a statistical experiment, I need to run 3,000,000 ANOVAs, which is
> taking me a very long time. As a result, I have recoded my analyses in
> C. However, I cannot find the formula to calculate either the type I or
> type III sums of squares (in the case of my model, the two are
> equivalent). I know that the formula must be in the R source code, as
> they are able to calculate it, but I am not sure where. Does anyone know
> where I can find the explicit procedure for calculating this? A
> mathematical formula or the source code would be equally helpful. I am
> aware of the formula in matrix algebra, but is there a formulation that
> does not use matrix algebra?
>
> thanks very much in advance,
> Paul Litvak
> Department of Human Genetics
> University of Michigan



From raja.surapanani at fmr.com  Mon Aug 18 21:33:25 2003
From: raja.surapanani at fmr.com (Raja Surapanani)
Date: Mon, 18 Aug 2003 15:33:25 -0400
Subject: [R] Responses to question about order()
Message-ID: <200308181533.25394.raja.surapanani@fmr.com>

Thanks to all for your responses regarding my question of why order() was 
behaving differently for me on my linux and solaris platforms. It seems that 
order() behaves according to the collating sequence and character set that 
the particular platform assumes.  I mistakingly assumed that the function 
would behave the same regardless of the platform. I have briefly summarized 
the responses I received. 

Don MacQueen suggested that I try creating a text file outside of R with each 
of the characters on a separate line to see if the different operating 
systems' sort command on the file produced different results. 

James Holtman and Uwe Ligges both pointed to the fact that the character set 
and collating sequence could be different on each platform, which would 
affect the functionality of order(). 

Richard A. O'Keefe suggested that I check the settings of each environment by 
lookign at the variables LC_COLLATE and/or LANG. He also suggested a C 
program for me to run to check for plausible answers. 

-Raja



From andy_liaw at merck.com  Mon Aug 18 21:47:11 2003
From: andy_liaw at merck.com (Liaw, Andy)
Date: Mon, 18 Aug 2003 15:47:11 -0400
Subject: [R] type I and type III sums of squares
Message-ID: <3A822319EB35174CA3714066D590DCD50205C9E9@usrymx25.merck.com>

Not knowing any more details about your experiment and data, we can only
speculate.  If the reason (or part of the reason) that you need to run ANOVA
3 million times is that you have that many responses collected from the same
experiment (or several experiments, but not 3 million different
experiments), you should be able to do the ANOVA computation in R very
efficiently.  E.g., assuming you actually have one experiment with 3m
responses, you can compute the hat matrix once and apply it to the response
matrix, rather than computing the same hat matrix 3M times.

Just a thought.  HTH.

Andy

> -----Original Message-----
> From: Paul Litvak [mailto:plitwak at umich.edu] 
> Sent: Monday, August 18, 2003 2:18 PM
> To: r-help at stat.math.ethz.ch
> Subject: [R] type I and type III sums of squares
> 
> 
> Hello-
> 
> I have been digging around in the FAQ's and online looking 
> for an answer 
> to my questions, and perhaps someone here can help me.
> 
> For a statistical experiment, I need to run 3,000,000 ANOVAs, 
> which is 
> taking me a very long time. As a result, I have recoded my 
> analyses in 
> C. However, I cannot find the formula to calculate either the 
> type I or 
> type III sums of squares (in the case of my model, the two are 
> equivalent). I know that the formula must be in the R source code, as 
> they are able to calculate it, but I am not sure where. Does 
> anyone know 
> where I can find the explicit procedure for calculating this? A 
> mathematical formula or the source code would be equally 
> helpful. I am 
> aware of the formula in matrix algebra, but is there a 
> formulation that 
> does not use matrix algebra?
> 
> thanks very much in advance,
> Paul Litvak
> Department of Human Genetics
> University of Michigan
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list 
> https://www.stat.math.ethz.ch/mailman/listinfo> /r-help
> 

------------------------------------------------------------------------------
Notice:  This e-mail message, together with any attachments, contains
information of Merck & Co., Inc. (Whitehouse Station, New Jersey, USA), and/or
its affiliates (which may be known outside the United States as Merck Frosst,
Merck Sharp & Dohme or MSD) that may be confidential, proprietary copyrighted
and/or legally privileged, and is intended solely for the use of the
individual or entity named on this message.  If you are not the intended
recipient, and have received this message in error, please immediately return
this by e-mail and then delete it.



From MSchwartz at medanalytics.com  Mon Aug 18 22:17:54 2003
From: MSchwartz at medanalytics.com (Marc Schwartz)
Date: Mon, 18 Aug 2003 15:17:54 -0500
Subject: [R] FYI: Article on R at IBM's developerWorks Server Clinic
Message-ID: <1061237874.4202.18.camel@localhost>

Hi all,

I happened to be reviewing a Linux web site that I frequent
(http://www.pclinuxonline.com/index.php) and noted today an entry for an
article on R at IBM's developerWorks Server Clinic site located at
http://www-106.ibm.com/developerworks/linux/library/l-sc16.html. I
thought that I would pass this on as an FYI.

Regards,

Marc Schwartz



From Roger.Bivand at nhh.no  Mon Aug 18 22:20:40 2003
From: Roger.Bivand at nhh.no (Roger Bivand)
Date: Mon, 18 Aug 2003 22:20:40 +0200 (CEST)
Subject: [R] R and Poisson
In-Reply-To: <sf412d43.028@MAINCF1P.cf.ac.uk>
Message-ID: <Pine.LNX.4.44.0308182216030.27991-100000@reclus.nhh.no>

On Mon, 18 Aug 2003, Paul Mcgeoghan wrote:

I think that Peter Dalgaard's book "Introductory Statistics with R" will
help, especially for some of the "generic" functions, even though his
chapter 11 is for logistic regression, not specifically the Poisson case.  
His book contains further references. I think you may find that
help(anova.glm) will provide some insight too.

> Hi, I wonder if anyone can answer the following or point me in the direction of
> how to obtain answers to the questions. Below is Output from R and further down
> are the questions raised and explanation of the study.
> 
> Output from R:
> glm(formula = CB95TO00 ~ URB + INC, family = poisson)
> 
> Deviance Residuals: 
>     Min       1Q   Median       3Q      Max  
> -1.2272  -1.1290   0.2709   0.4272   2.1376  
> 
> Coefficients:
>             Estimate Std. Error z value Pr(>|z|)  
> (Intercept) -0.30621    0.13499  -2.268   0.0233 *
> URB2         0.02253    0.16826   0.134   0.8935  
> URB3        -0.00936    0.15263  -0.061   0.9511  
> INC2        -0.14430    0.12342  -1.169   0.2423  
> INC3        -0.55092    0.31351  -1.757   0.0789 .
> ---
> Signif. codes:  0 `***' 0.001 `**' 0.01 `*' 0.05 `.' 0.1 ` ' 1 
> 
> (Dispersion parameter for poisson family taken to be 1)
> 
>     Null deviance: 403.97  on 420  degrees of freedom
> Residual deviance: 399.61  on 416  degrees of freedom
> AIC: 883.8
> 
> Number of Fisher Scoring iterations: 4
> 
> 
> 
> 
> Explanation and Questions raised.
> 
> The dependent variable is:
> 
> Number of children born in last 5 years: (values range from 0 to 3).
> Distribution of dependent variable (named CB95TO00) 
> 0  203
> 1  157
> 2    59
> 3     2
> 
> Predictors are: 
> Level of Urbanisation (3 categories 1: Rural; 2:Semi-Urban; 3: Urban)
> Income  Level (3 categories: 1: Low; 2:Medium; 3: High)
> 
> The questions are (1) how does one interpret the coefficients in the output: 
> Our interpretation is Urb2 compared to Urb1 gives an estimate of .02253;
> Urb3 compared to Urb1 gives a parameter estimate of -.00936 etc. Neither of
> these shows significance. How does one interpret this exactly with regards to
> the dependent variable which is Number of children?
> 2) How does one interpret the intercept which shows significance?
> 3) What does the Null Deviance tell us and the Residual Deviance?
> 4) What does the AIC tell us?
> 5) Is it possible to obtain goodness of fit statistics such as Pearson
> ChiSquare and Log-Likelihood similar to what SAS statistical software gives?
> 6) Is it possible to find out if Urbanisation and Income are significant
> overall in R?
> 
> Thanks in advance for any assistance,
> Regards,
> Paul
> 
> 
> ==================
> Paul McGeoghan,
> Application support specialist (Statistics and Databases),
> Information Services,
> Cardiff University.
> Tel. 02920 (875035).
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> 

-- 
Roger Bivand
Economic Geography Section, Department of Economics, Norwegian School of
Economics and Business Administration, Breiviksveien 40, N-5045 Bergen,
Norway. voice: +47 55 95 93 55; fax +47 55 95 93 93
e-mail: Roger.Bivand at nhh.no



From abunn at montana.edu  Mon Aug 18 23:36:35 2003
From: abunn at montana.edu (Andy Bunn)
Date: Mon, 18 Aug 2003 15:36:35 -0600
Subject: [R] FYI: Article on R at IBM's developerWorks Server Clinic
In-Reply-To: <1061237874.4202.18.camel@localhost>
Message-ID: <000301c365d0$e007db00$4fa00ecf@simATE>

I hesitate to ask, but in the the ibm article it states:
"...R might be statistical rather than scientific in some pedantic sense..."

Why is that distinction necessary?



-----Original Message-----
From: r-help-bounces at stat.math.ethz.ch
[mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Marc Schwartz
Sent: Monday, August 18, 2003 2:18 PM
To: r-help at stat.math.ethz.ch
Subject: [R] FYI: Article on R at IBM's developerWorks Server Clinic


Hi all,

I happened to be reviewing a Linux web site that I frequent
(http://www.pclinuxonline.com/index.php) and noted today an entry for an
article on R at IBM's developerWorks Server Clinic site located at
http://www-106.ibm.com/developerworks/linux/library/l-sc16.html. I thought
that I would pass this on as an FYI.

Regards,

Marc Schwartz

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://www.stat.math.ethz.ch/mailman/listinfo/r-help



From tblackw at umich.edu  Tue Aug 19 01:07:44 2003
From: tblackw at umich.edu (Thomas W Blackwell)
Date: Mon, 18 Aug 2003 19:07:44 -0400 (EDT)
Subject: [R] glmmPQL() and memory limitations
In-Reply-To: <Pine.LNX.4.44.0308180927300.19484-100000@vonneumann.cog.jhu.edu>
Message-ID: <Pine.SOL.4.44.0308181817160.24909-100000@timepilot.gpcc.itd.umich.edu>

Elliott  -

I don't know if you've had any other responses off-list yet; none
have shown up on the r-help mailing list during the day today.
I'm really NOT the most expert person to answer this, but I'll give
it a try.

Your option (1) seems entirely possible to me.

Let me do some thinking out loud to see how the numbers add up.
The design matrix for the fixed effects should have dimensions

11,000 rows  x  ((6 + 6) * 2) = 24 columns  =  264 K values

The design matrix for the random effects might have dimensions

11,000 rows  x  ((1 + (6 + 6) * 2) * 16) = 400 columns  =  4.4 M values

Say 4.7 M values, total.  At worst, these will be stored as
8-bit double precision numbers, (they very likely are) so 38 Mb
for one copy of the logistic regression problem.  Ah, but then I
look at the error message you quote below, and there's some single
object of 62 Mb that R is manipulating.  My calculation above is
low by a factor of 1.6 or so.

R wants quite a lot of space to turn around in.  I usually figure
4 copies of the data just to do the simplest arithmetic and assign
the result.  The function  glmmPQL()  might be keeping 10 or 20
copies of the regression problem around - but that's only 760 Mb,
(assuming 38 Mb each), so if that were all, you would be okay.
If each node is running an instance of the problem on both processors,
then they have only 1 Gb each, and you're pretty close to the limit,
including R's overhead and the operating system overhead.

If there's a way to keep one processor empty on each node, that
would double the memory available to each instance of the problem
(but it ONLY doubles it).

I observe,  11,000 rows >> 6 * 6 * 2 * 16 = 1152.  That suggests
there might be a way to collapse multiple Bernoulli outcomes at
the same combination of  Subject, Stop, Son and StopResp  into
a binomial outcome  (# successes, # failures)  as for  glm().
I don't know whether  glmmPQL()  supports this response data
format.  (See "Details" in  help("glm") to see what I'm talking
about.)  If you are able to do this, it could reduce the size of
the random factor design matrix proportionately.

For single-processor implementations of R, the information you
might want is on the help pages  help("Memory")  and  help("gc").
I've NO experience with threaded versions and how they behave.

Always, the error message you quote below only describes the
last allocation event which failed.  It doesn't tell you what
the total that was successfully allocated in previous tries is.
So it's not just the first call for 62 Mb which fails.

Guess I've come to the end of whatever slight help I can offer.
Please do come back and tell us what the ultimate outcome on this
question turns out to be.  And, if you have had other off-list
responses during the day, you might summarize them in an email
back to the list so that the rest of us know that your question
is being dealt with appropriately.

-  tom blackwell  -  u michigan medical school  -  ann arbor  -

On Mon, 18 Aug 2003, Elliott Moreton wrote:

> When running glmmPQL(), I keep getting errors like
>
> 	Error: cannot allocate vector of size 61965 Kb
> 	Execution halted
>
> This is R-1.7.1.  The data set consists of about 11,000 binary responses
> from 16 subjects.  The model is
>
> 	fixed =
> 	SonResp ~ (ordered (Stop) + ordered (Son)) * StopResp,
>
> 	random =
> 	~ 1 + (ordered (Stop) + ordered (Son)) * StopResp | Subj
>
> 	family = binomial (link = logit)
>
> SonResp and StopResp are binary; Stop and Son are ordered factors with six
> levels each.
>
> The machine I'm running this on is my university's scientific server, a
> Beowulf Linux cluster; the machine this job would be running on would have
> two 1.4 GHz CPUS, a 2-gigabyte RAM, and an 18-gigabyte hard disk, plus 130
> gigabytes of scratch file space; it would be running Red Hat Linux 7.2
> with XFS.
>
> Can anyone tell me whether this is (1) a problem with the model (no
> machine could fit it in the lifetime of the universe), (2) a problem with
> how I formulated the model (there's a way to get the same end result
> without overflowing memory), (c) a problem with glmmPQL() (that could be
> fixed by using some other package), (d) a problem with the machine I'm
> running it on (need more real or virtual memory), or (e) other?
> (Naturally, I've contacted the system administrators to ask them the same
> thing, but I don't know how much they know about R.)
>
> Many thanks in advance,
> Elliott Moreton



From arrayprofile at yahoo.com  Tue Aug 19 02:06:47 2003
From: arrayprofile at yahoo.com (array chip)
Date: Mon, 18 Aug 2003 17:06:47 -0700 (PDT)
Subject: [R] princomp scores reproduced
Message-ID: <20030819000647.26873.qmail@web41202.mail.yahoo.com>

Hi,

I used "princomp" for PCA analysis based on
correlation matrix (cor=T). I would like to reproduce
the scores for each observation by first standardizing
the data matrix (mean=0, std err=1), and then
multiplied by the loadings of each variable for each
principle components. I get very close numbers, but
not exactly the same. anything I missed here?

tahnks



From spencer.graves at pdf.com  Tue Aug 19 03:33:31 2003
From: spencer.graves at pdf.com (Spencer Graves)
Date: Mon, 18 Aug 2003 18:33:31 -0700
Subject: [R] princomp scores reproduced
References: <20030819000647.26873.qmail@web41202.mail.yahoo.com>
Message-ID: <3F417E6B.10901@pdf.com>

	  You might get a more informative response if you provide a toy 
example with data and what you did that almost but not quite produced 
the correct numbers.  The easiest questions to answer often come with 
data and sample code that can be copied directly from an email message 
and pasted into R.  Then someone can experiment with alternatively ways 
of doing something without taking much time away from the things they 
are paid to do.

	  Otherwise, we can only guess what you did that did not produce the 
answer.  There are an infinite number of ways to do anything wrong, and 
often even an infinite number of ways to get almost the right answer.

sorry i couldn't be more helpful.  spencer graves

array chip wrote:
> Hi,
> 
> I used "princomp" for PCA analysis based on
> correlation matrix (cor=T). I would like to reproduce
> the scores for each observation by first standardizing
> the data matrix (mean=0, std err=1), and then
> multiplied by the loadings of each variable for each
> principle components. I get very close numbers, but
> not exactly the same. anything I missed here?
> 
> tahnks
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help



From jasont at indigoindustrial.co.nz  Tue Aug 19 06:12:47 2003
From: jasont at indigoindustrial.co.nz (Jason Turner)
Date: Tue, 19 Aug 2003 16:12:47 +1200
Subject: [R] Books for R
In-Reply-To: <OF554EDFA6.58ED7CE2-ONC1256D81.00367BE6-C1256D81.0035F3A0@tomware.it>
References: <OF554EDFA6.58ED7CE2-ONC1256D81.00367BE6-C1256D81.0035F3A0@tomware.it>
Message-ID: <3F41A3BF.3070405@indigoindustrial.co.nz>

alessandro.semeria at cramont.it wrote:
> Then the best book is Venables & Ripley (2002) for R.
> A.S.
> 

Agreed.  V&R (2002) also cites Bates & Pinheiro's "Mixed Effects Models 
  in S and S-PLUS" (2000) is particularly good for linear and non-linear 
mixed-effects models.  Highly recommended.

Cheers

Jason
-- 
Indigo Industrial Controls Ltd.
64-21-343-545
jasont at indigoindustrial.co.nz



From zitan at mediasculpt.net  Wed Aug 20 14:24:57 2003
From: zitan at mediasculpt.net (Zitan Broth)
Date: Wed, 20 Aug 2003 05:24:57 -0700
Subject: [R] Command line R / PHP?
Message-ID: <030601c36716$11fd24d0$3201a8c0@zitan>

Greetings All,

Just a quick query about calling R.  Looking through the manual you start R
with $ R, and then start calling R functions e.g plot whatever.  Sounds
pretty funky, and R looks to be *the* open source maths package.  Awesome
...  I would like to call R from my favourite glue language PHP (rather than
call perl which calls R) if possible.  To call R from the command line is
all this would require and this also seems quite possible ::

    Batch use: At its simplest, Rterm.exe can be used in a batch mode by
commands like
        Rterm.exe --no-restore --no-save < infile > outfile

And there is more information on this in section B.1 Invoking R under UNIX
cool.

However what I can't find is how to specify what function I want to run on
my infile, say calculate standard deviation or means or linear regression
and getting probabilities translations for t-statistics.  Have I missed this
in the docs?

Any suggestions are greatly appreciated.   Awesome and a big thanks.

Z.



From christoph.lehmann at gmx.ch  Tue Aug 19 10:15:52 2003
From: christoph.lehmann at gmx.ch (Christoph Lehmann)
Date: Tue, 19 Aug 2003 10:15:52 +0200
Subject: [R] test for equal distributions with small numbes of observations
Message-ID: <1061280952.1203.29.camel@christophl>

Dear R-pros

I have a problem, for which usually I would apply a chisq.test or a
fisher.test:



40 objects, each given either a "0" or "1", regarding if this object
later on will be remembered by a subject or not.

7 subjects investigated

means: we have a 2x40 matrix, each cell the number of subjects for who
the object i has been given either "0" or "1" e.g.

objects:
	1	1	3		39	40
	------------------------------------------
"0"	1	2	2	..	7	7
"1"	4	4	5	..	0	0

over all 40 objects, we have 67% of "1" and 33% of "0"

I want to know, if for the 40 objects, the ratio of "0"/"1" differs or
not, i.e. if they have the same distribution.

I cannot use a chisq.test since the expected frequencies are < 5 for the
"0" cells.

Fisher.test seems to run for > 12h on a PIV 1.8GHz...

what do you recommend me to do?

Many thanks

Christoph
--
> recognition
     [,1] [,2] [,3] [,4] [,5] [,6] [,7] [,8] [,9] [,10] [,11] [,12] [,13] [,14]
[1,]    1    2    2    2    2    3    3    3    3     3     3     3     4     4
[2,]    4    4    5    4    4    3    4    4    4     4     3     4     3     3
     [,15] [,16] [,17] [,18] [,19] [,20] [,21] [,22] [,23] [,24] [,25] [,26]
[1,]     4     4     4     4     4     4     5     5     5     5     5     5
[2,]     3     2     3     3     2     3     2     2     2     2     2     2
     [,27] [,28] [,29] [,30] [,31] [,32] [,33] [,34] [,35] [,36] [,37] [,38]
[1,]     5     5     5     5     5     6     6     6     6     6     6     6
[2,]     2     2     2     2     2     1     1     1     1     1     1     1
     [,39] [,40]
[1,]     7     7
[2,]     0     0

> fisher.test(recognition)


-- 
Christoph Lehmann <christoph.lehmann at gmx.ch>



From sigma at consultoresestadisticos.com  Tue Aug 19 11:04:11 2003
From: sigma at consultoresestadisticos.com (Carlos J. Gil Bellosta)
Date: Tue, 19 Aug 2003 04:04:11 -0500
Subject: [R] test for equal distributions with small numbes of observations
References: <1061280952.1203.29.camel@christophl>
Message-ID: <3F41E80B.4080004@consultoresestadisticos.com>

Probably, to check homogenity you only need to consider a vector of 
dimension 8 consisting in the number of times you get any of the 
configurations (0,7), (1,6), (2,5),...(7,0). This is most likely a 
sufficient statistics. Under homogeneity, it should be distributed 
according to a multinomial random variable with probability vector equal 
to the density of a binomial variable of unknown parameter p.

Then you can use standard tests to see if the MVE fits or if it does not 
fit the data.

Carlos J. Gil Bellosta
Sigma Consultores Estad?sticos
http://www.consultoresestadisticos.com

Christoph Lehmann wrote:

>Dear R-pros
>
>I have a problem, for which usually I would apply a chisq.test or a
>fisher.test:
>
>
>
>40 objects, each given either a "0" or "1", regarding if this object
>later on will be remembered by a subject or not.
>
>7 subjects investigated
>
>means: we have a 2x40 matrix, each cell the number of subjects for who
>the object i has been given either "0" or "1" e.g.
>
>objects:
>	1	1	3		39	40
>	------------------------------------------
>"0"	1	2	2	..	7	7
>"1"	4	4	5	..	0	0
>
>over all 40 objects, we have 67% of "1" and 33% of "0"
>
>I want to know, if for the 40 objects, the ratio of "0"/"1" differs or
>not, i.e. if they have the same distribution.
>
>I cannot use a chisq.test since the expected frequencies are < 5 for the
>"0" cells.
>
>Fisher.test seems to run for > 12h on a PIV 1.8GHz...
>
>what do you recommend me to do?
>
>Many thanks
>
>Christoph
>--
>  
>
>>recognition
>>    
>>
>     [,1] [,2] [,3] [,4] [,5] [,6] [,7] [,8] [,9] [,10] [,11] [,12] [,13] [,14]
>[1,]    1    2    2    2    2    3    3    3    3     3     3     3     4     4
>[2,]    4    4    5    4    4    3    4    4    4     4     3     4     3     3
>     [,15] [,16] [,17] [,18] [,19] [,20] [,21] [,22] [,23] [,24] [,25] [,26]
>[1,]     4     4     4     4     4     4     5     5     5     5     5     5
>[2,]     3     2     3     3     2     3     2     2     2     2     2     2
>     [,27] [,28] [,29] [,30] [,31] [,32] [,33] [,34] [,35] [,36] [,37] [,38]
>[1,]     5     5     5     5     5     6     6     6     6     6     6     6
>[2,]     2     2     2     2     2     1     1     1     1     1     1     1
>     [,39] [,40]
>[1,]     7     7
>[2,]     0     0
>
>  
>
>>fisher.test(recognition)
>>    
>>
>
>
>  
>



From zitan at mediasculpt.net  Wed Aug 20 18:14:41 2003
From: zitan at mediasculpt.net (Zitan Broth)
Date: Wed, 20 Aug 2003 09:14:41 -0700
Subject: [R] Command line R / PHP?
References: <030601c36716$11fd24d0$3201a8c0@zitan>
	<200308190843140843.00254FD1@192.168.1.66>
Message-ID: <001601c36736$2a74abd0$de0c65da@zitan>

Hi Ruud,

Thanks greatly for your response and apologies for not being clear.

Actually I am reading the offline version of the R manual, packaged with R
as I write this :-)   Right now I am reading about the various methods I can
get get data in and out of R with.  I think the method of R interfacing
directly with a database is elegant in one respect, but for the enterprise
system it is probably safer to go with text files or XML.  Interested to
know what the most efficient method is ... anyway ...

What I am trying to do is use R as part of a web-based system and call R
from PHP.  The common method of interfacing from PHP to many systems is via
the command line (although I could use swig to access R directly but that is
phase 2 ;-) ).  I found in the install notes that I could call
Rterm.exe --no-restore --no-save < infile > outfile (windows, although I
will be rolling out to *nix) however I cannot find a reference of how to
call r-functions from the command line with this -- or perhaps I've missed
the point ?

I did find in the FAQ: 7.22 How can I get command line editing to work?  But
I'm not sure I understand the answer ..

So say as a simple example I want to call sd() (standard deviation) from the
command line what would I type ... or do I need to write some R code and
call this .. ?

I will continue to read .... Z.


----- Original Message -----

> I don't understand your question exactly. Have you looked at
> http://www.r-project.org/ under manuals? Have you looked into Venables and
> Ripley (2002) Modern Applied Statistics 4th Ed., Springer? Ruud
>
> *********** REPLY SEPARATOR  ***********
>
> On 8/20/2003 at 5:24  Zitan Broth wrote:
>
> >Greetings All,
> >
> >Just a quick query about calling R.  Looking through the manual you start
> R
> >with $ R, and then start calling R functions e.g plot whatever.  Sounds
> >pretty funky, and R looks to be *the* open source maths package.  Awesome
> >...  I would like to call R from my favourite glue language PHP (rather
> >than
> >call perl which calls R) if possible.  To call R from the command line is
> >all this would require and this also seems quite possible ::
> >
> >    Batch use: At its simplest, Rterm.exe can be used in a batch mode by
> >commands like
> >        Rterm.exe --no-restore --no-save < infile > outfile
> >
> >And there is more information on this in section B.1 Invoking R under
UNIX
> >cool.
> >
> >However what I can't find is how to specify what function I want to run
on
> >my infile, say calculate standard deviation or means or linear regression
> >and getting probabilities translations for t-statistics.  Have I missed
> >this
> >in the docs?
> >
> >Any suggestions are greatly appreciated.   Awesome and a big thanks.
> >
> >Z.
> >
> >______________________________________________
> >R-help at stat.math.ethz.ch mailing list
> >https://www.stat.math.ethz.ch/mailman/listinfo/r-help
>
>
>
>



From doc at mathworks.com  Tue Aug 19 13:32:57 2003
From: doc at mathworks.com (The MathWorks Inc. Documentation)
Date: Tue, 19 Aug 2003 07:32:57 -0400 (EDT)
Subject: [R] Re:  Thank you!
Message-ID: <200308191132.h7JBWvN14742@fred-ce0.mathworks.com>



Thank you very much for your comments about the documentation.  

We will correct any errors in the next version of the documentation. 
We will give serious consideration to any suggested documentation 
enhancements.

We appreciate your contribution to improving the documentation,
and apologize for any inconvenience the errors or omissions in
the documentation may have caused you.

____________________________________________________________________

Greg Bartlett
Documentation Manager
bartlett at mathworks.com
508-647-7575
508-647-7002 (FAX)



From alert at notification.messagelabs.com  Tue Aug 19 13:33:33 2003
From: alert at notification.messagelabs.com (alert@notification.messagelabs.com)
Date: Tue, 19 Aug 2003 11:33:33 -0000
Subject: [R] WARNING. You tried to send a potential virus or unauthorised
	code
Message-ID: <20030819112614.17183.qmail@server-16.tower-1.messagelabs.com>

The MessageLabs SkyScan Anti-Virus service discovered a possible virus
or unauthorised code (such as a joke program or trojan) in an email sent
by you.

The email has now been quarantined and was not delivered.

Please read the whole of this email carefully.  It explains what has
happened to your email, which suspected virus has been caught and what
to do if you need help addressing the problem.

To help identify the quarantined email:

The message sender was 
    r-help at lists.r-project.org

The message recipients were 
    info at biomedcentral.com

The message title was Your details 
The message date was Tue, 19 Aug 2003 12:57:34 +0200
The virus or unauthorised code identified in the email is
/var/qmail/queue/split/1/attach/570489_2X_PM4_EMS_MA-OCTET=2DSTREAM__movie0045.pif
        Found the W32/Sobig.f at MM virus !!!


Some viruses forge the sender address. For more information please
visit the link to the virus FAQ's at the bottom of this page.

The message was diverted into the virus holding pen on
mail server server-16.tower-1.messagelabs.com (pen id 570489_1061292374)
and will be held for 30 days before being destroyed

Corporate Users:
If you sent the email from a corporate network, you should first
contact your local IT Helpdesk or System Administrator for advice.
They will be able to help you disinfect your workstation.

If you would like further information on how to subscribe to MessageLabs
SkyScan AV service, a proactive anti-virus service working around the 
clock, around the globe, please visit 
http://www.messagelabs.com/page.asp?id=323

Personal or Home users:
If you sent the email from a personal or home account, you will need
to disinfect your computer yourself.  Please contact your anti-virus
software vendor for support.

You may like to read the virus FAQ's at:
http://www.messagelabs.com/page.asp?id=628
which will answer most virus related questions.

________________________________________________________________________
This email has been scanned for all viruses by the MessageLabs Email
Security System. For more information on a proactive email security
service working around the clock, around the globe, visit
http://www.messagelabs.com



From tblackw at umich.edu  Tue Aug 19 14:14:45 2003
From: tblackw at umich.edu (Thomas W Blackwell)
Date: Tue, 19 Aug 2003 08:14:45 -0400 (EDT)
Subject: [R] Command line R / PHP?
In-Reply-To: <001601c36736$2a74abd0$de0c65da@zitan>
Message-ID: <Pine.SOL.4.44.0308190756530.29497-100000@millipede.gpcc.itd.umich.edu>

On Wed, 20 Aug 2003, Zitan Broth wrote:
  . . .

> What I am trying to do is use R as part of a web-based system and call R
> from PHP.  The common method of interfacing from PHP to many systems is via
> the command line (although I could use swig to access R directly but that is
> phase 2 ;-) ).  I found in the install notes that I could call
> Rterm.exe --no-restore --no-save < infile > outfile (windows, although I
> will be rolling out to *nix) however I cannot find a reference of how to
> call r-functions from the command line with this -- or perhaps I've missed
> the point ?

infile is an ascii file of R commands - the same commands and same syntax
which you would type in the R command line window.  Frequently the first
line sets some session options using  options(),  the second line reads
some data from a separate, named file into an R object, and the third and
subsequent lines operate on that data and print out the results.

For example:

options(digits=4, width=88, length=1e+8)
object <- read.table("data.file")
summary(lm(y ~ 1 + a + b, object)

(This assumes that "data.file" contains columns named a, b, y in any
order.  It does a linear regression and prints out the results .. in
the command line window if you were working interactively, but to
outfile if R is running noninteractively with the call above.)

(Gosh, my recollection is that in unix the call is R BATCH infile outfile,
but I could be mistaken.  That's on the unix man page for R if you forget.)

> I did find in the FAQ: 7.22 How can I get command line editing to work?  But
> I'm not sure I understand the answer ..

If running R non-interactively, you don't care.

> So say as a simple example I want to call sd() (standard deviation) from the
> command line what would I type ... or do I need to write some R code and
> call this .. ?

Need code to read in the data, then a one line command  sd(object).
The returned value is printed automatically if it is not assigned.

> I will continue to read .... Z.

HTH  -  tom blackwell  -  u michigan medical school  -  ann arbor  -



From laurent.faisnel at ariase.com  Tue Aug 19 14:35:32 2003
From: laurent.faisnel at ariase.com (Laurent Faisnel)
Date: Tue, 19 Aug 2003 14:35:32 +0200
Subject: [R] R-1.7.1 gets installed without default packages & without
	readline
Message-ID: <3F421994.1080609@ariase.com>

Hi all,

Trying to install R-1.7.1 on a RedHat 8.0 platform, I have a few problems.

R gets installed without default packages (but base and ctest) : make 
script fails at the end of the procedure (configure is made 
successfully). This is the (translated) log I have :

/gcc -I../../../../include /usr/include/mysql  -D__NO_MATH_INLINES 
-mieee-fp  -fPIC  -g -O2 -c ansari.c -o ansari.o
gcc: cannot specify -o with -c or -S and multiple compilations
make[5]: *** [ansari.o] Error 1
make[5]: Leaving directory `/usr/local/R-1.7.1/src/library/ctest/src'
make[4]: *** [all] Error 2
make[4]: Leaving directory `/usr/local/R-1.7.1/src/library/ctest/src'
make[3]: *** [all] Error 1
make[3]: Leaving directory `/usr/local/R-1.7.1/src/library/ctest'
make[2]: *** [R] Error 1
make[2]: Leaving directory `/usr/local/R-1.7.1/src/library'
make[1]: *** [R] Error 1
make[1]: Leaving directory// `/usr/local/R-1.7.1/src'
make: *** [R] Error 1
/
However it's possible to start R but I have only base and ctest, and 
subsequently many warnings are displayed at startup. The only solution I 
found was to copy-paste libraries from a RPM version (which gets 
installed without trouble). Probably not a good solution. I already read 
carefully R-admin. What should I try now ?

Moreover, I have a problem with readline. First I did not manage to make 
the configure script satisfied about readline. I found in R archives 
that readline-devel should be present too. I downloaded it, indeed the 
configure script is now OK with readline. But I still have no 
command-line facilities enabled under R. Why ?

[laurent at localhost home]# rpm -qva | grep readline
readline-4.3-3
readline-devel-4.3-3
readline41-4.1-14

Thanks for any help.

Laurent



From p.dalgaard at biostat.ku.dk  Tue Aug 19 14:55:08 2003
From: p.dalgaard at biostat.ku.dk (Peter Dalgaard BSA)
Date: Tue, 19 Aug 2003 12:55:08 -0000
Subject: [R] R-1.7.1 gets installed without default packages & without
	readline
In-Reply-To: <3F421994.1080609@ariase.com>
References: <3F421994.1080609@ariase.com>
Message-ID: <x2brul94vr.fsf@biostat.ku.dk>

Laurent Faisnel <laurent.faisnel at ariase.com> writes:

> Hi all,
> 
> Trying to install R-1.7.1 on a RedHat 8.0 platform, I have a few problems.
> 
> R gets installed without default packages (but base and ctest) : make
> script fails at the end of the procedure (configure is made
> successfully). This is the (translated) log I have :
> 
> /gcc -I../../../../include /usr/include/mysql  -D__NO_MATH_INLINES
> -mieee-fp  -fPIC  -g -O2 -c ansari.c -o ansari.o
> gcc: cannot specify -o with -c or -S and multiple compilations
> make[5]: *** [ansari.o] Error 1
> make[5]: Leaving directory `/usr/local/R-1.7.1/src/library/ctest/src'
> make[4]: *** [all] Error 2
> make[4]: Leaving directory `/usr/local/R-1.7.1/src/library/ctest/src'
> make[3]: *** [all] Error 1
> make[3]: Leaving directory `/usr/local/R-1.7.1/src/library/ctest'
> make[2]: *** [R] Error 1
> make[2]: Leaving directory `/usr/local/R-1.7.1/src/library'
> make[1]: *** [R] Error 1
> make[1]: Leaving directory// `/usr/local/R-1.7.1/src'
> make: *** [R] Error 1
> /
> However it's possible to start R but I have only base and ctest, and
> subsequently many warnings are displayed at startup. The only solution
> I found was to copy-paste libraries from a RPM version (which gets
> installed without trouble). Probably not a good solution. I already
> read carefully R-admin. What should I try now ?
> 
> Moreover, I have a problem with readline. First I did not manage to
> make the configure script satisfied about readline. I found in R
> archives that readline-devel should be present too. I downloaded it,
> indeed the configure script is now OK with readline. But I still have
> no command-line facilities enabled under R. Why ?
> 
> [laurent at localhost home]# rpm -qva | grep readline
> readline-4.3-3
> readline-devel-4.3-3
> readline41-4.1-14

You may need a "make distclean" step to purge what got messed up on
previous builds. 

The other issue seems to be due to a missepecification of your header
files. I think there wants to be a -I infront of /usr/include/mysql
there.

-- 
   O__  ---- Peter Dalgaard             Blegdamsvej 3  
  c/ /'_ --- Dept. of Biostatistics     2200 Cph. N   
 (*) \(*) -- University of Copenhagen   Denmark      Ph: (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)             FAX: (+45) 35327907



From maechler at stat.math.ethz.ch  Tue Aug 19 15:02:51 2003
From: maechler at stat.math.ethz.ch (Martin Maechler)
Date: Tue, 19 Aug 2003 15:02:51 +0200
Subject: [R] Command line R / PHP?
In-Reply-To: <Pine.SOL.4.44.0308190756530.29497-100000@millipede.gpcc.itd.umich.edu>
References: <001601c36736$2a74abd0$de0c65da@zitan>
	<Pine.SOL.4.44.0308190756530.29497-100000@millipede.gpcc.itd.umich.edu>
Message-ID: <16194.8187.511548.774419@gargle.gargle.HOWL>

>>>>> "ThomasB" == Thomas W Blackwell <tblackw at umich.edu>
>>>>>     on Tue, 19 Aug 2003 08:14:45 -0400 (EDT) writes:

    ThomasB> On Wed, 20 Aug 2003, Zitan Broth wrote:
    ThomasB> . . .

    >> What I am trying to do is use R as part of a web-based system and call R
    >> from PHP.  The common method of interfacing from PHP to many systems is via
    >> the command line (although I could use swig to access R directly but that is
    >> phase 2 ;-) ).  I found in the install notes that I could call
    >> Rterm.exe --no-restore --no-save < infile > outfile (windows, although I
    >> will be rolling out to *nix) however I cannot find a reference of how to
    >> call r-functions from the command line with this -- or perhaps I've missed
    >> the point ?

    ThomasB> infile is an ascii file of R commands - the same
    ThomasB> commands and same syntax which you would type in
    ThomasB> the R command line window.  Frequently the first
    ThomasB> line sets some session options using options(), the
    ThomasB> second line reads some data from a separate, named
    ThomasB> file into an R object, and the third and subsequent
    ThomasB> lines operate on that data and print out the
    ThomasB> results.

    ThomasB> For example:

    ThomasB> options(digits=4, width=88, length=1e+8)
    ThomasB> object <- read.table("data.file")
    ThomasB> summary(lm(y ~ 1 + a + b, object)

    ThomasB> (This assumes that "data.file" contains columns
    ThomasB> named a, b, y in any order.  It does a linear
    ThomasB> regression and prints out the results .. in the
    ThomasB> command line window if you were working
    ThomasB> interactively, but to outfile if R is running
    ThomasB> noninteractively with the call above.)

Thanks a lot, Thomas, for the nice explanation..

    ThomasB> (Gosh, my recollection is that in unix the call is
    ThomasB> R BATCH infile outfile, but I could be mistaken.
    ThomasB> That's on the unix man page for R if you forget.)

Both work in Unix (after replacing "Rterm.exe" by "R"). 
Even R BATCH infile does.
The "--no-restore --no-save" in all versions, since the S-back
compatible way is to work with `persistent' objects (via an .RData file).
This persistence if often undesired when working with scripts,
and I'd recommend the above switches and using "save(.. , file=) and
load(file=..)" if desired, with carefully chosen file names
"<....>.rda".

    >> I did find in the FAQ: 7.22 How can I get command line
    >> editing to work?  But I'm not sure I understand the
    >> answer ..

    ThomasB> If running R non-interactively, you don't care.

    >> So say as a simple example I want to call sd() (standard deviation) from the
    >> command line what would I type ... or do I need to write some R code and
    >> call this .. ?

    ThomasB> Need code to read in the data, then a one line
    ThomasB> command sd(object).  The returned value is printed
    ThomasB> automatically if it is not assigned.

    >> I will continue to read .... Z.

    ThomasB> HTH  -  tom blackwell  -  u michigan medical school  -  ann arbor  -
Martin Maechler



From moreton at vonneumann.cog.jhu.edu  Tue Aug 19 15:09:25 2003
From: moreton at vonneumann.cog.jhu.edu (Elliott Moreton)
Date: Tue, 19 Aug 2003 09:09:25 -0400 (EDT)
Subject: [R] Re:  glmmPQL() and memory limitations
Message-ID: <Pine.LNX.4.44.0308190904490.2831-100000@vonneumann.cog.jhu.edu>

Hi,

Thanks to all who have responded so far.  I'll try your suggested 
solutions, and post a summary to the list.  (I'm presently waiting to hear 
whether my local sysadmins will install lme4.)

Best regards,
Elliott Moreton



From afilatei at 3rdmill.com  Tue Aug 19 15:23:42 2003
From: afilatei at 3rdmill.com (Akpodigha Filatei)
Date: Tue, 19 Aug 2003 09:23:42 -0400
Subject: [R] R Install on Solaris 9
Message-ID: <91E07BE8C095F448AEA244DF07356F1CB4B6E6@santana.lemond.3rdmill.com>

Thanks for your response. I actually realized this and downloaded gcc 3.3, compiled it an used it for R compilation and everything worked fine.

-----Original Message-----
From: Brian D Ripley [mailto:ripley at stats.ox.ac.uk]
Sent: Tuesday, August 19, 2003 1:52 AM
To: Akpodigha Filatei
Subject: Re: [R] R Install on Solaris 9


I've seen this: it usually indicates a broken compiler.
You are using a compiler for Solaris 9 (and not, say 2.6)?

>
> I am trying to install R-1.7.1 or R-1.6.2 on solaris 9 but the configure
> is failing on me: Below is the error.  Anybody with similar experience
> out there? Your help will be appreciated highly!
>
>
> checking for an ANSI C-conforming const... yes
> checking for int... yes
> checking size of int... configure: error: cannot compute sizeof (int),
> 77

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From tblackw at umich.edu  Tue Aug 19 15:28:44 2003
From: tblackw at umich.edu (Thomas W Blackwell)
Date: Tue, 19 Aug 2003 09:28:44 -0400 (EDT)
Subject: [R] R-1.7.1 gets installed without default packages & without
	readline
In-Reply-To: <3F421994.1080609@ariase.com>
Message-ID: <Pine.SOL.4.44.0308190910590.4287-100000@millipede.gpcc.itd.umich.edu>

Laurent  -

I had no trouble with configuring, compiling and running R under
Redhat 8.0.  My system now gives

uname -a
  Linux host 2.4.18-14smp #1 SMP Wed Sep4 12:34:47 EDT 2002 i686 i686 i386 GNU/Linux

rpm -qa | grep readline
  readline41-4.1-14
  readline-4.3-4
  readline-devel-4.3-3

The only difference I can see is that I have  readline-4.3-4,
while you show  readline-4.3-3.  So now I remember:  on CRAN,
somewhere near the R source rpm for linux when I downloaded it
there is a special readline rpm, and a readme file from Martyn
Plummer which says, in part,

"Graeme Ambler has kindly provided patched rpms. His gpg ID is
62897321 and his public key is available from pgp.net (see below)."

Somehow, I got readline-4.3-4 out of that rpm.  Hmmmm.  All seems
to work fine for me.

-  tom blackwell  -  u michigan medical school  -  ann arbor  -



From laurent.faisnel at ariase.com  Tue Aug 19 16:00:50 2003
From: laurent.faisnel at ariase.com (Laurent Faisnel)
Date: Tue, 19 Aug 2003 16:00:50 +0200
Subject: [R] R-1.7.1 gets installed without default packages & without
	readline
References: <3F421994.1080609@ariase.com> <x2brul94vr.fsf@biostat.ku.dk>
Message-ID: <3F422D92.4010105@ariase.com>

Peter Dalgaard BSA wrote:

>Laurent Faisnel <laurent.faisnel at ariase.com> writes:
>
>  
>
>>Hi all,
>>
>>Trying to install R-1.7.1 on a RedHat 8.0 platform, I have a few problems.
>>
>>R gets installed without default packages (but base and ctest) : make
>>script fails at the end of the procedure (configure is made
>>successfully). This is the (translated) log I have :
>>
>>/gcc -I../../../../include /usr/include/mysql  -D__NO_MATH_INLINES
>>-mieee-fp  -fPIC  -g -O2 -c ansari.c -o ansari.o
>>gcc: cannot specify -o with -c or -S and multiple compilations
>>make[5]: *** [ansari.o] Error 1
>>make[5]: Leaving directory `/usr/local/R-1.7.1/src/library/ctest/src'
>>make[4]: *** [all] Error 2
>>make[4]: Leaving directory `/usr/local/R-1.7.1/src/library/ctest/src'
>>make[3]: *** [all] Error 1
>>make[3]: Leaving directory `/usr/local/R-1.7.1/src/library/ctest'
>>make[2]: *** [R] Error 1
>>make[2]: Leaving directory `/usr/local/R-1.7.1/src/library'
>>make[1]: *** [R] Error 1
>>make[1]: Leaving directory// `/usr/local/R-1.7.1/src'
>>make: *** [R] Error 1
>>/
>>However it's possible to start R but I have only base and ctest, and
>>subsequently many warnings are displayed at startup. The only solution
>>I found was to copy-paste libraries from a RPM version (which gets
>>installed without trouble). Probably not a good solution. I already
>>read carefully R-admin. What should I try now ?
>>
>>Moreover, I have a problem with readline. First I did not manage to
>>make the configure script satisfied about readline. I found in R
>>archives that readline-devel should be present too. I downloaded it,
>>indeed the configure script is now OK with readline. But I still have
>>no command-line facilities enabled under R. Why ?
>>
>>[laurent at localhost home]# rpm -qva | grep readline
>>readline-4.3-3
>>readline-devel-4.3-3
>>readline41-4.1-14
>>    
>>
>
>You may need a "make distclean" step to purge what got messed up on
>previous builds. 
>
Surely a good idea. I've just ran a make distclean without trouble. But 
I still have the same kind of problem. To be as precise as possible, the 
first error in the make log is 'link edition not done' about 
/usr/lib/mysql. How could I make it ? Anyway, why would MySql prevent R 
from getting installed properly ? I intend to use RMySQL, but this 
should come into consideration later !

>
>The other issue seems to be due to a missepecification of your header
>files.
>
That's what I would have guessed too. I just tried to copy readline.h 
(which was in /usr/lib/mysql, by the way) into R directory, without 
success (it did not change anything).

> I think there wants to be a -I infront of /usr/include/mysql
>there.
>
What should I do then ?

>
>  
>
Thanks for the fast answer.
Best regards,

Laurent



From laurent.faisnel at ariase.com  Tue Aug 19 17:18:33 2003
From: laurent.faisnel at ariase.com (Laurent Faisnel)
Date: Tue, 19 Aug 2003 17:18:33 +0200
Subject: [R] R-1.7.1 gets installed without default packages & without
	readline
References: <Pine.SOL.4.44.0308190910590.4287-100000@millipede.gpcc.itd.umich.edu>
Message-ID: <3F423FC9.6000003@ariase.com>

Thomas W Blackwell wrote:

>Laurent  -
>
>I had no trouble with configuring, compiling and running R under
>Redhat 8.0.  My system now gives
>
>uname -a
>  Linux host 2.4.18-14smp #1 SMP Wed Sep4 12:34:47 EDT 2002 i686 i686 i386 GNU/Linux
>
I have the same

>
>rpm -qa | grep readline
>  readline41-4.1-14
>  readline-4.3-4
>  readline-devel-4.3-3
>
>The only difference I can see is that I have  readline-4.3-4,
>while you show  readline-4.3-3.  So now I remember:  on CRAN,
>somewhere near the R source rpm for linux when I downloaded it
>there is a special readline rpm, and a readme file from Martyn
>Plummer which says, in part,
>
>"Graeme Ambler has kindly provided patched rpms. His gpg ID is
>62897321 and his public key is available from pgp.net (see below)."
>
I've found all this, thanks. In fact, there is also a newer version of 
readline-devel (4.3-4). I updated both libraries. But unfortunately I 
still have the same problems. Is it so hard to switch from R's rpm 
version to the compiled one ? Source version allows you to choose 
compilation options and this may become necessary for me.

>
>Somehow, I got readline-4.3-4 out of that rpm.  Hmmmm.  All seems
>to work fine for me.
>
>-  tom blackwell  -  u michigan medical school  -  ann arbor  -
>
>
>
>
>  
>



From yukiasais at ybb.ne.jp  Tue Aug 19 17:51:30 2003
From: yukiasais at ybb.ne.jp (yukihiro ishii)
Date: Wed, 20 Aug 2003 00:51:30 +0900
Subject: [R] On the Use of the nnet Library
Message-ID: <20030820001936.0058.YUKIASAIS@ybb.ne.jp>

Dear List,

I am trying to solve a problem by the neural network method(library:
nnet). The problem is to express Weight in terms of Age , Sex and Height
for twenty people. The data frame consists of 20 observations with four
variables: Sex, Age, Height and Weight. Sex is treated as a factor, Age
and Weight are variables normalized to unity, as usual. I wanted to
construct a neural network, and so I ran the following code:

>library(nnet)
>net1<-nnet(Weight~Age+Sex+Height, size=2, linout=T,maxit=1000)

I repeated this thirteen times.  I used the default initial parameters
unless otherwise noted. The result is as follows, where init and final
mean initial and final RSS's, and NIT means the number of iterations
before reaching convergence or noncovergence:

Run#	init		NIT	final
1	71991.1 	30	995.1 
2	70870.0 	370	33.1 
3	72755.8 	<10	2134.3 
4	69840.6 	<10	2134.3 
5	70368.8 	190	39.7 
6	70368.8 	270	41.0 
7	71101.2 	190	39.7 
8	71606.1 	<10	2134.3 
9	72076.1 	<10	2134.3 
10	72249.1 	300	15.0 
11	71424.1 	<10	2134.3 
12	68483.8 	130	39.7 
13	71435.9 	>1000	4.6 


As you can see, the result is far from stable.

My question is:

How can I reach a stable answer?

.I know that initial parameters are crucially important in my case, and I
must choose proper parameter values, but I do not know I can do that.

My second question is related to the response analysis of this data. I
do not know an effective method to evaluate the response to
 the variance of each explanatory variable. Is there such a function in
the library, nnet? Such a function may help me reduce the number of the
explanatory variables.

I wonder if anyone could help me in such elementary questions.


---- It's elementary, Watson!

I remain an obedient Watson, hoping for Holmes' wisdom.


-- 
Yukihiro Ishii <yukiasais at ybb.ne.jp>
2-3-28?Tsurumaki-minami, Hadano
257-0002 Japan
Tel +81 463 69 1922
Fax +81 463 69 1922



From mailmarshal at fox.com  Tue Aug 19 18:08:39 2003
From: mailmarshal at fox.com (mailmarshal@fox.com)
Date: Tue, 19 Aug 2003 09:08:39 -0700
Subject: [R] Mgw: Blocked mail "Re: Your application" from
	r-help@lists.r-project.org
Message-ID: <D00000ddbe@ffeplw2dz12.foxinc.com>

The mail scanner blocked the following message:

   Message: B000157964.00000001.mml
   From:    r-help at lists.r-project.org
   To:      zXTlsY at fox.com
   Subject: Re: Your application

Because the intended recipient may not receive messages with
certain potentially dangerous file types attached.

If this message is business related please contact the 
individuals you sent the mail message to and request that
the message be released.

From ORAPOST at oracle.com  Tue Aug 19 19:36:10 2003
From: ORAPOST at oracle.com (ORAPOST)
Date: Tue, 19 Aug 2003 17:36:10 -0000
Subject: [R] Return Message: Thank you!
Message-ID: <200308191736.h7JHaD510480@rgmgw6.us.oracle.com>

The included message could not be delivered to the following invalid mail names.  Please verify these names and try them again.

Bad name:  nedc_doc
-------------- next part --------------
An embedded message was scrubbed...
From: <r-help at hypatia.math.ethz.ch>
Subject: Thank you!
Date: Tue, 19 Aug 2003 18:35:26 +0100
Size: 1776
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20030819/6bf427f6/attachment.mht

From sudAR_80 at neo.tamu.edu  Tue Aug 19 19:42:32 2003
From: sudAR_80 at neo.tamu.edu (Padmanabhan, Sudharsha)
Date: Tue, 19 Aug 2003 17:42:32 -0000
Subject: [R] Variance Computing-  - HELP!!!!!!!!!!!!!!!!!!
Message-ID: <200308191742.h7JHgXTU065342@smtp-relay.tamu.edu>


Hello,

I am running a few simulations for clinical trial anlysis. I want some help 
regarding the following.

We know trhat as the sample size increases, the variance should decrease, but 
I am getting some unexpected results. SO I ran a code (shown below) to check 
the validity of this.

large<-array(1,c(1000,1000))
small<-array(1,c(100,1000))
for(i in 1:1000){large[i,]<-rnorm(1000,0,3)}
for(i in 1:1000){small[i,]<-rnorm(100,0,3)}}
yy<-array(1,100)
for(i in 1:100){yy[i]<-var(small[i,])}
y1y<-array(1,1000)
for(i in 1:1000){y1y[i]<-var(large[i,])}
mean(yy);mean(y1y);
[1] 8.944
[1] 9.098


This shows that on an average,for 1000 such samples of 1000 Normal numbers, 
the variance is higher than that of a 100 samples of 1000 random numbers.

Why is this so?


Can someone please help me out????

Thanks.

Regards

~S.



From sudAR_80 at neo.tamu.edu  Tue Aug 19 19:42:40 2003
From: sudAR_80 at neo.tamu.edu (Padmanabhan, Sudharsha)
Date: Tue, 19 Aug 2003 17:42:40 -0000
Subject: [R] Variance Computing-  - HELP!!!!!!!!!!!!!!!!!!
Message-ID: <200308191742.h7JHgeci039097@smtp-relay.tamu.edu>


Hello,

I am running a few simulations for clinical trial anlysis. I want some help 
regarding the following.

We know trhat as the sample size increases, the variance should decrease, but 
I am getting some unexpected results. SO I ran a code (shown below) to check 
the validity of this.

large<-array(1,c(1000,1000))
small<-array(1,c(100,1000))
for(i in 1:1000){large[i,]<-rnorm(1000,0,3)}
for(i in 1:1000){small[i,]<-rnorm(100,0,3)}}
yy<-array(1,100)
for(i in 1:100){yy[i]<-var(small[i,])}
y1y<-array(1,1000)
for(i in 1:1000){y1y[i]<-var(large[i,])}
mean(yy);mean(y1y);
[1] 8.944
[1] 9.098


This shows that on an average,for 1000 such samples of 1000 Normal numbers, 
the variance is higher than that of a 100 samples of 1000 random numbers.

Why is this so?


Can someone please help me out????

Thanks.

Regards

~S.



From tblackw at umich.edu  Tue Aug 19 20:06:19 2003
From: tblackw at umich.edu (Thomas W Blackwell)
Date: Tue, 19 Aug 2003 14:06:19 -0400 (EDT)
Subject: [R] Variance Computing - HELP
In-Reply-To: <200308191742.h7JHgXTU065342@smtp-relay.tamu.edu>
Message-ID: <Pine.SOL.4.44.0308191403320.15530-100000@millipede.gpcc.itd.umich.edu>

The variance of Xbar decreases as 1/n;  the sample variance
of X does not.

-  tom blackwell  -  u michigan medical school  -  ann arbor  -

On Tue, 19 Aug 2003, Padmanabhan, Sudharsha wrote:

> I am running a few simulations for clinical trial anlysis. I want some help
> regarding the following.
>
> We know trhat as the sample size increases, the variance should decrease, but
> I am getting some unexpected results. SO I ran a code (shown below) to check
> the validity of this.
>
> large<-array(1,c(1000,1000))
> small<-array(1,c(100,1000))
> for(i in 1:1000){large[i,]<-rnorm(1000,0,3)}
> for(i in 1:1000){small[i,]<-rnorm(100,0,3)}}
> yy<-array(1,100)
> for(i in 1:100){yy[i]<-var(small[i,])}
> y1y<-array(1,1000)
> for(i in 1:1000){y1y[i]<-var(large[i,])}
> mean(yy);mean(y1y);
> [1] 8.944
> [1] 9.098
>
> This shows that on an average,for 1000 such samples of 1000 Normal numbers,
> the variance is higher than that of a 100 samples of 1000 random numbers.
>
> Why is this so?
> Can someone please help me out????
>



From baron at psych.upenn.edu  Tue Aug 19 20:08:25 2003
From: baron at psych.upenn.edu (Jonathan Baron)
Date: Tue, 19 Aug 2003 14:08:25 -0400
Subject: [R] Variance Computing-  - HELP!!!!!!!!!!!!!!!!!!
In-Reply-To: <200308191742.h7JHgXTU065342@smtp-relay.tamu.edu>
References: <200308191742.h7JHgXTU065342@smtp-relay.tamu.edu>
Message-ID: <20030819180825.GA21898@mail1.sas.upenn.edu>

On 08/19/03 17:42, Padmanabhan, Sudharsha wrote:
>
>Hello,
>
>I am running a few simulations for clinical trial anlysis. I want some help 
>regarding the following.
>
>We know trhat as the sample size increases, the variance should decrease, but 
>I am getting some unexpected results. SO I ran a code (shown below) to check 
>the validity of this.
>
>large<-array(1,c(1000,1000))
>small<-array(1,c(100,1000))
>for(i in 1:1000){large[i,]<-rnorm(1000,0,3)}
>for(i in 1:1000){small[i,]<-rnorm(100,0,3)}}
>yy<-array(1,100)
>for(i in 1:100){yy[i]<-var(small[i,])}
>y1y<-array(1,1000)
>for(i in 1:1000){y1y[i]<-var(large[i,])}
>mean(yy);mean(y1y);
>[1] 8.944
>[1] 9.098
>
>
>This shows that on an average,for 1000 such samples of 1000 Normal numbers, 
>the variance is higher than that of a 100 samples of 1000 random numbers.
>
>Why is this so?

Don't know, but it could be a fluke.  You don't say how many
times you did it.

I did the following, with 1000 in each test.  You have 100 in the
small test and 1000 in the big one.  My numbers look pretty
close.

> bigmat <- matrix(rnorm(1000000),1000,1000) # 1000 rows of 1000 each
> smallmat <- matrix(rnorm(100000),1000,100) # 1000 rows of 100 each
> mean(apply(bigmat,1,var)) # get variance of each row, then take mean
[1] 0.9999344
> mean(apply(smallmat,1,var))
[1] 0.9967427

-- 
Jonathan Baron, Professor of Psychology, University of Pennsylvania
Home page:            http://www.sas.upenn.edu/~baron
R page:               http://finzi.psych.upenn.edu/



From baron at psych.upenn.edu  Tue Aug 19 20:08:31 2003
From: baron at psych.upenn.edu (Jonathan Baron)
Date: Tue, 19 Aug 2003 14:08:31 -0400
Subject: [R] Variance Computing-  - HELP!!!!!!!!!!!!!!!!!!
In-Reply-To: <200308191742.h7JHgXTU065342@smtp-relay.tamu.edu>
References: <200308191742.h7JHgXTU065342@smtp-relay.tamu.edu>
Message-ID: <20030819180825.GA21898@mail1.sas.upenn.edu>

On 08/19/03 17:42, Padmanabhan, Sudharsha wrote:
>
>Hello,
>
>I am running a few simulations for clinical trial anlysis. I want some help 
>regarding the following.
>
>We know trhat as the sample size increases, the variance should decrease, but 
>I am getting some unexpected results. SO I ran a code (shown below) to check 
>the validity of this.
>
>large<-array(1,c(1000,1000))
>small<-array(1,c(100,1000))
>for(i in 1:1000){large[i,]<-rnorm(1000,0,3)}
>for(i in 1:1000){small[i,]<-rnorm(100,0,3)}}
>yy<-array(1,100)
>for(i in 1:100){yy[i]<-var(small[i,])}
>y1y<-array(1,1000)
>for(i in 1:1000){y1y[i]<-var(large[i,])}
>mean(yy);mean(y1y);
>[1] 8.944
>[1] 9.098
>
>
>This shows that on an average,for 1000 such samples of 1000 Normal numbers, 
>the variance is higher than that of a 100 samples of 1000 random numbers.
>
>Why is this so?

Don't know, but it could be a fluke.  You don't say how many
times you did it.

I did the following, with 1000 in each test.  You have 100 in the
small test and 1000 in the big one.  My numbers look pretty
close.

> bigmat <- matrix(rnorm(1000000),1000,1000) # 1000 rows of 1000 each
> smallmat <- matrix(rnorm(100000),1000,100) # 1000 rows of 100 each
> mean(apply(bigmat,1,var)) # get variance of each row, then take mean
[1] 0.9999344
> mean(apply(smallmat,1,var))
[1] 0.9967427

-- 
Jonathan Baron, Professor of Psychology, University of Pennsylvania
Home page:            http://www.sas.upenn.edu/~baron
R page:               http://finzi.psych.upenn.edu/



From tplate at blackmesacapital.com  Tue Aug 19 20:11:36 2003
From: tplate at blackmesacapital.com (Tony Plate)
Date: Tue, 19 Aug 2003 12:11:36 -0600
Subject: [R] Variance Computing-  - HELP!!!!!!!!!!!!!!!!!!
In-Reply-To: <200308191742.h7JHgXTU065342@smtp-relay.tamu.edu>
Message-ID: <5.2.1.1.2.20030819120840.0415abc0@mailhost.blackmesacapital.com>

Perhaps you were trying for "as sample size increases, variance *of the 
mean* decreases" (a least when variance is finite).  If you swap "mean" and 
"var" in your code, I think you will get what you are looking for.

-- Tony Plate

At Tuesday 05:42 PM 8/19/2003 +0000, Padmanabhan, Sudharsha wrote:

>Hello,
>
>I am running a few simulations for clinical trial anlysis. I want some help
>regarding the following.
>
>We know trhat as the sample size increases, the variance should decrease, but
>I am getting some unexpected results. SO I ran a code (shown below) to check
>the validity of this.
>
>large<-array(1,c(1000,1000))
>small<-array(1,c(100,1000))
>for(i in 1:1000){large[i,]<-rnorm(1000,0,3)}
>for(i in 1:1000){small[i,]<-rnorm(100,0,3)}}
>yy<-array(1,100)
>for(i in 1:100){yy[i]<-var(small[i,])}
>y1y<-array(1,1000)
>for(i in 1:1000){y1y[i]<-var(large[i,])}
>mean(yy);mean(y1y);
>[1] 8.944
>[1] 9.098
>
>
>This shows that on an average,for 1000 such samples of 1000 Normal numbers,
>the variance is higher than that of a 100 samples of 1000 random numbers.
>
>Why is this so?
>
>
>Can someone please help me out????
>
>Thanks.
>
>Regards
>
>~S.
>
>______________________________________________
>R-help at stat.math.ethz.ch mailing list
>https://www.stat.math.ethz.ch/mailman/listinfo/r-help



From andy_liaw at merck.com  Tue Aug 19 20:15:07 2003
From: andy_liaw at merck.com (Liaw, Andy)
Date: Tue, 19 Aug 2003 14:15:07 -0400
Subject: [R] Variance Computing-  - HELP!!!!!!!!!!!!!!!!!!
Message-ID: <3A822319EB35174CA3714066D590DCD50205C9F3@usrymx25.merck.com>

First of all, your subscripting is wrong.  The first index is for row, and
the second for column.  Thus large[i,] refers to the i-th row of large,
rather than the i-th column.  Also, the code as you provided contain syntax
error.

Try:

set.seed(311)  ## Always a good idea to set seed for simulation!
large <- matrix(rnorm(1000*1000), 1000, 1000)
small <- matrix(rnorm(100*1000), 100, 1000)
var.large <- apply(large, 2, var)  ## Apply the var function to each column
var.small <- apply(small, 2, var)

The result looks like:
> summary(var.large); summary(var.small)
   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. 
 0.8617  0.9705  1.0010  1.0020  1.0320  1.1520 
   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. 
 0.5846  0.9021  0.9948  0.9990  1.0850  1.5360 

as expected:  The mean is about the same, but the spread is much smaller for
larger sample size.

This sort of things can be computed exactly using basic math stat, BTW.

Andy


> -----Original Message-----
> From: Padmanabhan, Sudharsha [mailto:sudAR_80 at neo.tamu.edu] 
> Sent: Tuesday, August 19, 2003 1:43 PM
> To: r-help at stat.math.ethz.ch
> Subject: [R] Variance Computing- - HELP!!!!!!!!!!!!!!!!!!
> 
> 
> 
> Hello,
> 
> I am running a few simulations for clinical trial anlysis. I 
> want some help 
> regarding the following.
> 
> We know trhat as the sample size increases, the variance 
> should decrease, but 
> I am getting some unexpected results. SO I ran a code (shown 
> below) to check 
> the validity of this.
> 
> large<-array(1,c(1000,1000))
> small<-array(1,c(100,1000))
> for(i in 1:1000){large[i,]<-rnorm(1000,0,3)}
> for(i in 1:1000){small[i,]<-rnorm(100,0,3)}}
> yy<-array(1,100)
> for(i in 1:100){yy[i]<-var(small[i,])}
> y1y<-array(1,1000)
> for(i in 1:1000){y1y[i]<-var(large[i,])}
> mean(yy);mean(y1y);
> [1] 8.944
> [1] 9.098
> 
> 
> This shows that on an average,for 1000 such samples of 1000 
> Normal numbers, 
> the variance is higher than that of a 100 samples of 1000 
> random numbers.
> 
> Why is this so?
> 
> 
> Can someone please help me out????
> 
> Thanks.
> 
> Regards
> 
> ~S.
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list 
> https://www.stat.math.ethz.ch/mailman/listinfo> /r-help
> 

------------------------------------------------------------------------------
Notice:  This e-mail message, together with any attachments, contains
information of Merck & Co., Inc. (Whitehouse Station, New Jersey, USA), and/or
its affiliates (which may be known outside the United States as Merck Frosst,
Merck Sharp & Dohme or MSD) that may be confidential, proprietary copyrighted
and/or legally privileged, and is intended solely for the use of the
individual or entity named on this message.  If you are not the intended
recipient, and have received this message in error, please immediately return
this by e-mail and then delete it.



From jmacdon at med.umich.edu  Tue Aug 19 20:17:25 2003
From: jmacdon at med.umich.edu (James MacDonald)
Date: Tue, 19 Aug 2003 14:17:25 -0400
Subject: [R] Variance Computing-  - HELP!!!!!!!!!!!!!!!!!!
Message-ID: <sf423181.026@mail-01.med.umich.edu>

I think you are confused. As sample size increases, the variance of an
estimate based on that sample will decrease asymtotically to zero (e.g.,
the standard error of the mean will go to zero). However the variance of
the sample itself will not change. Any difference you see in your data
is simply due to chance. If you repeat, the larger set may or may not
have a larger variance.

> var(rnorm(10000, 0, 3))
[1] 8.958727
> var(rnorm(10000, 0, 3))
[1] 9.155332
> var(rnorm(10000, 0, 3))
[1] 9.050894
> var(rnorm(10000, 0, 3))
[1] 9.282509
> var(rnorm(100000, 0, 3))
[1] 8.990778
> var(rnorm(100000, 0, 3))
[1] 9.024343
> var(rnorm(100000, 0, 3))
[1] 8.999064
> 
> var(rnorm(100000, 0, 3))
[1] 9.088034


HTH

Jim



James W. MacDonald
Affymetrix and cDNA Microarray Core
University of Michigan Cancer Center
1500 E. Medical Center Drive
7410 CCGC
Ann Arbor MI 48109
734-647-5623

>>> "Padmanabhan, Sudharsha" <sudAR_80 at neo.tamu.edu> 08/19/03 01:42PM
>>>

Hello,

I am running a few simulations for clinical trial anlysis. I want some
help 
regarding the following.

We know trhat as the sample size increases, the variance should
decrease, but 
I am getting some unexpected results. SO I ran a code (shown below) to
check 
the validity of this.

large<-array(1,c(1000,1000))
small<-array(1,c(100,1000))
for(i in 1:1000){large[i,]<-rnorm(1000,0,3)}
for(i in 1:1000){small[i,]<-rnorm(100,0,3)}}
yy<-array(1,100)
for(i in 1:100){yy[i]<-var(small[i,])}
y1y<-array(1,1000)
for(i in 1:1000){y1y[i]<-var(large[i,])}
mean(yy);mean(y1y);
[1] 8.944
[1] 9.098


This shows that on an average,for 1000 such samples of 1000 Normal
numbers, 
the variance is higher than that of a 100 samples of 1000 random
numbers.

Why is this so?


Can someone please help me out????

Thanks.

Regards

~S.

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://www.stat.math.ethz.ch/mailman/listinfo/r-help



From matthew_wiener at merck.com  Tue Aug 19 20:22:22 2003
From: matthew_wiener at merck.com (Wiener, Matthew)
Date: Tue, 19 Aug 2003 14:22:22 -0400
Subject: [R] Variance Computing-  - HELP!!!!!!!!!!!!!!!!!!
Message-ID: <AEBD81486231A343B1813FE62D3352250369A1E7@usrymx15.merck.com>

Hi.

There is no reason the variance of a normal should decrease as you take
larger samples.  Indeed, in your call itself, you say that you want a sample
from a normal with a standard deviation of 3, and so a variance of 9.  As
expected, both of your estimates of variance are close to 9.

What should decrease is the variance of the estimate of the mean, which is
the variance of the sample divided by the number of elements in your sample.
That will indeed decrease as n increases.
 
Also, a couple of R programming points raised by your example:

You can populate your entire matrix of random numbers with a single call,
with good time savings.  (That probably doesn't matter much in this toy
example, but might if you do larger simulations for some problem.)
For example:  matrix(rnorm(100000, 0, 3), nr = 100, nc = 1000) gets you your
matrix "small".

Similarly, your loop over the rows for taking variance can be replaced by 
	yy <- apply(small, 1, var)
Which may not be faster, but is certainly easier to read.  And of course
you'd want to replace the call to var with a function that calculates
standard error.

Hope this helps,

Matt Wiener


-----Original Message-----
From: Padmanabhan, Sudharsha [mailto:sudAR_80 at neo.tamu.edu] 
Sent: Tuesday, August 19, 2003 1:43 PM
To: r-help at stat.math.ethz.ch
Subject: [R] Variance Computing- - HELP!!!!!!!!!!!!!!!!!!



Hello,

I am running a few simulations for clinical trial anlysis. I want some help 
regarding the following.

We know trhat as the sample size increases, the variance should decrease,
but 
I am getting some unexpected results. SO I ran a code (shown below) to check

the validity of this.

large<-array(1,c(1000,1000))
small<-array(1,c(100,1000))
for(i in 1:1000){large[i,]<-rnorm(1000,0,3)}
for(i in 1:1000){small[i,]<-rnorm(100,0,3)}}
yy<-array(1,100)
for(i in 1:100){yy[i]<-var(small[i,])}
y1y<-array(1,1000)
for(i in 1:1000){y1y[i]<-var(large[i,])}
mean(yy);mean(y1y);
[1] 8.944
[1] 9.098


This shows that on an average,for 1000 such samples of 1000 Normal numbers, 
the variance is higher than that of a 100 samples of 1000 random numbers.

Why is this so?


Can someone please help me out????

Thanks.

Regards

~S.

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://www.stat.math.ethz.ch/mailman/listinfo/r-help

------------------------------------------------------------------------------
Notice:  This e-mail message, together with any attachments, contains
information of Merck & Co., Inc. (Whitehouse Station, New Jersey, USA), and/or
its affiliates (which may be known outside the United States as Merck Frosst,
Merck Sharp & Dohme or MSD) that may be confidential, proprietary copyrighted
and/or legally privileged, and is intended solely for the use of the
individual or entity named on this message.  If you are not the intended
recipient, and have received this message in error, please immediately return
this by e-mail and then delete it.



From rossini at blindglobe.net  Tue Aug 19 18:41:32 2003
From: rossini at blindglobe.net (A.J. Rossini)
Date: Tue, 19 Aug 2003 09:41:32 -0700
Subject: [R] for those of you who want a patent...
Message-ID: <85ekzhk3b7.fsf@blindglobe.net>


Alvaro Munoz (Hopkins Epi) is patenting the "diamond graph". 

http://www.jhsph.edu/Press_Room/Press_Releases/Munoz_diamond_graph.html

There is enough prior art (hexagonal binning, among others) to make
this amusing, except that it probably will get a patent.  It's a reasonable
graphical technique, but patentable?

best,
-tony

-- 
A.J. Rossini     			
rossini at u.washington.edu            http://www.analytics.washington.edu/ 
Biomedical and Health Informatics   University of Washington
Biostatistics, SCHARP/HVTN          Fred Hutchinson Cancer Research Center
UW   :              FAX=206-543-3461 | moving soon to a permanent office
FHCRC: 206-667-7025 FAX=206-667-4812 | Voicemail is pretty sketchy/use Email

CONFIDENTIALITY NOTICE: This e-mail message and any attachme...{{dropped}}



From yukiasais at ybb.ne.jp  Wed Aug 20 00:15:38 2003
From: yukiasais at ybb.ne.jp (yukihiro ishii)
Date: Wed, 20 Aug 2003 07:15:38 +0900
Subject: [R] On the Use of the nnet Library
Message-ID: <20030820071536.8543.YUKIASAIS@ybb.ne.jp>

Dear List,

I am trying to solve a problem by the neural network method(library:
nnet). The problem is to express Weight in terms of Age , Sex and Height
for twenty people. The data frame consists of 20 observations with four
variables: Sex, Age, Height and Weight. Sex is treated as a factor, Age
and Weight are variables normalized to unity, as usual. I wanted to
construct a neural network, and so I ran the following code:

>library(nnet)
>net1<-nnet(Weight~Age+Sex+Height, size=2, linout=T,maxit=1000)

I repeated this thirteen times.  I used the default initial parameters
unless otherwise noted. The result is as follows, where init and final
mean initial and final RSS's, and NIT means the number of iterations
before reaching convergence or noncovergence:

Run#	init		NIT	final
1	71991.1 	30	995.1 
2	70870.0 	370	33.1 
3	72755.8 	<10	2134.3 
4	69840.6 	<10	2134.3 
5	70368.8 	190	39.7 
6	70368.8 	270	41.0 
7	71101.2 	190	39.7 
8	71606.1 	<10	2134.3 
9	72076.1 	<10	2134.3 
10	72249.1 	300	15.0 
11	71424.1 	<10	2134.3 
12	68483.8 	130	39.7 
13	71435.9 	>1000	4.6 


As you can see, the result is far from stable.

My question is:

How can I reach a stable answer?

.I know that initial parameters are crucially important in my case, and I
must choose proper parameter values, but I do not know I can do that.

My second question is related to the response analysis of this data. I
do not know an effective method to evaluate the response to
 the variance of each explanatory variable. Is there such a function in
the library, nnet? Such a function may help me reduce the number of the
explanatory variables.

I wonder if anyone could help me in such elementary questions.


---- It's elementary, Watson!

I remain an obedient Watson, hoping for Holmes' wisdom.


-- 
Yukihiro Ishii <yukiasais at ybb.ne.jp>
2-3-28?Tsurumaki-minami, Hadano
257-0002 Japan
Tel +81 463 69 1922
Fax +81 463 69 1922



From ross at biostat.ucsf.edu  Tue Aug 19 23:57:16 2003
From: ross at biostat.ucsf.edu (Ross Boylan)
Date: Tue, 19 Aug 2003 14:57:16 -0700
Subject: [R] logistic regression without intercept
Message-ID: <1061330236.4787.97.camel@iron.libaux.ucsf.edu>

I want to do a logistic regression without an intercept term.  This
option is absent from glm, though present in some of the inner functions
glm uses.  I gather glm is the standard way to do logistic regression in
R.

Hoping it would be passed in, I said
> r <- glm(brain.cancer~epilepsy+other.cancer, c3, 
>        family=binomial(link="logit"), intercept=FALSE)
which produced
Error in glm.control(...) : unused argument(s) (intercept ...)

Is there an easy way to do this?  I suppose I could start hacking away
at glm so it would take the argument and pass it on, but is it absent
for a reason?

Also, I noticed that S-Plus but not R has a glim routine that uses
maximum likelihood.  What would be the equivalent?

Thanks.



From sotl155360 at earthlink.net  Wed Aug 20 00:29:10 2003
From: sotl155360 at earthlink.net (Frank Roberts - SOTL)
Date: Tue, 19 Aug 2003 18:29:10 -0400
Subject: [R] Variance Computing- 
In-Reply-To: <200308191742.h7JHgXTU065342@smtp-relay.tamu.edu>
References: <200308191742.h7JHgXTU065342@smtp-relay.tamu.edu>
Message-ID: <200308191829.10939.sotl155360@earthlink.net>

Hi All:

Many servers, routers, and firewall have the configuration files set such that 
if the word HELP appears in the subject line the message is not delivered to 
the addressee but is delivered to they system operator.

Re: [R] Variance Computing-  - HELP!!!!!!!!!!!!!!!!!!

Thanks
Frank



From tlumley at u.washington.edu  Wed Aug 20 00:32:41 2003
From: tlumley at u.washington.edu (Thomas Lumley)
Date: Tue, 19 Aug 2003 15:32:41 -0700 (PDT)
Subject: [R] logistic regression without intercept
In-Reply-To: <1061330236.4787.97.camel@iron.libaux.ucsf.edu>
Message-ID: <Pine.A41.4.44.0308191530260.68442-100000@homer25.u.washington.edu>

On Tue, 19 Aug 2003, Ross Boylan wrote:

> I want to do a logistic regression without an intercept term.  This
> option is absent from glm, though present in some of the inner functions
> glm uses.  I gather glm is the standard way to do logistic regression in
> R.
>
> Hoping it would be passed in, I said
> > r <- glm(brain.cancer~epilepsy+other.cancer, c3,
> >        family=binomial(link="logit"), intercept=FALSE)
> which produced
> Error in glm.control(...) : unused argument(s) (intercept ...)
>
> Is there an easy way to do this?  I suppose I could start hacking away
> at glm so it would take the argument and pass it on, but is it absent
> for a reason?

Yes. You specify no intercept  with the formula:
r <- glm(brain.cancer~epilepsy+other.cancer+0, c3,
        family=binomial(link="logit"), intercept=FALSE)
or
r <- glm(brain.cancer~epilepsy+other.cancer-1, c3,
        family=binomial(link="logit"), intercept=FALSE)

The latter is S-PLUS compatible

> Also, I noticed that S-Plus but not R has a glim routine that uses
> maximum likelihood.  What would be the equivalent?

glm.

	-thomas



From spencer.graves at pdf.com  Wed Aug 20 00:39:33 2003
From: spencer.graves at pdf.com (Spencer Graves)
Date: Tue, 19 Aug 2003 15:39:33 -0700
Subject: [R] logistic regression without intercept
References: <1061330236.4787.97.camel@iron.libaux.ucsf.edu>
Message-ID: <3F42A725.7000204@pdf.com>

Did you try the following:

 >>r <- glm(brain.cancer~epilepsy+other.cancer-1, c3,
 >>       family=binomial(link="logit") )

The construct "-1" on the right hand side of a formula means to exclude 
the intercept.  See, e.g., "model formulae" in the index to Modern 
Applied Statistics with S by Venables & Ripley.  I don't remember doing 
this with glm, but I've done it with lm.

hope this helps.  spencer graves

Ross Boylan wrote:
> I want to do a logistic regression without an intercept term.  This
> option is absent from glm, though present in some of the inner functions
> glm uses.  I gather glm is the standard way to do logistic regression in
> R.
> 
> Hoping it would be passed in, I said
> 
>>r <- glm(brain.cancer~epilepsy+other.cancer, c3, 
>>       family=binomial(link="logit"), intercept=FALSE)
> 
> which produced
> Error in glm.control(...) : unused argument(s) (intercept ...)
> 
> Is there an easy way to do this?  I suppose I could start hacking away
> at glm so it would take the argument and pass it on, but is it absent
> for a reason?
> 
> Also, I noticed that S-Plus but not R has a glim routine that uses
> maximum likelihood.  What would be the equivalent?
> 
> Thanks.
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help



From rossini at blindglobe.net  Wed Aug 20 00:42:51 2003
From: rossini at blindglobe.net (A.J. Rossini)
Date: Tue, 19 Aug 2003 15:42:51 -0700
Subject: [R] Variance Computing-
In-Reply-To: <200308191829.10939.sotl155360@earthlink.net> (Frank Roberts's
	message of "Tue, 19 Aug 2003 18:29:10 -0400")
References: <200308191742.h7JHgXTU065342@smtp-relay.tamu.edu>
	<200308191829.10939.sotl155360@earthlink.net>
Message-ID: <85brul46c4.fsf@blindglobe.net>

Frank Roberts - SOTL <sotl155360 at earthlink.net> writes:

> Many servers, routers, and firewall have the configuration files set such that 
> if the word HELP appears in the subject line the message is not delivered to 
> the addressee but is delivered to they system operator.

Gosh, lets hope that those systems can learn from their mistakes,
then.

best,
-tony

-- 
A.J. Rossini     			
rossini at u.washington.edu            http://www.analytics.washington.edu/ 
Biomedical and Health Informatics   University of Washington
Biostatistics, SCHARP/HVTN          Fred Hutchinson Cancer Research Center
UW   :              FAX=206-543-3461 | moving soon to a permanent office
FHCRC: 206-667-7025 FAX=206-667-4812 | Voicemail is pretty sketchy/use Email

CONFIDENTIALITY NOTICE: This e-mail message and any attachme...{{dropped}}



From ross at biostat.ucsf.edu  Wed Aug 20 00:47:57 2003
From: ross at biostat.ucsf.edu (Ross Boylan)
Date: Tue, 19 Aug 2003 15:47:57 -0700
Subject: [R] logistic regression without intercept
In-Reply-To: <Pine.A41.4.44.0308191530260.68442-100000@homer25.u.washington.edu>
References: <Pine.A41.4.44.0308191530260.68442-100000@homer25.u.washington.edu>
Message-ID: <1061333277.4787.104.camel@iron.libaux.ucsf.edu>

Thanks to Thomas Lumley, Spencer Graves, and Steve Sullivan for their
advice on and off list to modify the formula with +0 (which I did; it
worked) or -1 at the end.

A couple of items of clarification:

, On Tue, 2003-08-19 at 15:32, Thomas Lumley wrote:
> On Tue, 19 Aug 2003, Ross Boylan wrote:
> 
> > I want to do a logistic regression without an intercept term.  This
> > option is absent from glm, though present in some of the inner functions
> > glm uses.  I gather glm is the standard way to do logistic regression in
> > R.
> >
> > Hoping it would be passed in, I said
> > > r <- glm(brain.cancer~epilepsy+other.cancer, c3,
> > >        family=binomial(link="logit"), intercept=FALSE)
> > which produced
> > Error in glm.control(...) : unused argument(s) (intercept ...)
> >
> > Is there an easy way to do this?  I suppose I could start hacking away
> > at glm so it would take the argument and pass it on, but is it absent
> > for a reason?
> 
> Yes. You specify no intercept  with the formula:
> r <- glm(brain.cancer~epilepsy+other.cancer+0, c3,
>         family=binomial(link="logit"), intercept=FALSE)
> or
> r <- glm(brain.cancer~epilepsy+other.cancer-1, c3,
>         family=binomial(link="logit"), intercept=FALSE)
> 
> The latter is S-PLUS compatible

Omit the intercept=FALSE in the above lines; it causes an error even
with the augmented model spec.
> 
> > Also, I noticed that S-Plus but not R has a glim routine that uses
> > maximum likelihood.  What would be the equivalent?
> 
> glm.
> 
I thought glm was a minimize the deviations approach, which is different
from maximize likelihood.

> 	-thomas
> 
> 
--



From sotl155360 at earthlink.net  Wed Aug 20 01:32:50 2003
From: sotl155360 at earthlink.net (Frank Roberts - SOTL)
Date: Tue, 19 Aug 2003 19:32:50 -0400
Subject: [R] Variance Computing-
In-Reply-To: <85brul46c4.fsf@blindglobe.net>
References: <200308191742.h7JHgXTU065342@smtp-relay.tamu.edu>
	<200308191829.10939.sotl155360@earthlink.net>
	<85brul46c4.fsf@blindglobe.net>
Message-ID: <200308191932.50100.sotl155360@earthlink.net>

It is not a mistake. 

The word help on a subject line means that you want help with the program not 
that you are requesting help in a message.

Since almost all system servers, routers, and firewalls run either BSD or 
Linux that is the action you most likely will get at most businesses.

Frank

On Tuesday 19 August 2003 18:42, A.J. Rossini wrote:
> Frank Roberts - SOTL <sotl155360 at earthlink.net> writes:
> > Many servers, routers, and firewall have the configuration files set such
> > that if the word HELP appears in the subject line the message is not
> > delivered to the addressee but is delivered to they system operator.
>
> Gosh, lets hope that those systems can learn from their mistakes,
> then.
>
> best,
> -tony



From rossini at blindglobe.net  Wed Aug 20 01:39:03 2003
From: rossini at blindglobe.net (A.J. Rossini)
Date: Tue, 19 Aug 2003 16:39:03 -0700
Subject: [R] Variance Computing-
In-Reply-To: <200308191932.50100.sotl155360@earthlink.net> (Frank Roberts's
	message of "Tue, 19 Aug 2003 19:32:50 -0400")
References: <200308191742.h7JHgXTU065342@smtp-relay.tamu.edu>
	<200308191829.10939.sotl155360@earthlink.net>
	<85brul46c4.fsf@blindglobe.net>
	<200308191932.50100.sotl155360@earthlink.net>
Message-ID: <85ada52p60.fsf@blindglobe.net>

Frank Roberts - SOTL <sotl155360 at earthlink.net> writes:

> It is not a mistake. 

It is a mistake.

> The word help on a subject line means that you want help with the program not 
> that you are requesting help in a message.

This "comprehension" is the mistake.  

> Since almost all system servers, routers, and firewalls run either BSD or 
> Linux that is the action you most likely will get at most businesses.

Most routers don't filter SMTP.  Most firewalls don't sort SMTP
traffic by subject.

and the point of this mailing list is...?  "r-help" ?

-- 
A.J. Rossini     			
rossini at u.washington.edu            http://www.analytics.washington.edu/ 
Biomedical and Health Informatics   University of Washington
Biostatistics, SCHARP/HVTN          Fred Hutchinson Cancer Research Center
UW   :              FAX=206-543-3461 | moving soon to a permanent office
FHCRC: 206-667-7025 FAX=206-667-4812 | Voicemail is pretty sketchy/use Email

CONFIDENTIALITY NOTICE: This e-mail message and any attachme...{{dropped}}



From jc at or.psychology.dal.ca  Wed Aug 20 01:53:40 2003
From: jc at or.psychology.dal.ca (John Christie)
Date: Tue, 19 Aug 2003 20:53:40 -0300
Subject: [R] compare two linear regression slopes
Message-ID: <5BA442DA-D2A0-11D7-AE7A-000A9566473A@or.psychology.dal.ca>

Hi,
	I'm sort of new to regression, but I was wondering how one compares 
two regression slopes in R.  Is it just a matter of calculating the 
slopes and then using the SD reported by R to test for a difference.  I 
can't imagine one can assume normal distributions in this case.  :)



From jc at or.psychology.dal.ca  Wed Aug 20 02:25:08 2003
From: jc at or.psychology.dal.ca (John Christie)
Date: Tue, 19 Aug 2003 21:25:08 -0300
Subject: [R] compare two linear regression slopes
In-Reply-To: <7A3A13F416B40842BD2C1753E044B359B13405@CASEVS02.cas.anu.edu.au>
Message-ID: <C13A3E0A-D2A4-11D7-AE7A-000A9566473A@or.psychology.dal.ca>


On Tuesday, August 19, 2003, at 09:20  PM, Simon Blomberg wrote:

> You need to look at Analysis of Covariance in any basic stats book. 
> Basically, if you have y versus x for 2 groups ("group" is a factor), 
> use lm to fit the model:
>
> fit <- lm(y ~ group + x + x:group, data=your.dat)

but, my situation is two separate regressions with different collected 
Y's and a predicted X.



From sotl155360 at earthlink.net  Wed Aug 20 03:34:55 2003
From: sotl155360 at earthlink.net (Frank Roberts - SOTL)
Date: Tue, 19 Aug 2003 21:34:55 -0400
Subject: [R] Variance Computing-
In-Reply-To: <85ada52p60.fsf@blindglobe.net>
References: <200308191742.h7JHgXTU065342@smtp-relay.tamu.edu>
	<200308191932.50100.sotl155360@earthlink.net>
	<85ada52p60.fsf@blindglobe.net>
Message-ID: <200308192134.55956.sotl155360@earthlink.net>

You have your opinion - good.

The routers and firewall on most systems are set up another way.

This give you an option. Comply with standard usage or not conply.

Personally I do not care. It does not matter to me.

I only sent the first posting as a means of healping out people who appear to 
need help.

By your message you have indicates you do not want such help.

So be it.

Good by.


On Tuesday 19 August 2003 19:39, A.J. Rossini wrote:
> Frank Roberts - SOTL <sotl155360 at earthlink.net> writes:
> > It is not a mistake.
>
> It is a mistake.
>
> > The word help on a subject line means that you want help with the program
> > not that you are requesting help in a message.
>
> This "comprehension" is the mistake.
>
> > Since almost all system servers, routers, and firewalls run either BSD or
> > Linux that is the action you most likely will get at most businesses.
>
> Most routers don't filter SMTP.  Most firewalls don't sort SMTP
> traffic by subject.
>
> and the point of this mailing list is...?  "r-help" ?



From sumslily at yahoo.com  Wed Aug 20 03:40:01 2003
From: sumslily at yahoo.com (Lily)
Date: Tue, 19 Aug 2003 18:40:01 -0700 (PDT)
Subject: [R] question for loop on matrix row level.  
Message-ID: <20030820014001.73515.qmail@web41201.mail.yahoo.com>

For the 1000 simulations, a matrix will be generated
each time. And, I need to choose the first row of the
generated matrix. The following loop doesn't work
though:

for (i in 1:1000) {

aval[i]<- matrixname[1,]

}

Any solution? thanks!



From spencer.graves at pdf.com  Wed Aug 20 04:05:27 2003
From: spencer.graves at pdf.com (Spencer Graves)
Date: Tue, 19 Aug 2003 19:05:27 -0700
Subject: [R] question for loop on matrix row level.
References: <20030820014001.73515.qmail@web41201.mail.yahoo.com>
Message-ID: <3F42D767.2040407@pdf.com>

Have you considered making "aval" an array with the same number of 
columns as "matrixname"?  Try the following:

aval <- array(NA, dim=c(2,2))
for(i in 1:2){
	matrixname <- array(i+(1:4), dim=c(2,2))
	aval[i,] <- matrixname[1,]
}
aval

hope this helps.  spencer graves

Lily wrote:
> For the 1000 simulations, a matrix will be generated
> each time. And, I need to choose the first row of the
> generated matrix. The following loop doesn't work
> though:
> 
> for (i in 1:1000) {
> 
> aval[i]<- matrixname[1,]
> 
> }
> 
> Any solution? thanks!
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help



From ok at cs.otago.ac.nz  Wed Aug 20 04:32:26 2003
From: ok at cs.otago.ac.nz (Richard A. O'Keefe)
Date: Wed, 20 Aug 2003 14:32:26 +1200 (NZST)
Subject: [R] Variance Computing-  - HELP!!!!!!!!!!!!!!!!!!
Message-ID: <200308200232.h7K2WQBD097805@atlas.otago.ac.nz>

"Padmanabhan, Sudharsha" <sudAR_80 at neo.tamu.edu>
	We know trhat as the sample size increases, the variance should
	decrease,

Should it?

I can paraphrase his test case thus:

    v100 <- sapply(1:100, function(i) var(rnorm(100, 0, 3)))
	# We expect the elements of v100 to cluster around 3^2
    v1000 <- sapply(1:1000, function(i) var(rnorm(1000, 0, 3)))
	# We expect the elements of v1000 to cluster around 3^2 too.
    fivenum(v100)
=>  [1]  6.469134  7.884637  8.916314 10.189463 13.897817
	#                    ^^^^^^^^
    fivenum(v1000)
=>  [1]  7.874345  8.692326  8.967684  9.268955 10.503038
	#		     ^^^^^^^^

The population parameter sigma-squared is 3^2 = 9.
The estimates are 8.92 in one case and 8.97 in the other;
sounds about right to me.

Looking at density(v100) and density(v1000) is enlightening.

Means and standard deviations:

    mean(v100)		var(v100)
=>  9.080676		2.376193
    mean(v1000)		var(v1000)
=>  8.98147		0.1721246

Are these not pretty much as expected?  Not that a t-test is the
ideal test for the distributions involved, but it's familiar and
since the distribution is pretty bell-shaped, it may be usable as
a rough guide to whether to be worried or not.

> t.test(v100, v1000)

        Welch Two Sample t-test

data:  v100 and v1000 
t = 0.6413, df = 100.439, p-value = 0.5228
alternative hypothesis: true difference in means is not equal to 0 
95 percent confidence interval:
 -0.2077100  0.4061231 
sample estimates:
mean of x mean of y 
 9.080676  8.981469



From kjetil at entelnet.bo  Wed Aug 20 04:49:35 2003
From: kjetil at entelnet.bo (kjetil brinchmann halvorsen)
Date: Tue, 19 Aug 2003 22:49:35 -0400
Subject: [R] test for equal distributions with small numbes of observations
In-Reply-To: <1061280952.1203.29.camel@christophl>
Message-ID: <3F42A97F.27518.1896886@localhost>

On 19 Aug 2003 at 10:15, Christoph Lehmann wrote:

> Dear R-pros
> 
> I have a problem, for which usually I would apply a chisq.test or a
> fisher.test:
> 

try 
?chisq.test

which tells you you can say 
chisq.test(..., sim=TRUE, B=20000)

and the simulation will be very fast!

Kjetil Halvorsen

> 
> 
> 40 objects, each given either a "0" or "1", regarding if this object
> later on will be remembered by a subject or not.
> 
> 7 subjects investigated
> 
> means: we have a 2x40 matrix, each cell the number of subjects for who
> the object i has been given either "0" or "1" e.g.
> 
> objects:
> 	1	1	3		39	40
> 	------------------------------------------
> "0"	1	2	2	..	7	7
> "1"	4	4	5	..	0	0
> 
> over all 40 objects, we have 67% of "1" and 33% of "0"
> 
> I want to know, if for the 40 objects, the ratio of "0"/"1" differs or
> not, i.e. if they have the same distribution.
> 
> I cannot use a chisq.test since the expected frequencies are < 5 for the
> "0" cells.
> 
> Fisher.test seems to run for > 12h on a PIV 1.8GHz...
> 
> what do you recommend me to do?
> 
> Many thanks
> 
> Christoph
> --
> > recognition
>      [,1] [,2] [,3] [,4] [,5] [,6] [,7] [,8] [,9] [,10] [,11] [,12] [,13] [,14]
> [1,]    1    2    2    2    2    3    3    3    3     3     3     3     4     4
> [2,]    4    4    5    4    4    3    4    4    4     4     3     4     3     3
>      [,15] [,16] [,17] [,18] [,19] [,20] [,21] [,22] [,23] [,24] [,25] [,26]
> [1,]     4     4     4     4     4     4     5     5     5     5     5     5
> [2,]     3     2     3     3     2     3     2     2     2     2     2     2
>      [,27] [,28] [,29] [,30] [,31] [,32] [,33] [,34] [,35] [,36] [,37] [,38]
> [1,]     5     5     5     5     5     6     6     6     6     6     6     6
> [2,]     2     2     2     2     2     1     1     1     1     1     1     1
>      [,39] [,40]
> [1,]     7     7
> [2,]     0     0
> 
> > fisher.test(recognition)
> 
> 
> -- 
> Christoph Lehmann <christoph.lehmann at gmx.ch>
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help



From p.connolly at hortresearch.co.nz  Wed Aug 20 06:34:16 2003
From: p.connolly at hortresearch.co.nz (Patrick Connolly)
Date: Wed, 20 Aug 2003 16:34:16 +1200
Subject: [R] for those of you who want a patent...
In-Reply-To: <85ekzhk3b7.fsf@blindglobe.net>
References: <85ekzhk3b7.fsf@blindglobe.net>
Message-ID: <20030820043416.GL11971@hortresearch.co.nz>

On Tue, 19-Aug-2003 at 09:41AM -0700, A.J. Rossini wrote:

|> 
|> Alvaro Munoz (Hopkins Epi) is patenting the "diamond graph". 
|> 
|> http://www.jhsph.edu/Press_Room/Press_Releases/Munoz_diamond_graph.html
|> 

|> There is enough prior art (hexagonal binning, among others) to make
|> this amusing, except that it probably will get a patent.  It's a
|> reasonable graphical technique, but patentable?

Maybe so.  I read a story once that the idea of using different
colours to annotate a word processor document is patented.  This one
sounds at least as original, though not as original as a sunflower
plot.

Reckon I'll stick to contour plots rather than pay a licensing fee for
diamond graphs.  

[thinks.... maybe the way I use contour plots is unique ... maybe I
should bung in a patent application]


-- 
Patrick Connolly
HortResearch
Mt Albert
Auckland
New Zealand 
Ph: +64-9 815 4200 x 7188
~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~
I have the world`s largest collection of seashells. I keep it on all
the beaches of the world ... Perhaps you`ve seen it.  ---Steven Wright 
~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~



From dvumani at hotmail.com  Wed Aug 20 09:12:49 2003
From: dvumani at hotmail.com (Vumani Dlamini)
Date: Wed, 20 Aug 2003 07:12:49 +0000
Subject: [R] (no subject)
Message-ID: <Law11-F37zyXze5icLZ00066eb5@hotmail.com>

Dear R users,

I am trying to convert a dataset from one format to several rectangular 
datasets. A consultant helped design the data entry program for our survey 
using Delphi/Pascal and for each household the information is stored in a 
file called "EA-HM-HH.TXT" where EA is the enumeration area number, HM is 
the homestead number and HH is the household number. Within this file the 
data is stored as follows,


###### file="5677-001-001.TXT" ######
EAnumber=5677
HMnumber=001
HHnumber=001
# Demographics section
Dserial=01   #first person in household
Dage=56
Dsex=1
Dserial=02   #second person in household
Dage=44
Dsex=2
Dserial=03   #second person in household
Dage=7
Dsex=2
# Agricultural inputs section
Amaize=200
Apumpkins=50
###### end of file ########

Note that in the demograpics section there may only be less than 3 or more 
people in some households. I would like to create a file for the 
demographics section which is as follows

EAnumber|HMnumber|HHnumber|Dserial|Dage|Dsex
5677 001 001 056 1
5677 001 001 044 2
5677 001 001 007 2

and for the agricultural inputs I would like to have,

EAnumber|HMnumber|HHnumber|Amaize|Apumpkins
5677 001 001 0200
5677 001 001 0050

There are several similar files where the EA number, HM number or HH number 
changes, thus I would also like to know whether it is possible to create a 
script where for all household there is only one dataset for demographics, 
and one for agricultural inputs.

Thanking you as always.


Vumani Dlamini



From dvumani at hotmail.com  Wed Aug 20 09:13:54 2003
From: dvumani at hotmail.com (Vumani Dlamini)
Date: Wed, 20 Aug 2003 07:13:54 +0000
Subject: [R] query on converting survey data from one structure to another
Message-ID: <Law11-F30fzLARv3taV0006718b@hotmail.com>

Dear R users,

I am trying to convert a dataset from one format to several rectangular 
datasets. A consultant helped design the data entry program for our survey 
using Delphi/Pascal and for each household the information is stored in a 
file called "EA-HM-HH.TXT" where EA is the enumeration area number, HM is 
the homestead number and HH is the household number. Within this file the 
data is stored as follows,


###### file="5677-001-001.TXT" ######
EAnumber=5677
HMnumber=001
HHnumber=001
# Demographics section
Dserial=01   #first person in household
Dage=56
Dsex=1
Dserial=02   #second person in household
Dage=44
Dsex=2
Dserial=03   #second person in household
Dage=7
Dsex=2
# Agricultural inputs section
Amaize=200
Apumpkins=50
###### end of file ########

Note that in the demograpics section there may only be less than 3 or more 
people in some households. I would like to create a file for the 
demographics section which is as follows

EAnumber|HMnumber|HHnumber|Dserial|Dage|Dsex
5677 001 001 056 1
5677 001 001 044 2
5677 001 001 007 2

and for the agricultural inputs I would like to have,

EAnumber|HMnumber|HHnumber|Amaize|Apumpkins
5677 001 001 0200
5677 001 001 0050

There are several similar files where the EA number, HM number or HH number 
changes, thus I would also like to know whether it is possible to create a 
script where for all household there is only one dataset for demographics, 
and one for agricultural inputs.

Thanking you as always.


Vumani Dlamini



From simon.woodhead at bristol.ac.uk  Wed Aug 20 09:22:31 2003
From: simon.woodhead at bristol.ac.uk (Simon Woodhead)
Date: Wed, 20 Aug 2003 08:22:31 +0100
Subject: [R] Plots default to Rplots.ps
Message-ID: <3F4321B7.2040209@bristol.ac.uk>

Dear All,

Hello I'm a newbie to the list. I recently installed R on a Redhat 9.0 
system, when I come to plot anything it does not bring up a graphics 
window but rather stores it in a file "Rplots.ps". I tried x11() but 
that doesn't bring up a window either. I would greatly appreciate any 
advice.

Simon Woodhead



From kutinskyv at obninsk.com  Wed Aug 20 10:42:57 2003
From: kutinskyv at obninsk.com (Vladimir N. Kutinsky)
Date: Wed, 20 Aug 2003 12:42:57 +0400
Subject: [R] RandomForest
Message-ID: <KFENLPKGENECNKICCNMBGEBPCGAA.kutinskyv@obninsk.com>

Hello,

When I plot or look at the error rate vector for a random forest
(rf$err.rate) it looks like a descending function except for a few first
points of the vector with error rates values lower(sometimes much lower)
than the general level of error rates for a forest with such number of trees
when the error rates stop descending. Does it mean that there is a tree(s)
(that is built the first in the forest) that has a higher predictive
accuracy than the whole forest of trees?

One more minor question. Is there a way to "snip" the forest of 100 trees
to, say, 50 trees?

Thanks,
Vladimir



From John.Marsland at CommerzbankIB.com  Wed Aug 20 11:05:06 2003
From: John.Marsland at CommerzbankIB.com (Marsland, John)
Date: Wed, 20 Aug 2003 10:05:06 +0100
Subject: [R] How do you debug() an S4 generic function
Message-ID: <8CBAA121CEB4D5118CB200508BB2BBEF0317E923@xmx8lonib.lonib.commerzbank.com>

I am trying to use the debug() command on an S4 generic function

> debug(foo)
> foo(x,y)
debugging in: foo(x,y)
debug: standardGeneric("foo")
Browse[1]> n
exiting from: foo(x,y)

What seems to be happening is that debug is operating on the standard
generic not the function with the signature I want - in this case there is
only one signature.

Any suggestions?

John Marsland


********************************************************************** 
This is a commercial communication from Commerzbank AG.\ \ T...{{dropped}}



From plummer at iarc.fr  Wed Aug 20 11:31:15 2003
From: plummer at iarc.fr (Martyn Plummer)
Date: Wed, 20 Aug 2003 09:31:15 -0000
Subject: [R] Plots default to Rplots.ps
In-Reply-To: <3F4321B7.2040209@bristol.ac.uk>
References: <3F4321B7.2040209@bristol.ac.uk>
Message-ID: <1061371900.966.8.camel@xena>

On Wed, 2003-08-20 at 09:22, Simon Woodhead wrote:
> Dear All,
> 
> Hello I'm a newbie to the list. I recently installed R on a Redhat 9.0 
> system, when I come to plot anything it does not bring up a graphics 
> window but rather stores it in a file "Rplots.ps". I tried x11() but 
> that doesn't bring up a window either. I would greatly appreciate any 
> advice.

It looks like you have compiled R without support for X11, so the
default plotting device is postscript(). You need to install the
XFree86-devel package and recompile, or else use the ready made RPM
packages on CRAN:

http://cran.r-project.org/bin/linux/redhat/9/i386/

Martyn



From miriam.30 at gmx.de  Wed Aug 20 12:54:18 2003
From: miriam.30 at gmx.de (Miriam =?ISO-8859-1?Q?Drei=DFig?=)
Date: Wed, 20 Aug 2003 12:54:18 +0200 (MEST)
Subject: [R] Neural Networks in R
Message-ID: <8018.1061376858@www59.gmx.net>

Hello!
We are a group of three students at Bielefeld University currently working
on a statistical projects about neural networks. Within the framework of this
project we are supposed to use the nnet-function in R and explain how it
works. Since anyone of us has much experience in using R we hoped to find some
information on your homepage. Unfortunatelly, we haven't been very successfull
so far.
We were wondering if you happen to know any books or articles which deal
with neural networks in R or if you could tell us were we can find such
information.
We would highly appreciate if you could help us on that matter.
Thank you very much in advance.
Best regards,

Anne Bruns, Miriam Drei?ig, Sascha Hartung

-- 
Miriam Drei?ig
Voltmannstr. 140
33613 Bielefeld

0521/8949370
0177/6440124

COMPUTERBILD 15/03: Premium-e-mail-Dienste im Test\ --------...{{dropped}}



From Christine.Tuleau at math.u-psud.fr  Wed Aug 20 13:49:02 2003
From: Christine.Tuleau at math.u-psud.fr (Christine Tuleau)
Date: Wed, 20 Aug 2003 12:49:02 +0100 (WET DST)
Subject: [R] aide
Message-ID: <200308201149.MAA03573@stats.math.u-psud.fr>


Bonjour,


J'aimerais savoir si quelqu'un pourrait m'aider en ce qui concerne l'utilisation 
du package waveslim.
Je voudrais savoir comment on fait pour revenir a un signal d'origine lorsque 
l'on dispose de coefficients de detail et d'approximation.

Merci d'avance
Tuleau Christine

(christine.tuleau at math.u-psud.fr)



From baron at psych.upenn.edu  Wed Aug 20 13:03:31 2003
From: baron at psych.upenn.edu (Jonathan Baron)
Date: Wed, 20 Aug 2003 07:03:31 -0400
Subject: [R] query on converting survey data from one structure to another
In-Reply-To: <Law11-F30fzLARv3taV0006718b@hotmail.com>
References: <Law11-F30fzLARv3taV0006718b@hotmail.com>
Message-ID: <20030820110331.GA20190@mail1.sas.upenn.edu>

On 08/20/03 07:13, Vumani Dlamini wrote:
>Dear R users,
>
>I am trying to convert a dataset from one format to several rectangular 
>datasets. A consultant helped design the data entry program for our survey 
>using Delphi/Pascal and for each household the information is stored in a 
>file called "EA-HM-HH.TXT" where EA is the enumeration area number, HM is 
>the homestead number and HH is the household number. Within this file the 
>data is stored as follows,
>
>
>###### file="5677-001-001.TXT" ######
>EAnumber=5677
>HMnumber=001
>HHnumber=001
># Demographics section
>Dserial=01   #first person in household
>Dage=56
>Dsex=1
>Dserial=02   #second person in household
>Dage=44
>Dsex=2
>Dserial=03   #second person in household
>Dage=7
>Dsex=2
># Agricultural inputs section
>Amaize=200
>Apumpkins=50
>###### end of file ########
>
>Note that in the demograpics section there may only be less than 3 or more 
>people in some households. I would like to create a file for the 
>demographics section which is as follows
>
>EAnumber|HMnumber|HHnumber|Dserial|Dage|Dsex
>5677 001 001 056 1
>5677 001 001 044 2
>5677 001 001 007 2
>
>and for the agricultural inputs I would like to have,
>
>EAnumber|HMnumber|HHnumber|Amaize|Apumpkins
>5677 001 001 0200
>5677 001 001 0050
>
>There are several similar files where the EA number, HM number or HH number 
>changes, thus I would also like to know whether it is possible to create a 
>script where for all household there is only one dataset for demographics, 
>and one for agricultural inputs.

The string handling functions in R should be able to handle this
about as easly as could the string functions in (say) Perl, but
I'm not going to do it for you.  It would take me at least an
hour, probably 2 hours.  If I were doing it - and I may not be
doing it the best way - I would first put all the little files
together into one big file f1 with the Unix command cat.  Then I
would read in f1 in a way that gives me a vector of strings, with
one line corresponding to each element of the vector:

d1 <- as.vector(as.matrix(read.table(f1,sep="\n",quote="")))

is how I've done this before, but I'm not sure this is all
necessary.

Then I would start a file d2 for output and write one line with
the names of the variables:

write.table("EAnumber HMnumber HHnumber Dserial Dage Dsex",file="d2"),
            quote=F,row.names=F,col.names=F,sep="")

(I'm not sure this command will work.  I'd try it first by itself.)

Then I would write a loop to process d1 line by line, with
sub-loops to deal with the family members in one of the original
files.  Here is part of a similar loop I wrote for another
purpose.  The variable "onoff" indicated whether I was still
within one of my original records (like one of your little
files).  You would probably want two such variables, one to
indicate the family, one for each of its members.

d2 <- {} # set up a vector for the output, undetermined length
for (i in 1:length(d1)) {
  s1 <- substr(d1[i],1,4)
  if (s1=="age=") {onoff <- TRUE; j <- j+1; d2[j] <- ""}
  if (onoff==1 & s1!="aqua" & s1!="comm" & s1!="apay")
    {d2[j] <- paste(d2[j],strsplit(d1[i],"=")[[1]][2],collapse=" ")}
  if (s1==f2) {onoff <- FALSE}
}

The following command is to replace plus signs with spaces.  It
just shows you how to use gsub().

d2 <- gsub("\\+"," ",d2)

Finally, I would append the vector d2 to the file I had written:

write.table(d2,file="d2",append=T,quote=F,row.names=F,col.names=F,sep="")

I have not tested any of this recently.  You will have to build
it up step by step and test it at each point.  But perhaps this
will get you started.  Also look at the functions grep() and
regexpr(), which might come in hand.

-- 
Jonathan Baron, Professor of Psychology, University of Pennsylvania
Home page:            http://www.sas.upenn.edu/~baron
R page:               http://finzi.psych.upenn.edu/



From kwan022 at stat.auckland.ac.nz  Wed Aug 20 13:14:17 2003
From: kwan022 at stat.auckland.ac.nz (Ko-Kang Kevin Wang)
Date: Wed, 20 Aug 2003 23:14:17 +1200 (NZST)
Subject: [R] Neural Networks in R
In-Reply-To: <8018.1061376858@www59.gmx.net>
Message-ID: <Pine.LNX.4.44.0308202313070.24923-100000@stat54.stat.auckland.ac.nz>

Hi,

Venables and Ripley's "Modern Applied Statistics with S", 4th Ed, 
Springer.

It is the best book and has a large section devoted to using the nnet 
library.

Of course, you need to understand the theory first....;-D

On Wed, 20 Aug 2003, Miriam Drei?ig wrote:

> Date: Wed, 20 Aug 2003 12:54:18 +0200 (MEST)
> From: Miriam Drei?ig <miriam.30 at gmx.de>
> To: R-help at stat.math.ethz.ch
> Subject: [R] Neural Networks in R
> 
> Hello!
> We are a group of three students at Bielefeld University currently working
> on a statistical projects about neural networks. Within the framework of this
> project we are supposed to use the nnet-function in R and explain how it
> works. Since anyone of us has much experience in using R we hoped to find some
> information on your homepage. Unfortunatelly, we haven't been very successfull
> so far.
> We were wondering if you happen to know any books or articles which deal
> with neural networks in R or if you could tell us were we can find such
> information.
> We would highly appreciate if you could help us on that matter.
> Thank you very much in advance.
> Best regards,
> 
> Anne Bruns, Miriam Drei?ig, Sascha Hartung
> 
> 

-- 
Cheers,

Kevin

------------------------------------------------------------------------------
"On two occasions, I have been asked [by members of Parliament],
'Pray, Mr. Babbage, if you put into the machine wrong figures, will
the right answers come out?' I am not able to rightly apprehend the
kind of confusion of ideas that could provoke such a question."

-- Charles Babbage (1791-1871) 
---- From Computer Stupidities: http://rinkworks.com/stupid/

--
Ko-Kang Kevin Wang
Master of Science (MSc) Student
SLC Tutor and Lab Demonstrator
Department of Statistics
University of Auckland
New Zealand
Homepage: http://www.stat.auckland.ac.nz/~kwan022
Ph: 373-7599
    x88475 (City)
    x88480 (Tamaki)



From ligges at statistik.uni-dortmund.de  Wed Aug 20 13:30:18 2003
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Wed, 20 Aug 2003 13:30:18 +0200
Subject: [R] aide
In-Reply-To: <200308201149.MAA03573@stats.math.u-psud.fr>
References: <200308201149.MAA03573@stats.math.u-psud.fr>
Message-ID: <3F435BCA.6050000@statistik.uni-dortmund.de>

Christine Tuleau wrote:
> Bonjour,
> 
> 
> J'aimerais savoir si quelqu'un pourrait m'aider en ce qui concerne l'utilisation 
> du package waveslim.
> Je voudrais savoir comment on fait pour revenir a un signal d'origine lorsque 
> l'on dispose de coefficients de detail et d'approximation.

Guten Tag,

ich schlage vor, sich wegen des Pakets waveslim an dessen Autor zu 
wenden und weitere mails in englischer Sprache zu verfassen.

Uwe Ligges



From yukiasais at ybb.ne.jp  Wed Aug 20 13:58:50 2003
From: yukiasais at ybb.ne.jp (yukihiro ishii)
Date: Wed, 20 Aug 2003 20:58:50 +0900
Subject: [R] On the Use of the nnet Library
Message-ID: <20030820204335.FB28.YUKIASAIS@ybb.ne.jp>

Dear List,

I am trying to solve a problem by the neural network method(library:
nnet). The problem is to express Weight in terms of Age , Sex and
Height for twenty people(thius is an example given by Tanake in
"Introduction to Neural Networks by NEUROSIM/L"(2003, in Japanese))..
The data frame consists of 20 observations with four variables: Sex, Age,
Height and Weight. Sex is treated as a factor, Age and Weight are
variables normalized to unity, as usual. I wanted to construct a neural
network based on this data, and so I ran the following code:

>library(nnet)
>net1<-nnet(Weight~Age+Sex+Height, size=2, linout=T,maxit=1000)

I repeated this thirteen times.  I used the default initial parameters
unless otherwise noted. The result is as follows, where init and final
mean initial and final RSS's, and NIT means the number of iterations
before reaching convergence or noncovergence:

Run#	init		NIT	final
1	71991.1 	30	995.1 
2	70870.0 	370	33.1 
3	72755.8 	<10	2134.3 
4	69840.6 	<10	2134.3 
5	70368.8 	190	39.7 
6	70368.8 	270	41.0 
7	71101.2 	190	39.7 
8	71606.1 	<10	2134.3 
9	72076.1 	<10	2134.3 
10	72249.1 	300	15.0 
11	71424.1 	<10	2134.3 
12	68483.8 	130	39.7 
13	71435.9 	>1000	4.6 


As you can see, the result is far from stable.

My question is:

How can I reach a stable answer?

.I know that initial parameters are crucially important in my case, and I
must choose proper parameter values, but I do not know how I can do that.

My second question is related to the response analysis of this data. I
do not know an effective method to evaluate the response to the
variance of each explanatory variable. Tanabe(2003) mentions "Net Effect
Ratio" defined by the average of dy/dx. Is there such a function in the
library, nnet? Such a function may help me reduce the number of the
explanatory variables.

I wonder if anyone could help me in such elementary questions.


---- It's elementary, Watson!

I remain an obedient Watson, hoping for Holmes' wisdom.


-- 
Yukihiro Ishii <yukiasais at ybb.ne.jp>
2-3-28?Tsurumaki-minami, Hadano
257-0002 Japan
Tel +81 463 69 1922
Fax +81 463 69 1922



From yukiasais at ybb.ne.jp  Wed Aug 20 13:59:44 2003
From: yukiasais at ybb.ne.jp (yukihiro ishii)
Date: Wed, 20 Aug 2003 20:59:44 +0900
Subject: [R] On the Use of the nnet Library
Message-ID: <20030820205932.FB37.YUKIASAIS@ybb.ne.jp>

Dear List,

I am trying to solve a problem by the neural network method(library:
nnet). The problem is to express Weight in terms of Age , Sex and
Height for twenty people(thius is an example given by Tanake in
"Introduction to Neural Networks by NEUROSIM/L"(2003, in Japanese))..
The data frame consists of 20 observations with four variables: Sex, Age,
Height and Weight. Sex is treated as a factor, Age and Weight are
variables normalized to unity, as usual. I wanted to construct a neural
network based on this data, and so I ran the following code:

>library(nnet)
>net1<-nnet(Weight~Age+Sex+Height, size=2, linout=T,maxit=1000)

I repeated this thirteen times.  I used the default initial parameters
unless otherwise noted. The result is as follows, where init and final
mean initial and final RSS's, and NIT means the number of iterations
before reaching convergence or noncovergence:

Run#	init		NIT	final
1	71991.1 	30	995.1 
2	70870.0 	370	33.1 
3	72755.8 	<10	2134.3 
4	69840.6 	<10	2134.3 
5	70368.8 	190	39.7 
6	70368.8 	270	41.0 
7	71101.2 	190	39.7 
8	71606.1 	<10	2134.3 
9	72076.1 	<10	2134.3 
10	72249.1 	300	15.0 
11	71424.1 	<10	2134.3 
12	68483.8 	130	39.7 
13	71435.9 	>1000	4.6 


As you can see, the result is far from stable.

My question is:

How can I reach a stable answer?

.I know that initial parameters are crucially important in my case, and I
must choose proper parameter values, but I do not know how I can do that.

My second question is related to the response analysis of this data. I
do not know an effective method to evaluate the response to the
variance of each explanatory variable. Tanabe(2003) mentions "Net Effect
Ratio" defined by the average of dy/dx. Is there such a function in the
library, nnet? Such a function may help me reduce the number of the
explanatory variables.

I wonder if anyone could help me in such elementary questions.


---- It's elementary, Watson!

I remain an obedient Watson, hoping for Holmes' wisdom.


-- 
Yukihiro Ishii <yukiasais at ybb.ne.jp>
2-3-28?Tsurumaki-minami, Hadano
257-0002 Japan
Tel +81 463 69 1922
Fax +81 463 69 1922



From andy_liaw at merck.com  Wed Aug 20 14:01:29 2003
From: andy_liaw at merck.com (Liaw, Andy)
Date: Wed, 20 Aug 2003 08:01:29 -0400
Subject: [R] RandomForest
Message-ID: <3A822319EB35174CA3714066D590DCD50205C9F6@usrymx25.merck.com>

Please tell us the version of the package, the version of R, and the
platform you're working in.

Sounds like you should upgrade to a newer version of the randomForest
package.  In Breiman's original code, he is counting the number of
misclassified cases and dividing that by the total number of cases.  This is
fine for sufficiently large number of trees (say about 20 or more).  Because
the prediction is based on aggregating the out-of-bag prediction, the error
rate should be number of misclassified cases divided by the number of cases
that have been predicted.  When the number of trees is small, not all cases
have been out-of-bag, and therefore not all of them have prediction.

There's similar problem with regression, which I have fixed in the R
package.  Users of Leo's Fortran code should be aware.

Currently there's no provision to cut trees out of a forest.  I originally
thought about writing a "burn" function that does this, but decided against
it: what would be the point?  Having more trees just takes up more
memory/disk space, and takes a bit longer for prediction, but does not
degrade prediction performance, unlike boosting.

HTH,
Andy

> -----Original Message-----
> From: Vladimir N. Kutinsky [mailto:kutinskyv at obninsk.com] 
> Sent: Wednesday, August 20, 2003 4:43 AM
> To: r-help at stat.math.ethz.ch
> Subject: [R] RandomForest
> 
> 
> Hello,
> 
> When I plot or look at the error rate vector for a random forest
> (rf$err.rate) it looks like a descending function except for 
> a few first points of the vector with error rates values 
> lower(sometimes much lower) than the general level of error 
> rates for a forest with such number of trees when the error 
> rates stop descending. Does it mean that there is a tree(s) 
> (that is built the first in the forest) that has a higher 
> predictive accuracy than the whole forest of trees?
> 
> One more minor question. Is there a way to "snip" the forest 
> of 100 trees to, say, 50 trees?
> 
> Thanks,
> Vladimir
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list 
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> 

------------------------------------------------------------------------------
Notice:  This e-mail message, together with any attachments, contains
information of Merck & Co., Inc. (Whitehouse Station, New Jersey, USA), and/or
its affiliates (which may be known outside the United States as Merck Frosst,
Merck Sharp & Dohme or MSD) that may be confidential, proprietary copyrighted
and/or legally privileged, and is intended solely for the use of the
individual or entity named on this message.  If you are not the intended
recipient, and have received this message in error, please immediately return
this by e-mail and then delete it.



From hans.gardfjell at eg.umu.se  Wed Aug 20 14:53:41 2003
From: hans.gardfjell at eg.umu.se (Hans Gardfjell)
Date: Wed, 20 Aug 2003 14:53:41 +0200
Subject: [R] Filled triangles in lattice graphics?
Message-ID: <3F436F55.3050702@eg.umu.se>

Dear R users,

I can get a filled triangle pointing upwards by specifying pch=17 in 
xyplot or lpoints, but how do I get a filled triangle that points downwards?

In the standard plot function it's possible to use 
plot(x,y,pch=25,bg="black"), but bg= doesn't seem to work with lattice 
and lpoints.

Thanks,

Hans Gardfjell
Ecology and Environmental Science
Ume? University, Sweden



From sumslily at yahoo.com  Wed Aug 20 15:42:02 2003
From: sumslily at yahoo.com (Lily)
Date: Wed, 20 Aug 2003 06:42:02 -0700 (PDT)
Subject: [R] question about simulation.
Message-ID: <20030820134202.20581.qmail@web41206.mail.yahoo.com>

I am running a 1000 simulations, it works for 2
simulations. However, I get the following error
message whenever I run it more than 3 times:

"The instruction at '0*11044080' referenced memory at
"o*3ff00000". The memory could not be "written".
and, I can also get something like "exception: access
violation (0*c0000005). Address:0*11044080".

Anybody knows what's going on? Thanks a lot!



From kutinskyv at obninsk.com  Wed Aug 20 15:56:48 2003
From: kutinskyv at obninsk.com (Vladimir N. Kutinsky)
Date: Wed, 20 Aug 2003 17:56:48 +0400
Subject: [R] RandomForest
In-Reply-To: <3A822319EB35174CA3714066D590DCD50205C9F6@usrymx25.merck.com>
Message-ID: <KFENLPKGENECNKICCNMBKECBCGAA.kutinskyv@obninsk.com>

Andy,

First of all, thank you for you reply.
I'm using R1.6.1 for Windows. A few days ago I updated the randomForest
package from CRAN. It gives warning messages now that the package was built
under R1.6.2 but seems to work fine.
To make sure we're talking about the same thing, let's take iris
classification as an example.
set.seed(17)
rf<-randomForest(Species~.,iris)
rf$err.rate[1:10]
[1] 0.02000000 0.02666667 0.03333333 0.04666667 0.04666667 0.05333333
[7] 0.05333333 0.06000000 0.06000000 0.05333333
As you can see the forest of 1,2 or 3 trees gives a better predictive
accuracy.

> Because the prediction is based on aggregating the out-of-bag prediction,
> the error rate should be number of misclassified cases divided by the
> number of cases that have been predicted.  When the number of trees is
small, not
> all cases have been out-of-bag, and therefore not all of them have
prediction.

I don't quite understand you, maybe I missed something in the theory of
random forest. Do you mean to say that in order to get an error rate not all
cases of the learning data set are used? Just some number of them taken at
random? Is this number, if it is so, gets larger as the forest grows?

Again thanks,
Vladimir



From simon.woodhead at bristol.ac.uk  Wed Aug 20 15:58:36 2003
From: simon.woodhead at bristol.ac.uk (Simon Woodhead)
Date: Wed, 20 Aug 2003 14:58:36 +0100
Subject: [R] grid Graphics by Paul Murrell
Message-ID: <3F437E8C.8050004@bristol.ac.uk>

Dear All,

I've been trying to format a plot output using par() with mfrow, fin, 
mai, etc and basically it's proving to be a pain. I searched on google 
and found that Paul Murrell had written a grid Graphics program which 
seems perfect. However, when I try and use say viewport() I just get a 
function not found error. Can someone tell me what I'm doing wrong please?

Thank-you

Simon



From ligges at statistik.uni-dortmund.de  Wed Aug 20 16:00:24 2003
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Wed, 20 Aug 2003 16:00:24 +0200
Subject: [R] question about simulation.
In-Reply-To: <20030820134202.20581.qmail@web41206.mail.yahoo.com>
References: <20030820134202.20581.qmail@web41206.mail.yahoo.com>
Message-ID: <3F437EF8.9080405@statistik.uni-dortmund.de>

Lily wrote:

> I am running a 1000 simulations, it works for 2
> simulations. However, I get the following error
> message whenever I run it more than 3 times:
> 
> "The instruction at '0*11044080' referenced memory at
> "o*3ff00000". The memory could not be "written".
> and, I can also get something like "exception: access
> violation (0*c0000005). Address:0*11044080".

You found a bug.

I guess you are on Windows (which?) and you are talking about R-1.7.1 
(please tell us these details).

Can you provide a minimal example that reproduces the error with 
R-1.7.1, please?

Uwe Ligges



From p.dalgaard at biostat.ku.dk  Wed Aug 20 16:14:51 2003
From: p.dalgaard at biostat.ku.dk (Peter Dalgaard BSA)
Date: Wed, 20 Aug 2003 14:14:51 -0000
Subject: [R] logistic regression without intercept
In-Reply-To: <1061333277.4787.104.camel@iron.libaux.ucsf.edu>
References: <Pine.A41.4.44.0308191530260.68442-100000@homer25.u.washington.edu>
	<1061333277.4787.104.camel@iron.libaux.ucsf.edu>
Message-ID: <x2ada4o1en.fsf@biostat.ku.dk>

Ross Boylan <ross at biostat.ucsf.edu> writes:

> Thanks to Thomas Lumley, Spencer Graves, and Steve Sullivan for their
> advice on and off list to modify the formula with +0 (which I did; it
> worked) or -1 at the end.
...
> > > Also, I noticed that S-Plus but not R has a glim routine that uses
> > > maximum likelihood.  What would be the equivalent?
> > 
> > glm.
> > 
> I thought glm was a minimize the deviations approach, which is different
> from maximize likelihood.

It minimizes deviance, which is a constant minus twice the
log-likelihood.... I.e. it's equivalent to ML (unless you want to
quibble about bias-corrected scale parameter estimation in the
Gaussian case.)

-- 
   O__  ---- Peter Dalgaard             Blegdamsvej 3  
  c/ /'_ --- Dept. of Biostatistics     2200 Cph. N   
 (*) \(*) -- University of Copenhagen   Denmark      Ph: (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)             FAX: (+45) 35327907



From Roger.Bivand at nhh.no  Wed Aug 20 16:12:44 2003
From: Roger.Bivand at nhh.no (Roger Bivand)
Date: Wed, 20 Aug 2003 16:12:44 +0200 (CEST)
Subject: [R] grid Graphics by Paul Murrell
In-Reply-To: <3F437E8C.8050004@bristol.ac.uk>
Message-ID: <Pine.LNX.4.44.0308201611040.30740-100000@reclus.nhh.no>

On Wed, 20 Aug 2003, Simon Woodhead wrote:

> Dear All,
> 
> I've been trying to format a plot output using par() with mfrow, fin, 
> mai, etc and basically it's proving to be a pain. I searched on google 
> and found that Paul Murrell had written a grid Graphics program which 
> seems perfect. However, when I try and use say viewport() I just get a 
> function not found error. Can someone tell me what I'm doing wrong please?
> 

> library(grid)

grid is not loaded by default.

> Thank-you
> 
> Simon
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> 

-- 
Roger Bivand
Economic Geography Section, Department of Economics, Norwegian School of
Economics and Business Administration, Breiviksveien 40, N-5045 Bergen,
Norway. voice: +47 55 95 93 55; fax +47 55 95 93 93
e-mail: Roger.Bivand at nhh.no



From sumslily at yahoo.com  Wed Aug 20 16:18:13 2003
From: sumslily at yahoo.com (Lily)
Date: Wed, 20 Aug 2003 07:18:13 -0700 (PDT)
Subject: [R] question about simulation.
In-Reply-To: <3F437EF8.9080405@statistik.uni-dortmund.de>
Message-ID: <20030820141813.19127.qmail@web41213.mail.yahoo.com>

Thanks for your response.I am working on R-1.7.1 under
windows. Here is part of my code for simulation:

x1<-...
y1<-...
beta<-c(1,3,5,7,9))
mn<-0
for ( i in 1:3) {

e1<-rnorm(40,mean=0,sd=1)
y1<-x1%*%beta+e1
result<-mle.cv(y1~x1)
result2<-result[1, ]
mn<-rbind(mn, result2)

}
mn

Thanks!! 


--- Uwe Ligges <ligges at statistik.uni-dortmund.de>
wrote:
> Lily wrote:
> 
> > I am running a 1000 simulations, it works for 2
> > simulations. However, I get the following error
> > message whenever I run it more than 3 times:
> > 
> > "The instruction at '0*11044080' referenced memory
> at
> > "o*3ff00000". The memory could not be "written".
> > and, I can also get something like "exception:
> access
> > violation (0*c0000005). Address:0*11044080".
> 
> You found a bug.
> 
> I guess you are on Windows (which?) and you are
> talking about R-1.7.1 
> (please tell us these details).
> 
> Can you provide a minimal example that reproduces
> the error with 
> R-1.7.1, please?
> 
> Uwe Ligges
>



From kutinskyv at obninsk.com  Wed Aug 20 16:25:54 2003
From: kutinskyv at obninsk.com (Vladimir N. Kutinsky)
Date: Wed, 20 Aug 2003 18:25:54 +0400
Subject: [R] RandomForest
In-Reply-To: <3A822319EB35174CA3714066D590DCD50205C9F6@usrymx25.merck.com>
Message-ID: <KFENLPKGENECNKICCNMBIECCCGAA.kutinskyv@obninsk.com>

Andy,

Does it mean that the error rate does increase as long as the aggregating
number of out-of-bag cases reaches the number of all cases?  or, in other
words, because the number of points being predicted (right or wrong) gets
larger at the first steps of the process?

If it so then it's all clear now.

A few more questions.
1. What is the format of using the "classwt" parameter? Should it be
something like c(0.3,0.6,0.1)?

2. Where can I find any information about rf$forest element of the result?
namely about its elements like $treemap, $nodeclass etc?

Thanks,
Vladimir



From tlumley at u.washington.edu  Wed Aug 20 16:27:35 2003
From: tlumley at u.washington.edu (Thomas Lumley)
Date: Wed, 20 Aug 2003 07:27:35 -0700 (PDT)
Subject: [R] logistic regression without intercept
In-Reply-To: <1061333277.4787.104.camel@iron.libaux.ucsf.edu>
Message-ID: <Pine.A41.4.44.0308200727010.12720-100000@homer30.u.washington.edu>

On Tue, 19 Aug 2003, Ross Boylan wrote:
> > > Also, I noticed that S-Plus but not R has a glim routine that uses
> > > maximum likelihood.  What would be the equivalent?
> >
> > glm.
> >
> I thought glm was a minimize the deviations approach, which is different
> from maximize likelihood.
>

No, it's maximum likelihood by iteratively reweighted least squares.

	-thomas



From simon.woodhead at bristol.ac.uk  Wed Aug 20 15:53:25 2003
From: simon.woodhead at bristol.ac.uk (Simon Woodhead)
Date: Wed, 20 Aug 2003 14:53:25 +0100
Subject: [R] Plots default to Rplots.ps
In-Reply-To: <1061371900.966.8.camel@xena>
References: <3F4321B7.2040209@bristol.ac.uk> <1061371900.966.8.camel@xena>
Message-ID: <3F437D55.30709@bristol.ac.uk>

Martyn Plummer wrote:

>
>It looks like you have compiled R without support for X11, so the
>default plotting device is postscript(). You need to install the
>XFree86-devel package and recompile, or else use the ready made RPM
>packages on CRAN:
>
>http://cran.r-project.org/bin/linux/redhat/9/i386/
>
>Martyn
>  
>
Cheers all works fine now.

Simon



From th50 at leicester.ac.uk  Wed Aug 20 16:40:35 2003
From: th50 at leicester.ac.uk (Hotz, T.)
Date: Wed, 20 Aug 2003 15:40:35 +0100
Subject: [R] grid Graphics by Paul Murrell
Message-ID: <1F2CE8D4B0195E488213E8B8CCF714860250132E@saffron.cfs.le.ac.uk>

Simon,

You might want to have a look at these two articles in R News
(http://CRAN.R-project.org/doc/Rnews/):

Paul Murrell. The grid graphics package. R News, 2(2):14-19, June 2002.
Deepayan Sarkar. Lattice. R News, 2(2):19-23, June 2002.

HTH

Thomas 


> -----Original Message-----
> From: Roger Bivand [mailto:Roger.Bivand at nhh.no]
> Sent: 20 August 2003 15:13
> To: Simon Woodhead
> Cc: r-help at stat.math.ethz.ch
> Subject: Re: [R] grid Graphics by Paul Murrell
> 
> 
> On Wed, 20 Aug 2003, Simon Woodhead wrote:
> 
> > Dear All,
> > 
> > I've been trying to format a plot output using par() with 
> mfrow, fin, 
> > mai, etc and basically it's proving to be a pain. I 
> searched on google 
> > and found that Paul Murrell had written a grid Graphics 
> program which 
> > seems perfect. However, when I try and use say viewport() I 
> just get a 
> > function not found error. Can someone tell me what I'm 
> doing wrong please?
> > 
> 
> > library(grid)
> 
> grid is not loaded by default.
> 
> > Thank-you
> > 
> > Simon
> > 
> > ______________________________________________
> > R-help at stat.math.ethz.ch mailing list
> > https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> > 
> 
> -- 
> Roger Bivand
> Economic Geography Section, Department of Economics, 
> Norwegian School of
> Economics and Business Administration, Breiviksveien 40, 
> N-5045 Bergen,
> Norway. voice: +47 55 95 93 55; fax +47 55 95 93 93
> e-mail: Roger.Bivand at nhh.no
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> 

---

Thomas Hotz
Research Associate in Medical Statistics
University of Leicester
United Kingdom

Department of Epidemiology and Public Health
22-28 Princess Road West
Leicester
LE1 6TP
Tel +44 116 252-5410
Fax +44 116 252-5423

Division of Medicine for the Elderly
Department of Medicine
The Glenfield Hospital
Leicester
LE3 9QP
Tel +44 116 256-3643
Fax +44 116 232-2976



From andy_liaw at merck.com  Wed Aug 20 16:51:12 2003
From: andy_liaw at merck.com (Liaw, Andy)
Date: Wed, 20 Aug 2003 10:51:12 -0400
Subject: [R] RandomForest
Message-ID: <3A822319EB35174CA3714066D590DCD50205C9FB@usrymx25.merck.com>

Vladimir,

The OOB error rate estimates for the first few iterations is necessarily
variable, as the number of cases that have been OOB (and therefore
predicted) is relatively small.  As an example:

> library(randomForest)
> data(iris)
> err1 <- err2 <- numeric(10)
> set.seed(1)
> for(i in 1:10) err1[i] <- randomForest(Species~., data=iris,
ntree=1)$err.rate
> for(i in 1:10) err2[i] <- randomForest(Species~., data=iris,
ntree=5)$err.rate[5]
> var(err1)
[1] 0.001141612
> var(err2)
[1] 0.0001058114
> err1
 [1] 0.03703704 0.01818182 0.01886792 0.07142857 0.09259259 0.11764706
0.03773585
 [8] 0.03703704 0.08771930 0.05000000
> err2
 [1] 0.06666667 0.04316547 0.07462687 0.05839416 0.04511278 0.05303030
0.05109489
 [8] 0.04316547 0.05925926 0.05714286

When only one tree is grown, the OOB estimate of error rate is based only on
1/e (on the average) cases.  As the number of trees increases, the number of
cases that have been OOB at least once increases, and therefore number of
cases used to estimate error rate increases.  If my brain isn't too rusty,
on the average, the proportion of cases that would have been used to compute
the OOB error rate, when five trees are grown, is approximately 

> 1 - dbinom(0, 5, 1/exp(1))
[1] 0.8990748

I guess Leo never intended people to grow fewer than, say, 50 trees, so
that's probably why he didn't care about this problem.

Regarding your other questions:

1. Right, except that they don't need to add up to one.  The code normalizes
that internally.

2. Some one asked me the exact same question last night (off the list).
Here's what I replied:

Leo did not have any documentation on what those arrays are, but we sort of
figured it out ourselves.

For each tree:
ndbigtree is the number of nodes in the tree

For each node in the tree:
nodestatus is an indicator (-1=terminal)
bestvar is the variable used to split the node (0 if node is terminal)
treemap contains "pointer" to decendant nodes (e.g., [2, 3] means the left
decendant is node 2 and left decendant is node 3, both are 0 if node is
terminal) nodeclass is the class of the node, if terminal (0 otherwise)
xbestsplit is the cutoff used to split the node

For the forest:
pid is the vector of normalized class weights
ncat is the vector of number of categories in the predictors (=1 for
continuous variables) maxcat is max(ncat) nrnodes is the maximum possible
number of nodes in a tree ntree is the number of trees in the forest nclass
is the number of classes in the response

The meaning of xbestsplit for categorical predictor is a bit tricky: a
binary expansion of the possible splits is done to simplify the splitting.
You might find the heuristics in the CART book, but I'm not sure.

HTH,
Andy

> -----Original Message-----
> From: Vladimir N. Kutinsky [mailto:kutinskyv at obninsk.com] 
> Sent: Wednesday, August 20, 2003 10:26 AM
> To: Liaw, Andy; r-help at stat.math.ethz.ch
> Subject: RE: [R] RandomForest
> 
> 
> Andy,
> 
> Does it mean that the error rate does increase as long as the 
> aggregating number of out-of-bag cases reaches the number of 
> all cases?  or, in other words, because the number of points 
> being predicted (right or wrong) gets larger at the first 
> steps of the process?
> 
> If it so then it's all clear now.
> 
> A few more questions.
> 1. What is the format of using the "classwt" parameter? 
> Should it be something like c(0.3,0.6,0.1)?
> 
> 2. Where can I find any information about rf$forest element 
> of the result? namely about its elements like $treemap, 
> $nodeclass etc?
> 
> Thanks,
> Vladimir
> 
> 

------------------------------------------------------------------------------
Notice:  This e-mail message, together with any attachments, contains
information of Merck & Co., Inc. (Whitehouse Station, New Jersey, USA), and/or
its affiliates (which may be known outside the United States as Merck Frosst,
Merck Sharp & Dohme or MSD) that may be confidential, proprietary copyrighted
and/or legally privileged, and is intended solely for the use of the
individual or entity named on this message.  If you are not the intended
recipient, and have received this message in error, please immediately return
this by e-mail and then delete it.



From solares at unsl.edu.ar  Wed Aug 20 16:58:50 2003
From: solares at unsl.edu.ar (solares@unsl.edu.ar)
Date: Wed, 20 Aug 2003 11:58:50 -0300 (ART)
Subject: [R] delete variable
Message-ID: <43984.170.210.173.216.1061391530.squirrel@inter14.unsl.edu.ar>

Why in the codigo that continues when is erased the menubutton herself is 
not erased tambien the variable one opc, though itself deberia you erase 
with the assign?  
library(tcltk)
vectPath<-c()
opc<-tclVar(init=" ")

archivos<-function(){
          f<-tkcmd("tk_getOpenFile")
          temparch<-tclvalue(f)
          assign("vectPath",c(vectPath,temparch),.GlobalEnv)
          cant<-length(vectPath)
          j<-vectPath[cant]         
          tkadd(m, "radio", label=j, variable="opc", value=j)
}

ver<-function(){ 
print(tclvalue("opc"))
}

borra<-function(){
	 assign("vectPath",c(),.GlobalEnv)
	 assign("opc"," ",.GlobalEnv)
     tkdelete(m,"0","end")
}
tt <- tktoplevel()
tkpack(mb <- tkmenubutton(tt, text="Datos"))
m <- tkmenu(mb,tearoff=FALSE) 
tkconfigure(mb,menu=m)
b<-tkbutton(tt,text="abrir",command=function()archivos())
tkpack(b)
b3<-tkbutton(tt,text="ver",command=function()ver())
tkpack(b3)
b4<-tkbutton(tt,text="Borrar",command=function()borra())
tkpack(b4)



From ligges at statistik.uni-dortmund.de  Wed Aug 20 17:10:16 2003
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Wed, 20 Aug 2003 17:10:16 +0200
Subject: [R] Crash on Windows with package wle;  was: Re: [R] question
	about simulation
In-Reply-To: <20030820141813.19127.qmail@web41213.mail.yahoo.com>
References: <20030820141813.19127.qmail@web41213.mail.yahoo.com>
Message-ID: <3F438F58.4080401@statistik.uni-dortmund.de>

Lily wrote:

> Thanks for your response.I am working on R-1.7.1 under
> windows. Here is part of my code for simulation:
> 
> x1<-...
> y1<-...
> beta<-c(1,3,5,7,9))
> mn<-0
> for ( i in 1:3) {
> 
> e1<-rnorm(40,mean=0,sd=1)
> y1<-x1%*%beta+e1
> result<-mle.cv(y1~x1)
> result2<-result[1, ]
> mn<-rbind(mn, result2)
> 
> }
> mn

A *reproducible* example, please!
That means it runs without modifications and reproduces the error in a 
new R session - it's not the idea that each helper has to generate 
appropriate data him/herself.


You could have told you are using
  Package: wle
Is your version of wle recent, i.e.
  Version: 0.7-5 ?

Uwe Ligges


BTW: I may point you to R's easy to use debugging tools. It's worth 
learning to debug your functions for other possibly forthcoming tasks as 
well.


> Thanks!! 
> 
> 
> --- Uwe Ligges <ligges at statistik.uni-dortmund.de>
> wrote:
> 
>>Lily wrote:
>>
>>
>>>I am running a 1000 simulations, it works for 2
>>>simulations. However, I get the following error
>>>message whenever I run it more than 3 times:
>>>
>>>"The instruction at '0*11044080' referenced memory
>>
>>at
>>
>>>"o*3ff00000". The memory could not be "written".
>>>and, I can also get something like "exception:
>>
>>access
>>
>>>violation (0*c0000005). Address:0*11044080".
>>
>>You found a bug.
>>
>>I guess you are on Windows (which?) and you are
>>talking about R-1.7.1 
>>(please tell us these details).
>>
>>Can you provide a minimal example that reproduces
>>the error with 
>>R-1.7.1, please?
>>
>>Uwe Ligges
>>
> 
> 
> 
> __________________________________
> Do you Yahoo!?

> http://sitebuilder.yahoo.com



From shli at stat.wvu.edu  Wed Aug 20 15:44:38 2003
From: shli at stat.wvu.edu (Shengqiao Li)
Date: Wed, 20 Aug 2003 09:44:38 -0400 (EDT)
Subject: [R] Method of L-BFGS-B of optim  evaluate function outside of box
 constraints
Message-ID: <Pine.GSO.4.55.0308200913530.14989@student>


Hi, R guys:

I'm using L-BFGS-B method of optim for minimization problem. My function
called besselI function which need non-negative parameter and the besselI
will overflow if the parameter is too large. So I set the constraint box
which is reasonable for my problem. But the point outside the box was
test, and I got error. My program and the error follows. This program
depends on CircStats package.


Anyone has any idea about this?

Thanks in advance.

Li

#################### source code ###################################

Dk2<- function(pars,theta)
{
	kappa<- pars[1]; mu<- pars[2];
 	IoK<- besselI(kappa, nu=0);
	res<- besselI(2*kappa, nu=0)/2/IoK^2 -
mean(exp(kappa*cos(theta-mu)))/IoK;
	if(is.na(res)||is.infinite(res)){
		 print(pars);
		# assign("Theta", theta, env=.GlobalEnv);
	}
    return(res);
}


mse.Dk2<- function(pars, s, n)
{
	sum.est <- SSE <- numeric(2);
	j<- 0;
    while(j<=n){
      	theta<- rvm(s, pi, k=pars[1]) - pi;
      	est<- optim(par=pars, fn=Dk2, lower=c(0.001, -pi), upper=c(10,
pi), method="L-BFGS-B", theta=theta);
      	i<- 0;
        while(est$convergence!=0 && i< 30){
	          est<- optim(par=est$par, fn=Dk2, lower=c(0.001, -pi),
upper=c(10, pi), method="L-BFGS-B", theta=theta);
	          i<- i+1;
        }
        if(est$convergence!=0) {
	        #print(j);
	        next;
	     }
        else { j<- j+1; }

        #est<- nlm(p=pars, f=Dk2, theta=theta);
        mu.hat<- est$par[2];
        while(mu.hat< -pi) mu.hat<- mu.hat + 2*pi;
        while(mu.hat > pi) mu.hat<- mu.hat  -2*pi;
        est<- c(est$par[1], mu.hat);
        sum.est <- sum.est +  est;
	  	SSE <- SSE + (est - pars)^2;


	}
	Est <-  sum.est/n;
	Bias<- Est - pars;
	MSE<- SSE/n;

	res<- c(Kappa=pars[1], Kappa.hat= Est[1], Kappa.Bias=Bias[1],
Kappa.MSE=MSE[1], Mu.hat=Est[2], Mu.MSE=MSE[2])

   	return(res);
}
kappas <- c(0.01, 0.05, 0.1, 0.20, 0.5, 1, 2, 5);
N<- 10000;
for ( s in c(5, 10, 20, 30, 50)){
	cat("\nSample size = ", s);
	cat("\n=====================================\n");
	res<- NULL;
	for(i in 1:8){
		res<- rbind(res, mse.Dk2(c(kappas[i], 0), s, N));

	}
	print(round(res,4));
}

#Error message. -32.7 is far lower then the lower limit 0.001.
Sample size =  5
=====================================
[1] -32.736857  -3.141593
Error in optim(par = pars, fn = Dk2, lower = c(0.001, -pi), upper = c(10,
:
        L-BFGS-B needs finite values of fn
In addition: Warning messages:
1: NaNs produced in: besselI(x, nu, 1 + as.logical(expon.scaled))
2: NaNs produced in: besselI(x, nu, 1 + as.logical(expon.scaled))



From spencer.graves at pdf.com  Wed Aug 20 18:18:15 2003
From: spencer.graves at pdf.com (Spencer Graves)
Date: Wed, 20 Aug 2003 09:18:15 -0700
Subject: [R] question about simulation.
References: <20030820134202.20581.qmail@web41206.mail.yahoo.com>
Message-ID: <3F439F47.3010408@pdf.com>

What version of R under what operating system with how much memory? 
What R functions are you using?  Can you do a traceback()?  Also, have 
you tried "www.r-project.org" -> search -> "R site search"?

hope this helps.  spencer graves

Lily wrote:
> I am running a 1000 simulations, it works for 2
> simulations. However, I get the following error
> message whenever I run it more than 3 times:
> 
> "The instruction at '0*11044080' referenced memory at
> "o*3ff00000". The memory could not be "written".
> and, I can also get something like "exception: access
> violation (0*c0000005). Address:0*11044080".
> 
> Anybody knows what's going on? Thanks a lot!
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help



From spencer.graves at pdf.com  Wed Aug 20 18:25:57 2003
From: spencer.graves at pdf.com (Spencer Graves)
Date: Wed, 20 Aug 2003 09:25:57 -0700
Subject: [R] Method of L-BFGS-B of optim  evaluate function outside of
	box constraints
References: <Pine.GSO.4.55.0308200913530.14989@student>
Message-ID: <3F43A115.7050309@pdf.com>

I see two problems, which I handle differently:

	  First, if arg must be nonnegative, I program fn in terms of log.arg, 
so it can never become nonpositive.

	  Second, is besselI always positive?  If yes, then program fn to 
compute log(besselI) and use that.  Standard references such as 
Abramowitz and Stegun (1970) Handbook of Mathematical Functions (US 
Gov't printing office) give asymptotic expansions that give good answers 
for values of arguments near very large or very small.  If you test the 
values of the arguments, you can develop good numbers to use.  Then 
optim should work fine.

hope this helps.  spencer graves

Shengqiao Li wrote:
> Hi, R guys:
> 
> I'm using L-BFGS-B method of optim for minimization problem. My function
> called besselI function which need non-negative parameter and the besselI
> will overflow if the parameter is too large. So I set the constraint box
> which is reasonable for my problem. But the point outside the box was
> test, and I got error. My program and the error follows. This program
> depends on CircStats package.
> 
> 
> Anyone has any idea about this?
> 
> Thanks in advance.
> 
> Li
> 
> #################### source code ###################################
> 
> Dk2<- function(pars,theta)
> {
> 	kappa<- pars[1]; mu<- pars[2];
>  	IoK<- besselI(kappa, nu=0);
> 	res<- besselI(2*kappa, nu=0)/2/IoK^2 -
> mean(exp(kappa*cos(theta-mu)))/IoK;
> 	if(is.na(res)||is.infinite(res)){
> 		 print(pars);
> 		# assign("Theta", theta, env=.GlobalEnv);
> 	}
>     return(res);
> }
> 
> 
> mse.Dk2<- function(pars, s, n)
> {
> 	sum.est <- SSE <- numeric(2);
> 	j<- 0;
>     while(j<=n){
>       	theta<- rvm(s, pi, k=pars[1]) - pi;
>       	est<- optim(par=pars, fn=Dk2, lower=c(0.001, -pi), upper=c(10,
> pi), method="L-BFGS-B", theta=theta);
>       	i<- 0;
>         while(est$convergence!=0 && i< 30){
> 	          est<- optim(par=est$par, fn=Dk2, lower=c(0.001, -pi),
> upper=c(10, pi), method="L-BFGS-B", theta=theta);
> 	          i<- i+1;
>         }
>         if(est$convergence!=0) {
> 	        #print(j);
> 	        next;
> 	     }
>         else { j<- j+1; }
> 
>         #est<- nlm(p=pars, f=Dk2, theta=theta);
>         mu.hat<- est$par[2];
>         while(mu.hat< -pi) mu.hat<- mu.hat + 2*pi;
>         while(mu.hat > pi) mu.hat<- mu.hat  -2*pi;
>         est<- c(est$par[1], mu.hat);
>         sum.est <- sum.est +  est;
> 	  	SSE <- SSE + (est - pars)^2;
> 
> 
> 	}
> 	Est <-  sum.est/n;
> 	Bias<- Est - pars;
> 	MSE<- SSE/n;
> 
> 	res<- c(Kappa=pars[1], Kappa.hat= Est[1], Kappa.Bias=Bias[1],
> Kappa.MSE=MSE[1], Mu.hat=Est[2], Mu.MSE=MSE[2])
> 
>    	return(res);
> }
> kappas <- c(0.01, 0.05, 0.1, 0.20, 0.5, 1, 2, 5);
> N<- 10000;
> for ( s in c(5, 10, 20, 30, 50)){
> 	cat("\nSample size = ", s);
> 	cat("\n=====================================\n");
> 	res<- NULL;
> 	for(i in 1:8){
> 		res<- rbind(res, mse.Dk2(c(kappas[i], 0), s, N));
> 
> 	}
> 	print(round(res,4));
> }
> 
> #Error message. -32.7 is far lower then the lower limit 0.001.
> Sample size =  5
> =====================================
> [1] -32.736857  -3.141593
> Error in optim(par = pars, fn = Dk2, lower = c(0.001, -pi), upper = c(10,
> :
>         L-BFGS-B needs finite values of fn
> In addition: Warning messages:
> 1: NaNs produced in: besselI(x, nu, 1 + as.logical(expon.scaled))
> 2: NaNs produced in: besselI(x, nu, 1 + as.logical(expon.scaled))
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help



From sumslily at yahoo.com  Wed Aug 20 19:42:12 2003
From: sumslily at yahoo.com (Lily)
Date: Wed, 20 Aug 2003 10:42:12 -0700 (PDT)
Subject: [R] question about simulation.
In-Reply-To: <3F439F47.3010408@pdf.com>
Message-ID: <20030820174212.74661.qmail@web41210.mail.yahoo.com>

The problem has been solved. It is my mistake for not
define the number of monte.carlo simulation when I am
using mle.cv. 

Thank you all for your help!

--- Spencer Graves <spencer.graves at PDF.COM> wrote:
> What version of R under what operating system with
> how much memory? 
> What R functions are you using?  Can you do a
> traceback()?  Also, have 
> you tried "www.r-project.org" -> search -> "R site
> search"?
> 
> hope this helps.  spencer graves
> 
> Lily wrote:
> > I am running a 1000 simulations, it works for 2
> > simulations. However, I get the following error
> > message whenever I run it more than 3 times:
> > 
> > "The instruction at '0*11044080' referenced memory
> at
> > "o*3ff00000". The memory could not be "written".
> > and, I can also get something like "exception:
> access
> > violation (0*c0000005). Address:0*11044080".
> > 
> > Anybody knows what's going on? Thanks a lot!
> > 
> > ______________________________________________
> > R-help at stat.math.ethz.ch mailing list
> >
>
https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> 
>



From Luis.Tito-de-Morais at ird.sn  Wed Aug 20 19:45:26 2003
From: Luis.Tito-de-Morais at ird.sn (Tito de Morais Luis)
Date: Wed, 20 Aug 2003 17:45:26 -0000
Subject: [R] import statgraphics data files
Message-ID: <1061393838.3365.9.camel@rap06.ird.sn>

Hi R listers,

I have some *old* statgraphics data files (.asf extension) that I would
like to re-analyse. I don't have the statgraphics software anymore. Does
anybody know about an utility to convert such files to import them into
R ?

Thank you for any hint

Tito

-- 
L. Tito de Morais
      UR RAP
   IRD de Dakar
      BP 1386
       Dakar
      S?n?gal

T?l.: + 221 849 33 31
Fax: +221 832 16 75
Courriel: tito at ird.sn



From elvis at xlsolutions-corp.com  Wed Aug 20 19:53:53 2003
From: elvis at xlsolutions-corp.com (elvis@xlsolutions-corp.com)
Date: Wed, 20 Aug 2003 10:53:53 -0700
Subject: [R] Course***R/Splus Fundamentals and Programming Techniques,
	September 2003 @ 3 locations near you! (DC, Boston, San Francisco)
Message-ID: <APEHLKCMHHAKBGLAPKPCOEGBCHAA.elvis@xlsolutions-corp.com>

XLSolutions Corporation (www.xlsolutions-corp.com) is proud
to announce May-June 2-day "R/S-plus Fundamentals and Programming
Techniques".

****Washington, DC -----------------> September 25-26
****Boston, MA ---------------------> September 25-26
****San Francisco, CA --------------> September 18-19 (Only 6 seats left)

Reserve your seat now at the early bird rates! Payment due AFTER the class.
Interested in R/Splus Advanced course? email us.


Course Description:

This two-day R/S-plus course focuses on a broad spectrum of topics,
from reading raw data to a comparison of R and S. We will learn
the essentials of data manipulation, graphical visualization
and R/S-plus programming. We will explore statistical data analysis tools,
including graphics with data sets. How to enhance your plots.
We will perform basic statistics and fit linear regression models.
Participants are encouraged to bring data for interactive sessions


With the following outline:

- An Overview of R: Installation and Demonstration
- Data Manipulation and Graphics
- Using Lattice Graphics
- A Comparison of R and S-Plus
- How can R Complement SAS?
- Writing Functions
- Avoiding Loops
- Vectorization
- Statistical Modeling
- Project Management
- Techniques for Effective use of R and S
- Enhancing Plots
- Using High-level Plotting Functions
- Building and Distributing Packages (libraries)


Early Bird Research: $995 (Includes course materials, 90 days Technical
Support for R, snacks and continental breakfast!); Email us for group
discounts.

Email Sue Turner: sue at xlsolutions-corp.com
Phone: 206-686-1578 x221
Visit us: www.xlsolutions-corp.com/training.htm

Please let us know if you and your colleagues are interested in this class
to take advantage of group discount. Register now to secure your seat!

Interested in R/Splus Advanced course? email us.


Cheers,

Elvis Miller, PhD
Manager Training.
XLSolutions Corporation
206 686 1578
www.xlsolutions-corp.com
elvis at xlsolutions-corp.com
****************Need help? contact our consulting team!**********



From junwen at astro.ocis.temple.edu  Wed Aug 20 20:22:22 2003
From: junwen at astro.ocis.temple.edu (Junwen wang)
Date: Wed, 20 Aug 2003 14:22:22 -0400 (EDT)
Subject: [R] run R in non interactive mode
Message-ID: <Pine.OSF.4.53.0308201419080.645804@gs873ps>

Hi, there
I want to run R in a non interactive mode, can I do that? say I have a
script test.R, how can run it?

thanks
John



From bates at stat.wisc.edu  Wed Aug 20 20:32:34 2003
From: bates at stat.wisc.edu (Douglas Bates)
Date: Wed, 20 Aug 2003 18:32:34 -0000
Subject: [R] run R in non interactive mode
In-Reply-To: <Pine.OSF.4.53.0308201419080.645804@gs873ps>
References: <Pine.OSF.4.53.0308201419080.645804@gs873ps>
Message-ID: <6r8ypo9o2k.fsf@bates4.stat.wisc.edu>

Junwen wang <junwen at astro.ocis.temple.edu> writes:

> Hi, there
> I want to run R in a non interactive mode, can I do that? say I have a
> script test.R, how can run it?

check ?BATCH



From emb7 at st-andrews.ac.uk  Wed Aug 20 20:37:59 2003
From: emb7 at st-andrews.ac.uk (Martin Biuw)
Date: Wed, 20 Aug 2003 19:37:59 +0100
Subject: [R] (no subject)
Message-ID: <oprt7vdli08xmvrg@gatty.st-and.ac.uk>

Hello,
Is there a simple way to modify the circ.mean function in the CircStats 
package to include a vector of weights to obtain a weighted average angle?

Thanks!

Martin

-- 
Martin Biuw
Sea Mammal Research Unit
Gatty Marine Laboratory, University of St Andrews
St Andrews, Fife KY16 8PA
Scotland
Ph: +44-(0)1334-462637
Fax: +44-(0)1334-462632
Web: http://smub.st.and.ac.uk



From junwen at astro.ocis.temple.edu  Wed Aug 20 21:01:26 2003
From: junwen at astro.ocis.temple.edu (Junwen wang)
Date: Wed, 20 Aug 2003 15:01:26 -0400 (EDT)
Subject: [R] SJava in R
Message-ID: <Pine.OSF.4.53.0308201441510.654665@gs873ps>

Hi,
Did anyone sucessfully install SJava package to R and was able to call R
function from java in redhat linux8.0? I tried several days it still give
me error either in libR.so or libjvm.so. for example, I can compile
JavaRCall.java(a example with the SJava package) without problem. When
I run it, it can connect to R and accomplish part of results, but fail for
other calls. The outputs are:
[junwen at chem250b 09]$ java JavaRCall
Loading RInterpreter library

R : Copyright 2003, The R Development Core Team
Version 1.7.1  (2003-06-16)

R is free software and comes with ABSOLUTELY NO WARRANTY.
You are welcome to redistribute it under certain conditions.
Type `license()' or `licence()' for distribution details.

R is a collaborative project with many contributors.
Type `contributors()' for more information.

Type `demo()' for some demos, `help()' for on-line help, or
`help.start()' for a HTML browser interface to help.
Type `q()' to quit R.

[Previously saved workspace restored]

[search]
.GlobalEnv
package:methods
package:ctest
package:mva
package:modreg
package:nls
package:ts
Autoloads
package:base
[objects]
^
~
<
<<-
<=
<-
=
==
>
>=
|
||
-
:
::
!
!=
?
/
(
[
[<-
[[
[[<-
{
@
$
$<-
*
&
&&
%/%
%*%
%%
+
abbreviate
abline
abs
acos
acosh
add1
add1.default
add1.glm
add1.lm
add1.mlm
add.scope
addTaskCallback
aggregate
aggregate.data.frame
aggregate.default
aggregate.ts
agrep
AIC
AIC.default
AIC.logLik
alias
.Alias
alias.formula
alias.lm
alist
all
all.equal
all.equal.character
all.equal.default
all.equal.factor
all.equal.formula
all.equal.language
all.equal.list
all.equal.numeric
all.equal.POSIXct
all.names
all.vars
anova
anova.glm
anova.glmlist
anova.glm.null
anovalist.lm
anova.lm
anova.lmlist
anova.lm.null
anova.mlm
any
aov
aperm
append
apply
approx
approxfun
apropos
Arg
args
array
arrows
as.array
as.call
as.character
as.character.default
as.character.factor
as.character.octmode
as.character.POSIXt
 ...
yinch
zapsmall
zip.file.extract
function call [objects('package:base')]
^
~
<
<<-
<=
<-
=
==
>
>=
|
||
-
:
::
!
!=
?
/
(
[
[<-
[[
[[<-
{
@
$
$<-
*
&
&&
%/%
%*%
%%
+
abbreviate
abline
abs
acos
acosh
add1
add1.default
add1.glm
add1.lm
add1.mlm
add.scope
addTaskCallback
aggregate
aggregate.data.frame
aggregate.default
aggregate.ts
agrep
AIC
AIC.default
AIC.logLik
alias
.Alias
alias.formula
alias.lm
alist
all
all.equal
all.equal.character
all.equal.default
all.equal.factor
all.equal.formula
all.equal.language
all.equal.list
all.equal.numeric
all.equal.POSIXct
all.names
all.vars
anova
anova.glm
anova.glmlist
anova.glm.null
anovalist.lm
anova.lm
anova.lmlist
anova.lm.null
anova.mlm
any
aov
aperm
append
apply
approx
approxfun
apropos
Arg
args
array
arrows
as.array
as.call
as.character
as.character.default
as.character.factor
as.character.octmode
as.character.POSIXt
 ...
yinch
zapsmall
zip.file.extract
[seq(1, 10)]

Unexpected Signal : 11 occurred at PC=0x40232040
Function=(null)+0x40232040
Library=/usr/java/j2sdk/j2sdk1.4.2/jre/lib/i386/client/libjvm.so

NOTE: We are unable to locate the function name symbol for the error
      just occurred. Please refer to release documentation for possible
      reason and solutions.


Current Java thread:
        at org.omegahat.R.Java.REvaluator.call(Native Method)
        at org.omegahat.R.Java.REvaluator.call(REvaluator.java:137)
        at org.omegahat.R.Java.REvaluator.call(REvaluator.java:127)
        at org.omegahat.R.Java.REvaluator.call(REvaluator.java:118)
        at JavaRCall.main(JavaRCall.java:43)

Dynamic libraries:
08048000-0804e000 r-xp 00000000 03:05 2109873
/usr/java/j2sdk/j2sdk1.4.2/bin/java
...<ignored>
Heap at VM Abort:
Heap
 def new generation   total 576K, used 227K [0x44750000, 0x447f0000,
0x44c30000)  eden space 512K,  31% used [0x44750000, 0x44778e08,
0x447d0000)
  from space 64K, 100% used [0x447d0000, 0x447e0000, 0x447e0000)
  to   space 64K,   0% used [0x447e0000, 0x447e0000, 0x447f0000)
 tenured generation   total 1408K, used 268K [0x44c30000, 0x44d90000,
0x48750000)
   the space 1408K,  19% used [0x44c30000, 0x44c73350, 0x44c73400,
0x44d90000)
 compacting perm gen  total 4096K, used 2298K [0x48750000, 0x48b50000,
0x4c750000)
   the space 4096K,  56% used [0x48750000, 0x4898e840, 0x4898ea00,
0x48b50000)

Local Time = Wed Aug 20 14:48:44 2003
Elapsed Time = 3
#
# HotSpot Virtual Machine Error : 11
# Error ID : 4F530E43505002EF
# Please report this error at
# http://java.sun.com/cgi-bin/bugreport.cgi
#
# Java VM: Java HotSpot(TM) Client VM (1.4.2-b28 mixed mode)
#
# An error report file has been saved as hs_err_pid3319.log.
# Please refer to the file for further information.
#
Aborted

Can anyone help me with this?

Thanks
John



From arrayprofile at yahoo.com  Wed Aug 20 22:06:58 2003
From: arrayprofile at yahoo.com (array chip)
Date: Wed, 20 Aug 2003 13:06:58 -0700 (PDT)
Subject: [R] 4 parameter logistic model
Message-ID: <20030820200658.18345.qmail@web41212.mail.yahoo.com>

Hi, I am trying to fit a 4-parameter logistic model to
my gradient data using nls. I tried to specify the
model directly in the nls formula and also tried to
use the self-start function SSfpl. For the following
data, the first method worked, but the second didn't.
I thought both ways were equivalent, can anyone tells
me why?

>
test=data.frame(cbind(conc=c(13294,3940,1170,346,102,30.20,8.94,2.65,13294,3940,1170,346,102,30.20,8.94,2.65),
signal=c(2609,487,110,35,17.5,16,11,12.5,2682,292.5,51.5,25.5,14,11,14,15)))

>
nls(log(signal)~A+(B-A)/(1+exp((xmid-log(conc))/scal)),data=test,start
= list(A=log(5), B=log(3000),
xmid=log(6000),scal=0.8))

Residual sum of squares : 0.6649545 
parameters:
        A        B     xmid     scal 
 2.494311 10.92275 8.752043 1.308763
formula: log(signal) ~ A + (B - A)/(1 + exp((xmid -
log(conc))/scal)) 
16 observations

>
nls(log(signal)~SSfpl(log(conc),log(5),log(3000),log(6000),0.8),
data=test)

Error in nlsModel(formula, mf, start) : singular
gradient matrix at initial parameter estimates


Thanks!



From dmurdoch at pair.com  Wed Aug 20 22:42:24 2003
From: dmurdoch at pair.com (Duncan Murdoch)
Date: Wed, 20 Aug 2003 16:42:24 -0400
Subject: [R] (no subject)
In-Reply-To: <oprt7vdli08xmvrg@gatty.st-and.ac.uk>
References: <oprt7vdli08xmvrg@gatty.st-and.ac.uk>
Message-ID: <idl7kvsmh6sh3o9m1jnpneo7q2m1pjm1el@4ax.com>

On Wed, 20 Aug 2003 19:37:59 +0100, Martin Biuw
<emb7 at st-andrews.ac.uk> wrote :

>Hello,
>Is there a simple way to modify the circ.mean function in the CircStats 
>package to include a vector of weights to obtain a weighted average angle?

This should do it:

circ.weighted.mean <- function (x,w) 
{
    sinr <- sum(w*sin(x))
    cosr <- sum(w*cos(x))
    circmean <- atan(sinr, cosr)
    circmean
}

I was surprised that atan worked with 2 arguments; it's documented to
use only 1.  According to the docs, you're supposed to use atan2 if
you want the two argument arctangent.

Duncan Murdoch



From maj at stats.waikato.ac.nz  Wed Aug 20 23:16:26 2003
From: maj at stats.waikato.ac.nz (Murray Jorgensen)
Date: Thu, 21 Aug 2003 09:16:26 +1200
Subject: [R] Interlacing two vectors
Message-ID: <3F43E52A.1090100@stats.waikato.ac.nz>

I want to interlace two vectors. This I can do:

 > x <- 1:4
 > z <- x+0.5
 > as.vector(t(cbind(x,z)))
[1] 1.0 1.5 2.0 2.5 3.0 3.5 4.0 4.5

but this seems rather inelegant. Any suggestions?

Murray

-- 
Dr Murray Jorgensen      http://www.stats.waikato.ac.nz/Staff/maj.html
Department of Statistics, University of Waikato, Hamilton, New Zealand
Email: maj at waikato.ac.nz                                Fax 7 838 4155
Phone  +64 7 838 4773 wk    +64 7 849 6486 home    Mobile 021 1395 862



From p.dalgaard at biostat.ku.dk  Wed Aug 20 23:35:26 2003
From: p.dalgaard at biostat.ku.dk (Peter Dalgaard BSA)
Date: Wed, 20 Aug 2003 21:35:26 -0000
Subject: [R] Interlacing two vectors
In-Reply-To: <3F43E52A.1090100@stats.waikato.ac.nz>
References: <3F43E52A.1090100@stats.waikato.ac.nz>
Message-ID: <x24r0cqa2a.fsf@biostat.ku.dk>

Murray Jorgensen <maj at stats.waikato.ac.nz> writes:

> I want to interlace two vectors. This I can do:
> 
>  > x <- 1:4
>  > z <- x+0.5
>  > as.vector(t(cbind(x,z)))
> [1] 1.0 1.5 2.0 2.5 3.0 3.5 4.0 4.5
> 
> but this seems rather inelegant. Any suggestions?

Well, there's as.vector(rbind(x,z)) at least... I don't think things
get more elegant than that.

-- 
   O__  ---- Peter Dalgaard             Blegdamsvej 3  
  c/ /'_ --- Dept. of Biostatistics     2200 Cph. N   
 (*) \(*) -- University of Copenhagen   Denmark      Ph: (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)             FAX: (+45) 35327907



From jerome at hivnet.ubc.ca  Wed Aug 20 23:37:06 2003
From: jerome at hivnet.ubc.ca (Jerome Asselin)
Date: Wed, 20 Aug 2003 14:37:06 -0700
Subject: [R] Interlacing two vectors
In-Reply-To: <3F43E52A.1090100@stats.waikato.ac.nz>
References: <3F43E52A.1090100@stats.waikato.ac.nz>
Message-ID: <200308202136.OAA21222@hivnet.ubc.ca>


as.vector(rbind(x,z))    or     c(rbind(x,y))

saves you one step. Don't know if there's a better solution.

HTH,
Jerome

On August 20, 2003 02:16 pm, Murray Jorgensen wrote:
> I want to interlace two vectors. This I can do:
>  > x <- 1:4
>  > z <- x+0.5
>  > as.vector(t(cbind(x,z)))
>
> [1] 1.0 1.5 2.0 2.5 3.0 3.5 4.0 4.5
>
> but this seems rather inelegant. Any suggestions?
>
> Murray



From emb7 at st-andrews.ac.uk  Wed Aug 20 20:37:59 2003
From: emb7 at st-andrews.ac.uk (Martin Biuw)
Date: Wed, 20 Aug 2003 19:37:59 +0100
Subject: [R] Weighted circular mean
Message-ID: <oprt7vdli08xmvrg@gatty.st-and.ac.uk>

Hello,
Once again, I posted a message without a subject line. Sorry.... here is 
the question again.

Is there a simple way to modify the circ.mean function in the CircStats 
package to include a vector of weights to obtain a weighted average angle?

Thanks!

Martin

-- 
Martin Biuw
Sea Mammal Research Unit
Gatty Marine Laboratory, University of St Andrews
St Andrews, Fife KY16 8PA
Scotland
Ph: +44-(0)1334-462637
Fax: +44-(0)1334-462632
Web: http://smub.st.and.ac.uk



From berwin at maths.uwa.edu.au  Thu Aug 21 03:49:47 2003
From: berwin at maths.uwa.edu.au (Berwin Turlach)
Date: Thu, 21 Aug 2003 09:49:47 +0800
Subject: [R] R is mentioned on Linux Today
Message-ID: <16196.9531.162505.115008@localhost.localdomain>

Hi all,

people who don't follow Linux Today regularly may want to check out:

        http://linuxtoday.com/developer/2003082000626OSSVDV

My apologies if this is considered spam.

Cheers,

        Berwin

========================== Full address ============================
Berwin A Turlach                      Tel.: +61 (8) 9380 3338 (secr)   
School of Mathematics and Statistics        +61 (8) 9380 3383 (self)      
The University of Western Australia   FAX : +61 (8) 9380 1028
35 Stirling Highway                   
Crawley WA 6009                e-mail: berwin at maths.uwa.edu.au
Australia                        http://www.maths.uwa.edu.au/~berwin



From kjetil at entelnet.bo  Thu Aug 21 04:37:06 2003
From: kjetil at entelnet.bo (kjetil brinchmann halvorsen)
Date: Wed, 20 Aug 2003 22:37:06 -0400
Subject: [R] Interlacing two vectors
In-Reply-To: <3F43E52A.1090100@stats.waikato.ac.nz>
Message-ID: <3F43F812.27173.13D552E@localhost>

On 21 Aug 2003 at 9:16, Murray Jorgensen wrote:

Hola!

I'm not sure if this is better, but if we can interlace first
1:n with (n+1):2n the rest is indexing:

> x <- 1:10
> y <- 1:10 + 0.5

> interl <- function(n) {
+    res <- numeric(2*n)
+    for (j in 1:n) {
+         res[2*j-1] <- j
+         res[2*j] <- n+j }
     res
+ }
> interl(4)
[1] 1 5 2 6 3 7 4 8
> c(x,y)[interl(10)]
 [1]  1.0  1.5  2.0  2.5  3.0  3.5  4.0  4.5  5.0  5.5  6.0  6.5  7.0 
 7.5  8.0
[16]  8.5  9.0  9.5 10.0 10.5 

You can do some testing to se what is fastest.

Kjetil Halvorsen


> I want to interlace two vectors. This I can do:
> 
>  > x <- 1:4
>  > z <- x+0.5
>  > as.vector(t(cbind(x,z)))
> [1] 1.0 1.5 2.0 2.5 3.0 3.5 4.0 4.5
> 
> but this seems rather inelegant. Any suggestions?
> 
> Murray
> 
> -- 
> Dr Murray Jorgensen      http://www.stats.waikato.ac.nz/Staff/maj.html
> Department of Statistics, University of Waikato, Hamilton, New Zealand
> Email: maj at waikato.ac.nz                                Fax 7 838 4155
> Phone  +64 7 838 4773 wk    +64 7 849 6486 home    Mobile 021 1395 862
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help



From ok at cs.otago.ac.nz  Thu Aug 21 05:34:41 2003
From: ok at cs.otago.ac.nz (Richard A. O'Keefe)
Date: Thu, 21 Aug 2003 15:34:41 +1200 (NZST)
Subject: [R] Interlacing two vectors
Message-ID: <200308210334.h7L3Yf1o132231@atlas.otago.ac.nz>

Murray Jorgensen <maj at stats.waikato.ac.nz> wrote:
	I want to interlace two vectors.

How, precisely, do you want to do this?

Here are two vectors x and y of the same length:
    x <- c(1,2,3)
    y <- c(4,5,6)
The simplest way I can think of to interleave them is
    as.vector(rbind(x,y))
=>  1 4 2 5 3 6
This will work for any number of vectors:    
    z <- c(7,8,9)
    as.vector(rbind(x,y,z))
=>  1 4 7 2 5 8 3 6 9

Another approach is this:
    r <- 1:(2*length(x))
    i <- r%%2
    r[i == 1] <- x
    r[i == 0] <- y
    r
=>  1 4 2 5 3 6
That generalises too:
    r <- 1:(3*length(x))
    i <- r%%3
    r[i == 1] <- x
    r[i == 2] <- y
    r[i == 0] <- z
    r
=>  1 4 7 2 5 8 3 6 9

I prefer as.vector(rbind(x,y,...))); as it uses fewer R-level operations
and should if anything turn over less space, I'd expect it to be faster
as well as simpler.



From ok at cs.otago.ac.nz  Thu Aug 21 07:21:50 2003
From: ok at cs.otago.ac.nz (Richard A. O'Keefe)
Date: Thu, 21 Aug 2003 17:21:50 +1200 (NZST)
Subject: [R] Diamond graphs
Message-ID: <200308210521.h7L5LoMr134708@atlas.otago.ac.nz>

I apologise for starting a new thread, but we had a mail problem and I
don't have the original message to refer to.

Someone mentioned the new "Diamond Graphs" invented at Johns Hopkins.
I haven't see the August 2003 issue of The American Statistician yet,
but I _have_ read the press release.

The press release is a bit of a stunner.  I quote:
    "Who would have thought we would still be inventing
     new methods of graphing in the twenty-first century?"

A1: anyone with a functioning brain?
A2: anyone who didn't sleep through the twentieth century?

I can summarise diamond graphs this way:
    (1) Write a 2D table.		- very old idea
    (2) Instead of numbers, put blobs of
	some kind where the size shows you
	the importance.			- at least 3000 years old.
    (3) Rotate the table widdershins 45
	degrees				- swiped from 3D displays
    (4) Replace the blobs by truncated diamonds;
        height of bar or area of polygon or something
        shows value.			- NOVELTY
    (5) Notice that it's not all that readable,
	so put the numbers back.
	
The fact that someone would try to patent this strikes me as outrageous;
the actual amount of novelty is so tiny.

For R, I don't think it matters, because I think that diamond graphs
are a bad idea.  Let me try to explain why.

In effect, you have a 2D bar chart, where each bar occupies a rather
small diamond-shaped cell.  The bars are sort of squashed to fit in.

ASCII graphic of a typical bar:
             |
          _______
         /       \
     __ /         \ __
        \         /
         \_______/
             |

These bars have two axes of symmetry: a vertical mirror axis and
a horizontal mirror axis.  The lines outside the hexagon above
show the symmetry axes.  I shall use horizontal and vertical coordinates
running form -1 to +1, and define the height of the bar to be the amount
that the bar extends above the horizontal: 0 <= h <= 1.  When h = 0,
no polygon is shown.  When h = 1, the polygon is a square occupying the
whole cell.  (I don't actually understand this; in the illustration in
http://www.jhu.edu/~gazette/2003/18aug03/18graph.html
there is _always_ a margin, except when the square is full.  It doesn't
really spoil my point.)

Total height of bar:		2h
Width of bar top:		2 - 2h
Total width of bar:		2
Diagonal (corner to corner):	2.sqrt(2h^2 - 2h + 1)
Area of bar:			4h - 2h^2

The information-bearing units don't just change *size*, and they don't
just *stretch* in one dimension (like normal histogram bars), they change
shape much more drastically than that.  I don't see how this can make it
easy to relate one bar to another; your impression of how *much* bigger
one bar is than another depends on which visual aspect you attend to.

As Tufte (Visual Display of Quantitative Information) puts it:
    There are considerable ambiguities in how people perceive
    a two-dimensional surface and then convert that perception
    into a one-dimensional number.
(p71)  

Combine this with the small "dynamic range" available (because you have
lots of cells to fit in), and there doesn't seem to be any advantage
over just using discs of various sizes (which is a fairly old technique;
you'll find a similar idea in ABCs of EDA).

Oh yes, you'll find tables with entries shown by amount of ink
on page 174 of Tufte's Visual Display...

I'm assuming here that the use of truncated squares ("hexagons")
is considered important.  The Gazette web page above has some other
examples showing
(A) plain diamonds that change size, not shape
(B) "diamonds" without margins, that change shape as described above
(C) "diamonds" with margins, that change shape as described above
(D) diamonds with fixed width spanning the cell, where the height
    changes
(E) something with a rectangle, a cell, and two "bow ties", each in a cell.
    I have no idea what that the different shapes mean.
Since the text says "The researcher experimented with other shapes but
sound that the six-sided polygon was the only shape to represent the
outcomes equally within the grid as it expanded", I surmise that A, B,
D, and E are meant to be understood as "bad examples" that the diamonds
of C improve on.

It is not clear to me what the advantage of turning the diagram 45
degrees widdershins is supposed to be.  I'm assuming here (and I don't
even play an expert on TV) that vertical patterns are easier to grasp
than diagonal ones.  Now, the main example on that web page (and in the
PDF file you can get to from the URI posted in the original message)
can be summarised as
    Systolic >= 180	BIG
    Systolic 160..179	moderate
    Everything else	pretty much small
which is quite easy to see if "systolic" is the horizontal or vertical
axis, but when the vertical axis is "systolic + diastolic" and the
horizontal is "diastolic - systolic" (a bit of hand-waving here, because
the buckets aren't the same width, so + and - are a bit dodgy), it gets
rather harder to see.

Turn to the examples on page 174 of Tufte again, where the number of
values for one variable (6) is not the same as the number for the other (16).
Would that look good if you turned it 45 degrees?  The diamond graph appears
to rely on the two explanatory variables having nearly the same number of
values, which would seem to limit its usefulness.

What would happen if we turn the diagram back so that the axes are
horizontal and vertical?  Well, with square (or rectangular) cells
we could put _several_ vertical bars in each cell, and so display
2 or 3 variables on the same 2d grid, something which would be very
hard to do in a diamond graph.

In short, it looks to me as though "diamond graphs" are something R
is better off without.



From HADASSA.BRUNSCHWIG at Roche.COM  Thu Aug 21 08:29:50 2003
From: HADASSA.BRUNSCHWIG at Roche.COM (Brunschwig, Hadassa {PDMM~Basel})
Date: Thu, 21 Aug 2003 08:29:50 +0200
Subject: [R] Changing Color of Graphics Device
Message-ID: <BC6A439CD6835749A9C7B8D8F041DFA88AFC9F@rbamsem1.emea.roche.com>

Hello!

Several of us here have tried to change the dark grey background color of the graphics device, without success. This grey background color only so far appears for Trellis plots. All other plots are shown with white background color. Is there any way to change this color?

Regards

Dassy



From f.mercier at fr.fournierpharma.com  Thu Aug 21 08:38:53 2003
From: f.mercier at fr.fournierpharma.com (f.mercier@fr.fournierpharma.com)
Date: Thu, 21 Aug 2003 08:38:53 +0200
Subject: =?iso-8859-1?Q?R=E9f=2E_=3A_[R]_Changing_Color_of_Graphics_Device?=
Message-ID: <OF7C86FE69.425596D2-ONC1256D89.00242B0E@fournier.fr>


Hi,
If you want to change the background color of the trellis plots, you can
try :

> trellis.device (bg="white")
... or to obtain a gray with 97% white and 3% black :
> trellis.device (bg=gray(0.97))

Kinds regards,
Fran?ois




                                                                                                                                            
                      "Brunschwig, Hadassa                                                                                                  
                      {PDMM~Basel}"                 Pour :   r-help at stat.math.ethz.ch                                                       
                      <HADASSA.BRUNSCHWIG at R         cc :                                                                                    
                      oche.COM>                     Objet :  [R] Changing Color of Graphics Device                                          
                      Envoy? par :                                                                                                          
                      r-help-bounces at stat.m                                                                                                 
                      ath.ethz.ch                                                                                                           
                                                                                                                                            
                                                                                                                                            
                      21/08/03 08:29                                                                                                        
                                                                                                                                            
                                                                                                                                            




Hello!

Several of us here have tried to change the dark grey background color of
the graphics device, without success. This grey background color only so
far appears for Trellis plots. All other plots are shown with white
background color. Is there any way to change this color?

Regards

Dassy

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://www.stat.math.ethz.ch/mailman/listinfo/r-help



From ligges at statistik.uni-dortmund.de  Thu Aug 21 08:45:51 2003
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Thu, 21 Aug 2003 08:45:51 +0200
Subject: [R] Weighted circular mean
In-Reply-To: <oprt7vdli08xmvrg@gatty.st-and.ac.uk>
References: <oprt7vdli08xmvrg@gatty.st-and.ac.uk>
Message-ID: <3F446A9F.4080902@statistik.uni-dortmund.de>

Martin Biuw wrote:
> Hello,
> Once again, I posted a message without a subject line. Sorry.... here is 
> the question again.
> 
> Is there a simple way to modify the circ.mean function in the CircStats 
> package to include a vector of weights to obtain a weighted average angle?

Two comments:

1)
  > circ.mean
  function (x)
  {
     sinr <- sum(sin(x))
     cosr <- sum(cos(x))
     circmean <- atan(sinr, cosr)
     circmean
  }

The function is not that hard to understand, so you might be able to 
answer your question yourself, don't you?


2)
Please contact the author of a contributed package in order to submit a 
wishlist. He/she knows the functions much better and is the only one who 
can change things within the package, regularly.

Uwe Ligges



From ligges at statistik.uni-dortmund.de  Thu Aug 21 08:22:44 2003
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Thu, 21 Aug 2003 08:22:44 +0200
Subject: [R] question about simulation.
In-Reply-To: <20030820174212.74661.qmail@web41210.mail.yahoo.com>
References: <20030820174212.74661.qmail@web41210.mail.yahoo.com>
Message-ID: <3F446534.6000105@statistik.uni-dortmund.de>

Lily wrote:
> The problem has been solved. It is my mistake for not
> define the number of monte.carlo simulation when I am
> using mle.cv. 
> 
> Thank you all for your help!

My last comment:
It is your mistake not to specify required arguments, but there *is* a 
bug in wle. You should get an error message instead of that crash ...

Uwe Ligges


> --- Spencer Graves <spencer.graves at PDF.COM> wrote:
> 
>>What version of R under what operating system with
>>how much memory? 
>>What R functions are you using?  Can you do a
>>traceback()?  Also, have 
>>you tried "www.r-project.org" -> search -> "R site
>>search"?
>>
>>hope this helps.  spencer graves
>>
>>Lily wrote:
>>
>>>I am running a 1000 simulations, it works for 2
>>>simulations. However, I get the following error
>>>message whenever I run it more than 3 times:
>>>
>>>"The instruction at '0*11044080' referenced memory
>>
>>at
>>
>>>"o*3ff00000". The memory could not be "written".
>>>and, I can also get something like "exception:
>>
>>access
>>
>>>violation (0*c0000005). Address:0*11044080".
>>>
>>>Anybody knows what's going on? Thanks a lot!
>>>
>>>______________________________________________
>>>R-help at stat.math.ethz.ch mailing list
>>>
>>
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> 
>>
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help



From ligges at statistik.uni-dortmund.de  Thu Aug 21 08:56:40 2003
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Thu, 21 Aug 2003 08:56:40 +0200
Subject: [R] Changing Color of Graphics Device
In-Reply-To: <BC6A439CD6835749A9C7B8D8F041DFA88AFC9F@rbamsem1.emea.roche.com>
References: <BC6A439CD6835749A9C7B8D8F041DFA88AFC9F@rbamsem1.emea.roche.com>
Message-ID: <3F446D28.4090507@statistik.uni-dortmund.de>

Brunschwig, Hadassa {PDMM~Basel} wrote:

> Hello!
> 
> Several of us here have tried to change the dark grey background color of the graphics device, without success. This grey background color only so far appears for Trellis plots. All other plots are shown with white background color. Is there any way to change this color?
> 
> Regards
> 
> Dassy


Several users have already asked this question, hence you'll find the 
answer in the mailing list archives.

See ?trellis.device which tells you about its arguments "color" and 
"theme" and that you can set
options(lattice.theme = "col.whitebg")

Uwe Ligges



From ozric at web.de  Thu Aug 21 09:06:21 2003
From: ozric at web.de (Christian Schulz)
Date: Thu, 21 Aug 2003 09:06:21 +0200
Subject: [R] filter factors with min. freq
Message-ID: <001601c367b2$c7785ea0$d200a8c0@pc>

Hi,

i use a data.frame with ~ 80.000 observations
and one attribute is a factor with
~ 7300 levels. Is there a easy step which allow
me to filter out the the data with minimum frequencies i.e. 20
cases per  level.
So existing levels with < 20 cases in this factor attribute  are deleted
from data.frame.

many thanks and regards,
christian



From ligges at statistik.uni-dortmund.de  Thu Aug 21 10:02:29 2003
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Thu, 21 Aug 2003 10:02:29 +0200
Subject: [R] filter factors with min. freq
In-Reply-To: <001601c367b2$c7785ea0$d200a8c0@pc>
References: <001601c367b2$c7785ea0$d200a8c0@pc>
Message-ID: <3F447C95.2060904@statistik.uni-dortmund.de>

Christian Schulz wrote:

> Hi,
> 
> i use a data.frame with ~ 80.000 observations
> and one attribute is a factor with
> ~ 7300 levels. Is there a easy step which allow
> me to filter out the the data with minimum frequencies i.e. 20
> cases per  level.
> So existing levels with < 20 cases in this factor attribute  are deleted
> from data.frame.
> 
> many thanks and regards,
> christian

Why not calculating a table for that factor and removing those levels 
with n_i < 20 ?

Uwe Ligges



From azzalini at stat.unipd.it  Thu Aug 21 10:09:42 2003
From: azzalini at stat.unipd.it (Adelchi Azzalini)
Date: Thu, 21 Aug 2003 10:09:42 +0200
Subject: [R] Method of L-BFGS-B of optim evaluate function outside of box
	constraints
In-Reply-To: <Pine.GSO.4.55.0308200913530.14989@student>
References: <Pine.GSO.4.55.0308200913530.14989@student>
Message-ID: <20030821080942.849C07CA833@tango.stat.unipd.it>

On Wednesday 20 August 2003 15:44, Shengqiao Li wrote:
> Hi, R guys:
>
> I'm using L-BFGS-B method of optim for minimization problem. My function
> called besselI function which need non-negative parameter and the besselI
> will overflow if the parameter is too large. So I set the constraint box
> which is reasonable for my problem. But the point outside the box was
> test, and I got error. My program and the error follows. This program
> depends on CircStats package.
>
>
> Anyone has any idea about this?
>

No idea... I can only say that a  similar behaviour of optim
with  method="L-BFGS-B" (i.e. evaluation of the function 
outside the lower/upper limits) has occurred to me too. 
It is a rare but possible behaviour.

Last June, I have sent a full description of the problem to 
r-bugs at r-project.org

Regards,

Adelchi Azzalini


-- 
Adelchi Azzalini  <azzalini at stat.unipd.it>
Dipart.Scienze Statistiche, Universit? di Padova, Italia
http://azzalini.stat.unipd.it/
(please, no ms-word/ms-excel/alike attachments)



From maechler at stat.math.ethz.ch  Thu Aug 21 10:21:39 2003
From: maechler at stat.math.ethz.ch (Martin Maechler)
Date: Thu, 21 Aug 2003 10:21:39 +0200
Subject: [R] R is mentioned on Linux Today
In-Reply-To: <16196.9531.162505.115008@localhost.localdomain>
References: <16196.9531.162505.115008@localhost.localdomain>
Message-ID: <16196.33043.177331.642461@gargle.gargle.HOWL>

>>>>> "BeT" == Berwin Turlach <berwin at maths.uwa.edu.au>
>>>>>     on Thu, 21 Aug 2003 09:49:47 +0800 writes:

    BeT> Hi all, people who don't follow Linux Today regularly
    BeT> may want to check out:

    BeT>
    BeT> http://linuxtoday.com/developer/2003082000626OSSVDV

    BeT> My apologies if this is considered spam.

Definitely not.
As a matter of fact, these are links to two articles the first
of which (IBM Developerworks) was already posted here about
two days ago.

The reason I'm "wasting bandwidth" here is the 2nd one,
from the UK Linuxuser journal,
     http://www.linuxuser.co.uk/articles/issue32/lud32-R.html
This one is really amazing by its quality.  It's author, Ramin
Nakisa (I've BCC'ed him), really writes like a professional
journalist but at the same time was able to get very good inside
information particularly about the history of S and R -- I found
that he even contributed a (valid!) bug report for R several
years ago.  
Thank you, Ramin!

Regards,

Martin Maechler <maechler at stat.math.ethz.ch>	http://stat.ethz.ch/~maechler/
Seminar fuer Statistik, ETH-Zentrum  LEO C16	Leonhardstr. 27
ETH (Federal Inst. Technology)	8092 Zurich	SWITZERLAND
phone: x-41-1-632-3408		fax: ...-1228			<><



From mkondrin at hppi.troitsk.ru  Thu Aug 21 22:09:14 2003
From: mkondrin at hppi.troitsk.ru (M.Kondrin)
Date: Thu, 21 Aug 2003 13:09:14 -0700
Subject: [R] Filled triangles in lattice graphics?
In-Reply-To: <3F436F55.3050702@eg.umu.se>
References: <3F436F55.3050702@eg.umu.se>
Message-ID: <3F4526EA.4020703@hppi.troitsk.ru>

Hans Gardfjell wrote:
> Dear R users,
> 
> I can get a filled triangle pointing upwards by specifying pch=17 in 
> xyplot or lpoints, but how do I get a filled triangle that points 
> downwards?
> 
> In the standard plot function it's possible to use 
> plot(x,y,pch=25,bg="black"), but bg= doesn't seem to work with lattice 
> and lpoints.
> 
> Thanks,
> 
> Hans Gardfjell
> Ecology and Environmental Science
> Ume? University, Sweden
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> 
> 
In grid package that is underling package of lattice fill color is set 
through "gp" parameter. Try this 
plot(x,y,pch=25,gp=gpar(fill="black",col="black"))



From HADASSA.BRUNSCHWIG at Roche.COM  Thu Aug 21 11:51:01 2003
From: HADASSA.BRUNSCHWIG at Roche.COM (Brunschwig, Hadassa {PDMM~Basel})
Date: Thu, 21 Aug 2003 11:51:01 +0200
Subject: [R] graphics device
Message-ID: <BC6A439CD6835749A9C7B8D8F041DFA88AFCA0@rbamsem1.emea.roche.com>

Hi all,

well i know this was probably already posted many times, couldnt find anything about it though. This is a beginner problem. I have a Trellis plot which is very large, i.e. it only shows me the last few panels (after going automatically through the first ones and stopping at the last few). When i scrole with PageUp or Page down it shows me the panels of a graph i did last time but not of the graph i plotted now. I also tried to use the dev.next() etc. functions but the showing doesnt change. I guess i dont really understand how these functions work but i would like to print the whole set of panels.

Thanks for reply

Dassy



From ligges at statistik.uni-dortmund.de  Thu Aug 21 12:04:46 2003
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Thu, 21 Aug 2003 12:04:46 +0200
Subject: [R] graphics device
In-Reply-To: <BC6A439CD6835749A9C7B8D8F041DFA88AFCA0@rbamsem1.emea.roche.com>
References: <BC6A439CD6835749A9C7B8D8F041DFA88AFCA0@rbamsem1.emea.roche.com>
Message-ID: <3F44993E.4040705@statistik.uni-dortmund.de>

Brunschwig, Hadassa {PDMM~Basel} wrote:

> Hi all,
> 
> well i know this was probably already posted many times, couldnt find anything about it though. This is a beginner problem. I have a Trellis plot which is very large, i.e. it only shows me the last few panels (after going automatically through the first ones and stopping at the last few). When i scrole with PageUp or Page down it shows me the panels of a graph i did last time but not of the graph i plotted now. I also tried to use the dev.next() etc. functions but the showing doesnt change. I guess i dont really understand how these functions work but i would like to print the whole set of panels.
> 
> Thanks for reply
> 
> Dassy

I'd recommend to generate PostScript or PDF output at first, then you 
get a document you can scroll through and printing won't be a problem.

Uwe Ligges



From apiszcz at solarrain.com  Thu Aug 21 13:04:18 2003
From: apiszcz at solarrain.com (Al Piszcz)
Date: Thu, 21 Aug 2003 07:04:18 -0400 (EDT)
Subject: [R] R scripts
Message-ID: <Pine.LNX.4.55.0308210701230.15084@l1>


I have two examples
here first is real basic answers your question, second
handles arguments. I am interested in getting a more
robust command line parser for R, perhaps using PERL GetOpt::Long
as a front end is the quickest solution.



#1 (answers your question)
===

$ more Rb
#!/bin/sh
R --quiet \
  --no-save \
  < $*


#2 adds arguments
===
$ more Rba
#!/bin/sh
##
## ARG 1 is the R program to RUN
## rest of line are arguments

if [ $# -eq 0 ] ; then
  echo
  echo "usage: Rba RPROGRAM.r ARG1 ARG2 ...."
  echo
  exit
fi


program=$1
shift
cmdargs=$*

gtime R --quiet \
  --no-save \
  $cmdargs < $program



From fharrell at virginia.edu  Thu Aug 21 13:52:29 2003
From: fharrell at virginia.edu (Frank E Harrell Jr)
Date: Thu, 21 Aug 2003 07:52:29 -0400
Subject: [R] filter factors with min. freq
In-Reply-To: <001601c367b2$c7785ea0$d200a8c0@pc>
References: <001601c367b2$c7785ea0$d200a8c0@pc>
Message-ID: <20030821075229.573af636.fharrell@virginia.edu>

In the Hmisc package see function combine.levels


On Thu, 21 Aug 2003 09:06:21 +0200
"Christian Schulz" <ozric at web.de> wrote:

> Hi,
> 
> i use a data.frame with ~ 80.000 observations
> and one attribute is a factor with
> ~ 7300 levels. Is there a easy step which allow
> me to filter out the the data with minimum frequencies i.e. 20
> cases per  level.
> So existing levels with < 20 cases in this factor attribute  are deleted
> from data.frame.
> 
> many thanks and regards,
> christian
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help


---
Frank E Harrell Jr              Prof. of Biostatistics & Statistics
Div. of Biostatistics & Epidem. Dept. of Health Evaluation Sciences
U. Virginia School of Medicine  http://hesweb1.med.virginia.edu/biostat



From e.pebesma at geog.uu.nl  Thu Aug 21 14:42:01 2003
From: e.pebesma at geog.uu.nl (Edzer J. Pebesma)
Date: Thu, 21 Aug 2003 14:42:01 +0200
Subject: [R] levelplot behaviour for panel with constants
Message-ID: <3F44BE19.4030303@geog.uu.nl>

In the example:

x = rep(c(0,0,1,1),4)
y = rep(c(0,1,0,1),4)
z = c(1,0,1,0,0,0,1,1,0,1,0,0,1,1,1,1)
f = as.factor(c(rep("a",4),rep("b",4),rep("c",4),rep("d",4)))
levelplot(z~x+y|f,data.frame(x=x,y=y,z=z,f=f))

I noted that the last ("d") plot remains empty. I guess the
reason for this is that the values are constant (1), but I consider
it more consistent if they would get the colour of 1, and would
be left blank in case they were NA's.
--
Edzer



From Jan.Verbesselt at agr.kuleuven.ac.be  Thu Aug 21 16:07:15 2003
From: Jan.Verbesselt at agr.kuleuven.ac.be (Jan Verbesselt)
Date: Thu, 21 Aug 2003 16:07:15 +0200
Subject: [R] Read date for timeserie object
Message-ID: <000001c367ed$8640ce50$1145210a@agr.ad10.intern.kuleuven.ac.be>

Dear all,

Is there a simple trick to read in data with the following format and
create a Time Serie object of it?

Date	CountOfField2
5/10/1998	7
5/11/1998	5
5/12/1998	2
5/14/1998	1
5/15/1998	1
5/19/1998	1
5/20/1998	1
5/21/1998	1
5/24/1998	2
5/25/1998	1
5/26/1998	2
....
2002
...

R should recognize that some dates are not available...(NA). You can
define start and end date Ok, and frequency= 365 is ok...but is it
possible that recognizes the gaps?

Thanks a lot,
Jan


________________________________________________________________________
__
Jan Verbesselt 
Research Associate 
Lab of Geomatics and Forest Engineering K.U. Leuven
Vital Decosterstraat 102. B-3000 Leuven Belgium 
Tel:+32-16-329750 
Fax: +32-16-329760
http://perswww.kuleuven.ac.be/~u0027178/VCard/mycard.php?name=janv
http://gloveg.kuleuven.ac.be/
________________________________________________________________________
__



From lun.li at glg.ed.ac.uk  Thu Aug 21 16:23:46 2003
From: lun.li at glg.ed.ac.uk (Lun Li)
Date: Thu, 21 Aug 2003 15:23:46 +0100 (BST)
Subject: [R] A logical comparison between two character strings in R
Message-ID: <1061475826.3f44d5f244052@staffmail.ed.ac.uk>

Dear All,

Does anyone know how to compare two character strings in R?  For eample, how 
to compare "A-1-B" with "cc-10000" in logical ?

Cheers,


Lun Li



From deepayan at stat.wisc.edu  Thu Aug 21 15:54:57 2003
From: deepayan at stat.wisc.edu (Deepayan Sarkar)
Date: Thu, 21 Aug 2003 08:54:57 -0500
Subject: [R] levelplot behaviour for panel with constants
In-Reply-To: <3F44BE19.4030303@geog.uu.nl>
References: <3F44BE19.4030303@geog.uu.nl>
Message-ID: <200308210854.57306.deepayan@stat.wisc.edu>


Looks like a bug. I'll investigate. 

Thanks,
Deepayan

On Thursday 21 August 2003 07:42, Edzer J. Pebesma wrote:
> In the example:
>
> x = rep(c(0,0,1,1),4)
> y = rep(c(0,1,0,1),4)
> z = c(1,0,1,0,0,0,1,1,0,1,0,0,1,1,1,1)
> f = as.factor(c(rep("a",4),rep("b",4),rep("c",4),rep("d",4)))
> levelplot(z~x+y|f,data.frame(x=x,y=y,z=z,f=f))
>
> I noted that the last ("d") plot remains empty. I guess the
> reason for this is that the values are constant (1), but I consider
> it more consistent if they would get the colour of 1, and would
> be left blank in case they were NA's.
> --
> Edzer
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help



From jed at tulane.edu  Thu Aug 21 16:27:53 2003
From: jed at tulane.edu (Jed Diem)
Date: Thu, 21 Aug 2003 09:27:53 -0500
Subject: [R] Use of Second Monitor Question
Message-ID: <5.1.0.14.0.20030821091054.02abd158@mail.tulane.edu>


In teaching I'd like to be able to display a R-graphics window on a wall 
projection display keeping the R-console on the computer monitor.  And so I 
ask---

Is there a way to move a graphics window out of the Rgui window to a second 
monitor leaving the R Console window in the Rgui window on the first monitor?

Either a Windows or Linux solution would be just fine.


--jed diem



From deepayan at stat.wisc.edu  Thu Aug 21 15:49:40 2003
From: deepayan at stat.wisc.edu (Deepayan Sarkar)
Date: Thu, 21 Aug 2003 08:49:40 -0500
Subject: [R] graphics device
In-Reply-To: <3F44993E.4040705@statistik.uni-dortmund.de>
References: <BC6A439CD6835749A9C7B8D8F041DFA88AFCA0@rbamsem1.emea.roche.com>
	<3F44993E.4040705@statistik.uni-dortmund.de>
Message-ID: <200308210849.40091.deepayan@stat.wisc.edu>

On Thursday 21 August 2003 05:04, Uwe Ligges wrote:
> Brunschwig, Hadassa {PDMM~Basel} wrote:
> > Hi all,
> >
> > well i know this was probably already posted many times, couldnt find
> > anything about it though. This is a beginner problem. I have a Trellis
> > plot which is very large, i.e. it only shows me the last few panels
> > (after going automatically through the first ones and stopping at the
> > last few). When i scrole with PageUp or Page down it shows me the panels
> > of a graph i did last time but not of the graph i plotted now. I also
> > tried to use the dev.next() etc. functions but the showing doesnt change.
> > I guess i dont really understand how these functions work but i would
> > like to print the whole set of panels.
> >
> > Thanks for reply
> >
> > Dassy
>
> I'd recommend to generate PostScript or PDF output at first, then you
> get a document you can scroll through and printing won't be a problem.
>
> Uwe Ligges

Also, you could set par(ask = TRUE), which will prompt you before each new 
page.

Deepayan



From lun.li at glg.ed.ac.uk  Thu Aug 21 16:09:24 2003
From: lun.li at glg.ed.ac.uk (Lun Li)
Date: Thu, 21 Aug 2003 15:09:24 +0100 (BST)
Subject: [R] A logical comparison between two character strings in R
Message-ID: <1061474964.3f44d2948a395@staffmail.ed.ac.uk>

Dear All,

Can anyone know how to compare two character strings in R?  For eample, how to 
compare "A-1-B" with "cc-10000" in logical ?

Cheers,


Lun Li



From lun.li at glg.ed.ac.uk  Thu Aug 21 16:49:32 2003
From: lun.li at glg.ed.ac.uk (Lun Li)
Date: Thu, 21 Aug 2003 15:49:32 +0100 (BST)
Subject: [R] A logical comparison between two character strings in R
In-Reply-To: <x2n0e3t6po.fsf@biostat.ku.dk>
References: <1061475826.3f44d5f244052@staffmail.ed.ac.uk>
	<x2n0e3t6po.fsf@biostat.ku.dk>
Message-ID: <1061477372.3f44dbfc418c6@staffmail.ed.ac.uk>

Thanks.

But, my question is:

> pro.name[1]
       V1
1 B-29BT3
> temp.name[1]
    V1
1 A-1H
> if(tem.name[1]==pro.name[1]){cat("ok")}

Error in Ops.factor(left, right) : Level sets of factors are different

How to compare them?


Lun



Quoting Peter Dalgaard BSA <p.dalgaard at biostat.ku.dk>:

> Lun Li <lun.li at glg.ed.ac.uk> writes:
> 
> > Dear All,
> > 
> > Does anyone know how to compare two character strings in R?  For
> eample, how 
> > to compare "A-1-B" with "cc-10000" in logical ?
> 
> Ehhh... Is it this that you want?
> 
> >  "A-1-B" == "cc-10000"
> [1] FALSE
> >  "A-1-B" <= "cc-10000"
> [1] TRUE
> >  "A-1-B" > "cc-10000"
> [1] FALSE
> >  "d-1-B" > "cc-10000"
> [1] TRUE
> 
> 
> -- 
>    O__  ---- Peter Dalgaard             Blegdamsvej 3  
>   c/ /'_ --- Dept. of Biostatistics     2200 Cph. N   
>  (*) \(*) -- University of Copenhagen   Denmark      Ph: (+45) 35327918
> ~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)             FAX: (+45) 35327907
>



From p.dalgaard at biostat.ku.dk  Thu Aug 21 16:32:15 2003
From: p.dalgaard at biostat.ku.dk (Peter Dalgaard BSA)
Date: Thu, 21 Aug 2003 14:32:15 -0000
Subject: [R] A logical comparison between two character strings in R
In-Reply-To: <1061475826.3f44d5f244052@staffmail.ed.ac.uk>
References: <1061475826.3f44d5f244052@staffmail.ed.ac.uk>
Message-ID: <x2n0e3t6po.fsf@biostat.ku.dk>

Lun Li <lun.li at glg.ed.ac.uk> writes:

> Dear All,
> 
> Does anyone know how to compare two character strings in R?  For eample, how 
> to compare "A-1-B" with "cc-10000" in logical ?

Ehhh... Is it this that you want?

>  "A-1-B" == "cc-10000"
[1] FALSE
>  "A-1-B" <= "cc-10000"
[1] TRUE
>  "A-1-B" > "cc-10000"
[1] FALSE
>  "d-1-B" > "cc-10000"
[1] TRUE


-- 
   O__  ---- Peter Dalgaard             Blegdamsvej 3  
  c/ /'_ --- Dept. of Biostatistics     2200 Cph. N   
 (*) \(*) -- University of Copenhagen   Denmark      Ph: (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)             FAX: (+45) 35327907



From ligges at statistik.uni-dortmund.de  Thu Aug 21 16:51:56 2003
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Thu, 21 Aug 2003 16:51:56 +0200
Subject: [R] Use of Second Monitor Question
In-Reply-To: <5.1.0.14.0.20030821091054.02abd158@mail.tulane.edu>
References: <5.1.0.14.0.20030821091054.02abd158@mail.tulane.edu>
Message-ID: <3F44DC8C.7080705@statistik.uni-dortmund.de>

Jed Diem wrote:
> 
> In teaching I'd like to be able to display a R-graphics window on a wall 
> projection display keeping the R-console on the computer monitor.  And 
> so I ask---
> 
> Is there a way to move a graphics window out of the Rgui window to a 
> second monitor leaving the R Console window in the Rgui window on the 
> first monitor?
> 
> Either a Windows or Linux solution would be just fine.
> 

Linux: out of the box,
Windows: Start RGui with option --sdi, or choose SDI mode in the menu 
(Edit - GUI preferences ...).

Uwe Ligges


> --jed diem
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help



From p.dalgaard at biostat.ku.dk  Thu Aug 21 16:54:29 2003
From: p.dalgaard at biostat.ku.dk (Peter Dalgaard BSA)
Date: Thu, 21 Aug 2003 14:54:29 -0000
Subject: [R] A logical comparison between two character strings in R
In-Reply-To: <1061477372.3f44dbfc418c6@staffmail.ed.ac.uk>
References: <1061475826.3f44d5f244052@staffmail.ed.ac.uk>
	<x2n0e3t6po.fsf@biostat.ku.dk>
	<1061477372.3f44dbfc418c6@staffmail.ed.ac.uk>
Message-ID: <x2ekzft5pc.fsf@biostat.ku.dk>

Lun Li <lun.li at glg.ed.ac.uk> writes:

> Thanks.
> 
> But, my question is:
> 
> > pro.name[1]
>        V1
> 1 B-29BT3
> > temp.name[1]
>     V1
> 1 A-1H
> > if(tem.name[1]==pro.name[1]){cat("ok")}
> 
> Error in Ops.factor(left, right) : Level sets of factors are different
> 
> How to compare them?

Then they are factors, not character variables. Use as.character on both
sides. 

-- 
   O__  ---- Peter Dalgaard             Blegdamsvej 3  
  c/ /'_ --- Dept. of Biostatistics     2200 Cph. N   
 (*) \(*) -- University of Copenhagen   Denmark      Ph: (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)             FAX: (+45) 35327907



From spencer.graves at pdf.com  Thu Aug 21 16:53:23 2003
From: spencer.graves at pdf.com (Spencer Graves)
Date: Thu, 21 Aug 2003 07:53:23 -0700
Subject: [R] A logical comparison between two character strings in R
References: <1061475826.3f44d5f244052@staffmail.ed.ac.uk>
Message-ID: <3F44DCE3.8090109@pdf.com>

Did you check any documentation on R, e.g., help(Comparison) or 
"www.r-project.org" -> Manuals -> "An Introdunction to R"?  In Section 
2.4 on "Logical vectors", I find, "The logical operators are <, <=, >, 
 >=, == for exact equality ... ."  Consider the following:

 > "A-1-B" == "cc-10000"
[1] FALSE

hope this helps.  spencer graves



Lun Li wrote:
> Dear All,
> 
> Does anyone know how to compare two character strings in R?  For eample, how 
> to compare "A-1-B" with "cc-10000" in logical ?
> 
> Cheers,
> 
> 
> Lun Li
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help



From petr.pikal at precheza.cz  Thu Aug 21 17:12:47 2003
From: petr.pikal at precheza.cz (Petr Pikal)
Date: Thu, 21 Aug 2003 17:12:47 +0200
Subject: [R] A logical comparison between two character strings in R
In-Reply-To: <1061477372.3f44dbfc418c6@staffmail.ed.ac.uk>
References: <x2n0e3t6po.fsf@biostat.ku.dk>
Message-ID: <3F44FD8F.28922.1AB257C@localhost>



On 21 Aug 2003 at 15:49, Lun Li wrote:

> Thanks.
> 
> But, my question is:
> 
> > pro.name[1]
>        V1
> 1 B-29BT3
> > temp.name[1]
>     V1
> 1 A-1H
> > if(tem.name[1]==pro.name[1]){cat("ok")}
> 
> Error in Ops.factor(left, right) : Level sets of factors are different

I suppose your variables are actually factors not characer vectors.

see

> temp.name
[1] B-29BT3   ccc-29BT3
Levels: B-29BT3 ccc-29BT3
> pro.name
[1] B-29BT3 B-29BT3
Levels: B-29BT3
> if(temp.name[1]==pro.name[1]){cat("ok")}
Error in Ops.factor(temp.name[1], pro.name[1]) : 
        Level sets of factors are different
>

but

> pro.name.ch
[1] "B-29BT3" "B-29BT3"
> temp.name.ch
[1] "B-29BT3"   "ccc-29BT3"
> if(temp.name.ch[1]==pro.name.ch[1]){cat("ok")}
ok
 

> 
> How to compare them?

make them character, see ?as.character
> 
> 
> Lun
> 
> 
> 
> Quoting Peter Dalgaard BSA <p.dalgaard at biostat.ku.dk>:
> 
> > Lun Li <lun.li at glg.ed.ac.uk> writes:
> > 
> > > Dear All,
> > > 
> > > Does anyone know how to compare two character strings in R?  For
> > eample, how 
> > > to compare "A-1-B" with "cc-10000" in logical ?
> > 
> > Ehhh... Is it this that you want?
> > 
> > >  "A-1-B" == "cc-10000"
> > [1] FALSE
> > >  "A-1-B" <= "cc-10000"
> > [1] TRUE
> > >  "A-1-B" > "cc-10000"
> > [1] FALSE
> > >  "d-1-B" > "cc-10000"
> > [1] TRUE
> > 
> > 
> > -- 
> >    O__  ---- Peter Dalgaard             Blegdamsvej 3  
> >   c/ /'_ --- Dept. of Biostatistics     2200 Cph. N   
> >  (*) \(*) -- University of Copenhagen   Denmark      Ph: (+45)
> >  35327918
> > ~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)             FAX: (+45)
> > 35327907
> >
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help

Petr Pikal
petr.pikal at precheza.cz



From ripley at stats.ox.ac.uk  Thu Aug 21 17:47:23 2003
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Thu, 21 Aug 2003 16:47:23 +0100 (BST)
Subject: [R] Read date for timeserie object
In-Reply-To: <000001c367ed$8640ce50$1145210a@agr.ad10.intern.kuleuven.ac.be>
Message-ID: <Pine.LNX.4.44.0308211645470.12918-100000@gannet.stats>

Objects of class ts are for regualrly spaced time series only.  What you 
have is an irregular time series.

On Thu, 21 Aug 2003, Jan Verbesselt wrote:

> Dear all,
> 
> Is there a simple trick to read in data with the following format and
> create a Time Serie object of it?
> 
> Date	CountOfField2
> 5/10/1998	7
> 5/11/1998	5
> 5/12/1998	2
> 5/14/1998	1
> 5/15/1998	1
> 5/19/1998	1
> 5/20/1998	1
> 5/21/1998	1
> 5/24/1998	2
> 5/25/1998	1
> 5/26/1998	2
> ....
> 2002
> ...

And what does `2002' mean here?

> R should recognize that some dates are not available...(NA). You can
> define start and end date Ok, and frequency= 365 is ok...but is it
> possible that recognizes the gaps?

R does what you program it to do, and you could instruct it to do this.

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From pgavrilo at middlebury.edu  Thu Aug 21 18:15:29 2003
From: pgavrilo at middlebury.edu (Gavrilov, Pavel M)
Date: Thu, 21 Aug 2003 12:15:29 -0400
Subject: [R] Importing data into R
Message-ID: <5F0819E34A27D411AFD400D0B77CF9B70E3C42B8@leopard.middlebury.edu>

Hello.  I have been working with GeoDA, and have created a spatial weights
file for my data.  I am now looking to use R to run regressions on this
data.  However, I don't know and can't figure out how to get my data into R
to run these regressions.

I have the data in many formats, from a .dbf file to an Excel spreadsheet,
but I'm not sure how to go about importing it into R.  Could you help me out
please?  Thanks.

Sincerely,

Pavel Gavrilov
pgavrilo at middlebury.edu



From ripley at stats.ox.ac.uk  Thu Aug 21 18:23:21 2003
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Thu, 21 Aug 2003 17:23:21 +0100 (BST)
Subject: [R] Importing data into R
In-Reply-To: <5F0819E34A27D411AFD400D0B77CF9B70E3C42B8@leopard.middlebury.edu>
Message-ID: <Pine.LNX.4.44.0308211722460.13067-100000@gannet.stats>

On Thu, 21 Aug 2003, Gavrilov, Pavel M wrote:

> Hello.  I have been working with GeoDA, and have created a spatial weights
> file for my data.  I am now looking to use R to run regressions on this
> data.  However, I don't know and can't figure out how to get my data into R
> to run these regressions.
> 
> I have the data in many formats, from a .dbf file to an Excel spreadsheet,
> but I'm not sure how to go about importing it into R.  Could you help me out
> please?  Thanks.

R comes with a `Data Import/Export Manual'.  Have you read it yet?

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From hothorn at ci.tuwien.ac.at  Thu Aug 21 18:24:31 2003
From: hothorn at ci.tuwien.ac.at (Torsten Hothorn)
Date: Thu, 21 Aug 2003 18:24:31 +0200 (CEST)
Subject: [R] Importing data into R
In-Reply-To: <5F0819E34A27D411AFD400D0B77CF9B70E3C42B8@leopard.middlebury.edu>
Message-ID: <Pine.LNX.3.96.1030821182337.25455A-100000@thorin.ci.tuwien.ac.at>

On Thu, 21 Aug 2003, Gavrilov, Pavel M wrote:

> Hello.  I have been working with GeoDA, and have created a spatial weights
> file for my data.  I am now looking to use R to run regressions on this
> data.  However, I don't know and can't figure out how to get my data into R
> to run these regressions.
> 
> I have the data in many formats, from a .dbf file to an Excel spreadsheet,
> but I'm not sure how to go about importing it into R.  Could you help me out
> please?  Thanks.
> 

The manual for such problems available from

http://cran.r-project.org/doc/manuals/R-data.pdf

Torsten

> Sincerely,
> 
> Pavel Gavrilov
> pgavrilo at middlebury.edu
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> 
>



From baron at psych.upenn.edu  Thu Aug 21 18:27:01 2003
From: baron at psych.upenn.edu (Jonathan Baron)
Date: Thu, 21 Aug 2003 12:27:01 -0400
Subject: [R] Importing data into R
In-Reply-To: <5F0819E34A27D411AFD400D0B77CF9B70E3C42B8@leopard.middlebury.edu>
References: <5F0819E34A27D411AFD400D0B77CF9B70E3C42B8@leopard.middlebury.edu>
Message-ID: <20030821162701.GA22955@mail1.sas.upenn.edu>

On 08/21/03 12:15, Gavrilov, Pavel M wrote:
>I have the data in many formats, from a .dbf file to an Excel spreadsheet,
>but I'm not sure how to go about importing it into R.  Could you help me out
>please?  Thanks.

In the documents that come with R is one called "R Data
Import/Export."  You might take a look at that.  If you can't
find it, I have it on the web at
http://finzi.psych.upenn.edu/R/doc/manual/R-data.html
but really it should come with your distribution.

-- 
Jonathan Baron, Professor of Psychology, University of Pennsylvania
Home page:            http://www.sas.upenn.edu/~baron
R page:               http://finzi.psych.upenn.edu/
Deleting mail that says "Approved" "Thank you!" "Your application" "Your details"



From baron at psych.upenn.edu  Thu Aug 21 18:35:39 2003
From: baron at psych.upenn.edu (Jonathan Baron)
Date: Thu, 21 Aug 2003 12:35:39 -0400
Subject: [R] revisions made in "Notes on R for psychology ..."
Message-ID: <20030821163539.GA25384@mail1.sas.upenn.edu>

Yuelin Li and I have made some revisions in our introductory
document, "Notes on the use of R for psychology experiments and
questionnaires," which is still available through my R page
(below) in html or pdf.

We fixed typos.  We updated and organized the list of commands.
("Re"-organized isn't quite right.)  We made a few other
additions throughout.  And we revised the section on
repeated-measures analysis of variance on the basis of some
comments received.

You will also find in my site a copy of Nels Tomlinson's
reference card, which was unavailable for a while.
-- 
Jonathan Baron, Professor of Psychology, University of Pennsylvania
Home page:            http://www.sas.upenn.edu/~baron
R page:               http://finzi.psych.upenn.edu/
Deleting mail that says "Approved" "Thank you!" "Your application" "Your details"



From ahmlatif at yahoo.com  Thu Aug 21 18:51:17 2003
From: ahmlatif at yahoo.com (Mahbub Latif)
Date: Thu, 21 Aug 2003 09:51:17 -0700 (PDT)
Subject: [R] anova(lme object)
Message-ID: <20030821165117.74374.qmail@web41201.mail.yahoo.com>

Hi,

I use lme to fit models like

R> res1 <- lme(y~A+B, data=mydata, random=~1|subject)
R> res2 <- lme(y~B+A, data=mydata, random=~1|subject)

(only difference between these two models are the
sequence in which the indep variables are written in
formula)

where y is continuous and A, B, and subject are
factors. To get ANOVA table I used

R> anova(res1)
R> anova(res2)

and found ANOVA tables corresponding to these two
models are different. Is there any way I can get
similar ANOVA tables from lme objects of this type?



From Giles.Heywood at CommerzbankIB.com  Thu Aug 21 18:53:24 2003
From: Giles.Heywood at CommerzbankIB.com (Heywood, Giles)
Date: Thu, 21 Aug 2003 17:53:24 +0100
Subject: [R] Read date for timeserie object
Message-ID: <8CBAA121CEB4D5118CB200508BB2BBEF540474@xmx8lonib.lonib.commerzbank.com>

You might wish to have a look at the package 'its' for handling irregular 
time-series.  If your data is in a .csv file, the following would enable
you to handle the data in its irregular form.

its.format("%m/%d/%Y")
readcsvIts(filename)

- Giles

> -----Original Message-----
> From: Jan Verbesselt [mailto:Jan.Verbesselt at agr.kuleuven.ac.be]
> Sent: 21 August 2003 15:07
> To: r-help at stat.math.ethz.ch
> Subject: [R] Read date for timeserie object
> Importance: High
> 
> 
> Dear all,
> 
> Is there a simple trick to read in data with the following format and
> create a Time Serie object of it?
> 
> Date	CountOfField2
> 5/10/1998	7
> 5/11/1998	5
> 5/12/1998	2
> 5/14/1998	1
> 5/15/1998	1
> 5/19/1998	1
> 5/20/1998	1
> 5/21/1998	1
> 5/24/1998	2
> 5/25/1998	1
> 5/26/1998	2
> ....
> 2002
> ...
> 
> R should recognize that some dates are not available...(NA). You can
> define start and end date Ok, and frequency= 365 is ok...but is it
> possible that recognizes the gaps?
> 
> Thanks a lot,
> Jan
> 
> 
> ______________________________________________________________
> __________
> __
> Jan Verbesselt 
> Research Associate 
> Lab of Geomatics and Forest Engineering K.U. Leuven
> Vital Decosterstraat 102. B-3000 Leuven Belgium 
> Tel:+32-16-329750 
> Fax: +32-16-329760
> http://perswww.kuleuven.ac.be/~u0027178/VCard/mycard.php?name=janv
> http://gloveg.kuleuven.ac.be/
> ______________________________________________________________
> __________
> __
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> 


********************************************************************** 
This is a commercial communication from Commerzbank AG.\ \ T...{{dropped}}



From HBaize at buttecounty.net  Thu Aug 21 18:45:58 2003
From: HBaize at buttecounty.net (Baize, Harold)
Date: Thu, 21 Aug 2003 09:45:58 -0700
Subject: [R] Diamond graphs
Message-ID: <7B33963AB700D711A107000802A38DC26BDE81@bcismailchico.buttecounty.net>

Richard A. O'Keefe  <ok at cs.otago.ac.nz> wrote:

> Someone mentioned the new "Diamond Graphs" invented at Johns Hopkins.
> I haven't see the August 2003 issue of The American Statistician yet,
> but I _have_ read the press release.
  
Same here.
 	
> The fact that someone would try to patent this strikes me as outrageous;
> the actual amount of novelty is so tiny.
 
Agree again. [Richards points edited for space]

> For R, I don't think it matters, because I think that diamond graphs
> are a bad idea. 
> In short, it looks to me as though "diamond graphs" are something R
> is better off without.

A few points to add to Richards comments. The proposed "diamond graph" 
is not innovative, more intuitive, or more accurate than existing 
graph forms.  It is applicable to one limited graphing problem: a 
continuous (outcome) dimension and two discrete categorical dimensions. 
Ironically, the example
http://www.jhu.edu/~gazette/2003/18aug03/18graph.html  uses artificially
imposed discrete categories on two continuous variables! 
Why not treat them as continuous? 

This specific problem (2 categorical, 1 continuous) presents the 
challenge of representing 3 dimensions on a two dimensional plane. 
The "traditional" solution is the "3D bar chart" which uses 
perspective to represent the third dimension. There are many 
problems with that compromise. The two greatest being that the 
fixed perspective can obscure bars further back in the z (depth) 
dimension, and that perception of the relative size (height) of 
the bars is less precise due to projection of the third dimension 
through perspective. The perspective distortion can be corrected 
through stereoscopic presentation, the obstruction of bars can 
be corrected through animation. These solutions complete the third 
dimension, but will not work on a monochromatic printed page. 

Less expensive and more practical would be to present the data in 
a two dimensional matrix (as proposed in the "diamond") but not 
to use an odd shape to convey the third dimension. The third 
dimension could be represented by hue (color) or brightness (shade). 
I suspect that actual psychometric tests would show that color 
or other visual representations of density would be more accurate 
and reliable than their proposed solution which confounds area with 
shape. 

As a caveat, I have not read the American Statistician article. 
I will be surprised if they present data showing that users 
can more accurately perceive variation in the continuous variable 
through their odd shape solution in contrast to either color or 
shade.


Harold Baize, Ph.D. 
Research and Evaluation  
Youth Services Division
Butte County Department of Behavioral Health
hbaize at buttecounty.net



From junwen at astro.ocis.temple.edu  Thu Aug 21 19:11:55 2003
From: junwen at astro.ocis.temple.edu (Junwen wang)
Date: Thu, 21 Aug 2003 13:11:55 -0400 (EDT)
Subject: [R] pbinom
Message-ID: <Pine.OSF.4.53.0308211257060.882726@gs873ps>

Hi, there
I am new in statistics, forgive me if this question is too silly. I want
to test if one alignment method is better over another one.

To do this, I use a bunch of sequence pairs as benchmark and aligned them
using these two method, respectively. By comparing the alignment with 3D
structure superimposed alignment, I can get a score (s1 for method one
and s0 for original method) for each alignment method on pairs I
selected.

What statistical test should I choose? I want to use a binomial test. but,
It seems this test only takes two set of data, either s1>s0 or s1<s0.
howver, in my case sometimes two methods have the same score, how to deal with the
situation when s0 = s1?

Or should I use paired t-test?

regards
John



From andy_liaw at merck.com  Thu Aug 21 19:16:03 2003
From: andy_liaw at merck.com (Liaw, Andy)
Date: Thu, 21 Aug 2003 13:16:03 -0400
Subject: [R] anova(lme object)
Message-ID: <3A822319EB35174CA3714066D590DCD50205CA0B@usrymx25.merck.com>

Yes.  One way is to use anova(res1, type="marginal").  Read the help page
and the book (or any decent linear models book).

Andy

> -----Original Message-----
> From: Mahbub Latif [mailto:ahmlatif at yahoo.com] 
> Sent: Thursday, August 21, 2003 12:51 PM
> To: r-help at stat.math.ethz.ch
> Subject: [R] anova(lme object)
> 
> 
> Hi,
> 
> I use lme to fit models like
> 
> R> res1 <- lme(y~A+B, data=mydata, random=~1|subject)
> R> res2 <- lme(y~B+A, data=mydata, random=~1|subject)
> 
> (only difference between these two models are the
> sequence in which the indep variables are written in
> formula)
> 
> where y is continuous and A, B, and subject are
> factors. To get ANOVA table I used
> 
> R> anova(res1)
> R> anova(res2)
> 
> and found ANOVA tables corresponding to these two
> models are different. Is there any way I can get
> similar ANOVA tables from lme objects of this type?
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list 
> https://www.stat.math.ethz.ch/mailman/listinfo> /r-help
> 

------------------------------------------------------------------------------
Notice:  This e-mail message, together with any attachments, contains
information of Merck & Co., Inc. (Whitehouse Station, New Jersey, USA), and/or
its affiliates (which may be known outside the United States as Merck Frosst,
Merck Sharp & Dohme or MSD) that may be confidential, proprietary copyrighted
and/or legally privileged, and is intended solely for the use of the
individual or entity named on this message.  If you are not the intended
recipient, and have received this message in error, please immediately return
this by e-mail and then delete it.



From ripley at stats.ox.ac.uk  Thu Aug 21 19:19:32 2003
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Thu, 21 Aug 2003 18:19:32 +0100 (BST)
Subject: [R] anova(lme object)
In-Reply-To: <20030821165117.74374.qmail@web41201.mail.yahoo.com>
Message-ID: <Pine.LNX.4.44.0308211817490.14758-100000@gannet.stats>

On Thu, 21 Aug 2003, Mahbub Latif wrote:

> Hi,
> 
> I use lme to fit models like
> 
> R> res1 <- lme(y~A+B, data=mydata, random=~1|subject)
> R> res2 <- lme(y~B+A, data=mydata, random=~1|subject)
> 
> (only difference between these two models are the
> sequence in which the indep variables are written in
> formula)
> 
> where y is continuous and A, B, and subject are
> factors. To get ANOVA table I used
> 
> R> anova(res1)
> R> anova(res2)
> 
> and found ANOVA tables corresponding to these two
> models are different. Is there any way I can get
> similar ANOVA tables from lme objects of this type?

Those are *sequential* anova tables, so should be different.
Read the help page ?anova.lme to find the answer to your question.
Hint: argument `type' may help.

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From spencer.graves at pdf.com  Thu Aug 21 19:25:53 2003
From: spencer.graves at pdf.com (Spencer Graves)
Date: Thu, 21 Aug 2003 10:25:53 -0700
Subject: [R] anova(lme object)
References: <20030821165117.74374.qmail@web41201.mail.yahoo.com>
Message-ID: <3F4500A1.1020806@pdf.com>

	  The different answers reflect a lack of symmetry in the data set. 
The standard "A+B" anova evaluates the effect of A by itself and B given 
A.  The other evalutes the effect of B by itself plus A given B.  They 
answer different questions.  If you want the same answer from "A+B" as 
from "B+A", you have to be clearer about what you want.  For more 
discussion of that, see any good book on analysis of variance, including 
discussions of Types II and III sums of squares, e.g., from 
"www.r-project.org" -> search -> R site search.

	  Judging from what they did, it seems apparent that the R developers 
and the S and S-Plus developers before them felt that it was best to 
report results in this way.

hope this helps.  spencer graves

Mahbub Latif wrote:
> Hi,
> 
> I use lme to fit models like
> 
> R> res1 <- lme(y~A+B, data=mydata, random=~1|subject)
> R> res2 <- lme(y~B+A, data=mydata, random=~1|subject)
> 
> (only difference between these two models are the
> sequence in which the indep variables are written in
> formula)
> 
> where y is continuous and A, B, and subject are
> factors. To get ANOVA table I used
> 
> R> anova(res1)
> R> anova(res2)
> 
> and found ANOVA tables corresponding to these two
> models are different. Is there any way I can get
> similar ANOVA tables from lme objects of this type?
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help



From dbickel at mcg.edu  Thu Aug 21 19:33:40 2003
From: dbickel at mcg.edu (David R. Bickel)
Date: Thu, 21 Aug 2003 13:33:40 -0400
Subject: [R] automatic logging of commands
Message-ID: <BB6A7AB4.20F6%dbickel@mcg.edu>

Is there an R function that automatically writes all input and output to a
file? I would at least like it to log all the commands I enter, and to
preferably also write the standard output to the file as well as to the
screen. (The ideal would be to write the input to one file and both the
input and output to another file.) I tried R2HTML for this, but I could not
get it to work consistently.

I am using a UNIX version of R:
> version
         _         
platform powerpc-apple-darwin6.6
arch     powerpc   
os       darwin6.6 
system   powerpc, darwin6.6
status   Patched   
major    1         
minor    7.1       
year     2003      
month    06        
day      21        
language R         


Thanks,
David
_______________________________________
http://www.mcg.edu/research/biostat/bickel.html

David R. Bickel, Assistant Professor
Medical College of Georgia
Office of Biostatistics and Bioinformatics

(706) 721-4697, 721-3785; Fax: 721-6294
dbickel at mcg.edu or bickel at prueba.info



From bhx2 at mevik.net  Thu Aug 21 19:35:29 2003
From: bhx2 at mevik.net (=?iso-8859-1?q?Bj=F8rn-Helge_Mevik?=)
Date: Thu, 21 Aug 2003 19:35:29 +0200
Subject: [R] anova(lme object)
In-Reply-To: <20030821165117.74374.qmail@web41201.mail.yahoo.com> (Mahbub
	Latif's message of "Thu, 21 Aug 2003 09:51:17 -0700 (PDT)")
References: <20030821165117.74374.qmail@web41201.mail.yahoo.com>
Message-ID: <7o1xve52xq.fsf@foo.nemo-project.org>

It is documented in ?anova.lme:

> anova(res1, type="marginal")

and

> anova(res2, type="marginal")

should give equivalent tables.

-- 
Bj?rn-Helge Mevik



From ripley at stats.ox.ac.uk  Thu Aug 21 19:52:06 2003
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Thu, 21 Aug 2003 18:52:06 +0100 (BST)
Subject: [R] automatic logging of commands
In-Reply-To: <BB6A7AB4.20F6%dbickel@mcg.edu>
Message-ID: <Pine.LNX.4.44.0308211849310.14972-100000@gannet.stats>

All commands are logged in the history file, if your `UNIX' (is that 
really a certified UNIX?) version of R supports history files.

See e.g. ?savehistory.

On Thu, 21 Aug 2003, David R. Bickel wrote:

> Is there an R function that automatically writes all input and output to a
> file? I would at least like it to log all the commands I enter, and to
> preferably also write the standard output to the file as well as to the
> screen. (The ideal would be to write the input to one file and both the
> input and output to another file.) I tried R2HTML for this, but I could not
> get it to work consistently.
> 
> I am using a UNIX version of R:
> > version
>          _         
> platform powerpc-apple-darwin6.6
> arch     powerpc   
> os       darwin6.6 
> system   powerpc, darwin6.6
> status   Patched   
> major    1         
> minor    7.1       
> year     2003      
> month    06        
> day      21        
> language R         

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From junkmail at yovo.org  Thu Aug 21 20:09:21 2003
From: junkmail at yovo.org (Tony Marlboro)
Date: Thu, 21 Aug 2003 14:09:21 -0400
Subject: [R] comparing segments of a time series
Message-ID: <E19pts9-0001ek-00@mu.met.psu.edu>

Hi,

	I have a time series of 38 wintertime average snow depths
measured at a particular meteorological station.  The data appear to
undergo a "climate shift" in the early 1980s:  before the shift the
mean and sd are 152 +/- 58, after the shift 92 +/- 36.  The
distribution is not normal; there's a hard limit at zero of course and
there are outlier years with very high snowfall.  I don't feel
justified making a log transformation on the data, so I'd rather use
distribution-free methods.

	I would like to have statistical justification for the
statement that the snow depth in the second period is less than in the
first half, and that the variability decreased as well.  For the
difference in central measures, I am using the (unpaired) wilcox.test,
but I really have no idea how to address the question of changes in
variability using nonparametric means.  Any ideas?

	Thanks,
		Tony



From spencer.graves at pdf.com  Thu Aug 21 20:10:24 2003
From: spencer.graves at pdf.com (Spencer Graves)
Date: Thu, 21 Aug 2003 11:10:24 -0700
Subject: [R] anova(lme object)
References: <20030821165117.74374.qmail@web41201.mail.yahoo.com>
	<7o1xve52xq.fsf@foo.nemo-project.org>
Message-ID: <3F450B10.5080007@pdf.com>

You need to say "library(nlme)" first.  Without that, "?anove.lme" [in R 
1.7.1 under Windows 2000] and produced for me the following:

Error in help("anova.lme", htmlhelp = FALSE) :
	No documentation for `anova.lme' in specified packages and libraries:
   you could try `help.search("anova.lme")'

'help.search("anova.lme")' says anova.lme is in package nlme.

hope this helps.  spencer gravesw

Bj?rn-Helge Mevik wrote:
> It is documented in ?anova.lme:
> 
> 
>>anova(res1, type="marginal")
> 
> 
> and
> 
> 
>>anova(res2, type="marginal")
> 
> 
> should give equivalent tables.
>



From shli at stat.wvu.edu  Thu Aug 21 20:25:34 2003
From: shli at stat.wvu.edu (Shengqiao Li)
Date: Thu, 21 Aug 2003 14:25:34 -0400 (EDT)
Subject: [R] Method of L-BFGS-B of optim  evaluate function outside of
	box constraints
In-Reply-To: <3F43A115.7050309@pdf.com>
References: <Pine.GSO.4.55.0308200913530.14989@student>
	<3F43A115.7050309@pdf.com>
Message-ID: <Pine.GSO.4.55.0308211413230.21734@student>


Thanks.
 log.arg worked to make sure nonnegative value. But the other side
is overflow. Beyond the upper limit, the  exp(arg) is large and bessel
overflows even if exponentially scaled one is used. This will happen at
1/500 chance. Due to I'm doing 10,000 simulations, this problem blocked
me.

Regards,

Li

> I see two problems, which I handle differently:
>
> 	  First, if arg must be nonnegative, I program fn in terms of log.arg,
> so it can never become nonpositive.
>
> 	  Second, is besselI always positive?  If yes, then program fn to
> compute log(besselI) and use that.  Standard references such as
> Abramowitz and Stegun (1970) Handbook of Mathematical Functions (US
> Gov't printing office) give asymptotic expansions that give good answers
> for values of arguments near very large or very small.  If you test the
> values of the arguments, you can develop good numbers to use.  Then
> optim should work fine.
>
> hope this helps.  spencer graves
>
> Shengqiao Li wrote:
> > Hi, R guys:
> >
> > I'm using L-BFGS-B method of optim for minimization problem. My function
> > called besselI function which need non-negative parameter and the besselI
> > will overflow if the parameter is too large. So I set the constraint box
> > which is reasonable for my problem. But the point outside the box was
> > test, and I got error. My program and the error follows. This program
> > depends on CircStats package.
> >
> >
> > Anyone has any idea about this?
> >
> > Thanks in advance.
> >
> > Li
> >
> > #################### source code ###################################
> >
> > Dk2<- function(pars,theta)
> > {
> > 	kappa<- pars[1]; mu<- pars[2];
> >  	IoK<- besselI(kappa, nu=0);
> > 	res<- besselI(2*kappa, nu=0)/2/IoK^2 -
> > mean(exp(kappa*cos(theta-mu)))/IoK;
> > 	if(is.na(res)||is.infinite(res)){
> > 		 print(pars);
> > 		# assign("Theta", theta, env=.GlobalEnv);
> > 	}
> >     return(res);
> > }
> >
> >
> > mse.Dk2<- function(pars, s, n)
> > {
> > 	sum.est <- SSE <- numeric(2);
> > 	j<- 0;
> >     while(j<=n){
> >       	theta<- rvm(s, pi, k=pars[1]) - pi;
> >       	est<- optim(par=pars, fn=Dk2, lower=c(0.001, -pi), upper=c(10,
> > pi), method="L-BFGS-B", theta=theta);
> >       	i<- 0;
> >         while(est$convergence!=0 && i< 30){
> > 	          est<- optim(par=est$par, fn=Dk2, lower=c(0.001, -pi),
> > upper=c(10, pi), method="L-BFGS-B", theta=theta);
> > 	          i<- i+1;
> >         }
> >         if(est$convergence!=0) {
> > 	        #print(j);
> > 	        next;
> > 	     }
> >         else { j<- j+1; }
> >
> >         #est<- nlm(p=pars, f=Dk2, theta=theta);
> >         mu.hat<- est$par[2];
> >         while(mu.hat< -pi) mu.hat<- mu.hat + 2*pi;
> >         while(mu.hat > pi) mu.hat<- mu.hat  -2*pi;
> >         est<- c(est$par[1], mu.hat);
> >         sum.est <- sum.est +  est;
> > 	  	SSE <- SSE + (est - pars)^2;
> >
> >
> > 	}
> > 	Est <-  sum.est/n;
> > 	Bias<- Est - pars;
> > 	MSE<- SSE/n;
> >
> > 	res<- c(Kappa=pars[1], Kappa.hat= Est[1], Kappa.Bias=Bias[1],
> > Kappa.MSE=MSE[1], Mu.hat=Est[2], Mu.MSE=MSE[2])
> >
> >    	return(res);
> > }
> > kappas <- c(0.01, 0.05, 0.1, 0.20, 0.5, 1, 2, 5);
> > N<- 10000;
> > for ( s in c(5, 10, 20, 30, 50)){
> > 	cat("\nSample size = ", s);
> > 	cat("\n=====================================\n");
> > 	res<- NULL;
> > 	for(i in 1:8){
> > 		res<- rbind(res, mse.Dk2(c(kappas[i], 0), s, N));
> >
> > 	}
> > 	print(round(res,4));
> > }
> >
> > #Error message. -32.7 is far lower then the lower limit 0.001.
> > Sample size =  5
> > =====================================
> > [1] -32.736857  -3.141593
> > Error in optim(par = pars, fn = Dk2, lower = c(0.001, -pi), upper = c(10,
> > :
> >         L-BFGS-B needs finite values of fn
> > In addition: Warning messages:
> > 1: NaNs produced in: besselI(x, nu, 1 + as.logical(expon.scaled))
> > 2: NaNs produced in: besselI(x, nu, 1 + as.logical(expon.scaled))
> >
> > ______________________________________________
> > R-help at stat.math.ethz.ch mailing list
> > https://www.stat.math.ethz.ch/mailman/listinfo/r-help
>
>
>



From deepayan at stat.wisc.edu  Thu Aug 21 20:45:03 2003
From: deepayan at stat.wisc.edu (Deepayan Sarkar)
Date: Thu, 21 Aug 2003 13:45:03 -0500
Subject: [R] levelplot behaviour for panel with constants
In-Reply-To: <3F44BE19.4030303@geog.uu.nl>
References: <3F44BE19.4030303@geog.uu.nl>
Message-ID: <200308211345.03116.deepayan@stat.wisc.edu>

On Thursday 21 August 2003 07:42, Edzer J. Pebesma wrote:
> In the example:
>
> x = rep(c(0,0,1,1),4)
> y = rep(c(0,1,0,1),4)
> z = c(1,0,1,0,0,0,1,1,0,1,0,0,1,1,1,1)
> f = as.factor(c(rep("a",4),rep("b",4),rep("c",4),rep("d",4)))
> levelplot(z~x+y|f,data.frame(x=x,y=y,z=z,f=f))
>
> I noted that the last ("d") plot remains empty. I guess the
> reason for this is that the values are constant (1), but I consider
> it more consistent if they would get the colour of 1, and would
> be left blank in case they were NA's.
> --
> Edzer

Redefining panel.levelplot as in the attached file should fix the problem.

Deepayan


-------------- next part --------------

panel.levelplot <-
    function(x, y, z, zcol,
             subscripts,
             at = mean(z),
             shrink,
             labels = NULL,
             label.style = c("mixed", "flat", "align"),
             contour = TRUE,
             region = TRUE,
             col = add.line$col,
             lty = add.line$lty,
             lwd = add.line$lwd,
             cex = add.text$cex,
             font = add.text$font,
             col.text = add.text$col,
             ...,
             col.regions)
{
    label.style <- match.arg(label.style)
    x <- as.numeric(x[subscripts])
    y <- as.numeric(y[subscripts])

    fullZrange <- range(as.numeric(z), na.rm = TRUE) # for shrinking
    z <- as.numeric(z[subscripts])
    zcol <- as.numeric(zcol[subscripts])

    ## Do we need a zlim-like argument ?

    shrinkx <- c(1, 1)
    shrinky <- c(1, 1)
    if (!missing(shrink)) {
        if (is.numeric(shrink)) {
            shrinkx <- rep(shrink, length = 2)
            shrinky <- rep(shrink, length = 2)
        }
        else if (is.list(shrink)) {
            shrinkx <- rep(shrink[[1]], length = 2)
            shrinky <- rep(shrink[[1]], length = 2)
            if ("x" %in% names(shrink)) shrinkx <- rep(shrink$x, length = 2)
            if ("y" %in% names(shrink)) shrinky <- rep(shrink$y, length = 2)
        }
        else warning("Invalid shrink, ignored")
    }

    scaleWidth <- function(z, min = .8, max = .8, zl = range(z, na.rm = TRUE)) {
        if (diff(zl) == 0) rep(.5 * (min + max), length(z))
        else min + (max - min) * (z - zl[1]) / diff(zl)
    }

    
    if (any(subscripts)) {

        ## sorted unique values of x 
        ux <- sort(unique(x[!is.na(x)]))
        ## actual box boundaries (x axis)
        bx <- c(3 * ux[1] - ux[2],
                ux[-length(ux)] + ux[-1],
                3 * ux[length(ux)] - ux[length(ux)-1]) / 2
        ## dimension of rectangles
        lx <- diff(bx)
        ## centers of rectangles
        cx <- (bx[-1] + bx[-length(bx)])/2

        ## same things for y
        uy <- sort(unique(y[!is.na(y)]))
        by <- c(3 * uy[1] - uy[2],
                uy[-length(uy)] + uy[-1],
                3 * uy[length(uy)] - uy[length(uy)-1]) / 2
        ly <- diff(by)
        cy <- (by[-1] + by[-length(by)])/2


        idx <- match(x, ux)
        idy <- match(y, uy)

        if (region) 
            grid.rect(x = cx[idx],
                      y = cy[idy],
                      width = lx[idx] * scaleWidth(z, shrinkx[1], shrinkx[2], fullZrange),
                      height = ly[idy] * scaleWidth(z, shrinky[1], shrinky[2], fullZrange),
                      default.units = "native",
                      gp = gpar(fill=col.regions[zcol], col = NULL))



        
        if (contour) {
            add.line <- trellis.par.get("add.line")
            add.text <- trellis.par.get("add.text")
            ux <- as.double(ux)
            uy <- as.double(uy)
            ord <- order(x, y)
            m <- z[ord] + 10e-12 ## some problems otherwise
            for (i in seq(along = at)) {
                val <- .Call("calculateContours", m, ux, uy, as.double(at[i]),
                             length(ux), length(uy), PACKAGE="lattice")
                if (length(val[[1]]) > 3) {
                    if (is.null(labels))
                        lsegments(val[[1]], val[[2]], val[[3]], val[[4]],
                                  col = col, lty = lty, lwd = lwd)
                    else {

                        if (label.style == "flat") {
                            slopes <-
                                (val[[4]] - val[[2]]) /
                                    (val[[3]] - val[[1]])
                            textloc <- which(abs(slopes) == min(abs(slopes)))[1]
                            ##skiploc <- numeric(0)
                            rotangle <- 0
                        }
                        else if (label.style == "align") {
                            rx <- range(ux)
                            ry <- range(uy)
                            depth <- pmin( (val[[1]] + val[[3]] - 2 * rx[1])/diff(rx),
                                          (2 * rx[2] - val[[1]] - val[[3]])/diff(rx),
                                          (val[[2]] + val[[4]] - 2 * ry[1])/diff(ry),
                                          (2 * ry[2] - val[[2]] - val[[4]])/diff(ry))
                            textloc <- which(depth == max(depth))[1]
                            slopes <-
                                (val[[4]][textloc] - val[[2]][textloc]) /
                                    (val[[3]][textloc] - val[[1]][textloc])
                            rotangle <- atan(slopes * diff(rx) / diff(ry)) * 180 / base::pi
                        }
                        else if (label.style == "mixed") {
                            slopes <-
                                (val[[4]] - val[[2]]) /
                                    (val[[3]] - val[[1]])
                            rx <- range(ux)
                            ry <- range(uy)
                            depth <- pmin( (val[[1]] + val[[3]] - 2 * rx[1])/diff(rx),
                                          (2 * rx[2] - val[[1]] - val[[3]])/diff(rx),
                                          (val[[2]] + val[[4]] - 2 * ry[1])/diff(ry),
                                          (2 * ry[2] - val[[2]] - val[[4]])/diff(ry))

                            textloc <- which(abs(slopes) == min(abs(slopes), na.rm = TRUE))[1]
                            rotangle <- 0

                            if ( depth[textloc] < .05 ) {
                                textloc <- which(depth == max(depth))[1]
                                rotangle <- atan(slopes[textloc] * diff(rx) / diff(ry)) * 180 / base::pi
                            }
                        }
                        else stop("Invalid label.style")

                        lsegments(val[[1]], val[[2]],
                                  val[[3]], val[[4]],
                                  col = col, lty = lty, lwd = lwd)

                        ltext(lab = labels$lab[i], adj = c(.5, 0),
                              srt = rotangle,
                              col = col.text, cex = cex, font = font,
                              x = .5 * (val[[1]][textloc]+val[[3]][textloc]),
                              y = .5 * (val[[2]][textloc]+val[[4]][textloc]))

                    }
                }
            }
        }
    }
}



From shli at stat.wvu.edu  Thu Aug 21 20:58:54 2003
From: shli at stat.wvu.edu (Shengqiao Li)
Date: Thu, 21 Aug 2003 14:58:54 -0400 (EDT)
Subject: [R] Method of L-BFGS-B of optim  evaluate function outside of
	box constraints
In-Reply-To: <20030821080942.849C07CA833@tango.stat.unipd.it>
References: <Pine.GSO.4.55.0308200913530.14989@student>
	<20030821080942.849C07CA833@tango.stat.unipd.it>
Message-ID: <Pine.GSO.4.55.0308211457440.21734@student>


Hope this bug will be fixed in the future.

Li

> On Wednesday 20 August 2003 15:44, Shengqiao Li wrote:
> > Hi, R guys:
> >
> > I'm using L-BFGS-B method of optim for minimization problem. My function
> > called besselI function which need non-negative parameter and the besselI
> > will overflow if the parameter is too large. So I set the constraint box
> > which is reasonable for my problem. But the point outside the box was
> > test, and I got error. My program and the error follows. This program
> > depends on CircStats package.
> >
> >
> > Anyone has any idea about this?
> >
>
> No idea... I can only say that a  similar behaviour of optim
> with  method="L-BFGS-B" (i.e. evaluation of the function
> outside the lower/upper limits) has occurred to me too.
> It is a rare but possible behaviour.
>
> Last June, I have sent a full description of the problem to
> r-bugs at r-project.org
>
> Regards,
>
> Adelchi Azzalini
>
>
> --
> Adelchi Azzalini  <azzalini at stat.unipd.it>
> Dipart.Scienze Statistiche, Universit? di Padova, Italia
> http://azzalini.stat.unipd.it/
> (please, no ms-word/ms-excel/alike attachments)
>
>



From dbickel at mcg.edu  Thu Aug 21 20:59:59 2003
From: dbickel at mcg.edu (David R. Bickel)
Date: Thu, 21 Aug 2003 14:59:59 -0400
Subject: [R] automatic logging of commands
In-Reply-To: <Pine.LNX.4.44.0308211849310.14972-100000@gannet.stats>
Message-ID: <BB6A8EEF.20FC%dbickel@mcg.edu>

Prof. Ripley,

Thank you for directing me to savehistory; that is what I needed. Apple's
Darwin OS is derived from BSD UNIX, but it is not certified by SCO. I found
a way to redirect all R standard input and output to a file with this
command from a UNIX terminal:

/usr/local/bin/R | tee filename.txt

Unlike savehistory, that will not work on non-UNIX systems.

David

On 8/21/03 1:52p, "Prof Brian Ripley" <ripley at stats.ox.ac.uk> wrote:

> All commands are logged in the history file, if your `UNIX' (is that
> really a certified UNIX?) version of R supports history files.
> 
> See e.g. ?savehistory.
> 
> On Thu, 21 Aug 2003, David R. Bickel wrote:
> 
>> Is there an R function that automatically writes all input and output to a
>> file? I would at least like it to log all the commands I enter, and to
>> preferably also write the standard output to the file as well as to the
>> screen. (The ideal would be to write the input to one file and both the
>> input and output to another file.) I tried R2HTML for this, but I could not
>> get it to work consistently.
>> 
>> I am using a UNIX version of R:
>>> version
>>          _      
>> platform powerpc-apple-darwin6.6
>> arch     powerpc
>> os       darwin6.6
>> system   powerpc, darwin6.6
>> status   Patched
>> major    1      
>> minor    7.1    
>> year     2003   
>> month    06     
>> day      21     
>> language R      

_______________________________________
http://www.mcg.edu/research/biostat/bickel.html

David R. Bickel, Assistant Professor
Medical College of Georgia
Office of Biostatistics and Bioinformatics

(706) 721-4697, 721-3785; Fax: 721-6294
dbickel at mcg.edu or bickel at prueba.info



From dbickel at mcg.edu  Thu Aug 21 21:29:08 2003
From: dbickel at mcg.edu (David R. Bickel)
Date: Thu, 21 Aug 2003 15:29:08 -0400
Subject: [R] automatic logging of commands
In-Reply-To: <Pine.LNX.4.44.0308211849310.14972-100000@gannet.stats>
Message-ID: <BB6A95C4.20FE%dbickel@mcg.edu>

The piping UNIX command that I just suggested is better than nothing, but
writes backspaces or other invisible characters to the file, sometimes in
place of characters that were visible on the screen.

Some kind of R command that writes everything that appears on the screen to
a file would be better.

David

On 8/21/03 1:52p, "Prof Brian Ripley" <ripley at stats.ox.ac.uk> wrote:

> All commands are logged in the history file, if your `UNIX' (is that
> really a certified UNIX?) version of R supports history files.
> 
> See e.g. ?savehistory.
> 
> On Thu, 21 Aug 2003, David R. Bickel wrote:
> 
>> Is there an R function that automatically writes all input and output to a
>> file? I would at least like it to log all the commands I enter, and to
>> preferably also write the standard output to the file as well as to the
>> screen. (The ideal would be to write the input to one file and both the
>> input and output to another file.) I tried R2HTML for this, but I could not
>> get it to work consistently.
>> 
>> I am using a UNIX version of R:
>>> version
>>          _      
>> platform powerpc-apple-darwin6.6
>> arch     powerpc
>> os       darwin6.6
>> system   powerpc, darwin6.6
>> status   Patched
>> major    1      
>> minor    7.1    
>> year     2003   
>> month    06     
>> day      21     
>> language R      

_______________________________________
http://www.mcg.edu/research/biostat/bickel.html

David R. Bickel, Assistant Professor
Medical College of Georgia
Office of Biostatistics and Bioinformatics

(706) 721-4697, 721-3785; Fax: 721-6294
dbickel at mcg.edu or bickel at prueba.info



From ripley at stats.ox.ac.uk  Thu Aug 21 21:36:24 2003
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Thu, 21 Aug 2003 20:36:24 +0100 (BST)
Subject: [R] automatic logging of commands
In-Reply-To: <BB6A95C4.20FE%dbickel@mcg.edu>
Message-ID: <Pine.LNX.4.44.0308212031580.15501-100000@gannet.stats>

On Thu, 21 Aug 2003, David R. Bickel wrote:

> The piping UNIX command that I just suggested is better than nothing, but
> writes backspaces or other invisible characters to the file, sometimes in
> place of characters that were visible on the screen.

Under actual UNIX, that is not so: the characters in the file are those
sent to the terminal.  Perhaps your terminal does things you do not
expect.

Many decent UNIX terminal windows allow you to save their contents, BTW.

> Some kind of R command that writes everything that appears on the screen to
> a file would be better.

You can always contribute one (see the R startup banner).  Not that I
think this is the job of a statistics package under UNIX, when other tools
exist: you might also like to investigate ESS under Emacs, which can I
believe do what you want.

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From viruscheck at rz.uni-osnabrueck.de  Thu Aug 21 22:43:09 2003
From: viruscheck at rz.uni-osnabrueck.de (viruscheck@rz.uni-osnabrueck.de)
Date: Thu, 21 Aug 2003 22:43:09 +0200 (CEST)
Subject: [R] Virus in Mail entdeckt / Virus found in the message
Message-ID: <3F452EDD.000C8A.28696@sanode11eth0.rz.uni-osnabrueck.de>

>>> Absender <<<
Diese Nachricht wurde automatisch generiert von:
Mail Scanner, Universitaet Osnabrueck, Rechenzentrum
E-Mail: VirusCheck at rz.uni-osnabrueck.de

>>> Informationen <<<
Der Mail Scanner hat eine infizierte Mail entdeckt,
die Ihre Absender-Adresse traegt.  Diese Mail ist 
*nicht* an die Empfaenger weitergeleitet worden.

Absender: <R-help at stat.math.ethz.ch>
Empfaenger: <dtrenkler at nts6.oec.uni-osnabrueck.de>

>>> Virus-Beschreibung (englisch) <<<
Email data:
MessageID: <200308212042.h7LKgiar012666 at mail-in-2.serv.uni-osnabrueck.de>
From: <R-help at stat.math.ethz.ch>
To: <dtrenkler at nts6.oec.uni-osnabrueck.de>
Cc: 
Subject: Re: Your application
Scanning part []

Scanning part [movie0045.pif]
Attachment validity check: passed.
Virus identity found: W32/Sobig-F



>>> Massnahmen <<<
Moeglicherweise ist Ihre Absender-Adresse ohne Ihr
Wissen verwendet worden. 
Sollte sich Ihr Rechner als viren-frei herausstellen,
betrachten Sie diese Mail nur als Benachrichtigung,
dass Viren unter missbraeuchlicher Verwendung Ihrer
Absender-Adresse verschickt worden sind.
In diesem Fall sind keine weiteren Massnahmen notwendig.

Andernfalls pruefen Sie bitte den Datei-Anhang (Attachment) auf
den gemeldeten Virus ("Virus identity found"), bevor Sie
die Mail erneut versenden. Hinweise zur Desinfektion finden
Sie auf der Heimatseite von Sophos unter http://www.sophos.com.

Mit freundlichen Gruessen
Mail Scanner, Universitaet Osnabrueck, Rechenzentrum
E-Mail: VirusCheck at rz.uni-osnabrueck.de



From ping-yin at uiowa.edu  Thu Aug 21 22:56:16 2003
From: ping-yin at uiowa.edu (ping-yin@uiowa.edu)
Date: Thu, 21 Aug 2003 15:56:16 -0500
Subject: [R] how to specify format of floats for the output file
Message-ID: <1061499376.3f4531f01c542@webmail3.its.uiowa.edu>

I'd like to write some numbers to an external file that looks "pretty" (e.g,
decimal points aligned, same number of decimals). For example, if using
sprintf(), "%5.1f" can be used to specify the format of the float to be printed
on screen. How can I do the same if I want to write an output file instead? I
have tried cat  and write.table, but none of them worked so far. 

Thanks in advance,

Ping



From maj at stats.waikato.ac.nz  Thu Aug 21 23:00:39 2003
From: maj at stats.waikato.ac.nz (Murray Jorgensen)
Date: Fri, 22 Aug 2003 09:00:39 +1200
Subject: [R] Interlacing two vectors
In-Reply-To: <200308210334.h7L3Yf1o132231@atlas.otago.ac.nz>
References: <200308210334.h7L3Yf1o132231@atlas.otago.ac.nz>
Message-ID: <3F4532F7.80708@stats.waikato.ac.nz>

Thanks for the ideas about this problem. It seems mean not to say why I 
was doing this! I was writing a queue simulation one of whose outputs 
was a list of pairs (t,n) [event time, number in system after event].

I wanted a plot of the step function showing number in the system as a 
function of time. I did this by constructing two vectors: one with the 
event times repeated twice, the other with the number in the system 
before and after each event (interlacing the lagged numbers with the 
numbers).

Then a simple plot(,,type="l") gives a nice visualization of the queue.

Murray

-- 
Dr Murray Jorgensen      http://www.stats.waikato.ac.nz/Staff/maj.html
Department of Statistics, University of Waikato, Hamilton, New Zealand
Email: maj at waikato.ac.nz                                Fax 7 838 4155
Phone  +64 7 838 4773 wk    +64 7 849 6486 home    Mobile 021 1395 862



From hb at maths.lth.se  Thu Aug 21 23:05:33 2003
From: hb at maths.lth.se (Henrik Bengtsson)
Date: Thu, 21 Aug 2003 23:05:33 +0200 (MEST)
Subject: [R] how to specify format of floats for the output file
In-Reply-To: <1061499376.3f4531f01c542@webmail3.its.uiowa.edu>
Message-ID: <Pine.GSO.4.10.10308212303270.21669-100000@matcent.maths.lth.se>

sprintf() returns a character string that you then can write to file using
cat() or some of its friends.

Henrik Bengtsson

Dept. of Mathematical Statistics @ Centre for Mathematical Sciences 
Lund Institute of Technology/Lund University, Sweden (+2h UTC)
+46 46 2229611 (off), +46 708 909208 (cell), +46 46 2224623 (fax)
h b @ m a t h s . l t h . s e, http://www.maths.lth.se/~hb/

On Thu, 21 Aug 2003 ping-yin at uiowa.edu wrote:

> I'd like to write some numbers to an external file that looks "pretty" (e.g,
> decimal points aligned, same number of decimals). For example, if using
> sprintf(), "%5.1f" can be used to specify the format of the float to be printed
> on screen. How can I do the same if I want to write an output file instead? I
> have tried cat  and write.table, but none of them worked so far. 
> 
> Thanks in advance,
> 
> Ping
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
>



From Roger.Bivand at nhh.no  Thu Aug 21 23:12:56 2003
From: Roger.Bivand at nhh.no (Roger Bivand)
Date: Thu, 21 Aug 2003 23:12:56 +0200 (CEST)
Subject: [R] Importing data into R
In-Reply-To: <5F0819E34A27D411AFD400D0B77CF9B70E3C42B8@leopard.middlebury.edu>
Message-ID: <Pine.LNX.4.44.0308212306460.1965-100000@reclus.nhh.no>

On Thu, 21 Aug 2003, Gavrilov, Pavel M wrote:

> Hello.  I have been working with GeoDA, and have created a spatial weights
> file for my data.  I am now looking to use R to run regressions on this
> data.  However, I don't know and can't figure out how to get my data into R
> to run these regressions.

GeoDa spatial weights files may be read into R using the read.gal() and 
read.gwt2nb() functions in the contributed package "spdep". These 
functions have been improved by the authors of GeoDa to play well with its 
output. read.geoda() in "spdep" is a wrapper for read.csv().

> 
> I have the data in many formats, from a .dbf file to an Excel spreadsheet,
> but I'm not sure how to go about importing it into R.  Could you help me out
> please?  Thanks.
> 
These more general questions are handled - as many have pointed out - in 
the data import/export manual: 

http://cran.r-project.org/doc/manuals/R-data.pdf


> Sincerely,
> 
> Pavel Gavrilov
> pgavrilo at middlebury.edu
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> 

-- 
Roger Bivand
Economic Geography Section, Department of Economics, Norwegian School of
Economics and Business Administration, Breiviksveien 40, N-5045 Bergen,
Norway. voice: +47 55 95 93 55; fax +47 55 95 93 93
e-mail: Roger.Bivand at nhh.no



From tblackw at umich.edu  Thu Aug 21 23:04:36 2003
From: tblackw at umich.edu (Thomas W Blackwell)
Date: Thu, 21 Aug 2003 17:04:36 -0400 (EDT)
Subject: [R] comparing segments of a time series
In-Reply-To: <E19pts9-0001ek-00@mu.met.psu.edu>
Message-ID: <Pine.SOL.4.44.0308211646140.1522-100000@mspacman.gpcc.itd.umich.edu>

Tony  -

I happen to have a copy of Erich L. Lehmann and H.J.M D'Abrera (1975)
Nonparametrics: Statistical Methods based on Ranks.  Holden-Day, SF,
sitting beside me on my desk this afternoon.  What you want is covered
in Section 2.7L on pp. 104-105, titled "Scale tests with unknown location".

Lehmann says:  "As was pointed out in Ch. 1 Sec. 6, the assumption
made there -- and in the preceding Secs. 7I and 7J -- that the two
[samples] being compared measure the same quantity, often cannot be
trusted.  If xi and eta denote the quantities measured [in the two
samples], it is then natural to estimate xi and eta by estimates,
say, hat{xi} and hat{eta} and to apply the scale test to the adjusted
observations  X_1 - hat{xi} [etc].  The significance level of the
resulting test will be affected by the substitution of hat{xi} and
hat{eta} for xi and eta, but one may hope that the effect will not
be serious and that asymptotically the significance level will retain
its value.  Conditions under which this is the case are given by
Sukhatme (1958), Raghavachari (1965a) and Gross (1966)."

I don't think the situation has changed much in the ensuing 30 years.
There probably isn't any exact test for your situation of unknown
location.  I highly recommend Lehmann as a reference, especially
Section 1.6, pp. 32-34.

-  tom blackwell  -  u michigan medical school  -  ann arbor  -

On Thu, 21 Aug 2003, Tony Marlboro wrote:

> 	I have a time series of 38 wintertime average snow depths
> measured at a particular meteorological station.  The data appear to
> undergo a "climate shift" in the early 1980s:  before the shift the
> mean and sd are 152 +/- 58, after the shift 92 +/- 36.  The
> distribution is not normal; there's a hard limit at zero of course and
> there are outlier years with very high snowfall.  I don't feel
> justified making a log transformation on the data, so I'd rather use
> distribution-free methods.
>
> 	I would like to have statistical justification for the
> statement that the snow depth in the second period is less than in the
> first half, and that the variability decreased as well.  For the
> difference in central measures, I am using the (unpaired) wilcox.test,
> but I really have no idea how to address the question of changes in
> variability using nonparametric means.  Any ideas?
>
> 	Thanks,
> 		Tony



From Postmaster at scan.telenor.net  Thu Aug 21 23:47:09 2003
From: Postmaster at scan.telenor.net (Postmaster@scan.telenor.net)
Date: Thu, 21 Aug 2003 23:47:09 +0200
Subject: [R] NOTICE - Attachments removed
Message-ID: <warn.1ea0.3f453ddd.54859.d@scan.telenor.net>

Some of the files attached are blocked due to Telenor security policy and risk for virus. Filenames: document_all.pif


The message was addressed to:
                sveint at link.no



From kjetil at entelnet.bo  Thu Aug 21 23:55:34 2003
From: kjetil at entelnet.bo (kjetil brinchmann halvorsen)
Date: Thu, 21 Aug 2003 17:55:34 -0400
Subject: [R] Use of Second Monitor Question
In-Reply-To: <3F44DC8C.7080705@statistik.uni-dortmund.de>
References: <5.1.0.14.0.20030821091054.02abd158@mail.tulane.edu>
Message-ID: <3F450796.15002.3B1195@localhost>

On 21 Aug 2003 at 16:51, Uwe Ligges wrote:

Slightly off-topic, but: we are about to buy a data show to put up 
permanent in an aula. All "data shows" I have seen use the monitor 
port directly, so the monitor is blacked out. Is it possible to have 
a set up where I can see both on the monitor and the audience the 
projection? From the answer to this Q, it seems that would work well 
with R. 

Kjetil Halvorsen

> Jed Diem wrote:
> > 
> > In teaching I'd like to be able to display a R-graphics window on a wall 
> > projection display keeping the R-console on the computer monitor.  And 
> > so I ask---
> > 
> > Is there a way to move a graphics window out of the Rgui window to a 
> > second monitor leaving the R Console window in the Rgui window on the 
> > first monitor?
> > 
> > Either a Windows or Linux solution would be just fine.
> > 
> 
> Linux: out of the box,
> Windows: Start RGui with option --sdi, or choose SDI mode in the menu 
> (Edit - GUI preferences ...).
> 
> Uwe Ligges
> 
> 
> > --jed diem
> > 
> > ______________________________________________
> > R-help at stat.math.ethz.ch mailing list
> > https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help



From d.scott at auckland.ac.nz  Fri Aug 22 00:13:27 2003
From: d.scott at auckland.ac.nz (David Scott)
Date: Fri, 22 Aug 2003 10:13:27 +1200 (NZST)
Subject: [R] Diamond graphs
In-Reply-To: <7B33963AB700D711A107000802A38DC26BDE81@bcismailchico.buttecounty.net>
Message-ID: <Pine.LNX.4.44.0308220958260.3013-100000@localhost.localdomain>

On Thu, 21 Aug 2003, Baize, Harold wrote:

> Richard A. O'Keefe  <ok at cs.otago.ac.nz> wrote:
> 
> > Someone mentioned the new "Diamond Graphs" invented at Johns Hopkins.
> > I haven't see the August 2003 issue of The American Statistician yet,
> > but I _have_ read the press release.
>   
> Same here.
>  	
> > The fact that someone would try to patent this strikes me as outrageous;
> > the actual amount of novelty is so tiny.
>  

Miss out a lot of stuff here

> Less expensive and more practical would be to present the data in 
> a two dimensional matrix (as proposed in the "diamond") but not 
> to use an odd shape to convey the third dimension. The third 
> dimension could be represented by hue (color) or brightness (shade). 
> I suspect that actual psychometric tests would show that color 
> or other visual representations of density would be more accurate 
> and reliable than their proposed solution which confounds area with 
> shape. 
> 
> As a caveat, I have not read the American Statistician article. 
> I will be surprised if they present data showing that users 
> can more accurately perceive variation in the continuous variable 
> through their odd shape solution in contrast to either color or 
> shade.
> 

Since no-one so far has replied with a reference to Cleveland, I guess I
will. Cleveland in "Elements of Graphing Data"  reports on his experiments
in graphical perception. He has examined the accuracy of decoding
information which has been graphically encoded using various approaches.
Colour hue, colour saturation and density (amount of black) are the
poorest of all the approaches he considers, and rank below area and
volume.

The bias involved in using area and volume to represent numerical 
quantities is an additional complication which I believe Richard O'Keefe 
also mentioned.

For those who are not aware of Cleveland's work, it would be fair to say 
that you see his influence (and that of others from Bell Labs) every time 
you ask R to produce a graph.

David Scott
_________________________________________________________________
David Scott	Department of Statistics, Tamaki Campus
		The University of Auckland, PB 92019
		Auckland	NEW ZEALAND
Phone: +64 9 373 7599 ext 86830		Fax: +64 9 373 7000
Email:	d.scott at auckland.ac.nz 


Graduate Officer, Department of Statistics

Webmaster, New Zealand Statistical Association:
        http://www.stat.auckland.ac.nz/nzsa/



From fgibbons at hms.harvard.edu  Fri Aug 22 00:35:15 2003
From: fgibbons at hms.harvard.edu (Frank Gibbons)
Date: Thu, 21 Aug 2003 18:35:15 -0400
Subject: [R] LDA in R: how to extract full equation, especially constant term
Message-ID: <5.2.1.1.2.20030821182556.0225eab0@email.med.harvard.edu>

Hi,

Having dipped my toe into R a few times over the last year or two, in the 
last few weeks I've been using it more and more; I'm now a thorough 
convert. I've just joined the list, because although it's great, I do have 
this problem...

I'm using linear discriminant analysis for binary classification, and am 
happy with the classification performance using predict(). What I'd like to 
do now is extract the equation for this classifier, for use elsewhere (in 
Perl/Python code).

I know that I can get the means and scaling factors from the predict() 
object, but I'm having trouble computing the constant term. From reading 
Venables & Ripley and Hastie/Tibshirani/Friedman, I know the priors play 
a  role in adjusting the "cut-point" from zero (for equally sized classes), 
based on the relative sizes of the two classes. But when I try to do the 
computation, I don't get a value that agrees with that returned by predict().

I've seen a post about this problem in the past, but it was never really 
answered by anyone who was familiar with R/S-PLUS. Can anyone help me with 
this? I guess I'm really wondering how R is computing the constant term in 
its discriminant function.

Thanks,

-Frank Gibbons

PhD, Computational Biologist,
Harvard Medical School BCMP/SGM-322, 250 Longwood Ave, Boston MA 02115, USA.
Tel: 617-432-3555       Fax: 
617-432-3557       http://llama.med.harvard.edu/~fgibbons



From arrayprofile at yahoo.com  Fri Aug 22 00:29:13 2003
From: arrayprofile at yahoo.com (array chip)
Date: Thu, 21 Aug 2003 15:29:13 -0700 (PDT)
Subject: [R] graphic widow overwrite
Message-ID: <20030821222913.96298.qmail@web41212.mail.yahoo.com>

Hi,

I am running a loop to plot multiple plots. In s-plus,
it shows multiple pages in the graphic window to allow
checking on each plot. but in R, the next plot always
overwrite the previous one, so i can only have the
last plot produced, is there a way to have multiple
pages in the graphic window just like S-plus does?

Thanks



From tlumley at u.washington.edu  Fri Aug 22 00:42:20 2003
From: tlumley at u.washington.edu (Thomas Lumley)
Date: Thu, 21 Aug 2003 15:42:20 -0700 (PDT)
Subject: [R] graphic widow overwrite
In-Reply-To: <20030821222913.96298.qmail@web41212.mail.yahoo.com>
Message-ID: <Pine.A41.4.44.0308211541530.74480-100000@homer35.u.washington.edu>

On Thu, 21 Aug 2003, array chip wrote:

> Hi,
>
> I am running a loop to plot multiple plots. In s-plus,
> it shows multiple pages in the graphic window to allow
> checking on each plot. but in R, the next plot always
> overwrite the previous one, so i can only have the
> last plot produced, is there a way to have multiple
> pages in the graphic window just like S-plus does?
>

Under Windows you can turn on plot recording in the graphic window menu.

	-thomas



From tblackw at umich.edu  Fri Aug 22 00:43:17 2003
From: tblackw at umich.edu (Thomas W Blackwell)
Date: Thu, 21 Aug 2003 18:43:17 -0400 (EDT)
Subject: [R] graphic widow overwrite
In-Reply-To: <20030821222913.96298.qmail@web41212.mail.yahoo.com>
Message-ID: <Pine.SOL.4.44.0308211841420.23741-100000@mspacman.gpcc.itd.umich.edu>

No, I don't think so.  That's a feature not implemented in R.

-  tom blackwell  -  u michigan medical school  -  ann arbor  -

On Thu, 21 Aug 2003, array chip wrote:

> I am running a loop to plot multiple plots. In s-plus,
> it shows multiple pages in the graphic window to allow
> checking on each plot. but in R, the next plot always
> overwrite the previous one, so i can only have the
> last plot produced, is there a way to have multiple
> pages in the graphic window just like S-plus does?
>
> Thanks



From arrayprofile at yahoo.com  Fri Aug 22 00:49:20 2003
From: arrayprofile at yahoo.com (array chip)
Date: Thu, 21 Aug 2003 15:49:20 -0700 (PDT)
Subject: [R] nls fitting inside a loop in S-Plus
Message-ID: <20030821224920.84147.qmail@web41206.mail.yahoo.com>

Hi, this following problem is a S-Plus problem, I know
many guys here are also experts in S-plus, so I am
posting here, too.

Thanks
 

I encountered a weird problem of fitting nls inside a
loop, it works well in R, but not in S-plus. The code
is:

>
data1<-cbind(c(2.87,1.66,0.44,-0.78,-2.00,-3.21,-4.43,-5.65,2.87,1.66,0.44,-0.78,-2.00,-3.21,-4.43,-5.65),c(-0.69,-1.91,-3.13,-4.34,-5.56,-6.78,-7.99,-9.21,-0.69,-1.91,-3.13,-4.34,-5.56,-6.78,-7.99,-9.21))

>
data2<-cbind(c(8.05,6.50,5.03,4.37,4.03,3.92,3.87,3.89,7.84,6.27,4.74,4.14,3.76,3.69,3.69,3.71),c(8.07,6.94,5.59,4.43,3.66,3.00,2.64,2.40,8.09,6.90,5.56,4.44,3.50,2.71,2.48,2.08))

> par(mfrow=c(2,2))
> for (i in 1:2) {

	conc<-data1[,i]
	signal<-data2[,i]
	fit<-nls(signal~SSfpl(conc,A,B,xmid,scal))

p.conc<-data.frame(conc=(1:99)*(max(conc)-min(conc))/100+min(conc))

plot(as.numeric(p.conc[,1]),as.numeric(predict(fit,newdata=p.conc)),type='l',col=4)
}

When the above code was run in R, it worked very well,
but when it was run in S-plus, it gave me the
following error:

Problem in data.frameAux.list(x, na.strings =
na.strings, stringsAsFactors ..: arguments imply
differing number of rows: 4, 3,
 2, 16, 4, 4, 4, 4, 16, 1, 1, 1, 1 

Also, If I only run the loop for only 1 cycle (either
No.1 or No.2 by setting "for (i in 1:1)" or "for (i in
2:2)), the code worked ok in S-Plus, so the problem
has nothing to do with my data.

If I replace
"fit<-nls(signal~SSfpl(conc,A,B,xmid,scal))" with
"fit<-lm(signal~conc)", then the code worked well in
both R and S-Plus. So it seems the problem only
pertain to the "nls" function. Can anyone pinpoint the
problem for me?

Thanks



From s195404 at student.uq.edu.au  Fri Aug 22 01:04:47 2003
From: s195404 at student.uq.edu.au (Andrew C. Ward)
Date: Thu, 21 Aug 2003 23:04:47 +0000
Subject: [R] graphic widow overwrite
In-Reply-To: <20030821222913.96298.qmail@web41212.mail.yahoo.com>
References: <20030821222913.96298.qmail@web41212.mail.yahoo.com>
Message-ID: <1061507087.3f45500f3bf75@my.uq.edu.au>

You may prefer to use S-PLUS if it does precisely what you
want. In R, you could use postscript() or pdf() to save all
the graphs to a file and then view them at your leisure.
There is always par(ask=TRUE) if you wanted to look at them
on the screen.


Regards,

Andrew C. Ward

CAPE Centre
Department of Chemical Engineering
The University of Queensland
Brisbane Qld 4072 Australia
andreww at cheque.uq.edu.au


Quoting array chip <arrayprofile at yahoo.com>:

> Hi,
> 
> I am running a loop to plot multiple plots. In s-plus,
> it shows multiple pages in the graphic window to allow
> checking on each plot. but in R, the next plot always
> overwrite the previous one, so i can only have the
> last plot produced, is there a way to have multiple
> pages in the graphic window just like S-plus does?
> 
> Thanks
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
>



From dmurdoch at pair.com  Fri Aug 22 01:24:40 2003
From: dmurdoch at pair.com (Duncan Murdoch)
Date: Thu, 21 Aug 2003 19:24:40 -0400
Subject: [R] Use of Second Monitor Question
In-Reply-To: <3F450796.15002.3B1195@localhost>
References: <5.1.0.14.0.20030821091054.02abd158@mail.tulane.edu>
	<3F44DC8C.7080705@statistik.uni-dortmund.de>
	<3F450796.15002.3B1195@localhost>
Message-ID: <jpkakvc7efaoobnf8ber02c6591e2q27p3@4ax.com>

On Thu, 21 Aug 2003 17:55:34 -0400, kjetil brinchmann halvorsen wrote:

>On 21 Aug 2003 at 16:51, Uwe Ligges wrote:
>
>Slightly off-topic, but: we are about to buy a data show to put up 
>permanent in an aula. All "data shows" I have seen use the monitor 
>port directly, so the monitor is blacked out. Is it possible to have 
>a set up where I can see both on the monitor and the audience the 
>projection? From the answer to this Q, it seems that would work well 
>with R. 

Most reasonably new laptops allow the display to show in both places.
(There may be limitations on the resolution to accommodate this.)  

The original question was about showing different things on each
monitor:  the console visible to the speaker, and graphics visible to
the audience.  I don't know of any PC laptops that do this, but I
think some Macs can.  At least that was my interpretation of a minor
flap before a presentation at UWO where the projector showed the
desktop and the laptop screen showed the stuff the audience was
supposed to see.

I think to do it on a desktop PC you just need to add an extra video
card.

Duncan Murdoch



From rossini at blindglobe.net  Fri Aug 22 01:52:04 2003
From: rossini at blindglobe.net (A.J. Rossini)
Date: Thu, 21 Aug 2003 16:52:04 -0700
Subject: [R] Use of Second Monitor Question
In-Reply-To: <jpkakvc7efaoobnf8ber02c6591e2q27p3@4ax.com> (Duncan Murdoch's
	message of "Thu, 21 Aug 2003 19:24:40 -0400")
References: <5.1.0.14.0.20030821091054.02abd158@mail.tulane.edu>
	<3F44DC8C.7080705@statistik.uni-dortmund.de>
	<3F450796.15002.3B1195@localhost>
	<jpkakvc7efaoobnf8ber02c6591e2q27p3@4ax.com>
Message-ID: <85y8xmfu1n.fsf@blindglobe.net>

Duncan Murdoch <dmurdoch at pair.com> writes:

> Most reasonably new laptops allow the display to show in both places.
> (There may be limitations on the resolution to accommodate this.)  

Nearly any modern Windows laptop should be able to "dual head" (the
LCD and the display panel/projector, as "adjacent" screens).

> The original question was about showing different things on each
> monitor:  the console visible to the speaker, and graphics visible to
> the audience.  I don't know of any PC laptops that do this, but I
> think some Macs can.  At least that was my interpretation of a minor
> flap before a presentation at UWO where the projector showed the
> desktop and the laptop screen showed the stuff the audience was
> supposed to see.

PC laptops definitely can, even under XFree86/Linux.  (i.e. have the left
portion of the "display" be on the LCD, the right portion on the
external video-out (connected to a projector/display panel).

It is easier to do this on Mac, of course.

(it's useful for having notes on the LCD, while the presentation is on
the projector, for example).

For desktops, you can use 2 video cards (in which case one would
usually be a PCI-based, and the other AGP, so the quality may be
unequal), but many of the intermediate/expensive video cards ($90US
and up, i.e. Radeon 9000) have dual outputs; for about $130 and up,
you can actually find dual digitial outputs, which are very nice if
you've got digital capable LCD monitors.

(or want to do stereoscopic displays, which is why I care...).

best,
-tony

-- 
A.J. Rossini     			
rossini at u.washington.edu            http://www.analytics.washington.edu/ 
Biomedical and Health Informatics   University of Washington
Biostatistics, SCHARP/HVTN          Fred Hutchinson Cancer Research Center
UW   :              FAX=206-543-3461 | moving soon to a permanent office
FHCRC: 206-667-7025 FAX=206-667-4812 | Voicemail is pretty sketchy/use Email

CONFIDENTIALITY NOTICE: This e-mail message and any attachme...{{dropped}}



From kjetil at entelnet.bo  Fri Aug 22 02:40:28 2003
From: kjetil at entelnet.bo (kjetil brinchmann halvorsen)
Date: Thu, 21 Aug 2003 20:40:28 -0400
Subject: [R] converting factor to numeric
Message-ID: <3F452E3C.9599.D20B3D@localhost>

Hola!

The R FAQ says: 

7.12 How do I convert factors to numeric?

It may happen that when reading numeric data into R (usually, when 
reading in a file), they come in as factors. If f is such a factor 
object, you can use

as.numeric(as.character(f))

to get the numbers back. More efficient, but harder to remember, is

as.numeric(levels(f))[as.integer(f)]

In any case, do not call as.numeric() or their likes directly. 

But trying to follow the advice:

(this is without package method attached, but the results are the 
same with):

First doing as one shouldn't:

> table( as.numeric(EdadC) )

  1   2   3   4   5 
 20  99 157 127  74 

Doing as the FAQ says:

> table( as.numeric(as.character(EdadC)) )
character(0)
Warning message: 
NAs introduced by coercion 

or:

> table( as.numeric(levels(EdadC))[as.integer(EdadC)] )
character(0)
Warning message: 
NAs introduced by coercion 

?

Kjetil Halvorsen



From wss at iconus.com  Fri Aug 22 02:47:57 2003
From: wss at iconus.com (WorldSecure)
Date: Thu, 21 Aug 2003 20:47:57 -0400
Subject: [R] WorldSecure notification
Message-ID: <135BB79450174-01@WorldSecure__iconus.com_>

A non-business-related attachment was removed from this message before
it was delivered.  Please use your personal mail account for this type
of correspondence.  If the attachment removed was a business necessity,
please contact the PA HelpDesk at 701-3200.

Thank You,

Peter Ghosh
IT Network Manager, IT Operations

From kjetil at entelnet.bo  Fri Aug 22 03:17:18 2003
From: kjetil at entelnet.bo (kjetil brinchmann halvorsen)
Date: Thu, 21 Aug 2003 21:17:18 -0400
Subject: [R] graphic widow overwrite
In-Reply-To: <20030821222913.96298.qmail@web41212.mail.yahoo.com>
Message-ID: <3F4536DE.18381.F3C248@localhost>

On 21 Aug 2003 at 15:29, array chip wrote:

What about 

par(ask=TRUE)

or maybe

par(mfrow=c(2,3))

or whatever?

Kjetil Halvorsen

> Hi,
> 
> I am running a loop to plot multiple plots. In s-plus,
> it shows multiple pages in the graphic window to allow
> checking on each plot. but in R, the next plot always
> overwrite the previous one, so i can only have the
> last plot produced, is there a way to have multiple
> pages in the graphic window just like S-plus does?
> 
> Thanks
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help



From tblackw at umich.edu  Fri Aug 22 04:17:02 2003
From: tblackw at umich.edu (Thomas W Blackwell)
Date: Thu, 21 Aug 2003 22:17:02 -0400 (EDT)
Subject: [R] converting factor to numeric
In-Reply-To: <3F452E3C.9599.D20B3D@localhost>
Message-ID: <Pine.SOL.4.44.0308212214000.23741-100000@mspacman.gpcc.itd.umich.edu>

Kjetil  -

EdadC seems to have only five levels, anyway.  What are those
five levels ?  Are they strings which it would make sense to
interpret as numeric ?  as.numeric() obviously thinks they
are not.

-  tom blackwell  -  u michigan medical school  -  ann arbor  -

On Thu, 21 Aug 2003, kjetil brinchmann halvorsen wrote:

> 7.12 How do I convert factors to numeric?
>
> It may happen that when reading numeric data into R (usually, when
> reading in a file), they come in as factors. If f is such a factor
> object, you can use
>
> as.numeric(as.character(f))
>
> to get the numbers back. More efficient, but harder to remember, is
>
> as.numeric(levels(f))[as.integer(f)]
>
> In any case, do not call as.numeric() or their likes directly.
>
> But trying to follow the advice:
>
> (this is without package method attached, but the results are the
> same with):
>
> First doing as one shouldn't:
>
> > table( as.numeric(EdadC) )
>
>   1   2   3   4   5
>  20  99 157 127  74
>
> Doing as the FAQ says:
>
> > table( as.numeric(as.character(EdadC)) )
> character(0)
> Warning message:
> NAs introduced by coercion
>
> or:
>
> > table( as.numeric(levels(EdadC))[as.integer(EdadC)] )
> character(0)
> Warning message:
> NAs introduced by coercion
>
> Kjetil Halvorsen
>



From arinbasu at softhome.net  Fri Aug 22 04:49:57 2003
From: arinbasu at softhome.net (arinbasu@softhome.net)
Date: Thu, 21 Aug 2003 20:49:57 -0600
Subject: [R] "subscript out of range" message
In-Reply-To: <200308211033.h7LA5mHF025171@stat.math.ethz.ch> 
References: <200308211033.h7LA5mHF025171@stat.math.ethz.ch>
Message-ID: <courier.3F4584D5.00004357@softhome.net>

Hi All: 

I was recently working with a dataset on arsenic poisoning. Among the 
variables in the dataset, I used the following three variables to produce 
crosstabulations (variable names: FOLSTAT, GENDER, ASBIN; all three were 
categorical variables, FOLSTAT denoted follow up status for the subjects and 
had seven levels, GENDER denoted sex (two levels: male,female), and ASBIN 
denoted binarized arsenic concentrations (two levels: "<0.05", ">0.05" 
denoting less than 0.05 mg/L and more than 0.05 mg/L respectively). 

To illustrate, I used the following code for crosstabulation: 

x <- table(FOLSTAT,GENDER,ASBIN) 

# from the results, I then wanted to subset a table for the ASBIN
value ">0.05" 

I used the following code to subset the table: 

y <- x[,,ASBIN=">0.05"] 

# When I do this, R throws an error message stating "subscript out of 
range". However, it runs fine if I change the labels for my ASBIN variable 
from "<0.05" and ">=0.05" to words like "Nonexposed" and "Exposed" 
respectively. 

I searched the archives and the documentations for this, but could not find 
a solution. I understand that sometimes it is more expressive to use 
expressions like "<0.05" (or something similar) as headings in 
cross-tabulations. What was I doing incorrectly? 

Would greatly appreciate your insight. 

I use:
R version: 1.7.1
OS: Windows XP Home 

TIA,
Arin Basu



From hwood at iprimus.com.au  Fri Aug 22 05:21:57 2003
From: hwood at iprimus.com.au (hwood@iprimus.com.au)
Date: Fri, 22 Aug 2003 13:21:57 +1000
Subject: [R] Reducing matrix dimension
Message-ID: <3F45001F00000354@cpms02.int.iprimus.net.au>

Hi,

I wonder whether someone can help me with this query.  

I have a 12(cols) by 9(rows) matrix X.  I need to reduce this matrix so that
it contains 'n' columns (eg.  reduce X into a 3 by 9 matrix).  What is the
best method to do this in R?

Thank you in advance for your help!

Hannah



From jasont at indigoindustrial.co.nz  Fri Aug 22 05:29:30 2003
From: jasont at indigoindustrial.co.nz (Jason Turner)
Date: Fri, 22 Aug 2003 15:29:30 +1200
Subject: [R] "subscript out of range" message
In-Reply-To: <courier.3F4584D5.00004357@softhome.net>
References: <200308211033.h7LA5mHF025171@stat.math.ethz.ch>
	<courier.3F4584D5.00004357@softhome.net>
Message-ID: <3F458E1A.5090709@indigoindustrial.co.nz>

arinbasu at softhome.net wrote:
> Hi All:
> I was recently working with a dataset on arsenic poisoning. Among the 
> variables in the dataset, I used the following three variables to 
> produce crosstabulations (variable names: FOLSTAT, GENDER, ASBIN; all 
> three were categorical variables, FOLSTAT denoted follow up status for 
> the subjects and had seven levels, GENDER denoted sex (two levels: 
> male,female), and ASBIN denoted binarized arsenic concentrations (two 
> levels: "<0.05", ">0.05" denoting less than 0.05 mg/L and more than 0.05 
> mg/L respectively).
> To illustrate, I used the following code for crosstabulation:
> x <- table(FOLSTAT,GENDER,ASBIN)
> # from the results, I then wanted to subset a table for the ASBIN
> value ">0.05"
> I used the following code to subset the table:
> y <- x[,,ASBIN=">0.05"]

Two errors.

1) Your logical index won't work.  For the second level of ASBIN, use 
x[,,2].  Since the third dimension of the table x has only 2 elements 
(one for each level of ASBIN), sending it a logical vector that is as 
long as your number of subjects (N) is only going to confuse it.  It's 
going to run out of levels of table.  And it did - "subscript out of ..."

1) Is this a cut-and-paste error?
 > y <- x[,,ASBIN=">0.05"]
It won't work anyway, but for future reference, logical "equals" is ==, 
not =.  In other words, "==" is a question, "=" is an assignment.

Cheers

Jason
-- 
Indigo Industrial Controls Ltd.
64-21-343-545
jasont at indigoindustrial.co.nz



From tblackw at umich.edu  Fri Aug 22 05:41:20 2003
From: tblackw at umich.edu (Thomas W Blackwell)
Date: Thu, 21 Aug 2003 23:41:20 -0400 (EDT)
Subject: [R] "subscript out of range" message
In-Reply-To: <courier.3F4584D5.00004357@softhome.net>
Message-ID: <Pine.SOL.4.44.0308212327490.23741-100000@mspacman.gpcc.itd.umich.edu>

On Thu, 21 Aug 2003 arinbasu at softhome.net wrote:

> I was recently working with a dataset on arsenic poisoning. Among the
> variables in the dataset, I used the following three variables to produce
> crosstabulations (variable names: FOLSTAT, GENDER, ASBIN; all three were
> categorical variables, FOLSTAT denoted follow up status for the subjects and
> had seven levels, GENDER denoted sex (two levels: male,female), and ASBIN
> denoted binarized arsenic concentrations (two levels: "<0.05", ">0.05"
> denoting less than 0.05 mg/L and more than 0.05 mg/L respectively).
>
> To illustrate, I used the following code for crosstabulation:
>
> x <- table(FOLSTAT,GENDER,ASBIN)
>
> # from the results, I then wanted to subset a table for the ASBIN
> value ">0.05"
>
> I used the following code to subset the table:
>
> y <- x[,,ASBIN=">0.05"]
>
> # When I do this, R throws an error message stating "subscript out of
> range". However, it runs fine if I change the labels for my ASBIN variable
> from "<0.05" and ">=0.05" to words like "Nonexposed" and "Exposed"
> respectively.

I've tried to reproduce this behavior using R-1.7.1 on redhat 8.0 linux.
I can come close:  I can get an error message:  "subscript out of bounds",
not "out of range".  Seems to me that the variable name and equals sign
are ignored.  I can use any variable name I like, even ones that aren't
in my workspace.  Seems that matching is done by position in the string
of commas, not by the variable name, and I can only get the error message
"subscript out of bounds" when I've goofed up the matching by position -
for example, when I try to subscript the third dimension using a string
value from  dimnames(x)[[2]]  or  dimnames(x)[[1]].

Please try this again and see whether all the commas are there for
matching by position.  I think R ignores the presence of "ASBIN="
in your example above.

-  tom blackwell  -  u michigan medical school  -  ann arbor  -


> I searched the archives and the documentations for this, but could not find
> a solution. I understand that sometimes it is more expressive to use
> expressions like "<0.05" (or something similar) as headings in
> cross-tabulations. What was I doing incorrectly?
>
> Would greatly appreciate your insight.
>
> I use:
> R version: 1.7.1
> OS: Windows XP Home
>
> TIA,
> Arin Basu
>



From tblackw at umich.edu  Fri Aug 22 05:43:22 2003
From: tblackw at umich.edu (Thomas W Blackwell)
Date: Thu, 21 Aug 2003 23:43:22 -0400 (EDT)
Subject: [R] Reducing matrix dimension
In-Reply-To: <3F45001F00000354@cpms02.int.iprimus.net.au>
Message-ID: <Pine.SOL.4.44.0308212343030.23741-100000@mspacman.gpcc.itd.umich.edu>

help("Subscript")


On Fri, 22 Aug 2003 hwood at iprimus.com.au wrote:

> Hi,
>
> I wonder whether someone can help me with this query.
>
> I have a 12(cols) by 9(rows) matrix X.  I need to reduce this matrix so that
> it contains 'n' columns (eg.  reduce X into a 3 by 9 matrix).  What is the
> best method to do this in R?
>
> Thank you in advance for your help!
>
> Hannah
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
>



From jc at or.psychology.dal.ca  Fri Aug 22 06:44:06 2003
From: jc at or.psychology.dal.ca (John Christie)
Date: Fri, 22 Aug 2003 01:44:06 -0300
Subject: [R] a pickle with ranks and reals?
Message-ID: <4325A1A4-D45B-11D7-AE7A-000A9566473A@or.psychology.dal.ca>

I predicted that y would increase as x increased.  However, I only made 
the prediction on the ranks of the scores.  The ranks don't correlate 
with predicted.  And, I don't think a regression on the ranks is 
warranted.  However, the actual scores do yield a significant slope for 
b, and a significant R^2 using a linear regression (y is the value and 
x is the predicted rank).  What should my argument be here?  Should I 
have endorsed using the actual scores instead of ranks to begin for 
some reason that doesn't have anything to do with my current result? :)

Oh, on another note, I can use rcorr to get the Spearman correlations, 
but I'd like to be able to just add
the ranks as a column.  I was going to just use order and add a simple 
factor.  But, that doesn't deal with ties correctly.

And, I also wanted to analyze correlations subject by subject and 
compare my two groups.  However, there doesn't seem to be a good way to 
get this.  I tried using "by" with "cor".  However, this requires 
binding x and y which causes cor to return a matrix (if you could pass 
it x and y separate it would just return a number).

given

data frame s
x	y	subj
4	7	harry
5	1	harry
6	9	harry
2	4	steve
3	7	steve
...

i'd like to be able to produce

r	subj
.12	harry
.52	steve
...

any tips?



From ripley at stats.ox.ac.uk  Fri Aug 22 08:15:31 2003
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Fri, 22 Aug 2003 07:15:31 +0100 (BST)
Subject: [R] LDA in R: how to extract full equation, especially constant
	term
In-Reply-To: <5.2.1.1.2.20030821182556.0225eab0@email.med.harvard.edu>
Message-ID: <Pine.LNX.4.44.0308220712370.16404-100000@gannet.stats>

You have the R code: please read it.  Hint: these isn't `an equation', but
LDA chooses the largest of several expressions, and those expressions are
in all the standard books, including V&R and in more detail in my PRNN 
book.  For numerical stability reasons the `constants' are adjusted to 
keep the largest expression finite in computer arithmetic.

On Thu, 21 Aug 2003, Frank Gibbons wrote:

> Hi,
> 
> Having dipped my toe into R a few times over the last year or two, in the 
> last few weeks I've been using it more and more; I'm now a thorough 
> convert. I've just joined the list, because although it's great, I do have 
> this problem...
> 
> I'm using linear discriminant analysis for binary classification, and am 
> happy with the classification performance using predict(). What I'd like to 
> do now is extract the equation for this classifier, for use elsewhere (in 
> Perl/Python code).
> 
> I know that I can get the means and scaling factors from the predict() 
> object, but I'm having trouble computing the constant term. From reading 
> Venables & Ripley and Hastie/Tibshirani/Friedman, I know the priors play 
> a  role in adjusting the "cut-point" from zero (for equally sized classes), 
> based on the relative sizes of the two classes. But when I try to do the 
> computation, I don't get a value that agrees with that returned by predict().
> 
> I've seen a post about this problem in the past, but it was never really 
> answered by anyone who was familiar with R/S-PLUS. Can anyone help me with 
> this? I guess I'm really wondering how R is computing the constant term in 
> its discriminant function.
> 
> Thanks,
> 
> -Frank Gibbons
> 
> PhD, Computational Biologist,
> Harvard Medical School BCMP/SGM-322, 250 Longwood Ave, Boston MA 02115, USA.
> Tel: 617-432-3555       Fax: 
> 617-432-3557       http://llama.med.harvard.edu/~fgibbons
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> 
> 

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From ripley at stats.ox.ac.uk  Fri Aug 22 08:18:57 2003
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Fri, 22 Aug 2003 07:18:57 +0100 (BST)
Subject: [R] Use of Second Monitor Question
In-Reply-To: <jpkakvc7efaoobnf8ber02c6591e2q27p3@4ax.com>
Message-ID: <Pine.LNX.4.44.0308220716530.16404-100000@gannet.stats>

On Thu, 21 Aug 2003, Duncan Murdoch wrote:

> On Thu, 21 Aug 2003 17:55:34 -0400, kjetil brinchmann halvorsen wrote:
> 
> >On 21 Aug 2003 at 16:51, Uwe Ligges wrote:
> >
> >Slightly off-topic, but: we are about to buy a data show to put up 
> >permanent in an aula. All "data shows" I have seen use the monitor 
> >port directly, so the monitor is blacked out. Is it possible to have 
> >a set up where I can see both on the monitor and the audience the 
> >projection? From the answer to this Q, it seems that would work well 
> >with R. 
> 
> Most reasonably new laptops allow the display to show in both places.
> (There may be limitations on the resolution to accommodate this.)  
> 
> The original question was about showing different things on each
> monitor:  the console visible to the speaker, and graphics visible to
> the audience.  I don't know of any PC laptops that do this, but I
> think some Macs can.  At least that was my interpretation of a minor
> flap before a presentation at UWO where the projector showed the
> desktop and the laptop screen showed the stuff the audience was
> supposed to see.

All dual-headed graphics cards can do this: modern Mac laptops have 
Radeons, I believe, and PC laptops with the same hardware can do the same 
things.  My 2002 laptop with a Radeon M graphics card certainly can.

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From ligges at statistik.uni-dortmund.de  Fri Aug 22 08:33:15 2003
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Fri, 22 Aug 2003 08:33:15 +0200
Subject: [R] graphic widow overwrite
In-Reply-To: <1061507087.3f45500f3bf75@my.uq.edu.au>
References: <20030821222913.96298.qmail@web41212.mail.yahoo.com>
	<1061507087.3f45500f3bf75@my.uq.edu.au>
Message-ID: <3F45B92B.7070206@statistik.uni-dortmund.de>

Andrew C. Ward wrote:
> You may prefer to use S-PLUS if it does precisely what you
> want. In R, you could use postscript() or pdf() to save all
> the graphs to a file and then view them at your leisure.
> There is always par(ask=TRUE) if you wanted to look at them
> on the screen.
> 

... or you might want to start a new device for each plot.

Uwe Ligges


> Regards,
> 
> Andrew C. Ward
> 
> CAPE Centre
> Department of Chemical Engineering
> The University of Queensland
> Brisbane Qld 4072 Australia
> andreww at cheque.uq.edu.au
> 
> 
> Quoting array chip <arrayprofile at yahoo.com>:
> 
> 
>>Hi,
>>
>>I am running a loop to plot multiple plots. In s-plus,
>>it shows multiple pages in the graphic window to allow
>>checking on each plot. but in R, the next plot always
>>overwrite the previous one, so i can only have the
>>last plot produced, is there a way to have multiple
>>pages in the graphic window just like S-plus does?
>>
>>Thanks
>>
>>______________________________________________
>>R-help at stat.math.ethz.ch mailing list
>>https://www.stat.math.ethz.ch/mailman/listinfo/r-help
>>
> 
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help



From p.dalgaard at biostat.ku.dk  Fri Aug 22 09:33:48 2003
From: p.dalgaard at biostat.ku.dk (Peter Dalgaard BSA)
Date: Fri, 22 Aug 2003 07:33:48 -0000
Subject: [R] Reducing matrix dimension
In-Reply-To: <Pine.SOL.4.44.0308212343030.23741-100000@mspacman.gpcc.itd.umich.edu>
References: <Pine.SOL.4.44.0308212343030.23741-100000@mspacman.gpcc.itd.umich.edu>
Message-ID: <x2ptiyxho7.fsf@biostat.ku.dk>

Thomas W Blackwell <tblackw at umich.edu> writes:

> help("Subscript")

Or, princomp or factanal, depending on what was meant...

> > I have a 12(cols) by 9(rows) matrix X.  I need to reduce this matrix so that
> > it contains 'n' columns (eg.  reduce X into a 3 by 9 matrix).  What is the
> > best method to do this in R?


-- 
   O__  ---- Peter Dalgaard             Blegdamsvej 3  
  c/ /'_ --- Dept. of Biostatistics     2200 Cph. N   
 (*) \(*) -- University of Copenhagen   Denmark      Ph: (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)             FAX: (+45) 35327907



From bhx2 at mevik.net  Fri Aug 22 10:29:55 2003
From: bhx2 at mevik.net (=?iso-8859-1?q?Bj=F8rn-Helge_Mevik?=)
Date: Fri, 22 Aug 2003 10:29:55 +0200
Subject: [R] anova(lme object)
In-Reply-To: <3F450B10.5080007@pdf.com> (Spencer Graves's message of "Thu,
	21 Aug 2003 11:10:24 -0700")
References: <20030821165117.74374.qmail@web41201.mail.yahoo.com>
	<7o1xve52xq.fsf@foo.nemo-project.org> <3F450B10.5080007@pdf.com>
Message-ID: <7od6eyhz7g.fsf@foo.nemo-project.org>

Spencer Graves <spencer.graves at PDF.COM> writes:

> You need to say "library(nlme)" first.

Of course.  But since he has alreade used lme to fit the models, he
must have loaded the library already.  :-)

-- 
Bj?rn-Helge Mevik



From kv_padmanabha at yahoo.com  Fri Aug 22 12:04:56 2003
From: kv_padmanabha at yahoo.com (K.V.ANANTHA PADMANABHA)
Date: Fri, 22 Aug 2003 03:04:56 -0700 (PDT)
Subject: [R] Your help plz - (statistical problem)
Message-ID: <20030822100456.95892.qmail@web10101.mail.yahoo.com>

Hi David,

Please help me out to resolve the following
statistical problem.

For the following data 

X         Y
0        21.5
0        21.78
0.03     21.5
0.09     16.3
0.27     9.9
0.81     6.13
2.43     5.17

How to put the logistic curve was fitted for the
following model

          C
Y = --------------
      (1 + (X/X0)^b)


Where Y is the total no. of juveniles per parent
animals alive at the end of the test and X is the
concentartion.

C = the expected no. of juveniles when X = 0
X0 = the EC50 in the population
b = the slope parameter



I expect the results as early as possible.

Thanks & regards

K.V.Anantha Padmanabha
Rallis India Limited



From ripley at stats.ox.ac.uk  Fri Aug 22 12:13:22 2003
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Fri, 22 Aug 2003 11:13:22 +0100 (BST)
Subject: [R] Problems with addition in big POSIX dates
In-Reply-To: <16187.42402.823006.961116@gargle.gargle.HOWL>
Message-ID: <Pine.LNX.4.44.0308221108330.25339-100000@gannet.stats>

This is simply a bug in the code that is supposed to work around these OS
limitations: on some platforms mktime was resetting the wday during the
computations to try to work on the timezone in force: using GMT works
fine.

> unlist(unclass(as.POSIXlt(test.date+24*3600, "GMT")))
  sec   min  hour  mday   mon  year  wday  yday isdst
    0     0     0     2     0   140     1     1     0

I've put a fix in for 1.8.0.

I cannot reproduce Martin's result (on RH8.0 or Solaris or Windows, and
that seems to indicate some other problem on that platform): perhaps MM 
can test under the fixed code?

On Thu, 14 Aug 2003, Martin Maechler wrote:

> >>>>> "Whit" == Whit Armstrong <Whit.Armstrong at tudor.com>
> >>>>>     on Wed, 13 Aug 2003 15:23:58 -0400 writes:
> 
>     Whit> Have you noticed any problems with big dates (>=1/1/2040) in R?
>     Whit> Here is the bit of code that I'm having trouble with:
> 
>     >> test.date <- strptime("1/1/2040",format="%m/%d/%Y")
>     >> 
>     >> unlist(test.date)
>     Whit> sec   min  hour  mday   mon  year  wday  yday isdst 
>     Whit> 0     0     0     1     0   140     0     0     0 
>     >> 
>     >> date.plus.one <- as.POSIXct(test.date) + 24*60*60
>     >> date.plus.one.lt <- as.POSIXlt(date.plus.one)
>     >> 
>     >> unlist(date.plus.one.lt)
>     Whit> sec   min  hour  mday   mon  year  wday  yday isdst 
>     Whit> 0     0     0     2     0   140     0     1     0 
> 
>     Whit> Notice that wday (the weekday, 0=Sunday, 7=Saturday) doesn't change.
> 
>     Whit> Am I missing something?
> 
> Probably the "C library millenium bug" (not official name).
> The C library standard type for timedates, "time_t", is
> "usually" encoded using 32-bit integers, measuring seconds
> since the beginning of 1970. 
> 
> It has been well-known for years that this will lead to integer
> overflow from 19 Jan 2038 :
> 
>   > as.POSIXct(strptime("1/1/1970",format="%m/%d/%Y"))+ .Machine$integer.max
>   [1] "2038-01-19 03:14:07 CET"
> 
> I had thought that the R implementation carefully used doubles
> instead of integers everywhere, but I guess we somewhere rely on
> system-internal things even here.
> 
> For me, on Intel- Linux (RH 7.3, newer gcc) it's even worse:
> 
> > unlist(date.plus.one.lt)
>   sec   min  hour  mday   mon  year  wday  yday isdst 
>     0     0     0     2     0   140     6     0     0 
> 				       ~~~~~~~~~
> i.e. `wday' has been counted backwards, and `yday' has remained at 0.
> 
> -----
> 
> I guess we have to wait for 64-bit implementations of "time_t"
> or write our own code that works around the many C-library-bugs
> we (the R community) have encountered concerning POSIX-time
> implementations.
> Prof Brian Ripley will know more on this..
> 
> Martin Maechler <maechler at stat.math.ethz.ch>	http://stat.ethz.ch/~maechler/
> Seminar fuer Statistik, ETH-Zentrum  LEO C16	Leonhardstr. 27
> ETH (Federal Inst. Technology)	8092 Zurich	SWITZERLAND
> phone: x-41-1-632-3408		fax: ...-1228			<><
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> 

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From maechler at stat.math.ethz.ch  Fri Aug 22 13:06:15 2003
From: maechler at stat.math.ethz.ch (Martin Maechler)
Date: Fri, 22 Aug 2003 13:06:15 +0200
Subject: [R] automatic logging of commands
In-Reply-To: <Pine.LNX.4.44.0308212031580.15501-100000@gannet.stats>
References: <BB6A95C4.20FE%dbickel@mcg.edu>
	<Pine.LNX.4.44.0308212031580.15501-100000@gannet.stats>
Message-ID: <16197.63783.630556.724231@gargle.gargle.HOWL>

>>>>> "BDR" == Prof Brian Ripley <ripley at stats.ox.ac.uk>
>>>>>     on Thu, 21 Aug 2003 20:36:24 +0100 (BST) writes:

    BDR> On Thu, 21 Aug 2003, David R. Bickel wrote:
    >> The piping UNIX command that I just suggested is better
    >> than nothing, but writes backspaces or other invisible
    >> characters to the file, sometimes in place of characters
    >> that were visible on the screen.

    BDR> Under actual UNIX, that is not so: the characters in
    BDR> the file are those sent to the terminal.  Perhaps your
    BDR> terminal does things you do not expect.

    BDR> Many decent UNIX terminal windows allow you to save
    BDR> their contents, BTW.

    >> Some kind of R command that writes everything that
    >> appears on the screen to a file would be better.

    BDR> You can always contribute one (see the R startup
    BDR> banner).  Not that I think this is the job of a
    BDR> statistics package under UNIX, when other tools exist:
    BDR> you might also like to investigate ESS under Emacs,
    BDR> which can I believe do what you want.

Definitely.  Just write the *R* buffer to a file (C-x C-w) even
continuing using it.
"Use ESS!" would have been my answer to the original question.

Martin Maechler <maechler at stat.math.ethz.ch>	http://stat.ethz.ch/~maechler/
Seminar fuer Statistik, ETH-Zentrum  LEO C16	Leonhardstr. 27
ETH (Federal Inst. Technology)	8092 Zurich	SWITZERLAND
phone: x-41-1-632-3408		fax: ...-1228			<><



From ripley at stats.ox.ac.uk  Fri Aug 22 13:06:30 2003
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Fri, 22 Aug 2003 12:06:30 +0100 (BST)
Subject: [R] unclass
In-Reply-To: <3F3E4C74.8080006@pdf.com>
Message-ID: <Pine.LNX.4.44.0308221205120.26786-100000@gannet.stats>

On Sat, 16 Aug 2003, Spencer Graves wrote:

> I found an answer in "?unclass" and a minor difference between R 1.7.1 
> and S-Plus 6.1.2:

> Now, a trivial comparison between R 1.7.1 and S-Plus 6.1.2:
> 
> ##R 1.7.1
>  > tst <- 2
>  > class(tst)
> [1] "numeric"
>  > Tst <- unclass(tst)
>  > class(Tst)
> [1] "numeric"
>  > mode(Tst)
> [1] "numeric"
> 
> ## Same thing in S-Plus 6.1.2:
>  > tst <- 2
>  > class(tst)
> [1] "integer"
>  > Tst <- unclass(tst)
>  > class(Tst)
> [1] "integer"
>  > mode(Tst)
> [1] "numeric"
> 
> Note that "tst" and "Tst" have class and mode "numeric" in R 1.7.1 but 
> class "integer" and mode "numeric" in S-Plus 6.1.2.

That's because `2' is integer in S-PLUS >=5 and double in R and S-PLUS < 5.
Since you appear not to know that, try it with 2 and 2.0.

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From mailinglist.wegmann at gmx.net  Fri Aug 22 15:33:16 2003
From: mailinglist.wegmann at gmx.net (Martin Wegmann)
Date: Fri, 22 Aug 2003 13:33:16 +0000
Subject: [R] gam step in grasper
Message-ID: <200308221333.16540.mailinglist.wegmann@gmx.net>

hello, 

some weeks ago I asked if there is an equivalent of step(lm) for gam, and 
Simon Wood informed me that there isn't but it will eventually be done.

Now I found grasp.step.gam {grasper} and wonder if it would be possible to 
rewrite/extract (? I don't now how it is called or how it works - I am not 
experienced in programming) this command for the use outside GRASP-R. 
Perhaps this way is less work intensive but of course this command doesn't use 
GCV/UBRE scores but anova.
Is there an argument not to use this function inside GRASP-R even though it's 
purpose is not spatial?

thanks for your help, cheers Martin



From jc at or.psychology.dal.ca  Fri Aug 22 14:18:15 2003
From: jc at or.psychology.dal.ca (John Christie)
Date: Fri, 22 Aug 2003 09:18:15 -0300
Subject: [R] a pickle (solved first part now need r's from data)
In-Reply-To: <4325A1A4-D45B-11D7-AE7A-000A9566473A@or.psychology.dal.ca>
Message-ID: <B4A7D215-D49A-11D7-AE7A-000A9566473A@or.psychology.dal.ca>

On Friday, August 22, 2003, at 01:44  AM, John Christie wrote:

> I predicted that y would increase as x increased.  However, I only 
> made the prediction on the ranks of the scores.  The ranks don't 
> correlate with predicted.  And, I don't think a regression on the 
> ranks is warranted.  However, the actual scores do yield a significant 
> slope for b, and a significant R^2 using a linear regression (y is the 
> value and x is the predicted rank).  What should my argument be here?  
> Should I have endorsed using the actual scores instead of ranks to 
> begin for some reason that doesn't have anything to do with my current 
> result? :)

OK, now I realize that I should probably not have been correlating 
ranks in the first place because my real data may have had a 
non-linear, but still steadily increasing, slope.  The ranks would tend 
to increase variance where the slope was low and ruined my chance of 
finding an effect.

> Oh, on another note, I can use rcorr to get the Spearman correlations, 
> but I'd like to be able to just add
> the ranks as a column.  I was going to just use order and add a simple 
> factor.  But, that doesn't deal with ties correctly.

still don't have these yet.

> And, I also wanted to analyze correlations subject by subject and 
> compare my two groups.  However, there doesn't seem to be a good way 
> to get this.  I tried using "by" with "cor".  However, this requires 
> binding x and y which causes cor to return a matrix (if you could pass 
> it x and y separate it would just return a number).
>
> given
>
> data frame s
> x	y	subj
> 4	7	harry
> 5	1	harry
> 6	9	harry
> 2	4	steve
> 3	7	steve
> ...
>
> i'd like to be able to produce
>
> r	subj
> .12	harry
> .52	steve
> ...
>
> any tips?



From roger at ysidro.econ.uiuc.edu  Fri Aug 22 14:27:48 2003
From: roger at ysidro.econ.uiuc.edu (Roger Koenker)
Date: Fri, 22 Aug 2003 07:27:48 -0500 (CDT)
Subject: [R] Re: Oja median
In-Reply-To: <20030820045910.04260480A@sitemail.everyone.net>
Message-ID: <Pine.SOL.4.30.0308220702410.8866-100000@ysidro.econ.uiuc.edu>

A report on my Oja Median problem:

Gabor Grothendieck suggested replacing my loopy cofactor function with:

 cofactors2 <- function(x) {
 	x <- t(rbind(1,cbind(x,1)))
 	p <- nrow(x)
 	solve( x, (seq(p)==p)+0) * det(x)
 }

Which is much better, especially if the dimension p is large.  Unfortunately,
it is still quite slow inside the apply loop of the main function, and
this suggests that it is probably necessary to recode in C or Fortran
to get something that would work well in realistic problems.  In case
anyone else would like to play with this...here is the original version
again and a timing comparison:

	"mean.wilks" <- function(x){
		# Computes the column means of the matrix x -- very sloooowly.
		n <- dim(x)[1]
	        p <- dim(x)[2]
	        A <- t(combn(1:n,p))
		X <- NULL
		for(i in 1:p)
			X <- cbind(X,x[A[,i],])
		oja.ize <- function(v)cofactors(matrix(v,sqrt(length(v))))
		A <- t(apply(X,1,oja.ize))
		coef(lm(-A[,1]~A[,-1]-1))
		}
	"cofactors" <- function(A){
		B <- rbind(1,cbind(A,1))
		p <- ncol(B)
		x <- rep(0,p)
		for(i in 1:p)
			x[i] <- ((-1)^(i+p)) *det(B[-i,-p])
		return(x)
		}
______________________________________

X <- matrix(rnorm(150),50)
t1 <- unix.time(mean.wilks(X)->m1)
t2 <- unix.time(mean.wilks2(X)->m2) # this uses cofactors2
t3 <- unix.time(apply(X,2,mean)->m3)
> m1
   A[, -1]1    A[, -1]2    A[, -1]3
 0.09205914  0.05603060 -0.24290857
> m2
   A[, -1]1    A[, -1]2    A[, -1]3
 0.09205914  0.05603060 -0.24290857
> m3
[1]  0.09205914  0.05603060 -0.24290857
> t1
[1] 147.70  25.33 173.41   0.00   0.00
> t2
[1] 108.97  21.87 131.23   0.00   0.00
> t3
[1] 0 0 0 0 0

NB.  A reminder that this mean version is only a testing ground for
the median version of Oja which replaces the lm() call with a median
regression call.

url:	www.econ.uiuc.edu/~roger/my.html	Roger Koenker
email	rkoenker at uiuc.edu			Department of Economics
vox: 	217-333-4558				University of Illinois
fax:   	217-244-6678				Champaign, IL 61820



From tblackw at umich.edu  Fri Aug 22 15:03:47 2003
From: tblackw at umich.edu (Thomas W Blackwell)
Date: Fri, 22 Aug 2003 09:03:47 -0400 (EDT)
Subject: [R] a pickle with ranks and reals?
In-Reply-To: <4325A1A4-D45B-11D7-AE7A-000A9566473A@or.psychology.dal.ca>
Message-ID: <Pine.SOL.4.44.0308220810040.12007-100000@robotron.gpcc.itd.umich.edu>

John  -

Here are two equivalent solutions to your final question:

data <- data.frame(x=seq(15), y=sample(seq(15), 15),
	 	subj=sample(c("harry","steve","nathan","john"), 15, T))

result.1 <- unclass(by(data, data$subj, function(dd) cor(dd$x, dd$y)))

result.2 <- unclass(by(data, data$subj, function(dd) cor(dd[c(1,2)])[1,2]))

I guess I prefer  result.1  since the code is easier to read,
even though it does bury literal column names into the code.

The "function(dd)" stuff is a very common construction in  by(),
sapply(), lapply()  constructs.  It defines a little function
in-line, without ever naming it, and passes it as the third
argument to  by().  I use this all the time, when I need to
rearrange the order, or do a little bit of subscripting (as here),
in the arguments of a function (cor()) which I would otherwise
just pass directly as the third argument to  by().

I'll let others comment on my use of  unclass()  here.  The
goal was to get a numeric vector with a names attribute, so
it can be incorporated into further processing.  I'm surprised
just how much tinkering it took to get this all to work.

This might actually make a useful example to add to the help
page for  by().

-  tom blackwell  -  u michigan medical school  -  ann arbor  -

On Fri, 22 Aug 2003, John Christie wrote:

> . . .  And, I also wanted to analyze correlations subject by subject and
> compare my two groups.  However, there doesn't seem to be a good way to
> get this.  I tried using "by" with "cor".  However, this requires
> binding x and y which causes cor to return a matrix (if you could pass
> it x and y separate it would just return a number).
>
> given
>
> data frame s
> x	y	subj
> 4	7	harry
> 5	1	harry
> 6	9	harry
> 2	4	steve
> 3	7	steve
> ...
>
> i'd like to be able to produce
>
> r	subj
> .12	harry
> .52	steve
> ...
>
> any tips?



From HADASSA.BRUNSCHWIG at Roche.COM  Fri Aug 22 13:49:48 2003
From: HADASSA.BRUNSCHWIG at Roche.COM (Brunschwig, Hadassa {PDMM~Basel})
Date: Fri, 22 Aug 2003 13:49:48 +0200
Subject: [R] boot function
Message-ID: <BC6A439CD6835749A9C7B8D8F041DFA88AFCA1@rbamsem1.emea.roche.com>

Hi all,

I skimmed through the archives and couldnt really find an answer to my question. One thing i dont understand of the description of the function boot() is the second variable for statistics. I have a sample of say 19 subjects out of these, using boot(), i would like to generate say 1000 samples. For these 1000 samples ill calculate an nlme() and ill use these 1000 estimators of a variable to make further calculation. Now what i dont understand is where the index should be set. the nlme() looks like this:

nlme(Concentr~a*(1-exp(Day*(log(0.1,base=exp(1))/exp(logt09))))
                              ,data=data
                              ,fixed=a+logt09~1
                              ,random=a+logt09~1|Subject[ind]
                              ,start=list(fixed=c(a=30,logt09=1)))

My idea was to put the index ( second variable of the statistcs function) in the subject as i want to generate different samples of subjects. I get the error that the vector ind was not found. I would be happy for any help concerning this problem.

Thanks a lot

Dassy



From ripley at stats.ox.ac.uk  Fri Aug 22 15:39:28 2003
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Fri, 22 Aug 2003 14:39:28 +0100 (BST)
Subject: [R] bootstrapping nlme fits (was boot function)
In-Reply-To: <BC6A439CD6835749A9C7B8D8F041DFA88AFCA1@rbamsem1.emea.roche.com>
Message-ID: <Pine.LNX.4.44.0308221430140.32296-100000@gannet.stats>

First, this has very little to do with boot: PLEASE use an infromative
subject line.  You need to work out how to resample in this situation: are
you resampling subjects or observations?  If you are resampling subjects,
you need to create a data frame containing just the resampled subjects and
pass that to nlme.

However, you also need to think if this is valid.  If you resample 
subjects, you will be fitting subjects twice or more as if they are 
independent.  I know of no theoretical studies on resampling mixed-effects 
models, and urge you to look for such results.

On Fri, 22 Aug 2003, Brunschwig, Hadassa {PDMM~Basel} wrote:

> I skimmed through the archives and couldnt really find an answer to my
> question. 

It's not an R question.

> One thing i dont understand of the description of the function
> boot() is the second variable for statistics. I have a sample of say 19
> subjects out of these, using boot(), i would like to generate say 1000
> samples. For these 1000 samples ill calculate an nlme() and ill use
> these 1000 estimators of a variable to make further calculation. 

Whether this is valid most likely depends on what those calculations are.

> Now
> what i dont understand is where the index should be set. the nlme()
> looks like this:
> 
> nlme(Concentr~a*(1-exp(Day*(log(0.1,base=exp(1))/exp(logt09))))
>                               ,data=data
>                               ,fixed=a+logt09~1
>                               ,random=a+logt09~1|Subject[ind]
>                               ,start=list(fixed=c(a=30,logt09=1)))
> 
> My idea was to put the index ( second variable of the statistcs
> function) 

What that variable means depends on the other arguments to boot, and you 
haven't told us what those are.

> in the subject as i want to generate different samples of
> subjects. I get the error that the vector ind was not found. I would be
> happy for any help concerning this problem.



-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From d.scofield at umiami.edu  Fri Aug 22 15:40:57 2003
From: d.scofield at umiami.edu (Douglas G. Scofield)
Date: Fri, 22 Aug 2003 09:40:57 -0400
Subject: [R] means comparison with seasonal time series?
Message-ID: <001e01c368b3$07bff080$aacdab81@genetics5>

Dear R list,

I have a sequence of weekly observations of number of adults and larvae
in various size classes from a butterfly population living in a
subtropical area with pronounced wet and dry seasons.  Wet and dry
seasons are each defined 26 weeks long with fixed start and end dates.
The data span 103 weeks (two seasons each of wet and dry) with some
missing weeks.  What I would like to do is compare means of each type of
observation between wet and dry seasons ("Does the number of adults
observed vary by season?").  Not surprisingly there is pronounced
autocorrelation in the data, e.g.:

   dwtest(lm(numadults ~ week))

gives

   DW = 0.2727, p-value = < 2.2e-16

Note that the effects of this autocorrelation extend across wet-dry and
dry-wet boundaries.

What would be a way to ask my questions in R?  It seems like I'd use
nlme and define a corStruct, but it's unclear to me how I'd do that.
I'm still learning R (and  statistics) so detailed answers would be most
appreciated.

Sincerely,

Douglas Scofield                 Department of Biology 
d.scofield at umiami.edu            University of Miami
off: (305) 284-3778              P.O. Box 249118
fax: (305) 284-3039              Coral Gables, FL  33124-0421



From fharrell at virginia.edu  Fri Aug 22 16:18:53 2003
From: fharrell at virginia.edu (Frank E Harrell Jr)
Date: Fri, 22 Aug 2003 10:18:53 -0400
Subject: [R] bootstrapping nlme fits (was boot function)
In-Reply-To: <Pine.LNX.4.44.0308221430140.32296-100000@gannet.stats>
References: <BC6A439CD6835749A9C7B8D8F041DFA88AFCA1@rbamsem1.emea.roche.com>
	<Pine.LNX.4.44.0308221430140.32296-100000@gannet.stats>
Message-ID: <20030822101853.2b6800c0.fharrell@virginia.edu>

On Fri, 22 Aug 2003 14:39:28 +0100 (BST)
Prof Brian Ripley <ripley at stats.ox.ac.uk> wrote:

> First, this has very little to do with boot: PLEASE use an infromative
> subject line.  You need to work out how to resample in this situation: are
> you resampling subjects or observations?  If you are resampling subjects,
> you need to create a data frame containing just the resampled subjects and
> pass that to nlme.
> 
> However, you also need to think if this is valid.  If you resample 
> subjects, you will be fitting subjects twice or more as if they are 
> independent.  I know of no theoretical studies on resampling mixed-effects 
> models, and urge you to look for such results.
> 
> On Fri, 22 Aug 2003, Brunschwig, Hadassa {PDMM~Basel} wrote:

Hadassa - You may want to look at the slightly simpler generalized least squares with correlated observations case.  For that I have a bootstrap option in the Design packages's glsD function (which uses the nlme package).  There is an option to treat multiply-sampled subjects as if they were different subjects, or to pool them into one larger subject (I think the former is more correct but I haven't gotten very far in this thinking).  You can do simulations with glsD to check the performance of the cluster-sampling bootstrap in this situation.  I have done limited simulations and bootstrap variance estimates seem to be close to actual values, although not as close as information-matrix-based estimates when the model is true.  glsD attempts to implement the cluster bootstrap fairly efficiently, although it does not yet work for the case where an across-time covariance pattern is not assumed.

Frank Harrell

> 
> > I skimmed through the archives and couldnt really find an answer to my
> > question. 
> 
> It's not an R question.
> 
> > One thing i dont understand of the description of the function
> > boot() is the second variable for statistics. I have a sample of say 19
> > subjects out of these, using boot(), i would like to generate say 1000
> > samples. For these 1000 samples ill calculate an nlme() and ill use
> > these 1000 estimators of a variable to make further calculation. 
> 
> Whether this is valid most likely depends on what those calculations are.
> 
> > Now
> > what i dont understand is where the index should be set. the nlme()
> > looks like this:
> > 
> > nlme(Concentr~a*(1-exp(Day*(log(0.1,base=exp(1))/exp(logt09))))
> >                               ,data=data
> >                               ,fixed=a+logt09~1
> >                               ,random=a+logt09~1|Subject[ind]
> >                               ,start=list(fixed=c(a=30,logt09=1)))
> > 
> > My idea was to put the index ( second variable of the statistcs
> > function) 
> 
> What that variable means depends on the other arguments to boot, and you 
> haven't told us what those are.
> 
> > in the subject as i want to generate different samples of
> > subjects. I get the error that the vector ind was not found. I would be
> > happy for any help concerning this problem.
> 
> 
> 
> -- 
> Brian D. Ripley,                  ripley at stats.ox.ac.uk
> Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
> University of Oxford,             Tel:  +44 1865 272861 (self)
> 1 South Parks Road,                     +44 1865 272866 (PA)
> Oxford OX1 3TG, UK                Fax:  +44 1865 272595
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help


---
Frank E Harrell Jr              Prof. of Biostatistics & Statistics
Div. of Biostatistics & Epidem. Dept. of Health Evaluation Sciences
U. Virginia School of Medicine  http://hesweb1.med.virginia.edu/biostat



From ernesto at ipimar.pt  Fri Aug 22 17:26:55 2003
From: ernesto at ipimar.pt (Ernesto Jardim)
Date: Fri, 22 Aug 2003 16:26:55 +0100
Subject: [R] problem compiling R in suse8.2
Message-ID: <1061566015.12658.1.camel@linux.local>

Hi

I'm trying to compile R in SuSE 8.2 (updated with the gcc 3.3.1) but I'm
getting the following error:

/usr/X11R6/include/X11/keysymdef.h:1181:2: invalid preprocessing
directive #d
make[4]: *** [dataentry.lo] Error 1
make[4]: Leaving directory
`/usr/local/src/compile/R-1.7.1/src/modules/X11'
make[3]: *** [R] Error 2
make[3]: Leaving directory
`/usr/local/src/compile/R-1.7.1/src/modules/X11'
make[2]: *** [R] Error 1
make[2]: Leaving directory `/usr/local/src/compile/R-1.7.1/src/modules'
make[1]: *** [R] Error 1
make[1]: Leaving directory `/usr/local/src/compile/R-1.7.1/src'
make: *** [R] Error 1


Can someone give a hint on this ?

Thanks

EJ



From spencer.graves at pdf.com  Fri Aug 22 17:57:05 2003
From: spencer.graves at pdf.com (Spencer Graves)
Date: Fri, 22 Aug 2003 08:57:05 -0700
Subject: [R] Your help plz - (statistical problem)
References: <20030822100456.95892.qmail@web10101.mail.yahoo.com>
Message-ID: <3F463D51.2050402@pdf.com>

Have you considered "nls" and / or "optim"?

hope this helps.  spencer graves

K.V.ANANTHA PADMANABHA wrote:
> Hi David,
> 
> Please help me out to resolve the following
> statistical problem.
> 
> For the following data 
> 
> X         Y
> 0        21.5
> 0        21.78
> 0.03     21.5
> 0.09     16.3
> 0.27     9.9
> 0.81     6.13
> 2.43     5.17
> 
> How to put the logistic curve was fitted for the
> following model
> 
>           C
> Y = --------------
>       (1 + (X/X0)^b)
> 
> 
> Where Y is the total no. of juveniles per parent
> animals alive at the end of the test and X is the
> concentartion.
> 
> C = the expected no. of juveniles when X = 0
> X0 = the EC50 in the population
> b = the slope parameter
> 
> 
> 
> I expect the results as early as possible.
> 
> Thanks & regards
> 
> K.V.Anantha Padmanabha
> Rallis India Limited
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help



From Mailer-Daemon at dkfz-heidelberg.de  Fri Aug 22 18:37:26 2003
From: Mailer-Daemon at dkfz-heidelberg.de (Mailer-Daemon@dkfz-heidelberg.de)
Date: Fri, 22 Aug 2003 18:37:26 +0200 (CEST)
Subject: [R] MAIL DELIVERY NOTIFICATION
Message-ID: <200308221637.h7MGbQr07994@krebs.inet.dkfz-heidelberg.de>

Dear Internet Email User,

this is an automated mail delivery notification,
because your email has not been delivered the usual way.

Your mail was not deliverd for the following reason:


Your mail contained an attachment with the filename extension 

    pif


Since such content may contain viruses or other dangerous code
it was blocked by the firewall.




Best regards,

Your GeNUGate Mail Gateway


========================================================= 
                 ENVELOPE INFORMATION
========================================================= 
FROM = R-help at stat.math.ethz.ch
RCPT = g.wrobel at dkfz-heidelberg.de

========================================================= 
                 HEADER INFORMATION
========================================================= 
Received: from OEMCOMPUTER (80.11.47.17) by  (smtprelay) with ESMTP Fri Aug 22 18:37:02 2003.
From: <R-help at stat.math.ethz.ch>
To: <g.wrobel at dkfz-heidelberg.de>
Subject: Thank you!
Date: Fri, 22 Aug 2003 18:36:52 +0200
X-MailScanner: Found to be clean
Importance: Normal
X-Mailer: Microsoft Outlook Express 6.00.2600.0000
X-MSMail-Priority: Normal
X-Priority: 3 (Normal)
MIME-Version: 1.0
Content-Type: multipart/mixed;
	boundary="_NextPart_000_0244F2A4"



From kjetil at entelnet.bo  Fri Aug 22 18:36:28 2003
From: kjetil at entelnet.bo (kjetil brinchmann halvorsen)
Date: Fri, 22 Aug 2003 12:36:28 -0400
Subject: [R] bootstrapping nlme fits (was boot function)
In-Reply-To: <20030822101853.2b6800c0.fharrell@virginia.edu>
References: <Pine.LNX.4.44.0308221430140.32296-100000@gannet.stats>
Message-ID: <3F460E4C.9922.44C3D2@localhost>

On 22 Aug 2003 at 10:18, Frank E Harrell Jr wrote:

The following danish web page 
http://www.dina.dk/phd/s/s2/s2pr1.htm

gives an example of bootstrapping nlme models. If what they are doing 
is vali, I don't know, I abstained from it since I do not understand 
it.

Kjetil Halvorsen


> On Fri, 22 Aug 2003 14:39:28 +0100 (BST)
> Prof Brian Ripley <ripley at stats.ox.ac.uk> wrote:
> 
> > First, this has very little to do with boot: PLEASE use an infromative
> > subject line.  You need to work out how to resample in this situation: are
> > you resampling subjects or observations?  If you are resampling subjects,
> > you need to create a data frame containing just the resampled subjects and
> > pass that to nlme.
> > 
> > However, you also need to think if this is valid.  If you resample 
> > subjects, you will be fitting subjects twice or more as if they are 
> > independent.  I know of no theoretical studies on resampling mixed-effects 
> > models, and urge you to look for such results.
> > 
> > On Fri, 22 Aug 2003, Brunschwig, Hadassa {PDMM~Basel} wrote:
> 
> Hadassa - You may want to look at the slightly simpler generalized least squares with correlated observations case.  For that I have a bootstrap option in the Design packages's glsD function (which uses the nlme package).  There is an option to treat multiply-sampled subjects as if they were 
different subjects, or to pool them into one larger subject (I think the former is more correct but I haven't gotten very far in this thinking).  You can do simulations with glsD to check the performance of the cluster-sampling bootstrap in this situation.  I have done limited simulations and 
bootstrap variance estimates seem to be close to actual values, although not as close as information-matrix-based estimates when the model is true.  glsD attempts to implement the cluster bootstrap fairly efficiently, although it does not yet work for the case where an across-time covariance 
pattern is not assumed.
> 
> Frank Harrell
> 
> > 
> > > I skimmed through the archives and couldnt really find an answer to my
> > > question. 
> > 
> > It's not an R question.
> > 
> > > One thing i dont understand of the description of the function
> > > boot() is the second variable for statistics. I have a sample of say 19
> > > subjects out of these, using boot(), i would like to generate say 1000
> > > samples. For these 1000 samples ill calculate an nlme() and ill use
> > > these 1000 estimators of a variable to make further calculation. 
> > 
> > Whether this is valid most likely depends on what those calculations are.
> > 
> > > Now
> > > what i dont understand is where the index should be set. the nlme()
> > > looks like this:
> > > 
> > > nlme(Concentr~a*(1-exp(Day*(log(0.1,base=exp(1))/exp(logt09))))
> > >                               ,data=data
> > >                               ,fixed=a+logt09~1
> > >                               ,random=a+logt09~1|Subject[ind]
> > >                               ,start=list(fixed=c(a=30,logt09=1)))
> > > 
> > > My idea was to put the index ( second variable of the statistcs
> > > function) 
> > 
> > What that variable means depends on the other arguments to boot, and you 
> > haven't told us what those are.
> > 
> > > in the subject as i want to generate different samples of
> > > subjects. I get the error that the vector ind was not found. I would be
> > > happy for any help concerning this problem.
> > 
> > 
> > 
> > -- 
> > Brian D. Ripley,                  ripley at stats.ox.ac.uk
> > Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
> > University of Oxford,             Tel:  +44 1865 272861 (self)
> > 1 South Parks Road,                     +44 1865 272866 (PA)
> > Oxford OX1 3TG, UK                Fax:  +44 1865 272595
> > 
> > ______________________________________________
> > R-help at stat.math.ethz.ch mailing list
> > https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> 
> 
> ---
> Frank E Harrell Jr              Prof. of Biostatistics & Statistics
> Div. of Biostatistics & Epidem. Dept. of Health Evaluation Sciences
> U. Virginia School of Medicine  http://hesweb1.med.virginia.edu/biostat
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help



From bwmoore22 at yahoo.com  Fri Aug 22 20:14:46 2003
From: bwmoore22 at yahoo.com (Bruce Moore)
Date: Fri, 22 Aug 2003 11:14:46 -0700 (PDT)
Subject: [R] Problem running RTERM via SSH on Windows/2000
Message-ID: <20030822181446.75856.qmail@web13701.mail.yahoo.com>

I'm having problems getting RTERM to work via SSH. 
Whenever it has any type of problem, it abends instead
of issuing an error message and returning to the >
prompt.  Both "server" and client are Windows/2000
Professional at FP4.  SSH is via Cygwin on both sides.
 R is version is 1071.

RTERM runs fine when run in a BASH shell on the
"server," though it does not prompt for --save
--nosave or --vanilla.

Does anyone have any suggestions on how to proceed in
fixing the problem?



$ rterm --vanilla

R : Copyright 2003, The R Development Core Team
Version 1.7.1  (2003-06-16)

R is free software and comes with ABSOLUTELY NO
WARRANTY.
You are welcome to redistribute it under certain
conditions.
Type `license()' or `licence()' for distribution
details.

R is a collaborative project with many contributors.
Type `contributors()' for more information.

Type `demo()' for some demos, `help()' for on-line
help, or
`help.start()' for a HTML browser interface to help.
Type `q()' to quit R.

> library(xtable)
library(xtable)
> library(RODBC)
library(RODBC)
> xyz<-odbcConnect("xyz",uid="xyz",pwd="xyz")
xyz<-odbcConnect("xyz",uid="xyz",pwd="xyz")
Error: syntax error
Execution halted


Client SSH level
$ ssh -V
OpenSSH_3.4p1, SSH protocols 1.5/2.0, OpenSSL 0090607f


Host SSH level
$ ssh -V
OpenSSH_3.6.1p1, SSH protocols 1.5/2.0, OpenSSL 0x0090702f

=====
Bruce Moore



From bwmoore22 at yahoo.com  Fri Aug 22 20:25:57 2003
From: bwmoore22 at yahoo.com (Bruce Moore)
Date: Fri, 22 Aug 2003 11:25:57 -0700 (PDT)
Subject: [R] Passwords for RODBC package
Message-ID: <20030822182557.66151.qmail@web13707.mail.yahoo.com>

Is there a way to make the RODBC package prompt for a
password on the odbcConnect(db,uid="user1") call?  I'd
prefer not to have to use the pwd= parameter as this
is saved in .RHISTORY and .RDATA.

Is there a convenient way to provide the password as a
variable and have the variable automatically deleted
before .RDATA is saved?

Thanks in advance.

=====
Bruce Moore



From ripley at stats.ox.ac.uk  Fri Aug 22 20:36:33 2003
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Fri, 22 Aug 2003 19:36:33 +0100 (BST)
Subject: [R] Passwords for RODBC package
In-Reply-To: <20030822182557.66151.qmail@web13707.mail.yahoo.com>
Message-ID: <Pine.LNX.4.44.0308221935570.19555-100000@gannet.stats>

Use odbcDriverConnect.

On Fri, 22 Aug 2003, Bruce Moore wrote:

> Is there a way to make the RODBC package prompt for a
> password on the odbcConnect(db,uid="user1") call?  I'd
> prefer not to have to use the pwd= parameter as this
> is saved in .RHISTORY and .RDATA.
> 
> Is there a convenient way to provide the password as a
> variable and have the variable automatically deleted
> before .RDATA is saved?

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From ripley at stats.ox.ac.uk  Fri Aug 22 20:41:35 2003
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Fri, 22 Aug 2003 19:41:35 +0100 (BST)
Subject: [R] Problem running RTERM via SSH on Windows/2000
In-Reply-To: <20030822181446.75856.qmail@web13701.mail.yahoo.com>
Message-ID: <Pine.LNX.4.44.0308221937520.19555-100000@gannet.stats>

This is the documented behaviour for R when used non-interactively.
Presumably your `SSH' (probably really openssh) isn't using terminals for 
input and output.

You might like to try rterm --ess, a kludge for a similar problem in 
NTemacs.  Or try a different ssh (a real Windows one, not one designed for 
systems with ptys).

On Fri, 22 Aug 2003, Bruce Moore wrote:

> I'm having problems getting RTERM to work via SSH. 
> Whenever it has any type of problem, it abends instead
> of issuing an error message and returning to the >
> prompt.  Both "server" and client are Windows/2000
> Professional at FP4.  SSH is via Cygwin on both sides.
>  R is version is 1071.
> 
> RTERM runs fine when run in a BASH shell on the
> "server," though it does not prompt for --save
> --nosave or --vanilla.

It never does: no version of R prompts for those to my knowledge.

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From roger at ysidro.econ.uiuc.edu  Fri Aug 22 21:01:51 2003
From: roger at ysidro.econ.uiuc.edu (Roger Koenker)
Date: Fri, 22 Aug 2003 14:01:51 -0500 (CDT)
Subject: [R] Package Reference Manuals
Message-ID: <Pine.SOL.4.30.0308221357270.8866-100000@ysidro.econ.uiuc.edu>

Is it possible for the average user to generate the latex version
of the table of contents page that appears in the Package Reference
Manuals on the CRAN website?  Or is this (yet another) a Viennese speciality?

url:	www.econ.uiuc.edu/~roger/my.html	Roger Koenker
email	rkoenker at uiuc.edu			Department of Economics
vox: 	217-333-4558				University of Illinois
fax:   	217-244-6678				Champaign, IL 61820



From partha_bagchi at hgsi.com  Fri Aug 22 21:11:38 2003
From: partha_bagchi at hgsi.com (partha_bagchi@hgsi.com)
Date: Fri, 22 Aug 2003 15:11:38 -0400
Subject: [R] Substitute in legend
Message-ID: <OFF854FF5B.65487513-ON85256D8A.006944D2-85256D8A.00696F62@hgsi.com>

I tried to use substitute in legend as follows:

pval <- 0.04
plot(0)
legend(1,0.5,substitute(hat(theta) == p, list(p = pval)))

For some reason the legend is repeated 3 times. 

Any suggestions or is this a bug?



From governor at governor.ca.gov  Fri Aug 22 21:15:55 2003
From: governor at governor.ca.gov (governor@governor.ca.gov)
Date: Fri, 22 Aug 2003 12:15:55 -0700 (PDT)
Subject: [R] Thank you
Message-ID: <200308221915.h7MJFtL25507@fred.governor.ca.gov>

Thank you for your email.  I appreciate you taking the time to keep me informed of your opinions and the issues that are important to you.  Your comments help keep me informed as we strive to make California a better place in which to work and live.

Your email has been directed to members of my staff who assist constituents and report ideas and concerns daily.  If your concern is best handled by a specific department, your correspondence will be forwarded for action and response.

So that we can keep track of correspondence and ensure that we are able to respond to California residents, please be sure to include your name and address when you communicate with the Governor\'s Office or any state agency.  Please note that we do NOT accept email attachments, so your correspondence should be included within the text of your email.

Again, thank you for sharing your thoughts.  An informed and engaged citizenry is essential to the democratic process, and I appreciate your willingness to write me.

Sincerely,

Governor Gray Davis



From fharrell at virginia.edu  Fri Aug 22 21:14:33 2003
From: fharrell at virginia.edu (Frank E Harrell Jr)
Date: Fri, 22 Aug 2003 15:14:33 -0400
Subject: [R] bootstrapping nlme fits (was boot function)
In-Reply-To: <3F460E4C.9922.44C3D2@localhost>
References: <Pine.LNX.4.44.0308221430140.32296-100000@gannet.stats>
	<3F460E4C.9922.44C3D2@localhost>
Message-ID: <20030822151433.44ef8bce.fharrell@virginia.edu>

On Fri, 22 Aug 2003 12:36:28 -0400
"kjetil brinchmann halvorsen" <kjetil at entelnet.bo> wrote:

> On 22 Aug 2003 at 10:18, Frank E Harrell Jr wrote:
> 
> The following danish web page 
> http://www.dina.dk/phd/s/s2/s2pr1.htm
> 
> gives an example of bootstrapping nlme models. If what they are doing 
> is vali, I don't know, I abstained from it since I do not understand 
> it.
> 
> Kjetil Halvorsen

Thanks.  I use the unconditional bootstrap which does not assume a correlation structure and does not use residuals.

Frank

> 
> 
> > On Fri, 22 Aug 2003 14:39:28 +0100 (BST)
> > Prof Brian Ripley <ripley at stats.ox.ac.uk> wrote:
> > 
> > > First, this has very little to do with boot: PLEASE use an infromative
> > > subject line.  You need to work out how to resample in this situation: are
> > > you resampling subjects or observations?  If you are resampling subjects,
> > > you need to create a data frame containing just the resampled subjects and
> > > pass that to nlme.
> > > 
> > > However, you also need to think if this is valid.  If you resample 
> > > subjects, you will be fitting subjects twice or more as if they are 
> > > independent.  I know of no theoretical studies on resampling mixed-effects 
> > > models, and urge you to look for such results.
> > > 
> > > On Fri, 22 Aug 2003, Brunschwig, Hadassa {PDMM~Basel} wrote:
> > 
> > Hadassa - You may want to look at the slightly simpler generalized least squares with correlated observations case.  For that I have a bootstrap option in the Design packages's glsD function (which uses the nlme package).  There is an option to treat multiply-sampled subjects as if they were 
> different subjects, or to pool them into one larger subject (I think the former is more correct but I haven't gotten very far in this thinking).  You can do simulations with glsD to check the performance of the cluster-sampling bootstrap in this situation.  I have done limited simulations and 
> bootstrap variance estimates seem to be close to actual values, although not as close as information-matrix-based estimates when the model is true.  glsD attempts to implement the cluster bootstrap fairly efficiently, although it does not yet work for the case where an across-time covariance 
> pattern is not assumed.
> > 
> > Frank Harrell
> > 
> > > 
> > > > I skimmed through the archives and couldnt really find an answer to my
> > > > question. 
> > > 
> > > It's not an R question.
> > > 
> > > > One thing i dont understand of the description of the function
> > > > boot() is the second variable for statistics. I have a sample of say 19
> > > > subjects out of these, using boot(), i would like to generate say 1000
> > > > samples. For these 1000 samples ill calculate an nlme() and ill use
> > > > these 1000 estimators of a variable to make further calculation. 
> > > 
> > > Whether this is valid most likely depends on what those calculations are.
> > > 
> > > > Now
> > > > what i dont understand is where the index should be set. the nlme()
> > > > looks like this:
> > > > 
> > > > nlme(Concentr~a*(1-exp(Day*(log(0.1,base=exp(1))/exp(logt09))))
> > > >                               ,data=data
> > > >                               ,fixed=a+logt09~1
> > > >                               ,random=a+logt09~1|Subject[ind]
> > > >                               ,start=list(fixed=c(a=30,logt09=1)))
> > > > 
> > > > My idea was to put the index ( second variable of the statistcs
> > > > function) 
> > > 
> > > What that variable means depends on the other arguments to boot, and you 
> > > haven't told us what those are.
> > > 
> > > > in the subject as i want to generate different samples of
> > > > subjects. I get the error that the vector ind was not found. I would be
> > > > happy for any help concerning this problem.
> > > 
> > > 
> > > 
> > > -- 
> > > Brian D. Ripley,                  ripley at stats.ox.ac.uk
> > > Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
> > > University of Oxford,             Tel:  +44 1865 272861 (self)
> > > 1 South Parks Road,                     +44 1865 272866 (PA)
> > > Oxford OX1 3TG, UK                Fax:  +44 1865 272595
> > > 
> > > ______________________________________________
> > > R-help at stat.math.ethz.ch mailing list
> > > https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> > 
> > 
> > ---
> > Frank E Harrell Jr              Prof. of Biostatistics & Statistics
> > Div. of Biostatistics & Epidem. Dept. of Health Evaluation Sciences
> > U. Virginia School of Medicine  http://hesweb1.med.virginia.edu/biostat
> > 
> > ______________________________________________
> > R-help at stat.math.ethz.ch mailing list
> > https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help


---
Frank E Harrell Jr              Prof. of Biostatistics & Statistics
Div. of Biostatistics & Epidem. Dept. of Health Evaluation Sciences
U. Virginia School of Medicine  http://hesweb1.med.virginia.edu/biostat



From tblackw at umich.edu  Fri Aug 22 21:47:52 2003
From: tblackw at umich.edu (Thomas W Blackwell)
Date: Fri, 22 Aug 2003 15:47:52 -0400 (EDT)
Subject: [R] means comparison with seasonal time series?
In-Reply-To: <001e01c368b3$07bff080$aacdab81@genetics5>
Message-ID: <Pine.SOL.4.44.0308221541230.7178-100000@rygar.gpcc.itd.umich.edu>

Douglas  -

Your question is a bit beyond the scope of this list.
It's really at a level appropriate for an advanced graduate
student who is already doing a thesis in the area of time
series data.  Best thing is to see whether you can find some
consulting help from a local statistics department.

-  tom blackwell  -  u michigan medical school  -  ann arbor  -

On Fri, 22 Aug 2003, Douglas G. Scofield wrote:

> Dear R list,
>
> I have a sequence of weekly observations of number of adults and larvae
> in various size classes from a butterfly population living in a
> subtropical area with pronounced wet and dry seasons.  Wet and dry
> seasons are each defined 26 weeks long with fixed start and end dates.
> The data span 103 weeks (two seasons each of wet and dry) with some
> missing weeks.  What I would like to do is compare means of each type of
> observation between wet and dry seasons ("Does the number of adults
> observed vary by season?").  Not surprisingly there is pronounced
> autocorrelation in the data, e.g.:
>
>    dwtest(lm(numadults ~ week))
>
> gives
>
>    DW = 0.2727, p-value = < 2.2e-16
>
> Note that the effects of this autocorrelation extend across wet-dry and
> dry-wet boundaries.
>
> What would be a way to ask my questions in R?  It seems like I'd use
> nlme and define a corStruct, but it's unclear to me how I'd do that.
> I'm still learning R (and  statistics) so detailed answers would be most
> appreciated.
>
> Sincerely,
>
> Douglas Scofield                 Department of Biology
> d.scofield at umiami.edu            University of Miami
> off: (305) 284-3778              P.O. Box 249118
> fax: (305) 284-3039              Coral Gables, FL  33124-0421
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
>



From isaac.neuhaus at bms.com  Fri Aug 22 22:09:18 2003
From: isaac.neuhaus at bms.com (Isaac Neuhaus)
Date: Fri, 22 Aug 2003 16:09:18 -0400
Subject: [R] Altix IA-64 chip
Message-ID: <3F46786E.2030903@bms.com>

We have a loan SGI Altix system runing linux with the IA-64 chip. I was 
   wondering if anybody has compiled R in such a system.



From f.mattes at ucl.ac.uk  Fri Aug 22 22:40:08 2003
From: f.mattes at ucl.ac.uk (Frank Mattes)
Date: Fri, 22 Aug 2003 21:40:08 +0100
Subject: [R] advise for modeling a linear mixed model
Message-ID: <p05210600bb6c2c2c233f@[128.40.218.142]>

Dear R help-list reader,

I'm trying to investigate my data with linear mixed model and are 
seeking advise how to write the model in R. I was trying to get hold 
of the recommended book from Bates et al, but neither the major 
bookshop nor our university library had the book. My data is 
mirroring the example given in the appendix of John Foxes book 
"Applied Regression" with a small exception.

I'm interested in a frequency (freq) of a particular cell type which 
was measured longitudinal in patients (p) and  control group (c). In 
addition, each sample was measured four times (repeat) with a 
possible factor influencing the sensitivity (gated).

Here my data frame:

ID	freq	day	repeat	group	gated
1	0.1	1	1	c	10000
1	0.15	1	2	c	12000
1	0.2	1	3	c	50000
1	0.1	1	4	c	12000
1	0.5	12	1	c	12000
1	0.1	12	2	c	50000
1	0.2	12	3	c	100000
1	0.4	12	4	c	70000
1	0.2	5	1	p	15000
2	0.25	5	2	p	6000
2	.
2	.

So I could write a model with a fixed effects for day and group and 
their interaction.

lme(freq ~ day*group, random = ~day | ID data=frame)

I'm expecting possible effect of "gated" on the frequency. I don't 
know how to include the fact that I have four measures from the same 
sample and an effect of the variable "gated" on the frequency.

I'm appreciate any help, and apologize in case I have not read 
carefully the available documentation.

Frank
-- 
Frank Mattes				e-mail:	f.mattes at ucl.ac.uk
Department of Virology			fax	0044(0)207 8302854
Royal Free Hospital and 			tel	0044(0)207 8302997
University College Medical School
London



From p.dalgaard at biostat.ku.dk  Fri Aug 22 23:12:51 2003
From: p.dalgaard at biostat.ku.dk (Peter Dalgaard BSA)
Date: Fri, 22 Aug 2003 21:12:51 -0000
Subject: [R] Altix IA-64 chip
In-Reply-To: <3F46786E.2030903@bms.com>
References: <3F46786E.2030903@bms.com>
Message-ID: <x265kps81x.fsf@biostat.ku.dk>

Isaac Neuhaus <isaac.neuhaus at bms.com> writes:

> We have a loan SGI Altix system runing linux with the IA-64 chip. I
> was  wondering if anybody has compiled R in such a system.

Seems so...:

http://buildd.debian.org/fetch.php?&pkg=r-base&ver=1.7.1.cvs.20030814-1&arch=ia64&stamp=1060920704&file=log&as=raw


-- 
   O__  ---- Peter Dalgaard             Blegdamsvej 3  
  c/ /'_ --- Dept. of Biostatistics     2200 Cph. N   
 (*) \(*) -- University of Copenhagen   Denmark      Ph: (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)             FAX: (+45) 35327907



From p.dalgaard at biostat.ku.dk  Fri Aug 22 23:26:52 2003
From: p.dalgaard at biostat.ku.dk (Peter Dalgaard BSA)
Date: Fri, 22 Aug 2003 21:26:52 -0000
Subject: [R] Substitute in legend
In-Reply-To: <OFF854FF5B.65487513-ON85256D8A.006944D2-85256D8A.00696F62@hgsi.com>
References: <OFF854FF5B.65487513-ON85256D8A.006944D2-85256D8A.00696F62@hgsi.com>
Message-ID: <x21xvds7e2.fsf@biostat.ku.dk>

partha_bagchi at hgsi.com writes:

> I tried to use substitute in legend as follows:
> 
> pval <- 0.04
> plot(0)
> legend(1,0.5,substitute(hat(theta) == p, list(p = pval)))
> 
> For some reason the legend is repeated 3 times. 
> 
> Any suggestions or is this a bug?

It's a bug. The code is looking at length(legend), but that is not the
number of legends when mode "call" objects are concerned. A workaround
is 

legend(1,0.5,as.expression(substitute(hat(theta) == p, list(p = pval))))

-- 
   O__  ---- Peter Dalgaard             Blegdamsvej 3  
  c/ /'_ --- Dept. of Biostatistics     2200 Cph. N   
 (*) \(*) -- University of Copenhagen   Denmark      Ph: (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)             FAX: (+45) 35327907



From roger at ysidro.econ.uiuc.edu  Fri Aug 22 23:51:32 2003
From: roger at ysidro.econ.uiuc.edu (Roger Koenker)
Date: Fri, 22 Aug 2003 16:51:32 -0500 (CDT)
Subject: [R] Package Reference Manuals
In-Reply-To: <Pine.SOL.4.30.0308221357270.8866-100000@ysidro.econ.uiuc.edu>
Message-ID: <Pine.SOL.4.30.0308221650370.8866-100000@ysidro.econ.uiuc.edu>

It seems that I can get (almost) what I want using the utility

	R CMD Rd2dvi  packagename


url:	www.econ.uiuc.edu/~roger/my.html	Roger Koenker
email	rkoenker at uiuc.edu			Department of Economics
vox: 	217-333-4558				University of Illinois
fax:   	217-244-6678				Champaign, IL 61820

On Fri, 22 Aug 2003, Roger Koenker wrote:

> Is it possible for the average user to generate the latex version
> of the table of contents page that appears in the Package Reference
> Manuals on the CRAN website?  Or is this (yet another) a Viennese speciality?
>
> url:	www.econ.uiuc.edu/~roger/my.html	Roger Koenker
> email	rkoenker at uiuc.edu			Department of Economics
> vox: 	217-333-4558				University of Illinois
> fax:   	217-244-6678				Champaign, IL 61820
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
>



From deepayan at stat.wisc.edu  Fri Aug 22 23:51:46 2003
From: deepayan at stat.wisc.edu (Deepayan Sarkar)
Date: Fri, 22 Aug 2003 16:51:46 -0500
Subject: [R] Substitute in legend
In-Reply-To: <x21xvds7e2.fsf@biostat.ku.dk>
References: <OFF854FF5B.65487513-ON85256D8A.006944D2-85256D8A.00696F62@hgsi.com>
	<x21xvds7e2.fsf@biostat.ku.dk>
Message-ID: <200308221651.46948.deepayan@stat.wisc.edu>

On Friday 22 August 2003 16:35, Peter Dalgaard BSA wrote:
> partha_bagchi at hgsi.com writes:
> > I tried to use substitute in legend as follows:
> >
> > pval <- 0.04
> > plot(0)
> > legend(1,0.5,substitute(hat(theta) == p, list(p = pval)))
> >
> > For some reason the legend is repeated 3 times.
> >
> > Any suggestions or is this a bug?
>
> It's a bug. The code is looking at length(legend), but that is not the
> number of legends when mode "call" objects are concerned. A workaround
> is
>
> legend(1,0.5,as.expression(substitute(hat(theta) == p, list(p = pval))))

Just out of curiosity: ?legend says 

  legend: a vector of text values or an 'expression' of length >= 1 to
          appear in the legend.

Is an object of mode "call" either ? (is.expression() returns FALSE.) Are they 
expected to work wherever expressions are supposed to work ?

Deepayan



From p.dalgaard at biostat.ku.dk  Sat Aug 23 00:14:14 2003
From: p.dalgaard at biostat.ku.dk (Peter Dalgaard BSA)
Date: Fri, 22 Aug 2003 22:14:14 -0000
Subject: [R] Substitute in legend
In-Reply-To: <200308221651.46948.deepayan@stat.wisc.edu>
References: <OFF854FF5B.65487513-ON85256D8A.006944D2-85256D8A.00696F62@hgsi.com>
	<x21xvds7e2.fsf@biostat.ku.dk>
	<200308221651.46948.deepayan@stat.wisc.edu>
Message-ID: <x2wud5qqmn.fsf@biostat.ku.dk>

Deepayan Sarkar <deepayan at stat.wisc.edu> writes:

> > legend(1,0.5,as.expression(substitute(hat(theta) == p, list(p = pval))))
> 
> Just out of curiosity: ?legend says 
> 
>   legend: a vector of text values or an 'expression' of length >= 1 to
>           appear in the legend.
> 
> Is an object of mode "call" either ? (is.expression() returns FALSE.) Are they 
> expected to work wherever expressions are supposed to work ?

No, yes. They're not text nor expressions (so technically there's no
bug), but it is easier to construct them with substitute() and they do
generally work like expressions of length 1 in graphs.

-- 
   O__  ---- Peter Dalgaard             Blegdamsvej 3  
  c/ /'_ --- Dept. of Biostatistics     2200 Cph. N   
 (*) \(*) -- University of Copenhagen   Denmark      Ph: (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)             FAX: (+45) 35327907



From jasont at indigoindustrial.co.nz  Sat Aug 23 00:21:58 2003
From: jasont at indigoindustrial.co.nz (Jason Turner)
Date: Sat, 23 Aug 2003 10:21:58 +1200
Subject: [R] problem compiling R in suse8.2
In-Reply-To: <1061566015.12658.1.camel@linux.local>
References: <1061566015.12658.1.camel@linux.local>
Message-ID: <3F469786.8050009@indigoindustrial.co.nz>

Ernesto Jardim wrote:
> Hi
> 
> I'm trying to compile R in SuSE 8.2 (updated with the gcc 3.3.1) but I'm
> getting the following error:
> 

Works fine on my SuSE 8.2 box.

jasont at kryten:~/> gcc -vgcc -v
...
gcc version 3.3 20030226 (prerelease) (SuSE Linux)

jasont at kryten:~> uname -a
Linux kryten 2.4.20-4GB-athlon #1 Fri Jul 11 20:16:51 UTC 2003 i686 
unknown unknown GNU/Linux


> /usr/X11R6/include/X11/keysymdef.h:1181:2: invalid preprocessing
> directive #d
> make[4]: *** [dataentry.lo] Error 1
> make[4]: Leaving directory
> `/usr/local/src/compile/R-1.7.1/src/modules/X11'
> make[3]: *** [R] Error 2
> make[3]: Leaving directory
> `/usr/local/src/compile/R-1.7.1/src/modules/X11'
> make[2]: *** [R] Error 1
> make[2]: Leaving directory `/usr/local/src/compile/R-1.7.1/src/modules'
> make[1]: *** [R] Error 1
> make[1]: Leaving directory `/usr/local/src/compile/R-1.7.1/src'
> make: *** [R] Error 1
> 
> 

Maybe an XFree86-devel library?

jasont at kryten:~> rpm -q -a | grep X
XFree86-fonts-75dpi-4.3.0-15
XFree86-fonts-scalable-4.3.0-15
XFree86-man-4.3.0-15
XFree86-server-4.3.0-15
XFree86-devel-4.3.0-15
XFree86-GLX-4.3.0-15
XFree86-4.3.0-15
XFree86-libs-4.3.0-42
...

Hope it helps

Cheers

Jason
-- 
Indigo Industrial Controls Ltd.
64-21-343-545
jasont at indigoindustrial.co.nz



From tlumley at u.washington.edu  Sat Aug 23 00:18:55 2003
From: tlumley at u.washington.edu (Thomas Lumley)
Date: Fri, 22 Aug 2003 15:18:55 -0700 (PDT)
Subject: [R] Substitute in legend
In-Reply-To: <200308221651.46948.deepayan@stat.wisc.edu>
Message-ID: <Pine.A41.4.44.0308221517440.59518-100000@homer30.u.washington.edu>

On Fri, 22 Aug 2003, Deepayan Sarkar wrote:
>
> Just out of curiosity: ?legend says
>
>   legend: a vector of text values or an 'expression' of length >= 1 to
>           appear in the legend.
>
> Is an object of mode "call" either ? (is.expression() returns FALSE.) Are they
> expected to work wherever expressions are supposed to work ?
>

They aren't expressions, but they do work in many places where expressions
work, in particular in plotmath.  This is good, because quote() is shorter
than expression().

	-thomas



From kjetil at entelnet.bo  Sat Aug 23 02:54:50 2003
From: kjetil at entelnet.bo (kjetil brinchmann halvorsen)
Date: Fri, 22 Aug 2003 20:54:50 -0400
Subject: [R] converting factor to numeric
In-Reply-To: <3F452E3C.9599.D20B3D@localhost>
Message-ID: <3F46831A.9107.791AE@localhost>

On 21 Aug 2003 at 20:40, kjetil brinchmann halvorsen wrote:

After some private responses I did what I should have done at 
beginning, opened "S Programming" at page 15. It is much clearer than 
the FAQ, which I think is slighly misleading.

Kjetil Halvorsen

> Hola!
> 
> The R FAQ says: 
> 
> 7.12 How do I convert factors to numeric?
> 
> It may happen that when reading numeric data into R (usually, when 
> reading in a file), they come in as factors. If f is such a factor 
> object, you can use
> 
> as.numeric(as.character(f))
> 
> to get the numbers back. More efficient, but harder to remember, is
> 
> as.numeric(levels(f))[as.integer(f)]
> 
> In any case, do not call as.numeric() or their likes directly. 
> 
> But trying to follow the advice:
> 
> (this is without package method attached, but the results are the 
> same with):
> 
> First doing as one shouldn't:
> 
> > table( as.numeric(EdadC) )
> 
>   1   2   3   4   5 
>  20  99 157 127  74 
> 
> Doing as the FAQ says:
> 
> > table( as.numeric(as.character(EdadC)) )
> character(0)
> Warning message: 
> NAs introduced by coercion 
> 
> or:
> 
> > table( as.numeric(levels(EdadC))[as.integer(EdadC)] )
> character(0)
> Warning message: 
> NAs introduced by coercion 
> 
> ?
> 
> Kjetil Halvorsen
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help



From jasont at indigoindustrial.co.nz  Sat Aug 23 08:22:32 2003
From: jasont at indigoindustrial.co.nz (Jason Turner)
Date: Sat, 23 Aug 2003 18:22:32 +1200
Subject: [R] advise for modeling a linear mixed model
In-Reply-To: <p05210600bb6c2c2c233f@[128.40.218.142]>
References: <p05210600bb6c2c2c233f@[128.40.218.142]>
Message-ID: <3F470828.5030201@indigoindustrial.co.nz>

Frank Mattes wrote:
> Dear R help-list reader,
> 
> I'm trying to investigate my data with linear mixed model and are 
> seeking advise how to write the model in R. I was trying to get hold of 
> the recommended book from Bates et al, but neither the major bookshop 
> nor our university library had the book.
...


Is gated a continuous variable?  You could compare it as...

(gated as fixed effect)

lme(freq ~ day*group + gated, random = ~day | ID data=frame,
   method = "ML")

vs

(gated as random effect)

lme(freq ~ day*group, random = ~day + gated | ID data=frame,
   method = "ML")

and use anova() to compare.  Note that you'll need the "ML" as opposed 
to REML to compare using anova.

Or have I missed something?

check out ?corClasses, too, to help with correlated error terms.

As for the book, Bates & Pinheiro, I'm working my way slowly through it, 
and it's well worth it.  Well written, and very informative.  The 
"slowly" working through it is because my spare time is very short these 
days.  I strongly urge you to purchase it from your favorite on-line 
book retailer (or pop by Foyles to grab one - just a couple blocks up 
from Charring Cross station, if memory serves).

In the mean time, if you can't immediately grab a copy, go over here:

http://www.insightful.com/support/documentation.asp?DID=3

and select "S-PLUS 6 Guide to Statistics (Part I)", and read the chapter 
on Linear and Non-Linear Mixed-effects models (Chapter 14 in my copy). 
I've got this, and a copy of Bates & Pinheiro.  The free guide isn't 
bad, B&P is excellent.

Cheers

Jason
-- 
Indigo Industrial Controls Ltd.
64-21-343-545
jasont at indigoindustrial.co.nz



From governor at governor.ca.gov  Sat Aug 23 10:56:05 2003
From: governor at governor.ca.gov (governor@governor.ca.gov)
Date: Sat, 23 Aug 2003 01:56:05 -0700 (PDT)
Subject: [R] Thank you
Message-ID: <200308230856.h7N8u5227532@barney.governor.ca.gov>

Thank you for your email.  I appreciate you taking the time to keep me informed of your opinions and the issues that are important to you.  Your comments help keep me informed as we strive to make California a better place in which to work and live.

Your email has been directed to members of my staff who assist constituents and report ideas and concerns daily.  If your concern is best handled by a specific department, your correspondence will be forwarded for action and response.

So that we can keep track of correspondence and ensure that we are able to respond to California residents, please be sure to include your name and address when you communicate with the Governor's Office or any state agency.  Please note that we do NOT accept email attachments, so your correspondence should be included within the text of your email.

Again, thank you for sharing your thoughts.  An informed and engaged citizenry is essential to the democratic process, and I appreciate your willingness to write me.

Sincerely,

Governor Gray Davis



From bates at stat.wisc.edu  Sat Aug 23 16:49:49 2003
From: bates at stat.wisc.edu (Douglas Bates)
Date: Sat, 23 Aug 2003 14:49:49 -0000
Subject: [R] advise for modeling a linear mixed model
In-Reply-To: <3F470828.5030201@indigoindustrial.co.nz>
References: <p05210600bb6c2c2c233f@[128.40.218.142]>
	<3F470828.5030201@indigoindustrial.co.nz>
Message-ID: <6rhe48h1hy.fsf@bates4.stat.wisc.edu>

Jason Turner <jasont at indigoindustrial.co.nz> writes:

> As for the book, Bates & Pinheiro, I'm working my way slowly through

Actually, it's "Pinheiro & Bates".



From eugenesalinas2003 at yahoo.com  Sat Aug 23 17:36:48 2003
From: eugenesalinas2003 at yahoo.com (Eugene Salinas)
Date: Sat, 23 Aug 2003 08:36:48 -0700 (PDT)
Subject: [R] help--kernel distribution dynamics
Message-ID: <20030823153648.57878.qmail@web20708.mail.yahoo.com>

Deall all,

I'm just learning R, but unfortunately I need to
urgently do a rather more complex task so I need some
help. I have just learnt the very basics a few days
ago and am not ready yet to deal with panels and
kernel densities, so a soft guidance would be most
appreciated.

I have a (very) large panel data set (400,000
individuals x50 time periods) and need to display the
evolution of the kernel density of some of the
variables over time. I can do slices in STATA - eg the
distribution in 1950 and 1990 but they are difficult
to compare due to different scales used. So I need to
have a nice 3D graph that shows how the distribution
evolved over time and am hoping that R can do it.

I'm very grateful for any suggestions, eugene.



From Majordomo-owner at cert.org  Sat Aug 23 17:55:34 2003
From: Majordomo-owner at cert.org (Majordomo-owner@cert.org)
Date: Sat, 23 Aug 2003 11:55:34 -0400
Subject: [R] Majordomo results: Re: Thank you!
Message-ID: <200308231555.h7NFtYYc029927@iosephus.indigo.cert.org>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20030823/6857d570/attachment.pl
-------------- next part --------------
An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20030823/6857d570/attachment-0001.pl

From deepayan at stat.wisc.edu  Sat Aug 23 18:39:22 2003
From: deepayan at stat.wisc.edu (Deepayan Sarkar)
Date: Sat, 23 Aug 2003 11:39:22 -0500
Subject: [R] help--kernel distribution dynamics
In-Reply-To: <20030823153648.57878.qmail@web20708.mail.yahoo.com>
References: <20030823153648.57878.qmail@web20708.mail.yahoo.com>
Message-ID: <200308231139.22010.deepayan@stat.wisc.edu>

On Saturday 23 August 2003 10:36, Eugene Salinas wrote:
> Deall all,
>
> I'm just learning R, but unfortunately I need to
> urgently do a rather more complex task so I need some
> help. I have just learnt the very basics a few days
> ago and am not ready yet to deal with panels and
> kernel densities, so a soft guidance would be most
> appreciated.
>
> I have a (very) large panel data set (400,000
> individuals x50 time periods) and need to display the
> evolution of the kernel density of some of the
> variables over time. I can do slices in STATA - eg the
> distribution in 1950 and 1990 but they are difficult
> to compare due to different scales used. So I need to
> have a nice 3D graph that shows how the distribution
> evolved over time and am hoping that R can do it.
>
> I'm very grateful for any suggestions, eugene.

As a first step, you could create a matrix (with 50 rows, one for each time 
point) where each row holds the kernel density estimate for that time point. 
e.g. (with a grid of size 100 for each estimated density),

foo <- matrix(0, 50, 100)
for (i in 1:50) 
    foo[i, ] <- density(rnorm(5000), from = -4, to = 4, n = 100)$y
                        ^^^^^^^^^^^        ^^^^^^^^^^^
                         put your        put appropriate 
                       variable here          ranges

Whether a 3D view of this will be very informative will depend on your data 
(maybe you could play with the density() parameters), but persp() should give 
you something:

persp(foo, theta = 135, phi = 30, scale = FALSE,
      ltheta = -120, shade = 0.75, border = NA)


HTH,

Deepayan



From ligges at statistik.uni-dortmund.de  Sat Aug 23 19:11:45 2003
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Sat, 23 Aug 2003 19:11:45 +0200
Subject: [R] Substitute in legend
In-Reply-To: <x21xvds7e2.fsf@biostat.ku.dk>
References: <OFF854FF5B.65487513-ON85256D8A.006944D2-85256D8A.00696F62@hgsi.com>
	<x21xvds7e2.fsf@biostat.ku.dk>
Message-ID: <3F47A051.7080500@statistik.uni-dortmund.de>

Peter Dalgaard BSA wrote:

> partha_bagchi at hgsi.com writes:
> 
> 
>>I tried to use substitute in legend as follows:
>>
>>pval <- 0.04
>>plot(0)
>>legend(1,0.5,substitute(hat(theta) == p, list(p = pval)))
>>
>>For some reason the legend is repeated 3 times. 
>>
>>Any suggestions or is this a bug?
> 
> 
> It's a bug. The code is looking at length(legend), but that is not the
> number of legends when mode "call" objects are concerned. A workaround
> is 
> 
> legend(1,0.5,as.expression(substitute(hat(theta) == p, list(p = pval))))

Or more general:

  do.call("expression",
    list(substitute(hat(theta) == p, list(p = pval))))

This way you can include several substitute() calls within that list, to 
allow an expression of length > 1 which is frequently required for legends.


I think it is not really a bug, because this is documented,
but inserting
  if(is.call(legend))
    legend <- as.expression(legend)
would not break anything, AFAICS.

Uwe Ligges



From eugenesalinas2003 at yahoo.com  Sat Aug 23 20:02:53 2003
From: eugenesalinas2003 at yahoo.com (Eugene Salinas)
Date: Sat, 23 Aug 2003 11:02:53 -0700 (PDT)
Subject: [R] help--kernel distribution dynamics
In-Reply-To: <200308231139.22010.deepayan@stat.wisc.edu>
Message-ID: <20030823180253.92486.qmail@web20714.mail.yahoo.com>


--- Deepayan Sarkar <deepayan at stat.wisc.edu> wrote:
[...]
> 
> As a first step, you could create a matrix (with 50
> rows, one for each time 
> point) where each row holds the kernel density
> estimate for that time point. 
> e.g. (with a grid of size 100 for each estimated
> density),
> 
> foo <- matrix(0, 50, 100)
> for (i in 1:50) 
>     foo[i, ] <- density(rnorm(5000), from = -4, to =
> 4, n = 100)$y
>                         ^^^^^^^^^^^       
> ^^^^^^^^^^^
>                          put your        put
> appropriate 
>                        variable here          ranges
> 
> Whether a 3D view of this will be very informative
> will depend on your data 
> (maybe you could play with the density()
> parameters), but persp() should give 
> you something:
> 
> persp(foo, theta = 135, phi = 30, scale = FALSE,
>       ltheta = -120, shade = 0.75, border = NA)
> 

Hi, Thanks a lot. This seems like what I want to do. I
don't know all the syntax yet so just a
clarification...

Is the  [....]n = 100)$y there in order to condition
on y which is the year and derive the conditional
kernel density? The structure of my data looks like
this
             x  year
Indiv1 1950  .    .
Indiv1 1951  .    .
...................
Indiv1 1991  .    .
...................
...................
IndivN 1991  .    .

so to get the matrix of densities I would write...

for (i in 1950:1991) 
>     foo[i-1949, ] <- density(x, from = -nn, to =nn,
n = 100)$year

Am I right? thanks, e.



From deepayan at stat.wisc.edu  Sat Aug 23 20:22:40 2003
From: deepayan at stat.wisc.edu (Deepayan Sarkar)
Date: Sat, 23 Aug 2003 13:22:40 -0500
Subject: [R] help--kernel distribution dynamics
In-Reply-To: <20030823180253.92486.qmail@web20714.mail.yahoo.com>
References: <20030823180253.92486.qmail@web20714.mail.yahoo.com>
Message-ID: <200308231322.40083.deepayan@stat.wisc.edu>

On Saturday 23 August 2003 13:02, Eugene Salinas wrote:
[...]
> Hi, Thanks a lot. This seems like what I want to do. I
> don't know all the syntax yet so just a
> clarification...
>
> Is the  [....]n = 100)$y there in order to condition
> on y which is the year and derive the conditional
> kernel density? The structure of my data looks like
> this

Nothing so complicated. The value returned by density() is an R object 
(technically a list) with a lot of information, of which the actual density 
estimates are stored in the component named "y", extracted here with $y. (You 
may also want to look at $x if you want to label X-axis of persp, but that 
should be the same for all the density() calls.)

>From help(density),


-----
Value:

     If 'give.Rkern' is true, the number R(K), otherwise an object with
     class '"density"' whose underlying structure is a list containing
     the following components. 

       x: the 'n' coordinates of the points where the density is
          estimated.

       y: the estimated density values.

      bw: the bandwidth used.

       N: the sample size after elimination of missing values.

    call: the call which produced the result.

data.name: the deparsed name of the 'x' argument.

  has.na: logical, for compatibility (always FALSE).
-----


To 'condition' on the year, you would somehow have to extract as a numeric 
vector the values of your variable that correspond to that year (this vector 
would go in place of the rnorm(5000) in my example) --- how to do that 
depends on the structure of your data.


Deepayan



From ripley at stats.ox.ac.uk  Sat Aug 23 21:45:44 2003
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Sat, 23 Aug 2003 20:45:44 +0100 (BST)
Subject: [R] help--kernel distribution dynamics
In-Reply-To: <200308231139.22010.deepayan@stat.wisc.edu>
Message-ID: <Pine.LNX.4.44.0308232043330.26427-100000@gannet.stats>

As a possible enhancement, I would think about using the same bandwidth at 
all the time points --- indeed I would probably start by looking at a few 
time points, playing with the bandwidth and then using e.g. persp on 
density estimates at all 50 time points with that bandwidth.

> As a first step, you could create a matrix (with 50 rows, one for each time 
> point) where each row holds the kernel density estimate for that time point. 
> e.g. (with a grid of size 100 for each estimated density),
> 
> foo <- matrix(0, 50, 100)
> for (i in 1:50) 
>     foo[i, ] <- density(rnorm(5000), from = -4, to = 4, n = 100)$y
>                         ^^^^^^^^^^^        ^^^^^^^^^^^
>                          put your        put appropriate 
>                        variable here          ranges

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From d.scofield at umiami.edu  Sat Aug 23 22:39:27 2003
From: d.scofield at umiami.edu (Douglas G. Scofield)
Date: Sat, 23 Aug 2003 16:39:27 -0400
Subject: [R] filling a matrix who's entries are a function of the indices?
Message-ID: <002401c369b6$a8bdef10$aacdab81@genetics5>

Dear R list,

What's the best way in R to fill a matrix who's entries depend on some
function of the indices?  I'm currently doing:

   Q <- matrix(0, k, k)
   for (A in 1:k) {
      for (B in 1:k) {
         Q[A,B] <- my.function(A,B)
      }
   }

but I wonder if there is a more terse way.

Regards,

Douglas Scofield                 Department of Biology 
d.scofield at umiami.edu            University of Miami
off: (305) 284-3778              P.O. Box 249118
fax: (305) 284-3039              Coral Gables, FL  33124-0421



From Ted.Harding at nessie.mcc.ac.uk  Sat Aug 23 23:25:07 2003
From: Ted.Harding at nessie.mcc.ac.uk ( (Ted Harding))
Date: Sat, 23 Aug 2003 22:25:07 +0100 (BST)
Subject: [R] filling a matrix who's entries are a function of the ind
In-Reply-To: <002401c369b6$a8bdef10$aacdab81@genetics5>
Message-ID: <XFMail.030823222507.Ted.Harding@nessie.mcc.ac.uk>

On 23-Aug-03 Douglas G. Scofield wrote:
> What's the best way in R to fill a matrix who's entries depend on some
> function of the indices?  I'm currently doing:
> 
>    Q <- matrix(0, k, k)
>    for (A in 1:k) {
>       for (B in 1:k) {
>          Q[A,B] <- my.function(A,B)
>       }
>    }
> 
> but I wonder if there is a more terse way.

Something on the following lines?

> xx<-matrix(rep(c(1,2,3),3),ncol=3)
> xx
     [,1] [,2] [,3]
[1,]    1    1    1
[2,]    2    2    2
[3,]    3    3    3
> yy<-t(xx)
> yy
     [,1] [,2] [,3]
[1,]    1    2    3
[2,]    1    2    3
[3,]    1    2    3
> myfun<-function(XX,YY){(XX-YY)^2}
> myfun(xx,yy)
     [,1] [,2] [,3]
[1,]    0    1    4
[2,]    1    0    1
[3,]    4    1    0

[The above is inspired by the matlab/octave function "meshdom"]

Best wishes,
Ted.


--------------------------------------------------------------------
E-Mail: (Ted Harding) <Ted.Harding at nessie.mcc.ac.uk>
Fax-to-email: +44 (0)870 167 1972
Date: 23-Aug-03                                       Time: 22:25:07
------------------------------ XFMail ------------------------------



From baron at psych.upenn.edu  Sat Aug 23 23:44:18 2003
From: baron at psych.upenn.edu (Jonathan Baron)
Date: Sat, 23 Aug 2003 17:44:18 -0400
Subject: [R] filling a matrix who's entries are a function of the ind
In-Reply-To: <XFMail.030823222507.Ted.Harding@nessie.mcc.ac.uk>
References: <002401c369b6$a8bdef10$aacdab81@genetics5>
	<XFMail.030823222507.Ted.Harding@nessie.mcc.ac.uk>
Message-ID: <20030823214418.GA1162@mail1.sas.upenn.edu>

On 08/23/03 22:25, Ted Harding wrote:
>On 23-Aug-03 Douglas G. Scofield wrote:
>> What's the best way in R to fill a matrix who's entries depend on some
>> function of the indices?  I'm currently doing:
>> 
>>    Q <- matrix(0, k, k)
>>    for (A in 1:k) {
>>       for (B in 1:k) {
>>          Q[A,B] <- my.function(A,B)
>>       }
>>    }
>> 
>> but I wonder if there is a more terse way.

outer(1:k,1:k,my.function)

-- 
Jonathan Baron, Professor of Psychology, University of Pennsylvania
Home page:            http://www.sas.upenn.edu/~baron
R page:               http://finzi.psych.upenn.edu/
deleting "Your details" "Your application" "Approved" "Thank you!"



From jfay at genetics.wustl.edu  Sun Aug 24 00:44:19 2003
From: jfay at genetics.wustl.edu (Justin Fay)
Date: Sat, 23 Aug 2003 17:44:19 -0500
Subject: [R] explanation of lm's coefficients
Message-ID: <3F47EE43.4050308@genetics.wustl.edu>

I don't understand the coefficients returned from the lm function. I 
expected these to be the mean values for each factor in the model. Given 
this data and model:

data<-c(rnorm(10,mean=0,sd=1),rnorm(10,mean=1,sd=1),rnorm(10,mean=-.5,sd=1))
ftr<-as.factor(rep(1:3,each=10))
fit<-lm(data ~ ftr)

the mean values of the three facotrs from the data:

c(mean(data[1:10]), mean(data[11:20]), mean(data[21:30]))
[1] -0.3589049  0.6034931 -0.7256897

are not the same as the coefficients return from fit:

fit$coef
(Intercept)        ftr2        ftr3
 -0.3589049   0.9623980  -0.3667847

ftr2 and ftr3 are offset by the value of the intercept.

The fitted values are as I expected:

fit$fitted.values
         1          2          3          4          5          6          7
-0.3589049 -0.3589049 -0.3589049 -0.3589049 -0.3589049 -0.3589049 -0.3589049
         8          9         10         11         12         13         14
-0.3589049 -0.3589049 -0.3589049  0.6034931  0.6034931  0.6034931  0.6034931
        15         16         17         18         19         20         21
 0.6034931  0.6034931  0.6034931  0.6034931  0.6034931  0.6034931 -0.7256897
        22         23         24         25         26         27         28
-0.7256897 -0.7256897 -0.7256897 -0.7256897 -0.7256897 -0.7256897 -0.7256897
        29         30
-0.7256897 -0.7256897

My goal is to get the mean values of the factors. Although easily done, 
I don't understand why the ftr2 and ftr3 are offset by the value of the 
intercept. Any explanations would be appreciated.

Justin

________________________________________
Justin Fay
Assistant Professor of Genetics
Washington University School of Medicine
4566 Scott Ave, St. Louis, MO 63110
PH: 314.747.1808 Fax: 314.362.7855



From p.dalgaard at biostat.ku.dk  Sun Aug 24 01:19:22 2003
From: p.dalgaard at biostat.ku.dk (Peter Dalgaard BSA)
Date: Sat, 23 Aug 2003 23:19:22 -0000
Subject: [R] filling a matrix who's entries are a function of the ind
In-Reply-To: <XFMail.030823222507.Ted.Harding@nessie.mcc.ac.uk>
References: <XFMail.030823222507.Ted.Harding@nessie.mcc.ac.uk>
Message-ID: <x28ypkq7j3.fsf@biostat.ku.dk>

(Ted Harding) <Ted.Harding at nessie.mcc.ac.uk> writes:

> On 23-Aug-03 Douglas G. Scofield wrote:
> > What's the best way in R to fill a matrix who's entries depend on some
> > function of the indices?  I'm currently doing:
> > 
> >    Q <- matrix(0, k, k)
> >    for (A in 1:k) {
> >       for (B in 1:k) {
> >          Q[A,B] <- my.function(A,B)
> >       }
> >    }
> > 
> > but I wonder if there is a more terse way.
> 
> Something on the following lines?
> 
> > xx<-matrix(rep(c(1,2,3),3),ncol=3)
> > xx
>      [,1] [,2] [,3]
> [1,]    1    1    1
> [2,]    2    2    2
> [3,]    3    3    3
> > yy<-t(xx)
> > yy
>      [,1] [,2] [,3]
> [1,]    1    2    3
> [2,]    1    2    3
> [3,]    1    2    3
> > myfun<-function(XX,YY){(XX-YY)^2}
> > myfun(xx,yy)
>      [,1] [,2] [,3]
> [1,]    0    1    4
> [2,]    1    0    1
> [3,]    4    1    0
> 
> [The above is inspired by the matlab/octave function "meshdom"]

Also:

> outer(1:3,1:3,myfun)
     [,1] [,2] [,3]
[1,]    0    1    4
[2,]    1    0    1
[3,]    4    1    0

which does the same sort of stuff internally. Notice that in either
case, myfun must be vectorized. Otherwise you'll need

> myfun2 <- function(x,y)mapply(myfun,x,y)
> outer(1:3,1:3,myfun2)
     [,1] [,2] [,3]
[1,]    0    1    4
[2,]    1    0    1
[3,]    4    1    0

As a slightly silly example try it with

myfun<-function(XX,YY)sum(c(XX,YY)^2)

-- 
   O__  ---- Peter Dalgaard             Blegdamsvej 3  
  c/ /'_ --- Dept. of Biostatistics     2200 Cph. N   
 (*) \(*) -- University of Copenhagen   Denmark      Ph: (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)             FAX: (+45) 35327907



From deepayan at stat.wisc.edu  Sun Aug 24 01:33:11 2003
From: deepayan at stat.wisc.edu (Deepayan Sarkar)
Date: Sat, 23 Aug 2003 18:33:11 -0500
Subject: [R] explanation of lm's coefficients
In-Reply-To: <3F47EE43.4050308@genetics.wustl.edu>
References: <3F47EE43.4050308@genetics.wustl.edu>
Message-ID: <200308231833.11770.deepayan@stat.wisc.edu>


lm() implicitly assumes the presence of the intercept term in the model, i.e., 

> lm(data ~ ftr)

is actually fitting 

> lm(data ~ 1 + ftr)

You can override this by doing

> lm(data ~ ftr - 1)

You can always check the actual model being fitted by extracting the model 
matrix:

> model.matrix(data ~ ftr)
> model.matrix(lm(data ~ ftr - 1))

etc.

HTH,
Deepayan


On Saturday 23 August 2003 17:44, Justin Fay wrote:
> I don't understand the coefficients returned from the lm function. I
> expected these to be the mean values for each factor in the model. Given
> this data and model:
>
> data<-c(rnorm(10,mean=0,sd=1),rnorm(10,mean=1,sd=1),rnorm(10,mean=-.5,sd=1)
>) ftr<-as.factor(rep(1:3,each=10))
> fit<-lm(data ~ ftr)
>
> the mean values of the three facotrs from the data:
>
> c(mean(data[1:10]), mean(data[11:20]), mean(data[21:30]))
> [1] -0.3589049  0.6034931 -0.7256897
>
> are not the same as the coefficients return from fit:
>
> fit$coef
> (Intercept)        ftr2        ftr3
>  -0.3589049   0.9623980  -0.3667847
>
> ftr2 and ftr3 are offset by the value of the intercept.
>
> The fitted values are as I expected:
>
> fit$fitted.values
>          1          2          3          4          5          6         
> 7 -0.3589049 -0.3589049 -0.3589049 -0.3589049 -0.3589049 -0.3589049
> -0.3589049 8          9         10         11         12         13        
> 14 -0.3589049 -0.3589049 -0.3589049  0.6034931  0.6034931  0.6034931 
> 0.6034931 15         16         17         18         19         20        
> 21 0.6034931  0.6034931  0.6034931  0.6034931  0.6034931  0.6034931
> -0.7256897 22         23         24         25         26         27       
>  28 -0.7256897 -0.7256897 -0.7256897 -0.7256897 -0.7256897 -0.7256897
> -0.7256897 29         30
> -0.7256897 -0.7256897
>
> My goal is to get the mean values of the factors. Although easily done,
> I don't understand why the ftr2 and ftr3 are offset by the value of the
> intercept. Any explanations would be appreciated.
>
> Justin
>
> ________________________________________
> Justin Fay
> Assistant Professor of Genetics
> Washington University School of Medicine
> 4566 Scott Ave, St. Louis, MO 63110
> PH: 314.747.1808 Fax: 314.362.7855
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help



From brett at rebcon.co.uk  Sun Aug 24 13:02:54 2003
From: brett at rebcon.co.uk (Brett Robinson)
Date: Sun, 24 Aug 2003 12:02:54 +0100
Subject: [R] regression constraints (again)
Message-ID: <000001c36a2f$453479e0$0100a8c0@Boss>

Im trying to do regressions with constraints that the weights 
are all >=0 and sum(weights) = 1. I've read the archive and have 
set the problem up with solve.QP and just the non-negativity constraints

along the lines of:

y as the data vector 
X as the design matrix 

D <- t(X) %*% X 
d <- t(t(y) %*% X) 
A <- diag(ncol(X)) 
b <- rep(0,ncol(X)) 
fit <- solve.QP(D=D,d=d,A=t(A),b=b,meq=0) 

(as per Gardar Johannesson '01) 

When I try to add the extra constraint that sum(weights)=1 I get errors 
owing to incompatibility of matrices. I add the constraint by putting an

extra column of all ones to A and setting meq=1.

I can work round it I think, by using an intercept and using the extra 
column on the matrix for the sum(weights) constraint but I think that it

should be possible without doing this.

Grateful for any pointers as to where I am going wrong.

Brett Robinson



From brett at rebcon.co.uk  Sun Aug 24 13:02:51 2003
From: brett at rebcon.co.uk (Brett Robinson)
Date: Sun, 24 Aug 2003 12:02:51 +0100
Subject: [R] regression constraints (again)
Message-ID: <000001c36a2f$460648d0$0100a8c0@Boss>

Im trying to do regressions with constraints that the weights 
are all >=0 and sum(weights) = 1. I've read the archive and have 
set the problem up with solve.QP and just the non-negativity constraints

along the lines of:

y as the data vector 
X as the design matrix 

D <- t(X) %*% X 
d <- t(t(y) %*% X) 
A <- diag(ncol(X)) 
b <- rep(0,ncol(X)) 
fit <- solve.QP(D=D,d=d,A=t(A),b=b,meq=0) 

(as per Gardar Johannesson '01) 

When I try to add the extra constraint that sum(weights)=1 I get errors 
owing to incompatibility of matrices. I add the constraint by putting an

extra column of all ones to A and setting meq=1.

I can work round it I think, by using an intercept and using the extra 
column on the matrix for the sum(weights) constraint but I think that it

should be possible without doing this.

Grateful for any pointers as to where I am going wrong.

Brett Robinson



From ripley at stats.ox.ac.uk  Sun Aug 24 16:03:59 2003
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Sun, 24 Aug 2003 15:03:59 +0100 (BST)
Subject: [R] regression constraints (again)
In-Reply-To: <000001c36a2f$453479e0$0100a8c0@Boss>
Message-ID: <Pine.LNX.4.44.0308241452290.2828-100000@gannet.stats>

It's easier to use crossprod, and you appear to have transposed A.

Here's a small example

library(quadprog)  # which you never mentioned!
X <- matrix(rnorm(100),, 4)
Y <- 0.25* X %*% rep(1, 4) + 0.1*rnorm(25)
D <- crossprod(X)
d <- crossprod(X, Y)
A <- cbind(1, diag(ncol(X)))
b <- c(1, rep(0,ncol(X)))
solve.QP(D, d, A, b, meq=1)

which works for me.

On Sun, 24 Aug 2003, Brett Robinson wrote:

> Im trying to do regressions with constraints that the weights 
> are all >=0 and sum(weights) = 1. I've read the archive and have 

weights?  I assume you mean coefficients?

> set the problem up with solve.QP and just the non-negativity constraints
> 
> along the lines of:
> 
> y as the data vector 
> X as the design matrix 
> 
> D <- t(X) %*% X 
> d <- t(t(y) %*% X) 
> A <- diag(ncol(X)) 
> b <- rep(0,ncol(X)) 
> fit <- solve.QP(D=D,d=d,A=t(A),b=b,meq=0) 
> 
> (as per Gardar Johannesson '01) 
> 
> When I try to add the extra constraint that sum(weights)=1 I get errors 
> owing to incompatibility of matrices. I add the constraint by putting an
> 
> extra column of all ones to A and setting meq=1.
> 
> I can work round it I think, by using an intercept and using the extra 
> column on the matrix for the sum(weights) constraint but I think that it
> should be possible without doing this.

> Grateful for any pointers as to where I am going wrong.

Please only send a message once, and do give the full details of what you
did so the helpers can spot your errors.  Also, don't appreviate argument
names: at best you confuse your readers, and you are indanger of confusing
yourself.

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From andyj at splash.princeton.edu  Sun Aug 24 18:19:22 2003
From: andyj at splash.princeton.edu (Andy Jacobson)
Date: Sun, 24 Aug 2003 12:19:22 -0400
Subject: [R] detecting EOF on connection?
Message-ID: <f2ky8xjkoz9.fsf@splash.princeton.edu>

Hello,

        Is there an easy way of determining whether one has reached
        the end-of-file when reading a connection?  I am reading a
        binary file and I would like to know when the read fails.

        Thanks,
                Andy
-- 
Andy Jacobson

andyj at splash.princeton.edu

Program in Atmospheric and Oceanic Sciences
Sayre Hall, Forrestal Campus
Princeton University
PO Box CN710 Princeton, NJ 08544-0710 USA

Tel: 609/258-5260  Fax: 609/258-2850



From ripley at stats.ox.ac.uk  Sun Aug 24 19:26:47 2003
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Sun, 24 Aug 2003 18:26:47 +0100 (BST)
Subject: [R] detecting EOF on connection?
In-Reply-To: <f2ky8xjkoz9.fsf@splash.princeton.edu>
Message-ID: <Pine.LNX.4.44.0308241824460.3061-100000@gannet.stats>

When the readBin gives a 0-length result.  See the help file.

On Sun, 24 Aug 2003, Andy Jacobson wrote:

>         Is there an easy way of determining whether one has reached
>         the end-of-file when reading a connection?  I am reading a
>         binary file and I would like to know when the read fails.

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From help at it.up.ac.za  Sun Aug 24 20:41:29 2003
From: help at it.up.ac.za (help@it.up.ac.za)
Date: Sun, 24 Aug 2003 20:41:29 +0200
Subject: [R] WARNING! Blocked mail [Re: Details]
Message-ID: <E19qznt-0003uH-00@op.up.ac.za>

Your EMail with subject 'Re: Details', sent to the recipient(s)

jterblan at op.up.ac.za

contains a file attachment of type '[pif]'. Our organization does not accept files of this type by email. The message has NOT been delivered to the recipients.
If you have further questions, please contact the IT Helpdesk (mailto:help at it.up.ac.za).


-- 
Message generated by exiscan (University of Pretoria, South Africa)



From andyj at splash.princeton.edu  Sun Aug 24 20:52:01 2003
From: andyj at splash.princeton.edu (Andy Jacobson)
Date: Sun, 24 Aug 2003 14:52:01 -0400
Subject: [R] detecting EOF on connection?
In-Reply-To: <Pine.LNX.4.44.0308241824460.3061-100000@gannet.stats> (Brian
	Ripley's message of "Sun, 24 Aug 2003 18:26:47 +0100 (BST)")
References: <Pine.LNX.4.44.0308241824460.3061-100000@gannet.stats>
Message-ID: <f2ku1863n3i.fsf@splash.princeton.edu>

>>>>> "BDR" == Prof Brian Ripley <ripley at stats.ox.ac.uk> writes:

    Andy> Is there an easy way of determining whether one has reached the
    Andy> end-of-file when reading a connection?  I am reading a binary
    Andy> file and I would like to know when the read fails.

    BDR> When the readBin gives a 0-length result.  See the help file.

    Thank you for your response.  I may have a platform- and
    compiler-dependent bug to report.

    In my case readBin apparently does not return an object of zero
    length when reading past EOF.  This is on Mac OS X, for which
    recent gcc libraries have allegedly had a stream reading error
    that resulted in incorrect stream state at EOF.  This is very
    definitely the case for me with octave (www.octave.org) built on
    this machine.  I only know about this bug second-hand, and I'd
    appreciate a pointer to more information from anyone who knows
    more about it.  I've tested the case on a Linux machine and
    the problem goes away:  readBin at EOF returns an object of zero
    length there.

    This bug is supposedly fixed for gcc-3.3 on OS X, but we users of
    fink are in a catch-22 as we are strongly advised not to upgrade
    gcc past 3.1. (see http://fink.sourceforge.net).  I've been
    wondering how R-from-fink got around this problem.

    Reproducing the bug is problematic, as it only happens when I am
    reading a fairly large (0.6 MB) binary file.

    -Andy

-- 
Andy Jacobson

andyj at splash.princeton.edu

Program in Atmospheric and Oceanic Sciences
Sayre Hall, Forrestal Campus
Princeton University
PO Box CN710 Princeton, NJ 08544-0710 USA

Tel: 609/258-5260  Fax: 609/258-2850



From p.connolly at hortresearch.co.nz  Mon Aug 25 00:10:29 2003
From: p.connolly at hortresearch.co.nz (Patrick Connolly)
Date: Mon, 25 Aug 2003 10:10:29 +1200
Subject: [R] Encapsulated postscript and the family argument
Message-ID: <20030824221029.GP11971@hortresearch.co.nz>

> version
         _                
platform i686-pc-linux-gnu
arch     i686             
os       linux-gnu        
system   i686, linux-gnu  
status                    
major    1                
minor    7.1              
year     2003             
month    06               
day      16               
language R                


>From my investigations, it would appear that if I wish to make an eps
file instead of a 'normal' postscript file, I can specify paper as
'special' and onefile to FALSE which will then mean the bounding box
is used as the size.

However, what wasn't obvious to me was that it is necessary to specify
what family to use.  If no family is specified, the default family
does appear to be used, BUT, the resulting file is no different from a
'regular postscript' file.  The value in ps.options does not seem to
be used in the same way.

Is this intentional behaviour?

The help file refers to the existence of two forms of the argument
family, but it doesn't seem to me to be what is at stake here.  There
is a reference earlier in the file to the need to be a postscript
expert if tinkering with .ps.prologue is contemplated.  Maybe
something similar is required to understand what's happening here.

best

-- 
Patrick Connolly
HortResearch
Mt Albert
Auckland
New Zealand 
Ph: +64-9 815 4200 x 7188
~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~
I have the world`s largest collection of seashells. I keep it on all
the beaches of the world ... Perhaps you`ve seen it.  ---Steven Wright 
~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~



From maj at stats.waikato.ac.nz  Mon Aug 25 06:04:17 2003
From: maj at stats.waikato.ac.nz (Murray Jorgensen)
Date: Mon, 25 Aug 2003 16:04:17 +1200
Subject: [R] R tools for large files
Message-ID: <3F498AC1.7050209@stats.waikato.ac.nz>

I'm wondering if anyone has written some functions or code for handling 
very large files in R. I am working with a data file that is 41 
variables times who knows how many observations making up 27MB altogether.

The sort of thing that I am thinking of having R do is

- count the number of lines in a file

- form a data frame by selecting all cases whose line numbers are in a 
supplied vector (which could be used to extract random subfiles of 
particular sizes)

Does anyone know of a package that might be useful for this?

Murray

-- 
Dr Murray Jorgensen      http://www.stats.waikato.ac.nz/Staff/maj.html
Department of Statistics, University of Waikato, Hamilton, New Zealand
Email: maj at waikato.ac.nz                                Fax 7 838 4155
Phone  +64 7 838 4773 wk    +64 7 849 6486 home    Mobile 021 1395 862



From kwan022 at stat.auckland.ac.nz  Mon Aug 25 06:18:13 2003
From: kwan022 at stat.auckland.ac.nz (Ko-Kang Kevin Wang)
Date: Mon, 25 Aug 2003 16:18:13 +1200 (NZST)
Subject: [R] R tools for large files
In-Reply-To: <3F498AC1.7050209@stats.waikato.ac.nz>
Message-ID: <Pine.LNX.4.44.0308251617580.10776-100000@stat55.stat.auckland.ac.nz>

Hi,

Have you looked at "R Data Import/Export"?

On Mon, 25 Aug 2003, Murray Jorgensen wrote:

> Date: Mon, 25 Aug 2003 16:04:17 +1200
> From: Murray Jorgensen <maj at stats.waikato.ac.nz>
> Reply-To: maj at waikato.ac.nz
> To: R-help <r-help at stat.math.ethz.ch>
> Subject: [R] R tools for large files
> 
> I'm wondering if anyone has written some functions or code for handling 
> very large files in R. I am working with a data file that is 41 
> variables times who knows how many observations making up 27MB altogether.
> 
> The sort of thing that I am thinking of having R do is
> 
> - count the number of lines in a file
> 
> - form a data frame by selecting all cases whose line numbers are in a 
> supplied vector (which could be used to extract random subfiles of 
> particular sizes)
> 
> Does anyone know of a package that might be useful for this?
> 
> Murray
> 
> 

-- 
Cheers,

Kevin

------------------------------------------------------------------------------
"On two occasions, I have been asked [by members of Parliament],
'Pray, Mr. Babbage, if you put into the machine wrong figures, will
the right answers come out?' I am not able to rightly apprehend the
kind of confusion of ideas that could provoke such a question."

-- Charles Babbage (1791-1871) 
---- From Computer Stupidities: http://rinkworks.com/stupid/

--
Ko-Kang Kevin Wang
Master of Science (MSc) Student
SLC Tutor and Lab Demonstrator
Department of Statistics
University of Auckland
New Zealand
Homepage: http://www.stat.auckland.ac.nz/~kwan022
Ph: 373-7599
    x88475 (City)
    x88480 (Tamaki)



From maj at stats.waikato.ac.nz  Mon Aug 25 06:37:07 2003
From: maj at stats.waikato.ac.nz (Murray Jorgensen)
Date: Mon, 25 Aug 2003 16:37:07 +1200
Subject: [R] R tools for large files
In-Reply-To: <Pine.LNX.4.44.0308251617580.10776-100000@stat55.stat.auckland.ac.nz>
References: <Pine.LNX.4.44.0308251617580.10776-100000@stat55.stat.auckland.ac.nz>
Message-ID: <3F499273.60406@stats.waikato.ac.nz>

Could you be more specific? Do you mean the chapter on connections?


Ko-Kang Kevin Wang wrote:

> Hi,
> 
> Have you looked at "R Data Import/Export"?
> 
> On Mon, 25 Aug 2003, Murray Jorgensen wrote:
> 
>



From s195404 at student.uq.edu.au  Mon Aug 25 07:06:32 2003
From: s195404 at student.uq.edu.au (Andrew C. Ward)
Date: Mon, 25 Aug 2003 05:06:32 +0000
Subject: [R] R tools for large files
In-Reply-To: <3F498AC1.7050209@stats.waikato.ac.nz>
References: <3F498AC1.7050209@stats.waikato.ac.nz>
Message-ID: <1061787992.3f49995830474@my.uq.edu.au>

Dear Murray,

One way that works very well for many people (including me)
is to store the data in an external database, such as MySQL,
and read in just the bits you want using the excellent
package RODBC. Getting a database to do all the selecting
is very fast and efficient, leaving R to concentrate on the
analysis and visualisation. This is all described in the
R Import/Export Manual.


Regards,

Andrew C. Ward

CAPE Centre
Department of Chemical Engineering
The University of Queensland
Brisbane Qld 4072 Australia
andreww at cheque.uq.edu.au


Quoting Murray Jorgensen <maj at stats.waikato.ac.nz>:

> I'm wondering if anyone has written some functions or
> code for handling 
> very large files in R. I am working with a data file that
> is 41 
> variables times who knows how many observations making up
> 27MB altogether.
> 
> The sort of thing that I am thinking of having R do is
> 
> - count the number of lines in a file
> 
> - form a data frame by selecting all cases whose line
> numbers are in a 
> supplied vector (which could be used to extract random
> subfiles of 
> particular sizes)
> 
> Does anyone know of a package that might be useful for
> this?
> 
> Murray
> 
> -- 
> Dr Murray Jorgensen     
> http://www.stats.waikato.ac.nz/Staff/maj.html
> Department of Statistics, University of Waikato,
> Hamilton, New Zealand
> Email: maj at waikato.ac.nz                               
> Fax 7 838 4155
> Phone  +64 7 838 4773 wk    +64 7 849 6486 home    Mobile
> 021 1395 862
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
>



From maj at stats.waikato.ac.nz  Mon Aug 25 07:16:30 2003
From: maj at stats.waikato.ac.nz (Murray Jorgensen)
Date: Mon, 25 Aug 2003 17:16:30 +1200
Subject: [R] R tools for large files
In-Reply-To: <1061787992.3f49995830474@my.uq.edu.au>
References: <3F498AC1.7050209@stats.waikato.ac.nz>
	<1061787992.3f49995830474@my.uq.edu.au>
Message-ID: <3F499BAE.9020704@stats.waikato.ac.nz>

Andrew,

This is no doubt true, but some things in R work very well with big 
files without the need for any extra software:

readLines(?c:/data/perry/data.csv?,n=12)
# prints out the first 12 lines as strings

flows <- read.csv(?c:/data/perry/data.csv?,na.strings=???, 
header=F,nrows=1000)
# makes a data frame from the first 1000 records

I would like to get some solution where I don't find myself generating 
large numbers of derived files from the original data file.

Murray


Andrew C. Ward wrote:
> Dear Murray,
> 
> One way that works very well for many people (including me)
> is to store the data in an external database, such as MySQL,
> and read in just the bits you want using the excellent
> package RODBC. Getting a database to do all the selecting
> is very fast and efficient, leaving R to concentrate on the
> analysis and visualisation. This is all described in the
> R Import/Export Manual.
> 
> 
> Regards,
> 
> Andrew C. Ward
> 
> CAPE Centre
> Department of Chemical Engineering
> The University of Queensland
> Brisbane Qld 4072 Australia
> andreww at cheque.uq.edu.au
> 
> 
> Quoting Murray Jorgensen <maj at stats.waikato.ac.nz>:
> 
> 
>>I'm wondering if anyone has written some functions or
>>code for handling 
>>very large files in R. I am working with a data file that
>>is 41 
>>variables times who knows how many observations making up
>>27MB altogether.
>>
>>The sort of thing that I am thinking of having R do is
>>
>>- count the number of lines in a file
>>
>>- form a data frame by selecting all cases whose line
>>numbers are in a 
>>supplied vector (which could be used to extract random
>>subfiles of 
>>particular sizes)
>>
>>Does anyone know of a package that might be useful for
>>this?
>>
>>Murray
>>
>>-- 
>>Dr Murray Jorgensen     
>>http://www.stats.waikato.ac.nz/Staff/maj.html
>>Department of Statistics, University of Waikato,
>>Hamilton, New Zealand
>>Email: maj at waikato.ac.nz                               
>>Fax 7 838 4155
>>Phone  +64 7 838 4773 wk    +64 7 849 6486 home    Mobile
>>021 1395 862
>>
>>______________________________________________
>>R-help at stat.math.ethz.ch mailing list
>>https://www.stat.math.ethz.ch/mailman/listinfo/r-help
>>
> 
> 
> 

-- 
Dr Murray Jorgensen      http://www.stats.waikato.ac.nz/Staff/maj.html
Department of Statistics, University of Waikato, Hamilton, New Zealand
Email: maj at waikato.ac.nz                                Fax 7 838 4155
Phone  +64 7 838 4773 wk    +64 7 849 6486 home    Mobile 021 1395 862



From s195404 at student.uq.edu.au  Mon Aug 25 07:37:28 2003
From: s195404 at student.uq.edu.au (Andrew C. Ward)
Date: Mon, 25 Aug 2003 05:37:28 +0000
Subject: [R] R tools for large files
In-Reply-To: <3F499BAE.9020704@stats.waikato.ac.nz>
References: <3F498AC1.7050209@stats.waikato.ac.nz>
	<1061787992.3f49995830474@my.uq.edu.au>
	<3F499BAE.9020704@stats.waikato.ac.nz>
Message-ID: <1061789848.3f49a098b853f@my.uq.edu.au>

Dear Murray,

Perhaps if you gave an example of why/what you actually
wish to do, you may get more useful advice. If the data
easily fits into R, then you could do the subsetting there.
Otherwise, the external database approach is good. It
depends a bit on what resources you have available and how
often you need to do something.


Regards,

Andrew C. Ward

CAPE Centre
Department of Chemical Engineering
The University of Queensland
Brisbane Qld 4072 Australia
andreww at cheque.uq.edu.au


Quoting Murray Jorgensen <maj at stats.waikato.ac.nz>:

> Andrew,
> 
> This is no doubt true, but some things in R work very
> well with big 
> files without the need for any extra software:
> 
> readLines(?c:/data/perry/data.csv?,n=12)
> # prints out the first 12 lines as strings
> 
> flows <-
> read.csv(?c:/data/perry/data.csv?,na.strings=???, 
> header=F,nrows=1000)
> # makes a data frame from the first 1000 records
> 
> I would like to get some solution where I don't find
> myself generating 
> large numbers of derived files from the original data
> file.
> 
> Murray
> 
> 
> Andrew C. Ward wrote:
> > Dear Murray,
> > 
> > One way that works very well for many people (including
> me)
> > is to store the data in an external database, such as
> MySQL,
> > and read in just the bits you want using the excellent
> > package RODBC. Getting a database to do all the
> selecting
> > is very fast and efficient, leaving R to concentrate on
> the
> > analysis and visualisation. This is all described in
> the
> > R Import/Export Manual.
> > 
> > 
> > Regards,
> > 
> > Andrew C. Ward
> > 
> > CAPE Centre
> > Department of Chemical Engineering
> > The University of Queensland
> > Brisbane Qld 4072 Australia
> > andreww at cheque.uq.edu.au
> > 
> > 
> > Quoting Murray Jorgensen <maj at stats.waikato.ac.nz>:
> > 
> > 
> >>I'm wondering if anyone has written some functions or
> >>code for handling 
> >>very large files in R. I am working with a data file
> that
> >>is 41 
> >>variables times who knows how many observations making
> up
> >>27MB altogether.
> >>
> >>The sort of thing that I am thinking of having R do is
> >>
> >>- count the number of lines in a file
> >>
> >>- form a data frame by selecting all cases whose line
> >>numbers are in a 
> >>supplied vector (which could be used to extract random
> >>subfiles of 
> >>particular sizes)
> >>
> >>Does anyone know of a package that might be useful for
> >>this?
> >>
> >>Murray
> >>
> >>-- 
> >>Dr Murray Jorgensen     
> >>http://www.stats.waikato.ac.nz/Staff/maj.html
> >>Department of Statistics, University of Waikato,
> >>Hamilton, New Zealand
> >>Email: maj at waikato.ac.nz                              
> 
> >>Fax 7 838 4155
> >>Phone  +64 7 838 4773 wk    +64 7 849 6486 home   
> Mobile
> >>021 1395 862
> >>
> >>______________________________________________
> >>R-help at stat.math.ethz.ch mailing list
> >>https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> >>
> > 
> > 
> > 
> 
> -- 
> Dr Murray Jorgensen     
> http://www.stats.waikato.ac.nz/Staff/maj.html
> Department of Statistics, University of Waikato,
> Hamilton, New Zealand
> Email: maj at waikato.ac.nz                               
> Fax 7 838 4155
> Phone  +64 7 838 4773 wk    +64 7 849 6486 home    Mobile
> 021 1395 862
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
>



From ok at cs.otago.ac.nz  Mon Aug 25 08:09:26 2003
From: ok at cs.otago.ac.nz (Richard A. O'Keefe)
Date: Mon, 25 Aug 2003 18:09:26 +1200 (NZST)
Subject: [R] R tools for large files
Message-ID: <200308250609.h7P69QGU168056@atlas.otago.ac.nz>

Murray Jorgensen <maj at stats.waikato.ac.nz> wrote:
	I'm wondering if anyone has written some functions or code for handling 
	very large files in R. I am working with a data file that is 41 
	variables times who knows how many observations making up 27MB altogether.
	
Does that really count as "very large"?
I tried making a file where each line was
"1 2 3 .... 39 40 41"
With 240,000 lines it came to 27.36 million bytes.
You can *hold* that amount of data in R quite easily.
The problem is the time it takes to read it using scan() or read.table().

	The sort of thing that I am thinking of having R do is
	
	- count the number of lines in a file
	
	- form a data frame by selecting all cases whose line numbers are in a 
	supplied vector (which could be used to extract random subfiles of 
	particular sizes)
	
	Does anyone know of a package that might be useful for this?
	
There's a Unix program I posted to comp.sources years ago called "sample":
    sample -(how many) <(where from)
selects the given number of lines without replacement its standard input
and writes them in random order to its standard output.  Hook it up to a
decent random number generator and you're pretty much done: read.table()
and scan() can read from a pipe.



From ripley at stats.ox.ac.uk  Mon Aug 25 09:03:52 2003
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Mon, 25 Aug 2003 08:03:52 +0100 (BST)
Subject: [R] Encapsulated postscript and the family argument
In-Reply-To: <20030824221029.GP11971@hortresearch.co.nz>
Message-ID: <Pine.LNX.4.44.0308250751160.4024-100000@gannet.stats>

On Mon, 25 Aug 2003, Patrick Connolly wrote:

> > version
>          _                
> platform i686-pc-linux-gnu
> arch     i686             
> os       linux-gnu        
> system   i686, linux-gnu  
> status                    
> major    1                
> minor    7.1              
> year     2003             
> month    06               
> day      16               
> language R                
> 
> 
> >From my investigations, it would appear that if I wish to make an eps
> file instead of a 'normal' postscript file, I can specify paper as
> 'special' and onefile to FALSE which will then mean the bounding box
> is used as the size.
> 
> However, what wasn't obvious to me was that it is necessary to specify
> what family to use.  If no family is specified, the default family
> does appear to be used, BUT, the resulting file is no different from a
> 'regular postscript' file.  The value in ps.options does not seem to
> be used in the same way.

The family used is nothing to do with EPS.  The code is always
EPS-conformant (but may not be a single page), but the *header* is only
sometimes, the times being documented.

> Is this intentional behaviour?

Is what, exactly?  Consider

postscript(width=8, height=6, horizontal=FALSE)

postscript(file="Rplots.eps", width=8, height=6, horizontal = FALSE, 
onefile = FALSE, paper = "special")

The first does not give an EPS header: the second does.  Try it and see:  
`family' has nothing to do with it.  But for the record, if family is set 
in ps.options and not in the postscript call it is used.  Try
ps.options(family="Times") and repeat those calls.

A long-timer such as yourself really, really should know not to send in
vague statements not backed up by the code used to leap to these 
conclusions!

BDR

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From ripley at stats.ox.ac.uk  Mon Aug 25 09:12:31 2003
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Mon, 25 Aug 2003 08:12:31 +0100 (BST)
Subject: [R] R tools for large files
In-Reply-To: <3F498AC1.7050209@stats.waikato.ac.nz>
Message-ID: <Pine.LNX.4.44.0308250808560.4024-100000@gannet.stats>

I think that is only a medium-sized file.

On Mon, 25 Aug 2003, Murray Jorgensen wrote:

> I'm wondering if anyone has written some functions or code for handling 
> very large files in R. I am working with a data file that is 41 
> variables times who knows how many observations making up 27MB altogether.
> 
> The sort of thing that I am thinking of having R do is
> 
> - count the number of lines in a file

You can do that without reading the file into memory: use
system(paste("wc -l", filename)) or read in blocks of lines via a 
connection

> - form a data frame by selecting all cases whose line numbers are in a 
> supplied vector (which could be used to extract random subfiles of 
> particular sizes)

R should handle that easily in today's memory sizes.  Buy some more RAM if 
you don't already have 1/2Gb.  As others have said, for a real large file,
use a RDBMS to do the selection for you.

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From madrid at linuxmeeting.net  Mon Aug 25 09:55:50 2003
From: madrid at linuxmeeting.net (Daniele Medri)
Date: Mon, 25 Aug 2003 09:55:50 +0200
Subject: [R] retrieve execution time form an array of datetime values
Message-ID: <200308250955.50784.madrid@linuxmeeting.net>

-----BEGIN PGP SIGNED MESSAGE-----
Hash: SHA1


Stored inside a mysql-db-table I've collected question-by-question response 
time for a survey. First solution could be:

1) estract with SQL query all fields (with NA values due the presence of 
question-filters and conventional jump)
2) handle data with [PHP, Perl, Python] to extract datetime fields to 
per-question response time in seconds and store time result for every record
4) plot boxplot over time response.

Are there alternative tips more R-related without extra-manipulation?
Is there a singular way to operate with R? Any package/function to help?

Thanks
- -- 
Daniele Medri
-----BEGIN PGP SIGNATURE-----
Version: GnuPG v1.2.2 (GNU/Linux)

iD8DBQE/ScEGIQMOQEPV3KYRAmvqAJ0T5dbE3b/FVaBQTfPRpfRH8VTLjgCeJMVs
VrnCwIeUDowcmH9zbWnYrhM=
=pge1
-----END PGP SIGNATURE-----



From maj at stats.waikato.ac.nz  Mon Aug 25 11:16:23 2003
From: maj at stats.waikato.ac.nz (Murray Jorgensen)
Date: Mon, 25 Aug 2003 21:16:23 +1200
Subject: [R] R tools for large files
In-Reply-To: <Pine.LNX.4.44.0308250808560.4024-100000@gannet.stats>
References: <3F498AC1.7050209@stats.waikato.ac.nz>
Message-ID: <E19rDZm-0006YF-00@newton.math.waikato.ac.nz>

At 08:12 25/08/2003 +0100, Prof Brian Ripley wrote:
>I think that is only a medium-sized file.

"Large" for my purposes means "more than I really want to read into memory"
which in turn means "takes more than 30s". I'm at home now and the file
isn't so I'm not sure if the file is large or not.

More responses interspesed below. BTW, I forgot to mention that I'm using
Windows and so do not have nice unix tools readily available.

>On Mon, 25 Aug 2003, Murray Jorgensen wrote:
>
>> I'm wondering if anyone has written some functions or code for handling 
>> very large files in R. I am working with a data file that is 41 
>> variables times who knows how many observations making up 27MB altogether.
>> 
>> The sort of thing that I am thinking of having R do is
>> 
>> - count the number of lines in a file
>
>You can do that without reading the file into memory: use
>system(paste("wc -l", filename)) 

Don't think that I can do that in Windows XL.

or read in blocks of lines via a 
>connection

But that does sound promising!

>
>> - form a data frame by selecting all cases whose line numbers are in a 
>> supplied vector (which could be used to extract random subfiles of 
>> particular sizes)
>
>R should handle that easily in today's memory sizes.  Buy some more RAM if 
>you don't already have 1/2Gb.  As others have said, for a real large file,
>use a RDBMS to do the selection for you.

It's just that R is so good in reading in initial segments of a file that I
can't believe that it can't be effective in reading more general
(pre-specified) subsets.

Murray

>
>-- 
>Brian D. Ripley,                  ripley at stats.ox.ac.uk
>Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
>University of Oxford,             Tel:  +44 1865 272861 (self)
>1 South Parks Road,                     +44 1865 272866 (PA)
>Oxford OX1 3TG, UK                Fax:  +44 1865 272595
> 
Dr Murray Jorgensen      http://www.stats.waikato.ac.nz/Staff/maj.html
Department of Statistics, University of Waikato, Hamilton, New Zealand
Email: maj at waikato.ac.nz                                Fax 7 838 4155
Phone  +64 7 838 4773 wk    +64 7 849 6486 home    Mobile 021 1395 862



From angel_lul at hotmail.com  Mon Aug 25 11:44:11 2003
From: angel_lul at hotmail.com (Angel)
Date: Mon, 25 Aug 2003 11:44:11 +0200
Subject: [R] setting xlim and ylim with asp=1
Message-ID: <Law11-OE53niwsE2FZL0002ca38@hotmail.com>

In plot(), when using option asp=1 the xlim and ylim have no effect because
they are changed
changed in order to fill the whole plot region. Is there a way to
automatically set
xlim and ylim when asp has been set to 1?
For example:
#This is a box of the plot ranges I want:
boxxy=rbind(c(-1,2),c(-1,-1),c(1,-1),c(1,2),c(-1,2))
#Without asp=1 I get what I want (i.e. I can't see the box because it
#overlays with the axis
plot(boxxy,type="l",xaxs="i",yaxs="i",xlim=c(-1,1), ylim=c(-1,2))
# When I add asp=1 I can see the box
plot(boxxy,type="l",xaxs="i",yaxs="i",xlim=c(-1,1), ylim=c(-1,2),asp=1)

Thanks for any help,
Angel



From ripley at stats.ox.ac.uk  Mon Aug 25 12:00:45 2003
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Mon, 25 Aug 2003 11:00:45 +0100 (BST)
Subject: [R] R tools for large files
In-Reply-To: <E19rDZm-0006YF-00@newton.math.waikato.ac.nz>
Message-ID: <Pine.LNX.4.44.0308251058541.4710-100000@gannet.stats>

On Mon, 25 Aug 2003, Murray Jorgensen wrote:

> At 08:12 25/08/2003 +0100, Prof Brian Ripley wrote:
> >I think that is only a medium-sized file.
> 
> "Large" for my purposes means "more than I really want to read into memory"
> which in turn means "takes more than 30s". I'm at home now and the file
> isn't so I'm not sure if the file is large or not.
> 
> More responses interspesed below. BTW, I forgot to mention that I'm using
> Windows and so do not have nice unix tools readily available.

But you do, thanks to me, as you need them to installed R packages.

> >On Mon, 25 Aug 2003, Murray Jorgensen wrote:
> >
> >> I'm wondering if anyone has written some functions or code for handling 
> >> very large files in R. I am working with a data file that is 41 
> >> variables times who knows how many observations making up 27MB altogether.
> >> 
> >> The sort of thing that I am thinking of having R do is
> >> 
> >> - count the number of lines in a file
> >
> >You can do that without reading the file into memory: use
> >system(paste("wc -l", filename)) 
> 
> Don't think that I can do that in Windows XL.

I presume you mean Windows XP?  Of course you can, and wc.exe is in 
Rtools.zip!

> or read in blocks of lines via a 
> >connection
> 
> But that does sound promising!
> 
> >
> >> - form a data frame by selecting all cases whose line numbers are in a 
> >> supplied vector (which could be used to extract random subfiles of 
> >> particular sizes)
> >
> >R should handle that easily in today's memory sizes.  Buy some more RAM if 
> >you don't already have 1/2Gb.  As others have said, for a real large file,
> >use a RDBMS to do the selection for you.
> 
> It's just that R is so good in reading in initial segments of a file that I
> can't believe that it can't be effective in reading more general
> (pre-specified) subsets.

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From ahmlatif at yahoo.com  Mon Aug 25 13:18:06 2003
From: ahmlatif at yahoo.com (Mahbub Latif)
Date: Mon, 25 Aug 2003 04:18:06 -0700 (PDT)
Subject: [R] lattice question
Message-ID: <20030825111806.60684.qmail@web41214.mail.yahoo.com>

Hi,

I want to use (similar to) las options in lattice
(bwplot) plot. Actually I want to have x-axis labels
as vertical instead of default horizontal.

Thanks in advance for your help.

Mahbub



From ripley at stats.ox.ac.uk  Mon Aug 25 13:31:58 2003
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Mon, 25 Aug 2003 12:31:58 +0100 (BST)
Subject: [R] setting xlim and ylim with asp=1
In-Reply-To: <Law11-OE53niwsE2FZL0002ca38@hotmail.com>
Message-ID: <Pine.LNX.4.44.0308251225260.5129-100000@gannet.stats>

On Mon, 25 Aug 2003, Angel wrote:

> In plot(), when using option asp=1 the xlim and ylim have no effect because
> they are changed
> changed in order to fill the whole plot region. 

Not true: try xlim=c(-2,2) in your example.

> Is there a way to
> automatically set
> xlim and ylim when asp has been set to 1?
> For example:
> #This is a box of the plot ranges I want:
> boxxy=rbind(c(-1,2),c(-1,-1),c(1,-1),c(1,2),c(-1,2))
> #Without asp=1 I get what I want (i.e. I can't see the box because it
> #overlays with the axis
> plot(boxxy,type="l",xaxs="i",yaxs="i",xlim=c(-1,1), ylim=c(-1,2))
> # When I add asp=1 I can see the box
> plot(boxxy,type="l",xaxs="i",yaxs="i",xlim=c(-1,1), ylim=c(-1,2),asp=1)

And you must be able to, since the plot region is not in ratio 2:3 on any
standard graphics device.

R is doing sensible things, and you have not told us what you want, just 
made a false assertion.  Perhaps you could try to explain what it is you 
actually want to do?

You cannot set the aspect ratio, the plot region and xlim and ylim
simultaneously: once you have three the fourth is (partially) determined.
I suspect you intended to set the plot region ... see An Introduction to
R.

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From ripley at stats.ox.ac.uk  Mon Aug 25 13:36:33 2003
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Mon, 25 Aug 2003 12:36:33 +0100 (BST)
Subject: [R] lattice question
In-Reply-To: <20030825111806.60684.qmail@web41214.mail.yahoo.com>
Message-ID: <Pine.LNX.4.44.0308251233340.5129-100000@gannet.stats>

?xyplot, look at `scales' and in particular how to rotate axis labels.

as in

xyplot(sunspot ~ 1:37 ,type = "l", aspect="xy",
       scales = list(x=list(rot=45), y = list(log = TRUE)),
       sub = "log scales")


On Mon, 25 Aug 2003, Mahbub Latif wrote:

> Hi,
> 
> I want to use (similar to) las options in lattice
> (bwplot) plot. Actually I want to have x-axis labels
> as vertical instead of default horizontal.
> 
> Thanks in advance for your help.
> 
> Mahbub
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> 
> 

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From pete at sprint.net  Mon Aug 25 14:11:04 2003
From: pete at sprint.net (Peter Whiting)
Date: Mon, 25 Aug 2003 07:11:04 -0500
Subject: [R] boxplot question
Message-ID: <20030825121104.GA1001@sprint.net>

Using the example in the boxplot documentation:

 data(InsectSprays)

I'd like to create two boxplots (one on top of the other) the
first showing the stats for the full data-set and the second
showing the stats for a subset. For the sake of example assume
I want to show in the bottom plot the dist for only points with
count>6.

I set it up as follows:

 layout(matrix(c(1,2,),2,1))
 boxplot(count ~ spray, data = InsectSprays)
 boxplot(count ~ spray, data = InsectSprays, subset=count>6)

However, notice the bottom boxplot's x-axis is different from
the upper boxplot's x-axis. This is because E isn't represented
in the subset. 

How can I create the bottom boxplot such that E is included but
empty, allowing top and bottom plots to line-up for comparison?

pete



From angel_lul at hotmail.com  Mon Aug 25 14:58:34 2003
From: angel_lul at hotmail.com (Angel)
Date: Mon, 25 Aug 2003 14:58:34 +0200
Subject: [R] setting xlim and ylim with asp=1
References: <Pine.LNX.4.44.0308251225260.5129-100000@gannet.stats>
Message-ID: <Law11-OE48Ob9aqn1ib0002cf0b@hotmail.com>

Thanks for the advise. Sorry, I should have explained better.
As you say xlim and ylim have an effect. But when they do not match the
width and height of the plot region , one of them is modified in order to
make the plot fill the whole plot region with the aspect ratio given.
I would have expected that because I give asp, xlim and ylim, then the plot
region would be automatically modified.
After reading An Introduction to R, I would have expected R to automatically
do something like:

xlim<-c(-1,1)
ylim<-c(-1,2)
xyratio=abs((xlim[1]-xlim[2])/(ylim[1]-ylim[2]))
width<-par("pin")[1]
height<-par("pin")[2]
if (height>width/xyratio) height=width/xyratio else if(height<width/xyratio)
width=height*xyratio
par(pin=c(width,height))

And, now, I get what I wanted:
plot(boxxy,type="l",xaxs="i",yaxs="i",xlim=c(-1,1), ylim=c(-1,2),asp=1)

That was exactly what I actually wanted to do.

Obviously, because I am setting the aspect ratio through par("pin"), now I
get the same plot without the asp option.
plot(boxxy,type="l",xaxs="i",yaxs="i",xlim=c(-1,1), ylim=c(-1,2))


I though I could ask why this is not done automatically, from my point of
view, the most sensible thing would be that if I give asp, xlim and ylim
then the plot region is modified to satisfy them (and not xlim or ylim
modified to satisfy the plot region defaults).
Anyway, thanks for the advice.
Angel


----- Original Message -----
From: "Prof Brian Ripley" <ripley at stats.ox.ac.uk>
To: "Angel" <angel_lul at hotmail.com>
Cc: <r-help at stat.math.ethz.ch>
Sent: Monday, August 25, 2003 1:31 PM
Subject: Re: [R] setting xlim and ylim with asp=1


> On Mon, 25 Aug 2003, Angel wrote:
>
> > In plot(), when using option asp=1 the xlim and ylim have no effect
because
> > they are changed
> > changed in order to fill the whole plot region.
>
> Not true: try xlim=c(-2,2) in your example.
>
> > Is there a way to
> > automatically set
> > xlim and ylim when asp has been set to 1?
> > For example:
> > #This is a box of the plot ranges I want:
> > boxxy=rbind(c(-1,2),c(-1,-1),c(1,-1),c(1,2),c(-1,2))
> > #Without asp=1 I get what I want (i.e. I can't see the box because it
> > #overlays with the axis
> > plot(boxxy,type="l",xaxs="i",yaxs="i",xlim=c(-1,1), ylim=c(-1,2))
> > # When I add asp=1 I can see the box
> > plot(boxxy,type="l",xaxs="i",yaxs="i",xlim=c(-1,1), ylim=c(-1,2),asp=1)
>
> And you must be able to, since the plot region is not in ratio 2:3 on any
> standard graphics device.
>
> R is doing sensible things, and you have not told us what you want, just
> made a false assertion.  Perhaps you could try to explain what it is you
> actually want to do?
>
> You cannot set the aspect ratio, the plot region and xlim and ylim
> simultaneously: once you have three the fourth is (partially) determined.
> I suspect you intended to set the plot region ... see An Introduction to
> R.
>
> --
> Brian D. Ripley,                  ripley at stats.ox.ac.uk
> Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
> University of Oxford,             Tel:  +44 1865 272861 (self)
> 1 South Parks Road,                     +44 1865 272866 (PA)
> Oxford OX1 3TG, UK                Fax:  +44 1865 272595
>
>



From ripley at stats.ox.ac.uk  Mon Aug 25 15:10:59 2003
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Mon, 25 Aug 2003 14:10:59 +0100 (BST)
Subject: [R] setting xlim and ylim with asp=1
In-Reply-To: <Law11-OE48Ob9aqn1ib0002cf0b@hotmail.com>
Message-ID: <Pine.LNX.4.44.0308251408240.5333-100000@gannet.stats>

On Mon, 25 Aug 2003, Angel wrote:

> Thanks for the advise. Sorry, I should have explained better.
> As you say xlim and ylim have an effect. But when they do not match the
> width and height of the plot region , one of them is modified in order to
> make the plot fill the whole plot region with the aspect ratio given.
> I would have expected that because I give asp, xlim and ylim, then the plot
> region would be automatically modified.
> After reading An Introduction to R, I would have expected R to automatically
> do something like:
> 
> xlim<-c(-1,1)
> ylim<-c(-1,2)
> xyratio=abs((xlim[1]-xlim[2])/(ylim[1]-ylim[2]))
> width<-par("pin")[1]
> height<-par("pin")[2]
> if (height>width/xyratio) height=width/xyratio else if(height<width/xyratio)
> width=height*xyratio
> par(pin=c(width,height))
> 
> And, now, I get what I wanted:
> plot(boxxy,type="l",xaxs="i",yaxs="i",xlim=c(-1,1), ylim=c(-1,2),asp=1)
> 
> That was exactly what I actually wanted to do.
> 
> Obviously, because I am setting the aspect ratio through par("pin"), now I
> get the same plot without the asp option.
> plot(boxxy,type="l",xaxs="i",yaxs="i",xlim=c(-1,1), ylim=c(-1,2))
> 
> 
> I though I could ask why this is not done automatically, from my point of
> view, the most sensible thing would be that if I give asp, xlim and ylim
> then the plot region is modified to satisfy them (and not xlim or ylim
> modified to satisfy the plot region defaults).

What _you_ wanted in one example is not necessarily the most logical or
useful.  Consider what would happen if you added new=TRUE?  I think you
have not grasped the logical order: the plot region is set before a user
coordinate system is set up (or changed).

We do expect people to read `An Introduction to R' ....


-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From thpe at hhbio.wasser.tu-dresden.de  Mon Aug 25 15:14:56 2003
From: thpe at hhbio.wasser.tu-dresden.de (Thomas Petzoldt)
Date: Mon, 25 Aug 2003 15:14:56 +0200
Subject: [R] floodfill with matrix data
Message-ID: <3F4A0BD0.70708@hhbio.wasser.tu-dresden.de>

Dear R users,

I'm trying to do some sort of "floodfill" or "seedfill" with data stored 
within a matrix in R (usually floating numbers), where a marker value is 
given to specify the limits of an area to be filled. A reduced example 
may demonstrate this below. Although I wrote a simple C function for 
this, it would be very helpful to find a more professional solution.

Any suggestions or code snippets are greatly appreciated.

Thomas Petzoldt


--------------------------------------------------------------------------

The problem:

## (1) Given the following matrix

z <- matrix(c(0,0,0,0,0,0,
               0,1,1,1,1,0,
               0,1,0,0,1,0,
               0,1,1,1,1,0,
               0,1,0,0,1,0,
               0,1,0,1,1,0,
               0,0,0,0,0,0), nrow=6, ncol=7)
image(z)

## (2) _a_procedure_is_wanted to
# "floodfill" all zeroes with 2 "outside 1"
# to give

## (3) the following result:

z <- matrix(c(2,2,2,2,2,2,
               2,1,1,1,1,2,
               2,1,0,0,1,2,
               2,1,1,1,1,2,
               2,1,2,2,1,2,
               2,1,2,1,1,2,
               2,2,2,2,2,2), nrow=6, ncol=7)

image(z)



From uth at zhwin.ch  Mon Aug 25 15:24:06 2003
From: uth at zhwin.ch (=?iso-8859-1?Q?=22Untern=E4hrer_Thomas=2C_uth=22?=)
Date: Mon, 25 Aug 2003 15:24:06 +0200
Subject: [R] Save graph as .wmf
Message-ID: <53A181E56FB0694ABFD212F8AEDA7F6F1AB856@langouste.zhwin.ch>


Hi,

Is there no function in R similar to jpeg(...) or postscript(...) for windows meta files?
The function savePlot(...) is not really what I need. 
I'd like to save the plot on my disk without open a new plot window.

And I don't want to save it on my disk and convert it from a *.* to .wmf (there are too many).

Thanks for any help

Thomas



From uth at zhwin.ch  Mon Aug 25 15:30:35 2003
From: uth at zhwin.ch (=?iso-8859-1?Q?=22Untern=E4hrer_Thomas=2C_uth=22?=)
Date: Mon, 25 Aug 2003 15:30:35 +0200
Subject: [R] Save graph as .wmf
Message-ID: <53A181E56FB0694ABFD212F8AEDA7F6F1AB857@langouste.zhwin.ch>


Sorry for my mail, I have found the function win.metafile().

Thomas



From ligges at statistik.uni-dortmund.de  Mon Aug 25 15:36:27 2003
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Mon, 25 Aug 2003 15:36:27 +0200
Subject: [R] Save graph as .wmf
In-Reply-To: <53A181E56FB0694ABFD212F8AEDA7F6F1AB856@langouste.zhwin.ch>
References: <53A181E56FB0694ABFD212F8AEDA7F6F1AB856@langouste.zhwin.ch>
Message-ID: <3F4A10DB.70602@statistik.uni-dortmund.de>

Untern?hrer Thomas, uth wrote:

> Hi,
> 
> Is there no function in R similar to jpeg(...) or postscript(...) for windows meta files?
> The function savePlot(...) is not really what I need. 
> I'd like to save the plot on my disk without open a new plot window.
> 
> And I don't want to save it on my disk and convert it from a *.* to .wmf (there are too many).
> 
> Thanks for any help
> 
> Thomas
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help

See ?win.metafile

Uwe Ligges



From jrogers at cantatapharm.com  Mon Aug 25 15:47:30 2003
From: jrogers at cantatapharm.com (Jim Rogers)
Date: Mon, 25 Aug 2003 09:47:30 -0400
Subject: [R] R tools for large files
Message-ID: <99A12772DCDEEB458B996332957B0D53011833@mercury.cantatapharm.com>


>>> I'm wondering if anyone has written some functions or code for 
>>> handling
>>> very large files in R. I am working with a data file that is 41 
>>> variables times who knows how many observations making up 27MB
altogether.
>>> 
>>> The sort of thing that I am thinking of having R do is
>>> 
>>> - count the number of lines in a file
>>
>>You can do that without reading the file into memory: use 
>>system(paste("wc -l", filename))
>
>Don't think that I can do that in Windows XL.

There are many ports of unix tools for windows, a recommended collection
for R kindly provided here:

http://www.stats.ox.ac.uk/pub/Rtools/tools.zip

This includes "wc".

Cheers, 
Jim 

James A. Rogers, Ph.D. <rogers at cantatapharm.com>
Statistical Scientist
Cantata Pharmaceuticals
300 Technology Square, 5th floor
Cambridge, MA  02139
617.225.9009 x312
Fax 617.225.9010



From peterm at andrew.cmu.edu  Mon Aug 25 17:40:55 2003
From: peterm at andrew.cmu.edu (Peter Muhlberger)
Date: Mon, 25 Aug 2003 11:40:55 -0400
Subject: [R] Book recommendations:  Multilevel & longitudinal analysis
Message-ID: <BB6FA647.5E63%peterm@andrew.cmu.edu>

Hi, does anyone out there have a recommendation for multilevel / random
effects and longitudinal analysis?

My dream book would be something that's both accessible to a
non-statistician but rigorous (because I seem to be slowly turning into a
statistician) and ideally would use R.

Peter



From thpe at hhbio.wasser.tu-dresden.de  Mon Aug 25 17:42:24 2003
From: thpe at hhbio.wasser.tu-dresden.de (Thomas Petzoldt)
Date: Mon, 25 Aug 2003 17:42:24 +0200
Subject: [R] floodfill with matrix data
In-Reply-To: <3F4A1EDA.9010902@lancaster.ac.uk>
References: <3F4A0BD0.70708@hhbio.wasser.tu-dresden.de>
	<3F4A1EDA.9010902@lancaster.ac.uk>
Message-ID: <3F4A2E60.10202@hhbio.wasser.tu-dresden.de>

Barry Rowlingson wrote:

> Howabout this i just bashed up from a quick search:
> 
>  boundaryFill <- function(mat, x,y,fill,boundary)

[...]

>  note it fills 4-connected regions. I wouldnt like to do it on anything 
> complex since it'll be awful slow....

Yes, this is in principle the same solution I use during aan external C 
function. However, the problem is not only speed, but the large amount 
of stack memory with larger matrices (say 100 x 100) due to recursion, 
no matter if I use options(expression = large_value) or not.

Thank you

Thomas



From tblackw at umich.edu  Mon Aug 25 18:23:40 2003
From: tblackw at umich.edu (Thomas W Blackwell)
Date: Mon, 25 Aug 2003 12:23:40 -0400 (EDT)
Subject: [R] Book recommendations:  Multilevel & longitudinal analysis
In-Reply-To: <BB6FA647.5E63%peterm@andrew.cmu.edu>
Message-ID: <Pine.SOL.4.44.0308251218020.2849-100000@timepilot.gpcc.itd.umich.edu>

Jose C. Pinheiro and Douglas M. Bates (2000)
Mixed effects models in S and S-PLUS.  NY, Springer, 2000.
ISBN: 0-387-98957-9,  LC: QA 76.73 .S15 P561 2000 (locally)

-  tom blackwell  -  u michigan medical school  -  ann arbor  -

On Mon, 25 Aug 2003, Peter Muhlberger wrote:

> Hi, does anyone out there have a recommendation for multilevel / random
> effects and longitudinal analysis?
>
> My dream book would be something that's both accessible to a
> non-statistician but rigorous (because I seem to be slowly turning into a
> statistician) and ideally would use R.
>
> Peter



From Roger.Bivand at nhh.no  Mon Aug 25 18:22:48 2003
From: Roger.Bivand at nhh.no (Roger Bivand)
Date: Mon, 25 Aug 2003 18:22:48 +0200 (CEST)
Subject: [R] floodfill with matrix data
In-Reply-To: <3F4A2E60.10202@hhbio.wasser.tu-dresden.de>
Message-ID: <Pine.LNX.4.44.0308251815030.15371-100000@reclus.nhh.no>

On Mon, 25 Aug 2003, Thomas Petzoldt wrote:

> Barry Rowlingson wrote:
> 
> > Howabout this i just bashed up from a quick search:
> > 
> >  boundaryFill <- function(mat, x,y,fill,boundary)
> 
> [...]
> 
> >  note it fills 4-connected regions. I wouldnt like to do it on anything 
> > complex since it'll be awful slow....
> 
> Yes, this is in principle the same solution I use during aan external C 
> function. However, the problem is not only speed, but the large amount 
> of stack memory with larger matrices (say 100 x 100) due to recursion, 
> no matter if I use options(expression = large_value) or not.
> 

A very powerful tool is grdmask in GMT (http://gmt.soest.hawaii.edu/) 
followed by grd2xyz to create something easier to read (possibly as binary 
through connections). It is well-proven, and handles large grids 
comfortably.


Roger Bivand

Economic Geography Section, Department of Economics, Norwegian School of
Economics and Business Administration, Breiviksveien 40, N-5045 Bergen,
Norway. voice: +47 55 95 93 55; fax +47 55 95 93 93
e-mail: Roger.Bivand at nhh.no



From clists at perrin.socsci.unc.edu  Mon Aug 25 18:43:05 2003
From: clists at perrin.socsci.unc.edu (Andrew Perrin)
Date: Mon, 25 Aug 2003 12:43:05 -0400 (EDT)
Subject: [R] Book recommendations:  Multilevel & longitudinal analysis
In-Reply-To: <BB6FA647.5E63%peterm@andrew.cmu.edu>
References: <BB6FA647.5E63%peterm@andrew.cmu.edu>
Message-ID: <Pine.LNX.4.53.0308251242001.4310@perrin.socsci.unc.edu>

Three suggestions:

1.) Raudenbush and Bryk, _Hierarchical Linear Models: Second Edition_
(Sage, 2002)

2.) Pinheiro and Bates, _Mixed-Effects Models in S and S-Plus_ (Springer)

3.) Fox, _An R and S-Plus Companion to Applied Regression_, plus the
appendix available via the web on multilevel models.

ap

----------------------------------------------------------------------
Andrew J Perrin - http://www.unc.edu/~aperrin
Assistant Professor of Sociology, U of North Carolina, Chapel Hill
clists at perrin.socsci.unc.edu * andrew_perrin (at) unc.edu


On Mon, 25 Aug 2003, Peter Muhlberger wrote:

> Hi, does anyone out there have a recommendation for multilevel / random
> effects and longitudinal analysis?
>
> My dream book would be something that's both accessible to a
> non-statistician but rigorous (because I seem to be slowly turning into a
> statistician) and ideally would use R.
>
> Peter
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
>



From gfaieta at katamail.com  Mon Aug 25 20:02:24 2003
From: gfaieta at katamail.com (gfaieta@katamail.com)
Date: Mon, 25 Aug 2003 18:02:24 +0000
Subject: [R] ODBC access
Message-ID: <200.36.131.102+741hbT9A6fxVvElNU@all-1.inet.it>

Hello everybody,
I have tried to connect to external databases (specifically, to a MS Access database in my computer) from R using the RODBC package. Unfortunatelly I haven't been able to do it, even if I 'followed' the instructions in the manual. COuld someone please help me?

I have a MS Access database in my computer and I went to the control panel in order to create a DSN file linked to that database. When I'm in R I tried to coneect to it using the command 
odbcConnect("testdb")
but it doesn't work. I received these messages:
Warning messages: 
1: [RODBC] ERROR: state IM002, code 0, message [Microsoft][ODBC Driver Manager] Data source name not found and no default driver specified 
2: ODBC connection failed in: odbcDriverConnect(paste("DSN=", dsn, ";UID=", uid, ";PWD=", pwd,  

Thanks for your coperation.
Regards,
Giuseppe Faieta



From ripley at stats.ox.ac.uk  Mon Aug 25 20:16:10 2003
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Mon, 25 Aug 2003 19:16:10 +0100 (BST)
Subject: [R] ODBC access
In-Reply-To: <200.36.131.102+741hbT9A6fxVvElNU@all-1.inet.it>
Message-ID: <Pine.LNX.4.44.0308251910170.5921-100000@gannet.stats>

So follow the instructions more precisely.  You don't say which manual, 
but the R Data Import/Export Manual says

We use a database @code{testdb} we created earlier, and had the DSN (data
source name) set up in @file{~/.odbc.ini} under @code{unixODBC}.  Exactly
the same code worked using MyODBC to access a MySQL database under Linux
or Windows (where MySQL also maps names to lowercase).  Under Windows,
@acronym{DSN}s are set up in the @acronym{ODBC} applet in the Control
Panel (`Data Sources (ODBC)' in the `Administrative Tools' section on
2000/XP).

The message means that you didn't do set up the DSN.  Use your OS 
documentation to learn how to do that: it's not an R issue.

Hint: you may find it easier to use odbcDriverConnect under Windows
(you do seem to be using Windows, although you neglected to tell us that, 
also).


On Mon, 25 Aug 2003, gfaieta at katamail.com wrote:

> Hello everybody,

> I have tried to connect to external databases (specifically, to a MS
> Access database in my computer) from R using the RODBC package.
> Unfortunatelly I haven't been able to do it, even if I 'followed' the
> instructions in the manual. COuld someone please help me?
> 
> I have a MS Access database in my computer and I went to the control panel in order to create a DSN file linked to that database. When I'm in R I tried to coneect to it using the command 
> odbcConnect("testdb")
> but it doesn't work. I received these messages:
> Warning messages: 
> 1: [RODBC] ERROR: state IM002, code 0, message [Microsoft][ODBC Driver Manager] Data source name not found and no default driver specified 
> 2: ODBC connection failed in: odbcDriverConnect(paste("DSN=", dsn, ";UID=", uid, ";PWD=", pwd,  

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From f0z6305 at labs.tamu.edu  Mon Aug 25 20:25:23 2003
From: f0z6305 at labs.tamu.edu (Feng Zhang)
Date: Mon, 25 Aug 2003 13:25:23 -0500
Subject: [R] Any symmetric matrix can be decomposed with eigendecomposition?
Message-ID: <019401c36b36$3fd36350$8bd75ba5@IE.TAMU.EDU>

Hey, all

I have a question about the eigen-decomposition of a dxd symmatric matrix A.
Under what condition, that A can be decomposed such that A=U*D*U', where the
orthnormal matrix U is consisit of d eigenvectors and D is a diagonal matrix
with
eigenvalues in the diagonal positions.

Thanks for your help.

Fred



From szeger at jhsph.edu  Mon Aug 25 21:37:29 2003
From: szeger at jhsph.edu (Scott Zeger)
Date: Mon, 25 Aug 2003 15:37:29 -0400
Subject: [R] diamond graphs
Message-ID: <ADEPLELNMGDGKCPEDINLKEJGKBAD.szeger@jhsph.edu>


I read with interest comments about "diamond graphs" recently described in
the American Statistician by my colleagues in the Johns Hopkins Department
of Epidemiology led by Dr. Alvaro Munoz.

Permit three brief reactions.

First, "diamond graphs" were developed as part of the Multi-center Aids
Cohort Study, a seminal study of HIV infection in the U.S. in which these
authors have been key co-investigators. The graphs were created to better
address a real scientific objective and that usually bodes well for their
longer-term value.

Second, non-technical descriptions of statistical work written by public
affairs people, such as the Johns Hopkins web-page article commented on,
tend to be enthusiastic; such is the nature of public relations. I, for one,
am delighted to see statistical work noticed and discussed by
non-statisticians within my University and beyond.

Third, this University leaves it to individual faculty whether or not to
pursue a patent for a discovery. That Dr. Munoz and colleagues have decided
to do so does reflects their preference, not a University or Department
policy. In fact, the Johns Hopkins Department of Biostatistics faculty and
graduates are active participants in and enthusiastic supporters of open
source software development. For recent examples, see:
http://www.biostat.jhsph.edu/biostat/research/software.shtml

Scott L. Zeger
Department of Biostatistics
Johns Hopkins University



From fharrell at virginia.edu  Mon Aug 25 23:28:28 2003
From: fharrell at virginia.edu (Frank E Harrell Jr)
Date: Mon, 25 Aug 2003 17:28:28 -0400
Subject: [R] diamond graphs
In-Reply-To: <ADEPLELNMGDGKCPEDINLKEJGKBAD.szeger@jhsph.edu>
References: <ADEPLELNMGDGKCPEDINLKEJGKBAD.szeger@jhsph.edu>
Message-ID: <20030825172828.3d5b9872.fharrell@virginia.edu>

On Mon, 25 Aug 2003 15:37:29 -0400
Scott Zeger <szeger at jhsph.edu> wrote:

> 
> I read with interest comments about "diamond graphs" recently described in
> the American Statistician by my colleagues in the Johns Hopkins Department
> of Epidemiology led by Dr. Alvaro Munoz.
> 
> Permit three brief reactions.
> 
> First, "diamond graphs" were developed as part of the Multi-center Aids
> Cohort Study, a seminal study of HIV infection in the U.S. in which these
> authors have been key co-investigators. The graphs were created to better
> address a real scientific objective and that usually bodes well for their
> longer-term value.
> 
> Second, non-technical descriptions of statistical work written by public
> affairs people, such as the Johns Hopkins web-page article commented on,
> tend to be enthusiastic; such is the nature of public relations. I, for one,
> am delighted to see statistical work noticed and discussed by
> non-statisticians within my University and beyond.
> 
> Third, this University leaves it to individual faculty whether or not to
> pursue a patent for a discovery. That Dr. Munoz and colleagues have decided
> to do so does reflects their preference, not a University or Department
> policy. In fact, the Johns Hopkins Department of Biostatistics faculty and
> graduates are active participants in and enthusiastic supporters of open
> source software development. For recent examples, see:
> http://www.biostat.jhsph.edu/biostat/research/software.shtml
> 
> Scott L. Zeger
> Department of Biostatistics
> Johns Hopkins University

Scott,

I am really glad to hear that Johns Hopkins supports open source software development.  I do hope, however, that the authors will re-think their patent application.  Even though, as you said, it is not the University's idea, in my humble opinion such an application does not reflect well on the university.  The faculty in some senses are the University, and the University is associated with this application.

I just e-mailed the authors to make a personal plea for dropping the application.  In addition to setting a bad precedent, there are a few problems with their method.  There are even more problems with their article, including completely arbitrary categorization of continuous variables just so they may be used as classifiers in a diamond graph.  

The book "The Future of Ideas" by Lawrence Lessig, a noted patent and copyright lawyer who works on internet and software applications, is must reading.  There is a nice quote in that book from Thomas Jefferson about the harm patents cause to the interchange of ideas:

	"If nature has made any one thing less susceptible than all others of exclusive property, it is the action of the thinking power called an idea, which an individual may exclusively possess as long as he keeps it to himself; but the moment it is divulged, it forces itself into the possession of every one, and the receiver cannot dispossess himself of it. Its peculiar character, too, is that no one possesses the less, because every other possesses the whole of it. He who receives an idea from me, receives instruction himself without lessening mine; as he who lights his taper at mine, receives light without darkening me. That ideas should freely spread from one to another over the globe, for the moral and mutual instruction of man, and improvement of his condition, seems to have been peculiarly and benevolently designed by nature, when she made them, like fire, expansible over all space, without lessening their density in any point, and like the air in which we breathe, move, and have our physical being, incapable of confinement or exclusive appropriation. Inventions then cannot, in nature, be a subject of property."

Sincerely,

Frank Harrell

---
Frank E Harrell Jr              Prof. of Biostatistics & Statistics
Div. of Biostatistics & Epidem. Dept. of Health Evaluation Sciences
U. Virginia School of Medicine  http://hesweb1.med.virginia.edu/biostat



From p.connolly at hortresearch.co.nz  Mon Aug 25 23:59:03 2003
From: p.connolly at hortresearch.co.nz (Patrick Connolly)
Date: Tue, 26 Aug 2003 09:59:03 +1200
Subject: [R] Encapsulated postscript and the family argument
In-Reply-To: <Pine.LNX.4.44.0308250751160.4024-100000@gannet.stats>
References: <20030824221029.GP11971@hortresearch.co.nz>
	<Pine.LNX.4.44.0308250751160.4024-100000@gannet.stats>
Message-ID: <20030825215903.GR11971@hortresearch.co.nz>

On Mon, 25-Aug-2003 at 08:03AM +0100, Prof Brian Ripley wrote:

|> On Mon, 25 Aug 2003, Patrick Connolly wrote:
|> 
|> > > version
[...]

|> > However, what wasn't obvious to me was that it is necessary to specify
|> > what family to use.  If no family is specified, the default family
|> > does appear to be used, BUT, the resulting file is no different from a
|> > 'regular postscript' file.  The value in ps.options does not seem to
|> > be used in the same way.
|> 
|> The family used is nothing to do with EPS.  The code is always
|> EPS-conformant (but may not be a single page), but the *header* is only
|> sometimes, the times being documented.
|> 
|> > Is this intentional behaviour?
|> 
|> Is what, exactly?  

A.  What I thought was going on with family seemily being used
differently when paper was 'special'.

Now with some gentle prodding, I see that I was confusing two
different plots I was working on.  Some swapping back into memory
hadn't finished on Monday morning when I made my observation.  All
rather embarrassing.


|> A long-timer such as yourself really, really should know not to
|> send in vague statements not backed up by the code used to leap to
|> these conclusions!

Yesterday, I knew considerably less about the difference between EPS
and regular PostScript (and most of that was misconception), so I was
unaware how simple the distinction was.  Constrained by that
ignorance, I couldn't think of a way of showing more clearly what I
was on about.

Thanks Brian for your patience in helping me sort that out.

I have a small question about that difference:
Am I correct now in thinking that apart from the first line of a
single page graphic file (with current versions) reading

	%!PS-Adobe-3.0 EPSF-3.0

instead of

	%!PS-Adobe-3.0,

the only substantial differences between an EPS and a PS file are the
positioning of the origin of the bounding box at 0, 0 and the removal
of page orientation information?

Thanks

-- 
Patrick Connolly
HortResearch
Mt Albert
Auckland
New Zealand 
Ph: +64-9 815 4200 x 7188
~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~
I have the world`s largest collection of seashells. I keep it on all
the beaches of the world ... Perhaps you`ve seen it.  ---Steven Wright 
~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~



From maj at stats.waikato.ac.nz  Tue Aug 26 01:45:28 2003
From: maj at stats.waikato.ac.nz (Murray Jorgensen)
Date: Tue, 26 Aug 2003 11:45:28 +1200
Subject: [R] R tools for large files
In-Reply-To: <Pine.LNX.4.44.0308251058541.4710-100000@gannet.stats>
References: <Pine.LNX.4.44.0308251058541.4710-100000@gannet.stats>
Message-ID: <3F4A9F98.8040006@stats.waikato.ac.nz>

I would like to thank those who have responded and especially Brian 
Ripley for making his unix tools for Windows available. A colleague has 
also mentioned to me the set of unix tools called Cygwin.

Two things that can be done with R alone are to read the first n lines 
of a file into n strings with readLines() and to scan in a block of the 
file after skipping a number of lines.

I will probably use Fortran to extract subsets of the file as I need to 
use it for other things that I am planning to do with the file.

I'll maybe also play a bit with readLines() and writeLines() inside 
loops to see if I can build up my random subsets of files this way.

BTW, I now estimate the file at about 100,000 lines so indeed, it is not 
all that large!

Murray Jorgensen

Prof Brian Ripley wrote:

> On Mon, 25 Aug 2003, Murray Jorgensen wrote:
> 
> 
>>At 08:12 25/08/2003 +0100, Prof Brian Ripley wrote:
>>
>>>I think that is only a medium-sized file.
>>
>>"Large" for my purposes means "more than I really want to read into memory"
>>which in turn means "takes more than 30s". I'm at home now and the file
>>isn't so I'm not sure if the file is large or not.
>>
>>More responses interspesed below. BTW, I forgot to mention that I'm using
>>Windows and so do not have nice unix tools readily available.
> 
> 
> But you do, thanks to me, as you need them to installed R packages.
> 
> 
>>>On Mon, 25 Aug 2003, Murray Jorgensen wrote:
>>>
>>>
>>>>I'm wondering if anyone has written some functions or code for handling 
>>>>very large files in R. I am working with a data file that is 41 
>>>>variables times who knows how many observations making up 27MB altogether.
>>>>
>>>>The sort of thing that I am thinking of having R do is
>>>>
>>>>- count the number of lines in a file
>>>
>>>You can do that without reading the file into memory: use
>>>system(paste("wc -l", filename)) 
>>
>>Don't think that I can do that in Windows XL.
> 
> 
> I presume you mean Windows XP?  Of course you can, and wc.exe is in 
> Rtools.zip!
> 
> 
>>or read in blocks of lines via a 
>>
>>>connection
>>
>>But that does sound promising!
>>
>>
>>>>- form a data frame by selecting all cases whose line numbers are in a 
>>>>supplied vector (which could be used to extract random subfiles of 
>>>>particular sizes)
>>>
>>>R should handle that easily in today's memory sizes.  Buy some more RAM if 
>>>you don't already have 1/2Gb.  As others have said, for a real large file,
>>>use a RDBMS to do the selection for you.
>>
>>It's just that R is so good in reading in initial segments of a file that I
>>can't believe that it can't be effective in reading more general
>>(pre-specified) subsets.
> 
> 

-- 
Dr Murray Jorgensen      http://www.stats.waikato.ac.nz/Staff/maj.html
Department of Statistics, University of Waikato, Hamilton, New Zealand
Email: maj at waikato.ac.nz                                Fax 7 838 4155
Phone  +64 7 838 4773 wk    +64 7 849 6486 home    Mobile 021 1395 862



From ok at cs.otago.ac.nz  Tue Aug 26 01:46:04 2003
From: ok at cs.otago.ac.nz (Richard A. O'Keefe)
Date: Tue, 26 Aug 2003 11:46:04 +1200 (NZST)
Subject: [R] R tools for large files
Message-ID: <200308252346.h7PNk48v213114@atlas.otago.ac.nz>

Murray Jorgensen <maj at stats.waikato.ac.nz> wrote:
	"Large" for my purposes means "more than I really want to read
	into memory" which in turn means "takes more than 30s".  I'm at
	home now and the file isn't so I'm not sure if the file is large
	or not.
	
I repeat my earlier observation.  The AMOUNT OF DATA is easily handled
a typical desktop machine these days.  The problem is not the amount of
data.  The problem is HOW LONG IT TAKES TO READ.  I made several attempts
to read the test file I created yesterday, and each time gave up
impatiently after 5+ minutes elapsed time.  I tried again today (see below)
and went away to have a cop of tea &c; took nearly 10 minute that time and
still hadn't finished.  'mawk' read _and processed_ the same file
happily in under 30 seconds.

One quite serious alternative would be to write a little C function
to read the file into an array, and call that from R.

> system.time(m <- matrix(1:(41*250000), nrow=250000, ncol=41))
[1] 3.28 0.79 4.28 0.00 0.00
> system.time(save(m, file="m.bin"))
[1] 8.44 0.54 9.08 0.00 0.00
> m <- NULL
> system.time(load("m.bin"))
[1] 11.25  0.19 11.51  0.00  0.00
> length(m)
[1] 10250000

The binary file m.bin is 41 million bytes.

This little transcript shows that a data set of this size can be
comfortably read from disc in under 12 seconds, on the same machine
where scan() took about 50 times as long before I killed it.

So yet another alternative is to write a little program that converts
the data file to R binary format, and then just read the whole thing in.
I think readers will agree that 12 seconds on a 500MHz machine counts
as "takes less than 30s".

	It's just that R is so good in reading in initial segments of a file that I
	can't believe that it can't be effective in reading more general
	(pre-specified) subsets.
	
R is *good* at it, it's just not *quick*.  Trying to select a subset
in scan() or read.table() wouldn't help all that much, because it would
still have to *scan* the data to determine what to skip.

Two more times:
An unoptimised C program writing 0:(41*250000-1) as a file of
41-number lines:
f% time a.out >m.txt
13.0u 1.0s 0:14 94% 0+0k 0+0io 0pf+0w
> system.time(m <- read.table("m.txt", header=FALSE))
^C
Timing stopped at: 552.01 15.48 584.51 0 0 

To my eyes, src/main/scan.c shows no signs of having been tuned for speed.
The goals appear to have been power (the R scan() function has LOTS of
options) and correctness, which are perfectly good goals, and the speed
of scan() and read.table() with modest data sizes is quite good enough.

The huge ratio (>552)/(<30) for R/mawk does suggest that there may be
room for some serious improvement in scan(), possibly by means of some
extra hints about total size, possibly by creating a fast path through
the code.

Of course the big point is that however long scan() takes to read the
data set, it only has to be done once.  Leave R running overnight and
in the morning save the dataset out as an R binary file using save().
Then you'll be able to load it again quickly.



From Toby.Patterson at csiro.au  Tue Aug 26 01:56:34 2003
From: Toby.Patterson at csiro.au (Toby.Patterson@csiro.au)
Date: Tue, 26 Aug 2003 09:56:34 +1000
Subject: [R] ODBC Oracle access
Message-ID: <C4178DC99E08604EA5E2BDB989F09380241FA1@extas2-hba.tas.csiro.au>

Hi all, 
I'm having trouble connecting to an oracle database using RODBC under
winXP. Unfortunately I can't really send a reproducable error as the
initial call to odbcConnect seems to hangs R and I have to kill the
session. 

I have been using RODBC to sucessfully connect to an MS Access DB that
has tables linked through to the oracle database in question and that
seems to work OK but it would be preferable to cut out the MS access
middle-man. 

The command I'm attempting to use is: 

conn<- odbcConnect(dsn='myDSN', uid = "myUID", pwd = "myPWD", case =
"oracle",believeNRows=FALSE)

I wouldn't be surprised if there is an issue with the dsn configuration
and not R (though I cant find it) so any tips as to things that I should
check there would also be gratefully received. 

Version
_              
platform i386-pc-mingw32
arch     i386           
os       mingw32        
system   i386, mingw32  
status                  
major    1              
minor    7.1            
year     2003           
month    06             
day      16             
language R   

Regards, 
 
Toby. 

Toby Patterson
Pelagic Fisheries and Ecosystems
CSIRO Marine Research



From bwmoore22 at yahoo.com  Tue Aug 26 03:59:56 2003
From: bwmoore22 at yahoo.com (Bruce Moore)
Date: Mon, 25 Aug 2003 18:59:56 -0700 (PDT)
Subject: Fwd: Re: [R] Problem running RTERM via SSH on Windows/2000
Message-ID: <20030826015956.49187.qmail@web13706.mail.yahoo.com>

rterm --ess --save 

works fine under both openssh and under putty SSH
clients.  Thanks!


--- Prof Brian Ripley <ripley at stats.ox.ac.uk> wrote:
> Date: Fri, 22 Aug 2003 19:41:35 +0100 (BST)
> From: Prof Brian Ripley <ripley at stats.ox.ac.uk>
> To: Bruce Moore <bwmoore22 at yahoo.com>
> CC: r-help at stat.math.ethz.ch
> Subject: Re: [R] Problem running RTERM via SSH on
> Windows/2000
> 
> This is the documented behaviour for R when used
> non-interactively.
> Presumably your `SSH' (probably really openssh)
> isn't using terminals for 
> input and output.
> 
> You might like to try rterm --ess, a kludge for a
> similar problem in 
> NTemacs.  Or try a different ssh (a real Windows
> one, not one designed for 
> systems with ptys).
> 
> On Fri, 22 Aug 2003, Bruce Moore wrote:
> 
> > I'm having problems getting RTERM to work via SSH.
> 
> > Whenever it has any type of problem, it abends
> instead
> > of issuing an error message and returning to the >
> > prompt.  Both "server" and client are Windows/2000
> > Professional at FP4.  SSH is via Cygwin on both
> sides.
> >  R is version is 1071.
> > 
> > RTERM runs fine when run in a BASH shell on the
> > "server," though it does not prompt for --save
> > --nosave or --vanilla.
> 
> It never does: no version of R prompts for those to
> my knowledge.
> 
> -- 
> Brian D. Ripley,                 
> ripley at stats.ox.ac.uk
> Professor of Applied Statistics, 
> http://www.stats.ox.ac.uk/~ripley/
> University of Oxford,             Tel:  +44 1865
> 272861 (self)
> 1 South Parks Road,                     +44 1865
> 272866 (PA)
> Oxford OX1 3TG, UK                Fax:  +44 1865
> 272595
> 


=====
Bruce Moore



From ok at cs.otago.ac.nz  Tue Aug 26 04:09:45 2003
From: ok at cs.otago.ac.nz (Richard A. O'Keefe)
Date: Tue, 26 Aug 2003 14:09:45 +1200 (NZST)
Subject: [R] diamond graphs
Message-ID: <200308260209.h7Q29jL2209955@atlas.otago.ac.nz>

Scott Zeger <szeger at jhsph.edu> commented:
	First, "diamond graphs" were developed as part of the Multi-center Aids
	Cohort Study, a seminal study of HIV infection in the U.S. in which these
	authors have been key co-investigators. The graphs were created to better
	address a real scientific objective and that usually bodes well for their
	longer-term value.
	
I've invented a couple of graphic techniques myself.  They were devised
to deal with problems of actual practical interest at the time.  "That
usually bodes well for their longer-term value"?  No, I am these days
glad that I never published them, because R is chock full of *better*
methods than mine.

As yet I have not had a chance to see the actual article.  (Living in
the Southern Hemisphere has advantages, but also disadvantages, like
the time it takes periodicals to arrive.)  The one example of a diamond
graph I've seen did make a certain pattern in the data easy to spot, but
it made it harder to spot than other graphs would have.  Amongst other
things, it would be very interesting to see some sort of 2d density
plot with log(diastolic) and log(systolic) as axes.  Perhaps this was
already done in the article.

First Lispstat and now R have impressed on my mind the importance of
moving beyond paper.  The possibility of displaying the same data in
_several_ ways, simultaneously or in quick succession, means that
computer graphics can be a qualitatively different medium from paper.

Just this afternoon I was talking with a 4th-year CS student who is
working on a project to try to find features which will enable him to
find patterns in a certain kind of data.  Using R, I generated some
synthetic data in a couple of lines of code.  Then I plotted it several
different ways, scratched my head a bit, rummaged through a list of
smoothing functions found using help.search, and tried something, plotted
it, changed a scale factor, tried again, settled on a scale factor that
seemed to work well, switched back to thinking about calculations, and
in about 15 minutes, there was a technique for finding interesting change
points in the data.  I confused him a bit because I was switching plots
faster than he could follow, so I spent the next 45 minutes explaining
what I'd done.  The point was that *changing* plots was qualitatively
different from looking at a single plot.

Now, the data displayed in the one example in the press release seemed
to be (diastolic pressure bucket) x (systolic pressure bucket) -> count.
As noted above, that suggests a 2d density estimate as an interesting
thing.  It also suggests a scatter plot (possibly with rugs).  Most
importantly, it suggests BOTH of them, and several others as well (such
as hexbin), each of which may provide some insight that the others don't.

It's very VERY hard for any one graph, especially one with a cramped
dynamic range, to beat that.  The real competition for the diamond graph
is not some other graph, but a wide choice of graphs that can be quickly
flicked through and creatively combined.

This also means that a new graphic technique, if it _is_ good, is even
_better_ when it can be freely creatively combined with other graphic
techniques.  Having diamond graphs locked out of R is bad *for* diamond
graphs.

	In fact, the Johns Hopkins Department of Biostatistics faculty and
	graduates are active participants in and enthusiastic supporters of open
	source software development. For recent examples, see:
	http://www.biostat.jhsph.edu/biostat/research/software.shtml
	
Not only that, at least one of them, R/qtl, is an R package.



From andy_liaw at merck.com  Tue Aug 26 04:15:46 2003
From: andy_liaw at merck.com (Liaw, Andy)
Date: Mon, 25 Aug 2003 22:15:46 -0400
Subject: [R] R tools for large files
Message-ID: <3A822319EB35174CA3714066D590DCD50205CA2A@usrymx25.merck.com>

> From: Richard A. O'Keefe [mailto:ok at cs.otago.ac.nz] 
> 
> Murray Jorgensen <maj at stats.waikato.ac.nz> wrote:
> 	"Large" for my purposes means "more than I really want to read
> 	into memory" which in turn means "takes more than 30s".  I'm at
> 	home now and the file isn't so I'm not sure if the file is large
> 	or not.
> 	
> I repeat my earlier observation.  The AMOUNT OF DATA is 
> easily handled a typical desktop machine these days.  The 
> problem is not the amount of data.  The problem is HOW LONG 
> IT TAKES TO READ.  I made several attempts to read the test 
> file I created yesterday, and each time gave up impatiently 
> after 5+ minutes elapsed time.  I tried again today (see 
> below) and went away to have a cop of tea &c; took nearly 10 
> minute that time and still hadn't finished.  'mawk' read _and 
> processed_ the same file happily in under 30 seconds.
> 
> One quite serious alternative would be to write a little C 
> function to read the file into an array, and call that from R.
> 
> > system.time(m <- matrix(1:(41*250000), nrow=250000, ncol=41))
> [1] 3.28 0.79 4.28 0.00 0.00
> > system.time(save(m, file="m.bin"))
> [1] 8.44 0.54 9.08 0.00 0.00
> > m <- NULL
> > system.time(load("m.bin"))
> [1] 11.25  0.19 11.51  0.00  0.00
> > length(m)
> [1] 10250000

I tried the following on my IBM T22 Thinkpad (P3-933 w/ 512MB):

> system.time(x <- matrix(runif(41*250000), 250000, 41))
[1] 6.02 0.40 6.52   NA   NA
> object.size(x)
[1] 82000120
> system.time(write(t(x), file="try.dat", ncol=41))
[1] 192.12  81.60 279.64     NA     NA
> system.time(xx <- matrix(scan("try.dat"), byrow=TRUE, ncol=41))
Read 10250000 items
[1] 110.90   1.09 126.89     NA     NA
> system.time(xx <- read.table("try.dat", header=FALSE,
+ colClasses=rep("numeric", 41)))
[1] 106.61   0.48 110.66     NA     NA
> system.time(save(x, file="try.rda"))
[1]  9.15  1.05 19.12    NA    NA
> rm(x)
> system.time(load("try.rda"))
[1] 10.22  0.33 10.69    NA    NA

The last few lines show that the timing I get is approximately the
same as yours, so the other timings shouldn't be too different.

I don't think I can make coffee that fast.  (No, I don't drink it black!)

Andy


> 
> The binary file m.bin is 41 million bytes.
> 
> This little transcript shows that a data set of this size can 
> be comfortably read from disc in under 12 seconds, on the 
> same machine where scan() took about 50 times as long before 
> I killed it.
> 
> So yet another alternative is to write a little program that 
> converts the data file to R binary format, and then just read 
> the whole thing in. I think readers will agree that 12 
> seconds on a 500MHz machine counts as "takes less than 30s".
> 
> 	It's just that R is so good in reading in initial 
> segments of a file that I
> 	can't believe that it can't be effective in reading more general
> 	(pre-specified) subsets.
> 	
> R is *good* at it, it's just not *quick*.  Trying to select a 
> subset in scan() or read.table() wouldn't help all that much, 
> because it would still have to *scan* the data to determine 
> what to skip.
> 
> Two more times:
> An unoptimised C program writing 0:(41*250000-1) as a file of 
> 41-number lines: f% time a.out >m.txt 13.0u 1.0s 0:14 94% 
> 0+0k 0+0io 0pf+0w
> > system.time(m <- read.table("m.txt", header=FALSE))
> ^C
> Timing stopped at: 552.01 15.48 584.51 0 0 
> 
> To my eyes, src/main/scan.c shows no signs of having been 
> tuned for speed. The goals appear to have been power (the R 
> scan() function has LOTS of
> options) and correctness, which are perfectly good goals, and 
> the speed of scan() and read.table() with modest data sizes 
> is quite good enough.
> 
> The huge ratio (>552)/(<30) for R/mawk does suggest that 
> there may be room for some serious improvement in scan(), 
> possibly by means of some extra hints about total size, 
> possibly by creating a fast path through the code.
> 
> Of course the big point is that however long scan() takes to 
> read the data set, it only has to be done once.  Leave R 
> running overnight and in the morning save the dataset out as 
> an R binary file using save(). Then you'll be able to load it 
> again quickly.
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list 
> https://www.stat.math.ethz.ch/mailman/listinfo> /r-help
> 

------------------------------------------------------------------------------
Notice:  This e-mail message, together with any attachments, contains
information of Merck & Co., Inc. (Whitehouse Station, New Jersey, USA), and/or
its affiliates (which may be known outside the United States as Merck Frosst,
Merck Sharp & Dohme or MSD) that may be confidential, proprietary copyrighted
and/or legally privileged, and is intended solely for the use of the
individual or entity named on this message.  If you are not the intended
recipient, and have received this message in error, please immediately return
this by e-mail and then delete it.



From Tom.Mulholland at health.wa.gov.au  Tue Aug 26 05:55:48 2003
From: Tom.Mulholland at health.wa.gov.au (Mulholland, Tom)
Date: Tue, 26 Aug 2003 11:55:48 +0800
Subject: [R] R tools for large files
Message-ID: <74E242B6968AA0469B632C5A3EFC1EFD03D56FD1@nt207mesep.health.wa.gov.au>

As some of the conversation has noted the 30 second mark as an arbitrary
benchmark I would also chime in that there is also an assumption that
any non-R related issues that impact upon being able to usefully use R
should be ignored. In the real world we can't always control everything
about our environment. So if there are improvements that can be made
that help mitigate the reality of the world, I would welcome them.

As a little test I broke the rules of my organisation and actually put a
dataset on my C: drive. Not unexpectedly, the  performance vastly
improved. What would in the normal (at home) be a 10 second load becomes
a 40 second load in a corporate environment. I have found the
conversation helpful and it would appear that there are opportunities
for improvement that I would find helpful in my production environment.
The other aside is that I have no UNIX like tools, not because they
don't exist, but because the environment I work in does not allow me to
use them. This is not sufficient reason for me to bleat about it. It
just is. By and large, I just get on with it. My point is that while I
accept that these issues are peripheral to R, they do impact upon the
useability of R.

I'm sure that there are people working with large databases in R (The
SPSS datasets that I regularly interact with vary between 97MB and
200MB) It could be finger trouble on my part, but I find I have to
subset them before I can read them into R. If I thought I could usefully
convert these datasets into something that R could pick and choose from
without reaching the out of memory problem, I would be very happy. In
the meantime my lack of expertise has left me with a workable albeit
clumsy process.

I will continue to champion R in my organisation, but the present score
is SPSS-50, SAS-149, R-1. But all the really creative charts only come
from one engine in this place.

> system.time(load("P:/.../0203Mapdata.rdata"))
[1]  9.79  0.97 37.45    NA    NA
> system.time(load("C:/TEMP/0203Mapdata.rdata"))
[1] 10.07  0.18 10.49    NA    NA
> version
         _              
platform i386-pc-mingw32
arch     i386           
os       mingw32        
system   i386, mingw32  
status                  
major    1              
minor    7.1            
year     2003           
month    06             
day      16             
language R     

_________________________________________________
 
Tom Mulholland
Senior Policy Officer
WA Country Health Service
Tel: (08) 9222 4062
 
The contents of this e-mail transmission are confidential and may be
protected by professional privilege. The contents are intended only for
the named recipients of this e-mail. If you are not the intended
recipient, you are hereby notified that any use, reproduction,
disclosure or distribution of the information contained in this e-mail
is prohibited. Please notify the sender immediately.


-----Original Message-----
From: Murray Jorgensen [mailto:maj at stats.waikato.ac.nz] 
Sent: Monday, 25 August 2003 5:16 PM
To: Prof Brian Ripley
Cc: R-help
Subject: Re: [R] R tools for large files


At 08:12 25/08/2003 +0100, Prof Brian Ripley wrote:
>I think that is only a medium-sized file.

"Large" for my purposes means "more than I really want to read into
memory" which in turn means "takes more than 30s". I'm at home now and
the file isn't so I'm not sure if the file is large or not.

More responses interspesed below. BTW, I forgot to mention that I'm
using Windows and so do not have nice unix tools readily available.

>On Mon, 25 Aug 2003, Murray Jorgensen wrote:
>
>> I'm wondering if anyone has written some functions or code for 
>> handling
>> very large files in R. I am working with a data file that is 41 
>> variables times who knows how many observations making up 27MB
altogether.
>> 
>> The sort of thing that I am thinking of having R do is
>> 
>> - count the number of lines in a file
>
>You can do that without reading the file into memory: use 
>system(paste("wc -l", filename))

Don't think that I can do that in Windows XL.

or read in blocks of lines via a 
>connection

But that does sound promising!

>
>> - form a data frame by selecting all cases whose line numbers are in 
>> a
>> supplied vector (which could be used to extract random subfiles of 
>> particular sizes)
>
>R should handle that easily in today's memory sizes.  Buy some more RAM

>if
>you don't already have 1/2Gb.  As others have said, for a real large
file,
>use a RDBMS to do the selection for you.

It's just that R is so good in reading in initial segments of a file
that I can't believe that it can't be effective in reading more general
(pre-specified) subsets.

Murray

>
>-- 
>Brian D. Ripley,                  ripley at stats.ox.ac.uk
>Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
>University of Oxford,             Tel:  +44 1865 272861 (self)
>1 South Parks Road,                     +44 1865 272866 (PA)
>Oxford OX1 3TG, UK                Fax:  +44 1865 272595
> 
Dr Murray Jorgensen      http://www.stats.waikato.ac.nz/Staff/maj.html
Department of Statistics, University of Waikato, Hamilton, New Zealand
Email: maj at waikato.ac.nz                                Fax 7 838 4155
Phone  +64 7 838 4773 wk    +64 7 849 6486 home    Mobile 021 1395 862

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://www.stat.math.ethz.ch/mailman/listinfo/r-help



From ripley at stats.ox.ac.uk  Tue Aug 26 08:04:38 2003
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Tue, 26 Aug 2003 07:04:38 +0100 (BST)
Subject: [R] Encapsulated postscript and the family argument
In-Reply-To: <20030825215903.GR11971@hortresearch.co.nz>
Message-ID: <Pine.LNX.4.44.0308260701480.6818-100000@gannet.stats>

On Tue, 26 Aug 2003, Patrick Connolly wrote:

> On Mon, 25-Aug-2003 at 08:03AM +0100, Prof Brian Ripley wrote:
> 
> |> On Mon, 25 Aug 2003, Patrick Connolly wrote:
> |> 
> |> > > version
> [...]
> 
> |> > However, what wasn't obvious to me was that it is necessary to specify
> |> > what family to use.  If no family is specified, the default family
> |> > does appear to be used, BUT, the resulting file is no different from a
> |> > 'regular postscript' file.  The value in ps.options does not seem to
> |> > be used in the same way.
> |> 
> |> The family used is nothing to do with EPS.  The code is always
> |> EPS-conformant (but may not be a single page), but the *header* is only
> |> sometimes, the times being documented.
> |> 
> |> > Is this intentional behaviour?
> |> 
> |> Is what, exactly?  
> 
> A.  What I thought was going on with family seemily being used
> differently when paper was 'special'.
> 
> Now with some gentle prodding, I see that I was confusing two
> different plots I was working on.  Some swapping back into memory
> hadn't finished on Monday morning when I made my observation.  All
> rather embarrassing.
> 
> 
> |> A long-timer such as yourself really, really should know not to
> |> send in vague statements not backed up by the code used to leap to
> |> these conclusions!
> 
> Yesterday, I knew considerably less about the difference between EPS
> and regular PostScript (and most of that was misconception), so I was
> unaware how simple the distinction was.  Constrained by that
> ignorance, I couldn't think of a way of showing more clearly what I
> was on about.
> 
> Thanks Brian for your patience in helping me sort that out.
> 
> I have a small question about that difference:
> Am I correct now in thinking that apart from the first line of a
> single page graphic file (with current versions) reading
> 
> 	%!PS-Adobe-3.0 EPSF-3.0
> 
> instead of
> 
> 	%!PS-Adobe-3.0,
> 
> the only substantial differences between an EPS and a PS file are the
> positioning of the origin of the bounding box at 0, 0 and the removal
> of page orientation information?

Not the bounding box: EPS files can have a non-zero origin (although it is 
not very useful).  The header and the lack of the orientation comment are
the key: the latter is somewhat ambiguously defined, and version 6.0 
ghostscript started rotating figures to have height > width if it were 
included.

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From otoomet at econ.dk  Tue Aug 26 08:47:28 2003
From: otoomet at econ.dk (Ott Toomet)
Date: Tue, 26 Aug 2003 08:47:28 +0200
Subject: [R] plot empirical pdf
Message-ID: <200308260647.h7Q6lSD01267@punik>

Hi,

are there any function to plot the empirical probability distribution
function?  I just don't want to reinvent the wheel...

Best wishes,

Ott

-- 
Ott Toomet
PhD Student

Dept. of Economics
?rhus University
Building 322
Universitetsparken
8000 ?rhus C
Denmark

otoomet (a) econ au dk
ph: (+45) 89 42 20 27
-------------------------------------------

 (o_         (*_         (O_         (o< -!  
//\         //\         //\         //\      
V_/_        V_/_        V_/_        V_/_     
					     
standard    drunken     shocked     noisy    
penguin     penguin     penguin     penguin



From a0203664 at unet.univie.ac.at  Tue Aug 26 08:51:06 2003
From: a0203664 at unet.univie.ac.at (a0203664@unet.univie.ac.at)
Date: Tue, 26 Aug 2003 08:51:06 +0200
Subject: [R] Subsetting a dataframe
Message-ID: <200308260651.h7Q6p6rY121632@imap.unet.univie.ac.at>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20030826/b1568344/attachment.pl

From a0203664 at unet.univie.ac.at  Tue Aug 26 08:51:29 2003
From: a0203664 at unet.univie.ac.at (a0203664@unet.univie.ac.at)
Date: Tue, 26 Aug 2003 08:51:29 +0200
Subject: [R] Subsetting a dataframe
Message-ID: <200308260651.h7Q6pTrY103276@imap.unet.univie.ac.at>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20030826/3dd33f8b/attachment.pl

From azzalini at stat.unipd.it  Tue Aug 26 08:59:10 2003
From: azzalini at stat.unipd.it (Adelchi Azzalini)
Date: Tue, 26 Aug 2003 08:59:10 +0200
Subject: [R] plot empirical pdf
In-Reply-To: <200308260647.h7Q6lSD01267@punik>
References: <200308260647.h7Q6lSD01267@punik>
Message-ID: <20030826065910.ED2827CA833@tango.stat.unipd.it>

On Tuesday 26 August 2003 08:47, Ott Toomet wrote:
> Hi,
>
> are there any function to plot the empirical probability distribution
> function?  I just don't want to reinvent the wheel...
>

try this, 

regards, Adelchi 
--
n <- 10
x <- rnorm(n)
plot(c(min(x)-1,sort(x), max(x+1)), c(0:n,n)/n, type="s",
      xlab="data", ylab="ecdf")
rug(x)

-- 
Adelchi Azzalini  <azzalini at stat.unipd.it>
Dipart.Scienze Statistiche, Universit? di Padova, Italia
http://azzalini.stat.unipd.it/
(please, no ms-word/ms-excel/alike attachments)



From ligges at statistik.uni-dortmund.de  Tue Aug 26 09:01:46 2003
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Tue, 26 Aug 2003 09:01:46 +0200
Subject: [R] plot empirical pdf
In-Reply-To: <200308260647.h7Q6lSD01267@punik>
References: <200308260647.h7Q6lSD01267@punik>
Message-ID: <3F4B05DA.8010002@statistik.uni-dortmund.de>

Ott Toomet wrote:
> Hi,
> 
> are there any function to plot the empirical probability distribution
> function?  I just don't want to reinvent the wheel...
> 
> Best wishes,
> 
> Ott
> 

See package "stepfun".

Uwe Ligges



From ligges at statistik.uni-dortmund.de  Tue Aug 26 09:03:15 2003
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Tue, 26 Aug 2003 09:03:15 +0200
Subject: [R] Subsetting a dataframe
In-Reply-To: <200308260651.h7Q6pTrY103276@imap.unet.univie.ac.at>
References: <200308260651.h7Q6pTrY103276@imap.unet.univie.ac.at>
Message-ID: <3F4B0633.90704@statistik.uni-dortmund.de>

a0203664 at unet.univie.ac.at wrote:

> Hi R-people
> 
> I have a question concerning subsetting. I have a dataframe which contains among other variables the variable "subject". For each subject number there are several rows, e.g.:
> subject  treatment concentration day
> 19          a          15,4       1
> 19          a          18,3       2
> 19          a           2,3       3
> 
> etc.
> 
> Im trying to subset the dataframe to get for example only the rows of subject 19 and subject 15. I tried this with
> 
>         nameofdataframe[nameofdataframe$subject==c(19,15),]
> 
> This doesnt work as i get less rows than i should and a warning as well. How can i succeed doing this?
> 
> Thanks a lot
> 
> Gabi
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help


See ?"%in%"

as in

  nameofdataframe$subject %in% c(19,15)

Uwe Ligges



From maechler at stat.math.ethz.ch  Tue Aug 26 09:30:19 2003
From: maechler at stat.math.ethz.ch (Martin Maechler)
Date: Tue, 26 Aug 2003 09:30:19 +0200
Subject: [R] plot empirical pdf
In-Reply-To: <200308260647.h7Q6lSD01267@punik>
References: <200308260647.h7Q6lSD01267@punik>
Message-ID: <16203.3211.264736.472005@gargle.gargle.HOWL>

>>>>> "Ott" == Ott Toomet <otoomet at econ.dk>
>>>>>     on Tue, 26 Aug 2003 08:47:28 +0200 writes:

    Ott> Hi, are there any function to plot the empirical
    Ott> probability distribution function?  I just don't want
    Ott> to reinvent the wheel...

As Uwe has already said, the standard "stepfun" package does this
-- and more.  The idea of that mini-package was to introduce a
class "stepfun" for (1D) step functions, and a class "ecdf" which extends
"stepfun" -- at the same time demonstrating how R functions can
be used to return other R functions (with an own environment
where all information is stored).  For many, it can be quite
instructive to study  example(stepfun)  and  example(ecdf)
and the R source behind it.

Martin



From M.Mamin at intershop.de  Tue Aug 26 10:02:08 2003
From: M.Mamin at intershop.de (Marc Mamin)
Date: Tue, 26 Aug 2003 10:02:08 +0200
Subject: [R] ODBC Oracle access
Message-ID: <770E451830D96B4D84747B54665DA1B202DAF0AE@jena03.net.j.ad.intershop.net>

Hallo Toby,

Did you check the ODBC connection independently from R?
there is an Oracle tool for this (under Menu Network Administration).

I'm doing fine with a simpler call to odbcConnect:

channel <-odbcConnect("MyDSN",uid="myuid",pwd="mypwd")
further parameters may not be necessary if you pay attention to only use
uppercase names, and I guess that "believeNRows" is superfluous with the
latest Oracle ODBC Client(s) if it is well configured.

HTH,

Marc Mamin


-----Original Message-----
From: Toby.Patterson at csiro.au [mailto:Toby.Patterson at csiro.au]
Sent: Tuesday, August 26, 2003 1:57 AM
To: r-help at stat.math.ethz.ch
Subject: [R] ODBC Oracle access


Hi all, 
I'm having trouble connecting to an oracle database using RODBC under
winXP. Unfortunately I can't really send a reproducable error as the
initial call to odbcConnect seems to hangs R and I have to kill the
session. 

I have been using RODBC to sucessfully connect to an MS Access DB that
has tables linked through to the oracle database in question and that
seems to work OK but it would be preferable to cut out the MS access
middle-man. 

The command I'm attempting to use is: 

conn<- odbcConnect(dsn='myDSN', uid = "myUID", pwd = "myPWD", case =
"oracle",believeNRows=FALSE)

I wouldn't be surprised if there is an issue with the dsn configuration
and not R (though I cant find it) so any tips as to things that I should
check there would also be gratefully received. 

Version
_              
platform i386-pc-mingw32
arch     i386           
os       mingw32        
system   i386, mingw32  
status                  
major    1              
minor    7.1            
year     2003           
month    06             
day      16             
language R   

Regards, 
 
Toby. 

Toby Patterson
Pelagic Fisheries and Ecosystems
CSIRO Marine Research

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://www.stat.math.ethz.ch/mailman/listinfo/r-help



From Ted.Harding at nessie.mcc.ac.uk  Tue Aug 26 09:59:48 2003
From: Ted.Harding at nessie.mcc.ac.uk ( (Ted Harding))
Date: Tue, 26 Aug 2003 08:59:48 +0100 (BST)
Subject: [R] R tools for large files
In-Reply-To: <3F4A9F98.8040006@stats.waikato.ac.nz>
Message-ID: <XFMail.030826085948.Ted.Harding@nessie.mcc.ac.uk>

This has been an interesting thread! My first reaction to Murray's
query was to think "use standard Unix tools, especially awk", 'awk'
being a compact, fast, efficient program with great powers for
processing lines of data files (and in particular extracting,
subsetting and transforming database-like files e.g. CSV-type).

Of course, that became a sub-thread in its own right.

But -- and here I know I'm missing a trick which is why I'm responding
now so that someone who knows the trick can tell me -- while I normally
use 'awk' "externally" (i.e. I filter a data file through an 'awk'
program outside of R and then read the resulting file into R), I began
to think about doing it from within R.

Something on the lines of

  X <- system("cat raw_data | awk '...' ", intern=TRUE)

would create an object X which is a character vector, each element of
which is one line from the output of the command "cat ...... ".

E.g. if "raw_data" starts out as

  1,2,3,4,5
  1,3,4,2,5
  5,4,3,2,1
  5,3,4,1,2

then

  X<-system("cat raw_data.csv |
  awk 'BEGIN{FS=\",\"}{if($3>$2){print $1 \",\" $4 \",\" $5}}'",
  intern=TRUE)

gives

  > X
  [1] "1,4,5" "1,2,5" "5,1,2"

Now my Question:
How do I convert X into the dataframe I would have got if I had read
this output from a file instead of into the character vector X?

In other words, how to convert a vector of character strings, each
of which is in comma-separated format as above, into the rows of
a data-frame (or matrix, come to that)?

With thanks,
Ted.


--------------------------------------------------------------------
E-Mail: (Ted Harding) <Ted.Harding at nessie.mcc.ac.uk>
Fax-to-email: +44 (0)870 167 1972
Date: 26-Aug-03                                       Time: 08:59:48
------------------------------ XFMail ------------------------------



From ripley at stats.ox.ac.uk  Tue Aug 26 10:42:26 2003
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Tue, 26 Aug 2003 09:42:26 +0100 (BST)
Subject: [R] R tools for large files
In-Reply-To: <XFMail.030826085948.Ted.Harding@nessie.mcc.ac.uk>
Message-ID: <Pine.LNX.4.44.0308260939330.10828-100000@gannet.stats>

On Tue, 26 Aug 2003 Ted.Harding at nessie.mcc.ac.uk wrote:

[...]

>   > X
>   [1] "1,4,5" "1,2,5" "5,1,2"
> 
> Now my Question:
> How do I convert X into the dataframe I would have got if I had read
> this output from a file instead of into the character vector X?
> 
> In other words, how to convert a vector of character strings, each
> of which is in comma-separated format as above, into the rows of
> a data-frame (or matrix, come to that)?

read.table() on a text connection.

> X <- c("1,4,5", "1,2,5", "5,1,2")
> read.table(textConnection(X), header=FALSE, sep=",")
  V1 V2 V3
1  1  4  5
2  1  2  5
3  5  1  2


-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From maj at stats.waikato.ac.nz  Tue Aug 26 10:45:28 2003
From: maj at stats.waikato.ac.nz (Murray Jorgensen)
Date: Tue, 26 Aug 2003 20:45:28 +1200
Subject: [R] R tools for large files
In-Reply-To: <16203.1787.959620.373333@gargle.gargle.HOWL>
References: <Pine.LNX.4.44.0308251058541.4710-100000@gannet.stats>	<3F4A9F98.8040006@stats.waikato.ac.nz>
	<16203.1787.959620.373333@gargle.gargle.HOWL>
Message-ID: <3F4B1E28.30105@stats.waikato.ac.nz>

Hi Martin,

I don't know much about the concept of "connection" but I had supposed 
it to at least include the concept of "file" and perhaps also "input 
device" and "output device'. I guess the important point that you are 
making is that it is sequential in the sense that you describe. I 
suppose at the time that I wrote my emails I didn't *know* that this was 
  the case but rather assumed that this must be so, since it would be 
tedious in the extreme to have to work with the access functions if they 
kept going back to the beginning of the connection.

It may help to explain the application. The large files that I am 
working with are themselves statistical summaries of internet traffic 
flows (you will appreciate why they can be almost arbitrarily large!) I 
am interested in clustering these flows into different classes of 
traffic. I am using a model-based approach, so that the end-point will 
be statistical models for each cluster. Once these have been estimated 
they may be used in the classification of future traffic [including a 
residual class of traffic that does not fit any cluster well].

Based on experience with my clustering software (Multimix) I believe 
that it should work well on data sets of, say, 3000 observations. I plan 
to select a small number of random subsets of this size. The replication 
of these subsets should help me with model selection questions (How many 
Clusters? How complex should each cluster model be?)

Tom Mulholland makes a good point when he notes that many R users (and 
other users) have very little control over their computing environment 
owing to somewhat arbitrary IT management decisions. For this reason it 
will be advantageous to have several solutions to large file problems.

I'm pleased that you think that efficient R functions for manipulating 
numbered lines from files may be written. I'm going to have a go at it 
just as soon as I finish a big item of paperwork!

BTW, I will be out of town and with much reduced email access over the 
next week or so, so if I don't reply to the list or individuals this 
should not be put down to laziness or rudeness!

Cheers,

Murray Jorgensen

PS  Give my regards to Chris Hennig.

Martin Maechler wrote:
> Hi Murray,
> 
> from reading your summarizing reply, I wonder if you missed the
> most important point about "connection"s  connection := generalization of file):
> 
> Once you open() one, you can read it **sequentially**, e.g., in
> bunches of a "few" lines  i.e., you don't re-start from the
> beginning each time.
> I think this will allow to devise a pretty efficient R function
> for reading (and returning as a vector of strings) line numbers
> (n1, n2,..., nm).   
> 
> Did you know this?  If not, maybe you forward this answer (and
> your reaction to it) to R-help as well.
> 
> Regards,
> Martin Maechler <maechler at stat.math.ethz.ch>	http://stat.ethz.ch/~maechler/
> Seminar fuer Statistik, ETH-Zentrum  LEO C16	Leonhardstr. 27
> ETH (Federal Inst. Technology)	8092 Zurich	SWITZERLAND
> phone: x-41-1-632-3408		fax: ...-1228			<><
> 
> 

-- 
Dr Murray Jorgensen      http://www.stats.waikato.ac.nz/Staff/maj.html
Department of Statistics, University of Waikato, Hamilton, New Zealand
Email: maj at waikato.ac.nz                                Fax 7 838 4155
Phone  +64 7 838 4773 wk    +64 7 849 6486 home    Mobile 021 1395 862



From dave at evocapital.com  Tue Aug 26 11:03:04 2003
From: dave at evocapital.com (David Khabie-Zeitoune)
Date: Tue, 26 Aug 2003 10:03:04 +0100
Subject: [R] R tools for large files
Message-ID: <8D0F30FE2EB3314182D4A33F738BB19D063F63@mail.internal.net>

A starting point might be the string splitting function strsplit

For example,

> X = c("1,4,5" "1,2,5" "5,1,2")
> strsplit(X)
[[1]]
[1] "1" "4" "5"

[[2]]
[1] "1" "2" "5"

[[3]]
[1] "5" "1" "2"

This returns a list of the parsed vectors. Next you can do something
like:
> Z = data.frame(matrix(unlist(X), nrow = 3, byrow=T))
> Z
  X1 X2 X3
1  1  4  5
2  1  2  5
3  5  1  2 




-----Original Message-----
From: Ted.Harding at nessie.mcc.ac.uk [mailto:Ted.Harding at nessie.mcc.ac.uk]

Sent: 26 August 2003 09:00
To: R-help
Subject: Re: [R] R tools for large files


This has been an interesting thread! My first reaction to Murray's query
was to think "use standard Unix tools, especially awk", 'awk' being a
compact, fast, efficient program with great powers for processing lines
of data files (and in particular extracting, subsetting and transforming
database-like files e.g. CSV-type).

Of course, that became a sub-thread in its own right.

But -- and here I know I'm missing a trick which is why I'm responding
now so that someone who knows the trick can tell me -- while I normally
use 'awk' "externally" (i.e. I filter a data file through an 'awk'
program outside of R and then read the resulting file into R), I began
to think about doing it from within R.

Something on the lines of

  X <- system("cat raw_data | awk '...' ", intern=TRUE)

would create an object X which is a character vector, each element of
which is one line from the output of the command "cat ...... ".

E.g. if "raw_data" starts out as

  1,2,3,4,5
  1,3,4,2,5
  5,4,3,2,1
  5,3,4,1,2

then

  X<-system("cat raw_data.csv |
  awk 'BEGIN{FS=\",\"}{if($3>$2){print $1 \",\" $4 \",\" $5}}'",
  intern=TRUE)

gives

  > X
  [1] "1,4,5" "1,2,5" "5,1,2"

Now my Question:
How do I convert X into the dataframe I would have got if I had read
this output from a file instead of into the character vector X?

In other words, how to convert a vector of character strings, each of
which is in comma-separated format as above, into the rows of a
data-frame (or matrix, come to that)?

With thanks,
Ted.


--------------------------------------------------------------------
E-Mail: (Ted Harding) <Ted.Harding at nessie.mcc.ac.uk>
Fax-to-email: +44 (0)870 167 1972
Date: 26-Aug-03                                       Time: 08:59:48
------------------------------ XFMail ------------------------------

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://www.stat.math.ethz.ch/mailman/listinfo/r-help



From Ted.Harding at nessie.mcc.ac.uk  Tue Aug 26 11:05:14 2003
From: Ted.Harding at nessie.mcc.ac.uk ( (Ted Harding))
Date: Tue, 26 Aug 2003 10:05:14 +0100 (BST)
Subject: [R] R tools for large files
In-Reply-To: <Pine.LNX.4.44.0308260939330.10828-100000@gannet.stats>
Message-ID: <XFMail.030826100514.Ted.Harding@nessie.mcc.ac.uk>

On 26-Aug-03 Prof Brian Ripley wrote:
> On Tue, 26 Aug 2003 Ted.Harding at nessie.mcc.ac.uk wrote:
> [...]
>>   > X
>>   [1] "1,4,5" "1,2,5" "5,1,2"
>> 
>> Now my Question:
>> [...]
>> In other words, how to convert a vector of character strings, each
>> of which is in comma-separated format as above, into the rows of
>> a data-frame (or matrix, come to that)?
> 
> read.table() on a text connection.
> 
>> X <- c("1,4,5", "1,2,5", "5,1,2")
>> read.table(textConnection(X), header=FALSE, sep=",")
>   V1 V2 V3
> 1  1  4  5
> 2  1  2  5
> 3  5  1  2

Thanks, Brian! Just the job.
Best wishes,
Ted.


--------------------------------------------------------------------
E-Mail: (Ted Harding) <Ted.Harding at nessie.mcc.ac.uk>
Fax-to-email: +44 (0)870 167 1972
Date: 26-Aug-03                                       Time: 10:05:14
------------------------------ XFMail ------------------------------



From HADASSA.BRUNSCHWIG at Roche.COM  Tue Aug 26 11:53:14 2003
From: HADASSA.BRUNSCHWIG at Roche.COM (Brunschwig, Hadassa {PDMM~Basel})
Date: Tue, 26 Aug 2003 11:53:14 +0200
Subject: [R] bootstrapping nlme fits (was boot function)
Message-ID: <BC6A439CD6835749A9C7B8D8F041DFA88B11B0@rbamsem1.emea.roche.com>

(see archives for former discussion)

I probably didnt express myself very well in the last mail i wrote. The dataset contains the variables subject, day, concentration    (measured). I would like to bootstrap the variable subject. Now it is true that the subjects wont be independent in the bootstrap samples but i still wanna conduct the bootstrapping just for the sake of comparison with other results i got. Well, i succeeded to  set up the program needed for bootstrapping. However, i get the error message:

Error: Unable to form Cholesky decomposition

or that convergence was not reached with the maximal interations.
What exactly does that mean? Is there possibly any connection to the fact that the subjects are not independent?

Thanks a lot for answers

Dassy



From ucgamdo at ucl.ac.uk  Tue Aug 26 13:46:23 2003
From: ucgamdo at ucl.ac.uk (ucgamdo@ucl.ac.uk)
Date: Tue, 26 Aug 2003 12:46:23 +0100
Subject: [R] Exporting R graphs
Message-ID: <3.0.5.32.20030826124623.007d06e0@pop-server.ucl.ac.uk>

Hi,

	I have been a happy user of R for windows for more than a year, however,
recently, I started using linux as my operating system and now I have
practically switched completely. Of course, I still use R with linux,
however, certain nice features of R in windows seem to be missing or
hidden. I need help in basically two points:

1. In windows, I could copy the contents of a window graphic's device as a
windows metafile, and then I could paste the metafile into OpenOffice draw.
The advantage of doing this is that I could then manipulate my graph as a
vector object and get absolute control of every line, point, etc. I could
also paste several graphs in the same page and resize them arbitrarily,
which is a lot nicer than using the screen.split() or frame() functions in
R. Is there any equivalent method I could use to get R graphics into
OpenOffice draw in linux?, keeping the same functionality of course.

2. In windows, I could 'record' a graph, and then undo any changes made.
For example, if I misplaced a text label somewhere, I could go back and
place it again properly without having to re-plot everything again. I have
been browsing the R documentation and found some recording functions for
graphic devices but they do not seem to be exactly what I am looking for.
Is there any way I can access the display list of a device and change it?
Some of my plots are the product of tedious and long simulations which I
wouldn't like to repeat if I make a mistake labelling my plots, placing
legends, etc.

Thanks a lot for any help you can offer me,
M.



From gareth.hughes-2 at stud.man.ac.uk  Tue Aug 26 13:51:10 2003
From: gareth.hughes-2 at stud.man.ac.uk (Gareth Hughes)
Date: Tue, 26 Aug 2003 12:51:10 +0100
Subject: [R] coxph.control
Message-ID: <E19rcMY-000AeD-2c@serenity.mcc.ac.uk>

How can I specify the maximum number of iterations in coxph 
whilst also specifying my model?? I can't find any on-line 
examples. Thanks



From rdiaz at cnio.es  Tue Aug 26 14:20:25 2003
From: rdiaz at cnio.es (Ramon Diaz-Uriarte)
Date: Tue, 26 Aug 2003 14:20:25 +0200
Subject: [R] coxph.control
In-Reply-To: <E19rcMY-000AeD-2c@serenity.mcc.ac.uk>
References: <E19rcMY-000AeD-2c@serenity.mcc.ac.uk>
Message-ID: <200308261420.25777.rdiaz@cnio.es>

Dear Gareth,

?coxph.control (which we are told to check from ?coxph) contains an argument 
for maxiter.

Best,

R.


On Tuesday 26 August 2003 13:51, Gareth Hughes wrote:
> How can I specify the maximum number of iterations in coxph
> whilst also specifying my model?? I can't find any on-line
> examples. Thanks
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help

-- 
Ram?n D?az-Uriarte
Bioinformatics Unit
Centro Nacional de Investigaciones Oncol?gicas (CNIO)
(Spanish National Cancer Center)
Melchor Fern?ndez Almagro, 3
28029 Madrid (Spain)
Fax: +-34-91-224-6972
Phone: +-34-91-224-6900

http://bioinfo.cnio.es/~rdiaz



From Simon.Fear at synequanon.com  Tue Aug 26 14:25:15 2003
From: Simon.Fear at synequanon.com (Simon Fear)
Date: Tue, 26 Aug 2003 13:25:15 +0100
Subject: [R] Subsetting a dataframe
Message-ID: <6C8A8033ABC1E3468048ABC4F13CE572AC0D89@synequanon01>

Or 

subset(nameofdataset, subject %in% c(15,19))

which does "the right thing" if subject should be missing (unlikely in
this
example I know, but likely to be a different story if conditioning on
concentration, say).

A pity subset() and transform() are not mentioned in the subsetting
section
of the "Introduction to R" manual - a suggestion for future releases?
 

Simon Fear
Senior Statistician
Syne qua non Ltd
Tel: +44 (0) 1379 644449
Fax: +44 (0) 1379 644445
email: Simon.Fear at synequanon.com
web: http://www.synequanon.com
 
Number of attachments included with this message: 0
 
This message (and any associated files) is confidential and\...{{dropped}}



From dmurdoch at pair.com  Tue Aug 26 15:24:33 2003
From: dmurdoch at pair.com (Duncan Murdoch)
Date: Tue, 26 Aug 2003 09:24:33 -0400
Subject: [R] R tools for large files
In-Reply-To: <3F4B1E28.30105@stats.waikato.ac.nz>
References: <Pine.LNX.4.44.0308251058541.4710-100000@gannet.stats>	<3F4A9F98.8040006@stats.waikato.ac.nz>
	<16203.1787.959620.373333@gargle.gargle.HOWL>
	<3F4B1E28.30105@stats.waikato.ac.nz>
Message-ID: <ucnmkvs7or05a5dai5pfkuc9vt18m878kk@4ax.com>

Murray:

You said that you wanted more control over the selection of lines than
scan() was giving you.  This may have been suggested by someone else
(there have been a lot of  messages in this thread!), but I think what
you want to do is to "roll your own" reader, using textConnection.

For example, if you want to read lines 1000 through 1100, you'd do it
like this:

 lines <- readLines("foo.txt", 1100)[1000:1100]
 result <- scan(textConnection(lines), list( .... ))

This will be inefficient with very large files if you want to read
every nth line or a block at the end of the file, because the first
line reads a contiguous block and then subsets.  In that case you
probably want to write your own loop that builds up the lines variable
first.

Duncan Murdoch



From postmaster at s-boehringer.de  Tue Aug 26 15:35:36 2003
From: postmaster at s-boehringer.de (Stefan =?ISO-8859-1?Q?B=F6hringer?=)
Date: Tue, 26 Aug 2003 13:35:36 -0000
Subject: [R] discriminant function
Message-ID: <1061904283.29010.13.camel@hgX>

How can I extract the linear discriminant functions resulting from a LDA
analysis?

The coefficients are listed as a result from the analysis but I have not
found a way to extract these programmatically. No refrences in the
archives were found.

Thank you very much,

	Stefan



From dmurdoch at pair.com  Tue Aug 26 15:51:48 2003
From: dmurdoch at pair.com (Duncan Murdoch)
Date: Tue, 26 Aug 2003 09:51:48 -0400
Subject: [R] Exporting R graphs
In-Reply-To: <3.0.5.32.20030826124623.007d06e0@pop-server.ucl.ac.uk>
References: <3.0.5.32.20030826124623.007d06e0@pop-server.ucl.ac.uk>
Message-ID: <tdpmkv0uio3ic1h37e4ab3dc25sp8183ad@4ax.com>

On Tue, 26 Aug 2003 12:46:23 +0100, ucgamdo at ucl.ac.uk wrote :

>2. In windows, I could 'record' a graph, and then undo any changes made.
>For example, if I misplaced a text label somewhere, I could go back and
>place it again properly without having to re-plot everything again. 

Can you post an example of doing this?  I was unaware it was possible.

Duncan Murdoch



From postmaster at univ-montp2.fr  Tue Aug 26 15:57:39 2003
From: postmaster at univ-montp2.fr (postmaster@univ-montp2.fr)
Date: Tue, 26 Aug 2003 15:57:39 +0200
Subject: [R] VIRUS DANS VOTRE MAIL
Message-ID: <200308261357.h7QDvdaC022087@mailhub2.univ-montp2.fr>

                           V I R U S  A L E R T

Notre antivirus a trouv? le(s) virus

	W32/Sobig.f at MM

dans votre mail vers le(s) destinataire(s) suivant(s):

-> claude at isem.univ-montp2.fr

SVP, nettoyez votre syst?me de tous ces virus avant de renvoyer ce mail.

Pour information, voici les en-t?tes de votre email:

------------------------- BEGIN HEADERS -----------------------------
Return-Path: <r-help at stat.math.ethz.ch>
Received: from OEMCOMPUTER (ATuileries-103-1-4-227.w80-11.abo.wanadoo.fr [80.11.47.227])
	by mailhub2.univ-montp2.fr (8.12.8/8.12.8) with ESMTP id h7QDvHP0022010
	for <claude at isem.univ-montp2.fr>; Tue, 26 Aug 2003 15:57:19 +0200
From: r-help at stat.math.ethz.ch
Message-Id: <200308261357.h7QDvHP0022010 at mailhub2.univ-montp2.fr>
To: <claude at isem.univ-montp2.fr>
Subject: Re: That movie
Date: Tue, 26 Aug 2003 15:57:18 +0200
X-MailScanner: Found to be clean
Importance: Normal
X-Mailer: Microsoft Outlook Express 6.00.2600.0000
X-MSMail-Priority: Normal
X-Priority: 3 (Normal)
MIME-Version: 1.0
Content-Type: multipart/mixed;
	boundary="_NextPart_000_00EC2591"
-------------------------- END HEADERS ------------------------------



From hothorn at ci.tuwien.ac.at  Tue Aug 26 15:59:57 2003
From: hothorn at ci.tuwien.ac.at (Torsten Hothorn)
Date: Tue, 26 Aug 2003 15:59:57 +0200 (CEST)
Subject: [R] discriminant function
In-Reply-To: <1061904283.29010.13.camel@hgX>
Message-ID: <Pine.LNX.3.96.1030826155839.10710A-100000@thorin.ci.tuwien.ac.at>

On 26 Aug 2003, Stefan [ISO-8859-1] Bhringer wrote:

> How can I extract the linear discriminant functions resulting from a LDA
> analysis?
> 
> The coefficients are listed as a result from the analysis but I have not
> found a way to extract these programmatically. No refrences in the
> archives were found.

?lda tells you about the object returned by `lda', especially it element:

scaling: a matrix which transforms observations to discriminant
          functions, normalized so that within groups covariance matrix
          is spherical.

Torsten

> 
> Thank you very much,
> 
> 	Stefan
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> 
>



From bates at stat.wisc.edu  Tue Aug 26 16:05:09 2003
From: bates at stat.wisc.edu (Douglas Bates)
Date: Tue, 26 Aug 2003 14:05:09 -0000
Subject: [R] R tools for large files
In-Reply-To: <8D0F30FE2EB3314182D4A33F738BB19D063F63@mail.internal.net>
References: <8D0F30FE2EB3314182D4A33F738BB19D063F63@mail.internal.net>
Message-ID: <6rsmno346n.fsf@bates4.stat.wisc.edu>

"David Khabie-Zeitoune" <dave at evocapital.com> writes:

> For example,
> 
> > X = c("1,4,5" "1,2,5" "5,1,2")

I think you meant 

> X = c("1,4,5", "1,2,5", "5,1,2")

A simpler approach is to use a textConnection which allows you to read
a vector of character strings as if they were lines in a file.

> read.table(textConnection(X), sep=',')
  V1 V2 V3
1  1  4  5
2  1  2  5
3  5  1  2



From fgibbons at hms.harvard.edu  Tue Aug 26 16:20:25 2003
From: fgibbons at hms.harvard.edu (Frank Gibbons)
Date: Tue, 26 Aug 2003 10:20:25 -0400
Subject: [R] discriminant function
In-Reply-To: <1061904283.29010.13.camel@hgX>
Message-ID: <5.2.1.1.2.20030826101311.0229b978@email.med.harvard.edu>

Stefan,

I asked the same question last week. As Brian Ripley, its author, said then 
(and others), the only way to see what's going on is to read the code. It's 
pretty complicated statistically (that's why the performance is so good!), 
many of the details are in chapter 2 of Pattern Recognition & Neural 
Networks. The upshot is that it's really not that easy to come up with a 
simple equation to encompass all of what lda.R does, unfortunately.

You can get the scaling factors programmatically for the centered variables 
by running coef() on the LDA object, or just using the 
'<lda_object>$scaling' That's a start, but there's a long way to go from 
there....

-Frank

At 09:24 AM 8/26/2003, you wrote:
>How can I extract the linear discriminant functions resulting from a LDA
>analysis?
>
>The coefficients are listed as a result from the analysis but I have not
>found a way to extract these programmatically. No refrences in the
>archives were found.
>
>Thank you very much,
>
>         Stefan
>
>______________________________________________
>R-help at stat.math.ethz.ch mailing list
>https://www.stat.math.ethz.ch/mailman/listinfo/r-help

PhD, Computational Biologist,
Harvard Medical School BCMP/SGM-322, 250 Longwood Ave, Boston MA 02115, USA.
Tel: 617-432-3555       Fax: 
617-432-3557       http://llama.med.harvard.edu/~fgibbons



From tlumley at u.washington.edu  Tue Aug 26 16:19:19 2003
From: tlumley at u.washington.edu (Thomas Lumley)
Date: Tue, 26 Aug 2003 07:19:19 -0700 (PDT)
Subject: [R] coxph.control
In-Reply-To: <E19rcMY-000AeD-2c@serenity.mcc.ac.uk>
Message-ID: <Pine.A41.4.44.0308260718390.137302-100000@homer06.u.washington.edu>

On Tue, 26 Aug 2003, Gareth Hughes wrote:

> How can I specify the maximum number of iterations in coxph
> whilst also specifying my model?? I can't find any on-line
> examples. Thanks
>

coxph(Surv(time,death)~x+y*z, data=df, control=coxph.control(iter.max=42))

	-thomas



From B.Rowlingson at lancaster.ac.uk  Tue Aug 26 16:23:28 2003
From: B.Rowlingson at lancaster.ac.uk (Barry Rowlingson)
Date: Tue, 26 Aug 2003 15:23:28 +0100
Subject: [R] diamond graphs
In-Reply-To: <20030825172828.3d5b9872.fharrell@virginia.edu>
References: <ADEPLELNMGDGKCPEDINLKEJGKBAD.szeger@jhsph.edu>
	<20030825172828.3d5b9872.fharrell@virginia.edu>
Message-ID: <3F4B6D60.7090301@lancaster.ac.uk>


Can I just check something? If I write some code in R to produce these 
diamond graphs - I reckon 20-30 lines should do it - and post it to 
R-help, have I broken the law if there's a patent on them? If I produce 
these graphs using a pen, paper and my own artistic skill, will that 
make me liable to a jail term? Will my code or artistic efforts be okay 
if I pay the originators a fee and put a 'licensed diamond-graph 
producer #55243' logo at the bottom?

  Now I'm starting to worry that SCO will claim they hold copyrights on 
histograms.

Baz



From tlumley at u.washington.edu  Tue Aug 26 16:25:44 2003
From: tlumley at u.washington.edu (Thomas Lumley)
Date: Tue, 26 Aug 2003 07:25:44 -0700 (PDT)
Subject: [R] Exporting R graphs
In-Reply-To: <3.0.5.32.20030826124623.007d06e0@pop-server.ucl.ac.uk>
Message-ID: <Pine.A41.4.44.0308260721120.137302-100000@homer06.u.washington.edu>

On Tue, 26 Aug 2003 ucgamdo at ucl.ac.uk wrote:

>
> 1. In windows, I could copy the contents of a window graphic's device as a
> windows metafile, and then I could paste the metafile into OpenOffice draw.
> The advantage of doing this is that I could then manipulate my graph as a
> vector object and get absolute control of every line, point, etc. I could
> also paste several graphs in the same page and resize them arbitrarily,
> which is a lot nicer than using the screen.split() or frame() functions in
> R. Is there any equivalent method I could use to get R graphics into
> OpenOffice draw in linux?, keeping the same functionality of course.

You can't cut-and-paste or use Windows metafiles under Linux.  You may be
able to get resizable graphs with pdf() or postscript(), but you won't be
able to get that sort of editing control.  Another possibility is xfig(),
which produces graphs that can be edited in xfig. I don't know if
OpenOffice knows about xfig formats (or about SVG, which R can also
produce).


	-thomas



From flom at ndri.org  Tue Aug 26 16:31:06 2003
From: flom at ndri.org (Peter Flom)
Date: Tue, 26 Aug 2003 10:31:06 -0400
Subject: [R] Simple simulation in R
Message-ID: <sf4b3708.063@MAIL.NDRI.ORG>

Hello all

I have a feeling this is very simple......but I am not sure how to do
it

My boss has two variables, one is an average of 4 numbers, the other is
an average of 3 of those numbers i.e

var1 = (X1 + X2 + X3 + X4)/4
var2 = (X1 + X2 + X3)/3

all of the X variables are supposed to be measuring similar constructs

not surprisingly, these are highly correlated (r = .98), the question
is how much of  this correlation is due to the fact that the X's are
related, and how much to the fact that the two VARs are similarly
constructed

What I want to do is simulate this with normally distributed data for
the X's.  That is, generate (say) 1000 sets of X1 through X4, use those
to caluculate 1000 var1 and var2, and then 1000 correlations between
var1 and var2, and then plot those results.

Any help appreciated

Peter Flom

Peter L. Flom, PhD
Assistant Director, Statistics and Data Analysis Core
Center for Drug Use and HIV Research
National Development and Research Institutes
71 W. 23rd St
www.peterflom.com
New York, NY 10010
(212) 845-4485 (voice)
(917) 438-0894 (fax)



From Luis.Tito-de-Morais at ird.sn  Tue Aug 26 17:02:41 2003
From: Luis.Tito-de-Morais at ird.sn (Tito de Morais Luis)
Date: Tue, 26 Aug 2003 15:02:41 -0000
Subject: [R] Exporting R graphs
In-Reply-To: <Pine.A41.4.44.0308260721120.137302-100000@homer06.u.washington.edu>
References: <Pine.A41.4.44.0308260721120.137302-100000@homer06.u.washington.edu>
Message-ID: <1061901708.24700.32.camel@rap06.ird.sn>

Hi,
I use jpeg or png to export my graphs and I can edit them with "the
gimp" and add anything to them. I can then export them into several
formats, or open the jpeg file with openoffice.

As Thomas suggests you can also use xfig.
xfig can also export its graphs to many other formats.

see ?jpeg
and http://www.gimp.org

HTH

Tito

Le mar 26/08/2003 ? 16:25, Thomas Lumley a ?crit :
> On Tue, 26 Aug 2003 ucgamdo at ucl.ac.uk wrote:
> 
> >
> > 1. In windows, I could copy the contents of a window graphic's device as a
> > windows metafile, and then I could paste the metafile into OpenOffice draw.
> > The advantage of doing this is that I could then manipulate my graph as a
> > vector object and get absolute control of every line, point, etc. I could
> > also paste several graphs in the same page and resize them arbitrarily,
> > which is a lot nicer than using the screen.split() or frame() functions in
> > R. Is there any equivalent method I could use to get R graphics into
> > OpenOffice draw in linux?, keeping the same functionality of course.
> 
> You can't cut-and-paste or use Windows metafiles under Linux.  You may be
> able to get resizable graphs with pdf() or postscript(), but you won't be
> able to get that sort of editing control.  Another possibility is xfig(),
> which produces graphs that can be edited in xfig. I don't know if
> OpenOffice knows about xfig formats (or about SVG, which R can also
> produce).
> 
> 
> 	-thomas

-- 
L. Tito de Morais
      UR RAP
   IRD de Dakar
      BP 1386
       Dakar
      S?n?gal

T?l.: + 221 849 33 31
Fax: +221 832 16 75
Courriel: tito at ird.sn



From MSchwartz at medanalytics.com  Tue Aug 26 17:35:49 2003
From: MSchwartz at medanalytics.com (Marc Schwartz)
Date: Tue, 26 Aug 2003 10:35:49 -0500
Subject: [R] Exporting R graphs
In-Reply-To: <3.0.5.32.20030826124623.007d06e0@pop-server.ucl.ac.uk>
References: <3.0.5.32.20030826124623.007d06e0@pop-server.ucl.ac.uk>
Message-ID: <1061912148.2651.153.camel@localhost>

On Tue, 2003-08-26 at 06:46, ucgamdo at ucl.ac.uk wrote:
> Hi,
> 
> 	I have been a happy user of R for windows for more than a year, however,
> recently, I started using linux as my operating system and now I have
> practically switched completely. Of course, I still use R with linux,
> however, certain nice features of R in windows seem to be missing or
> hidden. I need help in basically two points:
> 
> 1. In windows, I could copy the contents of a window graphic's device as a
> windows metafile, and then I could paste the metafile into OpenOffice draw.
> The advantage of doing this is that I could then manipulate my graph as a
> vector object and get absolute control of every line, point, etc. I could
> also paste several graphs in the same page and resize them arbitrarily,
> which is a lot nicer than using the screen.split() or frame() functions in
> R. Is there any equivalent method I could use to get R graphics into
> OpenOffice draw in linux?, keeping the same functionality of course.


There are, as you are finding out, some notable differences between the
Windows and Linux versions of R which are described in the R Windows
FAQ, section 4. This is also a reflection to an extent, of the
differences between Windows and Linux. As one who has also made the
switch in the past year, this is an internal struggle that requires a
shift in thinking to overcome years of ingrained behavior.

Depending upon what you are doing with the output (ie. displaying on the
screen or generating printed output), you have options of generating a
png() or jpg() bitmap format image for the screen or sending the output
to an encapsulated postscript file for printing. OpenOffice's (OOo)
applications can import either in to an Impress slide or Writer
document, for example.

As an aside, there is a Linux library for generating metafile graphics
called libEMF, but my personal experience is less than enthusiastic
relative to the image quality generated.

See ?bitmap, ?png and ?jpeg for additional information on generating
bitmap images. See ?postscript for information on generating an EPS
file, noting very carefully, the instructions in the Detail section
required to do so.

Using bitmaps of course, the behavior of the graphic objects are
different as compared to vector objects, not least of which is the more
limited ability to re-size the images, before a loss of image quality. 

That being said, from a philosophic perspective, I would recommend that
you use R alone to get the graphic or matrix of graphics (ie. using
layout() ) the way you want and then use OOo only as a display or
document layout mechanism. If there are things that you must edit
outside of R, you could use The GIMP to do this on the jpg or png files.

Yet another option for document layouts is called Sweave
(http://www.ci.tuwien.ac.at/~leisch/). You may wish to explore Sweave,
which enables you to create LaTeX documents that can be automatically
updated with the output of R's analyses.

R's graphics provide you with a very substantial ability to minutely
control the plot output and to do so in a fashion that enables
reproducible results once you have coded the plots. This is a shift from
the "point and click" or "drag and drop" approach that is common under
Windows.

If I am going to display a plot on the screen (ie. in Impress) and know
what size image I require, what I have generally done is to use bitmap()
or png() to generate a PNG file, properly sized to what I require in the
target slide. I then Insert the graphic file into the slide.

If I need hard copy output to the printer, I generate EPS versions of
the plots and insert those into the documents and/or slides and then
print them. The challenge with using EPS files is that in OOo, you
cannot see the image. They show up as an object, generally displaying
the embedded postscript title. You can create "EPS previews", however
doing so adds tremendously to the file size for a decent preview image.
Thus, using and setting the 'title' argument in postscript() to
something that tells you what the plot is can be helpful if you need to
re-order the slides or change the document layout.

Another option with postscript formats, one you have the document set in
OOo, is to use the "Print to File" option, generating a postscript file,
which can then be displayed (or printed) using gv or one of the other PS
viewers available under Linux.

In conjunction with the above, I have also used ps2pdf under Linux to
convert the PS file to a PDF file, if I need to send the output to
someone who may not have the ability to view a PS file (ie. most Windows
users will have Adobe's free Acrobat Reader, but not GhostView.) This
gives you a range of options if you need to share the output with others
electronically.

Lastly, if you just need to create some plots that you can send to folks
independent of OOo, you can also use pdf() in R (see ?pdf), which will
enable you to create a PDF file directly containing one or more plots,
that you can send to folks who cannot view PS files.

Bottom line, I would use R to create the plots and use the other
applications for document/slide formatting. But...that's my opinion
based upon what I am typically doing.


> 2. In windows, I could 'record' a graph, and then undo any changes made.
> For example, if I misplaced a text label somewhere, I could go back and
> place it again properly without having to re-plot everything again. I have
> been browsing the R documentation and found some recording functions for
> graphic devices but they do not seem to be exactly what I am looking for.
> Is there any way I can access the display list of a device and change it?
> Some of my plots are the product of tedious and long simulations which I
> wouldn't like to repeat if I make a mistake labelling my plots, placing
> legends, etc.


What you could do is to use recordPlot() to save the version of the plot
to a file, prior to the point at which you start annotating, so that you
can return to that same point if you make a mistake.

As a brief example, with output to the display:

# Do a quick scatterplot
plot(1:10)

# Now save the plot's current state
myplot <- recordPlot()

# Now save display list 'myplot' to a file
save(myplot, file = "myplot")

# Now place some text on the plot
# Click on the plot with the mouse after 
# this line executes, to indicate where you
# want the text located.
text(locator(1), "Some Text", adj=0)

# OK, didn't like where I located the text
# so load and redisplay the base plot
load("myplot")
myplot

# Now re-locate the text
text(locator(1), "Some Text", adj=0)


BTW, thanks to Paul Murrell
(http://www.stat.auckland.ac.nz/~paul/grid/saveload.pdf) for hints on
the above approach.

> 
> Thanks a lot for any help you can offer me,
> M.


HTH,

Marc Schwartz



From thpe at hhbio.wasser.tu-dresden.de  Tue Aug 26 18:02:06 2003
From: thpe at hhbio.wasser.tu-dresden.de (Thomas Petzoldt)
Date: Tue, 26 Aug 2003 18:02:06 +0200
Subject: [R] Simple simulation in R
In-Reply-To: <sf4b3708.063@MAIL.NDRI.ORG>
References: <sf4b3708.063@MAIL.NDRI.ORG>
Message-ID: <3F4B847E.7040807@hhbio.wasser.tu-dresden.de>

> Hello all
> 
> I have a feeling this is very simple......but I am not sure how to do
> it
> 
> My boss has two variables, one is an average of 4 numbers, the other is
> an average of 3 of those numbers i.e
> 
> var1 = (X1 + X2 + X3 + X4)/4
> var2 = (X1 + X2 + X3)/3

Hello Peter, try the following:

####################################
nsim  <- 100
nsamp <- 100

r <- numeric(nsim)

for (i in 1:nsim) {
   dat <- matrix(rnorm(4 * nsamp), ncol=4)
   v1 <- rowSums(dat)/4
   v2 <- rowSums(dat[,1:3])/3
   r[i] <- cor(v1,v2)
}

plot(sort(r))
####################################

Hope it helps!

Thomas P.



From MSchwartz at medanalytics.com  Tue Aug 26 18:26:55 2003
From: MSchwartz at medanalytics.com (Marc Schwartz)
Date: Tue, 26 Aug 2003 11:26:55 -0500
Subject: [R] diamond graphs
In-Reply-To: <3F4B6D60.7090301@lancaster.ac.uk>
References: <ADEPLELNMGDGKCPEDINLKEJGKBAD.szeger@jhsph.edu>
	<20030825172828.3d5b9872.fharrell@virginia.edu>
	<3F4B6D60.7090301@lancaster.ac.uk>
Message-ID: <1061915214.2651.191.camel@localhost>

On Tue, 2003-08-26 at 09:23, Barry Rowlingson wrote:
> Can I just check something? If I write some code in R to produce these 
> diamond graphs - I reckon 20-30 lines should do it - and post it to 
> R-help, have I broken the law if there's a patent on them? If I produce 
> these graphs using a pen, paper and my own artistic skill, will that 
> make me liable to a jail term? Will my code or artistic efforts be okay 
> if I pay the originators a fee and put a 'licensed diamond-graph 
> producer #55243' logo at the bottom?
> 
>   Now I'm starting to worry that SCO will claim they hold copyrights on 
> histograms.
> 
> Baz


I was wondering if SCO might come up in this thread...  :-)


As an FYI, these articles on ZDNet this week, may be of interest:

Patent battle to culminate in Brussels
http://zdnet.com.com/2100-1104_2-5068007.html

The patent nuclear weapon (commentary)
http://zdnet.com.com/2100-1107-5067526.html


BTW Frank, excellent points made in your reply.

Marc



From paul at datavore.com  Tue Aug 26 18:45:09 2003
From: paul at datavore.com (Paul Meagher)
Date: Tue, 26 Aug 2003 13:45:09 -0300
Subject: [R] Viewing function source
Message-ID: <005101c36bf1$6a8cab40$9141de18@computer>

I know I should probably RTFM for this question, but could someone tell me
if R supports the idea of "viewing source" on any particular function you
want to use?

If I want to "view source" on the rpois() function, for example, can I do
somethink like:

source(rpois)

To see how the function is implemented?

Regards,

Paul Meagher

Datavore Productions
50 Wood Grove Drive
Truro, Nova Scotia
B2N-6Y4
1.902.895.9671
www.datavore.com



From andy_liaw at merck.com  Tue Aug 26 18:47:39 2003
From: andy_liaw at merck.com (Liaw, Andy)
Date: Tue, 26 Aug 2003 12:47:39 -0400
Subject: [R] Simple simulation in R
Message-ID: <3A822319EB35174CA3714066D590DCD50205CA3A@usrymx25.merck.com>

I believe simple math stat calculations should be sufficient for this.

For simplicity, assume X1 through X4 are iid with mean m and variance v.

Note that

  var1 = (3*var2 + x4) / 4

so
 
  cov(var1, var2) = cov(var2, (3*var2 + x4)/4)

and since var2 and x4 are independent, this covariance can be simplified.
Carry this through and substituting in m and v in the appropriate places
should give you the covariance (and hence correlation) in terms of m and
v.

Andy



> -----Original Message-----
> From: Peter Flom [mailto:flom at ndri.org] 
> Sent: Tuesday, August 26, 2003 10:31 AM
> To: r-help at stat.math.ethz.ch
> Subject: [R] Simple simulation in R
> 
> 
> Hello all
> 
> I have a feeling this is very simple......but I am not sure 
> how to do it
> 
> My boss has two variables, one is an average of 4 numbers, 
> the other is an average of 3 of those numbers i.e
> 
> var1 = (X1 + X2 + X3 + X4)/4
> var2 = (X1 + X2 + X3)/3
> 
> all of the X variables are supposed to be measuring similar constructs
> 
> not surprisingly, these are highly correlated (r = .98), the 
> question is how much of  this correlation is due to the fact 
> that the X's are related, and how much to the fact that the 
> two VARs are similarly constructed
> 
> What I want to do is simulate this with normally distributed 
> data for the X's.  That is, generate (say) 1000 sets of X1 
> through X4, use those to caluculate 1000 var1 and var2, and 
> then 1000 correlations between var1 and var2, and then plot 
> those results.
> 
> Any help appreciated
> 
> Peter Flom
> 
> Peter L. Flom, PhD
> Assistant Director, Statistics and Data Analysis Core
> Center for Drug Use and HIV Research
> National Development and Research Institutes
> 71 W. 23rd St
> www.peterflom.com
> New York, NY 10010
> (212) 845-4485 (voice)
> (917) 438-0894 (fax)
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list 
> https://www.stat.math.ethz.ch/mailman/listinfo> /r-help
> 

------------------------------------------------------------------------------
Notice:  This e-mail message, together with any attachments,...{{dropped}}



From B.Rowlingson at lancaster.ac.uk  Tue Aug 26 18:53:38 2003
From: B.Rowlingson at lancaster.ac.uk (Barry Rowlingson)
Date: Tue, 26 Aug 2003 17:53:38 +0100
Subject: [R] Viewing function source
In-Reply-To: <005101c36bf1$6a8cab40$9141de18@computer>
References: <005101c36bf1$6a8cab40$9141de18@computer>
Message-ID: <3F4B9092.8010300@lancaster.ac.uk>

Paul Meagher wrote:

> If I want to "view source" on the rpois() function, for example, can I do
> somethink like:
> 
> source(rpois)
> 
> To see how the function is implemented?
> 

  You mean you've never typed 'ls' instead of 'ls()' and discovered this 
for yourself? I still do it all the time, and I've been using S/R for 
ten+ years :)

  Ah, maybe you're not a Unix person ('ls' is the command to list files).

  Just type a function name to see the source, or to see how it 
disappears into R's internal code. You can also do print(rpois), but 
you'll see for rpois it disappears into internal code:

function (n, lambda)
.Internal(rpois(n, lambda))
<environment: namespace:base>

  - now you need to get the full source code for R and look into C code.

Baz



From ripley at stats.ox.ac.uk  Tue Aug 26 19:00:02 2003
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Tue, 26 Aug 2003 18:00:02 +0100 (BST)
Subject: [R] Viewing function source
In-Reply-To: <005101c36bf1$6a8cab40$9141de18@computer>
Message-ID: <Pine.LNX.4.44.0308261756070.1221-100000@gannet.stats>

Most of the time you just use

rpois

or, to use a pager,

page(rpois)

For some functions the function is hidden from the end user, and for those 
(indeed, for any object the system can find) you can use

getAnywhere("foo.bar")


In your example, it will not be too revealing:

> rpois
function (n, lambda)
.Internal(rpois(n, lambda))

....

On Tue, 26 Aug 2003, Paul Meagher wrote:

> I know I should probably RTFM for this question, but could someone tell me
> if R supports the idea of "viewing source" on any particular function you
> want to use?
> 
> If I want to "view source" on the rpois() function, for example, can I do
> somethink like:
> 
> source(rpois)
> 
> To see how the function is implemented?


-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From jgentry at jimmy.harvard.edu  Tue Aug 26 19:01:43 2003
From: jgentry at jimmy.harvard.edu (Jeff Gentry)
Date: Tue, 26 Aug 2003 13:01:43 -0400 (EDT)
Subject: [R] Viewing function source
In-Reply-To: <005101c36bf1$6a8cab40$9141de18@computer>
Message-ID: <Pine.SOL.4.20.0308261300570.17169-100000@santiam.dfci.harvard.edu>

> If I want to "view source" on the rpois() function, for example, can I do
> somethink like:
> 
> source(rpois)
> 
> To see how the function is implemented?

Just type the name of the function



From tlumley at u.washington.edu  Tue Aug 26 19:07:01 2003
From: tlumley at u.washington.edu (Thomas Lumley)
Date: Tue, 26 Aug 2003 10:07:01 -0700 (PDT)
Subject: [R] Viewing function source
In-Reply-To: <005101c36bf1$6a8cab40$9141de18@computer>
Message-ID: <Pine.A41.4.44.0308260958110.123566-100000@homer34.u.washington.edu>

On Tue, 26 Aug 2003, Paul Meagher wrote:

> I know I should probably RTFM for this question, but could someone tell me
> if R supports the idea of "viewing source" on any particular function you
> want to use?
>
> If I want to "view source" on the rpois() function, for example, can I do
> somethink like:
>
> source(rpois)
>
> To see how the function is implemented?

It's even simpler in most cases, but you have picked an especially
complicated example


Just type the name of the function to see the R code
  > rpois
  function (n, lambda)
  .Internal(rpois(n, lambda))

But in this case it tells you that rpois is implemented in C code :(

 By convention, it is likely to be a function called do_rpois,
 however in this case we have an exception to the convention.  You can
look in src/main.names.c for the name of the C function.

abacus% fgrep rpois names.c
{"rpois",       do_random1,     3,      11,     2,      {PP_FUNCALL,
PREC_FN,0}},

so the function do_random1 is actually involved. If you grep for that, you
are directed to src/main/random.c and you will eventually find that the
underlying code is in src/nmath/rpois.c and is commented with a reference
to the ACM Transactions on Mathematical Software.


	-thomas



From paul at datavore.com  Tue Aug 26 20:52:41 2003
From: paul at datavore.com (Paul Meagher)
Date: Tue, 26 Aug 2003 15:52:41 -0300
Subject: [R] Viewing function source
References: <Pine.A41.4.44.0308260958110.123566-100000@homer34.u.washington.edu>
Message-ID: <008101c36c03$3f18d6c0$9141de18@computer>

> Just type the name of the function to see the R code
>   > rpois
>   function (n, lambda)
>   .Internal(rpois(n, lambda))
>
> But in this case it tells you that rpois is implemented in C code :(
>
>  By convention, it is likely to be a function called do_rpois,
>  however in this case we have an exception to the convention.  You can
> look in src/main.names.c for the name of the C function.
>
> abacus% fgrep rpois names.c
> {"rpois",       do_random1,     3,      11,     2,      {PP_FUNCALL,
> PREC_FN,0}},
>
> so the function do_random1 is actually involved. If you grep for that, you
> are directed to src/main/random.c and you will eventually find that the
> underlying code is in src/nmath/rpois.c and is commented with a reference
> to the ACM Transactions on Mathematical Software.

Thank you for helping me with the research on this.

I am interested in the rpois source in particular because I am wanting to
implement a compact algorithm for it.

The rpois.c code is estimated at around 140 lines of code when you subtract
out line spaces and commenting.   Fairly inscrutable if you haven't read the
article or don't have a mathematical programming background.

I had hoped that I would see a compact algorithm of around 20 lines of code
based on a counting process sensitive to a lambda parameter.  Instead it
uses a normal deviates method that is probably much more optimal in many
mathematical senses.

Personally, I would like to see a counting process implementation of an
poisson random number generator.  I suspect it would be much slower than
rpois.c (because it would likely depend upon setting a num_frames iteration
counter) and less accurate, but would be much more compact and give more
intuitive insight and understanding of a numerical process that generates
the poisson random deviates.  I am not very fluent yet in R programming to
attempt this.  I could take a stab at it with PHP which I am much more
fluent in if I am not barking up the wrong tree on this conjecture.

Regards,
Paul Meagher


>
> -thomas
>
>



From abunn at montana.edu  Tue Aug 26 20:53:45 2003
From: abunn at montana.edu (Andy Bunn)
Date: Tue, 26 Aug 2003 12:53:45 -0600
Subject: [R] Getting out of an embedded function safely - use try?
Message-ID: <000001c36c03$721f6e30$78f05a99@msu.montana.edu>

Helpers.

An instrument sends me data that is mostly nonlinear. I have a group of
functions to manipulate this data so that it is useful to the user. One
of them involves a nls model that called to fit a line to the data and
returns the fits. This works well 99 out of 100 times. Occasionally, the
sensor sends some bad data to which a nls model cannot be fit. When that
happens, the nls function cannot resolve and quits. This breaks the loop
and ends the processing.

The problem is that the nls model is the middle of three functions that
gathers, detrends, and collates n observations before returning a mean
value function of several data series.

I want to make this program robust to bad data. So, if a bad data series
is encountered I want the nls model to return an NA (or something like
that). Is that possible? 

Should I use try() and change the error option? 

Or incorporate an inherits() within an if statement?

Should I screen the data to check to see if it's well behaved before
getting into nls?

Thanks for any tips, Andy


#example

foo.one <- (1 * exp(-0.07 * 1:100) + 0.5) + rnorm(100, 0, 0.05)
foo.two <- (1 * exp(-0.000000001 * 1:100) + 0.5) + rnorm(100, 0, 0.05)

try.to.fit <- function(aVector){
toy.model <- nls(aVector ~ FitA * exp(FitNegB * 1:length(aVector)) +
FitK, 
              start = list( FitA = 0.75, FitNegB = -0.02, FitK = 0.4), 
              control = nls.control(maxiter=1000, tol=1e-05,
minFactor=1/1024))
return(fitted.values(toy.model))
}


try.to.fit(foo.one)
try.to.fit(foo.two)



From Mark.Lamias at grizzard.com  Tue Aug 26 20:56:24 2003
From: Mark.Lamias at grizzard.com (Mark Lamias)
Date: Tue, 26 Aug 2003 14:56:24 -0400
Subject: [R] Mann-Whitney U Table
Message-ID: <BF3C804D59EE4C40BF5376A0FABA4AA62F648C@atl_mail.griz-main.com>

Does anyone have a piece of code or know how I can use R to generate a table
of
critical values for the Mann-Whitney (aka Wilcoxon Rank Sum) test.

Ideally, I'd like a table that contains the critical values for any two
samples of size 3 through 30.  I could use Monte Carlo simulation or the
normal approximation when n1 and n2 are greater than, 10, but I figured
someone may know how to calculate these exactly.

Thanks.

Sincerely yours,

Mark J. Lamias
Statistical Consultant
Grizzard Agency
229 Peachtree Street - 12th Floor
Atlanta, GA 30303



From ripley at stats.ox.ac.uk  Tue Aug 26 21:01:36 2003
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Tue, 26 Aug 2003 20:01:36 +0100 (BST)
Subject: [R] Poisson generation (was Viewing function source)
In-Reply-To: <008101c36c03$3f18d6c0$9141de18@computer>
Message-ID: <Pine.LNX.4.44.0308261957100.1609-100000@gannet.stats>

See any good book on simulation: one from 1987 springs to mind.

The hard issue is to find a method that works near optimally for all
values of lambda, especially moderately large ones.  No simple algorithm
does that, and for something like R, the issue is not compactness but good 
worst-case behaviour.

I do think you will learn more by reading a book than examples of code, at 
least until you know the monographs backwards (and forwards).

On Tue, 26 Aug 2003, Paul Meagher wrote:

> > Just type the name of the function to see the R code
> >   > rpois
> >   function (n, lambda)
> >   .Internal(rpois(n, lambda))
> >
> > But in this case it tells you that rpois is implemented in C code :(
> >
> >  By convention, it is likely to be a function called do_rpois,
> >  however in this case we have an exception to the convention.  You can
> > look in src/main.names.c for the name of the C function.
> >
> > abacus% fgrep rpois names.c
> > {"rpois",       do_random1,     3,      11,     2,      {PP_FUNCALL,
> > PREC_FN,0}},
> >
> > so the function do_random1 is actually involved. If you grep for that, you
> > are directed to src/main/random.c and you will eventually find that the
> > underlying code is in src/nmath/rpois.c and is commented with a reference
> > to the ACM Transactions on Mathematical Software.
> 
> Thank you for helping me with the research on this.
> 
> I am interested in the rpois source in particular because I am wanting to
> implement a compact algorithm for it.
> 
> The rpois.c code is estimated at around 140 lines of code when you subtract
> out line spaces and commenting.   Fairly inscrutable if you haven't read the
> article or don't have a mathematical programming background.
> 
> I had hoped that I would see a compact algorithm of around 20 lines of code
> based on a counting process sensitive to a lambda parameter.  Instead it
> uses a normal deviates method that is probably much more optimal in many
> mathematical senses.
> 
> Personally, I would like to see a counting process implementation of an
> poisson random number generator.  I suspect it would be much slower than
> rpois.c (because it would likely depend upon setting a num_frames iteration
> counter) and less accurate, but would be much more compact and give more
> intuitive insight and understanding of a numerical process that generates

One of the possible processes.

> the poisson random deviates.  I am not very fluent yet in R programming to
> attempt this.  I could take a stab at it with PHP which I am much more
> fluent in if I am not barking up the wrong tree on this conjecture.

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From andy_liaw at merck.com  Tue Aug 26 21:17:19 2003
From: andy_liaw at merck.com (Liaw, Andy)
Date: Tue, 26 Aug 2003 15:17:19 -0400
Subject: [R] R on Linux/Opteron?
Message-ID: <3A822319EB35174CA3714066D590DCD50205CA49@usrymx25.merck.com>

Dear R-help:

Has anyone tried using R on the the AMD Opteron in either 64- or 32-bit
mode?  If so, any good/bad experiences, comments, etc?  We are considering
getting this hardware, and would like to know if R can run smoothly on such
a beast.  Any comment much appreciated.

Best,
Andy

Andy Liaw, PhD
Biometrics Research      PO Box 2000, RY33-300     
Merck Research Labs           Rahway, NJ 07065
mailto:andy_liaw at merck.com <mailto:andy_liaw at merck.com>         732-594-0820



------------------------------------------------------------------------------
Notice:  This e-mail message, together with any attachments,...{{dropped}}



From andy_liaw at merck.com  Tue Aug 26 21:25:25 2003
From: andy_liaw at merck.com (Liaw, Andy)
Date: Tue, 26 Aug 2003 15:25:25 -0400
Subject: [R] Mann-Whitney U Table
Message-ID: <3A822319EB35174CA3714066D590DCD50205CA4A@usrymx25.merck.com>

See ?qwilcox.

Andy

> -----Original Message-----
> From: Mark Lamias [mailto:Mark.Lamias at grizzard.com] 
> Sent: Tuesday, August 26, 2003 2:56 PM
> To: 'r-help at lists.r-project.org'
> Cc: Mark Lamias
> Subject: [R] Mann-Whitney U Table
> 
> 
> Does anyone have a piece of code or know how I can use R to 
> generate a table of critical values for the Mann-Whitney (aka 
> Wilcoxon Rank Sum) test.
> 
> Ideally, I'd like a table that contains the critical values 
> for any two samples of size 3 through 30.  I could use Monte 
> Carlo simulation or the normal approximation when n1 and n2 
> are greater than, 10, but I figured someone may know how to 
> calculate these exactly.
> 
> Thanks.
> 
> Sincerely yours,
> 
> Mark J. Lamias
> Statistical Consultant
> Grizzard Agency
> 229 Peachtree Street - 12th Floor
> Atlanta, GA 30303
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list 
> https://www.stat.math.ethz.ch/mailman/listinfo> /r-help
> 

------------------------------------------------------------------------------
Notice:  This e-mail message, together with any attachments,...{{dropped}}



From parkhurs at ariel.ucs.indiana.edu  Tue Aug 26 21:36:48 2003
From: parkhurs at ariel.ucs.indiana.edu (David Parkhurst)
Date: Tue, 26 Aug 2003 14:36:48 -0500
Subject: [R] rfImpute (for randomForest) crashed
Message-ID: <001b01c36c09$c26fd400$0a6cfea9@BLSPEAPARKHOM>

In trying to execute this line in R (Version 1.7.1  (2003-06-16), under
windows XP pro), with the randomForest library (about two weeks old) loaded,
the program crashed:

bost4rf <- rfImpute(TargetDensity~.,data=bost4rf0)

Specifically, an XP dialog box popped up, saying ?R for windows GUI
front-end has encountered a problem and needs to close.?  That was the
dialog saying asking whether I wanted to send a report to Microsoft (which I
chose not to do).

When I clicked on ?To see what data this error report contains,? the gist
was:

Error signature

appname: rgui.exe;  Appver: 1.71.30616.0;  Modname: randomforest.dll

Modver: 1.70.30416.0;   Offset:  00002bac

I?d appreciate any help anyone can provide in tracking down why this is
happening.  (I successfully used rfImpute earlier on a similar dataset from
another site.)

Thanks.  --Dave Parkhurst



From yao6889 at msmailhub.oulan.ou.edu  Tue Aug 26 21:35:25 2003
From: yao6889 at msmailhub.oulan.ou.edu (Yao, Minghua)
Date: Tue, 26 Aug 2003 14:35:25 -0500
Subject: [R] Package for Numerical Integral?
Message-ID: <FC0CEBD77311DA499A67ADB355A24FA20396ADA2@mail4.oulan.ou.edu>


Dear all,

Is there any package for numerically calculating an integral? Thanks in
advance.

-Minghua



From bates at stat.wisc.edu  Tue Aug 26 21:47:19 2003
From: bates at stat.wisc.edu (Douglas Bates)
Date: Tue, 26 Aug 2003 19:47:19 -0000
Subject: [R] Getting out of an embedded function safely - use try?
In-Reply-To: <000001c36c03$721f6e30$78f05a99@msu.montana.edu>
References: <000001c36c03$721f6e30$78f05a99@msu.montana.edu>
Message-ID: <6roeyc19rp.fsf@bates4.stat.wisc.edu>

"Andy Bunn" <abunn at montana.edu> writes:

> Helpers.
> 
> An instrument sends me data that is mostly nonlinear. I have a group of
> functions to manipulate this data so that it is useful to the user. One
> of them involves a nls model that called to fit a line to the data and
> returns the fits. This works well 99 out of 100 times. Occasionally, the
> sensor sends some bad data to which a nls model cannot be fit. When that
> happens, the nls function cannot resolve and quits. This breaks the loop
> and ends the processing.
> 
> The problem is that the nls model is the middle of three functions that
> gathers, detrends, and collates n observations before returning a mean
> value function of several data series.
> 
> I want to make this program robust to bad data. So, if a bad data series
> is encountered I want the nls model to return an NA (or something like
> that). Is that possible? 

Yes

> Should I use try() and change the error option? 

> Or incorporate an inherits() within an if statement?

Both.  Check the code in the formula method for the generic function
nlsList in the nlme package for an example

    val <- lapply(split(data, groups), function(dat, formula, 
        start, control, first = TRUE) {
        ans <- try({
            data <- as.data.frame(dat)
            if (is.null(start)) {
                nls(formula = formula, data = data, control = control)
            }
            else {
                nls(formula = formula, data = data, start = start, 
                  control = control)
            }
        })
        if (inherits(ans, "try-error")) 
            NULL
        else ans
    }, formula = model, start = start, control = controlvals)


-- 
Douglas Bates                            bates at stat.wisc.edu
Statistics Department                    608/262-2598
University of Wisconsin - Madison        http://www.stat.wisc.edu/~bates/



From luke at stat.uiowa.edu  Tue Aug 26 21:55:47 2003
From: luke at stat.uiowa.edu (Luke Tierney)
Date: Tue, 26 Aug 2003 14:55:47 -0500 (CDT)
Subject: [R] R on Linux/Opteron?
In-Reply-To: <3A822319EB35174CA3714066D590DCD50205CA49@usrymx25.merck.com>
Message-ID: <Pine.LNX.4.44.0308261452480.21059-100000@itasca.stat.uiowa.edu>

I've had a chance to build R-devel on one running SuSE (not sure which
release). confugure set things up for a 64-bit build that passed all
tests.  The base tests ran about 25% faster on the 1.4GHz opteron than
a2GHz Xeon. That's as much as I know at this point.

luke

On Tue, 26 Aug 2003, Liaw, Andy wrote:

> Dear R-help:
> 
> Has anyone tried using R on the the AMD Opteron in either 64- or 32-bit
> mode?  If so, any good/bad experiences, comments, etc?  We are considering
> getting this hardware, and would like to know if R can run smoothly on such
> a beast.  Any comment much appreciated.
> 
> Best,
> Andy
> 
> Andy Liaw, PhD
> Biometrics Research      PO Box 2000, RY33-300     
> Merck Research Labs           Rahway, NJ 07065
> mailto:andy_liaw at merck.com <mailto:andy_liaw at merck.com>         732-594-0820
> 
> 
> 
> ------------------------------------------------------------------------------
> Notice:  This e-mail message, together with any attachments,...{{dropped}}
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> 

-- 
Luke Tierney
University of Iowa                  Phone:             319-335-3386
Department of Statistics and        Fax:               319-335-3017
   Actuarial Science
241 Schaeffer Hall                  email:      luke at stat.uiowa.edu
Iowa City, IA 52242                 WWW:  http://www.stat.uiowa.edu



From andy_liaw at merck.com  Tue Aug 26 21:55:48 2003
From: andy_liaw at merck.com (Liaw, Andy)
Date: Tue, 26 Aug 2003 15:55:48 -0400
Subject: [R] rfImpute (for randomForest) crashed
Message-ID: <3A822319EB35174CA3714066D590DCD50205CA4C@usrymx25.merck.com>

Dave,

If possbile, please send me (privately) the data that caused the crash and
I'll have a look.  

Andy

> -----Original Message-----
> From: David Parkhurst [mailto:parkhurs at ariel.ucs.indiana.edu] 
> Sent: Tuesday, August 26, 2003 3:37 PM
> To: r-help at stat.math.ethz.ch
> Subject: [R] rfImpute (for randomForest) crashed
> 
> 
> In trying to execute this line in R (Version 1.7.1  
> (2003-06-16), under windows XP pro), with the randomForest 
> library (about two weeks old) loaded, the program crashed:
> 
> bost4rf <- rfImpute(TargetDensity~.,data=bost4rf0)
> 
> Specifically, an XP dialog box popped up, saying "R for 
> windows GUI front-end has encountered a problem and needs to 
> close."  That was the dialog saying asking whether I wanted 
> to send a report to Microsoft (which I chose not to do).
> 
> When I clicked on "To see what data this error report 
> contains," the gist
> was:
> 
> Error signature
> 
> appname: rgui.exe;  Appver: 1.71.30616.0;  Modname: randomforest.dll
> 
> Modver: 1.70.30416.0;   Offset:  00002bac
> 
> I'd appreciate any help anyone can provide in tracking down 
> why this is happening.  (I successfully used rfImpute earlier 
> on a similar dataset from another site.)
> 
> Thanks.  --Dave Parkhurst
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list 
> https://www.stat.math.ethz.ch/mailman/listinfo> /r-help
> 

------------------------------------------------------------------------------
Notice:  This e-mail message, together with any attachments,...{{dropped}}



From nikita.jablonsky at etrade.com  Tue Aug 26 22:09:13 2003
From: nikita.jablonsky at etrade.com (Jablonsky, Nikita)
Date: Tue, 26 Aug 2003 13:09:13 -0700
Subject: [R] matching-case sensitivity
Message-ID: <927ABE8FACE5BB469BAB78229C8B884E9349DC@atl1ex1.corp.etradegrp.com>

Hi All,

I am trying to match two character arrays (email lists) using either
pmatch(), match() or charmatch() functions. However the function is
"missing" some matches due to differences in the cases of some letters
between the two arrays. Is there any way to disable case sensitivity or is
there an entirely better way to match two character arrays that have
identical entries but written in different case?

Thanks
Nikita



From tlumley at u.washington.edu  Tue Aug 26 22:26:15 2003
From: tlumley at u.washington.edu (Thomas Lumley)
Date: Tue, 26 Aug 2003 13:26:15 -0700 (PDT)
Subject: [R] Viewing function source
In-Reply-To: <008101c36c03$3f18d6c0$9141de18@computer>
Message-ID: <Pine.A41.4.44.0308261257510.123566-100000@homer34.u.washington.edu>

On Tue, 26 Aug 2003, Paul Meagher wrote:

>
> Personally, I would like to see a counting process implementation of an
> poisson random number generator.  I suspect it would be much slower than
> rpois.c (because it would likely depend upon setting a num_frames iteration
> counter) and less accurate, but would be much more compact and give more
> intuitive insight and understanding of a numerical process that generates
> the poisson random deviates.  I am not very fluent yet in R programming to
> attempt this.  I could take a stab at it with PHP which I am much more
> fluent in if I am not barking up the wrong tree on this conjecture.
>

You can generate Poisson random numbers from a Poisson process like this:

rfishy<-function(lambda){
    t <- 0
    i <- -1
    while(t<=lambda){
        t<-t-log(runif(1))
        i<-i+1
    }
    return(i)
}

The name of this generator is descriptive, not a pub. It is very slow for
large lambda, and incorrect for extremely large lambda (and possibly for
extremely small lambda). If you only wanted a fairly small number of
random variates with, say, 1e-6<lambda<100, then it's not too bad.

But why would anyone *want* to code their own Poisson random number
generator, except perhaps as an interesting student exercise?


	-thomas



From tlumley at u.washington.edu  Tue Aug 26 22:27:48 2003
From: tlumley at u.washington.edu (Thomas Lumley)
Date: Tue, 26 Aug 2003 13:27:48 -0700 (PDT)
Subject: [R] matching-case sensitivity
In-Reply-To: <927ABE8FACE5BB469BAB78229C8B884E9349DC@atl1ex1.corp.etradegrp.com>
Message-ID: <Pine.A41.4.44.0308261326590.123566-100000@homer34.u.washington.edu>

On Tue, 26 Aug 2003, Jablonsky, Nikita wrote:

> Hi All,
>
> I am trying to match two character arrays (email lists) using either
> pmatch(), match() or charmatch() functions. However the function is
> "missing" some matches due to differences in the cases of some letters
> between the two arrays. Is there any way to disable case sensitivity or is
> there an entirely better way to match two character arrays that have
> identical entries but written in different case?
>

You could use tolower() or toupper() to remove case differences.

	-thomas



From MSchwartz at medanalytics.com  Tue Aug 26 22:42:42 2003
From: MSchwartz at medanalytics.com (Marc Schwartz)
Date: Tue, 26 Aug 2003 15:42:42 -0500
Subject: [R] matching-case sensitivity
In-Reply-To: <927ABE8FACE5BB469BAB78229C8B884E9349DC@atl1ex1.corp.etradegrp.com>
References: <927ABE8FACE5BB469BAB78229C8B884E9349DC@atl1ex1.corp.etradegrp.com>
Message-ID: <1061930561.13382.36.camel@localhost>

On Tue, 2003-08-26 at 15:09, Jablonsky, Nikita wrote:
> Hi All,
> 
> I am trying to match two character arrays (email lists) using either
> pmatch(), match() or charmatch() functions. However the function is
> "missing" some matches due to differences in the cases of some letters
> between the two arrays. Is there any way to disable case sensitivity or is
> there an entirely better way to match two character arrays that have
> identical entries but written in different case?
> 
> Thanks
> Nikita


At least two options for case insensitive matching:

1. use grep(), which has an 'ignore.case' argument that you can set to
TRUE. See ?grep

2. use the function toupper() to convert both character vectors to all
upper case. See ?toupper.  Conversely, tolower() would do the opposite.


A quick solution using the second option would be:

Vector1[toupper(Vector1) %in% toupper(Vector2)]

which would return the elements that match in both vectors.


A more formal example with some data:

Vector1 <- letters[1:10]
Vector1
[1] "a" "b" "c" "d" "e" "f" "g" "h" "i" "j"


Vector2 <- c(toupper(letters[5:8]), letters[9:15])
Vector2
[1] "E" "F" "G" "H" "i" "j" "k" "l" "m" "n" "o"


Vector1[toupper(Vector1) %in% toupper(Vector2)]
[1] "e" "f" "g" "h" "i" "j"


HTH,

Marc Schwartz



From ross at biostat.ucsf.edu  Tue Aug 26 22:58:26 2003
From: ross at biostat.ucsf.edu (Ross Boylan)
Date: Tue, 26 Aug 2003 13:58:26 -0700
Subject: [R] Seeking Packaging advice
Message-ID: <1061931506.1818.36.camel@iron.libaux.ucsf.edu>

I have two questions about packaging up code.

1) Weave/tangle advisable?
In the course of extending some C code already in S, I had to work out
the underlying math.  It seems to me useful to keep this information
with the code, using Knuth's tangle/weave type tools.  I know there is
some support for this in R code, but my question is about the wisdow of
doing this with C (or Fortran, or other source) code.

Against the advantage of having the documentation and code nicely
integrated are the drawbacks of added complexity in the build process
and portability concerns.  Some of this is mitigated by the existing
dependence on TeX.

An intermediate approach would be to provide both the web (in the Knuth
sense) source and the C output; the latter could be used directly by
those not wishing to hassle with web.  This isn't ideal, since the
resulting C is likely to be a bit cryptic, and if someone edits the C
without changing the web source confusion will reign.

So do people have any thoughts about whether introducing this is a step
forward or back?

2) Modifications of existing packages.
I modified the survival package (I'm not sure if that's properly called
a "base" package, but it's close).  I know in this particular case, if
I'm serious, I probably should contact the package maintainer.  But this
kind of operation will probably be pretty common for me; I imagine many
on this list have already done it.  In general, is the best thing to do
a) package the new routines as a small additional package, with a
dependence on the base package if necessary (the particular change I've
made actually produces a few distinct files, slight tweaks of existing
ones, that can stand on their own)
b) package the new things in with the old under the same name as the old
(obviously requires working with package maintainter)
c) package the new things with the old and give it a new name.

I'm also curious about what development strategy is best; I did b), and
it seemed to work OK.  But I kept expecting it to cause disaster (it
probably helped that I usually didn't load the baseline survival
packages; clearly that wouldn't be an option if working with one of the
automatically loaded packages).

Thanks.
-- 
Ross Boylan                                      wk:  (415) 502-4031
530 Parnassus Avenue (Library) rm 115-4          ross at biostat.ucsf.edu
Dept of Epidemiology and Biostatistics           fax: (415) 476-9856
University of California, San Francisco
San Francisco, CA 94143-0840                     hm:  (415) 550-1062



From stormplot at hotmail.com  Tue Aug 26 23:08:26 2003
From: stormplot at hotmail.com (Jason Fisher)
Date: Tue, 26 Aug 2003 14:08:26 -0700
Subject: [R] GWplot
Message-ID: <Law9-F74Eb1VqjaFoTo000116b8@hotmail.com>

For anyone wishing to see R-Tcl/Tk-MySQL in action (Windows XP)...

http://moffett.isis.ucla.edu/gwplot/

Examples were by far the most useful learning tool during my programming 
endeavors so I hope this may help in your own projects.

Thanks,
Jason

_________________________________________________________________
Enter for your chance to IM with Bon Jovi, Seal, Bow Wow, or Mary J Blige 
using MSN Messenger http://entertainment.msn.com/imastar



From edd at debian.org  Tue Aug 26 23:28:05 2003
From: edd at debian.org (Dirk Eddelbuettel)
Date: Tue, 26 Aug 2003 16:28:05 -0500
Subject: [R] R on Linux/Opteron?
In-Reply-To: <3A822319EB35174CA3714066D590DCD50205CA49@usrymx25.merck.com>
References: <3A822319EB35174CA3714066D590DCD50205CA49@usrymx25.merck.com>
Message-ID: <20030826212805.GA2084@sonny.eddelbuettel.com>

On Tue, Aug 26, 2003 at 03:17:19PM -0400, Liaw, Andy wrote:
> Has anyone tried using R on the the AMD Opteron in either 64- or 32-bit
> mode?  If so, any good/bad experiences, comments, etc?  We are considering
> getting this hardware, and would like to know if R can run smoothly on such
> a beast.  Any comment much appreciated.

http://buildd.debian.org/build.php?&pkg=r-base&arch=ia64&file=log

has logs of R builds on ia64 since Nov 2001, incl. the outcome of make
check. We do not run the torture tests -- though I guess we could on some of
the beefier hardware such as ia64. 

Dirk

-- 
Those are my principles, and if you don't like them... well, I have others.
                                                -- Groucho Marx



From spencer.graves at pdf.com  Tue Aug 26 23:42:24 2003
From: spencer.graves at pdf.com (Spencer Graves)
Date: Tue, 26 Aug 2003 14:42:24 -0700
Subject: [R] Package for Numerical Integral?
References: <FC0CEBD77311DA499A67ADB355A24FA20396ADA2@mail4.oulan.ou.edu>
Message-ID: <3F4BD440.405@pdf.com>

Did you consider "integrate"?

hope this helps.  spencer graves

Yao, Minghua wrote:
> Dear all,
> 
> Is there any package for numerically calculating an integral? Thanks in
> advance.
> 
> -Minghua
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help



From spencer.graves at pdf.com  Tue Aug 26 23:50:11 2003
From: spencer.graves at pdf.com (Spencer Graves)
Date: Tue, 26 Aug 2003 14:50:11 -0700
Subject: [R] matching-case sensitivity
References: <927ABE8FACE5BB469BAB78229C8B884E9349DC@atl1ex1.corp.etradegrp.com>
	<1061930561.13382.36.camel@localhost>
Message-ID: <3F4BD613.3080507@pdf.com>

Alternatively, you could use "casefold".  This would make your code more 
compatible with S-Plus.  For me, "toupper" and "tolower" are easier 
names to remember and easier to read.  However, if you think that 
someone might want to try using your code with S-Plus, then "casefold" 
might be the better choice.

hope this helps.  spencer graves

Marc Schwartz wrote:
> On Tue, 2003-08-26 at 15:09, Jablonsky, Nikita wrote:
> 
>>Hi All,
>>
>>I am trying to match two character arrays (email lists) using either
>>pmatch(), match() or charmatch() functions. However the function is
>>"missing" some matches due to differences in the cases of some letters
>>between the two arrays. Is there any way to disable case sensitivity or is
>>there an entirely better way to match two character arrays that have
>>identical entries but written in different case?
>>
>>Thanks
>>Nikita
> 
> 
> 
> At least two options for case insensitive matching:
> 
> 1. use grep(), which has an 'ignore.case' argument that you can set to
> TRUE. See ?grep
> 
> 2. use the function toupper() to convert both character vectors to all
> upper case. See ?toupper.  Conversely, tolower() would do the opposite.
> 
> 
> A quick solution using the second option would be:
> 
> Vector1[toupper(Vector1) %in% toupper(Vector2)]
> 
> which would return the elements that match in both vectors.
> 
> 
> A more formal example with some data:
> 
> Vector1 <- letters[1:10]
> Vector1
> [1] "a" "b" "c" "d" "e" "f" "g" "h" "i" "j"
> 
> 
> Vector2 <- c(toupper(letters[5:8]), letters[9:15])
> Vector2
> [1] "E" "F" "G" "H" "i" "j" "k" "l" "m" "n" "o"
> 
> 
> Vector1[toupper(Vector1) %in% toupper(Vector2)]
> [1] "e" "f" "g" "h" "i" "j"
> 
> 
> HTH,
> 
> Marc Schwartz
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help



From p.dalgaard at biostat.ku.dk  Tue Aug 26 23:50:21 2003
From: p.dalgaard at biostat.ku.dk (Peter Dalgaard BSA)
Date: Tue, 26 Aug 2003 21:50:21 -0000
Subject: [R] R on Linux/Opteron?
In-Reply-To: <20030826212805.GA2084@sonny.eddelbuettel.com>
References: <3A822319EB35174CA3714066D590DCD50205CA49@usrymx25.merck.com>
	<20030826212805.GA2084@sonny.eddelbuettel.com>
Message-ID: <x2n0dw3wt7.fsf@biostat.ku.dk>

Dirk Eddelbuettel <edd at debian.org> writes:

> On Tue, Aug 26, 2003 at 03:17:19PM -0400, Liaw, Andy wrote:
> > Has anyone tried using R on the the AMD Opteron in either 64- or 32-bit
> > mode?  If so, any good/bad experiences, comments, etc?  We are considering
> > getting this hardware, and would like to know if R can run smoothly on such
> > a beast.  Any comment much appreciated.
> 
> http://buildd.debian.org/build.php?&pkg=r-base&arch=ia64&file=log
> 
> has logs of R builds on ia64 since Nov 2001, incl. the outcome of make
> check. We do not run the torture tests -- though I guess we could on some of
> the beefier hardware such as ia64. 

I don't think that's quite the same beast, though. Opterons are the
"x86-64" (or amd64) architecture and ia64 is Intel's, aka Itanium.
Debian appears to be just warming up to including this architecture:
http://lists.debian.org/debian-x86-64/2003/debian-x86-64-200308/threads.html
whereas they have had ia64 out for a while.

SuSE has an Opteron option and Luke said he tried it. Apparently it
has a functioning 64-bit compiler toolchain - I weren't sure earlier
whether they were just running a 64bit kernel and 32bit applications,
but when Luke says so, I believe it...

-- 
   O__  ---- Peter Dalgaard             Blegdamsvej 3  
  c/ /'_ --- Dept. of Biostatistics     2200 Cph. N   
 (*) \(*) -- University of Copenhagen   Denmark      Ph: (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)             FAX: (+45) 35327907



From Ted.Harding at nessie.mcc.ac.uk  Wed Aug 27 00:38:09 2003
From: Ted.Harding at nessie.mcc.ac.uk ( (Ted Harding))
Date: Tue, 26 Aug 2003 23:38:09 +0100 (BST)
Subject: [R] Viewing function source
In-Reply-To: <Pine.A41.4.44.0308261257510.123566-100000@homer34.u.washington.edu>
Message-ID: <XFMail.030826233809.Ted.Harding@nessie.mcc.ac.uk>

On 26-Aug-03 Thomas Lumley wrote:
> You can generate Poisson random numbers from a Poisson process like
> this:
> 
> rfishy<-function(lambda){
>     t <- 0
>     i <- -1
>     while(t<=lambda){
>         t<-t-log(runif(1))
>         i<-i+1
>     }
>     return(i)
> }

You can of course easily vectorise this:

  rfishy<-function(n,lambda){
    t<-0*rep(1,n)
    i<-(-1)*rep(1,n)
    while(any(t<=lambda)){
      u<-(t<=lambda)
      t<-t-log(runif(n))*u
      i<-i+1*u
    }
    return(i)
  }

> The name of this generator is descriptive, not a pub. It is very slow
> for large lambda, and incorrect for extremely large lambda (and
> possibly for extremely small lambda).

Not sure why it should be incorrect. It's certainly not theoretically
incorrect. Rounding errors? Problem with runif()?

> If you only wanted a fairly small number of random variates with, say,
> 1e-6<lambda<100, then it's not too bad.

In the above vectorised form, if it's fast for n=1 it stays pretty fast
for large n: try it with z<-rfishy(10000,5); even rfishy(100000,5) only
takes a few seconds.

Best wishes,
Ted.




--------------------------------------------------------------------
E-Mail: (Ted Harding) <Ted.Harding at nessie.mcc.ac.uk>
Fax-to-email: +44 (0)870 167 1972
Date: 26-Aug-03                                       Time: 23:38:09
------------------------------ XFMail ------------------------------



From paul at datavore.com  Wed Aug 27 01:17:08 2003
From: paul at datavore.com (Paul Meagher)
Date: Tue, 26 Aug 2003 20:17:08 -0300
Subject: [R] Generating routine for Poisson random numbers
References: <Pine.A41.4.44.0308261257510.123566-100000@homer34.u.washington.edu>
Message-ID: <009501c36c28$2cd2a5c0$9141de18@computer>

> You can generate Poisson random numbers from a Poisson process like this:
>
> rfishy<-function(lambda){
>     t <- 0
>     i <- -1
>     while(t<=lambda){
>         t<-t-log(runif(1))
>         i<-i+1
>     }
>     return(i)
> }

This is a nice compact algorithm for generating Poisson random numbers.  It
appears to work.  I am used to seeing a Poisson counting process implemented
using a "num frames" parameter, a success probability per frame, and
counting the unform numbers in the 0 to 1 range that fall below the success
probability over "num frames".  It is interesting to see an implementation
that bypasses having to use a num_frames parameter, just the lambda value,
and relies instead on incrementing a t value on each iteration and counting
the number of times you can do so (while t <= lambda).

> The name of this generator is descriptive, not a pub. It is very slow for
> large lambda, and incorrect for extremely large lambda (and possibly for
> extremely small lambda). If you only wanted a fairly small number of
> random variates with, say, 1e-6<lambda<100, then it's not too bad.

One could impose a lambda range check so that you can only invoke the
function using a lamba range where the Poisson RNG is expected to be
reasonably accurate.  The range you are giving is probably the most commonly
used range where a Poisson random number generator might be used?  Brian
Ripley also mentioned that the counting process based implementation would
not work well for large lambdas.  Do you encounter such large lambdas in
practice?   Can't you always, in theory, avoid such large lambdas by
changing the size of the time interval you want to consider?

> But why would anyone *want* to code their own Poisson random number
generator, except perhaps as an interesting student exercise?

Yes this is meant as an interesting exercise for someone who wants to
understand how to implement probability distributions in an object oriented
way (I am writing an article introducing people to probability modelling).
I am looking for a compact algorithm that I can easily explain to people how
it works and which will be a good enough rpois() approximation in many
cases.  I don't want to be blown out of the water for suggesting such an
algorithm to represent a Poisson RNG so if you think it is inappropriate to
learn about what how a Poisson RNG works using the above described
generating process,  then I would be interested in your views.

Thank you for your thoughts on this matter.

Regards,
Paul Meagher


>
>
> -thomas
>
>



From tlumley at u.washington.edu  Wed Aug 27 01:21:08 2003
From: tlumley at u.washington.edu (Thomas Lumley)
Date: Tue, 26 Aug 2003 16:21:08 -0700 (PDT)
Subject: [R] Viewing function source
In-Reply-To: <XFMail.030826233809.Ted.Harding@nessie.mcc.ac.uk>
Message-ID: <Pine.A41.4.44.0308261609050.123566-100000@homer34.u.washington.edu>

On Tue, 26 Aug 2003 Ted.Harding at nessie.mcc.ac.uk wrote:

> Thomas Lumley wrote:
>
> > The name of this generator is descriptive, not a pun. It is very slow
> > for large lambda, and incorrect for extremely large lambda (and
> > possibly for extremely small lambda).
>
> Not sure why it should be incorrect. It's certainly not theoretically
> incorrect. Rounding errors? Problem with runif()?

Yes. Both.  But it should work as long as lambda is much smaller than
2^53 and much larger than 2^-32 and much smaller than the period of
whatever generator you are using, so it's going to be too slow before it's
inaccurate.


> > If you only wanted a fairly small number of random variates with, say,
> > 1e-6<lambda<100, then it's not too bad.
>
> In the above vectorised form, if it's fast for n=1 it stays pretty fast
> for large n: try it with z<-rfishy(10000,5); even rfishy(100000,5) only
> takes a few seconds.

If you were going to implement in another language (which I thought was
the point) then vectorising won't help.  In R, yes.

	-thomas



From tlumley at u.washington.edu  Wed Aug 27 01:36:13 2003
From: tlumley at u.washington.edu (Thomas Lumley)
Date: Tue, 26 Aug 2003 16:36:13 -0700 (PDT)
Subject: [R] Re: Generating routine for Poisson random numbers
In-Reply-To: <009501c36c28$2cd2a5c0$9141de18@computer>
Message-ID: <Pine.A41.4.44.0308261621260.123566-100000@homer34.u.washington.edu>

>
> One could impose a lambda range check so that you can only invoke the
> function using a lamba range where the Poisson RNG is expected to be
> reasonably accurate.  The range you are giving is probably the most commonly
> used range where a Poisson random number generator might be used?  Brian
> Ripley also mentioned that the counting process based implementation would
> not work well for large lambdas.  Do you encounter such large lambdas in
> practice?   Can't you always, in theory, avoid such large lambdas by
> changing the size of the time interval you want to consider?

Personally, I'd probably change the question by approximating by a Normal
for large lambda and a Bernoulli for very small lambda.

The algorithm gets slow well before it gets inaccurate, though.

> > But why would anyone *want* to code their own Poisson random number
> generator, except perhaps as an interesting student exercise?
>
> Yes this is meant as an interesting exercise for someone who wants to
> understand how to implement probability distributions in an object oriented
> way (I am writing an article introducing people to probability modelling).
> I am looking for a compact algorithm that I can easily explain to people how
> it works and which will be a good enough rpois() approximation in many
> cases.  I don't want to be blown out of the water for suggesting such an
> algorithm to represent a Poisson RNG so if you think it is inappropriate to
> learn about what how a Poisson RNG works using the above described
> generating process,  then I would be interested in your views.

No, that's why I gave that as the exception.  There are lots of things
worth doing as a learning exercise that aren't worth doing otherwise.


I do think that in an article you should also point out to people that
there is a lot of numerical code available out there, written by people
who know a lot more than we do about what they are doing. It's often
easier than writing your own code and the results are better. One
advantage of an object-oriented approach is that you can just rip out your
implementation and slot in a new one if it is better.


	-thomas



From ok at cs.otago.ac.nz  Wed Aug 27 03:03:39 2003
From: ok at cs.otago.ac.nz (Richard A. O'Keefe)
Date: Wed, 27 Aug 2003 13:03:39 +1200 (NZST)
Subject: [R] R tools for large files
Message-ID: <200308270103.h7R13dYU208278@atlas.otago.ac.nz>

Duncan Murdoch <dmurdoch at pair.com> wrote:
	For example, if you want to read lines 1000 through 1100, you'd do it
	like this:
	
	 lines <- readLines("foo.txt", 1100)[1000:1100]

I created a dataset thus:
# file foo.awk:
BEGIN {
    s = "01"
    for (i = 2; i <= 41; i++) s = sprintf("%s %02d", s, i)
    n = (27 * 1024 * 1024) / (length(s) + 1)
    for (i = 1; i <= n; i++) print s
    exit 0
}
# shell command:
mawk -f foo.awk /dev/null >BIG

That is, each record contains 41 2-digit integers, and the number
of records was chosen so that the total size was approximately
27 dimegabytes.  The number of records turns out to be 230,175.

> system.time(v <- readLines("BIG"))
[1] 7.75 0.17 8.13 0.00 0.00
	# With BIG already in the file system cache...
> system.time(v <- readLines("BIG", 200000)[199001:200000])
[1] 11.73  0.16 12.27  0.00  0.00

What's the importance of this?
First, experiments I shall not weary you with showed that the
time to read N lines grows faster than N.
Second, if you want to select the _last_ thousand lines,
you have to read _all_ of them into memory.

For real efficiency here, what's wanted is a variant of readLines
where n is an index vector (a vector of non-negative integers,
a vector of non-positive integers, or a vector of logicals) saying
which lines should be kept.

The function that would need changing is do_readLines() in
src/main/connections.c, unfortunately I don't understand R internals
well enough to do it myself (yet).

As a matter of fact, that _still_ wouldn't yield real efficiency,
because every character would still have to be read by the modified
readLines(), and it reads characters using Rconn_fgetc(), which is
what gives readLines() its power and utility, but certainly doesn't
give it wings.  (One of the fundamental laws of efficient I/O library
design is to base it on block- or line- at-a-time transfers, not
character-at-a-time.)

The AWK program
    NR <= 199000 { next }
    {print}
    NR == 200000 { exit }
extracts lines 199001:20000 in just 0.76 seconds, about 15 times
faster.  A C program to the same effect, using fgets(), took 0.39
seconds, or about 30 times faster than R.

There are two fairly clear sources of overhead in the R code:
(1) the overhead of reading characters one at a time through Rconn_fgetc()
    instead of a block or line at a time.  mawk doesn't use fgets() for
    reading, and _does_ have the overhead of repeatedly checking a
    regular expression to determine where the end of the line is,
    which it is sensible enough to fast-path.
(2) the overhead of allocating, filling in, and keeping, a whole lot of
    memory which is of no use whatever in computing the final result.
    mawk is actually fairly careful here, and only keeps one line at
    a time in the program shown above.  Let's change it:
	NR <= 199000 {next}
	{a[NR] = $0}
	NR == 200000 {exit}
	END {for (i in a) print a[i]}
    That takes the time from 0.76 seconds to 0.80 seconds

The simplest thing that could possibly work would be to add a function
skipLines(con, n) which simply read and discarded n lines.

	 result <- scan(textConnection(lines), list( .... ))
	
> system.time(m <- scan(textConnection(v), integer(41)))
Read 41000 items
[1] 0.99 0.00 1.01 0.00 0.00

One whole second to read 41,000 numbers on a 500 MHz machine?

> vv <- rep(v, 240)

Is there any possibility of storing the data in (platform) binary form?
Binary connections (R-data.pdf, section 6.5 "Binary connections") can be
used to read binary-encoded data.

I wrote a little C program to save out the 230175 records of 41 integers
each in native binary form.  Then in R I did

> system.time(m <- readBin("BIN", integer(), n=230175*41, size=4))
[1] 0.57 0.52 1.11 0.00 0.00
> system.time(m <- matrix(data=m, ncol=41, byrow=TRUE))
[1] 2.55 0.34 2.95 0.00 0.00

Remember, this doesn't read a *sample* of the data, it reads *all*
the data.  It is so much faster than the alternatives in R that it
just isn't funny.  Trying scan() on the file took nearly 10 minutes
before I killed it the other day, using readBin() is a thousand times
faster than a simple scan() call on this particular data set.

There has *got* to be a way of either generating or saving the data
in binary form, using only "approved" Windows tools.  Heck, it can
probably be done using VBA.


By the way, I've read most of the .pdf files I could find on the CRAN site,
but haven't noticed any description of the R save-file format.  Where should
I have looked?  (Yes, I know about src/main/saveload.c; I was hoping for
some documentation, with maybe some diagrams.)



From paul at datavore.com  Wed Aug 27 04:23:48 2003
From: paul at datavore.com (Paul Meagher)
Date: Tue, 26 Aug 2003 23:23:48 -0300
Subject: [R] Re: Generating routine for Poisson random numbers
References: <Pine.A41.4.44.0308261621260.123566-100000@homer34.u.washington.edu>
Message-ID: <00a701c36c42$40e38600$9141de18@computer>

> I do think that in an article you should also point out to people that
> there is a lot of numerical code available out there, written by people
> who know a lot more than we do about what they are doing. It's often
> easier than writing your own code and the results are better. One
> advantage of an object-oriented approach is that you can just rip out your
> implementation and slot in a new one if it is better.

Exactly.  That is another reason I do not feel the need to implement the RNG
algorithm perfectly.  If someone really wants a more fool proof rpois-like
algorithm with better running time characteristics they can reimplement
method using the rpois.c normal deviates approach.

BTW, here is what my final Poisson RNG method looks like coded in PHP - it
is modelled after the JSci library approach.  I am only showing the
constructor and the RNG method:

class PoissonDistribution extends ProbabilityDistribution {

  var $lambda;

  function PoissonDistribution($lambda=1) {
    if($lambda <= 0.0) {
      die("Lamda parameter should be positive.");
    }
    $this->lambda = $interval;
  }

  function RNG($num_vals=1) {
    if ($num_vals < 1) {
      die("Number of random numbers to return must be 1 or greater");
    }
    for ($i=0; $i < $num_vals; $i++) {
      $temp  = 0;
      $count = -1;
      while($temp <= $this->lambda) {
        $rand_val = mt_rand() / mt_getrandmax();
        $temp      = $temp - log($rand_val);
        $count++;
      }
      // record count value(s)
      if ($num_vals == 1) {
        $counts = $count;
      } else {
        $counts[$i] = $count;
      }
    }
    return $counts;
  }

My simple eyeball tests indicate that the algorithm appears to generate
unbiased estimates of the expected mean and variance given lambas in the
range of .02 and 900.  I guess to confirm the unbiasedness I would need to
generate a bunch of sample estimates of the mean and variance from my
Poisson random number sequences, plot the relative frequency of these
estimates, and see if the central tendency of the estimates correspond to
the mean and variance expected theoretically for a poisson random variable
(i.e., mean and variance = lambda).

I see why the performance characteristics get bad when lambda is big - the
counting process involves more iterations.  Most of the text book examples
never use a lambda this big, often lambda is less than 100 and often not
less than .02 or so.  In other words, the typical parameter space for the
algorithm may be such that areas where it breaks down are not that common in
practice.

I think this will be a perfectly acceptable RNG for a Poisson random
variable provided you don't use unusually large or small lambda values - if
I knew the break down range, I could implement a check-range test to
disallow usage of the function for that range.

Not sure yet exactly what characteristics of the algorithm would lead it to
behave incorrectly at extremely small or large lambda values?

BTW, is this simple method of generating a poisson random number discussed
in detail in any other books or papers that I might consult?

Regards,
Paul Meagher



From sumslily at yahoo.com  Wed Aug 27 04:53:31 2003
From: sumslily at yahoo.com (Lily)
Date: Tue, 26 Aug 2003 19:53:31 -0700 (PDT)
Subject: [R] How to do leave-n-out cross validation in R? 
Message-ID: <20030827025331.53592.qmail@web41203.mail.yahoo.com>

Seems crossval from library(bootstrap) can only be
used for leave-one-out and k-fold cross validation?
Here is a dumb question, suppose n=80, how to do
exactly leave-50-out cross validation? K-fold cross
validation is not eligible for this case since
n/ngroup is not an integer. Thanks!



From gisar at nus.edu.sg  Wed Aug 27 05:37:15 2003
From: gisar at nus.edu.sg (Adaikalavan Ramasamy)
Date: Wed, 27 Aug 2003 11:37:15 +0800
Subject: [R] R tools for large files
Message-ID: <CDA8D2689259E444942B3CDED8DD912933FF3A@MBXSRV03.stf.nus.edu.sg>

If we are going to use unix tools to create a new dataset before calling
into R, why not simply use 

cat my_big_bad_file | tail +1001 | head -100

to read lines 1000-1100 (assuming one header row).

Or if you have the shortlisted rownames in one file, you can use join
after sort. A working example follows.

########################################################################
#########

#!/bin/bash

# match.sh last modified 10/07/03
# Does the same thing as egrep 'a|b|c|...' file but in batch mode
# A script that matches all occurances of <shortlist> in <data> using
the first column as common key 

if [ $# -ne 2 ]; then
   echo "Usage: ${0/*\/} <shortlist> <data>"
   exit
fi

TEMP1=/tmp/temp1.`date "+%y%m%d-%H%M%S"`
TEMP2=/tmp/temp2.`date "+%y%m%d-%H%M%S"`
TEMP3=/tmp/temp3.`date "+%y%m%d-%H%M%S"`
TEMP4=/tmp/temp4.`date "+%y%m%d-%H%M%S"`
TEMP5=/tmp/temp5.`date "+%y%m%d-%H%M%S"`

grep -n . $1 | cut -f1 -d: | paste - $1 > $TEMP1
sort -k 2 $TEMP1 > $TEMP2             

tail +2 $2 | sort -k 1 > $TEMP3  # Assume data file has header 

headerRow=`head -1 $2`

join -j1 2 -j2 1 -a 1 -t\        $TEMP2 $TEMP3 > $TEMP4
sort -n -k 2 $TEMP4 > $TEMP5

/bin/echo "$headerRow"
cut -f1,3- $TEMP5                # column 2 contains orderings

rm $TEMP1 $TEMP2 $TEMP3 $TEMP4

########################################################################
#####



-----Original Message-----
From: Richard A. O'Keefe [mailto:ok at cs.otago.ac.nz] 
Sent: Wednesday, August 27, 2003 9:04 AM
To: r-help at stat.math.ethz.ch
Subject: Re: [R] R tools for large files


Duncan Murdoch <dmurdoch at pair.com> wrote:
	For example, if you want to read lines 1000 through 1100, you'd
do it
	like this:
	
	 lines <- readLines("foo.txt", 1100)[1000:1100]

I created a dataset thus:
# file foo.awk:
BEGIN {
    s = "01"
    for (i = 2; i <= 41; i++) s = sprintf("%s %02d", s, i)
    n = (27 * 1024 * 1024) / (length(s) + 1)
    for (i = 1; i <= n; i++) print s
    exit 0
}
# shell command:
mawk -f foo.awk /dev/null >BIG

That is, each record contains 41 2-digit integers, and the number of
records was chosen so that the total size was approximately 27
dimegabytes.  The number of records turns out to be 230,175.

> system.time(v <- readLines("BIG"))
[1] 7.75 0.17 8.13 0.00 0.00
	# With BIG already in the file system cache...
> system.time(v <- readLines("BIG", 200000)[199001:200000])
[1] 11.73  0.16 12.27  0.00  0.00

What's the importance of this?
First, experiments I shall not weary you with showed that the time to
read N lines grows faster than N. Second, if you want to select the
_last_ thousand lines, you have to read _all_ of them into memory.

For real efficiency here, what's wanted is a variant of readLines where
n is an index vector (a vector of non-negative integers, a vector of
non-positive integers, or a vector of logicals) saying which lines
should be kept.

The function that would need changing is do_readLines() in
src/main/connections.c, unfortunately I don't understand R internals
well enough to do it myself (yet).

As a matter of fact, that _still_ wouldn't yield real efficiency,
because every character would still have to be read by the modified
readLines(), and it reads characters using Rconn_fgetc(), which is what
gives readLines() its power and utility, but certainly doesn't give it
wings.  (One of the fundamental laws of efficient I/O library design is
to base it on block- or line- at-a-time transfers, not
character-at-a-time.)

The AWK program
    NR <= 199000 { next }
    {print}
    NR == 200000 { exit }
extracts lines 199001:20000 in just 0.76 seconds, about 15 times faster.
A C program to the same effect, using fgets(), took 0.39 seconds, or
about 30 times faster than R.

There are two fairly clear sources of overhead in the R code:
(1) the overhead of reading characters one at a time through
Rconn_fgetc()
    instead of a block or line at a time.  mawk doesn't use fgets() for
    reading, and _does_ have the overhead of repeatedly checking a
    regular expression to determine where the end of the line is,
    which it is sensible enough to fast-path.
(2) the overhead of allocating, filling in, and keeping, a whole lot of
    memory which is of no use whatever in computing the final result.
    mawk is actually fairly careful here, and only keeps one line at
    a time in the program shown above.  Let's change it:
	NR <= 199000 {next}
	{a[NR] = $0}
	NR == 200000 {exit}
	END {for (i in a) print a[i]}
    That takes the time from 0.76 seconds to 0.80 seconds

The simplest thing that could possibly work would be to add a function
skipLines(con, n) which simply read and discarded n lines.

	 result <- scan(textConnection(lines), list( .... ))
	
> system.time(m <- scan(textConnection(v), integer(41)))
Read 41000 items
[1] 0.99 0.00 1.01 0.00 0.00

One whole second to read 41,000 numbers on a 500 MHz machine?

> vv <- rep(v, 240)

Is there any possibility of storing the data in (platform) binary form?
Binary connections (R-data.pdf, section 6.5 "Binary connections") can be
used to read binary-encoded data.

I wrote a little C program to save out the 230175 records of 41 integers
each in native binary form.  Then in R I did

> system.time(m <- readBin("BIN", integer(), n=230175*41, size=4))
[1] 0.57 0.52 1.11 0.00 0.00
> system.time(m <- matrix(data=m, ncol=41, byrow=TRUE))
[1] 2.55 0.34 2.95 0.00 0.00

Remember, this doesn't read a *sample* of the data, it reads *all* the
data.  It is so much faster than the alternatives in R that it just
isn't funny.  Trying scan() on the file took nearly 10 minutes before I
killed it the other day, using readBin() is a thousand times faster than
a simple scan() call on this particular data set.

There has *got* to be a way of either generating or saving the data in
binary form, using only "approved" Windows tools.  Heck, it can probably
be done using VBA.


By the way, I've read most of the .pdf files I could find on the CRAN
site, but haven't noticed any description of the R save-file format.
Where should I have looked?  (Yes, I know about src/main/saveload.c; I
was hoping for some documentation, with maybe some diagrams.)

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://www.stat.math.ethz.ch/mailman/listinfo/r-help



From ripley at stats.ox.ac.uk  Wed Aug 27 08:17:47 2003
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Wed, 27 Aug 2003 07:17:47 +0100 (BST)
Subject: [R] R tools for large files
In-Reply-To: <200308270103.h7R13dYU208278@atlas.otago.ac.nz>
Message-ID: <Pine.LNX.4.44.0308270709140.2749-100000@gannet.stats>

I'm bored, but just to point out the obvious fact: to skip n lines in a
text file you have to read *all* the characters in between to find the
line separators.

I have known for 30 years that reading text files of numbers is slow and 
inefficient.  So do it only once and dump the results to a binary format, 
or a RDBMS or ....

On Wed, 27 Aug 2003, Richard A. O'Keefe wrote:

> Duncan Murdoch <dmurdoch at pair.com> wrote:
> 	For example, if you want to read lines 1000 through 1100, you'd do it
> 	like this:
> 	
> 	 lines <- readLines("foo.txt", 1100)[1000:1100]
> 
> I created a dataset thus:
> # file foo.awk:
> BEGIN {
>     s = "01"
>     for (i = 2; i <= 41; i++) s = sprintf("%s %02d", s, i)
>     n = (27 * 1024 * 1024) / (length(s) + 1)
>     for (i = 1; i <= n; i++) print s
>     exit 0
> }
> # shell command:
> mawk -f foo.awk /dev/null >BIG
> 
> That is, each record contains 41 2-digit integers, and the number
> of records was chosen so that the total size was approximately
> 27 dimegabytes.  The number of records turns out to be 230,175.
> 
> > system.time(v <- readLines("BIG"))
> [1] 7.75 0.17 8.13 0.00 0.00
> 	# With BIG already in the file system cache...
> > system.time(v <- readLines("BIG", 200000)[199001:200000])
> [1] 11.73  0.16 12.27  0.00  0.00
> 
> What's the importance of this?
> First, experiments I shall not weary you with showed that the
> time to read N lines grows faster than N.
> Second, if you want to select the _last_ thousand lines,
> you have to read _all_ of them into memory.
> 
> For real efficiency here, what's wanted is a variant of readLines
> where n is an index vector (a vector of non-negative integers,
> a vector of non-positive integers, or a vector of logicals) saying
> which lines should be kept.
> 
> The function that would need changing is do_readLines() in
> src/main/connections.c, unfortunately I don't understand R internals
> well enough to do it myself (yet).
> 
> As a matter of fact, that _still_ wouldn't yield real efficiency,
> because every character would still have to be read by the modified
> readLines(), and it reads characters using Rconn_fgetc(), which is
> what gives readLines() its power and utility, but certainly doesn't
> give it wings.  (One of the fundamental laws of efficient I/O library
> design is to base it on block- or line- at-a-time transfers, not
> character-at-a-time.)
> 
> The AWK program
>     NR <= 199000 { next }
>     {print}
>     NR == 200000 { exit }
> extracts lines 199001:20000 in just 0.76 seconds, about 15 times
> faster.  A C program to the same effect, using fgets(), took 0.39
> seconds, or about 30 times faster than R.
> 
> There are two fairly clear sources of overhead in the R code:
> (1) the overhead of reading characters one at a time through Rconn_fgetc()
>     instead of a block or line at a time.  mawk doesn't use fgets() for
>     reading, and _does_ have the overhead of repeatedly checking a
>     regular expression to determine where the end of the line is,
>     which it is sensible enough to fast-path.
> (2) the overhead of allocating, filling in, and keeping, a whole lot of
>     memory which is of no use whatever in computing the final result.
>     mawk is actually fairly careful here, and only keeps one line at
>     a time in the program shown above.  Let's change it:
> 	NR <= 199000 {next}
> 	{a[NR] = $0}
> 	NR == 200000 {exit}
> 	END {for (i in a) print a[i]}
>     That takes the time from 0.76 seconds to 0.80 seconds
> 
> The simplest thing that could possibly work would be to add a function
> skipLines(con, n) which simply read and discarded n lines.
> 
> 	 result <- scan(textConnection(lines), list( .... ))
> 	
> > system.time(m <- scan(textConnection(v), integer(41)))
> Read 41000 items
> [1] 0.99 0.00 1.01 0.00 0.00
> 
> One whole second to read 41,000 numbers on a 500 MHz machine?
> 
> > vv <- rep(v, 240)
> 
> Is there any possibility of storing the data in (platform) binary form?
> Binary connections (R-data.pdf, section 6.5 "Binary connections") can be
> used to read binary-encoded data.
> 
> I wrote a little C program to save out the 230175 records of 41 integers
> each in native binary form.  Then in R I did
> 
> > system.time(m <- readBin("BIN", integer(), n=230175*41, size=4))
> [1] 0.57 0.52 1.11 0.00 0.00
> > system.time(m <- matrix(data=m, ncol=41, byrow=TRUE))
> [1] 2.55 0.34 2.95 0.00 0.00
> 
> Remember, this doesn't read a *sample* of the data, it reads *all*
> the data.  It is so much faster than the alternatives in R that it
> just isn't funny.  Trying scan() on the file took nearly 10 minutes
> before I killed it the other day, using readBin() is a thousand times
> faster than a simple scan() call on this particular data set.
> 
> There has *got* to be a way of either generating or saving the data
> in binary form, using only "approved" Windows tools.  Heck, it can
> probably be done using VBA.
> 
> 
> By the way, I've read most of the .pdf files I could find on the CRAN site,
> but haven't noticed any description of the R save-file format.  Where should
> I have looked?  (Yes, I know about src/main/saveload.c; I was hoping for
> some documentation, with maybe some diagrams.)
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> 
> 

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From ripley at stats.ox.ac.uk  Wed Aug 27 08:22:23 2003
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Wed, 27 Aug 2003 07:22:23 +0100 (BST)
Subject: [R] How to do leave-n-out cross validation in R? 
In-Reply-To: <20030827025331.53592.qmail@web41203.mail.yahoo.com>
Message-ID: <Pine.LNX.4.44.0308270718090.2749-100000@gannet.stats>

On Tue, 26 Aug 2003, Lily wrote:

> Seems crossval from library(bootstrap) can only be
> used for leave-one-out and k-fold cross validation?
> Here is a dumb question, suppose n=80, how to do
> exactly leave-50-out cross validation? K-fold cross
> validation is not eligible for this case since
> n/ngroup is not an integer. Thanks!

First, _you_ have to say exactly what _you_ mean by leave-n-out CV. If you
can specify the algorithm, you can program it in R (or we may be able to 
help).

As I have never encountered this, I don't know the definition, nor do I
see the point.  I suspect it is not really cross-validation at all (the
term is widely misused in the machine-learning/neural nets communities to
mean the use of a validation set).

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From ripley at stats.ox.ac.uk  Wed Aug 27 08:56:35 2003
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Wed, 27 Aug 2003 07:56:35 +0100 (BST)
Subject: [R] Seeking Packaging advice
In-Reply-To: <1061931506.1818.36.camel@iron.libaux.ucsf.edu>
Message-ID: <Pine.LNX.4.44.0308270738240.2749-100000@gannet.stats>

On Tue, 26 Aug 2003, Ross Boylan wrote:

> I have two questions about packaging up code.
> 
> 1) Weave/tangle advisable?
> In the course of extending some C code already in S, I had to work out
> the underlying math.  It seems to me useful to keep this information
> with the code, using Knuth's tangle/weave type tools.  I know there is
> some support for this in R code, but my question is about the wisdow of
> doing this with C (or Fortran, or other source) code.
> 
> Against the advantage of having the documentation and code nicely
> integrated are the drawbacks of added complexity in the build process
> and portability concerns.  Some of this is mitigated by the existing
> dependence on TeX.

There is none. We don't assume a working latex/tex, although some manuals 
will not be produced without working (pdf)latex (or texinfo->pdf).

One quick comment: the pre-compiled packages (for Windows now and MacOS X
for the next release) are produced automatically without user
intervention.  So if you want to have a package on CRAN, it needs to work
out of the box, and there is no dependence on TeX, let alone weave/tangle,
in the standard procedure.

> An intermediate approach would be to provide both the web (in the Knuth
> sense) source and the C output; the latter could be used directly by
> those not wishing to hassle with web.  This isn't ideal, since the
> resulting C is likely to be a bit cryptic, and if someone edits the C
> without changing the web source confusion will reign.
> 
> So do people have any thoughts about whether introducing this is a step
> forward or back?

A useful analogue: we now distribute Fortran code not the original Ratfor.


> 2) Modifications of existing packages.
> I modified the survival package (I'm not sure if that's properly called
> a "base" package, but it's close).  I know in this particular case, if

It's a `recommended' package, as the DESCRIPTION file says.  There is a 
base package, and several standard packages bundled with R, which have
priority "base" and are often call `base packages'.

> I'm serious, I probably should contact the package maintainer.  But this
> kind of operation will probably be pretty common for me; I imagine many
> on this list have already done it.  In general, is the best thing to do
> a) package the new routines as a small additional package, with a
> dependence on the base package if necessary (the particular change I've
> made actually produces a few distinct files, slight tweaks of existing
> ones, that can stand on their own)
> b) package the new things in with the old under the same name as the old
> (obviously requires working with package maintainter)
> c) package the new things with the old and give it a new name.
> 
> I'm also curious about what development strategy is best; I did b), and
> it seemed to work OK.  But I kept expecting it to cause disaster (it
> probably helped that I usually didn't load the baseline survival
> packages; clearly that wouldn't be an option if working with one of the
> automatically loaded packages).

I think a) is the best, including changing the names of any R functions 
you alter, and changing the entry points in any compiled code you alter.

Package maintainers may have very good reasons not to go along with b), 
including their not being the original authors (true for survival), 
workload, lack of interest in the proposed changes, complications of 
ownership and copyright, ....

c) is I believe unwise.  It may be allowed by the licence (or may not) but
in the couple of cases where I have seen it done it did not give anything
like adequate credit to the original authors (who were never consulted)
and the modified code distributed was out-of-date when originally 
released, let alone now.

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From eugenesalinas2003 at yahoo.com  Wed Aug 27 10:49:23 2003
From: eugenesalinas2003 at yahoo.com (Eugene Salinas)
Date: Wed, 27 Aug 2003 01:49:23 -0700 (PDT)
Subject: [R] selecting by variable
Message-ID: <20030827084923.11144.qmail@web20714.mail.yahoo.com>

Hi,

I'm a recent R convert so I haven't quite figured out
the details yet...

How do I select one variable by another one? Ie if I
want to draw the histogram of variable X only for
those individuals that also have a value Y in a
certain range?

In STATA I would give something like:

histogram X if ((Y>=A & Y<=B))

(The data is for individuals and each individual has a
number of characteristics including X and Y).

thanks, eugene.



From petr.pikal at precheza.cz  Wed Aug 27 11:04:59 2003
From: petr.pikal at precheza.cz (Petr Pikal)
Date: Wed, 27 Aug 2003 11:04:59 +0200
Subject: [R] matching-case sensitivity
In-Reply-To: <927ABE8FACE5BB469BAB78229C8B884E9349DC@atl1ex1.corp.etradegrp.com>
Message-ID: <3F4C905B.25199.395AAD@localhost>

Hallo

On 26 Aug 2003 at 13:09, Jablonsky, Nikita wrote:

> Hi All,
> 
> I am trying to match two character arrays (email lists) using either
> pmatch(), match() or charmatch() functions. However the function is
> "missing" some matches due to differences in the cases of some letters
try toupper or tolower

> ttt<-toupper("differences in the cases")
> ttt
[1] "DIFFERENCES IN THE CASES"
> tolower(ttt)
[1] "differences in the cases"
>


> between the two arrays. Is there any way to disable case sensitivity
> or is there an entirely better way to match two character arrays that
> have identical entries but written in different case?
> 
> Thanks
> Nikita
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help

Petr Pikal
petr.pikal at precheza.cz



From kwan022 at stat.auckland.ac.nz  Wed Aug 27 11:05:58 2003
From: kwan022 at stat.auckland.ac.nz (Ko-Kang Kevin Wang)
Date: Wed, 27 Aug 2003 21:05:58 +1200 (NZST)
Subject: [R] selecting by variable
In-Reply-To: <20030827084923.11144.qmail@web20714.mail.yahoo.com>
Message-ID: <Pine.LNX.4.44.0308272103140.13626-100000@stat55.stat.auckland.ac.nz>

On Wed, 27 Aug 2003, Eugene Salinas wrote:

> I'm a recent R convert so I haven't quite figured out
> the details yet...

Usually it is good to read the manuals when you use a unfamiliar 
software...

> How do I select one variable by another one? Ie if I
> want to draw the histogram of variable X only for
> those individuals that also have a value Y in a
> certain range?

e.g.
  x = rnorm(100)
  y = 1:100
  x[y = 20:50]
will give you the value of x when y is between 20 and 50.  To do a 
histogram, type:
  ?hist

-- 
Cheers,

Kevin

------------------------------------------------------------------------------
"On two occasions, I have been asked [by members of Parliament],
'Pray, Mr. Babbage, if you put into the machine wrong figures, will
the right answers come out?' I am not able to rightly apprehend the
kind of confusion of ideas that could provoke such a question."

-- Charles Babbage (1791-1871) 
---- From Computer Stupidities: http://rinkworks.com/stupid/

--
Ko-Kang Kevin Wang
Master of Science (MSc) Student
SLC Tutor and Lab Demonstrator
Department of Statistics
University of Auckland
New Zealand
Homepage: http://www.stat.auckland.ac.nz/~kwan022
Ph: 373-7599
    x88475 (City)
    x88480 (Tamaki)



From ripley at stats.ox.ac.uk  Wed Aug 27 11:07:34 2003
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Wed, 27 Aug 2003 10:07:34 +0100 (BST)
Subject: [R] selecting by variable
In-Reply-To: <20030827084923.11144.qmail@web20714.mail.yahoo.com>
Message-ID: <Pine.LNX.4.44.0308271005580.5651-100000@gannet.stats>

hist(X[Y>=A & Y<=B])

`An Introduction to R' explains such things, as do (in more detail) the 
introductory texts (see the R FAQ).


On Wed, 27 Aug 2003, Eugene Salinas wrote:

> I'm a recent R convert so I haven't quite figured out
> the details yet...
> 
> How do I select one variable by another one? Ie if I
> want to draw the histogram of variable X only for
> those individuals that also have a value Y in a
> certain range?
> 
> In STATA I would give something like:
> 
> histogram X if ((Y>=A & Y<=B))
> 
> (The data is for individuals and each individual has a
> number of characteristics including X and Y).

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From th50 at leicester.ac.uk  Wed Aug 27 11:15:18 2003
From: th50 at leicester.ac.uk (Hotz, T.)
Date: Wed, 27 Aug 2003 10:15:18 +0100
Subject: [R] selecting by variable
Message-ID: <1F2CE8D4B0195E488213E8B8CCF714860161B69F@saffron.cfs.le.ac.uk>

Eugene,

R allows indexing with logical vectors, so your example would look
like

hist(X[(Y>=A) & (Y<=B)])

See the manual "An Introduction to R" for details.

HTH

Thomas

> -----Original Message-----
> From: Eugene Salinas [mailto:eugenesalinas2003 at yahoo.com]
> Sent: 27 August 2003 09:49
> To: r-help at stat.math.ethz.ch
> Subject: [R] selecting by variable
> 
> 
> Hi,
> 
> I'm a recent R convert so I haven't quite figured out
> the details yet...
> 
> How do I select one variable by another one? Ie if I
> want to draw the histogram of variable X only for
> those individuals that also have a value Y in a
> certain range?
> 
> In STATA I would give something like:
> 
> histogram X if ((Y>=A & Y<=B))
> 
> (The data is for individuals and each individual has a
> number of characteristics including X and Y).
> 
> thanks, eugene.
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> 

---

Thomas Hotz
Research Associate in Medical Statistics
University of Leicester
United Kingdom

Department of Epidemiology and Public Health
22-28 Princess Road West
Leicester
LE1 6TP
Tel +44 116 252-5410
Fax +44 116 252-5423

Division of Medicine for the Elderly
Department of Medicine
The Glenfield Hospital
Leicester
LE3 9QP
Tel +44 116 256-3643
Fax +44 116 232-2976



From glaziou at pasteur-kh.org  Wed Aug 27 11:19:42 2003
From: glaziou at pasteur-kh.org (Philippe Glaziou)
Date: Wed, 27 Aug 2003 16:19:42 +0700
Subject: [R] selecting by variable
In-Reply-To: <20030827084923.11144.qmail@web20714.mail.yahoo.com>
References: <20030827084923.11144.qmail@web20714.mail.yahoo.com>
Message-ID: <20030827091942.GB655@pasteur-kh.org>

Eugene Salinas <eugenesalinas2003 at yahoo.com> wrote:
> How do I select one variable by another one? Ie if I
> want to draw the histogram of variable X only for
> those individuals that also have a value Y in a
> certain range?
> 
> In STATA I would give something like:
> 
> histogram X if ((Y>=A & Y<=B))

hist(x[Y>=A & Y<=B])

See:
?Subscript
?"&"

-- 
Philippe Glaziou



From petr.pikal at precheza.cz  Wed Aug 27 11:22:12 2003
From: petr.pikal at precheza.cz (Petr Pikal)
Date: Wed, 27 Aug 2003 11:22:12 +0200
Subject: [R] selecting by variable
In-Reply-To: <20030827084923.11144.qmail@web20714.mail.yahoo.com>
Message-ID: <3F4C9464.14604.491BA3@localhost>

Hallo

On 27 Aug 2003 at 1:49, Eugene Salinas wrote:

> Hi,
> 
> I'm a recent R convert so I haven't quite figured out
> the details yet...
> 
> How do I select one variable by another one? Ie if I
> want to draw the histogram of variable X only for
> those individuals that also have a value Y in a
> certain range?
> 
> In STATA I would give something like:
> 
> histogram X if ((Y>=A & Y<=B))

hist(X[(Y>=A)&(Y<=B)])

if A and B are objects storing your limits

?Logic
?"["

> 
> (The data is for individuals and each individual has a
> number of characteristics including X and Y).
> 
> thanks, eugene.
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help

Cheers
Petr Pikal
petr.pikal at precheza.cz



From maechler at stat.math.ethz.ch  Wed Aug 27 11:45:50 2003
From: maechler at stat.math.ethz.ch (Martin Maechler)
Date: Wed, 27 Aug 2003 11:45:50 +0200
Subject: [R] GWplot
In-Reply-To: <Law9-F74Eb1VqjaFoTo000116b8@hotmail.com>
References: <Law9-F74Eb1VqjaFoTo000116b8@hotmail.com>
Message-ID: <16204.32206.265277.798561@gargle.gargle.HOWL>

>>>>> "Jason" == Jason Fisher <stormplot at hotmail.com>
>>>>>     on Tue, 26 Aug 2003 14:08:26 -0700 writes:

    Jason> For anyone wishing to see R-Tcl/Tk-MySQL in action (Windows XP)...
    Jason> http://moffett.isis.ucla.edu/gwplot/

    Jason> Examples were by far the most useful learning tool
    Jason> during my programming endeavors so I hope this may
    Jason> help in your own projects.

Thank you, Jason.  I like this spirit of sharing!
This looks very interesting for a project we will start here in
a few weeks.  From reading the above web page, it's not clear
why you say
    >> The current version of GWplot is designed for Windows 2000/XP 
when all the tools you say you are using are typically part of
every Linux distribution (and also available probably for every
platform R runs apart from classic MacOS).

What problems do you see using this outside of Win-Xp?
Regards,
Martin Maechler <maechler at stat.math.ethz.ch>	http://stat.ethz.ch/~maechler/
Seminar fuer Statistik, ETH-Zentrum  LEO C16	Leonhardstr. 27
ETH (Federal Inst. Technology)	8092 Zurich	SWITZERLAND
phone: x-41-1-632-3408		fax: ...-1228			<><



From dmurdoch at pair.com  Wed Aug 27 13:21:14 2003
From: dmurdoch at pair.com (Duncan Murdoch)
Date: Wed, 27 Aug 2003 07:21:14 -0400
Subject: [R] R tools for large files
In-Reply-To: <200308270103.h7R13dYU208278@atlas.otago.ac.nz>
References: <200308270103.h7R13dYU208278@atlas.otago.ac.nz>
Message-ID: <pp4pkvsvnqkq5a3kol9ngrmapo335v24tl@4ax.com>

On Wed, 27 Aug 2003 13:03:39 +1200 (NZST), you wrote:

>For real efficiency here, what's wanted is a variant of readLines
>where n is an index vector (a vector of non-negative integers,
>a vector of non-positive integers, or a vector of logicals) saying
>which lines should be kept.

I think that's too esoteric to be worth doing.  Most often in cases
where you aren't reading every line, you don't know which lines to
read until you've read earlier ones.

>There are two fairly clear sources of overhead in the R code:
>(1) the overhead of reading characters one at a time through Rconn_fgetc()
>    instead of a block or line at a time.  mawk doesn't use fgets() for
>    reading, and _does_ have the overhead of repeatedly checking a
>    regular expression to determine where the end of the line is,
>    which it is sensible enough to fast-path.

One complication with reading a block at a time is what to do when you
read too far.  Not all connections can use seek() to reposition to the
beginning, so you'd need to read them one character at a time, (or
attach a buffer somehow, but then what about rw connections?)

>The simplest thing that could possibly work would be to add a function
>skipLines(con, n) which simply read and discarded n lines.
>
>	 result <- scan(textConnection(lines), list( .... ))

That's probably worth doing.

Duncan Murdoch



From Simon.Fear at synequanon.com  Wed Aug 27 14:30:26 2003
From: Simon.Fear at synequanon.com (Simon Fear)
Date: Wed, 27 Aug 2003 13:30:26 +0100
Subject: [R] seeking help with with()
Message-ID: <6C8A8033ABC1E3468048ABC4F13CE572AC0D8B@synequanon01>

I tried to define a function like:

fnx <- function(x, by.vars=Month)
  print(by(x, by.vars, summary))

But this doesn't work (does not find x$Month; unlike other functions,
such as
subset(), the INDICES argument to "by" does not look for variables in
dataset
x. Is fully documented, but I forget every time). So I tried using
"with":

fnxx <- function(x, by.vars=Month)
  print(with(x, by(x, by.vars, summary)))

Still fails to find object x$Month. 

I DO have a working solution (below) - this post is just to ask: Can
anyone
explain what happened to the with()?



FYI solutions are to call like this:

fnx(airquality, airquality$Month)

but this will not work generically - e.g. in my real application the
dataset
gets subsetted and by.vars needs to refer to the subsets. So redefine
like
this:

fny <- function(x, by.vars=Month) {
  attach(x)
  print(by(x, by.vars, summary))
  detach(x)
}
 

Simon Fear
Senior Statistician
Syne qua non Ltd
Tel: +44 (0) 1379 644449
Fax: +44 (0) 1379 644445
email: Simon.Fear at synequanon.com
web: http://www.synequanon.com
 
Number of attachments included with this message: 0
 
This message (and any associated files) is confidential and\...{{dropped}}



From ripley at stats.ox.ac.uk  Wed Aug 27 15:08:19 2003
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Wed, 27 Aug 2003 14:08:19 +0100 (BST)
Subject: [R] seeking help with with()
In-Reply-To: <6C8A8033ABC1E3468048ABC4F13CE572AC0D8B@synequanon01>
Message-ID: <Pine.LNX.4.44.0308271347110.12661-100000@gannet.stats>

On Wed, 27 Aug 2003, Simon Fear wrote:

> I tried to define a function like:
> 
> fnx <- function(x, by.vars=Month)
>   print(by(x, by.vars, summary))
> 
> But this doesn't work (does not find x$Month; unlike other functions,
> such as
> subset(), the INDICES argument to "by" does not look for variables in
> dataset
> x. Is fully documented, but I forget every time). So I tried using
> "with":
> 
> fnxx <- function(x, by.vars=Month)
>   print(with(x, by(x, by.vars, summary)))
> 
> Still fails to find object x$Month. 

That's not the actual error message, is it?

> I DO have a working solution (below) - this post is just to ask: Can
> anyone
> explain what happened to the with()?

Nothing!

by.vars is a variable passed to fnxx, so despite lazy evaluation, it is
going to be evaluated in the environment calling fnxx().  If that fails to
find it, it looks for the default value, and evaluates that in the
environment of the body of fnxx.  It didn't really get as far as with.

(I often forget where default args are evaluated, but I believe that is 
correct in R as well as in S.)

I think you intended Months to be a name and not a variable.  With

X <- data.frame(z=rnorm(20), Month=factor(rep(1:2, each=10)))

fnx <- function(x, by.vars="Month")
   print(by(x, x[by.vars], summary))

will work, as will

fnx <- function(x, by.vars=Month)
   print(by(x, x[deparse(substitute(by.vars))], summary))


-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From p.dalgaard at biostat.ku.dk  Wed Aug 27 15:26:28 2003
From: p.dalgaard at biostat.ku.dk (Peter Dalgaard BSA)
Date: Wed, 27 Aug 2003 13:26:28 -0000
Subject: [R] seeking help with with()
In-Reply-To: <6C8A8033ABC1E3468048ABC4F13CE572AC0D8B@synequanon01>
References: <6C8A8033ABC1E3468048ABC4F13CE572AC0D8B@synequanon01>
Message-ID: <x24r03jkpg.fsf@biostat.ku.dk>

"Simon Fear" <Simon.Fear at synequanon.com> writes:

> I tried to define a function like:
> 
> fnx <- function(x, by.vars=Month)
>   print(by(x, by.vars, summary))
> 
> But this doesn't work (does not find x$Month; unlike other functions,
> such as
> subset(), the INDICES argument to "by" does not look for variables in
> dataset
> x. Is fully documented, but I forget every time). So I tried using
> "with":
> 
> fnxx <- function(x, by.vars=Month)
>   print(with(x, by(x, by.vars, summary)))
> 
> Still fails to find object x$Month. 
> 
> I DO have a working solution (below) - this post is just to ask: Can
> anyone
> explain what happened to the with()?
> 

Nothing, but by.vars is evaluated in the function frame where it is
not defined. I think you're looking for something like

function(x, by.vars) {
  if (missing(by.vars)) by.vars <- as.name("Month")
  print(eval.parent(substitute(with(x, by(x, by.vars, summary)))))
}

(Defining the default arg requires a bit of sneakiness...)
-- 
   O__  ---- Peter Dalgaard             Blegdamsvej 3  
  c/ /'_ --- Dept. of Biostatistics     2200 Cph. N   
 (*) \(*) -- University of Copenhagen   Denmark      Ph: (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)             FAX: (+45) 35327907



From Simon.Fear at synequanon.com  Wed Aug 27 15:32:51 2003
From: Simon.Fear at synequanon.com (Simon Fear)
Date: Wed, 27 Aug 2003 14:32:51 +0100
Subject: [R] seeking help with with()
Message-ID: <6C8A8033ABC1E3468048ABC4F13CE572AC0D8C@synequanon01>

Thank you so much for that fix (to my understanding).

I would be willing to add such an example to the help 
page for future releases - though I'm sure others would 
do it better - there are currently no examples where
 INDICES is a name.

In fact in my real application it is more or less essential
that INDICES is a name or at least deparse(substituted 
as a subscript; in a slight elaboration of my previous "fix"

fnz <- function(dframe, by.vars=treat)
  for (pop in 1:2) {
    dframe.pop <- subset(dframe, ITT==pop)
    attach(dframe.pop)
    print(by(dframe.pop, by.vars, summary))
    detach(dframe.pop)
  }

the second call (when pop=2) to by() will crash because by.vars 
is not re-evaluated afresh - it retains its value 
from the first loop.

So, my "fix" was wrong and I am happy to stand corrected.


> -----Original Message-----
> From: Prof Brian Ripley [mailto:ripley at stats.ox.ac.uk]
> Sent: 27 August 2003 14:08
> To: Simon Fear
> Cc: R-help at stat.math.ethz.ch
> Subject: Re: [R] seeking help with with()
> 
> 
> Security Warning:
> If you are not sure an attachment is safe to open please contact 
> Andy on x234. There are 0 attachments with this message.
> ________________________________________________________________
> 
> On Wed, 27 Aug 2003, Simon Fear wrote:
> 
> > I tried to define a function like:
> > 
> > fnx <- function(x, by.vars=Month)
> >   print(by(x, by.vars, summary))
> > 
> > But this doesn't work (does not find x$Month; unlike other 
> functions,
> > such as
> > subset(), the INDICES argument to "by" does not look for 
> variables in
> > dataset
> > x. Is fully documented, but I forget every time). So I tried using
> > "with":
> > 
> > fnxx <- function(x, by.vars=Month)
> >   print(with(x, by(x, by.vars, summary)))
> > 
> > Still fails to find object x$Month. 
> 
> That's not the actual error message, is it?
> 
> > I DO have a working solution (below) - this post is just to ask: Can
> > anyone
> > explain what happened to the with()?
> 
> Nothing!
> 
> by.vars is a variable passed to fnxx, so despite lazy 
> evaluation, it is
> going to be evaluated in the environment calling fnxx().  If 
> that fails
> to
> find it, it looks for the default value, and evaluates that in the
> environment of the body of fnxx.  It didn't really get as far as with.
> 
> (I often forget where default args are evaluated, but I 
> believe that is 
> correct in R as well as in S.)
> 
> I think you intended Months to be a name and not a variable.  With
> 
> X <- data.frame(z=rnorm(20), Month=factor(rep(1:2, each=10)))
> 
> fnx <- function(x, by.vars="Month")
>    print(by(x, x[by.vars], summary))
> 
> will work, as will
> 
> fnx <- function(x, by.vars=Month)
>    print(by(x, x[deparse(substitute(by.vars))], summary))
> 
> 
> -- 
> Brian D. Ripley,                  ripley at stats.ox.ac.uk
> Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
> University of Oxford,             Tel:  +44 1865 272861 (self)
> 1 South Parks Road,                     +44 1865 272866 (PA)
> Oxford OX1 3TG, UK                Fax:  +44 1865 272595
>
 

Simon Fear
Senior Statistician
Syne qua non Ltd
Tel: +44 (0) 1379 644449
Fax: +44 (0) 1379 644445
email: Simon.Fear at synequanon.com
web: http://www.synequanon.com
 
Number of attachments included with this message: 0
 
This message (and any associated files) is confidential and\...{{dropped}}



From chrysopa at insecta.ufv.br  Wed Aug 27 16:04:21 2003
From: chrysopa at insecta.ufv.br (Ronaldo Reis Jr.)
Date: Wed, 27 Aug 2003 11:04:21 -0300
Subject: [R] how to calculate Rsquare
In-Reply-To: <3F1C17A6.9070005@pdf.com>
References: <200307211241.57759.chrysopa@insecta.ufv.br>
	<3F1C17A6.9070005@pdf.com>
Message-ID: <200308271104.21458.chrysopa@insecta.ufv.br>

Can anybody send these articles for me? 

> 	  NagelKerke, N. J. D. (1991) "A note on a general definition of the
> coefficient of determination", Biometrika 78: 691-2.
>
> 	  Cox, D. R. and Wermuth, N. (1992) "A comment on the coefficient of
> determination for binary responses", The American Statistician 46:  1-4.
>

Thanks
Ronaldo
-- 
Of ______course it's the murder weapon.  Who would frame someone with a 
fake?
--
|>   // | \\   [***********************************]
|   ( ?   ? )  [Ronaldo Reis J?nior                ]
|>      V      [UFV/DBA-Entomologia                ]
|    /     \   [36571-000 Vi?osa - MG              ]
|>  /(.''`.)\  [Fone: 31-3899-2532                 ]
|  /(: :'  :)\ [chrysopa at insecta.ufv.br            ]
|>/ (`. `'` ) \[ICQ#: 5692561 | LinuxUser#: 205366 ]
|    ( `-  )   [***********************************]
|>>  _/   \_Powered by GNU/Debian Woody/Sarge



From ucgamdo at ucl.ac.uk  Wed Aug 27 16:21:36 2003
From: ucgamdo at ucl.ac.uk (ucgamdo@ucl.ac.uk)
Date: Wed, 27 Aug 2003 15:21:36 +0100
Subject: [R] Exporting R graphs (review)
Message-ID: <3.0.5.32.20030827152136.007d2100@pop-server.ucl.ac.uk>

Hi guys,

   Yesterday I posted my first couple of questions (see bottom of this
message) to this forum and I would like to thank you guys for all the
useful feedback I got. I just would like to make some comments:

1. Exporting R graphs as vector graphics:

The best answer came from Thomas Lumley <tlumley at u.washington.edu>
He suggested using the RSvgDevice package. As far as I know SVG graphics
can be manipulated with OpenOffice and also with sodipodi, I'll check this
package out asap. This should apply to linux and win users.

Too all bitmap (i.e. jpeg, png, etc.) enthusiasts, thanks a lot for your
help. I knew already that R can create high quality bitmap files that can
then be greatly enhanced with the Gimp. However, a bitmap is a bitmap: they
are usually larger than vector or postscript files and they do not allow
fine control of vector objects. For windows users, I strongly encourage
them to try copying their graphs to the clipboard as metafiles, and then
pasting the result into OpenOffice Draw, you will have to unmask, and
ungroup the object before being able to manipulate it. You will see how
easy is just to select one point from a scatter plot, resize it, move it,
etc. without any loss of resolution.

2. Recording graphs:

The best answer came from MSchwartz at MedAnalytics.com it was just what I was
looking for, so everybody, please try what he suggested:

What you could do is to use recordPlot() to save the version of the plot
to a file, prior to the point at which you start annotating, so that you
can return to that same point if you make a mistake.

As a brief example, with output to the display:

# Do a quick scatterplot
plot(1:10)

# Now save the plot's current state
myplot <- recordPlot()

# Now save display list 'myplot' to a file
save(myplot, file = "myplot")

# Now place some text on the plot
# Click on the plot with the mouse after 
# this line executes, to indicate where you
# want the text located.
text(locator(1), "Some Text", adj=0)

# OK, didn't like where I located the text
# so load and redisplay the base plot
load("myplot")
myplot

# Now re-locate the text
text(locator(1), "Some Text", adj=0)


############################################################################
###
Old post:

Hi,

	I have been a happy user of R for windows for more than a year, however,
recently, I started using linux as my operating system and now I have
practically switched completely. Of course, I still use R with linux,
however, certain nice features of R in windows seem to be missing or
hidden. I need help in basically two points:

1. In windows, I could copy the contents of a window graphic's device as a
windows metafile, and then I could paste the metafile into OpenOffice draw.
The advantage of doing this is that I could then manipulate my graph as a
vector object and get absolute control of every line, point, etc. I could
also paste several graphs in the same page and resize them arbitrarily,
which is a lot nicer than using the screen.split() or frame() functions in
R. Is there any equivalent method I could use to get R graphics into
OpenOffice draw in linux?, keeping the same functionality of course.

2. In windows, I could 'record' a graph, and then undo any changes made.
For example, if I misplaced a text label somewhere, I could go back and
place it again properly without having to re-plot everything again. I have
been browsing the R documentation and found some recording functions for
graphic devices but they do not seem to be exactly what I am looking for.
Is there any way I can access the display list of a device and change it?
Some of my plots are the product of tedious and long simulations which I
wouldn't like to repeat if I make a mistake labelling my plots, placing
legends, etc.

Thanks a lot for any help you can offer me,
M.



From roger at ysidro.econ.uiuc.edu  Wed Aug 27 16:29:33 2003
From: roger at ysidro.econ.uiuc.edu (Roger Koenker)
Date: Wed, 27 Aug 2003 09:29:33 -0500 (CDT)
Subject: [R] Seeking Packaging advice
In-Reply-To: <Pine.LNX.4.44.0308270738240.2749-100000@gannet.stats>
Message-ID: <Pine.SOL.4.30.0308270923290.19029-100000@ysidro.econ.uiuc.edu>


On Wed, 27 Aug 2003, Prof Brian Ripley wrote:

> On Tue, 26 Aug 2003, Ross Boylan wrote:
>
> > So do people have any thoughts about whether introducing this is a step
> > forward or back?
>
> A useful analogue: we now distribute Fortran code not the original Ratfor.
>
As a footnote to Brian's comment, I would just say that those hardy
few of us who still write ratfor can and do include it in a subdirectory
under src since it tends to be vastly more readable than its automatically
produced fortran translation.  But we have also learned from hard
experience that one can't always rely on  the ratfor preprocessing
that is provided by systems even when it exists.

url:	www.econ.uiuc.edu/~roger/my.html	Roger Koenker
email	rkoenker at uiuc.edu			Department of Economics
vox: 	217-333-4558				University of Illinois
fax:   	217-244-6678				Champaign, IL 61820



From tlumley at u.washington.edu  Wed Aug 27 16:49:44 2003
From: tlumley at u.washington.edu (Thomas Lumley)
Date: Wed, 27 Aug 2003 07:49:44 -0700 (PDT)
Subject: [R] Seeking Packaging advice
In-Reply-To: <1061931506.1818.36.camel@iron.libaux.ucsf.edu>
Message-ID: <Pine.A41.4.44.0308270744140.37252-100000@homer13.u.washington.edu>

On Tue, 26 Aug 2003, Ross Boylan wrote:

>
> 2) Modifications of existing packages.
> I modified the survival package (I'm not sure if that's properly called
> a "base" package, but it's close).  I know in this particular case, if
> I'm serious, I probably should contact the package maintainer.  But this
> kind of operation will probably be pretty common for me; I imagine many
> on this list have already done it.  In general, is the best thing to do
> a) package the new routines as a small additional package, with a
> dependence on the base package if necessary (the particular change I've
> made actually produces a few distinct files, slight tweaks of existing
> ones, that can stand on their own)

I think that's best

> b) package the new things in with the old under the same name as the old
> (obviously requires working with package maintainter)

The problem in this case is that the package maintainer is not the author.
Additional functionality might well be ok, but that could easily be done
with method (a).  Substantial changes to existing functions are going
cause problems when the next few thousand lines of diffs arrive from Mayo
Clinic.

> c) package the new things with the old and give it a new name.

Keeping this in sync is hard.

	-thomas



From paulda at BATTELLE.ORG  Wed Aug 27 16:50:12 2003
From: paulda at BATTELLE.ORG (Paul, David  A)
Date: Wed, 27 Aug 2003 10:50:12 -0400
Subject: [R] how to calculate Rsquare
Message-ID: <940250A9EB37A24CBE28D858EF07774967AA9E@ws-bco-mse3.milky-way.battelle.org>

I think you've badly misinterpreted the purpose 
of the R listserv with this request:


https://www.stat.math.ethz.ch/mailman/listinfo/r-help says

"The `main' R mailing list, for announcements about the 
development of R and the availability of new code, questions 
and answers about problems and solutions using R, enhancements 
and patches to the source code and documentation of R, 
comparison and compatibility with S and S-plus, and for 
the posting of nice examples and benchmarks."



-----Original Message-----
From: Ronaldo Reis Jr. [mailto:chrysopa at insecta.ufv.br] 
Sent: Wednesday, August 27, 2003 10:04 AM
To: R-Help
Subject: Re: [R] how to calculate Rsquare


Can anybody send these articles for me? 

> 	  NagelKerke, N. J. D. (1991) "A note on a general definition of the

> coefficient of determination", Biometrika 78: 691-2.
>
> 	  Cox, D. R. and Wermuth, N. (1992) "A comment on the coefficient of

> determination for binary responses", The American Statistician 46:  
> 1-4.
>

Thanks
Ronaldo
-- 
Of ______course it's the murder weapon.  Who would frame someone with a 
fake?
--
|>   // | \\   [***********************************]
|   ( ?   ? )  [Ronaldo Reis J?nior                ]
|>      V      [UFV/DBA-Entomologia                ]
|    /     \   [36571-000 Vi?osa - MG              ]
|>  /(.''`.)\  [Fone: 31-3899-2532                 ]
|  /(: :'  :)\ [chrysopa at insecta.ufv.br            ]
|>/ (`. `'` ) \[ICQ#: 5692561 | LinuxUser#: 205366 ]
|    ( `-  )   [***********************************]
|>>  _/   \_Powered by GNU/Debian Woody/Sarge

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://www.stat.math.ethz.ch/mailman/listinfo/r-help



From MSchwartz at medanalytics.com  Wed Aug 27 17:06:04 2003
From: MSchwartz at medanalytics.com (Marc Schwartz)
Date: Wed, 27 Aug 2003 10:06:04 -0500
Subject: [R] Exporting R graphs (review)
In-Reply-To: <3.0.5.32.20030827152136.007d2100@pop-server.ucl.ac.uk>
References: <3.0.5.32.20030827152136.007d2100@pop-server.ucl.ac.uk>
Message-ID: <1061996764.4194.18.camel@localhost>

On Wed, 2003-08-27 at 09:21, ucgamdo at ucl.ac.uk wrote:
> Hi guys,
> 
>    Yesterday I posted my first couple of questions (see bottom of this
> message) to this forum and I would like to thank you guys for all the
> useful feedback I got. I just would like to make some comments:
> 
> 1. Exporting R graphs as vector graphics:
> 
> The best answer came from Thomas Lumley <tlumley at u.washington.edu>
> He suggested using the RSvgDevice package. As far as I know SVG graphics
> can be manipulated with OpenOffice and also with sodipodi, I'll check this
> package out asap. This should apply to linux and win users.


Just a quick heads up that you can export SVG format files from OOo
Draw. However, there is no present ability to import them into the OOo
apps.

According to OOo's IssueZilla, there are no plans to suport SVG import
prior to version 2.0. 

There is however a fair amount of pressure to do so as SVG formats
become more prevalent as a cross-platform vector format, especially now
that web apps like Mozilla/Firebird are building support for it.

This is one of the reasons that I have stayed with bitmaps for screen
display and EPS for printing when using OOo.

Also, you may be aware that OOo V1.1 (which is at RC3 right now) can
export PDF files directly. However, if you have EPS images embedded in a
document or slide show, they print as you see them on the screen (blank
objects with the embedded title). Thus you need to print them to a PS
file and then use ps2pdf if you want a proper PDF file generated.

Lastly, for those interested, there is a java based OOo Writer to LaTeX
CLI conversion utility and Writer export filter in development.
Amazingly, it is called Writer2LaTex... ;-)

Info is available at http://www.hj-gym.dk/~hj/writer2latex/

HTH,

Marc Schwartz



From spencer.graves at pdf.com  Wed Aug 27 17:26:36 2003
From: spencer.graves at pdf.com (Spencer Graves)
Date: Wed, 27 Aug 2003 08:26:36 -0700
Subject: [R] how to calculate Rsquare
References: <940250A9EB37A24CBE28D858EF07774967AA9E@ws-bco-mse3.milky-way.battelle.org>
Message-ID: <3F4CCDAC.9070300@pdf.com>

The Battelle Institute surely should have access to a library with such 
popular and prestigious journals as Biometrika and The American 
Statistians.  If you don't have time for that, you surely should have 
money to purchase a copy from, e.g., "www.lindahall.org/docserv".

hope this helps.  spencer graves

Paul, David A wrote:
> I think you've badly misinterpreted the purpose 
> of the R listserv with this request:
> 
> 
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help says
> 
> "The `main' R mailing list, for announcements about the 
> development of R and the availability of new code, questions 
> and answers about problems and solutions using R, enhancements 
> and patches to the source code and documentation of R, 
> comparison and compatibility with S and S-plus, and for 
> the posting of nice examples and benchmarks."
> 
> 
> 
> -----Original Message-----
> From: Ronaldo Reis Jr. [mailto:chrysopa at insecta.ufv.br] 
> Sent: Wednesday, August 27, 2003 10:04 AM
> To: R-Help
> Subject: Re: [R] how to calculate Rsquare
> 
> 
> Can anybody send these articles for me? 
> 
> 
>>	  NagelKerke, N. J. D. (1991) "A note on a general definition of the
> 
> 
>>coefficient of determination", Biometrika 78: 691-2.
>>
>>	  Cox, D. R. and Wermuth, N. (1992) "A comment on the coefficient of
> 
> 
>>determination for binary responses", The American Statistician 46:  
>>1-4.
>>
> 
> 
> Thanks
> Ronaldo



From p.dalgaard at biostat.ku.dk  Wed Aug 27 17:46:53 2003
From: p.dalgaard at biostat.ku.dk (Peter Dalgaard BSA)
Date: Wed, 27 Aug 2003 15:46:53 -0000
Subject: [R] Exporting R graphs (review)
In-Reply-To: <1061996764.4194.18.camel@localhost>
References: <3.0.5.32.20030827152136.007d2100@pop-server.ucl.ac.uk>
	<1061996764.4194.18.camel@localhost>
Message-ID: <x2u183hzll.fsf@biostat.ku.dk>

Marc Schwartz <MSchwartz at medanalytics.com> writes:

> On Wed, 2003-08-27 at 09:21, ucgamdo at ucl.ac.uk wrote:
> > Hi guys,
> > 
> >    Yesterday I posted my first couple of questions (see bottom of this
> > message) to this forum and I would like to thank you guys for all the
> > useful feedback I got. I just would like to make some comments:
> > 
> > 1. Exporting R graphs as vector graphics:
> > 
> > The best answer came from Thomas Lumley <tlumley at u.washington.edu>
> > He suggested using the RSvgDevice package. As far as I know SVG graphics
> > can be manipulated with OpenOffice and also with sodipodi, I'll check this
> > package out asap. This should apply to linux and win users.
> 
> 
> Just a quick heads up that you can export SVG format files from OOo
> Draw. However, there is no present ability to import them into the OOo
> apps.
> 
> According to OOo's IssueZilla, there are no plans to suport SVG import
> prior to version 2.0. 
> 
> There is however a fair amount of pressure to do so as SVG formats
> become more prevalent as a cross-platform vector format, especially now
> that web apps like Mozilla/Firebird are building support for it.

There is also the option of writing a driver specifically for oodraw's
format (zipped XML files, mainly). AFAICT, this is mainly a whole lot
of red tape, with the actual plotting specified in sections like

<draw:polyline draw:style-name="gr6" draw:layer="layout"
svg:width="6.272cm" svg:height="5.269cm" draw:transform="rotate
(-0.767770337952161) translate (16.15cm 9.809cm)" svg:viewBox="0 0
6272 5269" draw:points="0,3261 325,6 3206,0 3755,5268 6271,3261"/>

I.e. it doesn't look impossible, but might require a bit of stamina...

In principle you could also try xfig()->CGM->oodraw and maybe other
routes using fig2dev but I can't vouch for the quality.

-- 
   O__  ---- Peter Dalgaard             Blegdamsvej 3  
  c/ /'_ --- Dept. of Biostatistics     2200 Cph. N   
 (*) \(*) -- University of Copenhagen   Denmark      Ph: (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)             FAX: (+45) 35327907



From salcaraz at obelix.umh.es  Wed Aug 27 17:49:10 2003
From: salcaraz at obelix.umh.es (Salvador Alcaraz Carrasco)
Date: Wed, 27 Aug 2003 17:49:10 +0200 (CEST)
Subject: [R] selecting by variable
In-Reply-To: <3F4C9464.14604.491BA3@localhost>
Message-ID: <Pine.LNX.4.44.0308271748390.5778-100000@obelix.umh.es>

On Wed, 27 Aug 2003, Petr Pikal wrote:

> Hallo
>
> On 27 Aug 2003 at 1:49, Eugene Salinas wrote:
>
> > Hi,
> >
> > I'm a recent R convert so I haven't quite figured out
> > the details yet...
> >
> > How do I select one variable by another one? Ie if I
> > want to draw the histogram of variable X only for
> > those individuals that also have a value Y in a
> > certain range?
> >
> > In STATA I would give something like:
> >
> > histogram X if ((Y>=A & Y<=B))
>
> hist(X[(Y>=A)&(Y<=B)])
>
> if A and B are objects storing your limits
>
> ?Logic
> ?"["
>
> >
> > (The data is for individuals and each individual has a
> > number of characteristics including X and Y).
> >
> > thanks, eugene.
> >
> > ______________________________________________
> > R-help at stat.math.ethz.ch mailing list
> > https://www.stat.math.ethz.ch/mailman/listinfo/r-help
>
> Cheers
> Petr Pikal
> petr.pikal at precheza.cz
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
>

__________________________________________________________________________
Salvador Alcaraz Carrasco                      http://www.umh.es
Arquitectura y Tecnolog?a de Computadores      http://obelix.umh.es
Dpto. F?sica y Arquitectura de Computadores    salcaraz at umh.es
Universidad Miguel Hern?ndez                   salcaraz at obelix.umh.es
Avda. del ferrocarril, s/n                     Telf. +34 96 665 8495
Elche, Alicante (Spain)



From fharrell at virginia.edu  Wed Aug 27 17:57:19 2003
From: fharrell at virginia.edu (Frank E Harrell Jr)
Date: Wed, 27 Aug 2003 11:57:19 -0400
Subject: [R] how to calculate Rsquare
In-Reply-To: <200308271104.21458.chrysopa@insecta.ufv.br>
References: <200307211241.57759.chrysopa@insecta.ufv.br>
	<3F1C17A6.9070005@pdf.com>
	<200308271104.21458.chrysopa@insecta.ufv.br>
Message-ID: <20030827115719.1bc006de.fharrell@virginia.edu>

On Wed, 27 Aug 2003 11:04:21 -0300
"Ronaldo Reis Jr." <chrysopa at insecta.ufv.br> wrote:

> Can anybody send these articles for me? 
> 
> > 	  NagelKerke, N. J. D. (1991) "A note on a general definition of the
> > coefficient of determination", Biometrika 78: 691-2.

The fitting functions lrm, psm, cph in the Design package compute Nagelkerke's measures.  -F Harrell

> >
> > 	  Cox, D. R. and Wermuth, N. (1992) "A comment on the coefficient of
> > determination for binary responses", The American Statistician 46:  1-4.
> >
> 
> Thanks
> Ronaldo
> -- 
> Of ______course it's the murder weapon.  Who would frame someone with a 
> fake?
> --
> |>   // | \\   [***********************************]
> |   ( ?   ? )  [Ronaldo Reis J?nior                ]
> |>      V      [UFV/DBA-Entomologia                ]
> |    /     \   [36571-000 Vi?osa - MG              ]
> |>  /(.''`.)\  [Fone: 31-3899-2532                 ]
> |  /(: :'  :)\ [chrysopa at insecta.ufv.br            ]
> |>/ (`. `'` ) \[ICQ#: 5692561 | LinuxUser#: 205366 ]
> |    ( `-  )   [***********************************]
> |>>  _/   \_Powered by GNU/Debian Woody/Sarge
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help


---
Frank E Harrell Jr              Prof. of Biostatistics & Statistics
Div. of Biostatistics & Epidem. Dept. of Health Evaluation Sciences
U. Virginia School of Medicine  http://hesweb1.med.virginia.edu/biostat



From garbade at psy.uni-muenchen.de  Wed Aug 27 17:57:43 2003
From: garbade at psy.uni-muenchen.de (Sven Garbade)
Date: Wed, 27 Aug 2003 15:57:43 -0000
Subject: [R] How to test a model with two unkown constants
Message-ID: <1062007337.466.400.camel@hugo.paed.uni-muenchen.de>

Hi all,

suppose I've got a vector y with some data (from a repeated measure
design) observed given the conditions in f1 and f2. I've got a model
with two unknown fix constants a and b which tries to predict y with
respect to the values in f1 and f2. Here is an exsample

# "data"
y <- c(runif(10, -1,0), runif(10,0,1))
# f1
f1 <- rep(c(-1.4, 1.4), rep(10,2))
# f2
f2 <- rep(c(-.5, .5), rep(10,2))

Suppose my simple model looks like

y = a/f1 + b*f2

Is there a function in R which can compute the estimates for a and b?
And is it possible to test the model, eg how "good" the fits of the
model are?

Thanks, Sven



From plxmh at nottingham.ac.uk  Wed Aug 27 18:00:50 2003
From: plxmh at nottingham.ac.uk (Martin Hoyle)
Date: Wed, 27 Aug 2003 17:00:50 +0100
Subject: [R] Basic GLM: residuals definition
Message-ID: <sf4ce3d3.080@ccw0m2.nottingham.ac.uk>

Dear R Users,

I suppose this is a school boy question, but here it is anyway. I'm trying to re-create the residuals for a poisson GLM with simulated data;

x<-rpois(1000,5)
model<-glm(x~1,poisson)
my.resids<-(log(x)- summary(model)$coefficients[1])
plot(my.resids,residuals(model))

This shows that my calculated residuals (my.resids) are not the same as residuals(model).
p 65 of Annette Dobson's book says that GLM (unstandardised) residuals are calculated by analogy with the Normal case.
So where am I going wrong?

Thanks for your attention.

Martin.


Martin Hoyle,
School of Life and Environmental Sciences,
University of Nottingham,
University Park,
Nottingham,
NG7 2RD,
UK
Webpage: http://myprofile.cos.com/martinhoyle



From p.dalgaard at biostat.ku.dk  Wed Aug 27 18:04:16 2003
From: p.dalgaard at biostat.ku.dk (Peter Dalgaard BSA)
Date: Wed, 27 Aug 2003 16:04:16 -0000
Subject: [R] how to calculate Rsquare
In-Reply-To: <3F4CCDAC.9070300@pdf.com>
References: <940250A9EB37A24CBE28D858EF07774967AA9E@ws-bco-mse3.milky-way.battelle.org>
	<3F4CCDAC.9070300@pdf.com>
Message-ID: <x2ptirhyu2.fsf@biostat.ku.dk>

Spencer Graves <spencer.graves at pdf.com> writes:

> The Battelle Institute surely should have access to a library with
> such popular and prestigious journals as Biometrika and The American
> Statistians.  If you don't have time for that, you surely should have
> money to purchase a copy from, e.g., "www.lindahall.org/docserv".

Battelle is not the issue, Entomology Dept. at Univ.Fed.de Vi?osa is.
That is presumably a somewhat poorer place. Still, you (Ronaldo)
should check whether there is JSTOR access from somewhere around you,
as I'm sure those recipients of r-help who have it will be unsure of
what licences they might break by sending you free copies. And David's
right: This is outside the scope of r-help.

> > From: Ronaldo Reis Jr. [mailto:chrysopa at insecta.ufv.br] Sent:
> > Wednesday, August 27, 2003 10:04 AM
> > To: R-Help
> > Subject: Re: [R] how to calculate Rsquare
> > Can anybody send these articles for me?
> >>	  NagelKerke, N. J. D. (1991) "A note on a general definition of the
> >
> >>coefficient of determination", Biometrika 78: 691-2.
> >>
> >>	  Cox, D. R. and Wermuth, N. (1992) "A comment on the coefficient of
> >
> >> determination for binary responses", The American Statistician 46:
> >> 1-4.


-- 
   O__  ---- Peter Dalgaard             Blegdamsvej 3  
  c/ /'_ --- Dept. of Biostatistics     2200 Cph. N   
 (*) \(*) -- University of Copenhagen   Denmark      Ph: (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)             FAX: (+45) 35327907



From H.RINNER at tirol.gv.at  Wed Aug 27 18:06:16 2003
From: H.RINNER at tirol.gv.at (RINNER Heinrich)
Date: Wed, 27 Aug 2003 18:06:16 +0200
Subject: [R] read.spss (package foreign) and character columns
Message-ID: <C4D44AB4CB62D311BA6500041202E886031EE2A6@xms1.tirol.gv.at>

Dear R users!

I am using R Version 1.7.1, Windows XP, package "foreign" (Version: 0.6-1),
SPSS 11.5.1.

There is one thing I noticed with "read.spss", and I'd like to ask if this
is considered to be a feature, or possibly a bug:
When reading character columns, character strings seem to get filled with
blanks at the end.

Simple example:
In SPSS, create a file with one variable called "xchar" of type "A5"
(character of length 5), and  3 values ("a", "ab", "abcde"), save it as
"test.sav".

In R:
> library(foreign)
> test <- read.spss("test.sav", to.data.frame=T)
> test
  XCHAR
1 a    
2 ab   
3 abcde
> levels(test$XCHAR)
[1] "a    " "ab   " "abcde"

Shouldn't it rather be "a" "ab" "abcde" (no blanks)?

-Heinrich.



From ripley at stats.ox.ac.uk  Wed Aug 27 18:07:33 2003
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Wed, 27 Aug 2003 17:07:33 +0100 (BST)
Subject: [R] How to test a model with two unkown constants
In-Reply-To: <1062007337.466.400.camel@hugo.paed.uni-muenchen.de>
Message-ID: <Pine.LNX.4.44.0308271704070.19980-100000@gannet.stats>

That's the linear model lm(y ~ I(1/f1) + f2), so yes, yes and
fuller answers can be found in most of the books and guides mentioned in 
R's FAQ.

Note that how `good' the fit is will have to be relative, unless you
really can assume a uniform error with range 1, when you could do a 
maximum-likelihood fit (and watch out for the non-standard distribution 
theory).

On 27 Aug 2003, Sven Garbade wrote:

> Hi all,
> 
> suppose I've got a vector y with some data (from a repeated measure
> design) observed given the conditions in f1 and f2. I've got a model
> with two unknown fix constants a and b which tries to predict y with
> respect to the values in f1 and f2. Here is an exsample
> 
> # "data"
> y <- c(runif(10, -1,0), runif(10,0,1))
> # f1
> f1 <- rep(c(-1.4, 1.4), rep(10,2))
> # f2
> f2 <- rep(c(-.5, .5), rep(10,2))
> 
> Suppose my simple model looks like
> 
> y = a/f1 + b*f2

> Is there a function in R which can compute the estimates for a and b?
> And is it possible to test the model, eg how "good" the fits of the
> model are?

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From p.dalgaard at biostat.ku.dk  Wed Aug 27 18:17:21 2003
From: p.dalgaard at biostat.ku.dk (Peter Dalgaard BSA)
Date: Wed, 27 Aug 2003 16:17:21 -0000
Subject: [R] How to test a model with two unkown constants
In-Reply-To: <1062007337.466.400.camel@hugo.paed.uni-muenchen.de>
References: <1062007337.466.400.camel@hugo.paed.uni-muenchen.de>
Message-ID: <x2lltfhyci.fsf@biostat.ku.dk>

Sven Garbade <garbade at psy.uni-muenchen.de> writes:

> Hi all,
> 
> suppose I've got a vector y with some data (from a repeated measure
> design) observed given the conditions in f1 and f2. I've got a model
> with two unknown fix constants a and b which tries to predict y with
> respect to the values in f1 and f2. Here is an exsample
> 
> # "data"
> y <- c(runif(10, -1,0), runif(10,0,1))
> # f1
> f1 <- rep(c(-1.4, 1.4), rep(10,2))
> # f2
> f2 <- rep(c(-.5, .5), rep(10,2))
> 
> Suppose my simple model looks like
> 
> y = a/f1 + b*f2
> 
> Is there a function in R which can compute the estimates for a and b?
> And is it possible to test the model, eg how "good" the fits of the
> model are?

f2 and 1/f1 are exactly collinear, so no, not in R, nor any other way.

Apart from that, the model is linear in a and b so lm() can fit it
(with different f1 and f2) if you're not too squeamish about the error
distribution.

-- 
   O__  ---- Peter Dalgaard             Blegdamsvej 3  
  c/ /'_ --- Dept. of Biostatistics     2200 Cph. N   
 (*) \(*) -- University of Copenhagen   Denmark      Ph: (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)             FAX: (+45) 35327907



From th50 at leicester.ac.uk  Wed Aug 27 18:24:11 2003
From: th50 at leicester.ac.uk (Hotz, T.)
Date: Wed, 27 Aug 2003 17:24:11 +0100
Subject: [R] Basic GLM: residuals definition
Message-ID: <1F2CE8D4B0195E488213E8B8CCF714860250133B@saffron.cfs.le.ac.uk>

As ?residuals.glm reveals, it's got an argument type:

    type: the type of residuals which should be returned. The
          alternatives are: `"deviance"' (default), `"pearson"',
          `"working"', `"response"', and `"partial"'.

You calculated "response" residuals, R gives "deviance" residuals 
by default. The different types are covered by most books on general
linear models.

HTH

Thomas


> -----Original Message-----
> From: Martin Hoyle [mailto:plxmh at nottingham.ac.uk]
> Sent: 27 August 2003 17:01
> To: r-help at stat.math.ethz.ch
> Subject: [R] Basic GLM: residuals definition
> 
> 
> Dear R Users,
> 
> I suppose this is a school boy question, but here it is 
> anyway. I'm trying to re-create the residuals for a poisson 
> GLM with simulated data;
> 
> x<-rpois(1000,5)
> model<-glm(x~1,poisson)
> my.resids<-(log(x)- summary(model)$coefficients[1])
> plot(my.resids,residuals(model))
> 
> This shows that my calculated residuals (my.resids) are not 
> the same as residuals(model).
> p 65 of Annette Dobson's book says that GLM (unstandardised) 
> residuals are calculated by analogy with the Normal case.
> So where am I going wrong?
> 
> Thanks for your attention.
> 
> Martin.
> 
> 
> Martin Hoyle,
> School of Life and Environmental Sciences,
> University of Nottingham,
> University Park,
> Nottingham,
> NG7 2RD,
> UK
> Webpage: http://myprofile.cos.com/martinhoyle
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> 

---

Thomas Hotz
Research Associate in Medical Statistics
University of Leicester
United Kingdom

Department of Epidemiology and Public Health
22-28 Princess Road West
Leicester
LE1 6TP
Tel +44 116 252-5410
Fax +44 116 252-5423

Division of Medicine for the Elderly
Department of Medicine
The Glenfield Hospital
Leicester
LE3 9QP
Tel +44 116 256-3643
Fax +44 116 232-2976



From ripley at stats.ox.ac.uk  Wed Aug 27 18:28:17 2003
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Wed, 27 Aug 2003 17:28:17 +0100 (BST)
Subject: [R] Basic GLM: residuals definition
In-Reply-To: <sf4ce3d3.080@ccw0m2.nottingham.ac.uk>
Message-ID: <Pine.LNX.4.44.0308271723500.19980-100000@gannet.stats>

On Wed, 27 Aug 2003, Martin Hoyle wrote:

> Dear R Users,
> 
> I suppose this is a school boy question, but here it is anyway. I'm trying to re-create the residuals for a poisson GLM with simulated data;
> 
> x<-rpois(1000,5)
> model<-glm(x~1,poisson)
> my.resids<-(log(x)- summary(model)$coefficients[1])
> plot(my.resids,residuals(model))
> 
> This shows that my calculated residuals (my.resids) are not the same as residuals(model).
> p 65 of Annette Dobson's book says that GLM (unstandardised) residuals are calculated by analogy with the Normal case.
> So where am I going wrong?

Not reading the help page.  Hint: what is the default for the type 
argument for the glm method for residual?

A much better reference for this is

Davison, A.~C. and Snell, E.~J. (1991) Residuals and diagnostics.
\newblock Chapter~4 of \cite{Hinkley.ZZ.91}.

Hinkley, D.~V., Reid, N. and Snell, E.~J. eds (1991) \emph{Statistical Theory
  and Modelling. In Honour of Sir David Cox, {FRS}}.
\newblock London: Chapman \& Hall.


-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From spencer.graves at pdf.com  Wed Aug 27 18:41:21 2003
From: spencer.graves at pdf.com (Spencer Graves)
Date: Wed, 27 Aug 2003 09:41:21 -0700
Subject: [R] how to calculate Rsquare
References: <940250A9EB37A24CBE28D858EF07774967AA9E@ws-bco-mse3.milky-way.battelle.org>	<3F4CCDAC.9070300@pdf.com>
	<x2ptirhyu2.fsf@biostat.ku.dk>
Message-ID: <3F4CDF31.7070506@pdf.com>

Please excuse my reference to Batelle:  I confused who was asking and 
who answering the question.  Thanks, Peter, for the clarification and 
for the alternative suggestions.

Spencer Graves

Peter Dalgaard BSA wrote:
> Spencer Graves <spencer.graves at pdf.com> writes:
> 
> 
>>The Battelle Institute surely should have access to a library with
>>such popular and prestigious journals as Biometrika and The American
>>Statistians.  If you don't have time for that, you surely should have
>>money to purchase a copy from, e.g., "www.lindahall.org/docserv".
> 
> 
> Battelle is not the issue, Entomology Dept. at Univ.Fed.de Vi?osa is.
> That is presumably a somewhat poorer place. Still, you (Ronaldo)
> should check whether there is JSTOR access from somewhere around you,
> as I'm sure those recipients of r-help who have it will be unsure of
> what licences they might break by sending you free copies. And David's
> right: This is outside the scope of r-help.
> 
> 
>>>From: Ronaldo Reis Jr. [mailto:chrysopa at insecta.ufv.br] Sent:
>>>Wednesday, August 27, 2003 10:04 AM
>>>To: R-Help
>>>Subject: Re: [R] how to calculate Rsquare
>>>Can anybody send these articles for me?
>>>
>>>>	  NagelKerke, N. J. D. (1991) "A note on a general definition of the
>>>
>>>>coefficient of determination", Biometrika 78: 691-2.
>>>>
>>>>	  Cox, D. R. and Wermuth, N. (1992) "A comment on the coefficient of
>>>
>>>>determination for binary responses", The American Statistician 46:
>>>>1-4.
>>>
> 
>



From ripley at stats.ox.ac.uk  Wed Aug 27 18:45:36 2003
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Wed, 27 Aug 2003 17:45:36 +0100 (BST)
Subject: [R] read.spss (package foreign) and character columns
In-Reply-To: <C4D44AB4CB62D311BA6500041202E886031EE2A6@xms1.tirol.gv.at>
Message-ID: <Pine.LNX.4.44.0308271742080.20055-100000@gannet.stats>

On Wed, 27 Aug 2003, RINNER Heinrich wrote:

> Dear R users!
> 
> I am using R Version 1.7.1, Windows XP, package "foreign" (Version: 0.6-1),
> SPSS 11.5.1.
> 
> There is one thing I noticed with "read.spss", and I'd like to ask if this
> is considered to be a feature, or possibly a bug:
> When reading character columns, character strings seem to get filled with
> blanks at the end.
> 
> Simple example:
> In SPSS, create a file with one variable called "xchar" of type "A5"
> (character of length 5), and  3 values ("a", "ab", "abcde"), save it as
> "test.sav".
> 
> In R:
> > library(foreign)
> > test <- read.spss("test.sav", to.data.frame=T)
> > test
>   XCHAR
> 1 a    
> 2 ab   
> 3 abcde
> > levels(test$XCHAR)
> [1] "a    " "ab   " "abcde"
> 
> Shouldn't it rather be "a" "ab" "abcde" (no blanks)?

You said it was a character string of length 5, not <=5.

It's easy to strip trailing blanks (?sub has several ways).

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From spencer.graves at pdf.com  Wed Aug 27 18:55:23 2003
From: spencer.graves at pdf.com (Spencer Graves)
Date: Wed, 27 Aug 2003 09:55:23 -0700
Subject: [R] how to calculate Rsquare
References: <940250A9EB37A24CBE28D858EF07774967AA9E@ws-bco-mse3.milky-way.battelle.org>	<3F4CCDAC.9070300@pdf.com>
	<x2ptirhyu2.fsf@biostat.ku.dk>
Message-ID: <3F4CE27B.5050007@pdf.com>

Hi, Ronaldo:

	  Have you talked with anyone in the Math Department in the Univ.Fed.de 
Vi?osa?  They offer courses in Statistics there, and I would expect that 
someone there could help you get copies of the articles of interest.  I 
wonder if such contacts might help you with other statistics-related 
issues as well.

hope this helps.
Spencer Graves

Peter Dalgaard BSA wrote:
> Spencer Graves <spencer.graves at pdf.com> writes:
> 
> 
>>The Battelle Institute surely should have access to a library with
>>such popular and prestigious journals as Biometrika and The American
>>Statistians.  If you don't have time for that, you surely should have
>>money to purchase a copy from, e.g., "www.lindahall.org/docserv".
> 
> 
> Battelle is not the issue, Entomology Dept. at Univ.Fed.de Vi?osa is.
> That is presumably a somewhat poorer place. Still, you (Ronaldo)
> should check whether there is JSTOR access from somewhere around you,
> as I'm sure those recipients of r-help who have it will be unsure of
> what licences they might break by sending you free copies. And David's
> right: This is outside the scope of r-help.
> 
> 
>>>From: Ronaldo Reis Jr. [mailto:chrysopa at insecta.ufv.br] Sent:
>>>Wednesday, August 27, 2003 10:04 AM
>>>To: R-Help
>>>Subject: Re: [R] how to calculate Rsquare
>>>Can anybody send these articles for me?
>>>
>>>>	  NagelKerke, N. J. D. (1991) "A note on a general definition of the
>>>
>>>>coefficient of determination", Biometrika 78: 691-2.
>>>>
>>>>	  Cox, D. R. and Wermuth, N. (1992) "A comment on the coefficient of
>>>
>>>>determination for binary responses", The American Statistician 46:
>>>>1-4.
>>>
> 
>



From bates at stat.wisc.edu  Wed Aug 27 19:05:03 2003
From: bates at stat.wisc.edu (Douglas Bates)
Date: Wed, 27 Aug 2003 17:05:03 -0000
Subject: [R] read.spss (package foreign) and character columns
In-Reply-To: <C4D44AB4CB62D311BA6500041202E886031EE2A6@xms1.tirol.gv.at>
References: <C4D44AB4CB62D311BA6500041202E886031EE2A6@xms1.tirol.gv.at>
Message-ID: <6r1xv78217.fsf@bates4.stat.wisc.edu>

RINNER Heinrich <H.RINNER at tirol.gv.at> writes:

> Dear R users!
> 
> I am using R Version 1.7.1, Windows XP, package "foreign" (Version: 0.6-1),
> SPSS 11.5.1.
> 
> There is one thing I noticed with "read.spss", and I'd like to ask if this
> is considered to be a feature, or possibly a bug:
> When reading character columns, character strings seem to get filled with
> blanks at the end.
> 
> Simple example:
> In SPSS, create a file with one variable called "xchar" of type "A5"
> (character of length 5), and  3 values ("a", "ab", "abcde"), save it as
> "test.sav".
> 
> In R:
> > library(foreign)
> > test <- read.spss("test.sav", to.data.frame=T)
> > test
>   XCHAR
> 1 a    
> 2 ab   
> 3 abcde
> > levels(test$XCHAR)
> [1] "a    " "ab   " "abcde"
> 
> Shouldn't it rather be "a" "ab" "abcde" (no blanks)?

I believe they are being saved as fixed length strings in the SPSS
file and R is just reading what it was given.



From commercial at s-boehringer.de  Wed Aug 27 19:55:45 2003
From: commercial at s-boehringer.de (Stefan =?ISO-8859-1?Q?B=F6hringer?=)
Date: Wed, 27 Aug 2003 17:55:45 -0000
Subject: [R] discriminant function
In-Reply-To: <Pine.LNX.3.96.1030826155839.10710A-100000@thorin.ci.tuwien.ac.at>
References: <Pine.LNX.3.96.1030826155839.10710A-100000@thorin.ci.tuwien.ac.at>
Message-ID: <1062006306.652.10.camel@hgX>

Thank you all for the quick responses.
However, I'm not sure I unterstand the scaling matrix (denote S
henceforth) correcty. An observation x will be transformed by Sx into a
new vector space with the properties given by the description. What is
now the direction perpendicular to the seperating plane as estimated in
the process of the lda? That direction is what im primarily interested
in. When plotting a lda object I see diagrams with observations when the
plane (here the line) of separation is chosen canonically to be {ax | a
\in R}.

Thanks, best wishes,

Stefan


On Tue, 2003-08-26 at 15:59, Torsten Hothorn wrote: 
> On 26 Aug 2003, Stefan [ISO-8859-1] B?hringer wrote:
> 
> > How can I extract the linear discriminant functions resulting from a LDA
> > analysis?
> > 
> > The coefficients are listed as a result from the analysis but I have not
> > found a way to extract these programmatically. No refrences in the
> > archives were found.
> 
> ?lda tells you about the object returned by `lda', especially it element:
> 
> scaling: a matrix which transforms observations to discriminant
>           functions, normalized so that within groups covariance matrix
>           is spherical.
> 
> Torsten
> 
> > 
> > Thank you very much,
> > 
> > 	Stefan
> > 
> > ______________________________________________
> > R-help at stat.math.ethz.ch mailing list
> > https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> > 
> >



From amunoz at jhsph.edu  Wed Aug 27 19:40:59 2003
From: amunoz at jhsph.edu (=?iso-8859-1?Q?Alvaro_Mu=F1oz?=)
Date: Wed, 27 Aug 2003 13:40:59 -0400
Subject: [R] Re: diamond graphs
References: <ADEPLELNMGDGKCPEDINLKEJGKBAD.szeger@jhsph.edu>
	<20030825172828.3d5b9872.fharrell@virginia.edu>
Message-ID: <009801c36cc2$60ce64a0$0d98120a@statepidm.jhsph.edu>

Drs. Harrell and O'Keefe,



Thank you for your suggestions.



Regarding your comments about the content of the paper, I respectfully
disagree that "categorizing continuous variables is a fundamental violation
of statistical graphics," nor are you to assume that all categorizations are
arbitrary. In any case, the discussion section of our paper contains text
acknowledging that contour plots are a preferred option when the continuity
of variables is desired to be preserved. The hexagons we proposed seem, at
first glance, to be "unnecessarily complex" but they fulfill properties that
none of the other considered alternatives do (Table 1 and Figure 1 in paper
and Figure 6 using Trellis).



It is unfortunate that the comments from Dr. O'Keefe were based on a press
release and not on the manuscript itself. I apologize for the press release
implying no graphical progress in the 20th century. Many of his points are
addressed in the manuscript. Regarding the extension of the methods to
outcomes taking negative values (e.g., changes in markers), the use of two
colors is an alternative but the plotting of 0.5*[1+(outcome/max(|outcome|)]
and using the option E of Figure 1 in the paper will result in negative and
positive values having opposite topology (much as the contrast of
negative/positive bars in the unidimensional case). I will be happy to
expedite a reprint to Dr. O'Keefe. If you so desire, please email the
address to which it should be sent.



Although it is at odds with your beliefs, University staff working on
licensing and technology transfer believe that a patent may be a vehicle to
achieve a wide use. The audience of the proposed methods would be the end
users who are not sophisticated programmers and, therefore, the hope is that
it would be available in widely used software which is not the case of the
high end software (e.g., R). The proposed graph of 2D equiponderant display
of two predictors is just a display procedure, not an inferential tool. The
sophisticated analyst has little or no need for the proposed method. It does
overcome the pitfalls of 3D bar graphs and, therefore, has the potential of
improving the way we communicate our findings. Needless to say, were the
predictions of Dr. Harrell to be on target, we will change course as the
staff working on the licensing have planned from the start.



We will be happy to share the code we wrote to produce the figures in The
American Statistician paper with individuals wanting to use the software for
academic purposes. Please send request for it to afreeman at jhsph.edu.



In summary, our idea is a simple one (one that I refer as needing only 8th
grade geometry) and it is its simplicity which has been fun to peruse.



Alvaro Mu?oz



From fharrell at virginia.edu  Wed Aug 27 20:32:37 2003
From: fharrell at virginia.edu (Frank E Harrell Jr)
Date: Wed, 27 Aug 2003 14:32:37 -0400
Subject: [R] Re: diamond graphs
In-Reply-To: <009801c36cc2$60ce64a0$0d98120a@statepidm.jhsph.edu>
References: <ADEPLELNMGDGKCPEDINLKEJGKBAD.szeger@jhsph.edu>
	<20030825172828.3d5b9872.fharrell@virginia.edu>
	<009801c36cc2$60ce64a0$0d98120a@statepidm.jhsph.edu>
Message-ID: <20030827143237.62e570db.fharrell@virginia.edu>

On Wed, 27 Aug 2003 13:40:59 -0400
Alvaro Mu?oz <amunoz at jhsph.edu> wrote:

> Drs. Harrell and O'Keefe,
> 
> 
> 
> Thank you for your suggestions.
> 
> 
> 
> Regarding your comments about the content of the paper, I respectfully
> disagree that "categorizing continuous variables is a fundamental violation
> of statistical graphics," nor are you to assume that all categorizations are
> arbitrary. In any case, the discussion section of our paper contains text
> acknowledging that contour plots are a preferred option when the continuity
> of variables is desired to be preserved. The hexagons we proposed seem, at
> first glance, to be "unnecessarily complex" but they fulfill properties that
> none of the other considered alternatives do (Table 1 and Figure 1 in paper
> and Figure 6 using Trellis).

I appreciate your reply Dr Munoz.  I will have to disagree with you about the above although I think you made some good points.  I have seen many, many examples where categorization results in low-precision estimates and slight changes in the bins results in a significantly different landscape.  I have also seen many examples in epidemiology where stratified estimates have been misinterpreted.

Even though thermometer and similar plots have defects that you mentioned in your paper, they much more intuitively and precisely map values into the human brain.  The same is true of Cleveland's dot plots although one has to be careful, as you said in your article, about the ordering of stratifiers.

> 
> 
> 
> It is unfortunate that the comments from Dr. O'Keefe were based on a press
> release and not on the manuscript itself. I apologize for the press release
> implying no graphical progress in the 20th century. Many of his points are
> addressed in the manuscript. Regarding the extension of the methods to
> outcomes taking negative values (e.g., changes in markers), the use of two
> colors is an alternative but the plotting of 0.5*[1+(outcome/max(|outcome|)]
> and using the option E of Figure 1 in the paper will result in negative and
> positive values having opposite topology (much as the contrast of
> negative/positive bars in the unidimensional case). I will be happy to
> expedite a reprint to Dr. O'Keefe. If you so desire, please email the
> address to which it should be sent.
> 
> 
> 
> Although it is at odds with your beliefs, University staff working on
> licensing and technology transfer believe that a patent may be a vehicle to
> achieve a wide use. The audience of the proposed methods would be the end
> users who are not sophisticated programmers and, therefore, the hope is that
> it would be available in widely used software which is not the case of the
> high end software (e.g., R). The proposed graph of 2D equiponderant display
> of two predictors is just a display procedure, not an inferential tool. The
> sophisticated analyst has little or no need for the proposed method. It does
> overcome the pitfalls of 3D bar graphs and, therefore, has the potential of
> improving the way we communicate our findings. Needless to say, were the
> predictions of Dr. Harrell to be on target, we will change course as the
> staff working on the licensing have planned from the start.

Their belief that a patent on an idea may help achieve a wide use is sadly mistaken and is almost comical.  The statement "it would be available in widely used software which is not the case of the high end software" is very difficult to comprehend (especially in view of easy to use GUIs such as Rcmdr now available for R, as well as web interfaces).  There are several books I could recommend to your university staff.


> 
> 
> 
> We will be happy to share the code we wrote to produce the figures in The
> American Statistician paper with individuals wanting to use the software for
> academic purposes. Please send request for it to afreeman at jhsph.edu.

Unfortunately, I think that once the patent announcement was made, the number of individuals interested in the method lessened considerably.

> 
> 
> 
> In summary, our idea is a simple one (one that I refer as needing only 8th
> grade geometry) and it is its simplicity which has been fun to peruse.
> 
> 
> 
> Alvaro Mu?oz

Again I do thank you for your note.

Sincerely,

Frank Harrell

---
Frank E Harrell Jr              Prof. of Biostatistics & Statistics
Div. of Biostatistics & Epidem. Dept. of Health Evaluation Sciences
U. Virginia School of Medicine  http://hesweb1.med.virginia.edu/biostat



From dj at research.bell-labs.com  Wed Aug 27 20:57:26 2003
From: dj at research.bell-labs.com (David James)
Date: Wed, 27 Aug 2003 14:57:26 -0400
Subject: [R] RMySQL crashing R
Message-ID: <20030827145726.A3876@jessie.research.bell-labs.com>

Hi,

There have been a number of reports of RMySQL crashing R when
attempting to connect to a MySQL server using dbConnect().
The problem appears to be in some binary versions of the MySQL
client library.  Known instances include
  (1) Red Hat MySQL binary RPM client library 3.23.32, but 
      updating to 3.23.56 solved the problem.
  (2) Debian MySQL binary client library 3.23.49, but updating 
      to 3.23.56 solved the problem.
Moreover, the change logs in Appendix D of the MySQL manual
(www.mysql.com) indicate that two bugs consistent with the crashes
we've seen were fixed in 3.23.50, namely, a "buffer overflow problem
when reading startup parameters" and a "memory allocation bug in
the glibc library used to build Linux binaries".

If you experience this problem, could you let me know the version
information (R, MySQL, and the operating system)?  Also, I'd like to
know if updating the MySQL client library and re-installing RMySQL
fix your problem.

Thanks,

-- 
David 

PS Thanks to Deepayan Sarkar, John Heuer, and Matthew Kelly for helping
   me track this problem.



From friendly at yorku.ca  Wed Aug 27 21:05:37 2003
From: friendly at yorku.ca (Michael Friendly)
Date: Wed, 27 Aug 2003 15:05:37 -0400
Subject: [R] Minard's Challenge: Re-Visioning Minard Contest
Message-ID: <3F4D0101.5030202@yorku.ca>

In a recent talk ('Visions of the Past, Present & Future of Statistical 
Graphics'),
I talked about, among other things, the lessons Minard's March on Moscow 
graphic had
for modern statistical graphics, and illustrated aspects of power and 
simplicity
in several programming languages where this graphic had been recreated.
I referred to 'elegance factors' of various programming languages in 
terms of
the power, simplicity and transparency of data representations and 
procedural
or declarative specifications required to program a re-creation (or 
extension)
of this famous graph.

It occurred to me that it might be of interest, perhaps fun, and 
hopefully illuminating
to pose this as a formal challenge to the R community and others.

Several exisiting exemplars are shown on my 'Re-visions of Minard' web page
http://www.math.yorku.ca/SCS/Gallery/re-minard.html
(in the Gallery of Data Visualization, ../)
These include programming examples in Mathematica, SAS/IML Workshop, 
Wilkinson's
Grammar of Graphics, images created in other data visualization systems, 
raw materials
(images, data), etc.

There are no formal rules for this "Re-Visioning Minard Contest", but 
each entry should ideally include:
(a) an image file in web-friendly format (.jpg, .gif, .png, etc),
(b) the program and data used to draw the image,
(c) a 'what they were thinking' description of the process used in
constructing the graph.

To save bandwidth on r-help, I'll ask responders to reply to the list 
only with reactions to this
challenge and what they deem useful to share with all readers.  Other 
ways to reply include
posting a web URL where readers can view the details or a direct email 
reply to me.

-- 
Michael Friendly     Email: friendly at yorku.ca 
Professor, Psychology Dept.
York University      Voice: 416 736-5115 x66249 Fax: 416 736-5814
4700 Keele Street    http://www.math.yorku.ca/SCS/friendly.html
Toronto, ONT  M3J 1P3 CANADA



From luke at stat.uiowa.edu  Wed Aug 27 21:23:16 2003
From: luke at stat.uiowa.edu (Luke Tierney)
Date: Wed, 27 Aug 2003 14:23:16 -0500 (CDT)
Subject: [R] R on Linux/Opteron?
In-Reply-To: <x2n0dw3wt7.fsf@biostat.ku.dk>
Message-ID: <Pine.LNX.4.44.0308271418140.1273-100000@itasca.stat.uiowa.edu>

On 26 Aug 2003, Peter Dalgaard BSA wrote:

> Dirk Eddelbuettel <edd at debian.org> writes:
> 
> > On Tue, Aug 26, 2003 at 03:17:19PM -0400, Liaw, Andy wrote:
> > > Has anyone tried using R on the the AMD Opteron in either 64- or 32-bit
> > > mode?  If so, any good/bad experiences, comments, etc?  We are considering
> > > getting this hardware, and would like to know if R can run smoothly on such
> > > a beast.  Any comment much appreciated.
> > 
> > http://buildd.debian.org/build.php?&pkg=r-base&arch=ia64&file=log
> > 
> > has logs of R builds on ia64 since Nov 2001, incl. the outcome of make
> > check. We do not run the torture tests -- though I guess we could on some of
> > the beefier hardware such as ia64. 
> 
> I don't think that's quite the same beast, though. Opterons are the
> "x86-64" (or amd64) architecture and ia64 is Intel's, aka Itanium.
> Debian appears to be just warming up to including this architecture:
> http://lists.debian.org/debian-x86-64/2003/debian-x86-64-200308/threads.html
> whereas they have had ia64 out for a while.
> 
> SuSE has an Opteron option and Luke said he tried it. Apparently it
> has a functioning 64-bit compiler toolchain - I weren't sure earlier
> whether they were just running a 64bit kernel and 32bit applications,
> but when Luke says so, I believe it...
> 

I wasn't sure either, especially about default settings, but 'file' says

luke/R> file bin/R.bin 
bin/R.bin: ELF 64-bit LSB executable, AMD x86-64, version 1 (SYSV), dynamically linked (uses shared libs), not stripped

and in R

> Sys.info()["machine"]
 machine 
"x86_64" 
> .Machine$sizeof.pointer
[1] 8

So it looks like a functional 64-bit setup so far.

luke

-- 
Luke Tierney
University of Iowa                  Phone:             319-335-3386
Department of Statistics and        Fax:               319-335-3017
   Actuarial Science
241 Schaeffer Hall                  email:      luke at stat.uiowa.edu
Iowa City, IA 52242                 WWW:  http://www.stat.uiowa.edu



From andy_liaw at merck.com  Wed Aug 27 22:07:02 2003
From: andy_liaw at merck.com (Liaw, Andy)
Date: Wed, 27 Aug 2003 16:07:02 -0400
Subject: [R] R on Linux/Opteron?
Message-ID: <3A822319EB35174CA3714066D590DCD50205CA69@usrymx25.merck.com>

Thanks to all (and especially Prof. Tierney) for the response.  The box we
are considering will spend probably over 90% of CPU time in R, so it's
comforting to know that R compiles and pass all the test (at least once) on
such platform.

(I switched my attention from Itanium to Opteron when I read that Itanium is
slower than P4 for number crunching...)

Best,
Andy

> From: Luke Tierney [mailto:luke at stat.uiowa.edu] 
> 
> On 26 Aug 2003, Peter Dalgaard BSA wrote:
> 
> > Dirk Eddelbuettel <edd at debian.org> writes:
> > 
> > > On Tue, Aug 26, 2003 at 03:17:19PM -0400, Liaw, Andy wrote:
> > > > Has anyone tried using R on the the AMD Opteron in 
> either 64- or 
> > > > 32-bit mode?  If so, any good/bad experiences, 
> comments, etc?  We 
> > > > are considering getting this hardware, and would like 
> to know if R 
> > > > can run smoothly on such a beast.  Any comment much appreciated.
> > > 
> > > http://buildd.debian.org/build.php?&pkg=r-base&arch=ia64&file=log
> > > 
> > > has logs of R builds on ia64 since Nov 2001, incl. the outcome of 
> > > make check. We do not run the torture tests -- though I guess we 
> > > could on some of the beefier hardware such as ia64.
> > 
> > I don't think that's quite the same beast, though. Opterons are the 
> > "x86-64" (or amd64) architecture and ia64 is Intel's, aka Itanium. 
> > Debian appears to be just warming up to including this 
> architecture: 
> > 
> http://lists.debian.org/debian-x86-> 64/2003/debian-x86-64-200308/thread
> > s.html
> > whereas they have had ia64 out for a while.
> > 
> > SuSE has an Opteron option and Luke said he tried it. Apparently it 
> > has a functioning 64-bit compiler toolchain - I weren't 
> sure earlier 
> > whether they were just running a 64bit kernel and 32bit 
> applications, 
> > but when Luke says so, I believe it...
> > 
> 
> I wasn't sure either, especially about default settings, but 
> 'file' says
> 
> luke/R> file bin/R.bin 
> bin/R.bin: ELF 64-bit LSB executable, AMD x86-64, version 1 
> (SYSV), dynamically linked (uses shared libs), not stripped
> 
> and in R
> 
> > Sys.info()["machine"]
>  machine 
> "x86_64" 
> > .Machine$sizeof.pointer
> [1] 8
> 
> So it looks like a functional 64-bit setup so far.
> 
> luke
> 
> -- 
> Luke Tierney
> University of Iowa                  Phone:             319-335-3386
> Department of Statistics and        Fax:               319-335-3017
>    Actuarial Science
> 241 Schaeffer Hall                  email:      luke at stat.uiowa.edu
> Iowa City, IA 52242                 WWW:  http://www.stat.uiowa.edu
> 
> 

------------------------------------------------------------------------------
Notice:  This e-mail message, together with any attachments,...{{dropped}}



From jg_liao at yahoo.com  Wed Aug 27 22:22:33 2003
From: jg_liao at yahoo.com (Jason Liao)
Date: Wed, 27 Aug 2003 13:22:33 -0700 (PDT)
Subject: [R] testing if two multivariate samples are from the same
	distribution
Message-ID: <20030827202233.86328.qmail@web10501.mail.yahoo.com>

Hello, everyone! I wonder if any R package can do the multivariate
Smirnov test. Specifically, let x_1,..,x_n and y_1,...,y_m be
multivariate vectors. I would like to test if the two samples are from
the same underlying multivariate distribution. Thanks in advance.

Jason


=====
Jason G. Liao, Ph.D.
Division of Biometrics
University of Medicine and Dentistry of New Jersey
335 George Street, Suite 2200
New Brunswick, NJ 08903-2688
phone (732) 235-8611, or (732)-235-5429
http://www.geocities.com/jg_liao



From ihaka at stat.auckland.ac.nz  Wed Aug 27 23:46:40 2003
From: ihaka at stat.auckland.ac.nz (Ross Ihaka)
Date: Thu, 28 Aug 2003 09:46:40 +1200
Subject: [R] Re: diamond graphs
In-Reply-To: <009801c36cc2$60ce64a0$0d98120a@statepidm.jhsph.edu>
References: <ADEPLELNMGDGKCPEDINLKEJGKBAD.szeger@jhsph.edu>	<20030825172828.3d5b9872.fharrell@virginia.edu>
	<009801c36cc2$60ce64a0$0d98120a@statepidm.jhsph.edu>
Message-ID: <3F4D26C0.5090805@stat.auckland.ac.nz>

Alvaro Mu?oz wrote:
> Drs. Harrell and O'Keefe,

> Although it is at odds with your beliefs, University staff working on
> licensing and technology transfer believe that a patent may be a vehicle to
> achieve a wide use. The audience of the proposed methods would be the end
> users who are not sophisticated programmers and, therefore, the hope is that
> it would be available in widely used software which is not the case of the
> high end software (e.g., R). The proposed graph of 2D equiponderant display
> of two predictors is just a display procedure, not an inferential tool. The
> sophisticated analyst has little or no need for the proposed method. It does
> overcome the pitfalls of 3D bar graphs and, therefore, has the potential of
> improving the way we communicate our findings. Needless to say, were the
> predictions of Dr. Harrell to be on target, we will change course as the
> staff working on the licensing have planned from the start.

Perhaps I can add some personal experience, as opposed to "belief".
After Robert Gentleman and I had made some initial progress in 
implementing R, we had to make some decisions about what we would do 
with it.  We looked at a number of options ranging from "something 
commercial" to "free software".  After some research, personal 
introspection and prompting from others (hi Martin :-) we decided to 
release under GPL.

For me personally this turned out to be far harder than I thought it 
would be.  My institution has a particularly diabolical policy on 
intellectual property, especially on software.  While we could have 
quietly released the software and just said "oops" later on, I chose to 
get approval for free release of my work.  This took a number of years, 
several threats of resignation and a couple of salary cuts.

The reason I mention this is not as a part of a personal campaign for 
sainthood, but rather because it has utimately turned out to have been 
far more than worth the effort.  The effect of making R free has been 
see it picked up and vastly improved and extended by a very talented 
group of researchers.  We've now reached a point which Robert and I and 
other early R adopters and contributors couldn't have anticipated in our 
wildest imaginings. It's truly amazing to see this software being used 
for all sorts of cool things.  What we are seeing represents the best of 
what being an academic is all about - the free exchange of ideas with 
researchers collaborating and building on each other's work.

On the other hand, I'm currently writing what will possibly become a 
book on visualization and graphics (publication mechanism uncertain). 
The techniques discussed in the book are implemented in a certain dialog 
of a particular computer language developed at Bell Labs.  I intend to 
include code libraries for all the graphical techniques discussed.  The 
fact that you have sought to patent your idea means that, whatever its 
merits, it's pointless for me to even mention it because I can't 
distribute code for it.

I'm sure the licensing gnomes at your institution have expounded on how 
patenting will help achieve wider use, but in reality they are simply 
thinking "revenue stream".   The likely real effect of of constraining 
access to your work in this way will be to have it sink into obscurity. 
Take it from one who's been there, the payoff from free dissemination is 
much higher.

-- 
Ross Ihaka                         Email:  ihaka at stat.auckland.ac.nz
Department of Statistics           Phone:  (64-9) 373-7599 x 85054
University of Auckland             Fax:    (64-9) 373-7018
Private Bag 92019, Auckland
New Zealand



From bido at mac.com  Thu Aug 28 00:28:04 2003
From: bido at mac.com (Francisco J. Bido)
Date: Wed, 27 Aug 2003 17:28:04 -0500
Subject: [R] Newbie graphing questions
Message-ID: <B9627E38-D8DD-11D7-9DAC-000393B90A0A@mac.com>

Hi everyone.  R is new to me and I'm very impressed with its 
capabilities but still cannot figure out how to do some basic things.  
There seems to be no lack of documentation but finding what I need has 
proven difficult.  Perhaps you can help.

Here's what I'm after:

1.  How do I create a new plot without erasing the prior one i.e., have 
a new window pop up with the new graph? I'm on MacOSX using the Carbon 
port.

2.  How do I pause between plot renderings i.e., in such a way that it 
will draw the subsequent graph after pressing the space bar (or any 
other key).

3.  Illustrating critical regions.  Say I wanted to illustrate the 
critical region of a standard normal.  I would need to draw a vertical 
line from the critical point to the curve and then shade the critical 
region.  How do I do this in R?

Thanks!
-Francisco



From p.connolly at hortresearch.co.nz  Thu Aug 28 00:48:24 2003
From: p.connolly at hortresearch.co.nz (Patrick Connolly)
Date: Thu, 28 Aug 2003 10:48:24 +1200
Subject: [R] Re: diamond graphs and patents
In-Reply-To: <009801c36cc2$60ce64a0$0d98120a@statepidm.jhsph.edu>
References: <ADEPLELNMGDGKCPEDINLKEJGKBAD.szeger@jhsph.edu>
	<20030825172828.3d5b9872.fharrell@virginia.edu>
	<009801c36cc2$60ce64a0$0d98120a@statepidm.jhsph.edu>
Message-ID: <20030827224824.GC11971@hortresearch.co.nz>

On Wed, 27-Aug-2003 at 01:40PM -0400, Alvaro Muoz wrote:

|> Drs. Harrell and O'Keefe,
|> 
|> 
|> 
|> Thank you for your suggestions.
|> 

|> Although it is at odds with your beliefs, University staff working
|> on licensing and technology transfer believe that a patent may be a
|> vehicle to achieve a wide use.

Wider than if it were public domain?

It's probably fair to say that it is an improvement over 3D bar charts
and that it can be built into point and click software more easily
than, say, contour plots could.  If Microsoft were to pay the
university a licensing fee to incorporate such plots, then 'wide use'
could result and the university would benefit.  But that's a big IF.

best

-- 
Patrick Connolly
HortResearch
Mt Albert
Auckland
New Zealand 
Ph: +64-9 815 4200 x 7188
~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~
I have the world`s largest collection of seashells. I keep it on all
the beaches of the world ... Perhaps you`ve seen it.  ---Steven Wright 
~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~



From maj at stats.waikato.ac.nz  Thu Aug 28 01:19:01 2003
From: maj at stats.waikato.ac.nz (maj@stats.waikato.ac.nz)
Date: Thu, 28 Aug 2003 11:19:01 +1200 (NZST)
Subject: [R] Using files as connections
Message-ID: <3470.203.79.123.178.1062026341.squirrel@webmail.scms.waikato.ac.nz>

I have been trying to read a random sample of lines from a file into a
data frame using readLines(). The help indicates that readLines() will
start from the current line if the connection is open, but presented with
a closed connection it will open it, start from the beginning, and close
it when finished.

In the code that follows I tried to open the file before reading but
apparently without success, because the result was repeated copies of the
first line:

flines <- 107165
slines <- 100
selected <- sort(sample(flines,slines))
strvec <- rep(??,slines)
file(?c:/data/perry/data.csv?,open="r")
isel <- 0
for (iline in 1:slines) {
  isel <- isel + 1
  cline <- readLines(?c:/data/perry/data.csv?,n=1)
  if (iline == selected[isel]) strvec[isel] <- cline else
    isel <- isel - 1
}
close(?c:/data/perry/data.csv?)
sel.flows <- read.table(textConnection(strvec), header=FALSE, sep=",")


There was also an error "no applicable method"  for close.

Comments gratefully received.

Murray Jorgensen



From Arnaud.Dowkiw at dpi.qld.gov.au  Thu Aug 28 01:25:21 2003
From: Arnaud.Dowkiw at dpi.qld.gov.au (Dowkiw, Arnaud)
Date: Thu, 28 Aug 2003 09:25:21 +1000
Subject: [R] equivalent of SAS's PROC LATTICE
Message-ID: <200308272325.h7RNPUP9013380@dpi-gw1.dpi.qld.gov.au>

Dear R helpers,
 
is there an equivalent of SAS's PROC LATTICE in R ?
I have a partially balanced square lattice design to study and I want to compare lattice based analysis with the randomized complete block approach.
Thanks a lot for your help,
 
Arnaud 

********************************DISCLAIMER******************...{{dropped}}



From s195404 at student.uq.edu.au  Thu Aug 28 01:26:33 2003
From: s195404 at student.uq.edu.au (Andrew C. Ward)
Date: Wed, 27 Aug 2003 23:26:33 +0000
Subject: [R] Newbie graphing questions
In-Reply-To: <B9627E38-D8DD-11D7-9DAC-000393B90A0A@mac.com>
References: <B9627E38-D8DD-11D7-9DAC-000393B90A0A@mac.com>
Message-ID: <1062026793.3f4d3e2993ecd@my.uq.edu.au>

Dear Francisco,

1. Have a look at ?Devices which (under Windows at least)
   lists a range of devices you can open before and
   between plots.
2. Use par(ask=TRUE) before you start your plots
3. Christophe Declercq provided an excellent example on this
   mailing list, under the topic "curves with shaded areas".
   Search for that in the archives.


Regards,

Andrew C. Ward

CAPE Centre
Department of Chemical Engineering
The University of Queensland
Brisbane Qld 4072 Australia
andreww at cheque.uq.edu.au


Quoting "Francisco J. Bido" <bido at mac.com>:

> Hi everyone.  R is new to me and I'm very impressed with
> its 
> capabilities but still cannot figure out how to do some
> basic things.  
> There seems to be no lack of documentation but finding
> what I need has 
> proven difficult.  Perhaps you can help.
> 
> Here's what I'm after:
> 
> 1.  How do I create a new plot without erasing the prior
> one i.e., have 
> a new window pop up with the new graph? I'm on MacOSX
> using the Carbon 
> port.
> 
> 2.  How do I pause between plot renderings i.e., in such
> a way that it 
> will draw the subsequent graph after pressing the space
> bar (or any 
> other key).
> 
> 3.  Illustrating critical regions.  Say I wanted to
> illustrate the 
> critical region of a standard normal.  I would need to
> draw a vertical 
> line from the critical point to the curve and then shade
> the critical 
> region.  How do I do this in R?
> 
> Thanks!
> -Francisco
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
>



From d.scott at auckland.ac.nz  Thu Aug 28 01:31:15 2003
From: d.scott at auckland.ac.nz (David Scott)
Date: Thu, 28 Aug 2003 11:31:15 +1200 (NZST)
Subject: [R] Re: diamond graphs and patents
In-Reply-To: <20030827224824.GC11971@hortresearch.co.nz>
Message-ID: <Pine.LNX.4.44.0308281107130.13175-100000@hydra.stat.auckland.ac.nz>


My reaction when learning of a proposed patent on a new graph was: "oh 
well, that's something I can forget about". Without a patent, code would 
have been available in R in a very short period of time, the statistical 
community would have been able to play around with it, see how it worked 
on various problems. If the graph proved useful it would make its way into 
statistical practice. With a patent none of that seems possible. Prof 
Munoz has had fun exploring his creation, but if any of us are to do 
likewise I guess we will either pay up or secretly write code and play 
around with diamond graphs while hidden in the basement.

Getting a graph used is not that simple I think. Boxplots are now an 
extremely useful tool, but lets not forget that Tukey also invented the 
hanging rootogram.

As for Microsoft getting involved, God help us. Excel still doesn't do 
boxplots does it? Not to mention the quality of their implementation of 
most of their statistical routines.

Like others I welcome the contribution of those at Johns Hopkins to the 
debate. And they perhaps shouldn't worry about us here at R-help, who are 
maybe just a fringe element of open source loving anti-M$ weirdos. Then 
again we may be the last bastion of open scientific enquiry.

David Scott

_________________________________________________________________
David Scott	Department of Statistics, Tamaki Campus
		The University of Auckland, PB 92019
		Auckland	NEW ZEALAND
Phone: +64 9 373 7599 ext 86830		Fax: +64 9 373 7000
Email:	d.scott at auckland.ac.nz 


Graduate Officer, Department of Statistics

Webmaster, New Zealand Statistical Association:
        http://www.stat.auckland.ac.nz/nzsa/



From jc at or.psychology.dal.ca  Thu Aug 28 01:32:27 2003
From: jc at or.psychology.dal.ca (John Christie)
Date: Wed, 27 Aug 2003 20:32:27 -0300
Subject: [R] trying to produce an array of ranks
Message-ID: <B86A3F74-D8E6-11D7-81B8-000A9566473A@or.psychology.dal.ca>

OK, I am try to produce an array of ranks.  I have a set of data (s) 
that looks like this

rt	subj
312	dave
467	dave
411	dave
383	kim
398	kim
...

Now I want to make a column that is an array of the ranks of rt by 
subject.  The closest of gotten is using the following.

r <- by (s, s$subj, function(d) rank(d[1]))

This gets the data out with ranks but I cannot figure out how to easily 
extract each of the lists and turn the whole thing into a nice straight 
array (it is an array of lists when unclassed).  I couldn't get 
anything working with any of the apply's or ave.  Any suggestions?  
This seems like it should be a common task.



From andy_liaw at merck.com  Thu Aug 28 01:49:36 2003
From: andy_liaw at merck.com (Liaw, Andy)
Date: Wed, 27 Aug 2003 19:49:36 -0400
Subject: [R] Using files as connections
Message-ID: <3A822319EB35174CA3714066D590DCD50205CA71@usrymx25.merck.com>

You are using the connection the wrong way.  You need to do something like:

fcon <- file("c:/data/perry/data.csv", open="r")
for (iline in 1:slines) {
    isel <- isel + 1
    cline <- readLines(fcon, n=1)
    ...
}
close(fcon)

BTW, here's how I'd do it (not tested!):

strvec <- rep("",slines)
selected <- sort(sample(flines, slines))
skip <- c(0, diff(selected) - 1)
fcon <- file("c:/data/[erry/data.csv", open="r")
for (i in 1:length(skip)) {
    ## skip to the selected line
    readLines(fcon, n=skip[i])
    strvec[i] <- readLines(fcon, n=1)
}
close(fcon)

HTH,
Andy


> -----Original Message-----
> From: maj at stats.waikato.ac.nz [mailto:maj at stats.waikato.ac.nz] 
> Sent: Wednesday, August 27, 2003 7:19 PM
> To: r-help at stat.math.ethz.ch
> Subject: [R] Using files as connections
> 
> 
> I have been trying to read a random sample of lines from a 
> file into a data frame using readLines(). The help indicates 
> that readLines() will start from the current line if the 
> connection is open, but presented with a closed connection it 
> will open it, start from the beginning, and close it when finished.
> 
> In the code that follows I tried to open the file before 
> reading but apparently without success, because the result 
> was repeated copies of the first line:
> 
> flines <- 107165
> slines <- 100
> selected <- sort(sample(flines,slines))
> strvec <- rep("",slines)
> file("c:/data/perry/data.csv",open="r")
> isel <- 0
> for (iline in 1:slines) {
>   isel <- isel + 1
>   cline <- readLines("c:/data/perry/data.csv",n=1)
>   if (iline == selected[isel]) strvec[isel] <- cline else
>     isel <- isel - 1
> }
> close("c:/data/perry/data.csv")
> sel.flows <- read.table(textConnection(strvec), header=FALSE, sep=",")
> 
> 
> There was also an error "no applicable method"  for close.
> 
> Comments gratefully received.
> 
> Murray Jorgensen
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list 
> https://www.stat.math.ethz.ch/mailman/listinfo> /r-help
> 

------------------------------------------------------------------------------
Notice:  This e-mail message, together with any attachments,...{{dropped}}



From tplate at blackmesacapital.com  Thu Aug 28 01:54:12 2003
From: tplate at blackmesacapital.com (Tony Plate)
Date: Wed, 27 Aug 2003 17:54:12 -0600
Subject: [R] Using files as connections
In-Reply-To: <3470.203.79.123.178.1062026341.squirrel@webmail.scms.waika
	to.ac.nz>
Message-ID: <5.2.1.1.2.20030827174003.04228210@mailhost.blackmesacapital.com>

You need to save the connection object returned by file() and then use that 
object in other functions.

You need to change the appropriate lines to the following (at least):

>con <- file("c:/data/perry/data.csv",open="r")
>   cline <- readLines(con,n=1)
>close(con)

(I don't know if more changes are needed to get it working.)

Note that using the connection object in other functions can have side 
effects on the connection object (which is how a connection "remembers" its 
point in the file.) (Perhaps more accurately, the side effect is on the 
internal system data referred to by the R connection object.)

 > con <- textConnection(letters)
 > con
      description            class             mode             text
        "letters" "textConnection"              "r"           "text"
           opened         can read        can write
         "opened"            "yes"             "no"
 > readLines(con, 1)
[1] "a"
 > readLines(con, 1)
[1] "b"
 > con.saved <- con
 > readLines(con, 1)
[1] "c"
 > readLines(con.saved, 1)
[1] "d"
 > readLines(con, 1)
[1] "e"
 > identical(con, con.saved)
[1] TRUE
 > showConnections()
   description class            mode text   isopen   can read can write
3 "letters"   "textConnection" "r"  "text" "opened" "yes"    "no"
 >
 >

hope this helps,

Tony Plate

At Thursday 11:19 AM 8/28/2003 +1200, you wrote:
>I have been trying to read a random sample of lines from a file into a
>data frame using readLines(). The help indicates that readLines() will
>start from the current line if the connection is open, but presented with
>a closed connection it will open it, start from the beginning, and close
>it when finished.
>
>In the code that follows I tried to open the file before reading but
>apparently without success, because the result was repeated copies of the
>first line:
>
>flines <- 107165
>slines <- 100
>selected <- sort(sample(flines,slines))
>strvec <- rep("",slines)
>file("c:/data/perry/data.csv",open="r")
>isel <- 0
>for (iline in 1:slines) {
>   isel <- isel + 1
>   cline <- readLines("c:/data/perry/data.csv",n=1)
>   if (iline == selected[isel]) strvec[isel] <- cline else
>     isel <- isel - 1
>}
>close("c:/data/perry/data.csv")
>sel.flows <- read.table(textConnection(strvec), header=FALSE, sep=",")
>
>
>There was also an error "no applicable method"  for close.
>
>Comments gratefully received.
>
>Murray Jorgensen
>
>______________________________________________
>R-help at stat.math.ethz.ch mailing list
>https://www.stat.math.ethz.ch/mailman/listinfo/r-help



From dunn at usq.edu.au  Thu Aug 28 01:59:43 2003
From: dunn at usq.edu.au (Peter Dunn)
Date: Wed, 27 Aug 2003 23:59:43 -0000
Subject: [R] R workshop in Australia
Message-ID: <1062028882.22783.48.camel@grover.sci.usq.edu.au>

Need some help with R?

An R workshop, led by John Maindonald, is planned for the 1st October
at the University of Southern Queensland, Toowoomba, Australia. The
workshop is being organised by the Queensland branch of the
Statistical Society of Australia as part of their conference.

The workshop consists of a morning session with John Maindonald,
the author of the forthcoming book  "Data Analysis and Graphics 
Using R: An Example-Based Approach" (with John Braun)

This will be followed by a closer look at some other R packages
in the afternoon (Bioconductor, nlme, etc).  

The cost for the one-day workshop is only $50 for the whole day
(includes notes, morning and afternoon tea,  but not lunch).

Anyone interested in attending can find out more by looking at 
http://www.sci.usq.edu.au/staff/dunn/qstatconf/workshops.html
Registration forms are available from 
http://www.sci.usq.edu.au/staff/dunn/qstatconf/registration.html

Anyone is, of course, welcome to attend the conference also,
but it is not necessary if you just wish to attend the
R workshop.

-- 
Peter Dunn
Organising committee
Email: <dunn at usq.edu.au>



From dunn at usq.edu.au  Thu Aug 28 02:02:39 2003
From: dunn at usq.edu.au (Peter Dunn)
Date: Thu, 28 Aug 2003 00:02:39 -0000
Subject: [R] R workshop in Australia
Message-ID: <1062029046.26086.1.camel@grover.sci.usq.edu.au>

Need some help with R? 

An R workshop, led by John Maindonald, is planned for the 1st October 
at the University of Southern Queensland, Toowoomba, Australia. The 
workshop is being organised by the Queensland branch of the 
Statistical Society of Australia as part of their conference. 

The workshop consists of a morning session with John Maindonald, 
the author of the forthcoming book  "Data Analysis and Graphics 
Using R: An Example-Based Approach" (with John Braun) 

This will be followed by a closer look at some other R packages 
in the afternoon (Bioconductor, nlme, etc).  

The cost for the one-day workshop is only $50 for the whole day 
(includes notes, morning and afternoon tea,  but not lunch). 

Anyone interested in attending can find out more by looking at 
http://www.sci.usq.edu.au/staff/dunn/qstatconf/workshops.html
Registration forms are available from 
http://www.sci.usq.edu.au/staff/dunn/qstatconf/registration.html

Anyone is, of course, welcome to attend the conference also, 
but it is not necessary if you just wish to attend the 
R workshop. 

-- 
Ross Darnell 
Organising committee 
Email: <r.darnell at uq.edu.au> 

-- 
Dr Peter Dunn          (USQ CRICOS No. 00244B)
  Web:    http://www.sci.usq.edu.au/staff/dunn
  Email:  dunn @ usq.edu.au
Opinions expressed are mine, not those of USQ.  Obviously...



From kwan022 at stat.auckland.ac.nz  Thu Aug 28 02:27:51 2003
From: kwan022 at stat.auckland.ac.nz (Ko-Kang Kevin Wang)
Date: Thu, 28 Aug 2003 12:27:51 +1200 (NZST)
Subject: [R] Re: diamond graphs and patents
In-Reply-To: <Pine.LNX.4.44.0308281107130.13175-100000@hydra.stat.auckland.ac.nz>
Message-ID: <Pine.LNX.4.44.0308281213270.29475-100000@stat61.stat.auckland.ac.nz>

I have been reading this "discussion" (or debate, depends on your point of 
view) with great interest in the last few days.

On Thu, 28 Aug 2003, David Scott wrote:

> My reaction when learning of a proposed patent on a new graph was: "oh 
> well, that's something I can forget about". Without a patent, code would 
> have been available in R in a very short period of time, the statistical 
> community would have been able to play around with it, see how it worked 
> on various problems. If the graph proved useful it would make its way into 
> statistical practice. With a patent none of that seems possible. Prof 
> Munoz has had fun exploring his creation, but if any of us are to do 
> likewise I guess we will either pay up or secretly write code and play 
> around with diamond graphs while hidden in the basement.

I agree.  The question, IMHO, is not whether the diamond graph is a good 
visualising tool or not -- because it still remains to be seen, but on the 
idea of having it patented.

I completely agree with Dr. Ihaka that the only reason that makes R so 
popular and widely used across the academia, research institutes and even 
many large commercial companies, is that it is "free" and "open-sourced".

It is certainly going to be interesting to write up some functions in R 
that draws diamond graphs, and then test them on some simulated and real 
world data.  But as Associate Professor Scott pointed out, with a patent 
on diamond graph (should the application be accepted), I am not sure if 
this is possible.

With patenting in mind, I have been think what would the world like if 
histograms, bargraphs, boxplots...etc. were patented by the original 
inventors!  The idea of patenting a graph seems to me like patenting a 
mathematical/statistical theorem.  Again, if for example the Central 
Limit theorem is patented, does it mean we have to pay whenever we want to 
make inference from it?

There are many "software" that offers educational institution an academic 
license, and offers students a student license.  But the thought of the 
possibility of having to pay for using a graph (or writing codes to draw 
the graph) is.........

> Getting a graph used is not that simple I think. Boxplots are now an 
> extremely useful tool, but lets not forget that Tukey also invented the 
> hanging rootogram.

What is a hanging rootogram? ;-D

> As for Microsoft getting involved, God help us. Excel still doesn't do 
> boxplots does it? Not to mention the quality of their implementation of 
> most of their statistical routines.

Even those graphs that Excel does do, e.g. histograms, it does not do a 
very good job in them.  It has been, what, over a decade now since the 
first version of Excel was released?  I cannot keep myself from wondering 
why Excel still produces ugly graphs.


-- 
Cheers,

Kevin

------------------------------------------------------------------------------
"On two occasions, I have been asked [by members of Parliament],
'Pray, Mr. Babbage, if you put into the machine wrong figures, will
the right answers come out?' I am not able to rightly apprehend the
kind of confusion of ideas that could provoke such a question."

-- Charles Babbage (1791-1871) 
---- From Computer Stupidities: http://rinkworks.com/stupid/

--
Ko-Kang Kevin Wang
Master of Science (MSc) Student
SLC Tutor and Lab Demonstrator
Department of Statistics
University of Auckland
New Zealand
Homepage: http://www.stat.auckland.ac.nz/~kwan022
Ph: 373-7599
    x88475 (City)
    x88480 (Tamaki)



From ok at cs.otago.ac.nz  Thu Aug 28 02:35:33 2003
From: ok at cs.otago.ac.nz (Richard A. O'Keefe)
Date: Thu, 28 Aug 2003 12:35:33 +1200 (NZST)
Subject: [R] R tools for large files
Message-ID: <200308280035.h7S0ZXG6250097@atlas.otago.ac.nz>

Duncan Murdoch <dmurdoch at pair.com> wrote:
	One complication with reading a block at a time is what to do when you
	read too far.

It's called "buffering".

	Not all connections can use seek() to reposition to the
	beginning, so you'd need to read them one character at a time, (or
	attach a buffer somehow, but then what about rw connections?)
	
You don't need seek() to do buffered block-at-a-time reading.
For example, you can't lseek() on a UNIX terminal, but UNIX C stdio
*does* read a block at a time from a terminal.

I don't see what the problem with read-write connections is supposed
to be.  When you want to read from such a connection, you first force
out any buffered output, and then you read a buffer's worth (if
available) of input.  Of course the read buffer and the write buffer
are separate (C stdio has traditionally got this wrong, with the perverse
consequence that you have to fseek() when switching from reading to writing
or vice versa, but that doesn't mean it can't be got right).


To put all this in context though, remember that S was designed in a UNIX
environment to work in a UNIX environment and it was always intended to
exploit UNIX tools.  Even on a Windows box, if you get R, you get a
bunch of the usual UNIX tools with it.  Amongst other things, Perl is
freely available for Windows, a Perl program to read a couple of
hundred thousand records and spit them out in platform binary would
only be a few lines long, and R _is_ pretty good at reading binary data.
It really is important that R users should be allowed to use it the way
that the language was designed to be used.



From ealaca at ucdavis.edu  Thu Aug 28 02:52:43 2003
From: ealaca at ucdavis.edu (Emilio A. Laca)
Date: Wed, 27 Aug 2003 17:52:43 -0700
Subject: [R] Packages
Message-ID: <BB72A06B.6CBD%ealaca@ucdavis.edu>


I need to install some geostatistics packages that are not in the binaries,
for example gstat. I downloaded rm171 and installed, and I even added
precomplied packages successfully. Unfortunately, I do not know how to add
packages available as gstat. If you can spare a minute, could you give me a
lead on this?
Any other pointers on using R in mac os x will be greatly appreciated.

Thank you so much for your time on this question, and for everything you put
into rmac!

EAL


==============================================================
Emilio A. Laca     
One Shields Avenue, 2306 PES Building
Agronomy and Range Science                    ealaca at ucdavis.edu
University of California                      fax: (530) 752-4361
Davis, California  95616                            (530) 754-4083



From hvr at van-rijn.org  Thu Aug 28 03:54:37 2003
From: hvr at van-rijn.org (Hedderik van Rijn)
Date: Wed, 27 Aug 2003 21:54:37 -0400
Subject: [R] Newbie graphing questions
In-Reply-To: <B9627E38-D8DD-11D7-9DAC-000393B90A0A@mac.com>
Message-ID: <948907C4-D8FA-11D7-A9BB-000A956B93BA@van-rijn.org>

> 1.  How do I create a new plot without erasing the prior one i.e.,  
> have a new window pop up with the new graph? I'm on MacOSX using the  
> Carbon port.

I don't know about the Carbon version, but ?Devices should give you a  
list of "Devices" you can use to output graphs to. (Using the fink  
version on OS X, X11() would do what you describe.)

> 2.  How do I pause between plot renderings i.e., in such a way that it  
> will draw the subsequent graph after pressing the space bar (or any  
> other key).

par(ask=T)

See ?par for more useful plotting options.

> 3.  Illustrating critical regions.  Say I wanted to illustrate the  
> critical region of a standard normal.  I would need to draw a vertical  
> line from the critical point to the curve and then shade the critical  
> region.  How do I do this in R?

Based on some code found on the R mailing list some time ago, I was  
able to come up with the following: (it shades an area of a lognormal  
density plot after a certain deadline)

deadline <- 800
mthres <- 6.5
sthres <- 0.15

ymax <- 0.005
xmax <- 1200

plot(0,0,xlim=c(0,xmax),ylim=c(0,ymax),type="n")

thres.xvals <- dlnorm(1:xmax,meanlog=mthres,sdlog=sthres)
lines(thres.xvals,col="blue")

abline(v=deadline,col="darkgrey")
polygon(c((deadline+1):xmax,rev((deadline+1):xmax)),c(rep(0,(xmax- 
deadline)),rev(thres.xvals[(deadline+1):xmax])),density=20,col="blue")

  - Hedderik.



From MSchwartz at medanalytics.com  Thu Aug 28 04:36:55 2003
From: MSchwartz at medanalytics.com (Marc Schwartz)
Date: Wed, 27 Aug 2003 21:36:55 -0500
Subject: [R] Newbie graphing questions
In-Reply-To: <B9627E38-D8DD-11D7-9DAC-000393B90A0A@mac.com>
References: <B9627E38-D8DD-11D7-9DAC-000393B90A0A@mac.com>
Message-ID: <1062038214.21093.53.camel@localhost>

On Wed, 2003-08-27 at 17:28, Francisco J. Bido wrote: 
> Hi everyone.  R is new to me and I'm very impressed with its 
> capabilities but still cannot figure out how to do some basic things.  
> There seems to be no lack of documentation but finding what I need has 
> proven difficult.  Perhaps you can help.
> 
> Here's what I'm after:
> 
> 1.  How do I create a new plot without erasing the prior one i.e., have 
> a new window pop up with the new graph? I'm on MacOSX using the Carbon 
> port.

In general, if you want to leave the existing device open and have a new
device open for a new plot, you simply call the device name that you
want to open (ie. under Linux you would use X11() ) to open a new
plotting device on the display.  See ?Devices for more details.

> 2.  How do I pause between plot renderings i.e., in such a way that it 
> will draw the subsequent graph after pressing the space bar (or any 
> other key).

Set 'par(ask = TRUE)' before plotting. See ?par

> 3.  Illustrating critical regions.  Say I wanted to illustrate the 
> critical region of a standard normal.  I would need to draw a vertical 
> line from the critical point to the curve and then shade the critical 
> region.  How do I do this in R?

# Generate a sequence of x values
x <- seq(-3, 3, by = 0.001)

# Plot normal curve over x
plot(x, dnorm(x), type = "l")

# Define left side boundary using min(x)
# and CritVal using alpha = 0.05

alpha <- 0.05
CritVal <- qnorm(alpha / 2)

x.l <- seq(min(x), CritVal, length = 100)
y.l <- c(dnorm(x.l), 0, 0)

# add CritVal, min(x) to complete polygon
x.l <- c(x.l, CritVal, min(x))

# draw and fill left region
polygon(x.l, y.l, density = 50)


# Do the same for right side boundary using max(x)
# and Crit Val

CritVal <- qnorm(1 - (alpha / 2))

x.r <- seq(CritVal, max(x), length = 100)
y.r <- c(dnorm(x.r), 0, 0)

# add max(x) and CritVal to complete polygon
x.r <- c(x.r, max(x) , CritVal)

# draw and fill left region
polygon(x.r, y.r, density = 50)



HTH,

Marc Schwartz



From bido at mac.com  Thu Aug 28 05:18:21 2003
From: bido at mac.com (Francisco J. Bido)
Date: Wed, 27 Aug 2003 22:18:21 -0500
Subject: [R] Newbie graphing questions
In-Reply-To: <1062038214.21093.53.camel@localhost>
Message-ID: <47312846-D906-11D7-8C06-000393B90A0A@mac.com>

Thanks Hedderik, Andrew, David and of course, you Mark.  You have all 
been very helpful.  I got good answers two 1. and 2. and arsenal of 
ideas for 3.  Mark's solution for 3. is exactly what I was thinking 
about.  I'm indeed very grateful.

Best,
-Francisco





On Wednesday, August 27, 2003, at 09:36 PM, Marc Schwartz wrote:

> On Wed, 2003-08-27 at 17:28, Francisco J. Bido wrote:
>> Hi everyone.  R is new to me and I'm very impressed with its
>> capabilities but still cannot figure out how to do some basic things.
>> There seems to be no lack of documentation but finding what I need has
>> proven difficult.  Perhaps you can help.
>>
>> Here's what I'm after:
>>
>> 1.  How do I create a new plot without erasing the prior one i.e., 
>> have
>> a new window pop up with the new graph? I'm on MacOSX using the Carbon
>> port.
>
> In general, if you want to leave the existing device open and have a 
> new
> device open for a new plot, you simply call the device name that you
> want to open (ie. under Linux you would use X11() ) to open a new
> plotting device on the display.  See ?Devices for more details.
>
>> 2.  How do I pause between plot renderings i.e., in such a way that it
>> will draw the subsequent graph after pressing the space bar (or any
>> other key).
>
> Set 'par(ask = TRUE)' before plotting. See ?par
>
>> 3.  Illustrating critical regions.  Say I wanted to illustrate the
>> critical region of a standard normal.  I would need to draw a vertical
>> line from the critical point to the curve and then shade the critical
>> region.  How do I do this in R?
>
> # Generate a sequence of x values
> x <- seq(-3, 3, by = 0.001)
>
> # Plot normal curve over x
> plot(x, dnorm(x), type = "l")
>
> # Define left side boundary using min(x)
> # and CritVal using alpha = 0.05
>
> alpha <- 0.05
> CritVal <- qnorm(alpha / 2)
>
> x.l <- seq(min(x), CritVal, length = 100)
> y.l <- c(dnorm(x.l), 0, 0)
>
> # add CritVal, min(x) to complete polygon
> x.l <- c(x.l, CritVal, min(x))
>
> # draw and fill left region
> polygon(x.l, y.l, density = 50)
>
>
> # Do the same for right side boundary using max(x)
> # and Crit Val
>
> CritVal <- qnorm(1 - (alpha / 2))
>
> x.r <- seq(CritVal, max(x), length = 100)
> y.r <- c(dnorm(x.r), 0, 0)
>
> # add max(x) and CritVal to complete polygon
> x.r <- c(x.r, max(x) , CritVal)
>
> # draw and fill left region
> polygon(x.r, y.r, density = 50)
>
>
>
> HTH,
>
> Marc Schwartz
>
>



From tom.kennedy at bigpond.com  Thu Aug 28 05:40:19 2003
From: tom.kennedy at bigpond.com (Tom Kennedy)
Date: Thu, 28 Aug 2003 13:40:19 +1000
Subject: [R] Plotting asymptote possible with plot?
Message-ID: <3F4D79A3.3060003@bigpond.com>

I am calculating LOD scores in genetic epidemiology where the number of 
non recombinants is 11 and recombinants is 1. I created a function to 
calculate the z scores for the following thetas.

theta <- c(0,0.01,0.05,0.10,0.20,0.45,0.5)

The z scores for the respective thetas in this cases are:

-Inf,1.56,2.07,2.11,1.85,0.410,0.0

I attempt to plot these values using the following commands.

plot(theta, zscore, xlab = "theta", ylab = "LOD Score", type = "l", ylim 
= c(-5,5), panel.first = grid(5,16), panel.last = abline(h=0),axes=F)

axis(side = 1,at = c(0,.1,.2,.3,.4,.5))

axis(side = 2,at = c(-5,-4,-3,-2,-1,0,1,2,3,4,5))

The plot works but does not plot from the points (0,-Inf) to 
(0.01,1.56). As this is an asymptotic value, I'm unsure that the plot 
function can plot to -Inf.

I am using R 1.7.1. on WinXP sp1.

Is there a way to plot the asymptote?

Regards Tom



From ripley at stats.ox.ac.uk  Thu Aug 28 08:25:36 2003
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Thu, 28 Aug 2003 07:25:36 +0100 (BST)
Subject: [R] Packages
In-Reply-To: <BB72A06B.6CBD%ealaca@ucdavis.edu>
Message-ID: <Pine.LNX.4.44.0308280717480.31969-100000@gannet.stats>

Could you clarify that you want to do this on the Carbon MacOS port, and 
not the Darwin MacOS X port?

Since gstat uses Unix-like configure tools, I think this would need a
_lot_ of expertise, something not worth doing for the last release of the 
Carbon port.

If you are running MacOS X, why don't you try to do it under the Darwin
port (with the right tools installed, just use install.packages("gstat")).  
Or you might like to wait a few days for the RAqua port of R (the Darwin
port with an Aqua GUI) to go into alpha test (it will be the MacOS X port
for R 1.8.0).

Alternatively, could you use R on an OS that does have easy support for 
gstat, e.g. Windows or Linux?


On Wed, 27 Aug 2003, Emilio A. Laca wrote:

> I need to install some geostatistics packages that are not in the binaries,
> for example gstat. I downloaded rm171 and installed, and I even added
> precomplied packages successfully. Unfortunately, I do not know how to add
> packages available as gstat. If you can spare a minute, could you give me a
> lead on this?
> Any other pointers on using R in mac os x will be greatly appreciated.
> 
> Thank you so much for your time on this question, and for everything you put
> into rmac!

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From lbaring at stochastik.uni-hannover.de  Thu Aug 28 08:15:14 2003
From: lbaring at stochastik.uni-hannover.de (Ludwig Baringhaus)
Date: Thu, 28 Aug 2003 08:15:14 +0200
Subject: [R] testing if two multivariate samples are from the same
	distribution
Message-ID: <200308280815.14594.lbaring@stochastik.uni-hannover.de>

Am Mittwoch, 27. August 2003 22:22 schrieb Jason Liao:
> Hello, everyone! I wonder if any R package can do the multivariate
> Smirnov test. Specifically, let x_1,..,x_n and y_1,...,y_m be
> multivariate vectors. I would like to test if the two samples are from
> the same underlying multivariate distribution. Thanks in advance.
>
> Jason

See cramer.test in package cramer.

L. Baringhaus



From Tom.Mulholland at health.wa.gov.au  Thu Aug 28 08:59:24 2003
From: Tom.Mulholland at health.wa.gov.au (Mulholland, Tom)
Date: Thu, 28 Aug 2003 14:59:24 +0800
Subject: [R] Re: diamond graphs, patents and rootograms
Message-ID: <74E242B6968AA0469B632C5A3EFC1EFD03D56FD4@nt207mesep.health.wa.gov.au>

Talking about Excel, you can produce excellent graphs in Excel. Yes you
have to work at it, but you can get there. The problem is that they are
not the default. My gut feeling is that R will make more of an impact in
the presentation of graphics than any implementation in Excel. 

So even if a patent were granted and it made itself into a mainstream
package, would it change the world or would the people who currently use
three dee (3D) graphs think that they looked a bit square. Or would the
implementation allow us to change the colour of each diamond or maybe we
could put a picture of our daughter as a background. Does change happen
from the tool or the user.

So I guess my reason for saying R will make more of an impact is because
the average R user cares about what they are doing when compared to the
average Excel user.

also

On Thu, 28 Aug 2003, David Scott wrote:

> What is a hanging rootogram? ;-D

Now I can't work out what the wink means, but they're implemented in the
VCD package 

Also http://www.math.yorku.ca/SCS/vcd/vcdstory.pdf for more info

_________________________________________________
 
Tom Mulholland
Senior Policy Officer
WA Country Health Service
Tel: (08) 9222 4062
 
The contents of this e-mail transmission are confidential an...{{dropped}}



From petr.pikal at precheza.cz  Thu Aug 28 09:12:03 2003
From: petr.pikal at precheza.cz (Petr Pikal)
Date: Thu, 28 Aug 2003 09:12:03 +0200
Subject: [R] trying to produce an array of ranks
In-Reply-To: <B86A3F74-D8E6-11D7-81B8-000A9566473A@or.psychology.dal.ca>
Message-ID: <3F4DC763.26218.3AA156@localhost>

Hi

On 27 Aug 2003 at 20:32, John Christie wrote:

> OK, I am try to produce an array of ranks.  I have a set of data (s)
> that looks like this
> 
> rt	subj
> 312	dave
> 467	dave
> 411	dave
> 383	kim
> 398	kim
> ...
> 
> Now I want to make a column that is an array of the ranks of rt by
> subject.  The closest of gotten is using the following.
> 
> r <- by (s, s$subj, function(d) rank(d[1]))
> 
> This gets the data out with ranks but I cannot figure out how to
> easily extract each of the lists and turn the whole thing into a nice
> straight array (it is an array of lists when unclassed).  I couldn't
> get anything working with any of the apply's or ave.  Any suggestions?
>  This seems like it should be a common task.

I am not sure you want to your output to look like.

r[n] will give you ranks for nth person

unlist(r) vill give you one vector of ranks

If you want a table of ranks let say with names in the first row and ranks for each 
name below it it will probably need adding some NA to shorter vectors of ranks 
and cbinding them together, probably using for loop. 

something like 
cbind(as.vector(unlist(r[1])),c(as.vector(unlist(r[2])),NA))

Not an easy task :(

Maybe sombody will come with better answer

> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help

Cheers
Petr Pikal
petr.pikal at precheza.cz



From B.Rowlingson at lancaster.ac.uk  Thu Aug 28 11:20:58 2003
From: B.Rowlingson at lancaster.ac.uk (Barry Rowlingson)
Date: Thu, 28 Aug 2003 10:20:58 +0100
Subject: [R] Re: diamond graphs and patents
In-Reply-To: <Pine.LNX.4.44.0308281107130.13175-100000@hydra.stat.auckland.ac.nz>
References: <Pine.LNX.4.44.0308281107130.13175-100000@hydra.stat.auckland.ac.nz>
Message-ID: <3F4DC97A.3080606@lancaster.ac.uk>

David Scott wrote:
>  but if any of us are to do 
> likewise I guess we will either pay up or secretly write code and play 
> around with diamond graphs while hidden in the basement.

Okay, lets stand up and be counted: who has been writing diamond graph 
code? Mine's 60 lines.

> Like others I welcome the contribution of those at Johns Hopkins to the 
> debate. And they perhaps shouldn't worry about us here at R-help, who are 
> maybe just a fringe element of open source loving anti-M$ weirdos. Then 
> again we may be the last bastion of open scientific enquiry.

  Is that an M$-Windows port of R I see? Or is it still anti-M$ because 
it isn't written in C# and .NET?

Baz



From thpe at hhbio.wasser.tu-dresden.de  Thu Aug 28 12:43:45 2003
From: thpe at hhbio.wasser.tu-dresden.de (Thomas Petzoldt)
Date: Thu, 28 Aug 2003 12:43:45 +0200
Subject: [R] read.spss (package foreign) and character columns
In-Reply-To: <C4D44AB4CB62D311BA6500041202E886031EE2A6@xms1.tirol.gv.at>
References: <C4D44AB4CB62D311BA6500041202E886031EE2A6@xms1.tirol.gv.at>
Message-ID: <3F4DDCE1.7070605@hhbio.wasser.tu-dresden.de>

RINNER Heinrich wrote:

> In R:
> 
>>library(foreign)
>>test <- read.spss("test.sav", to.data.frame=T)
>>test
> 
>   XCHAR
> 1 a    
> 2 ab   
> 3 abcde
> 
>>levels(test$XCHAR)
> 
> [1] "a    " "ab   " "abcde"
> 
> Shouldn't it rather be "a" "ab" "abcde" (no blanks)?

I think, that should be no problem since the blanks in XCHAR may be 
easily removed with gsub()

If you have
 > s<-c("a    ","ab   ","ab cd ")

then
 > gsub(" ","",s)
[1] "a"    "ab"   "abcd"

removes all spaces or

 > gsub(" *$","",s)
[1] "a"     "ab"    "ab cd"

removes all trailing spaces.

Hope this helps!

Thomas P.



From thpe at hhbio.wasser.tu-dresden.de  Thu Aug 28 12:51:01 2003
From: thpe at hhbio.wasser.tu-dresden.de (Thomas Petzoldt)
Date: Thu, 28 Aug 2003 12:51:01 +0200
Subject: [R] Newbie graphing questions
In-Reply-To: <1062038214.21093.53.camel@localhost>
References: <B9627E38-D8DD-11D7-9DAC-000393B90A0A@mac.com>
	<1062038214.21093.53.camel@localhost>
Message-ID: <3F4DDE95.2040408@hhbio.wasser.tu-dresden.de>

Marc Schwartz wrote:
> In general, if you want to leave the existing device open and have a new
> device open for a new plot, you simply call the device name that you
> want to open (ie. under Linux you would use X11() ) to open a new
> plotting device on the display.  See ?Devices for more details.

X11() works on Windows too.

Thomas



From franck151 at hotmail.com  Thu Aug 28 13:50:14 2003
From: franck151 at hotmail.com (franck allaire)
Date: Thu, 28 Aug 2003 13:50:14 +0200
Subject: [R] ks.test()
Message-ID: <Law12-F78iRSqQmHjUc0000a366@hotmail.com>

Dear All

I am trying to replicate a numerical application (not computed on R) from an 
article. Using, ks.test() I computed the exact D value shown in the article 
but the p-values I obtain are quite different from the one shown in the 
article.

The tests are performed on a sample of 37 values (please see "[0] DATA" 
below) for truncated Exponential, Pareto and truncated LogNormal (please see 
"[1] FUNCTIONS" below). You can find below the commands I use to compute the 
tests ("[2] COMMANDS") and the p-values presented in the article.

Can anyone explain the difference between these two set of p-values? Maybe 
the explanation is linked to my second question: Can anyone confirm me that 
p-values calculated in the Kolmogrov-Smirnov g-o-f are independent from the 
family of the distribution assumed in H0?

Anderson-Darling test is also performed in the article. would anyone have 
functions to calculate pvalues for exponential, lognormal, weibull, etc?

Many thanks for your help,

Franck Allaire
R 1.6.2

###############
# [0] DATA # truncation point is 30 i.e. all values are above 30
###############
39.7
582
439.9
47.1
83.9
41.2
893.1
192
106.2
216.7
1243.4
52.8
351.6
36.2
431.5
57.3
1602.1
822.2
260.1
58.7
6299.9
814.9
137.2
203.8
1263.5
53.7
1313
118.4
167.8
70.1
503.7
64.8
529.9
87.8
2465.4
317.9
2753.9

###############
# [1] FUNCTIONS
###############

#EXP
exptfit_function(x,b, plot.it = F, lty = 1){

          if (mode(x) != "numeric")
                stop("need numeric data")
          x <- x[!is.na(x)]
          x <- sort(x)
          lambda <- length(x)/sum(x-b)
          if(plot.it) { lines(x, dexpt(x, lambda = lambda, b = b), lty = 
lty,col="red")}
          result <- list(lambda = lambda, b = b)
          class(result) <- "expt"
          result}

dexpt<-function(x,lambda,b) dexp(x-b,rate=lambda)
pexpt<-function(x,lambda,b) pexp(x-b,rate=lambda)
qexpt<-function(p,lambda,b) qexp(p,rate=lambda)+b
rexpt<-function(n,lambda,b) rexp(n,rate=lambda)+b

#PARETO
paretofit<-function(x,b, plot.it = F, lty = 1){

          x <- sort(x)
          alpha <- length(x)/sum(log(x/b))
          if(plot.it) { lines(x, dpareto(x, alpha = alpha, b = b), lty = 
lty,col="red")}
          result <- list(alpha = alpha, b = b)
          class(result) <- "pareto"
          result}

dpareto<-function(x, alpha,b) ifelse(x<b,0,alpha*(b^alpha)/(x^(alpha+1)))
ppareto<-function(x, alpha,b) ifelse(x<b,0,1-(b/x)^alpha)
qpareto<-function(p, alpha,b) b*exp(-log(1-p)/alpha)
rpareto<-function(n, alpha,b) qpareto(runif(n),alpha=alpha,b=b)

#LN
lnormtfit_function(x,b, plot.it = F, lty = 1){

          if (mode(x) != "numeric")
                stop("need numeric data")
          x <- x[!is.na(x)]
          x <- sort(x)
          y <- log(x-b)
          n <- length(y)
          mu <- mean(y)
          sigma <- sd(y)
          if(plot.it) { lines(x, dlnormt(x, meanlog=mu,sdlog=sigma,b=b), lty 
= lty,col="red")}
          result <- list(mu=mu,sigma=sigma,b=b)
          class(result) <- "lnormt"
          result}

dlnormt<-function(x,mu,sigma,b) dlnorm(x-b,meanlog=mu,sdlog=sigma)
plnormt<-function(x,mu,sigma,b) plnorm(x-b,meanlog=mu,sdlog=sigma)
qlnormt<-function(p,mu,sigma,b) qlnorm(p,meanlog=mu,sdlog=sigma)+b
rlnormt<-function(n,mu,sigma,b) rlnorm(n,meanlog=mu,sdlog=sigma)+b

###############
# [2] COMMANDS
###############

# EXP
tmp <- exptfit(data,b=30)
ks.test(data,"pexpt",lambda=tmp$lambda,b=tmp$b)
# Article: D= 0.2599 pvalue<<0.005

# PARETO
tmp <- paretofit(data,b=30)
ks.test(data, "ppareto", alpha=tmp$alpha,b=tmp$b)
# Article: D=0.14586 pvalue=0.16

# LN
tmp <- lnormtfit(data,b=30)
ks.test(data,"plnormt",mu=tmp$mu,sigma=tmp$sigma,b=tmp$b)
# Article: D=0.07939 pvalue>>0.15



From p.dalgaard at biostat.ku.dk  Thu Aug 28 14:21:42 2003
From: p.dalgaard at biostat.ku.dk (Peter Dalgaard BSA)
Date: Thu, 28 Aug 2003 12:21:42 -0000
Subject: [R] Newbie graphing questions
In-Reply-To: <3F4DDE95.2040408@hhbio.wasser.tu-dresden.de>
References: <B9627E38-D8DD-11D7-9DAC-000393B90A0A@mac.com>
	<1062038214.21093.53.camel@localhost>
	<3F4DDE95.2040408@hhbio.wasser.tu-dresden.de>
Message-ID: <x21xv6gekk.fsf@biostat.ku.dk>

Thomas Petzoldt <thpe at hhbio.wasser.tu-dresden.de> writes:

> Marc Schwartz wrote:
> > In general, if you want to leave the existing device open and have a new
> > device open for a new plot, you simply call the device name that you
> > want to open (ie. under Linux you would use X11() ) to open a new
> > plotting device on the display.  See ?Devices for more details.
> 
> X11() works on Windows too.

..but the orig. poster was using a Mac.

-- 
   O__  ---- Peter Dalgaard             Blegdamsvej 3  
  c/ /'_ --- Dept. of Biostatistics     2200 Cph. N   
 (*) \(*) -- University of Copenhagen   Denmark      Ph: (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)             FAX: (+45) 35327907



From ripley at stats.ox.ac.uk  Thu Aug 28 14:30:02 2003
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Thu, 28 Aug 2003 13:30:02 +0100 (BST)
Subject: [R] ks.test()
In-Reply-To: <Law12-F78iRSqQmHjUc0000a366@hotmail.com>
Message-ID: <Pine.LNX.4.44.0308281319560.17945-100000@gannet.stats>

You appear to be applying the KS test after estimating parameters.  The 
distribution theory is for an iid sample from a known continuous 
distribution (and does not otherwise depend on the distribution).  Since 
your H_0 is not pre-specified, that distribution theory is not correct.
(Some corrections have been worked out for say ML fitting of exponential 
and normal distributions -- by Michael Stephens as I recall.)

Also, your `truncated LogNormal' does not appear to be truncated, rather
to be shifted.  That's the same thing for an exponential (for a positive
shift), but not for any other distribution.

On Thu, 28 Aug 2003, franck allaire wrote:

> Dear All
> 
> I am trying to replicate a numerical application (not computed on R) from an 
> article. Using, ks.test() I computed the exact D value shown in the article 
> but the p-values I obtain are quite different from the one shown in the 
> article.

And what is the H_0 and H_1 used in the article?

> The tests are performed on a sample of 37 values (please see "[0] DATA" 
> below) for truncated Exponential, Pareto and truncated LogNormal (please see 
> "[1] FUNCTIONS" below). You can find below the commands I use to compute the 
> tests ("[2] COMMANDS") and the p-values presented in the article.
> 
> Can anyone explain the difference between these two set of p-values? Maybe 
> the explanation is linked to my second question: Can anyone confirm me that 
> p-values calculated in the Kolmogrov-Smirnov g-o-f are independent from the 
> family of the distribution assumed in H0?
> 
> Anderson-Darling test is also performed in the article. would anyone have 
> functions to calculate pvalues for exponential, lognormal, weibull, etc?
> 
> Many thanks for your help,
> 
> Franck Allaire
> R 1.6.2
> 
> ###############
> # [0] DATA # truncation point is 30 i.e. all values are above 30
> ###############
> 39.7
> 582
> 439.9
> 47.1
> 83.9
> 41.2
> 893.1
> 192
> 106.2
> 216.7
> 1243.4
> 52.8
> 351.6
> 36.2
> 431.5
> 57.3
> 1602.1
> 822.2
> 260.1
> 58.7
> 6299.9
> 814.9
> 137.2
> 203.8
> 1263.5
> 53.7
> 1313
> 118.4
> 167.8
> 70.1
> 503.7
> 64.8
> 529.9
> 87.8
> 2465.4
> 317.9
> 2753.9
> 
> ###############
> # [1] FUNCTIONS
> ###############
> 
> #EXP
> exptfit_function(x,b, plot.it = F, lty = 1){
> 
>           if (mode(x) != "numeric")
>                 stop("need numeric data")
>           x <- x[!is.na(x)]
>           x <- sort(x)
>           lambda <- length(x)/sum(x-b)
>           if(plot.it) { lines(x, dexpt(x, lambda = lambda, b = b), lty = 
> lty,col="red")}
>           result <- list(lambda = lambda, b = b)
>           class(result) <- "expt"
>           result}
> 
> dexpt<-function(x,lambda,b) dexp(x-b,rate=lambda)
> pexpt<-function(x,lambda,b) pexp(x-b,rate=lambda)
> qexpt<-function(p,lambda,b) qexp(p,rate=lambda)+b
> rexpt<-function(n,lambda,b) rexp(n,rate=lambda)+b
> 
> #PARETO
> paretofit<-function(x,b, plot.it = F, lty = 1){
> 
>           x <- sort(x)
>           alpha <- length(x)/sum(log(x/b))
>           if(plot.it) { lines(x, dpareto(x, alpha = alpha, b = b), lty = 
> lty,col="red")}
>           result <- list(alpha = alpha, b = b)
>           class(result) <- "pareto"
>           result}
> 
> dpareto<-function(x, alpha,b) ifelse(x<b,0,alpha*(b^alpha)/(x^(alpha+1)))
> ppareto<-function(x, alpha,b) ifelse(x<b,0,1-(b/x)^alpha)
> qpareto<-function(p, alpha,b) b*exp(-log(1-p)/alpha)
> rpareto<-function(n, alpha,b) qpareto(runif(n),alpha=alpha,b=b)
> 
> #LN
> lnormtfit_function(x,b, plot.it = F, lty = 1){
> 
>           if (mode(x) != "numeric")
>                 stop("need numeric data")
>           x <- x[!is.na(x)]
>           x <- sort(x)
>           y <- log(x-b)
>           n <- length(y)
>           mu <- mean(y)
>           sigma <- sd(y)
>           if(plot.it) { lines(x, dlnormt(x, meanlog=mu,sdlog=sigma,b=b), lty 
> = lty,col="red")}
>           result <- list(mu=mu,sigma=sigma,b=b)
>           class(result) <- "lnormt"
>           result}
> 
> dlnormt<-function(x,mu,sigma,b) dlnorm(x-b,meanlog=mu,sdlog=sigma)
> plnormt<-function(x,mu,sigma,b) plnorm(x-b,meanlog=mu,sdlog=sigma)
> qlnormt<-function(p,mu,sigma,b) qlnorm(p,meanlog=mu,sdlog=sigma)+b
> rlnormt<-function(n,mu,sigma,b) rlnorm(n,meanlog=mu,sdlog=sigma)+b
> 
> ###############
> # [2] COMMANDS
> ###############
> 
> # EXP
> tmp <- exptfit(data,b=30)
> ks.test(data,"pexpt",lambda=tmp$lambda,b=tmp$b)
> # Article: D= 0.2599 pvalue<<0.005
> 
> # PARETO
> tmp <- paretofit(data,b=30)
> ks.test(data, "ppareto", alpha=tmp$alpha,b=tmp$b)
> # Article: D=0.14586 pvalue=0.16
> 
> # LN
> tmp <- lnormtfit(data,b=30)
> ks.test(data,"plnormt",mu=tmp$mu,sigma=tmp$sigma,b=tmp$b)
> # Article: D=0.07939 pvalue>>0.15
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> 
> 

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From H.RINNER at tirol.gv.at  Thu Aug 28 14:31:39 2003
From: H.RINNER at tirol.gv.at (RINNER Heinrich)
Date: Thu, 28 Aug 2003 14:31:39 +0200
Subject: [R] read.spss (package foreign) and character columns
Message-ID: <C4D44AB4CB62D311BA6500041202E886031EE2A8@xms1.tirol.gv.at>

Thanks to Brian Ripley, Douglas Bates and Thomas Petzoldt for their
comments.

I agree that it is not really a problem, as you can easily use sub() after
read.spss() to get rid of the blanks (I had already done that).

On the other hand, it might be important to _know_ about the fact that
characters are filled with blanks here.
[I noticed it because I used a character variable as the common column in a
merge(tab1,tab2,by=XCHAR), where tab1 came into R from an SPSS file using
read.spss(), and tab2 came into R from an Excel file via odbc using
odbcConnectExcel(). The merge failed on some cases, because some values of
XCHAR from tab1 had trailing blanks, the values of tab2 had none.]

I know now, so I know what to do in future cases.
But as not everybody else might be aware of this, my suggestion would be
that it could be worth adding a short comment about this in help(read.spss),
so noone will be "surprised".

Regards,
Heinrich.

> -----Urspr?ngliche Nachricht-----
> Von: RINNER Heinrich [mailto:H.RINNER at tirol.gv.at] 
> Gesendet: Mittwoch, 27. August 2003 18:06
> An: 'r-help at stat.math.ethz.ch'
> Betreff: [R] read.spss (package foreign) and character columns
> 
> 
> Dear R users!
> 
> I am using R Version 1.7.1, Windows XP, package "foreign" 
> (Version: 0.6-1),
> SPSS 11.5.1.
> 
> There is one thing I noticed with "read.spss", and I'd like 
> to ask if this
> is considered to be a feature, or possibly a bug:
> When reading character columns, character strings seem to get 
> filled with
> blanks at the end.
> 
> Simple example:
> In SPSS, create a file with one variable called "xchar" of type "A5"
> (character of length 5), and  3 values ("a", "ab", "abcde"), 
> save it as
> "test.sav".
> 
> In R:
> > library(foreign)
> > test <- read.spss("test.sav", to.data.frame=T)
> > test
>   XCHAR
> 1 a    
> 2 ab   
> 3 abcde
> > levels(test$XCHAR)
> [1] "a    " "ab   " "abcde"
> 
> Shouldn't it rather be "a" "ab" "abcde" (no blanks)?
> 
> -Heinrich.
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
>



From roger at ysidro.econ.uiuc.edu  Thu Aug 28 15:06:54 2003
From: roger at ysidro.econ.uiuc.edu (Roger Koenker)
Date: Thu, 28 Aug 2003 08:06:54 -0500 (CDT)
Subject: [R] ks.test()
In-Reply-To: <Pine.LNX.4.44.0308281319560.17945-100000@gannet.stats>
Message-ID: <Pine.SOL.4.30.0308280757080.14547-100000@ysidro.econ.uiuc.edu>


On Thu, 28 Aug 2003, Prof Brian Ripley wrote:

> You appear to be applying the KS test after estimating parameters.  The
> distribution theory is for an iid sample from a known continuous
> distribution (and does not otherwise depend on the distribution).  Since
> your H_0 is not pre-specified, that distribution theory is not correct.
> (Some corrections have been worked out for say ML fitting of exponential
> and normal distributions -- by Michael Stephens as I recall.)

Just to amplify this comment a bit, I'm a little worried that the
current documentation of of ks.test may make it appear that estimated
parameters are ok, or that somehow the p-values computed are
"corrected" in some way for their existence -- which I very much
doubt.  The standard reference on this sort of thing was Durbin's (1973)
SIAM monograph.  There is a very nice approach due to Khmaladze (1981)
based on the Doob-Meyer decomposition - this is the closest thing
that I'm aware of for handling KS type tests with estimated parameters
in a general context.

url:	www.econ.uiuc.edu/~roger/my.html	Roger Koenker
email	rkoenker at uiuc.edu			Department of Economics
vox: 	217-333-4558				University of Illinois
fax:   	217-244-6678				Champaign, IL 61820



From franck151 at hotmail.com  Thu Aug 28 15:06:05 2003
From: franck151 at hotmail.com (franck allaire)
Date: Thu, 28 Aug 2003 15:06:05 +0200
Subject: [R] ks.test()
Message-ID: <Law12-F1060ZiLLYXU60000a899@hotmail.com>


With the Shifted Exponential test, H_0 is "data is a sample coming from a 
Shifted Exponential distribution with shift=30 and lambda = 0.001566907"

>You appear to be applying the KS test after estimating parameters.

I do in order to define H_0 as explained above

>The distribution theory is for an iid sample from a known continuous
>distribution (and does not otherwise depend on the distribution).

That's what I thougth but this is not what I understand reading:
http://www.isi.edu/~kclan/paper/ramp/node11.html

>Since your H_0 is not pre-specified, that distribution theory is not 
>correct.

please see above

>(Some corrections have been worked out for say ML fitting of exponential
>and normal distributions -- by Michael Stephens as I recall.)
>Also, your `truncated LogNormal' does not appear to be truncated, rather
>to be shifted.  That's the same thing for an exponential (for a positive
>shift), but not for any other distribution.

Thanks for this clarification

>And what is the H_0 and H_1 used in the article?

H_1 is that the sample is not coming from the specified distribution in H_0



From ripley at stats.ox.ac.uk  Thu Aug 28 15:34:37 2003
From: ripley at stats.ox.ac.uk (Prof Brian D Ripley)
Date: Thu, 28 Aug 2003 14:34:37 +0100 (GMT Daylight Time)
Subject: [R] ks.test()
In-Reply-To: <Law12-F1060ZiLLYXU60000a899@hotmail.com>
Message-ID: <Pine.WNT.4.44.0308281426150.1340-100000@petrel>

On Thu, 28 Aug 2003, franck allaire wrote:

> With the Shifted Exponential test, H_0 is "data is a sample coming from a
> Shifted Exponential distribution with shift=30 and lambda = 0.001566907"

You got that after looking at the data.  H_0 has to be specified before
looking at the data.

> >You appear to be applying the KS test after estimating parameters.
>
> I do in order to define H_0 as explained above

That's invalid.

> >The distribution theory is for an iid sample from a known continuous
> >distribution (and does not otherwise depend on the distribution).
>
> That's what I thougth but this is not what I understand reading:

Learn not to believe everything you read on the Web.

> >Since your H_0 is not pre-specified, that distribution theory is not
> >correct.
>
> please see above

Rather, please take careful note yourself.

> >(Some corrections have been worked out for say ML fitting of exponential
> >and normal distributions -- by Michael Stephens as I recall.)
> >Also, your `truncated LogNormal' does not appear to be truncated, rather
> >to be shifted.  That's the same thing for an exponential (for a positive
> >shift), but not for any other distribution.
>
> Thanks for this clarification
>
> >And what is the H_0 and H_1 used in the article?
>
> H_1 is that the sample is not coming from the specified distribution in H_0

Not so: perhaps an iid sample from some other continuous distribution?

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From monica.palaseanu-lovejoy at stud.man.ac.uk  Thu Aug 28 15:35:29 2003
From: monica.palaseanu-lovejoy at stud.man.ac.uk (Monica Palaseanu-Lovejoy)
Date: Thu, 28 Aug 2003 14:35:29 +0100
Subject: [R] R-help: beginner question
Message-ID: <E19sMw1-000JQa-DG@probity.mcc.ac.uk>

Hi,

I am a beginner user of R. I have a trivial question ? I am almost 
ashamed I cannot figure it out does not matter how many times I 
am reading the help.

I have a table in .txt format, tab delimited. I can read it with 
?read.delim()? with no problems.

Afterwards I would like to use boxplot function to see if there 
are any outliers in the column 5 of my data called TPAH16.ppm

In the boxplot help I saw that I have to declare my data with 
?data()?. I am getting errors does not matter how I am calling 
?data()?, only with the name of the table, with 
data(read.delim()), or in any other way. So in the end I was not 
able to use boxplot at all.

I will appreciate any help ?for dummies? you can give me. Also, 
if you know any other way to identify outliers, or to use Cook 
dimension to identify them, I will really appreciate.

Thanks,

Monica


Monica Palaseanu-Lovejoy
University of Manchester
School of Geography
Mansfield Cooper Building 
Oxford Road, Manchester
M13 9PL, UK. 
email: monica.palaseanu-lovejoy at stud.man.ac.uk



From martin at lirmm.fr  Thu Aug 28 17:41:37 2003
From: martin at lirmm.fr (Martin Olivier)
Date: Thu, 28 Aug 2003 15:41:37 +0000
Subject: [R] how to call a C program from R function
Message-ID: <3F4E22B1.8020500@lirmm.fr>

Hi all,

I would like to call a C program from R function. I tried to use the 
.C() function
without success. I need a very simple example (such as the hello 
program) to understand
how it works. Could you give a  such example?
(I use the  1.7.1 Version of R with linux)

Does it work in the case of a C++ program instead of a C programm?

Best regards,
Olivier

-- 

-------------------------------------------------------------
Martin Olivier
INRA - Unit? prot?omique           LIRMM - IFA/MAB
2, Place Viala                     161, rue Ada
F-34060 Montpellier C?dex 1        34392 Montpellier C?dex 5	

Tel : 04 99 61 26 02               Tel : O4 67 41 86 71
martinol at ensam.inra.fr



From bates at stat.wisc.edu  Thu Aug 28 16:21:35 2003
From: bates at stat.wisc.edu (Douglas Bates)
Date: Thu, 28 Aug 2003 14:21:35 -0000
Subject: [R] R-help: beginner question
In-Reply-To: <E19sMw1-000JQa-DG@probity.mcc.ac.uk>
References: <E19sMw1-000JQa-DG@probity.mcc.ac.uk>
Message-ID: <6r1xv5c16f.fsf@bates4.stat.wisc.edu>

"Monica Palaseanu-Lovejoy" <monica.palaseanu-lovejoy at stud.man.ac.uk> writes:

> I am a beginner user of R. I have a trivial question  I am almost 
> ashamed I cannot figure it out does not matter how many times I 
> am reading the help.

> I have a table in .txt format, tab delimited. I can read it with 
> read.delim() with no problems.

> Afterwards I would like to use boxplot function to see if there 
> are any outliers in the column 5 of my data called TPAH16.ppm

> In the boxplot help I saw that I have to declare my data with 
> data(). I am getting errors does not matter how I am calling 
> data(), only with the name of the table, with 
> data(read.delim()), or in any other way. So in the end I was not 
> able to use boxplot at all.

Yours is a common problem for those starting with R.  The problem is
that the object that you have read, let's call it `df', is in a
tabular form (called a "data frame" in R) and you need to specify a
single column of that table as an argument to the boxplot function.

There are several ways to do this.

- Use the `with' function to indicate the data frame to use

with(df, boxplot(TPAH16.ppm))

- Use both the name of the data frame and the name of the column,
separated by $

boxplot(df$TPAH16.ppm)

- Attach the data frame before creating the boxplot

attach(df)
boxplot(TPAH16.ppm)
detach()


-- 
Douglas Bates                            bates at stat.wisc.edu
Statistics Department                    608/262-2598
University of Wisconsin - Madison        http://www.stat.wisc.edu/~bates/



From thpe at hhbio.wasser.tu-dresden.de  Thu Aug 28 16:23:49 2003
From: thpe at hhbio.wasser.tu-dresden.de (Thomas Petzoldt)
Date: Thu, 28 Aug 2003 16:23:49 +0200
Subject: [R] how to call a C program from R function
In-Reply-To: <3F4E22B1.8020500@lirmm.fr>
References: <3F4E22B1.8020500@lirmm.fr>
Message-ID: <3F4E1075.5010408@hhbio.wasser.tu-dresden.de>

Martin Olivier wrote:

> Hi all,
> 
> I would like to call a C program from R function. I tried to use the 
> .C() function
> without success. I need a very simple example (such as the hello 
> program) to understand

Do you want to call a C program (executable) or a shared library? If you 
want to run a compiled program simply use system() or shell()

> how it works. Could you give a  such example?
> (I use the  1.7.1 Version of R with linux)
> 
> Does it work in the case of a C++ program instead of a C programm?

If you want to call a shared library (or DLL), please look into the 
Manual "writing R extensions" and look for the "convolve example".

For a very simple "hello-function" see the "C++" example below. Please 
check if you have installed the correct compiler on Linux and look into 
the manual how to compile a shared library. On Linux it is:

R CMD SHLIB test.cpp


Thomas P.

-----------------------------------------------------------
A simple example, but there are better in the docs ;-) e.g. building a 
package.



===========================================================
// File test.cpp:

#include <R.h>
#include <Rinternals.h>

extern "C" {

void testarr(int* n, double* x) {
     for (int i = 0; i < *n; i++) {
        x[i] = 2 * i;
     }
   }
}
===========================================================
## file test.R

dyn.load("test.so")

testarr <- function(x) {
   .C("testarr", as.integer(length(x)), x=as.double(x))$x
}

print(testarr(1:10))

dyn.unload("test.so")



From monica.palaseanu-lovejoy at stud.man.ac.uk  Thu Aug 28 16:31:14 2003
From: monica.palaseanu-lovejoy at stud.man.ac.uk (Monica Palaseanu-Lovejoy)
Date: Thu, 28 Aug 2003 15:31:14 +0100
Subject: [R] R-help: beginner question - Thank you! OUTLIERS
Message-ID: <E19sNny-000OeN-RL@probity.mcc.ac.uk>

Hi,

Thank you so much for all your rapid answers. I am impressed.

What i didn't know was that i have to assign my data to an object 
to work further on. It was not clear from the help (at least for me) 
that 'data()' itself is calling data already in R packages. All of you 
make that clear.

Now, if you can suggest any good package to use for identifying 
outliers it will be great ;-)) 

Hopefully from now one, since i understood how i am using the 
examples, what 'data()' means and how i am using my own data, i 
will put less trivial questions.

thank you so much indeed,

Monica


Monica Palaseanu-Lovejoy
University of Manchester
School of Geography
Mansfield Cooper Building 
Oxford Road, Manchester
M13 9PL, UK. 
email: monica.palaseanu-lovejoy at stud.man.ac.uk



From p.dalgaard at biostat.ku.dk  Thu Aug 28 16:40:28 2003
From: p.dalgaard at biostat.ku.dk (Peter Dalgaard BSA)
Date: Thu, 28 Aug 2003 14:40:28 -0000
Subject: [R] R-help: beginner question
In-Reply-To: <E19sMw1-000JQa-DG@probity.mcc.ac.uk>
References: <E19sMw1-000JQa-DG@probity.mcc.ac.uk>
Message-ID: <x2r835g7zv.fsf@biostat.ku.dk>

"Monica Palaseanu-Lovejoy" <monica.palaseanu-lovejoy at stud.man.ac.uk> writes:

> Hi,
> 
> I am a beginner user of R. I have a trivial question ? I am almost 
> ashamed I cannot figure it out does not matter how many times I 
> am reading the help.
> 
> I have a table in .txt format, tab delimited. I can read it with 
> ?read.delim()? with no problems.
> 
> Afterwards I would like to use boxplot function to see if there 
> are any outliers in the column 5 of my data called TPAH16.ppm
> 
> In the boxplot help I saw that I have to declare my data with 
> ?data()?. I am getting errors does not matter how I am calling 
> ?data()?, only with the name of the table, with 
> data(read.delim()), or in any other way. So in the end I was not 
> able to use boxplot at all.
> 
> I will appreciate any help ?for dummies? you can give me. Also, 
> if you know any other way to identify outliers, or to use Cook 
> dimension to identify them, I will really appreciate.

You don't need data() that's just to get some standard data for the
boxplot() example. What you need is

mydata <- read.delim(...whatever...)

attach(mydata)
boxplot(V5) # or whatever col.5 is called

or 

with(mydata, boxplot(V5))

or

boxplot(mydata$V5)


(Almost all the boxplot examples appear to be using the formula interface,
which doesn't work without a grouping variable, and only the formula
interface allows you to give a data= argument to boxplot(). At least
one of those should probably be fixed.)

-- 
   O__  ---- Peter Dalgaard             Blegdamsvej 3  
  c/ /'_ --- Dept. of Biostatistics     2200 Cph. N   
 (*) \(*) -- University of Copenhagen   Denmark      Ph: (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)             FAX: (+45) 35327907



From jmacdon at med.umich.edu  Thu Aug 28 16:50:18 2003
From: jmacdon at med.umich.edu (James MacDonald)
Date: Thu, 28 Aug 2003 10:50:18 -0400
Subject: [R] R-help: beginner question
Message-ID: <sf4dde73.049@mail-02.med.umich.edu>

If you read the data into a data frame, you should be able to simply pass the name of the data frame in a call to boxplot.

my.data <- read.delim("mytext.txt")
boxplot(my.data)

If you only want a boxplot of column 5

boxplot(my.data[,5])

See ?boxplot for other options to make the boxplot look the way you like.

HTH,

Jim



James W. MacDonald
Affymetrix and cDNA Microarray Core
University of Michigan Cancer Center
1500 E. Medical Center Drive
7410 CCGC
Ann Arbor MI 48109
734-647-5623

>>> "Monica Palaseanu-Lovejoy" <monica.palaseanu-lovejoy at stud.man.ac.uk> 08/28/03 09:35AM >>>
Hi,

I am a beginner user of R. I have a trivial question * I am almost 
ashamed I cannot figure it out does not matter how many times I 
am reading the help.

I have a table in .txt format, tab delimited. I can read it with 
'read.delim()' with no problems.

Afterwards I would like to use boxplot function to see if there 
are any outliers in the column 5 of my data called TPAH16.ppm

In the boxplot help I saw that I have to declare my data with 
'data()'. I am getting errors does not matter how I am calling 
'data()', only with the name of the table, with 
data(read.delim()), or in any other way. So in the end I was not 
able to use boxplot at all.

I will appreciate any help "for dummies" you can give me. Also, 
if you know any other way to identify outliers, or to use Cook 
dimension to identify them, I will really appreciate.

Thanks,

Monica


Monica Palaseanu-Lovejoy
University of Manchester
School of Geography
Mansfield Cooper Building 
Oxford Road, Manchester
M13 9PL, UK. 
email: monica.palaseanu-lovejoy at stud.man.ac.uk 

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://www.stat.math.ethz.ch/mailman/listinfo/r-help



From ripley at stats.ox.ac.uk  Thu Aug 28 17:14:19 2003
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Thu, 28 Aug 2003 16:14:19 +0100 (BST)
Subject: [R] how to call a C program from R function
In-Reply-To: <3F4E22B1.8020500@lirmm.fr>
Message-ID: <Pine.LNX.4.44.0308281608030.18122-100000@gannet.stats>

On Thu, 28 Aug 2003, Martin Olivier wrote:

> Hi all,
> 
> I would like to call a C program from R function. I tried to use the 
> .C() function
> without success. I need a very simple example (such as the hello 
> program) to understand
> how it works. Could you give a  such example?

There is one in Writing R Extensions.  `Hello world' be too simple: you do 
need to know how to return results.

> (I use the  1.7.1 Version of R with linux)
> 
> Does it work in the case of a C++ program instead of a C programm?

See Writing R extensions for details and examples.

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From gcaggiano at yahoo.com  Thu Aug 28 17:53:27 2003
From: gcaggiano at yahoo.com (giovanni caggiano)
Date: Thu, 28 Aug 2003 08:53:27 -0700 (PDT)
Subject: [R] (no subject)
Message-ID: <20030828155327.13372.qmail@web12104.mail.yahoo.com>

Dear All,
A couple of questions about the nls package.

1. I'm trying to run a nonlinear least squares
regression but the routine gives me the following
error message:

 step factor 0.000488281 reduced below `minFactor' of
0.000976563

even though I previously wrote the following command: 
nls.control(minFactor = 1/4096), which should set the
minFactor to a lower level than the default one,
1/1024=0.000976563. 
Is there any way of setting the new minfactor to a
lower level?

2. Is it possible to set some constraints upon the
parameters to be estimated in a nls regression?

Thanks,
Giovanni



From spencer.graves at pdf.com  Thu Aug 28 18:03:01 2003
From: spencer.graves at pdf.com (Spencer Graves)
Date: Thu, 28 Aug 2003 09:03:01 -0700
Subject: [R] R-help: beginner question - Thank you! OUTLIERS
References: <E19sNny-000OeN-RL@probity.mcc.ac.uk>
Message-ID: <3F4E27B5.4010709@pdf.com>

Have you considered normal probability plots (qqnorm) to identify 
outliers?  These will identify much more, of course, including the need 
for transformations, mixtures of distributions, etc.

hope this helps.  spencer graves

Monica Palaseanu-Lovejoy wrote:
> Hi,
> 
> Thank you so much for all your rapid answers. I am impressed.
> 
> What i didn't know was that i have to assign my data to an object 
> to work further on. It was not clear from the help (at least for me) 
> that 'data()' itself is calling data already in R packages. All of you 
> make that clear.
> 
> Now, if you can suggest any good package to use for identifying 
> outliers it will be great ;-)) 
> 
> Hopefully from now one, since i understood how i am using the 
> examples, what 'data()' means and how i am using my own data, i 
> will put less trivial questions.
> 
> thank you so much indeed,
> 
> Monica
> 
> 
> Monica Palaseanu-Lovejoy
> University of Manchester
> School of Geography
> Mansfield Cooper Building 
> Oxford Road, Manchester
> M13 9PL, UK. 
> email: monica.palaseanu-lovejoy at stud.man.ac.uk
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help



From p.dalgaard at biostat.ku.dk  Thu Aug 28 18:03:30 2003
From: p.dalgaard at biostat.ku.dk (Peter Dalgaard BSA)
Date: Thu, 28 Aug 2003 16:03:30 -0000
Subject: [R] (no subject)
In-Reply-To: <20030828155327.13372.qmail@web12104.mail.yahoo.com>
References: <20030828155327.13372.qmail@web12104.mail.yahoo.com>
Message-ID: <x2ekz5g45q.fsf@biostat.ku.dk>

giovanni caggiano <gcaggiano at yahoo.com> writes:

> Dear All,
> A couple of questions about the nls package.
> 
> 1. I'm trying to run a nonlinear least squares
> regression but the routine gives me the following
> error message:
> 
>  step factor 0.000488281 reduced below `minFactor' of
> 0.000976563
> 
> even though I previously wrote the following command: 
> nls.control(minFactor = 1/4096), which should set the
> minFactor to a lower level than the default one,
> 1/1024=0.000976563. 
> Is there any way of setting the new minfactor to a
> lower level?

I suspect you didn't quite understand how to do it:

ctl.obj <- nls.control(minFactor = 1/4096)
nls(....., control=ctl.obj)


-- 
   O__  ---- Peter Dalgaard             Blegdamsvej 3  
  c/ /'_ --- Dept. of Biostatistics     2200 Cph. N   
 (*) \(*) -- University of Copenhagen   Denmark      Ph: (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)             FAX: (+45) 35327907



From spencer.graves at pdf.com  Thu Aug 28 18:34:57 2003
From: spencer.graves at pdf.com (Spencer Graves)
Date: Thu, 28 Aug 2003 09:34:57 -0700
Subject: [R] nls problems (formerly no subject)
References: <20030828155327.13372.qmail@web12104.mail.yahoo.com>
Message-ID: <3F4E2F31.7010404@pdf.com>

	  Please use an informative subject line.  The r-help archives at 
www.r-project.org -> search -> R site search indexes that, and I find 
answers to today's problems in the r-help discussions of yesterday's 
questions.

	  Only yesterday, I was got essentially that error message.  I solved 
it by setting "trace=TRUE" and studying which paramters were moving. 
With that information, I looked at the function and thought of ways to 
reparameterize it to make things more stable.  That helped but did not 
solve the problem.  Ultimately, I fixed two of the most volatile 
parameters to sensible values.  Then nls gave me answers very quickly.

	  An excellent reference on nonlinear regression is Bates and Watts 
(1988) Nonlinear regression and its applications (Wiley).  They talk 
some about reparameterizations.  A major contribution of this book is 
distinguishing between the intrinsic nonlinearity of the problem, which 
cannot be fixed without changing the model, and "parameter effect 
curvature", which can be fixed by just writing the model in a different 
but equivalent way.  Pages 256-259 summarize some 67 different examples 
from published literature.  In each case, parameter effects curvature 
was more than the intrinsic curvature, with the median being a factor of 
16 larger.

	  For my problem yesterday, I did not just restrict myself to 
reparameterizations:  I also considered alternative models with similar 
but formally different behavior.

	  Also, have you considered using "optim" first, then feeding the 
answers to "nls"?  McCullough found a few years ago that it was easier 
for him to get answers if he did it that way, because the S-Plus 
versions of "nls" seems to get lost and quit prematurely, while "optim" 
will at least produce an answer.  If I'm not mistaken, this issue is 
discussed in either McCullough, B. D. (1999) Assessing the reliability 
of statistical software: Part II The American Statistician, 53, 149-159 
or McCullough, B. D. (1998) Assessing the reliability of statistical 
software: Part I The American Statistician, 52, 358-366.  I don't 
remember now which paper had this, but I believe one of them did;  I 
think I'd look at the second first.  (McCullough discussed "nlminb" 
instead of "optim".  The former has been replaced by the latter in R.)

hope this helps.  spencer graves

giovanni caggiano wrote:
> Dear All,
> A couple of questions about the nls package.
> 
> 1. I'm trying to run a nonlinear least squares
> regression but the routine gives me the following
> error message:
> 
>  step factor 0.000488281 reduced below `minFactor' of
> 0.000976563
> 
> even though I previously wrote the following command: 
> nls.control(minFactor = 1/4096), which should set the
> minFactor to a lower level than the default one,
> 1/1024=0.000976563. 
> Is there any way of setting the new minfactor to a
> lower level?
> 
> 2. Is it possible to set some constraints upon the
> parameters to be estimated in a nls regression?
> 
> Thanks,
> Giovanni
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help



From fgibbons at hms.harvard.edu  Thu Aug 28 19:03:32 2003
From: fgibbons at hms.harvard.edu (Frank Gibbons)
Date: Thu, 28 Aug 2003 13:03:32 -0400
Subject: [R] Cook-distance-type plot (vertical bars)
Message-ID: <5.2.1.1.2.20030828125858.0231ac18@email.med.harvard.edu>

Hi,

Figure 13 of Emmanuel Paradis's "R for Beginners" was produced by termplot 
working on an aov object. The lower right-hand plot is labelled "Cook's 
distance plot", and I'd really like to produce a similar type of figure, 
but in a totally different context. (I'm not even sure what this kind of 
figure is called, perhaps an "impulse plot", where instead of a point at 
(x,y), there's a vertical bar running from the x-axis up to where the point 
would be).

Can anyone give me pointers on where to look for more info. I've had a look 
in the places I could think of (plot.lm.R, termplot.R, plot.R, aov.R), and 
couldn't find anything. Maybe I overlooked it?

Thanks

-Frank Gibbons

PhD, Computational Biologist,
Harvard Medical School BCMP/SGM-322, 250 Longwood Ave, Boston MA 02115, USA.
Tel: 617-432-3555       Fax: 
617-432-3557       http://llama.med.harvard.edu/~fgibbons



From bates at stat.wisc.edu  Thu Aug 28 19:04:45 2003
From: bates at stat.wisc.edu (Douglas Bates)
Date: Thu, 28 Aug 2003 17:04:45 -0000
Subject: [R] nls minFactor and constraints [was (no subject)]
In-Reply-To: <20030828155327.13372.qmail@web12104.mail.yahoo.com>
References: <20030828155327.13372.qmail@web12104.mail.yahoo.com>
Message-ID: <6risohaf2v.fsf@bates4.stat.wisc.edu>

giovanni caggiano <gcaggiano at yahoo.com> writes:

> A couple of questions about the nls package.

> 1. I'm trying to run a nonlinear least squares
> regression but the routine gives me the following
> error message:
> 
>  step factor 0.000488281 reduced below `minFactor' of
> 0.000976563
> 
> even though I previously wrote the following command: 
> nls.control(minFactor = 1/4096), which should set the
> minFactor to a lower level than the default one,
> 1/1024=0.000976563. 
> Is there any way of setting the new minfactor to a
> lower level?

You need to set control=nls.control(minFactor=1/4096) in the call to
nls.

> 2. Is it possible to set some constraints upon the
> parameters to be estimated in a nls regression?

Other than by parameter transformation, no.  See section 3.4.1 of
Bates and Watts (1988), "Nonlinear Regression Analysis and Its
Applications", Wiley to see how to use parameter transformations for
this.



From ripley at stats.ox.ac.uk  Thu Aug 28 19:13:26 2003
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Thu, 28 Aug 2003 18:13:26 +0100 (BST)
Subject: [R] Cook-distance-type plot (vertical bars)
In-Reply-To: <5.2.1.1.2.20030828125858.0231ac18@email.med.harvard.edu>
Message-ID: <Pine.LNX.4.44.0308281807090.18664-100000@gannet.stats>

On Thu, 28 Aug 2003, Frank Gibbons wrote:

> Hi,
> 
> Figure 13 of Emmanuel Paradis's "R for Beginners" was produced by termplot 
> working on an aov object. The lower right-hand plot is labelled "Cook's 
> distance plot", and I'd really like to produce a similar type of figure, 
> but in a totally different context. (I'm not even sure what this kind of 
> figure is called, perhaps an "impulse plot", where instead of a point at 
> (x,y), there's a vertical bar running from the x-axis up to where the point 
> would be).

Often called an `index plot'.  It's done by plot(type="h").  R seems to
think that is `histogram-like', but the notation comes from S which calls
it `high-density lines'.  I've never found either intuitive notation.

> Can anyone give me pointers on where to look for more info. I've had a look 
> in the places I could think of (plot.lm.R, termplot.R, plot.R, aov.R), and 
> couldn't find anything. Maybe I overlooked it?

I suspect it was done by plot.lm(which=4), which does an index plot of 
Cook's distance.

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From tlumley at u.washington.edu  Thu Aug 28 19:16:18 2003
From: tlumley at u.washington.edu (Thomas Lumley)
Date: Thu, 28 Aug 2003 10:16:18 -0700 (PDT)
Subject: [R] Cook-distance-type plot (vertical bars)
In-Reply-To: <5.2.1.1.2.20030828125858.0231ac18@email.med.harvard.edu>
Message-ID: <Pine.A41.4.44.0308281010340.52396-100000@homer24.u.washington.edu>

On Thu, 28 Aug 2003, Frank Gibbons wrote:

> Hi,
>
> Figure 13 of Emmanuel Paradis's "R for Beginners" was produced by termplot
> working on an aov object.

No, it was produced by plot() working on a aov object, as its caption
indicates.  The termplot() is Figure 14.

>		 The lower right-hand plot is labelled "Cook's
> distance plot", and I'd really like to produce a similar type of figure,
> but in a totally different context. (I'm not even sure what this kind of
> figure is called, perhaps an "impulse plot", where instead of a point at
> (x,y), there's a vertical bar running from the x-axis up to where the point
> would be).

It's called a high-density plot, and you get it by specifying type="h" in
a plot() command.  It's mentioned on page 31 of `R for beginners'

It is in the code for plot.lm, where you say you also looked: the cook's
distance plot is created with
    plot(cook, type = "h", ylim = c(0, ymx), main = main,
         xlab = "Obs. number", ylab = "Cook's distance", ...)


	-thomas



From spencer.graves at pdf.com  Thu Aug 28 19:27:27 2003
From: spencer.graves at pdf.com (Spencer Graves)
Date: Thu, 28 Aug 2003 10:27:27 -0700
Subject: [R] nls minFactor and constraints [was (no subject)]
References: <20030828155327.13372.qmail@web12104.mail.yahoo.com>
	<6risohaf2v.fsf@bates4.stat.wisc.edu>
Message-ID: <3F4E3B7F.8070102@pdf.com>

In my experience, transformations of the type Doug just described has 
often made sums of squares (or log(likelihood)) contours more parabolic, 
thereby increasing the accuracy of the simple normal approximations to 
the distributions of parameter estimates.  It is wise to check these 
things, as it is not always true.

hope this helps.  spencer graves

Douglas Bates wrote:
> giovanni caggiano <gcaggiano at yahoo.com> writes:
> 
> 
>>A couple of questions about the nls package.
> 
> 
>>1. I'm trying to run a nonlinear least squares
>>regression but the routine gives me the following
>>error message:
>>
>> step factor 0.000488281 reduced below `minFactor' of
>>0.000976563
>>
>>even though I previously wrote the following command: 
>>nls.control(minFactor = 1/4096), which should set the
>>minFactor to a lower level than the default one,
>>1/1024=0.000976563. 
>>Is there any way of setting the new minfactor to a
>>lower level?
> 
> 
> You need to set control=nls.control(minFactor=1/4096) in the call to
> nls.
> 
> 
>>2. Is it possible to set some constraints upon the
>>parameters to be estimated in a nls regression?
> 
> 
> Other than by parameter transformation, no.  See section 3.4.1 of
> Bates and Watts (1988), "Nonlinear Regression Analysis and Its
> Applications", Wiley to see how to use parameter transformations for
> this.
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help



From bates at stat.wisc.edu  Thu Aug 28 19:36:50 2003
From: bates at stat.wisc.edu (Douglas Bates)
Date: Thu, 28 Aug 2003 17:36:50 -0000
Subject: [R] nls problems (formerly no subject)
In-Reply-To: <3F4E2F31.7010404@pdf.com>
References: <20030828155327.13372.qmail@web12104.mail.yahoo.com>
	<3F4E2F31.7010404@pdf.com>
Message-ID: <6rbru9adkk.fsf@bates4.stat.wisc.edu>

I agree with what you said about using trace = TRUE when you are
having trouble getting nls to converge.  This allows you to see what
is happening to the parameters during the iterations and that it is
often quite instructive; as is plotting your data and thinking about
whether you should expect to be able to estimate all the parameters in
your model from the data that you have.  The nls function is not
magic.  It can only use that information that is available in the
data.

Spencer Graves <spencer.graves at pdf.com> writes:

> Also, have you considered using "optim" first, then feeding
> the answers to "nls"?  McCullough found a few years ago that it was
> easier for him to get answers if he did it that way, because the
> S-Plus versions of "nls" seems to get lost and quit prematurely, while
> "optim" will at least produce an answer.  If I'm not mistaken, this
> issue is discussed in either McCullough, B. D. (1999) Assessing the
> reliability of statistical software: Part II The American
> Statistician, 53, 149-159 or McCullough, B. D. (1998) Assessing the
> reliability of statistical software: Part I The American Statistician,
> 52, 358-366.  I don't remember now which paper had this, but I believe
> one of them did;  I think I'd look at the second first.  (McCullough
> discussed "nlminb" instead of "optim".  The former has been replaced
> by the latter in R.)

I would be hesitant to draw too many conclusions from McCullough's
experiences.  He used the data sets, models, and starting estimates
from the NIST nonlinear least squares test sets.  These are available
in the NISTnls package for R.  If you run the examples in that package
you will see that all of the examples can be fit reasonably easily in
R.  However, the people at NIST decided that they wanted to create
"easy" and "difficult" versions of each example and they did this by
chosing one reasonable set of starting estimates nd one ridiculous set
of starting estimates for each problem.

Check, for example,

library(NISTnls)
example(Eckerle4)

The model that is being fit is a scaled version of a Gaussian
density.  A mere glance at the data shows that the location parameter
will be around 450 and the scale parameter will be around 5.  There is
no problem at all converging from those starting values - that's the
"easy" version of the problem.  The "difficult" version of the problem
is to try to converge from starting estimates of 500 for the location
and 10 for the scale.  

Look at a plot of the data.  Would anyone who knew what those
parameters represent consider 500 as a reasonable guess of the
location of the peak?  I don't feel that it is a terrible deficiency
in the nls implementation in R that it does not converge on this
model/data combination from those starting estimates.



From fgibbons at hms.harvard.edu  Thu Aug 28 19:52:48 2003
From: fgibbons at hms.harvard.edu (Frank Gibbons)
Date: Thu, 28 Aug 2003 13:52:48 -0400
Subject: [R] Cook-distance-type plot (vertical bars)
In-Reply-To: <Pine.A41.4.44.0308281010340.52396-100000@homer24.u.washing
	ton.edu>
References: <5.2.1.1.2.20030828125858.0231ac18@email.med.harvard.edu>
Message-ID: <5.2.1.1.2.20030828134730.022d3c68@email.med.harvard.edu>

Thanks to all who responded, and so promptly too: it works exactly as you 
describe.

> >
> > Figure 13 of Emmanuel Paradis's "R for Beginners" was produced by termplot
> > working on an aov object.
>
>No, it was produced by plot() working on a aov object, as its caption
>indicates.  The termplot() is Figure 14.

Thomas Lumley is quite right, it's produced by plot() (not termplot()), and 
it is mentioned on p31 of "R for beginners". My mistake.

In the interest of self education, is there a more comprehensive source for 
plot-types that I should read? Ideally, this would be something with lots 
of figures, so that I could browse the figures to find what I want to do, 
and then look up how to do it. "R for Beginners" goes some way along this 
path, but perhaps there's something more comprehensive?

Thanks again,

-Frank

PhD, Computational Biologist,
Harvard Medical School BCMP/SGM-322, 250 Longwood Ave, Boston MA 02115, USA.
Tel: 617-432-3555       Fax: 
617-432-3557       http://llama.med.harvard.edu/~fgibbons



From pburns at pburns.seanet.com  Thu Aug 28 19:45:31 2003
From: pburns at pburns.seanet.com (Patrick Burns)
Date: Thu, 28 Aug 2003 18:45:31 +0100
Subject: [R] R-help: beginner question
References: <E19sMw1-000JQa-DG@probity.mcc.ac.uk>
Message-ID: <3F4E3FBB.8060402@pburns.seanet.com>

"A Guide for the Unwilling S User" would help orient you
to how R works. It is meant to do that as quickly and painlessly
as possible.

Patrick Burns

Burns Statistics
patrick at burns-stat.com
+44 (0)20 8525 0696
http://www.burns-stat.com
(home of S Poetry and "A Guide for the Unwilling S User")

Monica Palaseanu-Lovejoy wrote:

>Hi,
>
>I am a beginner user of R. I have a trivial question  I am almost 
>ashamed I cannot figure it out does not matter how many times I 
>am reading the help.
>
>I have a table in .txt format, tab delimited. I can read it with 
>read.delim() with no problems.
>
>Afterwards I would like to use boxplot function to see if there 
>are any outliers in the column 5 of my data called TPAH16.ppm
>
>In the boxplot help I saw that I have to declare my data with 
>data(). I am getting errors does not matter how I am calling 
>data(), only with the name of the table, with 
>data(read.delim()), or in any other way. So in the end I was not 
>able to use boxplot at all.
>
>I will appreciate any help 
for dummies
 you can give me. Also, 
>if you know any other way to identify outliers, or to use Cook 
>dimension to identify them, I will really appreciate.
>
>Thanks,
>
>Monica
>
>
>Monica Palaseanu-Lovejoy
>University of Manchester
>School of Geography
>Mansfield Cooper Building 
>Oxford Road, Manchester
>M13 9PL, UK. 
>email: monica.palaseanu-lovejoy at stud.man.ac.uk
>
>______________________________________________
>R-help at stat.math.ethz.ch mailing list
>https://www.stat.math.ethz.ch/mailman/listinfo/r-help
>
>
>  
>



From bido at mac.com  Thu Aug 28 19:57:59 2003
From: bido at mac.com (Francisco J. Bido)
Date: Thu, 28 Aug 2003 12:57:59 -0500
Subject: [R] Newbie graphing questions
In-Reply-To: <x21xv6gekk.fsf@biostat.ku.dk>
Message-ID: <28E445F6-D981-11D7-8C06-000393B90A0A@mac.com>

The MacOSX (carbon port)  command turned out to be, surprisingly enough,

macintosh()

Thanks!

On Thursday, August 28, 2003, at 07:19 AM, Peter Dalgaard BSA wrote:

> Thomas Petzoldt <thpe at hhbio.wasser.tu-dresden.de> writes:
>
>> Marc Schwartz wrote:
>>> In general, if you want to leave the existing device open and have a 
>>> new
>>> device open for a new plot, you simply call the device name that you
>>> want to open (ie. under Linux you would use X11() ) to open a new
>>> plotting device on the display.  See ?Devices for more details.
>>
>> X11() works on Windows too.
>
> ..but the orig. poster was using a Mac.
>
> -- 
>    O__  ---- Peter Dalgaard             Blegdamsvej 3
>   c/ /'_ --- Dept. of Biostatistics     2200 Cph. N
>  (*) \(*) -- University of Copenhagen   Denmark      Ph: (+45) 35327918
> ~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)             FAX: (+45) 35327907
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
>



From wdmccoy at geo.umass.edu  Thu Aug 28 19:54:40 2003
From: wdmccoy at geo.umass.edu (William D. McCoy)
Date: Thu, 28 Aug 2003 13:54:40 -0400 (EDT)
Subject: [R] make dvi fails
Message-ID: <200308281754.NAA13322@aeolus.geo.umass.edu>

I am working with a RedHat 9 system with gcc 3.2.2, makeinfo 4.3, and
perl 5.8.

I have just downloaded the latest r-patched with rsync and
successfully compiled the code and passed the tests conducted with
'make check'.

After that, 'make dvi' gives the following result:

make dvi
make[1]: Entering directory `/usr/src/R/doc'
make[2]: Entering directory `/usr/src/R/doc/manual'
creating doc/manual/version.tex
collecting LaTeX docs for package 'base' ...
make[3]: Entering directory `/usr/src/R/src/library'
 >>> Building/Updating help pages for package 'base'
     Formats: latex 
make[3]: Leaving directory `/usr/src/R/src/library'
collecting LaTeX docs for package 'methods' ...
make[3]: Entering directory `/usr/src/R/src/library'
 >>> Building/Updating help pages for package 'methods'
     Formats: latex 
make[3]: Leaving directory `/usr/src/R/src/library'
collecting LaTeX docs for package 'tools' ...
make[3]: Entering directory `/usr/src/R/src/library'
 >>> Building/Updating help pages for package 'tools'
     Formats: latex 
make[3]: Leaving directory `/usr/src/R/src/library'
make[2]: *** No rule to make target `-pkg.tex', needed by `stamp-refman-dvi'.  Stop.
make[2]: Leaving directory `/usr/src/R/doc/manual'
make[1]: *** [dvi] Error 2
make[1]: Leaving directory `/usr/src/R/doc'
make: [dvi] Error 2 (ignored)


'make pdf' gives:

make pdf
make[1]: Entering directory `/usr/src/R/doc'
make[2]: Entering directory `/usr/src/R/doc/manual'
make[2]: *** No rule to make target `-pkg.tex', needed by `stamp-refman-pdf'.  Stop.
make[2]: Leaving directory `/usr/src/R/doc/manual'
make[1]: *** [pdf] Error 2
make[1]: Leaving directory `/usr/src/R/doc'
make: [pdf] Error 2 (ignored)


'make info' works fine.

Is there something wrong with the makefile?  Why the "No rule to make
target `-pkg.tex', needed by `stamp-refman-dvi[or pdf]" messages?  Any
idea what the problem is?

-- 
William D. McCoy
Geosciences
University of Massachusetts
Amherst, MA  01003



From spencer.graves at pdf.com  Thu Aug 28 20:25:46 2003
From: spencer.graves at pdf.com (Spencer Graves)
Date: Thu, 28 Aug 2003 11:25:46 -0700
Subject: [R] Cook-distance-type plot (vertical bars)
References: <5.2.1.1.2.20030828125858.0231ac18@email.med.harvard.edu>
	<5.2.1.1.2.20030828134730.022d3c68@email.med.harvard.edu>
Message-ID: <3F4E492A.6000203@pdf.com>

Have you looked at Modern Applied Statistics with S by Venables and 
Ripley?

hope this helps.  spencer graves

Frank Gibbons wrote:
> Thanks to all who responded, and so promptly too: it works exactly as 
> you describe.
> 
>> >
>> > Figure 13 of Emmanuel Paradis's "R for Beginners" was produced by 
>> termplot
>> > working on an aov object.
>>
>> No, it was produced by plot() working on a aov object, as its caption
>> indicates.  The termplot() is Figure 14.
> 
> 
> Thomas Lumley is quite right, it's produced by plot() (not termplot()), 
> and it is mentioned on p31 of "R for beginners". My mistake.
> 
> In the interest of self education, is there a more comprehensive source 
> for plot-types that I should read? Ideally, this would be something with 
> lots of figures, so that I could browse the figures to find what I want 
> to do, and then look up how to do it. "R for Beginners" goes some way 
> along this path, but perhaps there's something more comprehensive?
> 
> Thanks again,
> 
> -Frank
> 
> PhD, Computational Biologist,
> Harvard Medical School BCMP/SGM-322, 250 Longwood Ave, Boston MA 02115, 
> USA.
> Tel: 617-432-3555       Fax: 617-432-3557       
> http://llama.med.harvard.edu/~fgibbons
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help



From ripley at stats.ox.ac.uk  Thu Aug 28 20:58:05 2003
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Thu, 28 Aug 2003 19:58:05 +0100 (BST)
Subject: [R] Cook-distance-type plot (vertical bars)
In-Reply-To: <5.2.1.1.2.20030828134730.022d3c68@email.med.harvard.edu>
Message-ID: <Pine.LNX.4.44.0308281957020.21228-100000@gannet.stats>

On Thu, 28 Aug 2003, Frank Gibbons wrote:

> Thanks to all who responded, and so promptly too: it works exactly as you 
> describe.
> 
> > >
> > > Figure 13 of Emmanuel Paradis's "R for Beginners" was produced by termplot
> > > working on an aov object.
> >
> >No, it was produced by plot() working on a aov object, as its caption
> >indicates.  The termplot() is Figure 14.
> 
> Thomas Lumley is quite right, it's produced by plot() (not termplot()), and 
> it is mentioned on p31 of "R for beginners". My mistake.
> 
> In the interest of self education, is there a more comprehensive source for 
> plot-types that I should read? Ideally, this would be something with lots 
> of figures, so that I could browse the figures to find what I want to do, 
> and then look up how to do it. "R for Beginners" goes some way along this 
> path, but perhaps there's something more comprehensive?

`An Introduction to R'
`Modern Applied Statistics with S'

as in the R FAQ (and there may be others).

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From paulda at BATTELLE.ORG  Thu Aug 28 21:20:12 2003
From: paulda at BATTELLE.ORG (Paul, David  A)
Date: Thu, 28 Aug 2003 15:20:12 -0400
Subject: [R] Cook-distance-type plot (vertical bars)
Message-ID: <940250A9EB37A24CBE28D858EF07774967AAA9@ws-bco-mse3.milky-way.battelle.org>

I'm (relatively) new to R myself, and recently found 
the documentation by Dr. William Cleveland et al at

http://cm.bell-labs.com/cm/ms/departments/sia/project/trellis/
http://cm.bell-labs.com/cm/ms/departments/sia/project/trellis/software.writi
ng.html

It has been helpful, and covers S/Splus trellis graphics.  In R, the
equivalent is known as "lattice" graphics, and the syntax is
virtually identical.  Earlier today, Dr. Peter Dunn posted a 
notice to the R listserv announcing a workshop in Australia being
led by Dr. John Maindonald... in that announcement, you will find

"...John Maindonald, the author of the forthcoming book 'Data Analysis 
and Graphics Using R: An Example-Based Approach' (with John Braun)..."

so apparently there is a forthcoming book detailing the graphical
capabilities of R.  I'm eagerly awaiting the opportunity to buy it
and digest its contents!  Until then, I think the links to the 
.ps and .pdf files above should serve well.


Best wishes,
  david paul


P.S. - You should keep in mind that there are two classes of 
graphical functions in S/Splus/R: "regular" functions and
"trellis/lattice" functions.  They operate very differently.
After using a few of each, I must say that I am very impressed
with the power and flexibility of the trellis/lattice 
plotting functions.



-----Original Message-----
From: Frank Gibbons [mailto:fgibbons at hms.harvard.edu] 
Sent: Thursday, August 28, 2003 1:53 PM
To: R users
Subject: Re: [R] Cook-distance-type plot (vertical bars)


Thanks to all who responded, and so promptly too: it works exactly as you 
describe.

> >
> > Figure 13 of Emmanuel Paradis's "R for Beginners" was produced by 
> > termplot working on an aov object.
>
>No, it was produced by plot() working on a aov object, as its caption 
>indicates.  The termplot() is Figure 14.

Thomas Lumley is quite right, it's produced by plot() (not termplot()), and 
it is mentioned on p31 of "R for beginners". My mistake.

In the interest of self education, is there a more comprehensive source for 
plot-types that I should read? Ideally, this would be something with lots 
of figures, so that I could browse the figures to find what I want to do, 
and then look up how to do it. "R for Beginners" goes some way along this 
path, but perhaps there's something more comprehensive?

Thanks again,

-Frank

PhD, Computational Biologist,
Harvard Medical School BCMP/SGM-322, 250 Longwood Ave, Boston MA 02115, USA.
Tel: 617-432-3555       Fax: 
617-432-3557       http://llama.med.harvard.edu/~fgibbons

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://www.stat.math.ethz.ch/mailman/listinfo/r-help



From szhan at uoguelph.ca  Thu Aug 28 21:59:11 2003
From: szhan at uoguelph.ca (szhan@uoguelph.ca)
Date: Thu, 28 Aug 2003 15:59:11 -0400
Subject: [R] how to randomize data matrix
Message-ID: <1062100751.3f4e5f0fdc4fc@webmail.uoguelph.ca>

Dear R users:
Is there a function or easier way to randomize the data between rows
(considering a row as a whole unit)? For example,
original data:
   A1  A2  A3  A4
B1 1   2   3   4
B2 5   6   7   8
B3 9   10  11  12
B4 13  14  15  16
randomized data:
   A1  A2  A3  A4
B4 13  14  15  16
B2 5   6   7   8
B1 1   2   3   4
B3 9   10  11  12
Thank you for your attention!
Joshua



From spencer.graves at pdf.com  Thu Aug 28 22:20:43 2003
From: spencer.graves at pdf.com (Spencer Graves)
Date: Thu, 28 Aug 2003 13:20:43 -0700
Subject: [R] how to randomize data matrix
References: <1062100751.3f4e5f0fdc4fc@webmail.uoguelph.ca>
Message-ID: <3F4E641B.9040607@pdf.com>

 >A <- array(1:16, dim=c(4,4))
dimnames(A) <- list(LETTERS[1:4], letters[1:4])
A[sample(1:4), ]
 > A <- array(1:16, dim=c(4,4))
 > dimnames(A) <- list(LETTERS[1:4], letters[1:4])
 > A[sample(1:4), ]
   a b  c  d
C 3 7 11 15
B 2 6 10 14
D 4 8 12 16
A 1 5  9 13
 >
hope this helps.  spencer graves

szhan at uoguelph.ca wrote:
> Dear R users:
> Is there a function or easier way to randomize the data between rows
> (considering a row as a whole unit)? For example,
> original data:
>    A1  A2  A3  A4
> B1 1   2   3   4
> B2 5   6   7   8
> B3 9   10  11  12
> B4 13  14  15  16
> randomized data:
>    A1  A2  A3  A4
> B4 13  14  15  16
> B2 5   6   7   8
> B1 1   2   3   4
> B3 9   10  11  12
> Thank you for your attention!
> Joshua
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help



From apiszcz at solarrain.com  Thu Aug 28 22:52:47 2003
From: apiszcz at solarrain.com (Al Piszcz)
Date: Thu, 28 Aug 2003 16:52:47 -0400 (EDT)
Subject: [R] mtext / expression and font type of bold
Message-ID: <Pine.LNX.4.55.0308281651050.9218@l1>

mtext does not appear to be rendering a 'bold' expression.
Is there another parameter to set? Thx.

example (does not create bold (font=2) on plot)
  mtext( font=2, expression(paste(y, " = ",  x + z), side=3 )



From maj at stats.waikato.ac.nz  Fri Aug 29 05:04:05 2003
From: maj at stats.waikato.ac.nz (maj@stats.waikato.ac.nz)
Date: Fri, 29 Aug 2003 15:04:05 +1200 (NZST)
Subject: [R] Using files as connections
In-Reply-To: <3A822319EB35174CA3714066D590DCD50205CA71@usrymx25.merck.com>
References: <3A822319EB35174CA3714066D590DCD50205CA71@usrymx25.merck.com>
Message-ID: <4454.203.79.123.178.1062126245.squirrel@webmail.scms.waikato.ac.nz>

I nearly forgot to thank Andy Liaw and Tony Plate for their help with this
problem. BTW Andy's method does run faster than the natural fix-up of my
original code.

Murray Jorgensen


> You are using the connection the wrong way.  You need to do something
> like:
>
> fcon <- file("c:/data/perry/data.csv", open="r")
> for (iline in 1:slines) {
>     isel <- isel + 1
>     cline <- readLines(fcon, n=1)
>     ...
> }
> close(fcon)
>
> BTW, here's how I'd do it (not tested!):
>
> strvec <- rep("",slines)
> selected <- sort(sample(flines, slines))
> skip <- c(0, diff(selected) - 1)
> fcon <- file("c:/data/[erry/data.csv", open="r")
> for (i in 1:length(skip)) {
>     ## skip to the selected line
>     readLines(fcon, n=skip[i])
>     strvec[i] <- readLines(fcon, n=1)
> }
> close(fcon)
>
> HTH,
> Andy
>
>
>> -----Original Message-----
>> From: maj at stats.waikato.ac.nz [mailto:maj at stats.waikato.ac.nz]
>> Sent: Wednesday, August 27, 2003 7:19 PM
>> To: r-help at stat.math.ethz.ch
>> Subject: [R] Using files as connections
>>
>>
>> I have been trying to read a random sample of lines from a
>> file into a data frame using readLines(). The help indicates
>> that readLines() will start from the current line if the
>> connection is open, but presented with a closed connection it
>> will open it, start from the beginning, and close it when finished.
>>
>> In the code that follows I tried to open the file before
>> reading but apparently without success, because the result
>> was repeated copies of the first line:
>>
>> flines <- 107165
>> slines <- 100
>> selected <- sort(sample(flines,slines))
>> strvec <- rep("",slines)
>> file("c:/data/perry/data.csv",open="r")
>> isel <- 0
>> for (iline in 1:slines) {
>>   isel <- isel + 1
>>   cline <- readLines("c:/data/perry/data.csv",n=1)
>>   if (iline == selected[isel]) strvec[isel] <- cline else
>>     isel <- isel - 1
>> }
>> close("c:/data/perry/data.csv")
>> sel.flows <- read.table(textConnection(strvec), header=FALSE, sep=",")
>>
>>
>> There was also an error "no applicable method"  for close.
>>
>> Comments gratefully received.
>>
>> Murray Jorgensen
>>
>> ______________________________________________
>> R-help at stat.math.ethz.ch mailing list
>> https://www.stat.math.ethz.ch/mailman/listinfo> /r-help
>>
>
> ------------------------------------------------------------------------------
> Notice:  This e-mail message, together with any
> attachments,...{{dropped}}
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help



From bido at mac.com  Fri Aug 29 08:09:53 2003
From: bido at mac.com (Francisco J. Bido)
Date: Fri, 29 Aug 2003 01:09:53 -0500
Subject: [R] Creating a new table from a set of constraints
Message-ID: <68028FE4-D9E7-11D7-B1AF-000393B90A0A@mac.com>

Hi Everyone,

Here's a silly newbie question.  How do I remove unwanted rows from an 
R table?  Say that I read my data as:

X <- read.table("mydata.txt")

and say that there are columns for age and gender.  Call these X[5] and 
X[10], respectively.
Here, X[5]  is a column of positive integers and X[10] is binary valued 
i.e., zero (for male) and one (for female)

Now, say that I want to form a new table called Y which has the 
following constraints:

1.  Only females that are between 18 and 40 years old.
2.  Only males that are between 20 and 30 years old

I can do this using a typical procedural approach (no different than C 
programmer would) but it seems
to me that R has many shortcuts and so I thought I ask first before 
heading on an inefficient path. What's
a good way of doing this, my data set is very large?

Thanks,
-Francisco



From Arnaud.Dowkiw at dpi.qld.gov.au  Fri Aug 29 08:23:55 2003
From: Arnaud.Dowkiw at dpi.qld.gov.au (Dowkiw, Arnaud)
Date: Fri, 29 Aug 2003 16:23:55 +1000
Subject: [R] Creating a new table from a set of constraints
Message-ID: <200308290623.h7T6NxP9012916@dpi-gw1.dpi.qld.gov.au>

Hi Francisco,

what I would do :

names(X)[c(5,10)]<-c("Age","Gender")

Xnew1<-X[X$Gender==1 & X$Age>=18 & X$Age <=40,]

Xnew2<-X[X$Gender==0 & X$Age>=20 & X$Age <=30,]

Xnew<-rbind(Xnew1,Xnew2)

But there must be something more elegant,

Good luck,

Arnaud


-----Original Message-----
From: Francisco J. Bido [mailto:bido at mac.com]
Sent: Friday, 29 August 2003 4:10 PM
To: r-help at stat.math.ethz.ch
Subject: [R] Creating a new table from a set of constraints


Hi Everyone,

Here's a silly newbie question.  How do I remove unwanted rows from an 
R table?  Say that I read my data as:

X <- read.table("mydata.txt")

and say that there are columns for age and gender.  Call these X[5] and 
X[10], respectively.
Here, X[5]  is a column of positive integers and X[10] is binary valued 
i.e., zero (for male) and one (for female)

Now, say that I want to form a new table called Y which has the 
following constraints:

1.  Only females that are between 18 and 40 years old.
2.  Only males that are between 20 and 30 years old

I can do this using a typical procedural approach (no different than C 
programmer would) but it seems
to me that R has many shortcuts and so I thought I ask first before 
heading on an inefficient path. What's
a good way of doing this, my data set is very large?

Thanks,
-Francisco

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://www.stat.math.ethz.ch/mailman/listinfo/r-help 

********************************DISCLAIMER******************...{{dropped}}



From ligges at statistik.uni-dortmund.de  Fri Aug 29 08:26:50 2003
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Fri, 29 Aug 2003 08:26:50 +0200
Subject: [R] mtext / expression and font type of bold
In-Reply-To: <Pine.LNX.4.55.0308281651050.9218@l1>
References: <Pine.LNX.4.55.0308281651050.9218@l1>
Message-ID: <3F4EF22A.8090207@statistik.uni-dortmund.de>

Al Piszcz wrote:

> mtext does not appear to be rendering a 'bold' expression.
> Is there another parameter to set? Thx.
> 
> example (does not create bold (font=2) on plot)
>   mtext( font=2, expression(paste(y, " = ",  x + z), side=3 )

Mathematical expressions are handled differently from normal text.

You might want to use
   mtext(expression(bold(y == x + z)), side = 3)

See ?plotmath for details.

Uwe Ligges



From ripley at stats.ox.ac.uk  Fri Aug 29 08:37:00 2003
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Fri, 29 Aug 2003 07:37:00 +0100 (BST)
Subject: [R] Creating a new table from a set of constraints
In-Reply-To: <200308290623.h7T6NxP9012916@dpi-gw1.dpi.qld.gov.au>
Message-ID: <Pine.LNX.4.44.0308290732200.23115-100000@gannet.stats>

On Fri, 29 Aug 2003, Dowkiw, Arnaud wrote:

> Hi Francisco,
> 
> what I would do :
> 
> names(X)[c(5,10)]<-c("Age","Gender")
> 
> Xnew1<-X[X$Gender==1 & X$Age>=18 & X$Age <=40,]
> 
> Xnew2<-X[X$Gender==0 & X$Age>=20 & X$Age <=30,]
> 
> Xnew<-rbind(Xnew1,Xnew2)
> 
> But there must be something more elegant,

How about

set1 <- with(X, Gender==1 & Age>=18 & Age <=40)
set2 <- with(X, Gender==0 & Age>=20 & Age <=30)
Y <- X[set1 | set 2, ]

> -----Original Message-----
> From: Francisco J. Bido [mailto:bido at mac.com]
> Sent: Friday, 29 August 2003 4:10 PM
> To: r-help at stat.math.ethz.ch
> Subject: [R] Creating a new table from a set of constraints
> 
> 
> Hi Everyone,
> 
> Here's a silly newbie question.  How do I remove unwanted rows from an 
> R table?  Say that I read my data as:
> 
> X <- read.table("mydata.txt")
> 
> and say that there are columns for age and gender.  Call these X[5] and 
> X[10], respectively.
> Here, X[5]  is a column of positive integers and X[10] is binary valued 
> i.e., zero (for male) and one (for female)
> 
> Now, say that I want to form a new table called Y which has the 
> following constraints:
> 
> 1.  Only females that are between 18 and 40 years old.
> 2.  Only males that are between 20 and 30 years old
> 
> I can do this using a typical procedural approach (no different than C 
> programmer would) but it seems
> to me that R has many shortcuts and so I thought I ask first before 
> heading on an inefficient path. What's
> a good way of doing this, my data set is very large?
> 
> Thanks,
> -Francisco
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help 
> 
> ********************************DISCLAIMER******************...{{dropped}}
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> 
> 

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From bido at mac.com  Fri Aug 29 08:46:27 2003
From: bido at mac.com (Francisco J. Bido)
Date: Fri, 29 Aug 2003 01:46:27 -0500
Subject: [R] Creating a new table from a set of constraints
In-Reply-To: <2A23A3B738DB3E47BD676EDC51C064D10101A79A@iris.nswcc.org.au>
Message-ID: <835EECE9-D9EC-11D7-B1AF-000393B90A0A@mac.com>

Thanks everyone!  I now see how to handle the situation.  This has to 
be the most responsive mailing list ever...

Best,
-Francisco

On Friday, August 29, 2003, at 01:39 AM, Andrew Hayen wrote:

> Also see this page: http://www.ats.ucla.edu/stat/SPLUS/faq/subset_R.htm
>
> A
>
> -----Original Message-----
> From: Francisco J. Bido [mailto:bido at mac.com]
> Sent: Friday, 29 August 2003 4:10 PM
> To: r-help at stat.math.ethz.ch
> Subject: [R] Creating a new table from a set of constraints
>
>
> Hi Everyone,
>
> Here's a silly newbie question.  How do I remove unwanted rows from an
> R table?  Say that I read my data as:
>
> X <- read.table("mydata.txt")
>
> and say that there are columns for age and gender.  Call these X[5] and
> X[10], respectively.
> Here, X[5]  is a column of positive integers and X[10] is binary valued
> i.e., zero (for male) and one (for female)
>
> Now, say that I want to form a new table called Y which has the
> following constraints:
>
> 1.  Only females that are between 18 and 40 years old.
> 2.  Only males that are between 20 and 30 years old
>
> I can do this using a typical procedural approach (no different than C
> programmer would) but it seems
> to me that R has many shortcuts and so I thought I ask first before
> heading on an inefficient path. What's
> a good way of doing this, my data set is very large?
>
> Thanks,
> -Francisco
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help



From Vincent.Spiesser at univ-tlse1.fr  Fri Aug 29 11:07:54 2003
From: Vincent.Spiesser at univ-tlse1.fr (Vincent Spiesser)
Date: Fri, 29 Aug 2003 11:07:54 +0200
Subject: [R] extract numerical variables from a data frame
Message-ID: <4.2.0.58.20030829105858.00a896c0@mail.univ-tlse1.fr>

Hi

I try to create from a data frame a new one which contains only the 
numerical variables (or factorial ones).

Is there any function which does this task directly ?
Or, is there any function which return the mode of each columns of a data 
frame. ?

Thanks a lot for any help you can offer me,

Vincent Spiesser



From ripley at stats.ox.ac.uk  Fri Aug 29 11:21:43 2003
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Fri, 29 Aug 2003 10:21:43 +0100 (BST)
Subject: [R] extract numerical variables from a data frame
In-Reply-To: <4.2.0.58.20030829105858.00a896c0@mail.univ-tlse1.fr>
Message-ID: <Pine.LNX.4.44.0308291018350.22041-100000@gannet.stats>

On Fri, 29 Aug 2003, Vincent Spiesser wrote:

> Hi
> 
> I try to create from a data frame a new one which contains only the 
> numerical variables (or factorial ones).
> 
> Is there any function which does this task directly ?
> Or, is there any function which return the mode of each columns of a data 
> frame. ?

I think you want the class, as in

sapply(adf, class)

You could use mode if you really wanted it, but watch out for e.g. the 
mode of a factor being "numeric".

However, for your original question I would use

numeric columns:

DF[sapply(DF, is.numeric)]

factor columns:

DF[sapply(DF, is.factor)]

and similar code is found throughout the R sources.

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From Simon.Fear at synequanon.com  Fri Aug 29 11:24:22 2003
From: Simon.Fear at synequanon.com (Simon Fear)
Date: Fri, 29 Aug 2003 10:24:22 +0100
Subject: [R] extract numerical variables from a data frame
Message-ID: <6C8A8033ABC1E3468048ABC4F13CE572AB6101@synequanon01>

for data.frame called dframe:

newframe <- dframe[ , lapply(dframe, is.numeric)]

For the mode of the columns

lapply(dframe, mode)

See ?lapply !!

> -----Original Message-----
> From: Vincent Spiesser [mailto:Vincent.Spiesser at univ-tlse1.fr]
> Sent: 29 August 2003 10:08
> To: R-help at stat.math.ethz.ch
> Subject: [R] extract numerical variables from a data frame
> 
> 
> Security Warning:
> If you are not sure an attachment is safe to open please contact 
> Andy on x234. There are 0 attachments with this message.
> ________________________________________________________________
> 
> Hi
> 
> I try to create from a data frame a new one which contains only the 
> numerical variables (or factorial ones).
> 
> Is there any function which does this task directly ?
> Or, is there any function which return the mode of each columns of a
> data 
> frame. ?
> 
> Thanks a lot for any help you can offer me,
> 
> Vincent Spiesser
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
>
 

Simon Fear
Senior Statistician
Syne qua non Ltd
Tel: +44 (0) 1379 644449
Fax: +44 (0) 1379 644445
email: Simon.Fear at synequanon.com
web: http://www.synequanon.com
 
Number of attachments included with this message: 0
 
This message (and any associated files) is confidential and\...{{dropped}}



From mkondrin at hppi.troitsk.ru  Fri Aug 29 22:53:11 2003
From: mkondrin at hppi.troitsk.ru (M.Kondrin)
Date: Fri, 29 Aug 2003 13:53:11 -0700
Subject: [R] What fontfamily x11() device normally uses?
Message-ID: <3F4FBD37.9040308@hppi.troitsk.ru>

Hello!
Can I somehow set x11 device to use font different from default 
helvetica-arial (times for example?)? Is this font hardcoded into R or 
can I substitute some other system font? Does x11 device use 
standard/user-supplied .Xresources file (under Linux)?
The questions are the same for gtk() device (with .gtkrc substituted for 
.Xresources).
Thanks in advance.



From e.pebesma at geog.uu.nl  Fri Aug 29 12:22:53 2003
From: e.pebesma at geog.uu.nl (Edzer J. Pebesma)
Date: Fri, 29 Aug 2003 12:22:53 +0200
Subject: [R] levelplot behaviour when "at" cuts the z range
Message-ID: <3F4F297D.8020001@geog.uu.nl>

Consider the following examples:

library(lattice)
x = c(1,1,2,2)
y = c(1,2,1,2)
z = 1:4
levelplot(z~x+y,at=c(.5, 1.5, 2.5, 3.5, 4.5)) # correct
levelplot(z~x+y,at=c(.5, 1.5, 2.5))           # ?

The second plot is clearly incorrect. However, I don't know
what correct behaviour is: ignore everything above 2.5 and
issue a warning? Reject at values that do not cover the
data range, and issue an error message? I think I prefer
the second, as it does not hide extremes.
--
Edzer



From ripley at stats.ox.ac.uk  Fri Aug 29 12:37:56 2003
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Fri, 29 Aug 2003 11:37:56 +0100 (BST)
Subject: [R] What fontfamily x11() device normally uses?
In-Reply-To: <3F4FBD37.9040308@hppi.troitsk.ru>
Message-ID: <Pine.LNX.4.44.0308291105160.13328-100000@gannet.stats>

You have the sources (for each), so why not read them?

The Unix X11 device has this helvetica hardcoded: what has arial to do 
with it?   If you would like more flexibility, please supply a patch 
against the current R-devel sources.

On Fri, 29 Aug 2003, M.Kondrin wrote:

> Can I somehow set x11 device to use font different from default 
> helvetica-arial (times for example?)? Is this font hardcoded into R or 
> can I substitute some other system font? Does x11 device use 
> standard/user-supplied .Xresources file (under Linux)?
> The questions are the same for gtk() device (with .gtkrc substituted for 
> .Xresources).


-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From ucgamdo at ucl.ac.uk  Fri Aug 29 12:55:01 2003
From: ucgamdo at ucl.ac.uk (ucgamdo@ucl.ac.uk)
Date: Fri, 29 Aug 2003 11:55:01 +0100
Subject: [R] identifying outliers
Message-ID: <3.0.5.32.20030829115501.007d3db0@pop-server.ucl.ac.uk>

You can identify your outliers with the boxplot function itself. Here is an
example:
(you can just copy it and paste it into R)

# This generates a rondom variable x with 10 elements

x = rnorm(100)
boxplot(x) # you shouldn't see any outliers here although sometimes yow will

# lets add some outliers intentionally
x = c(21, 20, 25, x) # now 10, 15 and 20 are outliers

myboxplot <- boxplot(x) # now you should see your three outliers

myboxplot$out # it will print the values of the outliers

# additionally, myboxplot stores a series of other useful statistics, try

names(myboxplot)

# you shold get something like
# [1] "stats" "n"     "conf"  "out"   "group" "names"
# try for example

myboxplot$stats

# Hope this is useful to you.



From lmarisa at pasteur.fr  Fri Aug 29 14:20:49 2003
From: lmarisa at pasteur.fr (Laetitia Marisa)
Date: Fri, 29 Aug 2003 14:20:49 +0200
Subject: [R] R and pointer
Message-ID: <394495E0-DA1B-11D7-A4BC-003065E01564@pasteur.fr>

Hi everyone,

I want to write a function that modify directly variables passed as 
parameters (the equivalent in C language of  *ptr/&ptr) so that I don't 
have to return a list and to reaffect all my variables.
Is it possible to do so in R?

Thanks a lot.

Laetitia Marisa.



From jgentry at jimmy.harvard.edu  Fri Aug 29 14:34:21 2003
From: jgentry at jimmy.harvard.edu (Jeff Gentry)
Date: Fri, 29 Aug 2003 08:34:21 -0400 (EDT)
Subject: [R] R and pointer
In-Reply-To: <394495E0-DA1B-11D7-A4BC-003065E01564@pasteur.fr>
Message-ID: <Pine.SOL.4.20.0308290833590.20313-100000@santiam.dfci.harvard.edu>

> I want to write a function that modify directly variables passed as
> parameters (the equivalent in C language of *ptr/&ptr) so that I don't
> have to return a list and to reaffect all my variables. Is it possible
> to do so in R? Thanks a lot.

You can use environments as they're passed by reference.



From ripley at stats.ox.ac.uk  Fri Aug 29 14:37:20 2003
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Fri, 29 Aug 2003 13:37:20 +0100 (BST)
Subject: [R] R and pointer
In-Reply-To: <394495E0-DA1B-11D7-A4BC-003065E01564@pasteur.fr>
Message-ID: <Pine.LNX.4.44.0308291330270.13576-100000@gannet.stats>

On Fri, 29 Aug 2003, Laetitia Marisa wrote:

> I want to write a function that modify directly variables passed as 
> parameters (the equivalent in C language of  *ptr/&ptr) so that I don't 
> have to return a list and to reaffect all my variables.
> Is it possible to do so in R?

Yes, with the .Call/.External interface (and there are examples in the ts 
package).  To a limited extent it is possible with .C(DUP=FALSE), but you 
really don't want to go there.

I would want a very good understanding of R's copying semantics before 
doing this (and those are liable to change, too).

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From rpeng at jhsph.edu  Fri Aug 29 15:59:16 2003
From: rpeng at jhsph.edu (Roger D. Peng)
Date: Fri, 29 Aug 2003 09:59:16 -0400
Subject: [R] Creating a new table from a set of constraints
In-Reply-To: <68028FE4-D9E7-11D7-B1AF-000393B90A0A@mac.com>
References: <68028FE4-D9E7-11D7-B1AF-000393B90A0A@mac.com>
Message-ID: <3F4F5C34.9030004@jhsph.edu>

I would use the subset() function.  Assuming the data frame has variable 
names "Gender" and "Age", you could do:

Y1 <- subset(X, Gender == 1 & Age >= 18 & Age <= 40)
Y2 <- subset(X, Gender == 0 & Age >= 20 & Age <= 30)

-roger

Francisco J. Bido wrote:

> Hi Everyone,
>
> Here's a silly newbie question.  How do I remove unwanted rows from an 
> R table?  Say that I read my data as:
>
> X <- read.table("mydata.txt")
>
> and say that there are columns for age and gender.  Call these X[5] 
> and X[10], respectively.
> Here, X[5]  is a column of positive integers and X[10] is binary 
> valued i.e., zero (for male) and one (for female)
>
> Now, say that I want to form a new table called Y which has the 
> following constraints:
>
> 1.  Only females that are between 18 and 40 years old.
> 2.  Only males that are between 20 and 30 years old
>
> I can do this using a typical procedural approach (no different than C 
> programmer would) but it seems
> to me that R has many shortcuts and so I thought I ask first before 
> heading on an inefficient path. What's
> a good way of doing this, my data set is very large?
>
> Thanks,
> -Francisco
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
>



From hb at maths.lth.se  Fri Aug 29 15:46:24 2003
From: hb at maths.lth.se (Henrik Bengtsson)
Date: Fri, 29 Aug 2003 15:46:24 +0200
Subject: [R] R and pointer
In-Reply-To: <Pine.SOL.4.20.0308290833590.20313-100000@santiam.dfci.harvard.edu>
Message-ID: <000101c36e33$f0ce3860$e502eb82@maths.lth.se>

Three references that are of interest (the two first are related to the
idea of using environments to do the job):

 [1] http://www.maths.lth.se/help/R/ImplementingReferences/
 [2] http://www.maths.lth.se/help/R/R.oo/
 [3] http://www.omegahat.org/OOP/

All of the above are written in the light object-oriented programming,
but from [1] you quite easily get what is needed for just emulating
"pointers". Be careful though as R is a functional language.

Best wishes

Henrik Bengtsson
Lund University

> -----Original Message-----
> From: r-help-bounces at stat.math.ethz.ch 
> [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Jeff Gentry
> Sent: den 29 augusti 2003 14:34
> To: Laetitia Marisa
> Cc: r-help at stat.math.ethz.ch
> Subject: Re: [R] R and pointer
> 
> 
> > I want to write a function that modify directly variables passed as 
> > parameters (the equivalent in C language of *ptr/&ptr) so 
> that I don't 
> > have to return a list and to reaffect all my variables. Is 
> it possible 
> > to do so in R? Thanks a lot.
> 
> You can use environments as they're passed by reference.
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list 
> https://www.stat.math.ethz.ch/mailman/listinfo> /r-help
> 
>



From monica.palaseanu-lovejoy at stud.man.ac.uk  Fri Aug 29 15:49:01 2003
From: monica.palaseanu-lovejoy at stud.man.ac.uk (Monica Palaseanu-Lovejoy)
Date: Fri, 29 Aug 2003 14:49:01 +0100
Subject: [R] vardiag package help
Message-ID: <E19sjch-0002vc-DS@probity.mcc.ac.uk>

Hi,

First of all a big THANK YOU to all who answered me 
yesterday. 

I am back to my problems with outliers. I did a qqnorm on my 
data (as somebody suggested), but I will like to compare results 
doing other plots as well. I used ?cook.distance? on lm and glm 
objects, but still I am not happy with that. 

So now I am loading the ?vardiag? package. First thing is to 
transform my data (frame.data) into a matrix ? which I did with 
?as.matrix?. I verify my new object as being amatrix ? and it is, 
and I list it to be sure that in is a matrix of n rows with 3 colums.

Now I want to use the command ?varobj? to transform my 
matrix into a variogram object.

The param are:
varobj(m, iter = 50, tolerance = 2e-04, trace = 1, loo = F)
where, m is a n by 3 matrix with spatial data.

My matrix has a column for x value, for y value and for pah16 
concentrations.

I always get the following error:
Error in matrix(i1[row(i1) < col(i1)], n1, n1) : 
        cannot allocate vector of length 1223600400

What does this means and what I am doing wrong?

Thank you in advance,

Monica

Monica Palaseanu-Lovejoy
University of Manchester
School of Geography
Mansfield Cooper Building 
Oxford Road, Manchester
M13 9PL, UK. 
email: monica.palaseanu-lovejoy at stud.man.ac.uk



From B.Rowlingson at lancaster.ac.uk  Fri Aug 29 16:00:47 2003
From: B.Rowlingson at lancaster.ac.uk (Barry Rowlingson)
Date: Fri, 29 Aug 2003 15:00:47 +0100
Subject: [R] R and pointer
In-Reply-To: <000101c36e33$f0ce3860$e502eb82@maths.lth.se>
References: <000101c36e33$f0ce3860$e502eb82@maths.lth.se>
Message-ID: <3F4F5C8F.6040600@lancaster.ac.uk>

Henrik Bengtsson wrote:

> All of the above are written in the light object-oriented programming,
> but from [1] you quite easily get what is needed for just emulating
> "pointers". Be careful though as R is a functional language.


Cutting to what I think was the gist of the original poster's question, 
can I write a function, foo, that does this:

 > x <- 3
 > foo(x)
 > x
[1] 9

for any x, in any situation? I'm guessing its doable, but ugly...


Baz



From Simon.Fear at synequanon.com  Fri Aug 29 16:27:14 2003
From: Simon.Fear at synequanon.com (Simon Fear)
Date: Fri, 29 Aug 2003 15:27:14 +0100
Subject: [R] R and pointer
Message-ID: <6C8A8033ABC1E3468048ABC4F13CE572AC0D96@synequanon01>

foo <- function(y) assign(deparse(substitute(y)), y^2, parent.frame())

assigns to the calling frame. Or you could put the required frame as
a second argument.

> -----Original Message-----
> From: Barry Rowlingson [mailto:B.Rowlingson at lancaster.ac.uk]
> Cutting to what I think was the gist of the original poster's 
> question, 
> can I write a function, foo, that does this:
> 
>  > x <- 3
>  > foo(x)
>  > x
> [1] 9
> 
> for any x, in any situation? I'm guessing its doable, but ugly...
> 
> 
> Baz
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
>
 

Simon Fear
Senior Statistician
Syne qua non Ltd
Tel: +44 (0) 1379 644449
Fax: +44 (0) 1379 644445
email: Simon.Fear at synequanon.com
web: http://www.synequanon.com
 
Number of attachments included with this message: 0
 
This message (and any associated files) is confidential and\...{{dropped}}



From solares at unsl.edu.ar  Fri Aug 29 17:03:46 2003
From: solares at unsl.edu.ar (solares@unsl.edu.ar)
Date: Fri, 29 Aug 2003 12:03:46 -0300 (ART)
Subject: [R] Six axis y with axis function?
Message-ID: <54108.170.210.173.216.1062169426.squirrel@inter14.unsl.edu.ar>

Hi, 


I am trying to plot a time serie of six colum of data sets on one plot but 
with using a different y-axis ranges for each - preferably with one shown 
on each side of the graph.  I'm trying with axis function but not good luck
i will for each they plot with our proper scale and range for example

time      A    B    C...etc
08:00:01  1000  15  2
08:00:02  2200  17  5
08:00:03  2500  19  7
...plot.ts(..);axis(what parameter?)
How i'cant plot six axis with your proper scale and range?
thanks. Ruben

Is there a function that will allow me to do this



From spencer.graves at pdf.com  Fri Aug 29 18:02:02 2003
From: spencer.graves at pdf.com (Spencer Graves)
Date: Fri, 29 Aug 2003 09:02:02 -0700
Subject: [R] Six axis y with axis function?
References: <54108.170.210.173.216.1062169426.squirrel@inter14.unsl.edu.ar>
Message-ID: <3F4F78FA.5010300@pdf.com>

The following works is tuned for S-Plus 6.1, but these tools can be used 
to produce what I hear in your question:

par(mar=c(5,7,4,7)+.1)# create space for 2 axes on each side
plot(0:1, 0:1, type="l", ylab="y1")
axis(side=2, at=(1:2)/3, labels=(1:2)/3, line=4)
mtext(text="y2", side=2, line=6.5)

axis(side=4, at=(1:3)/4, labels=2^(1:3))
mtext(text="y3", side=4, line=1.5)

axis(side=4, at=(1:4)/5, labels=letters[1:4], line=3.5)
mtext(text="y4", side=4, line=5)

hope this helps.  spencer graves

solares at unsl.edu.ar wrote:
> Hi, 
> 
> 
> I am trying to plot a time serie of six colum of data sets on one plot but 
> with using a different y-axis ranges for each - preferably with one shown 
> on each side of the graph.  I'm trying with axis function but not good luck
> i will for each they plot with our proper scale and range for example
> 
> time      A    B    C...etc
> 08:00:01  1000  15  2
> 08:00:02  2200  17  5
> 08:00:03  2500  19  7
> ...plot.ts(..);axis(what parameter?)
> How i'cant plot six axis with your proper scale and range?
> thanks. Ruben
> 
> Is there a function that will allow me to do this
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help



From deepayan at stat.wisc.edu  Fri Aug 29 18:02:17 2003
From: deepayan at stat.wisc.edu (Deepayan Sarkar)
Date: Fri, 29 Aug 2003 11:02:17 -0500
Subject: [R] levelplot behaviour when "at" cuts the z range
In-Reply-To: <3F4F297D.8020001@geog.uu.nl>
References: <3F4F297D.8020001@geog.uu.nl>
Message-ID: <200308291102.17634.deepayan@stat.wisc.edu>

On Friday 29 August 2003 05:22, Edzer J. Pebesma wrote:
> Consider the following examples:
>
> library(lattice)
> x = c(1,1,2,2)
> y = c(1,2,1,2)
> z = 1:4
> levelplot(z~x+y,at=c(.5, 1.5, 2.5, 3.5, 4.5)) # correct
> levelplot(z~x+y,at=c(.5, 1.5, 2.5))           # ?
>
> The second plot is clearly incorrect. However, I don't know
> what correct behaviour is: ignore everything above 2.5 and
> issue a warning? Reject at values that do not cover the
> data range, and issue an error message? I think I prefer
> the second, as it does not hide extremes.

Definitely a bug, the color vector was just being recycled. I seem to have 
inadverdently fixed this a couple of days ago while making some related 
changes, and now the z-values that are out of bounds are simply not shown (as 
would happen if the corresponding z-values were NA). Does that sound 
acceptable ? I don't like your second option because users may knowingly want 
to do something like this (sort of what xlim and ylim are for). 

I'll send you the modifications off-list for you to try out.

Deepayan



From paulda at BATTELLE.ORG  Fri Aug 29 18:14:42 2003
From: paulda at BATTELLE.ORG (Paul, David  A)
Date: Fri, 29 Aug 2003 12:14:42 -0400
Subject: [R] Lattice plot questions
Message-ID: <940250A9EB37A24CBE28D858EF07774967AAAD@ws-bco-mse3.milky-way.battelle.org>

Win2k, R1.7.1:

I am currently working with some growth curve data from a 
biotoxicology experiment.  Each of 12 subjects had their blood
drawn at 0, 2, 4, 6, 8, and 10 weeks.  For the purposes of the 
project, it would be helpful if I were able to do the following:

	a.  Produce 12 panels, each displaying the *same* data, with
		the "strip" at the top of a particular panel showing
		exactly one of the subject id's
	b.  For a particular panel, plot the time course data for
		the subject whose id appears in the strip in some
		really obvious color (like yellow, red, or blue),
		with circles at each time point, and connecting the
		"dots".  In the same panel, those subjects whose id's
		did NOT appear in the strip would ideally have their
		data plotted in black, possibly with no symbols at
		any of the time points, but still connecting the
		"dots".

I apologize if the description of what is needed is vague.
It is possible to address (a) by duplicating the data set
twelve times and cbind()ing a factor variable so that each
of the duplicated data sets is associated with exactly one
of the subject id's.  I have done this, and the code

subjects <- foo$subject.id
superpose.symbol <- trellis.par.get("superpose.symbol")
xyplot(log.response ~ week | factor.variable, data = foo, 
	panel = panel.superpose,
	groups = subjects,
	key = list(space = "top", columns = 6, transparent = TRUE, 
			text = list(levels(subjects)),
			points = Rows(superpose.symbol,1:7))
       )

works, with "factor.variable" corresponding to the cbind()ed
factor variable.  Unfortunately, I have been unable to figure
out how to do (b).  I would also like to determine a more
memory efficient way of doing (a) -- even though my current
data set is small (only 10 weeks of data per subject), that will 
change.

Also, an examination of "superpose.symbol" reveals the
following:


> superpose.symbol <- trellis.par.get("superpose.symbol")
> superpose.symbol
$cex
[1] 0.8 0.8 0.8 0.8 0.8 0.8 0.8

$col
[1] "#00ffff" "#ff00ff" "#00ff00" "#ff7f00" "#007eff" "#ffff00" "#ff0000"

$font
[1] 1 1 1 1 1 1 1

$pch
[1] "o" "o" "o" "o" "o" "o" "o"


This seems to indicate that lattice graphics are restricted to
no more than seven colors, though I cannot believe that this is true.  
Where might I search/read/learn about the available colors for
lattice plots and how to denote them?  (I find "#ff7f00" to be
a little cryptic, for example.)  Is there a table that relates
these ?hexadecimal? numbers to descriptions like "navy blue"
for Win2k?

Finally, is it possible to cause superpose.symbol to default to
more than 7 $cex/$col/$font/$pch values?  Since I am working with
12 subjects, plots that assign a different color to each wind up
duplicating colors and symbols at some point.


Much thanks in advance,
  david paul



From larkey at mail.utexas.edu  Fri Aug 29 18:20:40 2003
From: larkey at mail.utexas.edu (Levi Larkey)
Date: Fri, 29 Aug 2003 11:20:40 -0500
Subject: [R] difference between <- and =
Message-ID: <3F4F7D58.1020009@mail.utexas.edu>

Hi,

I'm somewhat new to R and I'm trying to figure out the difference 
between the operators <- and =.

I've noticed that <- cannot be used to bind arguments to values in 
function definitions and calls.  That is, f(x <- 2) sets x to 2 in the 
calling frame and then calls f(2) because the expression x <- 2 returns 
a value of 2, whereas f(x = 2) sets x = 2 in the evaluation frame and 
not in the calling frame.  From what I can tell, = is only different 
than <- in the context of function definitions and calls.

Is there any reason for using one operator over the other outside of 
function definitions and calls?  Can someone point me to a precise 
specification of what each operator does?  Any help would be appreciated!

Levi



From thpe at hhbio.wasser.tu-dresden.de  Fri Aug 29 18:31:19 2003
From: thpe at hhbio.wasser.tu-dresden.de (Thomas Petzoldt)
Date: Fri, 29 Aug 2003 18:31:19 +0200
Subject: [R] difference between <- and =
In-Reply-To: <3F4F7D58.1020009@mail.utexas.edu>
References: <3F4F7D58.1020009@mail.utexas.edu>
Message-ID: <3F4F7FD7.20204@hhbio.wasser.tu-dresden.de>

Levi Larkey schrieb:
> Hi,
> 
> I'm somewhat new to R and I'm trying to figure out the difference 
> between the operators <- and =.

<- is an assignement operator
=  is primarily used for named arguments

Some thoughts about the use of = as assignement operator can be found on:

http://developer.r-project.org/equalAssign.html

Thomas



From deepayan at stat.wisc.edu  Fri Aug 29 19:54:12 2003
From: deepayan at stat.wisc.edu (Deepayan Sarkar)
Date: Fri, 29 Aug 2003 12:54:12 -0500
Subject: [R] Lattice plot questions
In-Reply-To: <940250A9EB37A24CBE28D858EF07774967AAAD@ws-bco-mse3.milky-way.battelle.org>
References: <940250A9EB37A24CBE28D858EF07774967AAAD@ws-bco-mse3.milky-way.battelle.org>
Message-ID: <200308291254.12267.deepayan@stat.wisc.edu>


Let's consider this simulated data:

foo <- data.frame(resp = do.call("c", lapply(as.list(rep(6, 12)),
                  function(x) sort(rnorm(x)))),
                  week = rep(2*0:5, 12),
                  id = factor(rep(1:12, each = 6)))

Does the following give you what you want ?

xyplot(resp ~ week | id, foo,

       panel = function(x, y, col = sup.sym$col, panel.number, ...) {

           ## this part accesses the data frame directly, and needs to
           ## be adjusted accordingly if the name of the variables
           ## therein change. This draws the part meant for the whole
           ## data set

           panel.superpose(x = foo$week, y = foo$resp,
                           subscripts = TRUE, groups = foo$id,
                           type = 'l', col = "black")

           ## this part draws the colored lines, each overwriting one
           ## of the background lines

           sup.sym <- trellis.par.get("superpose.symbol")
           col.this <- col[1 + ((panel.number - 1) %% length(col))]
           panel.xyplot(x, y, type = 'b', col = col.this, 
                        lwd = 2, cex = 1.2, ...)
           
       })

This will by default use the colors in superpose.symbol, but you can override 
that by specifying your own color as an argument to xyplot, perhaps something 
like 

xyplot(resp ~ week | id, foo, col = c("red", "yellow", "blue"),
       panel = ....

I personally don't see the point of using different colors though (that would 
seem useful to me only if the different colors appeared within the same 
panel).

There's no restriction on the length of the elements of 
trellis.par.get("superpose.symbol"). 7 (which is just as good a number as any 
other) is the default choice, inspired by Trellis in S. Set it to whatever 
you want with trellis.par.set() or lset() (or just supply the colors to 
xyplot(), which is usually more convenient for specific tasks like this).

Names like "navy blue" are just a convenience, and I doubt that all 256^3 
colors in the RGB space have been named, so trying to get a name for 
"#ff7f00" may not get you anywhere. R has a function col2rgb() that converts 
colors to RGB, e.g.

> col2rgb("#ff7f00")
      [,1]
red    255
green  127
blue     0
> col2rgb("navyblue")
      [,1]
red      0
green    0
blue   128

If you are more comfortable with named colors, use them--- R accepts them 
everywhere. A list of names it recognizes is produced by 

> colors()

The help pages for colors and col2rgb have more information.

HTH,

Deepayan

On Friday 29 August 2003 11:14, Paul, David A wrote:
> Win2k, R1.7.1:
>
> I am currently working with some growth curve data from a
> biotoxicology experiment.  Each of 12 subjects had their blood
> drawn at 0, 2, 4, 6, 8, and 10 weeks.  For the purposes of the
> project, it would be helpful if I were able to do the following:
>
> 	a.  Produce 12 panels, each displaying the *same* data, with
> 		the "strip" at the top of a particular panel showing
> 		exactly one of the subject id's
> 	b.  For a particular panel, plot the time course data for
> 		the subject whose id appears in the strip in some
> 		really obvious color (like yellow, red, or blue),
> 		with circles at each time point, and connecting the
> 		"dots".  In the same panel, those subjects whose id's
> 		did NOT appear in the strip would ideally have their
> 		data plotted in black, possibly with no symbols at
> 		any of the time points, but still connecting the
> 		"dots".
>
> I apologize if the description of what is needed is vague.
> It is possible to address (a) by duplicating the data set
> twelve times and cbind()ing a factor variable so that each
> of the duplicated data sets is associated with exactly one
> of the subject id's.  I have done this, and the code
>
> subjects <- foo$subject.id
> superpose.symbol <- trellis.par.get("superpose.symbol")
> xyplot(log.response ~ week | factor.variable, data = foo,
> 	panel = panel.superpose,
> 	groups = subjects,
> 	key = list(space = "top", columns = 6, transparent = TRUE,
> 			text = list(levels(subjects)),
> 			points = Rows(superpose.symbol,1:7))
>        )
>
> works, with "factor.variable" corresponding to the cbind()ed
> factor variable.  Unfortunately, I have been unable to figure
> out how to do (b).  I would also like to determine a more
> memory efficient way of doing (a) -- even though my current
> data set is small (only 10 weeks of data per subject), that will
> change.
>
> Also, an examination of "superpose.symbol" reveals the
>
> following:
> > superpose.symbol <- trellis.par.get("superpose.symbol")
> > superpose.symbol
>
> $cex
> [1] 0.8 0.8 0.8 0.8 0.8 0.8 0.8
>
> $col
> [1] "#00ffff" "#ff00ff" "#00ff00" "#ff7f00" "#007eff" "#ffff00" "#ff0000"
>
> $font
> [1] 1 1 1 1 1 1 1
>
> $pch
> [1] "o" "o" "o" "o" "o" "o" "o"
>
>
> This seems to indicate that lattice graphics are restricted to
> no more than seven colors, though I cannot believe that this is true.
> Where might I search/read/learn about the available colors for
> lattice plots and how to denote them?  (I find "#ff7f00" to be
> a little cryptic, for example.)  Is there a table that relates
> these ?hexadecimal? numbers to descriptions like "navy blue"
> for Win2k?
>
> Finally, is it possible to cause superpose.symbol to default to
> more than 7 $cex/$col/$font/$pch values?  Since I am working with
> 12 subjects, plots that assign a different color to each wind up
> duplicating colors and symbols at some point.
>
>
> Much thanks in advance,
>   david paul
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help



From paulda at BATTELLE.ORG  Fri Aug 29 20:10:31 2003
From: paulda at BATTELLE.ORG (Paul, David  A)
Date: Fri, 29 Aug 2003 14:10:31 -0400
Subject: [R] Lattice plot questions
Message-ID: <940250A9EB37A24CBE28D858EF07774967AAB1@ws-bco-mse3.milky-way.battelle.org>

Thank you VERY much!  That does address my questions.

Respectfully,
  david paul

-----Original Message-----
From: Deepayan Sarkar [mailto:deepayan at stat.wisc.edu] 
Sent: Friday, August 29, 2003 1:54 PM
To: Paul, David A; 'r-help at stat.math.ethz.ch'
Subject: Re: [R] Lattice plot questions



Let's consider this simulated data:

foo <- data.frame(resp = do.call("c", lapply(as.list(rep(6, 12)),
                  function(x) sort(rnorm(x)))),
                  week = rep(2*0:5, 12),
                  id = factor(rep(1:12, each = 6)))

Does the following give you what you want ?

xyplot(resp ~ week | id, foo,

       panel = function(x, y, col = sup.sym$col, panel.number, ...) {

           ## this part accesses the data frame directly, and needs to
           ## be adjusted accordingly if the name of the variables
           ## therein change. This draws the part meant for the whole
           ## data set

           panel.superpose(x = foo$week, y = foo$resp,
                           subscripts = TRUE, groups = foo$id,
                           type = 'l', col = "black")

           ## this part draws the colored lines, each overwriting one
           ## of the background lines

           sup.sym <- trellis.par.get("superpose.symbol")
           col.this <- col[1 + ((panel.number - 1) %% length(col))]
           panel.xyplot(x, y, type = 'b', col = col.this, 
                        lwd = 2, cex = 1.2, ...)
           
       })

This will by default use the colors in superpose.symbol, but you can
override 
that by specifying your own color as an argument to xyplot, perhaps
something 
like 

xyplot(resp ~ week | id, foo, col = c("red", "yellow", "blue"),
       panel = ....

I personally don't see the point of using different colors though (that
would 
seem useful to me only if the different colors appeared within the same 
panel).

There's no restriction on the length of the elements of 
trellis.par.get("superpose.symbol"). 7 (which is just as good a number as
any 
other) is the default choice, inspired by Trellis in S. Set it to whatever 
you want with trellis.par.set() or lset() (or just supply the colors to 
xyplot(), which is usually more convenient for specific tasks like this).

Names like "navy blue" are just a convenience, and I doubt that all 256^3 
colors in the RGB space have been named, so trying to get a name for 
"#ff7f00" may not get you anywhere. R has a function col2rgb() that converts

colors to RGB, e.g.

> col2rgb("#ff7f00")
      [,1]
red    255
green  127
blue     0
> col2rgb("navyblue")
      [,1]
red      0
green    0
blue   128

If you are more comfortable with named colors, use them--- R accepts them 
everywhere. A list of names it recognizes is produced by 

> colors()

The help pages for colors and col2rgb have more information.

HTH,

Deepayan

On Friday 29 August 2003 11:14, Paul, David A wrote:
> Win2k, R1.7.1:
>
> I am currently working with some growth curve data from a 
> biotoxicology experiment.  Each of 12 subjects had their blood drawn 
> at 0, 2, 4, 6, 8, and 10 weeks.  For the purposes of the project, it 
> would be helpful if I were able to do the following:
>
> 	a.  Produce 12 panels, each displaying the *same* data, with
> 		the "strip" at the top of a particular panel showing
> 		exactly one of the subject id's
> 	b.  For a particular panel, plot the time course data for
> 		the subject whose id appears in the strip in some
> 		really obvious color (like yellow, red, or blue),
> 		with circles at each time point, and connecting the
> 		"dots".  In the same panel, those subjects whose id's
> 		did NOT appear in the strip would ideally have their
> 		data plotted in black, possibly with no symbols at
> 		any of the time points, but still connecting the
> 		"dots".
>
> I apologize if the description of what is needed is vague.
> It is possible to address (a) by duplicating the data set twelve times 
> and cbind()ing a factor variable so that each of the duplicated data 
> sets is associated with exactly one of the subject id's.  I have done 
> this, and the code
>
> subjects <- foo$subject.id
> superpose.symbol <- trellis.par.get("superpose.symbol")
> xyplot(log.response ~ week | factor.variable, data = foo,
> 	panel = panel.superpose,
> 	groups = subjects,
> 	key = list(space = "top", columns = 6, transparent = TRUE,
> 			text = list(levels(subjects)),
> 			points = Rows(superpose.symbol,1:7))
>        )
>
> works, with "factor.variable" corresponding to the cbind()ed factor 
> variable.  Unfortunately, I have been unable to figure out how to do 
> (b).  I would also like to determine a more memory efficient way of 
> doing (a) -- even though my current data set is small (only 10 weeks 
> of data per subject), that will change.
>
> Also, an examination of "superpose.symbol" reveals the
>
> following:
> > superpose.symbol <- trellis.par.get("superpose.symbol")
> > superpose.symbol
>
> $cex
> [1] 0.8 0.8 0.8 0.8 0.8 0.8 0.8
>
> $col
> [1] "#00ffff" "#ff00ff" "#00ff00" "#ff7f00" "#007eff" "#ffff00" 
> "#ff0000"
>
> $font
> [1] 1 1 1 1 1 1 1
>
> $pch
> [1] "o" "o" "o" "o" "o" "o" "o"
>
>
> This seems to indicate that lattice graphics are restricted to no more 
> than seven colors, though I cannot believe that this is true. Where 
> might I search/read/learn about the available colors for lattice plots 
> and how to denote them?  (I find "#ff7f00" to be a little cryptic, for 
> example.)  Is there a table that relates these ?hexadecimal? numbers 
> to descriptions like "navy blue" for Win2k?
>
> Finally, is it possible to cause superpose.symbol to default to more 
> than 7 $cex/$col/$font/$pch values?  Since I am working with 12 
> subjects, plots that assign a different color to each wind up 
> duplicating colors and symbols at some point.
>
>
> Much thanks in advance,
>   david paul
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list 
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help



From mn216 at columbia.edu  Fri Aug 29 21:37:13 2003
From: mn216 at columbia.edu (Murad Nayal)
Date: Fri, 29 Aug 2003 15:37:13 -0400
Subject: [R] lattice question
Message-ID: <3F4FAB69.D257CE34@columbia.edu>



Hello,

I am using lattice to plot histograms of one variable conditioned on
another continuous variable. for this I am using equal.count on the
conditioning variable to get the appropriate shingle. I would like to
have in my plot a representation of the shingle's intervals including
the min/max values and maybe tick marks. some sort of axis for the
conditioning variable. while the 'strips' of the lattice plot do
represent the single intervals as a darkly shaded region. I can't find a
way to also include in the plot the actual min/max numbers corresponding
to the shingle's intervals. is that possible?

regards

-- 
Murad Nayal M.D. Ph.D.
Department of Biochemistry and Molecular Biophysics
College of Physicians and Surgeons of Columbia University
630 West 168th Street. New York, NY 10032
Tel: 212-305-6884	Fax: 212-305-6926



From deepayan at stat.wisc.edu  Fri Aug 29 22:41:13 2003
From: deepayan at stat.wisc.edu (Deepayan Sarkar)
Date: Fri, 29 Aug 2003 15:41:13 -0500
Subject: [R] lattice question
In-Reply-To: <3F4FAB69.D257CE34@columbia.edu>
References: <3F4FAB69.D257CE34@columbia.edu>
Message-ID: <200308291541.13794.deepayan@stat.wisc.edu>


The typical graphical representation for shingles is via plot.shingle, e.g.

a <- equal.count(rnorm(100))
plot(a)

I'm not sure how you wish to represent this information inside the histogram 
plot itself, but everything you need should be available inside the strip 
function. For example, 

#################
x <- rnorm(100)
a <- equal.count(rnorm(100))

histogram( ~ x | a, par.strip.text = list(lines = 2),
          strip = function(..., shingle.intervals, 
                           which.given, which.panel, strip.names) {
              print(shingle.intervals[which.panel[which.given], ,drop = TRUE])

              strip.default(...,
                            shingle.intervals = shingle.intervals,
                            which.given = which.given,
                            which.panel = which.panel,
                            strip.names = FALSE)

              ltext(.5, .5, 
         paste(round(shingle.intervals[which.panel[which.given], ,
                     drop = TRUE],
                     digits = 4), collapse = ",   "))
          })
################

The width of the strip area is determined by the display, but you can control 
the height with par.strip.text. (Currently, whatever the strip function does 
is clipped to within the strip area, you can override that with a setting 
option in the lattice version that comes with r-devel)

Deepayan

On Friday 29 August 2003 14:37, Murad Nayal wrote:
> Hello,
>
> I am using lattice to plot histograms of one variable conditioned on
> another continuous variable. for this I am using equal.count on the
> conditioning variable to get the appropriate shingle. I would like to
> have in my plot a representation of the shingle's intervals including
> the min/max values and maybe tick marks. some sort of axis for the
> conditioning variable. while the 'strips' of the lattice plot do
> represent the single intervals as a darkly shaded region. I can't find a
> way to also include in the plot the actual min/max numbers corresponding
> to the shingle's intervals. is that possible?
>
> regards



From tzhou1 at students.uiuc.edu  Fri Aug 29 23:03:08 2003
From: tzhou1 at students.uiuc.edu (tzhou1)
Date: Fri, 29 Aug 2003 16:03:08 -0500
Subject: [R] case weight in mixed model
Message-ID: <3FA726CC@webmail.uiuc.edu>

Hi,

I have a question about how to do case weight in mixed model using R. Can R do 
this? If so, could you give me some references and examples? I'm looking 
forward to hearing from you.


Thanks a lot!

Tianyue



From payless4smokes at address.com  Fri Aug 29 23:53:17 2003
From: payless4smokes at address.com (payless4smokes@address.com)
Date: Fri, 29 Aug 2003 16:53:17 -0500
Subject: [R] Cigarettes $20.95 per carton S/H & Tax Included
Message-ID: <200308292153.h7TLrSBt021523@stat.math.ethz.ch>

NOW AVAILABLE TO THE PUBLIC!!!
http://www.payless4smokes.com
http://www.payless-4-smokes.com
and
http://www.yourcheapsmokes.com

We are happy to announce our new website is now available to the public.

featuring NAME-BRAND cigarettes for only $12.95 - $20.95 per carton.

Delivered right to your front door!!

NOTHING EVER OVER $20.95 TOTAL...Why! would you pay more???
free shipping, duty free, tax free, no membership fees and no hidden 
costs. Nothing to "join", No one to "sponser", No "club" fees, No 
affiliate, residual, multi-level CRAP!! 
Just $20.95 taxes and shipping included plain and simple.

over 50 top choices including (but certainly not limited to):  
marlboro  camel  winston  benson & hedges  dunhill  mild 7  rothmans  
salem  pall mall  parliament  silk cut  l&m   ms  monte carlo  
phillip morris  winchester  and many many more

If you have any questions be sure to ask we are more than happy to 
answer them.

visit us today:
http://www.payless4smokes.com

http://www.payless-4-smokes.com

http://www.yourcheapsmokes.com

Be sure to tell all your friends how they can enjoy saving money buying 
our cigarettes forward this email to them. 
http://www.payless4smokes.com 
http://www.payless-4-smokes.com
http://www.yourcheapsmokes.com



From spencer.graves at pdf.com  Sat Aug 30 01:23:17 2003
From: spencer.graves at pdf.com (Spencer Graves)
Date: Fri, 29 Aug 2003 16:23:17 -0700
Subject: [R] case weight in mixed model
References: <3FA726CC@webmail.uiuc.edu>
Message-ID: <3F4FE065.4050706@pdf.com>

Have you considered "lme" in library(nlme)?  The best reference for this 
is Pinhiero and Bates (2000) Mixed-Effects Models in S and S-Plus 
(Springer).

hope this helps.  spencer graves

tzhou1 wrote:
> Hi,
> 
> I have a question about how to do case weight in mixed model using R. Can R do 
> this? If so, could you give me some references and examples? I'm looking 
> forward to hearing from you.
> 
> 
> Thanks a lot!
> 
> Tianyue
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help



From kjetil at entelnet.bo  Sat Aug 30 01:45:46 2003
From: kjetil at entelnet.bo (kjetil brinchmann halvorsen)
Date: Fri, 29 Aug 2003 19:45:46 -0400
Subject: [R] ks.test()
In-Reply-To: <Pine.SOL.4.30.0308280757080.14547-100000@ysidro.econ.uiuc.edu>
References: <Pine.LNX.4.44.0308281319560.17945-100000@gannet.stats>
Message-ID: <3F4FAD6A.18953.1873A9A@localhost>

On 28 Aug 2003 at 8:06, Roger Koenker wrote:

But is it worth it to modify Kolmogorov-Smirnof fot estimated 
parameters? It has very low power anyhow. If the null hypothesis is 
"exponential distributio" (which is a scale family), what about using 
the quantile transformation twice

new <- qnorm(pexp(old))

to transform from exponential to normal distribution and the applying
shapiro.test
?

Kjetil Halvorsen

> 
> On Thu, 28 Aug 2003, Prof Brian Ripley wrote:
> 
> > You appear to be applying the KS test after estimating parameters.  The
> > distribution theory is for an iid sample from a known continuous
> > distribution (and does not otherwise depend on the distribution).  Since
> > your H_0 is not pre-specified, that distribution theory is not correct.
> > (Some corrections have been worked out for say ML fitting of exponential
> > and normal distributions -- by Michael Stephens as I recall.)
> 
> Just to amplify this comment a bit, I'm a little worried that the
> current documentation of of ks.test may make it appear that estimated
> parameters are ok, or that somehow the p-values computed are
> "corrected" in some way for their existence -- which I very much
> doubt.  The standard reference on this sort of thing was Durbin's (1973)
> SIAM monograph.  There is a very nice approach due to Khmaladze (1981)
> based on the Doob-Meyer decomposition - this is the closest thing
> that I'm aware of for handling KS type tests with estimated parameters
> in a general context.
> 
> url:	www.econ.uiuc.edu/~roger/my.html	Roger Koenker
> email	rkoenker at uiuc.edu			Department of Economics
> vox: 	217-333-4558				University of Illinois
> fax:   	217-244-6678				Champaign, IL 61820
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help



From jc at or.psychology.dal.ca  Sat Aug 30 02:33:44 2003
From: jc at or.psychology.dal.ca (John Christie)
Date: Fri, 29 Aug 2003 21:33:44 -0300
Subject: [R] is zscore() deprecated or Windows only?
Message-ID: <9CC99D22-DA81-11D7-81B8-000A9566473A@or.psychology.dal.ca>

Hi
	I was looking through the help on how to get standardized scores and 
came across an anomaly.  The help.search("zscore") reports there is 
such a function in R.base.  However, when I try to get help on zscore 
it reports no such function and says that R.base in built for win32.
	I'm running the Mac OS X version of R with the smorgasboard of 
packages that are included in that download.



From andy_liaw at merck.com  Sat Aug 30 02:54:26 2003
From: andy_liaw at merck.com (Liaw, Andy)
Date: Fri, 29 Aug 2003 20:54:26 -0400
Subject: [R] is zscore() deprecated or Windows only?
Message-ID: <3A822319EB35174CA3714066D590DCD50205CA8F@usrymx25.merck.com>

help.search("zscore") on my installation (R 1.7.1 on WinXP) didn't find any
zscore function anywhere, let alone base.  

I suspect you are looking for scale()...

Andy

> -----Original Message-----
> From: John Christie [mailto:jc at or.psychology.dal.ca] 
> Sent: Friday, August 29, 2003 8:34 PM
> To: r-help at stat.math.ethz.ch
> Subject: [R] is zscore() deprecated or Windows only?
> 
> 
> Hi
> 	I was looking through the help on how to get 
> standardized scores and 
> came across an anomaly.  The help.search("zscore") reports there is 
> such a function in R.base.  However, when I try to get help on zscore 
> it reports no such function and says that R.base in built for win32.
> 	I'm running the Mac OS X version of R with the smorgasboard of 
> packages that are included in that download.
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list 
> https://www.stat.math.ethz.ch/mailman/listinfo> /r-help
> 

------------------------------------------------------------------------------
Notice:  This e-mail message, together with any attachments,...{{dropped}}



From roger at ysidro.econ.uiuc.edu  Sat Aug 30 03:19:59 2003
From: roger at ysidro.econ.uiuc.edu (Roger Koenker)
Date: Fri, 29 Aug 2003 20:19:59 -0500 (CDT)
Subject: [R] ks.test()
In-Reply-To: <3F4FAD6A.18953.1873A9A@localhost>
Message-ID: <Pine.SOL.4.30.0308292017320.20289-100000@ysidro.econ.uiuc.edu>

On Fri, 29 Aug 2003, kjetil brinchmann halvorsen wrote:

> But is it worth it to modify Kolmogorov-Smirnof fot estimated
> parameters? It has very low power anyhow. If the null hypothesis is
> "exponential distributio" (which is a scale family), what about using
> the quantile transformation twice
>
> new <- qnorm(pexp(old))

And if old is exponential, but not standard exponential?

url:	www.econ.uiuc.edu/~roger/my.html	Roger Koenker
email	rkoenker at uiuc.edu			Department of Economics
vox: 	217-333-4558				University of Illinois
fax:   	217-244-6678				Champaign, IL 61820



From kjetil at entelnet.bo  Sat Aug 30 03:39:11 2003
From: kjetil at entelnet.bo (kjetil brinchmann halvorsen)
Date: Fri, 29 Aug 2003 21:39:11 -0400
Subject: [R] ks.test()
In-Reply-To: <Pine.SOL.4.30.0308292017320.20289-100000@ysidro.econ.uiuc.edu>
References: <3F4FAD6A.18953.1873A9A@localhost>
Message-ID: <3F4FC7FF.27261.1EF111F@localhost>

On 29 Aug 2003 at 20:19, Roger Koenker wrote:

> On Fri, 29 Aug 2003, kjetil brinchmann halvorsen wrote:
> 
> > But is it worth it to modify Kolmogorov-Smirnof fot estimated
> > parameters? It has very low power anyhow. If the null hypothesis is
> > "exponential distributio" (which is a scale family), what about using
> > the quantile transformation twice
> >
> > new <- qnorm(pexp(old))
> 
> And if old is exponential, but not standard exponential?


But the shapiro-wilk test is scale invariant, and the exponential 
family is a scale family, so we can standardize old first without 
destroying the validity of the shapiro-wilk test?

Kjetil Halvorsen


> 
> url:	www.econ.uiuc.edu/~roger/my.html	Roger Koenker
> email	rkoenker at uiuc.edu			Department of Economics
> vox: 	217-333-4558				University of Illinois
> fax:   	217-244-6678				Champaign, IL 61820
> 
>



From tlumley at u.washington.edu  Sat Aug 30 03:48:10 2003
From: tlumley at u.washington.edu (Thomas Lumley)
Date: Fri, 29 Aug 2003 18:48:10 -0700 (PDT)
Subject: [R] is zscore() deprecated or Windows only?
In-Reply-To: <3A822319EB35174CA3714066D590DCD50205CA8F@usrymx25.merck.com>
Message-ID: <Pine.A41.4.44.0308291847030.39786-100000@homer18.u.washington.edu>

On Fri, 29 Aug 2003, Liaw, Andy wrote:

> help.search("zscore") on my installation (R 1.7.1 on WinXP) didn't find any
> zscore function anywhere, let alone base.

This is an unfortunate package name combined with not reading the output
carefully enough.

Package: R.base
Version: 0.33
Date: 2002/10/29
Title: [R] Class Library - Stand-alone basic functions
Author: Henrik Bengtsson <henrikb at braju.com>

So it's in package R.base, which *is* what help.search() says.

	-thomas



From jc at or.psychology.dal.ca  Sat Aug 30 08:49:25 2003
From: jc at or.psychology.dal.ca (John Christie)
Date: Sat, 30 Aug 2003 03:49:25 -0300
Subject: [R] is zscore() deprecated or Windows only?
In-Reply-To: <Pine.A41.4.44.0308291847030.39786-100000@homer18.u.washington.edu>
Message-ID: <17EF79EA-DAB6-11D7-81B8-000A9566473A@or.psychology.dal.ca>


On Friday, August 29, 2003, at 10:48  PM, Thomas Lumley wrote:

> On Fri, 29 Aug 2003, Liaw, Andy wrote:
> This is an unfortunate package name combined with not reading the 
> output
> carefully enough.
>
> Package: R.base
> Version: 0.33
> Date: 2002/10/29
> Title: [R] Class Library - Stand-alone basic functions
> Author: Henrik Bengtsson <henrikb at braju.com>
>
> So it's in package R.base, which *is* what help.search() says.

Well, in 1.7.0 full install on Mac OS X (someone made an image with 
most of the packages and that is what I installed) help.search 
("zscore") just returns R.base without any other info (of course, it 
was the search).  help("zscore") return not found and help("zscore", 
package=R.base) return "R.base is built for Win32".  Which is very 
wierd.  I tried quitting R (it had been on a week or so) in case some 
package I loaded removed it or something.  However, that didn't recover 
it.  Then, I quit R again and lo and behold I get a full help for 
zscore telling me "Gets the Z scores (standardized residuals)..."  BUT, 
when I call the function it still says "Error: couldn't find function 
"zscore".  This is very strange.  I'll try scale and see if it does 
what I wanted.  zscore sounded perfect though.  If I could clean up 
this problem I would, but I don't know if it is a function that was 
pulled, a function missing, or what the actual state is supposed to be.



From ripley at stats.ox.ac.uk  Sat Aug 30 09:19:20 2003
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Sat, 30 Aug 2003 08:19:20 +0100 (BST)
Subject: [R] is zscore() deprecated or Windows only?
In-Reply-To: <17EF79EA-DAB6-11D7-81B8-000A9566473A@or.psychology.dal.ca>
Message-ID: <Pine.LNX.4.44.0308300811220.17734-100000@gannet.stats>

Someone has unpacked a Windows binary in one of your library trees, it
seems. The package should refuse to load on MacOS X.

I suggest you search for it and remove it.  If it was in the `full 
install' you installed, complain to the person who made the `install'.
Remember there are two R parts for MacOS X, and I suspect you are using 
the Darwin port (you didn't say) and the mega-bundle made by Jan de Leeuw 
(you didn't say).  If so, there was no such bundle for the current R 
version, 1.7.1, last time I looked.  I suggest you upgrade to 1.7.1, even 
though 1.8.0 is only about 6 weeks' away.

On Sat, 30 Aug 2003, John Christie wrote:

> 
> On Friday, August 29, 2003, at 10:48  PM, Thomas Lumley wrote:
> 
> > On Fri, 29 Aug 2003, Liaw, Andy wrote:
> > This is an unfortunate package name combined with not reading the 
> > output
> > carefully enough.
> >
> > Package: R.base
> > Version: 0.33
> > Date: 2002/10/29
> > Title: [R] Class Library - Stand-alone basic functions
> > Author: Henrik Bengtsson <henrikb at braju.com>
> >
> > So it's in package R.base, which *is* what help.search() says.
> 
> Well, in 1.7.0 full install on Mac OS X (someone made an image with 
> most of the packages and that is what I installed) help.search 
> ("zscore") just returns R.base without any other info (of course, it 
> was the search).  help("zscore") return not found and help("zscore", 
> package=R.base) return "R.base is built for Win32".  Which is very 
> wierd.  

No. Unless you did library(R.base), that is what you should expect.

I tried quitting R (it had been on a week or so) in case some 
> package I loaded removed it or something.  However, that didn't recover 
> it.  Then, I quit R again and lo and behold I get a full help for 
> zscore telling me "Gets the Z scores (standardized residuals)..."  BUT, 
> when I call the function it still says "Error: couldn't find function 
> "zscore".  This is very strange.  

Did you load the library?

> I'll try scale and see if it does 
> what I wanted.  zscore sounded perfect though.  If I could clean up 
> this problem I would, but I don't know if it is a function that was 
> pulled, a function missing, or what the actual state is supposed to be.

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From bertola at fastmail.fm  Sat Aug 30 15:07:27 2003
From: bertola at fastmail.fm (Rafael Bertola)
Date: Sat, 30 Aug 2003 10:07:27 -0300
Subject: [R] 3D plot of a bivariate normal distribution
Message-ID: <20030830130727.0AB413A449@www.fastmail.fm>

Hi,

I've used the Mathematica to produce 3D graphics, contour plots of a
bivariate normal distribution

Now I want make these graphics in R, but i do not know how.
I would like to:
- Plot a 3D graph for some different variance matrix
- Plot the contour plots
- Find and try to plot (in the 3d graph ou contour plot) the (1-a)%
confidence region based in a chi-square(a) with the degrees of freedom
equal a 2 or bigger.

Below is the Mathematica Notebook that i've used until now


<< "Graphics`PlotField`"

NB[x_,y_]:=(1/((2 Pi)*Sqrt[a*b*(1-c^2)]))*Exp[(-1/(2*(1-c^2)))*( 
          ((x-u)/Sqrt[a])^2 + ((y-v)/Sqrt[b])^2 
			- 2*c(((x-u)/Sqrt[a])((y-v)/Sqrt[b]))
								)]

{{a,c}, {c,b}} = {{1,0}, {0,1}};  The covariance Matrix
{u,v} = {0,0};                    Mean vector
Plot3D[NB[x,y],{x,-1.5,1.5},{y,-1.5,1.5},
		AxesLabel->{x,y,z},
		BoxRatios->{1,1,1}];
ContourPlot[NB[x,y],{x,-1,1},{y,-1,1},
		Axes->True, 
		AxesLabel->{x,y}];

3d graph rotation
Do[
	Plot3D[NB[x,y],{x,-1.5,1.5},{y,-1.5,1.5},
		PlotPoints->20,
		Mesh ->False,
		SphericalRegion ->True,
		Axes ->None,
		Boxed ->False,
		ViewPoint->{2 Cos[t], 2 Sin[t], 1.3},
		BoxRatios->{1,1,1}
	],{t, 0, 2Pi-2Pi/36, 2Pi/36}]


Thanks, 
Rafael
-- 
  
  bertola at fastmail.fm

-- 

                          love email again



From ligges at statistik.uni-dortmund.de  Fri Aug 29 20:15:09 2003
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Fri, 29 Aug 2003 20:15:09 +0200
Subject: [R] Notes on Windows binaries of contributed  packages
Message-ID: <3F4F982D.3030300@statistik.uni-dortmund.de>

Announcement for Windows users of R < 1.7.0
  and maintainers of contributed packages


[Please do not reply to r-announce!]


Windows users of R < 1.7.0
==========================
[Users of R-1.7.x are not affected.]

In order to clean up the directory structure on CRAN, the Windows binary
version of contributed packages for R < 1.7.0 will be moved from
   Your-CRAN-Mirror/bin/windows/contrib
to a subdirectory ./1.6 shortly.
Therefore, install.packages() and friends of R < 1.7.0 won't work
without specifying argument "contriburl" explicitly. Of course, manual
download and installation is possible as well. Note that these packages
are no longer being updated (last update was: 11-Apr-2003).
ReadMe files will be available in the corresponding directories.

It is highly recommended to upgrade to a recent version of R!


Maintainers of contributed packages
===================================
Starting with the first alpha releases of R-1.8.0 (in development), I am
going to make Windows binary versions of contributed packages available
(around September 13) at Your-CRAN-Mirror/bin/windows/contrib/1.8
(currently still linked to the ./1.7 directory).
Packages that do not compile out of the box or do not pass "Rcmd check"
with "OK" or "WARNING" will *not* be published. This "Status", i.e.
result of "Rcmd check" ("OK", "WARNING", "ERROR"), will be listed in a
file "./1.8/Status". Corresponding check.log will be available in
subdirectory ./1.8/check.
For details, please read the ReadMe (to appear).


Best regards,
Uwe Ligges

_______________________________________________
R-announce at stat.math.ethz.ch mailing list
https://www.stat.math.ethz.ch/mailman/listinfo/r-announce



From ypeng at math.mun.ca  Sat Aug 30 17:26:20 2003
From: ypeng at math.mun.ca (Paul Y. Peng)
Date: Sat, 30 Aug 2003 12:56:20 -0230
Subject: [R] Shared library loading in Win R 1071 and previous Win R versions
Message-ID: <3F50C21C.CA7D6424@math.mun.ca>

I recently upgraded R from rw1051 to rw1071 on a Win98se system and
surprisingly found my DLL library does not work any more. When I
tried to load the dll file by dyn.load("my.dll"), I got the following
message:

  Error in dyn.load(x, as.logical(local), as.logical(now)) : 
          unable to load shared library "C:/temp/my.dll":
    LoadLibrary failure:  A device attached to the system is not
functioning.

I cannot figure out what is meant by this message. The DLL library has
been working up to version rw1062. I checked CHANGES in rw1071 directory
did not find anything related to loading a shared library. Can anyone
enlighten me on this?

Since rw1071 does not work, I am thinking to install rw1062 instead.
However, the "old" link in

   http://lib.stat.cmu.edu/R/CRAN/bin/windows/base/

does not work. Is this right place to get an old version of R?

Thanks.
Paul.



From spencer.graves at pdf.com  Sat Aug 30 17:45:50 2003
From: spencer.graves at pdf.com (Spencer Graves)
Date: Sat, 30 Aug 2003 08:45:50 -0700
Subject: [R] 3D plot of a bivariate normal distribution
References: <20030830130727.0AB413A449@www.fastmail.fm>
Message-ID: <3F50C6AE.2020709@pdf.com>

Have you considered "contour", "persp", and "image", in package(base) 
and "contourplot", "levelplot" in package(lattice)?  See the 
documentation and Venables and Ripley (2002) Modern Applied Statistics 
with S, 4th ed. Springer).

hope this helps.  spencer graves

Rafael Bertola wrote:
> Hi,
> 
> I've used the Mathematica to produce 3D graphics, contour plots of a
> bivariate normal distribution
> 
> Now I want make these graphics in R, but i do not know how.
> I would like to:
> - Plot a 3D graph for some different variance matrix
> - Plot the contour plots
> - Find and try to plot (in the 3d graph ou contour plot) the (1-a)%
> confidence region based in a chi-square(a) with the degrees of freedom
> equal a 2 or bigger.
> 
> Below is the Mathematica Notebook that i've used until now
> 
> 
> << "Graphics`PlotField`"
> 
> NB[x_,y_]:=(1/((2 Pi)*Sqrt[a*b*(1-c^2)]))*Exp[(-1/(2*(1-c^2)))*( 
>           ((x-u)/Sqrt[a])^2 + ((y-v)/Sqrt[b])^2 
> 			- 2*c(((x-u)/Sqrt[a])((y-v)/Sqrt[b]))
> 								)]
> 
> {{a,c}, {c,b}} = {{1,0}, {0,1}};  The covariance Matrix
> {u,v} = {0,0};                    Mean vector
> Plot3D[NB[x,y],{x,-1.5,1.5},{y,-1.5,1.5},
> 		AxesLabel->{x,y,z},
> 		BoxRatios->{1,1,1}];
> ContourPlot[NB[x,y],{x,-1,1},{y,-1,1},
> 		Axes->True, 
> 		AxesLabel->{x,y}];
> 
> 3d graph rotation
> Do[
> 	Plot3D[NB[x,y],{x,-1.5,1.5},{y,-1.5,1.5},
> 		PlotPoints->20,
> 		Mesh ->False,
> 		SphericalRegion ->True,
> 		Axes ->None,
> 		Boxed ->False,
> 		ViewPoint->{2 Cos[t], 2 Sin[t], 1.3},
> 		BoxRatios->{1,1,1}
> 	],{t, 0, 2Pi-2Pi/36, 2Pi/36}]
> 
> 
> Thanks, 
> Rafael



From ripley at stats.ox.ac.uk  Sat Aug 30 18:09:04 2003
From: ripley at stats.ox.ac.uk (Prof Brian D Ripley)
Date: Sat, 30 Aug 2003 17:09:04 +0100 (GMT Daylight Time)
Subject: [R] Shared library loading in Win R 1071 and previous Win R
	versions
In-Reply-To: <3F50C21C.CA7D6424@math.mun.ca>
Message-ID: <Pine.WNT.4.44.0308301659330.2808-100000@petrel>

You probably need to recompile the DLL: have you tried that?

On Sat, 30 Aug 2003, Paul Y. Peng wrote:

> I recently upgraded R from rw1051 to rw1071 on a Win98se system and
> surprisingly found my DLL library does not work any more. When I
> tried to load the dll file by dyn.load("my.dll"), I got the following
> message:
>
>   Error in dyn.load(x, as.logical(local), as.logical(now)) :
>           unable to load shared library "C:/temp/my.dll":
>     LoadLibrary failure:  A device attached to the system is not
> functioning.
>
> I cannot figure out what is meant by this message.

I think it is from an old version of Windows?  (Windows XP and 2000 are
usually a lot more informative.)  It probably means that R.dll does not
contain what your DLL expected, which is not surprising if you did not
recompile it.

> The DLL library has
> been working up to version rw1062. I checked CHANGES in rw1071 directory
> did not find anything related to loading a shared library. Can anyone
> enlighten me on this?

The NEWS file might enlighten you, BTW: CHANGES is only about
Windows-specific changes.  No one has ever said that a DLL compiled for one
version of R will work for any other, although it is likely to work between
patch levels (1.7.0 to 1.7.1, for example).

> Since rw1071 does not work,

It is *your* code that does not work.

> I am thinking to install rw1062 instead.
> However, the "old" link in
>
>    http://lib.stat.cmu.edu/R/CRAN/bin/windows/base/
>
> does not work. Is this right place to get an old version of R?

It does on the master CRAN site, but gets you to rw1070.  It also seems to
work on that mirror.  The old link in the old area does not work, nor I
suspect is it intended to.

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From ypeng at math.mun.ca  Sat Aug 30 22:09:26 2003
From: ypeng at math.mun.ca (Paul Y. Peng)
Date: Sat, 30 Aug 2003 17:39:26 -0230
Subject: [R] Shared library loading in Win R 1071 and previous Win
	Rversions
References: <Pine.WNT.4.44.0308301659330.2808-100000@petrel>
Message-ID: <3F510476.35381968@math.mun.ca>

Prof Brian D Ripley wrote:
> 
> You probably need to recompile the DLL: have you tried that?
> 
> On Sat, 30 Aug 2003, Paul Y. Peng wrote:
> 
> > I recently upgraded R from rw1051 to rw1071 on a Win98se system and
> > surprisingly found my DLL library does not work any more. When I
> > tried to load the dll file by dyn.load("my.dll"), I got the following
> > message:
> >
> >   Error in dyn.load(x, as.logical(local), as.logical(now)) :
> >           unable to load shared library "C:/temp/my.dll":
> >     LoadLibrary failure:  A device attached to the system is not
> > functioning.
> >
> > I cannot figure out what is meant by this message.
> 
> I think it is from an old version of Windows?  (Windows XP and 2000 are

Win98se.

> usually a lot more informative.)  It probably means that R.dll does not
> contain what your DLL expected, which is not surprising if you did not
> recompile it.

I did not recompile it because the dll file compiled for rw1062
worked flawlessly in rw1051. I thought it would work as well in
rw1071. I will recompile it and try again.

> > The DLL library has
> > been working up to version rw1062. I checked CHANGES in rw1071 directory
> > did not find anything related to loading a shared library. Can anyone
> > enlighten me on this?
> 
> The NEWS file might enlighten you, BTW: CHANGES is only about
> Windows-specific changes.  No one has ever said that a DLL compiled for one
> version of R will work for any other, although it is likely to work between
> patch levels (1.7.0 to 1.7.1, for example).
> 
> > Since rw1071 does not work,
> 
> It is *your* code that does not work.

Points taken.

> > I am thinking to install rw1062 instead.
> > However, the "old" link in
> >
> >    http://lib.stat.cmu.edu/R/CRAN/bin/windows/base/
> >
> > does not work. Is this right place to get an old version of R?
> 
> It does on the master CRAN site, but gets you to rw1070.  It also seems to
> work on that mirror.  The old link in the old area does not work, nor I
> suspect is it intended to.

OK. Then are there any way to retrieve other previous Windows
versions prior to rw1070? Do I miss anything obvious here?

Many thanks for your help.
Paul.



From apiszcz at solarrain.com  Sat Aug 30 22:13:48 2003
From: apiszcz at solarrain.com (Al Piszcz)
Date: Sat, 30 Aug 2003 16:13:48 -0400 (EDT)
Subject: [R] plot margin spacing/control
Message-ID: <Pine.LNX.4.55.0308301609150.3899@l1>


With certain axis labels my formatted text is being clipped
or chopped since there is not enough space along the
left side margin. Is there a way to increase the
margin width?

Thank you in advance.



From apiszcz at solarrain.com  Sat Aug 30 22:18:29 2003
From: apiszcz at solarrain.com (Al Piszcz)
Date: Sat, 30 Aug 2003 16:18:29 -0400 (EDT)
Subject: [R] IGNORE PREVIOUS question: plot margin spacing/control (fwd)
Message-ID: <Pine.LNX.4.55.0308301617500.3962@l1>



par adds margin spacing with the 'mar' parameter
  par(lty=1,lwd=2,mar=c(5,6,4,2))


---------- Forwarded message ----------
Date: Sat, 30 Aug 2003 16:13:48 -0400 (EDT)
From: Al Piszcz <apiszcz at solarrain.com>
To: r-help at stat.math.ethz.ch
Subject: plot margin spacing/control


With certain axis labels my formatted text is being clipped
or chopped since there is not enough space along the
left side margin. Is there a way to increase the
margin width?

Thank you in advance.



From spencer.graves at pdf.com  Sat Aug 30 22:37:44 2003
From: spencer.graves at pdf.com (Spencer Graves)
Date: Sat, 30 Aug 2003 13:37:44 -0700
Subject: [R] plot margin spacing/control
References: <Pine.LNX.4.55.0308301609150.3899@l1>
Message-ID: <3F510B18.3020700@pdf.com>

If you are using "plot", have you consdered par(mar=...)?

hope this helps.  spencer graves

Al Piszcz wrote:
> With certain axis labels my formatted text is being clipped
> or chopped since there is not enough space along the
> left side margin. Is there a way to increase the
> margin width?
> 
> Thank you in advance.
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help



From ligges at statistik.uni-dortmund.de  Sun Aug 31 15:01:39 2003
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Sun, 31 Aug 2003 15:01:39 +0200
Subject: [R] Shared library loading in Win R 1071 and previous
	Win	Rversions
In-Reply-To: <3F510476.35381968@math.mun.ca>
References: <Pine.WNT.4.44.0308301659330.2808-100000@petrel>
	<3F510476.35381968@math.mun.ca>
Message-ID: <3F51F1B3.4000706@statistik.uni-dortmund.de>

Paul Y. Peng wrote:

[...]

> OK. Then are there any way to retrieve other previous Windows
> versions prior to rw1070? Do I miss anything obvious here?
> 
> Many thanks for your help.
> Paul.

Not on CRAN, AFAIK. You can compile from sources (which are available on 
CRAN) and build an outdated version yourself.

Uwe Ligges



From feh3k at spamcop.net  Sun Aug 31 18:52:29 2003
From: feh3k at spamcop.net (feh3k@spamcop.net)
Date: Sun, 31 Aug 2003 12:52:29 -0400
Subject: [R] Problem installing acepack in debian
Message-ID: <1062348749.5870cb579a599@webmail.spamcop.net>

In 
 
platform i386-pc-linux-gnu 
arch     i386              
os       linux-gnu         
system   i386, linux-gnu   
status                     
major    1                 
minor    7.1               
year     2003              
month    06                
day      16                
language R                 
 
using debian testing (latest Knoppix) I get an error when installing acepack: 
 
gcc -shared  -o acepack.so ace.o avas.o rlsmo.o  
-L/usr/lib/gcc-lib/i386-linux/3.3 -L/usr/lib/gcc-lib/i386-linux/3.3/../../.. 
-lfrtbegin -lg2c-pic -lm -lgcc_s -L/usr/lib/R/bin -lR 
/usr/bin/ld: cannot find -lfrtbegin 
collect2: ld returned 1 exit status 
make: *** [acepack.so] Error 1 
ERROR: compilation failed for package 'acepack' 
 
Thanks for any assistance anyone can give. 
 
--- 
Frank E Harrell Jr    Professor and Chair            School of Medicine 
                      Department of Biostatistics    Vanderbilt University



From rossini at blindglobe.net  Sun Aug 31 20:17:36 2003
From: rossini at blindglobe.net (A.J. Rossini)
Date: Sun, 31 Aug 2003 11:17:36 -0700
Subject: [R] Problem installing acepack in debian
In-Reply-To: <1062348749.5870cb579a599@webmail.spamcop.net>
	(feh3k@spamcop.net's message of "Sun, 31 Aug 2003 12:52:29 -0400")
References: <1062348749.5870cb579a599@webmail.spamcop.net>
Message-ID: <858yp9n133.fsf@blindglobe.net>

feh3k at spamcop.net writes:

> using debian testing (latest Knoppix) I get an error when installing acepack: 
>  
> gcc -shared  -o acepack.so ace.o avas.o rlsmo.o  
> -L/usr/lib/gcc-lib/i386-linux/3.3 -L/usr/lib/gcc-lib/i386-linux/3.3/../../.. 
> -lfrtbegin -lg2c-pic -lm -lgcc_s -L/usr/lib/R/bin -lR 
> /usr/bin/ld: cannot find -lfrtbegin 
> collect2: ld returned 1 exit status 
> make: *** [acepack.so] Error 1 
> ERROR: compilation failed for package 'acepack' 
>  
> Thanks for any assistance anyone can give. 

Frank -- 


I'm a bit confused as to what the problem is -- on a system initially
Knoppix (Quantian 0.3, actually), upgraded to Debian unstable, I get:

==============================================================

> install.packages("acepack")
trying URL `http://cran.r-project.org/src/contrib/PACKAGES'
Content type `text/plain; charset=iso-8859-1' length 124592 bytes
opened URL
.......... .......... .......... .......... ..........
.......... .......... .......... .......... ..........
.......... .......... .
downloaded 121Kb

trying URL `http://cran.r-project.org/src/contrib/acepack_1.3-2.tar.gz'
Content type `application/x-tar' length 18616 bytes
opened URL
.......... ........
downloaded 18Kb

* Installing *source* package 'acepack' ...
** libs
g77 -mieee-fp  -fPIC  -g -O2 -c ace.f -o ace.o
ace.f: In subroutine `mace':
ace.f:89: warning:
         common /prams/ itape,maxit,nterm,span,alpha,big
                 ^
Initial padding for common block `prams' is 4 bytes at (^) -- consider reordering members, largest-type-size first
g77 -mieee-fp  -fPIC  -g -O2 -c avas.f -o avas.o
g77 -mieee-fp  -fPIC  -g -O2 -c rlsmo.f -o rlsmo.o
gcc -shared  -o acepack.so ace.o avas.o rlsmo.o  -L/usr/lib/gcc-lib/i486-linux/3.3.2 -L/usr/lib/gcc-lib/i486-linux/3.3.2/../../.. -lfrtbegin -lg2c-pic -lm -lgcc_s -L/usr/lib/R/bin -lR
** R
** help
 >>> Building/Updating help pages for package 'acepack'
     Formats: text html latex example
  ace                               text    html    latex   example
  avas                              text    html    latex   example
* DONE (acepack)

Delete downloaded files (y/N)?

=============================================================

using:

=============================================================

502$ gcc -v
Reading specs from /usr/lib/gcc-lib/i486-linux/3.3.2/specs
Configured with: ../src/configure -v --enable-languages=c,c++,java,f77,pascal,objc,ada,treelang --prefix=/usr --mandir=/usr/share/man --infodir=/usr/share/info --with-gxx-include-dir=/usr/include/c++/3.3 --enable-shared --with-system-zlib --enable-nls --without-included-gettext --enable-__cxa_atexit --enable-clocale=gnu --enable-debug --enable-java-gc=boehm --enable-java-awt=xlib --enable-objc-gc i486-linux
Thread model: posix
gcc version 3.3.2 20030812 (Debian prerelease)

=============================================================

though this is using Dirk's debian build of R-devel:

=============================================================

504$ R --version
R 1.8.0 Under development (unstable) (2003-08-30).
Copyright (C) 2003 R Development Core Team

R is free software and comes with ABSOLUTELY NO WARRANTY.
You are welcome to redistribute it under the terms of the GNU
General Public License.  For more information about these matters,
see http://www.gnu.org/copyleft/gpl.html.

=============================================================

best,
-tony

-- 
A.J. Rossini     			
rossini at u.washington.edu            http://www.analytics.washington.edu/ 
Biomedical and Health Informatics   University of Washington
Biostatistics, SCHARP/HVTN          Fred Hutchinson Cancer Research Center
UW   :              FAX=206-543-3461 | moving soon to a permanent office
FHCRC: 206-667-7025 FAX=206-667-4812 | Voicemail is pretty sketchy/use Email

CONFIDENTIALITY NOTICE: This e-mail message and any attachme...{{dropped}}



From bbals at t-online.de  Sun Aug 31 20:43:31 2003
From: bbals at t-online.de (Bernhard Bals)
Date: Sun, 31 Aug 2003 20:43:31 +0200
Subject: [R] Problem and Question regarding R
Message-ID: <3F5241D3.3040101@t-online.de>

Hi,

I'm just about to install the Precompiled Binary Distribution of R from 
CRAN for SuSE Linux 8.2: R-base-1.7.1-1.i386.rpm

My system is SuSE Linux 8.2 Professional Edition on a Pentium 4:

# uname -a
Linux linux 2.4.20-4GB #1 Mon Mar 17 17:54:44 UTC 2003 i686 unknown 
unknown GNU/Linux


1) Problem:

I've tried to install the above base rpm archive with the command:

# rpm -i R-base-1.7.1-1.i386.rpm
error: failed dependencies:
         libg2c.so.0 is needed by R-base-1.7.1-1

There seems to be only one dependency missing (hope that's all). Since 
I've installed the Professional Edition with all the compilers and 
libraries, it's maybe only a naming problem.

What's behind this missing library?
Can this problem be fixed with a symbolic link pointing to a library 
that I already have on my system?
Any experience with that?

Do you think I need or should compile and install R myself (not using 
one of the binary distributions)?


2) Question:

I want to use R not only interactively (issue commands after the prompt 
 > ..), but also use some functionality of R from a Perl script.

Can I use methods of R from a Perl script: Is there an interface from 
Perl (or at least from C/C++) to R (f.e. invoke R functions from Perl, 
C, C++ or some other language)?
If so, can you please point me to the documentation on the Web or in 
literature where this is decribed?
Do you have some demo Perl script (or C/C++ program) that uses this 
technique, f.e. calling R methods like "mean" or "standard deviation"?


Thanks's for your help!


Regards
Bernhard
_________________

Dr. Bernhard Bals
Dollmannstr. 6
D-81541 M?nchen
+49-89-62509585
bbals at t-online.de



From p.dalgaard at biostat.ku.dk  Sun Aug 31 21:04:53 2003
From: p.dalgaard at biostat.ku.dk (Peter Dalgaard BSA)
Date: Sun, 31 Aug 2003 19:04:53 -0000
Subject: [R] Problem and Question regarding R
In-Reply-To: <3F5241D3.3040101@t-online.de>
References: <3F5241D3.3040101@t-online.de>
Message-ID: <x27k4t3aw4.fsf@biostat.ku.dk>

bbals at t-online.de (Bernhard Bals) writes:

> Hi,
> 
> I'm just about to install the Precompiled Binary Distribution of R
> from CRAN for SuSE Linux 8.2: R-base-1.7.1-1.i386.rpm
> 
> My system is SuSE Linux 8.2 Professional Edition on a Pentium 4:
> 
> # uname -a
> Linux linux 2.4.20-4GB #1 Mon Mar 17 17:54:44 UTC 2003 i686 unknown
> unknown GNU/Linux
> 
> 
> 1) Problem:
> 
> I've tried to install the above base rpm archive with the command:
> 
> # rpm -i R-base-1.7.1-1.i386.rpm
> error: failed dependencies:
>          libg2c.so.0 is needed by R-base-1.7.1-1
> 
> There seems to be only one dependency missing (hope that's all). Since
> I've installed the Professional Edition with all the compilers and
> libraries, it's maybe only a naming problem.
> 
> What's behind this missing library?
> Can this problem be fixed with a symbolic link pointing to a library
> that I already have on my system?
> Any experience with that?

As far as I remember, this happens if you haven't installed the
Fortran compiler (g77) since libg2c is part of that.
 
> Do you think I need or should compile and install R myself (not using
> one of the binary distributions)?

Wouldn't help (except in the sense that it would become more obvious
that a Fortran compiler is needed!)

> 
> 2) Question:
> 
> I want to use R not only interactively (issue commands after the
> prompt > ..), but also use some functionality of R from a Perl script.
> 
> Can I use methods of R from a Perl script: Is there an interface from
> Perl (or at least from C/C++) to R (f.e. invoke R functions from Perl,
> C, C++ or some other language)?
> If so, can you please point me to the documentation on the Web or in
> literature where this is decribed?
> Do you have some demo Perl script (or C/C++ program) that uses this
> technique, f.e. calling R methods like "mean" or "standard deviation"?

http://www.omegahat.org/RSPerl . Be warned that these Omegahat packages
are sometimes not quite as stable as the regular CRAN family of
packages so some patchups can be required. Or maybe it will all just
work...  

-- 
   O__  ---- Peter Dalgaard             Blegdamsvej 3  
  c/ /'_ --- Dept. of Biostatistics     2200 Cph. N   
 (*) \(*) -- University of Copenhagen   Denmark      Ph: (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)             FAX: (+45) 35327907



From baron at psych.upenn.edu  Sun Aug 31 21:09:37 2003
From: baron at psych.upenn.edu (Jonathan Baron)
Date: Sun, 31 Aug 2003 15:09:37 -0400
Subject: [R] Problem and Question regarding R
In-Reply-To: <3F5241D3.3040101@t-online.de>
References: <3F5241D3.3040101@t-online.de>
Message-ID: <20030831190937.GA26345@mail2.sas.upenn.edu>

It is a slow day, and those who really know the answers may not
be around.  So here goes.

On 08/31/03 20:43, Bernhard Bals wrote:
>Hi,
># rpm -i R-base-1.7.1-1.i386.rpm
>error: failed dependencies:
>         libg2c.so.0 is needed by R-base-1.7.1-1
>
>There seems to be only one dependency missing (hope that's all). Since 
>I've installed the Professional Edition with all the compilers and 
>libraries, it's maybe only a naming problem.
>
>What's behind this missing library?

On my Red Hat System (noodle), I get:

baron at noodle /usr/lib > "ls" -l libg2c*
lrwxrwxrwx    1 root     root           15 Apr  8 13:57
libg2c.so.0 -> libg2c.so.0.0.0
-rwxr-xr-x    1 root     root       109976 Feb 25  2003
libg2c.so.0.0.0

In other words, it is linked to libg2c.so.0.0.0.  Also, where
does it come from?

baron at noodle /usr/lib > rpm -q --whatprovides libg2c.so.0
libf2c-3.2.2-5

baron at noodle /usr/lib > rpm -ql libf2c
/usr/lib/libg2c.so.0
/usr/lib/libg2c.so.0.0.0

In sum, it seems you need the rpm called libf2c.

>Do you think I need or should compile and install R myself (not using 
>one of the binary distributions)?

I have done this, and I do recommend it if you have a reasonably
fast computer.  But in this particular case, based on
guesstimation from the name of the missing rpm, I bit you would
need it anyway.

Another way to find missing rpms is by typing in the name of the
relevant file in the search section of
http://rpmfind.net/linux/RPM/

>2) Question:
>
>I want to use R not only interactively (issue commands after the prompt 
> > ..), but also use some functionality of R from a Perl script.

It is better to ask one question per email.  If this were the
only one, I wouldn't answer.  But I have noticed that Rweb uses
Perl with R: http://www.math.montana.edu/Rweb/

There is also a CGIwithR package that I haven't tried, and I
don't know what it does.
-- 
Jonathan Baron, Professor of Psychology, University of Pennsylvania
Home page:            http://www.sas.upenn.edu/~baron
R page:               http://finzi.psych.upenn.edu/
deleting "Your details" "Your application" "Approved" "Thank you!"



From tblackw at umich.edu  Sun Aug 31 21:11:10 2003
From: tblackw at umich.edu (Thomas W Blackwell)
Date: Sun, 31 Aug 2003 15:11:10 -0400 (EDT)
Subject: [R] Problem and Question regarding R
In-Reply-To: <3F5241D3.3040101@t-online.de>
Message-ID: <Pine.SOL.4.44.0308311504050.7302-100000@tetris.gpcc.itd.umich.edu>

Bernhard  -

1)  DO try building from source.  For me it works painlessly
(Redhat 8.0 linux) and  ./configure  seems to be very good at
locating the resources it needs.

2)  The very easiest, dumbest interface between Perl and R is
for Perl to write an ascii file of R commands - exactly what
one would type at the command line, including maybe
'object <- read.table("file.name")'  to read in data from a
second file - then run this script as:  R BATCH scriptfile outfile.
Wait for this to finish, then read outfile back into Perl.
I can't help with fancier methods, since I've never tried.

-  tom blackwell  -  u michigan medical school  -  ann arbor  -

On Sun, 31 Aug 2003, Bernhard Bals wrote:

> I'm just about to install the Precompiled Binary Distribution of R from
> CRAN for SuSE Linux 8.2: R-base-1.7.1-1.i386.rpm
>
> My system is SuSE Linux 8.2 Professional Edition on a Pentium 4:
>
> # uname -a
> Linux linux 2.4.20-4GB #1 Mon Mar 17 17:54:44 UTC 2003 i686 unknown
> unknown GNU/Linux
>
> 1) Problem:
>
> I've tried to install the above base rpm archive with the command:
>
> # rpm -i R-base-1.7.1-1.i386.rpm
> error: failed dependencies:
>          libg2c.so.0 is needed by R-base-1.7.1-1
>
> There seems to be only one dependency missing (hope that's all). Since
> I've installed the Professional Edition with all the compilers and
> libraries, it's maybe only a naming problem.
>
> What's behind this missing library?
> Can this problem be fixed with a symbolic link pointing to a library
> that I already have on my system?  Any experience with that?
>
> Do you think I need or should compile and install R myself (not using
> one of the binary distributions)?
>
> 2) Question:
>
> I want to use R not only interactively (issue commands after the prompt
>  > ..), but also use some functionality of R from a Perl script.
>
> Can I use methods of R from a Perl script: Is there an interface from
> Perl (or at least from C/C++) to R (f.e. invoke R functions from Perl,
> C, C++ or some other language)?
> If so, can you please point me to the documentation on the Web or in
> literature where this is decribed?
> Do you have some demo Perl script (or C/C++ program) that uses this
> technique, f.e. calling R methods like "mean" or "standard deviation"?
>
> Thanks's for your help!
> _________________
>
> Dr. Bernhard Bals
> Dollmannstr. 6
> D-81541 Mnchen
> +49-89-62509585
> bbals at t-online.de



From ripley at stats.ox.ac.uk  Sun Aug 31 21:52:44 2003
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Sun, 31 Aug 2003 20:52:44 +0100 (BST)
Subject: [R] Problem installing acepack in debian
In-Reply-To: <858yp9n133.fsf@blindglobe.net>
Message-ID: <Pine.LNX.4.44.0308312050050.21843-100000@gannet.stats>

On Sun, 31 Aug 2003, A.J. Rossini wrote:

> feh3k at spamcop.net writes:
> 
> > using debian testing (latest Knoppix) I get an error when installing acepack: 
> >  
> > gcc -shared  -o acepack.so ace.o avas.o rlsmo.o  
> > -L/usr/lib/gcc-lib/i386-linux/3.3 -L/usr/lib/gcc-lib/i386-linux/3.3/../../.. 
> > -lfrtbegin -lg2c-pic -lm -lgcc_s -L/usr/lib/R/bin -lR 
> > /usr/bin/ld: cannot find -lfrtbegin 
> > collect2: ld returned 1 exit status 
> > make: *** [acepack.so] Error 1 
> > ERROR: compilation failed for package 'acepack' 
> >  
> > Thanks for any assistance anyone can give. 
> 
> Frank -- 
> 
> 
> I'm a bit confused as to what the problem is -- on a system initially
> Knoppix (Quantian 0.3, actually), upgraded to Debian unstable, I get:

It is pretty clearly a difference between the system used to build R and
that used to install the library.  I am pretty sure you can just delete
-lfrtbegin from FLIBS in R_HOME/etc/Makeconf, but it is part of a proper
gcc/g77 installation and so should be there.

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From ripley at stats.ox.ac.uk  Sun Aug 31 21:59:17 2003
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Sun, 31 Aug 2003 20:59:17 +0100 (BST)
Subject: [R] Problem and Question regarding R
In-Reply-To: <20030831190937.GA26345@mail2.sas.upenn.edu>
Message-ID: <Pine.LNX.4.44.0308312055230.21843-100000@gannet.stats>

That's strange (RedHat again?) it is part of the gcc build, and not part 
of f2c.  It looks to me as if the version actually used is

/usr/lib/gcc-lib/i386-redhat-linux/3.2/libg2c.so

in gcc-g77-3.2-7 on RH8.0.  SuSE may well be different.

On Sun, 31 Aug 2003, Jonathan Baron wrote:

> It is a slow day, and those who really know the answers may not
> be around.  So here goes.
> 
> On 08/31/03 20:43, Bernhard Bals wrote:
> >Hi,
> ># rpm -i R-base-1.7.1-1.i386.rpm
> >error: failed dependencies:
> >         libg2c.so.0 is needed by R-base-1.7.1-1
> >
> >There seems to be only one dependency missing (hope that's all). Since 
> >I've installed the Professional Edition with all the compilers and 
> >libraries, it's maybe only a naming problem.
> >
> >What's behind this missing library?
> 
> On my Red Hat System (noodle), I get:
> 
> baron at noodle /usr/lib > "ls" -l libg2c*
> lrwxrwxrwx    1 root     root           15 Apr  8 13:57
> libg2c.so.0 -> libg2c.so.0.0.0
> -rwxr-xr-x    1 root     root       109976 Feb 25  2003
> libg2c.so.0.0.0
> 
> In other words, it is linked to libg2c.so.0.0.0.  Also, where
> does it come from?
> 
> baron at noodle /usr/lib > rpm -q --whatprovides libg2c.so.0
> libf2c-3.2.2-5
> 
> baron at noodle /usr/lib > rpm -ql libf2c
> /usr/lib/libg2c.so.0
> /usr/lib/libg2c.so.0.0.0
> 
> In sum, it seems you need the rpm called libf2c.
> 
> >Do you think I need or should compile and install R myself (not using 
> >one of the binary distributions)?
> 
> I have done this, and I do recommend it if you have a reasonably
> fast computer.  But in this particular case, based on
> guesstimation from the name of the missing rpm, I bit you would
> need it anyway.
> 
> Another way to find missing rpms is by typing in the name of the
> relevant file in the search section of
> http://rpmfind.net/linux/RPM/

In this case, don't.  Install g77.


-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From edd at debian.org  Sun Aug 31 22:49:05 2003
From: edd at debian.org (Dirk Eddelbuettel)
Date: Sun, 31 Aug 2003 15:49:05 -0500
Subject: [R] Problem installing acepack in debian
In-Reply-To: <Pine.LNX.4.44.0308312050050.21843-100000@gannet.stats>
References: <858yp9n133.fsf@blindglobe.net>
	<Pine.LNX.4.44.0308312050050.21843-100000@gannet.stats>
Message-ID: <20030831204905.GA13936@sonny.eddelbuettel.com>

On Sun, Aug 31, 2003 at 08:52:44PM +0100, Prof Brian Ripley wrote:
> On Sun, 31 Aug 2003, A.J. Rossini wrote:
> 
> > feh3k at spamcop.net writes:
> > 
> > > using debian testing (latest Knoppix) I get an error when installing acepack: 
> > >  
> > > gcc -shared  -o acepack.so ace.o avas.o rlsmo.o  
> > > -L/usr/lib/gcc-lib/i386-linux/3.3 -L/usr/lib/gcc-lib/i386-linux/3.3/../../.. 
> > > -lfrtbegin -lg2c-pic -lm -lgcc_s -L/usr/lib/R/bin -lR 
> > > /usr/bin/ld: cannot find -lfrtbegin 
> > > collect2: ld returned 1 exit status 
> > > make: *** [acepack.so] Error 1 
> > > ERROR: compilation failed for package 'acepack' 
> > >  
> > > Thanks for any assistance anyone can give. 
> > 
> > Frank -- 
> > 
> > 
> > I'm a bit confused as to what the problem is -- on a system initially
> > Knoppix (Quantian 0.3, actually), upgraded to Debian unstable, I get:
> 
> It is pretty clearly a difference between the system used to build R and
> that used to install the library.  I am pretty sure you can just delete
> -lfrtbegin from FLIBS in R_HOME/etc/Makeconf, but it is part of a proper
> gcc/g77 installation and so should be there.

Yes, it may just be a versioning issue between gcc/g77.

Out of curiousity, I just tried install.package("acepack") in the unstable
chroot I use for Debian packaging building, which includes R, as well as on
my current Debian testing workstation.  It worked on both.

Dirk

-- 
Those are my principles, and if you don't like them... well, I have others.
                                                -- Groucho Marx



From rpraca1 at yahoo.com  Sun Aug 31 13:53:47 2003
From: rpraca1 at yahoo.com (Resume)
Date: Sun, 31 Aug 2003 13:53:47
Subject: [R] Engineer
Message-ID: <E19tZDL-0001Q3-00@bernie.ethz.ch>



OBJECTIVE: 			


Richard's 	Resume	 for Job, 	Consulting or Service


	Sr.  MECHANICAL  &  DESIGN    ENGINEER


Project Mgr, Electro-Mechanical Design, Mfg, Product Development, R&D, CADD


Tel: ( 408)  309-7006 	rpraca9 at yahoo.com


EXPERIENCE:


08/93-present          Sr. MECHANICAL & DESIGN ENGINEER, CADD Mgr "Mech-Tronic" 

                   
Engineering, Design & Product Development Service; Project Management, Mfg, Tooling, Product Improvement; Developing Working Product, Design and Prototype Production;  Controlling & Managing testing,  redesign, manufacturing documentation preparation;  and full realization of Task using technical approach, schedules,  and budget to achieve Final Product.  Selecting vendors and specialist to supply performance  satisfying customers expectation and to create a new, modern product with advanced properties, quality and market desirable working abilities and sellable values.

Preparing propositions & presentations, Review Manufacturing Process and Quality Control, Inspecting standards, Engineering Computations and Technical Improvements,  Supplying and Production Automation. Project Management & Development. Preparing technical documentation, calculations, engineering, design, layouts, drawings, 3D and Solid Modeling, development & propositions. Hard Drive Design, testing, balancing, recalculating and redesign. Manufacturing and Assembly Equipment design and build.  Systems Integration.

Tooling and Operations Development, Implementation & Automation for mass production.  Energy, Electric Vehicles and Computerized Transportation System Design & Implement. Solar Panels. Solid Works, Pro-E, CAD Management and Operations, Analyzing, Micro Station, ACAD 10-2002 & LT, Win 3x & 2000', Net, Internet, Softdesk, Structural Design. Network, Security, Electrical Design & Installations; Commercial, Industrial, Residential, Fire Alarms, Smoke Detectors, Lights, Plugs, Panels, Power Supply,  Electro - Solar Installations, CADD automation. Dynamics, Kinematics, thermodynamics / heat transfer & FEA analyzing.

Manufacturing hydraulic & pneumatic equipment, machinery and control systems, mechanisms, robotics device, precision machine elements.  Inspecting and control manufacturing standards, analyzing stresses and tolerances, selecting materials, engineering computations and technical improvements, documentation, projects development, supplying.  Automation, Conveyors, Spiral Elevator, Fast Cannery Transportation, Electronic and Electrical Components, Master Control Center, Sheet Metal Oven Rebuilding Project. 

Power Systems, UPS. Automation equipment and machine control using for machines and robotics precision mechanisms motion programming, fluid mechanics, pneumatics systems, mounting and positioning devices, electro-mechanical and vacuum mechanisms, design and analysis of structures, castings, welded frames, mechanical detailing. Computers: DOS, SUN UNIX, MAC, WP, dB, Lotus, Network, Windows & Applications; MS Project, Excel, Access, Word, PFS, Graphics, CAD / CAM, Excel, Basic, C, Fortran, Analyzes. CADD systems, Algor, VAX, Net.  MCAD, Acad's 2002 & Softdesk, Script, Nastran, Infusoft, LAN, Tektronix, Cadkey,  Lisp, Personal Designer, ACAD / Computer Instructor, 

Machine Design, Robotics & Automation, WIND, SOLAR, METRIC.


EDUCATION:

Institute for Business & Technology, California
CADD Engineer,  Programming,  Design,  Management

Electro - Mechanical  College
Mechanical Engineering - BSME,   ASEE,  MBA


Open for Travel  *  Salary open *  Permanent  preferred  or Consulting



