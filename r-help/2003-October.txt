From dmurdoch at pair.com  Wed Oct  1 00:02:11 2003
From: dmurdoch at pair.com (Duncan Murdoch)
Date: Tue, 30 Sep 2003 18:02:11 -0400
Subject: [R] Problem with R1.8beta
In-Reply-To: <1ee.1095b94e.2cab39d3@aol.com>
References: <1ee.1095b94e.2cab39d3@aol.com>
Message-ID: <uosjnv4tf49qpki6sul7li3p4pb7a0vldi@4ax.com>

On Tue, 30 Sep 2003 15:56:03 EDT, you wrote:

>Hi,
>I trying to use r.180beta on a XPPro PC with Athlon 2.0
>I downloaded the zip file and when I unzipped it using winzip last version
>I had this message:
>C:\Program Files\R\rw1080beta\doc\manual\refman.pdf
>An error occured while trying to copy a file:
>The source file is corrupted.
>
>Anything to do with this or shall I just skip it ?

I'd check the md5sum for the file to see whether your download really
was corrupted or not.

Duncan Murdoch



From jasont at indigoindustrial.co.nz  Wed Oct  1 00:04:23 2003
From: jasont at indigoindustrial.co.nz (Jason Turner)
Date: Wed, 01 Oct 2003 10:04:23 +1200
Subject: [R] variables
In-Reply-To: <20030930213854.7462.qmail@web10505.mail.yahoo.com>
References: <20030930213854.7462.qmail@web10505.mail.yahoo.com>
Message-ID: <3F79FDE7.4050705@indigoindustrial.co.nz>

forkusam wrote:
> Hi,
> can someone please help me.
> I will give a simple example of my problem. 
> p <- function()
> {
>  i <- 1
>  sr <- function(){
>  i<-i+3
>  i<- sqrt(i)
> }
> cat(i) 
> }
>  This is just an example. My main problem is defining
> i like a global variable which I can use in the sub-
> and main functions without any complicated switches. 
> Thanks in advance.
> cilver

Within the function p(), it should "just work".  Try this slight 
variation of your example:

p <- function()
{
   i <- 1
   sr <- function(){
     i<-i+3
     i<- sqrt(i)
     i #explicitly return the newly calculated value
   }
   cat("sr() returned ",sr(),"\n")
   #now check if i was actually modified outside sr()
   cat("i = ",i,"\n")

}

When I load this into R...

 > p()
sr() returned  2
i =  1

note that i is not visible outside the function p().  It disappears when 
p() exits.

 > i
Error: Object "i" not found

The internal function variables being available to sub-fuctions is an 
example of "lexical scoping", and it's one of the things that makes R so 
nice to work with.  Try demo(scoping) for a very interesting use of 
lexical scoping.

Cheers

Jason

-- 
Indigo Industrial Controls Ltd.
http://www.indigoindustrial.co.nz
64-21-343-545
jasont at indigoindustrial.co.nz



From glaziou at pasteur-kh.org  Wed Oct  1 03:16:41 2003
From: glaziou at pasteur-kh.org (Philippe Glaziou)
Date: Wed, 1 Oct 2003 08:16:41 +0700
Subject: [R] Remove comma (,) in data set
In-Reply-To: <3F79B4A8.7020009@cs.uu.nl>
References: <3F79B4A8.7020009@cs.uu.nl>
Message-ID: <20031001011641.GB24370@pasteur-kh.org>

Muhammad Subianto <subianto at cs.uu.nl> wrote:
> I am new learning R. Now, I have a data set like:
> 
> 24,2,3,3,1,1,2,3,0,1
> 45,1,3,10,1,1,3,4,0,1
> ... with 3730 rows
> 
> I want to remove comma (,) in data set. The result like:
> 
> 24 2 3  3 1 1 2 3 0 1
> 45 1 3 10 1 1 3 4 0 1
> ...
> 
> How can I do it. Thanks you for your help.


Assuming your dataset is stored in a flat ascii file, you need
not use R for this trivial translation of the character "," into
a space " ". From your shell (i.e., if standard shell tools are
available on your machine), type the following:

cat file1 | tr , " " > file2

-- 
Philippe Glaziou
Institut Pasteur du Cambodge



From s.mcclatchie at niwa.co.nz  Wed Oct  1 08:31:22 2003
From: s.mcclatchie at niwa.co.nz (Sam McClatchie)
Date: Wed, 01 Oct 2003 18:31:22 +1200
Subject: [R] problem downloading Red Hat R-1.7.1-1.i386.rpm 
Message-ID: <3F7A74BA.4090406@niwa.cri.nz>

System info:
Red Hat 9.0
R Version 1.7.0
ESS 5.1.21
Emacs 21.2.1
-------------------

Colleagues

At work I've had to migrate from Mandrake 9.1 to Red Hat 9.0 and I'm 
reinstalling R. I am having a problem downloading from 
R-1.7.1-1.i386.rpm from /bin/linux/redhat/9/i386. Basically the download 
does not complete. I am using mozilla, and when I click on the link the 
screen corrupts with odd symbols. Is there a problem with the site? (I 
don't have any problem downloading the user-contributed packages or the 
R-1.7.1.tgz.

Sam
-- 
Sam McClatchie, Research scientist (fisheries acoustics))))))))))
NIWA (National Institute of Water & Atmospheric Research Ltd)
PO Box 14 901, Kilbirnie, Wellington, New Zealand
s.mcclatchie at niwa.cri.nz
Research home page <http://www.smcc.150m.com/>
                         /\
              >><xX(&>
                      /// \\\
                     //// \\\\
                    ///  <%)Xx><<
                   /////  \\\\\\
             ><(((@>
       ><(((%>     ..>><xX(?>O<?)Xx><<



From baron at psych.upenn.edu  Wed Oct  1 08:57:28 2003
From: baron at psych.upenn.edu (Jonathan Baron)
Date: Wed, 1 Oct 2003 02:57:28 -0400
Subject: [R] problem downloading Red Hat R-1.7.1-1.i386.rpm
In-Reply-To: <3F7A74BA.4090406@niwa.cri.nz>
References: <3F7A74BA.4090406@niwa.cri.nz>
Message-ID: <20031001065728.GA29180@mail1.sas.upenn.edu>

On 10/01/03 18:31, Sam McClatchie wrote:
>At work I've had to migrate from Mandrake 9.1 to Red Hat 9.0 and I'm 
>reinstalling R. I am having a problem downloading from 
>R-1.7.1-1.i386.rpm from /bin/linux/redhat/9/i386. Basically the download 
>does not complete. I am using mozilla, and when I click on the link the 
>screen corrupts with odd symbols. Is there a problem with the site? (I 
>don't have any problem downloading the user-contributed packages or the 
>R-1.7.1.tgz.

This happens at the master CRAN site, but not at
http://cran.us.r-project.org
so I think it is something about the way the file is encoded by
the web page.  (I cannot figure out what it is.  The pages look
identical.  There was a problem like this once before where R was
invoking a RealPlayer plugin, but that doesn't happen now.)

So you can get it from another site.  Or you can just wait until
the screen is finished filling up with (apparent) junk and then
save that file using the File menu in Mozilla (upper left).

Jon
-- 
Jonathan Baron, Professor of Psychology, University of Pennsylvania
Home page:            http://www.sas.upenn.edu/~baron
R page:               http://finzi.psych.upenn.edu/



From hodgess at gator.dt.uh.edu  Wed Oct  1 09:04:26 2003
From: hodgess at gator.dt.uh.edu (Erin Hodgess)
Date: Wed, 1 Oct 2003 02:04:26 -0500
Subject: [R] permission problem on R install
Message-ID: <200310010704.h9174QQ03874@gator.dt.uh.edu>

Dear R People:

I'm trying to compile R for Windows from source.

I get to the Making package base and then

c:\rsource\R-1.7.1\bin\rterm.exe permission denied

How do I change the permissions, please?

Thanks again
R-1.1.1

Sincerely,
Erin
mailto: hodgess at gator.uhd.edu



From p.dalgaard at biostat.ku.dk  Wed Oct  1 09:04:51 2003
From: p.dalgaard at biostat.ku.dk (Peter Dalgaard BSA)
Date: Wed, 01 Oct 2003 07:04:51 -0000
Subject: [R] problem downloading Red Hat R-1.7.1-1.i386.rpm
In-Reply-To: <3F7A74BA.4090406@niwa.cri.nz>
References: <3F7A74BA.4090406@niwa.cri.nz>
Message-ID: <x2oex1e8t9.fsf@biostat.ku.dk>

Sam McClatchie <s.mcclatchie at niwa.co.nz> writes:

> System info:
> Red Hat 9.0
> R Version 1.7.0
> ESS 5.1.21
> Emacs 21.2.1
> -------------------
> 
> Colleagues
> 
> At work I've had to migrate from Mandrake 9.1 to Red Hat 9.0 and I'm
> reinstalling R. I am having a problem downloading from
> R-1.7.1-1.i386.rpm from /bin/linux/redhat/9/i386. Basically the
> download does not complete. I am using mozilla, and when I click on
> the link the screen corrupts with odd symbols. Is there a problem with
> the site? (I don't have any problem downloading the user-contributed
> packages or the R-1.7.1.tgz.
> 
> Sam

Mozilla gets confused about the .rpm ending I think. Just use
Shift-Button1 to download, or right-click, "save target as..".

-- 
   O__  ---- Peter Dalgaard             Blegdamsvej 3  
  c/ /'_ --- Dept. of Biostatistics     2200 Cph. N   
 (*) \(*) -- University of Copenhagen   Denmark      Ph: (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)             FAX: (+45) 35327907



From ahenningsen at email.uni-kiel.de  Wed Oct  1 09:09:02 2003
From: ahenningsen at email.uni-kiel.de (Arne Henningsen)
Date: Wed, 1 Oct 2003 09:09:02 +0200
Subject: [R] problem downloading Red Hat R-1.7.1-1.i386.rpm
In-Reply-To: <3F7A74BA.4090406@niwa.cri.nz>
References: <3F7A74BA.4090406@niwa.cri.nz>
Message-ID: <200310010909.02555.ahenningsen@email.uni-kiel.de>

*right* click the link and then left click "Save Target As" (or something 
similar".

Arne 

On Wednesday 01 October 2003 08:31, Sam McClatchie wrote:
> System info:
> Red Hat 9.0
> R Version 1.7.0
> ESS 5.1.21
> Emacs 21.2.1
> -------------------
>
> Colleagues
>
> At work I've had to migrate from Mandrake 9.1 to Red Hat 9.0 and I'm
> reinstalling R. I am having a problem downloading from
> R-1.7.1-1.i386.rpm from /bin/linux/redhat/9/i386. Basically the download
> does not complete. I am using mozilla, and when I click on the link the
> screen corrupts with odd symbols. Is there a problem with the site? (I
> don't have any problem downloading the user-contributed packages or the
> R-1.7.1.tgz.
>
> Sam

-- 
Arne Henningsen
Department of Agricultural Economics
Christian-Albrechts-University Kiel
24098 Kiel, Germany
Tel: +49-431-880-4445
Fax: +49-431-880-1397 
ahenningsen at email.uni-kiel.de
http://www.uni-kiel.de/agrarpol/ahenningsen/



From Joke.Allemeersch at esat.kuleuven.ac.be  Wed Oct  1 10:08:52 2003
From: Joke.Allemeersch at esat.kuleuven.ac.be (Joke Allemeersch)
Date: Wed, 01 Oct 2003 08:08:52 -0000
Subject: [R] installing DBI_0.1-6.tar.gz
Message-ID: <1064995769.31972.8.camel@miyagikyo.esat.kuleuven.ac.be>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20031001/65efd3e8/attachment.pl

From diriano at rz.uni-potsdam.de  Wed Oct  1 10:34:23 2003
From: diriano at rz.uni-potsdam.de (Diego Riano)
Date: Wed, 01 Oct 2003 08:34:23 -0000
Subject: [R] truncating axis
In-Reply-To: <OF31B1BAC8.71291EFE-ON85256DAC.006EDCC0@hc-sc.gc.ca>
References: <OF31B1BAC8.71291EFE-ON85256DAC.006EDCC0@hc-sc.gc.ca>
Message-ID: <1064996052.19197.0.camel@molbio21>

Hello

Does anyone know how to truncate and axis in R?

Thanks

Diego
-- 
_______________________________________
Diego Mauricio Riano Pachon
Biologist
Institute of Biology and Biochemistry
Potsdam University
Karl-Liebknecht-Str. 24-25
Haus 20
14476 Golm
Germany
Tel:0331/977-2809
http://bioinf.ibun.unal.edu.co/~gotem
http://www.geocities.com/dmrp.geo/



From jasont at indigoindustrial.co.nz  Wed Oct  1 10:44:56 2003
From: jasont at indigoindustrial.co.nz (Jason Turner)
Date: Wed, 01 Oct 2003 20:44:56 +1200
Subject: [R] truncating axis
In-Reply-To: <1064996052.19197.0.camel@molbio21>
References: <OF31B1BAC8.71291EFE-ON85256DAC.006EDCC0@hc-sc.gc.ca>
	<1064996052.19197.0.camel@molbio21>
Message-ID: <3F7A9408.8020309@indigoindustrial.co.nz>

Diego Riano wrote:

> Hello
> 
> Does anyone know how to truncate and axis in R?
> 

Not sure what you mean, but do to plot arguments xlim and ylim do what 
you want?  See help(par) for these and other plot controls.

Cheers

Jason
-- 
Indigo Industrial Controls Ltd.
http://www.indigoindustrial.co.nz
64-21-343-545
jasont at indigoindustrial.co.nz



From kwan022 at stat.auckland.ac.nz  Wed Oct  1 10:55:36 2003
From: kwan022 at stat.auckland.ac.nz (Ko-Kang Kevin Wang)
Date: Wed, 1 Oct 2003 20:55:36 +1200 (NZST)
Subject: [R] truncating axis
In-Reply-To: <1064996052.19197.0.camel@molbio21>
Message-ID: <Pine.LNX.4.44.0310012054290.24122-100000@stat55.stat.auckland.ac.nz>

On 1 Oct 2003, Diego Riano wrote:

> Does anyone know how to truncate and axis in R?

What do you mean by "truncate"?  Do you mean to change the range of the 
axes?  If so then something like
  plot(x, xlim = range(), ylim = range())
should do (just put the desired range....


-- 
Cheers,

Kevin

------------------------------------------------------------------------------
"On two occasions, I have been asked [by members of Parliament],
'Pray, Mr. Babbage, if you put into the machine wrong figures, will
the right answers come out?' I am not able to rightly apprehend the
kind of confusion of ideas that could provoke such a question."

-- Charles Babbage (1791-1871) 
---- From Computer Stupidities: http://rinkworks.com/stupid/

--
Ko-Kang Kevin Wang
Master of Science (MSc) Student
SLC Tutor and Lab Demonstrator
Department of Statistics
University of Auckland
New Zealand
Homepage: http://www.stat.auckland.ac.nz/~kwan022
Ph: 373-7599
    x88475 (City)
    x88480 (Tamaki)



From andrejk at zrc-sazu.si  Wed Oct  1 11:57:36 2003
From: andrejk at zrc-sazu.si (Andrej Kveder)
Date: Wed, 1 Oct 2003 11:57:36 +0200
Subject: [R] FW: error predicting values from the LME
Message-ID: <FHEEJBDDCNPPNJEACDJAAEOFDDAA.andrejk@zrc-sazu.si>

Thanks for the hint. And it's a general one I intend to use more often.
I managed to sort out the problem proceeding with the simpler models first
and getting to more komplex ones. Even the one I presented in my question
suddenly worked.
However I think I found the problem, but I'm unable to solve it. With the
problematic prediction I used the prespecified model using the as.formula.
This is my exapmle:

# first way
m.gr<-as.formula("y ~ v11+v21+v22+v23 | inter")
m.fix<-as.formula("y ~ v11+v21+v22+v23+v21:v11+v22:v11+v23:v11")
m.ran<-as.formula("~ v11 | inter")
d.n.gr.1<-grouped.data(m.gr data=d.n)
m.1<-lme(m.fix, random=m.ran, data=d.n.gr.1, na.action=na.omit)
dnNew<-subset(d.n,is.na(d.n$y),select=c(v11,v21,v22,v23,inter))
p.1<-predict(m.1,dnNew)
# second way
d.n.gr.2<-grouped.data(y ~ v11+v21+v22+v23 | inter data=d.n)
m.2<-lme(y ~ v11+v21+v22+v23+v21:v11+v22:v11+v23:v11, random=~ v11 | inter,
data=d.n.gr.2, na.action=na.omit)
p.2<-predict(m.1,dnNew)

Second way works, while the first way doesn't. I get all the model
specifications from the first one including fitted and residuals, everything
that is derived from the lme object itself. However with the predictions I
get the error.

Error in eval(expr, envir, enclos) : 1 argument passed to "$" which requires
2.

I have saved the 3 objects: both models and the test dataset and attached it
to the message in hope to help solve my problem, however my message was
rejected as being to big (the attached file is 80K). If somebody has an idea
how to solve the problem, I can send the data to him/her.

Is there a solution to the problem?

Thanks for the help so far.
I would appreciate any other suggestions.


Andrej



-----Original Message-----
From: Spencer Graves [mailto:spencer.graves at pdf.com]
Sent: Tuesday, September 30, 2003 3:13 PM
To: Andrej Kveder
Cc: R-Help
Subject: Re: [R] FW: error predicting values from the LME


      Could you dumb it down to a toy example with 4 observations for a
model like "y ~ 1 | inter", 2 observations for each of 2 levels of
"inter"?  If that works, then you can play with the example that works
and the example that doesn't;  this is one of the strategies mentioned
in Poly (1971) How to Solve It (Princeton U. Pr.).  With luck, this will
help you figure out what you need to do to get the answers you want.  If
not, it should help you produce a small toy example that doesn't work,
which you can then send us.  Please include a data.frame call, so
someone can copy your example into R and try it in 2 seconds.  That
should increase the chances that you would get a helpful reply.

      Also, have you read Pinhiero and Bates (2000) Mixed-Effect Models
in S and S-Plus (Springer)?  I've found that book to be indispensible
for using "lme".

      hope this helps.  spencer graves

Andrej Kveder wrote:

>HI all,
>
>I might add some more information in order to possibly solve my problem.
I'm
>really stuck and no obvious solutions do the trick.
>I'm using R 1.7.1 on Windows 2000 with the packages regurarly updated.
>I'm using hypothetical data constructed as a pseudo population conforming
to
>a certain Var-Cov structure.
>I might add that just
>
>
>
>>predict(level2)
>>
>>
>
>works. But when I add the new dataset it doesn't. Following a suggestion I
>even tried refactoring of the grouping variable (inter) after I created the
>subset. It didn't work. I have no other factor variables in the model. I
>really have got no clue what could be wrong.
>
>There is a sample from my data:
>
>
>>dnNew
>>
>>
>Grouped Data: y ~ v11 + v21 + v22 + v23 | inter
>         v11             v21          v22         v23    inter
>4 5.55186635 5.6620022 24.18033 5.003409 1
>13 2.03852426 5.6620022 24.18033 5.003409 1
>15 2.19825772 7.5676798 31.03986 4.746891 2
>16 4.51368278 7.5676798 31.03986 4.746891 2
>18 3.35322702 7.5676798 31.03986 4.746891 2
>19 2.46414346 7.5676798 31.03986 4.746891 2
>20 2.66670834 7.5676798 31.03986 4.746891 2
>
>and this is the model:
>
>
>>level2
>>
>>
>Linear mixed-effects model fit by REML
>Data: d.n.gr.2
>Log-restricted-likelihood: -533.0011
>Fixed: model$fixed
>(Intercept) v11 v21 v22 v23 v11:v21
>3.205519074 0.298941539 -0.017743958 0.016007280 -0.410760471 0.002700954
>v11:v22 v11:v23
>-0.003680952 -0.018005717
>Random effects:
>Formula: ~v11 | inter
>Structure: General positive-definite, Log-Cholesky parametrization
>StdDev Corr
>(Intercept) 0.385620605 (Intr)
>v11 0.003147431 -0.048
>Residual 0.450012367
>Number of Observations: 729
>Number of Groups: 50
>If this give you some more insight to my problem.
>
>I would reallly appreciate any suggestion.
>
>Thanks
>
>Andrej
>
>-----Original Message-----
>From: Andrej Kveder [mailto:andrejk at zrc-sazu.si]
>Sent: Monday, September 29, 2003 7:05 PM
>To: R-Help
>Subject: predicting values from the LME
>
>
>Dear listers,
>
>I experinced a problem prdicting the values using the LME with multilevel
>data.
>I have NA's in my dependent variable and the model is fitted only on the
>completed cases.
>I want to estimate the predicted values for the rest of the data (those
>cases with missing dep. variable)
>I extracted a subset from the original file containing the variables used
in
>the model as well as the second level indicator.
>I used the following command
>
>p<-predict(level2,newdata=d.n.new,level=0:1)
>
>where level2 is my LME model.
>But, I get the following error:
>
>Error in eval(expr, envir, enclos) : 1 argument passed to "$" which
requires
>2.
>
>I tried with omitting the level specification (which is 0 by default) and I
>transformed the new data to be groupedData with no luck.
>
>I have tried the example from the Pinheiro,Bates book and it works - mine
>doesn't. Does anybody have an idea what could be wrong?
>
>Thanks for all the suggestions.
>
>Andrej
>
>_________
>Andrej Kveder, M.A.
>researcher
>Institute of Medical Sciences SRS SASA; Novi trg 2, SI-1000 Ljubljana,
>Slovenia
>phone: +386 1 47 06 440   fax: +386 1 42 61 493
>
>	[[alternative HTML version deleted]]
>
>______________________________________________
>R-help at stat.math.ethz.ch mailing list
>https://www.stat.math.ethz.ch/mailman/listinfo/r-help
>
>



From Joke.Allemeersch at esat.kuleuven.ac.be  Wed Oct  1 12:01:02 2003
From: Joke.Allemeersch at esat.kuleuven.ac.be (Joke Allemeersch)
Date: Wed, 01 Oct 2003 10:01:02 -0000
Subject: [R] R-1.7.1 for Redhat 9
Message-ID: <1065002499.31972.53.camel@miyagikyo.esat.kuleuven.ac.be>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20031001/5dd166b9/attachment.pl

From Carl-Goran.Pettersson at evp.slu.se  Wed Oct  1 12:10:40 2003
From: Carl-Goran.Pettersson at evp.slu.se (C-G Pettersson)
Date: Wed, 01 Oct 2003 12:10:40 +0200
Subject: [R] Evaluating outer, numeric, variables in an lme object.
Message-ID: <5.2.0.9.0.20031001111907.00bc6570@mail1.slu.se>

Hello!
I?m working with a dateset from twelve fertilizer experiments (Trial) with 
a block structure of three replicats (Block). The treatment levels consist 
of application method and product but only one intensity. The factor TrCode 
could for example be BC(CAN) for broadcast calcium ammonium nitrate. I?ve 
used the following call to fit a lme-object to the data:

ejna1t4b.lme <- lme( Yield ~ TrCode, data = ejna1t4,
+                       random = ~ 1 | Trial/Block)

This seems, for me, to work quite well. Or should I code differently?

What I want to do now, is to evaluate the impact from a load of soil and 
weather data where I only have one reading from each Trial/variable. These 
variables are all numeric.

Is this possible in the lme context?
It is naturally possible to calculate treatment means of all Trials and run 
a regression analysis on the new dataset, but I have a feeling there are 
smarter ways of doing this. Or?

Is there anything on this type of problem in MASS or Pinheiro & Bates? So 
far, if I have found the proper texts, I have not understood it... ;-)

Thanks
/CG



CG Pettersson, MSc, PhD-stud
Department of Ecology and Crop Production Science, SLU
P.O. Box 7043
S-750 07 Uppsala, Sweden
Phone: +46 (0)18 67 12 24; Fax: +46 (0)18 67 29 06
        +46 (0)70 330 66 85



From m.sutter at pmodwrc.ch  Wed Oct  1 12:40:02 2003
From: m.sutter at pmodwrc.ch (Marcel Sutter)
Date: Wed, 01 Oct 2003 12:40:02 +0200
Subject: [R] Text cutoff in legends
Message-ID: <3F7AAF02.2050703@pmodwrc.ch>

Dear r-help

If I display plots on the X11 device, the legends looks fine. But if 
make an EPS, the longer entry in the legend is cutoff (and also in the 
.pdf I do from the .eps)

Can you give me a hint and tell me what I do wrong, please ?

postscript("plot.eps",paper="special",horizontal=F,onefile=F,width=8.0,height=7.0)
dev.set (2)
dev.copy (which=3)
dev.off (3)

R Version 1.6.1  (2002-11-01) on SuSE Linux 8.0


Thank you very much

Marcel

-- 
Marcel Sutter
PMOD/WRC
Dorfstrasse 33
CH-7260 Davos Dorf
Switzerland

http://www.pmodwrc.ch



From andy_liaw at merck.com  Wed Oct  1 12:43:42 2003
From: andy_liaw at merck.com (Liaw, Andy)
Date: Wed, 01 Oct 2003 06:43:42 -0400
Subject: [R] R-1.7.1 for Redhat 9
Message-ID: <3A822319EB35174CA3714066D590DCD50205CBE5@usrymx25.merck.com>

Tell us the output of something like

  tail -20 /volume1/scratch/jallemee/R-1.7.1/tests/Examples/base-Ex.Rout

so we know where it bombed (and may be how it bombed).

Andy

> -----Original Message-----
> From: Joke Allemeersch [mailto:Joke.Allemeersch at esat.kuleuven.ac.be] 
> Sent: Wednesday, October 01, 2003 6:02 AM
> To: r-help at stat.math.ethz.ch
> Subject: [R] R-1.7.1 for Redhat 9
> 
> 
> Dear,
> 
> I have problems with installing R-1.7.1 for Redhat 9.  When I 
> applied `./configure' and `make', I get the following error:
> 
> make check
> make[1]: Entering directory `/volume1/scratch/jallemee/R-1.7.1/tests'
> make[2]: Entering directory `/volume1/scratch/jallemee/R-1.7.1/tests'
> make[3]: Entering directory 
> `/volume1/scratch/jallemee/R-1.7.1/tests/Examples'
> make[4]: Entering directory 
> `/volume1/scratch/jallemee/R-1.7.1/tests/Examples'
> make[4]: `Makedeps' is up to date.
> make[4]: Leaving directory 
> `/volume1/scratch/jallemee/R-1.7.1/tests/Examples'
> make[4]: Entering directory 
> `/volume1/scratch/jallemee/R-1.7.1/tests/Examples'
> running code in 'base-Ex.R' ...make[4]: *** [base-Ex.Rout] Error 1
> make[4]: Leaving directory 
> `/volume1/scratch/jallemee/R-1.7.1/tests/Examples'
> make[3]: *** [test-Examples-Base] Error 2
> make[3]: Leaving directory 
> `/volume1/scratch/jallemee/R-1.7.1/tests/Examples'
> make[2]: *** [test-Examples] Error 2
> make[2]: Leaving directory `/volume1/scratch/jallemee/R-1.7.1/tests'
> make[1]: *** [test-all-basics] Error 1
> make[1]: Leaving directory `/volume1/scratch/jallemee/R-1.7.1/tests'
> make: *** [check] Error 2
> 
> 
> I found on the web that one has to change something in the 
> file src/main/arithmetic.c, but this was already done in my 
> file, so that can`t cause the error.  Is there somebody who 
> knows how to solve this?
> 
> Thanks a lot,
> 
> kind regards, 
> Joke
> 
> 
> Joke Allemeersch 
> Kasteelpark Arenberg 10 
> 3001 Heverlee (Leuven) 
> http://www.esat.kuleuven.ac.be/~dna/BioI/
> 
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list 
> https://www.stat.math.ethz.ch/mailman/listinfo> /r-help
>



From jasont at indigoindustrial.co.nz  Wed Oct  1 12:54:11 2003
From: jasont at indigoindustrial.co.nz (Jason Turner)
Date: Wed, 01 Oct 2003 22:54:11 +1200
Subject: [R] Text cutoff in legends
In-Reply-To: <3F7AAF02.2050703@pmodwrc.ch>
References: <3F7AAF02.2050703@pmodwrc.ch>
Message-ID: <3F7AB253.3020208@indigoindustrial.co.nz>

Marcel Sutter wrote:

> Dear r-help
> 
> If I display plots on the X11 device, the legends looks fine. But if 
> make an EPS, the longer entry in the legend is cutoff (and also in the 
> .pdf I do from the .eps)
> 
> Can you give me a hint and tell me what I do wrong, please ?

Probably nothing.  The X11 and postscript devices have different 
boundaries.  The plot is made to fit them, and the legend added to that. 
   I don't usually use dev.copy(); I just repeat the plot commands after 
the postscript() device is opened.  Try that; if that doesn't work, 
adjust your legend placement.

> postscript("plot.eps",paper="special",horizontal=F,onefile=F,width=8.0,height=7.0) 
> 
> dev.set (2)
> dev.copy (which=3)
> dev.off (3)
> 
> R Version 1.6.1  (2002-11-01) on SuSE Linux 8.0

Might as well upgrade R while you're at it.  1.8 will be out Real Soon Now.

Cheers

Jason
-- 
Indigo Industrial Controls Ltd.
http://www.indigoindustrial.co.nz
64-21-343-545
jasont at indigoindustrial.co.nz



From paul_divi at yahoo.gr  Wed Oct  1 13:08:01 2003
From: paul_divi at yahoo.gr (=?iso-8859-7?q?Paul=20Divid?=)
Date: Wed, 1 Oct 2003 12:08:01 +0100 (BST)
Subject: [R] Simulation of Levy Processes and Fractional Brownian motion
Message-ID: <20031001110801.72011.qmail@web12907.mail.yahoo.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20031001/9aaf0131/attachment.pl

From Joke.Allemeersch at esat.kuleuven.ac.be  Wed Oct  1 13:12:39 2003
From: Joke.Allemeersch at esat.kuleuven.ac.be (Joke Allemeersch)
Date: Wed, 01 Oct 2003 11:12:39 -0000
Subject: [R] R-1.7.1 for Redhat 9
In-Reply-To: <3A822319EB35174CA3714066D590DCD50205CBE5@usrymx25.merck.com>
References: <3A822319EB35174CA3714066D590DCD50205CBE5@usrymx25.merck.com>
Message-ID: <1065006794.31972.61.camel@miyagikyo.esat.kuleuven.ac.be>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20031001/d529b23f/attachment.pl

From Ted.Harding at nessie.mcc.ac.uk  Wed Oct  1 14:09:51 2003
From: Ted.Harding at nessie.mcc.ac.uk ( (Ted Harding))
Date: Wed, 01 Oct 2003 13:09:51 +0100 (BST)
Subject: [R] Simulation of Levy Processes and Fractional Brownian mot
In-Reply-To: <20031001110801.72011.qmail@web12907.mail.yahoo.com>
Message-ID: <XFMail.031001130951.Ted.Harding@nessie.mcc.ac.uk>

On 01-Oct-03 Paul Divid wrote:
> Dear R users, 
>  
> Are there any functions that can be used for the
> Simulation of Levy Processes and Fractional Brownian motion?

Warning: computer simulation of fractal/chaotic processes can be
danegrously misleading:

  chaos<-function(xstart,n) {
    x<-xstart; y<-x;
    for(i in (1:n)){
      if(y<=0.5){y<-2*y} else {y<-2*(1-y)}
      x<-c(x,y)
    }
    return(x)
  }

Mathematically, this function is genuinely chaotic, for almost all
values of 0 < xstart < 1.

Try it with

  chaos(sqrt(2)/2,100)

With tongue (somewhat) in cheek,
Ted.



--------------------------------------------------------------------
E-Mail: (Ted Harding) <Ted.Harding at nessie.mcc.ac.uk>
Fax-to-email: +44 (0)870 167 1972
Date: 01-Oct-03                                       Time: 13:09:51
------------------------------ XFMail ------------------------------



From will_harvey03 at yahoo.com  Wed Oct  1 14:56:34 2003
From: will_harvey03 at yahoo.com (Will Harvey)
Date: Wed, 1 Oct 2003 05:56:34 -0700 (PDT)
Subject: [R] Solving a tridiagonal system
Message-ID: <20031001125634.99679.qmail@web12903.mail.yahoo.com>

I need to find solutions to a tridiagonal system. By
this I mean a set of linear equations Ax = d where A
is a square matrix containing elements A[i,i-1],
A[i,i] and A[i,i+1] for i in 1:nrow, and zero
elsewhere. R is probably not the ideal way to do this,
but this is part of a larger problem that requires R.

In my application it is much easier (and much faster)
to generate the diagonal and off-diagonal elements of
A as vectors, i.e. a = A[i,i-1], b = A[i,i] and c =
A[i,i+1]. So I have three vectors that define A, along
with a solution vector d. The conventional method of
solving such systems is to use the so-called "Thomas
algorithm", see e.g. 
<http://www.enseeiht.fr/hmf/travaux/CD0001/travaux/optmfn/hi/01pa/hyb74/node24.html>.
This is very easy to code, but much more difficult to
"vectorize". Is anyone aware of a library that
contains a fast implementation of this algorithm?

Another alternative is to use backsolve. I can easily
eliminate the lower diagonal a, but I'm still left
with b and c, whereas backsolve requires a matrix.
Again, I can write a function to read b and c into a
matrix, but this requires loops, and is too slow. Is
there a vectorized way of doing it? Of course, the
diag command works for b, but what about c? In Octave,
diag allows for an offset, but R apparently does not.

I would appreciate any and all assistance you experts
can offer. Thanks in advance.

Will Harvey



From ucgamdo at ucl.ac.uk  Wed Oct  1 15:09:36 2003
From: ucgamdo at ucl.ac.uk (ucgamdo@ucl.ac.uk)
Date: Wed, 01 Oct 2003 14:09:36 +0100
Subject: [R] Re: Mandelbrot set and C code
In-Reply-To: <16233.27375.199834.532442@gargle.gargle.HOWL>
References: <3.0.5.32.20030917124551.007aaab0@pop-server.ucl.ac.uk>
	<3.0.5.32.20030917124551.007aaab0@pop-server.ucl.ac.uk>
Message-ID: <3.0.5.32.20031001140936.007e2100@pop-server.ucl.ac.uk>

      I decided to take on the 'proper' solution to calculate the
Mandelbrot set in R, i.e. to do the raw calculations in C and then link
that code with R. I thought it would be a hard task, but I was pleasantly
surprised when I saw how easily was to write the bit of C code (I am not a
C programmer myself!) and how even easier was to link it with R using .C()
and R CMD SHLIB. If you are interested (or anyone else) here is the (very
fast) code to calculate the mandelbrot set, I hope you enjoy playing with
it as much as I did.

# BEGINNING OF R CODE: ################################################

#######################################################################
# Function to calculate the Mandelbrot set. This function calls a     #
# C routine in order to perform the calculations faster.              #
#                                                                     #
# Written by Mario dos Reis. September 2003                           #
#######################################################################

mandelbrot <- function(x = c(-3, 1),        # x limits
                       y = c(-1.8, 1.8),    # y limits
                       nx = 600,            # x resolution
                       ny = 600,            # y resolution
                       iter = 20)           # maximun number of iterations
{
  xcoo <- seq(x[1], x[2], len = nx) # x coordinates
  ycoo <- seq(y[1], y[2], len = ny) # y coordinates
  set = numeric(nx*ny)              # this will store the output of
                                    # the C routine

  # This is the call to the C function itself
  the.set = .C("mandelbrot",
    xcoo = as.double(xcoo),
    ycoo = as.double(ycoo),
    nx = as.integer(nx),
    ny = as.integer(ny),
    set = as.integer(set),
    iter = as.integer(iter))$set

  # Create a list with elements x, y and z,
  # suitable for image(), persp(), etc. and return it.
  return(list(x = xcoo, y = ycoo, z = matrix(the.set, ncol = ny, byrow = T)));
}
# END OF R CODE. #######################################################


/* BEGINNING OF C CODE: ***********************************/

#include <stdio.h>

/***********************************************************
 *  Function to evaluate a series of complex numbers that  *
 *  form a subset of the Argand plane, to see if           *
 *  they belong to the Mandelbrot set.                     *
 *  This function has been written with the intention of   *
 *  linking it to some R code in order to create a         *
 *  practical way to play with the set.                    *
 *                                                         *
 *  Written by Mario dos Reis. September 2003              *
 *                                                         *
 ***********************************************************/

void mandelbrot(double *xcoo, double *ycoo, int *nx, 
	     int *ny, int *set, int *iter)

/* 'xcoo' and 'ycoo' are the x and y coordinates respectively
 *  of all the points to be evaluated.
 * 'nx' and 'ny' number of divisions along the x and y axes
 * 'set' will store the practical output of the function
 * 'iter' is the maximun number of iterations
 */

{
    int i, j, k;
    double z[2], c[2], oldz[2];

    for(i = 0; i < *nx; i++) {
	
	for(j = 0; j < *ny; j++) {
	  
	    /* initialise the complex point to be tested
	     * c[0] (or z[0]) is the real part
	     * c[1] (or z[1]) is the imaginary part
	     */
	    c[0] = xcoo[i]; c[1] = ycoo[j];
	    z[0] = 0;       z[1] = 0;
	    
	    for(k = 1; k < *iter + 1; k++) {

		oldz[0] = z[0]; oldz[1] = z[1];

		// the mandelbrot mapping z -> z^2 + c
		z[0] = oldz[0]*oldz[0] - oldz[1]*oldz[1] + c[0];
		z[1] = 2 * oldz[0]*oldz[1] + c[1];

		if((z[0]*z[0] + z[1]*z[1]) > 4) {
		    
		    break;
		}
	    }

	    /* fills the set vector
	     * notice the trick 'i * (*ny) + j' to find
	     * the apporpiate position of the output in the 
	     * vector set. The R function will take care of
	     * transforming this vector into a suitable matrix
	     * for plotting, etc.
	     */
	    set[i * (*ny) + j] = k;
	}
    }
    return;
}
/* END OF C CODE. ******************************************

To anyone interested in trying out these functions, just save the C code
into a file, say 'mandelbrot.c', to be able to use the C code with R a
shared object needs to be created. This is done simple running the
following command from the directory where 'mandelbrot.c' is stored (note
this code was tried in Linux, for Win users I think the instructions are
similar but you need you check it out):

R CMD SHLIB mandelbrot.c

This will create the file 'mandelbrot.so' in that same directory.

Start R from this directory and paste the above R code into the console. Type

> dyn.load("mandelbrot.so")

in order to load the shared object. Now you are ready to appreaciate in
full detail the set.

> image(mandelbrot(), col = c(heat.colors(49), "black"))

This function is many orders of magnitude faster than the one I had
published previously
(see http://maths.newcastle.edu.au/~rking/R/help/03b/3390.html) 
that function also had a slight error, it tested  (z[1] + z[2])^2 > 4
instead of z[1]^2 + z[2]^2 > 4 which creates the asymmetric result you
pointed out in the outer regions of the set. The C code is also at
extremely fast with high zooms and iteratons, try:

xcoo = c(-1.1854735004165451, -1.1854647240344973)
ycoo = c(-0.3057632375298113, -0.3057544611477636)

frac <- mandelbrot(x = xcoo, y = ycoo, iter = 2000) # took ~ 12s in P4 2.2GHz
image(frac, col = c(heat.colors(49), "black"))

This is at least 19x faster than the code you suggested me. (same settings
took ~ 230s in the same machine).

Regards,
Mario.

At 10:21 18/09/03 +0200, you wrote:
>
>    Mario> Well, I started playing with fractals in R, and wrote
>    Mario> a function to generate de Mandelbrot set, which might
>    Mario> be of interest to some people
>
>    Mario>
###################################################################
>    Mario> # Mandelbrot set
>    Mario>
###################################################################
>
>    Mario> mandelbrot <- function(x = c(-3.0, 1.0),   # x coordinates
>    Mario>                        y = c(-1.8, 1.8),   # y coordinates
>    Mario>                        b = .05,            # by 'steps'
>    Mario>                        iter = 20)          # maximum number of
iterations
>
>    <..................>
>    <..................>
>    <..................>
>
>Well, only a bit more than year ago I had posted my version of
>mandelbrot() and a drawing function, see
>
>   http://finzi.psych.upenn.edu/R/Rhelp02a/archive/5898.html
>
>   [ I have a slightly updated version of the R code, that is now
>     available as ftp://stat.ethz.ch/U/maechler/R/Mandelbrot.R
>   ]
>
>which is an order of magnitude more efficient than yours (22 x
>faster for your "b = 0.01", *1), *2)
>and with an iterImage() function for drawing its result in
>several (simple) color schemes.
>I agree that the proper solution would *definitely* use C code!
>
>Your idea of using persp() -- while seen frequently in fractal books,
>is new ``for R'' however, and nice!  Thank you.
>
>
>Regards,
>Martin
>
>*1) mainly because I only have the iteration loop
>    and do the rest vectorized, but also because I have a check
>    for symmetry and make use of it when appropriate
>
>*2) For speed comparison, note that I use 10 x more iterations
>    and a much higher resolution by default
>
>*3) When I do the comparison, I see that your function's result
>    slightly differs from mine -- not for the Mandelbrot set
>    itself, but for the very ``outer points'' (the dark orange
>    ones in the image plot). Your result is
>    asymmetric and hence can't be quite correct.
>
>



From dj at research.bell-labs.com  Wed Oct  1 15:24:47 2003
From: dj at research.bell-labs.com (David James)
Date: Wed, 1 Oct 2003 09:24:47 -0400
Subject: [R] installing DBI_0.1-6.tar.gz
In-Reply-To: <1064995769.31972.8.camel@miyagikyo.esat.kuleuven.ac.be>;
	from Joke.Allemeersch@esat.kuleuven.ac.be on Wed, Oct 01, 2003 at
	10:09:29AM +0200
References: <1064995769.31972.8.camel@miyagikyo.esat.kuleuven.ac.be>
Message-ID: <20031001092447.B7169@jessie.research.bell-labs.com>

Hi,

I've never seen R core dumping during a package installation,
if indeed the "1857 Segmentation Fault" message applies to R.
Could you give us more details (OS, etc.)?  Also, have you had
problems with any other packages or is this the only one you're
having problems with, etc.?

Regards,

--
David

Joke Allemeersch wrote:
> Dear,
> 
> I tried to install the DBI package in R-1.7.1, but this gave the
> following error:
> 
> 
>         /volume1/scratch/jallemee/R/lib/R/bin/INSTALL: line 1:  1856
>         Done                    ( echo
>         "options(save.image.defaults=${save_image_defaults})"; if test
>         -s R_PROFILE.R; then
>             cat R_PROFILE.R;
>         fi; echo "invisible(.libPaths(c(\"${lib}\", .libPaths())))"; cat
>         "${R_PACKAGE_DIR}/R/${pkg}" )
>               1857 Segmentation fault      | ${R_EXE} --slave --save
>         ${save_args}
>         ERROR: execution of package source for 'DBI' failed
> 
> Can somebody help me?  
> Thanks a lot,
> 
> Kind regards,
> Joke.
> 
> 
> Joke Allemeersch 
> Kasteelpark Arenberg 10 
> 3001 Heverlee (Leuven) 
> http://www.esat.kuleuven.ac.be/~dna/BioI/
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help

-- 
David A. James
Statistics Research, Room 2C-253            Phone:  (908) 582-3082       
Bell Labs, Lucent Technologies              Fax:    (908) 582-3340
Murray Hill, NJ 09794-0636



From tblackw at umich.edu  Wed Oct  1 16:08:16 2003
From: tblackw at umich.edu (Thomas W Blackwell)
Date: Wed, 1 Oct 2003 10:08:16 -0400 (EDT)
Subject: [R] Solving a tridiagonal system
In-Reply-To: <20031001125634.99679.qmail@web12903.mail.yahoo.com>
References: <20031001125634.99679.qmail@web12903.mail.yahoo.com>
Message-ID: <Pine.SOL.4.58.0310011005450.13984@tetris.gpcc.itd.umich.edu>

Will  -

Take a look at Roger Koenker's package  SparseMatrix,
available from CRAN.  Look also for some other package
from Roger which depends on SparseMatrix, but has a
different name.  It's a place to look.  I don't recall
whether it will answer your need or not.

-  tom blackwell  -  u michigan medical school  -  ann arbor  -

On Wed, 1 Oct 2003, Will Harvey wrote:

> I need to find solutions to a tridiagonal system. By
> this I mean a set of linear equations Ax = d where A
> is a square matrix containing elements A[i,i-1],
> A[i,i] and A[i,i+1] for i in 1:nrow, and zero
> elsewhere. R is probably not the ideal way to do this,
> but this is part of a larger problem that requires R.
>
> In my application it is much easier (and much faster)
> to generate the diagonal and off-diagonal elements of
> A as vectors, i.e. a = A[i,i-1], b = A[i,i] and c =
> A[i,i+1]. So I have three vectors that define A, along
> with a solution vector d. The conventional method of
> solving such systems is to use the so-called "Thomas
> algorithm", see e.g.
> <http://www.enseeiht.fr/hmf/travaux/CD0001/travaux/optmfn/hi/01pa/hyb74/node24.html>.
> This is very easy to code, but much more difficult to
> "vectorize". Is anyone aware of a library that
> contains a fast implementation of this algorithm?
>
> Another alternative is to use backsolve. I can easily
> eliminate the lower diagonal a, but I'm still left
> with b and c, whereas backsolve requires a matrix.
> Again, I can write a function to read b and c into a
> matrix, but this requires loops, and is too slow. Is
> there a vectorized way of doing it? Of course, the
> diag command works for b, but what about c? In Octave,
> diag allows for an offset, but R apparently does not.
>
> I would appreciate any and all assistance you experts
> can offer. Thanks in advance.
>
> Will Harvey
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
>



From jrgonzalez at ico.scs.es  Wed Oct  1 16:17:42 2003
From: jrgonzalez at ico.scs.es (Juan Ramon Gonzalez)
Date: Wed, 1 Oct 2003 16:17:42 +0200
Subject: [R] Acces violation ???
Message-ID: <000701c38826$c72f3380$1100a8c0@ico.scs.es>

Dear R-listers,

I have created a "dll" and I call it from a R function. However R stops and
shows the message:

"Unhandled exception in Rgui.exe (R.DLL): 0xC0000005: Access Violation"

I get this error in other situations and I solved it verifying that all
parameters are called fine. (e.g., double precision -> as.double, integer ->
as.integer,...). I have verify this. Then, I have tried to debug Fortran
program in order to verify why (or where) function doesn't work.
Surprisingly the subroutine arrives until the end.

What is happening?

Thank you in advance

Juan



From roger at ysidro.econ.uiuc.edu  Wed Oct  1 16:32:41 2003
From: roger at ysidro.econ.uiuc.edu (Roger Koenker)
Date: Wed, 1 Oct 2003 09:32:41 -0500 (CDT)
Subject: [R] Solving a tridiagonal system
In-Reply-To: <Pine.SOL.4.58.0310011005450.13984@tetris.gpcc.itd.umich.edu>
Message-ID: <Pine.SOL.4.30.0310010931240.18694-100000@ysidro.econ.uiuc.edu>

SparseM is really intended for arbitrary sparse structure,
for banded structural there are much more efficient methods,
some of which are, if I'm not mistaken, now available in lapack.


url:	www.econ.uiuc.edu/~roger/my.html	Roger Koenker
email	rkoenker at uiuc.edu			Department of Economics
vox: 	217-333-4558				University of Illinois
fax:   	217-244-6678				Champaign, IL 61820

On Wed, 1 Oct 2003, Thomas W Blackwell wrote:

> Will  -
>
> Take a look at Roger Koenker's package  SparseMatrix,
> available from CRAN.  Look also for some other package
> from Roger which depends on SparseMatrix, but has a
> different name.  It's a place to look.  I don't recall
> whether it will answer your need or not.
>
> -  tom blackwell  -  u michigan medical school  -  ann arbor  -
>
> On Wed, 1 Oct 2003, Will Harvey wrote:
>
> > I need to find solutions to a tridiagonal system. By
> > this I mean a set of linear equations Ax = d where A
> > is a square matrix containing elements A[i,i-1],
> > A[i,i] and A[i,i+1] for i in 1:nrow, and zero
> > elsewhere. R is probably not the ideal way to do this,
> > but this is part of a larger problem that requires R.
> >
> > In my application it is much easier (and much faster)
> > to generate the diagonal and off-diagonal elements of
> > A as vectors, i.e. a = A[i,i-1], b = A[i,i] and c =
> > A[i,i+1]. So I have three vectors that define A, along
> > with a solution vector d. The conventional method of
> > solving such systems is to use the so-called "Thomas
> > algorithm", see e.g.
> > <http://www.enseeiht.fr/hmf/travaux/CD0001/travaux/optmfn/hi/01pa/hyb74/node24.html>.
> > This is very easy to code, but much more difficult to
> > "vectorize". Is anyone aware of a library that
> > contains a fast implementation of this algorithm?
> >
> > Another alternative is to use backsolve. I can easily
> > eliminate the lower diagonal a, but I'm still left
> > with b and c, whereas backsolve requires a matrix.
> > Again, I can write a function to read b and c into a
> > matrix, but this requires loops, and is too slow. Is
> > there a vectorized way of doing it? Of course, the
> > diag command works for b, but what about c? In Octave,
> > diag allows for an offset, but R apparently does not.
> >
> > I would appreciate any and all assistance you experts
> > can offer. Thanks in advance.
> >
> > Will Harvey
> >
> > ______________________________________________
> > R-help at stat.math.ethz.ch mailing list
> > https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> >
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
>



From ggrothendieck at myway.com  Wed Oct  1 16:36:30 2003
From: ggrothendieck at myway.com (Gabor Grothendieck)
Date: Wed,  1 Oct 2003 10:36:30 -0400 (EDT)
Subject: [R] Solving a tridiagonal system
Message-ID: <20031001143630.3CA0C3A76@xmxpita.myway.com>


The following will create a matrix x with given sub diagonal,
diagonal and super diagonal.

# define test vectors for a, b and c
va <- -(1:4); vb <- 11:15; vc <- 1:4

# diag.num is a matrix whose ith super diagonal equals i and 
#  sub diagonal equals -i
diag.num <- -outer(seq(vb),seq(vb),"-")

x <- diag(vb)
x[diag.num == 1] <- va
x[diag.num == -1] <- vc

---

Date: Wed, 1 Oct 2003 05:56:34 -0700 (PDT) 
From: Will Harvey <will_harvey03 at yahoo.com>
To: <r-help at stat.math.ethz.ch> 
Subject: [R] Solving a tridiagonal system 
 
I need to find solutions to a tridiagonal system. By
this I mean a set of linear equations Ax = d where A
is a square matrix containing elements A[i,i-1],
A[i,i] and A[i,i+1] for i in 1:nrow, and zero
elsewhere. R is probably not the ideal way to do this,
but this is part of a larger problem that requires R.

In my application it is much easier (and much faster)
to generate the diagonal and off-diagonal elements of
A as vectors, i.e. a = A[i,i-1], b = A[i,i] and c =
A[i,i+1]. So I have three vectors that define A, along
with a solution vector d. The conventional method of
solving such systems is to use the so-called "Thomas
algorithm", see e.g. 
<http://www.enseeiht.fr/hmf/travaux/CD0001/travaux/optmfn/hi/01pa/hyb74/node24.html>;.
This is very easy to code, but much more difficult to
"vectorize". Is anyone aware of a library that
contains a fast implementation of this algorithm?

Another alternative is to use backsolve. I can easily
eliminate the lower diagonal a, but I'm still left
with b and c, whereas backsolve requires a matrix.
Again, I can write a function to read b and c into a
matrix, but this requires loops, and is too slow. Is
there a vectorized way of doing it? Of course, the
diag command works for b, but what about c? In Octave,
diag allows for an offset, but R apparently does not.

I would appreciate any and all assistance you experts
can offer. Thanks in advance.

Will Harvey

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://www.stat.math.ethz.ch/mailman/listinfo/r-help

 
 
 
 message or move to Select a Folder----- Folders ------InboxDraftsSentTrashBulk Mail---- My Folders ----R
 Back to Inbox | <Prev Next> 
 
   As AttachmentAs Inline Text 
 
 



    
 
 




Make My Way Your Home Page  |  Spread the Word

My Settings: Overview | Search | Email | Chat | Portfolio | Calendar | Groups | Profile 
 

IMPORTANT: We do not present our users with pop-ups, banners or any other non-text advertising. Nor do we send email
to our users. If you see or receive one of these items, it is coming from an outside source, either as a result of something you
have previously downloaded or as an "exit" pop-up from the site you just visited. It is not coming from our site.

Privacy Policy   Terms of Service   Partner with Us   Our Mission   Advertise with Us   Sign In   Sign Out   Help Center

© 2002-2003 My Way

 
 

  
 

        

_______________________________________________
No banners. No pop-ups. No kidding.
Introducing My Way - http://www.myway.com



From ggrothendieck at myway.com  Wed Oct  1 16:45:11 2003
From: ggrothendieck at myway.com (Gabor Grothendieck)
Date: Wed,  1 Oct 2003 10:45:11 -0400 (EDT)
Subject: [R] Solving a tridiagonal system
Message-ID: <20031001144511.4211E3A8A@xmxpita.myway.com>


I just noticed that you defined a to be the lower diagonal
whereas I had it as the upper diagonal so the previous email
should be as follows to correspond to your notation:


# define test vectors for a, b and c
va <- -(1:4); vb <- 11:15; vc <- 1:4

# diag.num is a matrix whose ith diagonal equals i
diag.num <- -outer(seq(vb),seq(vb),"-")

x <- diag(vb)
x[diag.num == -1] <- va
x[diag.num == 1] <- vc


From: Gabor Grothendieck <ggrothendieck at myway.com>
To: <will_harvey03 at yahoo.com>, <r-help at stat.math.ethz.ch> 
Subject: RE: [R] Solving a tridiagonal system 

 
 

The following will create a matrix x with given sub diagonal,
diagonal and super diagonal.

# define test vectors for a, b and c
va <- -(1:4); vb <- 11:15; vc <- 1:4

# diag.num is a matrix whose ith super diagonal equals i and 
# sub diagonal equals -i
diag.num <- -outer(seq(vb),seq(vb),"-")

x <- diag(vb)
x[diag.num == 1] <- va
x[diag.num == -1] <- vc

---

Date: Wed, 1 Oct 2003 05:56:34 -0700 (PDT) 
From: Will Harvey <will_harvey03 at yahoo.com>
To: <r-help at stat.math.ethz.ch> 
Subject: [R] Solving a tridiagonal system 

I need to find solutions to a tridiagonal system. By
this I mean a set of linear equations Ax = d where A
is a square matrix containing elements A[i,i-1],
A[i,i] and A[i,i+1] for i in 1:nrow, and zero
elsewhere. R is probably not the ideal way to do this,
but this is part of a larger problem that requires R.

In my application it is much easier (and much faster)
to generate the diagonal and off-diagonal elements of
A as vectors, i.e. a = A[i,i-1], b = A[i,i] and c =
A[i,i+1]. So I have three vectors that define A, along
with a solution vector d. The conventional method of
solving such systems is to use the so-called "Thomas
algorithm", see e.g. 
<http://www.enseeiht.fr/hmf/travaux/CD0001/travaux/optmfn/hi/01pa/hyb74/node24.html>;;.
This is very easy to code, but much more difficult to
"vectorize". Is anyone aware of a library that
contains a fast implementation of this algorithm?

Another alternative is to use backsolve. I can easily
eliminate the lower diagonal a, but I'm still left
with b and c, whereas backsolve requires a matrix.
Again, I can write a function to read b and c into a
matrix, but this requires loops, and is too slow. Is
there a vectorized way of doing it? Of course, the
diag command works for b, but what about c? In Octave,
diag allows for an offset, but R apparently does not.

I would appreciate any and all assistance you experts
can offer. Thanks in advance.

Will Harvey

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://www.stat.math.ethz.ch/mailman/listinfo/r-help




message or move to Select a Folder----- Folders ------InboxDraftsSentTrashBulk Mail---- My Folders ----R
Back to Inbox | <Prev Next> 

As AttachmentAs Inline Text 












Make My Way Your Home Page | Spread the Word

My Settings: Overview | Search | Email | Chat | Portfolio | Calendar | Groups | Profile 


IMPORTANT: We do not present our users with pop-ups, banners or any other non-text advertising. Nor do we send email
to our users. If you see or receive one of these items, it is coming from an outside source, either as a result of something you
have previously downloaded or as an "exit" pop-up from the site you just visited. It is not coming from our site.

Privacy Policy Terms of Service Partner with Us Our Mission Advertise with Us Sign In Sign Out Help Center

© 2002-2003 My Way









_______________________________________________
No banners. No pop-ups. No kidding.
Introducing My Way - http://www.myway.com

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://www.stat.math.ethz.ch/mailman/listinfo/r-help

 
 
 
 message or move to Select a Folder----- Folders ------InboxDraftsSentTrashBulk Mail---- My Folders ----R
 Back to Inbox | <Prev Next> 
 
   As AttachmentAs Inline Text 
 
 



    
 
 




Make My Way Your Home Page  |  Spread the Word

My Settings: Overview | Search | Email | Chat | Portfolio | Calendar | Groups | Profile 
 

IMPORTANT: We do not present our users with pop-ups, banners or any other non-text advertising. Nor do we send email
to our users. If you see or receive one of these items, it is coming from an outside source, either as a result of something you
have previously downloaded or as an "exit" pop-up from the site you just visited. It is not coming from our site.

Privacy Policy   Terms of Service   Partner with Us   Our Mission   Advertise with Us   Sign In   Sign Out   Help Center

© 2002-2003 My Way

 
 

  
 

        

_______________________________________________
No banners. No pop-ups. No kidding.
Introducing My Way - http://www.myway.com



From andy_liaw at merck.com  Wed Oct  1 16:54:17 2003
From: andy_liaw at merck.com (Liaw, Andy)
Date: Wed, 01 Oct 2003 10:54:17 -0400
Subject: [R] Solving a tridiagonal system
Message-ID: <3A822319EB35174CA3714066D590DCD50205CBEE@usrymx25.merck.com>

I see that the LAPACK routine DGTSV is in the R source, and defined in
R_ext/lapack.h,  but I don't know how to get to it from R.

Andy

> -----Original Message-----
> From: Roger Koenker [mailto:roger at ysidro.econ.uiuc.edu] 
> Sent: Wednesday, October 01, 2003 10:33 AM
> To: Thomas W Blackwell
> Cc: r-help at stat.math.ethz.ch
> Subject: Re: [R] Solving a tridiagonal system
> 
> 
> SparseM is really intended for arbitrary sparse structure,
> for banded structural there are much more efficient methods, 
> some of which are, if I'm not mistaken, now available in lapack.
> 
> 
> url:	www.econ.uiuc.edu/~roger/my.html	Roger Koenker
> email	rkoenker at uiuc.edu			Department of Economics
> vox: 	217-333-4558				University of Illinois
> fax:   	217-244-6678				
> Champaign, IL 61820
> 
> On Wed, 1 Oct 2003, Thomas W Blackwell wrote:
> 
> > Will  -
> >
> > Take a look at Roger Koenker's package  SparseMatrix, 
> available from 
> > CRAN.  Look also for some other package from Roger which depends on 
> > SparseMatrix, but has a different name.  It's a place to look.  I 
> > don't recall whether it will answer your need or not.
> >
> > -  tom blackwell  -  u michigan medical school  -  ann arbor  -
> >
> > On Wed, 1 Oct 2003, Will Harvey wrote:
> >
> > > I need to find solutions to a tridiagonal system. By
> > > this I mean a set of linear equations Ax = d where A
> > > is a square matrix containing elements A[i,i-1],
> > > A[i,i] and A[i,i+1] for i in 1:nrow, and zero
> > > elsewhere. R is probably not the ideal way to do this,
> > > but this is part of a larger problem that requires R.
> > >
> > > In my application it is much easier (and much faster)
> > > to generate the diagonal and off-diagonal elements of
> > > A as vectors, i.e. a = A[i,i-1], b = A[i,i] and c = 
> A[i,i+1]. So I 
> > > have three vectors that define A, along with a solution vector d. 
> > > The conventional method of solving such systems is to use the 
> > > so-called "Thomas algorithm", see e.g.
> > > 
> <http://www.enseeiht.fr/hmf/travaux/CD0001/travaux/optmfn/hi/0
1pa/hyb74/node24.html>.
> > This is very easy to code, but much more difficult to
> > "vectorize". Is anyone aware of a library that
> > contains a fast implementation of this algorithm?
> >
> > Another alternative is to use backsolve. I can easily eliminate the 
> > lower diagonal a, but I'm still left with b and c, whereas backsolve 
> > requires a matrix. Again, I can write a function to read b and c 
> > into a matrix, but this requires loops, and is too slow. Is
> > there a vectorized way of doing it? Of course, the
> > diag command works for b, but what about c? In Octave,
> > diag allows for an offset, but R apparently does not.
> >
> > I would appreciate any and all assistance you experts
> > can offer. Thanks in advance.
> >
> > Will Harvey
> >
> > ______________________________________________
> > R-help at stat.math.ethz.ch mailing list 
> > https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> >
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list 
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
>

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://www.stat.math.ethz.ch/mailman/listinfo/r-help



From will_harvey03 at yahoo.com  Wed Oct  1 16:56:55 2003
From: will_harvey03 at yahoo.com (Will Harvey)
Date: Wed, 1 Oct 2003 07:56:55 -0700 (PDT)
Subject: [R] Solving a tridiagonal system
In-Reply-To: <20031001144511.4211E3A8A@xmxpita.myway.com>
Message-ID: <20031001145655.91179.qmail@web12902.mail.yahoo.com>

Thanks. This works, but still seems too slow for my
application. It looks like my best option is to figure
out how to interface the LAPACK routine DGTSV to R.
The inputs to DGTSV are four vectors: the three
diagonals and the RHS vector.

Will


--- Gabor Grothendieck <ggrothendieck at myway.com>
wrote:
> 
> I just noticed that you defined a to be the lower
> diagonal
> whereas I had it as the upper diagonal so the
> previous email
> should be as follows to correspond to your notation:
> 
> 
> # define test vectors for a, b and c
> va <- -(1:4); vb <- 11:15; vc <- 1:4
> 
> # diag.num is a matrix whose ith diagonal equals i
> diag.num <- -outer(seq(vb),seq(vb),"-")
> 
> x <- diag(vb)
> x[diag.num == -1] <- va
> x[diag.num == 1] <- vc
> 
> 
> From: Gabor Grothendieck <ggrothendieck at myway.com>
> To: <will_harvey03 at yahoo.com>,
> <r-help at stat.math.ethz.ch> 
> Subject: RE: [R] Solving a tridiagonal system 
> 
>  
>  
> 
> The following will create a matrix x with given sub
> diagonal,
> diagonal and super diagonal.
> 
> # define test vectors for a, b and c
> va <- -(1:4); vb <- 11:15; vc <- 1:4
> 
> # diag.num is a matrix whose ith super diagonal
> equals i and 
> # sub diagonal equals -i
> diag.num <- -outer(seq(vb),seq(vb),"-")
> 
> x <- diag(vb)
> x[diag.num == 1] <- va
> x[diag.num == -1] <- vc
> 
> ---
> 
> Date: Wed, 1 Oct 2003 05:56:34 -0700 (PDT) 
> From: Will Harvey <will_harvey03 at yahoo.com>
> To: <r-help at stat.math.ethz.ch> 
> Subject: [R] Solving a tridiagonal system 
> 
> I need to find solutions to a tridiagonal system. By
> this I mean a set of linear equations Ax = d where A
> is a square matrix containing elements A[i,i-1],
> A[i,i] and A[i,i+1] for i in 1:nrow, and zero
> elsewhere. R is probably not the ideal way to do
> this,
> but this is part of a larger problem that requires
> R.
> 
> In my application it is much easier (and much
> faster)
> to generate the diagonal and off-diagonal elements
> of
> A as vectors, i.e. a = A[i,i-1], b = A[i,i] and c =
> A[i,i+1]. So I have three vectors that define A,
> along
> with a solution vector d. The conventional method of
> solving such systems is to use the so-called "Thomas
> algorithm", see e.g. 
>
<http://www.enseeiht.fr/hmf/travaux/CD0001/travaux/optmfn/hi/01pa/hyb74/node24.html>;;.
> This is very easy to code, but much more difficult
> to
> "vectorize". Is anyone aware of a library that
> contains a fast implementation of this algorithm?
> 
> Another alternative is to use backsolve. I can
> easily
> eliminate the lower diagonal a, but I'm still left
> with b and c, whereas backsolve requires a matrix.
> Again, I can write a function to read b and c into a
> matrix, but this requires loops, and is too slow. Is
> there a vectorized way of doing it? Of course, the
> diag command works for b, but what about c? In
> Octave,
> diag allows for an offset, but R apparently does
> not.
> 
> I would appreciate any and all assistance you
> experts
> can offer. Thanks in advance.
> 
> Will Harvey
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
>
https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> 
> 
> 
> 
> message or move to Select a Folder----- Folders
> ------InboxDraftsSentTrashBulk Mail---- My Folders
> ----R
> Back to Inbox | <Prev Next> 
> 
> As AttachmentAs Inline Text 
> 
> 
> 
> 
> 
> 
> 
> 
> 
> 
> 
> 
> Make My Way Your Home Page | Spread the Word
> 
> My Settings: Overview | Search | Email | Chat |
> Portfolio | Calendar | Groups | Profile 
> 
> 
> IMPORTANT: We do not present our users with pop-ups,
> banners or any other non-text advertising. Nor do we
> send email
> to our users. If you see or receive one of these
> items, it is coming from an outside source, either
> as a result of something you
> have previously downloaded or as an "exit" pop-up
> from the site you just visited. It is not coming
> from our site.
> 
> Privacy Policy Terms of Service Partner with Us Our
> Mission Advertise with Us Sign In Sign Out Help
> Center
> 
> © 2002-2003 My Way
> 
> 
> 
> 
> 
> 
> 
> 
> 
> _______________________________________________
> No banners. No pop-ups. No kidding.
> Introducing My Way - http://www.myway.com
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
>
https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> 
>  
>  
>  
>  message or move to Select a Folder----- Folders
> ------InboxDraftsSentTrashBulk Mail---- My Folders
> ----R
>  Back to Inbox | <Prev Next> 
>  
>    As AttachmentAs Inline Text 
>  
>  
> 
> 
> 
>     
>  
>  
> 
> 
> 
> 
> Make My Way Your Home Page  |  Spread the Word
> 
> My Settings: Overview | Search | Email | Chat |
> Portfolio | Calendar | Groups | Profile 
>  
> 
> IMPORTANT: We do not present our users with pop-ups,
> banners or any other non-text advertising. Nor do we
> send email
> to our users. If you see or receive one of these
> items, it is coming from an outside source, either
> as a result of something you
> have previously downloaded or as an "exit" pop-up
> from the site you just visited. It is not coming
> from our site.
> 
> Privacy Policy   Terms of Service   Partner with Us 
>  Our Mission   Advertise with Us   Sign In   Sign
> Out   Help Center
> 
> © 2002-2003 My Way
> 
>  
>  
> 
=== message truncated ===



From tpapp at axelero.hu  Wed Oct  1 17:14:51 2003
From: tpapp at axelero.hu (Tamas Papp)
Date: Wed, 1 Oct 2003 17:14:51 +0200
Subject: [R] fitting Markov chains
Message-ID: <20031001151451.GA9660@localhost>

I need to find a computationally simple process for the movement of
interest rates. In this simplified model, an interest rate can have
3--5 possible values, and its movement is characterized by a matrix of
transition probabilities (ie, it is a Markov process).

I would like to estimate this process from a given set of data. 

For example, let the interest rate time series be:

7 3 8 2 5 9 6

Assume that the discretized variable can take the following values:
(3, 5, 8), then we find the nearest discrete point and give its index:

3 1 3 1 2 3 2

Then estimate the transition probabilities.

I have the following questions:

- how should I select the discrete set of values that the variable can
assume? Eg simply get the maximum and minimum, and divide this
interval into, say, three pieces? Or estimate the mean, and make the
other two values mean plus-minus one standard deviation?

- once the variable is discretized, how do I transform each data point
to its discretized value (its index)?

- the most important: how should I estimate the transition
probabilities?

References to introductory literature on estimating Markov chains like
this would be welcome. Most importantly, I need to know how robust an
estimation is to selecting the discrete points, or is there a simple
"goodness of fit" estimation.

Thanks,

Tamas Papp

-- 
Tam?s K. Papp
E-mail: tpapp at axelero.hu (preferred, especially for large messages)
        tpapp at westel900.net
Please try to send only (latin-2) plain text, not HTML or other garbage.



From dmurdoch at pair.com  Wed Oct  1 17:24:44 2003
From: dmurdoch at pair.com (Duncan Murdoch)
Date: Wed, 01 Oct 2003 11:24:44 -0400
Subject: [R] Acces violation ???
In-Reply-To: <000701c38826$c72f3380$1100a8c0@ico.scs.es>
References: <000701c38826$c72f3380$1100a8c0@ico.scs.es>
Message-ID: <a8slnv88fb6c69hlgnc9n8re7q4ahkfdhg@4ax.com>

On Wed, 1 Oct 2003 16:17:42 +0200, "Juan Ramon Gonzalez"
<jrgonzalez at ico.scs.es> wrote :

>Dear R-listers,
>
>I have created a "dll" and I call it from a R function. However R stops and
>shows the message:
>
>"Unhandled exception in Rgui.exe (R.DLL): 0xC0000005: Access Violation"
>
>I get this error in other situations and I solved it verifying that all
>parameters are called fine. (e.g., double precision -> as.double, integer ->
>as.integer,...). I have verify this. Then, I have tried to debug Fortran
>program in order to verify why (or where) function doesn't work.
>Surprisingly the subroutine arrives until the end.
>
>What is happening?

Most likely it's a clash of calling conventions.  R uses the "cdecl"
calling convention, which means it expects to clean up the stack after
a call.  If your Fortran uses the more common "stdcall" convention, it
will have already removed parameters from the stack.  Removing things
twice is bad.

I've got a web page
<http://www.stats.uwo.ca/faculty/murdoch/software/compilingDLLs/> that
gives some advice on writing DLLs for R.  It also gives pointers to
the existing documentation in the R manuals, etc.

Duncan Murdoch



From carlapiccini at hotmail.com  Wed Oct  1 17:25:24 2003
From: carlapiccini at hotmail.com (carla piccini)
Date: Wed, 01 Oct 2003 15:25:24 +0000
Subject: [R] SOS Cor
Message-ID: <BAY9-F62O9vXLsdAefp0000d5e5@hotmail.com>


   Ho iniziato ad utilizzare R da pochi giorni e onestamente ho delle
   difficolt?.
   Devo correlare una variabile con il resto del dataframe (quindi una
   variabile con tutte), ma sbaglio nella sintassi e onestamente non so
   dove. Le singole correlazioni riesco a farle, ma ? un lavoro un p?
   dispersivo.
   Mi hanno consigliato di utilizzare cbind oppure as.matrix, ma non so
   continuare.
   Il problema seguente ? cercare di correlare una variabile con altre
   che sono io a scegliere per motivi di ricerca, ma anche qui non so
   come selezionare le colonne che a me interessano (non sono intervalli,
   ma sono disposte a salti).
   Qualcuno mi pu? aiutare? GRAZIE CARLA.
     _________________________________________________________________

   Nuovo MSN Messenger 6.0: con sfondi, foto e giochi. [1]Provalo subito!

References

   1. http://g.msn.com/8HMAITIT/2728??PS=


From hodgess at gator.dt.uh.edu  Wed Oct  1 17:45:19 2003
From: hodgess at gator.dt.uh.edu (Erin Hodgess)
Date: Wed, 1 Oct 2003 10:45:19 -0500
Subject: [R] permission problem on R install
Message-ID: <200310011545.h91FjJB11899@gator.dt.uh.edu>

Dear R People:

Here is the exact sequence of events:

Making package base

adding build stamp to DESCRIPTION

C:/rsource/R-1.7.1/bin/rterm.exe: permission denied

MAKE[2]:***[C:/rsource/R-1.7.1/library/base/zzzz] Error 126

MAKE[1]:***[pkg-base] Error 2

MAKE:***[rpackage] Error 2


Also, I'm using version 2.0.0-3.exe of Mingw.

I tried the most recent version, and it got hung up because
of changes in the type sizes.

This doesn't make sense, does it?

Thanks,
Erin

mailto: hodgess at gator.uhd.edu



From kjetil at entelnet.bo  Wed Oct  1 18:19:29 2003
From: kjetil at entelnet.bo (kjetil brinchmann halvorsen)
Date: Wed, 01 Oct 2003 12:19:29 -0400
Subject: [R] fitting Markov chains
In-Reply-To: <20031001151451.GA9660@localhost>
Message-ID: <3F7AC651.9163.FD92BB@localhost>

On 1 Oct 2003 at 17:14, Tamas Papp wrote:

> I need to find a computationally simple process for the movement of
> interest rates. In this simplified model, an interest rate can have
> 3--5 possible values, and its movement is characterized by a matrix of
> transition probabilities (ie, it is a Markov process).
> 
> I would like to estimate this process from a given set of data. 
> 
> For example, let the interest rate time series be:
> 
> 7 3 8 2 5 9 6
> 
> Assume that the discretized variable can take the following values:
> (3, 5, 8), then we find the nearest discrete point and give its index:
> 
> 3 1 3 1 2 3 2
> 
> Then estimate the transition probabilities.
> 
> I have the following questions:
> 
> - how should I select the discrete set of values that the variable can
> assume? Eg simply get the maximum and minimum, and divide this
> interval into, say, three pieces? Or estimate the mean, and make the
> other two values mean plus-minus one standard deviation?
> 

?Try with simulation what is best, after yuou have solved the 
estimation problem?

> - once the variable is discretized, how do I transform each data point
> to its discretized value (its index)?
> 
> - the most important: how should I estimate the transition
> probabilities?
> 
> References to introductory literature on estimating Markov chains like
> this would be welcome. 

A good reference covering this is U. Narayan Bhat: "Elements of 
Applied Stochastic Processes". For an almost ML solution, condition 
on the  first observation, then take each transition i -> somewhere 
as an observation from the multinomial distribution given by the i'th 
row of the transition matrix, and use the multinomial estimates. 

Kjetil Halvorsen


Most importantly, I need to know how robust an
> estimation is to selecting the discrete points, or is there a simple
> "goodness of fit" estimation.
> 
> Thanks,
> 
> Tamas Papp
> 
> -- 
> Tam?s K. Papp
> E-mail: tpapp at axelero.hu (preferred, especially for large messages)
>         tpapp at westel900.net
> Please try to send only (latin-2) plain text, not HTML or other garbage.
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help



From arrayprofile at yahoo.com  Wed Oct  1 18:41:26 2003
From: arrayprofile at yahoo.com (array chip)
Date: Wed, 1 Oct 2003 09:41:26 -0700 (PDT)
Subject: [R] lme vs. aov with Error term again
Message-ID: <20031001164126.59376.qmail@web41205.mail.yahoo.com>

Hi all,

Sent the following question yesterday, but haven't got
any suggestions yet. So just trying again, can anyone
comment on the problem that I have? Thank you!

-------------

Hi,

I have a question about using "lme" and "aov" for the
following dataset. If I understand correctly, using
"aov" with an Error term in the formula is equivalent
to using "lme" with default settings, i.e. both assume
compound symmetry correlation structure. And I have
found that equivalency in the past. However, with the
follwing dataset, I got different answers with using
"lme" and using "aov", can anyone explain what
happened here? I have 2 differnt response variables
"x" and "y" in the following dataset, they are
actually slightly different (only 3 values of them are
different). With "y", I achieved the equivalency
between "lme" and "aov"; but with "x", I got different
p values for the ANOVA table.

-------

x<-c(-0.0649,-0.0923,-0.0623,0.1809,0.0719,0.1017,0.0144,-0.1727,-0.1332,0.0986,0.304,-0.4093,0.2054,0.251,-0.1062,0.3833,0.0649,0.2908,0.1073,0.0919,0.1167,0.2369,0.306,0.1379)

y<-c(-0.0649,-0.0923,0.32,0.08,0.0719,0.1017,0.05,-0.1727,-0.1332,0.15,0.304,-0.4093,0.2054,0.251,-0.1062,0.3833,0.0649,0.2908,0.1073,0.0919,0.1167,0.2369,0.306,0.1379)

treat<-as.factor(c(1,1,1,1,1,1,1,1,1,1,1,1,2,2,2,2,2,2,2,2,2,2,2,2))
time<-as.factor(c(1,1,1,1,2,2,2,2,3,3,3,3,1,1,1,1,2,2,2,2,3,3,3,3))
sex<-as.factor(c('F','F','M','M','F','F','M','M','F','F','M','M','F','F','M','M','F','F','M','M','F','F','M','M'))
subject<-as.factor(c(rep(1:4,3),rep(5:8,3)))
xx<-cbind(x=data.frame(x),y=y,treat=treat,time=time,sex=sex,subject=subject)

######## using x as dependable variable

xx.lme<-lme(x~treat*sex*time,random=~1|subject,xx)
xx.aov<-aov(x~treat*sex*time+Error(subject),xx)

summary(xx.aov)

Error: subject
          Df   Sum Sq  Mean Sq F value  Pr(>F)  
treat      1 0.210769 0.210769  6.8933 0.05846 .
sex        1 0.005775 0.005775  0.1889 0.68627  
treat:sex  1 0.000587 0.000587  0.0192 0.89649  
Residuals  4 0.122304 0.030576                  
---
Signif. codes:  0 `***' 0.001 `**' 0.01 `*' 0.05 `.'
0.1 ` ' 1 

Error: Within
               Df  Sum Sq Mean Sq F value Pr(>F)
time            2 0.00102 0.00051  0.0109 0.9891
treat:time      2 0.00998 0.00499  0.1066 0.9002
sex:time        2 0.02525 0.01263  0.2696 0.7704
treat:sex:time  2 0.03239 0.01619  0.3458 0.7178
Residuals       8 0.37469 0.04684 

anova(xx.lme)
               numDF denDF  F-value p-value
(Intercept)        1     8 3.719117  0.0899
treat              1     4 5.089022  0.0871
sex                1     4 0.139445  0.7278
time               2     8 0.012365  0.9877
treat:sex          1     4 0.014175  0.9110
treat:time         2     8 0.120538  0.8880
sex:time           2     8 0.304878  0.7454
treat:sex:time     2     8 0.391012  0.6886

#### using y as dependable variable

xx.lme2<-lme(y~treat*sex*time,random=~1|subject,xx)
xx.aov2<-aov(y~treat*sex*time+Error(subject),xx)

summary(xx.aov2)

Error: subject
          Df   Sum Sq  Mean Sq F value Pr(>F)
treat      1 0.147376 0.147376  2.0665 0.2239
sex        1 0.000474 0.000474  0.0067 0.9389
treat:sex  1 0.006154 0.006154  0.0863 0.7836
Residuals  4 0.285268 0.071317               

Error: Within
               Df   Sum Sq  Mean Sq F value Pr(>F)
time            2 0.009140 0.004570  0.1579 0.8565
treat:time      2 0.012598 0.006299  0.2177 0.8090
sex:time        2 0.043132 0.021566  0.7453 0.5049
treat:sex:time  2 0.069733 0.034866  1.2050 0.3488
Residuals       8 0.231480 0.028935               

anova(xx.lme2)
               numDF denDF   F-value p-value
(Intercept)        1     8 3.0667809  0.1180
treat              1     4 2.0664919  0.2239
sex                1     4 0.0066516  0.9389
time               2     8 0.1579473  0.8565
treat:sex          1     4 0.0862850  0.7836
treat:time         2     8 0.2177028  0.8090
sex:time           2     8 0.7453185  0.5049
treat:sex:time     2     8 1.2049883  0.3488



From RBaskin at ahrq.gov  Wed Oct  1 18:33:25 2003
From: RBaskin at ahrq.gov (RBaskin@ahrq.gov)
Date: Wed, 1 Oct 2003 12:33:25 -0400 
Subject: [R] truncated multivariate normal 
Message-ID: <3598558AD728D41183350008C7CF291C0F16B94B@exchange1.ahrq.gov>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20031001/72a0fea6/attachment.pl

From aniko.szabo at hci.utah.edu  Wed Oct  1 18:42:16 2003
From: aniko.szabo at hci.utah.edu (Aniko Szabo)
Date: Wed, 1 Oct 2003 10:42:16 -0600
Subject: [R] Macintosh binaries; was: Rcmdr and Macintosh
Message-ID: <F062093E456F8C4A9566C5CA59726C6E2DA51C@EMAIL.hci.utah.edu>

I am a Windows user and trying to make life easier for my Mac-using students. After investigating the Mac situation more, it appears that the key to easy use is having precompiled binaries. However the packages I am interested in (Rcmdr and its required package, car) do not have one. I do not have easy access to a Mac, so I have a plea to Macintosh users: could someone please create them if it is not too difficult? I am planning to use R 1.7.1. Please forgive me if my request is inappropriate.

Aniko

This e-mail may contain confidential and/or privileged infor...{{dropped}}



From sudar_80 at neo.tamu.edu  Wed Oct  1 18:49:15 2003
From: sudar_80 at neo.tamu.edu (Padmanabhan, Sudharsha)
Date: Wed, 1 Oct 2003 16:49:15 -0000
Subject: [R] Power computation
Message-ID: <200310011649.h91GnFQu061099@smtp-relay.tamu.edu>

Hello,

Thanks everyone who replied to my earlier Q.

I ran into a problem. I am analysng multicenter clinical trials. The model is

Y=treatment + center + treatment*center. I have the effect sizes and the 
patient distributions for each scenario in the simulation.

Can someone tell me how to compute the power in this case? May be by using a 
F-test or something like that.

Thank you very much.

Krishna S P



From rxg218 at psu.edu  Wed Oct  1 18:55:03 2003
From: rxg218 at psu.edu (Rajarshi Guha)
Date: Wed, 01 Oct 2003 16:55:03 -0000
Subject: [R] question about predictions with linear models
Message-ID: <1065027555.3878.10.camel@ra.chem.psu.edu>

Hi,
  this question is probably very obvious but I just cant see where I
might be going wrong. 

I'm using the lm() function to generate a linear model and then make
predictions using a different set of data.

To generate the model I do (tdata & pdata are matrices of observations
and parameters, tdepv, pdepv are response vectors)

x <- as.data.frame(tdata)
x$tdepv <- tdepv
lnegth(tdepv) = 140

model <- lm(x$tdepv ~ x$V1 + x$V2 + x$V3 + x$V4, x)

pred <- predict(model, x)
length(pred) = 140

y <- as.data.frame(pdata)
y$pdepv <- pdepv
length(pdepv) = 16

pred <- predict(model, y)
length(pred) = 140

But I expect that length(pred) = 16

Why do I get a different length? Furthermore, the original formula
specified the variable tdepv which is not in the dataframe that I send
to predict() - should I also make a variable called tdepv in the
dataframe y?

Thanks,

-------------------------------------------------------------------
Rajarshi Guha <rxg218 at psu.edu> <http://jijo.cjb.net>
GPG Fingerprint: 0CCA 8EE2 2EEB 25E2 AB04 06F7 1BB9 E634 9B87 56EE
-------------------------------------------------------------------
The way to love anything is to realize that it might be lost.



From Ted.Harding at nessie.mcc.ac.uk  Wed Oct  1 18:49:21 2003
From: Ted.Harding at nessie.mcc.ac.uk ( (Ted Harding))
Date: Wed, 01 Oct 2003 17:49:21 +0100 (BST)
Subject: [R] fitting Markov chains
In-Reply-To: <20031001151451.GA9660@localhost>
Message-ID: <XFMail.031001174921.Ted.Harding@nessie.mcc.ac.uk>

On 01-Oct-03 Tamas Papp wrote:
> I need to find a computationally simple process for the movement of
> interest rates. In this simplified model, an interest rate can have
> 3--5 possible values, and its movement is characterized by a matrix of
> transition probabilities (ie, it is a Markov process).
> 
> I would like to estimate this process from a given set of data. 
> 
> For example, let the interest rate time series be:
> 7 3 8 2 5 9 6
> Assume that the discretized variable can take the following values:
> (3, 5, 8), then we find the nearest discrete point and give its index:
> 3 1 3 1 2 3 2
> Then estimate the transition probabilities.
> 
> I have the following questions:
> - how should I select the discrete set of values that the variable can
> assume? Eg simply get the maximum and minimum, and divide this
> interval into, say, three pieces? Or estimate the mean, and make the
> other two values mean plus-minus one standard deviation?

I would suggest dividing the interval, trying to get equal numbers
in each interval. If (e.g. because the originals are discrete)
this can't quite be done, try to find an similar arrangement which
maximises the minimum count.

> - once the variable is discretized, how do I transform each data point
> to its discretized value (its index)?

? Something like
 a<-c(0,1,2,3,4);b<-c(1,2,3,4,5)
 x<-3.5;which((a<x)&(x<=b))
 [1] 4
 x<-1.5;which((a<x)&(x<=b))
 [1] 2

> - the most important: how should I estimate the transition
> probabilities?

Most simply as Nij/Ni. where Ni. is the number of times the system
has been in state i, and Nij is the number of transitions from i to j.

Statistical properties of even this estimator can be a bit elusive
because the "sample sizes" Ni. are random; the above recommendation
about how to select intervals will probably help on this front.

Also, after grouping the system is no longer a Markov Chain (even
assuming it really was in the first place) since the probability
of transition from i to j will depend somewhat on which member within i
was arrived at, and this will depend (though hopefullly not much)
on where it was before. By refining your grouping you can diminish
this effect, though you will also reduce the Ni. and hence the
quality of estimation.

To discuss such issues one really need to know more about your data,
in particular how long the series are, what groupings are you thinking
of, and how the series behave.

Also, you don't seem to be thinking about trends, cycles etc. ...

Ted.


--------------------------------------------------------------------
E-Mail: (Ted Harding) <Ted.Harding at nessie.mcc.ac.uk>
Fax-to-email: +44 (0)870 167 1972
Date: 01-Oct-03                                       Time: 17:49:21
------------------------------ XFMail ------------------------------



From Jesse.Whittington at pc.gc.ca  Wed Oct  1 18:56:26 2003
From: Jesse.Whittington at pc.gc.ca (Jesse.Whittington@pc.gc.ca)
Date: Wed, 1 Oct 2003 10:56:26 -0600
Subject: [R] hypergeometric & population estimates
Message-ID: <OF653EB184.75DCD72D-ON87256DB2.005C57AB@apca.gc.ca>

"help"

We want to estimate the number of caribou in Jasper.  We recently conducted
an aerial survey and saw 70 uncollared caribou and 8 of 11 collared
caribou.  We want to estimate the number of caribou in this population with
95% confidence limits.  Gary White uses the hypergeometric distribution and
determines the population estimates using maximum likelihood and 95%CL as
-2LogLikelihoods.  Below, I determined the population estimate using
dhyper(x,m,n,k) and maximizing the density value as a function of n, but do
not know how I should calculate MLE with this distribution.


x <- 8         # number resighted caribou (white balls drawn)
m <-11         # number collared caribou (white balls total)
k <- 70        #  number caribou seen (# balls drawn)
n <- 1:500     #  ?? unknown number of uncollared caribou (# black balls)

d <- unlist(lapply(n, function(i) dhyper(x,m,i,k)))     # density estimate
for each value of n
data <- data.frame(estimate = n+m, d)
data <- data[is.finite(data$d), ]             # filter out NA's

max.d <- max(data$d)
pop.estimate <- data[data$d == max.d, 1]

Thank-you for your assistance,
Jesse



From bates at stat.wisc.edu  Wed Oct  1 19:11:08 2003
From: bates at stat.wisc.edu (Douglas Bates)
Date: Wed, 01 Oct 2003 17:11:08 -0000
Subject: [R] Solving a tridiagonal system
In-Reply-To: <3A822319EB35174CA3714066D590DCD50205CBEE@usrymx25.merck.com>
References: <3A822319EB35174CA3714066D590DCD50205CBEE@usrymx25.merck.com>
Message-ID: <6rpthg288l.fsf@bates4.stat.wisc.edu>

"Liaw, Andy" <andy_liaw at merck.com> writes:

> I see that the LAPACK routine DGTSV is in the R source, and defined in
> R_ext/lapack.h,  but I don't know how to get to it from R.

You need to write a separate package containing C or Fortran wrapper
functions if you want to access Lapack routines reliably.  (There is
another way but it is system dependent.)  This is peculiar to Lapack
routines and is a result of the way that we configure them as a
separate shared object or DLL.  We have discussed incorporating Lapack
in the R executable rather than as a separate shared object but that
would nearly double the size of the executable.  As we migrate more of
the linear algebra in R to Lapack we may be forced to do that.  It
shouldn't be a problem for those with a reasonable amount of
memory. On my system it would change the executable from 1.2 MB to 2.1
MB but I have 1.5 GB of memory so that isn't a big deal to me.
However we still have as an objective that R should be able to run on
machines with limited amounts of memory, as small as 8 MB.

We would like to know if users are still running R on computers with
very limited amounts of memory, say less than 32 MB.  If that is not
the case then it may be better for us to incorporate Lapack in the
executable.

If anyone wants to communicate off-list on exactly what the package
would contain I can describe that.



From flom at ndri.org  Wed Oct  1 20:02:15 2003
From: flom at ndri.org (Peter Flom)
Date: Wed, 01 Oct 2003 14:02:15 -0400
Subject: [R] sas.get problem
Message-ID: <sf7ade7e.040@MAIL.NDRI.ORG>

When I try

 citypro <- sas.get('c:/ndri/cvar/data', member = 'citypro2')


I get the following errors and warnings:


'sas' is not recognized as an internal or external command,
operable program or batch file.
'less' is not recognized as an internal or external command,
operable program or batch file.
Error in sas.get("c:/ndri/cvar/data", member = "citypro2") : 
        SAS job failed with status 1
In addition: Warning messages: 
1: c:/ndri/cvar/data/formats.sc? or formats.sas7bcat  not found.
Formatting ignored. 
 in: sas.get("c:/ndri/cvar/data", member = "citypro2") 
2: cmd execution failed with error code 1 in: shell(cmd, wait = TRUE) 
3: cmd execution failed with error code 1 in: shell(cmd, wait = TRUE) 
'rm' is not recognized as an internal or external command,
operable program or batch file.


I have no idea what these mean.....any help appreciated


thanks

Peter

Peter L. Flom, PhD
Assistant Director, Statistics and Data Analysis Core
Center for Drug Use and HIV Research
National Development and Research Institutes
71 W. 23rd St
www.peterflom.com
New York, NY 10010
(212) 845-4485 (voice)
(917) 438-0894 (fax)



From dmurdoch at pair.com  Wed Oct  1 20:23:23 2003
From: dmurdoch at pair.com (Duncan Murdoch)
Date: Wed, 01 Oct 2003 14:23:23 -0400
Subject: [R] sas.get problem
In-Reply-To: <sf7ade7e.040@MAIL.NDRI.ORG>
References: <sf7ade7e.040@MAIL.NDRI.ORG>
Message-ID: <9p6mnv4k6rm7u68js0ne6728r9t8shrm3m@4ax.com>

On Wed, 01 Oct 2003 14:02:15 -0400, "Peter Flom" <flom at ndri.org> wrote
:

>When I try
>
> citypro <- sas.get('c:/ndri/cvar/data', member = 'citypro2')
>
>
>I get the following errors and warnings:
>
>
>'sas' is not recognized as an internal or external command,
>operable program or batch file.

That's an S-PLUS function rather than an R function, isn't it?  Does
some R package supply sas.get?

I believe the S-PLUS implementation requires that you have SAS
installed, and it looks like the error message is saying that you
don't.  But if you're using S-PLUS, you'd be better asking on the
s-news mailing list.

Duncan Murdoch



From RBaskin at ahrq.gov  Wed Oct  1 20:26:19 2003
From: RBaskin at ahrq.gov (RBaskin@ahrq.gov)
Date: Wed, 1 Oct 2003 14:26:19 -0400 
Subject: [R] hypergeometric & population estimates
Message-ID: <3598558AD728D41183350008C7CF291C0F16B94C@exchange1.ahrq.gov>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20031001/2fda85bb/attachment.pl

From jmacdon at med.umich.edu  Wed Oct  1 20:42:38 2003
From: jmacdon at med.umich.edu (James MacDonald)
Date: Wed, 01 Oct 2003 14:42:38 -0400
Subject: [R] sas.get problem
Message-ID: <sf7ae7eb.029@med-gwia-02a.med.umich.edu>

It's actually in the Hmisc package. The error looks like Hmisc hasn't
been attached yet, so you might try library(Hmisc) first.

In addition, it does appear from the help file that you need SAS
installed to output the ASCII files that are required to import the
data.

HTH,

Jim



James W. MacDonald
Affymetrix and cDNA Microarray Core
University of Michigan Cancer Center
1500 E. Medical Center Drive
7410 CCGC
Ann Arbor MI 48109
734-647-5623

>>> Duncan Murdoch <dmurdoch at pair.com> 10/01/03 02:23PM >>>
On Wed, 01 Oct 2003 14:02:15 -0400, "Peter Flom" <flom at ndri.org> wrote
:

>When I try
>
> citypro <- sas.get('c:/ndri/cvar/data', member = 'citypro2')
>
>
>I get the following errors and warnings:
>
>
>'sas' is not recognized as an internal or external command,
>operable program or batch file.

That's an S-PLUS function rather than an R function, isn't it?  Does
some R package supply sas.get?

I believe the S-PLUS implementation requires that you have SAS
installed, and it looks like the error message is saying that you
don't.  But if you're using S-PLUS, you'd be better asking on the
s-news mailing list.

Duncan Murdoch

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://www.stat.math.ethz.ch/mailman/listinfo/r-help



From MSchwartz at medanalytics.com  Wed Oct  1 20:43:25 2003
From: MSchwartz at medanalytics.com (Marc Schwartz)
Date: Wed, 01 Oct 2003 13:43:25 -0500
Subject: [R] sas.get problem
In-Reply-To: <9p6mnv4k6rm7u68js0ne6728r9t8shrm3m@4ax.com>
References: <sf7ade7e.040@MAIL.NDRI.ORG>
	<9p6mnv4k6rm7u68js0ne6728r9t8shrm3m@4ax.com>
Message-ID: <1065033804.4838.36.camel@localhost>

On Wed, 2003-10-01 at 13:23, Duncan Murdoch wrote:
> On Wed, 01 Oct 2003 14:02:15 -0400, "Peter Flom" <flom at ndri.org> wrote
> :
> 
> >When I try
> >
> > citypro <- sas.get('c:/ndri/cvar/data', member = 'citypro2')
> >
> >
> >I get the following errors and warnings:
> >
> >
> >'sas' is not recognized as an internal or external command,
> >operable program or batch file.
> 
> That's an S-PLUS function rather than an R function, isn't it?  Does
> some R package supply sas.get?
> 
> I believe the S-PLUS implementation requires that you have SAS
> installed, and it looks like the error message is saying that you
> don't.  But if you're using S-PLUS, you'd be better asking on the
> s-news mailing list.
> 
> Duncan Murdoch


Duncan,

sas.get() is in Frank Harrell's Hmisc package on CRAN (recently
available there) for R.

I would defer to Frank's comments here, but reading the .PDF file that
is on CRAN, it does confirm that one must be able to run SAS in order to
use sas.get():

"Note

You must be able to run SAS (by typing sas) on your system. If the S
command !sas does not start SAS, then this function cannot work."

HTH,

Marc Schwartz



From macq at llnl.gov  Wed Oct  1 21:45:22 2003
From: macq at llnl.gov (Don MacQueen)
Date: Wed, 1 Oct 2003 12:45:22 -0700
Subject: [R] Macintosh binaries; was: Rcmdr and Macintosh
In-Reply-To: <F062093E456F8C4A9566C5CA59726C6E2DA51C@EMAIL.hci.utah.edu>
References: <F062093E456F8C4A9566C5CA59726C6E2DA51C@EMAIL.hci.utah.edu>
Message-ID: <p0521060bbba0db2ddc28@[128.115.153.6]>

Did you try install.packages()?

I know this is not exactly what you're looking for, but with the 
sometimes-called 'Darwin' version of R 1.7.1 for Mac OS X, installed 
from source code, not a precompiled binary, and using X Windows,
   install.packages('Rcmdr')
   install.packages('car')
were successful.

>  version
          _                     
platform powerpc-apple-darwin6.6
arch     powerpc               
os       darwin6.6             
system   powerpc, darwin6.6    
status                         
major    1                     
minor    7.1                   
year     2003                  
month    06                    
day      16                    
language R                     

I'm not up to date on whether install.packages() will work from a 
precompiled binary for OS X.

I haven't tried it, but I do not expect install.packages() to work if 
the Mac OS is 9.x or before, as these are not unix-based. (and 
support for R pre-OS X is waning if not completely gone)

-Don

At 10:42 AM -0600 10/1/03, Aniko Szabo wrote:
>I am a Windows user and trying to make life easier for my Mac-using 
>students. After investigating the Mac situation more, it appears 
>that the key to easy use is having precompiled binaries. However the 
>packages I am interested in (Rcmdr and its required package, car) do 
>not have one. I do not have easy access to a Mac, so I have a plea 
>to Macintosh users: could someone please create them if it is not 
>too difficult? I am planning to use R 1.7.1. Please forgive me if my 
>request is inappropriate.
>
>Aniko
>
>This e-mail may contain confidential and/or privileged infor...{{dropped}}
>
>______________________________________________
>R-help at stat.math.ethz.ch mailing list
>https://www.stat.math.ethz.ch/mailman/listinfo/r-help


-- 
--------------------------------------
Don MacQueen
Environmental Protection Department
Lawrence Livermore National Laboratory
Livermore, CA, USA



From MSchwartz at medanalytics.com  Wed Oct  1 21:59:24 2003
From: MSchwartz at medanalytics.com (Marc Schwartz)
Date: Wed, 01 Oct 2003 14:59:24 -0500
Subject: [R] sas.get problem
In-Reply-To: <1065033804.4838.36.camel@localhost>
References: <sf7ade7e.040@MAIL.NDRI.ORG>
	<9p6mnv4k6rm7u68js0ne6728r9t8shrm3m@4ax.com>
	<1065033804.4838.36.camel@localhost>
Message-ID: <1065038364.4838.53.camel@localhost>

On Wed, 2003-10-01 at 13:43, Marc Schwartz wrote:
> On Wed, 2003-10-01 at 13:23, Duncan Murdoch wrote:
> > On Wed, 01 Oct 2003 14:02:15 -0400, "Peter Flom" <flom at ndri.org> wrote
> > :
> > 
> > >When I try
> > >
> > > citypro <- sas.get('c:/ndri/cvar/data', member = 'citypro2')
> > >
> > >
> > >I get the following errors and warnings:
> > >
> > >
> > >'sas' is not recognized as an internal or external command,
> > >operable program or batch file.
> > 
> > That's an S-PLUS function rather than an R function, isn't it?  Does
> > some R package supply sas.get?
> > 
> > I believe the S-PLUS implementation requires that you have SAS
> > installed, and it looks like the error message is saying that you
> > don't.  But if you're using S-PLUS, you'd be better asking on the
> > s-news mailing list.
> > 
> > Duncan Murdoch
> 
> 
> Duncan,
> 
> sas.get() is in Frank Harrell's Hmisc package on CRAN (recently
> available there) for R.
> 
> I would defer to Frank's comments here, but reading the .PDF file that
> is on CRAN, it does confirm that one must be able to run SAS in order to
> use sas.get():
> 
> "Note
> 
> You must be able to run SAS (by typing sas) on your system. If the S
> command !sas does not start SAS, then this function cannot work."
> 
> HTH,
> 
> Marc Schwartz


A quick follow up. I installed Frank's Hmisc under R 1.8.0 beta. If
Hmisc is not attached (ie. library(Hmisc)), you get:

> citypro <- sas.get('c:/ndri/cvar/data', member = 'citypro2')
Error: couldn't find function "sas.get"

which you would reasonably expect.

Thus, it seems more likely that Dr. Flom either does not have SAS on his
system or perhaps the SAS executable file (sas.exe? IIRC) is not in Dr.
Flom's 'PATH' on his system if SAS is installed.

HTH,

Marc



From flom at ndri.org  Wed Oct  1 22:03:56 2003
From: flom at ndri.org (Peter Flom)
Date: Wed, 01 Oct 2003 16:03:56 -0400
Subject: [R] sas.get problem
Message-ID: <sf7afaf7.052@MAIL.NDRI.ORG>

Thanks to all who responded

Marc Schwartz wrote
<<<<<
A quick follow up. I installed Frank's Hmisc under R 1.8.0 beta. If
Hmisc is not attached (ie. library(Hmisc)), you get:

> citypro <- sas.get('c:/ndri/cvar/data', member = 'citypro2')
Error: couldn't find function "sas.get"

which you would reasonably expect.

Thus, it seems more likely that Dr. Flom either does not have SAS on
his
system or perhaps the SAS executable file (sas.exe? IIRC) is not in
Dr.
Flom's 'PATH' on his system if SAS is installed.
>>>

This does seem to be the problem

Thanks again

Peter



From solares at unsl.edu.ar  Wed Oct  1 22:30:47 2003
From: solares at unsl.edu.ar (solares@unsl.edu.ar)
Date: Wed, 1 Oct 2003 17:30:47 -0300 (ART)
Subject: [R] curious error with tkcheckbutton
Message-ID: <48285.170.210.173.216.1065040247.squirrel@inter14.unsl.edu.ar>

Hello, the following code produces an error when executing it, it is a code 
that produces 6 checkbutton that at 
the beginning are empty, when selecting the first checkbox he says that  
doesn't know the second 
variable tcl, he say: 

[1] "1"
Error in structure(.External("dotTcl", ..., PACKAGE = "tcltk"), class 
= "tclObj") : 
        [tcl] can't read "::RTcl2": no such variable.

when six variables tcl has been created in the array b and they exist:

> b
[1] "::RTcl1" "::RTcl2" "::RTcl3" "::RTcl4" "::RTcl5" "::RTcl6"

the script is(I'm use R 1.7.1 for win, the fail is in the first execution
of the script):

library(tcltk)
tt<-tktoplevel()
f<-tkframe(tt)
tkpack(f)
j<-6
nbre<-c("c1","c2","c3","c4","c5","c6")
b<-c()
i<-1
 while (i<=j){
  aux<-paste("b",i,sep="")
  aux<-tclVar(init=0)
  b<-c(b,as.character(aux))
  tclvalue(b[i])<-0
  i<-i+1
 }
 i<-1
 while (i<=j){
 t<-tkcheckbutton(f,text=nbre[i],variable=eval(b
[i]),relief="raised",command=function()ver())
 tkpack(t)
 i<-i+1
 }
 
 ver<-function(){
 k<-1
 while (k<=j){
  print(tclvalue(b[k]))
  k<-k+1
  }
  }

 any help will be welcome. Thank you Ruben



From jasont at indigoindustrial.co.nz  Wed Oct  1 22:09:49 2003
From: jasont at indigoindustrial.co.nz (Jason Turner)
Date: Thu, 02 Oct 2003 08:09:49 +1200
Subject: [R] Re: Mandelbrot set and C code
In-Reply-To: <3.0.5.32.20031001140936.007e2100@pop-server.ucl.ac.uk>
References: <3.0.5.32.20030917124551.007aaab0@pop-server.ucl.ac.uk>	<3.0.5.32.20030917124551.007aaab0@pop-server.ucl.ac.uk>
	<3.0.5.32.20031001140936.007e2100@pop-server.ucl.ac.uk>
Message-ID: <3F7B348D.30509@indigoindustrial.co.nz>

ucgamdo at ucl.ac.uk wrote:

>       I decided to take on the 'proper' solution to calculate the
> Mandelbrot set in R, i.e. to do the raw calculations in C and then link
> that code with R. 

This is very cool.  I did a slight tweak to your mandelbrot.R code, so 
that x can be a list with components x and y.  This allows you to keep 
zooming in using your mouse to click on the plot (one of the incredibly 
nifty features of such sets).

Using the "tweaked" version below, call the function as you suggested:

image(mandelbrot(), col = c(heat.colors(49), "black"))

Then use locator(2) to define your next view:

image(mandelbrot(locator(2)), col = c(heat.colors(49), "black"))

Click the mouse in the corners of the zoom window, and away you go. 
Lather. Rinse. Repeat.

Cheers

Jason
-- 
Indigo Industrial Controls Ltd.
http://www.indigoindustrial.co.nz
64-21-343-545
jasont at indigoindustrial.co.nz



From rnews at kernstat.com  Wed Oct  1 23:05:47 2003
From: rnews at kernstat.com (Remington, Richard)
Date: Wed, 01 Oct 2003 15:05:47 -0600
Subject: [R] hypergeometric & population estimates
In-Reply-To: <3598558AD728D41183350008C7CF291C0F16B94C@exchange1.ahrq.gov>
References: <3598558AD728D41183350008C7CF291C0F16B94C@exchange1.ahrq.gov>
Message-ID: <3F7B41AB.6070207@kernstat.com>

# another vote for 107

n <- 1:500
x <- 8
m <- 11
totaldrawn <- 78
MLE <- floor(m * totaldrawn / x)
likelihood <- choose(m,x)*choose(n-m,totaldrawn-x)/choose(n,totaldrawn)
plot(n, likelihood)
abline(v=MLE)

RBaskin at ahrq.gov wrote:

> I'm not sure I understand your notation:
> (1) We recently conducted an aerial survey and saw 70 uncollared caribou and
> 8 of 11 collared caribou.
> (2) k <- 70        #  number caribou seen (# balls drawn)
> 
> It's the number of balls drawn parenthetical remark that bothers me - I
> think the total number of balls drawn should be 78 and the number of
> non-white balls drawn is 70.
> 
> If
> x <- 8         # number resighted caribou (white balls drawn)
> m <-11         # number collared caribou (white balls total)
> totaldrawn <- 78        #  number caribou seen (total # balls drawn)
> 
> I believe that the maximum likelihood estimator you are looking for is given
> by
> 
> MLE <- floor(m * totaldrawn / x)  #floor(11 * 78 / 8) = 107
> 
> I believe the trick is to look at f(n) = P(x|m,totaldrawn,n) as a function
> of n and consider the ratio f(n) / f(n-1).  If this ratio is greater than 1
> the function is increasing and if the ratio is less than 1 the function is
> decreasing.  Then algebraically show that the maximum occurs at floor(m *
> totaldrawn / x).
> 
> Bytheway, this MLE includes both collared and uncollared balls so it may be
> that you are looking for 107 - 11 as your estimate??
> 
> hth
> Bob
> Usual disclaimers...
> 
> -----Original Message-----
> From: Jesse.Whittington at pc.gc.ca [mailto:Jesse.Whittington at pc.gc.ca] 
> Sent: Wednesday, October 01, 2003 12:56 PM
> To: R-help at stat.math.ethz.ch
> Subject: [R] hypergeometric & population estimates
> 
> "help"
> 
> We want to estimate the number of caribou in Jasper.  We recently conducted
> an aerial survey and saw 70 uncollared caribou and 8 of 11 collared
> caribou.  We want to estimate the number of caribou in this population with
> 95% confidence limits.  Gary White uses the hypergeometric distribution and
> determines the population estimates using maximum likelihood and 95%CL as
> -2LogLikelihoods.  Below, I determined the population estimate using
> dhyper(x,m,n,k) and maximizing the density value as a function of n, but do
> not know how I should calculate MLE with this distribution.
> 
> 
> x <- 8         # number resighted caribou (white balls drawn)
> m <-11         # number collared caribou (white balls total)
> k <- 70        #  number caribou seen (# balls drawn)
> n <- 1:500     #  ?? unknown number of uncollared caribou (# black balls)
> d <- unlist(lapply(n, function(i) dhyper(x,m,i,k)))     # density estimate
> for each value of n
> data <- data.frame(estimate = n+m, d)
> data <- data[is.finite(data$d), ]             # filter out NA's
> 
> max.d <- max(data$d)
> pop.estimate <- data[data$d == max.d, 1]
> 
> Thank-you for your assistance,
> Jesse
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> 

-- 

Richard E. Remington III
Statistician
KERN Statistical Services, Inc.
PO Box 1046
Boise, ID 83701
Tel: 208.426.0113
KernStat.com



From al at genetics.utah.edu  Wed Oct  1 23:17:07 2003
From: al at genetics.utah.edu (Allan Tingey)
Date: Wed, 01 Oct 2003 15:17:07 -0600
Subject: [R] R on fairly large  machines
Message-ID: <3F7B4453.7020109@genetics.utah.edu>

Hi,

We are considering using R on a large problem that will require four 64 
bit cpu's and 64Gb of ram and some flavor of unix.  We are just 
wondering if R is going to be happy in this environment.  If have 
experience with this please let me know.

Thanks,
Allan Tingey
al at genetics.utah.edu



From wei_geng at yahoo.com  Wed Oct  1 23:36:31 2003
From: wei_geng at yahoo.com (Wei Geng)
Date: Wed, 1 Oct 2003 14:36:31 -0700
Subject: [R] lda source code
Message-ID: <NFBBJNANOLLHKDKICGPMMEHECMAA.wei_geng@yahoo.com>

I am new to R. Trying to find out how lda() {in MASS R1.8.0 Windows} was
implemented in R. Does anyone know where to find out lda source code ?
Thanks.

Wei



From jasont at indigoindustrial.co.nz  Wed Oct  1 23:47:19 2003
From: jasont at indigoindustrial.co.nz (Jason Turner)
Date: Thu, 02 Oct 2003 09:47:19 +1200
Subject: [R] lda source code
In-Reply-To: <NFBBJNANOLLHKDKICGPMMEHECMAA.wei_geng@yahoo.com>
References: <NFBBJNANOLLHKDKICGPMMEHECMAA.wei_geng@yahoo.com>
Message-ID: <3F7B4B67.4040507@indigoindustrial.co.nz>

Wei Geng wrote:

> I am new to R. Trying to find out how lda() {in MASS R1.8.0 Windows} was
> implemented in R. Does anyone know where to find out lda source code ?
> Thanks.

Here:

http://cran.r-project.org

Hint: MASS is a *package*.  You want to view its *source*.

Same with most other R packages.  Or just about anything else you want 
to know about R.

Cheers

Jason
-- 
Indigo Industrial Controls Ltd.
http://www.indigoindustrial.co.nz
64-21-343-545
jasont at indigoindustrial.co.nz



From spencer.graves at pdf.com  Wed Oct  1 23:56:44 2003
From: spencer.graves at pdf.com (Spencer Graves)
Date: Wed, 01 Oct 2003 14:56:44 -0700
Subject: [R] lda source code
In-Reply-To: <NFBBJNANOLLHKDKICGPMMEHECMAA.wei_geng@yahoo.com>
References: <NFBBJNANOLLHKDKICGPMMEHECMAA.wei_geng@yahoo.com>
Message-ID: <3F7B4D9C.1060705@pdf.com>

Consider the following: 

 > library(MASS)
 > lda
function (x, ...)
UseMethod("lda")
<environment: namespace:MASS>
 > methods(lda)
[1] "lda.data.frame" "lda.default"    "lda.formula"    "lda.matrix"   

Now type "lda.data.frame" or "lda.default", etc., at a command prompt to 
see the corresponding R code. 

Is this what you want?
spencer graves

Wei Geng wrote:

>I am new to R. Trying to find out how lda() {in MASS R1.8.0 Windows} was
>implemented in R. Does anyone know where to find out lda source code ?
>Thanks.
>
>Wei
>
>______________________________________________
>R-help at stat.math.ethz.ch mailing list
>https://www.stat.math.ethz.ch/mailman/listinfo/r-help
>  
>



From wei_geng at yahoo.com  Thu Oct  2 00:00:50 2003
From: wei_geng at yahoo.com (Wei Geng)
Date: Wed, 1 Oct 2003 15:00:50 -0700
Subject: [R] lda source code
In-Reply-To: <3F7B4B67.4040507@indigoindustrial.co.nz>
Message-ID: <NFBBJNANOLLHKDKICGPMOEHFCMAA.wei_geng@yahoo.com>

Hi Jason, Spencer,

Thanks for the prompt response. The strange thing about MASS is that it's
not in "Package Sources" as most of other R packages are. It seems to come
with the binary R installation. I checked out the Rxx/library/MASS on my
laptop, there are source code (script) for Venables & Ripley's book but no
source code for lda().

The "lda.data.frame" or "lda.default" at prompt (after loading MASS
"library(MASS)") has
"Error: Object "lda.data.frame" not found"

Wei



From fgibbons at hms.harvard.edu  Thu Oct  2 00:13:00 2003
From: fgibbons at hms.harvard.edu (Frank Gibbons)
Date: Wed, 01 Oct 2003 18:13:00 -0400
Subject: [R] lda source code
In-Reply-To: <3F7B4B67.4040507@indigoindustrial.co.nz>
References: <NFBBJNANOLLHKDKICGPMMEHECMAA.wei_geng@yahoo.com>
	<NFBBJNANOLLHKDKICGPMMEHECMAA.wei_geng@yahoo.com>
Message-ID: <5.2.1.1.2.20031001180450.02114f50@email.med.harvard.edu>

Wei Geng,

I asked the same question about six weeks ago, so let me try to answer it. 
The source for the entire package 'MASS' is in a single file, I believe (at 
least this is true on my Linux setup). The exact location of that file 
you'll have to determine by searching the directory/folder where you 
installed it. The function 'lda' is implemented entirely in R itself, like 
much of its functionality. Look at the functions 'lda' and 'predict.lda' in 
this file for details.

Comments are sparse in most R code, from what I gather, but if you look in 
"Pattern Recognition and Neural Networks" by Brian Ripley (one of the 
authors of this package), you'll find a discussion in section 2.4 
'Predictive classification' that covers much of what's going on, from what 
I've been able to glean.

I hope that helps. There are certainly others out there who are more au 
fait with this than me.

-Frank Gibbons


At 05:47 PM 10/1/2003, you wrote:
>Wei Geng wrote:
>
>>I am new to R. Trying to find out how lda() {in MASS R1.8.0 Windows} was
>>implemented in R. Does anyone know where to find out lda source code ?
>>Thanks.
>
>Here:
>
>http://cran.r-project.org
>
>Hint: MASS is a *package*.  You want to view its *source*.
>
>Same with most other R packages.  Or just about anything else you want to 
>know about R.
>
>Cheers
>
>Jason
>--
>Indigo Industrial Controls Ltd.
>http://www.indigoindustrial.co.nz
>64-21-343-545
>jasont at indigoindustrial.co.nz
>
>______________________________________________
>R-help at stat.math.ethz.ch mailing list
>https://www.stat.math.ethz.ch/mailman/listinfo/r-help

PhD, Computational Biologist,
Harvard Medical School BCMP/SGM-322, 250 Longwood Ave, Boston MA 02115, USA.
Tel: 617-432-3555       Fax: 
617-432-3557       http://llama.med.harvard.edu/~fgibbons



From gavin.simpson at ucl.ac.uk  Thu Oct  2 00:06:09 2003
From: gavin.simpson at ucl.ac.uk (Gavin Simpson)
Date: Wed, 01 Oct 2003 23:06:09 +0100
Subject: [R] lda source code
In-Reply-To: <NFBBJNANOLLHKDKICGPMMEHECMAA.wei_geng@yahoo.com>
References: <NFBBJNANOLLHKDKICGPMMEHECMAA.wei_geng@yahoo.com>
Message-ID: <3F7B4FD1.3090008@ucl.ac.uk>

Wei Geng wrote:
> Does anyone know where to find out lda source code ?

Try typing lda.default at the prompt. That should get you started. Also see:

 > methods(lda)

as lda.default isn't the only bit of code used in lda()

Alternatively, grab the source from CRAN and read it at your leisure.

HTH

Gav

-- 
%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%
Gavin Simpson                     [T] +44 (0)20 7679 5522
ENSIS Research Fellow             [F] +44 (0)20 7679 7565
ENSIS Ltd. & ECRC                 [E] gavin.simpson at ucl.ac.uk
UCL Department of Geography       [W] http://www.ucl.ac.uk/~ucfagls/cv/
26 Bedford Way                    [W] http://www.ucl.ac.uk/~ucfagls/
London.  WC1H 0AP.
%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%



From gavin.simpson at ucl.ac.uk  Thu Oct  2 00:12:42 2003
From: gavin.simpson at ucl.ac.uk (Gavin Simpson)
Date: Wed, 01 Oct 2003 23:12:42 +0100
Subject: [R] lda source code
In-Reply-To: <NFBBJNANOLLHKDKICGPMOEHFCMAA.wei_geng@yahoo.com>
References: <NFBBJNANOLLHKDKICGPMOEHFCMAA.wei_geng@yahoo.com>
Message-ID: <3F7B515A.3040705@ucl.ac.uk>

Wei Geng wrote:

> Hi Jason, Spencer,
> 
> Thanks for the prompt response. The strange thing about MASS is that it's
> not in "Package Sources" as most of other R packages are. It seems to come
> with the binary R installation. I checked out the Rxx/library/MASS on my
> laptop, there are source code (script) for Venables & Ripley's book but no
> source code for lda().

Wei, MASS is actually distributed in a bundle called VR, which is on 
CRAN. VR as in Venables and Ripley, the authors of MASS (the book). The 
VR bundle contains MASS, nnet, spatial and class packages.

The reason MASS comes with your binary installation is that the VR 
bundle has Recommended status - and should therefore be available in all 
binary distributions.

HTH,

G
-- 
%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%
Gavin Simpson                     [T] +44 (0)20 7679 5522
ENSIS Research Fellow             [F] +44 (0)20 7679 7565
ENSIS Ltd. & ECRC                 [E] gavin.simpson at ucl.ac.uk
UCL Department of Geography       [W] http://www.ucl.ac.uk/~ucfagls/cv/
26 Bedford Way                    [W] http://www.ucl.ac.uk/~ucfagls/
London.  WC1H 0AP.
%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%



From spencer.graves at pdf.com  Thu Oct  2 00:15:34 2003
From: spencer.graves at pdf.com (Spencer Graves)
Date: Wed, 01 Oct 2003 15:15:34 -0700
Subject: [R] lda source code
In-Reply-To: <NFBBJNANOLLHKDKICGPMOEHFCMAA.wei_geng@yahoo.com>
References: <NFBBJNANOLLHKDKICGPMOEHFCMAA.wei_geng@yahoo.com>
Message-ID: <3F7B5206.3040107@pdf.com>

With R 1.7.1 on Windows 2000, I got fine R source code for each of the 4 
options.  What version of R and what exactly did you do?  I can't 
reproduce your error. 

hope this helps.  spencer graves

Wei Geng wrote:

>Hi Jason, Spencer,
>
>Thanks for the prompt response. The strange thing about MASS is that it's
>not in "Package Sources" as most of other R packages are. It seems to come
>with the binary R installation. I checked out the Rxx/library/MASS on my
>laptop, there are source code (script) for Venables & Ripley's book but no
>source code for lda().
>
>The "lda.data.frame" or "lda.default" at prompt (after loading MASS
>"library(MASS)") has
>"Error: Object "lda.data.frame" not found"
>
>Wei
>
>______________________________________________
>R-help at stat.math.ethz.ch mailing list
>https://www.stat.math.ethz.ch/mailman/listinfo/r-help
>  
>



From jasont at indigoindustrial.co.nz  Thu Oct  2 00:09:20 2003
From: jasont at indigoindustrial.co.nz (Jason Turner)
Date: Thu, 02 Oct 2003 10:09:20 +1200
Subject: [R] lda source code
In-Reply-To: <NFBBJNANOLLHKDKICGPMOEHFCMAA.wei_geng@yahoo.com>
References: <NFBBJNANOLLHKDKICGPMOEHFCMAA.wei_geng@yahoo.com>
Message-ID: <3F7B5090.8070308@indigoindustrial.co.nz>

Wei Geng wrote:
> Hi Jason, Spencer,
> 
> Thanks for the prompt response. The strange thing about MASS is that it's
> not in "Package Sources" as most of other R packages are. It seems to come
> with the binary R installation. I checked out the Rxx/library/MASS on my
> laptop, there are source code (script) for Venables & Ripley's book but no
> source code for lda().

Ah.  On CRAN, the MASS library is part of a bundle called VR.  Download 
the source for that.

There are a few bundles on the CRAN source pages - if you encounter this 
problem again, just follow the Package Sources link, and search the page 
for "MASS" (or whatever the package name is).  Use "Edit->Find", or 
Ctrl-F, or whatever.  In the case of MASS, you find this one:

VR:

     Functions and datasets to support Venables and Ripley, `Modern 
Applied Statistics with S' (4th edition).
     Bundle of:	MASS class nnet spatial

It's the "Bundle of" part you're looking for.

Cheers

Jason
-- 
Indigo Industrial Controls Ltd.
http://www.indigoindustrial.co.nz
64-21-343-545
jasont at indigoindustrial.co.nz



From rpeng at jhsph.edu  Thu Oct  2 00:16:32 2003
From: rpeng at jhsph.edu (Roger D. Peng)
Date: Wed, 01 Oct 2003 18:16:32 -0400
Subject: [R] lda source code
In-Reply-To: <NFBBJNANOLLHKDKICGPMMEHECMAA.wei_geng@yahoo.com>
References: <NFBBJNANOLLHKDKICGPMMEHECMAA.wei_geng@yahoo.com>
Message-ID: <3F7B5240.9070709@jhsph.edu>

You are using a *beta* version of R 1.8.0 and in that version 
lda.default is not visible to the user (hidden in a namespace).  You can 
access it though by using the ::: (triple colon) operator, as in

>  library(MASS)
>  MASS:::lda.default

Actually, the first library() call is not necessary.

-roger


Wei Geng wrote:

>I am new to R. Trying to find out how lda() {in MASS R1.8.0 Windows} was
>implemented in R. Does anyone know where to find out lda source code ?
>Thanks.
>
>Wei
>
>______________________________________________
>R-help at stat.math.ethz.ch mailing list
>https://www.stat.math.ethz.ch/mailman/listinfo/r-help
>
>  
>



From hodgess at gator.dt.uh.edu  Thu Oct  2 00:34:24 2003
From: hodgess at gator.dt.uh.edu (Erin Hodgess)
Date: Wed, 1 Oct 2003 17:34:24 -0500
Subject: [R] install from source again
Message-ID: <200310012234.h91MYOe31709@gator.dt.uh.edu>

What does Error 255 stand for in the make bitmapdll, please?

Thanks,
erin
hodgess at gator.uhd.edu



From jasont at indigoindustrial.co.nz  Thu Oct  2 02:28:39 2003
From: jasont at indigoindustrial.co.nz (Jason Turner)
Date: Thu, 02 Oct 2003 12:28:39 +1200
Subject: [R] Re: Mandelbrot set and C code
In-Reply-To: <3F7B348D.30509@indigoindustrial.co.nz>
References: <3.0.5.32.20030917124551.007aaab0@pop-server.ucl.ac.uk>	<3.0.5.32.20030917124551.007aaab0@pop-server.ucl.ac.uk>	<3.0.5.32.20031001140936.007e2100@pop-server.ucl.ac.uk>
	<3F7B348D.30509@indigoindustrial.co.nz>
Message-ID: <3F7B7137.3020408@indigoindustrial.co.nz>

Jason Turner wrote:

> ...I did a slight tweak to your mandelbrot.R code, so
> that x can be a list with components x and y.  This allows you to keep 
> zooming in using your mouse to click on the plot (one of the incredibly 
> nifty features of such sets).
> 
> Using the "tweaked" version below, call the function as you suggested:
> 
> image(mandelbrot(), col = c(heat.colors(49), "black"))
> 
> Then use locator(2) to define your next view:
> 
> image(mandelbrot(locator(2)), col = c(heat.colors(49), "black"))
> 

Of course, I would've been nice if I'd included the tweaked version.  D'ho!

#######################################################################
# Function to calculate the Mandelbrot set. This function calls a     #
# C routine in order to perform the calculations faster.              #
#                                                                     #
# Written by Mario dos Reis. September 2003                           #
# Modified: added if(is.list(x)){...} at start to check if co-ords    #
#           are from locator() or similar - jason turner oct 2 2003   #
#######################################################################

mandelbrot <- function(x = c(-3, 1),        # x limits
                        y = c(-1.8, 1.8),    # y limits
                        nx = 600,            # x resolution
                        ny = 600,            # y resolution
                        iter = 20)           # maximun number of iterations
{
   if(is.list(x)) {
     y <- range(x$y)
     x <- range(x$x)
   }
   xcoo <- seq(x[1], x[2], len = nx) # x coordinates
   ycoo <- seq(y[1], y[2], len = ny) # y coordinates
   set = numeric(nx*ny)              # this will store the output of
                                     # the C routine

   # This is the call to the C function itself
   the.set = .C("mandelbrot",
     xcoo = as.double(xcoo),
     ycoo = as.double(ycoo),
     nx = as.integer(nx),
     ny = as.integer(ny),
     set = as.integer(set),
     iter = as.integer(iter))$set

   # Create a list with elements x, y and z,
   # suitable for image(), persp(), etc. and return it.
   return(list(x = xcoo, y = ycoo, z = matrix(the.set, ncol = ny, byrow 
= T)));
}


-- 
Indigo Industrial Controls Ltd.
http://www.indigoindustrial.co.nz
64-21-343-545
jasont at indigoindustrial.co.nz



From szhan at uoguelph.ca  Thu Oct  2 03:15:11 2003
From: szhan at uoguelph.ca (szhan@uoguelph.ca)
Date: Wed,  1 Oct 2003 21:15:11 -0400
Subject: [R] how calculate mean for each group 
Message-ID: <1065057311.3f7b7c1f6ba6e@webmail.uoguelph.ca>

Hello, R experts:
I got data like this:
group   duplicate   treatment 
A         Y          5
A         Y          3
A         N          6
B         Y          2
B         N          4
B         Y          1
How to sort the data and calculate the average treatment value for each group 
in two level of duplicate. Results like this:
group   duplicate   treatment 
A         Y          4
A         N          6
B         Y          1.5
B         N          4
Thank you in advance.

Josh



From spencer.graves at pdf.com  Thu Oct  2 04:19:04 2003
From: spencer.graves at pdf.com (Spencer Graves)
Date: Wed, 01 Oct 2003 19:19:04 -0700
Subject: [R] how calculate mean for each group
In-Reply-To: <1065057311.3f7b7c1f6ba6e@webmail.uoguelph.ca>
References: <1065057311.3f7b7c1f6ba6e@webmail.uoguelph.ca>
Message-ID: <3F7B8B18.2090102@pdf.com>

Have you considered "aggregate"? 

hope this helps.  spencer graves

szhan at uoguelph.ca wrote:

>Hello, R experts:
>I got data like this:
>group   duplicate   treatment 
>A         Y          5
>A         Y          3
>A         N          6
>B         Y          2
>B         N          4
>B         Y          1
>How to sort the data and calculate the average treatment value for each group 
>in two level of duplicate. Results like this:
>group   duplicate   treatment 
>A         Y          4
>A         N          6
>B         Y          1.5
>B         N          4
>Thank you in advance.
>
>Josh
>
>______________________________________________
>R-help at stat.math.ethz.ch mailing list
>https://www.stat.math.ethz.ch/mailman/listinfo/r-help
>  
>



From MSchwartz at medanalytics.com  Thu Oct  2 04:38:33 2003
From: MSchwartz at medanalytics.com (Marc Schwartz)
Date: Wed, 01 Oct 2003 21:38:33 -0500
Subject: [R] how calculate mean for each group
In-Reply-To: <1065057311.3f7b7c1f6ba6e@webmail.uoguelph.ca>
References: <1065057311.3f7b7c1f6ba6e@webmail.uoguelph.ca>
Message-ID: <1065062313.4838.73.camel@localhost>

On Wed, 2003-10-01 at 20:15, szhan at uoguelph.ca wrote:
> Hello, R experts:
> I got data like this:
> group   duplicate   treatment 
> A         Y          5
> A         Y          3
> A         N          6
> B         Y          2
> B         N          4
> B         Y          1
> How to sort the data and calculate the average treatment value for each group 
> in two level of duplicate. Results like this:
> group   duplicate   treatment 
> A         Y          4
> A         N          6
> B         Y          1.5
> B         N          4
> Thank you in advance.
> 
> Josh


# Create a dataframe
df <- data.frame(group = c(rep("A", 3), rep("B", 3)), 
                 duplicate = c("Y", "Y", "N", "Y", "N", "Y"), 
                 treatment = c(5, 3, 6, 2, 4, 1))

# Use aggregate
aggregate(df$treatment, list(df$group, df$duplicate), mean)
  Group.1 Group.2   x
1       A       N 6.0
2       B       N 4.0
3       A       Y 4.0
4       B       Y 1.5

Aggregate returns a data frame in this case, so that you can then set
the colnames and order the output if you wish.

HTH,

Marc Schwartz



From ligges at statistik.uni-dortmund.de  Thu Oct  2 08:49:26 2003
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Thu, 02 Oct 2003 08:49:26 +0200
Subject: [R] lda source code
In-Reply-To: <5.2.1.1.2.20031001180450.02114f50@email.med.harvard.edu>
References: <NFBBJNANOLLHKDKICGPMMEHECMAA.wei_geng@yahoo.com>	<NFBBJNANOLLHKDKICGPMMEHECMAA.wei_geng@yahoo.com>
	<5.2.1.1.2.20031001180450.02114f50@email.med.harvard.edu>
Message-ID: <3F7BCA76.2020500@statistik.uni-dortmund.de>

Frank Gibbons wrote:
> Wei Geng,
> 
> I asked the same question about six weeks ago, so let me try to answer 
> it. The source for the entire package 'MASS' is in a single file, I 
> believe (at least this is true on my Linux setup). 


The thread gets boring, but let me correct this "belief":

NO! There is one file including all R code, if you are looking into the 
binary installation of a package, and then the C/Fortran sources are not 
more visible.

Look into the source distribution of a package for a more structured 
view of things!

Uwe Ligges




 > The exact location of
> that file you'll have to determine by searching the directory/folder 
> where you installed it. The function 'lda' is implemented entirely in R 
> itself, like much of its functionality. Look at the functions 'lda' and 
> 'predict.lda' in this file for details.
> 
> Comments are sparse in most R code, from what I gather, but if you look 
> in "Pattern Recognition and Neural Networks" by Brian Ripley (one of the 
> authors of this package), you'll find a discussion in section 2.4 
> 'Predictive classification' that covers much of what's going on, from 
> what I've been able to glean.
> 
> I hope that helps. There are certainly others out there who are more au 
> fait with this than me.
> 
> -Frank Gibbons
> 
> 
> At 05:47 PM 10/1/2003, you wrote:
> 
>> Wei Geng wrote:
>>
>>> I am new to R. Trying to find out how lda() {in MASS R1.8.0 Windows} was
>>> implemented in R. Does anyone know where to find out lda source code ?
>>> Thanks.
>>
>>
>> Here:
>>
>> http://cran.r-project.org
>>
>> Hint: MASS is a *package*.  You want to view its *source*.
>>
>> Same with most other R packages.  Or just about anything else you want 
>> to know about R.
>>
>> Cheers
>>
>> Jason
>> -- 
>> Indigo Industrial Controls Ltd.
>> http://www.indigoindustrial.co.nz
>> 64-21-343-545
>> jasont at indigoindustrial.co.nz
>>
>> ______________________________________________
>> R-help at stat.math.ethz.ch mailing list
>> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> 
> 
> PhD, Computational Biologist,
> Harvard Medical School BCMP/SGM-322, 250 Longwood Ave, Boston MA 02115, 
> USA.
> Tel: 617-432-3555       Fax: 617-432-3557       
> http://llama.med.harvard.edu/~fgibbons
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help



From ligges at statistik.uni-dortmund.de  Thu Oct  2 08:52:14 2003
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Thu, 02 Oct 2003 08:52:14 +0200
Subject: [R] install from source again
In-Reply-To: <200310012234.h91MYOe31709@gator.dt.uh.edu>
References: <200310012234.h91MYOe31709@gator.dt.uh.edu>
Message-ID: <3F7BCB1E.7040608@statistik.uni-dortmund.de>

Erin Hodgess wrote:

> What does Error 255 stand for in the make bitmapdll, please?

It depends. Some more lines of the error message would be helpful.
Did have put the required sources (jpeg-6b, libpng) at the correct 
loactions in order to make the bitmapdll?

Uwe Ligges



> Thanks,
> erin
> hodgess at gator.uhd.edu
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help



From ebetzinger at free.fr  Thu Oct  2 09:58:44 2003
From: ebetzinger at free.fr (ebetzinger@free.fr)
Date: Thu, 2 Oct 2003 8:58:44 +0100
Subject: [R] trouble with R com and python
Message-ID: <20031002065800.7FFE4C10F@postfix3-1.free.fr>


hi there

i tried to use R from python
my pc is an intel 1600 with win98; python is python 2.2; R is rw1062, and R (D)COM Server V1.2  

when i use the sample code  provided in the readme.html, no problem, but when i try to read a file, a get a traceback error
(please see code enclosed)

has anyone ever tried to read file using python, or send commands from libraries (MASS...)  ?
is there a tutorial for Rcom ( well, i tried to find one with google, but it didn't work  :-((   )



thanks in advance  ;-)

regards


emmanuel



the python code is the following   :

---------------------------
---------------------------
Python 2.2.2 (#37, Oct 14 2002, 17:02:34) [MSC 32 bit (Intel)] on win32
Type "copyright", "credits" or "license" for more information.
IDLE 0.8 -- press F1 for help

>>> from win32com.client import Dispatch
>>> sc=Dispatch("StatConnectorSrv.StatConnector")
>>> sc.Init("R")
>>> res = sc.Evaluate("2+2")
>>> res
4.0
>>> res = sc.Evaluate("library(modreg)")
>>> sc.Evaluate("library(modreg)")
(u'methods', u'ctest', u'mva', u'modreg', u'nls', u'ts', u'base')
>>> sc.Evaluate("library(MASS)")
(u'MASS', u'methods', u'ctest', u'mva', u'modreg', u'nls', u'ts', u'base')

##### no pb till here!!


###            but if i try to read a file 

>>> sc.Evaluate('do<-read.table("c:/data.txt", header=TRUE)')
Traceback (most recent call last):
  File "<pyshell#11>", line 1, in ?
    sc.Evaluate('do<-read.table("c:/data.txt", header=TRUE)')
  File "<COMObject StatConnectorSrv.StatConnector>", line 2, in Evaluate
  File "C:\PYTHON22\lib\site-packages\win32com\client\dynamic.py", line 237, in _ApplyTypes_
    result = apply(self._oleobj_.InvokeTypes, (dispid, LCID, wFlags, retType, argTypes) + args)
com_error: (-2147352567, 'Une exception est apparue.', (0, None, None, None, 0, -2147221500), None)


++++ nota bene    :  : " Une exception est apparue."        means    " an exception occured"


>>>



From ligges at statistik.uni-dortmund.de  Thu Oct  2 09:17:26 2003
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Thu, 02 Oct 2003 09:17:26 +0200
Subject: [R] permission problem on R install
In-Reply-To: <200310011545.h91FjJB11899@gator.dt.uh.edu>
References: <200310011545.h91FjJB11899@gator.dt.uh.edu>
Message-ID: <3F7BD106.7060102@statistik.uni-dortmund.de>

Erin Hodgess wrote:

> Dear R People:
> 
> Here is the exact sequence of events:
> 
> Making package base
> 
> adding build stamp to DESCRIPTION
> 
> C:/rsource/R-1.7.1/bin/rterm.exe: permission denied

Do you have the *recent* version of the tool set from Brian Ripley's 
page? Do you use "tar" to unpack the sources?
Look into the permissions of the directory/files, sometimes strange 
permissions are related to inheritance of permissions from a main 
directory, and look for the reason why you don't have access to the file.


> MAKE[2]:***[C:/rsource/R-1.7.1/library/base/zzzz] Error 126
> 
> MAKE[1]:***[pkg-base] Error 2
> 
> MAKE:***[rpackage] Error 2
> 
> 
> Also, I'm using version 2.0.0-3.exe of Mingw.
> 
> I tried the most recent version, and it got hung up because
> of changes in the type sizes.
>
> This doesn't make sense, does it?

No, if you have "cleaned" the sources with
  make clean
before. Compiling with MinGW's gcc-3.3.1 works perfectly, at least for 
R-1.8.0 beta.

Uwe Ligges


> Thanks,
> Erin
> 
> mailto: hodgess at gator.uhd.edu
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help



From p.dalgaard at biostat.ku.dk  Thu Oct  2 10:02:49 2003
From: p.dalgaard at biostat.ku.dk (Peter Dalgaard BSA)
Date: Thu, 02 Oct 2003 08:02:49 -0000
Subject: [R] R on fairly large  machines
In-Reply-To: <3F7B4453.7020109@genetics.utah.edu>
References: <3F7B4453.7020109@genetics.utah.edu>
Message-ID: <x2isn812wg.fsf@biostat.ku.dk>

Allan Tingey <al at genetics.utah.edu> writes:

> Hi,
> 
> We are considering using R on a large problem that will require four
> 64 bit cpu's and 64Gb of ram and some flavor of unix.  We are just
> wondering if R is going to be happy in this environment.  If have
> experience with this please let me know.

You're probably going into uncharted territory at least to some
extent, but various people have been gathering experiences with 64-bit
platforms. 

I'm not rich enough to have hands-on experience, but my impression is
that things are quite dependent on the precise platform. The free
software OS's are generally quite well standardized so that you can
use (near-)standard procedures to get going. The commercial ones will
require some elbow grease to get compiler switches and library
settings right, and to get various support libraries (readline, jpeg,
bz, etc.) in place. 

Then again, there are preciously few platforms that support that kind
of memory size - I know that Sun will, but the speeds that I have seen
from their CPUs have not been impressive.

-- 
   O__  ---- Peter Dalgaard             Blegdamsvej 3  
  c/ /'_ --- Dept. of Biostatistics     2200 Cph. N   
 (*) \(*) -- University of Copenhagen   Denmark      Ph: (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)             FAX: (+45) 35327907



From maechler at stat.math.ethz.ch  Thu Oct  2 10:54:10 2003
From: maechler at stat.math.ethz.ch (Martin Maechler)
Date: Thu, 2 Oct 2003 10:54:10 +0200
Subject: [R] fitting Markov chains
In-Reply-To: <20031001151451.GA9660@localhost>
References: <20031001151451.GA9660@localhost>
Message-ID: <16251.59314.906605.998196@gargle.gargle.HOWL>

>>>>> "Tamas" == Tamas Papp <tpapp at axelero.hu>
>>>>>     on Wed, 1 Oct 2003 17:14:51 +0200 writes:

    Tamas> I need to find a computationally simple process for the movement of
    Tamas> interest rates. In this simplified model, an interest rate can have
    Tamas> 3--5 possible values, and its movement is characterized by a matrix of
    Tamas> transition probabilities (ie, it is a Markov process).

    Tamas> I would like to estimate this process from a given set of data. 

    Tamas> For example, let the interest rate time series be:

    Tamas> 7 3 8 2 5 9 6

    Tamas> Assume that the discretized variable can take the following values:
    Tamas> (3, 5, 8), then we find the nearest discrete point and give its index:

    Tamas> 3 1 3 1 2 3 2

    Tamas> Then estimate the transition probabilities.

    Tamas> I have the following questions:

    Tamas> - how should I select the discrete set of values that the variable can
    Tamas> assume? Eg simply get the maximum and minimum, and divide this
    Tamas> interval into, say, three pieces? Or estimate the mean, and make the
    Tamas> other two values mean plus-minus one standard deviation?

(see below)

    Tamas> - once the variable is discretized, how do I
    Tamas>   transform each data point to its discretized value
    Tamas>   (its index)?

that's the trivial part of answering your questions:  Use  cut()

    Tamas> - the most important: how should I estimate the transition
    Tamas> probabilities?

We (mainly Peter Buhlmann and his coworkers) have had good experiences
with discretizing such financial time series and then fitting so
called "Variable Length Markov Chains" (VLMCs).
 
To your 1st question: When doing so, one easy (and robust!)
approach was to use the sample quantiles (of the marginal
distribution), e.g. use the three quartiles,
 quantile(x, prob = (1:3)/4)  as cut points {in cut(), above}.
Much less easy is to determine how *many* values you want to
chose for cutting.  I think they have rarely used many pieces,
sometimes even had good results with binary discretization ---
iff used with VLMCs.

So this comes back to the question of model estimation.
A VLMC does *not* assume first-order Markov {as you do above},
but rather allows to look further back into the past, something
quite realistic in some cases. Now the "VL" part, "Variable
Length" is the novel idea of a past the length of which
*depends* on your current state (context).

There's the CRAN package "VLMC" 
  (http://cran.r-project.org/src/contrib/PACKAGES.html#VLMC)
and we have an applied paper 
    M?chler, M. and B?hlmann, P. (2003). 
    Variable Length Markov Chains: Methodology, Computing and Software.
    To appear in Journal of Computational and Graphical Statistics.

a pre-version of which you can get from
  http://stat.ethz.ch/Research-Reports/104.html

The paper contains references to the fundamental (mathematical) 
research on this.


    Tamas> References to introductory literature on estimating Markov chains like
    Tamas> this would be welcome. Most importantly, I need to know how robust an
    Tamas> estimation is to selecting the discrete points, or is there a simple
    Tamas> "goodness of fit" estimation.

Using  quantiles  instead of  mean/sd  for chosing cut points is
certainly robust.
one goodness of fit (GOF) measure we (also use with VLMCs) is 
`the'  AIC (-2 log likelihood). However you can only use it to compare
nested models; which doesn't help you here (I think) for chosing
the number of cutpoints. 
More applied is to use predictions {from a fitted model} and
compare them with the actual values.  Because of discretization
you get a confusion matrix on which you can consider quite a few
different GOF measures.  For realistic GOF measurement you also
need to do something like crossvalidation which in its simplest
case would be to work with "training" and "testing" data.

The whole topic is not at all trivial.
We had one Ph.D. thesis on this (plus extensions..).
Hoping this helps.



From vito.muggeo at giustizia.it  Thu Oct  2 11:03:38 2003
From: vito.muggeo at giustizia.it (Vito Muggeo)
Date: Thu, 2 Oct 2003 11:03:38 +0200
Subject: [R] pmatch questions
Message-ID: <009701c388c4$174262a0$5c13070a@PROCGEN>

Dear all,
Below there are two, simple - I suppose, questions on using pmatch():

> pmatch("xx", c("cc","xxa"))
[1] 2
> pmatch("a", c("cc","xxa"))
[1] NA
> pmatch("xx", c("cc","xxa","xxb"))
[1] NA

I would like that the second call returns also 2, and the third call returns
c(2,3)
is it possible?

many thanks
vito



From uth at zhwin.ch  Thu Oct  2 11:19:09 2003
From: uth at zhwin.ch (=?iso-8859-1?Q?=22Untern=E4hrer_Thomas=2C_uth=22?=)
Date: Thu, 2 Oct 2003 11:19:09 +0200
Subject: AW: [R] pmatch questions
Message-ID: <53A181E56FB0694ABFD212F8AEDA7F6F2589F0@langouste.zhwin.ch>


Hi,

This is maybe not a really nice solution but it gives you what you want

which(regexpr("a", c("cc","xxa"))!=-1)
2

which(regexpr("xx", c("cc","xxa","xxb"))!=-1)
2 3

HTH

Thomas


-----Urspr?ngliche Nachricht-----
Von: Vito Muggeo [mailto:vito.muggeo at giustizia.it] 
Gesendet: Donnerstag, 2. Oktober 2003 11:04
An: r-help at stat.math.ethz.ch
Betreff: [R] pmatch questions


Dear all,
Below there are two, simple - I suppose, questions on using pmatch():

> pmatch("xx", c("cc","xxa"))
[1] 2
> pmatch("a", c("cc","xxa"))
[1] NA
> pmatch("xx", c("cc","xxa","xxb"))
[1] NA

I would like that the second call returns also 2, and the third call returns
c(2,3)
is it possible?

many thanks
vito

______________________________________________
R-help at stat.math.ethz.ch mailing list https://www.stat.math.ethz.ch/mailman/listinfo/r-help



From ligges at statistik.uni-dortmund.de  Thu Oct  2 11:27:09 2003
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Thu, 02 Oct 2003 11:27:09 +0200
Subject: [R] pmatch questions
In-Reply-To: <009701c388c4$174262a0$5c13070a@PROCGEN>
References: <009701c388c4$174262a0$5c13070a@PROCGEN>
Message-ID: <3F7BEF6D.2050403@statistik.uni-dortmund.de>

Vito Muggeo wrote:

> Dear all,
> Below there are two, simple - I suppose, questions on using pmatch():
> 
> 
>>pmatch("xx", c("cc","xxa"))
> 
> [1] 2
> 
>>pmatch("a", c("cc","xxa"))
> 
> [1] NA
> 
>>pmatch("xx", c("cc","xxa","xxb"))
> 
> [1] NA
> 
> I would like that the second call returns also 2, and the third call returns
> c(2,3)
> is it possible?

You are looking for grep().

Uwe Ligges


> many thanks
> vito
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help



From maechler at stat.math.ethz.ch  Thu Oct  2 11:37:26 2003
From: maechler at stat.math.ethz.ch (Martin Maechler)
Date: Thu, 2 Oct 2003 11:37:26 +0200
Subject: [R] question about predictions with linear models
In-Reply-To: <1065027555.3878.10.camel@ra.chem.psu.edu>
References: <1065027555.3878.10.camel@ra.chem.psu.edu>
Message-ID: <16251.61910.810873.95085@gargle.gargle.HOWL>

>>>>> "Rajarshi" == Rajarshi Guha <rxg218 at psu.edu>
>>>>>     on 01 Oct 2003 12:59:16 -0400 writes:

    Rajarshi> Hi,
    Rajarshi> this question is probably very obvious but I just cant see where I
    Rajarshi> might be going wrong. 

    Rajarshi> I'm using the lm() function to generate a linear
    Rajarshi> model and then make predictions using a different
    Rajarshi> set of data.

    Rajarshi> To generate the model I do (tdata & pdata are
    Rajarshi> matrices of observations and parameters, tdepv,
    Rajarshi> pdepv are response vectors)

This is not reproducible for us, since we don't have your data,
and the following is partly definitely not the R code you used.
(you didn't use "lnegth()" nor did you ***set**  "length(pred) = 140",
 did you?)
All we can guess is that you had missing values in your data,
and you did *not* consider setting  options(na.action = "na.exclude")
{read  ?na.exclude  first}.


    Rajarshi> x <- as.data.frame(tdata)
    Rajarshi> x$tdepv <- tdepv
    Rajarshi> lnegth(tdepv) = 140

    Rajarshi> model <- lm(x$tdepv ~ x$V1 + x$V2 + x$V3 + x$V4, x)

    Rajarshi> pred <- predict(model, x)
    Rajarshi> length(pred) = 140

    Rajarshi> y <- as.data.frame(pdata)
    Rajarshi> y$pdepv <- pdepv
    Rajarshi> length(pdepv) = 16

    Rajarshi> pred <- predict(model, y)
    Rajarshi> length(pred) = 140

    Rajarshi> But I expect that length(pred) = 16

    Rajarshi> Why do I get a different length? Furthermore, the original formula
    Rajarshi> specified the variable tdepv which is not in the dataframe that I send
    Rajarshi> to predict() - should I also make a variable called tdepv in the
    Rajarshi> dataframe y?

    Rajarshi> Thanks,

    Rajarshi> -------------------------------------------------------------------
    Rajarshi> Rajarshi Guha <rxg218 at psu.edu> <http://jijo.cjb.net>
    Rajarshi> GPG Fingerprint: 0CCA 8EE2 2EEB 25E2 AB04 06F7 1BB9 E634 9B87 56EE
    Rajarshi> -------------------------------------------------------------------



From maechler at stat.math.ethz.ch  Thu Oct  2 11:46:20 2003
From: maechler at stat.math.ethz.ch (Martin Maechler)
Date: Thu, 2 Oct 2003 11:46:20 +0200
Subject: [R] pmatch questions
In-Reply-To: <009701c388c4$174262a0$5c13070a@PROCGEN>
References: <009701c388c4$174262a0$5c13070a@PROCGEN>
Message-ID: <16251.62444.934058.897390@gargle.gargle.HOWL>

>>>>> "Vito" == Vito Muggeo <vito.muggeo at giustizia.it>
>>>>>     on Thu, 2 Oct 2003 11:03:38 +0200 writes:

    Vito> Dear all,
    Vito> Below there are two, simple - I suppose, questions on using pmatch():

    >> pmatch("xx", c("cc","xxa"))
    Vito> [1] 2
    >> pmatch("a", c("cc","xxa"))
    Vito> [1] NA
    >> pmatch("xx", c("cc","xxa","xxb"))
    Vito> [1] NA

    Vito> I would like that the second call returns also 2, and
    Vito> the third call returns c(2,3) is it possible?

yes, if you use  grep() instead of pmatch()

{and grep() *is* mentioned in the "See Also" section of
 help(pmatch) which you have consulted before asking, right ?}

    Vito> many thanks

you're welcome.
Martin



From forkusam at yahoo.com  Thu Oct  2 11:56:11 2003
From: forkusam at yahoo.com (forkusam)
Date: Thu, 2 Oct 2003 02:56:11 -0700 (PDT)
Subject: [R] r editors
Message-ID: <20031002095611.98396.qmail@web10505.mail.yahoo.com>

Hi ,
I am programming on a windows system and have problems
using notepad which is my main editor.Each time I try
to open the editor from the R IDE, R crashes.
So I always have to copy my codes from notepad and
paste in R to run them.
CAn someone tell me if I am doing anything wrong or is
there a better editor(freeware) which I could get.
thanks
cilver

=====
=====================
Sylvie B. Forkusam
Eppelheimer Str.52/A2-5-2
69115 Heidelberg, Germany
Tel: (0049)-06221/346913
Mobile: 0179-6816276



From kwan022 at stat.auckland.ac.nz  Thu Oct  2 12:07:56 2003
From: kwan022 at stat.auckland.ac.nz (Ko-Kang Kevin Wang)
Date: Thu, 2 Oct 2003 22:07:56 +1200 (NZST)
Subject: [R] r editors
In-Reply-To: <20031002095611.98396.qmail@web10505.mail.yahoo.com>
Message-ID: <Pine.LNX.4.44.0310022206320.31661-100000@stat55.stat.auckland.ac.nz>

On Thu, 2 Oct 2003, forkusam wrote:
> 
> I am programming on a windows system and have problems
> using notepad which is my main editor.Each time I try
> to open the editor from the R IDE, R crashes.

Not sure what you meant by R IDE, do you mean Rgui?

> CAn someone tell me if I am doing anything wrong or is
> there a better editor(freeware) which I could get.

If you are really "programming" in R, you really should get Emacs/ESS....

-- 
Cheers,

Kevin

------------------------------------------------------------------------------
"On two occasions, I have been asked [by members of Parliament],
'Pray, Mr. Babbage, if you put into the machine wrong figures, will
the right answers come out?' I am not able to rightly apprehend the
kind of confusion of ideas that could provoke such a question."

-- Charles Babbage (1791-1871) 
---- From Computer Stupidities: http://rinkworks.com/stupid/

--
Ko-Kang Kevin Wang
Master of Science (MSc) Student
SLC Tutor and Lab Demonstrator
Department of Statistics
University of Auckland
New Zealand
Homepage: http://www.stat.auckland.ac.nz/~kwan022
Ph: 373-7599
    x88475 (City)
    x88480 (Tamaki)



From Rau at demogr.mpg.de  Thu Oct  2 12:17:31 2003
From: Rau at demogr.mpg.de (Rau, Roland)
Date: Thu, 2 Oct 2003 12:17:31 +0200
Subject: [R] r editors
Message-ID: <3699CDBC4ED5D511BE6400306E1C0D8114CD1B@hermes.demogr.mpg.de>

Hi,

I can recommend two editors:
- Emacs/XEmacs in conjunction with the ESS-package (ESS= Emacs Speaks
Statistics). It is free software. However, it takes a while until it is
working nicely - especially if you have no experience with Emacs/XEmacs. One
of the nice features is that you can run R from within XEmacs (I guess also
within Emacs, but I am using XEmacs).
Some URLs:
http://www.gnu.org/directory/text/editors/emacs.html
http://www.xemacs.org
http://stat.ethz.ch/ESS/

- WinEdt is not freeware but I think it is a nice working environment if you
include Uwe Ligges' plug-in which can be accessed from:
http://cran.r-project.org/contrib/extra/winedt/
Best thing is probably to download the shareware version of WinEdt (from
www.winedt.com), include the R-WinEdt-plug-in and try out if this setting
suits you.

Best,
Roland





> -----Original Message-----
> From:	forkusam [SMTP:forkusam at yahoo.com]
> Sent:	Thursday, October 02, 2003 11:56 AM
> To:	r-help at stat.math.ethz.ch
> Subject:	[R] r editors
> 
> Hi ,
> I am programming on a windows system and have problems
> using notepad which is my main editor.Each time I try
> to open the editor from the R IDE, R crashes.
> So I always have to copy my codes from notepad and
> paste in R to run them.
> CAn someone tell me if I am doing anything wrong or is
> there a better editor(freeware) which I could get.
> thanks
> cilver
> 
> =====
> =====================
> Sylvie B. Forkusam
> Eppelheimer Str.52/A2-5-2
> 69115 Heidelberg, Germany
> Tel: (0049)-06221/346913
> Mobile: 0179-6816276
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help


+++++
This mail has been sent through the MPI for Demographic Research.  Should you receive a mail that is apparently from a MPI user without this text displayed, then the address has most likely been faked.   If you are uncertain about the validity of this message, please check the mail header or ask your system administrator for assistance.



From ligges at statistik.uni-dortmund.de  Thu Oct  2 12:21:01 2003
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Thu, 02 Oct 2003 12:21:01 +0200
Subject: [R] r editors
In-Reply-To: <20031002095611.98396.qmail@web10505.mail.yahoo.com>
References: <20031002095611.98396.qmail@web10505.mail.yahoo.com>
Message-ID: <3F7BFC0D.4030701@statistik.uni-dortmund.de>

forkusam wrote:

> Hi ,
> I am programming on a windows system and have problems
> using notepad which is my main editor.Each time I try
> to open the editor from the R IDE, R crashes.

I guess it looks like R freezes (and R does *not* crash), because R 
waits until the editor is closed again. That's happens for every editor.
Why don't you start your favorite editor in another way?

Uwe Ligges


> So I always have to copy my codes from notepad and
> paste in R to run them.
> CAn someone tell me if I am doing anything wrong or is
> there a better editor(freeware) which I could get.
> thanks
> cilver
> 
> =====
> =====================
> Sylvie B. Forkusam
> Eppelheimer Str.52/A2-5-2
> 69115 Heidelberg, Germany
> Tel: (0049)-06221/346913
> Mobile: 0179-6816276
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help



From landau at poleia.lip6.fr  Thu Oct  2 12:41:15 2003
From: landau at poleia.lip6.fr (landau@poleia.lip6.fr)
Date: Thu, 2 Oct 2003 12:41:15 +0200 (CEST)
Subject: [R] var on a vector
Message-ID: <53017.81.57.147.173.1065091275.squirrel@mailia.lip6.fr>

Hello

I am an "R" newbie. I have a problem with computing a variance on a vector.

> data(cars)
> variance <- function (x) mean(x^2)-mean(x)^2;
> variance(cars[,1])
[1] 27.4
> var(cars[,1])
[1] 27.95918

What did I assume/understand wrong ?

TIA

-- 
Samuel Landau



From dmurdoch at pair.com  Thu Oct  2 12:49:21 2003
From: dmurdoch at pair.com (Duncan Murdoch)
Date: Thu, 02 Oct 2003 06:49:21 -0400
Subject: [R] r editors
In-Reply-To: <20031002095611.98396.qmail@web10505.mail.yahoo.com>
References: <20031002095611.98396.qmail@web10505.mail.yahoo.com>
Message-ID: <vg0onvcmdac4h3s4tab7ibr4ckhsjrsbe7@4ax.com>

On Thu, 2 Oct 2003 02:56:11 -0700 (PDT), you wrote:

>Hi ,
>I am programming on a windows system and have problems
>using notepad which is my main editor.Each time I try
>to open the editor from the R IDE, R crashes.

It shouldn't crash; can you give more details of how you're opening
the editor, and what the symptoms of the crash are?

One thing that might be happening:  if you use

 edit(myfunction)

then Notepad will start, and R will wait for it to finish before doing
anything else.  It hasn't crashed, but it looks like it has until you
shut down Notepad.

Duncan Murdoch



From marten.bjellerup at ehv.vxu.se  Thu Oct  2 12:50:13 2003
From: marten.bjellerup at ehv.vxu.se (=?Windows-1252?Q?M=E5rten_Bjellerup?=)
Date: Thu, 02 Oct 2003 12:50:13 +0200
Subject: [R] Query: What is 'Trellis'?
Message-ID: <007801c388d2$f56c6fe0$5a4d2fc2@MBJEHV>

I'm an R-beginner and have found the function 'panel.mathdensity' in the full manual. R can't find the function and under 'Description' in the manual it says that they "are available in Trellis". What is it and where can I find the function?

Regards,

M?rten


M?rten Bjellerup
Doctoral Student in Economics
School of Management and Economics
V?xj? University
SE-351 95  V?xj?
Sweden

Tel: +46 470 708410 
Fax: +46 470 82478 
Mobile: +46 70 969 88 88 
Mail: marten.bjellerup at ehv.vxu.se 
Web: http://www.ehv.vxu.se
-------------------------------------
"Forecasting is like trying to
drive a car blindfolded and
following directions given 
by a person who is looking
out of the back window"



From vito.muggeo at giustizia.it  Thu Oct  2 13:10:53 2003
From: vito.muggeo at giustizia.it (Vito Muggeo)
Date: Thu, 2 Oct 2003 13:10:53 +0200
Subject: R: [R] r editors
References: <20031002095611.98396.qmail@web10505.mail.yahoo.com>
Message-ID: <00f701c388d5$db368220$5c13070a@PROCGEN>

In addition to WinEdt and (X)Emacs of course,
See also,
http://www.crimsoneditor.com/ (freeware)
http://www.jedit.org/ (freeware)
http://www.editpadpro.com/ (non-freeware)

I know that there exist Syntax Highlighting files for R, but I don't know
where you can find them.

best,
vito

----- Original Message -----
From: forkusam <forkusam at yahoo.com>
To: <r-help at stat.math.ethz.ch>
Sent: Thursday, October 02, 2003 11:56 AM
Subject: [R] r editors


> Hi ,
> I am programming on a windows system and have problems
> using notepad which is my main editor.Each time I try
> to open the editor from the R IDE, R crashes.
> So I always have to copy my codes from notepad and
> paste in R to run them.
> CAn someone tell me if I am doing anything wrong or is
> there a better editor(freeware) which I could get.
> thanks
> cilver
>
> =====
> =====================
> Sylvie B. Forkusam
> Eppelheimer Str.52/A2-5-2
> 69115 Heidelberg, Germany
> Tel: (0049)-06221/346913
> Mobile: 0179-6816276
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help



From wolski at molgen.mpg.de  Thu Oct  2 13:30:24 2003
From: wolski at molgen.mpg.de (Wolski)
Date: Thu, 02 Oct 2003 13:30:24 +0200
Subject: [R] "[[<-","[[" default? 
Message-ID: <200310021330240070.00DDA51E@harry.molgen.mpg.de>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20031002/f159c523/attachment.pl

From glaziou at pasteur-kh.org  Thu Oct  2 13:44:04 2003
From: glaziou at pasteur-kh.org (Philippe Glaziou)
Date: Thu, 2 Oct 2003 18:44:04 +0700
Subject: [R] var on a vector
In-Reply-To: <53017.81.57.147.173.1065091275.squirrel@mailia.lip6.fr>
References: <53017.81.57.147.173.1065091275.squirrel@mailia.lip6.fr>
Message-ID: <20031002114404.GA27800@pasteur-kh.org>

landau at poleia.lip6.fr <landau at poleia.lip6.fr> wrote:
> I am an "R" newbie. I have a problem with computing a variance
> on a vector.
> 
> > data(cars) variance <- function (x) mean(x^2)-mean(x)^2;
> > variance(cars[,1])
> [1] 27.4
> > var(cars[,1])
> [1] 27.95918
> 
> What did I assume/understand wrong ?


You wrongly assumed that the 'var' help file would not help.
However, that page states that the denominator n-1 is used.

> 27.4*50/49
[1] 27.95918

-- 
Philippe



From Arne.Muller at aventis.com  Thu Oct  2 13:17:25 2003
From: Arne.Muller at aventis.com (Arne.Muller@aventis.com)
Date: Thu, 2 Oct 2003 13:17:25 +0200
Subject: [R] multi-dimensional hash
Message-ID: <C80ECAFA2ACC1B45BE45D133ED660ADE410B06@crbsmxsusr04.pharma.aventis.com>

Hello,

I was wondering what's the best data structure in R for a multi-dimensional
lookup table, and how to implement it. I've several categories say "A", "B",
"C" ... and within each of these categories there are other categories such
as "a", "b", "c", ... . There can be up to 5 dimensions. The actual value for
[A][a]... is then a vector.

	I'm looking forward to any suggestions,
	+thanks very much for your help,

	Arne



From Matthias.Kohl at uni-bayreuth.de  Thu Oct  2 14:55:07 2003
From: Matthias.Kohl at uni-bayreuth.de (Matthias Kohl)
Date: Thu, 02 Oct 2003 13:55:07 +0100
Subject: [R] var on a vector
In-Reply-To: <53017.81.57.147.173.1065091275.squirrel@mailia.lip6.fr>
References: <53017.81.57.147.173.1065091275.squirrel@mailia.lip6.fr>
Message-ID: <3F7C202B.8030905@uni-bayreuth.de>

landau at poleia.lip6.fr schrieb:

>Hello
>
>I am an "R" newbie. I have a problem with computing a variance on a vector.
>
>  
>
>>data(cars)
>>variance <- function (x) mean(x^2)-mean(x)^2;
>>variance(cars[,1])
>>    
>>
>[1] 27.4
>  
>
>>var(cars[,1])
>>    
>>
>[1] 27.95918
>
>What did I assume/understand wrong ?
>
>TIA
>
>  
>
Hello,

help(var) says:

The denominator n - 1 is used which gives an unbiased estimator of
the (co)variance for i.i.d. observations.

Try:
n <- length(cars[,1])
var(cars[,1])*(n-1)/n

Matthias Kohl



From rpeng at jhsph.edu  Thu Oct  2 14:17:43 2003
From: rpeng at jhsph.edu (Roger D. Peng)
Date: Thu, 02 Oct 2003 08:17:43 -0400
Subject: [R] Query: What is 'Trellis'?
In-Reply-To: <007801c388d2$f56c6fe0$5a4d2fc2@MBJEHV>
References: <007801c388d2$f56c6fe0$5a4d2fc2@MBJEHV>
Message-ID: <3F7C1767.8090009@jhsph.edu>

You need to load the `lattice' library in R.  Try

>  library(lattice)
>  panel.mathdensity
function (dmath = dnorm, args = list(mean = 0, sd = 1), n = 50,
    col, col.line = reference.line$col, lwd = reference.line$lwd,
    lty = reference.line$lty, ...)
{
    reference.line <- trellis.par.get("reference.line")
    if (!missing(col)) {
        if (missing(col.line))
            col.line <- col
    }
    x <- do.breaks(endpoints = current.viewport()$xscale, nint = n)
    y <- do.call("dmath", c(list(x = x), args))
    llines(x = x, y = y, col = col.line, lwd = lwd, lty = lty,
        ...)
}


M?rten Bjellerup wrote:

>I'm an R-beginner and have found the function 'panel.mathdensity' in the full manual. R can't find the function and under 'Description' in the manual it says that they "are available in Trellis". What is it and where can I find the function?
>
>Regards,
>
>M?rten
>
>
>M?rten Bjellerup
>Doctoral Student in Economics
>School of Management and Economics
>V?xj? University
>SE-351 95  V?xj?
>Sweden
>
>Tel: +46 470 708410 
>Fax: +46 470 82478 
>Mobile: +46 70 969 88 88 
>Mail: marten.bjellerup at ehv.vxu.se 
>Web: http://www.ehv.vxu.se
>-------------------------------------
>"Forecasting is like trying to
>drive a car blindfolded and
>following directions given 
>by a person who is looking
>out of the back window"
>
>______________________________________________
>R-help at stat.math.ethz.ch mailing list
>https://www.stat.math.ethz.ch/mailman/listinfo/r-help
>
>  
>



From hb at maths.lth.se  Thu Oct  2 14:35:30 2003
From: hb at maths.lth.se (Henrik Bengtsson)
Date: Thu, 2 Oct 2003 14:35:30 +0200
Subject: [R] "[[<-","[[" default? 
In-Reply-To: <200310021330240070.00DDA51E@harry.molgen.mpg.de>
Message-ID: <001c01c388e1$aac25ef0$e502eb82@maths.lth.se>

It depends, but a quick answer is that you in general should use
NextMethod() as far as possible to avoid ad hoc solutions and tricky
side effects. Example:

x <- list(a=1:10, b=base::letters);
class(x) <- "ClassA";

"[[.ClassA" <- function(object, name) {
  # Here you are allowed to do something, cf. UseMethod().
  name <- tolower(name);  # e.g. make x[["A"]] equal to x[["a"]].
  NextMethod("[[");
}

If ClassA inherits from ClassB and there is a function "[[.ClassB"()
defined, then this will of course *not* call "[[.default"(), but
"[[.ClassB"(). However, this what you should expect from a good
object-oriented design!

Maybe the above answers your question. If not, continue to read.

I can't recall the example, but I have indeed seen a case where we
wanted to called the default method explicitly. To do this, the only
thing you can do is unfortunately to unclass() your object and this
could have side effects that you do not want:

"[[.ClassA" <- function(object, name) {
  # Here you are allowed to do something, cf. UseMethod().
  name <- tolower(name);  # e.g. make x[["A"]] equal to x[["a"]].
  unclass(object)[[name]];
}

unclass() should normally be avoided since it is a nasty workaround.
Probably not in this case with "[[", but with other generic functions,
it can happen that default method in turn calls other generic functions.
Since you have unclassed the object the wrong method will be called by
those downstream generic functions. 

My conclusion is that, and maybe this is what you are fishing for, there
should really exist a generic function and a separate default function
for "[[". What do the R-developers think?

The following will of course not work (it will generate an infinite
recursive call):

"[[.ClassA" <- function(object, name) {
  # Here you are allowed to do something, cf. UseMethod().
  name <- tolower(name);  # e.g. make x[["A"]] equal to x[["a"]].
  fcn <- .Primitive("[[");
  # or fcn <- get("[[", mode="function");
  fcn(object, name);
}

Henrik Bengtsson
Lund University

> -----Original Message-----
> From: r-help-bounces at stat.math.ethz.ch 
> [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Wolski
> Sent: den 2 oktober 2003 13:30
> To: r-help at stat.math.ethz.ch
> Subject: [R] "[[<-","[[" default? 
> 
> 
> Hi!
> 
> I have implemented class specific behaviour of 
> "[[<-.myclass"<-function(). How it is posible to call the 
> "[[.default" on an object of myclass?
> 
> Eryk
> 
> Dipl. bio-chem. Eryk Witold Wolski    @    MPI-MG Dep. 
> Vertebrate Genomics
> Ihnestrasse 73 14195 Berlin          'v'
> tel: 0049-30-84131285               /   \
> mail: wolski at molgen.mpg.de        ---W-W----
> 
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list 
> https://www.stat.math.ethz.ch/mailman/listinfo> /r-help



From partha_bagchi at hgsi.com  Thu Oct  2 14:30:04 2003
From: partha_bagchi at hgsi.com (partha_bagchi@hgsi.com)
Date: Thu, 2 Oct 2003 08:30:04 -0400
Subject: [R] r editors
Message-ID: <OF58502E97.2923D224-ON85256DB3.00441B0F-85256DB3.0044AC71@hgsi.com>

I can also recommend another editor (for the Windows platform) that is 
freeware and comes with syntax highlighting for R and a variety of other 
languages. It called SourceEdit. It is a wonderful editor by Joacim 
Anderson and I highly recommend it. You can download it from:

http://www.brixoft.com/







"Rau, Roland" <Rau at demogr.mpg.de>
Sent by: r-help-bounces at stat.math.ethz.ch
10/02/2003 06:17 AM

 
        To:     "'forkusam at yahoo.com'" <forkusam at yahoo.com>, r-help at stat.math.ethz.ch
        cc: 
        Subject:        RE: [R] r editors


Hi,

I can recommend two editors:
- Emacs/XEmacs in conjunction with the ESS-package (ESS= Emacs Speaks
Statistics). It is free software. However, it takes a while until it is
working nicely - especially if you have no experience with Emacs/XEmacs. 
One
of the nice features is that you can run R from within XEmacs (I guess 
also
within Emacs, but I am using XEmacs).
Some URLs:
http://www.gnu.org/directory/text/editors/emacs.html
http://www.xemacs.org
http://stat.ethz.ch/ESS/

- WinEdt is not freeware but I think it is a nice working environment if 
you
include Uwe Ligges' plug-in which can be accessed from:
http://cran.r-project.org/contrib/extra/winedt/
Best thing is probably to download the shareware version of WinEdt (from
www.winedt.com), include the R-WinEdt-plug-in and try out if this setting
suits you.

Best,
Roland





> -----Original Message-----
> From: forkusam [SMTP:forkusam at yahoo.com]
> Sent: Thursday, October 02, 2003 11:56 AM
> To:   r-help at stat.math.ethz.ch
> Subject:      [R] r editors
>
> Hi ,
> I am programming on a windows system and have problems
> using notepad which is my main editor.Each time I try
> to open the editor from the R IDE, R crashes.
> So I always have to copy my codes from notepad and
> paste in R to run them.
> CAn someone tell me if I am doing anything wrong or is
> there a better editor(freeware) which I could get.
> thanks
> cilver
>
> =====
> =====================
> Sylvie B. Forkusam
> Eppelheimer Str.52/A2-5-2
> 69115 Heidelberg, Germany
> Tel: (0049)-06221/346913
> Mobile: 0179-6816276
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help


+++++
This mail has been sent through the MPI for Demographic Research.  Should 
you receive a mail that is apparently from a MPI user without this text 
displayed, then the address has most likely been faked.   If you are 
uncertain about the validity of this message, please check the mail header 
or ask your system administrator for assistance.

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://www.stat.math.ethz.ch/mailman/listinfo/r-help

--
This message has been scanned for viruses and
dangerous content by MailScanner, and is
believed to be clean.



From jccorrea at unalmed.edu.co  Thu Oct  2 15:08:50 2003
From: jccorrea at unalmed.edu.co (Juan Carlos Correa Morales)
Date: Thu, 02 Oct 2003 08:08:50 -0500 (GMT)
Subject: R: [R] r editors
In-Reply-To: <00f701c388d5$db368220$5c13070a@PROCGEN>
Message-ID: <Pine.GSO.4.44.0310020807450.8281-100000@eris.unalmed.edu.co>

Hi:

Also ConTEXT (freeware) is a nice editor with sintax highlighting scripts
for R.


On Thu, 2 Oct 2003, Vito Muggeo wrote:

> In addition to WinEdt and (X)Emacs of course,
> See also,
> http://www.crimsoneditor.com/ (freeware)
> http://www.jedit.org/ (freeware)
> http://www.editpadpro.com/ (non-freeware)
>
> I know that there exist Syntax Highlighting files for R, but I don't know
> where you can find them.
>
> best,
> vito
>
> ----- Original Message -----
> From: forkusam <forkusam at yahoo.com>
> To: <r-help at stat.math.ethz.ch>
> Sent: Thursday, October 02, 2003 11:56 AM
> Subject: [R] r editors
>
>
> > Hi ,
> > I am programming on a windows system and have problems
> > using notepad which is my main editor.Each time I try
> > to open the editor from the R IDE, R crashes.
> > So I always have to copy my codes from notepad and
> > paste in R to run them.
> > CAn someone tell me if I am doing anything wrong or is
> > there a better editor(freeware) which I could get.
> > thanks
> > cilver
> >
> > =====
> > =====================
> > Sylvie B. Forkusam
> > Eppelheimer Str.52/A2-5-2
> > 69115 Heidelberg, Germany
> > Tel: (0049)-06221/346913
> > Mobile: 0179-6816276
> >
> > ______________________________________________
> > R-help at stat.math.ethz.ch mailing list
> > https://www.stat.math.ethz.ch/mailman/listinfo/r-help
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
>



From tblackw at umich.edu  Thu Oct  2 15:11:49 2003
From: tblackw at umich.edu (Thomas W Blackwell)
Date: Thu, 2 Oct 2003 09:11:49 -0400 (EDT)
Subject: [R] multi-dimensional hash
In-Reply-To: <C80ECAFA2ACC1B45BE45D133ED660ADE410B06@crbsmxsusr04.pharma.aventis.com>
References: <C80ECAFA2ACC1B45BE45D133ED660ADE410B06@crbsmxsusr04.pharma.aventis.com>
Message-ID: <Pine.SOL.4.58.0310020837120.7744@timepilot.gpcc.itd.umich.edu>

Arne  -

In the past, I've used a data frame for the lookup table and
the "and" of individual logical vectors to select rows from it.
Here's a simplified version of the selector function I wrote.
My mail editor does not balance parentheses, so I don't guarantee
that this version is syntactically correct.  Various sanity
checks have been omitted for clarity.

 #  return row numbers for the conjunction of matches to multiple
 #  named columns of data.  match is to the cross-product of indiv
 #  args and rows are returned in data set order.  the values to
 #  select for should be supplied as named arguments in ... .

select <- function(data, ...)
     {	test <- list(...)
	col.index <- match(names(test), names(data), 0)

	seq(length(data[[1]]))[ apply(sapply(seq(along=test),
	   function(i,d,n,t) match(d[[n[i]]], t[[i]], 0) > 0,
	   data, col.index, test), 1, all) ]			  }

In my own code, select() is called by any of several wrapper
functions which implement specific queries, and supply the names
of the columns on which they want to select.  The results from
several queries are saved as integer row numbers, then combined
and re-sorted before I actually subscript the data frame with
them to extract the data.

HTH  -  tom blackwell  -  u michigan medical school  -  ann arbor  -

On Thu, 2 Oct 2003 Arne.Muller at aventis.com wrote:

> I was wondering what's the best data structure in R for a multi-dimensional
> lookup table, and how to implement it. I've several categories say "A", "B",
> "C" ... and within each of these categories there are other categories such
> as "a", "b", "c", ... . There can be up to 5 dimensions. The actual value for
> [A][a]... is then a vector.
>
> 	I'm looking forward to any suggestions,
> 	+thanks very much for your help,
>
> 	Arne
>



From hdoran at nasdc.org  Thu Oct  2 15:16:59 2003
From: hdoran at nasdc.org (Harold Doran)
Date: Thu, 2 Oct 2003 09:16:59 -0400
Subject: [R] Doubly Multivariate LME
Message-ID: <66578BFC0BA55348B5907A0F798EE93013A0A6@ernesto.NASDC.ORG>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20031002/b77693c6/attachment.pl

From sudar_80 at neo.tamu.edu  Thu Oct  2 15:46:55 2003
From: sudar_80 at neo.tamu.edu (Padmanabhan, Sudharsha)
Date: Thu, 2 Oct 2003 13:46:55 -0000
Subject: [R] Power calculation
Message-ID: <200310021346.h92DktAu065924@smtp-relay.tamu.edu>


HI,

I ran into trouble trying to do a power computation.

In the presence of interaction, I am simulating clinical trials in multiple 
centers.


Suppose I have 3 centers, each with 20 patients, 10 in active drug and 10 in 
placebo and I have efficacy data for the 60 patients, ie, initial and final 
response variable scores for all 60 subjects. How do I compute the power of 
the study? (May be using an f-test?)

Regards

Krishna S P



From spencer.graves at pdf.com  Thu Oct  2 15:47:52 2003
From: spencer.graves at pdf.com (Spencer Graves)
Date: Thu, 02 Oct 2003 06:47:52 -0700
Subject: [R] how calculate mean for each group
In-Reply-To: <1065062313.4838.73.camel@localhost>
References: <1065057311.3f7b7c1f6ba6e@webmail.uoguelph.ca>
	<1065062313.4838.73.camel@localhost>
Message-ID: <3F7C2C88.6080204@pdf.com>

An alternative to renaming columns in the ouput of aggregate is to 
provide names in the "by" list as follows: 

aggregate(df$treatment, list(gp=df$group, dup=df$duplicate), mean)

hope this helps.  spencer graves

Marc Schwartz wrote:

>On Wed, 2003-10-01 at 20:15, szhan at uoguelph.ca wrote:
>  
>
>>Hello, R experts:
>>I got data like this:
>>group   duplicate   treatment 
>>A         Y          5
>>A         Y          3
>>A         N          6
>>B         Y          2
>>B         N          4
>>B         Y          1
>>How to sort the data and calculate the average treatment value for each group 
>>in two level of duplicate. Results like this:
>>group   duplicate   treatment 
>>A         Y          4
>>A         N          6
>>B         Y          1.5
>>B         N          4
>>Thank you in advance.
>>
>>Josh
>>    
>>
>
>
># Create a dataframe
>df <- data.frame(group = c(rep("A", 3), rep("B", 3)), 
>                 duplicate = c("Y", "Y", "N", "Y", "N", "Y"), 
>                 treatment = c(5, 3, 6, 2, 4, 1))
>
># Use aggregate
>aggregate(df$treatment, list(df$group, df$duplicate), mean)
>  Group.1 Group.2   x
>1       A       N 6.0
>2       B       N 4.0
>3       A       Y 4.0
>4       B       Y 1.5
>
>Aggregate returns a data frame in this case, so that you can then set
>the colnames and order the output if you wish.
>
>HTH,
>
>Marc Schwartz
>
>______________________________________________
>R-help at stat.math.ethz.ch mailing list
>https://www.stat.math.ethz.ch/mailman/listinfo/r-help
>  
>



From MSchwartz at medanalytics.com  Thu Oct  2 16:37:15 2003
From: MSchwartz at medanalytics.com (Marc Schwartz)
Date: Thu, 02 Oct 2003 09:37:15 -0500
Subject: [R] how calculate mean for each group
In-Reply-To: <3F7C2C88.6080204@pdf.com>
References: <1065057311.3f7b7c1f6ba6e@webmail.uoguelph.ca>
	<1065062313.4838.73.camel@localhost>  <3F7C2C88.6080204@pdf.com>
Message-ID: <1065105435.4838.151.camel@localhost>

On Thu, 2003-10-02 at 08:47, Spencer Graves wrote:
> An alternative to renaming columns in the ouput of aggregate is to 
> provide names in the "by" list as follows: 
> 
> aggregate(df$treatment, list(gp=df$group, dup=df$duplicate), mean)
> 
> hope this helps.  spencer graves

SNIP

Spencer,

Yeah, knew that. Using the above you would get:

> aggregate(df$treatment, list(gp=df$group, dup=df$duplicate), mean)
  gp dup   x
1  A   N 6.0
2  B   N 4.0
3  A   Y 4.0
4  B   Y 1.5

Which still leaves the mean column generically labeled as 'x'.

To take it one more step, given the way in which aggregate.data.frame is
coded and the way in which df$treatment is passed as a vector, you could
use the following to label the mean column as 'treatment':

df <- data.frame(group = c(rep("A", 3), rep("B", 3)),
                 duplicate = c("Y", "Y", "N", "Y", "N", "Y"),
                 treatment = c(5, 3, 6, 2, 4, 1))

attach(df)

aggregate(as.data.frame(treatment), 
          list(group = group, duplicate = duplicate), mean)

which yields:

  group duplicate treatment
1     A         N       6.0
2     B         N       4.0
3     A         Y       4.0
4     B         Y       1.5


Remember to 'detach(df)'.

Doing it this way, 'treatment' retains the name attribute when passed to
aggregate, rather than as a vector.

Thanks for pointing that out.

Regards,

Marc



From marten.bjellerup at ehv.vxu.se  Thu Oct  2 16:56:11 2003
From: marten.bjellerup at ehv.vxu.se (=?iso-8859-1?Q?M=E5rten_Bjellerup?=)
Date: Thu, 02 Oct 2003 16:56:11 +0200
Subject: [R] Query: weighting cells in histogram
Message-ID: <00b401c388f5$519eea00$5a4d2fc2@MBJEHV>

I have the 'breaks' for the histogram ('hist') but I want weight the cells instead of using actual observations. I thought that using freq=FALSE implied that the numbers in 'x' were weights but this turned out to be wrong.
Any help and/or comment is very much appreciated.

Regards,

M?rten

M?rten Bjellerup
Doctoral Student in Economics
School of Management and Economics
V?xj? University
SE-351 95  V?xj?
Sweden

Tel: +46 470 708410 
Fax: +46 470 82478 
Mobile: +46 70 969 88 88 
Mail: marten.bjellerup at ehv.vxu.se 
Web: http://www.ehv.vxu.se
-------------------------------------
"Forecasting is like trying to
drive a car blindfolded and
following directions given 
by a person who is looking
out of the back window"



From deepayan at stat.wisc.edu  Thu Oct  2 16:59:34 2003
From: deepayan at stat.wisc.edu (Deepayan Sarkar)
Date: Thu, 2 Oct 2003 09:59:34 -0500
Subject: [R] Query: What is 'Trellis'?
In-Reply-To: <007801c388d2$f56c6fe0$5a4d2fc2@MBJEHV>
References: <007801c388d2$f56c6fe0$5a4d2fc2@MBJEHV>
Message-ID: <200310020959.34788.deepayan@stat.wisc.edu>


See

http://cm.bell-labs.com/cm/ms/departments/sia/project/trellis/

This is implemented in the lattice package in R, which you can load by 

library(lattice)

and perhaps start with 

help(Lattice)

On Thursday 02 October 2003 05:50, M?rten Bjellerup wrote:
> I'm an R-beginner and have found the function 'panel.mathdensity' in the
> full manual. 

Just out of curiosity, which manual would that be ?

> R can't find the function and under 'Description' in the
> manual it says that they "are available in Trellis". What is it and where
> can I find the function?

Deepayan



From tblackw at umich.edu  Thu Oct  2 17:13:56 2003
From: tblackw at umich.edu (Thomas W Blackwell)
Date: Thu, 2 Oct 2003 11:13:56 -0400 (EDT)
Subject: [R] Query: weighting cells in histogram
In-Reply-To: <00b401c388f5$519eea00$5a4d2fc2@MBJEHV>
References: <00b401c388f5$519eea00$5a4d2fc2@MBJEHV>
Message-ID: <Pine.SOL.4.58.0310021105330.2978@timepilot.gpcc.itd.umich.edu>

Marten  -

I don't know exactly what interpretation you have in mind
for weights, but if you assign the value of  hist()  to a
variable tmp, you can then assign the component  tmp$counts
any value you like, and plot the result as a histogram
using  plot(tmp).  See the section "Value:" in  help("hist").

-  tom blackwell  -  u michigan medical school  -  ann arbor  -

On Thu, 2 Oct 2003, [iso-8859-1] M?rten Bjellerup wrote:

> I have the 'breaks' for the histogram ('hist') but I want
> to weight the cells instead of using actual observations.
> I thought that using freq=FALSE implied that the numbers
> in 'x' were weights but this turned out to be wrong.
> Any help and/or comment is very much appreciated.
>
> Regards,
>
> M?rten
>
> M?rten Bjellerup
> Doctoral Student in Economics
> School of Management and Economics
> V?xj? University
> SE-351 95  V?xj?
> Sweden
>
> Tel: +46 470 708410
> Fax: +46 470 82478
> Mobile: +46 70 969 88 88
> Mail: marten.bjellerup at ehv.vxu.se
> Web: http://www.ehv.vxu.se



From Simon.Fear at synequanon.com  Thu Oct  2 17:11:53 2003
From: Simon.Fear at synequanon.com (Simon Fear)
Date: Thu, 2 Oct 2003 16:11:53 +0100
Subject: [R] how calculate mean for each group
Message-ID: <6C8A8033ABC1E3468048ABC4F13CE572AC0E03@synequanon01>

A little more succinctly and I hope also helpful:

> with(df, aggregate(list(mean=treatment),
      list(group = group, duplicate = duplicate), mean))

Note that the name of the summary of treatment is likely
best to be different from the name of the treatment
variable itself. It is tempting to do something fancy picking
up the name of the called function automatically, but
I mostly use an unnamed summary function (x) { ...}, so I won't
bother.

(BTW did I ever mention here how much I prefer with() to 
attach()/forget.to.detach() ?)

> From: Marc Schwartz [mailto:MSchwartz at medanalytics.com]
> Sent: 02 October 2003 15:37

> attach(df)
> aggregate(as.data.frame(treatment), 
>           list(group = group, duplicate = duplicate), mean)
> ### Remember to 'detach(df)'.
> 
> Doing it this way, 'treatment' retains the name attribute 
> when passed to aggregate, rather than as a vector.
 

Simon Fear
Senior Statistician
Syne qua non Ltd
Tel: +44 (0) 1379 644449
Fax: +44 (0) 1379 644445
email: Simon.Fear at synequanon.com
web: http://www.synequanon.com
 
Number of attachments included with this message: 0
 
This message (and any associated files) is confidential and\...{{dropped}}



From rxg218 at psu.edu  Thu Oct  2 17:20:52 2003
From: rxg218 at psu.edu (Rajarshi Guha)
Date: Thu, 02 Oct 2003 15:20:52 -0000
Subject: [R] using a string as the formula in rlm
Message-ID: <1065108309.7607.4.camel@ra.chem.psu.edu>

Hi,
 I am trying to build a series of rlm models. I have my data frame and
the models will be built using various coulmns of the data frame.

Thus a series of models would be

m1 <- rlm(V1 ~ V2 + V3 + V4, data)
m2 <- rlm(V1 ~ V2 + V5 + V7, data)
m3 <- rlm(V1 ~ V2 + V8 + V9, data)

I would like to automate this. Is it possible to use a string in place
of the formula?

I tried doing:

fmla <- sprintf('V1 ~ V%g + V%g + V%g',2,3,4)
m1 <-rlm(fmla,data)

I get:

Error in qr(x) : NA/NaN/Inf in foreign function call (arg 1)
In addition: Warning message:
NAs introduced by coercion

I can understand why this method results in the error but is there any
way to get around this?

Thanks,

-------------------------------------------------------------------
Rajarshi Guha <rxg218 at psu.edu> <http://jijo.cjb.net>
GPG Fingerprint: 0CCA 8EE2 2EEB 25E2 AB04 06F7 1BB9 E634 9B87 56EE
-------------------------------------------------------------------
Gods are fragile things; they may be killed by a whiff of
science or a dose of common sense.
-- Chapman Cohen



From spencer.graves at pdf.com  Thu Oct  2 17:28:20 2003
From: spencer.graves at pdf.com (Spencer Graves)
Date: Thu, 02 Oct 2003 08:28:20 -0700
Subject: [R] Query: weighting cells in histogram
In-Reply-To: <00b401c388f5$519eea00$5a4d2fc2@MBJEHV>
References: <00b401c388f5$519eea00$5a4d2fc2@MBJEHV>
Message-ID: <3F7C4414.7070407@pdf.com>

      "hist" returns a list, to which you can apply your weights. 

      Alternatively, in R 1.7.1, typing "hist" [without the quotes (")] 
at a commands prompt reveals a call to UseMethod.  'methods("hist")' 
identifies functions "hist.default" and "hist.POSIXt".  Typing 
"hist.default" prodused the R code, which you can then customize any way 
you want.  The protocol in R 1.8 for this is different, but the same 
concepts should apply, I believe. 

      hope this helps.  spencer graves

M?rten Bjellerup wrote:

>I have the 'breaks' for the histogram ('hist') but I want weight the cells instead of using actual observations. I thought that using freq=FALSE implied that the numbers in 'x' were weights but this turned out to be wrong.
>Any help and/or comment is very much appreciated.
>
>Regards,
>
>M?rten
>
>M?rten Bjellerup
>Doctoral Student in Economics
>School of Management and Economics
>V?xj? University
>SE-351 95  V?xj?
>Sweden
>
>Tel: +46 470 708410 
>Fax: +46 470 82478 
>Mobile: +46 70 969 88 88 
>Mail: marten.bjellerup at ehv.vxu.se 
>Web: http://www.ehv.vxu.se
>-------------------------------------
>"Forecasting is like trying to
>drive a car blindfolded and
>following directions given 
>by a person who is looking
>out of the back window"
>
>______________________________________________
>R-help at stat.math.ethz.ch mailing list
>https://www.stat.math.ethz.ch/mailman/listinfo/r-help
>  
>



From andy_liaw at merck.com  Thu Oct  2 17:39:19 2003
From: andy_liaw at merck.com (Liaw, Andy)
Date: Thu, 02 Oct 2003 11:39:19 -0400
Subject: [R] using a string as the formula in rlm
Message-ID: <3A822319EB35174CA3714066D590DCD50205CC02@usrymx25.merck.com>

It may help you to read Bill Venables' column in R News
http://cran.r-project.org/doc/Rnews/Rnews_2002-2.pdf, pages 24-26.

Andy

> -----Original Message-----
> From: Rajarshi Guha [mailto:rxg218 at psu.edu] 
> Sent: Thursday, October 02, 2003 11:25 AM
> To: r-help at stat.math.ethz.ch
> Subject: [R] using a string as the formula in rlm
> 
> 
> Hi,
>  I am trying to build a series of rlm models. I have my data 
> frame and the models will be built using various coulmns of 
> the data frame.
> 
> Thus a series of models would be
> 
> m1 <- rlm(V1 ~ V2 + V3 + V4, data)
> m2 <- rlm(V1 ~ V2 + V5 + V7, data)
> m3 <- rlm(V1 ~ V2 + V8 + V9, data)
> 
> I would like to automate this. Is it possible to use a string 
> in place of the formula?
> 
> I tried doing:
> 
> fmla <- sprintf('V1 ~ V%g + V%g + V%g',2,3,4)
> m1 <-rlm(fmla,data)
> 
> I get:
> 
> Error in qr(x) : NA/NaN/Inf in foreign function call (arg 1)
> In addition: Warning message:
> NAs introduced by coercion
> 
> I can understand why this method results in the error but is 
> there any way to get around this?
> 
> Thanks,
> 
> -------------------------------------------------------------------
> Rajarshi Guha <rxg218 at psu.edu> <http://jijo.cjb.net>
> GPG Fingerprint: 0CCA 8EE2 2EEB 25E2 AB04 06F7 1BB9 E634 9B87 56EE
> -------------------------------------------------------------------
> Gods are fragile things; they may be killed by a whiff of 
> science or a dose of common sense.
> -- Chapman Cohen
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list 
> https://www.stat.math.ethz.ch/mailman/listinfo> /r-help
>



From MSchwartz at medanalytics.com  Thu Oct  2 17:41:34 2003
From: MSchwartz at medanalytics.com (Marc Schwartz)
Date: Thu, 02 Oct 2003 10:41:34 -0500
Subject: [R] how calculate mean for each group
In-Reply-To: <6C8A8033ABC1E3468048ABC4F13CE572AC0E03@synequanon01>
References: <6C8A8033ABC1E3468048ABC4F13CE572AC0E03@synequanon01>
Message-ID: <1065109294.4838.184.camel@localhost>

On Thu, 2003-10-02 at 10:11, Simon Fear wrote:
> A little more succinctly and I hope also helpful:
> 
> > with(df, aggregate(list(mean=treatment),
>       list(group = group, duplicate = duplicate), mean))
> 
> Note that the name of the summary of treatment is likely
> best to be different from the name of the treatment
> variable itself. It is tempting to do something fancy picking
> up the name of the called function automatically, but
> I mostly use an unnamed summary function (x) { ...}, so I won't
> bother.
> 
> (BTW did I ever mention here how much I prefer with() to 
> attach()/forget.to.detach() ?)


LOL...

I supposed that this might be the time to raise the possibility of an
"Obfuscated R" contest?  ;-)

Thanks Simon.

Marc



From ligges at statistik.uni-dortmund.de  Thu Oct  2 17:43:01 2003
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Thu, 02 Oct 2003 17:43:01 +0200
Subject: [R] using a string as the formula in rlm
In-Reply-To: <1065108309.7607.4.camel@ra.chem.psu.edu>
References: <1065108309.7607.4.camel@ra.chem.psu.edu>
Message-ID: <3F7C4785.9030506@statistik.uni-dortmund.de>

Rajarshi Guha wrote:

> Hi,
>  I am trying to build a series of rlm models. I have my data frame and
> the models will be built using various coulmns of the data frame.
> 
> Thus a series of models would be
> 
> m1 <- rlm(V1 ~ V2 + V3 + V4, data)
> m2 <- rlm(V1 ~ V2 + V5 + V7, data)
> m3 <- rlm(V1 ~ V2 + V8 + V9, data)
> 
> I would like to automate this. Is it possible to use a string in place
> of the formula?
> 
> I tried doing:
> 
> fmla <- sprintf('V1 ~ V%g + V%g + V%g',2,3,4)

See ?as.formula

Uwe Ligges


> m1 <-rlm(fmla,data)
> 
> I get:
> 
> Error in qr(x) : NA/NaN/Inf in foreign function call (arg 1)
> In addition: Warning message:
> NAs introduced by coercion
> 
> I can understand why this method results in the error but is there any
> way to get around this?
> 
> Thanks,
> 
> -------------------------------------------------------------------
> Rajarshi Guha <rxg218 at psu.edu> <http://jijo.cjb.net>
> GPG Fingerprint: 0CCA 8EE2 2EEB 25E2 AB04 06F7 1BB9 E634 9B87 56EE
> -------------------------------------------------------------------
> Gods are fragile things; they may be killed by a whiff of
> science or a dose of common sense.
> -- Chapman Cohen
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help



From spencer.graves at pdf.com  Thu Oct  2 17:47:54 2003
From: spencer.graves at pdf.com (Spencer Graves)
Date: Thu, 02 Oct 2003 08:47:54 -0700
Subject: [R] using a string as the formula in rlm
In-Reply-To: <1065108309.7607.4.camel@ra.chem.psu.edu>
References: <1065108309.7607.4.camel@ra.chem.psu.edu>
Message-ID: <3F7C48AA.4090204@pdf.com>

Have you considered the following: 

      fmla <- formula(sprintf('V1 ~ V%g + V%g + V%g',2,3,4))

hope this helps.  spencer graves

Rajarshi Guha wrote:

>Hi,
> I am trying to build a series of rlm models. I have my data frame and
>the models will be built using various coulmns of the data frame.
>
>Thus a series of models would be
>
>m1 <- rlm(V1 ~ V2 + V3 + V4, data)
>m2 <- rlm(V1 ~ V2 + V5 + V7, data)
>m3 <- rlm(V1 ~ V2 + V8 + V9, data)
>
>I would like to automate this. Is it possible to use a string in place
>of the formula?
>
>I tried doing:
>
>fmla <- sprintf('V1 ~ V%g + V%g + V%g',2,3,4)
>m1 <-rlm(fmla,data)
>
>I get:
>
>Error in qr(x) : NA/NaN/Inf in foreign function call (arg 1)
>In addition: Warning message:
>NAs introduced by coercion
>
>I can understand why this method results in the error but is there any
>way to get around this?
>
>Thanks,
>
>-------------------------------------------------------------------
>Rajarshi Guha <rxg218 at psu.edu> <http://jijo.cjb.net>
>GPG Fingerprint: 0CCA 8EE2 2EEB 25E2 AB04 06F7 1BB9 E634 9B87 56EE
>-------------------------------------------------------------------
>Gods are fragile things; they may be killed by a whiff of
>science or a dose of common sense.
>-- Chapman Cohen
>
>______________________________________________
>R-help at stat.math.ethz.ch mailing list
>https://www.stat.math.ethz.ch/mailman/listinfo/r-help
>  
>



From MSchwartz at medanalytics.com  Thu Oct  2 17:50:56 2003
From: MSchwartz at medanalytics.com (Marc Schwartz)
Date: Thu, 02 Oct 2003 10:50:56 -0500
Subject: [R] using a string as the formula in rlm
In-Reply-To: <1065108309.7607.4.camel@ra.chem.psu.edu>
References: <1065108309.7607.4.camel@ra.chem.psu.edu>
Message-ID: <1065109856.4838.192.camel@localhost>

On Thu, 2003-10-02 at 10:25, Rajarshi Guha wrote:
> Hi,
>  I am trying to build a series of rlm models. I have my data frame and
> the models will be built using various coulmns of the data frame.
> 
> Thus a series of models would be
> 
> m1 <- rlm(V1 ~ V2 + V3 + V4, data)
> m2 <- rlm(V1 ~ V2 + V5 + V7, data)
> m3 <- rlm(V1 ~ V2 + V8 + V9, data)
> 
> I would like to automate this. Is it possible to use a string in place
> of the formula?
> 
> I tried doing:
> 
> fmla <- sprintf('V1 ~ V%g + V%g + V%g',2,3,4)
> m1 <-rlm(fmla,data)
> 
> I get:
> 
> Error in qr(x) : NA/NaN/Inf in foreign function call (arg 1)
> In addition: Warning message:
> NAs introduced by coercion
> 
> I can understand why this method results in the error but is there any
> way to get around this?
> 


See ?as.formula

You can then construct an expression and use something like:

m1 <- rlm(as.formula(expression), data)

HTH,

Marc Schwartz



From aarumugacse at rediffmail.com  Thu Oct  2 17:58:26 2003
From: aarumugacse at rediffmail.com (Arunkumar  Arumugam)
Date: Thu, 02 Oct 2003 15:58:26 -0000
Subject: [R] (no subject)
Message-ID: <20030930173355.17304.qmail@mailweb34.rediffmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20031002/ecdb0d5f/attachment.pl

From ggrothendieck at myway.com  Thu Oct  2 18:02:27 2003
From: ggrothendieck at myway.com (Gabor Grothendieck)
Date: Thu,  2 Oct 2003 12:02:27 -0400 (EDT)
Subject: [R] r editors
Message-ID: <20031002160227.1DFFA39EC@xmxpita.myway.com>



What you want to do is startup up a second window/process
separately from R so you have R open in one window and your 
editor open in another window.  When you make a change to 
the source, just write it out without closing the editor, 
move to the R window and then source it from R:  

   source("/abc.r")

You can grab this line from the command history using up arrow
so you don't actually have to type it except the first time.

That way you can easily go back and forth between R and your
editor.  This should work with notepad or with any other
editor.

While we are on the topic of editors, the free gvim editor 
available at:

   http://www.vim.org

has R syntax highlighting.  It is vi enhanced with a GUI, 
more powerful command set, and cross platform availability 
including Windows (which is where I used it).  There is
also an alternate user interface for it, which I have not
used, called cream, available at:

   http://cream.sourceforge.net

---

From: forkusam <forkusam at yahoo.com>
 
Hi ,
I am programming on a windows system and have problems
using notepad which is my main editor.Each time I try
to open the editor from the R IDE, R crashes.
So I always have to copy my codes from notepad and
paste in R to run them.
CAn someone tell me if I am doing anything wrong or is
there a better editor(freeware) which I could get.
thanks
cilver

  
 

        

_______________________________________________
No banners. No pop-ups. No kidding.
Introducing My Way - http://www.myway.com



From rexbryan1 at comcast.net  Thu Oct  2 18:14:38 2003
From: rexbryan1 at comcast.net (Rex Bryan Dell1700)
Date: Thu, 2 Oct 2003 10:14:38 -0600
Subject: [R] R] Re: Mandelbrot set and C code -- 
References: <200310021009.h92A3grZ005492@stat.math.ethz.ch>
Message-ID: <004d01c38900$49df92a0$3182fd0c@dell1700>

Wizards...

With regards to Mario's Mandelbrot.c programming -- would
some kind wizard show how to compile and run his code on
a Win installation.  I'm looking for a simple "cookbook" example in
the same manner that Mario show for Linux.
I have Win2000 on this machine.

REX
--------------- Original Message ---------------------

Date: Wed, 01 Oct 2003 14:09:36 +0100
From: ucgamdo at ucl.ac.uk
Subject: [R] Re: Mandelbrot set and C code
To: Martin Maechler <maechler at stat.math.ethz.ch>
Cc: r-help at stat.math.ethz.ch
Message-ID: <3.0.5.32.20031001140936.007e2100 at pop-server.ucl.ac.uk>
Content-Type: text/plain; charset="us-ascii"
-- Snip --
To anyone interested in trying out these functions, just save the C code
into a file, say 'mandelbrot.c', to be able to use the C code with R a
shared object needs to be created. This is done simple running the
following command from the directory where 'mandelbrot.c' is stored (note
this code was tried in Linux, for Win users I think the instructions are
similar but you need you check it out):

R CMD SHLIB mandelbrot.c

This will create the file 'mandelbrot.so' in that same directory.

Start R from this directory and paste the above R code into the console.
Type

> dyn.load("mandelbrot.so")

--snip--



From mn216 at columbia.edu  Thu Oct  2 18:30:37 2003
From: mn216 at columbia.edu (Murad Nayal)
Date: Thu, 02 Oct 2003 12:30:37 -0400
Subject: [R] EMACS/ESS problems
Message-ID: <3F7C52AD.9C7B531E@columbia.edu>



Hello all,

since we're on the topic of R-editors. I am using emacs/ess on a unix
workstation (to interact with R and have been having a little problem. I
usually write the R commands I need to run in a separate buffer then
copy and paste them into the *R* buffer for evaluation. The problem is,
if any command is spread over multiple lines emacs/R hangs when I paste
it in the R buffer for evaluation. if I use a debugger to see what's
going on in both programs they're usually waiting on a select statement
(input/output). Anybody has had to deal with a similar situation. any
advice for a workaround? both emacs/ess are relatively recent versions
(installed a few months ago). I tried using ess-eval-buffer/region
instead of cutting and pasting and the same thing happens for me.

many thanks

-- 
Murad Nayal M.D. Ph.D.
Department of Biochemistry and Molecular Biophysics
College of Physicians and Surgeons of Columbia University
630 West 168th Street. New York, NY 10032
Tel: 212-305-6884	Fax: 212-305-6926



From arrayprofile at yahoo.com  Thu Oct  2 18:42:21 2003
From: arrayprofile at yahoo.com (array chip)
Date: Thu, 2 Oct 2003 09:42:21 -0700 (PDT)
Subject: [R] lme vs. aov with Error term
Message-ID: <20031002164221.10855.qmail@web41211.mail.yahoo.com>

Hi,

I have a question about using "lme" and "aov" for the
following dataset. If I understand correctly, using
"aov" with an Error term in the formula is equivalent
to using "lme" with default settings, i.e. both assume
compound symmetry correlation structure. And I have
found that equivalency in the past. However, with the
follwing dataset, I got different answers with using
"lme" and using "aov", can anyone explain what
happened here? I have 2 differnt response variables
"x" and "y" in the following dataset, they are
actually slightly different (only 3 values of them are
different). With "y", I achieved the equivalency
between "lme" and "aov"; but with "x", I got different
p values for the ANOVA table.

-------

x<-c(-0.0649,-0.0923,-0.0623,0.1809,0.0719,0.1017,0.0144,-0.1727,-0.1332,0.0986,0.304,-0.4093,0.2054,0.251,-0.1062,0.3833,0.0649,0.2908,0.1073,0.0919,0.1167,0.2369,0.306,0.1379)

y<-c(-0.0649,-0.0923,0.32,0.08,0.0719,0.1017,0.05,-0.1727,-0.1332,0.15,0.304,-0.4093,0.2054,0.251,-0.1062,0.3833,0.0649,0.2908,0.1073,0.0919,0.1167,0.2369,0.306,0.1379)

treat<-as.factor(c(1,1,1,1,1,1,1,1,1,1,1,1,2,2,2,2,2,2,2,2,2,2,2,2))
time<-as.factor(c(1,1,1,1,2,2,2,2,3,3,3,3,1,1,1,1,2,2,2,2,3,3,3,3))
sex<-as.factor(c('F','F','M','M','F','F','M','M','F','F','M','M','F','F','M','M','F','F','M','M','F','F','M','M'))
subject<-as.factor(c(rep(1:4,3),rep(5:8,3)))
xx<-cbind(x=data.frame(x),y=y,treat=treat,time=time,sex=sex,subject=subject)

######## using x as dependable variable

xx.lme<-lme(x~treat*sex*time,random=~1|subject,xx)
xx.aov<-aov(x~treat*sex*time+Error(subject),xx)

summary(xx.aov)

Error: subject
          Df   Sum Sq  Mean Sq F value  Pr(>F)  
treat      1 0.210769 0.210769  6.8933 0.05846 .
sex        1 0.005775 0.005775  0.1889 0.68627  
treat:sex  1 0.000587 0.000587  0.0192 0.89649  
Residuals  4 0.122304 0.030576                  
---
Signif. codes:  0 `***' 0.001 `**' 0.01 `*' 0.05 `.'
0.1 ` ' 1 

Error: Within
               Df  Sum Sq Mean Sq F value Pr(>F)
time            2 0.00102 0.00051  0.0109 0.9891
treat:time      2 0.00998 0.00499  0.1066 0.9002
sex:time        2 0.02525 0.01263  0.2696 0.7704
treat:sex:time  2 0.03239 0.01619  0.3458 0.7178
Residuals       8 0.37469 0.04684 

anova(xx.lme)
               numDF denDF  F-value p-value
(Intercept)        1     8 3.719117  0.0899
treat              1     4 5.089022  0.0871
sex                1     4 0.139445  0.7278
time               2     8 0.012365  0.9877
treat:sex          1     4 0.014175  0.9110
treat:time         2     8 0.120538  0.8880
sex:time           2     8 0.304878  0.7454
treat:sex:time     2     8 0.391012  0.6886

#### using y as dependable variable

xx.lme2<-lme(y~treat*sex*time,random=~1|subject,xx)
xx.aov2<-aov(y~treat*sex*time+Error(subject),xx)

summary(xx.aov2)

Error: subject
          Df   Sum Sq  Mean Sq F value Pr(>F)
treat      1 0.147376 0.147376  2.0665 0.2239
sex        1 0.000474 0.000474  0.0067 0.9389
treat:sex  1 0.006154 0.006154  0.0863 0.7836
Residuals  4 0.285268 0.071317               

Error: Within
               Df   Sum Sq  Mean Sq F value Pr(>F)
time            2 0.009140 0.004570  0.1579 0.8565
treat:time      2 0.012598 0.006299  0.2177 0.8090
sex:time        2 0.043132 0.021566  0.7453 0.5049
treat:sex:time  2 0.069733 0.034866  1.2050 0.3488
Residuals       8 0.231480 0.028935               

anova(xx.lme2)
               numDF denDF   F-value p-value
(Intercept)        1     8 3.0667809  0.1180
treat              1     4 2.0664919  0.2239
sex                1     4 0.0066516  0.9389
time               2     8 0.1579473  0.8565
treat:sex          1     4 0.0862850  0.7836
treat:time         2     8 0.2177028  0.8090
sex:time           2     8 0.7453185  0.5049
treat:sex:time     2     8 1.2049883  0.3488



From rossini at blindglobe.net  Thu Oct  2 18:47:31 2003
From: rossini at blindglobe.net (A.J. Rossini)
Date: Thu, 02 Oct 2003 09:47:31 -0700
Subject: [R] EMACS/ESS problems
In-Reply-To: <3F7C52AD.9C7B531E@columbia.edu> (Murad Nayal's message of
	"Thu, 02 Oct 2003 12:30:37 -0400")
References: <3F7C52AD.9C7B531E@columbia.edu>
Message-ID: <85ekxv37to.fsf@blindglobe.net>


1. I've never seen this behavior, ever.  Do you get the same with C-c C-r
(highlight region, then C-c C-r sends to the R process in Emacs).  Or,
if you use C-c C-n to step through the lines....? 

2. ess-help at stat.math.ethz.ch might be a better place to send this.


Murad Nayal <mn216 at columbia.edu> writes:

> Hello all,
>
> since we're on the topic of R-editors. I am using emacs/ess on a unix
> workstation (to interact with R and have been having a little problem. I
> usually write the R commands I need to run in a separate buffer then
> copy and paste them into the *R* buffer for evaluation. The problem is,
> if any command is spread over multiple lines emacs/R hangs when I paste
> it in the R buffer for evaluation. if I use a debugger to see what's
> going on in both programs they're usually waiting on a select statement
> (input/output). Anybody has had to deal with a similar situation. any
> advice for a workaround? both emacs/ess are relatively recent versions
> (installed a few months ago). I tried using ess-eval-buffer/region
> instead of cutting and pasting and the same thing happens for me.
>
> many thanks
>
> -- 
> Murad Nayal M.D. Ph.D.
> Department of Biochemistry and Molecular Biophysics
> College of Physicians and Surgeons of Columbia University
> 630 West 168th Street. New York, NY 10032
> Tel: 212-305-6884	Fax: 212-305-6926
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
>

-- 
rossini at u.washington.edu            http://www.analytics.washington.edu/ 
Biomedical and Health Informatics   University of Washington
Biostatistics, SCHARP/HVTN          Fred Hutchinson Cancer Research Center
UW (Tu/Th/F): 206-616-7630 FAX=206-543-3461 | Voicemail is unreliable
FHCRC  (M/W): 206-667-7025 FAX=206-667-4812 | use Email

CONFIDENTIALITY NOTICE: This e-mail message and any attachme...{{dropped}}



From mn216 at columbia.edu  Thu Oct  2 18:55:30 2003
From: mn216 at columbia.edu (Murad Nayal)
Date: Thu, 02 Oct 2003 12:55:30 -0400
Subject: [R] EMACS/ESS problems
References: <3F7C52AD.9C7B531E@columbia.edu> <85ekxv37to.fsf@blindglobe.net>
Message-ID: <3F7C5881.883456F4@columbia.edu>


Hi,


"A.J. Rossini" wrote:
> 
> 1. I've never seen this behavior, ever.  Do you get the same with C-c C-r
> (highlight region, then C-c C-r sends to the R process in Emacs).  Or,
> if you use C-c C-n to step through the lines....?

maybe my environment is not set up correctly. C-c C-r doesn't do
anything:

highlight: (either in a text buffer or *ESS*)

v = c(1,
      2,
      3)

C-c C-r
switch to *R*
v
Error: Object "v" not found

OR

ess-eval-region
prints in the one line buffer at the bottom: 
Starting evalutions...
and hangs (I have to stop it with C-g)

now if v was defined on one line 
v=c(1,2,3)
ess-eval-region returns with Finished evaluation. but the v vector is
still not defined in R

I am sure I am doing something silly. just can't figure out what?

> 
> 2. ess-help at stat.math.ethz.ch might be a better place to send this.

thanks for the advice. I'll try that too.

> 
> Murad Nayal <mn216 at columbia.edu> writes:
> 
> > Hello all,
> >
> > since we're on the topic of R-editors. I am using emacs/ess on a unix
> > workstation (to interact with R and have been having a little problem. I
> > usually write the R commands I need to run in a separate buffer then
> > copy and paste them into the *R* buffer for evaluation. The problem is,
> > if any command is spread over multiple lines emacs/R hangs when I paste
> > it in the R buffer for evaluation. if I use a debugger to see what's
> > going on in both programs they're usually waiting on a select statement
> > (input/output). Anybody has had to deal with a similar situation. any
> > advice for a workaround? both emacs/ess are relatively recent versions
> > (installed a few months ago). I tried using ess-eval-buffer/region
> > instead of cutting and pasting and the same thing happens for me.
> >
> > many thanks
> >
> > --
> > Murad Nayal M.D. Ph.D.
> > Department of Biochemistry and Molecular Biophysics
> > College of Physicians and Surgeons of Columbia University
> > 630 West 168th Street. New York, NY 10032
> > Tel: 212-305-6884     Fax: 212-305-6926
> >
> > ______________________________________________
> > R-help at stat.math.ethz.ch mailing list
> > https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> >
> 
> --
> rossini at u.washington.edu            http://www.analytics.washington.edu/
> Biomedical and Health Informatics   University of Washington
> Biostatistics, SCHARP/HVTN          Fred Hutchinson Cancer Research Center
> UW (Tu/Th/F): 206-616-7630 FAX=206-543-3461 | Voicemail is unreliable
> FHCRC  (M/W): 206-667-7025 FAX=206-667-4812 | use Email



From andy_liaw at merck.com  Thu Oct  2 19:01:20 2003
From: andy_liaw at merck.com (Liaw, Andy)
Date: Thu, 02 Oct 2003 13:01:20 -0400
Subject: [R] R] Re: Mandelbrot set and C code --
Message-ID: <3A822319EB35174CA3714066D590DCD50205CC07@usrymx25.merck.com>

You need to install the tools needed to build packages from source.  You can
read about it in Q3.1 of R for Windows FAQ.

Andy

> -----Original Message-----
> From: Rex Bryan Dell1700 [mailto:rexbryan1 at comcast.net] 
> Sent: Thursday, October 02, 2003 12:15 PM
> To: r-help at stat.math.ethz.ch
> Subject: [R] R] Re: Mandelbrot set and C code --
> 
> 
> Wizards...
> 
> With regards to Mario's Mandelbrot.c programming -- would
> some kind wizard show how to compile and run his code on
> a Win installation.  I'm looking for a simple "cookbook" 
> example in the same manner that Mario show for Linux. I have 
> Win2000 on this machine.
> 
> REX
> --------------- Original Message ---------------------
> 
> Date: Wed, 01 Oct 2003 14:09:36 +0100
> From: ucgamdo at ucl.ac.uk
> Subject: [R] Re: Mandelbrot set and C code
> To: Martin Maechler <maechler at stat.math.ethz.ch>
> Cc: r-help at stat.math.ethz.ch
> Message-ID: <3.0.5.32.20031001140936.007e2100 at pop-server.ucl.ac.uk>
> Content-Type: text/plain; charset="us-ascii"
> -- Snip --
> To anyone interested in trying out these functions, just save 
> the C code into a file, say 'mandelbrot.c', to be able to use 
> the C code with R a shared object needs to be created. This 
> is done simple running the following command from the 
> directory where 'mandelbrot.c' is stored (note this code was 
> tried in Linux, for Win users I think the instructions are 
> similar but you need you check it out):
> 
> R CMD SHLIB mandelbrot.c
> 
> This will create the file 'mandelbrot.so' in that same directory.
> 
> Start R from this directory and paste the above R code into 
> the console. Type
> 
> > dyn.load("mandelbrot.so")
> 
> --snip--
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list 
> https://www.stat.math.ethz.ch/mailman/listinfo> /r-help
>



From kjetil at entelnet.bo  Thu Oct  2 19:37:30 2003
From: kjetil at entelnet.bo (kjetil brinchmann halvorsen)
Date: Thu, 02 Oct 2003 13:37:30 -0400
Subject: [R] how calculate mean for each group
In-Reply-To: <6C8A8033ABC1E3468048ABC4F13CE572AC0E03@synequanon01>
Message-ID: <3F7C2A1A.9245.49F828@localhost>

On 2 Oct 2003 at 16:11, Simon Fear wrote:

> 
> (BTW did I ever mention here how much I prefer with() to 
> attach()/forget.to.detach() ?)
> 
 Agreed. I learnt about with() a few weeks ago and hasn't used 
attach()/forget to detach() since. 

Kjetil Halvorsen



From morozov at uclink.berkeley.edu  Thu Oct  2 19:40:45 2003
From: morozov at uclink.berkeley.edu (morozov)
Date: Thu, 2 Oct 2003 10:40:45 -0700
Subject: [R] indexing a vector
Message-ID: <3F81F564@bearmail.berkeley.edu>

Dear All:
I'd like to know how to sort and then index a vector of floats by several 
levels in R.
For example


>x<-rnorm(100)
> MyLevels<-quantile(x,probs=c(0,.5,1))
> MyLevels
         0%         50%        100% 
-2.11978442 -0.03770613  2.00186397

next i want to replace each x[i] in x by 1,2,3 or 4 depending on which 
quantile that x[i] falls. How do I do that in a "vector" fashion?

I tried something like

>factor(x,levels=as.numeric(MyLevels))

but that doesnt work.


Thank you very much,
Vlad



From TyagiAnupam at aol.com  Thu Oct  2 19:54:59 2003
From: TyagiAnupam at aol.com (TyagiAnupam@aol.com)
Date: Thu, 2 Oct 2003 13:54:59 EDT
Subject: [R] EMACS/ESS problems
Message-ID: <194.1f50b8b2.2cadc073@aol.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20031002/8e3f9573/attachment.pl

From mn216 at columbia.edu  Thu Oct  2 20:06:31 2003
From: mn216 at columbia.edu (Murad Nayal)
Date: Thu, 02 Oct 2003 14:06:31 -0400
Subject: [R] EMACS/ESS problems
References: <194.1f50b8b2.2cadc073@aol.com>
Message-ID: <3F7C6927.CF3D050C@columbia.edu>


Hi,

It didn't help in this case

in *scratch*

v = c(1,  #
      2,  #
      3);

highlight then ess-eval-region (hangs)
C-g then switch to *R* you see:

> v = c(1,  #
+       2,  #
+ 

I had to type ')' to stop the continuation

Also per Sundar's suggestion

> .Options$continue
[1] "+ "

I am hoping some of this might give someone a clue as to what's going
on!



TyagiAnupam at aol.com wrote:
> 
> May be putting the comment character, #,  at the end of the lines that
> are continued will solve do the problem. As in,
> 
> a <- data.frame(cbind(v1, v2, # 1st line
>                     v3, v4, # 2nd line
>                     v5))
> 
> In a message dated 10/2/03 10:22:07 AM Pacific Daylight Time,
> rossini at blindglobe.net writes:
> 
> > 1. I've never seen this behavior, ever.  Do you get the same with
> > C-c C-r
> > (highlight region, then C-c C-r sends to the R process in Emacs).
> > Or,
> > if you use C-c C-n to step through the lines....?
> >
> > 2. ess-help at stat.math.ethz.ch might be a better place to send this.
> >
> >
> > Murad Nayal <mn216 at columbia.edu> writes:
> >
> > >Hello all,
> > >
> > >since we're on the topic of R-editors. I am using emacs/ess on a
> > unix
> > >workstation (to interact with R and have been having a little
> > problem. I
> > >usually write the R commands I need to run in a separate buffer
> > then
> > >copy and paste them into the *R* buffer for evaluation. The problem
> > is,
> > >if any command is spread over multiple lines emacs/R hangs when I
> > paste
> > >it in the R buffer for evaluation. if I use a debugger to see
> > what's
> > >going on in both programs they're usually waiting on a select
> > statement
> > >(input/output). Anybody has had to deal with a similar situation.
> > any
> > >advice for a workaround? both emacs/ess are relatively recent
> > versions
> > >(installed a few months ago). I tried using ess-eval-buffer/region
> > >instead of cutting and pasting and the same thing happens for me.
> > >
> > >many thanks
> > >
> > >--
> > >Murad Nayal M.D. Ph.D.
> > >Department of Biochemistry and Molecular Biophysics
> > >College of Physicians and Surgeons of Columbia University
> > >630 West 168th Street. New York, NY 10032
> > >Tel: 212-305-6884   Fax: 212-305-6926
> >

-- 
Murad Nayal M.D. Ph.D.
Department of Biochemistry and Molecular Biophysics
College of Physicians and Surgeons of Columbia University
630 West 168th Street. New York, NY 10032
Tel: 212-305-6884	Fax: 212-305-6926



From donghu at itsa.ucsf.edu  Thu Oct  2 20:22:34 2003
From: donghu at itsa.ucsf.edu (donghu@itsa.ucsf.edu)
Date: Thu, 02 Oct 2003 11:22:34 PDT
Subject: [R] Help with ANOVA
Message-ID: <200310021822.h92IMY318824@itsa.ucsf.edu>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20031002/19f2913a/attachment.pl

From TyagiAnupam at aol.com  Thu Oct  2 20:25:35 2003
From: TyagiAnupam at aol.com (TyagiAnupam@aol.com)
Date: Thu, 2 Oct 2003 14:25:35 EDT
Subject: [R] EMACS/ESS problems
Message-ID: <55.48c76901.2cadc79f@aol.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20031002/c0bef207/attachment.pl

From jasont at indigoindustrial.co.nz  Thu Oct  2 20:38:57 2003
From: jasont at indigoindustrial.co.nz (Jason Turner)
Date: Fri, 03 Oct 2003 06:38:57 +1200
Subject: [R] indexing a vector
In-Reply-To: <3F81F564@bearmail.berkeley.edu>
References: <3F81F564@bearmail.berkeley.edu>
Message-ID: <3F7C70C1.4000909@indigoindustrial.co.nz>

morozov wrote:

> Dear All:
> I'd like to know how to sort and then index a vector of floats by several 
> levels in R.
> For example
> 
> 
> 
>>x<-rnorm(100)
>>MyLevels<-quantile(x,probs=c(0,.5,1))
>>MyLevels
> 
>          0%         50%        100% 
> -2.11978442 -0.03770613  2.00186397
> 
> next i want to replace each x[i] in x by 1,2,3 or 4 depending on which 
> quantile that x[i] falls. How do I do that in a "vector" fashion?

I think rank() is the function you're looking for.

Cheers

Jason
-- 
Indigo Industrial Controls Ltd.
http://www.indigoindustrial.co.nz
64-21-343-545
jasont at indigoindustrial.co.nz



From pgreen at umich.edu  Thu Oct  2 20:40:25 2003
From: pgreen at umich.edu (Paul Green)
Date: Thu, 02 Oct 2003 14:40:25 -0400
Subject: [R] combination levels
Message-ID: <5.1.0.14.2.20031002143549.00b14220@mailkardia.sph.umich.edu>

Is there a function in R that will give all
combination levels. For example, I see
there is a "choose" function that gives

 >choose(5,2)
10

but I want
	
	1 2
	1 3
	1 4
	1 5
	2 3
	2 4
	2 5
	3 4
	3 5
	4 5

This is a simple example. Actually I would like
to do something like 10 choose 4.

Paul



From arrayprofile at yahoo.com  Thu Oct  2 20:54:21 2003
From: arrayprofile at yahoo.com (array chip)
Date: Thu, 2 Oct 2003 11:54:21 -0700 (PDT)
Subject: [R] RE: [S] lme vs. aov with Error term
In-Reply-To: <D45BC82E21E16149AC5A3F706350626405AB09FA@usrymx14.merck.com>
Message-ID: <20031002185421.16472.qmail@web41215.mail.yahoo.com>

Hi Bert,

Thanks for the suggestions. I tried lme with different
control parameters, and also tried using "ML", instaed
of "REML", but still got the same answers. 

Yes, I hope some gurus on this list could give me some
hints.

Thanks


--- "Gunter, Bert" <bert_gunter at merck.com> wrote:
> But they are close. This is almost certainly a
> numeric issue -- if you set
> your control parameters in lme so that you run it
> longer (make the stopping
> criteria tighter), I'll bet you converge to the same
> results.
> 
> ... or it might be some some similar abstruse
> problem in aov.
> 
> Interesting, though. I'll be interested in hearing
> what the "gurus" have to
> say (of which I am NOT one)
> 
> Cheers,
> 
> Bert Gunter
> Biometrics Research RY 33-300
> Merck & Company
> P.O. Box 2000
> Rahway, NJ 07065-0900
> Phone: (732) 594-7765
> mailto: bert_gunter at merck.com
> 
> "The business of the statistician is to catalyze the
> scientific learning
> process."      -- George E.P. Box
> 
> 
> 
> -----Original Message-----
> From: array chip [mailto:arrayprofile at yahoo.com] 
> Sent: Thursday, October 02, 2003 12:42 PM
> To: s-news at wubios.wustl.edu
> Cc: R-help at stat.math.ethz.ch
> Subject: [S] lme vs. aov with Error term
> 
> 
> Hi,
> 
> I have a question about using "lme" and "aov" for
> the
> following dataset. If I understand correctly, using
> "aov" with an Error term in the formula is
> equivalent
> to using "lme" with default settings, i.e. both
> assume
> compound symmetry correlation structure. And I have
> found that equivalency in the past. However, with
> the
> follwing dataset, I got different answers with using
> "lme" and using "aov", can anyone explain what
> happened here? I have 2 differnt response variables
> "x" and "y" in the following dataset, they are
> actually slightly different (only 3 values of them
> are
> different). With "y", I achieved the equivalency
> between "lme" and "aov"; but with "x", I got
> different
> p values for the ANOVA table.
> 
> -------
> 
>
x<-c(-0.0649,-0.0923,-0.0623,0.1809,0.0719,0.1017,0.0144,-0.1727,-0.1332,0.0
>
986,0.304,-0.4093,0.2054,0.251,-0.1062,0.3833,0.0649,0.2908,0.1073,0.0919,0.
> 1167,0.2369,0.306,0.1379)
> 
>
y<-c(-0.0649,-0.0923,0.32,0.08,0.0719,0.1017,0.05,-0.1727,-0.1332,0.15,0.304
>
,-0.4093,0.2054,0.251,-0.1062,0.3833,0.0649,0.2908,0.1073,0.0919,0.1167,0.23
> 69,0.306,0.1379)
> 
>
treat<-as.factor(c(1,1,1,1,1,1,1,1,1,1,1,1,2,2,2,2,2,2,2,2,2,2,2,2))
>
time<-as.factor(c(1,1,1,1,2,2,2,2,3,3,3,3,1,1,1,1,2,2,2,2,3,3,3,3))
>
sex<-as.factor(c('F','F','M','M','F','F','M','M','F','F','M','M','F','F','M'
> ,'M','F','F','M','M','F','F','M','M'))
> subject<-as.factor(c(rep(1:4,3),rep(5:8,3)))
>
xx<-cbind(x=data.frame(x),y=y,treat=treat,time=time,sex=sex,subject=subject)
> 
> ######## using x as dependable variable
> 
> xx.lme<-lme(x~treat*sex*time,random=~1|subject,xx)
> xx.aov<-aov(x~treat*sex*time+Error(subject),xx)
> 
> summary(xx.aov)
> 
> Error: subject
>           Df   Sum Sq  Mean Sq F value  Pr(>F)  
> treat      1 0.210769 0.210769  6.8933 0.05846 .
> sex        1 0.005775 0.005775  0.1889 0.68627  
> treat:sex  1 0.000587 0.000587  0.0192 0.89649  
> Residuals  4 0.122304 0.030576                  
> ---
> Signif. codes:  0 `***' 0.001 `**' 0.01 `*' 0.05 `.'
> 0.1 ` ' 1 
> 
> Error: Within
>                Df  Sum Sq Mean Sq F value Pr(>F)
> time            2 0.00102 0.00051  0.0109 0.9891
> treat:time      2 0.00998 0.00499  0.1066 0.9002
> sex:time        2 0.02525 0.01263  0.2696 0.7704
> treat:sex:time  2 0.03239 0.01619  0.3458 0.7178
> Residuals       8 0.37469 0.04684 
> 
> anova(xx.lme)
>                numDF denDF  F-value p-value
> (Intercept)        1     8 3.719117  0.0899
> treat              1     4 5.089022  0.0871
> sex                1     4 0.139445  0.7278
> time               2     8 0.012365  0.9877
> treat:sex          1     4 0.014175  0.9110
> treat:time         2     8 0.120538  0.8880
> sex:time           2     8 0.304878  0.7454
> treat:sex:time     2     8 0.391012  0.6886
> 
> #### using y as dependable variable
> 
> xx.lme2<-lme(y~treat*sex*time,random=~1|subject,xx)
> xx.aov2<-aov(y~treat*sex*time+Error(subject),xx)
> 
> summary(xx.aov2)
> 
> Error: subject
>           Df   Sum Sq  Mean Sq F value Pr(>F)
> treat      1 0.147376 0.147376  2.0665 0.2239
> sex        1 0.000474 0.000474  0.0067 0.9389
> treat:sex  1 0.006154 0.006154  0.0863 0.7836
> Residuals  4 0.285268 0.071317               
> 
> Error: Within
>                Df   Sum Sq  Mean Sq F value Pr(>F)
> time            2 0.009140 0.004570  0.1579 0.8565
> treat:time      2 0.012598 0.006299  0.2177 0.8090
> sex:time        2 0.043132 0.021566  0.7453 0.5049
> treat:sex:time  2 0.069733 0.034866  1.2050 0.3488
> Residuals       8 0.231480 0.028935               
> 
> anova(xx.lme2)
>                numDF denDF   F-value p-value
> (Intercept)        1     8 3.0667809  0.1180
> treat              1     4 2.0664919  0.2239
> sex                1     4 0.0066516  0.9389
> time               2     8 0.1579473  0.8565
> treat:sex          1     4 0.0862850  0.7836
> treat:time         2     8 0.2177028  0.8090
> sex:time           2     8 0.7453185  0.5049
> treat:sex:time     2     8 1.2049883  0.3488
> 
> 
> __________________________________
> Do you Yahoo!?

> search
> http://shopping.yahoo.com
>
--------------------------------------------------------------------
> This message was distributed by
> s-news at lists.biostat.wustl.edu.  To
> unsubscribe send e-mail to
> s-news-request at lists.biostat.wustl.edu with

>



From kwan022 at stat.auckland.ac.nz  Thu Oct  2 21:06:36 2003
From: kwan022 at stat.auckland.ac.nz (Ko-Kang Kevin Wang)
Date: Fri, 3 Oct 2003 07:06:36 +1200 (NZST)
Subject: [R] (no subject)
In-Reply-To: <20030930173355.17304.qmail@mailweb34.rediffmail.com>
Message-ID: <Pine.LNX.4.44.0310030705260.1439-100000@stat55.stat.auckland.ac.nz>

On 30 Sep 2003, Arunkumar  Arumugam wrote:

> I tried testing some of the programs and functions.
> I read in the manual that plot(x,y) functions automatically 
> generates a  graphical window and plot. that does not happens in 
> my installation.

Ummm...can you be a bit more precise?

What exactly has happened when you typed plot(x, y)?  Have you created two 
vectors called x and y?  Were they any error messages, if so what are the 
messages?

-- 
Cheers,

Kevin

------------------------------------------------------------------------------
"On two occasions, I have been asked [by members of Parliament],
'Pray, Mr. Babbage, if you put into the machine wrong figures, will
the right answers come out?' I am not able to rightly apprehend the
kind of confusion of ideas that could provoke such a question."

-- Charles Babbage (1791-1871) 
---- From Computer Stupidities: http://rinkworks.com/stupid/

--
Ko-Kang Kevin Wang
Master of Science (MSc) Student
SLC Tutor and Lab Demonstrator
Department of Statistics
University of Auckland
New Zealand
Homepage: http://www.stat.auckland.ac.nz/~kwan022
Ph: 373-7599
    x88475 (City)
    x88480 (Tamaki)



From mn216 at columbia.edu  Thu Oct  2 21:12:00 2003
From: mn216 at columbia.edu (Murad Nayal)
Date: Thu, 02 Oct 2003 15:12:00 -0400
Subject: [R] EMACS/ESS problems
References: <55.48c76901.2cadc79f@aol.com>
Message-ID: <3F7C7880.1F2EA739@columbia.edu>


Hi Tyagi

This seems to work, but not as C-c C-n, rather by running
ess-eval-line-and-step as a command (M-x ...). Where does that C-c C-n
binding occur? as in, what is the ESS buffer? I usually start R by (M-x
R) which puts me in an interactive R session (mode iESS). This also
creates another *ESS* buffer that seems to contain ess messages. and
this buffer does not seem to have C-c C-n binding either. what am i
missing?

thanks a lot

TyagiAnupam at aol.com wrote:
> 
> Try eval-line&step C-c C-n in ESS (no cut and paste), and omit the semi-colon
> at the end of the statement.
> 
> In a message dated 10/2/03 11:12:37 AM Pacific Daylight Time,
> mn216 at columbia.edu writes:
> 
> > Hi,
> >
> > It didn't help in this case
> >
> > in *scratch*
> >
> > v = c(1,  #
> >    2,  #
> >    3);
> >
> > highlight then ess-eval-region (hangs)
> > C-g then switch to *R* you see:
> >
> > >v = c(1,  #
> > +     2,  #
> > +
> >
> > I had to type ')' to stop the continuation
> >
> > Also per Sundar's suggestion
> >
> > >.Options$continue
> > [1] "+ "
> >
> > I am hoping some of this might give someone a clue as to what's going
> > on!
> >
> >
> 
>         [[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help

-- 
Murad Nayal M.D. Ph.D.
Department of Biochemistry and Molecular Biophysics
College of Physicians and Surgeons of Columbia University
630 West 168th Street. New York, NY 10032
Tel: 212-305-6884	Fax: 212-305-6926



From MSchwartz at medanalytics.com  Thu Oct  2 21:36:19 2003
From: MSchwartz at medanalytics.com (Marc Schwartz)
Date: Thu, 02 Oct 2003 14:36:19 -0500
Subject: [R] indexing a vector
In-Reply-To: <3F81F564@bearmail.berkeley.edu>
References: <3F81F564@bearmail.berkeley.edu>
Message-ID: <1065123379.6915.19.camel@localhost>

On Thu, 2003-10-02 at 12:40, morozov wrote:
> Dear All:
> I'd like to know how to sort and then index a vector of floats by several 
> levels in R.
> For example
> 
> 
> >x<-rnorm(100)
> > MyLevels<-quantile(x,probs=c(0,.5,1))
> > MyLevels
>          0%         50%        100% 
> -2.11978442 -0.03770613  2.00186397
> 
> next i want to replace each x[i] in x by 1,2,3 or 4 depending on which 
> quantile that x[i] falls. How do I do that in a "vector" fashion?
> 
> I tried something like
> 
> >factor(x,levels=as.numeric(MyLevels))
> 
> but that doesnt work.
> 
> 
> Thank you very much,
> Vlad


You might want to look at ?cut, which returns a factor based upon
defining breakpoints in a continuous vector. You can break the vector
'x' and relabel the levels to give you what you need.

For example:

x <- rnorm(100)

MyLevels <- factor(cut(x, quantile(x), include.lowest = TRUE), 
                   labels = 1:4)

table(MyLevels)
MyLevels
 1  2  3  4
25 25 25 25

HTH,

Marc Schwartz



From ihaka at stat.auckland.ac.nz  Thu Oct  2 21:41:39 2003
From: ihaka at stat.auckland.ac.nz (Ross Ihaka)
Date: Fri, 03 Oct 2003 07:41:39 +1200
Subject: [R] indexing a vector
In-Reply-To: <3F81F564@bearmail.berkeley.edu>
References: <3F81F564@bearmail.berkeley.edu>
Message-ID: <3F7C7F73.5020607@stat.auckland.ac.nz>

morozov wrote:

>>x<-rnorm(100)
>>MyLevels<-quantile(x,probs=c(0,.5,1))
>>MyLevels
> 
>          0%         50%        100% 
> -2.11978442 -0.03770613  2.00186397
> 
> next i want to replace each x[i] in x by 1,2,3 or 4 depending on which 
> quantile that x[i] falls. How do I do that in a "vector" fashion?

Take a look at the "cut" function.


-- 
Ross Ihaka                         Email:  ihaka at stat.auckland.ac.nz
Department of Statistics           Phone:  (64-9) 373-7599 x 85054
University of Auckland             Fax:    (64-9) 373-7018
Private Bag 92019, Auckland
New Zealand



From MSchwartz at medanalytics.com  Thu Oct  2 21:43:09 2003
From: MSchwartz at medanalytics.com (Marc Schwartz)
Date: Thu, 02 Oct 2003 14:43:09 -0500
Subject: [R] combination levels
In-Reply-To: <5.1.0.14.2.20031002143549.00b14220@mailkardia.sph.umich.edu>
References: <5.1.0.14.2.20031002143549.00b14220@mailkardia.sph.umich.edu>
Message-ID: <1065123789.6915.22.camel@localhost>

On Thu, 2003-10-02 at 13:40, Paul Green wrote:
> Is there a function in R that will give all
> combination levels. For example, I see
> there is a "choose" function that gives
> 
>  >choose(5,2)
> 10
> 
> but I want
> 	
> 	1 2
> 	1 3
> 	1 4
> 	1 5
> 	2 3
> 	2 4
> 	2 5
> 	3 4
> 	3 5
> 	4 5
> 
> This is a simple example. Actually I would like
> to do something like 10 choose 4.
> 
> Paul


See the combinations() function in the 'gregmisc' package on CRAN:

> combinations(5, 2)
      [,1] [,2]
 [1,]    1    2
 [2,]    1    3
 [3,]    1    4
 [4,]    1    5
 [5,]    2    3
 [6,]    2    4
 [7,]    2    5
 [8,]    3    4
 [9,]    3    5
[10,]    4    5

HTH,

Marc Schwartz



From arrayprofile at yahoo.com  Thu Oct  2 21:55:29 2003
From: arrayprofile at yahoo.com (array chip)
Date: Thu, 2 Oct 2003 12:55:29 -0700 (PDT)
Subject: [R] Re: [S] lme vs. aov with Error term
In-Reply-To: <OFEB86B80A.65F27DF2-ON85256DB3.00687AC3-85256DB3.006A341B@EU.novartis.net>
Message-ID: <20031002195529.96713.qmail@web41201.mail.yahoo.com>

Jose,

Thank you very much for the explanation! 

what about if I specify using "ML" instead of "REML"
in the lme? I found that I still got different answers
even I use "ML" in the lme call. And in any case,
should I trust "lme" more than "aov", or vice versa?

Thanks again


--- jose.pinheiro at pharma.novartis.com wrote:
> > I have a question about using "lme" and "aov" for
> the
> > following dataset. If I understand correctly,
> using
> > "aov" with an Error term in the formula is
> equivalent
> > to using "lme" with default settings, i.e. both
> assume
> > compound symmetry correlation structure.
> 
> Not necessarily, only when the maximum of the REML
> objective
> function falls within the parameter space. This
> obscure observation
> translates, in your case,  into saying that, when
> the "Subject"
> mean square error is larger than the "Within" MSE
> (in the aov
> notation), then the two fits should coincide. This
> is the case for
> your y response:
> 
> Error: subject
>           Df   Sum Sq  Mean Sq F value Pr(>F)
> . . .
> Residuals  4 0.285268 0.071317 
> 
> Error: Within
>                Df   Sum Sq  Mean Sq F value Pr(>F)
> . . .
> Residuals       8 0.231480 0.028935 
> 
> However, when the Subject MSE is smaller than the
> Within MSE, the
> maximum of the REML objective occurs at a negative
> value of the
> between-subject variance. Because the lme algorithm
> forces the
> estimated parameters to remain within the parameter
> space, the
> solution you got probably corresponds to a boundary
> of the parameter
> space (the Subject Std. Dev must be fairly small).
> This is what
> happens for your "x" response. 
> 
> Error: subject
>           Df   Sum Sq  Mean Sq F value  Pr(>F) 
> . . .
> Residuals  4 0.122304 0.030576 
> ---
> 
> 
> Error: Within
>                Df  Sum Sq Mean Sq F value Pr(>F)
> . . .
> Residuals       8 0.37469 0.04684 
> 
> Hope this helps.
> 
> Best,
> --José
> 
>
---------------------------------------------------------
> José Pinheiro
> Biostatistics, Novartis Pharmaceuticals 
> One Health Plaza, Room 419/2115, East Hanover, NJ
> 07936  
>
---------------------------------------------------------
>



From Karin at preusting.fol.nl  Thu Oct  2 21:57:52 2003
From: Karin at preusting.fol.nl (Karin & Gerard Preusting)
Date: Thu, 2 Oct 2003 21:57:52 +0200
Subject: [R] Search for a text string and write position related data to a
	file
Message-ID: <001a01c3891f$79f18a10$0b91d3d5@EIGENAARS6P1DU>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20031002/4a57a9fd/attachment.pl

From p.dalgaard at biostat.ku.dk  Thu Oct  2 22:56:26 2003
From: p.dalgaard at biostat.ku.dk (Peter Dalgaard BSA)
Date: Thu, 02 Oct 2003 20:56:26 -0000
Subject: [R] Search for a text string and write position related data to a
	file
In-Reply-To: <001a01c3891f$79f18a10$0b91d3d5@EIGENAARS6P1DU>
References: <001a01c3891f$79f18a10$0b91d3d5@EIGENAARS6P1DU>
Message-ID: <x21xtvuzkv.fsf@biostat.ku.dk>

"Karin & Gerard Preusting" <Karin at preusting.fol.nl> writes:

> Dear all,
> 
> I just started using R today. What I need to do is find a text
> string in a (large) text file and than copy the some position
> related lines (or text) to an other text file. Is this possible with
> R?

It's hardly a typical R application, but it can be done using a fairly
straightforward while loop. See documentation for readLines,
writeLines, and grep. (On Unix-like systems, tools like awk or perl
will probably get the job done more quickly).

-- 
   O__  ---- Peter Dalgaard             Blegdamsvej 3  
  c/ /'_ --- Dept. of Biostatistics     2200 Cph. N   
 (*) \(*) -- University of Copenhagen   Denmark      Ph: (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)             FAX: (+45) 35327907



From rmh at surfer.sbm.temple.edu  Thu Oct  2 22:59:27 2003
From: rmh at surfer.sbm.temple.edu (Rich Heiberger)
Date: Thu, 2 Oct 2003 16:59:27 -0400 (EDT)
Subject: [R] EMACS/ESS problems
References: <3F7C52AD.9C7B531E@columbia.edu> <85ekxv37to.fsf@blindglobe.net>
Message-ID: <200310022059.h92KxRnm942917@surfer.sbm.temple.edu>

It looks like you are not using ESS correctly.  ESS is designed to
work from a buffer containing a file whose name has the ".r" extension.
Thus, open a file, for example,
   C-x C-f myfile.r
and then start using R.

My diagnosis is based on your line
> highlight: (either in a text buffer or *ESS*)

ESS won't work from a text buffer because R code has different
requirements from ordinary written paragraphs in English or another
natural language.

The *ESS* buffer is part of the background mechanism that makes ESS work.
It is not intended that a user ever look at the *ESS* buffer.

One other issue that your original email suggests.  We do not
recommend the statement
   v=c(1,2,3)
for assignment.  It is much better to use the assignment arrow
   v <- c(1,2,3)
(with spaces on both sides of the arrow for legibility).  It is true
that the "=" will usually do what you expect, but there are some subtle
differences (mostly in argument lists to functions).  While I can expand on
the reasons, for the moment I just want to suggest that you get into the
habit of using the assignment arrow.

Rich



From ross at biostat.ucsf.edu  Fri Oct  3 01:09:38 2003
From: ross at biostat.ucsf.edu (Ross Boylan)
Date: Thu, 02 Oct 2003 16:09:38 -0700
Subject: [R] Plot can't forget bad parameters
Message-ID: <1065136178.1132.39.camel@iron.libaux.ucsf.edu>

When I give plot some bad paramaters, it keeps giving me error messages
forever after.  I think the last time this happened, I even got the
error messages for completely unrelated, non-graphical functions.

Here's a recent example:
> plot(it[[31]][,c(1, 3)], type="b", usr=c(0, 20, -20, 5))
Warning messages: 
1: parameter "usr" couldn't be set in high-level plot() function 
2: parameter "usr" couldn't be set in high-level plot() function 
3: parameter "usr" couldn't be set in high-level plot() function 
4: parameter "usr" couldn't be set in high-level plot() function 
5: parameter "usr" couldn't be set in high-level plot() function 
6: parameter "usr" couldn't be set in high-level plot() function 
> plot(it[[31]][,c(1, 3)], type="b", ylim=c(-20, 5))
Warning messages: 
1: parameter "usr" couldn't be set in high-level plot() function 
2: parameter "usr" couldn't be set in high-level plot() function 
3: parameter "usr" couldn't be set in high-level plot() function 
4: parameter "usr" couldn't be set in high-level plot() function 
5: parameter "usr" couldn't be set in high-level plot() function 
6....

Is this a bug, or have I missed something?
R 1.7.1
-- 
Ross Boylan                                      wk:  (415) 502-4031
530 Parnassus Avenue (Library) rm 115-4          ross at biostat.ucsf.edu
Dept of Epidemiology and Biostatistics           fax: (415) 476-9856
University of California, San Francisco
San Francisco, CA 94143-0840                     hm:  (415) 550-1062



From rxg218 at psu.edu  Fri Oct  3 01:21:43 2003
From: rxg218 at psu.edu (Rajarshi Guha)
Date: Thu, 02 Oct 2003 23:21:43 -0000
Subject: [R] serializing R objects
Message-ID: <1065136776.2762.0.camel@localhost.localdomain>

Hi,
  is there any way that an R object (say a list of rlm models) could be
serialized to disk to be read in at a later time?

-------------------------------------------------------------------
Rajarshi Guha <rxg218 at psu.edu> <http://jijo.cjb.net>
GPG Fingerprint: 0CCA 8EE2 2EEB 25E2 AB04 06F7 1BB9 E634 9B87 56EE
-------------------------------------------------------------------
Entropy isn't what it used to be.



From tlumley at u.washington.edu  Fri Oct  3 02:16:45 2003
From: tlumley at u.washington.edu (Thomas Lumley)
Date: Thu, 2 Oct 2003 17:16:45 -0700 (PDT)
Subject: [R] Plot can't forget bad parameters
In-Reply-To: <1065136178.1132.39.camel@iron.libaux.ucsf.edu>
References: <1065136178.1132.39.camel@iron.libaux.ucsf.edu>
Message-ID: <Pine.A41.4.58.0310021713080.203530@homer04.u.washington.edu>

On Thu, 2 Oct 2003, Ross Boylan wrote:

> When I give plot some bad paramaters, it keeps giving me error messages
> forever after.  I think the last time this happened, I even got the
> error messages for completely unrelated, non-graphical functions.
<snip>
> Is this a bug, or have I missed something?
> R 1.7.1

It's to do with redrawing.  When the graphics device is redrawn on the
screen (eg if it is covered up and then exposed) the commands that created
it are replayed. (in fact, I think they may be replayed twice on some
systems).


It's not entirely a bug, since the redrawing is necessary: we don't keep a
bitmap, and even if we did we'd have to redraw on resize.  Suppressing
warnings on the redraw would make sense, though.

After you do a new plot the warnings should stop resurfacing.

	-thomas



From ross at biostat.ucsf.edu  Fri Oct  3 02:20:21 2003
From: ross at biostat.ucsf.edu (Ross Boylan)
Date: Thu, 02 Oct 2003 17:20:21 -0700
Subject: [R] Plot can't forget bad parameters
In-Reply-To: <C4178DC99E08604EA5E2BDB989F093800131EA4C@extas2-hba.tas.csiro.au>
References: <C4178DC99E08604EA5E2BDB989F093800131EA4C@extas2-hba.tas.csiro.au>
Message-ID: <1065140421.1136.52.camel@iron.libaux.ucsf.edu>

On Thu, 2003-10-02 at 16:54, Toby.Patterson at csiro.au wrote:
> Try 
> 
> rm(last.warning)
> 
> -----Original Message-----
> From: Ross Boylan [mailto:ross at biostat.ucsf.edu] 
> Sent: Friday, 3 October 2003 9:10 AM
> To: r-help
> Subject: [R] Plot can't forget bad parameters
> 
> 
> When I give plot some bad paramaters, it keeps giving me error messages
> forever after.  I think the last time this happened, I even got the
> error messages for completely unrelated, non-graphical functions.
> 
> Here's a recent example:
> > plot(it[[31]][,c(1, 3)], type="b", usr=c(0, 20, -20, 5))
> Warning messages: 
> 1: parameter "usr" couldn't be set in high-level plot() function 
> 2: parameter "usr" couldn't be set in high-level plot() function 
> 3: parameter "usr" couldn't be set in high-level plot() function 
> 4: parameter "usr" couldn't be set in high-level plot() function 
> 5: parameter "usr" couldn't be set in high-level plot() function 
> 6: parameter "usr" couldn't be set in high-level plot() function 
> > plot(it[[31]][,c(1, 3)], type="b", ylim=c(-20, 5))
> Warning messages: 
> 1: parameter "usr" couldn't be set in high-level plot() function 
> 2: parameter "usr" couldn't be set in high-level plot() function 
> 3: parameter "usr" couldn't be set in high-level plot() function 
> 4: parameter "usr" couldn't be set in high-level plot() function 
> 5: parameter "usr" couldn't be set in high-level plot() function 
> 6....
> 
> Is this a bug, or have I missed something?
> R 1.7.1

Well, that's very odd.  I did the rm(), and it printed out the warnings
again.  Then I repeated the two commands above--only this time, there
were no warnings after the second (i.e., things worked OK).



From MSchwartz at medanalytics.com  Fri Oct  3 02:29:09 2003
From: MSchwartz at medanalytics.com (Marc Schwartz)
Date: Thu, 02 Oct 2003 19:29:09 -0500
Subject: [R] serializing R objects
In-Reply-To: <1065136776.2762.0.camel@localhost.localdomain>
References: <1065136776.2762.0.camel@localhost.localdomain>
Message-ID: <1065140949.6915.28.camel@localhost>

On Thu, 2003-10-02 at 18:19, Rajarshi Guha wrote:
> Hi,
>   is there any way that an R object (say a list of rlm models) could be
> serialized to disk to be read in at a later time?
> 


See ?save and ?load

HTH,

Marc Schwartz



From mn216 at columbia.edu  Fri Oct  3 03:38:02 2003
From: mn216 at columbia.edu (Murad Nayal)
Date: Thu, 02 Oct 2003 21:38:02 -0400
Subject: [R] EMACS/ESS problems
References: <3F7C52AD.9C7B531E@columbia.edu> <85ekxv37to.fsf@blindglobe.net>
	<200310022059.h92KxRnm942917@surfer.sbm.temple.edu>
Message-ID: <3F7CD2FA.E401D4F@columbia.edu>



that was exactly what I was missing, Everything now works as advertised.
Thank you all so much for the help. you just turned my already very
satisfying experience using R into a even more enjoyable one.

all the best

Rich Heiberger wrote:
> 
> It looks like you are not using ESS correctly.  ESS is designed to
> work from a buffer containing a file whose name has the ".r" extension.
> Thus, open a file, for example,
>    C-x C-f myfile.r
> and then start using R.
> 
> My diagnosis is based on your line
> > highlight: (either in a text buffer or *ESS*)
> 
> ESS won't work from a text buffer because R code has different
> requirements from ordinary written paragraphs in English or another
> natural language.
> 
> The *ESS* buffer is part of the background mechanism that makes ESS work.
> It is not intended that a user ever look at the *ESS* buffer.
> 
> One other issue that your original email suggests.  We do not
> recommend the statement
>    v=c(1,2,3)
> for assignment.  It is much better to use the assignment arrow
>    v <- c(1,2,3)
> (with spaces on both sides of the arrow for legibility).  It is true
> that the "=" will usually do what you expect, but there are some subtle
> differences (mostly in argument lists to functions).  While I can expand on
> the reasons, for the moment I just want to suggest that you get into the
> habit of using the assignment arrow.
> 
> Rich

-- 
Murad Nayal M.D. Ph.D.
Department of Biochemistry and Molecular Biophysics
College of Physicians and Surgeons of Columbia University
630 West 168th Street. New York, NY 10032
Tel: 212-305-6884	Fax: 212-305-6926



From mai at uky.edu  Fri Oct  3 11:26:38 2003
From: mai at uky.edu (Mai Zhou)
Date: Fri, 03 Oct 2003 05:26:38 -0400
Subject: [R] how to get condition number from lm output
Message-ID: <1065173198.9c912b60mai@uky.edu>

Dear r-help, 

I ran 

output <- lm(formu, data=mydata)

I want to look at the condition number (to see if the matrix is near singular). How?

Also, I use the function stepAIC from MASS to select models, how can I see the condition numbers in each step?

While I am at it. I find a minor bug: 
R 1.7.1 on window XP, when you copy and paste by click
one button, then the input stopped, until you click the
"copy and paste" button again and click OK.

Thanks in advance.

Mai Zhou
mai at ms.uky.edu



From ligges at statistik.uni-dortmund.de  Fri Oct  3 12:19:06 2003
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Fri, 03 Oct 2003 12:19:06 +0200
Subject: [R] how to get condition number from lm output
References: <1065173198.9c912b60mai@uky.edu>
Message-ID: <3F7D4D1A.5175214E@statistik.uni-dortmund.de>



Mai Zhou wrote:
> 
> Dear r-help,
> 
> I ran
> 
> output <- lm(formu, data=mydata)
> 
> I want to look at the condition number (to see if the matrix is near singular). How?


 kappa(output)

 
> Also, I use the function stepAIC from MASS to select models, how can I see the condition numbers in each step?

See the argument "keep" described in ?stepAIC
So you can use something like

 stepAIC(. . . . ., keep = function(fit, AIC) kappa(fit))
 

> While I am at it. I find a minor bug:
> R 1.7.1 on window XP, when you copy and paste by click
> one button, then the input stopped, until you click the
> "copy and paste" button again and click OK.

ALready fixed for R-1.8.0 (beta).

Uwe Ligges



> Thanks in advance.
> 
> Mai Zhou
> mai at ms.uky.edu
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help



From ligges at statistik.uni-dortmund.de  Fri Oct  3 12:23:48 2003
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Fri, 03 Oct 2003 12:23:48 +0200
Subject: [R] how to get condition number from lm output
References: <1065173198.9c912b60mai@uky.edu>
	<3F7D4D1A.5175214E@statistik.uni-dortmund.de>
Message-ID: <3F7D4E34.AC939657@statistik.uni-dortmund.de>



Uwe Ligges wrote:
> 
> Mai Zhou wrote:
> >
> > Dear r-help,
> >
> > I ran
> >
> > output <- lm(formu, data=mydata)
> >
> > I want to look at the condition number (to see if the matrix is near singular). How?
> 
>  kappa(output)
> 
> 
> > Also, I use the function stepAIC from MASS to select models, how can I see the condition numbers in each step?
> 
> See the argument "keep" described in ?stepAIC
> So you can use something like
> 
>  stepAIC(. . . . ., keep = function(fit, AIC) kappa(fit))

Let me add: To print it also to the console, use:

 stepAIC(. . . . ., keep = 
     function(x,y) {k <- kappa(x); cat("kappa =", k, "\n"); k})

Uwe Ligges


> 
> > While I am at it. I find a minor bug:
> > R 1.7.1 on window XP, when you copy and paste by click
> > one button, then the input stopped, until you click the
> > "copy and paste" button again and click OK.
> 
> ALready fixed for R-1.8.0 (beta).
> 
> Uwe Ligges
> 
> > Thanks in advance.
> >
> > Mai Zhou
> > mai at ms.uky.edu
> >
> > ______________________________________________
> > R-help at stat.math.ethz.ch mailing list
> > https://www.stat.math.ethz.ch/mailman/listinfo/r-help



From f.calboli at ucl.ac.uk  Fri Oct  3 13:38:52 2003
From: f.calboli at ucl.ac.uk (Federico Calboli)
Date: Fri, 03 Oct 2003 11:38:52 -0000
Subject: [R] foo.RData or foo.r?
Message-ID: <1065181187.2755.34.camel@monkey>

Dear All,

I suspect this is kind of dumb, but when I was under the thrall of the
dark lord (read, using a W2K box), all my work in R files came out as
foo.RData. I moved on to GNU/Linux, and all the old .RData files keep on
working as they used. No problems in loading and stuff. But I use R from
the terminal. Assuming I decide to switch to emacs, do I need to save my
work as foo.r? what about my old files? shall I simply "mv" them to
foo.r? all in all, foo.Rdata and foo.r, does it make any difference?

Regards,

Federico
-- 



=================================

Federico C. F. Calboli

Department of Biology
University College London
Darwin Building 
Gower Street
London
WC1E 6BT

tel: 020 7679 4395
fax: 020 7679 7096

f.calboli at ucl.ac.uk



From p.dalgaard at biostat.ku.dk  Fri Oct  3 13:54:20 2003
From: p.dalgaard at biostat.ku.dk (Peter Dalgaard BSA)
Date: Fri, 03 Oct 2003 11:54:20 -0000
Subject: [R] foo.RData or foo.r?
In-Reply-To: <1065181187.2755.34.camel@monkey>
References: <1065181187.2755.34.camel@monkey>
Message-ID: <x2smmabklz.fsf@biostat.ku.dk>

Federico Calboli <f.calboli at ucl.ac.uk> writes:

> Dear All,
> 
> I suspect this is kind of dumb, but when I was under the thrall of the
> dark lord (read, using a W2K box), all my work in R files came out as
> foo.RData. I moved on to GNU/Linux, and all the old .RData files keep on
> working as they used. No problems in loading and stuff. But I use R from
> the terminal. Assuming I decide to switch to emacs, do I need to save my
> work as foo.r? what about my old files? shall I simply "mv" them to
> foo.r? all in all, foo.Rdata and foo.r, does it make any difference?

Nonono... foo.r (or foo.R) is for R *source* code, functions, scripts,
and such. .RData files work exactly as always.

-- 
   O__  ---- Peter Dalgaard             Blegdamsvej 3  
  c/ /'_ --- Dept. of Biostatistics     2200 Cph. N   
 (*) \(*) -- University of Copenhagen   Denmark      Ph: (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)             FAX: (+45) 35327907



From ramasamya at gis.a-star.edu.sg  Fri Oct  3 14:16:29 2003
From: ramasamya at gis.a-star.edu.sg (Adaikalavan RAMASAMY)
Date: Fri, 3 Oct 2003 20:16:29 +0800
Subject: [R] foo.RData or foo.r?
Message-ID: <6D9E9B9DF347EF4385F6271C64FB8D56075F91@BIONIC.biopolis.one-north.com>

*.R is for the script file and is ASCII type.
*.Rdata (or sometimes *.rda) is the usual extension for R data and
contains binary information.

If you try to cat a *.Rdata file, you will end up with gibberish as it
is binary.

Try opening *.Rdata with emacs if you can. Emacs will recognise it as a
fundamental type and not as an ESS type. There might be ways to
associate Rdata files with ESS. But other people might not consider
reading your "*.Rdata" files.

--
Adaikalavan Ramasamy 



-----Original Message-----
From: Federico Calboli [mailto:f.calboli at ucl.ac.uk] 
Sent: Friday, October 03, 2003 7:40 PM
To: r-help at stat.math.ethz.ch
Subject: [R] foo.RData or foo.r?


Dear All,

I suspect this is kind of dumb, but when I was under the thrall of the
dark lord (read, using a W2K box), all my work in R files came out as
foo.RData. I moved on to GNU/Linux, and all the old .RData files keep on
working as they used. No problems in loading and stuff. But I use R from
the terminal. Assuming I decide to switch to emacs, do I need to save my
work as foo.r? what about my old files? shall I simply "mv" them to
foo.r? all in all, foo.Rdata and foo.r, does it make any difference?

Regards,

Federico
-- 



=================================

Federico C. F. Calboli

Department of Biology
University College London
Darwin Building 
Gower Street
London
WC1E 6BT

tel: 020 7679 4395
fax: 020 7679 7096

f.calboli at ucl.ac.uk

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://www.stat.math.ethz.ch/mailman/listinfo/r-help



From f.calboli at ucl.ac.uk  Fri Oct  3 14:59:04 2003
From: f.calboli at ucl.ac.uk (Federico Calboli)
Date: Fri, 03 Oct 2003 12:59:04 -0000
Subject: [R] foo.RData or foo.r?
In-Reply-To: <6D9E9B9DF347EF4385F6271C64FB8D56075F91@BIONIC.biopolis.one-north.com>
References: <6D9E9B9DF347EF4385F6271C64FB8D56075F91@BIONIC.biopolis.one-north.com>
Message-ID: <1065185979.2755.40.camel@monkey>

On Fri, 2003-10-03 at 13:16, Adaikalavan RAMASAMY wrote:
> *.R is for the script file and is ASCII type.
> *.Rdata (or sometimes *.rda) is the usual extension for R data and
> contains binary information.
> 
> If you try to cat a *.Rdata file, you will end up with gibberish as it
> is binary.
> 
> Try opening *.Rdata with emacs if you can. Emacs will recognise it as a
> fundamental type and not as an ESS type. There might be ways to
> associate Rdata files with ESS. But other people might not consider
> reading your "*.Rdata" files.
> 
I dunno about this, but if I open R under emacs first and then I load my
foo.RData, it loads fine. Mind you, I am totally new to emacs, I even
had to install ESS from RPM, as I could not figure out how to do it from
source... 

Cheers,

Federico
-- 



=================================

Federico C. F. Calboli

Department of Biology
University College London
Darwin Building 
Gower Street
London
WC1E 6BT

tel: 020 7679 4395
fax: 020 7679 7096

f.calboli at ucl.ac.uk



From tblackw at umich.edu  Fri Oct  3 15:10:02 2003
From: tblackw at umich.edu (Thomas W Blackwell)
Date: Fri, 3 Oct 2003 09:10:02 -0400 (EDT)
Subject: [R] Query: weighting cells in histogram
In-Reply-To: <00e401c3899d$e364a300$5a4d2fc2@MBJEHV>
References: <00b401c388f5$519eea00$5a4d2fc2@MBJEHV>
	<Pine.SOL.4.58.0310021105330.2978@timepilot.gpcc.itd.umich.edu>
	<00e401c3899d$e364a300$5a4d2fc2@MBJEHV>
Message-ID: <Pine.SOL.4.58.0310030829060.794@timepilot.gpcc.itd.umich.edu>

Marten  -

The context of the example makes your question much clearer.
You want to display an empirical distribution for which you
have only selected quantiles.  For quick, I would display it
as the empirical cdf:

plot(c(0,breaks), c(0:10) / 10, type="s", xlab="income",
   ylab="cumulative fraction")  #  I'm just making up the labels !

However, your question asks to display this as a histogram.
For that, I would use the function  barplot(), and tinker
with the options until you get the display that you want.
The function  hist()  mainly deals with reducing a sample
(the actual data) to a structure that can be displayed as
a barplot.  In earlier versions of R and Splus,  hist()
actually called  barplot()  rather than  plot()  to display
its result.

The question you will need to ask yourself, as you display
the empirical distribution in histogram fashion, is:
should the *height* of each bar or the *area* of each bar
be proportional to fraction of the total mass which it
represents ?

The convention is to make the area of each bar proportional
to the mass, since most people want to interpret a histogram
as an empirical density estimate.  Accommodating this convention
is what the  hist()  argument "freq" is all about.

-  tom blackwell  -  u michigan medical school  -  ann arbor  -

On Fri, 3 Oct 2003, [iso-8859-1] M?rten Bjellerup wrote:

> Thank you for your help bu I'm not sure what you mean.
> I have a cumulative distribution in the form:
>
> cum.freq.         'breaks'
> -> 10%                5
> -> 20%                15
> -> 30%                50
> -> 40%                150
> etc
>
> I want to assign the weight 10% to each cell (0-5, 5-15, 15-50 etc)
> and not the actual observataions (which I don't have).  However, that's
> not how 'x' is specified.
>
> Thank you again,
>
> M?rten
>
> ----- Original Message -----
> From: "Thomas W Blackwell" <tblackw at umich.edu>
> To: "M?rten Bjellerup" <marten.bjellerup at ehv.vxu.se>
> Cc: <r-help at stat.math.ethz.ch>
> Sent: Thursday, October 02, 2003 5:13 PM
> Subject: Re: [R] Query: weighting cells in histogram
>
> Marten  -
>
> I don't know exactly what interpretation you have in mind
> for weights, but if you assign the value of  hist()  to a
> variable tmp, you can then assign the component  tmp$counts
> any value you like, and plot the result as a histogram
> using  plot(tmp).  See the section "Value:" in  help("hist").
>
> -  tom blackwell  -  u michigan medical school  -  ann arbor  -
>
> On Thu, 2 Oct 2003, [iso-8859-1] M?rten Bjellerup wrote:
>
> > I have the 'breaks' for the histogram ('hist') but I want
> > to weight the cells instead of using actual observations.
> > I thought that using freq=FALSE implied that the numbers
> > in 'x' were weights but this turned out to be wrong.
> > Any help and/or comment is very much appreciated.
> >
> > Regards,
> >
> > M?rten
> >
> > M?rten Bjellerup
> > Doctoral Student in Economics
> > School of Management and Economics
> > V?xj? University
> > SE-351 95  V?xj?
> > Sweden
> >
> > Tel: +46 470 708410
> > Fax: +46 470 82478
> > Mobile: +46 70 969 88 88
> > Mail: marten.bjellerup at ehv.vxu.se
> > Web: http://www.ehv.vxu.se



From jose.pinheiro at pharma.novartis.com  Fri Oct  3 15:16:14 2003
From: jose.pinheiro at pharma.novartis.com (jose.pinheiro@pharma.novartis.com)
Date: Fri, 3 Oct 2003 09:16:14 -0400
Subject: [R] Stat. Computing and Stat. Graphics 2004 Chambers Award
	competition
Message-ID: <OF60F8545C.EF09E531-ON85256DB4.0048C9FC-85256DB4.0048C7D2@EU.novartis.net>

The Statistical Computing Section of the American Statistical
Association announces the competition for the John M. Chambers
Statistical Software Award. In 1998 the Association for Computing
Machinery presented its Software System Award to John Chambers for the
design and development of S.  Dr. Chambers generously donated his
award to the Statistical Computing Section to endow an annual prize
for statistical software written by an undergraduate or graduate
student.  The prize carries with it a cash award of $1000, plus a
substantial allowance for travel to the annual Joint Statistical
Meetings where the award will be presented. Enclosed below is the full
text of the award announcement. More details can be found at the Stat.
Computing Section website at http://www.statcomputing.org. 



Best Regards,

--Jos? Pinheiro

Awards Chair
ASA Statistical Computing Section
-------------- next part --------------
An embedded and charset-unspecified text was scrubbed...
Name: ChambersAward2004.txt
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20031003/b98ed59d/ChambersAward2004.txt

From ndrew at efn.org  Fri Oct  3 16:20:23 2003
From: ndrew at efn.org (Nick Drew)
Date: Fri, 3 Oct 2003 07:20:23 -0700
Subject: R: [R] r editors
References: <Pine.GSO.4.44.0310020807450.8281-100000@eris.unalmed.edu.co>
Message-ID: <001d01c389b9$7c488b70$0a0010ac@Family>

On a similar note, what are some alternative data editors that one might use
instead of the default R data editor? I'm not interested in Excel but
something that's freeware, easy to install and use. Any recommendations?

~Nick

----- Original Message ----- 
From: "Juan Carlos Correa Morales" <jccorrea at unalmed.edu.co>
To: "Vito Muggeo" <vito.muggeo at giustizia.it>
Cc: <forkusam at yahoo.com>; <r-help at stat.math.ethz.ch>
Sent: Thursday, October 02, 2003 6:08 AM
Subject: Re: R: [R] r editors


> Hi:
>
> Also ConTEXT (freeware) is a nice editor with sintax highlighting scripts
> for R.
>
>
> On Thu, 2 Oct 2003, Vito Muggeo wrote:
>
> > In addition to WinEdt and (X)Emacs of course,
> > See also,
> > http://www.crimsoneditor.com/ (freeware)
> > http://www.jedit.org/ (freeware)
> > http://www.editpadpro.com/ (non-freeware)
> >
> > I know that there exist Syntax Highlighting files for R, but I don't
know
> > where you can find them.
> >
> > best,
> > vito
> >
> > ----- Original Message -----
> > From: forkusam <forkusam at yahoo.com>
> > To: <r-help at stat.math.ethz.ch>
> > Sent: Thursday, October 02, 2003 11:56 AM
> > Subject: [R] r editors
> >
> >
> > > Hi ,
> > > I am programming on a windows system and have problems
> > > using notepad which is my main editor.Each time I try
> > > to open the editor from the R IDE, R crashes.
> > > So I always have to copy my codes from notepad and
> > > paste in R to run them.
> > > CAn someone tell me if I am doing anything wrong or is
> > > there a better editor(freeware) which I could get.
> > > thanks
> > > cilver
> > >
> > > =====
> > > =====================
> > > Sylvie B. Forkusam
> > > Eppelheimer Str.52/A2-5-2
> > > 69115 Heidelberg, Germany
> > > Tel: (0049)-06221/346913
> > > Mobile: 0179-6816276
> > >
> > > ______________________________________________
> > > R-help at stat.math.ethz.ch mailing list
> > > https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> >
> > ______________________________________________
> > R-help at stat.math.ethz.ch mailing list
> > https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> >
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
>



From Ahmad_Abu_Hammour/STUDENTS/FIRE%FIRE at fordham.edu  Fri Oct  3 19:17:44 2003
From: Ahmad_Abu_Hammour/STUDENTS/FIRE%FIRE at fordham.edu (Ahmad_Abu_Hammour/STUDENTS/FIRE%FIRE@fordham.edu)
Date: Fri, 3 Oct 2003 13:17:44 -0400
Subject: [R] "deriv" and chain rule and matrix derivatives
Message-ID: <OF968F2309.8F73A3F0-ON85256CD5.005A500F-85256DB4.005F0209@fordham.edu>


   Hi,
   Anybody knows if I c   MATRIX
   derivation.
   Simple examples:
   y=expression(log(x))
   z=express   deriv(z,"x") # chain rule
   f=expression(det(x)) # where x is an (nxn) matrix
   deriv(f, "x")
   Is ther   Thank you ver   Ahmad Abu Hammour
   PS. by the way I tried to send the same email using my http-ba   account hammour at msn.com, but I could not, although I set send email to
   

From ggrothendieck at myway.com  Fri Oct  3 21:27:28 2003
From: ggrothendieck at myway.com (Gabor Grothendieck)
Date: Fri,  3 Oct 2003 15:27:28 -0400 (EDT)
Subject: R: [R] r editors
Message-ID: <20031003192728.6146D39A9@xmxpita.myway.com>





I have not used this enough to really recommend it or not
but if you want to check it out yourself have a look at
EpiData at:

   http://www.epidata.dk/

---

From: Nick Drew <ndrew at efn.org>
 
On a similar note, what are some alternative data editors that one might use
instead of the default R data editor? I'm not interested in Excel but
something that's freeware, easy to install and use. Any recommendations? 
 
 


_______________________________________________
No banners. No pop-ups. No kidding.
Introducing My Way - http://www.myway.com



From jasont at indigoindustrial.co.nz  Fri Oct  3 21:30:03 2003
From: jasont at indigoindustrial.co.nz (Jason Turner)
Date: Sat, 04 Oct 2003 07:30:03 +1200
Subject: [R] foo.RData or foo.r?
In-Reply-To: <1065185979.2755.40.camel@monkey>
References: <6D9E9B9DF347EF4385F6271C64FB8D56075F91@BIONIC.biopolis.one-north.com>
	<1065185979.2755.40.camel@monkey>
Message-ID: <3F7DCE3B.4060809@indigoindustrial.co.nz>

Federico Calboli wrote:
> On Fri, 2003-10-03 at 13:16, Adaikalavan RAMASAMY wrote:
> 
>>*.R is for the script file and is ASCII type.
>>*.Rdata (or sometimes *.rda) is the usual extension for R data and
>>contains binary information.
>>
>>If you try to cat a *.Rdata file, you will end up with gibberish as it
>>is binary.
>>
>>Try opening *.Rdata with emacs if you can. Emacs will recognise it as a
>>fundamental type and not as an ESS type. There might be ways to
>>associate Rdata files with ESS. But other people might not consider
>>reading your "*.Rdata" files.
>>
> 
> I dunno about this, but if I open R under emacs first and then I load my
> foo.RData, it loads fine.

By "fine", you mean it looks like normal R code?

On Un*x systems(including Linux), the extension is a matter of 
convenience, not necessity.  It's possible to save your R code, 
transcripts, etc with any extension you want (.R, .S, or even silly 
things, like .exe if you want).  And Emacs will still cheerfully read 
them just fine.

However, just because you *can*, doesn't mean you *should* ;)

There is a *convention* in R that workspace images or saved objects 
(using save.image() or save() ) should have filenames that end in .RData 
.  R code should have filenames that end in .R .  This isn't necessary, 
but it helps keep things tidy and easy to organise.  ESS also has some 
built-ins to recognise .R as R code, so it can do nice things like 
highlight syntax, indent well, and send code to an R session to be 
evaluated.

Short answer:  anything that's R code *should* be named with .R at the 
end.  Anthing R created after you told it save() or save.image() or 
answered "y" to the "Save workspace image" question *should* have a name 
ending with .RData .  None of this is strictly necessary, but there are 
a bunch of nice things that happen if you do.

Clear as mud?  ;)

Cheers

Jason
-- 
Indigo Industrial Controls Ltd.
http://www.indigoindustrial.co.nz
64-21-343-545
jasont at indigoindustrial.co.nz



From jasont at indigoindustrial.co.nz  Fri Oct  3 21:37:34 2003
From: jasont at indigoindustrial.co.nz (Jason Turner)
Date: Sat, 04 Oct 2003 07:37:34 +1200
Subject: R: [R] r editors
In-Reply-To: <001d01c389b9$7c488b70$0a0010ac@Family>
References: <Pine.GSO.4.44.0310020807450.8281-100000@eris.unalmed.edu.co>
	<001d01c389b9$7c488b70$0a0010ac@Family>
Message-ID: <3F7DCFFE.5060205@indigoindustrial.co.nz>

Nick Drew wrote:

> On a similar note, what are some alternative data editors that one might use
> instead of the default R data editor? I'm not interested in Excel but
> something that's freeware, easy to install and use. Any recommendations?

If you don't like Excel for financial or license reasons, Open office 
works quite well these days (www.openoffice.org).

If you don't like Excel because you find spreadsheets clunky, the next 
step for me perl or some such thing.

Something in between?  I'm all ears.

Cheers

Jason
-- 
Indigo Industrial Controls Ltd.
http://www.indigoindustrial.co.nz
64-21-343-545
jasont at indigoindustrial.co.nz



From ligges at statistik.uni-dortmund.de  Fri Oct  3 22:25:09 2003
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Fri, 03 Oct 2003 22:25:09 +0200
Subject: R: [R] r editors
References: <Pine.GSO.4.44.0310020807450.8281-100000@eris.unalmed.edu.co>
	<001d01c389b9$7c488b70$0a0010ac@Family>
	<3F7DCFFE.5060205@indigoindustrial.co.nz>
Message-ID: <3F7DDB25.71712D40@statistik.uni-dortmund.de>

Jason Turner wrote:
> 
> Nick Drew wrote:
> 
> > On a similar note, what are some alternative data editors that one might use
> > instead of the default R data editor? I'm not interested in Excel but
> > something that's freeware, easy to install and use. Any recommendations?
> 
> If you don't like Excel for financial or license reasons, Open office
> works quite well these days (www.openoffice.org).
> 
> If you don't like Excel because you find spreadsheets clunky, the next
> step for me perl or some such thing.
> 
> Something in between?  I'm all ears.
> 
> Cheers
> 
> Jason


I think Nick is talking about a tool to edit an R objeckt like a
matrix/vector/data.frame within a spreadsheet.

You might want to take a look at the Omegahat package RGnumeric, which
provides a bi-directional interface to the Open Source Gnumeric spread
sheet tool (http://www.omegahat.org/RGnumeric/index.html). I never tried
it out, though. I think there is no native way to use a spread sheet
editor directly on R objects.

Uwe Ligges



From rxg218 at psu.edu  Fri Oct  3 23:03:55 2003
From: rxg218 at psu.edu (Rajarshi Guha)
Date: Fri, 03 Oct 2003 21:03:55 -0000
Subject: [R] allocating memory in a C module
Message-ID: <1065214917.2484.13.camel@localhost.localdomain>

Hi,
  I'm using a package that has a number of formats. I have C code to
parse these formats the results of which are generally integer arrays.

I would like to utilize these modules in R rather than writing R code to
read in these files (and also to learn about R extensions).

Essentially what I want is to do something like this:

load_tsets <- function(t,c,p)
.C( "c_load_tsets",
as.integer(t),
as.integer(c),
as.integer(p))

t,c,p should be R list objects, and empty when supplied to the function
load_tsets. On return each one will contain a list of integers. Thus in
the C function I need to allocate storage for the 3 arrays.

I've tried using Realloc but when I run the function the returned list
does not show an increase in length. I've included a simplified version
of the C code below:

#include<R.h>

void t(int *v)
{
    int i;
    v = (int*) Realloc(v,5,int*);
    for (i = 0; i < 5; i++)
        v[i] = i*i;
    printf("Hello\n");
    return;
}

I load it into R by doing:

> dyn.load('t.so')
> ft <- function(x)
+    .C('t',
+    as.integer(x))
>

Then I do

> x <- c()
> ret <- ft(x)
Hello
> ret 
[[1]]
numeric(0)

Do I need to go to the .Call interface? 

Is there any code that comes with the R distro (or if anybody has some
similar code that I could look at)?

Any pointers would be appreciated,
thanks,

-------------------------------------------------------------------
Rajarshi Guha <rxg218 at psu.edu> <http://jijo.cjb.net>
GPG Fingerprint: 0CCA 8EE2 2EEB 25E2 AB04 06F7 1BB9 E634 9B87 56EE
-------------------------------------------------------------------
All life evolves by the differential survival of replicating entities.
-- Dawkins



From tlumley at u.washington.edu  Sat Oct  4 02:06:32 2003
From: tlumley at u.washington.edu (Thomas Lumley)
Date: Fri, 3 Oct 2003 17:06:32 -0700 (PDT)
Subject: [R] allocating memory in a C module
In-Reply-To: <1065214917.2484.13.camel@localhost.localdomain>
References: <1065214917.2484.13.camel@localhost.localdomain>
Message-ID: <Pine.A41.4.58.0310031652560.122140@homer03.u.washington.edu>

On Fri, 3 Oct 2003, Rajarshi Guha wrote:

> Hi,
>   I'm using a package that has a number of formats. I have C code to
> parse these formats the results of which are generally integer arrays.
>
> I would like to utilize these modules in R rather than writing R code to
> read in these files (and also to learn about R extensions).
>
> Essentially what I want is to do something like this:
>
> load_tsets <- function(t,c,p)
> .C( "c_load_tsets",
> as.integer(t),
> as.integer(c),
> as.integer(p))
>
> t,c,p should be R list objects

I think you mean vectors: as.integer() doesn't work on lists.

>				, and empty when supplied to the function
> load_tsets. On return each one will contain a list of integers. Thus in
> the C function I need to allocate storage for the 3 arrays.
>
> I've tried using Realloc but when I run the function the returned list
> does not show an increase in length.

No, it wouldn't.  You can't change the size of an object inside .C.
Since R has no way to find out how big the object is (C doesn't provide
such a facility), it must assume it is the same size that was passed in.


>
> Do I need to go to the .Call interface?

Yes.

> Is there any code that comes with the R distro (or if anybody has some
> similar code that I could look at)?

Here's some code that does a pointless example

In C:


#include "R.h"

/* function takes no arguments and returns an object */
SEXP pie(){

	int n;
	SEXP alist, avector;

	/* work out how long a list to return */
	n = 2;
	/* create the list */
	PROTECT(alist = allocVector(VECSXP, n));

	/* create the first vector and populate it */
	n = 4;
	PROTECT(avector = allocVector(INTSXP, n));
	INTEGER(avector)[0] = 3;
	INTEGER(avector)[1] = 1;
	INTEGER(avector)[2] = 4;
	INTEGER(avector)[3] = 1;
	/* stick it into the list */
	SET_VECTOR_ELT(alist, 0, avector);
	UNPROTECT(1);

       /* create the second vector and populate it */
        n = 4;
        PROTECT(avector = allocVector(INTSXP, n));
        INTEGER(avector)[0] = 5;
        INTEGER(avector)[1] = 9;
        INTEGER(avector)[2] = 2;
        INTEGER(avector)[3] = 7;
        /* stick it into the list */
        SET_VECTOR_ELT(alist, 1, avector);
        UNPROTECT(1);

	UNPROTECT(1);  /* avector*/
	return avector;

}


in R

dyn.load("pie.so")
.Call("pie")




Thomas Lumley			Assoc. Professor, Biostatistics
tlumley at u.washington.edu	University of Washington, Seattle



From tlumley at u.washington.edu  Sat Oct  4 02:10:47 2003
From: tlumley at u.washington.edu (Thomas Lumley)
Date: Fri, 3 Oct 2003 17:10:47 -0700 (PDT)
Subject: [R] allocating memory in a C module (fwd)
Message-ID: <Pine.A41.4.58.0310031707410.122140@homer03.u.washington.edu>


I pressed ^X in the wrong window.  Here's the right code:


Here's some code that does a pointless example

In C:


#include "Rinternals.h"

/* function takes no arguments and returns an object */
SEXP pie(){

	int n;
	SEXP alist, avector;

	/* work out how long a list to return */
	n = 2;
	/* create the list */
	PROTECT(alist = allocVector(VECSXP, n));

	/* create the first vector and populate it */
	n = 4;
	PROTECT(avector = allocVector(INTSXP, n));
	INTEGER(avector)[0] = 3;
	INTEGER(avector)[1] = 1;
	INTEGER(avector)[2] = 4;
	INTEGER(avector)[3] = 1;
	/* stick it into the list */
	SET_VECTOR_ELT(alist, 0, avector);
	UNPROTECT(1);

       /* create the second vector and populate it */
        n = 4;
        PROTECT(avector = allocVector(INTSXP, n));
        INTEGER(avector)[0] = 5;
        INTEGER(avector)[1] = 9;
        INTEGER(avector)[2] = 2;
        INTEGER(avector)[3] = 7;
        /* stick it into the list */
        SET_VECTOR_ELT(alist, 1, avector);
        UNPROTECT(1);

	UNPROTECT(1);  /* alist*/
	return alist;

}


in R

dyn.load("pie.so")
.Call("pie")


	-thomas

Thomas Lumley			Assoc. Professor, Biostatistics
tlumley at u.washington.edu	University of Washington, Seattle



From rxg218 at psu.edu  Sat Oct  4 06:51:30 2003
From: rxg218 at psu.edu (Rajarshi Guha)
Date: Sat, 04 Oct 2003 04:51:30 -0000
Subject: [R] More questions about R extension programming
Message-ID: <1065242972.2386.46.camel@localhost.localdomain>

On Fri, 2003-10-03 at 17:01, Rajarshi Guha wrote:
> Hi,
>   I'm using a package that has a number of formats. I have C code to
> parse these formats the results of which are generally integer arrays.
> 
> I would like to utilize these modules in R rather than writing R code to
> read in these files (and also to learn about R extensions).

Thanks for the pointers to the above question. I have a few more!

1) I would like my C function to be passed a character string. Thus I
define the function as

SEXP _loadsets(SEXP filename)
{
    FILE *f;

    PROTECT(filename = AS_CHARACTER(filename));
    f = fopen(filename,"r");
    UNPROTECT(1);
.....
.....
}

However compiling it gives me:

loadset.c: In function `_loadsets':
loadset.c:25: warning: passing arg 1 of `fopen' from incompatible
pointer type
gcc -shared -L/usr/local/lib -o loadset.so loadset.o

How can I coerce/convert the SEXP type to a char*?

2) The function returns a list object whose elements themselves are
lists. Is there any way I can make those elements arrays rather than
lists? I see in Rinternals.h there is a function defined as

Rf_allocArray(SEXPTYPE, SEXP);

What do I need to supply for the second argument? Could I use this
function rather than allocVector() to create an actual array object? And
then say set elements with 

INTEGER(anarray)[0] = 1

or are the 'getter'/'setter' functions for arrays different ( I could'nt
seem to find any) ?

3) I'm a little puzzled since I allocate a list (say length = 2) object
by

alist = allocVector(VECSXP,2);

and use the same syntax for a vector object. From what I understand a
vector is the same as a list. Is this true?
 
4) When writing C code does it make sense to differentiate between a
list object and an array object? Or is it better to simply coerce the
returned list objects (via as.array()) from within R.

-------------------------------------------------------------------
Rajarshi Guha <rxg218 at psu.edu> <http://jijo.cjb.net>
GPG Fingerprint: 0CCA 8EE2 2EEB 25E2 AB04 06F7 1BB9 E634 9B87 56EE
-------------------------------------------------------------------
There is no truth to the allegation that statisticians are mean.
They are just your standard normal deviates.



From matogar at um.es  Sat Oct  4 08:18:26 2003
From: matogar at um.es (Manuel Ato Garcia)
Date: Sat, 04 Oct 2003 08:18:26 +0200
Subject: [R] mixed effects with nlme
Message-ID: <3.0.3.32.20031004081826.00e90230@gaia.fcu.um.es>


Dear R users:

 I have some difficulties analizing data with mixed effects NLME and the
last version of R. More concretely, I have a repeated measures design with
a single group and 2 experimental factors (say A and B) and my interest is
to compare additive and nonadditive models. 

   suj  rv    A       B
1   s1   4   a1      b1
2   s1   5   a1      b2
3   s1   7   a1      b3
4   s1   1   a2      b1
5   s1   4   a2      b2
6   s1   2   a2      b3
7   s2   6   a1      b1
8   s2   8   a1      b2
9   s2  10   a1      b3
10  s2   3   a2      b1
11  s2   6   a2      b2
12  s2   6   a2      b3
13  s3   1   a1      b1
14  s3   6   a1      b2
15  s3   5   a1      b3
16  s3   3   a2      b1
17  s3   5   a2      b2
18  s3   4   a2      b3
19  s4   2   a1      b1
20  s4  10   a1      b2
21  s4  12   a1      b3
22  s4   1   a2      b1
23  s4   4   a2      b2
24  s4   7   a2      b3
25  s5   5   a1      b1
26  s5  10   a1      b2
27  s5  10   a1      b3
28  s5   5   a2      b1
29  s5   6   a2      b2
30  s5   5   a2      b3
31  s6   1   a1      b1
32  s6   7   a1      b2
33  s6   8   a1      b3
34  s6   2   a2      b1
35  s6   8   a2      b2
36  s6   7   a2      b3

It is very easy to fit these data with base R function AOV:

NonAdditive model:
 aov(rv ~ A*B + Error(suj+suj/A+suj/B)

Additive model:
 aov(rv ~ A*B + Error(suj)

and also easy with SAS MIXED (I missed some obvious lines):

NonAdditive model
 model vr = A B A*B;
 random suj A*suj B*suj;
 repeated / type=cs subj=suj;

Additive model;
 model vr = A B A*B /ddfm=satterth;
 repeated / type=cs subj=suj;
 
Using LME I do not find any problems to fit the additive model with

 lme(vr~A*B, random=~1|suj, cor=corCompSymm())

but I have found some difficulties fitting the nonadditive model.

 Can anyone help me?
 
 Thanks in advance.

	Manuel Ato
	Dpto. Psic.B?sica y Metodolog?a
	Apartado 4021
	30080 MURCIA (Spain)
	e-mail: matogar at um.es



From maechler at stat.math.ethz.ch  Sat Oct  4 12:05:36 2003
From: maechler at stat.math.ethz.ch (Martin Maechler)
Date: Sat, 4 Oct 2003 12:05:36 +0200
Subject: [R] foo.RData or foo.r?
In-Reply-To: <3F7DCE3B.4060809@indigoindustrial.co.nz>
References: <6D9E9B9DF347EF4385F6271C64FB8D56075F91@BIONIC.biopolis.one-north.com>
	<1065185979.2755.40.camel@monkey>
	<3F7DCE3B.4060809@indigoindustrial.co.nz>
Message-ID: <16254.39792.825196.158106@gargle.gargle.HOWL>

Just one thing to add to Jason's very helpful posting and
explanations:

>>>>> "Jason" == Jason Turner <jasont at indigoindustrial.co.nz>
>>>>>     on Sat, 04 Oct 2003 07:30:03 +1200 writes:

	  .....
	  .....

    Jason> Short answer: anything that's R code *should* be
    Jason> named with .R at the end.  Anthing R created after
    Jason> you told it save() or save.image() or answered "y" to
    Jason> the "Save workspace image" question *should* have a
    Jason> name ending with .RData .  None of this is strictly
    Jason> necessary, but there are a bunch of nice things that
    Jason> happen if you do.

.rda  has been considered as okay as
.RData  by many (incl R core developers);  look e.g. at "2." in
the `Details' section of  help(data).

Martin Maechler <maechler at stat.math.ethz.ch>	http://stat.ethz.ch/~maechler/
Seminar fuer Statistik, ETH-Zentrum  LEO C16	Leonhardstr. 27
ETH (Federal Inst. Technology)	8092 Zurich	SWITZERLAND
phone: x-41-1-632-3408		fax: ...-1228			<><



From cjosephlu at seed.net.tw  Sat Oct  4 13:52:27 2003
From: cjosephlu at seed.net.tw (cjosephlu@seed.net.tw)
Date: Sat,  4 Oct 2003 19:52:27 +0800
Subject: [R] How to use panel.qqmathline?
Message-ID: <1065268347.3f7eb47bdd7f1@webmail.seed.net.tw>

Dear R users:

How can I use panel.qqmathline, in package lattice, to add
straight lines onto the plots generated by qqmath?
I read help pages of qqmath, panel.qqmathline, xyplot, ...,
but just can't one example that shows how to make it work.

For example, 

> data(sleep)
> qqnorm(~ extra | group, data=sleep, aspect=1)

how can I use panel.qqmathline?

Thanks very much for your help,

Joseph Lu
Department of Statistics
National Cheng-Kung University
Tainan, Taiwan

-------------------------------------------------
This mail sent through Seednet Webmail
http://webmail.seed.net.tw



From f.calboli at ucl.ac.uk  Sat Oct  4 14:48:22 2003
From: f.calboli at ucl.ac.uk (Federico Calboli)
Date: Sat, 04 Oct 2003 12:48:22 -0000
Subject: [R] foo.Rdata or foo.r?
Message-ID: <1065271767.2753.48.camel@monkey>

Dear All,

thanks for your clarifications. When I said that my .RData files load
fines, I should have added that the files are images that contain
objetcs, not code. In the bad ol' days under the thrall of the dark
ruler (W2K), I would write all my code in text files, saved as foo.txt,
and then copy the code with cut 'n' paste when needed... sins of a
reckless youth, I daresay, but now I am older and wiser, and I'll start
saving my code as .R files.

Cheers,

Federico
  
-- 



=================================

Federico C. F. Calboli

Department of Biology
University College London
Darwin Building 
Gower Street
London
WC1E 6BT

tel: 020 7679 4395
fax: 020 7679 7096

f.calboli at ucl.ac.uk



From tlumley at u.washington.edu  Sat Oct  4 15:32:19 2003
From: tlumley at u.washington.edu (Thomas Lumley)
Date: Sat, 4 Oct 2003 06:32:19 -0700 (PDT)
Subject: [R] More questions about R extension programming
In-Reply-To: <1065242972.2386.46.camel@localhost.localdomain>
References: <1065242972.2386.46.camel@localhost.localdomain>
Message-ID: <Pine.A41.4.58.0310040623560.15002@homer18.u.washington.edu>

On Fri, 4 Oct 2003, Rajarshi Guha wrote:

> On Fri, 2003-10-03 at 17:01, Rajarshi Guha wrote:
> > Hi,
> >   I'm using a package that has a number of formats. I have C code to
> > parse these formats the results of which are generally integer arrays.
> >
> > I would like to utilize these modules in R rather than writing R code to
> > read in these files (and also to learn about R extensions).
>
> Thanks for the pointers to the above question. I have a few more!
>
> 1) I would like my C function to be passed a character string. Thus I
> define the function as
>
> SEXP _loadsets(SEXP filename)
> {
>     FILE *f;
>
>     PROTECT(filename = AS_CHARACTER(filename));
>     f = fopen(filename,"r");
>     UNPROTECT(1);
> .....
> .....
> }
>
> However compiling it gives me:
>
> loadset.c: In function `_loadsets':
> loadset.c:25: warning: passing arg 1 of `fopen' from incompatible
> pointer type
> gcc -shared -L/usr/local/lib -o loadset.so loadset.o
>
> How can I coerce/convert the SEXP type to a char*?

Your code can't possibly be right, because it defined filename as SEXP and
passes it to a function that accepts const char *.

You want

fopen(CHARACTER(STRING_ELT(filename,0)), "r")

Since filename is a vector of strings (with one element) you need
STRING_ELT to extract one of the strings. Now you have a SEXP and need
CHARACTER() to extract a pointer to the actual chars, just as I used
INTEGER() to extract a pointer to the actual ints in an integer vector
SEXP.

> 2) The function returns a list object whose elements themselves are
> lists.

No. It returns a list whose elements are integer vectors.

>		 Is there any way I can make those elements arrays rather than
> lists?

It's probably easiest to do this manipulation in R afterwards

>
> 3) I'm a little puzzled since I allocate a list (say length = 2) object
> by
>
> alist = allocVector(VECSXP,2);
>
> and use the same syntax for a vector object. From what I understand a
> vector is the same as a list. Is this true?

No (or yes, but not in the way you mean).  A list contains vectors or
lists as elements.  A vector contains numbers or strings.  The first
element to allocVector says what sort of thing you are allocating.

> 4) When writing C code does it make sense to differentiate between a
> list object and an array object? Or is it better to simply coerce the
> returned list objects (via as.array()) from within R.

It is often but not always sensible to do all the manipulation of
complicated attributes in R.  I think you mean array(), not as.array().

	-thomas


Thomas Lumley			Assoc. Professor, Biostatistics
tlumley at u.washington.edu	University of Washington, Seattle



From silika.tereshchenko at unisg.ch  Sat Oct  4 15:39:20 2003
From: silika.tereshchenko at unisg.ch (silika.tereshchenko@unisg.ch)
Date: Sat, 4 Oct 2003 15:39:20 +0200
Subject: [R] (no subject)
Message-ID: <OF19D6A84E.2DA9750E-ONC1256DB5.0049A4A9@unisg.ch>

Dear all,


I have the following question. I have to fit the hierarchical model for the
hypothesis concern the individual-level effects by controlling for the
individual -level attributes and national-level contextual effects  on
individuals by using R.

O have to obtain the estimates of the impact of the second-level (national:
GDP per capita) effects on individuals ( in this instance the impact of the
GDP per capita on the attitudes towards the EU enlargement) by allowing
national differences in both slopes (GDP per capita) and interceps.

In R programm for the fitting the hierarchical models i can use the nlme
package. I found a literature (Bryk and Raudenbush) for the hierarchical
models and understood how to build this models by using the survey data.
The question arise if I?m thinking about the combination of the datasets:
the GDP per capita that will be hold as a constant and the survey data for
each respondent.

My question is how I could solve this problem and merge the data in such a
form that should be appropriate for the hieararchical regression model.



Thank a lot in advance for the help,

Silika Tereshchenko



From tlumley at u.washington.edu  Sat Oct  4 15:48:25 2003
From: tlumley at u.washington.edu (Thomas Lumley)
Date: Sat, 4 Oct 2003 06:48:25 -0700 (PDT)
Subject: [R] More questions about R extension programming
In-Reply-To: <Pine.A41.4.58.0310040623560.15002@homer18.u.washington.edu>
References: <1065242972.2386.46.camel@localhost.localdomain>
	<Pine.A41.4.58.0310040623560.15002@homer18.u.washington.edu>
Message-ID: <Pine.A41.4.58.0310040646360.15002@homer18.u.washington.edu>


You should look at the code in the "foreign" package, which is largely
about reading data into C

	-thomas

Thomas Lumley			Assoc. Professor, Biostatistics
tlumley at u.washington.edu	University of Washington, Seattle



From andrejk at zrc-sazu.si  Sat Oct  4 16:35:18 2003
From: andrejk at zrc-sazu.si (Andrej Kveder)
Date: Sat, 4 Oct 2003 16:35:18 +0200
Subject: [R] r editors
In-Reply-To: <20031002095611.98396.qmail@web10505.mail.yahoo.com>
Message-ID: <FHEEJBDDCNPPNJEACDJAOECJDEAA.andrejk@zrc-sazu.si>

I may be a bit late in replying, but there is another editor which is
shareware and has highlighting for R (and a lot of other languages as well).
The TextPad:
http://www.texpad.com
http://www.textpad.com/add-ons/files/syntax/r.zip

I use in many ways possible. I also combine it with the source statement in
R, as was suggested by Gabor.
I have nothing but the best to tell about the TextPad.

Andrej

_________
Andrej Kveder, M.A.
researcher
Institute of Medical Sciences SRS SASA; Novi trg 2, SI-1000 Ljubljana,
Slovenia
phone: +386 1 47 06 440   fax: +386 1 42 61 493


-----Original Message-----
From: r-help-bounces at stat.math.ethz.ch
[mailto:r-help-bounces at stat.math.ethz.ch]On Behalf Of forkusam
Sent: Thursday, October 02, 2003 11:56 AM
To: r-help at stat.math.ethz.ch
Subject: [R] r editors


Hi ,
I am programming on a windows system and have problems
using notepad which is my main editor.Each time I try
to open the editor from the R IDE, R crashes.
So I always have to copy my codes from notepad and
paste in R to run them.
CAn someone tell me if I am doing anything wrong or is
there a better editor(freeware) which I could get.
thanks
cilver

=====
=====================
Sylvie B. Forkusam
Eppelheimer Str.52/A2-5-2
69115 Heidelberg, Germany
Tel: (0049)-06221/346913
Mobile: 0179-6816276

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://www.stat.math.ethz.ch/mailman/listinfo/r-help



From spencer.graves at pdf.com  Sat Oct  4 17:10:06 2003
From: spencer.graves at pdf.com (Spencer Graves)
Date: Sat, 04 Oct 2003 08:10:06 -0700
Subject: [R] mixed effects with nlme
In-Reply-To: <3.0.3.32.20031004081826.00e90230@gaia.fcu.um.es>
References: <3.0.3.32.20031004081826.00e90230@gaia.fcu.um.es>
Message-ID: <3F7EE2CE.2050002@pdf.com>

1.  Have you reviewed Pinhiero and Bates (2000) Mixed Effects Models in 
S and S-Plus (Springer)?  I think the answer is there. 

2.  Observing the philosophy that a quick response that may contain an 
error is often better than a perfect but late response, I will offer a 
suggestion I have not checked:  Have you considered "random = 
~1+A+B|subj"? 

hope this helps.  spencer graves

Manuel Ato Garcia wrote:

>Dear R users:
>
> I have some difficulties analizing data with mixed effects NLME and the
>last version of R. More concretely, I have a repeated measures design with
>a single group and 2 experimental factors (say A and B) and my interest is
>to compare additive and nonadditive models. 
>
>   suj  rv    A       B
>1   s1   4   a1      b1
>2   s1   5   a1      b2
>3   s1   7   a1      b3
>4   s1   1   a2      b1
>5   s1   4   a2      b2
>6   s1   2   a2      b3
>7   s2   6   a1      b1
>8   s2   8   a1      b2
>9   s2  10   a1      b3
>10  s2   3   a2      b1
>11  s2   6   a2      b2
>12  s2   6   a2      b3
>13  s3   1   a1      b1
>14  s3   6   a1      b2
>15  s3   5   a1      b3
>16  s3   3   a2      b1
>17  s3   5   a2      b2
>18  s3   4   a2      b3
>19  s4   2   a1      b1
>20  s4  10   a1      b2
>21  s4  12   a1      b3
>22  s4   1   a2      b1
>23  s4   4   a2      b2
>24  s4   7   a2      b3
>25  s5   5   a1      b1
>26  s5  10   a1      b2
>27  s5  10   a1      b3
>28  s5   5   a2      b1
>29  s5   6   a2      b2
>30  s5   5   a2      b3
>31  s6   1   a1      b1
>32  s6   7   a1      b2
>33  s6   8   a1      b3
>34  s6   2   a2      b1
>35  s6   8   a2      b2
>36  s6   7   a2      b3
>
>It is very easy to fit these data with base R function AOV:
>
>NonAdditive model:
> aov(rv ~ A*B + Error(suj+suj/A+suj/B)
>
>Additive model:
> aov(rv ~ A*B + Error(suj)
>
>and also easy with SAS MIXED (I missed some obvious lines):
>
>NonAdditive model
> model vr = A B A*B;
> random suj A*suj B*suj;
> repeated / type=cs subj=suj;
>
>Additive model;
> model vr = A B A*B /ddfm=satterth;
> repeated / type=cs subj=suj;
> 
>Using LME I do not find any problems to fit the additive model with
>
> lme(vr~A*B, random=~1|suj, cor=corCompSymm())
>
>but I have found some difficulties fitting the nonadditive model.
>
> Can anyone help me?
> 
> Thanks in advance.
>
>	Manuel Ato
>	Dpto. Psic.B?sica y Metodolog?a
>	Apartado 4021
>	30080 MURCIA (Spain)
>	e-mail: matogar at um.es
>
>______________________________________________
>R-help at stat.math.ethz.ch mailing list
>https://www.stat.math.ethz.ch/mailman/listinfo/r-help
>  
>



From tblackw at umich.edu  Sat Oct  4 17:10:36 2003
From: tblackw at umich.edu (Thomas W Blackwell)
Date: Sat, 4 Oct 2003 11:10:36 -0400 (EDT)
Subject: [R] (no subject)
In-Reply-To: <OF19D6A84E.2DA9750E-ONC1256DB5.0049A4A9@unisg.ch>
References: <OF19D6A84E.2DA9750E-ONC1256DB5.0049A4A9@unisg.ch>
Message-ID: <Pine.SOL.4.58.0310041059010.8552@zektor.gpcc.itd.umich.edu>

Silika  -

By far the best reference for the nlme package is the book by its
authors:

J.C. Pinheiro and D.M. Bates.  Mixed effects models in S and Splus.
Springer, NY, 2000.  ISBN (US) 0-387-989579.  (There is a different,
European, ISBN number but my library catalog doesn't give it.)

This citation is NOT given in  help("lme"),  presumably because the
help page pre-dates the book.  The Laird and Ware (1982) article
cited on the help page may also be useful.

-  tom blackwell  -  u michigan medical school  -  ann arbor  -

On Sat, 4 Oct 2003 silika.tereshchenko at unisg.ch wrote:

> Dear all,
>
> I have the following question. I have to fit the hierarchical model for the
> hypothesis concern the individual-level effects by controlling for the
> individual -level attributes and national-level contextual effects  on
> individuals by using R.
>
> O have to obtain the estimates of the impact of the second-level (national:
> GDP per capita) effects on individuals ( in this instance the impact of the
> GDP per capita on the attitudes towards the EU enlargement) by allowing
> national differences in both slopes (GDP per capita) and interceps.
>
> In R programm for the fitting the hierarchical models i can use the nlme
> package. I found a literature (Bryk and Raudenbush) for the hierarchical
> models and understood how to build this models by using the survey data.
> The question arise if I?m thinking about the combination of the datasets:
> the GDP per capita that will be hold as a constant and the survey data for
> each respondent.
>
> My question is how I could solve this problem and merge the data in such a
> form that should be appropriate for the hieararchical regression model.
>
> Thank a lot in advance for the help,
>
> Silika Tereshchenko



From spencer.graves at pdf.com  Sat Oct  4 17:14:54 2003
From: spencer.graves at pdf.com (Spencer Graves)
Date: Sat, 04 Oct 2003 08:14:54 -0700
Subject: [R] hierarchical mixed-effects model?  [was (no subject)]
In-Reply-To: <OF19D6A84E.2DA9750E-ONC1256DB5.0049A4A9@unisg.ch>
References: <OF19D6A84E.2DA9750E-ONC1256DB5.0049A4A9@unisg.ch>
Message-ID: <3F7EE3EE.5060201@pdf.com>

Please use an informative subject. 

Have you consulted the book Pinhiero and Bates (2000) Mixed-Effects 
Models in S and S-Plus (Springer)?  I was able to use lme successfully 
only after reading the first few chapters of this book and working 
through the examples therein.  I have not used nlme, but if I wanted to 
use it, I would start with this book. 

hope this helps.  spencer graves

silika.tereshchenko at unisg.ch wrote:

>Dear all,
>
>
>I have the following question. I have to fit the hierarchical model for the
>hypothesis concern the individual-level effects by controlling for the
>individual -level attributes and national-level contextual effects  on
>individuals by using R.
>
>O have to obtain the estimates of the impact of the second-level (national:
>GDP per capita) effects on individuals ( in this instance the impact of the
>GDP per capita on the attitudes towards the EU enlargement) by allowing
>national differences in both slopes (GDP per capita) and interceps.
>
>In R programm for the fitting the hierarchical models i can use the nlme
>package. I found a literature (Bryk and Raudenbush) for the hierarchical
>models and understood how to build this models by using the survey data.
>The question arise if I?m thinking about the combination of the datasets:
>the GDP per capita that will be hold as a constant and the survey data for
>each respondent.
>
>My question is how I could solve this problem and merge the data in such a
>form that should be appropriate for the hieararchical regression model.
>
>
>
>Thank a lot in advance for the help,
>
>Silika Tereshchenko
>
>______________________________________________
>R-help at stat.math.ethz.ch mailing list
>https://www.stat.math.ethz.ch/mailman/listinfo/r-help
>  
>



From henric.nilsson at statisticon.se  Sat Oct  4 16:49:18 2003
From: henric.nilsson at statisticon.se (Henric Nilsson)
Date: Sat, 04 Oct 2003 16:49:18 +0200
Subject: [R] How to use panel.qqmathline?
In-Reply-To: <1065268347.3f7eb47bdd7f1@webmail.seed.net.tw>
Message-ID: <5.2.1.1.0.20031004155916.03b8fd30@10.0.10.66>

At 19:52 2003-10-04 +0800, you wrote:

>How can I use panel.qqmathline, in package lattice, to add
>straight lines onto the plots generated by qqmath?
>[...]
> > data(sleep)
> > qqnorm(~ extra | group, data=sleep, aspect=1)

Try

qqmath(~ extra | group,  data = sleep, aspect = 1,
             distribution = qnorm, prepanel = prepanel.qqmathline,
             panel = function(x, y, ...)  {
               panel.qqmathline(y, distribution = qnorm, ...)
               panel.qqmath(x, y, ...)
             })

Hope this helps,
Henric

---------------------------------------------------------------------------------------
Henric Nilsson, Statistician

Statisticon AB, ?stra ?gatan 31, SE-753 22 UPPSALA
Phone (Direct): +46 (0)18 18 22 37
Mobile: +46 (0)70 211 68 36
Fax: +46 (0)18 18 22 33

<http://www.statisticon.se>



From harald.bartel at prozentor.de  Sat Oct  4 17:29:09 2003
From: harald.bartel at prozentor.de (Harald Bartel)
Date: Sat, 04 Oct 2003 17:29:09 +0200
Subject: [R] subset VAR time series models
Message-ID: <3F7EE745.9020608@prozentor.de>

Dear R user,

I would like to estimate a multivariate VAR model (vector autoregressive 
model) with arbitrary individual linear parameter constraints. So far I 
used the "ar" command from package ts for unrestricted models. 
Unfortunately, this command does not offer the possibility to estimate 
subset VAR models.

Is there a package or command that I can use?

Thank you very much!
Harald

-- 
------------------------------------------------------
PROZENTOR GmbH
Financial Content Provider
Bergstr. 67
10115 Berlin

Fon: +49 (0) 30 28 44 59-42
Fax: +49 (0) 30 28 44 59-59

email: harald.bartel at prozentor.de

PROZENTOR GmbH: http://www.prozentor.de
kostenlose Aktienprognosen: http://www.happyYuppie.com



From edd at debian.org  Sat Oct  4 18:00:21 2003
From: edd at debian.org (Dirk Eddelbuettel)
Date: Sat, 4 Oct 2003 11:00:21 -0500
Subject: [R] subset VAR time series models
In-Reply-To: <3F7EE745.9020608@prozentor.de>
References: <3F7EE745.9020608@prozentor.de>
Message-ID: <20031004160021.GA29293@sonny.eddelbuettel.com>

On Sat, Oct 04, 2003 at 05:29:09PM +0200, Harald Bartel wrote:
> I would like to estimate a multivariate VAR model (vector autoregressive 
> model) with arbitrary individual linear parameter constraints. So far I 
> used the "ar" command from package ts for unrestricted models. 
> Unfortunately, this command does not offer the possibility to estimate 
> subset VAR models.
> 
> Is there a package or command that I can use?

No, I don't think so (though it sometimes hard to keep up with the growth in
the CRAN archive).  Paul Gilbert's dse package may be the closest.

But contributions are certainly welcome, it would be nice to have a VAR
package available on CRAN.

Dirk

-- 
Those are my principles, and if you don't like them... well, I have others.
                                                -- Groucho Marx



From cliff at ms.washington.edu  Sat Oct  4 19:42:54 2003
From: cliff at ms.washington.edu (Cliff Lunneborg)
Date: Sat, 4 Oct 2003 10:42:54 -0700
Subject: [R] Point and click
Message-ID: <001701c38a9e$f163a0d0$52b2eb0c@C56909A>

The following query raises the question: What is it that students learn
from point and click dialogs?"

Date: Fri, 03 Oct 2003 16:57:42 -0400
From:
To: s-news at lists.biostat.wustl.edu
Subject: Splus question
Message-ID: <BAY9-F52Q7OoK42LpqI000238f3 at hotmail.com>

I don't know if this is the right list to post this question. If not,
please let me know where I should post this. I have a dataframe with 3
variables: ID, Y, Group Group is either 1, 2, 3. I am trying to run a
t-test to compare the three groups on outcome Y. I know how to do this
using the point and click dialogs. But can't get it to work on the
command line. When I type: t.test(y, group)  it just compares y and
group
as though they represent the two samples. I tried doing something with
tapply(y, group) but don't know how that works. If someone knows, please
email me. Thanks.

**********************************************************
Cliff Lunneborg, Professor Emeritus, Statistics &
Psychology, University of Washington, Seattle
cliff at ms.washington.edu



From Ted.Harding at nessie.mcc.ac.uk  Sat Oct  4 21:20:29 2003
From: Ted.Harding at nessie.mcc.ac.uk ( (Ted Harding))
Date: Sat, 04 Oct 2003 20:20:29 +0100 (BST)
Subject: [R] Point and click
In-Reply-To: <001701c38a9e$f163a0d0$52b2eb0c@C56909A>
Message-ID: <XFMail.031004202029.Ted.Harding@nessie.mcc.ac.uk>

On 04-Oct-03 Cliff Lunneborg wrote:
> The following query raises the question: What is it that students learn
> from point and click dialogs?"

Two things, I think.

1. How not to think.
2. How to be unaware of possible error.

Maybe others.

Ted.

> 
> Date: Fri, 03 Oct 2003 16:57:42 -0400
> From:
> To: s-news at lists.biostat.wustl.edu
> Subject: Splus question
> Message-ID: <BAY9-F52Q7OoK42LpqI000238f3 at hotmail.com>
> 
> I don't know if this is the right list to post this question. If not,
> please let me know where I should post this. I have a dataframe with 3
> variables: ID, Y, Group Group is either 1, 2, 3. I am trying to run a
> t-test to compare the three groups on outcome Y. I know how to do this
> using the point and click dialogs. But can't get it to work on the
> command line. When I type: t.test(y, group)  it just compares y and
> group
> as though they represent the two samples. I tried doing something with
> tapply(y, group) but don't know how that works. If someone knows,
> please
> email me. Thanks.
> 
> **********************************************************
> Cliff Lunneborg, Professor Emeritus, Statistics &
> Psychology, University of Washington, Seattle
> cliff at ms.washington.edu
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help

--------------------------------------------------------------------
E-Mail: (Ted Harding) <Ted.Harding at nessie.mcc.ac.uk>
Fax-to-email: +44 (0)870 167 1972
Date: 04-Oct-03                                       Time: 20:20:29
------------------------------ XFMail ------------------------------



From stievie at utanet.at  Sat Oct  4 23:46:23 2003
From: stievie at utanet.at (stievie@utanet.at)
Date: Sat, 04 Oct 2003 23:46:23 +0200
Subject: [R] Point and click
Message-ID: <01c38ac0$Blat.v2.2$fb42a250@utanet.at>

Hi , you wrote on 04.10.03 at 21:20:29
> On 04-Oct-03 Cliff Lunneborg wrote:
> > The following query raises the question: What is it that students learn
> > from point and click dialogs?"
> 
> Two things, I think.
> 
> 1. How not to think.
> 2. How to be unaware of possible error.

Maybe they learn this at University, and not from point and click dialogs?

Sorry, couldn't resist

stefan (a professional student since years ;-)



From bournephysio at shaw.ca  Sun Oct  5 03:38:40 2003
From: bournephysio at shaw.ca (Doug Bourne)
Date: Sat, 04 Oct 2003 18:38:40 -0700
Subject: [R] Placing legends
Message-ID: <3F7F7620.5010402@shaw.ca>

I'm trying to automate making a bunch of figures and I need a way to 
automate legend position.  As I understand it the legend is placed based 
on coordinates.  I don't know before hand what the coordinates are going 
to be.  On one graph my y axis might go from -50 to -10.  On another it 
might go from 0 to 180.

Is there any way to query where the origin is?  Or is there another way 
to place the legend?

Thanks,
Doug Bourne
Graduate Student
University of British Columbia



From jasont at indigoindustrial.co.nz  Sun Oct  5 04:02:48 2003
From: jasont at indigoindustrial.co.nz (Jason Turner)
Date: Sun, 05 Oct 2003 15:02:48 +1300
Subject: [R] Placing legends
In-Reply-To: <3F7F7620.5010402@shaw.ca>
References: <3F7F7620.5010402@shaw.ca>
Message-ID: <3F7F7BC8.4020206@indigoindustrial.co.nz>

Doug Bourne wrote:

> I'm trying to automate making a bunch of figures and I need a way to 
> automate legend position.  As I understand it the legend is placed based 
> on coordinates.  I don't know before hand what the coordinates are going 
> to be.  On one graph my y axis might go from -50 to -10.  On another it 
> might go from 0 to 180.
> 

Easiest way is to:
1) make your plot.
2) change to co-ordinate system.
3) add the legend.

plot(...some stuff...)
par(usr=c(0,1,0,1))
# say you want you legend at 60% across, 80% up.
legend(x=0.6,y=0.8,...legend stuff...)

?par has this.  Check out "usr"

Cheers

Jason

-- 
Indigo Industrial Controls Ltd.
http://www.indigoindustrial.co.nz
64-21-343-545
jasont at indigoindustrial.co.nz



From jasont at indigoindustrial.co.nz  Sun Oct  5 04:33:34 2003
From: jasont at indigoindustrial.co.nz (Jason Turner)
Date: Sun, 05 Oct 2003 15:33:34 +1300
Subject: [R] Point and click
In-Reply-To: <001701c38a9e$f163a0d0$52b2eb0c@C56909A>
References: <001701c38a9e$f163a0d0$52b2eb0c@C56909A>
Message-ID: <3F7F82FE.8060604@indigoindustrial.co.nz>

Cliff Lunneborg wrote:

> The following query raises the question: What is it that students learn
> from point and click dialogs?"

[long boring ramble - I'm sure you know how to find the delete key when 
it gets tedious ;) ]

I think it's fair to say that they don't learn the software, or its 
internal structures/methods/etc at all (Mac users have always told me 
that this is the point; I'm not getting into that holy war).

I will state one thing - the point and click interface *can* teach about 
the underlying software mechanisms, but it has to be a well thought-out 
interface.

Example - I never used a computer that had a heirarchical filesystem 
until I went to university.  I then had to learn two:  IBM PC clones 
were used to teach us AutoCAD (ever seen AutoCAD run on an 8086 with 
640K of RAM?  It's not pretty ;).  I also worked on the student 
newspaper, which was a Mac-only shop.  The Macs came later; I was 
working in layout initially, back when layout really involved printouts, 
big cardboard sheets, and wax to stick the articles to the cardboard.

I did not understand directory structure on the PCs at all.  It never 
clicked.  I simply "parroted" the commands I was taught to use, and 
managed to stay out of trouble.  "Polly wanna .dwg file."  Then I worked 
on the Macs.  The display of folders made it clear to me, in about five 
seconds.  I realised right away what I'd been missing, and flew back 
into the PC world with a bit more insight.

As a result, I know that nice point-and-drool GUIs can educate about the 
underlying design and approach, but the design really must be considered 
very carefully.  It's also important to take away the "crutch" from time 
to time, too (going back to PCs, in the above example).

I've yet to see a GUI for a statistical software system that meets this 
criteria, and I can't imagine what one would look like (I'm an engineer, 
not a designer ;).  I don't even know if it can be done. I'm sure 
anybody who understood heirarchical filesystems prior to GUIs would've 
thought similar things - that if one can't understand something so 
basic, there's really no simplifying or alternate explanation that'll 
work.  Had that thinking prevailed, I might've been another engineer who 
doesn't know a thing about how computers actually work (there's no 
shortage, believe me).

Just my $0.05 NZ (exchange rates, and all)

Cheers

Jason
-- 
Indigo Industrial Controls Ltd.
http://www.indigoindustrial.co.nz
64-21-343-545
jasont at indigoindustrial.co.nz



From wang at galton.uchicago.edu  Sun Oct  5 12:00:51 2003
From: wang at galton.uchicago.edu (Yong Wang)
Date: Sun, 5 Oct 2003 05:00:51 -0500 (CDT)
Subject: [R] please help me on this problem
In-Reply-To: <mailman.195.1065346749.27000.r-help@stat.math.ethz.ch>
Message-ID: <Pine.GSO.4.05.10310050450220.11578-100000@aitken.uchicago.edu>

Dear all:
I am totally new to R, actually, statistics software.I get two very
simple, even stupid question:
1) where I should put the data file in order to use it , I tried to build
a
work dir in library( package:base) and save the text file (data) there,
then, I use read.table(filename), not work; I tried the full path, still
not work. I must have done something wrong.
2) is it possible to create a data file in R instead of put data in a txt
file and then save the file under R?
I tried to find answer from the introduction, failed.
I really appreciate your help, thank you very much.
best



From jasont at indigoindustrial.co.nz  Sun Oct  5 12:24:01 2003
From: jasont at indigoindustrial.co.nz (Jason Turner)
Date: Sun, 05 Oct 2003 23:24:01 +1300
Subject: [R] please help me on this problem
In-Reply-To: <Pine.GSO.4.05.10310050450220.11578-100000@aitken.uchicago.edu>
References: <Pine.GSO.4.05.10310050450220.11578-100000@aitken.uchicago.edu>
Message-ID: <3F7FF141.7070804@indigoindustrial.co.nz>

Yong Wang wrote:

> Dear all:
> I am totally new to R, actually, statistics software.I get two very
> simple, even stupid question:
> 1) where I should put the data file in order to use it , I tried to build
> a
> work dir in library( package:base) and save the text file (data) there,

I'm not sure where such a work directory might be, but it sounds like 
you need to show a TA what you did.

It sounds like you need to carefully read the FAQ.

help.start()

the follow the "Frequently Asked Questions" link, and (if you're using R 
on windows) the "FAQ for Windows Port"

> then, I use read.table(filename), not work; I tried the full path, still
> not work. I must have done something wrong.

Quite likely.  But without more details, there's no way to say more. 
There isn't only one way to make a mistake, so there isn't one answer to 
"it didn't work".

Read the FAQ.  It really does help.  Read it carefully, and not in a 
frame of mind where you're saying to yourself "I'll never understand 
this, this can't possibly help."  You will, it can.

> 2) is it possible to create a data file in R instead of put data in a txt
> file and then save the file under R?

Yes, after you've got a grip on the above.  *After* the above is clear, 
read the help pages for "save" and "load".

Read the FAQ.

> I tried to find answer from the introduction, failed.

But not the FAQ, presumably.

> I really appreciate your help, thank you very much.

No problem.  By the way, read the FAQ.

Cheers

Jason
-- 
Indigo Industrial Controls Ltd.
http://www.indigoindustrial.co.nz
64-21-343-545
jasont at indigoindustrial.co.nz



From baron at psych.upenn.edu  Sun Oct  5 12:26:57 2003
From: baron at psych.upenn.edu (Jonathan Baron)
Date: Sun, 5 Oct 2003 06:26:57 -0400
Subject: [R] please help me on this problem
In-Reply-To: <Pine.GSO.4.05.10310050450220.11578-100000@aitken.uchicago.edu>
References: <mailman.195.1065346749.27000.r-help@stat.math.ethz.ch>
	<Pine.GSO.4.05.10310050450220.11578-100000@aitken.uchicago.edu>
Message-ID: <20031005102657.GA17683@mail1.sas.upenn.edu>

See the section called "Basic method" in
http://www.psych.upenn.edu/~baron/rpsych/rpsych.html

On 10/05/03 05:00, Yong Wang wrote:
>Dear all:
>I am totally new to R, actually, statistics software.I get two very
>simple, even stupid question:
>1) where I should put the data file in order to use it , I tried to build
>a
>work dir in library( package:base) and save the text file (data) there,
>then, I use read.table(filename), not work; I tried the full path, still
>not work. I must have done something wrong.

I'm surprised that it didn't work with the full path.  I assume
the error message indicated that it could not find the file, as
opposed to some other sort of error.  Relative paths work too.

You don't say what operating system you are using, and it
probably matters.  But on Linux/Unix I usually start R from the
directory where the data files are, and I have several of these
for different projects.  (Actually I use ESS, so I start R from
within (X)emacs.  When I say alt-x, R, I'm asked for a directory,
and, if I'm not already in that directory, I enter it using a
relative path name.)

In sum, at least in Unix/Linux, you can put your data anywhere,
and either you start R from where the data are or you use a
relative or full path name.

>2) is it possible to create a data file in R instead of put data in a txt
>file and then save the file under R?

Yes, but this isn't the best way to do things unless the data
file is pretty small.  You can use c().  It is sometimes helpful
to use matrix() with the argument "horiz=T" as in Example 2 in
"Advanced analysis of variance examples" in
http://www.psych.upenn.edu/~baron/rpsych/rpsych.html

See also scan() - that is, look at the help for it.

And you can also use edit().  For example:
m1 <- matrix(0,5,5)
m2 <- edit(m1)

To save the object you've created, use write(), write.table(),
write.matrix(), dput(), or save().  These differ in formatting
and in how you read it back in.

Jon
-- 
Jonathan Baron, Professor of Psychology, University of Pennsylvania
Home page:            http://www.sas.upenn.edu/~baron
R page:               http://finzi.psych.upenn.edu/



From ligges at statistik.uni-dortmund.de  Sun Oct  5 12:43:42 2003
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Sun, 05 Oct 2003 12:43:42 +0200
Subject: [R] please help me on this problem
References: <Pine.GSO.4.05.10310050450220.11578-100000@aitken.uchicago.edu>
Message-ID: <3F7FF5DE.41E104C8@statistik.uni-dortmund.de>

Yong Wang wrote:
> 
> Dear all:
> I am totally new to R, actually, statistics software.I get two very
> simple, even stupid question:

But you are not new to e-mails. So, please don't specify a stupid
subject line, but a sensible one. (OK, the rest of this message's body
is unspecific as well, hence the subject might be quite sensible in a
way, tough.)


> 1) where I should put the data file in order to use it , I tried to build
> a work dir in library( package:base) and save the text file (data) there,
> then, I use read.table(filename), not work; I tried the full path, still
> not work. I must have done something wrong.

The working directory is the one you are starting R in, or what you have
set with setwd(). It's not a good idea to create a working directory
within the library section. Specifying the full path should always work.

What you did wrong is:
- not reading the manual "An Introduction to R" carefully
- not reading the manual "R Data Import/Export"
- beeing to unspecific. After having read the manuals and help pages,
you may ask a specific question, but including error messages and code
you used (we cannot help if we cannot look at your mistakes).


> 2) is it possible to create a data file in R instead of put data in a txt
> file and then save the file under R?

Yes. Exporting Data (including ASCII formats) is also covered in the "R
Data Import/Export" manual.

Uwe Ligges

> I tried to find answer from the introduction, failed.
> I really appreciate your help, thank you very much.
> best
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help



From feh3k at spamcop.net  Sun Oct  5 13:04:36 2003
From: feh3k at spamcop.net (Frank E Harrell Jr)
Date: Sun, 5 Oct 2003 07:04:36 -0400
Subject: [R] Placing legends
In-Reply-To: <3F7F7620.5010402@shaw.ca>
References: <3F7F7620.5010402@shaw.ca>
Message-ID: <20031005070436.336b8fbf.feh3k@spamcop.net>

On Sat, 04 Oct 2003 18:38:40 -0700
Doug Bourne <bournephysio at shaw.ca> wrote:

> I'm trying to automate making a bunch of figures and I need a way to 
> automate legend position.  As I understand it the legend is placed based 
> on coordinates.  I don't know before hand what the coordinates are going 
> to be.  On one graph my y axis might go from -50 to -10.  On another it 
> might go from 0 to 180.
> 
> Is there any way to query where the origin is?  Or is there another way 
> to place the legend?
> 
> Thanks,
> Doug Bourne
> Graduate Student
> University of British Columbia
> 

If you want to go a bit farther, the labcurve function in the Hmisc package can put a legend on the most empty portion of the plot automatically, or it can label curves directly, where they are most separated.
---
Frank E Harrell Jr    Professor and Chair            School of Medicine
                      Department of Biostatistics    Vanderbilt University



From dmurdoch at pair.com  Sun Oct  5 13:58:10 2003
From: dmurdoch at pair.com (Duncan Murdoch)
Date: Sun, 05 Oct 2003 07:58:10 -0400
Subject: [R] please help me on this problem
In-Reply-To: <20031005102657.GA17683@mail1.sas.upenn.edu>
References: <mailman.195.1065346749.27000.r-help@stat.math.ethz.ch>
	<Pine.GSO.4.05.10310050450220.11578-100000@aitken.uchicago.edu>
	<20031005102657.GA17683@mail1.sas.upenn.edu>
Message-ID: <2s00ov8bhuqam2lvmie3sa0eo4r8249fvr@4ax.com>

On Sun, 5 Oct 2003 06:26:57 -0400, you wrote:

>I'm surprised that it didn't work with the full path.  I assume
>the error message indicated that it could not find the file, as
>opposed to some other sort of error.  Relative paths work too.

If Yong was using Windows, it's likely he specified the path as
something like "c:\data\mydata.txt".  Because S was invented on Unix,
it uses the backslash as an escape character, and this doesn't work.
As various docs say, he needs to write it as "c:\\data\\mydata.txt" or
"c:/data/mydata.txt".  Even better, use something like

 filename <- file.choose()
 read.table(filename)

or 

 read.table(file.choose())

It would make sense to have file.choose() as the default value for
file arguments just about everywhere, but almost none of the
file-using functions do that now.

>You don't say what operating system you are using, and it
>probably matters.  But on Linux/Unix I usually start R from the
>directory where the data files are, and I have several of these
>for different projects. 

One of the design flaws in Windows is that it's very hard for the user
to know the current directory.   Most program shortcuts (including the
default one created by the R installer) change the directory on
startup, and R doesn't tell you the current directory unless you ask
(using getwd()).

>In sum, at least in Unix/Linux, you can put your data anywhere,
>and either you start R from where the data are or you use a
>relative or full path name.

The same is true in Windows.

Duncan Murdoch



From h_m_ at po.harenet.ne.jp  Sun Oct  5 14:46:04 2003
From: h_m_ at po.harenet.ne.jp (Hiroto Miyoshi)
Date: Sun, 5 Oct 2003 21:46:04 +0900
Subject: [R] stepAIC problem
Message-ID: <002c01c38b3e$a4292920$0b01a8c0@hirotohome>

Dear R-users

I have a probelm running stepAIC in R1.7.1
I wrote a program which used stepAIC as a part of it,
and it worked fine while I was using the previous version of
R1.7.0. However, I found the program did not work any more.
Now, R produces a message which tells
"Error in as.data.frame.default(data) :
can't coerce function into a data.frame" every time I
run the part of stepAIC.
Even the following small program shows the same problem.
Is stepAIC of the new version altered?
Could anyone help me please?

--small example
> library(MASS)
> x1<-runif(100)
> x2<-runif(100)
> x3<-runif(100)
> x4<-runif(100)
> x5<-runif(100)
> y<-x1+x2+x3+runif(100)
> t<-data.frame(y=y,x1=x1,x2=x2,x3=x3,x4=x4,x5=x5)
> x<-lm(y~x1+x2+x3+x4+x5,data=t)
> stepAIC(x)
Start:  AIC= -247.61
 y ~ x1 + x2 + x3 + x4 + x5

       Df Sum of Sq      RSS      AIC
- x5    1 3.747e-06    7.456 -249.608
- x4    1     0.026    7.483 -249.254
<none>                 7.456 -247.609
- x1    1     4.866   12.322 -199.375
- x2    1     8.182   15.639 -175.543
- x3    1     8.597   16.054 -172.922
Error in as.data.frame.default(data) : can't coerce function into a
data.frame

---------------------------
Hiroto Miyoshi???????
h_m_ at po.harenet.ne.jp



From Arne.Muller at aventis.com  Sun Oct  5 14:51:05 2003
From: Arne.Muller at aventis.com (Arne.Muller@aventis.com)
Date: Sun, 5 Oct 2003 14:51:05 +0200
Subject: [R] Jonckheere-Terpstra test
Message-ID: <C80ECAFA2ACC1B45BE45D133ED660ADECDCBEE@crbsmxsusr04.pharma.aventis.com>

Hello,

can anybody here explain what a Jonckheere-Terpstra test is and whether it is
implemented in R? I just know it's a non-parametric test, otherwise I've no
clue about it ;-( . Are there alternatives to this test?

	thanks for help,

	Arne



From rossini at blindglobe.net  Sun Oct  5 14:55:50 2003
From: rossini at blindglobe.net (A.J. Rossini)
Date: Sun, 05 Oct 2003 05:55:50 -0700
Subject: [R] stepAIC problem
In-Reply-To: <002c01c38b3e$a4292920$0b01a8c0@hirotohome> (Hiroto Miyoshi's
	message of "Sun, 5 Oct 2003 21:46:04 +0900")
References: <002c01c38b3e$a4292920$0b01a8c0@hirotohome>
Message-ID: <85llrzhmi1.fsf@blindglobe.net>

"Hiroto Miyoshi" <h_m_ at po.harenet.ne.jp> writes:

> Could anyone help me please?
>
> --small example
>> library(MASS)
>> x1<-runif(100)
>> x2<-runif(100)
>> x3<-runif(100)
>> x4<-runif(100)
>> x5<-runif(100)
>> y<-x1+x2+x3+runif(100)
>> t<-data.frame(y=y,x1=x1,x2=x2,x3=x3,x4=x4,x5=x5)

It is going to be very unhappy if it tries to call the t() transpose
function.   Please rename this variable and try again.

>> x<-lm(y~x1+x2+x3+x4+x5,data=t)
>> stepAIC(x)
> Start:  AIC= -247.61
>  y ~ x1 + x2 + x3 + x4 + x5
>
>        Df Sum of Sq      RSS      AIC
> - x5    1 3.747e-06    7.456 -249.608
> - x4    1     0.026    7.483 -249.254
> <none>                 7.456 -247.609
> - x1    1     4.866   12.322 -199.375
> - x2    1     8.182   15.639 -175.543
> - x3    1     8.597   16.054 -172.922
> Error in as.data.frame.default(data) : can't coerce function into a
> data.frame


best,
-tony

-- 
rossini at u.washington.edu            http://www.analytics.washington.edu/ 
Biomedical and Health Informatics   University of Washington
Biostatistics, SCHARP/HVTN          Fred Hutchinson Cancer Research Center
UW (Tu/Th/F): 206-616-7630 FAX=206-543-3461 | Voicemail is unreliable
FHCRC  (M/W): 206-667-7025 FAX=206-667-4812 | use Email

CONFIDENTIALITY NOTICE: This e-mail message and any attachme...{{dropped}}



From rossini at blindglobe.net  Sun Oct  5 14:59:24 2003
From: rossini at blindglobe.net (A.J. Rossini)
Date: Sun, 05 Oct 2003 05:59:24 -0700
Subject: [R] Jonckheere-Terpstra test
In-Reply-To: <C80ECAFA2ACC1B45BE45D133ED660ADECDCBEE@crbsmxsusr04.pharma.aventis.com>
	(Arne Muller's message of "Sun, 5 Oct 2003 14:51:05 +0200")
References: <C80ECAFA2ACC1B45BE45D133ED660ADECDCBEE@crbsmxsusr04.pharma.aventis.com>
Message-ID: <85he2nhmc3.fsf@blindglobe.net>

<Arne.Muller at aventis.com> writes:

> can anybody here explain what a Jonckheere-Terpstra test is and whether it is
> implemented in R? I just know it's a non-parametric test, otherwise I've no
> clue about it ;-( . Are there alternatives to this test?

Search google.

The third one (at least on my hit) references a description I used
when teaching back in the dark ages.  The others seem relevant, too.

best,
-tony

-- 
rossini at u.washington.edu            http://www.analytics.washington.edu/ 
Biomedical and Health Informatics   University of Washington
Biostatistics, SCHARP/HVTN          Fred Hutchinson Cancer Research Center
UW (Tu/Th/F): 206-616-7630 FAX=206-543-3461 | Voicemail is unreliable
FHCRC  (M/W): 206-667-7025 FAX=206-667-4812 | use Email

CONFIDENTIALITY NOTICE: This e-mail message and any attachme...{{dropped}}



From cjosephlu at seed.net.tw  Sun Oct  5 16:10:55 2003
From: cjosephlu at seed.net.tw (cjosephlu@seed.net.tw)
Date: Sun,  5 Oct 2003 22:10:55 +0800
Subject: [R] How to use panel.qqmathline?
In-Reply-To: <5.2.1.1.0.20031004155916.03b8fd30@10.0.10.66>
References: <5.2.1.1.0.20031004155916.03b8fd30@10.0.10.66>
Message-ID: <1065363055.3f80266f04cae@webmail.seed.net.tw>

Mr. Nilsson,

Thanks very much for your help!

>From Henric Nilsson <henric.nilsson at statisticon.se>:
> At 19:52 2003-10-04 +0800, you wrote:
> 
> >How can I use panel.qqmathline, in package lattice, to add
> >straight lines onto the plots generated by qqmath?
> >[...]
> > > data(sleep)
> > > qqnorm(~ extra | group, data=sleep, aspect=1)
> 
> Try
> 
> qqmath(~ extra | group,  data = sleep, aspect = 1,
>              distribution = qnorm, prepanel = prepanel.qqmathline,
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
This is the part that I did not figure out!
Actually, I thought I can leave "distribution = qnorm" out 
since it is default.

>              panel = function(x, y, ...)  {
>                panel.qqmathline(y, distribution = qnorm, ...)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
Again, I thought "distribution = qnorm" can be left out since
it is specified before.
Since it causes error without "distribution = qnorm",
I was confused by the lines in help(panel.qqmathline):

distribution: quantile function for reference theoretical distribution.
          This is automatically passed in when this is used as a panel
          function in `qqmath'.

I don't understand how the "automatically passed in" can work.

>                panel.qqmath(x, y, ...)
>              })
> 
> Hope this helps,
> Henric
> 
> ------------------------------------------------------------------------------
---------
> Henric Nilsson, Statistician
> 
> Statisticon AB, Östra Ågatan 31, SE-753 22 UPPSALA
> Phone (Direct): +46 (0)18 18 22 37
> Mobile: +46 (0)70 211 68 36
> Fax: +46 (0)18 18 22 33
> 
> <http://www.statisticon.se>
> 

Best regards,

Joseph Lu
Department of Statistics
National Cheng-Kung University
Tainan, Taiwan


-------------------------------------------------
This mail sent through Seednet Webmail
http://webmail.seed.net.tw



From jens.schmidt at mailbox.tu-dresden.de  Sun Oct  5 20:16:26 2003
From: jens.schmidt at mailbox.tu-dresden.de (Jens Schmidt)
Date: Sun, 5 Oct 2003 20:16:26 +0200 (MEST)
Subject: [R] Creating survival object
Message-ID: <Pine.A41.4.44.0310052006310.18646-100000@rcs7.urz.tu-dresden.de>

Hi,

I have survival data from a population:

time    alive       treatment
0       20          A
3       12          A
5       10          A
8       7           A
10      7           A
15      5           A
20      5           A


(time=0 is start of the experiment and initial number of individuals is
alive=20; time=20 is end of experiment)

If I don't misunderstood the survival documentation, always a survival
object is required, i.e. created with function Surv(time, event)

So I have to transform my data (assuming no censoring, event=0 is alive
and event=1 is dead):

time    event   treatment
20      0       A
20      0       A
20      0       A
20      0       A
20      0       A
15      1       A
15      1       A
8       1       A
8       1       A
8       1       A
5       1       A
5       1       A
3       1       A
3       1       A
3       1       A
3       1       A
3       1       A
3       1       A
3       1       A
3       1       A


Is there a function or any other [R] code doing this?

or

Is it possible  to perform survival analysis direct from
population data set?

Thanks for any help
Jens

-- 
Jens Schmidt                    TU Dresden -Inst. f. Hydrobiologie-
Tel. +49-(0)351-463-36284       Mommsenstrasse 13
Fax  +49-(0)351-463-37108       01062 Dresden
                              	R.F.A.



From tblackw at umich.edu  Sun Oct  5 21:02:11 2003
From: tblackw at umich.edu (Thomas W Blackwell)
Date: Sun, 5 Oct 2003 15:02:11 -0400 (EDT)
Subject: [R] Creating survival object
In-Reply-To: <Pine.A41.4.44.0310052006310.18646-100000@rcs7.urz.tu-dresden.de>
References: <Pine.A41.4.44.0310052006310.18646-100000@rcs7.urz.tu-dresden.de>
Message-ID: <Pine.SOL.4.58.0310051454550.8227@rygar.gpcc.itd.umich.edu>

Jens  -

After reading Help("Surv"), yes, I think you have interpreted
the two required arguments to Surv() correctly.  I don't know
of a ready-made function to do the transformation you illustrate.

If I had to program it myself, I would use  diff(), rep()  and
order().

-  tom blackwell  -  u michigan medical school  -  ann arbor  -

On Sun, 5 Oct 2003, Jens Schmidt wrote:

> I have survival data from a population:
>
> time    alive       treatment
> 0       20          A
> 3       12          A
> 5       10          A
> 8       7           A
> 10      7           A
> 15      5           A
> 20      5           A
>
> (time=0 is start of the experiment and initial number of individuals is
> alive=20; time=20 is end of experiment)
>
> If I don't misunderstood the survival documentation, always a survival
> object is required, i.e. created with function Surv(time, event)
>
> So I have to transform my data (assuming no censoring, event=0 is alive
> and event=1 is dead):
>
> time    event   treatment
> 20      0       A
> 20      0       A
> 20      0       A
> 20      0       A
> 20      0       A
> 15      1       A
> 15      1       A
> 8       1       A
> 8       1       A
> 8       1       A
> 5       1       A
> 5       1       A
> 3       1       A
> 3       1       A
> 3       1       A
> 3       1       A
> 3       1       A
> 3       1       A
> 3       1       A
> 3       1       A
>
> Is there a function or any other [R] code doing this?
>
> or
>
> Is it possible  to perform survival analysis direct from
> population data set?
>
> Thanks for any help
> Jens
> --
> Jens Schmidt                    TU Dresden -Inst. f. Hydrobiologie-
> Tel. +49-(0)351-463-36284       Mommsenstrasse 13
> Fax  +49-(0)351-463-37108       01062 Dresden
>                               	R.F.A.



From tlumley at u.washington.edu  Sun Oct  5 21:48:08 2003
From: tlumley at u.washington.edu (Thomas Lumley)
Date: Sun, 5 Oct 2003 12:48:08 -0700 (PDT)
Subject: [R] Point and click
In-Reply-To: <001701c38a9e$f163a0d0$52b2eb0c@C56909A>
References: <001701c38a9e$f163a0d0$52b2eb0c@C56909A>
Message-ID: <Pine.A41.4.58.0310051236450.98866@homer29.u.washington.edu>

On Sat, 4 Oct 2003, Cliff Lunneborg wrote:

> The following query raises the question: What is it that students learn
> from point and click dialogs?"

If the dialog constructs the same command line that you would type and
shows you what it is, they can learn a lot. SPSS and Stata do this (IMO
Stata does it better: SPSS tends to include unnecessary options, and
doesn't default to showing you the command line).

More importantly, the *point* of interface design is to prevent people
from having to learn things that they don't need to know.  The fact that a
comparison of two means is called a t-test and a comparison of three means
is called ANOVA would not ideally be on my list of the top fifty most
important ideas in statistics.  If the software and textbooks were better,
this fact would be of purely historical interest even to statisticians.


	-thomas

>
> Date: Fri, 03 Oct 2003 16:57:42 -0400
> From:
> To: s-news at lists.biostat.wustl.edu
> Subject: Splus question
> Message-ID: <BAY9-F52Q7OoK42LpqI000238f3 at hotmail.com>
>
> I don't know if this is the right list to post this question. If not,
> please let me know where I should post this. I have a dataframe with 3
> variables: ID, Y, Group Group is either 1, 2, 3. I am trying to run a
> t-test to compare the three groups on outcome Y. I know how to do this
> using the point and click dialogs. But can't get it to work on the
> command line. When I type: t.test(y, group)  it just compares y and
> group
> as though they represent the two samples. I tried doing something with
> tapply(y, group) but don't know how that works. If someone knows, please
> email me. Thanks.
>
> **********************************************************
> Cliff Lunneborg, Professor Emeritus, Statistics &
> Psychology, University of Washington, Seattle
> cliff at ms.washington.edu
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
>

Thomas Lumley			Assoc. Professor, Biostatistics
tlumley at u.washington.edu	University of Washington, Seattle



From ray at mcs.vuw.ac.nz  Sun Oct  5 12:03:56 2003
From: ray at mcs.vuw.ac.nz (Ray Brownrigg)
Date: Sun, 5 Oct 2003 23:03:56 +1300 (NZDT)
Subject: [R] [R-pkgs] R maps now for Windows
Message-ID: <200310051003.h95A3ulY018425@tahi.mcs.vuw.ac.nz>

The original S maps package, previously only available in R on Unix
systems is now available in R for Windows.

maps_2.0-2 and mapdata_2.0-2 (see below) in both source and Windows
binary form (for rw1071) have been uploaded to CRAN, hopefully soon to
appear.

The key aspect of this package which differentiates it from other
GIS related R packages under development is that it is standalone
within R, i.e. it does not rely on the presence of any other
software.

Features are:
1) Built-in databases for usa, state (states of the USA), county
(counties of the USA), world, world2 (Pacific-centric world) and nz.
2) Databases supplied in source form with accompanying software to
generate the binary forms, allowing users to edit the data or add new
databases (basic format is latitude/longitude pairs).
3) A database of world cities (generally >~100,000 population).
4) A companion package called mapdata provides high-resolution versions
of world, world2 and nz, as well as china (provinces) and rivers.
5) A companion package called mapproj provides a variety of map 
   projections (from Doug McIlroy).
6) While being backwards-compatible with the original 'S maps library',
it also has many new features, including:
   - displaying region names on a map,
   - finding the map region containing a given point (point-in-polygon),
   - computing areas of map regions, and
   - interpolating data aggregated over map regions (for statistical maps).

The maps and mapdata packages are maintained by Ray Brownrigg with
generous assistance from Tom Minka.

The mapproj package is available at 
   http://www.stat.cmu.edu/~minka/software/maps/

Ray Brownrigg
Tom Minka
October 2003

_______________________________________________
R-packages mailing list
R-packages at stat.math.ethz.ch
https://www.stat.math.ethz.ch/mailman/listinfo/r-packages



From rxg218 at psu.edu  Sun Oct  5 22:15:51 2003
From: rxg218 at psu.edu (Rajarshi Guha)
Date: Sun, 05 Oct 2003 20:15:51 -0000
Subject: [R] clicking on a point on a plot
Message-ID: <1065385217.23948.7.camel@ra.chem.psu.edu>

Hi,
  is this feature available in R?

I plot some data using plot(). Clicking on a point I'd like information
regarding that point. I realize that I could use locator() but this
gives me the X & Y values. Is it possible to somehow use locator to give
more information regarding the point chosen - say, which vector the
point was taken from.

It seems that R itself does not do this - is there any code/package
which would let me do something like this?

Thanks,

-------------------------------------------------------------------
Rajarshi Guha <rxg218 at psu.edu> <http://jijo.cjb.net>
GPG Fingerprint: 0CCA 8EE2 2EEB 25E2 AB04 06F7 1BB9 E634 9B87 56EE
-------------------------------------------------------------------
The emacs religion: to be saved, control excess...



From rajarshi at presidency.com  Sun Oct  5 22:41:51 2003
From: rajarshi at presidency.com (Rajarshi Guha)
Date: Sun, 05 Oct 2003 20:41:51 -0000
Subject: [R] clicking on a point on a plot
In-Reply-To: <1065385217.23948.7.camel@ra.chem.psu.edu>
References: <1065385217.23948.7.camel@ra.chem.psu.edu>
Message-ID: <1065386775.23948.17.camel@ra.chem.psu.edu>

On Sun, 2003-10-05 at 16:29, Roger D. Peng wrote:
> Did you try identify()?
> 
> -roger

Ahh. Thanks.

I have to say, never having used SPSS/SAS/Stata etc (and not being a
professional statistician), R is an amazing piece of software. My thanks
and appreciation go out to all those who've helped make this program as
well as the people on this list for the support!!

-------------------------------------------------------------------
Rajarshi Guha <rxg218 at psu.edu> <http://jijo.cjb.net>
GPG Fingerprint: 0CCA 8EE2 2EEB 25E2 AB04 06F7 1BB9 E634 9B87 56EE
-------------------------------------------------------------------
All great ideas are controversial, or have been at one time.



From ale.ambrosi at unipd.it  Sun Oct  5 22:53:37 2003
From: ale.ambrosi at unipd.it (alessandro ambrosi)
Date: Sun, 05 Oct 2003 22:53:37 +0200
Subject: [R] monotone spline
In-Reply-To: <002c01c38b3e$a4292920$0b01a8c0@hirotohome>
Message-ID: <5.1.0.14.2.20031005223214.0436bd30@pop.unipd.it>


Hi all!

I have a simple question and I hope someone can give me some hints.
I have to fit a constrained nonparametric model. More specifically
what to do is fiting a monotone spline.
Can someone suggest me a (hopely easy :-) ) way or some references
to do this with R.
I had a look at the R packages but I didn't find any function to do this.


Thank you in advance
Best,
Alessandro



From bournephysio at shaw.ca  Mon Oct  6 00:33:46 2003
From: bournephysio at shaw.ca (bournephysio@shaw.ca)
Date: Sun, 05 Oct 2003 15:33:46 -0700
Subject: [R] Placing legends
Message-ID: <46093445f4e8.45f4e8460934@shaw.ca>

Thanks for the help everyone.

par(usr) works really well for now.  I'm putting the legend above the plot.  I will check out the other methods if I have the time.

I am really impressed with this program and community.  Almost every question I have can be found with a quick search of the r-project site.  The question I emailed was answered quickly and by several people.

Thanks again,
Doug



From maechler at stat.math.ethz.ch  Mon Oct  6 09:01:20 2003
From: maechler at stat.math.ethz.ch (Martin Maechler)
Date: Mon, 6 Oct 2003 09:01:20 +0200
Subject: [R] monotone spline
In-Reply-To: <5.1.0.14.2.20031005223214.0436bd30@pop.unipd.it>
References: <002c01c38b3e$a4292920$0b01a8c0@hirotohome>
	<5.1.0.14.2.20031005223214.0436bd30@pop.unipd.it>
Message-ID: <16257.4928.896536.681026@gargle.gargle.HOWL>

>>>>> "alessandro" == alessandro ambrosi <ale.ambrosi at unipd.it>
>>>>>     on Sun, 05 Oct 2003 22:53:37 +0200 writes:

    alessandro> Hi all!

    alessandro> I have a simple question and I hope someone can
    alessandro> give me some hints.  I have to fit a constrained
    alessandro> nonparametric model. More specifically what to
    alessandro> do is fiting a monotone spline.  Can someone
    alessandro> suggest me a (hopely easy :-) ) way or some
    alessandro> references to do this with R.  I had a look at
    alessandro> the R packages but I didn't find any function to
    alessandro> do this.

You should learn to use  help.search()
    {yes, everyone, even if you prefer the HTML help pages!!}

In this case, even if you only have the default packages
(standard + Recommended), you'd get

  > help.search("monotone")
  Help files with alias or concept or title matching 'monotone' using
  fuzzy matching:

  mono.con(mgcv)          Monotonicity constraints for a cubic
			  regression spline.
  isoreg(modreg)          Isotonic / Monotone Regression
  backSpline(splines)     Monotone Inverse Spline

which is probably sufficient.

-- 
Martin Maechler <maechler at stat.math.ethz.ch>	http://stat.ethz.ch/~maechler/
Seminar fuer Statistik, ETH-Zentrum  LEO C16	Leonhardstr. 27
ETH (Federal Inst. Technology)	8092 Zurich	SWITZERLAND
phone: x-41-1-632-3408		fax: ...-1228			<><



From Alexander.Ploner at meb.ki.se  Mon Oct  6 09:24:28 2003
From: Alexander.Ploner at meb.ki.se (Alexander Ploner)
Date: Mon, 6 Oct 2003 09:24:28 +0200
Subject: [R] Placing legends
In-Reply-To: <3F7F7BC8.4020206@indigoindustrial.co.nz>
Message-ID: <005001c38bda$f00c1180$2266ed82@CPQD3114>

> > I'm trying to automate making a bunch of figures and I need a way to
> > automate legend position.  As I understand it the legend is 
> placed based 
> > on coordinates.  I don't know before hand what the 
> coordinates are going 
> > to be.  On one graph my y axis might go from -50 to -10.  
> On another it 
> > might go from 0 to 180.

I've been using the following utility - just use named arguments to
pass on to legend, i.e. "legend=", "pch=" etc.

"Legend" <- function (x="right", y="top", ..., offset=0.25)
{
# Name: Legend
# Desc: fixes a legend to one of the corners of a plot
# Auth: Alexander.Ploner at meb.ki.se

    x <- match.arg(x,c("left","center","right"))
    y <- match.arg(y,c("bottom","center","top"))
    cc <- par("usr")
    if (x=="left") {
        x  <- cc[1] + xinch(offset)
        xjust <- 0
    } else if (x=="center") {
        x  <- (cc[1] + cc[2])/2
        xjust <- 0.5
    } else if (x=="right") {
        x  <- cc[2] - xinch(offset)
        xjust <- 1
    }
    if (y=="bottom") {
        y  <- cc[3] + yinch(offset)
        yjust <- 0
    } else if (y=="center") {
        y  <- (cc[3] + cc[4])/2
        yjust <- 0.5
    } else if (y=="top") {
        y  <- cc[4] - yinch(offset)
        yjust <- 1
    }
    legend(x, y, xjust=xjust, yjust=yjust, ...)

}

HTH,

alexander

Alexander.Ploner at meb.ki.se
Phone: 46-8-524-82329
Fax  : 46-8-31 11 01
Medical Epidemiology & Biostatistics
Karolinska Institutet,
P.O. Box 281, SE-171 77 Stockholm



From e.pebesma at geog.uu.nl  Mon Oct  6 10:03:26 2003
From: e.pebesma at geog.uu.nl (Edzer J. Pebesma)
Date: Mon, 06 Oct 2003 10:03:26 +0200
Subject: [R] More questions about R extension programming
Message-ID: <3F8121CE.3060402@geog.uu.nl>

I have been strugling to get character data in and out of c functions
using the .Call() interface, but I don't succeed getting them back
into R.

The example:

#include "R.h"
#include "Rdefines.h"

char *cp = "xxxyyyy";
SEXP do_char(SEXP c) {
    int i;
    SEXP s;

    for (i = 0; i < LENGTH(c); i++)
        printf("%d:[%s]\n", i, CHARACTER_DATA(STRING_ELT(c,i)));
        /* CHARACTER_VALUE() does not work here! */
    s = NEW_CHARACTER(1);
    CHARACTER_POINTER(s) = cp;
    c = NEW_CHARACTER(3);
    SET_STRING_ELT(c,0, s);
    SET_STRING_ELT(c,1, s);
    SET_STRING_ELT(c,2, s);
    return c;
}

works only for the first half; but I can't manage to fill the string
and return it back.

Note that I had to use CHARACTER_DATA instead of CHARACTER,
which Thomas Lumley suggested last Saturday.
--
Edzer



From kimm at pet.auh.dk  Mon Oct  6 11:05:57 2003
From: kimm at pet.auh.dk (Kim Mouridsen)
Date: Mon, 6 Oct 2003 11:05:57 +0200
Subject: [R] Jonckheere-Terpstra test
In-Reply-To: <C80ECAFA2ACC1B45BE45D133ED660ADECDCBEE@crbsmxsusr04.pharma.aventis.com>
Message-ID: <000001c38be9$0e14f080$ce65030a@pckim>

The Jonckheere-Terpstra test is a distribution-free test for ordered
alternatives in a one-way layout. More specifically, assume

X_ij = m + t_j + e_ij,		i=1,...,n_j and j=1,...,k,

where the errors are idependent and identically distributed. Then you
can use the Jonckheere-Terpstra to test

H_0:    t_1=t_2=...=t_k
against
H_A:    t_1<=t_2<=...<=t_k,

where at least one of the inequalities is strict.

To my knowledge there is no R code for this test but the test statistic
is not too hard to calculate (you have to calculate some Mann-Whitney
counts) and the p-value can be found in a table - or in case you have
many observations you can use a large-sample approximation. 

The original article appeared in 
Biometrika, Vol. 41, No. 1/2. (Jun., 1954), pp. 133-145

But it is probably easier to read page 120-123 in
"Nonparametric statistical methods" by Hollander and Wolfe (1973) Wiley
& Sons.

A NOT-distribution-free alternative to this test is described in 
Biometrika, Vol. 72, No. 2. (Aug., 1985), pp. 476-480.

Kim.
 

-----Original Message-----
From: r-help-bounces at stat.math.ethz.ch
[mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of
Arne.Muller at aventis.com
Sent: 5. oktober 2003 14:51
To: r-help at stat.math.ethz.ch
Subject: [R] Jonckheere-Terpstra test

Hello,

can anybody here explain what a Jonckheere-Terpstra test is and whether
it is
implemented in R? I just know it's a non-parametric test, otherwise I've
no
clue about it ;-( . Are there alternatives to this test?

	thanks for help,

	Arne

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://www.stat.math.ethz.ch/mailman/listinfo/r-help



From christoph.lehmann at gmx.ch  Mon Oct  6 11:17:35 2003
From: christoph.lehmann at gmx.ch (Christoph Lehmann)
Date: Mon, 06 Oct 2003 11:17:35 +0200
Subject: [R] conversion of some lines of matlab code to R
Message-ID: <1065431855.1158.21.camel@christophl>

Dear R-users

I got some matlab code (just a few lines) for the deconvolution of a
signal:

function q=Dconvolve(funct)
w=squeeze(size(funct)); % gets the length
t=0:3:(w(1,1)-1)*3;
h=Dehemo(t,1.25,3);
r=fft(h);
rinv = 1./r;
q = real(ifft( fft(funct(:)).*rinv(:)));
%plot(t/0.75,q/max(q),'r-',t/0.75,funct/max(funct),'g-')

function h=DeHemo(t,tau,N)
h=(t/tau).^(N-1).*exp(-t/tau)./(tau*factorial(N-1));

since I don't know matlab: is there any one who could tell me, how these
lines would look like in R, means, how I could do this deconvolution in
R?

Many thanks

Cheers

Christoph
-- 
Christoph Lehmann <christoph.lehmann at gmx.ch>



From e.pebesma at geog.uu.nl  Mon Oct  6 10:19:00 2003
From: e.pebesma at geog.uu.nl (Edzer J. Pebesma)
Date: Mon, 06 Oct 2003 10:19:00 +0200
Subject: [R] auto.key = TRUE in xyplot() draws max. 7 groups
Message-ID: <3F812574.5080409@geog.uu.nl>

In the example below:

library(lattice)
n = 100
a = rnorm(n)
b = rnorm(n)
c = sample(c(1:7), n, repl=TRUE)
xyplot(a ~ b, groups = c, auto.key = TRUE)
c = sample(c(1:8), n, repl=TRUE)
xyplot(a ~ b, groups = c, auto.key = TRUE)

a key is drawn for the first, but not for the
second graph. I suppose it has to do with the
number of groups. If not a bug, maybe
this should go into the documentation?
--
Edzer



From elsawy at ysbl.york.ac.uk  Mon Oct  6 13:25:32 2003
From: elsawy at ysbl.york.ac.uk (Karim Elsawy)
Date: Mon, 06 Oct 2003 12:25:32 +0100
Subject: [R] polynomial fit
Message-ID: <3F81512C.DB20CEC4@ysbl.york.ac.uk>

Hi
I would like to fit a function H = F(x,y,z) but I do not know the
analytical expression
of H, is there a way to get the best 3d polynomial (in x,y,z) which fits
H
e.g
H<-seq(2,200,len=50)
x<-seq(1,50,len=50)
y<-seq(50,300,len=50)
x<-seq(30,80,len=50)

so it is  a sort of fitting a 4 dimensional surface to a regular grid

any suggestions are very much appreciated
best regards
Karim



From CMiller at PICR.man.ac.uk  Mon Oct  6 13:28:24 2003
From: CMiller at PICR.man.ac.uk (Crispin Miller)
Date: Mon, 6 Oct 2003 12:28:24 +0100
Subject: [R] Apply and its friends
Message-ID: <BAA35444B19AD940997ED02A6996AAE00B14B0@sanmail.picr.man.ac.uk>

Hi,
Forgive a very basic question...
I need to take two lists-of-lists, and apply a function to each pair of elements in the lists  to return a single list...
For example

l1 <- list(1:5,6:10,2:15)
l2 <- list(1:8,4:12,1:19,4:20)

I could easily do an lapply across each of them, but is there a function that does a sort-of pairwise-apply across both together?

Does anybody know of a good document that discusses this kind of basic matrix/list/vector manipulation in R?
Regards,
crispin
 
--------------------------------------------------------

 
This email is confidential and intended solely for the use o...{{dropped}}



From tblackw at umich.edu  Mon Oct  6 14:13:09 2003
From: tblackw at umich.edu (Thomas W Blackwell)
Date: Mon, 6 Oct 2003 08:13:09 -0400 (EDT)
Subject: [R] Apply and its friends
In-Reply-To: <BAA35444B19AD940997ED02A6996AAE00B14B0@sanmail.picr.man.ac.uk>
References: <BAA35444B19AD940997ED02A6996AAE00B14B0@sanmail.picr.man.ac.uk>
Message-ID: <Pine.SOL.4.58.0310060808240.16736@millipede.gpcc.itd.umich.edu>

Crispin  -

This is a familiar problem.  The only way I know of to do it is:

result <- lapply(seq(along=list.a), function(i,a,b) do.it(a[[i]], b[[i]]), list.a, list.b)

Here,  do.it()  is the function which operates on two elements.
I use this construction frequently.

-  tom blackwell  -  u michigan medical school  -  ann arbor  -

On Mon, 6 Oct 2003, Crispin Miller wrote:

> I need to take two lists-of-lists, and apply a function to each
> pair of elements in the lists  to return a single list...
> For example
>
> l1 <- list(1:5,6:10,2:15)
> l2 <- list(1:8,4:12,1:19,4:20)
>
> I could easily do an lapply across each of them, but is there a
> function that does a sort-of pairwise-apply across both together?
>
> Does anybody know of a good document that discusses this kind of
> basic matrix/list/vector manipulation in R?
> Regards,
> crispin



From Ted.Harding at nessie.mcc.ac.uk  Mon Oct  6 14:10:25 2003
From: Ted.Harding at nessie.mcc.ac.uk ( (Ted Harding))
Date: Mon, 06 Oct 2003 13:10:25 +0100 (BST)
Subject: [R] Jonckheere-Terpstra test
In-Reply-To: <000001c38be9$0e14f080$ce65030a@pckim>
Message-ID: <XFMail.031006131025.Ted.Harding@nessie.mcc.ac.uk>

On 06-Oct-03 Kim Mouridsen wrote:
> The Jonckheere-Terpstra test is a distribution-free test for ordered
> alternatives in a one-way layout. More specifically, assume
> 
> X_ij = m + t_j + e_ij,                i=1,...,n_j and j=1,...,k,
> 
> where the errors are idependent and identically distributed. Then you
> can use the Jonckheere-Terpstra to test
> 
> H_0:    t_1=t_2=...=t_k
> against
> H_A:    t_1<=t_2<=...<=t_k,
> 
> where at least one of the inequalities is strict.

To be more precise: the Jonkheere test applies where H_A is stochastically
increasing in j (P[X_j <= x] >= P[X_j+1 <= x], for all x, with inequality
for at least one j). When k=2, the Jonkheere statistic is the same as the
Mann-Whitney U. For k > 2, it is the sum for j1 = 1:(k-1) of the sum
for j2 = (j1+1):k of the Mann-Whitney Us for samples j1 and j2. To apply
it cleanly, there is an assumption of no ties (as if variables were
continuous).

> To my knowledge there is no R code for this test but the test statistic
> is not too hard to calculate (you have to calculate some Mann-Whitney
> counts) and the p-value can be found in a table - or in case you have
> many observations you can use a large-sample approximation. 

I once published an algorithm for the exact distribution in JRSS C
(Applied Statistics), 1984, pp. 1-6.

This was devised in the days of programmable calculators and 64K 4MHz
micros when RAM and computation time were major issues. However, the code
is easily implemented and may be as straightforward as any, even now.

Let W denote the Jonkheere statistic. To compute the distribution of W
over the range 0:M use the following algorithm (which applies as it
stands to the M-W U statistic with sample sizes m, n).
NOTE that indexing starts at 0.

U[M,m,n]:
[A]  f(0) = 1 ; f(1) = ... = f(M) = 0
[B]  If (n+1 > M) go to [C]
     P = min(m+n,M)
     for( t = (n+1):P )
       for( u = M:t ) ## Note reverse order: t = M,M-1,...,t
         f(u) = f(u) - f(u-t)

[C]  Q = min(m,M)
     for( s = 1:Q )
       for( u = s:M )
         f(u) = f(u) + f(u-s)

For Jongkheere with sample sizes n-1, n_2, ... , n_k, let

     N_i = n_(i+1) + ... + n_k ,  i = 1, 2, ... , k-1

Run the above first from [A] with m = n_1, n = N_1 and then repeatedly
from [B] with m = n_2, n = N_2; ... ; m = n_(k-1), n = N_(k-1) = n_k.

At the end, f(0), f(1), ... , f(M) will contain the frequencies (counts)
of the numbers of ways in which a value W = 0, 1, ... , M can arise
(purely combinatorial). On H_0, all redistributions of sample values
are equally likely, so to get the probability distribution divide by
the number of all possible reallocations (the combinatorial number of
choices of (n_1, n_2, ... , n_k) out of N = sum(n_i)).

Or you can compute the complete frequency distribution and divide each
term by the sum of all.

The above 'algorithmicises' the algebra of the generating function for
the counts.

Have fun!
Ted.


--------------------------------------------------------------------
E-Mail: (Ted Harding) <Ted.Harding at nessie.mcc.ac.uk>
Fax-to-email: +44 (0)870 167 1972
Date: 06-Oct-03                                       Time: 13:10:25
------------------------------ XFMail ------------------------------



From tblackw at umich.edu  Mon Oct  6 14:17:53 2003
From: tblackw at umich.edu (Thomas W Blackwell)
Date: Mon, 6 Oct 2003 08:17:53 -0400 (EDT)
Subject: [R] polynomial fit
In-Reply-To: <3F81512C.DB20CEC4@ysbl.york.ac.uk>
References: <3F81512C.DB20CEC4@ysbl.york.ac.uk>
Message-ID: <Pine.SOL.4.58.0310060816430.16736@millipede.gpcc.itd.umich.edu>

Karim  -

try function  gam()  in package  mgcv.

library("mgcv")
help("gam")

-  tom blackwell  -  u michigan medical school  -  ann arbor  -

On Mon, 6 Oct 2003, Karim Elsawy wrote:

> I would like to fit a function H = F(x,y,z) but I do not know the
> analytical expression
> of H, is there a way to get the best 3d polynomial (in x,y,z) which fits
> H
> e.g
> H<-seq(2,200,len=50)
> x<-seq(1,50,len=50)
> y<-seq(50,300,len=50)
> x<-seq(30,80,len=50)
>
> so it is  a sort of fitting a 4 dimensional surface to a regular grid
>
> any suggestions are very much appreciated
> best regards
> Karim



From CMiller at PICR.man.ac.uk  Mon Oct  6 14:17:56 2003
From: CMiller at PICR.man.ac.uk (Crispin Miller)
Date: Mon, 6 Oct 2003 13:17:56 +0100
Subject: [R] Apply and its friends
Message-ID: <BAA35444B19AD940997ED02A6996AAE0B5C1AC@sanmail.picr.man.ac.uk>

Smart! 

Thanks
Crispin


> -----Original Message-----
> From: Berwin Turlach [mailto:berwin at maths.uwa.edu.au]
> Sent: 06 October 2003 13:11
> To: Crispin Miller
> Subject: Re: [R] Apply and its friends
> 
> 
> >>>>> "CM" == Crispin Miller <CMiller at picr.man.ac.uk> writes:
> 
>     CM> Hi,
>     CM> Forgive a very basic question...
>     CM> I need to take two lists-of-lists, and apply a function to
>     CM> each pair of elements in the lists  to return a single
>     CM> list... 
>     CM> For example
> 
>     CM> l1 <- list(1:5,6:10,2:15)
>     CM> l2 <- list(1:8,4:12,1:19,4:20)
> 
>     CM> I could easily do an lapply across each of them, but is there
>     CM> a function that does a sort-of pairwise-apply across both
>     CM> together?
> Not sure how you can do this in your case since one list has 3
> components and the other 4. :)
> 
> But something like this should work:
> 
>   n <- min( length(l1), length(l2) ) 
>   res <- sapply(1:n, function(x) sum(l1[[x]]) - sum(l2[[x]]) )
> 
> The first line calculates the minimum length of the two list.  The
> second one calls sapply with a function.  The first argument is
> essentially a vector with the components that we would like to use.
> In this case I take the difference of the sum as an example.
> 
> If you don't like to make use of global variables, you can pass on the
> list on which you want to operate explicitly to the function that you
> call in sapply:
> 
>   res <- sapply(1:n, function(x, d1, d2) sum(d1[[x]]) - 
> sum(d2[[x]]), d1=l1, d2=l2)
> 
> Hope this helps.
> 
> Cheers,
> 
>         Berwin Turlach
> 
> ========================== Full address ============================
> Berwin A Turlach                      Tel.: +61 (8) 9380 3338 
> (secr)   
> School of Mathematics and Statistics        +61 (8) 9380 3383 
> (self)      
> The University of Western Australia   FAX : +61 (8) 9380 1028
> 35 Stirling Highway                   
> Crawley WA 6009                e-mail: berwin at maths.uwa.edu.au
> Australia                        http://www.maths.uwa.edu.au/~berwin
> 
>
 
--------------------------------------------------------

 
This email is confidential and intended solely for the use o...{{dropped}}



From bolker at zoo.ufl.edu  Mon Oct  6 14:34:46 2003
From: bolker at zoo.ufl.edu (Ben Bolker)
Date: Mon, 6 Oct 2003 08:34:46 -0400 (EDT)
Subject: [R] Apply and its friends
In-Reply-To: <Pine.SOL.4.58.0310060808240.16736@millipede.gpcc.itd.umich.edu>
Message-ID: <Pine.LNX.4.44.0310060834170.9003-100000@bolker.zoo.ufl.edu>

 
  As of a recent version (1.7.1?), see ?mapply

  Ben


On Mon, 6 Oct 2003, Thomas W Blackwell wrote:

> Crispin  -
> 
> This is a familiar problem.  The only way I know of to do it is:
> 
> result <- lapply(seq(along=list.a), function(i,a,b) do.it(a[[i]], b[[i]]), list.a, list.b)
> 
> Here,  do.it()  is the function which operates on two elements.
> I use this construction frequently.
> 
> -  tom blackwell  -  u michigan medical school  -  ann arbor  -
> 
> On Mon, 6 Oct 2003, Crispin Miller wrote:
> 
> > I need to take two lists-of-lists, and apply a function to each
> > pair of elements in the lists  to return a single list...
> > For example
> >
> > l1 <- list(1:5,6:10,2:15)
> > l2 <- list(1:8,4:12,1:19,4:20)
> >
> > I could easily do an lapply across each of them, but is there a
> > function that does a sort-of pairwise-apply across both together?
> >
> > Does anybody know of a good document that discusses this kind of
> > basic matrix/list/vector manipulation in R?
> > Regards,
> > crispin
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> 

-- 
620B Bartram Hall                            bolker at zoo.ufl.edu
Zoology Department, University of Florida    http://www.zoo.ufl.edu/bolker
Box 118525                                   (ph)  352-392-5697
Gainesville, FL 32611-8525                   (fax) 352-392-3704



From andy_liaw at merck.com  Mon Oct  6 15:08:22 2003
From: andy_liaw at merck.com (Liaw, Andy)
Date: Mon, 06 Oct 2003 09:08:22 -0400
Subject: [R] polynomial fit
Message-ID: <3A822319EB35174CA3714066D590DCD50205CC21@usrymx25.merck.com>

Or even mars() in the mda package...

Andy

> From: Thomas W Blackwell [mailto:tblackw at umich.edu] 
> 
> Karim  -
> 
> try function  gam()  in package  mgcv.
> 
> library("mgcv")
> help("gam")
> 
> -  tom blackwell  -  u michigan medical school  -  ann arbor  -
> 
> On Mon, 6 Oct 2003, Karim Elsawy wrote:
> 
> > I would like to fit a function H = F(x,y,z) but I do not know the 
> > analytical expression of H, is there a way to get the best 3d 
> > polynomial (in x,y,z) which fits H
> > e.g
> > H<-seq(2,200,len=50)
> > x<-seq(1,50,len=50)
> > y<-seq(50,300,len=50)
> > x<-seq(30,80,len=50)
> >
> > so it is  a sort of fitting a 4 dimensional surface to a 
> regular grid
> >
> > any suggestions are very much appreciated
> > best regards
> > Karim
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list 
> https://www.stat.math.ethz.ch/mailman/listinfo> /r-help
>



From tlumley at u.washington.edu  Mon Oct  6 15:30:17 2003
From: tlumley at u.washington.edu (Thomas Lumley)
Date: Mon, 6 Oct 2003 06:30:17 -0700 (PDT)
Subject: [R] Apply and its friends
In-Reply-To: <BAA35444B19AD940997ED02A6996AAE00B14B0@sanmail.picr.man.ac.uk>
References: <BAA35444B19AD940997ED02A6996AAE00B14B0@sanmail.picr.man.ac.uk>
Message-ID: <Pine.A41.4.58.0310060630010.116554@homer40.u.washington.edu>

On Mon, 6 Oct 2003, Crispin Miller wrote:

> Hi, Forgive a very basic question... I need to take two lists-of-lists,
> and apply a function to each pair of elements in the lists to return a
> single list... For example
>
> l1 <- list(1:5,6:10,2:15)
> l2 <- list(1:8,4:12,1:19,4:20)
>
> I could easily do an lapply across each of them, but is there a function
> that does a sort-of pairwise-apply across both together?

mapply

	-thomas



From yves_claveau at yahoo.ca  Mon Oct  6 15:37:53 2003
From: yves_claveau at yahoo.ca (=?iso-8859-1?q?Yves=20Claveau?=)
Date: Mon, 6 Oct 2003 09:37:53 -0400 (EDT)
Subject: [R] Off topic - Differences among stat packages in GLM results
In-Reply-To: <3A822319EB35174CA3714066D590DCD50205CC21@usrymx25.merck.com>
Message-ID: <20031006133753.72623.qmail@web9402.mail.yahoo.com>

Dear colleagues,
	I have performed the same analysis using the GLM
module of three statistical softwares: SYSTAT 10, JMP
4.0.2 and R 1.6.2 (see below for more details).
Although SYSTAT and R give roughly the same level of
significance for all variables, JMP yield a 20 percent
difference in probability for a categorical variable.
In fact, this difference is so important that I can
call this variable significant. Incidentally, Tukey's
test is in accordance with this result. Which
statistical software should I believe?

	Thank you in advance for your insight.

	Yves Claveau



DETAILS ON PERFORMED STATISTICAL ANALYSES

The categorical variable I am writing about is ESP

The model used is:

ptro=CONSTANT+classl+ht+esp+classl*ht+classl*esp+ht*esp+classl*ht*esp

Where:
- ptro is the dependent variable
- CONSTANT the constant in the model (defaut
procedure)
- classl a categorical variable with two classes
- ht a continuous variable
- esp a categorical variable with two classes


The results for each package are:

R 1.6.2

Call:
glm(formula = PTRO ~ ESP. * HT * CLASSL., family =
gaussian, 
    data = dataa)

Deviance Residuals: 
      Min         1Q     Median         3Q        Max 

-20.21973   -4.41060   -0.03971    4.77046   14.29097 


Coefficients:
                          Estimate Std. Error t value
Pr(>|t|)    
(Intercept)    35.54604    4.65265   7.640 3.41e-09
***
ESP            -13.12051   12.32455  -1.065    0.294  
 
HT             0.08005    0.04374   1.830    0.075 .  
CLASSL         1.09480    5.54809   0.197    0.845    
ESP:HT         0.01694    0.12375   0.137    0.892    
ESP:CLASSL     5.89693   15.41378   0.383    0.704    
HT:CLASSL      -0.01952    0.04682  -0.417    0.679   

ESP:HT:CLASSL  -0.05547    0.13217  -0.420    0.677   

---
Signif. codes:  0 `***' 0.001 `**' 0.01 `*' 0.05 `.'
0.1 ` ' 1 

(Dispersion parameter for gaussian family taken to be
59.17901)

    Null deviance: 4567.3  on 45  degrees of freedom
Residual deviance: 2248.8  on 38  degrees of freedom
AIC: 327.46

Number of Fisher Scoring iterations: 2


SYSTAT 10

Dep Var: PTRO   N: 49   Multiple R: 0.7241   Squared
multiple R: 0.5244
 
Analysis of Variance
Source    Sum-of-Squares  df  Mean-Square F-ratio  P
 
ESP             113.6878  1   113.6878  1.6551  
0.2055
CLASSL          20.6118   1   20.6118   0.3001  
0.5868
HT              239.7713  1   239.7713  3.4908  
0.0689
CLASSL*HT       26.3909   1   26.3909   0.3842  
0.5388
CLASSL*ESP      5.9755   1   5.9755   0.0870   0.7695
ESP*HT          2.6415   1   2.6415   0.0385   0.8455
CLASSL*ESP*HT   12.9459   1   12.9459   0.1885  
0.6665
 
Error                  2816.1893    41      68.6875


JMP 4

RSquare	0.52438
RSquare Adj	0.443177
Root Mean Square Error	8.287795
Mean of Response	42.78898
Observations (or Sum Wgts)	49

Analysis of Variance
Source	DF	Sum of Squares	Mean Square	F Ratio
Model	7	3104.9018	443.557	6.4576
Error	41	2816.1893	68.688	Prob > F
C. Total	48	5921.0910		<.0001

Effect Tests
Source   Nparm  DF  Sum of Squares  F Ratio   Prob > F
  
ESP             1   1   636.09249   9.2607   0.0041   
CLASSL          1   1   8.26185   0.1203   0.7305   
HT              1   1   239.77125   3.4908   0.0689   
HT*CLASSL       1   1   26.39087   0.3842   0.5388   
ESP*CLASSL      1   1   12.18491   0.1774   0.6758   
ESP*HT          1   1   2.64154   0.0385   0.8455   
ESP*HT*CLASSL   1   1   12.94593   0.1885   0.6665   




__________________________________________________________
L?che-vitrine ou l?che-?cran ?
magasinage.yahoo.ca



From tlumley at u.washington.edu  Mon Oct  6 16:02:32 2003
From: tlumley at u.washington.edu (Thomas Lumley)
Date: Mon, 6 Oct 2003 07:02:32 -0700 (PDT)
Subject: [R] Off topic - Differences among stat packages in GLM results
In-Reply-To: <20031006133753.72623.qmail@web9402.mail.yahoo.com>
References: <20031006133753.72623.qmail@web9402.mail.yahoo.com>
Message-ID: <Pine.A41.4.58.0310060656180.116554@homer40.u.washington.edu>

On Mon, 6 Oct 2003, [iso-8859-1] Yves Claveau wrote:

> Dear colleagues,
> 	I have performed the same analysis using the GLM
> module of three statistical softwares: SYSTAT 10, JMP
> 4.0.2 and R 1.6.2 (see below for more details).
> Although SYSTAT and R give roughly the same level of
> significance for all variables, JMP yield a 20 percent
> difference in probability for a categorical variable.
> In fact, this difference is so important that I can
> call this variable significant. Incidentally, Tukey's
> test is in accordance with this result. Which
> statistical software should I believe?


It looks at though you have asked for three different analyses from the
three packages. Certainly the analysis you asked R for is not the same as
the others.

If you run the anova() function on your model in R you should get one of
the other two analyses.  I think Systat gives the things SAS calls Type II
sums of squares, in which case JMP is presumably giving real sums of
squares and will agree with anova().

	=thomas


> 	Thank you in advance for your insight.
>
> 	Yves Claveau
>
>
>
> DETAILS ON PERFORMED STATISTICAL ANALYSES
>
> The categorical variable I am writing about is ESP
>
> The model used is:
>
> ptro=CONSTANT+classl+ht+esp+classl*ht+classl*esp+ht*esp+classl*ht*esp
>
> Where:
> - ptro is the dependent variable
> - CONSTANT the constant in the model (defaut
> procedure)
> - classl a categorical variable with two classes
> - ht a continuous variable
> - esp a categorical variable with two classes
>
>
> The results for each package are:
>
> R 1.6.2
>
> Call:
> glm(formula = PTRO ~ ESP. * HT * CLASSL., family =
> gaussian,
>     data = dataa)
>
> Deviance Residuals:
>       Min         1Q     Median         3Q        Max
>
> -20.21973   -4.41060   -0.03971    4.77046   14.29097
>
>
> Coefficients:
>                           Estimate Std. Error t value
> Pr(>|t|)
> (Intercept)    35.54604    4.65265   7.640 3.41e-09
> ***
> ESP            -13.12051   12.32455  -1.065    0.294
>
> HT             0.08005    0.04374   1.830    0.075 .
> CLASSL         1.09480    5.54809   0.197    0.845
> ESP:HT         0.01694    0.12375   0.137    0.892
> ESP:CLASSL     5.89693   15.41378   0.383    0.704
> HT:CLASSL      -0.01952    0.04682  -0.417    0.679
>
> ESP:HT:CLASSL  -0.05547    0.13217  -0.420    0.677
>
> ---
> Signif. codes:  0 `***' 0.001 `**' 0.01 `*' 0.05 `.'
> 0.1 ` ' 1
>
> (Dispersion parameter for gaussian family taken to be
> 59.17901)
>
>     Null deviance: 4567.3  on 45  degrees of freedom
> Residual deviance: 2248.8  on 38  degrees of freedom
> AIC: 327.46
>
> Number of Fisher Scoring iterations: 2
>
>
> SYSTAT 10
>
> Dep Var: PTRO   N: 49   Multiple R: 0.7241   Squared
> multiple R: 0.5244
>
> Analysis of Variance
> Source    Sum-of-Squares  df  Mean-Square F-ratio  P
>
> ESP             113.6878  1   113.6878  1.6551
> 0.2055
> CLASSL          20.6118   1   20.6118   0.3001
> 0.5868
> HT              239.7713  1   239.7713  3.4908
> 0.0689
> CLASSL*HT       26.3909   1   26.3909   0.3842
> 0.5388
> CLASSL*ESP      5.9755   1   5.9755   0.0870   0.7695
> ESP*HT          2.6415   1   2.6415   0.0385   0.8455
> CLASSL*ESP*HT   12.9459   1   12.9459   0.1885
> 0.6665
>
> Error                  2816.1893    41      68.6875
>
>
> JMP 4
>
> RSquare	0.52438
> RSquare Adj	0.443177
> Root Mean Square Error	8.287795
> Mean of Response	42.78898
> Observations (or Sum Wgts)	49
>
> Analysis of Variance
> Source	DF	Sum of Squares	Mean Square	F Ratio
> Model	7	3104.9018	443.557	6.4576
> Error	41	2816.1893	68.688	Prob > F
> C. Total	48	5921.0910		<.0001
>
> Effect Tests
> Source   Nparm  DF  Sum of Squares  F Ratio   Prob > F
>
> ESP             1   1   636.09249   9.2607   0.0041
> CLASSL          1   1   8.26185   0.1203   0.7305
> HT              1   1   239.77125   3.4908   0.0689
> HT*CLASSL       1   1   26.39087   0.3842   0.5388
> ESP*CLASSL      1   1   12.18491   0.1774   0.6758
> ESP*HT          1   1   2.64154   0.0385   0.8455
> ESP*HT*CLASSL   1   1   12.94593   0.1885   0.6665
>
>
>
>
> __________________________________________________________
> L?che-vitrine ou l?che-?cran ?
> magasinage.yahoo.ca
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
>

Thomas Lumley			Assoc. Professor, Biostatistics
tlumley at u.washington.edu	University of Washington, Seattle



From rdiaz at cnio.es  Mon Oct  6 16:31:07 2003
From: rdiaz at cnio.es (Ramon Diaz-Uriarte)
Date: Mon, 6 Oct 2003 16:31:07 +0200
Subject: [R] documentation typo in coxph?
Message-ID: <200310061631.07016.rdiaz@cnio.es>

Dear All,

I think there is a typo in the documentation for coxph (library survival).
The help says:


 eps: convergence threshold.  Iteration will continue until the
          relative change in the log-likelihood is less than eps. 
          Default is .0001. 

However, if I do "coxph.control()" I get:
> coxph.control()
$eps
[1] 1e-09

So the actual eps being used is not 10-4 but 10-9. 

Best,

Ram?n


> version
         _                
platform i386-pc-linux-gnu
arch     i386             
os       linux-gnu        
system   i386, linux-gnu  
status                    
major    1                
minor    7.1              
year     2003             
month    06               
day      16               
language R                
> 
-- 
Ram?n D?az-Uriarte
Bioinformatics Unit
Centro Nacional de Investigaciones Oncol?gicas (CNIO)
(Spanish National Cancer Center)
Melchor Fern?ndez Almagro, 3
28029 Madrid (Spain)
Fax: +-34-91-224-6972
Phone: +-34-91-224-6900

http://bioinfo.cnio.es/~rdiaz



From christoph.lehmann at gmx.ch  Mon Oct  6 16:37:24 2003
From: christoph.lehmann at gmx.ch (Christoph Lehmann)
Date: Mon, 06 Oct 2003 16:37:24 +0200
Subject: [R] convolution question
Message-ID: <1065451043.1158.48.camel@christophl>

Dear R-users
I have a question about convolution, using the convolve() command:

the following code gives a function, which will be convolved with a
train of delta functions. Can anybody tell me, why the convolved
function doesn't have the same length as the original train of delta
functions?

tau <- 1.25
N <- 3
t <- seq(0,15,by=3)
h <- (t/tau)^(N-1)*exp(-t/tau)/(tau*prod(N-1))
plot(h,type='o') 
stick <- rep(0,100)
ones <- round(runif(66)*99)+1
stick[ones] <-1
stick

X11() #open another graphics device
plot(stick)

convolution <- convolve(stick,h,conj=FALSE,type='filter')
X11() #open another graphics device
plot(convolution,type='o')




many thanks!
-- 
Christoph Lehmann <christoph.lehmann at gmx.ch>



From hdoran at nasdc.org  Mon Oct  6 16:50:47 2003
From: hdoran at nasdc.org (Harold Doran)
Date: Mon, 6 Oct 2003 10:50:47 -0400
Subject: [R] Selecting a random sample for lmList()
Message-ID: <66578BFC0BA55348B5907A0F798EE93013A0AE@ernesto.NASDC.ORG>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20031006/85e723b5/attachment.pl

From devitta at cs.tcd.ie  Mon Oct  6 16:59:07 2003
From: devitta at cs.tcd.ie (Ann Devitt)
Date: Mon, 6 Oct 2003 15:59:07 +0100 (IST)
Subject: [R] prcomp or princomp
Message-ID: <Pine.GSO.4.10.10310061500250.22440-100000@wilde.cs.tcd.ie>

Hello, 
is there any documentation on doing principal components analysis with R 
besides the R manual itself which is not very informative
Thank you
Ann



From jfox at mcmaster.ca  Mon Oct  6 17:21:09 2003
From: jfox at mcmaster.ca (John Fox)
Date: Mon, 06 Oct 2003 11:21:09 -0400
Subject: [R] Selecting a random sample for lmList()
In-Reply-To: <66578BFC0BA55348B5907A0F798EE93013A0AE@ernesto.NASDC.ORG>
Message-ID: <5.1.0.14.2.20031006111910.01ff7868@127.0.0.1>

Dear Harold,

There are examples of sampling and plotting hierarchical and longitudinal 
data in this manner in the appendix on mixed models to my R and S-PLUS 
Companion to Applied Regression; the appendix is available at 
<http://socserv.socsci.mcmaster.ca/jfox/Books/Companion/appendix-mixed-models.pdf>.

I hope that this helps,
  John

At 10:50 AM 10/6/2003 -0400, you wrote:
>Dear List:
>
>I have a data set with over 7000 students with about 4 observations over 
>time per student. I want to examine the within-group fits of a random 
>sample of this group as it takes forever to compute and draw all 7000 
>regressions.
>
>Here is the code I have used so far.
>
> >group<-groupedData(math~year|childid, data=scores)
> >group.list<-lmList(group)
> >plot(augPred(group.list))
>
>How might I select a random sample of say 100 students so that I can 
>visually examine their trajectories?
>
>Thank you
>

-----------------------------------------------------
John Fox
Department of Sociology
McMaster University
Hamilton, Ontario, Canada L8S 4M4
email: jfox at mcmaster.ca
phone: 905-525-9140x23604
web: www.socsci.mcmaster.ca/jfox



From tblackw at umich.edu  Mon Oct  6 17:40:28 2003
From: tblackw at umich.edu (Thomas W Blackwell)
Date: Mon, 6 Oct 2003 11:40:28 -0400 (EDT)
Subject: [R] prcomp or princomp
In-Reply-To: <Pine.GSO.4.10.10310061500250.22440-100000@wilde.cs.tcd.ie>
References: <Pine.GSO.4.10.10310061500250.22440-100000@wilde.cs.tcd.ie>
Message-ID: <Pine.SOL.4.58.0310061139210.15583@zektor.gpcc.itd.umich.edu>

Ann  -

Some useful references are given in the help for each function.
Please DO READ the help.

-  tom blackwell  -  u michigan medical school  -  ann arbor  -

On Mon, 6 Oct 2003, Ann Devitt wrote:

> Hello,
> is there any documentation on doing principal components analysis with R
> besides the R manual itself which is not very informative
> Thank you
> Ann



From thpe at hhbio.wasser.tu-dresden.de  Mon Oct  6 17:39:36 2003
From: thpe at hhbio.wasser.tu-dresden.de (Thomas Petzoldt)
Date: Mon, 06 Oct 2003 17:39:36 +0200
Subject: [R] prcomp or princomp
In-Reply-To: <Pine.GSO.4.10.10310061500250.22440-100000@wilde.cs.tcd.ie>
References: <Pine.GSO.4.10.10310061500250.22440-100000@wilde.cs.tcd.ie>
Message-ID: <3F818CB8.20806@hhbio.wasser.tu-dresden.de>

Ann Devitt schrieb:

> Hello, 
> is there any documentation on doing principal components analysis with R 
> besides the R manual itself which is not very informative

yes, see the reference cited on the princomp() - help page:

Venables, W. N. and B. D. Ripley (2002). Modern Applied Statistics with 
S, Springer-Verlag.

http://www.stats.ox.ac.uk/pub/MASS4/


Thomas



From borgulya at gyer2.sote.hu  Mon Oct  6 17:54:13 2003
From: borgulya at gyer2.sote.hu (Gabor Borgulya)
Date: Mon, 06 Oct 2003 15:54:13 -0000
Subject: [R] tick marks: 0, 12, 24, 36 ...
Message-ID: <1065455742.3653.37.camel@catv-d5de952e.bp04catv.broadband.hu>

Dear R-help list,

I have a problem with the tick marks of a Kaplan-Meier survival plot.
Here is a sample:

follow.up<-c(10,20,30,40,50,60,70,80,90,100) #months
dead<-c(1,1,1,0,1,1,0,0,0,0)
KM <-survfit(Surv(follow.up, dead))
plot(KM)

The result is a nice plot. However, our research group thinks it may be
a better idea to place the ticks to the years on the time scale, i.e. 0,
12, 24, 36 etc. months. Is this possible with R?

I tried to look it up and the most relevant manual page was ?axis,
option at - but I could not make this work together with the plot
command. 

plot(KM, axis=axis(1, at=c(0,12,24,36))) #this does not what I wanted

Any ideas?


Thank you,

Gabor



From ligges at statistik.uni-dortmund.de  Mon Oct  6 18:00:46 2003
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Mon, 06 Oct 2003 18:00:46 +0200
Subject: [R] prcomp or princomp
In-Reply-To: <Pine.GSO.4.10.10310061500250.22440-100000@wilde.cs.tcd.ie>
References: <Pine.GSO.4.10.10310061500250.22440-100000@wilde.cs.tcd.ie>
Message-ID: <3F8191AE.2070107@statistik.uni-dortmund.de>

Ann Devitt wrote:

> Hello, 
> is there any documentation on doing principal components analysis with R 
> besides the R manual itself which is not very informative
> Thank you
> Ann


Have a look into the References given in ?prcomp and ?princomp.

Uwe Ligges



From D.R.J.Pleydell at pgr.salford.ac.uk  Mon Oct  6 18:14:54 2003
From: D.R.J.Pleydell at pgr.salford.ac.uk (David Pleydell)
Date: Mon, 06 Oct 2003 16:14:54 -0000
Subject: [R] non-linear trends in kriging model
In-Reply-To: <200310011024.h91AA4ro018543@stat.math.ethz.ch>
Message-ID: <5.2.0.9.0.20031006171053.02381a00@mail.salford.ac.uk>

Thanks Paulo

Option 1 seems to work nicely, but I'm not sure how to specify the 
likelihood function so that I can calculate AIC for comparison with models 
fitted with likfit?

David

At 12:24 01/10/2003 +0200, you wrote:
>David
>
>Indeed, the trend term in geoR must be a linear one.
>You can:
>1) fit you model by alternating between using nls() and passing the
>residuals to the functions in geoR
>or
>2) try trying to fit using the functions in the nlme() package
>
>Regards
>P.J.
>
>On Tue, 30 Sep 2003, David Pleydell wrote:
>
> > Hi
> > I am struggling to fit a non-linear trend using the
> > likfit function in geoR.
> >
> > Specifically I want a sigmoidal function, something
> > like SSfpl in the nls package to fit the trend.  But
> > it seems trend.spatial in geoR only works with lm or
> > glm type models.
> >
> > Any ideas how I can specify the model to calculate the
> > kriging parameters using REML, including the
> > parameters of a sigmoidal trend function (such as
> > SSfpl)?
> >
> > Thanks
> > David
> >
> > ______________________________________________
> > R-help at stat.math.ethz.ch mailing list
> > https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> >
> >
>
>Paulo Justiniano Ribeiro Jr
>Departamento de Estat?stica
>Universidade Federal do Paran?
>Caixa Postal 19.081
>CEP 81.531-990
>Curitiba, PR  -  Brasil
>Tel: (+55) 41 361 3471
>Fax: (+55) 41 361 3141
>e-mail: pj at est.ufpr.br
>http://www.est.ufpr.br/~paulojus

************************************************
David Pleydell
D 31 Peel Building
Telford Institute of Environmental Systems
School of Environment and Life Sciences
University of Salford
M5 4WT
phone: +44 161 2952094
fax:  +44 161 295 5015



From mmiller3 at iupui.edu  Mon Oct  6 18:18:06 2003
From: mmiller3 at iupui.edu (Michael A. Miller)
Date: Mon, 06 Oct 2003 11:18:06 -0500
Subject: [R] Point and click
In-Reply-To: <3F7F82FE.8060604@indigoindustrial.co.nz> (Jason Turner's
	message of "Sun, 05 Oct 2003 15:33:34 +1300")
References: <001701c38a9e$f163a0d0$52b2eb0c@C56909A>
	<3F7F82FE.8060604@indigoindustrial.co.nz>
Message-ID: <87zngegx1d.fsf@lumen.indyrad.iupui.edu>

>>>>> "Jason" == Jason Turner <jasont at indigoindustrial.co.nz> writes:

    > Cliff Lunneborg wrote:
    >> The following query raises the question: What is it that
    >> students learn from point and click dialogs?"

[...]
    > I will state one thing - the point and click interface
    > *can* teach about the underlying software mechanisms, but
    > it has to be a well thought-out interface.
[...]
    > I did not understand directory structure on the PCs at all.
    > It never clicked.  I simply "parroted" the commands I was
    > taught to use, and managed to stay out of trouble.  "Polly
    > wanna .dwg file."  Then I worked on the Macs.  The display
    > of folders made it clear to me, in about five seconds.  I
    > realised right away what I'd been missing, and flew back
    > into the PC world with a bit more insight.

A counter example: there is a tech here who I've tried to teach
to use R.  He was hired in part because of his computer
expertise.  I told him to make a list of filenames (on a unix
system) with ls and pipe it into a file.  Then, with a text
editor, use that as the basis for a script that processes the
listed files.  He has "programming" experience on his resume, so
I figured this would be no trouble at all.  I was astonished to
find that he did not understand that, when editing the text file,
deleting a filename did not actually delete the file itself.
Everything he knew came from a gui world where "cutting" or
"deleting" meant an operation of the file, so when he saw the
filename, he didn't make a distinction between a text string and
the file itself.

That example helped to show me just how great the separation
between my approach and that of our "expert" windows/mac users
can be.  I came to the conclusion that there is no way for me to
bridge it alone.  If a user has the inclination and ability to
work on both sided, they'll do it and I'll help.  If they don't,
I can't drag them across and will only injure myself if I try.

To bring this back to be somewhat on topic for this list, my
point is that R has a well designed interface.  It may not be all
things to all users, but it is really good for the users that are
willing to learn it.  While there might be some more powerful
interfaces possible with a gui and a mouse (identify and xgobi
for example), there will always be users who aren't willing or
able to look beyond it or learn the details.  The same goes with
the syntax of t.test - some people are able and willing to learn
it and some aren't.

Mike

-- 
Michael A. Miller                               mmiller3 at iupui.edu
  Imaging Sciences, Department of Radiology, IU School of Medicine



From bates at stat.wisc.edu  Mon Oct  6 18:36:08 2003
From: bates at stat.wisc.edu (Douglas Bates)
Date: Mon, 06 Oct 2003 16:36:08 -0000
Subject: [R] Selecting a random sample for lmList()
In-Reply-To: <66578BFC0BA55348B5907A0F798EE93013A0AE@ernesto.NASDC.ORG>
References: <66578BFC0BA55348B5907A0F798EE93013A0AE@ernesto.NASDC.ORG>
Message-ID: <6r3ce6s4pb.fsf@bates4.stat.wisc.edu>

(Didn't I show this at the workshop you attended in June?)

You want to sample the levels of childid and subset the group
accordingly.

samp <- sample(levels(group$childid), 100)
subgroup <- subset(group, childid %in% samp)

You could, if you want, combine these into

subgroup <- subset(group, childid %in% sample(levels(group$childid), 100))

In this method each child has equal probability of being chosen.  If
you want the probability of being chosen to be proportional to the
number of observations on the child then use

subgroup <- subset(group, childid %in% sample(group$childid, 100))

There is a small chance that your sample will end up with fewer than
100 children but you could work around that by sampling more than 100
and taking the first 100 in the sample.

"Harold Doran" <hdoran at nasdc.org> writes:

> I have a data set with over 7000 students with about 4 observations
> over time per student. I want to examine the within-group fits of a
> random sample of this group as it takes forever to compute and draw
> all 7000 regressions.
>  
> Here is the code I have used so far. 
>  
> >group<-groupedData(math~year|childid, data=scores)
> >group.list<-lmList(group)
> >plot(augPred(group.list))
>  
> How might I select a random sample of say 100 students so that I can visually examine their trajectories?
>  
> Thank you
>  
> ------
> Harold C. Doran
> Director of Research and Evaluation
> New American Schools
> 675 N. Washington Street, Suite 220
> Alexandria, Virginia 22314
> 703.647.1628
>  <http://www.edperform.net/>  
>  
>  
>  
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help

-- 
Douglas Bates                            bates at stat.wisc.edu
Statistics Department                    608/262-2598
University of Wisconsin - Madison        http://www.stat.wisc.edu/~bates/



From ligges at statistik.uni-dortmund.de  Mon Oct  6 18:41:48 2003
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Mon, 06 Oct 2003 18:41:48 +0200
Subject: [R] tick marks: 0, 12, 24, 36 ...
In-Reply-To: <1065455742.3653.37.camel@catv-d5de952e.bp04catv.broadband.hu>
References: <1065455742.3653.37.camel@catv-d5de952e.bp04catv.broadband.hu>
Message-ID: <3F819B4C.7030300@statistik.uni-dortmund.de>

Gabor Borgulya wrote:

> Dear R-help list,
> 
> I have a problem with the tick marks of a Kaplan-Meier survival plot.
> Here is a sample:
> 
> follow.up<-c(10,20,30,40,50,60,70,80,90,100) #months
> dead<-c(1,1,1,0,1,1,0,0,0,0)
> KM <-survfit(Surv(follow.up, dead))
> plot(KM)
> 
> The result is a nice plot. However, our research group thinks it may be
> a better idea to place the ticks to the years on the time scale, i.e. 0,
> 12, 24, 36 etc. months. Is this possible with R?
> 
> I tried to look it up and the most relevant manual page was ?axis,
> option at - but I could not make this work together with the plot
> command. 
> 
> plot(KM, axis=axis(1, at=c(0,12,24,36))) #this does not what I wanted

Almost. See ?axis:

  plot(KM, xaxt="n")
  axis(1, at = seq(0, 100, 12))

Uwe Ligges


> Any ideas?

> 
> Thank you,
> 
> Gabor
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help



From MSchwartz at medanalytics.com  Mon Oct  6 18:41:36 2003
From: MSchwartz at medanalytics.com (Marc Schwartz)
Date: Mon, 06 Oct 2003 11:41:36 -0500
Subject: [R] tick marks: 0, 12, 24, 36 ...
In-Reply-To: <1065455742.3653.37.camel@catv-d5de952e.bp04catv.broadband.hu>
References: <1065455742.3653.37.camel@catv-d5de952e.bp04catv.broadband.hu>
Message-ID: <1065458496.26875.18.camel@localhost.localdomain>

On Mon, 2003-10-06 at 10:55, Gabor Borgulya wrote:
> Dear R-help list,
> 
> I have a problem with the tick marks of a Kaplan-Meier survival plot.
> Here is a sample:
> 
> follow.up<-c(10,20,30,40,50,60,70,80,90,100) #months
> dead<-c(1,1,1,0,1,1,0,0,0,0)
> KM <-survfit(Surv(follow.up, dead))
> plot(KM)
> 
> The result is a nice plot. However, our research group thinks it may be
> a better idea to place the ticks to the years on the time scale, i.e. 0,
> 12, 24, 36 etc. months. Is this possible with R?
> 
> I tried to look it up and the most relevant manual page was ?axis,
> option at - but I could not make this work together with the plot
> command. 
> 
> plot(KM, axis=axis(1, at=c(0,12,24,36))) #this does not what I wanted
> 
> Any ideas?
> 
> 
> Thank you,
> 
> Gabor


Try the following:

# Plot without the x axis being labeled
plot(KM, xaxt = "n")

# Now draw the x axis labels and tick marks
axis(1, at = c(0, 12, 24, 36, 48, 60, 72, 84, 96))

HTH,

Marc Schwartz



From spencer.graves at pdf.com  Mon Oct  6 19:01:48 2003
From: spencer.graves at pdf.com (Spencer Graves)
Date: Mon, 06 Oct 2003 10:01:48 -0700
Subject: [R] tick marks: 0, 12, 24, 36 ...
In-Reply-To: <1065455742.3653.37.camel@catv-d5de952e.bp04catv.broadband.hu>
References: <1065455742.3653.37.camel@catv-d5de952e.bp04catv.broadband.hu>
Message-ID: <3F819FFC.6030509@pdf.com>

Hopefully, someone who knows will answer.  In case that won't happen, 
I'll tell you what I would do that would likely produce the desired 
result fairly quickly -- but longer than it would take me to test it: 

1.  class(KM) = "survfit". 

2.  methods(plot) includes "plot.survfit". 

3.  ?plot.survfit reveals options I don't fully understand.  I'd work 
the examples and experiment to see if I could do what I wanted easily 
with the existing function. 

4.  If that didn't work, I'd list "plot.survfit", copy it to a file, 
find the "plot" command, and add "axes=FALSE" to the argument list.  I'd 
follow that with "axis(1, ...)" to add the x-axis I want and "axis(2)" 
to add the default y axis. 

      This general approach has worked in other contexts, so I believe 
something like this would likely work here. 

hope this helps.  spencer graves

Gabor Borgulya wrote:

>Dear R-help list,
>
>I have a problem with the tick marks of a Kaplan-Meier survival plot.
>Here is a sample:
>
>follow.up<-c(10,20,30,40,50,60,70,80,90,100) #months
>dead<-c(1,1,1,0,1,1,0,0,0,0)
>KM <-survfit(Surv(follow.up, dead))
>plot(KM)
>
>The result is a nice plot. However, our research group thinks it may be
>a better idea to place the ticks to the years on the time scale, i.e. 0,
>12, 24, 36 etc. months. Is this possible with R?
>
>I tried to look it up and the most relevant manual page was ?axis,
>option at - but I could not make this work together with the plot
>command. 
>
>plot(KM, axis=axis(1, at=c(0,12,24,36))) #this does not what I wanted
>
>Any ideas?
>
>
>Thank you,
>
>Gabor
>
>______________________________________________
>R-help at stat.math.ethz.ch mailing list
>https://www.stat.math.ethz.ch/mailman/listinfo/r-help
>  
>



From Weiming.Zhang at uchsc.edu  Mon Oct  6 19:20:06 2003
From: Weiming.Zhang at uchsc.edu (Weiming Zhang)
Date: Mon, 06 Oct 2003 17:20:06 -0000
Subject: [R] bioconductor installation problem
Message-ID: <1065460840.2492.8.camel@molecule.uchsc.edu>

Hi,

I am having some problems on installing bioconductor packages. I got 7
warnings and those 7 packages were not compiled correctly. I installed
Graphviz by using RPM, but not sure about HDF (I downloaded it and did
what doc says I should do but rhdf5 could not find it). I am running R
1.7.1 on RH linux 7.2. Could anybody give me some idea?

Thank you very much. 

wz

Packages Rgraphviz and rhdf5 require special libraries to be installed.
Please see the URL http://www.bioconductor.org/faq.html#Other Notes for
more details on installing these packages if they fail
to install properly

Warning messages: 
1: Installation of package Rgraphviz had non-zero exit status in:
installPkg(fileName, pkg, pkgVer, type, lib, repEntry, versForce) 
2: Installation of package RBGL had non-zero exit status in:
installPkg(fileName, pkg, pkgVer, type, lib, repEntry, versForce) 
3: Installation of package Ruuid had non-zero exit status in:
installPkg(fileName, pkg, pkgVer, type, lib, repEntry, versForce) 
4: Installation of package affy had non-zero exit status in:
installPkg(fileName, pkg, pkgVer, type, lib, repEntry, versForce) 
5: Installation of package genefilter had non-zero exit status in:
installPkg(fileName, pkg, pkgVer, type, lib, repEntry, versForce) 
6: Installation of package multtest had non-zero exit status in:
installPkg(fileName, pkg, pkgVer, type, lib, repEntry, versForce) 
7: Installation of package rhdf5 had non-zero exit status in:
installPkg(fileName, pkg, pkgVer, type, lib, repEntry, versForce)



From deepayan at stat.wisc.edu  Mon Oct  6 19:27:44 2003
From: deepayan at stat.wisc.edu (Deepayan Sarkar)
Date: Mon, 6 Oct 2003 12:27:44 -0500
Subject: [R] auto.key = TRUE in xyplot() draws max. 7 groups
In-Reply-To: <3F812574.5080409@geog.uu.nl>
References: <3F812574.5080409@geog.uu.nl>
Message-ID: <200310061227.44445.deepayan@stat.wisc.edu>

On Monday 06 October 2003 03:19, Edzer J. Pebesma wrote:
> In the example below:
>
> library(lattice)
> n = 100
> a = rnorm(n)
> b = rnorm(n)
> c = sample(c(1:7), n, repl=TRUE)
> xyplot(a ~ b, groups = c, auto.key = TRUE)
> c = sample(c(1:8), n, repl=TRUE)
> xyplot(a ~ b, groups = c, auto.key = TRUE)
>
> a key is drawn for the first, but not for the
> second graph. I suppose it has to do with the
> number of groups. If not a bug, maybe
> this should go into the documentation?

Definitely a bug (in Rows). If you are using R 1.7.1, redefining Rows as 
follows should fix the problem.

Rows <- function (x, which) 
{
    for (i in seq(along = x)) x[[i]] <- 
        rep(x[[i]], length = max(which, length(which)))[which]
    x
}

Deepayan



From tpapp at axelero.hu  Mon Oct  6 20:40:17 2003
From: tpapp at axelero.hu (Tamas Papp)
Date: Mon, 6 Oct 2003 20:40:17 +0200
Subject: [R] visualizing transition probability matrices
Message-ID: <20031006184017.GA2142@localhost>

Dear List,

I have a couple of (~200) 3x3 transition probability matrices (ie each
defines a Markov chain). They are all estimated from the same
underlying process, so it ie meaningful to take their elemetwise mean
and standard deviation. [1]

First question: supposing that they are given in a list l, how do I
get their elementwise mean and standard deviation? Fortunately, the
mean of trans. prob. matrices also remains a transition probability
matrix. I'm not sure if taking the standard deviation is meaningful,
though. Any suggestions?

Second (more important): what would be the best way of visualizing the
3x3 mean and standard deviation matrices? For the mean, I thought of a
barplot where the layers add up to 1, but I have no experience with
visualization of statistical data (at least, reading this list I
learned that I should avoid circles, pie-charts and their ilk).

Thanks,

Tamas

[1] You could ask why I don't just estimate the trans. prob. matrix
just once from the whole dataset. The problem is that the process is
meant to model (real) interest rates with only three levels, and
involves many peculiar assumptions. By the way, many thanks to Kjetil
Brinchmann Halvorsen, Rolf Turner, Ted Harding, Patrick Burns and
Martin Maechler for giving detailed and helpful replies to my earlier
question about Markov chains in R.

-- 
Tam?s K. Papp
E-mail: tpapp at axelero.hu (preferred, especially for large messages)
        tpapp at westel900.net
Please try to send only (latin-2) plain text, not HTML or other garbage.



From tblackw at umich.edu  Mon Oct  6 21:08:59 2003
From: tblackw at umich.edu (Thomas W Blackwell)
Date: Mon, 6 Oct 2003 15:08:59 -0400 (EDT)
Subject: [R] visualizing transition probability matrices
In-Reply-To: <20031006184017.GA2142@localhost>
References: <20031006184017.GA2142@localhost>
Message-ID: <Pine.SOL.4.58.0310061505270.9002@zektor.gpcc.itd.umich.edu>

On Mon, 6 Oct 2003, Tamas Papp wrote:

> I have a couple of (~200) 3x3 transition probability matrices (ie each
> defines a Markov chain). They are all estimated from the same
> underlying process, so it ie meaningful to take their elemetwise mean
> and standard deviation. [1]
>
> First question: supposing that they are given in a list l, how do I
> get their elementwise mean and standard deviation? Fortunately, the
> mean of trans. prob. matrices also remains a transition probability
> matrix. I'm not sure if taking the standard deviation is meaningful,
> though. Any suggestions?
>
> Second (more important): what would be the best way of visualizing the
> 3x3 mean and standard deviation matrices? For the mean, I thought of a
> barplot where the layers add up to 1, but I have no experience with
> visualization of statistical data (at least, reading this list I
> learned that I should avoid circles, pie-charts and their ilk).

Gosh, a 3x3 transition matrix has only two non-trivial eigenvectors,
one of which corresponds to the limiting distribution, so I would
simply plot 2 x 200 = 400 points in 3 dimensions, and look at the
matrices themselves, rather than at a mean and standard deviation.

-  tom blackwell  -  u michigan medical school  -  ann arbor  -

> Thanks,
> Tamas
>
> [1] You could ask why I don't just estimate the trans. prob. matrix
> just once from the whole dataset. The problem is that the process is
> meant to model (real) interest rates with only three levels, and
> involves many peculiar assumptions. By the way, many thanks to Kjetil
> Brinchmann Halvorsen, Rolf Turner, Ted Harding, Patrick Burns and
> Martin Maechler for giving detailed and helpful replies to my earlier
> question about Markov chains in R.
> --
> Tam?s K. Papp
> E-mail: tpapp at axelero.hu (preferred, especially for large messages)
>         tpapp at westel900.net
>



From ripley at stats.ox.ac.uk  Mon Oct  6 21:28:20 2003
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Mon, 6 Oct 2003 20:28:20 +0100 (BST)
Subject: [R] tick marks: 0, 12, 24, 36 ...
In-Reply-To: <3F819FFC.6030509@pdf.com>
Message-ID: <Pine.LNX.4.44.0310062026160.1563-100000@gannet.stats>

On Mon, 6 Oct 2003, Spencer Graves wrote:

> Hopefully, someone who knows will answer.  In case that won't happen, 
> I'll tell you what I would do that would likely produce the desired 
> result fairly quickly -- but longer than it would take me to test it: 
> 
> 1.  class(KM) = "survfit". 
> 
> 2.  methods(plot) includes "plot.survfit". 
> 
> 3.  ?plot.survfit reveals options I don't fully understand.  I'd work 
> the examples and experiment to see if I could do what I wanted easily 
> with the existing function. 
> 
> 4.  If that didn't work, I'd list "plot.survfit", copy it to a file, 
> find the "plot" command, and add "axes=FALSE" to the argument list.  I'd 
> follow that with "axis(1, ...)" to add the x-axis I want and "axis(2)" 
> to add the default y axis. 

axes=FALSE is an argument of plot.default, but not other methods.
The par arguments xaxt and yaxt are the general ways to supress the 
plotting of axes.

>       This general approach has worked in other contexts, so I believe 
> something like this would likely work here. 


-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From alanc at umit.maine.edu  Mon Oct  6 21:38:19 2003
From: alanc at umit.maine.edu (Alan Cobo-Lewis)
Date: Mon, 06 Oct 2003 15:38:19 -0400
Subject: [R] Re: Use of the Foreign package to import Stata files
Message-ID: <fc.004c4d191400d602004c4d191400d602.1400e749@umit.maine.edu>

Long ago (Sat, 2 Nov 2002), Bill Hart <w.hart at sbcglobal.net> wrote: 
> An R newbie here. I am using R 1.6 currently and have 
> (successfully, I think) installed the Foreign package. 
> Tried to import a data file created with Stata 7.0 
> SE. Had minor problems with syntax then R decided 
> that my file was not really a Stata file. It rejected 
> the file saying 'Not created by Stata 5-7/SE'. Any 
> suggestions on how to convince R that this is really, 
> truly a Stata file? 
> 
Thomas Lumley (tlumley at u.washington.edu) responded:
> You need the version of read.dta that I wrote yesterday after finally 
> obtaining some 7/SE datasets for debugging. If you can compile packages 
> then I can send you the code, otherwise you will have to wait for the next 
> version of the foreign package. 

But sometimes older works better than newer. The read.dta() function in R 1.7.0's foreign package (package 0.6-3) fails to recognize as a Stata file what seems to be a genuine Stata file, even though R 1.6.2's foreign package (package 0.5-9) imports
the same file just work fine. Using R 1.7 and rolling the package back to foreign 0.5 doesn't seem to fix the problem (haven't been able to roll R1.7 all the way back to foreign 0.5-9 yet, just to a newer 0.5 whose binary is still posted on cran).

An example of such a Stata file it at
http://www.ats.ucla.edu/stat/stata/examples/rwg/concord1.dta

Does anyone have a hint or fix?

alan

--
Alan B. Cobo-Lewis, Ph.D.		(207) 581-3840 tel
Department of Psychology		(207) 581-6128 fax
University of Maine
Orono, ME 04469-5742     		alanc at maine.edu

http://www.umaine.edu/visualperception



From jizhu at umich.edu  Mon Oct  6 22:27:18 2003
From: jizhu at umich.edu (Ji Zhu)
Date: Mon, 6 Oct 2003 16:27:18 -0400 (EDT)
Subject: [R] [Help] relative error in rpart() (classification)
Message-ID: <Pine.GSO.4.31.0310061622460.6231-100000@fisher.stat.lsa.umich.edu>


Dear all,

Could any one tell me how the output "relative error" (in CP table) is
defined in rpart() in the classification case?  Thank you.

Regards,

Ji

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
Ji Zhu                          439 West Hall
Assistant Professor             550 East University
Department of Statistics        Ann Arbor, MI 48109
University of Michigan          (734) 936-2577 (O)
jizhu at umich.edu                 (734) 763-4676 (F)
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~



From rxg218 at psu.edu  Mon Oct  6 23:03:05 2003
From: rxg218 at psu.edu (Rajarshi Guha)
Date: Mon, 06 Oct 2003 21:03:05 -0000
Subject: [R] vif() from Design and car
Message-ID: <1065474449.16378.5.camel@ra.chem.psu.edu>

Hi,
  I've been generating linear models with lm(). I wanted to look at the
VIF's for the coefficient. Using the vif() function from the package
Design, I would get unusually high VIF's.

However using vif() from the car package I get more reasonable values
(ie in line with the quality of the model).

What is the difference between the two vif functions? (I dont have
access to the reference in the Design's vif() help page)

Thanks,

-------------------------------------------------------------------
Rajarshi Guha <rxg218 at psu.edu> <http://jijo.cjb.net>
GPG Fingerprint: 0CCA 8EE2 2EEB 25E2 AB04 06F7 1BB9 E634 9B87 56EE
-------------------------------------------------------------------
Q: What do you get when you put a spinning flywheel in a casket and
turn a corner?
A: A funeral precession.



From rab at nauticom.net  Mon Oct  6 23:08:54 2003
From: rab at nauticom.net (Rick Bilonick)
Date: Mon, 06 Oct 2003 17:08:54 -0400
Subject: [R] Cannot Install rimage in R-1.7.1 (RH 9.0)
Message-ID: <3F81D9E6.8020405@nauticom.net>

The rimage install can't find the ffw header. Any idea why?

Rick B.

 > install.packages("rimage")
trying URL `http://cran.r-project.org/src/contrib/PACKAGES'
Content type `text/plain; charset=iso-8859-1' length 130159 bytes
opened URL
.......... .......... .......... .......... ..........
.......... .......... .......... .......... ..........
.......... .......... .......
downloaded 127Kb
 
trying URL `http://cran.r-project.org/src/contrib/rimage_0.5-1.tar.gz'
Content type `application/x-tar' length 80044 bytes
opened URL
.......... .......... .......... .......... ..........
.......... .......... ........
downloaded 78Kb
 
* Installing *source* package 'rimage' ...
checking for g++... g++
checking for C++ compiler default output... a.out
checking whether the C++ compiler works... yes
checking whether we are cross compiling... no
checking for suffix of executables...
checking for suffix of object files... o
checking whether we are using the GNU C++ compiler... yes
checking whether g++ accepts -g... yes
checking for gcc... gcc
checking whether we are using the GNU C compiler... yes
checking whether gcc accepts -g... yes
checking for gcc option to accept ANSI C... none needed
checking how to run the C preprocessor... gcc -E
checking for egrep... grep -E
checking for ANSI C header files... yes
checking for sys/types.h... yes
checking for sys/stat.h... yes
checking for stdlib.h... yes
checking for string.h... yes
checking for memory.h... yes
checking for strings.h... yes
checking for inttypes.h... yes
checking for stdint.h... yes
checking for unistd.h... yes
checking fftw.h usability... no
checking fftw.h presence... no
checking for fftw.h... no
configure: error: Sorry, can't find fftw header
ERROR: configuration failed for package 'rimage'
 
Delete downloaded files (y/N)?
The packages are in /tmp/Rtmp14443/Rinstdir327b23c6
Warning message:
Installation of package rimage had non-zero exit status in: 
install.packages("rimage")



From schau at umich.edu  Mon Oct  6 23:35:12 2003
From: schau at umich.edu (Stefanie Chau)
Date: Mon, 6 Oct 2003 17:35:12 -0400
Subject: [R] installation of R
Message-ID: <000a01c38c51$bcddd4c0$6401a8c0@sanarb01.mi.comcast.net>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20031006/24345411/attachment.pl

From bates at stat.wisc.edu  Mon Oct  6 23:38:48 2003
From: bates at stat.wisc.edu (Douglas Bates)
Date: Mon, 06 Oct 2003 21:38:48 -0000
Subject: [R] Cannot Install rimage in R-1.7.1 (RH 9.0)
In-Reply-To: <3F81D9E6.8020405@nauticom.net>
References: <3F81D9E6.8020405@nauticom.net>
Message-ID: <6risn2nizk.fsf@bates4.stat.wisc.edu>

Rick Bilonick <rab at nauticom.net> writes:

> The rimage install can't find the ffw header. Any idea why?

Because you haven't installed the fftw package for your Linux
distribution first?

Sources, .rpm and .deb files are at http://www.fftw.org/

> 
> Rick B.
> 
>  > install.packages("rimage")
> trying URL `http://cran.r-project.org/src/contrib/PACKAGES'
> Content type `text/plain; charset=iso-8859-1' length 130159 bytes
> opened URL
> .......... .......... .......... .......... ..........
> .......... .......... .......... .......... ..........
> .......... .......... .......
> downloaded 127Kb
>  trying URL `http://cran.r-project.org/src/contrib/rimage_0.5-1.tar.gz'
> 
> Content type `application/x-tar' length 80044 bytes
> opened URL
> .......... .......... .......... .......... ..........
> .......... .......... ........
> downloaded 78Kb
>  * Installing *source* package 'rimage' ...
> 
> checking for g++... g++
> checking for C++ compiler default output... a.out
> checking whether the C++ compiler works... yes
> checking whether we are cross compiling... no
> checking for suffix of executables...
> checking for suffix of object files... o
> checking whether we are using the GNU C++ compiler... yes
> checking whether g++ accepts -g... yes
> checking for gcc... gcc
> checking whether we are using the GNU C compiler... yes
> checking whether gcc accepts -g... yes
> checking for gcc option to accept ANSI C... none needed
> checking how to run the C preprocessor... gcc -E
> checking for egrep... grep -E
> checking for ANSI C header files... yes
> checking for sys/types.h... yes
> checking for sys/stat.h... yes
> checking for stdlib.h... yes
> checking for string.h... yes
> checking for memory.h... yes
> checking for strings.h... yes
> checking for inttypes.h... yes
> checking for stdint.h... yes
> checking for unistd.h... yes
> checking fftw.h usability... no
> checking fftw.h presence... no
> checking for fftw.h... no
> configure: error: Sorry, can't find fftw header
> ERROR: configuration failed for package 'rimage'
>  Delete downloaded files (y/N)?
> 
> The packages are in /tmp/Rtmp14443/Rinstdir327b23c6
> Warning message:
> Installation of package rimage had non-zero exit status in:
> install.packages("rimage")



From tblackw at umich.edu  Mon Oct  6 23:44:15 2003
From: tblackw at umich.edu (Thomas W Blackwell)
Date: Mon, 6 Oct 2003 17:44:15 -0400 (EDT)
Subject: [R] installation of R
In-Reply-To: <000a01c38c51$bcddd4c0$6401a8c0@sanarb01.mi.comcast.net>
References: <000a01c38c51$bcddd4c0$6401a8c0@sanarb01.mi.comcast.net>
Message-ID: <Pine.SOL.4.58.0310061742550.27657@zektor.gpcc.itd.umich.edu>

This is a question you should be able to answer from
the R web site,  www.r-project.org.

-  tom blackwell  -  u michigan medical school  -  ann arbor  -

On Mon, 6 Oct 2003, Stefanie Chau wrote:

> I wish to install R on my computer but I do not
> know how to do this. I have a Windows ME.
> Please advise on how to install R.
> Thank you.
>
> Stefanie
>



From tlumley at u.washington.edu  Mon Oct  6 23:58:35 2003
From: tlumley at u.washington.edu (Thomas Lumley)
Date: Mon, 6 Oct 2003 14:58:35 -0700 (PDT)
Subject: [R] Re: Use of the Foreign package to import Stata files
In-Reply-To: <fc.004c4d191400d602004c4d191400d602.1400e749@umit.maine.edu>
References: <fc.004c4d191400d602004c4d191400d602.1400e749@umit.maine.edu>
Message-ID: <Pine.A41.4.58.0310061439450.153520@homer12.u.washington.edu>

On Mon, 6 Oct 2003, Alan Cobo-Lewis wrote:

>
> But sometimes older works better than newer. The read.dta() function in
> R 1.7.0's foreign package (package 0.6-3) fails to recognize as a Stata
> file what seems to be a genuine Stata file, even though R 1.6.2's
> foreign package (package 0.5-9) imports the same file just work fine.
> Using R 1.7 and rolling the package back to foreign 0.5 doesn't seem to
> fix the problem (haven't been able to roll R1.7 all the way back to
> foreign 0.5-9 yet, just to a newer 0.5 whose binary is still posted on
> cran).
>
> An example of such a Stata file it at
> http://www.ats.ucla.edu/stat/stata/examples/rwg/concord1.dta
>

I don't know why it used to work.  The file begins with 'h' (0x68), which
isn't in my list of valid Stata file types.  On the other hand, Stata has
no problem with it.  It's possible that it is a version 4 file (which
would make sense, since version 5 file start with 'i' and increase from
there) but that wouldn't explain why it used to work.

With the stataread package (which predates 0.5-x of foreign) I get "Not a
Stata version 5-7 data file".

	-thomas



From kwan022 at stat.auckland.ac.nz  Mon Oct  6 23:57:16 2003
From: kwan022 at stat.auckland.ac.nz (Ko-Kang Kevin Wang)
Date: Tue, 7 Oct 2003 10:57:16 +1300 (NZDT)
Subject: [R] installation of R
In-Reply-To: <000a01c38c51$bcddd4c0$6401a8c0@sanarb01.mi.comcast.net>
Message-ID: <Pine.LNX.4.44.0310071056470.8023-100000@stat54.stat.auckland.ac.nz>

Go to http://www.r-project.org/ then Documentation -> FAQs -> R For 
Windows FAQ.

On Mon, 6 Oct 2003, Stefanie Chau wrote:

> Date: Mon, 6 Oct 2003 17:35:12 -0400
> From: Stefanie Chau <schau at umich.edu>
> To: R-help at stat.math.ethz.ch
> Subject: [R] installation of R
> 
> Hi, 
> 
> I wish to install R on my computer but I do not know how to do this. I have a Windows ME.
> Please advise on how to install R.
> Thank you.
> 
> Stefanie
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> 

-- 
Cheers,

Kevin

------------------------------------------------------------------------------
"On two occasions, I have been asked [by members of Parliament],
'Pray, Mr. Babbage, if you put into the machine wrong figures, will
the right answers come out?' I am not able to rightly apprehend the
kind of confusion of ideas that could provoke such a question."

-- Charles Babbage (1791-1871) 
---- From Computer Stupidities: http://rinkworks.com/stupid/

--
Ko-Kang Kevin Wang
Master of Science (MSc) Student
SLC Tutor and Lab Demonstrator
Department of Statistics
University of Auckland
New Zealand
Homepage: http://www.stat.auckland.ac.nz/~kwan022
Ph: 373-7599
    x88475 (City)
    x88480 (Tamaki)



From tlumley at u.washington.edu  Tue Oct  7 00:14:21 2003
From: tlumley at u.washington.edu (Thomas Lumley)
Date: Mon, 6 Oct 2003 15:14:21 -0700 (PDT)
Subject: [R] Re: Use of the Foreign package to import Stata files
In-Reply-To: <Pine.A41.4.58.0310061439450.153520@homer12.u.washington.edu>
References: <fc.004c4d191400d602004c4d191400d602.1400e749@umit.maine.edu>
	<Pine.A41.4.58.0310061439450.153520@homer12.u.washington.edu>
Message-ID: <Pine.A41.4.58.0310061512440.153520@homer12.u.washington.edu>

On Mon, 6 Oct 2003, Thomas Lumley wrote:

> On Mon, 6 Oct 2003, Alan Cobo-Lewis wrote:
>
> >
> > But sometimes older works better than newer. The read.dta() function in
> > R 1.7.0's foreign package (package 0.6-3) fails to recognize as a Stata
> > file what seems to be a genuine Stata file, even though R 1.6.2's
> > foreign package (package 0.5-9) imports the same file just work fine.

Are the data correct with 1.6.2? It's looking increasingly strange that
foreign ever read these data.  The file doesn't seem to have the timestamp
field that versions 5-8 include, which should mess up the reading of the
rest of the file.

	-thomas



From fzagmutt at hotmail.com  Tue Oct  7 00:16:33 2003
From: fzagmutt at hotmail.com (Francisco J. Zagmutt Vergara)
Date: Mon, 06 Oct 2003 22:16:33 +0000
Subject: [R] Log transformed values and contrasts in LME
Message-ID: <Law15-F101sLwFFVLyl0000e97b@hotmail.com>

Dear All

This is probably a very basic question for this list but I just wanted to 
make sure that I am doing things right:

I have an LME model with 4 categorical variables and 2 continuous variables 
(analysis of covariance model).  I had to use a log transformation on the 
data to achieve normality (log(x)-.1) and then I used contrast treatment to 
compare differences between a baseline level and the other level of the same 
categorical variable (as far as I understand R picks automatically the level 
with the smaller marginal mean to make these comparisons).  When I want to 
interpret my coefficients in terms of the original (non-transformed data) is 
not as simple as using:

>exp(beta)-.1

since (I believe) the hypotheses testing with the contrasts is 
log(Beta1)-Log(beta2)=0

So I though that a way to go around this is to remove the intercept from the 
original model to get a "cell-means" model which would  basically give me 
the average log transformed value of the outcome variable for each category:

log(beta1) +log(beta2)+...log(betan)

and then transform those values to the original data form and subtract the 
means to obtain an estimate of the difference between the means that I 
tested with the contrast:

>b1<-exp(beta) -.1
>b2<-exp(beta)-.1
>b1-b2

Is this conceptually right? I would not be making any hypothesis testing 
with the new model, just getting an estimate of the actual difference 
between the level means so I think that his could be a valid approach.  I 
would still report the test statistics and significance values for the 
contrasts from the original model but just would include the estimation of 
the means from the second model.  Am I in the right path?


Many thanks for your help!!

Francisco

_________________________________________________________________
?Est?s buscando un auto nuevo?  http://www.yupimsn.com/autos/



From markhall at gol.com  Tue Oct  7 00:24:08 2003
From: markhall at gol.com (Mark Hall)
Date: Tue, 07 Oct 2003 07:24:08 +0900
Subject: Fw: Re: [R] Point and click
Message-ID: <20031007071358.45D6.MARKHALL@gol.com>



>To bring this back to be somewhat on topic for this list, my
>point is that R has a well designed interface.  It may not be all
>things to all users, but it is really good for the users that are
>willing to learn it.  While there might be some more powerful
>interfaces possible with a gui and a mouse (identify and xgobi
>for example), there will always be users who aren't willing or
>able to look beyond it or learn the details.  The same goes with
>the syntax of t.test - some people are able and willing to learn
>it and some aren't.

>Mike


The other thing that I like about R in its present and past versions is that it has forced
me to deal much more with the statistics side of analyses than when I was an SPSS
user.  If nothing else, it has made me realize there are a variety of ways to do
principal component analysis! 

Best, MEH

-- 
 Mark Hall
Niigata Prefectural Museum of History

<>



From jarsuaga at cc.ucsf.edu  Tue Oct  7 00:36:44 2003
From: jarsuaga at cc.ucsf.edu (Javier Arsuaga)
Date: Mon, 06 Oct 2003 22:36:44 -0000
Subject: [R] Installing R in Linux 8.0
Message-ID: <1065480208.27045.6.camel@udp019098uds.ucsf.edu>

I am trying to install R in Linux 8.0 and I downloaded
"R-1.7.1-1.i386.rpm" and did rpm -hiv R-1.7.1-1.i386.rpm 
and I am getting the following message:

warning: R-1.7.1-1.i386.rpm: V3 DSA signature: NOKEY, key ID
97d3544e
error: Failed dependencies:
        libblas.so.3 is needed by R-1.7.1-1

        Then I went and look for libblas (which is a linear algebra
library) that I found in

        http://www.hklpg.org/RPM/libblas.so.3.html

        and downloaded
		blas-3.0-9.1mlx.i386.rpm

        Tried to do rpm -hiv blas-3.0-9.1mlx.i386.rpm 

	and got

        error: cannot get exclusive lock on /var/lib/rpm/Packages
error: cannot open Packages index using db3 - Operation not permitted
(1)
error: cannot open Packages database in /var/lib/rpm

        Any suggestions, idea, help...?????

        Thanks a lot



From jfox at mcmaster.ca  Tue Oct  7 00:42:04 2003
From: jfox at mcmaster.ca (John Fox)
Date: Mon, 06 Oct 2003 18:42:04 -0400
Subject: [R] vif() from Design and car
In-Reply-To: <1065474449.16378.5.camel@ra.chem.psu.edu>
Message-ID: <5.1.0.14.2.20031006183806.02010ab0@127.0.0.1>

Dear Rajarshi,

I don't have the book to which Frank Harrell refers in the documentation 
for the vif function in the Design package, but looking at the code for the 
function, I believe that there's an error:

vif <-
function (fit)
{
     v <- Varcov(fit, regcoef.only = TRUE)
     nam <- dimnames(v)[[1]]
     ns <- num.intercepts(fit)
#    v <- solve(v)
     if (ns > 0) {
         v <- v[-(1:ns), -(1:ns), drop = FALSE]
         nam <- nam[-(1:ns)]
     }
     d <- diag(v)^0.5
     v <- diag(solve(v/(d %o% d)))
     names(v) <- nam
     v
}

If you remove the line that I've commented-out, the function will, I 
believe, return the usual results.

I imagine that Frank can comment on this more definitively.

I hope that this helps,
  John

At 05:07 PM 10/6/2003 -0400, Rajarshi Guha wrote:
>Hi,
>   I've been generating linear models with lm(). I wanted to look at the
>VIF's for the coefficient. Using the vif() function from the package
>Design, I would get unusually high VIF's.
>
>However using vif() from the car package I get more reasonable values
>(ie in line with the quality of the model).
>
>What is the difference between the two vif functions? (I dont have
>access to the reference in the Design's vif() help page)
>
>Thanks,
>
>-------------------------------------------------------------------
>Rajarshi Guha <rxg218 at psu.edu> <http://jijo.cjb.net>

-----------------------------------------------------
John Fox
Department of Sociology
McMaster University
Hamilton, Ontario, Canada L8S 4M4
email: jfox at mcmaster.ca
phone: 905-525-9140x23604
web: www.socsci.mcmaster.ca/jfox



From p.dalgaard at biostat.ku.dk  Tue Oct  7 00:58:07 2003
From: p.dalgaard at biostat.ku.dk (Peter Dalgaard BSA)
Date: Mon, 06 Oct 2003 22:58:07 -0000
Subject: [R] Installing R in Linux 8.0
In-Reply-To: <1065480208.27045.6.camel@udp019098uds.ucsf.edu>
References: <1065480208.27045.6.camel@udp019098uds.ucsf.edu>
Message-ID: <x2ad8egefh.fsf@biostat.ku.dk>

Javier Arsuaga <jarsuaga at cc.ucsf.edu> writes:

> I am trying to install R in Linux 8.0 and I downloaded
> "R-1.7.1-1.i386.rpm" and did rpm -hiv R-1.7.1-1.i386.rpm 
> and I am getting the following message:
> 
> warning: R-1.7.1-1.i386.rpm: V3 DSA signature: NOKEY, key ID
> 97d3544e
> error: Failed dependencies:
>         libblas.so.3 is needed by R-1.7.1-1
> 
>         Then I went and look for libblas (which is a linear algebra
> library) that I found in
> 
>         http://www.hklpg.org/RPM/libblas.so.3.html
> 
>         and downloaded
> 		blas-3.0-9.1mlx.i386.rpm
> 
>         Tried to do rpm -hiv blas-3.0-9.1mlx.i386.rpm 
> 
> 	and got
> 
>         error: cannot get exclusive lock on /var/lib/rpm/Packages
> error: cannot open Packages index using db3 - Operation not permitted
> (1)
> error: cannot open Packages database in /var/lib/rpm
> 
>         Any suggestions, idea, help...?????

Wouldn't know about the locking issue -- usually means that you have
two processes trying to do RPM things. However, the "mlx" in the RPM
you found suggests that it may be for a non-RedHat distribution
(muLinux??). There's supposed to be a blas in RedHat itself:

[pd at turmalin R]$ rpm -qf /usr/lib/libblas.so.3
blas-3.0-18

so I suggest you get that and install instead.  I believe Martyn
recently compiled a list of RedHat packages that were necessary to
install the RPMs and placed them somewhere easily findable.

-- 
   O__  ---- Peter Dalgaard             Blegdamsvej 3  
  c/ /'_ --- Dept. of Biostatistics     2200 Cph. N   
 (*) \(*) -- University of Copenhagen   Denmark      Ph: (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)             FAX: (+45) 35327907



From cwills at ucsd.edu  Tue Oct  7 01:06:52 2003
From: cwills at ucsd.edu (Christopher Wills)
Date: Mon, 6 Oct 2003 16:06:52 -0700
Subject: [R] randomizing within factors and combining unequal-sized arrays
Message-ID: <p05111a03bba79c346876@[137.110.21.179]>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20031006/cd1eb8f7/attachment.pl

From MSchwartz at medanalytics.com  Tue Oct  7 01:34:38 2003
From: MSchwartz at medanalytics.com (Marc Schwartz)
Date: Mon, 06 Oct 2003 18:34:38 -0500
Subject: [R] Installing R in Linux 8.0
In-Reply-To: <x2ad8egefh.fsf@biostat.ku.dk>
References: <1065480208.27045.6.camel@udp019098uds.ucsf.edu>
	<x2ad8egefh.fsf@biostat.ku.dk>
Message-ID: <1065483277.4752.12.camel@localhost.localdomain>

On Mon, 2003-10-06 at 18:00, Peter Dalgaard BSA wrote:
> Javier Arsuaga <jarsuaga at cc.ucsf.edu> writes:
> 
> > I am trying to install R in Linux 8.0 and I downloaded
> > "R-1.7.1-1.i386.rpm" and did rpm -hiv R-1.7.1-1.i386.rpm 
> > and I am getting the following message:
> > 
> > warning: R-1.7.1-1.i386.rpm: V3 DSA signature: NOKEY, key ID
> > 97d3544e
> > error: Failed dependencies:
> >         libblas.so.3 is needed by R-1.7.1-1
> > 
> >         Then I went and look for libblas (which is a linear algebra
> > library) that I found in
> > 
> >         http://www.hklpg.org/RPM/libblas.so.3.html
> > 
> >         and downloaded
> > 		blas-3.0-9.1mlx.i386.rpm
> > 
> >         Tried to do rpm -hiv blas-3.0-9.1mlx.i386.rpm 
> > 
> > 	and got
> > 
> >         error: cannot get exclusive lock on /var/lib/rpm/Packages
> > error: cannot open Packages index using db3 - Operation not permitted
> > (1)
> > error: cannot open Packages database in /var/lib/rpm
> > 
> >         Any suggestions, idea, help...?????
> 
> Wouldn't know about the locking issue -- usually means that you have
> two processes trying to do RPM things. However, the "mlx" in the RPM
> you found suggests that it may be for a non-RedHat distribution
> (muLinux??). There's supposed to be a blas in RedHat itself:
> 
> [pd at turmalin R]$ rpm -qf /usr/lib/libblas.so.3
> blas-3.0-18
> 
> so I suggest you get that and install instead.  I believe Martyn
> recently compiled a list of RedHat packages that were necessary to
> install the RPMs and placed them somewhere easily findable.


The RH BLAS RPM can be installed from the RPM Package Manager under
"Engineering and Scientific" Applications if you have the RH
installation CDs.

Alternatively, you can get it from RPMSeek at:

http://rpmseek.com/rpm/blas-3.0-18.i386.html?hl=com&cs=blas:PN:0:0:0:0:212061

The lock message, assuming that it may be the result of multiple
processes or an incomplete process, can generally be removed by using:

rm -f /var/lib/rpm/__db*

as root.

HTH,

Marc Schwartz



From tlumley at u.washington.edu  Tue Oct  7 01:38:17 2003
From: tlumley at u.washington.edu (Thomas Lumley)
Date: Mon, 6 Oct 2003 16:38:17 -0700 (PDT)
Subject: [R] Re: Use of the Foreign package to import Stata files
In-Reply-To: <Pine.A41.4.58.0310061512440.153520@homer12.u.washington.edu>
References: <fc.004c4d191400d602004c4d191400d602.1400e749@umit.maine.edu>
	<Pine.A41.4.58.0310061439450.153520@homer12.u.washington.edu>
	<Pine.A41.4.58.0310061512440.153520@homer12.u.washington.edu>
Message-ID: <Pine.A41.4.58.0310061637230.153520@homer12.u.washington.edu>


I just found an old 1.6.2 (Mac OS/Darwin) lying around and it doesn't
recognize the file as a stata file (with foreign 0.5-8)

	-thomas


On Mon, 6 Oct 2003, Thomas Lumley wrote:

> On Mon, 6 Oct 2003, Thomas Lumley wrote:
>
> > On Mon, 6 Oct 2003, Alan Cobo-Lewis wrote:
> >
> > >
> > > But sometimes older works better than newer. The read.dta() function in
> > > R 1.7.0's foreign package (package 0.6-3) fails to recognize as a Stata
> > > file what seems to be a genuine Stata file, even though R 1.6.2's
> > > foreign package (package 0.5-9) imports the same file just work fine.
>
> Are the data correct with 1.6.2? It's looking increasingly strange that
> foreign ever read these data.  The file doesn't seem to have the timestamp
> field that versions 5-8 include, which should mess up the reading of the
> rest of the file.
>
> 	-thomas
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
>

Thomas Lumley			Assoc. Professor, Biostatistics
tlumley at u.washington.edu	University of Washington, Seattle



From borgulya at gyer2.sote.hu  Tue Oct  7 02:20:03 2003
From: borgulya at gyer2.sote.hu (Gabor Borgulya)
Date: Tue, 07 Oct 2003 00:20:03 -0000
Subject: win.metafile [RE: [R] insert eps into microsft word]
In-Reply-To: <8D0F30FE2EB3314182D4A33F738BB19D0BA76F@sqlsrvr.evocapital.com>
References: <8D0F30FE2EB3314182D4A33F738BB19D0BA76F@sqlsrvr.evocapital.com>
Message-ID: <1065486098.3625.111.camel@catv-d5de952e.bp04catv.broadband.hu>

2003-09-10, sze keltez?ssel David Khabie-Zeitoune ezt ?rta:
> Have you tried win.metafile (I'm assuming you are using Windows)?
> E.g.
> 
> win.metafile(file = "c:/test.wmf")
> plot(rnorm(100))
> dev.off()

Hi!

I could not run this example:
Error: couldn't find function "win.metafile"

I am using Linux. Does this function only exist in the windows version
of R? Why? Does it need some functions of the OS itself? Can the Linux
version of R be invoked or compiled with a switch enabling WMF support?

Eps is perfect for a paper, but I would need the same graphs in a MS
Power Point presentation es well. I tried OpenOffice, but it could not
import the eps image.

Do I have to re-run my script on a windows PC?

Gabor



From feh3k at spamcop.net  Tue Oct  7 02:55:29 2003
From: feh3k at spamcop.net (Frank E Harrell Jr)
Date: Mon, 6 Oct 2003 20:55:29 -0400
Subject: [R] vif() from Design and car
In-Reply-To: <5.1.0.14.2.20031006183806.02010ab0@127.0.0.1>
References: <1065474449.16378.5.camel@ra.chem.psu.edu>
	<5.1.0.14.2.20031006183806.02010ab0@127.0.0.1>
Message-ID: <20031006205529.4c00445d.feh3k@spamcop.net>

On Mon, 06 Oct 2003 18:42:04 -0400
John Fox <jfox at mcmaster.ca> wrote:

> Dear Rajarshi,
> 
> I don't have the book to which Frank Harrell refers in the documentation 
> for the vif function in the Design package, but looking at the code for the 
> function, I believe that there's an error:
> 
> vif <-
> function (fit)
> {
>      v <- Varcov(fit, regcoef.only = TRUE)
>      nam <- dimnames(v)[[1]]
>      ns <- num.intercepts(fit)
> #    v <- solve(v)
>      if (ns > 0) {
>          v <- v[-(1:ns), -(1:ns), drop = FALSE]
>          nam <- nam[-(1:ns)]
>      }
>      d <- diag(v)^0.5
>      v <- diag(solve(v/(d %o% d)))
>      names(v) <- nam
>      v
> }
> 
> If you remove the line that I've commented-out, the function will, I 
> believe, return the usual results.

Thanks for the correction John and for pointing out the problem Rajarshi.  I'll put the correction in the next release.

Frank

> 
> I imagine that Frank can comment on this more definitively.
> 
> I hope that this helps,
>   John
> 
> At 05:07 PM 10/6/2003 -0400, Rajarshi Guha wrote:
> >Hi,
> >   I've been generating linear models with lm(). I wanted to look at the
> >VIF's for the coefficient. Using the vif() function from the package
> >Design, I would get unusually high VIF's.
> >
> >However using vif() from the car package I get more reasonable values
> >(ie in line with the quality of the model).
> >
> >What is the difference between the two vif functions? (I dont have
> >access to the reference in the Design's vif() help page)
> >
> >Thanks,
> >
> >-------------------------------------------------------------------
> >Rajarshi Guha <rxg218 at psu.edu> <http://jijo.cjb.net>
> 
> -----------------------------------------------------
> John Fox
> Department of Sociology
> McMaster University
> Hamilton, Ontario, Canada L8S 4M4
> email: jfox at mcmaster.ca
> phone: 905-525-9140x23604
> web: www.socsci.mcmaster.ca/jfox
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help


---
Frank E Harrell Jr    Professor and Chair            School of Medicine
                      Department of Biostatistics    Vanderbilt University



From lisas at salford-systems.com  Tue Oct  7 03:17:29 2003
From: lisas at salford-systems.com (Lisa Solomon)
Date: Mon, 06 Oct 2003 18:17:29 -0700
Subject: [R] Data Mining Contest, $1500 Grand Prize,
 Please forward to students and professors.
Message-ID: <3F821429.7040907@salford-systems.com>

Apologies for cross-posting.

As part of the CART 2004 Data Mining Conference, we have organized a 
student competition focusing on the data mining technology of Leo 
Breiman, Jerome Friedman, Richard Olshen, and Charles Stone (CART, MARS, 
PRIM, and TreeNet/MART).   We see this as an opportunity for students to 
solve a real world data mining problem of personal interest to them 
while working with cutting-edge data mining software tools.   Professors 
might want to incorporate the contest into projects related to class work.

Complete details can be found on the conference website: 
http://www.cartdatamining.com
Please forward this email to students who would have an interest in 
participating in a Student Data Mining Competition.  There is a $1500 
Grand Prize.

If you have any further questions, please let me know.

Sincerely,
Lisa Solomon, Contest Administrator



From Ted.Harding at nessie.mcc.ac.uk  Tue Oct  7 02:56:04 2003
From: Ted.Harding at nessie.mcc.ac.uk ( (Ted Harding))
Date: Tue, 07 Oct 2003 01:56:04 +0100 (BST)
Subject: win.metafile [RE: [R] insert eps into microsft word]
In-Reply-To: <1065486098.3625.111.camel@catv-d5de952e.bp04catv.broadband.hu>
Message-ID: <XFMail.031007015604.Ted.Harding@nessie.mcc.ac.uk>

On 07-Oct-03 Gabor Borgulya wrote:
> I could not run this example:
> Error: couldn't find function "win.metafile"
> 
> I am using Linux. Does this function only exist in the windows version
> of R? Why? Does it need some functions of the OS itself? Can the Linux
> version of R be invoked or compiled with a switch enabling WMF support?
> 
> Eps is perfect for a paper, but I would need the same graphs in a MS
> Power Point presentation es well. I tried OpenOffice, but it could not
> import the eps image.
> 
> Do I have to re-run my script on a windows PC?

If on Linux, install (if you have not already) ImageMagick.

This can convert between all sorts of formats including (if your version
supports it which mine doesn't) Windows Metafile.

If it doesn't, you have options like GIF or JPEG which Windows can import.
Admittedly they're basically bitmaps, so won't scale nicely if you have to
increase their size, but they may well do nicely for your PP presentation.
e.g.:

   convert file.eps file.jpg

NB InageMagick is a suite of several programs, one of which is 'convert'.
But, to find out what formats you might be able to convert between, see
'man ImageMagick'. And, to find out what formats you really have
available, run

  convert -list format

Hoping this helps,
Ted.


--------------------------------------------------------------------
E-Mail: (Ted Harding) <Ted.Harding at nessie.mcc.ac.uk>
Fax-to-email: +44 (0)870 167 1972
Date: 07-Oct-03                                       Time: 01:56:04
------------------------------ XFMail ------------------------------



From MSchwartz at medanalytics.com  Tue Oct  7 03:21:15 2003
From: MSchwartz at medanalytics.com (Marc Schwartz)
Date: Mon, 06 Oct 2003 20:21:15 -0500
Subject: win.metafile [RE: [R] insert eps into microsft word]
In-Reply-To: <1065486098.3625.111.camel@catv-d5de952e.bp04catv.broadband.hu>
References: <8D0F30FE2EB3314182D4A33F738BB19D0BA76F@sqlsrvr.evocapital.com>
	<1065486098.3625.111.camel@catv-d5de952e.bp04catv.broadband.hu>
Message-ID: <1065489674.4752.39.camel@localhost.localdomain>

On Mon, 2003-10-06 at 19:21, Gabor Borgulya wrote:
> 2003-09-10, sze keltez?ssel David Khabie-Zeitoune ezt ?rta:
> > Have you tried win.metafile (I'm assuming you are using Windows)?
> > E.g.
> > 
> > win.metafile(file = "c:/test.wmf")
> > plot(rnorm(100))
> > dev.off()
> 
> Hi!
> 
> I could not run this example:
> Error: couldn't find function "win.metafile"
> 
> I am using Linux. Does this function only exist in the windows version
> of R? Why? Does it need some functions of the OS itself? Can the Linux
> version of R be invoked or compiled with a switch enabling WMF support?
> 
> Eps is perfect for a paper, but I would need the same graphs in a MS
> Power Point presentation es well. I tried OpenOffice, but it could not
> import the eps image.
> 
> Do I have to re-run my script on a windows PC?
> 
> Gabor


The WMF/EMF format is available on Windows only, since it depends upon
Windows OS functionality.

There is a WMF/EMF library for Linux (called libemf), but I can tell you
from personal experience that it does a poor job quality-wise.

There is discussion from the OpenOffice folks on the possibility of
supporting SVG format imports possibly in version 2.x, but not before
then. SVG would provide scalable vector format functionality (like WMF)
for cross-platform support. But not yet.

What I do is to generate an EPS file using R and import that in to
OpenOffice. I can then print it to a PS file and if need be, use ps2pdf
to create a PDF file version.

The trick with R in creating EPS files is to follow the instructions
under ?postscript very closely.  OpenOffice will import R's EPS files if
generated properly.

Most importantly is to set the following arguments:

horizontal = FALSE, onefile = FALSE, paper = "special"

So a call to postscript might be:

postscript("MyGraph.eps", horizontal = FALSE, 
           onefile = FALSE, paper = "special")
plot(...)
dev.off()


That being said, if you import the EPS file into OpenOffice, you will
not see the image, unless you spend the time to create an image preview,
which will be both poor quality and substantially increase the file
size. You will only see a box with an "X" in it as a place holder.

If you need to be able to actually see the slide in OpenOffice or
PowerPoint, you would be best served (under Linux) to generate a PNG
file. See ?png for more information. You need to specify the image size
for your target so that you do not have to worry about re-sizing or you
will lose image quality, since bitmap formats do not re-size well. Once
you create the PNG file, you can then import that into OpenOffice.

An alternative to the above approach would be to use bitmap(), which
accomplishes the generation of the image file a bit differently using
ghostscript. You might try both approaches to see what looks best to
your eye. YMMV.

HTH,

Marc Schwartz



From gblevins at mn.rr.com  Tue Oct  7 05:21:18 2003
From: gblevins at mn.rr.com (Greg Blevins)
Date: Mon, 6 Oct 2003 20:21:18 -0700
Subject: [R] Problem getting an ifelse statment to work
Message-ID: <005d01c38c82$12ecc5d0$1c361d41@glblpyirxqz5lp>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20031006/1ca2b02b/attachment.pl

From andy_liaw at merck.com  Tue Oct  7 03:52:44 2003
From: andy_liaw at merck.com (Liaw, Andy)
Date: Mon, 06 Oct 2003 21:52:44 -0400
Subject: [R] Problem getting an ifelse statment to work
Message-ID: <3A822319EB35174CA3714066D590DCD50205CC31@usrymx25.merck.com>

This is a "long" way; i.e., not necessarily efficient:

> qs2
 [1] 2 1 1 4 4 4 1 1 1 4 2 4 3 1 4 3 3 2 4 3
> qs9
 [1] 4 4 1 3 4 3 1 3 1 4 1 2 3 3 4 4 1 4 2 3
> decision <- function(a, b) {
+     if (a == 1 || b == 1) return(1)
+     if (a == 2 || b == 2) return(2)
+     if (a == 3 || b == 3) return(3)
+     if (a == 4 || b == 4) return(4)
+     NA
+ }
> mapply(decision, qs2, qs9)
 [1] 2 1 1 3 4 3 1 1 1 4 1 2 3 1 4 3 1 2 2 3

Hope this is what you want.

Andy


> -----Original Message-----
> From: Greg Blevins [mailto:gblevins at mn.rr.com] 
> Sent: Monday, October 06, 2003 11:21 PM
> To: R-Help
> Subject: [R] Problem getting an ifelse statment to work
> 
> 
> Hello R experts,
> 
> I trust I have a simple request.  I have a dataframe which 
> among its contents are two variables, qs2 and qs9, which have 
> the following frequencies.
> 
> > table(qs2)
> qs2
>  1  2  3  4 
> 40 22 11 29 
> 
> > table(qs9)
> qs9
>   1   2   3   4 
> 162 172  91 179 
> 
> I simply want to create a new variable which I have called 
> SchCode that would be filled based on the following logic 
> (written in Systat syntax):
> 
> if qs2 = 1 or qs9 = 1 then let SchCode = 1
> if qs2 = 2 or qs9 = 2 then let SchCode = 2 
> if qs2 = 3 or qs9 = 3 then let SchCode = 3 
> if qs2 = 4 or qs9 = 4 then let SchCode = 4
> 
> I have looked through my two Ripley texts, searched the 
> R-help, and have tried various ifelse statements, but I 
> cannot get it right.  Help would be appreciated.
> 
> Thanks,
> Greg Blevins
> The Market Solutions Group
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list 
> https://www.stat.math.ethz.ch/mailman/listinfo> /r-help
>



From jmanry at bacbarcodes.com  Tue Oct  7 04:19:08 2003
From: jmanry at bacbarcodes.com (Janet Manry)
Date: Mon, 6 Oct 2003 21:19:08 -0500
Subject: [R] NaN values returned by cmdscale
Message-ID: <001e01c38c79$63d21670$6e01a8c0@JANETWS>

Hello all,

I'm using R1.7.1 on Linux, generating sammon-optimized MDS plots from
distance matrices. This is a calculation I run routinely, often on
sample sets of up to 100 samples. This time, with three samples, the
sammon function returned an error (shown below), which I tracked down to
the cmdscale function it uses to find a starting configuration. In
short, cmdscale is returning NaN values for the second-dimension
coordinates.

I was able to sidestep this problem by adding the following line to the
sammon code, in order to give a default configuration under this
circumstance.    

y[!is.finite(y)] <- 0 

The result is no errors and the three points lined up along one axis.

I'm guessing that the problem has to do with the small distance between
samples 2 and 3 -- that cmdscale is trying to find coordinates for two
things are essentially on the same spot (within its level of precision).
My question is whether anyone else has seen this problem, and is this
the best way to solve it?

Many thanks,

Janet Manry


Initial distance matrix:
================
[1,] 0.00000000 0.080600084 0.075761312
[2,] 0.08060008 0.000000000 0.004737846
[3,] 0.07576131 0.004737846 0.000000000


Function call:  
============
coordinates <- sammon(newDmat,trace=FALSE)$points

Error message:
===============
Error in sammon(newDmat, trace = FALSE) : NA/NaN/Inf in foreign
function call (arg 4)
In addition: Warning messages:
1: some of the first2eigenvalues are < 0 in: cmdscale(d, k)
2: NaNs produced in: sqrt(ev)

Output from using just cmdscale on the same distance matrix, printing
eigen values and vectors:
===================
 $points
             [,1] [,2]
 [1,]  0.05212395  NaN
 [2,] -0.02847993  NaN
 [3,] -0.02364402  NaN

 $eig
 [1]  4.087052e-03 -6.088049e-20

 $x
 NULL

 $ac
 [1] 0

 $GOF
 [1] 1 1

 Warning messages:
 1: some of the first2eigenvalues are < 0 in: cmdscale(newDmat, eig =
 TRUE)
 2: NaNs produced in: sqrt(ev)

Applying La.eigen to the same distance matrix
==================================
$values
[1]  0.113015163 -0.004734047 -0.108281116

 $vectors
           [,1]         [,2]       [,3]
 [1,] 0.6995002 -0.002659579  0.7146275
 [2,] 0.5194820 -0.684822933 -0.5110342
 [3,] 0.4907524  0.728704657 -0.4776522



====================
Janet Manry, M.S.
Bioinformatics/Development

Bacterial BarCodes, Inc.
Houston, TX USA
jmanry at bacbarcodes.com



From ok at cs.otago.ac.nz  Tue Oct  7 06:18:47 2003
From: ok at cs.otago.ac.nz (Richard A. O'Keefe)
Date: Tue, 7 Oct 2003 17:18:47 +1300 (NZDT)
Subject: [R] Problem getting an ifelse statment to work
Message-ID: <200310070418.h974IlKw153428@atlas.otago.ac.nz>

Something I see fairly often in R code is heavy use of 'return',
like this example today:

	> decision <- function(a, b) {
	+     if (a == 1 || b == 1) return(1)
	+     if (a == 2 || b == 2) return(2)
	+     if (a == 3 || b == 3) return(3)
	+     if (a == 4 || b == 4) return(4)
	+     NA
	+ }

Why do people write code like that instead of

    decision <- function (a,b) {
	if (a == 1 || b == 1) 1 else
	if (a == 2 || b == 2) 2 else
	if (a == 3 || b == 3) 3 else
	if (a == 4 || b == 4) 4 else
	NA
    }

	> mapply(decision, qs2, qs9)
	 [1] 2 1 1 3 4 3 1 1 1 4 1 2 3 1 4 3 1 2 2 3
	
In this case, what's wrong with pmin(qs2, qs9)?



From Toby.Patterson at csiro.au  Tue Oct  7 07:37:38 2003
From: Toby.Patterson at csiro.au (Toby.Patterson@csiro.au)
Date: Tue, 7 Oct 2003 16:37:38 +1100
Subject: [R] Automatic differentiation of compiled code. 
Message-ID: <C4178DC99E08604EA5E2BDB989F09380241FE5@extas2-hba.tas.csiro.au>

Hi all, 
I wondered if anyone had used automatic differentiation libraries (e.g
ADIC/ADOL etc.) in conjunction with R? I have a C shared library that
calculates a likelihood within a wrapper function that gets minimized by
optim. This works OK but it would be nice to also have a function that
calculates the gradient etc of the compiled code. 

So I'm not sure how/if this might work in practice and am still at the
stage of making (very) vague enquiries, so any tips or examples would be
very gratefully received. 

Cheers 
Toby 

Toby Patterson
Pelagic Fisheries and Ecosystems
CSIRO Marine Research
PO Box 1538, Hobart Tas, 7001



From niel.hens at luc.ac.be  Tue Oct  7 09:30:25 2003
From: niel.hens at luc.ac.be (Niel Hens)
Date: Tue, 7 Oct 2003 09:30:25 +0200
Subject: [R] use of glm and cubic splines
Message-ID: <000601c38ca4$e038cd50$3904bec1@pcniel>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20031007/a62b8ecc/attachment.pl

From plummer at iarc.fr  Tue Oct  7 10:11:11 2003
From: plummer at iarc.fr (Martyn Plummer)
Date: Tue, 07 Oct 2003 08:11:11 -0000
Subject: [R] Installing R in Linux 8.0
In-Reply-To: <x2ad8egefh.fsf@biostat.ku.dk>
References: <1065480208.27045.6.camel@udp019098uds.ucsf.edu> 
	<x2ad8egefh.fsf@biostat.ku.dk>
Message-ID: <1065514538.4716.8.camel@xena>

On Tue, 2003-10-07 at 01:00, Peter Dalgaard BSA wrote:
> Javier Arsuaga <jarsuaga at cc.ucsf.edu> writes:
> 
> > I am trying to install R in Linux 8.0 and I downloaded
> > "R-1.7.1-1.i386.rpm" and did rpm -hiv R-1.7.1-1.i386.rpm 
> > and I am getting the following message:
> > 
> > warning: R-1.7.1-1.i386.rpm: V3 DSA signature: NOKEY, key ID
> > 97d3544e
> > error: Failed dependencies:
> >         libblas.so.3 is needed by R-1.7.1-1
> > 
> >         Then I went and look for libblas (which is a linear algebra
> > library) that I found in
> > 
> >         http://www.hklpg.org/RPM/libblas.so.3.html
> > 
> >         and downloaded
> > 		blas-3.0-9.1mlx.i386.rpm
> > 
> >         Tried to do rpm -hiv blas-3.0-9.1mlx.i386.rpm 
> > 
> > 	and got
> > 
> >         error: cannot get exclusive lock on /var/lib/rpm/Packages
> > error: cannot open Packages index using db3 - Operation not permitted
> > (1)
> > error: cannot open Packages database in /var/lib/rpm
> > 
> >         Any suggestions, idea, help...?????
> 
> Wouldn't know about the locking issue -- usually means that you have
> two processes trying to do RPM things. However, the "mlx" in the RPM
> you found suggests that it may be for a non-RedHat distribution
> (muLinux??). There's supposed to be a blas in RedHat itself:
> 
> [pd at turmalin R]$ rpm -qf /usr/lib/libblas.so.3
> blas-3.0-18
> 
> so I suggest you get that and install instead.  I believe Martyn
> recently compiled a list of RedHat packages that were necessary to
> install the RPMs and placed them somewhere easily findable.

That was for Red Hat 9. Javier appears to be using Red Hat 8.0 (NB
Javier NOT Linux 8.0). I haven't got round to doing a list for 8.0.

The R RPMS for Red Hat only depend on other packages that are
distributed with Red Hat Linux.  Your primary source for the required
RPMS should therefore be the set of CDs that you used to install Red
Hat, or the Red Hat web site:

http://www.redhat.com/download/mirror.html

The error message means that you forgot to become super user (root)
before attempting to install R.

Martyn



From fm3a004 at math.uni-hamburg.de  Tue Oct  7 10:27:33 2003
From: fm3a004 at math.uni-hamburg.de (Christian Hennig)
Date: Tue, 7 Oct 2003 10:27:33 +0200 (MEST)
Subject: [R] Installing R with all packages
Message-ID: <Pine.GSO.3.95q.1031007095815.2810C-100000@sun15.math.uni-hamburg.de>

Hi,

I want R to be installed on a UNIX network (Solaris). I am not the system
administrator and so I cannot do it myself.
The system administrator wants to know which packages I want, and it may be 
a lot. 
Is there an easy way to download and install all packages at once?
Is it a good idea? (There is a good chance that I do not need the some 
few packages that do not install well during such a procedure, and so I would
like to tell the sysadmin to do it even if it will not be 100% successful).

Unfortunately it may be even more compicated, because it may be (I was not
able to find it out absolutely surely) that our net 
architecture does not allow to do it via 
install.packages from within R. So what is the easiest way to
do it from outside R?

If it is not a good idea to install all packages 
at once, what is the easiest way to
download and install a list of, say, 30 packages? R CMD INSTALL accepts a
list as input; but how to download a list of 30 packages at once?

(The sysadmin would really appreciate if we could do it in a way that 
later additions of packages are reduced to a minimum.)

Best,
Christian


***********************************************************************
Christian Hennig
Fachbereich Mathematik-SPST/ZMS, Universitaet Hamburg
hennig at math.uni-hamburg.de, http://www.math.uni-hamburg.de/home/hennig/
#######################################################################
ich empfehle www.boag-online.de



From CMiller at PICR.man.ac.uk  Tue Oct  7 10:56:37 2003
From: CMiller at PICR.man.ac.uk (Crispin Miller)
Date: Tue, 7 Oct 2003 09:56:37 +0100
Subject: [R] Optimising code
Message-ID: <BAA35444B19AD940997ED02A6996AAE0B5C1B5@sanmail.picr.man.ac.uk>

Hi,
Does anyone have any advice on speeding up R functions (short of re-implementing them in C :-) )?

I have a function that applies a wilcoxon test to 12 sets of about a quarter of a million pairs (and takes about 3 hours). I've replaced the inner loop I had originally with a function call via mapply, and also considered different approximations  of the wilcoxon, rather than that which is implemented in wilcox.test, but that makes little difference 9and if anything slows things down :-).

Cheers,
Crispin
 
--------------------------------------------------------

 
This email is confidential and intended solely for the use o...{{dropped}}



From ale.ambrosi at unipd.it  Tue Oct  7 11:03:07 2003
From: ale.ambrosi at unipd.it (alessandro ambrosi)
Date: Tue, 07 Oct 2003 11:03:07 +0200
Subject: [R] monotone spline
In-Reply-To: <16257.4928.896536.681026@gargle.gargle.HOWL>
References: <5.1.0.14.2.20031005223214.0436bd30@pop.unipd.it>
	<002c01c38b3e$a4292920$0b01a8c0@hirotohome>
	<5.1.0.14.2.20031005223214.0436bd30@pop.unipd.it>
Message-ID: <5.1.0.14.2.20031007105403.043a6b50@pop.unipd.it>

Dear all,

Sorry for the bad use of the list I did;
I'm very young, only 4 days old to this list :-)

Thanks a lot for your help and your hints:

At 09.01 06/10/03 +0200, you wrote:
>   mono.con(mgcv)          Monotonicity constraints for a cubic
>                           regression spline.

That is what I was looking for. It was under my eyes...

Thank you for your help.
Best,
Alessandro



From ligges at statistik.uni-dortmund.de  Tue Oct  7 11:05:05 2003
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Tue, 07 Oct 2003 11:05:05 +0200
Subject: [R] Installing R with all packages
In-Reply-To: <Pine.GSO.3.95q.1031007095815.2810C-100000@sun15.math.uni-hamburg.de>
References: <Pine.GSO.3.95q.1031007095815.2810C-100000@sun15.math.uni-hamburg.de>
Message-ID: <3F8281C1.1010407@statistik.uni-dortmund.de>

Christian Hennig wrote:

> Hi,
> 
> I want R to be installed on a UNIX network (Solaris). I am not the system
> administrator and so I cannot do it myself.
> The system administrator wants to know which packages I want, and it may be 
> a lot. 
> Is there an easy way to download and install all packages at once?

Yes. Combination of CRAN.packages() and install.packages() will do, but 
it's for sure an overhead to install all packages given you are the only 
one who uses R on that machine.


> Is it a good idea? (There is a good chance that I do not need the some 
> few packages that do not install well during such a procedure, and so I would
> like to tell the sysadmin to do it even if it will not be 100% successful).
> 
> Unfortunately it may be even more compicated, because it may be (I was not
> able to find it out absolutely surely) that our net 
> architecture does not allow to do it via 
> install.packages from within R. So what is the easiest way to
> do it from outside R?
> 
> If it is not a good idea to install all packages 
> at once, what is the easiest way to
> download and install a list of, say, 30 packages? R CMD INSTALL accepts a
> list as input; but how to download a list of 30 packages at once?

E.g. download.file() within R or "wget" outside are the tools I'd choose.


> (The sysadmin would really appreciate if we could do it in a way that 
> later additions of packages are reduced to a minimum.)

Have you told the sysadmin that some packages are updated very frequently?


I propose to set up your own library section (in a directory accessible 
by yourself), where you can install packages independently from the 
sysadmin.

Uwe Ligges



From ripley at stats.ox.ac.uk  Tue Oct  7 11:03:45 2003
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Tue, 7 Oct 2003 10:03:45 +0100 (BST)
Subject: [R] Installing R with all packages
In-Reply-To: <Pine.GSO.3.95q.1031007095815.2810C-100000@sun15.math.uni-hamburg.de>
Message-ID: <Pine.LNX.4.44.0310070958330.2938-100000@gannet.stats>

I once copied all the contrib package sources from CRAN to a local
directory (I used an ftp client, but you could use rsync or wget : just
don't do it recursively or you will get the Archive too) and ran

R CMD INSTALL *.tar.gz

The only overhead in installing all packages at once is disc space:
we seem to have 320Mb in R_HOME/library here.

On Tue, 7 Oct 2003, Christian Hennig wrote:

> Hi,
> 
> I want R to be installed on a UNIX network (Solaris). I am not the system
> administrator and so I cannot do it myself.
> The system administrator wants to know which packages I want, and it may be 
> a lot. 
> Is there an easy way to download and install all packages at once?
> Is it a good idea? (There is a good chance that I do not need the some 
> few packages that do not install well during such a procedure, and so I would
> like to tell the sysadmin to do it even if it will not be 100% successful).
> 
> Unfortunately it may be even more compicated, because it may be (I was not
> able to find it out absolutely surely) that our net 
> architecture does not allow to do it via 
> install.packages from within R. So what is the easiest way to
> do it from outside R?
> 
> If it is not a good idea to install all packages 
> at once, what is the easiest way to
> download and install a list of, say, 30 packages? R CMD INSTALL accepts a
> list as input; but how to download a list of 30 packages at once?
> 
> (The sysadmin would really appreciate if we could do it in a way that 
> later additions of packages are reduced to a minimum.)
> 
> Best,
> Christian
> 
> 
> ***********************************************************************
> Christian Hennig
> Fachbereich Mathematik-SPST/ZMS, Universitaet Hamburg
> hennig at math.uni-hamburg.de, http://www.math.uni-hamburg.de/home/hennig/
> #######################################################################
> ich empfehle www.boag-online.de
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> 
> 

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From ligges at statistik.uni-dortmund.de  Tue Oct  7 11:20:56 2003
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Tue, 07 Oct 2003 11:20:56 +0200
Subject: [R] Optimising code
In-Reply-To: <BAA35444B19AD940997ED02A6996AAE0B5C1B5@sanmail.picr.man.ac.uk>
References: <BAA35444B19AD940997ED02A6996AAE0B5C1B5@sanmail.picr.man.ac.uk>
Message-ID: <3F828578.6080005@statistik.uni-dortmund.de>

Crispin Miller wrote:

> Hi,
> Does anyone have any advice on speeding up R functions (short of re-implementing them in C :-) )?

Some strategies are in every good book on S/R.


> I have a function that applies a wilcoxon test to 12 sets of about a quarter of a million pairs

... and let me guess: everything is significiant to an almost arbitrary 
value of \alpha?


> (and takes about 3 hours). I've replaced the inner loop I had originally with a function call via mapply, and also considered different approximations  of the wilcoxon, rather than that which is implemented in wilcox.test, but that makes little difference 9and if anything slows things down :-).

Are you using wilcox.test(), or a self written function?

Uwe Ligges


> Cheers,
> Crispin



From rossini at blindglobe.net  Tue Oct  7 11:35:35 2003
From: rossini at blindglobe.net (A.J. Rossini)
Date: Tue, 07 Oct 2003 02:35:35 -0700
Subject: [R] Installing R with all packages
In-Reply-To: <Pine.GSO.3.95q.1031007095815.2810C-100000@sun15.math.uni-hamburg.de>
	(Christian
	Hennig's message of "Tue, 7 Oct 2003 10:27:33 +0200 (MEST)")
References: <Pine.GSO.3.95q.1031007095815.2810C-100000@sun15.math.uni-hamburg.de>
Message-ID: <85brstpezc.fsf@blindglobe.net>

Christian Hennig <fm3a004 at math.uni-hamburg.de> writes:

> I want R to be installed on a UNIX network (Solaris). I am not the system
> administrator and so I cannot do it myself.
> The system administrator wants to know which packages I want, and it may be 
> a lot. 
> Is there an easy way to download and install all packages at once?
> Is it a good idea? (There is a good chance that I do not need the some 
> few packages that do not install well during such a procedure, and so I would
> like to tell the sysadmin to do it even if it will not be 100% successful).

Others have commented on how to do it outside of R.  If you end up
being able to do it within R, consider the following hack'd function: 

installNewCRANPackages <- function() {
  test2 <-  packageStatus()$avail["Status"]
  install.packages(row.names(test2)[which(test2$Status=="not installed")])
}

best,
-tony

-- 
rossini at u.washington.edu            http://www.analytics.washington.edu/ 
Biomedical and Health Informatics   University of Washington
Biostatistics, SCHARP/HVTN          Fred Hutchinson Cancer Research Center
UW (Tu/Th/F): 206-616-7630 FAX=206-543-3461 | Voicemail is unreliable
FHCRC  (M/W): 206-667-7025 FAX=206-667-4812 | use Email

CONFIDENTIALITY NOTICE: This e-mail message and any attachme...{{dropped}}



From yairsnir at hotmail.com  Tue Oct  7 11:37:38 2003
From: yairsnir at hotmail.com (Yair Snir)
Date: Tue, 07 Oct 2003 11:37:38 +0200
Subject: [R] I need your help....
Message-ID: <BAY8-F44g9wHRQaXibm000114da@hotmail.com>


   Hello,

   I have a problem, I can't install the package 'mgu74av2cdf'. I
   downloaded the zip file, yet when asked the R console to install it
   from a zip file, I got the answer:

   "Error in file(file, "r") : unable to open connection
   In addition: Warning messages:
   1: error -1 in extracting from zip file
   2: cannot open file `mgu74av2cdf/DESCRIPTION' "

   What can I do? please help me

   Thank you in advance, Yair Snir
 


From CMiller at PICR.man.ac.uk  Tue Oct  7 11:43:23 2003
From: CMiller at PICR.man.ac.uk (Crispin Miller)
Date: Tue, 7 Oct 2003 10:43:23 +0100
Subject: FW: [R] Optimising code
Message-ID: <BAA35444B19AD940997ED02A6996AAE0B5C1BB@sanmail.picr.man.ac.uk>


>> I have a function that applies a wilcoxon test to 12 sets of about a quarter of a million pairs

> ... and let me guess: everything is significiant to an almost arbitrary 
> value of \alpha?

:-) For each of quarter of a million sets, I do a wilcoxon between two pairs each containing twenty numbers...
I do this 12 times...


> > (and takes about 3 hours). I've replaced the inner loop I 
> had originally with a function call via mapply, and also 
> considered different approximations  of the wilcoxon, rather 
> than that which is implemented in wilcox.test, but that makes 
> little difference 9and if anything slows things down :-).
> 
> Are you using wilcox.test(), or a self written function?
> 

I started off with wilcox.test, and tried to speed it up by hacking out as much from the original function as possible, since a fair proportion of the code being executed was logic deciding between the different flavours of the test (I didn't expect that to make much difference, but tried it anyway), there was also a call to pnorm() which I replaced with a numeric approximation. This is not inlined into the original function.

Crispin
 
--------------------------------------------------------

 
This email is confidential and intended solely for the use o...{{dropped}}



From ligges at statistik.uni-dortmund.de  Tue Oct  7 12:05:30 2003
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Tue, 07 Oct 2003 12:05:30 +0200
Subject: [R] I need your help....
In-Reply-To: <BAY8-F44g9wHRQaXibm000114da@hotmail.com>
References: <BAY8-F44g9wHRQaXibm000114da@hotmail.com>
Message-ID: <3F828FEA.5060709@statistik.uni-dortmund.de>

Yair Snir wrote:

>    Hello,
> 
>    I have a problem, I can't install the package 'mgu74av2cdf'. I
>    downloaded the zip file, yet when asked the R console to install it
>    from a zip file, I got the answer:
> 
>    "Error in file(file, "r") : unable to open connection
>    In addition: Warning messages:
>    1: error -1 in extracting from zip file
>    2: cannot open file `mgu74av2cdf/DESCRIPTION' "
> 
>    What can I do? please help me

So it is probably not a valid binary package. Where is "the" package 
from (it's not a CRAN package)?

Uwe Ligges


>    Thank you in advance, Yair Snir
>  
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help



From ligges at statistik.uni-dortmund.de  Tue Oct  7 12:18:23 2003
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Tue, 07 Oct 2003 12:18:23 +0200
Subject: FW: [R] Optimising code
In-Reply-To: <BAA35444B19AD940997ED02A6996AAE0B5C1BB@sanmail.picr.man.ac.uk>
References: <BAA35444B19AD940997ED02A6996AAE0B5C1BB@sanmail.picr.man.ac.uk>
Message-ID: <3F8292EF.7090300@statistik.uni-dortmund.de>

Crispin Miller wrote:

>>>I have a function that applies a wilcoxon test to 12 sets of about a quarter of a million pairs
> 
> 
>>... and let me guess: everything is significiant to an almost arbitrary 
>>value of \alpha?
> 
> 
> :-) For each of quarter of a million sets, I do a wilcoxon between two pairs each containing twenty numbers...
> I do this 12 times...

Ah. Sorry for misunderstanding.


>>>(and takes about 3 hours). I've replaced the inner loop I 
>>
>>had originally with a function call via mapply, and also 
>>considered different approximations  of the wilcoxon, rather 
>>than that which is implemented in wilcox.test, but that makes 
>>little difference 9and if anything slows things down :-).
>>
>>Are you using wilcox.test(), or a self written function?
>>
> 
> 
> I started off with wilcox.test, and tried to speed it up by hacking out as much from the original function as possible, since a fair proportion of the code being executed was logic deciding between the different flavours of the test (I didn't expect that to make much difference, but tried it anyway), there was also a call to pnorm() which I replaced with a numeric approximation. This is not inlined into the original function.


In that case you cannot do very much except for writing it in a language 
like C / Fortran, if you really need the speed.

Uwe Ligges

> Crispin
>  
> --------------------------------------------------------
> 
>  
> This email is confidential and intended solely for the use o...{{dropped}}
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help



From p.dalgaard at biostat.ku.dk  Tue Oct  7 12:30:45 2003
From: p.dalgaard at biostat.ku.dk (Peter Dalgaard BSA)
Date: Tue, 07 Oct 2003 10:30:45 -0000
Subject: [R] I need your help....
In-Reply-To: <3F828FEA.5060709@statistik.uni-dortmund.de>
References: <BAY8-F44g9wHRQaXibm000114da@hotmail.com>
	<3F828FEA.5060709@statistik.uni-dortmund.de>
Message-ID: <x265j1jq1o.fsf@biostat.ku.dk>

Uwe Ligges <ligges at statistik.uni-dortmund.de> writes:

> Yair Snir wrote:
> 
> >    Hello,
> >    I have a problem, I can't install the package 'mgu74av2cdf'. I
> >    downloaded the zip file, yet when asked the R console to install it
> >    from a zip file, I got the answer:
> >    "Error in file(file, "r") : unable to open connection
> >    In addition: Warning messages:
> >    1: error -1 in extracting from zip file
> >    2: cannot open file `mgu74av2cdf/DESCRIPTION' "
> >    What can I do? please help me
> 
> So it is probably not a valid binary package. Where is "the" package
> from (it's not a CRAN package)?

It's an Affymetrix thingie, so the BioC list would be the more obvious
place to ask. Notice that there are R data packages, e.g.

http://biowww.dfci.harvard.edu/~bioconductor/data/cdfenvs/Win32/mgu74av2cdf_0.1.zip

and (if I understand correctly) S-PLUS data libraries like

http://www.insightful.com/support/ArrayAnalyzer/DataLibs/CDFLibs/mgu74cv2cdf.zip

and I suspect they are not interchangeable even though both come in
.zip files.

-- 
   O__  ---- Peter Dalgaard             Blegdamsvej 3  
  c/ /'_ --- Dept. of Biostatistics     2200 Cph. N   
 (*) \(*) -- University of Copenhagen   Denmark      Ph: (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)             FAX: (+45) 35327907



From xye at emich.edu  Tue Oct  7 12:40:18 2003
From: xye at emich.edu (xinyue ye)
Date: Tue, 7 Oct 2003 06:40:18 -0400
Subject: [R] If I produce lots of figures at one time,
	how can I make R graphics keep all the figures already made? 
Message-ID: <005c01c38cbf$66c40370$5c804ca4@XY>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20031007/869484cb/attachment.pl

From bernd.weiss at uni-koeln.de  Tue Oct  7 13:07:45 2003
From: bernd.weiss at uni-koeln.de (Bernd Weiss)
Date: Tue, 07 Oct 2003 13:07:45 +0200
Subject: [R] Adjusting for within-cluster correlation: robcov() in
	Design-package and 'ids' in survey-package
Message-ID: <3F82BAA1.18649.102A170@localhost>

Dear all,

I would like to know if it possible to use the the robcov()-command in the Design-
package in order to obtain a robust variance-estimate that adjusts for within-cluster 
correlation. Does the ids-option in the survey-package the same job? 

TIA,

Bernd



From glaziou at pasteur-kh.org  Tue Oct  7 13:21:40 2003
From: glaziou at pasteur-kh.org (Philippe Glaziou)
Date: Tue, 7 Oct 2003 18:21:40 +0700
Subject: [R] If I produce lots of figures at one time,
	how can I make R graphics keep all the figures already made?
In-Reply-To: <005c01c38cbf$66c40370$5c804ca4@XY>
References: <005c01c38cbf$66c40370$5c804ca4@XY>
Message-ID: <20031007112140.GD2552@pasteur-kh.org>

xinyue ye <xye at emich.edu> wrote:
> If I produce lots of figures at one time, how can I make R
> graphics keep all the figures already made?  thanks,


Open as many graphics devices as needed.

See the examples in:

?dev.list

-- 
Philippe



From CMiller at PICR.man.ac.uk  Tue Oct  7 13:21:19 2003
From: CMiller at PICR.man.ac.uk (Crispin Miller)
Date: Tue, 7 Oct 2003 12:21:19 +0100
Subject: FW: [R] Optimising code
Message-ID: <BAA35444B19AD940997ED02A6996AAE00B14B3@sanmail.picr.man.ac.uk>

Yup, my mistake for not being very clear! Alas the computer is already very fast - looks like it's C for me...

> -----Original Message-----
> From: Patrick Burns [mailto:pburns at pburns.seanet.com]
> Sent: 07 October 2003 12:00
> Cc: Crispin Miller
> Subject: Re: FW: [R] Optimising code
> 
> 
> I had misunderstood also.  I agree with Uwe either move
> to C or buy a faster computer -- or at least another one.
> 
> Pat
> 
> Uwe Ligges wrote:
> 
> > Crispin Miller wrote:
> >
> >>>> I have a function that applies a wilcoxon test to 12 
> sets of about 
> >>>> a quarter of a million pairs
> >>>
> >>
> >>
> >>> ... and let me guess: everything is significiant to an almost 
> >>> arbitrary value of \alpha?
> >>
> >>
> >>
> >> :-) For each of quarter of a million sets, I do a wilcoxon between 
> >> two pairs each containing twenty numbers...
> >> I do this 12 times...
> >
> >
> > Ah. Sorry for misunderstanding.
> >
> >
> >>>> (and takes about 3 hours). I've replaced the inner loop I 
> >>>
> >>>
> >>> had originally with a function call via mapply, and also 
> considered 
> >>> different approximations  of the wilcoxon, rather than 
> that which is 
> >>> implemented in wilcox.test, but that makes little 
> difference 9and if 
> >>> anything slows things down :-).
> >>>
> >>> Are you using wilcox.test(), or a self written function?
> >>>
> >>
> >>
> >> I started off with wilcox.test, and tried to speed it up 
> by hacking 
> >> out as much from the original function as possible, since a fair 
> >> proportion of the code being executed was logic deciding 
> between the 
> >> different flavours of the test (I didn't expect that to make much 
> >> difference, but tried it anyway), there was also a call to pnorm() 
> >> which I replaced with a numeric approximation. This is not inlined 
> >> into the original function.
> >
> >
> >
> > In that case you cannot do very much except for writing it in a 
> > language like C / Fortran, if you really need the speed.
> >
> > Uwe Ligges
> >
> >> Crispin
> >>  
> >> --------------------------------------------------------
> >>
> >>  
> >> This email is confidential and intended solely for the use 
> >> o...{{dropped}}
> >>
> >> ______________________________________________
> >> R-help at stat.math.ethz.ch mailing list
> >> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> >
> >
> > ______________________________________________
> > R-help at stat.math.ethz.ch mailing list
> > https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> >
> >
> 
> 
> 
>
 
--------------------------------------------------------

 
This email is confidential and intended solely for the use o...{{dropped}}



From thpe at hhbio.wasser.tu-dresden.de  Tue Oct  7 13:31:39 2003
From: thpe at hhbio.wasser.tu-dresden.de (Thomas Petzoldt)
Date: Tue, 07 Oct 2003 13:31:39 +0200
Subject: [R] If I produce lots of figures at one time,	how can I make
	R graphics keep all the figures already made?
In-Reply-To: <005c01c38cbf$66c40370$5c804ca4@XY>
References: <005c01c38cbf$66c40370$5c804ca4@XY>
Message-ID: <3F82A41B.7090209@hhbio.wasser.tu-dresden.de>

xinyue ye schrieb:

> If I produce lots of figures at one time, how can I make R graphics keep all the figures already made? 
> thanks,

write it to a postscript() or pdf() file or to several bitmaps, see 
png() for examples.

If you are on Windows, there is a Recording function in the menu, if the 
graphics window is activated.


Thomas Petzoldt



From ramasamya at gis.a-star.edu.sg  Tue Oct  7 13:46:51 2003
From: ramasamya at gis.a-star.edu.sg (Adaikalavan RAMASAMY)
Date: Tue, 7 Oct 2003 19:46:51 +0800
Subject: FW: [R] Optimising code
Message-ID: <6D9E9B9DF347EF4385F6271C64FB8D56075F9E@BIONIC.biopolis.one-north.com>

If you are only interested in calculating the wilcoxon statistic (as one
would with calculating permutated p-values for example), the following
should suffice.

length.na <- function(x, ...){
  tmp <- !(is.na(x) | is.infinite(x))
  length(x[tmp], ...)
}

rank.na <- function(x){
  y <- x[!is.na(x)]; 
  x[!is.na(x)] <- rank(y);
  return(x)
}

wstats.fn <- function(data=NULL, g1=NULL, g2=NULL){
  temp <- t( apply(data, 1, rank.na));
  n1 <- apply(temp[ ,g1], 1, length.na);
  n2 <- apply(temp[ ,g2], 1, length.na);

  wstats <- rowSums( temp[,g1], na.rm=TRUE ) - n1*(n2 + 1)/2;
  return(wstats);
}

m <- matrix( rnorm( 2500*100 ), ncol=100 )
w1 <- wstats.fn(data=m, g1=1:50, g2=51:100)
w2 <- apply(m, 1, function(x) wilcox.test( x[1:50], x[51:100] )$stat)

On my machine, it take 2.2s to calculate w1 compared to 15.6s for w2.
rank.na and length.na are not necessary if your data does not have
missing values. 

Since you are doing large enough times it might be wise to code in C. My
C version of this is at least 20-40 times faster than the one in R but
like the one above it does not return p-values.


--
Adaikalavan Ramasamy 


-----Original Message-----
From: Uwe Ligges [mailto:ligges at statistik.uni-dortmund.de] 
Sent: Tuesday, October 07, 2003 6:18 PM
To: Crispin Miller
Cc: R-help (E-mail)
Subject: Re: FW: [R] Optimising code


Crispin Miller wrote:

>>>I have a function that applies a wilcoxon test to 12 sets of about a 
>>>quarter of a million pairs
> 
> 
>>... and let me guess: everything is significiant to an almost 
>>arbitrary
>>value of \alpha?
> 
> 
> :-) For each of quarter of a million sets, I do a wilcoxon between two

> pairs each containing twenty numbers... I do this 12 times...

Ah. Sorry for misunderstanding.


>>>(and takes about 3 hours). I've replaced the inner loop I
>>
>>had originally with a function call via mapply, and also
>>considered different approximations  of the wilcoxon, rather 
>>than that which is implemented in wilcox.test, but that makes 
>>little difference 9and if anything slows things down :-).
>>
>>Are you using wilcox.test(), or a self written function?
>>
> 
> 
> I started off with wilcox.test, and tried to speed it up by hacking 
> out as much from the original function as possible, since a fair 
> proportion of the code being executed was logic deciding between the 
> different flavours of the test (I didn't expect that to make much 
> difference, but tried it anyway), there was also a call to pnorm() 
> which I replaced with a numeric approximation. This is not inlined 
> into the original function.


In that case you cannot do very much except for writing it in a language

like C / Fortran, if you really need the speed.

Uwe Ligges

> Crispin
>  
> --------------------------------------------------------
> 
>  
> This email is confidential and intended solely for the use 
> o...{{dropped}}
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list 
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://www.stat.math.ethz.ch/mailman/listinfo/r-help



From laura at env.leeds.ac.uk  Tue Oct  7 14:03:57 2003
From: laura at env.leeds.ac.uk (Laura Quinn)
Date: Tue, 7 Oct 2003 13:03:57 +0100 (BST)
Subject: [R] Beginner's query - segmentation fault
Message-ID: <Pine.LNX.4.44.0310071259390.28305-100000@env-pc-phd13>

I am dealing with a huge matrix in R (20 columns, 54000 rows) and have
lots of missing values within the dataset which are currently displayed as
the value "-999.00" I am trying to create a new matrix (or change the
existing one) to display these values as "NA" so that I can then perform
the necessary analysis on the columns within the matrix.

The matrix name is temp and the column names are t1 to t20 inclusive.

I have tried the following command:

temp$t1[temp$t1 == -999.00] <- NA

and it returns a segmentation fault, can someone tell me what I am doing
wrong?

Thanks
Laura



From henric.nilsson at statisticon.se  Tue Oct  7 14:14:29 2003
From: henric.nilsson at statisticon.se (Henric Nilsson)
Date: Tue, 07 Oct 2003 14:14:29 +0200
Subject: [R] If I produce lots of figures at one time,
	how can I make R graphics keep all the figures already made?
In-Reply-To: <3F82A41B.7090209@hhbio.wasser.tu-dresden.de>
References: <005c01c38cbf$66c40370$5c804ca4@XY>
	<005c01c38cbf$66c40370$5c804ca4@XY>
Message-ID: <5.2.1.1.0.20031007140617.031b3908@10.0.10.66>

At 13:31 2003-10-07 +0200, you wrote:

>>If I produce lots of figures at one time, how can I make R graphics keep 
>>all the figures already made? thanks,
>If you are on Windows, there is a Recording function in the menu, if the 
>graphics window is activated.

Actually, the recording of plots can be switched on by windows(record = TRUE).

Henric



From p.dalgaard at biostat.ku.dk  Tue Oct  7 14:25:23 2003
From: p.dalgaard at biostat.ku.dk (Peter Dalgaard BSA)
Date: Tue, 07 Oct 2003 12:25:23 -0000
Subject: [R] Beginner's query - segmentation fault
In-Reply-To: <Pine.LNX.4.44.0310071259390.28305-100000@env-pc-phd13>
References: <Pine.LNX.4.44.0310071259390.28305-100000@env-pc-phd13>
Message-ID: <x21xtpjkro.fsf@biostat.ku.dk>

Laura Quinn <laura at env.leeds.ac.uk> writes:

> I am dealing with a huge matrix in R (20 columns, 54000 rows) and have
> lots of missing values within the dataset which are currently displayed as
> the value "-999.00" I am trying to create a new matrix (or change the
> existing one) to display these values as "NA" so that I can then perform
> the necessary analysis on the columns within the matrix.
> 
> The matrix name is temp and the column names are t1 to t20 inclusive.
> 
> I have tried the following command:
> 
> temp$t1[temp$t1 == -999.00] <- NA
> 
> and it returns a segmentation fault, can someone tell me what I am doing
> wrong?

Not telling us which system and which version you are using, and not
giving us a reproducible example... OK, the latter can be tricky, but
does it happen all the time? Only after doing X? Also if you deal with
a subset of data? 

The command as such should work as far as I can see, and segmentation
faults should basically not happen unless the user has been messing
about at the C code level. 

(BTW, that's a data frame, not a "matrix", I assume.)

-- 
   O__  ---- Peter Dalgaard             Blegdamsvej 3  
  c/ /'_ --- Dept. of Biostatistics     2200 Cph. N   
 (*) \(*) -- University of Copenhagen   Denmark      Ph: (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)             FAX: (+45) 35327907



From ripley at stats.ox.ac.uk  Tue Oct  7 14:29:48 2003
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Tue, 7 Oct 2003 13:29:48 +0100 (BST)
Subject: [R] Beginner's query - segmentation fault
In-Reply-To: <Pine.LNX.4.44.0310071259390.28305-100000@env-pc-phd13>
Message-ID: <Pine.LNX.4.44.0310071322130.3397-100000@gannet.stats>

On Tue, 7 Oct 2003, Laura Quinn wrote:

> I am dealing with a huge matrix in R (20 columns, 54000 rows) and have
> lots of missing values within the dataset which are currently displayed as
> the value "-999.00" I am trying to create a new matrix (or change the
> existing one) to display these values as "NA" so that I can then perform
> the necessary analysis on the columns within the matrix.
> 
> The matrix name is temp and the column names are t1 to t20 inclusive.
> 
> I have tried the following command:
> 
> temp$t1[temp$t1 == -999.00] <- NA
> 
> and it returns a segmentation fault, can someone tell me what I am doing
> wrong?

Well, R should not segfault, so there is bug here somewhere.  However, I
don't think what you have described can actually work. Is temp really a
matrix?  If so temp$t1 will return NULL, and you should get an error
message.


If temp is a matrix

temp[temp == -999.00] <- NA

will do what you want.


If as is more likely temp is a data frame with all columns numeric,
there are several ways to do this, e.g.

temp[] <- lapply(temp, function(x) ifelse(x == -999, NA, x))

temp[as.matrix(temp) == -999] <- NA  # only in recent versions of R

as well as explicit looping over columns.

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From ripley at stats.ox.ac.uk  Tue Oct  7 14:31:56 2003
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Tue, 7 Oct 2003 13:31:56 +0100 (BST)
Subject: [R] If I produce lots of figures at one time, how can I make R
	graphics keep all the figures already made?
In-Reply-To: <5.2.1.1.0.20031007140617.031b3908@10.0.10.66>
Message-ID: <Pine.LNX.4.44.0310071330180.3397-100000@gannet.stats>

On Tue, 7 Oct 2003, Henric Nilsson wrote:

> At 13:31 2003-10-07 +0200, you wrote:
> 
> >>If I produce lots of figures at one time, how can I make R graphics keep 
> >>all the figures already made? thanks,
> >If you are on Windows, there is a Recording function in the menu, if the 
> >graphics window is activated.
> 
> Actually, the recording of plots can be switched on by windows(record = TRUE).

and on any platform you can use recordPlot() and replayPlot(x) to record 
and redisplay plots (on a screen device only).

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From ramasamya at gis.a-star.edu.sg  Tue Oct  7 14:29:40 2003
From: ramasamya at gis.a-star.edu.sg (Adaikalavan RAMASAMY)
Date: Tue, 7 Oct 2003 20:29:40 +0800
Subject: [R] Beginner's query - segmentation fault
Message-ID: <6D9E9B9DF347EF4385F6271C64FB8D56075FA0@BIONIC.biopolis.one-north.com>

I cannot explain the segmentation fault but try this instead (which
works for matrices) 

temp[which(temp==-999, arr.ind=T)] <- NA

Are you sure temp is matrix and not a dataframe ? Use class(temp) to
find out.

Also, if you are getting these "-999.00" because you have read files
containing them, it might just be easier to code the missing values when
reading in. Try read.table( file="lala.txt",  na.strings = "-999.00").

--
Adaikalavan Ramasamy 



-----Original Message-----
From: Laura Quinn [mailto:laura at env.leeds.ac.uk] 
Sent: Tuesday, October 07, 2003 8:04 PM
To: r-help at stat.math.ethz.ch
Subject: [R] Beginner's query - segmentation fault


I am dealing with a huge matrix in R (20 columns, 54000 rows) and have
lots of missing values within the dataset which are currently displayed
as the value "-999.00" I am trying to create a new matrix (or change the
existing one) to display these values as "NA" so that I can then perform
the necessary analysis on the columns within the matrix.

The matrix name is temp and the column names are t1 to t20 inclusive.

I have tried the following command:

temp$t1[temp$t1 == -999.00] <- NA

and it returns a segmentation fault, can someone tell me what I am doing
wrong?

Thanks
Laura

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://www.stat.math.ethz.ch/mailman/listinfo/r-help



From ligges at statistik.uni-dortmund.de  Tue Oct  7 14:37:26 2003
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Tue, 07 Oct 2003 14:37:26 +0200
Subject: [R] Beginner's query - segmentation fault
In-Reply-To: <Pine.LNX.4.44.0310071259390.28305-100000@env-pc-phd13>
References: <Pine.LNX.4.44.0310071259390.28305-100000@env-pc-phd13>
Message-ID: <3F82B386.2020308@statistik.uni-dortmund.de>

Laura Quinn wrote:

> I am dealing with a huge matrix in R (20 columns, 54000 rows) and have
> lots of missing values within the dataset which are currently displayed as
> the value "-999.00" I am trying to create a new matrix (or change the
> existing one) to display these values as "NA" so that I can then perform
> the necessary analysis on the columns within the matrix.
> 
> The matrix name is temp and the column names are t1 to t20 inclusive.
> 
> I have tried the following command:
> 
> temp$t1[temp$t1 == -999.00] <- NA
>
> and it returns a segmentation fault, can someone tell me what I am doing
> wrong?

The crash for this inappropriate usage has already been fixed for 
R-1.7.1, so you are using an outdated version, I guess.


1. If temp is a matrix, you have to use matrix indexing, not data.frame 
or list indexing, see the manuals.

Now, we have got the (still wrong) line
   temp[temp[ ,"t1"] == -999.00, "t1"] <- NA

2. Use "is.na(x) <- TRUE" instead of "x <- NA":
   is.na(temp[temp[ ,"t1"] == -999.00, "t1"]) <- TRUE


Or change all values "-999" to "NA" in the whole matrix by
   is.na(temp[temp == -999.00]) <- TRUE


Uwe Ligges


> Thanks
> Laura
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help



From jmacdon at med.umich.edu  Tue Oct  7 14:42:33 2003
From: jmacdon at med.umich.edu (James MacDonald)
Date: Tue, 07 Oct 2003 08:42:33 -0400
Subject: [R] I need your help....
Message-ID: <sf827c7e.099@med-gwia-01a.med.umich.edu>

This is a question for the Bioconductor listserv, not R-help. In
addition, all current versions of affy will automagically download and
install any cdfenvs that you may need, so there is no need to do this
manually. If anything, you may need to update your version of affy.

Jim



James W. MacDonald
Affymetrix and cDNA Microarray Core
University of Michigan Cancer Center
1500 E. Medical Center Drive
7410 CCGC
Ann Arbor MI 48109
734-647-5623

>>> Peter Dalgaard BSA <p.dalgaard at biostat.ku.dk> 10/07/03 06:33AM >>>
Uwe Ligges <ligges at statistik.uni-dortmund.de> writes:

> Yair Snir wrote:
> 
> >    Hello,
> >    I have a problem, I can't install the package 'mgu74av2cdf'. I
> >    downloaded the zip file, yet when asked the R console to install
it
> >    from a zip file, I got the answer:
> >    "Error in file(file, "r") : unable to open connection
> >    In addition: Warning messages:
> >    1: error -1 in extracting from zip file
> >    2: cannot open file `mgu74av2cdf/DESCRIPTION' "
> >    What can I do? please help me
> 
> So it is probably not a valid binary package. Where is "the" package
> from (it's not a CRAN package)?

It's an Affymetrix thingie, so the BioC list would be the more obvious
place to ask. Notice that there are R data packages, e.g.

http://biowww.dfci.harvard.edu/~bioconductor/data/cdfenvs/Win32/mgu74av2cdf_0.1.zip


and (if I understand correctly) S-PLUS data libraries like

http://www.insightful.com/support/ArrayAnalyzer/DataLibs/CDFLibs/mgu74cv2cdf.zip


and I suspect they are not interchangeable even though both come in
.zip files.

-- 
   O__  ---- Peter Dalgaard             Blegdamsvej 3  
  c/ /'_ --- Dept. of Biostatistics     2200 Cph. N   
 (*) \(*) -- University of Copenhagen   Denmark      Ph: (+45)
35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)             FAX: (+45)
35327907

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://www.stat.math.ethz.ch/mailman/listinfo/r-help



From laura at env.leeds.ac.uk  Tue Oct  7 15:00:33 2003
From: laura at env.leeds.ac.uk (Laura Quinn)
Date: Tue, 7 Oct 2003 14:00:33 +0100 (BST)
Subject: [R] Beginner's query - segmentation fault
In-Reply-To: <Pine.LNX.4.44.0310071322130.3397-100000@gannet.stats>
Message-ID: <Pine.LNX.4.44.0310071358140.28305-100000@env-pc-phd13>

thanks, have used

temp [temp==0]<- NA

and this seems to have worked, though it won't let me access individual
columns (ie temp$t1 etc) to work on - is there any real advantage in using
a matrix, or would i be better advised to deal with dataframes? (I have
double checked and temp is currently a matrix).



On Tue, 7 Oct 2003, Prof Brian Ripley wrote:

> On Tue, 7 Oct 2003, Laura Quinn wrote:
>
> > I am dealing with a huge matrix in R (20 columns, 54000 rows) and have
> > lots of missing values within the dataset which are currently displayed as
> > the value "-999.00" I am trying to create a new matrix (or change the
> > existing one) to display these values as "NA" so that I can then perform
> > the necessary analysis on the columns within the matrix.
> >
> > The matrix name is temp and the column names are t1 to t20 inclusive.
> >
> > I have tried the following command:
> >
> > temp$t1[temp$t1 == -999.00] <- NA
> >
> > and it returns a segmentation fault, can someone tell me what I am doing
> > wrong?
>
> Well, R should not segfault, so there is bug here somewhere.  However, I
> don't think what you have described can actually work. Is temp really a
> matrix?  If so temp$t1 will return NULL, and you should get an error
> message.
>
>
> If temp is a matrix
>
> temp[temp == -999.00] <- NA
>
> will do what you want.
>
>
> If as is more likely temp is a data frame with all columns numeric,
> there are several ways to do this, e.g.
>
> temp[] <- lapply(temp, function(x) ifelse(x == -999, NA, x))
>
> temp[as.matrix(temp) == -999] <- NA  # only in recent versions of R
>
> as well as explicit looping over columns.
>
> --
> Brian D. Ripley,                  ripley at stats.ox.ac.uk
> Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
> University of Oxford,             Tel:  +44 1865 272861 (self)
> 1 South Parks Road,                     +44 1865 272866 (PA)
> Oxford OX1 3TG, UK                Fax:  +44 1865 272595
>
>



From paulojus at est.ufpr.br  Tue Oct  7 15:03:19 2003
From: paulojus at est.ufpr.br (Paulo Justiniano Ribeiro Jr)
Date: Tue, 7 Oct 2003 10:03:19 -0300 (BRT)
Subject: [R] Installing R with all packages
In-Reply-To: <3F8281C1.1010407@statistik.uni-dortmund.de>
References: <Pine.GSO.3.95q.1031007095815.2810C-100000@sun15.math.uni-hamburg.de>
	<3F8281C1.1010407@statistik.uni-dortmund.de>
Message-ID: <Pine.LNX.4.56.0310071002370.15831@gauss.est.ufpr.br>

Hi

I use the script in the attached file to do this

Cheers
P.J.



On Tue, 7 Oct 2003, Uwe Ligges wrote:

> Christian Hennig wrote:
>
> > Hi,
> >
> > I want R to be installed on a UNIX network (Solaris). I am not the system
> > administrator and so I cannot do it myself.
> > The system administrator wants to know which packages I want, and it may be
> > a lot.
> > Is there an easy way to download and install all packages at once?
>
> Yes. Combination of CRAN.packages() and install.packages() will do, but
> it's for sure an overhead to install all packages given you are the only
> one who uses R on that machine.
>
>
> > Is it a good idea? (There is a good chance that I do not need the some
> > few packages that do not install well during such a procedure, and so I would
> > like to tell the sysadmin to do it even if it will not be 100% successful).
> >
> > Unfortunately it may be even more compicated, because it may be (I was not
> > able to find it out absolutely surely) that our net
> > architecture does not allow to do it via
> > install.packages from within R. So what is the easiest way to
> > do it from outside R?
> >
> > If it is not a good idea to install all packages
> > at once, what is the easiest way to
> > download and install a list of, say, 30 packages? R CMD INSTALL accepts a
> > list as input; but how to download a list of 30 packages at once?
>
> E.g. download.file() within R or "wget" outside are the tools I'd choose.
>
>
> > (The sysadmin would really appreciate if we could do it in a way that
> > later additions of packages are reduced to a minimum.)
>
> Have you told the sysadmin that some packages are updated very frequently?
>
>
> I propose to set up your own library section (in a directory accessible
> by yourself), where you can install packages independently from the
> sysadmin.
>
> Uwe Ligges
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
>
>

Paulo Justiniano Ribeiro Jr
Departamento de Estat?stica
Universidade Federal do Paran?
Caixa Postal 19.081
CEP 81.531-990
Curitiba, PR  -  Brasil
Tel: (+55) 41 361 3471
Fax: (+55) 41 361 3141
e-mail: pj at est.ufpr.br
http://www.est.ufpr.br/~paulojus
-------------- next part --------------
#!/bin/bash
RPACKUP=`pwd`
echo ==================================
echo  downloading packages sources
echo ==================================
cd /home/DOWNLOADS/R
rsync -rvz --delete --include "contrib/" --include "*.tar.gz" --exclude "*" cran.r-project.org::CRAN/src/ .
cd contrib
for pack in `ls *.tar.gz`
do
R CMD INSTALL -c $pack
cd $RPACKUP

From ligges at statistik.uni-dortmund.de  Tue Oct  7 15:05:14 2003
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Tue, 07 Oct 2003 15:05:14 +0200
Subject: [R] Beginner's query - segmentation fault
In-Reply-To: <6D9E9B9DF347EF4385F6271C64FB8D56075FA0@BIONIC.biopolis.one-north.com>
References: <6D9E9B9DF347EF4385F6271C64FB8D56075FA0@BIONIC.biopolis.one-north.com>
Message-ID: <3F82BA0A.8020500@statistik.uni-dortmund.de>

Adaikalavan RAMASAMY wrote:
> I cannot explain the segmentation fault but try this instead (which
> works for matrices) 
> 
> temp[which(temp==-999, arr.ind=T)] <- NA

No! Please *do* use is.na()<- !!!

Uwe Ligges

> Are you sure temp is matrix and not a dataframe ? Use class(temp) to
> find out.
> 
> Also, if you are getting these "-999.00" because you have read files
> containing them, it might just be easier to code the missing values when
> reading in. Try read.table( file="lala.txt",  na.strings = "-999.00").
> 
> --
> Adaikalavan Ramasamy 
> 
> 
> 
> -----Original Message-----
> From: Laura Quinn [mailto:laura at env.leeds.ac.uk] 
> Sent: Tuesday, October 07, 2003 8:04 PM
> To: r-help at stat.math.ethz.ch
> Subject: [R] Beginner's query - segmentation fault
> 
> 
> I am dealing with a huge matrix in R (20 columns, 54000 rows) and have
> lots of missing values within the dataset which are currently displayed
> as the value "-999.00" I am trying to create a new matrix (or change the
> existing one) to display these values as "NA" so that I can then perform
> the necessary analysis on the columns within the matrix.
> 
> The matrix name is temp and the column names are t1 to t20 inclusive.
> 
> I have tried the following command:
> 
> temp$t1[temp$t1 == -999.00] <- NA
> 
> and it returns a segmentation fault, can someone tell me what I am doing
> wrong?
> 
> Thanks
> Laura
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help



From ripley at stats.ox.ac.uk  Tue Oct  7 15:04:38 2003
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Tue, 7 Oct 2003 14:04:38 +0100 (BST)
Subject: [R] Beginner's query - segmentation fault
In-Reply-To: <Pine.LNX.4.44.0310071358140.28305-100000@env-pc-phd13>
Message-ID: <Pine.LNX.4.44.0310071401130.3522-100000@gannet.stats>

On Tue, 7 Oct 2003, Laura Quinn wrote:

> thanks, have used
> 
> temp [temp==0]<- NA
> 
> and this seems to have worked, though it won't let me access individual
> columns (ie temp$t1 etc) to work on - is there any real advantage in using
> a matrix, or would i be better advised to deal with dataframes? (I have
> double checked and temp is currently a matrix).

Things are going to be a lot faster for a numerical matrix than a data 
frame: the advantage of data frames is that the columns can be of
different types.

BTW, you should really use  temp[, "t1"] for a data frame or a matrix:
temp$t1 works for data frames, `by the back door' and has a number of bugs 
(including failing to detect errors which corrupt the data frame) prior to 
1.8.0 (to be).


> 
> 
> 
> On Tue, 7 Oct 2003, Prof Brian Ripley wrote:
> 
> > On Tue, 7 Oct 2003, Laura Quinn wrote:
> >
> > > I am dealing with a huge matrix in R (20 columns, 54000 rows) and have
> > > lots of missing values within the dataset which are currently displayed as
> > > the value "-999.00" I am trying to create a new matrix (or change the
> > > existing one) to display these values as "NA" so that I can then perform
> > > the necessary analysis on the columns within the matrix.
> > >
> > > The matrix name is temp and the column names are t1 to t20 inclusive.
> > >
> > > I have tried the following command:
> > >
> > > temp$t1[temp$t1 == -999.00] <- NA
> > >
> > > and it returns a segmentation fault, can someone tell me what I am doing
> > > wrong?
> >
> > Well, R should not segfault, so there is bug here somewhere.  However, I
> > don't think what you have described can actually work. Is temp really a
> > matrix?  If so temp$t1 will return NULL, and you should get an error
> > message.
> >
> >
> > If temp is a matrix
> >
> > temp[temp == -999.00] <- NA
> >
> > will do what you want.
> >
> >
> > If as is more likely temp is a data frame with all columns numeric,
> > there are several ways to do this, e.g.
> >
> > temp[] <- lapply(temp, function(x) ifelse(x == -999, NA, x))
> >
> > temp[as.matrix(temp) == -999] <- NA  # only in recent versions of R
> >
> > as well as explicit looping over columns.
> >
> > --
> > Brian D. Ripley,                  ripley at stats.ox.ac.uk
> > Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
> > University of Oxford,             Tel:  +44 1865 272861 (self)
> > 1 South Parks Road,                     +44 1865 272866 (PA)
> > Oxford OX1 3TG, UK                Fax:  +44 1865 272595
> >
> >
> 
> 

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From dmurdoch at pair.com  Tue Oct  7 15:28:58 2003
From: dmurdoch at pair.com (Duncan Murdoch)
Date: Tue, 07 Oct 2003 09:28:58 -0400
Subject: [R] "is.na(x) <- TRUE" versus "x <- NA" (was: Beginner's query -
	segmentation fault)
In-Reply-To: <3F82B386.2020308@statistik.uni-dortmund.de>
References: <Pine.LNX.4.44.0310071259390.28305-100000@env-pc-phd13>
	<3F82B386.2020308@statistik.uni-dortmund.de>
Message-ID: <dff5ovki12ugbtoqo7mm5ionm5afe3smor@4ax.com>

On Tue, 07 Oct 2003 14:37:26 +0200, Uwe Ligges
<ligges at statistik.uni-dortmund.de> wrote :

>2. Use "is.na(x) <- TRUE" instead of "x <- NA":
>   is.na(temp[temp[ ,"t1"] == -999.00, "t1"]) <- TRUE

I hadn't heard this advice before.  The online help ?is.na gives this
cryptic advice:

     Function 'is.na<-' may provide a safer way to set missingness. It
     behaves differently for factors, for example.

I assume it means "safer than assigning NA", and "differently than
assigning NA", but how exactly is it safer, and how is it different?

Duncan



From alanc at umit.maine.edu  Tue Oct  7 15:46:55 2003
From: alanc at umit.maine.edu (Alan Cobo-Lewis)
Date: Tue, 07 Oct 2003 09:46:55 -0400
Subject: [R] Re: Use of the Foreign package to import Stata files
In-Reply-To: <Pine.A41.4.58.0310061439450.153520@homer12.u.washington.edu>
References: <fc.004c4d191400d602004c4d191400d602.1400e749@umit.maine.edu>
	<Pine.A41.4.58.0310061439450.153520@homer12.u.washington.edu>
Message-ID: <fc.004c4d1914040c57004c4d191400d602.1404153e@umit.maine.edu>


Regarding an old version of foreign allegedly being able to read the file at
http://www.ats.ucla.edu/stat/stata/examples/rwg/concord1.dta
Thomas Lumley <tlumley at u.washington.edu> responded to my call for help:
>I don't know why it used to work.  The file begins with 'h' (0x68), which
>isn't in my list of valid Stata file types.  On the other hand, Stata has
>no problem with it.  It's possible that it is a version 4 file (which
>would make sense, since version 5 file start with 'i' and increase from
>there) but that wouldn't explain why it used to work.
>
>With the stataread package (which predates 0.5-x of foreign) I get "Not a
>Stata version 5-7 data file".

Thomas Lumley <tlumley at u.washington.edu> continued:
>I just found an old 1.6.2 (Mac OS/Darwin) lying around and it doesn't
>recognize the file as a stata file (with foreign 0.5-8)

Thomas,

My apologies. It turns out that using R 1.6.2 and an old foreign library, I *can't* read the concord1.dta file at that url, though I *can* read a concord1.dta file that I'd downloaded several months ago. So either the archive at www.ats.ucla.edu
overwrote good files with bad, or maybe their site doesn't match an alternate source for the same data files. Either way, I'm sorry to have sent you on a wild goose chase. The error was not in your package. Thank you for your prompt replies.

alan

--
Alan B. Cobo-Lewis, Ph.D.		(207) 581-3840 tel
Department of Psychology		(207) 581-6128 fax
University of Maine
Orono, ME 04469-5742     		alanc at maine.edu

http://www.umaine.edu/visualperception



From ligges at statistik.uni-dortmund.de  Tue Oct  7 16:00:35 2003
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Tue, 07 Oct 2003 16:00:35 +0200
Subject: [R] Beginner's query - segmentation fault
In-Reply-To: <Pine.LNX.4.44.0310071358140.28305-100000@env-pc-phd13>
References: <Pine.LNX.4.44.0310071358140.28305-100000@env-pc-phd13>
Message-ID: <3F82C703.5010802@statistik.uni-dortmund.de>

Laura Quinn wrote:

> thanks, have used
> 
> temp [temp==0]<- NA

Please use
   is.na(temp[temp==0]) <- TRUE


> and this seems to have worked, though it won't let me access individual
> columns (ie temp$t1 etc) 

No! temp$t1 is a list element or column of a data.frame, but not a 
column of a matrix. *PLEASE*, read manuals, help pages, or books on R 
how to use index / extract elements.

Please read my previous answer on how to access individual columns.


> to work on - is there any real advantage in using
> a matrix, or would i be better advised to deal with dataframes? (I have
> double checked and temp is currently a matrix).

Working on matrices is supposed to be faster. But matrices have the 
restriction of one data type for all columns (e.g. numeric).

Uwe Ligges


> On Tue, 7 Oct 2003, Prof Brian Ripley wrote:
> 
> 
>>On Tue, 7 Oct 2003, Laura Quinn wrote:
>>
>>
>>>I am dealing with a huge matrix in R (20 columns, 54000 rows) and have
>>>lots of missing values within the dataset which are currently displayed as
>>>the value "-999.00" I am trying to create a new matrix (or change the
>>>existing one) to display these values as "NA" so that I can then perform
>>>the necessary analysis on the columns within the matrix.
>>>
>>>The matrix name is temp and the column names are t1 to t20 inclusive.
>>>
>>>I have tried the following command:
>>>
>>>temp$t1[temp$t1 == -999.00] <- NA
>>>
>>>and it returns a segmentation fault, can someone tell me what I am doing
>>>wrong?
>>
>>Well, R should not segfault, so there is bug here somewhere.  However, I
>>don't think what you have described can actually work. Is temp really a
>>matrix?  If so temp$t1 will return NULL, and you should get an error
>>message.
>>
>>
>>If temp is a matrix
>>
>>temp[temp == -999.00] <- NA
>>
>>will do what you want.
>>
>>
>>If as is more likely temp is a data frame with all columns numeric,
>>there are several ways to do this, e.g.
>>
>>temp[] <- lapply(temp, function(x) ifelse(x == -999, NA, x))
>>
>>temp[as.matrix(temp) == -999] <- NA  # only in recent versions of R
>>
>>as well as explicit looping over columns.
>>
>>--
>>Brian D. Ripley,                  ripley at stats.ox.ac.uk
>>Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
>>University of Oxford,             Tel:  +44 1865 272861 (self)
>>1 South Parks Road,                     +44 1865 272866 (PA)
>>Oxford OX1 3TG, UK                Fax:  +44 1865 272595
>>
>>
> 
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help



From CMiller at PICR.man.ac.uk  Tue Oct  7 15:59:40 2003
From: CMiller at PICR.man.ac.uk (Crispin Miller)
Date: Tue, 7 Oct 2003 14:59:40 +0100
Subject: [R] .First.lib doesn't appear to be running after calling library()
Message-ID: <BAA35444B19AD940997ED02A6996AAE0B5C1C3@sanmail.picr.man.ac.uk>

Hi - so I've dusted off the C bits of my brain and gotten a library written for my package...
 
It passes R CMD check ok, and I've put a file called '.First.lib.R' in the pacakge's 'R'  subdirectory. Its permissions are 644.

It says:

.First.lib <- function(lib,pkg) {
   library.dynam("foo",pkg,lib);
   require(affy,quietly=TRUE);
}


I build and INSTALL the package, start R and then call library(foo).
I deduce that my '.First.lib' isn't running because the affy library doesn't get loaded - and neither does my dynamic library (which complies ok and results in 'foo.so' being put in the 'src/' directory of the package)...

Any ideas what I'm doing wrong?

Crispin
 
--------------------------------------------------------

 
This email is confidential and intended solely for the use o...{{dropped}}



From tlumley at u.washington.edu  Tue Oct  7 16:09:58 2003
From: tlumley at u.washington.edu (Thomas Lumley)
Date: Tue, 7 Oct 2003 07:09:58 -0700 (PDT)
Subject: [R] Adjusting for within-cluster correlation: robcov() in
	Design-package and 'ids' in survey-package
In-Reply-To: <3F82BAA1.18649.102A170@localhost>
References: <3F82BAA1.18649.102A170@localhost>
Message-ID: <Pine.A41.4.58.0310070708430.129748@homer40.u.washington.edu>

On Tue, 7 Oct 2003, Bernd Weiss wrote:

> Dear all,
>
> I would like to know if it possible to use the the robcov()-command in
> the Design- package in order to obtain a robust variance-estimate that
> adjusts for within-cluster correlation. Does the ids-option in the
> survey-package the same job?
>

Yes and roughly yes.

The survey package version also can handle stratification and sampling
from a finite population, making it more complicated to use.


	-thomas



From ligges at statistik.uni-dortmund.de  Tue Oct  7 16:11:49 2003
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Tue, 07 Oct 2003 16:11:49 +0200
Subject: [R] Re: "is.na(x) <- TRUE" versus "x <- NA"
In-Reply-To: <dff5ovki12ugbtoqo7mm5ionm5afe3smor@4ax.com>
References: <Pine.LNX.4.44.0310071259390.28305-100000@env-pc-phd13>
	<3F82B386.2020308@statistik.uni-dortmund.de>
	<dff5ovki12ugbtoqo7mm5ionm5afe3smor@4ax.com>
Message-ID: <3F82C9A5.4050403@statistik.uni-dortmund.de>

Duncan Murdoch wrote:

> On Tue, 07 Oct 2003 14:37:26 +0200, Uwe Ligges
> <ligges at statistik.uni-dortmund.de> wrote :
> 
> 
>>2. Use "is.na(x) <- TRUE" instead of "x <- NA":
>>  is.na(temp[temp[ ,"t1"] == -999.00, "t1"]) <- TRUE
> 
> 
> I hadn't heard this advice before.  The online help ?is.na gives this
> cryptic advice:
> 
>      Function 'is.na<-' may provide a safer way to set missingness. It
>      behaves differently for factors, for example.
> 
> I assume it means "safer than assigning NA", and "differently than
> assigning NA", but how exactly is it safer, and how is it different?

Hmmmm. I was sure I had constructed an problematic example for a course 
I hold last year in Dortmund, but I cannot find it again. Maybe 
something for NA assignmets (into character vectors, I remember?) has 
been fixed in the meantime.

Anyway, I have had the experience that there are cases, where "is.na(x) 
<- TRUE" is the more secure way of doing it.

Additionally, it's mentioned in ?NA and in "S Programming", for example.

Uwe



From tlumley at u.washington.edu  Tue Oct  7 16:16:42 2003
From: tlumley at u.washington.edu (Thomas Lumley)
Date: Tue, 7 Oct 2003 07:16:42 -0700 (PDT)
Subject: [R] use of glm and cubic splines
In-Reply-To: <000601c38ca4$e038cd50$3904bec1@pcniel>
References: <000601c38ca4$e038cd50$3904bec1@pcniel>
Message-ID: <Pine.A41.4.58.0310070714170.129748@homer40.u.washington.edu>

On Tue, 7 Oct 2003, Niel Hens wrote:

>
> When I fit a model like glm(gb~bs(age,df=6)), I obtain 7 estimates. Can
> anybody explain me where they stand for?

Well, yes and no.  They are coefficients of terms in the b-spline basis,
but that probably isn't very illuminating.

You can use termplot() on your fitted model to see the smooth curve that
has been fitted and is specified by these estimates. The only splines
where you get simple interpretations of individual coefficients are
linear splines.

	-thomas



From andy_liaw at merck.com  Tue Oct  7 16:32:29 2003
From: andy_liaw at merck.com (Liaw, Andy)
Date: Tue, 07 Oct 2003 10:32:29 -0400
Subject: [R] .First.lib doesn't appear to be running after calling
	lib rary()
Message-ID: <3A822319EB35174CA3714066D590DCD50205CC3A@usrymx25.merck.com>

>From the R-exts manual:

====
The R subdirectory contains R code files. The code files to be installed
must start with a (lower or upper case) letter and have one of the
extensions .R, .S, .q, .r, or .s. We recommend using .R, as this extension
seems to be not used by any other software. It should be possible to read in
the files using source(), so R objects must be created by assignments. Note
that there need be no connection between the name of the file and the R
objects created by it. If necessary, one of these files (historically zzz.R)
should use library.dynam() inside .First.lib() to load compiled code. 
====

So the problem is the "." that the filename starts with.

HTH,
Andy

> From: Crispin Miller [mailto:CMiller at PICR.man.ac.uk] 
> 
> Hi - so I've dusted off the C bits of my brain and gotten a 
> library written for my package...
>  
> It passes R CMD check ok, and I've put a file called 
> '.First.lib.R' in the pacakge's 'R'  subdirectory. Its 
> permissions are 644.
> 
> It says:
> 
> .First.lib <- function(lib,pkg) {
>    library.dynam("foo",pkg,lib);
>    require(affy,quietly=TRUE);
> }
> 
> 
> I build and INSTALL the package, start R and then call 
> library(foo). I deduce that my '.First.lib' isn't running 
> because the affy library doesn't get loaded - and neither 
> does my dynamic library (which complies ok and results in 
> 'foo.so' being put in the 'src/' directory of the package)...
> 
> Any ideas what I'm doing wrong?
> 
> Crispin
>  
> --------------------------------------------------------
> 
>  
> This email is confidential and intended solely for the use 
> o...{{dropped}}
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list 
> https://www.stat.math.ethz.ch/mailman/listinfo> /r-help
>



From CMiller at PICR.man.ac.uk  Tue Oct  7 16:25:42 2003
From: CMiller at PICR.man.ac.uk (Crispin Miller)
Date: Tue, 7 Oct 2003 15:25:42 +0100
Subject: [R] .First.lib doesn't appear to be running after calling lib
	rary()
Message-ID: <BAA35444B19AD940997ED02A6996AAE0B5C1C4@sanmail.picr.man.ac.uk>

Thanks - it is indeed the first '.' that's the problem...
Crispin


> -----Original Message-----
> From: Liaw, Andy [mailto:andy_liaw at merck.com]
> Sent: 07 October 2003 15:21
> To: Crispin Miller
> Subject: RE: [R] .First.lib doesn't appear to be running after calling
> lib rary()
> 
> 
> I put .First.lib in the file "zzz.R", and it works for me.  
> My guess is that
> the problem is using a filename that starts with ".".  I see 
> many packages
> put the .First.lib in zzz.R, so I just follow that.
> 
> HTH,
> Andy
> 
> > -----Original Message-----
> > From: Crispin Miller [mailto:CMiller at PICR.man.ac.uk] 
> > Sent: Tuesday, October 07, 2003 10:00 AM
> > To: R-help (E-mail)
> > Subject: [R] .First.lib doesn't appear to be running after 
> > calling library()
> > 
> > 
> > Hi - so I've dusted off the C bits of my brain and gotten a 
> > library written for my package...
> >  
> > It passes R CMD check ok, and I've put a file called 
> > '.First.lib.R' in the pacakge's 'R'  subdirectory. Its 
> > permissions are 644.
> > 
> > It says:
> > 
> > .First.lib <- function(lib,pkg) {
> >    library.dynam("foo",pkg,lib);
> >    require(affy,quietly=TRUE);
> > }
> > 
> > 
> > I build and INSTALL the package, start R and then call 
> > library(foo). I deduce that my '.First.lib' isn't running 
> > because the affy library doesn't get loaded - and neither 
> > does my dynamic library (which complies ok and results in 
> > 'foo.so' being put in the 'src/' directory of the package)...
> > 
> > Any ideas what I'm doing wrong?
> > 
> > Crispin
> >  
> > --------------------------------------------------------
> > 
> >  
> > This email is confidential and intended solely for the use 
> > o...{{dropped}}
> > 
> > ______________________________________________
> > R-help at stat.math.ethz.ch mailing list 
> > https://www.stat.math.ethz.ch/mailman/listinfo> /r-help
> > 
> 
>
 
--------------------------------------------------------

 
This email is confidential and intended solely for the use o...{{dropped}}



From tblackw at umich.edu  Tue Oct  7 16:33:28 2003
From: tblackw at umich.edu (Thomas W Blackwell)
Date: Tue, 7 Oct 2003 10:33:28 -0400 (EDT)
Subject: [R] Problem getting an ifelse statment to work
In-Reply-To: <3A822319EB35174CA3714066D590DCD50205CC31@usrymx25.merck.com>
References: <3A822319EB35174CA3714066D590DCD50205CC31@usrymx25.merck.com>
Message-ID: <Pine.SOL.4.58.0310071019330.2491@timepilot.gpcc.itd.umich.edu>

Greg  -

I am puzzled that the total counts in  table(qs2)  and  table(qs9)
could be different, if these are in fact two columns from the same
data frame.  I'm guessing that there are NAs in one or both columns,
in addition to the digits 1,2,3,4, and that  table()  by default
does not show them.  (It doesn't.)

If there are NAs in either column, I would expect both  ifelse()
and the logic in both Andy's and Richard's code to not produce the
result you have in mind.  Here's something which might work as is,
or might need some extension:

new <- ifelse(is.na(qs2), ifelse(is.na(qs9), 0, qs9), qs2)

You can find out how many NAs there are in column qs2 by doing

sum(is.na(qs2))

Do let us know what finally works.

-  tom blackwell  -  u michigan medical school  -  ann arbor  -

On Mon, 6 Oct 2003, Liaw, Andy wrote:

> This is a "long" way; i.e., not necessarily efficient:
>
> > qs2
>  [1] 2 1 1 4 4 4 1 1 1 4 2 4 3 1 4 3 3 2 4 3
> > qs9
>  [1] 4 4 1 3 4 3 1 3 1 4 1 2 3 3 4 4 1 4 2 3
> > decision <- function(a, b) {
> +     if (a == 1 || b == 1) return(1)
> +     if (a == 2 || b == 2) return(2)
> +     if (a == 3 || b == 3) return(3)
> +     if (a == 4 || b == 4) return(4)
> +     NA
> + }
> > mapply(decision, qs2, qs9)
>  [1] 2 1 1 3 4 3 1 1 1 4 1 2 3 1 4 3 1 2 2 3
>
> Hope this is what you want.
>
> Andy
>
> > -----Original Message-----
> > From: Greg Blevins [mailto:gblevins at mn.rr.com]
> > Sent: Monday, October 06, 2003 11:21 PM
> > Subject: [R] Problem getting an ifelse statment to work
> >
> > I trust I have a simple request.  I have a dataframe which
> > among its contents are two variables, qs2 and qs9, which have
> > the following frequencies.
> >
> > > table(qs2)
> > qs2
> >  1  2  3  4
> > 40 22 11 29
> >
> > > table(qs9)
> > qs9
> >   1   2   3   4
> > 162 172  91 179
> >
> > I simply want to create a new variable which I have called
> > SchCode that would be filled based on the following logic
> > (written in Systat syntax):
> >
> > if qs2 = 1 or qs9 = 1 then let SchCode = 1
> > if qs2 = 2 or qs9 = 2 then let SchCode = 2
> > if qs2 = 3 or qs9 = 3 then let SchCode = 3
> > if qs2 = 4 or qs9 = 4 then let SchCode = 4
> >
> > I have looked through my two Ripley texts, searched the
> > R-help, and have tried various ifelse statements, but I
> > cannot get it right.  Help would be appreciated.
> >
> > Thanks,
> > Greg Blevins
> > The Market Solutions Group
> >



From ripley at stats.ox.ac.uk  Tue Oct  7 16:38:30 2003
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Tue, 7 Oct 2003 15:38:30 +0100 (BST)
Subject: [R] .First.lib doesn't appear to be running after calling
	library()
In-Reply-To: <BAA35444B19AD940997ED02A6996AAE0B5C1C3@sanmail.picr.man.ac.uk>
Message-ID: <Pine.LNX.4.44.0310071534400.3782-100000@gannet.stats>

On Tue, 7 Oct 2003, Crispin Miller wrote:

> Hi - so I've dusted off the C bits of my brain and gotten a library written for my package...
>  
> It passes R CMD check ok, and I've put a file called '.First.lib.R' in the pacakge's 'R'  subdirectory. Its permissions are 644.
> 
> It says:
> 
> .First.lib <- function(lib,pkg) {
>    library.dynam("foo",pkg,lib);
>    require(affy,quietly=TRUE);
> }
> 
> 
> I build and INSTALL the package, start R and then call library(foo).
> I deduce that my '.First.lib' isn't running because the affy library doesn't get loaded - and neither does my dynamic library (which complies ok and results in 'foo.so' being put in the 'src/' directory of the package)...
> 
> Any ideas what I'm doing wrong?

Don't use a file name starting with a dot.  Assuming this is Unix and R 
1.7.1, INSTALL does

    Rfiles=`LC_COLLATE=C ls R/*.[RSqrs] R/${R_OSTYPE}/*.[RSqrs] 2>/dev/null`

and ls will omit file names starting with a dot.

`Writing R Extensions' says

The @file{R} subdirectory contains @R{} code files.  The code files to
be installed must start with a (lower or upper case) letter and have one
of the extensions @file{.R}, @file{.S}, @file{.q}, @file{.r}, or
@file{.s}. 

but it seems we don't quite enforce that on either Unix nor Windows.

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From yao6889 at msmailhub.oulan.ou.edu  Tue Oct  7 16:41:24 2003
From: yao6889 at msmailhub.oulan.ou.edu (Yao, Minghua)
Date: Tue, 7 Oct 2003 09:41:24 -0500 
Subject: [R] Sorting matrix or data frame
Message-ID: <FC0CEBD77311DA499A67ADB355A24FA20396ADCC@mail4.oulan.ou.edu>

Dear all,

Could anyone please tell me how to sort a matrix or a data frame against a
column/row/component?
Many thanks.

-MY



From simon at stats.gla.ac.uk  Tue Oct  7 16:50:12 2003
From: simon at stats.gla.ac.uk (Simon Wood)
Date: Tue, 7 Oct 2003 15:50:12 +0100 (BST)
Subject: [R] use of glm and cubic splines
In-Reply-To: <Pine.A41.4.58.0310070714170.129748@homer40.u.washington.edu>
References: <000601c38ca4$e038cd50$3904bec1@pcniel>
	<Pine.A41.4.58.0310070714170.129748@homer40.u.washington.edu>
Message-ID: <Pine.SOL.4.58.0310071547340.12809@moon.stats.gla.ac.uk>

> >
> > When I fit a model like glm(gb~bs(age,df=6)), I obtain 7 estimates. Can
> > anybody explain me where they stand for?
>
> Well, yes and no.  They are coefficients of terms in the b-spline basis,
> but that probably isn't very illuminating.
>
> You can use termplot() on your fitted model to see the smooth curve that
> has been fitted and is specified by these estimates. The only splines
> where you get simple interpretations of individual coefficients are
> linear splines.

- if you use gam() from package mgcv and specify the "cr" basis then the
coefficients are the height of the spline at the knots (this only works
for smooths of one variable and "cr" is not the default basis).

Simon

_____________________________________________________________________
> Simon Wood simon at stats.gla.ac.uk        www.stats.gla.ac.uk/~simon/
>>  Department of Statistics, University of Glasgow, Glasgow, G12 8QQ
>>>   Direct telephone: (0)141 330 4530          Fax: (0)141 330 4814



From jmacdon at med.umich.edu  Tue Oct  7 16:50:53 2003
From: jmacdon at med.umich.edu (James MacDonald)
Date: Tue, 07 Oct 2003 10:50:53 -0400
Subject: [R] R-1.8.0 memory.limit()
Message-ID: <sf829a96.004@med-gwia-02a.med.umich.edu>

Using R-1.8.0 (d/l and compiled on 2003-10-01) on WinXP, I seem to be
unable to determine the maximum memory allocated to R. The help still
says to use memory.limit(size=NA), but this returns the value NA.

In addition, I have set --max-mem-size=2G but I run out of memory
somewhere around 500Mb (which is why I am trying to find out how much
memory is allocated). I don't have any other programs open, so I should
certainly be able to address more memory than that.

Any ideas?

TIA

Jim



James W. MacDonald
Affymetrix and cDNA Microarray Core
University of Michigan Cancer Center
1500 E. Medical Center Drive
7410 CCGC
Ann Arbor MI 48109
734-647-5623



From macq at llnl.gov  Tue Oct  7 17:02:08 2003
From: macq at llnl.gov (Don MacQueen)
Date: Tue, 7 Oct 2003 08:02:08 -0700
Subject: [R] Installing R with all packages
In-Reply-To: <Pine.GSO.3.95q.1031007095815.2810C-100000@sun15.math.uni-hamburg.de>
References: <Pine.GSO.3.95q.1031007095815.2810C-100000@sun15.math.uni-hamburg.de>
Message-ID: <p05210601bba882df29cd@[128.115.153.6]>

If it's really true that you will be the only user of R, then it 
might be easier to build it in some place where you do have the 
necessary permissions. You would then follow the suggestion in the 
provided INSTALL file (from R 1.7.1):

---- quote ----
INSTALLATION

You do not need to install R to run it: you can run R by the script
`bin/R' which you can link or copy to any convenient place in your path.
---- end quote ----


Considering that R itself is updated fairly often, your sysadmin may 
appreciate if you are able to take care of maintaining R yourself.

If you are not the only user, your sysadmin might be willing to make 
the necessary copy or link in some directory in the standard user's 
path, leaving the installation in some place you own. Then you can do 
all the updates and package installations, and the sysadmin only 
needs to update the links/copies when R itself is updated. That's how 
we do it here, and it works very well.

-Don

At 10:27 AM +0200 10/7/03, Christian Hennig wrote:
>Hi,
>
>I want R to be installed on a UNIX network (Solaris). I am not the system
>administrator and so I cannot do it myself.
>The system administrator wants to know which packages I want, and it may be
>a lot.
>Is there an easy way to download and install all packages at once?
>Is it a good idea? (There is a good chance that I do not need the some
>few packages that do not install well during such a procedure, and so I would
>like to tell the sysadmin to do it even if it will not be 100% successful).
>
>Unfortunately it may be even more compicated, because it may be (I was not
>able to find it out absolutely surely) that our net
>architecture does not allow to do it via
>install.packages from within R. So what is the easiest way to
>do it from outside R?
>
>If it is not a good idea to install all packages
>at once, what is the easiest way to
>download and install a list of, say, 30 packages? R CMD INSTALL accepts a
>list as input; but how to download a list of 30 packages at once?
>
>(The sysadmin would really appreciate if we could do it in a way that
>later additions of packages are reduced to a minimum.)
>
>Best,
>Christian
>
>
>***********************************************************************
>Christian Hennig
>Fachbereich Mathematik-SPST/ZMS, Universitaet Hamburg
>hennig at math.uni-hamburg.de, http://www.math.uni-hamburg.de/home/hennig/
>#######################################################################
>ich empfehle www.boag-online.de
>
>______________________________________________
>R-help at stat.math.ethz.ch mailing list
>https://www.stat.math.ethz.ch/mailman/listinfo/r-help


-- 
--------------------------------------
Don MacQueen
Environmental Protection Department
Lawrence Livermore National Laboratory
Livermore, CA, USA



From ligges at statistik.uni-dortmund.de  Tue Oct  7 17:36:31 2003
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Tue, 07 Oct 2003 17:36:31 +0200
Subject: [R] Sorting matrix or data frame
In-Reply-To: <FC0CEBD77311DA499A67ADB355A24FA20396ADCC@mail4.oulan.ou.edu>
References: <FC0CEBD77311DA499A67ADB355A24FA20396ADCC@mail4.oulan.ou.edu>
Message-ID: <3F82DD7F.4020900@statistik.uni-dortmund.de>

Yao, Minghua wrote:

> Dear all,
> 
> Could anyone please tell me how to sort a matrix or a data frame against a
> column/row/component?

See ?order

Uwe Ligges

> Many thanks.
> 
> -MY
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help



From borgulya at gyer2.sote.hu  Tue Oct  7 17:49:43 2003
From: borgulya at gyer2.sote.hu (Gabor Borgulya)
Date: Tue, 07 Oct 2003 15:49:43 -0000
Subject: [R] Sorting matrix or data frame
In-Reply-To: <FC0CEBD77311DA499A67ADB355A24FA20396ADCC@mail4.oulan.ou.edu>
References: <FC0CEBD77311DA499A67ADB355A24FA20396ADCC@mail4.oulan.ou.edu>
Message-ID: <1065541877.2698.12.camel@catv-d5de952e>

Dear Minghua Yao,

I suggest you looking up the archives of the mailing list. The answers
to the following message made things clear to me:


Wayne Jones <JonesW at kssg.com>
                         C?mzett: 
R-Help <R-help at stat.math.ethz.ch>
                           T?rgy: 
[R] Sorting a data frame
                           D?tum: 
Wed, 16 Jul 2003 14:42:09 +0100

G?bor



From adler at lifesci.ucsb.edu  Tue Oct  7 18:05:42 2003
From: adler at lifesci.ucsb.edu (Peter Adler)
Date: Tue, 07 Oct 2003 09:05:42 -0700
Subject: [R] multiple comparisons
Message-ID: <6.0.0.22.1.20031007090347.01adde70@lifesci.ucsb.edu>

I'm having trouble finding an R equivalent to the S-Plus "multicomp" 
function, which does post-hoc comparisons of treatments means in 
ANOVAs.  Am I missing something obvious?

Thanks, Peter
------------------------
Peter Adler, PhD
Dept. Ecology, Evolution and Marine Biology
University of California
Santa Barbara, CA 93106
tel: (805) 893-7416



From ligges at statistik.uni-dortmund.de  Tue Oct  7 18:28:57 2003
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Tue, 07 Oct 2003 18:28:57 +0200
Subject: [R] R-1.8.0 memory.limit()
In-Reply-To: <sf829a96.004@med-gwia-02a.med.umich.edu>
References: <sf829a96.004@med-gwia-02a.med.umich.edu>
Message-ID: <3F82E9C9.5000108@statistik.uni-dortmund.de>

James MacDonald wrote:

> Using R-1.8.0 (d/l and compiled on 2003-10-01) on WinXP, I seem to be
> unable to determine the maximum memory allocated to R. The help still
> says to use memory.limit(size=NA), but this returns the value NA.
> 
> In addition, I have set --max-mem-size=2G but I run out of memory
> somewhere around 500Mb (which is why I am trying to find out how much
> memory is allocated). I don't have any other programs open, so I should
> certainly be able to address more memory than that.
> 
> Any ideas?

Yes. 2GB is a bit more than the possible theoretical maximal value for a 
single process on 32-bit Windows. Instead, try starting with, e.g., 
"--max-mem-size=1700M" and you'll get

 > memory.limit()
[1] 1782579200

Uwe Ligges


> TIA
> 
> Jim
> 
> 
> 
> James W. MacDonald
> Affymetrix and cDNA Microarray Core
> University of Michigan Cancer Center
> 1500 E. Medical Center Drive
> 7410 CCGC
> Ann Arbor MI 48109
> 734-647-5623
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help



From baron at psych.upenn.edu  Tue Oct  7 18:47:26 2003
From: baron at psych.upenn.edu (Jonathan Baron)
Date: Tue, 7 Oct 2003 12:47:26 -0400
Subject: [R] multiple comparisons
In-Reply-To: <6.0.0.22.1.20031007090347.01adde70@lifesci.ucsb.edu>
References: <6.0.0.22.1.20031007090347.01adde70@lifesci.ucsb.edu>
Message-ID: <20031007164726.GB19697@mail2.sas.upenn.edu>

On 10/07/03 09:05, Peter Adler wrote:
>I'm having trouble finding an R equivalent to the S-Plus "multicomp" 
>function, which does post-hoc comparisons of treatments means in 
>ANOVAs.  Am I missing something obvious?

The package called multcomp?  I don't know if it is the same.

-- 
Jonathan Baron, Professor of Psychology, University of Pennsylvania
Home page:            http://www.sas.upenn.edu/~baron
R page:               http://finzi.psych.upenn.edu/



From christoph.lehmann at gmx.ch  Tue Oct  7 19:19:00 2003
From: christoph.lehmann at gmx.ch (Christoph Lehmann)
Date: Tue, 07 Oct 2003 19:19:00 +0200
Subject: [R] plot width in Sweave
Message-ID: <1065547140.2934.9.camel@christophl>

Hi

I didn't find this in the manual: I need to change the width of a plot
while I use sweave, so which command/parameters should I insert below,
to change the width of a plot

\begin{figure}[htbp]
  \begin{center}
<<echo=TRUE, fig=TRUE>>=
plot(Re(q),ylab ="",type="o",col="blue",lwd=1, sub=mystring)
@     
    \caption{Original stick function (stimulus train)}
  \end{center}
\end{figure}

many thanks

christoph
-- 
Christoph Lehmann <christoph.lehmann at gmx.ch>



From zeileis at ci.tuwien.ac.at  Tue Oct  7 19:57:24 2003
From: zeileis at ci.tuwien.ac.at (Achim Zeileis)
Date: Tue, 7 Oct 2003 19:57:24 +0200
Subject: [R] plot width in Sweave
In-Reply-To: <1065547140.2934.9.camel@christophl>
References: <1065547140.2934.9.camel@christophl>
Message-ID: <200310071757.h97HvOCj023878@thorin.ci.tuwien.ac.at>

On Tuesday 07 October 2003 19:19, Christoph Lehmann wrote:

> Hi
>
> I didn't find this in the manual: I need to change the width of a
> plot while I use sweave, so which command/parameters should I insert
> below, to change the width of a plot
>
> \begin{figure}[htbp]
>   \begin{center}
> <<echo=TRUE, fig=TRUE>>=
> plot(Re(q),ylab ="",type="o",col="blue",lwd=1, sub=mystring)
> @
>     \caption{Original stick function (stimulus train)}
>   \end{center}
> \end{figure}


Depends what exactly you want to change. If you to change the width of 
the eps/pdf file you could say

  <<echo=TRUE,fig=TRUE,height=4,width=6>>=

as you would do in a postscript() or pdf() call in R.


If you want to increase the width of the plot in LaTeX, you could do

  \setkeys{Gin}{width=0.8\textwidth}

which corresponds to the setting of

  \includegraphics[width=0.8\textwidth]{mygraphic}

hth,
Z


> many thanks
>
> christoph



From ripley at stats.ox.ac.uk  Tue Oct  7 20:32:22 2003
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Tue, 7 Oct 2003 19:32:22 +0100 (BST)
Subject: [R] multiple comparisons
In-Reply-To: <20031007164726.GB19697@mail2.sas.upenn.edu>
Message-ID: <Pine.LNX.4.44.0310071928260.4208-100000@gannet.stats>

On Tue, 7 Oct 2003, Jonathan Baron wrote:

> On 10/07/03 09:05, Peter Adler wrote:
> >I'm having trouble finding an R equivalent to the S-Plus "multicomp" 
> >function, which does post-hoc comparisons of treatments means in 
> >ANOVAs.  Am I missing something obvious?
> 
> The package called multcomp?  I don't know if it is the same.

It is not, but can often be used in a different way to get similar
results.  There is also function TukeyHSD() in base R (which is often the 
simplest competitive method).

THe MASS scripts show some of the ways to use these in R to emulate
S-PLUS's multicomp.

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From rab at nauticom.net  Tue Oct  7 21:39:16 2003
From: rab at nauticom.net (Rick Bilonick)
Date: Tue, 07 Oct 2003 15:39:16 -0400
Subject: [R] Still Cannot Install rimage in R-1.7.1 (RH 9.0) Even With fftw
	Installed
Message-ID: <3F831664.5020003@nauticom.net>

I'm still having problems installing rimage - the installation can't 
find the fftw headers. As suggested, I installed the fftw rpm (for RH 9 
from freshrpms). It installed without any errors or warnings. Yet I get 
exactly the same error message - it can't find the fftw headers.

What do I have to do to get the headers?

Rick B.



From jmacdon at med.umich.edu  Tue Oct  7 21:54:24 2003
From: jmacdon at med.umich.edu (James MacDonald)
Date: Tue, 07 Oct 2003 15:54:24 -0400
Subject: [R] R-1.8.0 memory.limit()
Message-ID: <sf82e1c4.052@med-gwia-01a.med.umich.edu>

Thanks Uwe,

What is the theoretical limit for 32 bit Windows anyway?

BTW, --max-mem-size=2000M works (although maybe not all usable?)

> memory.limit()
[1] 2097152000

In R-1.7.1, --max-mem-size=2G *did* work (I get the same results as
above).

Jim



James W. MacDonald
Affymetrix and cDNA Microarray Core
University of Michigan Cancer Center
1500 E. Medical Center Drive
7410 CCGC
Ann Arbor MI 48109
734-647-5623

>>> Uwe Ligges <ligges at statistik.uni-dortmund.de> 10/07/03 12:28PM >>>
James MacDonald wrote:

> Using R-1.8.0 (d/l and compiled on 2003-10-01) on WinXP, I seem to
be
> unable to determine the maximum memory allocated to R. The help
still
> says to use memory.limit(size=NA), but this returns the value NA.
> 
> In addition, I have set --max-mem-size=2G but I run out of memory
> somewhere around 500Mb (which is why I am trying to find out how
much
> memory is allocated). I don't have any other programs open, so I
should
> certainly be able to address more memory than that.
> 
> Any ideas?

Yes. 2GB is a bit more than the possible theoretical maximal value for
a 
single process on 32-bit Windows. Instead, try starting with, e.g., 
"--max-mem-size=1700M" and you'll get

 > memory.limit()
[1] 1782579200

Uwe Ligges


> TIA
> 
> Jim
> 
> 
> 
> James W. MacDonald
> Affymetrix and cDNA Microarray Core
> University of Michigan Cancer Center
> 1500 E. Medical Center Drive
> 7410 CCGC
> Ann Arbor MI 48109
> 734-647-5623
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help



From pgilbert at bank-banque-canada.ca  Tue Oct  7 21:59:31 2003
From: pgilbert at bank-banque-canada.ca (Paul Gilbert)
Date: Tue, 07 Oct 2003 15:59:31 -0400
Subject: [R] subset VAR time series models
In-Reply-To: <20031004160021.GA29293@sonny.eddelbuettel.com>
References: <3F7EE745.9020608@prozentor.de>
	<20031004160021.GA29293@sonny.eddelbuettel.com>
Message-ID: <3F831B23.9020107@bankofcanada.ca>



Dirk Eddelbuettel wrote:

>On Sat, Oct 04, 2003 at 05:29:09PM +0200, Harald Bartel wrote:
>  
>
>>I would like to estimate a multivariate VAR model (vector autoregressive 
>>model) with arbitrary individual linear parameter constraints. So far I 
>>used the "ar" command from package ts for unrestricted models. 
>>Unfortunately, this command does not offer the possibility to estimate 
>>subset VAR models.
>>
>>Is there a package or command that I can use?
>>    
>>
>
>No, I don't think so (though it sometimes hard to keep up with the growth in
>the CRAN archive).  Paul Gilbert's dse package may be the closest.
>
dse has some tools that might help you do this. If the constaints are 
very simple (certain parameters set equal to certain values) then it is 
straight forward in dse. Otherwise there will be some work involved.

>
>But contributions are certainly welcome, it would be nice to have a VAR
>package available on CRAN.
>
>Dirk
>
>  
>



From xiao.gang.fan1 at libertysurf.fr  Tue Oct  7 21:58:47 2003
From: xiao.gang.fan1 at libertysurf.fr (Fan)
Date: Tue, 07 Oct 2003 21:58:47 +0200
Subject: [R] Compiling/using R on IBM AIX
Message-ID: <3F831AF7.9050102@libertysurf.fr>

Hi,

I plan to use R on IBM AIX machines (4.x.x), and hoping some feedbacks
from R users on this OS. I've looked at the R search site, my impression
is that there would be very few peoples who are currently using R
on AIX, correct me if I'm wrong.

To compile R on AIX,  I have two little questions:
1. which compiles would be recommanded (IBM C & Fortran compilers vs GNU compiles),
    and are there any restrictions on the version to be used ?
2. is Perl 5.0.0 usable instead of the version 5.6.1 recommanded ?

Many thanks in advance for your help
--
Fan



From jmacdon at med.umich.edu  Tue Oct  7 22:00:06 2003
From: jmacdon at med.umich.edu (James MacDonald)
Date: Tue, 07 Oct 2003 16:00:06 -0400
Subject: [R] Still Cannot Install rimage in R-1.7.1 (RH 9.0) Even
	With fftwInstalled
Message-ID: <sf82e321.019@med-gwia-02a.med.umich.edu>

Aren't the headers usually in the devel package (e.g.,
fftw-devel-2.1.5-0.dag.rh73.i386.rpm)? You might try installing the
devel package too.

HTH

Jim



James W. MacDonald
Affymetrix and cDNA Microarray Core
University of Michigan Cancer Center
1500 E. Medical Center Drive
7410 CCGC
Ann Arbor MI 48109
734-647-5623

>>> Rick Bilonick <rab at nauticom.net> 10/07/03 03:39PM >>>
I'm still having problems installing rimage - the installation can't 
find the fftw headers. As suggested, I installed the fftw rpm (for RH 9

from freshrpms). It installed without any errors or warnings. Yet I get

exactly the same error message - it can't find the fftw headers.

What do I have to do to get the headers?

Rick B.

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://www.stat.math.ethz.ch/mailman/listinfo/r-help



From p.dalgaard at biostat.ku.dk  Tue Oct  7 22:00:24 2003
From: p.dalgaard at biostat.ku.dk (Peter Dalgaard BSA)
Date: Tue, 07 Oct 2003 20:00:24 -0000
Subject: [R] Still Cannot Install rimage in R-1.7.1 (RH 9.0) Even With
	fftw Installed
In-Reply-To: <3F831664.5020003@nauticom.net>
References: <3F831664.5020003@nauticom.net>
Message-ID: <x2he2k4y00.fsf@biostat.ku.dk>

Rick Bilonick <rab at nauticom.net> writes:

> I'm still having problems installing rimage - the installation can't
> find the fftw headers. As suggested, I installed the fftw rpm (for RH
> 9 from freshrpms). It installed without any errors or warnings. Yet I
> get exactly the same error message - it can't find the fftw headers.
> 
> What do I have to do to get the headers?

Is there an fftw-devel package?

-- 
   O__  ---- Peter Dalgaard             Blegdamsvej 3  
  c/ /'_ --- Dept. of Biostatistics     2200 Cph. N   
 (*) \(*) -- University of Copenhagen   Denmark      Ph: (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)             FAX: (+45) 35327907



From heimdal at aracnet.com  Tue Oct  7 23:27:38 2003
From: heimdal at aracnet.com (heimdal@aracnet.com)
Date: Tue, 7 Oct 2003 21:27:38 GMT
Subject: [R] persp or scatterplot3d?
Message-ID: <200310072125.h97LPBpx025549@obsidian.spiritone.com>

Hello,

I have three x-y plots whose interesting data occurs at the same time. When I
place them in the same plot, they tend to obscure each other. I was hoping to
use persp or scatterplot3d to plot them 3-dimensionally, so that the plot is
visually useful. My data is simple:

x, for all three data sets, is a set of 885 ISOdates, where x <- x0 + seq(1,
885).

y is the value of "io", "sys", or "usr" that occurs at time "x". These values
range between 0 - 8. A simple 3-d line plot of each would suffice. Would
scatterplot3d be able to do this, or must I adapt my data to use persp? Any
comments would be helpful. I have data sets and snapshots of the plots in
question if desired.

Thanks for your help,

John



From ok at cs.otago.ac.nz  Tue Oct  7 23:50:02 2003
From: ok at cs.otago.ac.nz (Richard A. O'Keefe)
Date: Wed, 8 Oct 2003 10:50:02 +1300 (NZDT)
Subject: [R] persp or scatterplot3d?
Message-ID: <200310072150.h97Lo2ip219734@atlas.otago.ac.nz>

heimdal at aracnet.com
	[has three x-y plots with the same horizontal axis and similar
	 vertical axes.  He wants to see the three variables together,
	 but if the graphs are plotted in the same rectangle, they
	 obscure each other.  He wants to make a "visually useful" plot.]

Why not just do
    oldpar <- par()
    par(mfrow=c(3,1))
    plot(x, y1)
    plot(x, y2)
    plot(x, y3)
    par(oldpar)


	y is the value of "io", "sys", or "usr" that occurs at time "x".

This sounds like the amount of time being spent by a computer on
various activities.  A plot that might be revealing, although it
would lose the time dimension, would be a ternary plot (see
help("ternaryplot", package="vcd")); the time dimension could be
partly recovered by chopping the times into equal-length subintervals
and plotting a separate ternary diagram for each, in a matrix of
ternary diagrams.



From Sinnwell.Jason at mayo.edu  Wed Oct  8 00:21:17 2003
From: Sinnwell.Jason at mayo.edu (Jason Sinnwell)
Date: Tue, 7 Oct 2003 17:21:17 -0500 (CDT)
Subject: [R] C dynamic library error on Solaris 7
Message-ID: <200310072221.h97MLHt20149@respect.mayo.edu>

I am on Solaris 7, using R_1.7.1 developing packages for both Splus and R.
We have a working R-package for haplo.score and now converting to haplo.stats.

R CMD INSTALL -l /people/biostat3/sinnwell/Rdir/library haplo.stats_1.1.0.tar.gz
works like a charm-- as comparable to our working version of haplo.score.

But problems are when I'm loading the library within R.
Within my package I have three C files, one of which I believe is causing the 
dynamic library error below.  

> library(haplo.stats, lib.loc="/people/biostat3/sinnwell/Rdir/library")
Error in dyn.load(x, as.logical(local), as.logical(now)) : 
        unable to load shared library 
"/people/biostat3/sinnwell/Rdir/library/haplo.stats/libs/haplo.stats.so":
  ld.so.1: /opt/tools/R/current-version/lib/R/bin/R.bin: fatal: relocation 
error: file 
/people/biostat3/sinnwell/Rdir/library/haplo.stats/libs/haplo.stats.so: symbol 
errmsg: referenced symbol not found
Error in library(haplo.stats, lib.loc = 
"/people/biostat3/sinnwell/Rdir/library") : 
        .First.lib failed

When I remove this one C file from the package, though remaining R-functions 
depend on it, the library loads beautifully.  We've followed guidelines in 
Venebles and Ripley's S Programming to handle Calloc, Free, and S_alloc in all 
three files.  I think there is a possibility the problem is there.

I looked at R-help archive emails with similar error messages, and those looked 
like fortran compiler version issues.  Am I also dealing with a compiler issue?  

FYI:  
We have the whole haplo.stats package working in S.
Also, zzz.R for haplo.stats package:

.First.lib <- function(lib, pkg) {
   library.dynam("haplo.stats", pkg, lib)
}


Thank you for your help,

Jason 


-------------------------
Jason P. Sinnwell, M.S.
Mayo Clinic, Rochester
Health Sciences Research
Division of Biostatistics
507.284.3270



From ok at cs.otago.ac.nz  Wed Oct  8 00:27:45 2003
From: ok at cs.otago.ac.nz (Richard A. O'Keefe)
Date: Wed, 8 Oct 2003 11:27:45 +1300 (NZDT)
Subject: is.na(v)<-b (was: Re: [R] Beginner's query - segmentation fault)
Message-ID: <200310072227.h97MRjmr249752@atlas.otago.ac.nz>

I am puzzled by the advice to use is.na(x) <- TRUE instead of x <- NA.

?NA says
     Function `is.na<-' may provide a safer way to set missingness. It
     behaves differently for factors, for example.

However, "MAY provide" is a bit scary, and it doesn't say WHAT the
difference in behaviour is.

I must say that "is.na(x) <- ..." is rather repugnant, because it doesn't
work.  What do I mean?  Well, as the designers of SETL who many years ago
coined the term "sinister function call" to talk about f(...)<-...,
pointed out, if you do
    f(x) <- y
then afterwards you expect
    f(x) == y
to be true.  So let's try it:

    > x <- c(1,NA,3)
    > is.na(x) <- c(FALSE,FALSE,TRUE)
    > x
    [1]  1 NA NA
    > is.na(x)
    [1] FALSE  TRUE  TRUE
                        vvvvv
So I _assigned_ c(FALSE,FALSE,TRUE) to is.na(x),
    but I _got_ c(FALSE,TRUE, TRUE)> instead.
                        ^^^^^
That is not how a well behaved sinister function call should work,
and it's enough to scare someone off is.na()<- forever.

The obvious way to set elements of a variable to missing is ... <- NA.
Wouldn't it be better if that just plain worked?

Can someone give an example of is.na()<- and <-NA working differently
with a factor?  I just tried it:

    > x <- factor(c(3,1,4,1,5,9))
    > y <- x
    > is.na(x) <- x==1
    > y[y==1] <- NA
    > x
    [1] 3    <NA> 4    <NA> 5    9   
    Levels: 1 3 4 5 9
    > y
    [1] 3    <NA> 4    <NA> 5    9   
    Levels: 1 3 4 5 9

Both approaches seem to have given the same answer.  What did I miss?



From Ted.Harding at nessie.mcc.ac.uk  Wed Oct  8 02:00:30 2003
From: Ted.Harding at nessie.mcc.ac.uk ( (Ted Harding))
Date: Wed, 08 Oct 2003 01:00:30 +0100 (BST)
Subject: [R] Saving workspace image
Message-ID: <XFMail.031008010030.Ted.Harding@nessie.mcc.ac.uk>

Hi folks,

On quitting R with q(), is it possible to save the workspace
to a directory other than the one R was started from?

(I sometimes have a project "master" directory with the major
R code and data in that directory, but divisions of the project
having their specific stuff in sub-directories. So if I quit while
running a sub-project, I'd like to save the workspace back into its
sub-directory. I see that this could be achieved by starting R in that
sub-directory, and invoking "master" code as source("../whatever"),
but this would be a lot less convenient than doing it the other way
round).

With thanks,
Ted.


--------------------------------------------------------------------
E-Mail: (Ted Harding) <Ted.Harding at nessie.mcc.ac.uk>
Fax-to-email: +44 (0)870 167 1972
Date: 08-Oct-03                                       Time: 01:00:30
------------------------------ XFMail ------------------------------



From MSchwartz at medanalytics.com  Wed Oct  8 03:06:11 2003
From: MSchwartz at medanalytics.com (Marc Schwartz)
Date: Tue, 07 Oct 2003 20:06:11 -0500
Subject: [R] Saving workspace image
In-Reply-To: <XFMail.031008010030.Ted.Harding@nessie.mcc.ac.uk>
References: <XFMail.031008010030.Ted.Harding@nessie.mcc.ac.uk>
Message-ID: <1065575170.4657.70.camel@localhost.localdomain>

On Tue, 2003-10-07 at 19:00, Ted.Harding at nessie.mcc.ac.uk wrote:
> Hi folks,
> 
> On quitting R with q(), is it possible to save the workspace
> to a directory other than the one R was started from?
> 
> (I sometimes have a project "master" directory with the major
> R code and data in that directory, but divisions of the project
> having their specific stuff in sub-directories. So if I quit while
> running a sub-project, I'd like to save the workspace back into its
> sub-directory. I see that this could be achieved by starting R in that
> sub-directory, and invoking "master" code as source("../whatever"),
> but this would be a lot less convenient than doing it the other way
> round).
> 
> With thanks,
> Ted.


Ted,

Use:

setwd("PathToDir")

prior to quitting. That will set your working directory.

Alternatively, you can always use:

save.image("PathToDir/.RData")

to save the workspace image explicitly where you want it.

See ?setwd and ?save.image

HTH,

Marc Schwartz



From s195404 at student.uq.edu.au  Wed Oct  8 05:28:06 2003
From: s195404 at student.uq.edu.au (Andrew C. Ward)
Date: Wed,  8 Oct 2003 03:28:06 +0000
Subject: [R] Getting alternative editor to stay as a foreground task
Message-ID: <1065583686.3f83844664a7a@my.uq.edu.au>

I apologise in advance if this question ought to be
directed elsewhere. I'm trying to use the R sytnax
highlighter for JEdit. This was mentioned in R-help a while 
ago and I'm hoping a few R users will have a (probably
obvious) solution.
(http://finzi.psych.upenn.edu/R/Rhelp02/archive/8812.html)

Usually when one goes "fix(myfun)", the function is opened
in the text editor, R stops responding until the editing is
done, and changes are saved back into myfun. With JEdit,
however, control passes straight back to R and changes made
within JEdit aren't reflected in myfun.

Any advice would be very welcome. I'm using R 1.7.1 on 
Windows 2000.


Regards,

Andrew C. Ward

CAPE Centre
Department of Chemical Engineering
The University of Queensland
Brisbane Qld 4072 Australia



From ligges at statistik.uni-dortmund.de  Wed Oct  8 09:05:31 2003
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Wed, 08 Oct 2003 09:05:31 +0200
Subject: [R] R-1.8.0 memory.limit()
In-Reply-To: <sf82e1c4.052@med-gwia-01a.med.umich.edu>
References: <sf82e1c4.052@med-gwia-01a.med.umich.edu>
Message-ID: <3F83B73B.6080202@statistik.uni-dortmund.de>

James MacDonald wrote:

> Thanks Uwe,
> 
> What is the theoretical limit for 32 bit Windows anyway?
> BTW, --max-mem-size=2000M works (although maybe not all usable?)

I don't know the usable amount exactly, one has to look into Microsoft's 
docs. It must be a bit smaller than 2GB (differently from the problem of 
~3.7GB usable out of 4GB, because of other address spaces).

I've just looked into the sources: R calculates with integers and of 
course (too stupid I had not realized it at once) has the same 32bit 
limit! So try 2047M (last "working" one) and 2049M which becomes negative.

>>memory.limit()
> 
> [1] 2097152000
> 
> In R-1.7.1, --max-mem-size=2G *did* work (I get the same results as
> above).

In R-1.7.1  (2003-06-16) on WinNT4.0 it did *not*.

Uwe


> Jim
> 
> 
> 
> James W. MacDonald
> Affymetrix and cDNA Microarray Core
> University of Michigan Cancer Center
> 1500 E. Medical Center Drive
> 7410 CCGC
> Ann Arbor MI 48109
> 734-647-5623
> 
> 
>>>>Uwe Ligges <ligges at statistik.uni-dortmund.de> 10/07/03 12:28PM >>>
> 
> James MacDonald wrote:
> 
> 
>>Using R-1.8.0 (d/l and compiled on 2003-10-01) on WinXP, I seem to
> 
> be
> 
>>unable to determine the maximum memory allocated to R. The help
> 
> still
> 
>>says to use memory.limit(size=NA), but this returns the value NA.
>>
>>In addition, I have set --max-mem-size=2G but I run out of memory
>>somewhere around 500Mb (which is why I am trying to find out how
> 
> much
> 
>>memory is allocated). I don't have any other programs open, so I
> 
> should
> 
>>certainly be able to address more memory than that.
>>
>>Any ideas?
> 
> 
> Yes. 2GB is a bit more than the possible theoretical maximal value for
> a 
> single process on 32-bit Windows. Instead, try starting with, e.g., 
> "--max-mem-size=1700M" and you'll get
> 
>  > memory.limit()
> [1] 1782579200
> 
> Uwe Ligges
> 
> 
> 
>>TIA
>>
>>Jim
>>
>>
>>
>>James W. MacDonald
>>Affymetrix and cDNA Microarray Core
>>University of Michigan Cancer Center
>>1500 E. Medical Center Drive
>>7410 CCGC
>>Ann Arbor MI 48109
>>734-647-5623
>>
>>______________________________________________
>>R-help at stat.math.ethz.ch mailing list
>>https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> 
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help



From ligges at statistik.uni-dortmund.de  Wed Oct  8 10:28:13 2003
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Wed, 08 Oct 2003 10:28:13 +0200
Subject: is.na(v)<-b (was: Re: [R] Beginner's query - segmentation fault)
In-Reply-To: <200310072227.h97MRjmr249752@atlas.otago.ac.nz>
References: <200310072227.h97MRjmr249752@atlas.otago.ac.nz>
Message-ID: <3F83CA9D.30106@statistik.uni-dortmund.de>

Richard A. O'Keefe wrote:

> I am puzzled by the advice to use is.na(x) <- TRUE instead of x <- NA.
> 
> ?NA says
>      Function `is.na<-' may provide a safer way to set missingness. It
>      behaves differently for factors, for example.
> 
> However, "MAY provide" is a bit scary, and it doesn't say WHAT the
> difference in behaviour is.
> 
> I must say that "is.na(x) <- ..." is rather repugnant, because it doesn't
> work.  What do I mean?  Well, as the designers of SETL who many years ago
> coined the term "sinister function call" to talk about f(...)<-...,
> pointed out, if you do
>     f(x) <- y
> then afterwards you expect
>     f(x) == y
> to be true.  So let's try it:
> 
>     > x <- c(1,NA,3)
>     > is.na(x) <- c(FALSE,FALSE,TRUE)
>     > x
>     [1]  1 NA NA
>     > is.na(x)
>     [1] FALSE  TRUE  TRUE
>                         vvvvv
> So I _assigned_ c(FALSE,FALSE,TRUE) to is.na(x),
>     but I _got_ c(FALSE,TRUE, TRUE)> instead.
>                         ^^^^^
> That is not how a well behaved sinister function call should work,
> and it's enough to scare someone off is.na()<- forever.
> 
> The obvious way to set elements of a variable to missing is ... <- NA.
> Wouldn't it be better if that just plain worked?
> 
> Can someone give an example of is.na()<- and <-NA working differently
> with a factor?  I just tried it:
> 
>     > x <- factor(c(3,1,4,1,5,9))
>     > y <- x
>     > is.na(x) <- x==1
>     > y[y==1] <- NA
>     > x
>     [1] 3    <NA> 4    <NA> 5    9   
>     Levels: 1 3 4 5 9
>     > y
>     [1] 3    <NA> 4    <NA> 5    9   
>     Levels: 1 3 4 5 9
> 
> Both approaches seem to have given the same answer.  What did I miss?


As mentioned in another mail to R-help. I'm pretty sure there was (is?) 
a problem with character (and/or factor) and assignment of NAs, but I 
cannot (re)produce an example. I think something for the "x <- NA" case 
has been fixed during the last year.
What prevents me to think I'm completely confused is that the is.na()<- 
usage is proposed in: ?NA, S Programming, the R Language Definition 
manual, R's News file, but I cannot find it in the green book right now.

Uwe Ligges



From mikewhite.diu at tiscali.co.uk  Wed Oct  8 10:25:41 2003
From: mikewhite.diu at tiscali.co.uk (Mike White)
Date: Wed, 8 Oct 2003 09:25:41 +0100
Subject: [R] SIMCA algorithm implementation
Message-ID: <003401c38d75$c335f850$2267e150@FSSFQCV7BGDVED>

Dear All
Is there a SIMCA (Soft Independent Modelling Class Analogy) implementation
on R or does anyone know if is it possible to replicate the SIMCA algorithm
using existing R functions?

Thanks
Mike White



From anne.piotet at m-td.com  Wed Oct  8 11:21:10 2003
From: anne.piotet at m-td.com (Anne Piotet)
Date: Wed, 8 Oct 2003 11:21:10 +0200
Subject: [R] plotting results from leaps library
Message-ID: <002201c38d7d$838473f0$6c00a8c0@mtd4>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20031008/37376dba/attachment.pl

From xavier.fim at eresmas.net  Wed Oct  8 12:05:31 2003
From: xavier.fim at eresmas.net (Xavier =?iso-8859-1?q?Fern=E1ndez=20i=20Mar=EDn?=)
Date: Wed, 8 Oct 2003 12:05:31 +0200
Subject: [R] Generating automatic plots
Message-ID: <200310081205.31272.xavier.fim@eresmas.net>

Hello,

I have been trying to write a small program to generate automatic plots. What 
I want is to draw boxplots for some variables in my data frame, contrasting 
with a variable called 'missing' that has value 1 if some variable in a 
concrete case has at least one missing value, in order to check if the cases 
that don't enter in my analysis are biased.

The first attempt was to start generating a list with the variables of 
contrast and then apply the list to the plots:

> varlist <- c("var1", "var2", "var3", "var4", ...)

> for (i in 1:length(varlist)) {
+  jpeg("varlist[i].jpg", width=480, heigth=480)
+  boxplot (varlist[i] ~ missing, xlab="missing values", ylab="varlist[i])
+  }

But I got 'Error in model.frame(formula, rownames, variables, varnames, 
extranames: invalid variable type'

I suppose that is because I forget something related to the extraction of 
values in vectors, but I can't find it on the R manual neither in other books 
about R that I have checked.

Is there a way to draw plots using commands like the one above?

Thanks,


Xavier



From Simon.Fear at synequanon.com  Wed Oct  8 12:08:51 2003
From: Simon.Fear at synequanon.com (Simon Fear)
Date: Wed, 8 Oct 2003 11:08:51 +0100
Subject: is.na(v)<-b (was: Re: [R] Beginner's query - segmentation fault)
Message-ID: <6C8A8033ABC1E3468048ABC4F13CE572AC0E0B@synequanon01>

Note this behaviour:

> a<-"a"
> a<-NA
> mode(a)
[1] "logical"
> a<-"a"
> is.na(a) <- T
> mode(a)
[1] "character"

However after either way of assigning NA to a, is.na(a) is true,
and it prints as NA, so I can't see it's ever likely to matter. [Why
do I say these things? Expect usual flood of examples where it 
does matter.]

Also if a is a character vector, a[2] <- NA coerces the NA to
as.character(NA); again, just as one would hope/expect.

I have to echo Richard O'K's remark: if <- NA can ever go wrong,
is that not a bug rather than a feature?
 

Simon Fear
Senior Statistician
Syne qua non Ltd
Tel: +44 (0) 1379 644449
Fax: +44 (0) 1379 644445
email: Simon.Fear at synequanon.com
web: http://www.synequanon.com
 
Number of attachments included with this message: 0
 
This message (and any associated files) is confidential and\...{{dropped}}



From gavin.simpson at ucl.ac.uk  Wed Oct  8 12:39:25 2003
From: gavin.simpson at ucl.ac.uk (Gavin Simpson)
Date: Wed, 08 Oct 2003 11:39:25 +0100
Subject: [R] plotting results from leaps library
In-Reply-To: <002201c38d7d$838473f0$6c00a8c0@mtd4>
References: <002201c38d7d$838473f0$6c00a8c0@mtd4>
Message-ID: <3F83E95D.8040306@ucl.ac.uk>

Anne Piotet wrote:
> Hi In trying to fit a linear model , I use the leaps() function to 
> determine wich predictors I should include in my model. I would like 
> to plot the Mallow's Cp criteria against p with the indexes of 
> selected model variates as points labels
> 
> Is there already such a function? (I could not find it)
> 
> Thanks Anne
> 
> [[alternative HTML version deleted]]
> 
> ______________________________________________ 
> R-help at stat.math.ethz.ch mailing list 
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> 
> 

try subsets() in John Fox's car package. You need to use regsubsets() in 
package leaps, not leaps(), but the plot.subsets() function in car would 
appear to produce what you want.

HTH

Gav

-- 
%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%
Gavin Simpson                     [T] +44 (0)20 7679 5522
ENSIS Research Fellow             [F] +44 (0)20 7679 7565
ENSIS Ltd. & ECRC                 [E] gavin.simpson at ucl.ac.uk
UCL Department of Geography       [W] http://www.ucl.ac.uk/~ucfagls/cv/
26 Bedford Way                    [W] http://www.ucl.ac.uk/~ucfagls/
London.  WC1H 0AP.
%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%



From jasont at indigoindustrial.co.nz  Wed Oct  8 12:43:17 2003
From: jasont at indigoindustrial.co.nz (Jason Turner)
Date: Wed, 08 Oct 2003 23:43:17 +1300
Subject: [R] Generating automatic plots
In-Reply-To: <200310081205.31272.xavier.fim@eresmas.net>
References: <200310081205.31272.xavier.fim@eresmas.net>
Message-ID: <3F83EA45.4040204@indigoindustrial.co.nz>

Xavier Fern?ndez i Mar?n wrote:
...
>>varlist <- c("var1", "var2", "var3", "var4", ...)

Instead of a character vector with the names, it'd make life easier if 
you had a list of the vectors...

# make sure you use the naming - makes life easier later.
mylist <- list(var1=var1, var2=var2, var3=var3, var4=var4)

Then...

for(i in seq(along=mylist)) {  # seq(along=...) is safest.
   jpeg(names(mylist)[i], width=...)
   boxplot(mylist[[i]] ~ missing, xlab=...)
   dev.off() # you didn't have this.  it's required.
}

# but I don't see how "missing" gets correctly passed in your example,
# so I'm not sure what it's supposed to do.

> I suppose that is because I forget something related to the extraction of 
> values in vectors, but I can't find it on the R manual neither in other books 
> about R that I have checked.

Nope.  It's about passing a strings of text to plot, instead of actual 
data.  You'd have to actually parse the string to produce the value. 
And that's getting tricky (at least, it's tricky to my brain).  The 
above is pretty simple by comparison.

Cheers

Jason

-- 
Indigo Industrial Controls Ltd.
http://www.indigoindustrial.co.nz
64-21-343-545
jasont at indigoindustrial.co.nz



From ripley at stats.ox.ac.uk  Wed Oct  8 12:49:29 2003
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Wed, 8 Oct 2003 11:49:29 +0100 (BST)
Subject: is.na(v)<-b (was: Re: [R] Beginner's query - segmentation fault)
In-Reply-To: <6C8A8033ABC1E3468048ABC4F13CE572AC0E0B@synequanon01>
Message-ID: <Pine.LNX.4.44.0310081145260.1110-100000@gannet.stats>

On Wed, 8 Oct 2003, Simon Fear wrote:

> Note this behaviour:
> 
> > a<-"a"
> > a<-NA
> > mode(a)
> [1] "logical"
> > a<-"a"
> > is.na(a) <- T
> > mode(a)
> [1] "character"
> 
> However after either way of assigning NA to a, is.na(a) is true,
> and it prints as NA, so I can't see it's ever likely to matter. [Why
> do I say these things? Expect usual flood of examples where it 
> does matter.]
> 
> Also if a is a character vector, a[2] <- NA coerces the NA to
> as.character(NA); again, just as one would hope/expect.
> 
> I have to echo Richard O'K's remark: if <- NA can ever go wrong,
> is that not a bug rather than a feature?

I don't think it can ever `go wrong', but it can do things other than the 
user intends.  The intention of is.na<- is clearer, and so perhaps user 
error is less likely?  That is the thinking behind the function, anyway.

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From zeileis at ci.tuwien.ac.at  Wed Oct  8 12:49:42 2003
From: zeileis at ci.tuwien.ac.at (Achim Zeileis)
Date: Wed, 8 Oct 2003 12:49:42 +0200
Subject: [R] Generating automatic plots
In-Reply-To: <200310081205.31272.xavier.fim@eresmas.net>
References: <200310081205.31272.xavier.fim@eresmas.net>
Message-ID: <200310081049.h98AnhvS017485@thorin.ci.tuwien.ac.at>

On Wednesday 08 October 2003 12:05, Xavier Fern?ndez i Mar?n wrote:

> Hello,
>
> I have been trying to write a small program to generate automatic
> plots. What I want is to draw boxplots for some variables in my data
> frame, contrasting with a variable called 'missing' that has value 1
> if some variable in a concrete case has at least one missing value,
> in order to check if the cases that don't enter in my analysis are
> biased.
>
> The first attempt was to start generating a list with the variables
> of
>
> contrast and then apply the list to the plots:
> > varlist <- c("var1", "var2", "var3", "var4", ...)
> >
> > for (i in 1:length(varlist)) {
>
> +  jpeg("varlist[i].jpg", width=480, heigth=480)

here you want

  paste(varlist[i], ".jpg", sep = "")

> +  boxplot (varlist[i] ~ missing, xlab="missing values",

and here
  mydataframe[, varlist[i]] ~ missing

hth,
Z

> ylab="varlist[i]) +  }
>
> But I got 'Error in model.frame(formula, rownames, variables,
> varnames, extranames: invalid variable type'
>
> I suppose that is because I forget something related to the
> extraction of values in vectors, but I can't find it on the R manual
> neither in other books about R that I have checked.
>
> Is there a way to draw plots using commands like the one above?
>
> Thanks,
>
>
> Xavier
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help



From Thomas.Bock at ptb.de  Wed Oct  8 11:27:26 2003
From: Thomas.Bock at ptb.de (Thomas Bock)
Date: Wed, 08 Oct 2003 11:27:26 +0200
Subject: [R] Why does a[which(b == c[d])] not work?
Message-ID: <3F83D87E.2060807@ptb.de>

Dear list,

I can not understand why the expression in
the subject does not work correct:

 > dcrn[which(fn == inve[2])]
numeric(0)
 > inve[2]
[1] 406.7
 > dcrn[which(fn == 406.7)]
 [1] 1.3994e-07 1.3988e-07 1.3953e-07 1.3966e-07 1.3953e-07 1.3968e-07

Is this a kick self problem or an bug?

Thaks very much
Thomas



From zeileis at ci.tuwien.ac.at  Wed Oct  8 13:45:50 2003
From: zeileis at ci.tuwien.ac.at (Achim Zeileis)
Date: Wed, 8 Oct 2003 13:45:50 +0200
Subject: [R] Why does a[which(b == c[d])] not work?
In-Reply-To: <3F83D87E.2060807@ptb.de>
References: <3F83D87E.2060807@ptb.de>
Message-ID: <200310081145.h98Bjoc9018171@thorin.ci.tuwien.ac.at>

On Wednesday 08 October 2003 11:27, Thomas Bock wrote:

> Dear list,
>
> I can not understand why the expression in
>
> the subject does not work correct:
>  > dcrn[which(fn == inve[2])]
>
> numeric(0)
>
>  > inve[2]
>
> [1] 406.7
>
>  > dcrn[which(fn == 406.7)]
>
>  [1] 1.3994e-07 1.3988e-07 1.3953e-07 1.3966e-07 1.3953e-07
> 1.3968e-07

The reason is that you shouldn't compare numeric output like that, 
consider the following example:

R> x <- 406.7 + 1e-5 
R> x
[1] 406.7
R> x == 406.7
[1] FALSE

whereas

R> x <- 406.7 + 1e-20 
R> x
[1] 406.7
R> x == 406.7
[1] TRUE

that is
  1.) `==' comparisons have a certain tolerance
  2.) the print output is not necessarily "precisely" your number

Instead of using `==' you should use a comparison with a certain 
tolerance you can specify...

hth,
Z


> Is this a kick self problem or an bug?
>
> Thaks very much
> Thomas
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help



From p.dalgaard at biostat.ku.dk  Wed Oct  8 13:48:55 2003
From: p.dalgaard at biostat.ku.dk (Peter Dalgaard BSA)
Date: Wed, 08 Oct 2003 11:48:55 -0000
Subject: [R] Why does a[which(b == c[d])] not work?
In-Reply-To: <3F83D87E.2060807@ptb.de>
References: <3F83D87E.2060807@ptb.de>
Message-ID: <x27k3ggd81.fsf@biostat.ku.dk>

Thomas Bock <Thomas.Bock at ptb.de> writes:

> Dear list,
> 
> I can not understand why the expression in
> the subject does not work correct:
> 
>  > dcrn[which(fn == inve[2])]
> numeric(0)
>  > inve[2]
> [1] 406.7
>  > dcrn[which(fn == 406.7)]
>  [1] 1.3994e-07 1.3988e-07 1.3953e-07 1.3966e-07 1.3953e-07 1.3968e-07
> 
> Is this a kick self problem or an bug?

Kick self, most likely. What is 

inve[2] - 406.7

-- 
   O__  ---- Peter Dalgaard             Blegdamsvej 3  
  c/ /'_ --- Dept. of Biostatistics     2200 Cph. N   
 (*) \(*) -- University of Copenhagen   Denmark      Ph: (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)             FAX: (+45) 35327907



From Simon.Fear at synequanon.com  Wed Oct  8 13:48:27 2003
From: Simon.Fear at synequanon.com (Simon Fear)
Date: Wed, 8 Oct 2003 12:48:27 +0100
Subject: is.na(v)<-b (was: Re: [R] Beginner's query - segmentation fault)
Message-ID: <6C8A8033ABC1E3468048ABC4F13CE572AC0E0E@synequanon01>

Well, that's a convincing argument, but maybe 
it's the name that's worrying some of us. Maybe it would be 
more intuitive if called set.na (sorry, I mean setNA).

Also "is.na<-" cannot be used to create a new variable of 
NAs, so is not a universal method,  which is a shame for its 
advocates.

I note also that for a vector you can assign a new NA using 
either TRUE or FALSE:

> a <- 1:3
> is.na(a[4])<-F
> a
[1]  1  2  3 NA

For a list,  assigning F leaves the "new" element set to NULL.

Mind you, I suspect this would be a particularly stupid thing 
to do, so I'm not going to lose any sleep over R's reaction to it.

> 
> -----Original Message-----
> From: Prof Brian Ripley [mailto:ripley at stats.ox.ac.uk]
> I don't think it can ever `go wrong', but it can do things other than
> the 
> user intends.  The intention of is.na<- is clearer, and so 
> perhaps user 
> error is less likely?  That is the thinking behind the 
> function, anyway.
 

Simon Fear
Senior Statistician
Syne qua non Ltd
Tel: +44 (0) 1379 644449
Fax: +44 (0) 1379 644445
email: Simon.Fear at synequanon.com
web: http://www.synequanon.com
 
Number of attachments included with this message: 0
 
This message (and any associated files) is confidential and\...{{dropped}}



From lwalters at cs.uct.ac.za  Wed Oct  8 13:56:24 2003
From: lwalters at cs.uct.ac.za (Lourens Olivier Walters)
Date: Wed, 08 Oct 2003 13:56:24 +0200
Subject: [R] fitdistr, mle's and gamma distribution
In-Reply-To: <3F79E26F.7000104@pdf.com>
References: <1064950780.547.59.camel@stochastic>  <3F79E26F.7000104@pdf.com>
Message-ID: <1065614184.600.193.camel@stochastic>

Thanks, your advice worked. I don't have much experience with maths, and
therefore tried to stay away from dealing with optimization, but going
down to this level opens a lot of possibilities. For the record, the
code I used, as you suggested:

###############
shape <- mean(data)^2/var(data)
scale <- var(data)/mean(data)
gamma.param1 <- shape
gamma.param2 <- scale
log.gamma.param1 <- log(gamma.param1)
log.gamma.param2 <- log(gamma.param2)
                                                                                                                                              gammaLoglik <- function(params, negative=TRUE){
   lglk <- sum(dgamma(data, shape=exp(params[1]), scale=exp(params[2]),
log=TRUE))
   if(negative)
      return(-lglk)
   else
      return(lglk)
}

optim.list <- optim(c(log.gamma.param1, log.gamma.param2), gammaLoglik)
gamma.param1 <- exp(optim.list$par[1])
gamma.param2 <- exp(optim.list$par[2])
################

optim converges and the parameters provide a much better fit to the data
than the method of moments parameters do. 

Armed with this new knowledge, I attempted to write a log(likelihood)
optimization method for the Pareto distribution. My ignorance of math
however shows, as the code does not work. 

Here is what I did:

################
pareto.density <- function(x, alpha, beta, log=FALSE){
   # Pareto distribution not defined for x < alpha
   # Test for values x < alpha, taking into account floating point error
   test.vals <- x-alpha
   error.vals <- test.vals[test.vals<0]
   if (length(error.vals)>0)
      stop("\nERROR: x > alpha in pareto.distr(x, alpha, beta,
+ log=FALSE)")
   density <- beta * (alpha^beta) * (x^(-beta-1))
   if(log)
      return(log(density))
   else
      return(density)
}

data.sorted <- sort(data)
alpha.val <- data.sorted[1]
beta.val <- 1/((1/n) * sum(log(data.sorted/alpha.val)))
log.alpha.val <- log(alpha.val)
log.beta.val <- log(beta.val)

paretoLoglik <- function(params, negative=TRUE){
   lglk <- sum(pareto.density(data.sorted, alpha=exp(params[1]),
+ beta=exp(params[2]), log=TRUE))
   if(negative)
      return(-lglk)
   else
      return(lglk)
}

optim.list <- optim(c(log.alpha.val, log.beta.val), paretoLoglik,
+ method="L-BFGS-B", lower=c(log.alpha.val, 0), upper=c(log.alpha.val,
+ Inf))
pareto.param1 <- exp(optim.list$par[1])
pareto.param2 <- exp(optim.list$par[2])
#################

I fixed the alpha parameter as my Pareto density function produces an
error if a datavalue > alpha.

I get the following output:

> source("browsesessoffplotfitted.R")
Error in optim(c(log.alpha.val, log.beta.val), paretoLoglik, method =
+ "L-BFGS-B",  :
        non-finite finite-difference value [0]

Any ideas would be appreciated, otherwise its back to method of moments
for the Pareto distribution for me :)

Thanks
Lourens

On Tue, 2003-09-30 at 22:07, Spencer Graves wrote:

>       In my experience, the most likely cause of this problem is that 
> optim may try to test nonpositive values for shape or scale.  I avoid 
> this situation by programming the log(likelihood) in terms of log(shape) 
> and log(scale) as follows: 
> 
>  > gammaLoglik <-
> + function(x, logShape, logScale, negative=TRUE){
> + lglk <- sum(dgamma(x, shape=exp(logShape), scale=exp(logScale),
> + log=TRUE))
> + if(negative) return(-lglk) else return(lglk)
> + }
>  > tst <- rgamma(10, 1)
>  > gammaLoglik(tst, 0, 0)
> [1] 12.29849
> 
> Then I then call optim directly to minimize the negative of the 
> log(likelihood). 
> 
>       If I've guessed correctly, this should fix the problem.  If not, 
> let us know. 
> 
>       hope this helps.  spencer graves



From Pascal.Niklaus at unibas.ch  Wed Oct  8 14:19:16 2003
From: Pascal.Niklaus at unibas.ch (Pascal A. Niklaus)
Date: Wed, 08 Oct 2003 14:19:16 +0200
Subject: [R] Contrast specified with C() - R vs S-Plus problem
Message-ID: <3F8400C4.8040503@unibas.ch>

Hi,

For a n-level factor, I'd like to specify the first contrast and have
the remaining n-2 constructed automatically so that the set is
orthogonal. I then test the contrasts with summary.lm(anova-object).

In S-Plus, the following works:

    >y.anova <- aov( y ~ C(CO2,c(1,0,-1)) )
    >summary.lm(y.anova)

In R, it fails with the following error:

    >levels(CO2)
    [1] ""  "A" "C" "E"

    >y.anova <- aov(y + C(CO2,c(1,0,-1)) )
    Error in "contrasts<-"(*tmp*, value = contr) :
            wrong number of contrast matrix rows

What is the way to do this in R?

Thanks

Pascal

--


Dr. Pascal A. Niklaus
Institute of Botany
University of Basel
Sch?nbeinstrasse 6
CH-4056 Basel / Switzerland



From p.dalgaard at biostat.ku.dk  Wed Oct  8 14:28:31 2003
From: p.dalgaard at biostat.ku.dk (Peter Dalgaard BSA)
Date: Wed, 08 Oct 2003 12:28:31 -0000
Subject: [R] R-1.8.0 is released
Message-ID: <x23ce4gcd6.fsf@biostat.ku.dk>

I've rolled up R-1.8.0.tgz a short while ago. This is a new version
with major changes (see below). Notably, the Macintosh version for OS
X has been substantially improved; the old Carbon interface is no
longer being supported.

Also notice that the underscore will no longer work as an assignment
operator.

There is also a bunch of new functions and an assortment of bugs have
been fixed.

You can get it from

http://cran.us.r-project.org/src/base/R-1.8.0.tgz

or wait for it to be mirrored at a CRAN site nearer to you. Binaries
for various platforms will appear in due course.
 
There is also a version split for floppies.

These are the md5sums for the freshly created files, in case you wish
to check that they are uncorrupted:

bb019d7e12e38ac8a8bc247f06cc6b42  R-1.8.0.tgz
80fd20fdd2b995ab69cfd8cde80267cf  R-1.8.0.tgz-split.aa
074140872c9c015089543c9497a3be87  R-1.8.0.tgz-split.ab
f0e6005491839dd27636020f6475f4e2  R-1.8.0.tgz-split.ac
e7e817911d57e3c88959d6b86c061dd5  R-1.8.0.tgz-split.ad
52e9aff83357b65926f422abececba92  R-1.8.0.tgz-split.ae
6c6965e7f97b627280326ec6c436ab48  R-1.8.0.tgz-split.af
00ce1b1384f403ba76d8de302e92da1d  R-1.8.0.tgz-split.ag

On behalf of the R Core Team,

        Peter Dalgaard


Here's the relevant part of the NEWS file


		CHANGES IN R VERSION 1.8.0


MACOS CHANGES

    o	As from this release there is only one R port for the
	Macintosh, which runs only on MacOS X.	(The `Carbon' port has
	been discontinued, and the `Darwin' port is part of the new
	version.)   The current version can be run either as a
	command-line application or as an `Aqua' console.  There is a
	`Quartz' device quartz(), and the download and installation of
	both source and binary packages is supported from the Aqua
	console.  Those CRAN and BioC packages which build under MacOS
	X have binary versions updated daily.


USER-VISIBLE CHANGES

    o	The defaults for glm.control(epsilon=1e-8, maxit=25) have been
	tightened: this will produce more accurate results, slightly
	slower.

    o	sub, gsub, grep, regexpr, chartr, tolower, toupper, substr,
	substring, abbreviate and strsplit now handle missing values
	differently from "NA".

    o	Saving data containing name space references no longer warns
	about name spaces possibly being unavailable on load.

    o	On Unix-like systems interrupt signals now set a flag that is
	checked periodically rather than calling longjmp from the
	signal handler.	 This is analogous to the behavior on Windows.
	This reduces responsiveness to interrupts but prevents bugs
	caused by interrupting computations in a way that leaves the
	system in an inconsistent state.  It also reduces the number
	of system calls, which can speed up computations on some
	platforms and make R more usable with systems like Mosix.


CHANGES TO THE LANGUAGE

    o	Error and warning handling has been modified to incorporate a
	flexible condition handling mechanism.	See the online
	documentation of 'tryCatch' and 'signalCondition'.  Code that
	does not use these new facilities should remain unaffected.

    o	A triple colon operator can be used to access values of internal
	variables in a name space (i.e. a:::b is the value of the internal
	variable b in name space a).

    o	Non-syntactic variable names can now be specified by inclusion
	between backticks `Like This`.	The deparse() code has been
	changed to output non-syntactical names with this convention,
	when they occur as operands in expressions.  This is controlled
	by a `backtick' argument, which is by default TRUE for
	composite expressions and FALSE for single symbols.  This
	should give minimal interference with existing code.

    o	Variables in formulae can be quoted by backticks, and such
	formulae can be used in the common model-fitting functions.
	terms.formula() will quote (by backticks) non-syntactic names
	in its "term.labels" attribute.	 [Note that other code using
	terms objects may expect syntactic names and/or not accept
	quoted names: such code will still work if the new feature is
	not used.]


NEW FEATURES

    o	New function bquote() does partial substitution like LISP backquote.

    o	capture.output() takes arbitrary connections for `file' argument.

    o	contr.poly() has a new `scores' argument to use as the base set
	for the polynomials.

    o	cor() has a new argument `method = c("pearson","spearman","kendall")'
	as cor.test() did forever. The two rank based measures do work with
	all three missing value strategies.

    o	New utility function cov2cor() {Cov -> Corr matrix}.

    o	cut.POSIXt() now allows `breaks' to be more general intervals
	as allowed for the `by' argument to seq.POSIXt().

    o	data() now has an 'envir' argument.

    o	det() uses an LU decomposition and LAPACK.  The `method'
	argument to det() no longer has any effect.

    o	dev.control() now accepts "enable" as well as "inhibit".
	(Wishlist PR#3424)

    o	*, - and / work more generally on "difftime" objects, which now
	have a diff() method.

    o	dt(*, ncp = V)	is now implemented, thanks to Claus Ekstroem.

    o	dump() only quotes object names in the file where necessary.

    o	eval() of a promise forces the promise

    o	file.path() now returns an empty character vector if given at
	least one zero-length argument.

    o	format() and hence print() make an effort to handle corrupt
	data frames, with a warning.

    o	format.info() now also works with `nsmall' in analogy with
	format.default().

    o	gamma(n) is very slightly more precise for integer n in 11:50.

    o	? and help() will accept more un-quoted arguments, e.g. NULL.

    o	The "?" operator has new forms for querying documentation on
	S4 methods.  See the online documentation.

    o	New argument frame.plot = axes (== TRUE)  for filled.contour().

    o	New argument fixed = TRUE for grep() and regexpr() to avoid the
	need to escape strings to match.

    o	grep(x, ..., value = TRUE) preserves names of x.

    o	hist.POSIXt() can now pass arguments to hist.default()

    o	legend() and symbols() now make use of xy.coords() and accept
	a wider range of coordinate specifications.

    o	Added function library.dynam.unload() to call dyn.unload() on
	a loaded DLL and tidy up.  This is called for all the standard
	packages in namespaces with DLLs if their namespaces are unloaded.

    o	lm(singular.ok = FALSE) is now implemented.

    o	Empty lm() and glm() fits are now handled by the normal
	code: there are no methods for classes "lm.null" and
	"glm.null".  Zero-rank fits are handled consistently.

    o	make.names() has improvements, and there is a new auxiliary
	function make.unique().	 (Based on code contributed by Tom
	Minka, since converted to a .Internal function.)  In
	particular make.names() now recognises that names beginning
	with a dot are valid and that reserved words are not.

    o	methods() has a print method which asterisks functions which
	are not user-visible.  methods(class = "foo") now lists
	non-visible functions, and checks that there is a matching generic.

    o	model.matrix() now warns when it removes the response from the
	rhs of the formula: that this happens is now documented on its
	help page.

    o	New option `locatorBell' to control the confirmation beep
	during the use of locator() and identify().

    o	New option("scipen") provides some user control over the
	printing of numbers in fixed-point or exponential notation.
	(Contributed by David Brahm.)

    o	plot.formula() now accepts horizontal=TRUE and works correctly
	when boxplots are produced.  (Wishlist PR#1207)	 The code has
	been much simplified and corrected.

    o	polygon() and rect() now interpret density < 0 or NA to mean
	filling (by colour) is desired: this allows filling and
	shading to be mixed in one call, e.g. from legend().

    o	The predict() methods for classes lm, glm, mlm and lqs take a
	`na.action' argument that controls how missing values in
	`newdata' are handled (and defaults to predicting NA).
	[Previously the value of getOption("na.action") was used and
	this by default omitted cases with missing values, even if set
	to `na.exclude'.]

    o	print.summary.glm() now reports omitted coefficients in the
	same way as print.summary.lm(), and both show them as NAs in
	the table of coefficients.

    o	print.table() has a new argument `zero.print' and is now
	documented.

    o	rank(x, na.last = "keep") now preserves NAs in `x', and the
	argument `ties.method' allows to use non-averaging ranks in the
	presence of ties.

    o	read.table()'s 'as.is' argument can be character, naming columns
	not to be converted.

    o	rep() is now a generic function, with default, POSIXct and
	POSIXlt methods.  For efficiency, the base code uses rep.int()
	rather than rep() where possible.

    o	New function replicate() for repeated evaluation of expression
	and collection of results, wrapping a common use of sapply()
	for simulation purposes.

    o	rev() is now a generic function, with default and dendrogram
	methods.

    o	serialize() and unserialize() functions are available for
	low-level serialization to connections.

    o	socketSelect() allows waiting on multiple sockets.

    o	sort(method = "quick", decreasing = TRUE) is now implemented.

    o	sort.list() has methods "quick" (a wrapper for sort(method =
	"quick", index.return = TRUE) and "radix" (a very fast method
	for small integers).  The default "shell" method works faster
	on long vectors with many ties.

    o	stripchart() now has `log', `add' and `at' arguments.

    o	strsplit(x, *) now preserves names() but won't work for
	non-character `x' anymore {formerly used as.character(x),
	destroying names(x)}.

    o	textConnection() now has a local argument for use with output
	connections.  local = TRUE means the variable containing the
	output is assigned in the frame of the caller.

    o	Using UseMethod() with more than two arguments now gives a
	warning (as R-lang.texi has long claimed it did).

    o	New function vignette() for viewing or listing vignettes.

    o	which.min(x) and which.max(x) now preserve names.

    o	xy.coords() coerces "POSIXt" objects to "POSIXct", allowing
	lines etc to added to plot.POSIXlt() plots.

    o	.Machine has a new entry, sizeof.pointer.

    o	.Random.seed is only looked for and stored in the user's
	workspace.  Previously the first place a variable of that name
	was found on the search path was used.

    o	Subscripting for data.frames has been rationalized:

	- Using a single argument now ignores any `drop' argument
	  (with a warning).  Previously using `drop' inhibited list-like
	  subscripting.

	- adf$name <- value now checks for the correct length of
	  `value', replicating a whole number of times if needed.

	- adf[j] <- value and adf[[j]] <- value did not convert
	  character vectors to factors, but adf[,j] <- value did.
	  Now none do.	Nor is a list `value' coerced to a data frame
	  (thereby coercing character elements to factors).

	- Where replicating the replacement value a whole number of
	  times will produce the right number of values, this is
	  always done (rather than some times but not others).

	- Replacement list values can include NULL elements.

	- Subsetting a data frame can no longer produce duplicate
	  column names.

	- Subsetting with drop=TRUE no longer sometimes drops
	  dimensions on matrix or data frame columns of the data frame.

	- Attributes are no longer stripped when replacing part of a column.

	- Columns added in replacement operations will always be
	  named, using the names of a list value if appropriate.

	- as.data.frame.list() did not cope with list names such as
	  `check.rows',	 and formatting/printing data frames with such
	  column names now works.

	- Row names in extraction are still made unique, but without
	  forcing them to be syntactic names.

	- adf[x] <- list() failed if x was of length zero.


    o	Setting dimnames to a factor now coerces to character, as S
	does.  (Earlier versions of R used the internal codes.)

    o	When coercion of a list fails, a meaningful error message is given.

    o	Adding to NULL with [[ ]] generates a list if more than one
	element is added (as S does).

    o	There is a new command-line flag --args that causes the rest of
	the command line to be skipped (but recorded in commandArgs()
	for further processing).

    o	S4 generic functions and method dispatch have been modified to
	make the generic functions more self-contained (e.g., usable
	in apply-type operations) and potentially to speed dispatch.

    o	The data editor is no longer limited to 65535 rows, and will
	be substantially faster for large numbers of columns.

    o	Standalone Rmath now has a get_seed function as requested (PR#3160).

    o	GC timing is not enabled until the first call to gc.time(); it
	can be disabled by calling gc.time(FALSE).  This can speed up
	the garbage collector and reduce system calls on some
	platforms.

    o	textConnection() now has a local argument for use with output
	connections. local = TRUE means the variable containing the output
	is assigned in the frame of the caller.


STANDARD PACKAGES

    o	New package 'mle'. This is a simple package to find maximum
	likelihood estimates, and perform likelihood profiling and
	approximate confidence limits based upon it.  A well-behaved
	likelihood function is assumed, and it is the responsibility
	of the user to gauge the applicability of the asymptotic
	theory.  This package is based on S4 methods and classes.

    o	Changes in package 'mva':

	- factanal() now returns the test statistic and P-value formerly
	  computed in the print method.

	- heatmap() has many more arguments, partly thanks to Wolfgang
	  Huber and Andy Liaw.

	- Arguments `unit' and `hmin' of plclust() are now implemented.

	- prcomp() now accepts complex matrices, and there is biplot()
	  method for its output (in the real case).

	- dendrograms are slightly better documented, methods working with
	  "label", not "text" attribute.  New rev() method for dendrograms.

	- plot.dendrogram() has an explicit `frame.plot' argument
	  defaulting to FALSE (instead of an implicit one defaulting to TRUE).

    o	Changes in package 'tcltk':

	- The package is now in a namespace.  To remove it you will
	  now need to use unloadNamespace("tcltk").

	- The interface to Tcl has been made much more efficient by
	  evaluating Tcl commands via a vector of Tcl objects rather
	  than by constructing the string representation.

	- An interface to Tcl arrays has been introduced.

	- as.tclObj() has gained a `drop' argument to resolve an
	  ambiguity for vectors of length one.

    o	Changes in package 'tools':

	- Utilities for testing and listing files, manipulating file
	  paths, and delimited pattern matching are now exported.

	- Functions checkAssignFuns(), checkDocArgs() and checkMethods()
	  have been renamed to checkReplaceFuns(), checkDocFiles(), and
	  checkS3methods, to given better descriptions of what they do.

	- R itself is now used for analyzing the markup in the \usage
	  sections.  Hence in particular, replacement functions or S3
	  replacement methods are no longer ignored.

	- checkDocFiles() now also determines 'over-documented' arguments
	  which are given in the \arguments section but not in \usage.

	- checkDocStyle() and checkS3Methods() now know about internal S3
	  generics and S3 group generics.

	- S4 classes and methods are included in the QC tests.
	  Warnings will be issued from undoc() for classes  and
	  methods defined but not documented.  Default methods
	  automatically generated from nongeneric functions do not
	  need to be documented.

	- New (experimental) functions codocClasses() and codocData()
	  for code/documentation consistency checking for S4 classes and
	  data sets.

    o	Changes in package 'ts':

	- arima.sim() now checks for inconsistent order specification
	  (as requested in PR#3495: it was previously documented not to).

	- decompose() has a new argument `filter'.

	- HoltWinters() has new arguments `optim.start' and
	  `optim.control', and returns more components in the fitted
	  values.  The plot method allows `ylim' to be set.

	- plot.ts() has a new argument `nc' controlling the number of
	  columns (with default the old behaviour for plot.mts).

	- StructTS() now allows the first value of the series to be
	  missing (although it is better to omit leading NAs). (PR#3990)


USING PACKAGES

    o	library() has a pos argument, controlling where the package is
	attached (defaulting to pos=2 as before).

    o	require() now maintains a list of required packages in the
	toplevel environment (typically, .GlobalEnv).  Two features
	use this:  detach() now warns if a package is detached that is
	required by an attached package, and packages that install
	with saved images no longer need to use require() in the
	.First as well as in the main source.

    o	Packages with name spaces can now be installed using --save.

    o	Packages that use S4 classes and methods should now work with
	or without saved images (saved images are still recommended
	for efficiency), writing setMethod(), etc. calls with the
	default for argument `where'.  The topenv() function and
	sys.source() have been changed correspondingly.	 See the
	online help.

    o	Users can specify in the DESCRIPTION file the collation order
	for files in the R source directory of a package.


DOCUMENTATION CHANGES

    o	Changes in R documentation format:

	- New logical markup commands for emphasizing (\strong) and
	  quoting (\sQuote and \dQuote) text, for indicating the usage
	  of an S4 method (\S4method), and for indicating specific kinds
	  of text (\acronym, \cite, \command, \dfn, \env, \kbd, \option,
	  \pkg, \samp, \var).

	- New markup \preformatted for pre-formatted blocks of text
	  (like \example but within another section).  (Based on a
	  contribution by Greg Warnes.)

	- New markup \concept for concept index entries for use by
	  help.search().

    o	Rdconv now produces more informative output from the special
	\method{GENERIC}{CLASS} markup for indicating the usage of S3
	methods, providing the CLASS info in a comment.

    o	\dontrun sections are now marked within comments in the
	user-readable versions of the converted help pages.

    o	\dontshow is now the preferred name for \testonly.


INSTALLATION CHANGES

    o	The zlib code in the sources is used unless the external
	version found is at least version 1.1.4 (up from 1.1.3).

    o	The regression checks now have to be passed exactly, except
	those depending on recommended packages (which cannot be
	assumed to be present).

    o	The target make check-all now runs R CMD check on all the
	recommended packages (and not just runs their examples).

    o	There are new macros DYLIB_* for building dynamic libraries,
	and these are used for the dynamic Rmath library (which was
	previously built as a shared object).

    o	If a system function log1p is found, it is tested for accuracy
	and if inadequate the substitute function in src/nmath is
	used, with name remapped to Rlog1p.  (Apparently needed on
	OpenBSD/NetBSD.)


C-LEVEL FACILITIES

    o	There is a new installed header file R_ext/Parse.h which
	allows R_ParseVector to be called by those writing extensions.
	(Note that the interface is changed from that used in the
	unexported header Parse.h in earlier versions, and is not
	guaranteed to remain unchanged.)

    o	The header R_ext/Mathlib.h has been removed.  It was replaced by
	Rmath.h in R 1.2.0.

    o	PREXPR has been replaced by two macros, PREXPR for obtaining the
	expression and PRCODE for obtaining the code for use in eval.
	The macro BODY_EXPR has been added for use with closures.
	For a closure with a byte compiled body, the macro BODY_EXPR
	returns the expression that was compiled; if the body is not
	compiled then the body is returned.  This is to support byte
	compilation.

    o	Internal support for executing byte compiled code has been added.
	A compiler for producing byte compiled code will be made available
	separately and should become part of a future R release.

    o	On Unix-like systems calls to the popen() and system() C library
	functions now go through R_popen and R_system.	On Mac OS X these
	suspend SIGALRM interrupts around the library call.  (Related to
	PR#1140.)


UTILITIES

    o	R CMD check accepts "ORPHANED" as package maintainer. Package
	maintainers can now officially orphan a package, i.e., resign
	from maintaining a package.

    o	R CMD INSTALL (Unix only) is now 'safe': if the attempt to
	install a package fails, leftovers are removed.	 If the package
	was already installed, the old version is restored.

    o	R CMD build excludes possible (obsolete) data and vignette
	indices in DCF format (and hence also no longer rebuilds them).

    o	R CMD check now tests whether file names are valid across file
	systems and supported operating system platforms.  There is some
	support for code/documentation consistency checking for data
	sets and S4 classes.  Replacement functions and S3 methods in
	\usage sections are no longer ignored.

    o	R CMD Rdindex has been removed.


DEPRECATED & DEFUNCT

    o	The assignment operator `_' has been removed.

    o	printNoClass() is defunct.

    o	The classic MacOS port is no longer supported, and its files
	have been removed from the sources.

    o	The deprecated argument 'white' of parse() has been removed.

    o	Methods pacf/plot.mts() have been removed and their functionality
	incorporated into pacf.default/plot.ts().

    o	print.coefmat() is deprecated in favour of printCoefmat()
	(which is identical apart from the default for na.print which
	is changed from "" to "NA", and better handling of the 0-rank
	case where all coefficients are missing).

    o	codes() and codes<-() are deprecated, as almost all uses
	misunderstood what they actually do.

    o	The use of multi-argument return() calls is deprecated: use a
	(named) list instead.

    o	anovalist.lm (replaced in 1.2.0) is now deprecated.

    o	- and Ops methods for POSIX[cl]t objects are removed: the
	POSIXt methods have been used since 1.3.0.

    o	glm.fit.null(), lm.fit.null() and lm.wfit.null() are deprecated.

    o	Classes "lm.null" and "glm.null" are deprecated and all of their
	methods have been removed.

    o	Method weights.lm(), a copy of weights.default(), has been removed.

    o	print.atomic() is now deprecated.

    o	The back-compatibility entry point Rf_log1p in standalone
	Rmath has been removed.


BUG FIXES

    o	ARMAacf() sometimes gave too many results or failed if `lag.max'
	was used.

    o	Functions anova.glm(), contrasts(), getS3method(), glm() and
	make.tables() were applying get() without asking for a
	function and/or not starting the search in the environment of
	the caller.

    o	as.data.frame.matrix() ignored the `row.names' argument.

    o	as.data.frame.list(optional = TRUE) was converting names, and
	hence data.frame(list(...), check.names = FALSE) was.  (PR#3280)

    o	as.dist(m) {mva} now obeys `diag=TRUE' or `upper=TRUE' in all cases.

    o	as.double(list()) etc was regarded as an error, because of a
	bug in isVectorizable.

    o	On some platforms the wday component of the result of
	as.POSIXlt() was corrupted when trying to guess the DST offset
	at dates the OS was unable to handle.

    o	ave(x, g) didn't work when `g' had unused levels.

    o	biplot.default() allows xlim and ylim to be set.  (PR#3168)

    o	bgroup with a null (.) delimiter was setting font to Greek.  (PR#3099)

    o	body() and formals() were looking for named functions in
	different places: they now both look starting at the
	environment in which they are called.  Several documentation
	errors for these functions have been corrected.

    o	boxplot() was ignoring cex.axis.  (PR#2628)

    o	cut.POSIXt() now passes on ... to cut.default(), as documented.

    o	crossprod() now works for 1d arrays with unnamed dimnames (PR#4092).

    o	data() sometimes failed with multiple files, as the paths
	variable got corrupted.

    o	data.frame() failed with a nonsensical error message if it
	grabbed row names from an argument that was subsequently
	recycled.  Now they are discarded, with a warning.

    o	data.matrix() was documented to replace factors by their
	codes, but in fact re-coded into the alphabetical ordering of
	the levels.

    o	decompose() with even frequency used an asymmetric moving
	average window.

    o	demo() was using `topic' as a regexp rather than an exact match.

    o	dotchart() now does recycle the `color' argument and better
	documents the `bg' one (PR#4343).

    o	getAnywhere() didn't not correctly check for S3 methods, when
	the generic or the class name contains a "." (PR#4275).

    o	file.copy() ignored the overwrite argument.  (PR#3529)

    o	filter(method="recursive") was unnecessarily requiring the
	time series to be longer than the filter.

    o	format(*, nsmall = m) with m > 0 now returns exponential format
	less often.

    o	get() and exists() were ignoring the `mode' argument for
	variables in base.  The error message for get() now mentions
	the mode requested if not "any".  A bug in setting the NAMED
	field in do_get was fixed.

    o	getS3method(f, cl, optional=TRUE) now returns NULL if `f' does
	not exist.

    o	HoltWinters() would segfault if only gamma was optimized, and
	not converge if gamma=0 and seasonal="mult".

    o	hyperref.cfg now contains definitions for colors it uses.

    o	identify.default() detects zero-length arguments.  (PR#4057)

    o	legend() allows shading without filling again.

    o	legend(x, y, leg) doesn't triple `leg' anymore when it is a call.

    o	Corrected many problems with 0-rank (but not necessarily empty
	model) lm() and glm() fits.

    o	lm.influence() now handles 0-rank models, and names its output
	appropriately.	It also ensures that hat values are not greater
	than one, and rounds values within rounding error of one.

    o	The `method' argument to loess() did not work.	(PR#3332)

    o	lsfit() was returning incorrect residuals for 0-rank fits.

    o	methods("$") and methods("$<-") were failing to find methods.

    o	methods() and getS3method() find methods if the generic
	dispatches on a name other than its own.  (The cases of
	coefficients() and fitted.values() were fixed in 1.7.1.)

    o	model.matrix.default() was throwing an error on 0-term models,
	but now handles them correctly.

    o	Printing `nls' objects misbehaved when `data' was a composite
	expression.

    o	.NotYetImplemented() gave "Error in .NotYet...(): .."

    o	numericDeriv() was failing if the first argument was a name
	rather than a call.  (PR#3746)

    o	pacf() was failing if called on a one-column matrix.

    o	paste() applied to 0-length vectors gave "" not a 0-length vector.

    o	The length of a string specification of par(lty=) is now checked: it
	should be 2, 4, 6 or 8.

    o	Using lty=as.integer(NA) and as.double(NA) were being accepted
	but giving nonsensical results.	 Those are not documented
	valid values for lty.  (PR#3217)

    o	Erroneously calling par(new=TRUE) with no plot was not caught
	and so could result in invalid graphics files.	(PR#4037)

    o	par(tck=) was being interpreted incorrectly.  It is now
	documented in the same way as S, and now behaves as
	documented.  (PR#3504)

    o	plclust() [and hence plot.hclust()] sometimes now uses correct `ylim's
	also in unusual cases.	(PR#4197)

    o	plot.POSIX[cl]t no longer passes col, lty, lwd to axis.POSIXt.

    o	The png(), jpeg(), png() and win.metafile() devices now
	enforce the length limit on the filename.  (PR#3466)

    o	pnorm(x, 1, 0) does not give NaN anymore;
	also, pnorm(x, m, s=Inf) == lim{s -> Inf} pnorm(x,m,s).
	Similar changes for dnorm(), cf PR#1218.

    o	On some machines the internal rounding used in postscript() was
	imperfect, causing unnecessarily verbose output (-0.00 instead of
	0) and problems with make check.

    o	qqnorm()'s result now keeps NAs from its input. (PR#3750)

    o	rank() sometimes preserved and sometimes dropped names.

    o	readBin(what = "foo") didn't convert `what' to its type.  (PR#4043)

    o	reorder.dendrogram() now properly resets the "midpoint" attributes
	such that reorder()ed dendrograms now plot properly.

    o	rmultinom(1,100, c(3, 4, 2, 0,0))[3] was NA. (PR#4431)

    o	sapply() for matrix result does not return list(NULL,NULL) dimnames
	anymore.

    o	scan() now interprets quoting in fields to be skipped.	(PR#4128)

    o	seq.POSIXt(from, to, by="DSTday") was failing or calculating
	the length incorrectly.

    o	sort() and unique.default() were failing on 0-level factors.

    o	step() adds a fuzz for reduction in AIC for 0-df terms.	 (PR#3491)

    o	str(x) gives better output when x is of mode "(".  Its "dendrogram"
	method obeys the `give.attr' argument which now defaults to FALSE.

    o	strwidth(f) and strheight(f) could seg.fault when `f' was a
	function.  The fix [to C-level coerceVector()] now gives an error
	instead of passing through.  This may catch other potential
	problems.

    o	Sweave() reports the chunk number rather than the driver call when
	a try error gets caught.

    o	trunc.POSIXt(x) for 0-length x does not return invalid structures
	anymore.  (PR#3763).

    o	warnings() now returns NULL instead of an error when no warnings
	have occured yet.  (PR#4389)

    o	Using write.table() setting the `dec' argument and with no
	numeric columns failed.	 (PR#3532)

    o	$<- did not duplicate when it needed to.

    o	Recursive indexing of lists had too little error-checking.
	(related to PR#3324)

    o	Removed warning about names in persistent strings when a
	namespace is saved.

    o	Fixed some malformed error messages in the methods package.

    o	pipes were not opening properly when profiling on a Mac. (PR#1140)

    o	Lapack error messages (PR#3494) and call to DGEQP3 (PR#2867) are
	corrected.

    o	Rd conversion was limiting a file to 1000 pairs of braces,
	without any warning.  Now the limit is 10000, with a warning.
	(PR#3400)

    o	In the tcltk package, the tkimage.*() commands were defined
	nonsensically as widget commands. They have been redefined to be
	more useful now.

    o	Registered group generics were not being used. (PR#3536)

    o	Subsetting data frames did not always correctly detect that
	non-existent columns were specified.

    o	There are many more checks for over-running internal buffers,
	almost always reporting errors.

    o	Added some buffer overflow checking in gram.y.

    o	Internals for complex assignment did not check that function name
	was a symbol, which could cause a segfault.

    o	Fixed bug in S4 methods dispatch that made local variables in the
	generic visible when executing the body of a method, thus violating
	lexical scope.



-- 
   O__  ---- Peter Dalgaard             Blegdamsvej 3  
  c/ /'_ --- Dept. of Biostatistics     2200 Cph. N   
 (*) \(*) -- University of Copenhagen   Denmark      Ph: (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)             FAX: (+45) 35327907

_______________________________________________
R-announce at stat.math.ethz.ch mailing list
https://www.stat.math.ethz.ch/mailman/listinfo/r-announce



From ripley at stats.ox.ac.uk  Wed Oct  8 14:51:29 2003
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Wed, 8 Oct 2003 13:51:29 +0100 (BST)
Subject: [R] Contrast specified with C() - R vs S-Plus problem
In-Reply-To: <3F8400C4.8040503@unibas.ch>
Message-ID: <Pine.LNX.4.44.0310081346190.11596-100000@gannet.stats>

On Wed, 8 Oct 2003, Pascal A. Niklaus wrote:

> Hi,
> 
> For a n-level factor, I'd like to specify the first contrast and have
> the remaining n-2 constructed automatically so that the set is
> orthogonal. I then test the contrasts with summary.lm(anova-object).
> 
> In S-Plus, the following works:
> 
>     >y.anova <- aov( y ~ C(CO2,c(1,0,-1)) )
>     >summary.lm(y.anova)

I can't reproduce that in S-PLUS 6.1, and it is not as documented:

   contr
          what contrasts to use. May be one of four standard names ( helmert,
          poly, treatment, or sum), a function, or a matrix with as many rows as
          there are levels to the factor.

You haven't given a matrix, and your vector gets converted to a 3x1 
matrix when there are four levels.  I suspect you have not got the same 
data in S-PLUS and R, but you haven't given us anything to check that.


> In R, it fails with the following error:
> 
>     >levels(CO2)
>     [1] ""  "A" "C" "E"
> 
>     >y.anova <- aov(y + C(CO2,c(1,0,-1)) )
>     Error in "contrasts<-"(*tmp*, value = contr) :
>             wrong number of contrast matrix rows
> 
> What is the way to do this in R?

There are four levels, so

> CO2 <- factor(c("", "A", "C", "E"))
> attributes(C(CO2, as.matrix(1:4)))
$levels
[1] ""  "A" "C" "E"

$class
[1] "factor"

$contrasts
  [,1]       [,2]       [,3]
     1  0.0236068  0.5472136
A    2 -0.4393447 -0.7120227
C    3  0.8078689 -0.2175955
E    4 -0.3921311  0.3824045

does as you ask.

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From spencer.graves at pdf.com  Wed Oct  8 15:07:31 2003
From: spencer.graves at pdf.com (Spencer Graves)
Date: Wed, 08 Oct 2003 06:07:31 -0700
Subject: [R] fitdistr, mle's and gamma distribution
In-Reply-To: <1065614184.600.193.camel@stochastic>
References: <1064950780.547.59.camel@stochastic> <3F79E26F.7000104@pdf.com>
	<1065614184.600.193.camel@stochastic>
Message-ID: <3F840C13.7060001@pdf.com>

      I'm sorry, but I don't have time to read all your code.  However, 
I saw that you tested for x > alpha in your Pareto distribution 
example.  Have you considered reparameterizing to estimate log.del = 
log(alpha-min(x))?  Pass log.del as part of the vector of parameters to 
estimate, then immediately inside the function, compute

      alpha <- (min(x)+exp(log.del))

       I've fixed many convergence problems with simple tricks like this. 

      hope this helps.  spencer graves

Lourens Olivier Walters wrote:

>Thanks, your advice worked. I don't have much experience with maths, and
>therefore tried to stay away from dealing with optimization, but going
>down to this level opens a lot of possibilities. For the record, the
>code I used, as you suggested:
>
>###############
>shape <- mean(data)^2/var(data)
>scale <- var(data)/mean(data)
>gamma.param1 <- shape
>gamma.param2 <- scale
>log.gamma.param1 <- log(gamma.param1)
>log.gamma.param2 <- log(gamma.param2)
>                                                                                                                                              gammaLoglik <- function(params, negative=TRUE){
>   lglk <- sum(dgamma(data, shape=exp(params[1]), scale=exp(params[2]),
>log=TRUE))
>   if(negative)
>      return(-lglk)
>   else
>      return(lglk)
>}
>
>optim.list <- optim(c(log.gamma.param1, log.gamma.param2), gammaLoglik)
>gamma.param1 <- exp(optim.list$par[1])
>gamma.param2 <- exp(optim.list$par[2])
>################
>
>optim converges and the parameters provide a much better fit to the data
>than the method of moments parameters do. 
>
>Armed with this new knowledge, I attempted to write a log(likelihood)
>optimization method for the Pareto distribution. My ignorance of math
>however shows, as the code does not work. 
>
>Here is what I did:
>
>################
>pareto.density <- function(x, alpha, beta, log=FALSE){
>   # Pareto distribution not defined for x < alpha
>   # Test for values x < alpha, taking into account floating point error
>   test.vals <- x-alpha
>   error.vals <- test.vals[test.vals<0]
>   if (length(error.vals)>0)
>      stop("\nERROR: x > alpha in pareto.distr(x, alpha, beta,
>+ log=FALSE)")
>   density <- beta * (alpha^beta) * (x^(-beta-1))
>   if(log)
>      return(log(density))
>   else
>      return(density)
>}
>
>data.sorted <- sort(data)
>alpha.val <- data.sorted[1]
>beta.val <- 1/((1/n) * sum(log(data.sorted/alpha.val)))
>log.alpha.val <- log(alpha.val)
>log.beta.val <- log(beta.val)
>
>paretoLoglik <- function(params, negative=TRUE){
>   lglk <- sum(pareto.density(data.sorted, alpha=exp(params[1]),
>+ beta=exp(params[2]), log=TRUE))
>   if(negative)
>      return(-lglk)
>   else
>      return(lglk)
>}
>
>optim.list <- optim(c(log.alpha.val, log.beta.val), paretoLoglik,
>+ method="L-BFGS-B", lower=c(log.alpha.val, 0), upper=c(log.alpha.val,
>+ Inf))
>pareto.param1 <- exp(optim.list$par[1])
>pareto.param2 <- exp(optim.list$par[2])
>#################
>
>I fixed the alpha parameter as my Pareto density function produces an
>error if a datavalue > alpha.
>
>I get the following output:
>
>  
>
>>source("browsesessoffplotfitted.R")
>>    
>>
>Error in optim(c(log.alpha.val, log.beta.val), paretoLoglik, method =
>+ "L-BFGS-B",  :
>        non-finite finite-difference value [0]
>
>Any ideas would be appreciated, otherwise its back to method of moments
>for the Pareto distribution for me :)
>
>Thanks
>Lourens
>
>On Tue, 2003-09-30 at 22:07, Spencer Graves wrote:
>
>  
>
>>      In my experience, the most likely cause of this problem is that 
>>optim may try to test nonpositive values for shape or scale.  I avoid 
>>this situation by programming the log(likelihood) in terms of log(shape) 
>>and log(scale) as follows: 
>>
>> > gammaLoglik <-
>>+ function(x, logShape, logScale, negative=TRUE){
>>+ lglk <- sum(dgamma(x, shape=exp(logShape), scale=exp(logScale),
>>+ log=TRUE))
>>+ if(negative) return(-lglk) else return(lglk)
>>+ }
>> > tst <- rgamma(10, 1)
>> > gammaLoglik(tst, 0, 0)
>>[1] 12.29849
>>
>>Then I then call optim directly to minimize the negative of the 
>>log(likelihood). 
>>
>>      If I've guessed correctly, this should fix the problem.  If not, 
>>let us know. 
>>
>>      hope this helps.  spencer graves
>>    
>>
>
>______________________________________________
>R-help at stat.math.ethz.ch mailing list
>https://www.stat.math.ethz.ch/mailman/listinfo/r-help
>  
>



From maechler at stat.math.ethz.ch  Wed Oct  8 15:06:24 2003
From: maechler at stat.math.ethz.ch (Martin Maechler)
Date: Wed, 8 Oct 2003 15:06:24 +0200
Subject: [R] Why does a[which(b == c[d])] not work?
In-Reply-To: <3F83D87E.2060807@ptb.de>
References: <3F83D87E.2060807@ptb.de>
Message-ID: <16260.3024.956280.549340@gargle.gargle.HOWL>

Your question has been answered by Achim and Peter Dalgaard (at least).

Just a note:

Using  
       a[which(logic)] 
looks like a clumsy and inefficient way of writing
       a[ logic ]

and I think you shouldn't propagate its use ...

Martin Maechler <maechler at stat.math.ethz.ch>	http://stat.ethz.ch/~maechler/
Seminar fuer Statistik, ETH-Zentrum  LEO C16	Leonhardstr. 27
ETH (Federal Inst. Technology)	8092 Zurich	SWITZERLAND
phone: x-41-1-632-3408		fax: ...-1228			<><



From Arne.Muller at aventis.com  Wed Oct  8 15:14:00 2003
From: Arne.Muller at aventis.com (Arne.Muller@aventis.com)
Date: Wed, 8 Oct 2003 15:14:00 +0200
Subject: [R] New R - recompiling all packages
Message-ID: <C80ECAFA2ACC1B45BE45D133ED660ADECDCC07@crbsmxsusr04.pharma.aventis.com>

Hi All,

I'm running R 1.7.1, and I've installed some additional packages such a
Bioconductor. Do I've to re-install all the additional packages when ugrading
to R 1.8.0 (i.e. are there compile in dependencies)?

	thanks for your help,

	Arne



From Pascal.Niklaus at unibas.ch  Wed Oct  8 15:59:01 2003
From: Pascal.Niklaus at unibas.ch (Pascal A. Niklaus)
Date: Wed, 08 Oct 2003 15:59:01 +0200
Subject: [R] Contrast specified with C() - R vs S-Plus problem
In-Reply-To: <Pine.LNX.4.44.0310081346190.11596-100000@gannet.stats>
References: <Pine.LNX.4.44.0310081346190.11596-100000@gannet.stats>
Message-ID: <3F841825.8060007@unibas.ch>

Thanks for the reply. Below is the solution and the S-Plus and R code
that does the same (for documentation).

>I can't reproduce that in S-PLUS 6.1, and it is not as documented:
>  
>
In S-Plus 2000, C() complements the contrast matrix with orthogonal 
contrasts if only the first is given.

> CO2 <- factor(rep(c("A","C","E"),each=8))

> m <- rep(seq(0,3,length=8),3) + rep(c(0,1,2),each=8)

> summary.aov(aov(m ~ C(CO2,c(1,0,-1))))

                    Df Sum of Sq  Mean Sq  F Value       Pr(F)

C(CO2, c(1, 0, -1))  2  16.00000 8.000000 7.259259 0.004013532

          Residuals 21  23.14286 1.102041

> summary.lm(aov(m ~ C(CO2,c(1,0,-1))))

Coefficients:

                        Value Std. Error  t value Pr(>|t|)

         (Intercept)   2.5000   0.2143    11.6667   0.0000

C(CO2, c(1, 0, -1))1  -1.0000   0.2624    -3.8103   0.0010

C(CO2, c(1, 0, -1))2   0.0000   0.3712     0.0000   1.0000

Residual standard error: 1.05 on 21 degrees of freedom

Multiple R-Squared: 0.4088

F-statistic: 7.259 on 2 and 21 degrees of freedom, the p-value is 0.004014

Correlation of Coefficients:

                     (Intercept) C(CO2, c(1, 0, -1))1

C(CO2, c(1, 0, -1))1 0

C(CO2, c(1, 0, -1))2 0           0


In the meanwhile, I found that the same can be done in R like this:

> library(gregmisc)

> CO2 <- factor(rep(c("A","C","E"),each=8))

> m <- rep(seq(0,3,length=8),3) + rep(c(0,1,2),each=8)

> contrastsCO2 <- rbind("A vs E"=c(1,0,-1))

> a <- aov(m ~ CO2, contrasts=list( CO2=make.contrasts(contrastsCO2)))

> summary(a)

            Df Sum Sq Mean Sq F value   Pr(>F)

CO2          2 16.000   8.000  7.2593 0.004014 **

Residuals   21 23.143   1.102

> summary.lm(a)

Coefficients:

              Estimate Std. Error  t value Pr(>|t|)

(Intercept)  2.500e+00  2.143e-01    11.67 1.22e-10 ***

CO2A vs E   -2.000e+00  5.249e-01    -3.81  0.00102 **

CO2C2        9.774e-17  3.712e-01 2.63e-16  1.00000

---

Signif. codes:  0 `***' 0.001 `**' 0.01 `*' 0.05 `.' 0.1 ` ' 1

Residual standard error: 1.05 on 21 degrees of freedom

Multiple R-Squared: 0.4088,     Adjusted R-squared: 0.3525

F-statistic: 7.259 on 2 and 21 DF,  p-value: 0.004014



From ripley at stats.ox.ac.uk  Wed Oct  8 15:58:50 2003
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Wed, 8 Oct 2003 14:58:50 +0100 (BST)
Subject: [R] New R - recompiling all packages
In-Reply-To: <C80ECAFA2ACC1B45BE45D133ED660ADECDCC07@crbsmxsusr04.pharma.aventis.com>
Message-ID: <Pine.LNX.4.44.0310081455030.12286-100000@gannet.stats>

Most packages do not need to be re-installed, but R 1.8.0 has some nicer 
formatting of help pages so you may want to do so, and I am reinstalling 
all packages that make use of the methods package and so have saved 
images.

For Bioconductor you need to ask on their list, but my understanding is 
that many of the packages have been updated for 1.8.0 and so you need to 
get new versions.  Packages effects and tree have updated versions for 
1.8.0 which update.packages() will pick up once your CRAN mirror gets 
updated (they are currently in 1.8.0/Other).

On Wed, 8 Oct 2003 Arne.Muller at aventis.com wrote:

> Hi All,
> 
> I'm running R 1.7.1, and I've installed some additional packages such a
> Bioconductor. Do I've to re-install all the additional packages when ugrading
> to R 1.8.0 (i.e. are there compile in dependencies)?
> 
> 	thanks for your help,
> 
> 	Arne

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From lwalters at cs.uct.ac.za  Wed Oct  8 16:00:17 2003
From: lwalters at cs.uct.ac.za (Lourens Olivier Walters)
Date: Wed, 08 Oct 2003 16:00:17 +0200
Subject: [R] fitdistr, mle's and gamma distribution
In-Reply-To: <Pine.LNX.4.44.0309301725130.25293-100000@bolker.zoo.ufl.edu>
References: <Pine.LNX.4.44.0309301725130.25293-100000@bolker.zoo.ufl.edu>
Message-ID: <1065621617.4701.204.camel@stochastic>

Thanks for the help, the wrapper function was very useful. I managed to
solve the problem using Spencer Graves' suggestion. I am analyzing the
interarrival times between HTTP packets on a campus network. The dataset
actually has more than 14 Million entries! It represents the traffic
generated by approximately 3000 users browsing the web for 30 days. I
have to be careful to always remove unused objects from my workspace,
but otherwise I have so far managed to cope with 512Mb of memory on a
Pentium 600Mhz. 

Lourens

On Tue, 2003-09-30 at 23:25, Ben Bolker wrote:
>   PS.  11 MILLION entries??
> 
> On Tue, 30 Sep 2003, Ben Bolker wrote:
> 
> > 
> >   Spencer Graves's suggestion of using shape and scale parameters on a log 
> > scale is a good one.
> > 
> >   To do specifically what you want (check values for which the objective 
> > function is called and see what happens) you can do the following 
> > (untested!), which makes a local copy of dgamma that you can mess with:
> > 
> > dgamma.old <- dgamma
> > dgamma <- function(x,shape,rate,...) {
> >    d <- dgamma.old(x,shape,rate,...)
> >    cat(shape,rate,d,"\n")
> >    return(d)
> > }



From A.Nuzzo at motorola.com  Wed Oct  8 16:13:35 2003
From: A.Nuzzo at motorola.com (Nuzzo Art-CINT116)
Date: Wed, 8 Oct 2003 09:13:35 -0500 
Subject: [R] Bootstrap Question
Message-ID: <A752C16E6296D711942200065BFCB6949E0C8E@il02exm10>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20031008/8776a501/attachment.pl

From Arne.Muller at aventis.com  Wed Oct  8 16:25:13 2003
From: Arne.Muller at aventis.com (Arne.Muller@aventis.com)
Date: Wed, 8 Oct 2003 16:25:13 +0200
Subject: [R] updating via CRAN and http
Message-ID: <C80ECAFA2ACC1B45BE45D133ED660ADECDCC0A@crbsmxsusr04.pharma.aventis.com>

Hello,

thanks for the tips on updating packages for 1.8.0. The updating is a real
problem for me, since I've to do it sort of manually using my web-browser or
wget. I'm behind a firewall that requires http/ftp authentification (username
and passwd) for every request it sends to a server outside our intranet.
Therefore all the nice tools for automatic updating (cran, cpan ...) don't
for me (I've tried).

I understand that the non-paranoid rest of the world can't be bothered, but
is there any intenstion to include such authentification into the update
procedures of R? I think for ftp it's kind of tricky, but at least for http
the authentification seems to be straight forward.

	kind regards,

	Arne



From ripley at stats.ox.ac.uk  Wed Oct  8 16:35:03 2003
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Wed, 8 Oct 2003 15:35:03 +0100 (BST)
Subject: [R] Contrast specified with C() - R vs S-Plus problem
In-Reply-To: <3F841825.8060007@unibas.ch>
Message-ID: <Pine.LNX.4.44.0310081531250.22123-100000@gannet.stats>

On Wed, 8 Oct 2003, Pascal A. Niklaus wrote:

> Thanks for the reply. Below is the solution and the S-Plus and R code
> that does the same (for documentation).
> 
> >I can't reproduce that in S-PLUS 6.1, and it is not as documented:
> >  
> >
> In S-Plus 2000, C() complements the contrast matrix with orthogonal 
> contrasts if only the first is given.

And so does R.  You just did not specify it correctly in R.  The code you 
quote works in R as well, but your original example had four levels, not 
three.

>  CO2 <- factor(rep(c("A","C","E"),each=8))
> m <- rep(seq(0,3,length=8),3) + rep(c(0,1,2),each=8)
> summary.aov(aov(m ~ C(CO2,c(1,0,-1))))
                    Df Sum Sq Mean Sq F value   Pr(>F)
C(CO2, c(1, 0, -1))  2 16.000   8.000  7.2593 0.004014
Residuals           21 23.143   1.102

Please do not make out your user errors to be S-R differences.
Did you actually try your example in R?

> > CO2 <- factor(rep(c("A","C","E"),each=8))
> 
> > m <- rep(seq(0,3,length=8),3) + rep(c(0,1,2),each=8)
> 
> > summary.aov(aov(m ~ C(CO2,c(1,0,-1))))
> 
>                     Df Sum of Sq  Mean Sq  F Value       Pr(F)
> 
> C(CO2, c(1, 0, -1))  2  16.00000 8.000000 7.259259 0.004013532
> 
>           Residuals 21  23.14286 1.102041
> 
> > summary.lm(aov(m ~ C(CO2,c(1,0,-1))))
> 
> Coefficients:
> 
>                         Value Std. Error  t value Pr(>|t|)
> 
>          (Intercept)   2.5000   0.2143    11.6667   0.0000
> 
> C(CO2, c(1, 0, -1))1  -1.0000   0.2624    -3.8103   0.0010
> 
> C(CO2, c(1, 0, -1))2   0.0000   0.3712     0.0000   1.0000
> 
> Residual standard error: 1.05 on 21 degrees of freedom
> 
> Multiple R-Squared: 0.4088
> 
> F-statistic: 7.259 on 2 and 21 degrees of freedom, the p-value is 0.004014
> 
> Correlation of Coefficients:
> 
>                      (Intercept) C(CO2, c(1, 0, -1))1
> 
> C(CO2, c(1, 0, -1))1 0
> 
> C(CO2, c(1, 0, -1))2 0           0
> 
>
-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From ggrothendieck at myway.com  Wed Oct  8 16:54:47 2003
From: ggrothendieck at myway.com (Gabor Grothendieck)
Date: Wed,  8 Oct 2003 10:54:47 -0400 (EDT)
Subject: is.na(v)<-b (was: Re: [R] Beginner's query - segmentation fault)
Message-ID: <20031008145447.6A08339BA@xmxpita.myway.com>



Also, presumably is.na<- could be redefined by the user for particular
classes so if you got in the habit of setting NAs that way it would
generalize better.

--- 
Date: Wed, 8 Oct 2003 11:49:29 +0100 (BST) 
From: Prof Brian Ripley <ripley at stats.ox.ac.uk>

I don't think it can ever `go wrong', but it can do things other than the 
user intends. The intention of is.na<- is clearer, and so perhaps user 
error is less likely? That is the thinking behind the function, anyway.



_______________________________________________
No banners. No pop-ups. No kidding.
Introducing My Way - http://www.myway.com



From tplate at blackmesacapital.com  Wed Oct  8 16:58:32 2003
From: tplate at blackmesacapital.com (Tony Plate)
Date: Wed, 08 Oct 2003 08:58:32 -0600
Subject: [R] Why does a[which(b == c[d])] not work?
In-Reply-To: <16260.3024.956280.549340@gargle.gargle.HOWL>
References: <3F83D87E.2060807@ptb.de>
 <3F83D87E.2060807@ptb.de>
Message-ID: <5.2.1.1.2.20031008085253.041d3830@mailhost.blackmesacapital.com>

At Wednesday 03:06 PM 10/8/2003 +0200, Martin Maechler wrote:
>Your question has been answered by Achim and Peter Dalgaard (at least).
>
>Just a note:
>
>Using
>        a[which(logic)]
>looks like a clumsy and inefficient way of writing
>        a[ logic ]
>
>and I think you shouldn't propagate its use ...

What then is the recommended way of treating an NA in the logical subset as 
a FALSE? (Or were you just talking about the given example, which didn't 
have this issue.  However, you admonition seemed more general.)

As in:
 > x <- 1:4
 > y <- c(1,2,NA,4)
 > x[y %% 2 == 0]
[1]  2 NA  4
 > x[which(y %% 2 == 0)]
[1] 2 4
 >

Sometimes one might want the first result, but more usually, I want the 
second, and using which() seems a convenient way to get it.

-- Tony Plate



From spencer.graves at pdf.com  Wed Oct  8 17:05:47 2003
From: spencer.graves at pdf.com (Spencer Graves)
Date: Wed, 08 Oct 2003 08:05:47 -0700
Subject: [R] fitdistr, mle's and gamma distribution
In-Reply-To: <1065621617.4701.204.camel@stochastic>
References: <Pine.LNX.4.44.0309301725130.25293-100000@bolker.zoo.ufl.edu>
	<1065621617.4701.204.camel@stochastic>
Message-ID: <3F8427CB.6060107@pdf.com>

      Are you interested in turning that into a monitor, processing each 
day's data sequentially or even each entry as it arrived?  If yes, you 
may wish to evaluate the "Foundations of Monitoring" documents 
downloadable from "www.prodsyse.com".  If you have any questions about 
that, I might be able to help. 

      hope this helps.  spencer graves

Lourens Olivier Walters wrote:

>Thanks for the help, the wrapper function was very useful. I managed to
>solve the problem using Spencer Graves' suggestion. I am analyzing the
>interarrival times between HTTP packets on a campus network. The dataset
>actually has more than 14 Million entries! It represents the traffic
>generated by approximately 3000 users browsing the web for 30 days. I
>have to be careful to always remove unused objects from my workspace,
>but otherwise I have so far managed to cope with 512Mb of memory on a
>Pentium 600Mhz. 
>
>Lourens
>
>On Tue, 2003-09-30 at 23:25, Ben Bolker wrote:
>  
>
>>  PS.  11 MILLION entries??
>>
>>On Tue, 30 Sep 2003, Ben Bolker wrote:
>>
>>    
>>
>>>  Spencer Graves's suggestion of using shape and scale parameters on a log 
>>>scale is a good one.
>>>
>>>  To do specifically what you want (check values for which the objective 
>>>function is called and see what happens) you can do the following 
>>>(untested!), which makes a local copy of dgamma that you can mess with:
>>>
>>>dgamma.old <- dgamma
>>>dgamma <- function(x,shape,rate,...) {
>>>   d <- dgamma.old(x,shape,rate,...)
>>>   cat(shape,rate,d,"\n")
>>>   return(d)
>>>}
>>>      
>>>
>
>______________________________________________
>R-help at stat.math.ethz.ch mailing list
>https://www.stat.math.ethz.ch/mailman/listinfo/r-help
>  
>



From m.mader at gsf.de  Wed Oct  8 17:13:52 2003
From: m.mader at gsf.de (Michael Mader)
Date: Wed, 08 Oct 2003 17:13:52 +0200
Subject: [R] Rdinfo verbosity increased in 1.8.0?
Message-ID: <3F8429B0.C50AD156@gsf.de>

Hi,

my out-of-the-box installation of R-1.8.0 on Tru64 (OSFv5.1) results in
an tremendously increased verbosity of Rdinfo.

Is it really intended that Rdinfo complaints about "minor mistakes" like
missing empty lines at the end given the very variable quality of
Rd-pages?

Is there a way to decrease Rdinfo's standard verbosity?

Regards

Michael
-- 
Michael T. Mader
Institute for Bioinformatics/MIPS, GSF
Ingolstaedter Landstrasse 1
D-80937 Neuherberg
0049-89-3187-3576

XML can be a simplifying choice or a complicating one. There is a lot of
hype surrounding it, but don't become a fashion victim by either
adopting or rejecting it uncritically. Choose carefully and bear the
KISS principle in mind.
	Eric Steven Raymond: The Art of Unix Programming, 2003



From ripley at stats.ox.ac.uk  Wed Oct  8 17:19:59 2003
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Wed, 8 Oct 2003 16:19:59 +0100 (BST)
Subject: [R] updating via CRAN and http
In-Reply-To: <C80ECAFA2ACC1B45BE45D133ED660ADECDCC0A@crbsmxsusr04.pharma.aventis.com>
Message-ID: <Pine.LNX.4.44.0310081617230.524-100000@gannet.stats>

On Wed, 8 Oct 2003 Arne.Muller at aventis.com wrote:

> Hello,
> 
> thanks for the tips on updating packages for 1.8.0. The updating is a real
> problem for me, since I've to do it sort of manually using my web-browser or
> wget. I'm behind a firewall that requires http/ftp authentification (username
> and passwd) for every request it sends to a server outside our intranet.
> Therefore all the nice tools for automatic updating (cran, cpan ...) don't
> for me (I've tried).
> 
> I understand that the non-paranoid rest of the world can't be bothered, but
> is there any intenstion to include such authentification into the update
> procedures of R? I think for ftp it's kind of tricky, but at least for http
> the authentification seems to be straight forward.

It's available for http: see ?download.file, and you can even configure 
that to use wget.

Your comments are very much misplaced: we *have* bothered to provide the 
facilities for you.

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From bates at stat.wisc.edu  Wed Oct  8 17:40:24 2003
From: bates at stat.wisc.edu (Douglas Bates)
Date: Wed, 08 Oct 2003 15:40:24 -0000
Subject: [R] R-1.8.0 sources available via rsync
Message-ID: <6r7k3fbvcj.fsf@bates4.stat.wisc.edu>

The sources for R-1.8.0 are now available via rsync to the host
rsync.r-project.org

Use

 rsync rsync.r-project.org::

to get the listing of possible versions.

_______________________________________________
R-announce at stat.math.ethz.ch mailing list
https://www.stat.math.ethz.ch/mailman/listinfo/r-announce



From tord.snall at ebc.uu.se  Wed Oct  8 17:47:24 2003
From: tord.snall at ebc.uu.se (Tord Snall)
Date: Wed, 08 Oct 2003 17:47:24 +0200
Subject: [R] binomial glm warnings revisited
Message-ID: <3.0.6.32.20031008174724.00c59560@mail.anst.uu.se>

Dear all,

Last autumn there was some discussion on the list of the warning
Warning message: 
fitted probabilities numerically 0 or 1 occurred in: (if
(is.empty.model(mt)) glm.fit.null else glm.fit)(x = X, y = Y,  

when fitting binomial GLMs with many 0 and few 1.

Parts of replies:
"You should be able to tell which coefficients are infinite -- the
coefficients and their standard errors will be large. When this happens the
standard errors and the p-values reported by summary.glm() for those
variables are useless."
"My guess is that the deviances and coefficients are entirely ok. I'd
expect that problems in the general area that Thomas mentions to reveal
themselves as a failure to converge."

I have this problem with my data. In a GLM, I have 269 zeroes and only 1 one:

summary(dbh)
Coefficients:
            Estimate Std. Error z value Pr(>|z|)
(Intercept)   0.1659     3.8781   0.043    0.966
dbh          -0.5872     0.5320  -1.104    0.270

> drop1(dbh, test = "Chisq")
Single term deletions
Model:
MPext ~ dbh
       Df Deviance     AIC     LRT Pr(Chi)  
<none>      9.9168 13.9168                  
dbh     1  13.1931 15.1931  3.2763 0.07029 .

I now wonder, is the drop1() function output 'reliable'?

If so, is then the estimates from MASS confint() also 'reliable'? It gives
the same warning.

Waiting for profiling to be done...
                2.5 %      97.5 %
(Intercept) -6.503472 -0.77470556
abund       -1.962549 -0.07496205
There were 20 warnings (use warnings() to see them)


Thanks in advance for your reply.


Sincerely,
Tord




-----------------------------------------------------------------------
Tord Sn?ll
Avd. f v?xtekologi, Evolutionsbiologiskt centrum, Uppsala universitet
Dept. of Plant Ecology, Evolutionary Biology Centre, Uppsala University
Villav?gen 14			
SE-752 36 Uppsala, Sweden
Tel: 018-471 28 82 (int +46 18 471 28 82) (work)
Tel: 018-25 71 33 (int +46 18 25 71 33) (home)
Fax: 018-55 34 19 (int +46 18 55 34 19) (work)
E-mail: Tord.Snall at ebc.uu.se
Check this: http://www.vaxtbio.uu.se/resfold/snall.htm!



From mmarques at inescporto.pt  Wed Oct  8 17:40:46 2003
From: mmarques at inescporto.pt (Mark Marques)
Date: Wed, 8 Oct 2003 16:40:46 +0100
Subject: [R] Lattice cloud() funtion bug in R1.8.0beta
Message-ID: <70535265859.20031008164046@power.inescn.pt>


  Cloud() function does not display anything with R1.8.0beta
  in WindowsXP ...
  Does any one noticed this ?
  others functions from lattice seem working properly.
  does it work in the "final" 1.8.0 for windows ?



From Arne.Muller at aventis.com  Wed Oct  8 17:43:21 2003
From: Arne.Muller at aventis.com (Arne.Muller@aventis.com)
Date: Wed, 8 Oct 2003 17:43:21 +0200
Subject: [R] updating via CRAN and http
Message-ID: <C80ECAFA2ACC1B45BE45D133ED660ADECDCC0D@crbsmxsusr04.pharma.aventis.com>

Sorry, I didn' mean it the nasty way. I wouldn't have been surprised if the
R-team had told me the authentification with the firewall is my problem (i.e.
a special case that cannot be dealt with by th R-team). 

Yess, and off course I should have had a much closer lookk into the docu.
Thanks again for the hint + please forgive!

	+regards,

	Arne

> -----Original Message-----
> From: Prof Brian Ripley [mailto:ripley at stats.ox.ac.uk]
> Sent: 08 October 2003 17:20
> To: Muller, Arne PH/FR
> Cc: r-help at stat.math.ethz.ch
> Subject: Re: [R] updating via CRAN and http
> 
> 
> On Wed, 8 Oct 2003 Arne.Muller at aventis.com wrote:
> 
> > Hello,
> > 
> > thanks for the tips on updating packages for 1.8.0. The 
> updating is a real
> > problem for me, since I've to do it sort of manually using 
> my web-browser or
> > wget. I'm behind a firewall that requires http/ftp 
> authentification (username
> > and passwd) for every request it sends to a server outside 
> our intranet.
> > Therefore all the nice tools for automatic updating (cran, 
> cpan ...) don't
> > for me (I've tried).
> > 
> > I understand that the non-paranoid rest of the world can't 
> be bothered, but
> > is there any intenstion to include such authentification 
> into the update
> > procedures of R? I think for ftp it's kind of tricky, but 
> at least for http
> > the authentification seems to be straight forward.
> 
> It's available for http: see ?download.file, and you can even 
> configure 
> that to use wget.
> 
> Your comments are very much misplaced: we *have* bothered to 
> provide the 
> facilities for you.
> 
> -- 
> Brian D. Ripley,                  ripley at stats.ox.ac.uk
> Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
> University of Oxford,             Tel:  +44 1865 272861 (self)
> 1 South Parks Road,                     +44 1865 272866 (PA)
> Oxford OX1 3TG, UK                Fax:  +44 1865 272595
> 
>



From edd at debian.org  Wed Oct  8 17:52:52 2003
From: edd at debian.org (Dirk Eddelbuettel)
Date: Wed, 8 Oct 2003 10:52:52 -0500
Subject: [R] updating via CRAN and http
In-Reply-To: <C80ECAFA2ACC1B45BE45D133ED660ADECDCC0A@crbsmxsusr04.pharma.aventis.com>
References: <C80ECAFA2ACC1B45BE45D133ED660ADECDCC0A@crbsmxsusr04.pharma.aventis.com>
Message-ID: <20031008155252.GA6538@sonny.eddelbuettel.com>

On Wed, Oct 08, 2003 at 04:25:13PM +0200, Arne.Muller at aventis.com wrote:
> Hello,
> 
> thanks for the tips on updating packages for 1.8.0. The updating is a real
> problem for me, since I've to do it sort of manually using my web-browser or
> wget. I'm behind a firewall that requires http/ftp authentification (username
> and passwd) for every request it sends to a server outside our intranet.
> Therefore all the nice tools for automatic updating (cran, cpan ...) don't
> for me (I've tried).

Please try to read the help pages, FAQ or mailing list archives. Trust me
that you are not the only corporate user around here. This question comes up
really frequently.

> I understand that the non-paranoid rest of the world can't be bothered, but
> is there any intenstion to include such authentification into the update
> procedures of R? I think for ftp it's kind of tricky, but at least for http
> the authentification seems to be straight forward.

Years ago a small patch of mine was integrated which allowed for wget as a
download methid.  Set e.g.

	options(download.file.method="wget")
	options(CRAN="http://cran.us.r-project.org")

in ~/.Rprofile. Get yourself a copy of wget, set it up with a suitable
~/.wgetrc as e.g. per

	http_proxy=http://some.where.on.the.intra.net:8080
	proxy=on
	proxy-user=abcdefgh
	proxy-passwd=XXXXXXXX

and make sure you change the file to read-only.  Test it with

        wget http://www.google.com
	
and if that works you will be fine for downloads within R too, including
package upgrades and normal http/ftp access as e.g. used by get.hist.quote()
in the tseries package.

This worked reliably for me in various environments, locations and operating
system, including several Win* variants.

Hth, Dirk

-- 
Those are my principles, and if you don't like them... well, I have others.
                                                -- Groucho Marx



From christoph.lehmann at gmx.ch  Wed Oct  8 18:28:10 2003
From: christoph.lehmann at gmx.ch (Christoph Lehmann)
Date: Wed, 08 Oct 2003 18:28:10 +0200
Subject: [R] using  split.screen() in Sweave
Message-ID: <1065630490.3448.23.camel@christophl>

Dear R and sweave users

A further problem, which I couldn't resolve, using the manual: In R I
use the split.screen command to put e.g. two timecourses one above the
other into one plot:

split.screen(c(2,1))
screen(1)
plot(stick,type='h', col="red",lwd=2)
screen(2)
plot(deconvolution.amplitude,type='h',col="blue",lwd=2)

Is there a similar way, doing this in Sweave?

many thanks

Christoph

-- 
Christoph Lehmann <christoph.lehmann at gmx.ch>



From ggrothendieck at myway.com  Wed Oct  8 18:38:28 2003
From: ggrothendieck at myway.com (Gabor Grothendieck)
Date: Wed,  8 Oct 2003 12:38:28 -0400 (EDT)
Subject: [R] Why does a[which(b == c[d])] not work?
Message-ID: <20031008163828.518AF398B@xmxpita.myway.com>


Here are some different ways of doing this. Don't know whether any
could be considered superior to the others.

# y[x==5] regarding NAs in x as not matching
x <- c(5, NA, 7, 5, NA, 3)
y <- c(1,  2, 3, 4,  5, 6)
subset(y,x==5)
y[x %in% 5]
y[x %in% c(5)]
y[which(x==5)]


--- 
Date: Wed, 08 Oct 2003 08:58:32 -0600 
From: Tony Plate <tplate at blackmesacapital.com>
 
At Wednesday 03:06 PM 10/8/2003 +0200, Martin Maechler wrote:
>Your question has been answered by Achim and Peter Dalgaard (at least).
>
>Just a note:
>
>Using
> a[which(logic)]
>looks like a clumsy and inefficient way of writing
> a[ logic ]
>
>and I think you shouldn't propagate its use ...

What then is the recommended way of treating an NA in the logical subset as 
a FALSE? (Or were you just talking about the given example, which didn't 
have this issue. However, you admonition seemed more general.)

As in:
> x <- 1:4
> y <- c(1,2,NA,4)
> x[y %% 2 == 0]
[1] 2 NA 4
> x[which(y %% 2 == 0)]
[1] 2 4
>

Sometimes one might want the first result, but more usually, I want the 
second, and using which() seems a convenient way to get it.

-- Tony Plate


_______________________________________________
No banners. No pop-ups. No kidding.
Introducing My Way - http://www.myway.com



From dyang at nrcan.gc.ca  Wed Oct  8 19:05:29 2003
From: dyang at nrcan.gc.ca (Yang, Richard)
Date: Wed, 8 Oct 2003 13:05:29 -0400 
Subject: [R] Installing GLMMGibbs problems
Message-ID: <F0E0B899CB43D5118D220002A55113CF02F927F9@s2-edm-r1.nofc.cfs.nrcan.gc.ca>

Dear all;

Installing the GLMMGibbs package to my Solaris Unix box, I got an compiling
error:

		ars.c:497:10: missing terminating " character
		ars.c: In function `dump_arse':
		ars.c:498: error: parse error before "mylesj"

		.....

The compiling error was reported to the list on Jul 3, 2003. According to
Prof. Brain Ripley this is a known problem with the package and gcc 3.3, and
the maintainer has been informed ...

Because I was unable to reach the maintainer by email (my email to
mylesj at icrf.icnet.uk on Oct. 2, bounced), I saved the GLMMGibbs package from
the tmp directory, edited the offensive lines in src/ars.c and saved the
file. Following a test compiling the edited ars.c without error, I re-tarred
and gzipped the saved GLMMGibbs package, stored the new
GLMMGibbs_0.5-1.tar.gz in a directory Rsources.

I expected to install the package from the saved directory (similar to
"install package(s) from local zip files..." on W2K) and used the
install.packages():

> install.packages("GLMMGibbs", lib="/PATH/Rsources/R-1.7.1/library",
destdir = "/PATH/Rsources")

Instead of using the local zip file from the Rsources directory, R
downloaded the GLMMGibbs_0.5-1.tar.gz from CRAN:

	trying URL
`http://cran.r-project.org/src/contrib/GLMMGibbs_0.5-1.tar.gz'
	Content type `application/x-tar' length 346260 bytes
	opened URL
	.....

	and produced the same compiling error. 

The question becomes how to install packages from local zip files on Solaris
or Linux platforms? 

In the absence of the original maintainer, could someone in R team edit the
source file ars.c in compliance with gcc 3.3? 

TIA,

Richard



From jasont at indigoindustrial.co.nz  Wed Oct  8 19:25:33 2003
From: jasont at indigoindustrial.co.nz (Jason Turner)
Date: Thu, 09 Oct 2003 06:25:33 +1300
Subject: [R] using  split.screen() in Sweave
In-Reply-To: <1065630490.3448.23.camel@christophl>
References: <1065630490.3448.23.camel@christophl>
Message-ID: <3F84488D.4010104@indigoindustrial.co.nz>

Christoph Lehmann wrote:

> Dear R and sweave users
> 
> A further problem, which I couldn't resolve, using the manual: In R I
> use the split.screen command to put e.g. two timecourses one above the
> other into one plot:
> 
> split.screen(c(2,1))
> screen(1)
> plot(stick,type='h', col="red",lwd=2)
> screen(2)
> plot(deconvolution.amplitude,type='h',col="blue",lwd=2)
> 
> Is there a similar way, doing this in Sweave?

I've never used split.screen.  Check out ?par, and read the entries for 
mfcol and mfrow.  Also check out ?layout.

HTH

Jason
-- 
Indigo Industrial Controls Ltd.
http://www.indigoindustrial.co.nz
64-21-343-545
jasont at indigoindustrial.co.nz



From jasont at indigoindustrial.co.nz  Wed Oct  8 19:45:49 2003
From: jasont at indigoindustrial.co.nz (Jason Turner)
Date: Thu, 09 Oct 2003 06:45:49 +1300
Subject: [R] Why does a[which(b == c[d])] not work?
In-Reply-To: <200310081145.h98Bjoc9018171@thorin.ci.tuwien.ac.at>
References: <3F83D87E.2060807@ptb.de>
	<200310081145.h98Bjoc9018171@thorin.ci.tuwien.ac.at>
Message-ID: <3F844D4D.40005@indigoindustrial.co.nz>

Achim Zeileis wrote:

> On Wednesday 08 October 2003 11:27, Thomas Bock wrote:
...
>>I can not understand why the expression in
>>
>>the subject does not work correct:
>> > dcrn[which(fn == inve[2])]
>>
>>numeric(0)
>>
>> > inve[2]
>>
>>[1] 406.7
>>
...
>   1.) `==' comparisons have a certain tolerance
>   2.) the print output is not necessarily "precisely" your number
> 
> Instead of using `==' you should use a comparison with a certain 
> tolerance you can specify...

I usually specify...

tol <- sqrt(.Machine$double.eps)
dcrn[(fn - inve[2]) < tol]

See ?.Machine for details.

Cheers

Jason
-- 
Indigo Industrial Controls Ltd.
http://www.indigoindustrial.co.nz
64-21-343-545
jasont at indigoindustrial.co.nz



From spencer.graves at pdf.com  Wed Oct  8 19:54:37 2003
From: spencer.graves at pdf.com (Spencer Graves)
Date: Wed, 08 Oct 2003 10:54:37 -0700
Subject: [R] binomial glm warnings revisited
In-Reply-To: <3.0.6.32.20031008174724.00c59560@mail.anst.uu.se>
References: <3.0.6.32.20031008174724.00c59560@mail.anst.uu.se>
Message-ID: <3F844F5D.40806@pdf.com>

      This seems to me to be a special case of the general problem of a 
parameter on a boundary.  Another example is the case of a variance 
component that is zero.  For this latter problem, Pinhiero and Bates 
(2000) Mixed-Effects Models in S and S-Plus (Springer, sec. 2.4.1) 
present simulation results showing that a 50-50 mixture of chi-square(0) 
and chi-square(1), for example, provide an excellent approximation to 
the actual sampling distribution of the 2*log(likelihood ratio). 

      Recent discussions of this and related questions on this list and 
elsewhere produced the following list of articles that may be helpful: 

      Donald Andrews (2001) "Testing When a Parameter In on the Boundary 
of the Maintained Hypothesis", Econometrica, 69:  683-734.

      Donald Andrews (2000) "Inconsistency of the Bootstrap When a 
Parameter Is on the Boundary of the Parameter Space", Econometrica, 68:  
388-405.

      Donald Andrews (1999) "Estimation When a Parameter Is on a 
Boundary", Econometrica, 67:  1341-1383.

      Rousseeuw, P. J. and Christmann, A. (2003) Robustness against 
separations
and outliers in logistic regression, Computational Statistics & Data
Analysis, Vol. 43, pp. 315-332

      ### Unfortunately, I have not had time to review these, so I can't 
comment further. 

      hope this helps.  spencer graves

Tord Snall wrote:

>Dear all,
>
>Last autumn there was some discussion on the list of the warning
>Warning message: 
>fitted probabilities numerically 0 or 1 occurred in: (if
>(is.empty.model(mt)) glm.fit.null else glm.fit)(x = X, y = Y,  
>
>when fitting binomial GLMs with many 0 and few 1.
>
>Parts of replies:
>"You should be able to tell which coefficients are infinite -- the
>coefficients and their standard errors will be large. When this happens the
>standard errors and the p-values reported by summary.glm() for those
>variables are useless."
>"My guess is that the deviances and coefficients are entirely ok. I'd
>expect that problems in the general area that Thomas mentions to reveal
>themselves as a failure to converge."
>
>I have this problem with my data. In a GLM, I have 269 zeroes and only 1 one:
>
>summary(dbh)
>Coefficients:
>            Estimate Std. Error z value Pr(>|z|)
>(Intercept)   0.1659     3.8781   0.043    0.966
>dbh          -0.5872     0.5320  -1.104    0.270
>
>  
>
>>drop1(dbh, test = "Chisq")
>>    
>>
>Single term deletions
>Model:
>MPext ~ dbh
>       Df Deviance     AIC     LRT Pr(Chi)  
><none>      9.9168 13.9168                  
>dbh     1  13.1931 15.1931  3.2763 0.07029 .
>
>I now wonder, is the drop1() function output 'reliable'?
>
>If so, is then the estimates from MASS confint() also 'reliable'? It gives
>the same warning.
>
>Waiting for profiling to be done...
>                2.5 %      97.5 %
>(Intercept) -6.503472 -0.77470556
>abund       -1.962549 -0.07496205
>There were 20 warnings (use warnings() to see them)
>
>
>Thanks in advance for your reply.
>
>
>Sincerely,
>Tord
>
>
>
>
>-----------------------------------------------------------------------
>Tord Sn?ll
>Avd. f v?xtekologi, Evolutionsbiologiskt centrum, Uppsala universitet
>Dept. of Plant Ecology, Evolutionary Biology Centre, Uppsala University
>Villav?gen 14			
>SE-752 36 Uppsala, Sweden
>Tel: 018-471 28 82 (int +46 18 471 28 82) (work)
>Tel: 018-25 71 33 (int +46 18 25 71 33) (home)
>Fax: 018-55 34 19 (int +46 18 55 34 19) (work)
>E-mail: Tord.Snall at ebc.uu.se
>Check this: http://www.vaxtbio.uu.se/resfold/snall.htm!
>
>______________________________________________
>R-help at stat.math.ethz.ch mailing list
>https://www.stat.math.ethz.ch/mailman/listinfo/r-help
>  
>



From jasont at indigoindustrial.co.nz  Wed Oct  8 20:00:34 2003
From: jasont at indigoindustrial.co.nz (Jason Turner)
Date: Thu, 09 Oct 2003 07:00:34 +1300
Subject: [R] Why does a[which(b == c[d])] not work?
In-Reply-To: <3F844D4D.40005@indigoindustrial.co.nz>
References: <3F83D87E.2060807@ptb.de>	<200310081145.h98Bjoc9018171@thorin.ci.tuwien.ac.at>
	<3F844D4D.40005@indigoindustrial.co.nz>
Message-ID: <3F8450C2.8010908@indigoindustrial.co.nz>

Whoops.  Hit "send" too quickly.

Jason Turner wrote:
> tol <- sqrt(.Machine$double.eps)
> dcrn[(fn - inve[2]) < tol]

that should be
dcrn[abs(fn - inve[2]) < tol]

-- 
Indigo Industrial Controls Ltd.
http://www.indigoindustrial.co.nz
64-21-343-545
jasont at indigoindustrial.co.nz



From tord.snall at ebc.uu.se  Wed Oct  8 21:27:01 2003
From: tord.snall at ebc.uu.se (Tord Snall)
Date: Wed, 08 Oct 2003 21:27:01 +0200
Subject: [R] binomial glm warnings revisited
In-Reply-To: <3F844F5D.40806@pdf.com>
References: <3.0.6.32.20031008174724.00c59560@mail.anst.uu.se>
	<3.0.6.32.20031008174724.00c59560@mail.anst.uu.se>
Message-ID: <3.0.6.32.20031008212701.00cc17b8@mail.anst.uu.se>

Dear Spencer,

Thanks very much for your reply. I am a biologist, and thus not used to
read stats papers. I will however give it a try. 

Hmm, I searched in mails with Subject 'warning', 'glm', 'fitted' without
finding the answer, but perhaps it is hidden in mails with other Subject.

Additional replies from others are most welcome. 


Sincerely,
Tord


At 10:54 2003-10-08 -0700, Spencer Graves wrote:
>      This seems to me to be a special case of the general problem of a 
>parameter on a boundary.  Another example is the case of a variance 
>component that is zero.  For this latter problem, Pinhiero and Bates 
>(2000) Mixed-Effects Models in S and S-Plus (Springer, sec. 2.4.1) 
>present simulation results showing that a 50-50 mixture of chi-square(0) 
>and chi-square(1), for example, provide an excellent approximation to 
>the actual sampling distribution of the 2*log(likelihood ratio). 

>
>      Recent discussions of this and related questions on this list and 
>elsewhere produced the following list of articles that may be helpful: 
>
>      Donald Andrews (2001) "Testing When a Parameter In on the Boundary 
>of the Maintained Hypothesis", Econometrica, 69:  683-734.
>
>      Donald Andrews (2000) "Inconsistency of the Bootstrap When a 
>Parameter Is on the Boundary of the Parameter Space", Econometrica, 68:  
>388-405.
>
>      Donald Andrews (1999) "Estimation When a Parameter Is on a 
>Boundary", Econometrica, 67:  1341-1383.
>
>      Rousseeuw, P. J. and Christmann, A. (2003) Robustness against 
>separations
>and outliers in logistic regression, Computational Statistics & Data
>Analysis, Vol. 43, pp. 315-332
>
>      ### Unfortunately, I have not had time to review these, so I can't 
>comment further. 
>
>      hope this helps.  spencer graves
>
>Tord Snall wrote:
>
>>Dear all,
>>
>>Last autumn there was some discussion on the list of the warning
>>Warning message: 
>>fitted probabilities numerically 0 or 1 occurred in: (if
>>(is.empty.model(mt)) glm.fit.null else glm.fit)(x = X, y = Y,  
>>
>>when fitting binomial GLMs with many 0 and few 1.
>>
>>Parts of replies:
>>"You should be able to tell which coefficients are infinite -- the
>>coefficients and their standard errors will be large. When this happens the
>>standard errors and the p-values reported by summary.glm() for those
>>variables are useless."
>>"My guess is that the deviances and coefficients are entirely ok. I'd
>>expect that problems in the general area that Thomas mentions to reveal
>>themselves as a failure to converge."
>>
>>I have this problem with my data. In a GLM, I have 269 zeroes and only 1
one:
>>
>>summary(dbh)
>>Coefficients:
>>            Estimate Std. Error z value Pr(>|z|)
>>(Intercept)   0.1659     3.8781   0.043    0.966
>>dbh          -0.5872     0.5320  -1.104    0.270
>>
>>  
>>
>>>drop1(dbh, test = "Chisq")
>>>    
>>>
>>Single term deletions
>>Model:
>>MPext ~ dbh
>>       Df Deviance     AIC     LRT Pr(Chi)  
>><none>      9.9168 13.9168                  
>>dbh     1  13.1931 15.1931  3.2763 0.07029 .
>>
>>I now wonder, is the drop1() function output 'reliable'?
>>
>>If so, is then the estimates from MASS confint() also 'reliable'? It gives
>>the same warning.
>>
>>Waiting for profiling to be done...
>>                2.5 %      97.5 %
>>(Intercept) -6.503472 -0.77470556
>>abund       -1.962549 -0.07496205
>>There were 20 warnings (use warnings() to see them)
>>
>>
>>Thanks in advance for your reply.
>>
>>
>>Sincerely,
>>Tord
>>
>>
>>
>>
>>-----------------------------------------------------------------------
>>Tord Sn?ll
>>Avd. f v?xtekologi, Evolutionsbiologiskt centrum, Uppsala universitet
>>Dept. of Plant Ecology, Evolutionary Biology Centre, Uppsala University
>>Villav?gen 14			
>>SE-752 36 Uppsala, Sweden
>>Tel: 018-471 28 82 (int +46 18 471 28 82) (work)
>>Tel: 018-25 71 33 (int +46 18 25 71 33) (home)
>>Fax: 018-55 34 19 (int +46 18 55 34 19) (work)
>>E-mail: Tord.Snall at ebc.uu.se
>>Check this: http://www.vaxtbio.uu.se/resfold/snall.htm!
>>
>>______________________________________________
>>R-help at stat.math.ethz.ch mailing list
>>https://www.stat.math.ethz.ch/mailman/listinfo/r-help
>>  
>>
>
>

-----------------------------------------------------------------------
Tord Sn?ll
Avd. f v?xtekologi, Evolutionsbiologiskt centrum, Uppsala universitet
Dept. of Plant Ecology, Evolutionary Biology Centre, Uppsala University
Villav?gen 14			
SE-752 36 Uppsala, Sweden
Tel: 018-471 28 82 (int +46 18 471 28 82) (work)
Tel: 018-25 71 33 (int +46 18 25 71 33) (home)
Fax: 018-55 34 19 (int +46 18 55 34 19) (work)
E-mail: Tord.Snall at ebc.uu.se
Check this: http://www.vaxtbio.uu.se/resfold/snall.htm!



From andrejk at zrc-sazu.si  Wed Oct  8 21:39:50 2003
From: andrejk at zrc-sazu.si (Andrej Kveder)
Date: Wed, 8 Oct 2003 21:39:50 +0200
Subject: [R] 2 questions regarding base-n and identifing digits
Message-ID: <FHEEJBDDCNPPNJEACDJAAEGHDEAA.andrejk@zrc-sazu.si>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20031008/36fbf1ab/attachment.pl

From p.dalgaard at biostat.ku.dk  Wed Oct  8 21:40:12 2003
From: p.dalgaard at biostat.ku.dk (Peter Dalgaard BSA)
Date: Wed, 08 Oct 2003 19:40:12 -0000
Subject: [R] binomial glm warnings revisited
In-Reply-To: <3F844F5D.40806@pdf.com>
References: <3.0.6.32.20031008174724.00c59560@mail.anst.uu.se>
	<3F844F5D.40806@pdf.com>
Message-ID: <x2pth7jz2d.fsf@biostat.ku.dk>

Spencer Graves <spencer.graves at pdf.com> writes:

>       This seems to me to be a special case of the general problem of
> a parameter on a boundary.  

Umm, no... 

> >I have this problem with my data. In a GLM, I have 269 zeroes and
> >only 1 one:

I don't think that necessarily gets you a parameter estimate on the
boundary. Only if the single "1" is smaller or bigger than all the others
should that happen. 

> >summary(dbh)
> >Coefficients:
> >            Estimate Std. Error z value Pr(>|z|)
> >(Intercept)   0.1659     3.8781   0.043    0.966
> >dbh          -0.5872     0.5320  -1.104    0.270
> >
> >
> >>drop1(dbh, test = "Chisq")
> >>
> >Single term deletions
> >Model:
> >MPext ~ dbh
> >       Df Deviance     AIC     LRT Pr(Chi)  <none>      9.9168
> > 13.9168                  dbh     1  13.1931 15.1931  3.2763 0.07029 .
> >
> >I now wonder, is the drop1() function output 'reliable'?
> >
> >If so, is then the estimates from MASS confint() also 'reliable'? It gives
> >the same warning.

> >(Intercept) -6.503472 -0.77470556
> >abund       -1.962549 -0.07496205
> >There were 20 warnings (use warnings() to see them)

During profiling, you may be pushing one of the parameter near the
extremes and get a model where the fitted p's are very close to 0/1.
That's not necessarily a sign of unreliability -- the procedure is to
set one parameter to a sequence of fixed values and optimize over the
other, and it might just be the case that the optimizations have been
wandering a bit far from the optimum. (I'd actually be more suspicious
about the fact that the name of the predictor suddenly changed....)

However, if you have only one "1" you are effectively asking whether
one observation has a different mean than the other 269, and you have
to consider the sensitivity to the distribution of the predictor. As
far as I can see, you end up with the test of the null hypothesis
beta==0 being essentially equivalent to a two sample t test between
the mean of the "0" group and that of the "1" group, so with only one
observation in one of the groups, the normal approximation of the test
hinges quite strongly on a normal distribution of the predictor
itself.

-- 
   O__  ---- Peter Dalgaard             Blegdamsvej 3  
  c/ /'_ --- Dept. of Biostatistics     2200 Cph. N   
 (*) \(*) -- University of Copenhagen   Denmark      Ph: (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)             FAX: (+45) 35327907



From spencer.graves at pdf.com  Wed Oct  8 21:57:15 2003
From: spencer.graves at pdf.com (Spencer Graves)
Date: Wed, 08 Oct 2003 12:57:15 -0700
Subject: [R] binomial glm warnings revisited
In-Reply-To: <x2pth7jz2d.fsf@biostat.ku.dk>
References: <3.0.6.32.20031008174724.00c59560@mail.anst.uu.se>	<3F844F5D.40806@pdf.com>
	<x2pth7jz2d.fsf@biostat.ku.dk>
Message-ID: <3F846C1B.8030906@pdf.com>

Thanks, Peter:  You are absolutely correct.  Thanks again for the 
correction.  Spencer Graves

Peter Dalgaard BSA wrote:

>Spencer Graves <spencer.graves at pdf.com> writes:
>
>  
>
>>      This seems to me to be a special case of the general problem of
>>a parameter on a boundary.  
>>    
>>
>
>Umm, no... 
>
>  
>
>>>I have this problem with my data. In a GLM, I have 269 zeroes and
>>>only 1 one:
>>>      
>>>
>
>I don't think that necessarily gets you a parameter estimate on the
>boundary. Only if the single "1" is smaller or bigger than all the others
>should that happen. 
>
>  
>
>>>summary(dbh)
>>>Coefficients:
>>>           Estimate Std. Error z value Pr(>|z|)
>>>(Intercept)   0.1659     3.8781   0.043    0.966
>>>dbh          -0.5872     0.5320  -1.104    0.270
>>>
>>>
>>>      
>>>
>>>>drop1(dbh, test = "Chisq")
>>>>
>>>>        
>>>>
>>>Single term deletions
>>>Model:
>>>MPext ~ dbh
>>>      Df Deviance     AIC     LRT Pr(Chi)  <none>      9.9168
>>>13.9168                  dbh     1  13.1931 15.1931  3.2763 0.07029 .
>>>
>>>I now wonder, is the drop1() function output 'reliable'?
>>>
>>>If so, is then the estimates from MASS confint() also 'reliable'? It gives
>>>the same warning.
>>>      
>>>
>
>  
>
>>>(Intercept) -6.503472 -0.77470556
>>>abund       -1.962549 -0.07496205
>>>There were 20 warnings (use warnings() to see them)
>>>      
>>>
>
>During profiling, you may be pushing one of the parameter near the
>extremes and get a model where the fitted p's are very close to 0/1.
>That's not necessarily a sign of unreliability -- the procedure is to
>set one parameter to a sequence of fixed values and optimize over the
>other, and it might just be the case that the optimizations have been
>wandering a bit far from the optimum. (I'd actually be more suspicious
>about the fact that the name of the predictor suddenly changed....)
>
>However, if you have only one "1" you are effectively asking whether
>one observation has a different mean than the other 269, and you have
>to consider the sensitivity to the distribution of the predictor. As
>far as I can see, you end up with the test of the null hypothesis
>beta==0 being essentially equivalent to a two sample t test between
>the mean of the "0" group and that of the "1" group, so with only one
>observation in one of the groups, the normal approximation of the test
>hinges quite strongly on a normal distribution of the predictor
>itself.
>
>  
>



From ozric at web.de  Wed Oct  8 22:02:14 2003
From: ozric at web.de (Christian Schulz)
Date: Wed, 8 Oct 2003 22:02:14 +0200
Subject: [R] winedt or another editor for linux
Message-ID: <000a01c38dd7$94144fb0$d200a8c0@pc>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20031008/1cd5b03b/attachment.pl

From zeileis at ci.tuwien.ac.at  Wed Oct  8 22:28:29 2003
From: zeileis at ci.tuwien.ac.at (Achim Zeileis)
Date: Wed, 8 Oct 2003 22:28:29 +0200
Subject: [R] winedt or another editor for linux
In-Reply-To: <000a01c38dd7$94144fb0$d200a8c0@pc>
References: <000a01c38dd7$94144fb0$d200a8c0@pc>
Message-ID: <200310082028.h98KSTfj024630@thorin.ci.tuwien.ac.at>

On Wednesday 08 October 2003 22:02, Christian Schulz wrote:

> Have got anybody experience using winedt and R in Linux
> - perhaps with wine
> Or exist another editor with the ability
> to parse r code into R in Linux,
> because k-edit etc. using only syntax-highlithing ???

I guess you have looked at (X)Emacs in connection with ESS (Emacs 
Speaks Statistics)? That is of course much more than an editor and not 
only available for Linux...

hth,
Z

> many thanks, christian
>
>
>
>
> 	[[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help



From andy_liaw at merck.com  Wed Oct  8 22:53:21 2003
From: andy_liaw at merck.com (Liaw, Andy)
Date: Wed, 08 Oct 2003 16:53:21 -0400
Subject: [R] 2 questions regarding base-n and identifing digits
Message-ID: <3A822319EB35174CA3714066D590DCD50205CC50@usrymx25.merck.com>

> From: Andrej Kveder [mailto:andrejk at zrc-sazu.si] 
> 
> Dear listers,
> 
> I have two questions:
> (1)
> Is there a way in R to change the base-n of the calculations. 
> I wnat to run some calculations either in binary (base-2) or 
> base-4. Is there a way to specify that in R - to chnage from 
> the decimal?

I don't have any good answers, but just this:  Arithmetics in any base is
the same.  I believe you just need a way to convert decimal to other bases
and printed out as such.  E.g., 1 + 5 = 6 = 110 in binary.  You just need
something that returns 110 when given 6.  Personally I'd write such a thing
in C because I don't know any better.
 
> (2)
> I also want to extract the digits from a larger number and 
> store them as a vector. I could do it through converting top 
> string, parsing the string and converting back. Is there 
> another way. It would look something like that:
> > a<-1234
> > a
> [1] 1234
> > b<-foo(a)
> > b
>      [,1] [,2] [,3] [,4]
> [1,]    1    2    3    4

Would the following do what you want?

> a <- c(1234, 5678, 90)
> foo <- function(x) strsplit(as.character(x), "")
> foo(a)
[[1]]
[1] "1" "2" "3" "4"

[[2]]
[1] "5" "6" "7" "8"

[[3]]
[1] "9" "0"

HTH,
Andy
 
> Thanks!
> 
> Andrej
> 
> _________
> Andrej Kveder, M.A.
> researcher
> Institute of Medical Sciences SRS SASA; Novi trg 2, SI-1000 
> Ljubljana, Slovenia
> phone: +386 1 47 06 440   fax: +386 1 42 61 493
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list 
> https://www.stat.math.ethz.ch/mailman/listinfo> /r-help
>



From ok at cs.otago.ac.nz  Wed Oct  8 23:22:46 2003
From: ok at cs.otago.ac.nz (Richard A. O'Keefe)
Date: Thu, 9 Oct 2003 10:22:46 +1300 (NZDT)
Subject: is.na(v)<-b (was: Re: [R] Beginner's query - segmentation fault)
Message-ID: <200310082122.h98LMkHS249791@atlas.otago.ac.nz>

Simon Fear <Simon.Fear at synequanon.com> suggested that

	> a<-"a"
	> a<-NA
	> mode(a)
	[1] "logical"
	> a<-"a"
	> is.na(a) <- T
	> mode(a)
	[1] "character"

might be a relevant difference between assigning NA and using is.na.
But the analogy is flawed:  is.na(x) <- operates on the _elements_ of
x, while x <- affects the variable x.  When you assign NA to
_elements_ of a vector, the mode does not change:

    > a <- "a"	
    > is.na(a) <- TRUE
    > mode(a)
    [1] "character"
    > b <- "b"
    > b[TRUE] <- NA
    > mode(b)
    [1] "character"
    > c <- "c"
    > c[1] <- NA
    > mode(c)
    [1] "character"



From ok at cs.otago.ac.nz  Wed Oct  8 23:36:35 2003
From: ok at cs.otago.ac.nz (Richard A. O'Keefe)
Date: Thu, 9 Oct 2003 10:36:35 +1300 (NZDT)
Subject: is.na(v)<-b (was: Re: [R] Beginner's query - segmentation fault)
Message-ID: <200310082136.h98LaZjH093108@atlas.otago.ac.nz>

Concerning  x[i] <- NA  vs  is.na(x[i]) <- TRUE
Brian Ripley wrote:

	I don't think it can ever `go wrong', but it can do things other
	than the user intends.

If the user writes x[i] <- NA, the user has clearly indicated his intention
that the i element(s) of x should become NA.  There isn't any clearer way to
say that.  The only way it could ever do something "other than the user
intends" is if the mode of x changes or the selected elements are set to
something other than NA.

The ?NA help page *hints* that this might be the case, but does not give
an example.

The question remains, *WHAT* can x[i]<-NA do that might be other than
what the user intends?  An example (especially one added to the ?NA help)
would be very useful.

	The intention of is.na<- is clearer,

I find this extremely puzzling.  x[i] <- NA is an extremely clear and
obvious way of saying "I want the i element(s) of x to become NA".
is.na(x) <- ... is not only an indirect way of doing this, it is a way
which is confusing and error-prone.

Bear in mind that one way of implementing something is is.na() would be
to associate a bit with each element of a vector; is.na() would test and
"is.na<-"() would set that bit.  It would be possible to have a language
exactly like R -except- that

    x <- 1
    is.na(x) <- TRUE
    x
=>  NA
    is.na(x) <- FALSE
    x
=>  1

would work.  The very existence of an "is.na<-" which accepts a logical
vector containing FALSE as well as TRUE strongly suggests this.  But it
doesn't work like that.  As I've pointed out, 
	is.logical(m) && length(m) == length(x) && done{is.na(x) <- m}
    =>  identical(is.na(x), m)
is the kind of behaviour that has been associated with well-behaved
sinister function calls for several decades, and yet this is not a fact
about R.

	and so perhaps user error is less likely?

I see no reason to believe this; the bad behaviour of "is.na<-" surely
makes user error *more* likely rather than *less* likely.



From rmshee0 at email.uky.edu  Wed Oct  8 23:49:54 2003
From: rmshee0 at email.uky.edu (Sheetz, Michael)
Date: Wed, 8 Oct 2003 17:49:54 -0400
Subject: [R] (no subject)
Message-ID: <7D1BD77F4206B9468B26D409CA587CFD0414478A@e2kbe1.ad.uky.edu>

Good afternoon,

We currently have R installed on our HP Superdome. However, we are getting ready to migrate from RISC to Itanium 2 chips running HP-UX (not Linux).

Does the latest version of R run on HP-UX Itanium 2?

Any information would be greatly appreciated.


Michael
___________________________________________

R. Michael Sheetz, PhD
High Performance Computing Group
326 A McVey Hall
University of Kentucky
Lexington, KY  40506

Phone: (859) 257-2900 ext 289



From ok at cs.otago.ac.nz  Wed Oct  8 23:53:58 2003
From: ok at cs.otago.ac.nz (Richard A. O'Keefe)
Date: Thu, 9 Oct 2003 10:53:58 +1300 (NZDT)
Subject: [R] Why does a[which(b == c[d])] not work?
Message-ID: <200310082153.h98Lrwgb071381@atlas.otago.ac.nz>

Achim Zeileis <zeileis at ci.tuwien.ac.at> wrote:
	R> x <- 406.7 + 1e-20 
	R> x
	[1] 406.7
	R> x == 406.7
	[1] TRUE
	
	that is
	  1.) `==' comparisons have a certain tolerance

No, all.equal() supports tolerance, == does not.

Consider
	> .Machine$double.eps 
	[1] 2.220446e-16

That is, the smallest relative difference that can be represented on
my (fairly typical IEEE-conforming) machine is 2 in 10 to the 16th.

1e-20/406.7 is 2.458815e-23, which is a factor of 10 million too small
a relative difference for the hardware to be able to represent.  So in

	x <- 406.7 + 1e-20

the value of x is identical to 406.7 in every bit.

	Instead of using `==' you should use a comparison with a certain 
	tolerance you can specify...
	
Such as ?all.equal



From gerifalte28 at hotmail.com  Thu Oct  9 00:09:28 2003
From: gerifalte28 at hotmail.com (Francisco Vergara)
Date: Wed, 08 Oct 2003 22:09:28 +0000
Subject: [R] (no subject)
Message-ID: <Law14-F94QTUf65Y5Qc00010493@hotmail.com>

Hi everybody

I want to specify the contrasts to build a cell means model on LME when 
there are several fixed effect  as factors in the model and also 
interactions between them.  Can anybody give me a hint on how to do 
accomplish this? How can I override the default "Contr.Treatment" for linear 
models?  I tried removing the intercept but this will only give me the mean 
values just for the first factor included in the model but then will use 
"contrast treatment" for all the other factors.

Thanks for your help!

Francisco

_________________________________________________________________
Help protect your PC.  Get a FREE computer virus scan online from McAfee.



From gerifalte28 at hotmail.com  Thu Oct  9 00:13:43 2003
From: gerifalte28 at hotmail.com (Francisco Vergara)
Date: Wed, 08 Oct 2003 22:13:43 +0000
Subject: [R] Sorry! Previous subject was cell means model in LME
Message-ID: <LAW14-F68Cp2YpMM07I000106f0@hotmail.com>

Hi everybody

I want to specify the contrasts to build a cell means model on LME (each 
coefficient is the mean value of the independent variable for the specific 
category of a factor variable) when there are several factors as fixed 
effect in the model and also interactions between them.  Can anybody give me 
a hint on how to do accomplish this? Also, how can I override the default 
"Contr.Treatment" for linear models?  I tried removing the intercept but 
this gave me the mean values just for the first factor included in the model 
and then used "contrast treatment" for all the other factors.

Thanks for your help!

Francisco

_________________________________________________________________
Instant message in style with MSN Messenger 6.0. Download it now FREE!  
http://msnmessenger-download.com



From dmurdoch at pair.com  Thu Oct  9 00:26:06 2003
From: dmurdoch at pair.com (Duncan Murdoch)
Date: Wed, 08 Oct 2003 18:26:06 -0400
Subject: is.na(v)<-b (was: Re: [R] Beginner's query - segmentation fault)
In-Reply-To: <200310082136.h98LaZjH093108@atlas.otago.ac.nz>
References: <200310082136.h98LaZjH093108@atlas.otago.ac.nz>
Message-ID: <3l39ov4is5eg7vfog58tur5tvov364desj@4ax.com>

<Tongue in cheek>

But surely 

 is.na(x) <- is.na(x)

is clearer than

 x[is.na(x)] <- NA

(neither of which is a no-op).

</Tongue in cheek>



From Simon.Blomberg at anu.edu.au  Thu Oct  9 01:18:48 2003
From: Simon.Blomberg at anu.edu.au (Simon Blomberg)
Date: Thu, 9 Oct 2003 09:18:48 +1000
Subject: [R] R-1.8.0 is released
Message-ID: <7A3A13F416B40842BD2C1753E044B359012280D6@CASEVS02.cas.anu.edu.au>

I'd just like to thank the R-Core team for the new version (all OS's), and especially Stefano Iacus for his work on the old Carbon MacOS port and his continuing work on the OS X port.

Cheers,

Simon.

Simon Blomberg, PhD
Depression & Anxiety Consumer Research Unit
Centre for Mental Health Research
Australian National University
http://www.anu.edu.au/cmhr/
Simon.Blomberg at anu.edu.au  +61 (2) 6125 3379

>



From matogar at um.es  Thu Oct  9 02:00:53 2003
From: matogar at um.es (Manuel Ato Garcia)
Date: Thu, 09 Oct 2003 02:00:53 +0200
Subject: [R] Re: Mixed effects with nlme
Message-ID: <3.0.3.32.20031009020053.00e77a48@gaia.fcu.um.es>

Hi, R-users:

 Last week I send a request for help to this list. I have receive until now
two kindly responses from Spencer Graves and Renauld Lancelot. They both 
point interesting things to fit an adequate model to my data but
unfortunately 
it persists without a satisfactory solution. 

 I propose again the same problem but using with a different dataset
(Assay), taken from Pinheiro and Bates' book on page 163, that is relevant
with crossed 
random effects. I have fitted the same model (p. 165)

>fmAssay <- lme(logDens ~ sample * dilut, Assay, random=pdBlocked(list(,
         pdIdent(~1), pdIdent(~sample-1),pdIdent(~dilut-1))) )

and the results with "anova" function (p. 166) are
 
             numDF denDF  F-value p-value
(Intercept)      1    29 537.6294  <.0001
sample           5    29  11.2103  <.0001
dilut            4    29 420.5458  <.0001
sample:dilut    20    29   1.6072  0.1192

 The problem is that with this approach one obtains correct F-values, but 
using a common residual term for DenDF that is a combination of (Block +
Block:sample + Block:dilut). Then probability values are different to that
obtained when we used the classical AOV funtion to fit the same model,
because in this case each term is mapped with a error term (so "sample"
uses "Block:sample", "dilut" uses "Block:dilut", and "sample:dilut" uses
"Block:sample:dilut"):

>mod<-aov(logDens ~ sample*dilut + Error(Block+Block/sample+Block/dilut),
data=Assay)
>summary(mod)

Error: Block
          Df    Sum Sq   Mean Sq F value Pr(>F)
Residuals  1 0.0083115 0.0083115               

Error: Block:sample
          Df   Sum Sq  Mean Sq F value   Pr(>F)
sample     5 0.276153 0.055231  11.213 0.009522
Residuals  5 0.024627 0.004925                 

Error: Block:dilut
          Df Sum Sq Mean Sq F value    Pr(>F)
dilut      4 3.7491  0.9373  420.79 1.684e-05
Residuals  4 0.0089  0.0022                  

Error: Within
             Df   Sum Sq  Mean Sq F value Pr(>F)
sample:dilut 20 0.055525 0.002776  1.6069 0.1486
Residuals    20 0.034555 0.001728  


 Obviously, the interest on linear mixed effects is open with the
possibility of modeling with correlated and/or heterocedastic errors, and
this end cannot
be pursued with AOV function.

 Summarizing, the problem is that I have not found a way to obtain with
NLME the same results (DF, F-ratios and probabilities) that I get with AOV and
multistratum errors. Is this an inconvenience of program?, probably due
to the impossibility to use multiple nested arguments as 

 random(~1|Block/sample+dilut) or  random(~1|Block/sample*dilut)
 
SAS MIXED can also fit these data and obtain correct results by means of a
combination of "random" and "repeated" arguments:

 model = sample dilut sample*dilut;
 random = Block sample*Block dilut*Block;
 repeated /type=cs Sub=Block;


              Type 3 Tests of Fixed Effects

                      Num     Den
            Effect     DF      DF    F Value    Pr > F
            sample      5       5      11.21    0.0095
             dilut      4       4     420.79    <.0001
      sample*dilut     20      20       1.61    0.1486


May be possible obtain the same results with NLME function?

 I would appreciate any kind of help.

 Best regards,


			Manuel Ato
			University of Murcia
			Spain
			e-mail: matogar at um.es



From edd at debian.org  Thu Oct  9 02:03:44 2003
From: edd at debian.org (Dirk Eddelbuettel)
Date: Wed, 8 Oct 2003 19:03:44 -0500
Subject: [R] Debian packages of 1.8.0 available
Message-ID: <20031009000344.GA11118@sonny.eddelbuettel.com>


Debian packages of R 1.8.0 were uploaded earlier for i386.

Binaries for alpha, ia64, hppa and powerpc are already in the package pool;
the arm, mipsel and s390 architecture have their built completled and will
be added to the pool shortly while the remaining architecures should follow
over the next few days. 

Similar to prior practice, we will probably make a testing release (for
i386) available at CRAN as well.

Regards, Dirk

-- 
Those are my principles, and if you don't like them... well, I have others.
                                                -- Groucho Marx



From Ted.Harding at nessie.mcc.ac.uk  Thu Oct  9 01:09:16 2003
From: Ted.Harding at nessie.mcc.ac.uk ( (Ted Harding))
Date: Thu, 09 Oct 2003 00:09:16 +0100 (BST)
Subject: [R] 2 questions regarding base-n and identifing digits
In-Reply-To: <3A822319EB35174CA3714066D590DCD50205CC50@usrymx25.merck.com>
Message-ID: <XFMail.031009000916.Ted.Harding@nessie.mcc.ac.uk>

On 08-Oct-03 Liaw, Andy wrote:
>> From: Andrej Kveder [mailto:andrejk at zrc-sazu.si] 
>> I have two questions:
>> (1)
>> Is there a way in R to change the base-n of the calculations. 
>> I wnat to run some calculations either in binary (base-2) or 
>> base-4. Is there a way to specify that in R - to chnage from 
>> the decimal?
> 
> I don't have any good answers, but just this:  Arithmetics in any base
> is the same.  I believe you just need a way to convert decimal to other
> bases and printed out as such.  E.g., 1 + 5 = 6 = 110 in binary.  You
> just need something that returns 110 when given 6.  Personally I'd
> write such a thing in C because I don't know any better.

Indeed! A pity that R's sprintf (though described as a wrapper for C's
sprintf) does not accept the C format conversion specifiers "%X" (for
hexadecimal) or, better stil, "%o" (for octal), because then Andy's
strsplit usage below could give you one of 16 or 8 characters which could
be used to index a lookup of a bit pattern!

Ted.

> Would the following do what you want?
> 
>> a <- c(1234, 5678, 90)
>> foo <- function(x) strsplit(as.character(x), "")
>> foo(a)
> [[1]]
> [1] "1" "2" "3" "4"
> 
> [[2]]
> [1] "5" "6" "7" "8"
> 
> [[3]]
> [1] "9" "0"

--------------------------------------------------------------------
E-Mail: (Ted Harding) <Ted.Harding at nessie.mcc.ac.uk>
Fax-to-email: +44 (0)870 167 1972
Date: 09-Oct-03                                       Time: 00:09:16
------------------------------ XFMail ------------------------------



From ggrothendieck at myway.com  Thu Oct  9 02:20:19 2003
From: ggrothendieck at myway.com (Gabor Grothendieck)
Date: Wed,  8 Oct 2003 20:20:19 -0400 (EDT)
Subject: [R] 2 questions regarding base-n and identifing digits
Message-ID: <20031009002019.188293990@xmxpita.myway.com>



Check out:

http://www.wiwi.uni-bielefeld.de/~wolf/software/R-wtools/decodeencode/decodeencode.rev


--- 
Date: Wed, 8 Oct 2003 21:39:50 +0200 
From: Andrej Kveder <andrejk at zrc-sazu.si>
 
 
Dear listers,

I have two questions:
(1)
Is there a way in R to change the base-n of the calculations. I wnat to run
some calculations either in binary (base-2) or base-4. Is there a way to
specify that in R - to chnage from the decimal?

(2)
I also want to extract the digits from a larger number and store them as a
vector.
I could do it through converting top string, parsing the string and
converting back. Is there another way.
It would look something like that:
> a<-1234
> a
[1] 1234
> b<-foo(a)
> b
[,1] [,2] [,3] [,4]
[1,] 1 2 3 4

Thanks!

Andrej

_________
Andrej Kveder, M.A.
researcher
Institute of Medical Sciences SRS SASA; Novi trg 2, SI-1000 Ljubljana,
Slovenia
phone: +386 1 47 06 440 fax: +386 1 42 61 493


_______________________________________________
No banners. No pop-ups. No kidding.
Introducing My Way - http://www.myway.com



From PAlspach at hortresearch.co.nz  Thu Oct  9 02:32:55 2003
From: PAlspach at hortresearch.co.nz (Peter Alspach)
Date: Thu, 09 Oct 2003 13:32:55 +1300
Subject: [R] Scoping rules
Message-ID: <sf85639a.009@hrp3.palm.cri.nz>

Dear List members:

I'm using R1.7.1 (Windows 2000) and having difficulty with scoping. 
I've studied the FAQ and on-line manuals and think I have identified
the
source of my difficulty, but cannot work out the solution.

For the purposes of illustration.  I have three functions as defined
below:

fnA <- function(my.x)
{
  list(first=as.character(substitute(my.x)), second=sqrt(my.x))
}

fnB <- function(AA)
{
  tmp.x <- get(AA$first)
  tmp.x^2
}

fnC <- function()
{
  x <- 1:2
  y <- fnA(x)
  z <- fnB(y)
  c(x,y,z)
}

fnA() has a vector as an argument and returns the name of the vector
and the square root of its elements in a list.  fn(B) takes the result
of fn(A) as its argument, gets the appropriate vector and computes the
square of its elements.  These work fine when called at the command
line.

fnC() defines a local vector x and calls fnA() which operates on this
vector.  Then fnB() is called, but it operates on a global vector x in
GlobalEnv (or returns an error is x doesn't exist there) - but I want
it
to operate on the local vector.

I think this is related to the enclosing environment of all three
functions being GlobalEnv (since they were created at the command
line),
but the parent environment of fnB() being different when invoked from
within fnC().

My questions:

1  Have I correctly understood the issue ?
2  How do I make fnB() operate on the local vector rather than the
global one ?
3  And, as an aside, I have used as.character(substitute(my.x)) to
pass
the name - but deparse(substitute(my.x)) also works.  Is there any
reason to prefer one over the other?

Thank you ...........


Peter Alspach



______________________________________________________
The contents of this e-mail are privileged and/or confidential to the
named recipient and are not to be used by any other person and/or
organisation. If you have received this e-mail in error, please notify 
the sender and delete all material pertaining to this e-mail.



From rpeng at jhsph.edu  Thu Oct  9 03:07:28 2003
From: rpeng at jhsph.edu (Roger D. Peng)
Date: Wed, 08 Oct 2003 21:07:28 -0400
Subject: [R] Scoping rules
In-Reply-To: <sf85639a.009@hrp3.palm.cri.nz>
References: <sf85639a.009@hrp3.palm.cri.nz>
Message-ID: <3F84B4D0.3060706@jhsph.edu>

It seems like you want in fnB

get(AA$first, envir = parent.frame(1))

but I'm entirely clear on why your original function doesn't work.  My 
understanding was that get() should search through the parent frames.

-roger

Peter Alspach wrote:
> Dear List members:
> 
> I'm using R1.7.1 (Windows 2000) and having difficulty with scoping. 
> I've studied the FAQ and on-line manuals and think I have identified
> the
> source of my difficulty, but cannot work out the solution.
> 
> For the purposes of illustration.  I have three functions as defined
> below:
> 
> fnA <- function(my.x)
> {
>   list(first=as.character(substitute(my.x)), second=sqrt(my.x))
> }
> 
> fnB <- function(AA)
> {
>   tmp.x <- get(AA$first)
>   tmp.x^2
> }
> 
> fnC <- function()
> {
>   x <- 1:2
>   y <- fnA(x)
>   z <- fnB(y)
>   c(x,y,z)
> }
> 
> fnA() has a vector as an argument and returns the name of the vector
> and the square root of its elements in a list.  fn(B) takes the result
> of fn(A) as its argument, gets the appropriate vector and computes the
> square of its elements.  These work fine when called at the command
> line.
> 
> fnC() defines a local vector x and calls fnA() which operates on this
> vector.  Then fnB() is called, but it operates on a global vector x in
> GlobalEnv (or returns an error is x doesn't exist there) - but I want
> it
> to operate on the local vector.
> 
> I think this is related to the enclosing environment of all three
> functions being GlobalEnv (since they were created at the command
> line),
> but the parent environment of fnB() being different when invoked from
> within fnC().
> 
> My questions:
> 
> 1  Have I correctly understood the issue ?
> 2  How do I make fnB() operate on the local vector rather than the
> global one ?
> 3  And, as an aside, I have used as.character(substitute(my.x)) to
> pass
> the name - but deparse(substitute(my.x)) also works.  Is there any
> reason to prefer one over the other?
> 
> Thank you ...........
> 
> 
> Peter Alspach
> 
> 
> 
> ______________________________________________________
> The contents of this e-mail are privileged and/or confidential to the
> named recipient and are not to be used by any other person and/or
> organisation. If you have received this e-mail in error, please notify 
> the sender and delete all material pertaining to this e-mail.
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
>



From christoff_pale at yahoo.com  Thu Oct  9 03:32:18 2003
From: christoff_pale at yahoo.com (christoff pale)
Date: Wed, 8 Oct 2003 18:32:18 -0700 (PDT)
Subject: [R] double precision
Message-ID: <20031009013218.12934.qmail@web40803.mail.yahoo.com>

Hi,
I am new to R, but have extensive experience in
matlab.
I have searched on the web and in Venabels & Ripley
book but I was unable to find the equivalent of the
eps function in matlab.
eps  returns the Floating point relative accuracy.

thanks



From rpeng at jhsph.edu  Thu Oct  9 03:37:53 2003
From: rpeng at jhsph.edu (Roger D. Peng)
Date: Wed, 08 Oct 2003 21:37:53 -0400
Subject: [R] double precision
In-Reply-To: <20031009013218.12934.qmail@web40803.mail.yahoo.com>
References: <20031009013218.12934.qmail@web40803.mail.yahoo.com>
Message-ID: <3F84BBF1.5020509@jhsph.edu>

Check out ?.Machine

-roger

christoff pale wrote:

> Hi,
> I am new to R, but have extensive experience in
> matlab.
> I have searched on the web and in Venabels & Ripley
> book but I was unable to find the equivalent of the
> eps function in matlab.
> eps  returns the Floating point relative accuracy.
> 
> thanks
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
>



From tlumley at u.washington.edu  Thu Oct  9 04:17:48 2003
From: tlumley at u.washington.edu (Thomas Lumley)
Date: Wed, 8 Oct 2003 19:17:48 -0700 (PDT)
Subject: [R] Scoping rules
In-Reply-To: <3F84B4D0.3060706@jhsph.edu>
References: <sf85639a.009@hrp3.palm.cri.nz> <3F84B4D0.3060706@jhsph.edu>
Message-ID: <Pine.A41.4.58.0310081917040.79772@homer11.u.washington.edu>

On Wed, 8 Oct 2003, Roger D. Peng wrote:

> It seems like you want in fnB
>
> get(AA$first, envir = parent.frame(1))
>
> but I'm entirely clear on why your original function doesn't work.  My
> understanding was that get() should search through the parent frames.

No, get() searches through the enclosing frames, like ordinary variable
lookup.  The only way to get dynamic scope is to specify it explicitly

	-thomas

> -roger
>
> Peter Alspach wrote:
> > Dear List members:
> >
> > I'm using R1.7.1 (Windows 2000) and having difficulty with scoping.
> > I've studied the FAQ and on-line manuals and think I have identified
> > the
> > source of my difficulty, but cannot work out the solution.
> >
> > For the purposes of illustration.  I have three functions as defined
> > below:
> >
> > fnA <- function(my.x)
> > {
> >   list(first=as.character(substitute(my.x)), second=sqrt(my.x))
> > }
> >
> > fnB <- function(AA)
> > {
> >   tmp.x <- get(AA$first)
> >   tmp.x^2
> > }
> >
> > fnC <- function()
> > {
> >   x <- 1:2
> >   y <- fnA(x)
> >   z <- fnB(y)
> >   c(x,y,z)
> > }
> >
> > fnA() has a vector as an argument and returns the name of the vector
> > and the square root of its elements in a list.  fn(B) takes the result
> > of fn(A) as its argument, gets the appropriate vector and computes the
> > square of its elements.  These work fine when called at the command
> > line.
> >
> > fnC() defines a local vector x and calls fnA() which operates on this
> > vector.  Then fnB() is called, but it operates on a global vector x in
> > GlobalEnv (or returns an error is x doesn't exist there) - but I want
> > it
> > to operate on the local vector.
> >
> > I think this is related to the enclosing environment of all three
> > functions being GlobalEnv (since they were created at the command
> > line),
> > but the parent environment of fnB() being different when invoked from
> > within fnC().
> >
> > My questions:
> >
> > 1  Have I correctly understood the issue ?
> > 2  How do I make fnB() operate on the local vector rather than the
> > global one ?
> > 3  And, as an aside, I have used as.character(substitute(my.x)) to
> > pass
> > the name - but deparse(substitute(my.x)) also works.  Is there any
> > reason to prefer one over the other?
> >
> > Thank you ...........
> >
> >
> > Peter Alspach
> >
> >
> >
> > ______________________________________________________
> > The contents of this e-mail are privileged and/or confidential to the
> > named recipient and are not to be used by any other person and/or
> > organisation. If you have received this e-mail in error, please notify
> > the sender and delete all material pertaining to this e-mail.
> >
> > ______________________________________________
> > R-help at stat.math.ethz.ch mailing list
> > https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> >
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
>

Thomas Lumley			Assoc. Professor, Biostatistics
tlumley at u.washington.edu	University of Washington, Seattle



From ripley at stats.ox.ac.uk  Thu Oct  9 04:28:56 2003
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Thu, 9 Oct 2003 03:28:56 +0100 (BST)
Subject: [R] Scoping rules
In-Reply-To: <3F84B4D0.3060706@jhsph.edu>
Message-ID: <Pine.LNX.4.44.0310090304530.6951-100000@gannet.stats>

On Wed, 8 Oct 2003, Roger D. Peng wrote:

> It seems like you want in fnB
> 
> get(AA$first, envir = parent.frame(1))
> 
> but I'm entirely clear on why your original function doesn't work.  My 
> understanding was that get() should search through the parent frames.

Where did you get that idea from?  Argument `inherits' to get() defaults
to TRUE and is defined as

inherits: should the enclosing frames of the environment be inspected?

Note `enclosing', not `parent'.  So the normal R scope rules apply when
looking for an object from the frame of fnB, which as its environment (aka
enclosing frame) is the workspace (.GlobalEnv) is to look in the local
frame and then along the search path.

[This does seem a fairly common misconception, so if you do have any idea 
in which document it arises it would be good to know.]

> Peter Alspach wrote:
> > Dear List members:
> > 
> > I'm using R1.7.1 (Windows 2000) and having difficulty with scoping. 
> > I've studied the FAQ and on-line manuals and think I have identified
> > the
> > source of my difficulty, but cannot work out the solution.
> > 
> > For the purposes of illustration.  I have three functions as defined
> > below:
> > 
> > fnA <- function(my.x)
> > {
> >   list(first=as.character(substitute(my.x)), second=sqrt(my.x))
> > }
> > 
> > fnB <- function(AA)
> > {
> >   tmp.x <- get(AA$first)
> >   tmp.x^2
> > }
> > 
> > fnC <- function()
> > {
> >   x <- 1:2
> >   y <- fnA(x)
> >   z <- fnB(y)
> >   c(x,y,z)
> > }
> > 
> > fnA() has a vector as an argument and returns the name of the vector
> > and the square root of its elements in a list.  fn(B) takes the result
> > of fn(A) as its argument, gets the appropriate vector and computes the
> > square of its elements.  These work fine when called at the command
> > line.
> > 
> > fnC() defines a local vector x and calls fnA() which operates on this
> > vector.  Then fnB() is called, but it operates on a global vector x in
> > GlobalEnv (or returns an error is x doesn't exist there) - but I want
> > it
> > to operate on the local vector.

I am not sure what you really want to do here, but R works best if you 
pass functions an object to work on, and not the name of an object.
Normally this sort of thing is best avoided, but if it is really needed it 
is normally simpler to find the object in the calling function (your fnC).

> > I think this is related to the enclosing environment of all three
> > functions being GlobalEnv (since they were created at the command
> > line),
> > but the parent environment of fnB() being different when invoked from
> > within fnC().
> > 
> > My questions:
> > 
> > 1  Have I correctly understood the issue ?
> > 2  How do I make fnB() operate on the local vector rather than the
> > global one ?
> > 3  And, as an aside, I have used as.character(substitute(my.x)) to
> > pass
> > the name - but deparse(substitute(my.x)) also works.  Is there any
> > reason to prefer one over the other?

deparse() is preferred.  One subtle reason is what happens with very long 
expressions (and note deparse may give more than one line of output), 
but the main reason is what happens with expressions such as calls:

> fn1 <- function(x) as.character(substitute(x))
> fn2 <- function(x) deparse(substitute(x))
> fn1(log(x))
[1] "log" "x"  
> fn2(log(x))
[1] "log(x)"


It is normally dangerous to use get() on the result of 
deparse(substitute()), as the latter may be the character representation 
of an expression and not a simple name.

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From rpeng at jhsph.edu  Thu Oct  9 04:38:50 2003
From: rpeng at jhsph.edu (Roger D. Peng)
Date: Wed, 08 Oct 2003 22:38:50 -0400
Subject: [R] Scoping rules
In-Reply-To: <Pine.LNX.4.44.0310090304530.6951-100000@gannet.stats>
References: <Pine.LNX.4.44.0310090304530.6951-100000@gannet.stats>
Message-ID: <3F84CA3A.3020001@jhsph.edu>

I think I misinterpreted this section of the help file for get():

      If `inherits' is `FALSE', only the first frame of the specified
      environment is inspected.  If `inherits' is `TRUE', the search is
      continued up through the parent frames until a bound value of the
      right mode is found.

Should this read "parent environments" instead of "parent frames"?  I'm 
taking this from R 1.7.1.

-roger

Prof Brian Ripley wrote:

> On Wed, 8 Oct 2003, Roger D. Peng wrote:
> 
> 
>>It seems like you want in fnB
>>
>>get(AA$first, envir = parent.frame(1))
>>
>>but I'm entirely clear on why your original function doesn't work.  My 
>>understanding was that get() should search through the parent frames.
> 
> 
> Where did you get that idea from?  Argument `inherits' to get() defaults
> to TRUE and is defined as
> 
> inherits: should the enclosing frames of the environment be inspected?
> 
> Note `enclosing', not `parent'.  So the normal R scope rules apply when
> looking for an object from the frame of fnB, which as its environment (aka
> enclosing frame) is the workspace (.GlobalEnv) is to look in the local
> frame and then along the search path.
> 
> [This does seem a fairly common misconception, so if you do have any idea 
> in which document it arises it would be good to know.]
> 
> 
>>Peter Alspach wrote:
>>
>>>Dear List members:
>>>
>>>I'm using R1.7.1 (Windows 2000) and having difficulty with scoping. 
>>>I've studied the FAQ and on-line manuals and think I have identified
>>>the
>>>source of my difficulty, but cannot work out the solution.
>>>
>>>For the purposes of illustration.  I have three functions as defined
>>>below:
>>>
>>>fnA <- function(my.x)
>>>{
>>>  list(first=as.character(substitute(my.x)), second=sqrt(my.x))
>>>}
>>>
>>>fnB <- function(AA)
>>>{
>>>  tmp.x <- get(AA$first)
>>>  tmp.x^2
>>>}
>>>
>>>fnC <- function()
>>>{
>>>  x <- 1:2
>>>  y <- fnA(x)
>>>  z <- fnB(y)
>>>  c(x,y,z)
>>>}
>>>
>>>fnA() has a vector as an argument and returns the name of the vector
>>>and the square root of its elements in a list.  fn(B) takes the result
>>>of fn(A) as its argument, gets the appropriate vector and computes the
>>>square of its elements.  These work fine when called at the command
>>>line.
>>>
>>>fnC() defines a local vector x and calls fnA() which operates on this
>>>vector.  Then fnB() is called, but it operates on a global vector x in
>>>GlobalEnv (or returns an error is x doesn't exist there) - but I want
>>>it
>>>to operate on the local vector.
> 
> 
> I am not sure what you really want to do here, but R works best if you 
> pass functions an object to work on, and not the name of an object.
> Normally this sort of thing is best avoided, but if it is really needed it 
> is normally simpler to find the object in the calling function (your fnC).
> 
> 
>>>I think this is related to the enclosing environment of all three
>>>functions being GlobalEnv (since they were created at the command
>>>line),
>>>but the parent environment of fnB() being different when invoked from
>>>within fnC().
>>>
>>>My questions:
>>>
>>>1  Have I correctly understood the issue ?
>>>2  How do I make fnB() operate on the local vector rather than the
>>>global one ?
>>>3  And, as an aside, I have used as.character(substitute(my.x)) to
>>>pass
>>>the name - but deparse(substitute(my.x)) also works.  Is there any
>>>reason to prefer one over the other?
> 
> 
> deparse() is preferred.  One subtle reason is what happens with very long 
> expressions (and note deparse may give more than one line of output), 
> but the main reason is what happens with expressions such as calls:
> 
> 
>>fn1 <- function(x) as.character(substitute(x))
>>fn2 <- function(x) deparse(substitute(x))
>>fn1(log(x))
> 
> [1] "log" "x"  
> 
>>fn2(log(x))
> 
> [1] "log(x)"
> 
> 
> It is normally dangerous to use get() on the result of 
> deparse(substitute()), as the latter may be the character representation 
> of an expression and not a simple name.
>



From david.airey at vanderbilt.edu  Thu Oct  9 04:51:49 2003
From: david.airey at vanderbilt.edu (David Airey)
Date: Wed, 8 Oct 2003 21:51:49 -0500
Subject: [R] nlme & lme for complex mixed ANOVAs
Message-ID: <875DC9F4-FA03-11D7-B30F-003065D59C7A@vanderbilt.edu>

Dear List,

I downloaded R for the first time yesterday, in the hopes that I might 
deal more effectively with a complex repeated measures experimental 
design involving inbred strains of laboratory mice. The design below, 
somewhat simplified, cannot be computed with standard ANOVA, because 
something called the X'X matrix is too large. The design has the 
following factors:

Between-subject factors (levels):
	inbred mouse strain (20, "twenty")
	sex (2)
Animals:
	10 per sex*strain combination (400 total)
Within-subject factors:
	drug (3)
	trial set (4)
	stimulus characteristic 1 (2)
	stimulus characteristic 2 (2)

My question for the R community is, does R have in nlme or lme the 
ability to compute this problem on a typical desktop PC? In Stata, for 
instance, which has a matrix size limit of 11,000, the problem above 
will not fit, using standard ANOVA. Matrix size can be determined by 
listing all the terms of the full model and multiplying the levels of 
each factor withing a term and summing all terms. I said simplified in 
the first paragraph above, because if I include the day of drug 
challenge, the model oversteps the number of within-subject factors 
allowed. So, surprisingly to me, Stata/SE, which I bought for large 
data sets, is too small! Not that I don't like Stata, but I am annoyed 
that I must find another tool to use. I understand that SAS Proc Mixed 
will compute the problem, because it may handle the covariance matrix 
in some kind of piecemeal fashion (perhaps by animal but I've no 
confirmation of this, except that it zips through a comparable data set 
on someone else's computer). However, I am running Apple OS X and don't 
have SAS on my machine. I don't really understand what's going on 
underneath these programs respective hoods, but my question here is 
whether R computes mixed models in such a way as to require a matrix 
size like Stata or like SAS, when mixed models of this size are 
presented?

Sincerely,

-Dave



From jeaneid at chass.utoronto.ca  Thu Oct  9 05:16:31 2003
From: jeaneid at chass.utoronto.ca (Jean Eid)
Date: Wed, 8 Oct 2003 23:16:31 -0400
Subject: [R] residual lag scatter plot
Message-ID: <Pine.SGI.4.40.0310082302430.4104754-100000@origin.chass.utoronto.ca>

Dear all,

I am looking for a function that  can scatter  plot the residuals obtained
from a longitudinal model i.e. plot  e_{i,j} e_{i,k} for
all j< k = 1,..n ( I have 7 observations for each subject). something
similar to the pairs() function.
I can do it the long way by constructing a residual matrix and use pairs.
However, I am wondering if there is a builty in function to do this.



From nusbj at hotmail.com  Thu Oct  9 07:21:32 2003
From: nusbj at hotmail.com (Zhen Pang)
Date: Thu, 09 Oct 2003 13:21:32 +0800
Subject: [R] run R under unix
Message-ID: <Sea2-F5p7IpGwC6RZT200003497@hotmail.com>

Dear

I used to use R under windows. I always save my code in a txt file. When I 
want to run something, I just copy and paste to the windows.

I recently need to do a big simulation work which will cost 5 or 6 days. I 
am afraid the memory of windows can not cope with this situation.

I now want to run the code under unix. However, I do not know how to run 
this code in txt file under unix. I just log in to the unix and enter the R, 
what should I do next?

One more question is: if I log off my computer when R is running under unix 
(i.e., disconnect my computer from server), will I get the result when I log 
in my computer next time?

Thanks!

_________________________________________________________________
Get 10mb of inbox space with MSN Hotmail Extra Storage



From jasont at indigoindustrial.co.nz  Thu Oct  9 07:52:36 2003
From: jasont at indigoindustrial.co.nz (Jason Turner)
Date: Thu, 09 Oct 2003 18:52:36 +1300
Subject: [R] run R under unix
In-Reply-To: <Sea2-F5p7IpGwC6RZT200003497@hotmail.com>
References: <Sea2-F5p7IpGwC6RZT200003497@hotmail.com>
Message-ID: <3F84F7A4.7060808@indigoindustrial.co.nz>

Zhen Pang wrote:
...
> I now want to run the code under unix. However, I do not know how to run 
> this code in txt file under unix. I just log in to the unix and enter 
> the R, what should I do next?
> 
> One more question is: if I log off my computer when R is running under 
> unix (i.e., disconnect my computer from server), will I get the result 
> when I log in my computer next time?
...

You'll lose it if you run R in the normal, interactive way.  Running it 
in the background will allow you to log out and still have it running, but!

1) If you're not the only person using this machine, you learn the 
command "nice" before you begin.
2) I'm not certain you'll be able to produce jpeg or png graphics when 
backgrounded; your backgrounded task needs access to the windowing 
system for graphics rendering, and local security policy might prohibit 
this.
3) Save early, save often.  You probably already know that, but it bears 
repeating.

Here are some suggested steps to run your simulation in background mode. 
  Unfortunately, the exact commands will depend on which version of Unix 
you're using, and what command shells are available.  Consult your local 
expert.

1) transfer the text file of commands to the unix machine.  FTP, using 
ASCII mode is safest.
2) log onto the Unix machine.
3) run sh, ksh, or bash.  (the syntax for what follows is different for 
the C shell, and I don't know it).
4) using a stripped-down version of your script which will complete in a 
short time (say, a minute or two), just to check that things work, type

nohup nice 15 R < my.small.script >my.output 2>&1 &

(again, learn what "nice" means before you use it.  This may not be 
suitable, and it's impossible for me to tell from here if it is).

I know that's not the full answer, but only someone who knows the local 
setup can give you that answer.

Cheers

Jason
-- 
Indigo Industrial Controls Ltd.
http://www.indigoindustrial.co.nz
64-21-343-545
jasont at indigoindustrial.co.nz



From ligges at statistik.uni-dortmund.de  Thu Oct  9 08:37:37 2003
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Thu, 09 Oct 2003 08:37:37 +0200
Subject: [R] Lattice cloud() funtion bug in R1.8.0beta
In-Reply-To: <70535265859.20031008164046@power.inescn.pt>
References: <70535265859.20031008164046@power.inescn.pt>
Message-ID: <3F850231.3060007@statistik.uni-dortmund.de>

Mark Marques wrote:

 >   Cloud() function does not display anything with R1.8.0beta
 >   in WindowsXP ...
 >   Does any one noticed this ?


No. Works in the latest beta on my machine.


 >   others functions from lattice seem working properly.
 >   does it work in the "final" 1.8.0 for windows ?


Yes.

Uwe Ligges



From ripley at stats.ox.ac.uk  Thu Oct  9 08:45:03 2003
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Thu, 9 Oct 2003 07:45:03 +0100 (BST)
Subject: [R] Scoping rules
In-Reply-To: <3F84CA3A.3020001@jhsph.edu>
Message-ID: <Pine.LNX.4.44.0310090743451.7551-100000@gannet.stats>

On Wed, 8 Oct 2003, Roger D. Peng wrote:

> I think I misinterpreted this section of the help file for get():
> 
>       If `inherits' is `FALSE', only the first frame of the specified
>       environment is inspected.  If `inherits' is `TRUE', the search is
>       continued up through the parent frames until a bound value of the
>       right mode is found.
> 
> Should this read "parent environments" instead of "parent frames"?  I'm 
> taking this from R 1.7.1.

It should read `enclosing' not `parent' for a start.

Thanks: we will try to get this consistent.

> 
> -roger
> 
> Prof Brian Ripley wrote:
> 
> > On Wed, 8 Oct 2003, Roger D. Peng wrote:
> > 
> > 
> >>It seems like you want in fnB
> >>
> >>get(AA$first, envir = parent.frame(1))
> >>
> >>but I'm entirely clear on why your original function doesn't work.  My 
> >>understanding was that get() should search through the parent frames.
> > 
> > 
> > Where did you get that idea from?  Argument `inherits' to get() defaults
> > to TRUE and is defined as
> > 
> > inherits: should the enclosing frames of the environment be inspected?
> > 
> > Note `enclosing', not `parent'.  So the normal R scope rules apply when
> > looking for an object from the frame of fnB, which as its environment (aka
> > enclosing frame) is the workspace (.GlobalEnv) is to look in the local
> > frame and then along the search path.
> > 
> > [This does seem a fairly common misconception, so if you do have any idea 
> > in which document it arises it would be good to know.]
> > 
> > 
> >>Peter Alspach wrote:
> >>
> >>>Dear List members:
> >>>
> >>>I'm using R1.7.1 (Windows 2000) and having difficulty with scoping. 
> >>>I've studied the FAQ and on-line manuals and think I have identified
> >>>the
> >>>source of my difficulty, but cannot work out the solution.
> >>>
> >>>For the purposes of illustration.  I have three functions as defined
> >>>below:
> >>>
> >>>fnA <- function(my.x)
> >>>{
> >>>  list(first=as.character(substitute(my.x)), second=sqrt(my.x))
> >>>}
> >>>
> >>>fnB <- function(AA)
> >>>{
> >>>  tmp.x <- get(AA$first)
> >>>  tmp.x^2
> >>>}
> >>>
> >>>fnC <- function()
> >>>{
> >>>  x <- 1:2
> >>>  y <- fnA(x)
> >>>  z <- fnB(y)
> >>>  c(x,y,z)
> >>>}
> >>>
> >>>fnA() has a vector as an argument and returns the name of the vector
> >>>and the square root of its elements in a list.  fn(B) takes the result
> >>>of fn(A) as its argument, gets the appropriate vector and computes the
> >>>square of its elements.  These work fine when called at the command
> >>>line.
> >>>
> >>>fnC() defines a local vector x and calls fnA() which operates on this
> >>>vector.  Then fnB() is called, but it operates on a global vector x in
> >>>GlobalEnv (or returns an error is x doesn't exist there) - but I want
> >>>it
> >>>to operate on the local vector.
> > 
> > 
> > I am not sure what you really want to do here, but R works best if you 
> > pass functions an object to work on, and not the name of an object.
> > Normally this sort of thing is best avoided, but if it is really needed it 
> > is normally simpler to find the object in the calling function (your fnC).
> > 
> > 
> >>>I think this is related to the enclosing environment of all three
> >>>functions being GlobalEnv (since they were created at the command
> >>>line),
> >>>but the parent environment of fnB() being different when invoked from
> >>>within fnC().
> >>>
> >>>My questions:
> >>>
> >>>1  Have I correctly understood the issue ?
> >>>2  How do I make fnB() operate on the local vector rather than the
> >>>global one ?
> >>>3  And, as an aside, I have used as.character(substitute(my.x)) to
> >>>pass
> >>>the name - but deparse(substitute(my.x)) also works.  Is there any
> >>>reason to prefer one over the other?
> > 
> > 
> > deparse() is preferred.  One subtle reason is what happens with very long 
> > expressions (and note deparse may give more than one line of output), 
> > but the main reason is what happens with expressions such as calls:
> > 
> > 
> >>fn1 <- function(x) as.character(substitute(x))
> >>fn2 <- function(x) deparse(substitute(x))
> >>fn1(log(x))
> > 
> > [1] "log" "x"  
> > 
> >>fn2(log(x))
> > 
> > [1] "log(x)"
> > 
> > 
> > It is normally dangerous to use get() on the result of 
> > deparse(substitute()), as the latter may be the character representation 
> > of an expression and not a simple name.
> > 
> 
> 

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From p.dalgaard at biostat.ku.dk  Thu Oct  9 09:22:32 2003
From: p.dalgaard at biostat.ku.dk (Peter Dalgaard BSA)
Date: Thu, 09 Oct 2003 07:22:32 -0000
Subject: [R] Scoping rules
In-Reply-To: <Pine.LNX.4.44.0310090743451.7551-100000@gannet.stats>
References: <Pine.LNX.4.44.0310090743451.7551-100000@gannet.stats>
Message-ID: <x2k77e7u04.fsf@biostat.ku.dk>

Prof Brian Ripley <ripley at stats.ox.ac.uk> writes:

> > Should this read "parent environments" instead of "parent frames"?  I'm 
> > taking this from R 1.7.1.
> 
> It should read `enclosing' not `parent' for a start.
> 
> Thanks: we will try to get this consistent.

Beware: Robert has been pretty insistent in using the term "parent
environment" for the thing that is ENCLOS() in C code. E.g. we have
the functions parent.env() for lexical scope and parent.frame() for
dynamic scope. So Roger might well have the better suggestion for
achieving consistency. 

        -p

-- 
   O__  ---- Peter Dalgaard             Blegdamsvej 3  
  c/ /'_ --- Dept. of Biostatistics     2200 Cph. N   
 (*) \(*) -- University of Copenhagen   Denmark      Ph: (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)             FAX: (+45) 35327907



From Detlef.Steuer at unibw-hamburg.de  Thu Oct  9 09:36:43 2003
From: Detlef.Steuer at unibw-hamburg.de (Detlef Steuer)
Date: Thu, 9 Oct 2003 09:36:43 +0200
Subject: [R] SuSE rpms for R-1.8.0 available
Message-ID: <20031009093643.0a8b0e05.steuer@unibw-hamburg.de>

Rpms for R-base and all R-contrib packages (that compile on my machines) have been uploaded to CRAN.

For the time beeing SuSE 7.3, 8.[012] are supported. 9.0 rpms will be created asap. 

This is the last build for 7.3.

Have fun,
dst

-- 
"The whole problem with the world is that fools and fanatics are always
 so certain of themselves, but wiser people so full of doubts." Russell

Detlef Steuer --- http://fawn.unibw-hamburg.de/steuer.html
***** Encrypted mail preferred *****



From andrejk at zrc-sazu.si  Thu Oct  9 10:09:22 2003
From: andrejk at zrc-sazu.si (Andrej Kveder)
Date: Thu, 9 Oct 2003 10:09:22 +0200
Subject: [R] 2 questions regarding base-n and identifing digits + the
	source problem description
In-Reply-To: <20031009002019.188293990@xmxpita.myway.com>
Message-ID: <FHEEJBDDCNPPNJEACDJAEEICDEAA.andrejk@zrc-sazu.si>

Thanks for the suggestions..

The strsplit function works great for the second question...
I will have to come up with another solution to the first one, since my
problem would definetly involve a lot of calculations and I think that
current methods would cause major computational congestion...
BUt here is the outline of my problem... if anybody has an idea...
 In practice I start with 4 possible correlations: -0.9, -0.1, 0.1, 0.9 and
want to construct all possible 3x3 correaltion matrices with those values.
So I need to do all possible permutations of a given length with
replication. I have a set of 4 distinct values and want to create the
permutations of length 3.
My idea was as folows: specifing the sequence in base-4 form 0 to 333 would
account for all possible combinations.
I checked some of the existing functions which were either recursive and
slow or without replication. I think there was a thread on the list just a
short while ago.
If anybody has a hint to follow, i would be more then glad to try it out...

Thanks again

Andrej

_________
Andrej Kveder, M.A.
researcher
Institute of Medical Sciences SRS SASA; Novi trg 2, SI-1000 Ljubljana,
Slovenia
phone: +386 1 47 06 440   fax: +386 1 42 61 493



From Rau at demogr.mpg.de  Thu Oct  9 10:10:03 2003
From: Rau at demogr.mpg.de (Rau, Roland)
Date: Thu, 9 Oct 2003 10:10:03 +0200
Subject: [R] Previous Commands
Message-ID: <3699CDBC4ED5D511BE6400306E1C0D8114CD42@hermes.demogr.mpg.de>

Dear All,

yesterday I took the R-1.8.0-source file and compiled it on my own. As I am
using Linux just for a couple of weeks, it was my first compiling session
with ./configure, make, ....
Everything went fine, except for one thing: if I want to look at the
commands history by using the cursor keys, it does not work. Instead of
displaying the previous commands, it returns something like "[[A".
Is this a common problem?
Does anyone have a solution?

I had R-1.7.1 installed previously via an RPM-package for my Mandrake 9.1
distribution and everything went fine.

Thanks in advance,
ROland




+++++
This mail has been sent through the MPI for Demographic Research.  Should you receive a mail that is apparently from a MPI user without this text displayed, then the address has most likely been faked.   If you are uncertain about the validity of this message, please check the mail header or ask your system administrator for assistance.



From glaziou at pasteur-kh.org  Thu Oct  9 10:30:43 2003
From: glaziou at pasteur-kh.org (Philippe Glaziou)
Date: Thu, 9 Oct 2003 15:30:43 +0700
Subject: [R] Previous Commands
In-Reply-To: <3699CDBC4ED5D511BE6400306E1C0D8114CD42@hermes.demogr.mpg.de>
References: <3699CDBC4ED5D511BE6400306E1C0D8114CD42@hermes.demogr.mpg.de>
Message-ID: <20031009083043.GB21787@pasteur-kh.org>

Rau, Roland <Rau at demogr.mpg.de> wrote:
> yesterday I took the R-1.8.0-source file and compiled it on my
> own. As I am using Linux just for a couple of weeks, it was my
> first compiling session with ./configure, make, ....
> Everything went fine, except for one thing: if I want to look
> at the commands history by using the cursor keys, it does not
> work. Instead of displaying the previous commands, it returns
> something like "[[A".  Is this a common problem?  Does anyone
> have a solution?


You may need to install the ncurses dev libraries.

-- 
Philippe Glaziou



From lwalters at cs.uct.ac.za  Thu Oct  9 10:31:55 2003
From: lwalters at cs.uct.ac.za (Lourens Olivier Walters)
Date: Thu, 09 Oct 2003 10:31:55 +0200
Subject: [R] fitdistr, mle's and gamma distribution
Message-ID: <1065688314.550.15.camel@stochastic>

Thanks, again I managed to get convergence, although looking at the
resultant Pareto distribution plotted in R, it doesn't seem that the log
likelihood estimated parameters are more accurate than the methods of
moments estimator parameters - I will have to look into it. For the
record, here is the code I used: 

###########

data.sorted <- sort(data)
alpha.val <- data.sorted[1]
beta.val <- 1/((1/n) * sum(log(data.sorted/alpha.val)))

log.alpha.val <- log(alpha.val)
# Optimize log.del = log(0.0000001), which is the log(difference 
# between alpha and log likelihood estimated alpha), chose small 
# difference to start of with
log.del = -16.11809565

paretoLoglik <- function(params, negative=TRUE){
  alpha <- data.sorted[1]+exp(params[1])
  lglk <- sum(pareto.density(data.sorted, alpha=exp(alpha),
+ beta=exp(params[2]), log=TRUE))
  if(negative)
     return(-lglk)
  else
     return(lglk)
}

optim.list <- optim(c(log.del, log.beta.val), paretoLoglik,
+ method="BFGS")
pareto.param1 <- exp(optim.list$par[1]) + alpha.val
pareto.param2 <- exp(optim.list$par[2])

##########

On Wed, 2003-10-08 at 15:07, Spencer Graves wrote:
>       I'm sorry, but I don't have time to read all your code. 
However, 
> I saw that you tested for x > alpha in your Pareto distribution 
> example.  Have you considered reparameterizing to estimate log.del = 
> log(alpha-min(x))?  Pass log.del as part of the vector of parameters
to 
> estimate, then immediately inside the function, compute
> 
>       alpha <- (min(x)+exp(log.del))
> 
>        I've fixed many convergence problems with simple tricks like
this. 
> 
>       hope this helps.  spencer graves



From jasont at indigoindustrial.co.nz  Thu Oct  9 10:36:23 2003
From: jasont at indigoindustrial.co.nz (Jason Turner)
Date: Thu, 09 Oct 2003 21:36:23 +1300
Subject: [R] Previous Commands
In-Reply-To: <3699CDBC4ED5D511BE6400306E1C0D8114CD42@hermes.demogr.mpg.de>
References: <3699CDBC4ED5D511BE6400306E1C0D8114CD42@hermes.demogr.mpg.de>
Message-ID: <3F851E07.7000201@indigoindustrial.co.nz>

Rau, Roland wrote:

> Dear All,
> 
> yesterday I took the R-1.8.0-source file and compiled it on my own. As I am
> using Linux just for a couple of weeks, it was my first compiling session
> with ./configure, make, ....
> Everything went fine, except for one thing: if I want to look at the
> commands history by using the cursor keys, it does not work. Instead of
> displaying the previous commands, it returns something like "[[A".
> Is this a common problem?
> Does anyone have a solution?
> 
> I had R-1.7.1 installed previously via an RPM-package for my Mandrake 9.1
> distribution and everything went fine.

You need the readline-devel libraries installed.  Depending on what 
Linux distro you're currently on (still Mandrake?), they may be 
available as an RPM or Debian package.  Install them, then if you still 
have the source directory, do a "make distclean ; configure ; make", 
then make install and try again.  If you've deleted the source 
directory, just untar the source and rebuild.

Cheers

Jason
-- 
Indigo Industrial Controls Ltd.
http://www.indigoindustrial.co.nz
64-21-343-545
jasont at indigoindustrial.co.nz



From p.dalgaard at biostat.ku.dk  Thu Oct  9 10:48:56 2003
From: p.dalgaard at biostat.ku.dk (Peter Dalgaard BSA)
Date: Thu, 09 Oct 2003 08:48:56 -0000
Subject: [R] Previous Commands
In-Reply-To: <20031009083043.GB21787@pasteur-kh.org>
References: <3699CDBC4ED5D511BE6400306E1C0D8114CD42@hermes.demogr.mpg.de>
	<20031009083043.GB21787@pasteur-kh.org>
Message-ID: <x24qyi4ww9.fsf@biostat.ku.dk>

Philippe Glaziou <glaziou at pasteur-kh.org> writes:

> Rau, Roland <Rau at demogr.mpg.de> wrote:
> > yesterday I took the R-1.8.0-source file and compiled it on my
> > own. As I am using Linux just for a couple of weeks, it was my
> > first compiling session with ./configure, make, ....
> > Everything went fine, except for one thing: if I want to look
> > at the commands history by using the cursor keys, it does not
> > work. Instead of displaying the previous commands, it returns
> > something like "[[A".  Is this a common problem?  Does anyone
> > have a solution?
> 
> 
> You may need to install the ncurses dev libraries.

and/or readline-devel

-- 
   O__  ---- Peter Dalgaard             Blegdamsvej 3  
  c/ /'_ --- Dept. of Biostatistics     2200 Cph. N   
 (*) \(*) -- University of Copenhagen   Denmark      Ph: (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)             FAX: (+45) 35327907



From ripley at stats.ox.ac.uk  Thu Oct  9 10:49:22 2003
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Thu, 9 Oct 2003 09:49:22 +0100 (BST)
Subject: [R] Previous Commands
In-Reply-To: <20031009083043.GB21787@pasteur-kh.org>
Message-ID: <Pine.LNX.4.44.0310090947150.10543-100000@gannet.stats>

On Thu, 9 Oct 2003, Philippe Glaziou wrote:

> Rau, Roland <Rau at demogr.mpg.de> wrote:
> > yesterday I took the R-1.8.0-source file and compiled it on my
> > own. As I am using Linux just for a couple of weeks, it was my
> > first compiling session with ./configure, make, ....
> > Everything went fine, except for one thing: if I want to look
> > at the commands history by using the cursor keys, it does not
> > work. Instead of displaying the previous commands, it returns
> > something like "[[A".  Is this a common problem?  Does anyone
> > have a solution?
> 
> 
> You may need to install the ncurses dev libraries.

More likely it is the readline-del[ev] RPMs/.debs/.... that you need.
Take a look at the R-admin manual (e.g. doc/html/R-admin.html in the 
sources).  

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From glaziou at pasteur-kh.org  Thu Oct  9 10:54:20 2003
From: glaziou at pasteur-kh.org (Philippe Glaziou)
Date: Thu, 9 Oct 2003 15:54:20 +0700
Subject: [R] Previous Commands
In-Reply-To: <20031009083043.GB21787@pasteur-kh.org>
References: <3699CDBC4ED5D511BE6400306E1C0D8114CD42@hermes.demogr.mpg.de>
	<20031009083043.GB21787@pasteur-kh.org>
Message-ID: <20031009085420.GC21787@pasteur-kh.org>

Philippe Glaziou <glaziou at pasteur-kh.org> wrote:
> Rau, Roland <Rau at demogr.mpg.de> wrote:
> > yesterday I took the R-1.8.0-source file and compiled it on my
> > own. As I am using Linux just for a couple of weeks, it was my
> > first compiling session with ./configure, make, ....
> > Everything went fine, except for one thing: if I want to look
> > at the commands history by using the cursor keys, it does not
> > work. Instead of displaying the previous commands, it returns
> > something like "[[A".  Is this a common problem?  Does anyone
> > have a solution?
> 
> 
> You may need to install the ncurses dev libraries.

oops, sorry, I meant readline. And then re-configure.

-- 
Philippe



From bobbie at tzv.fal.de  Thu Oct  9 11:12:11 2003
From: bobbie at tzv.fal.de (Bobbie van der Westhuizen)
Date: Thu, 9 Oct 2003 11:12:11 +0200
Subject: [R] Correlations
Message-ID: <200310091112.11427.bobbie@tzv.fal.de>

Hi I am trying out R for the first time and it seems to work nice.

I also try it out on a LINUX platform aswell as on Windows. I downloaded the 
new version of R for windows (version 8.1.0) but something look not right in 
the calculations of correlations between traits when I use the next command:

>cor(fc,use="pairwise")

The correlation return between the same trait (that must be 1.000) is not 1 
and sometimes it is 0.37?? When I run the same scrip with the same data in 
Linux the correlation is correctly, 1.000. And when I run the following scrip 
in 1.8.0 for windows it returns the correct correlation of 1.000.

>cor.test(BIRHTMASS,BIRHTMASS,na.rm=T,method="pearson")

Could there be a bug in this version of Windows?



From vito.muggeo at giustizia.it  Thu Oct  9 09:52:49 2003
From: vito.muggeo at giustizia.it (Vito Muggeo)
Date: Thu, 9 Oct 2003 09:52:49 +0200
Subject: [R] [R-pkgs] new package: segmented
Message-ID: <011a01c38e3a$59eda620$5c13070a@PROCGEN>

A few days ago I uploaded to CRAN a new package called segmented.

The package contains functions to fit (generalized) linear models with
`segmented' (or `broken-line' or `piecewise linear') relationships between
the response and one or more explanatory variables according to methodology
described in

Muggeo VMR (2003), Estimating regression models with unknown break-points,
Statistics in Medicine, 22, 3055-3071.

I would be very grateful for comments and suggestions,

best,
vito

_______________________________________________
R-packages mailing list
R-packages at stat.math.ethz.ch
https://www.stat.math.ethz.ch/mailman/listinfo/r-packages



From grassi at psico.univ.trieste.it  Thu Oct  9 11:38:20 2003
From: grassi at psico.univ.trieste.it (Michele Grassi)
Date: Thu, 9 Oct 2003 11:38:20 +0200 (MEST)
Subject: [R] simulate binary data from a logistic regression model
Message-ID: <200310090938.LAA07190@server.psico.univ.trieste.it>

Hi.
How can i simulate a binary data set from a logistic 
regression model?I need to manipulate parameters and so 
obtain my set of data.
I want to show the improve in analyzing binary data 
with GLM(binomial) model instead of classical ANOVA or 
NON-MODELS procedures(relative risk-odds ratio-Pearson 
test of godness of fit...)
Can you say me what is the right function to use?
Do you know any interesting simulation in the web?

Thank you.
Michele.



From Simon.Fear at synequanon.com  Thu Oct  9 12:26:25 2003
From: Simon.Fear at synequanon.com (Simon Fear)
Date: Thu, 9 Oct 2003 11:26:25 +0100
Subject: is.na(v)<-b (was: Re: [R] Beginner's query - segmentation fault)
Message-ID: <6C8A8033ABC1E3468048ABC4F13CE572AC0E11@synequanon01>


> -----Original Message-----
> From: Richard A. O'Keefe [mailto:ok at cs.otago.ac.nz]
<snip>

> The very existence of an "is.na<-" which accepts  a logical
> vector containing FALSE as well as TRUE ...

And don't forget this is not the only usage of is.na<-. In fact it is 
designed to take any valid indexing value. For example:

> a<-1:10
> is.na(a) <- 1:5
> a
 [1] NA NA NA NA NA  6  7  8  9 10

Wow. I really hate that. Someone tell me again why this is
better than a[1:5] <- NA ??
 

Simon Fear
Senior Statistician
Syne qua non Ltd
Tel: +44 (0) 1379 644449
Fax: +44 (0) 1379 644445
email: Simon.Fear at synequanon.com
web: http://www.synequanon.com
 
Number of attachments included with this message: 0
 
This message (and any associated files) is confidential and\...{{dropped}}



From arc at arcriswell.com  Thu Oct  9 05:36:34 2003
From: arc at arcriswell.com (Andrew Criswell)
Date: Thu, 9 Oct 2003 10:36:34 +0700
Subject: [R] Exact logistic regression models
References: <1065057311.3f7b7c1f6ba6e@webmail.uoguelph.ca>
	<1065062313.4838.73.camel@localhost>
Message-ID: <006301c38e16$8ed7adf0$0c8256d2@bangkok514f9fe>

Hello All:

Is there such a function in the R libraries: exact logistic regression?

Thanks,
ANDREW



From mrufino at icm.csic.es  Thu Oct  9 12:34:08 2003
From: mrufino at icm.csic.es (Marta Rufino)
Date: Thu, 09 Oct 2003 12:34:08 +0200
Subject: [R] polygon border
Message-ID: <5.2.1.1.1.20031009123205.01715ae8@cucafera.icm.csic.es>

Hello,

Does anyone know how a function to find automaticly the border (polygon) of 
a cloud of points?
It is for a PCA kind of analysis, where I wanted to present as the limits 
for each group, instead of all points...

thank you in advance
Marta



From wolfram at fischer-zim.ch  Thu Oct  9 12:40:25 2003
From: wolfram at fischer-zim.ch (Wolfram Fischer)
Date: Thu, 9 Oct 2003 12:40:25 +0200
Subject: [R] lattice/levelplot: panels with values can be empty
Message-ID: <20031009104025.GA3004@s1x.local>

I tried:
	library(lattice)

	F0  <- c( 'A', 'A', 'B', 'B' )
	F1  <- c(  1 ,  1 ,  1 ,  2 )
	F2  <- c(  8 ,  9 ,  8 ,  9 )
	VAL <- c(  20,  50,  10,  60 )
	df <- data.frame( F0, F1, F2, VAL )

	levelplot( VAL ~ F1 * F2 | F0, data=df )

I got an empty field for F0 == 'A'
and a colored field for F0 == 'B'.
I expected two colored fields. - What can I do?

Thanks. Wolfram



From Phguardiol at aol.com  Thu Oct  9 12:51:06 2003
From: Phguardiol at aol.com (Phguardiol@aol.com)
Date: Thu, 9 Oct 2003 06:51:06 EDT
Subject: [R] Source package installation for WinXPpro
Message-ID: <15b.25dc51d5.2cb6979a@aol.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20031009/35bc3942/attachment.pl

From calenge at biomserv.univ-lyon1.fr  Thu Oct  9 13:00:15 2003
From: calenge at biomserv.univ-lyon1.fr (=?iso-8859-1?Q?Cl=E9ment?= Calenge)
Date: Thu, 09 Oct 2003 13:00:15 +0200
Subject: [R] polygon border
In-Reply-To: <5.2.1.1.1.20031009123205.01715ae8@cucafera.icm.csic.es>
Message-ID: <5.1.0.14.0.20031009125926.00bb9160@biomserv.univ-lyon1.fr>

see ?chull

hope this helps,

Cl?ment.

At 12:34 09/10/2003 +0200, Marta Rufino wrote:
>Hello,
>
>Does anyone know how a function to find automaticly the border (polygon) 
>of a cloud of points?
>It is for a PCA kind of analysis, where I wanted to present as the limits 
>for each group, instead of all points...
>
>thank you in advance
>Marta
>
>______________________________________________
>R-help at stat.math.ethz.ch mailing list
>https://www.stat.math.ethz.ch/mailman/listinfo/r-help



From vito.muggeo at giustizia.it  Thu Oct  9 13:18:23 2003
From: vito.muggeo at giustizia.it (Vito Muggeo)
Date: Thu, 9 Oct 2003 13:18:23 +0200
Subject: R: [R] Exact logistic regression models
References: <1065057311.3f7b7c1f6ba6e@webmail.uoguelph.ca><1065062313.4838.73.camel@localhost>
	<006301c38e16$8ed7adf0$0c8256d2@bangkok514f9fe>
Message-ID: <01b601c38e57$10c12c20$5c13070a@PROCGEN>

No, as far as I know.

However the brlr package performs bias-reduced logistic regression that
might be useful for you. (for instance yields ML estimates when there is
separation in data and even CMLE do not exist)
See the reference in the help file ?brlr,

best,
vito



----- Original Message -----
From: Andrew Criswell <arc at arcriswell.com>
To: <R-help at stat.math.ethz.ch>
Sent: Thursday, October 09, 2003 5:36 AM
Subject: [R] Exact logistic regression models


> Hello All:
>
> Is there such a function in the R libraries: exact logistic regression?
>
> Thanks,
> ANDREW
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help



From sperber at ufv.br  Thu Oct  9 12:58:08 2003
From: sperber at ufv.br (Carlos Sperber)
Date: Thu, 09 Oct 2003 07:58:08 -0300
Subject: [R] robust regression and poisson errors
Message-ID: <5.1.0.14.0.20031009074232.00a9ab90@pop.ufv.br>

We have done a multiple regression (glm) with non-orthogonal design and two 
correlated explanatory variables, and poisson errors, because the response 
variable is a count (number of species). The parameter estimates calculated 
were opposite to the tendency observed in the plot. As we excluded the most 
influencial point, the results changed radically, but as we took the second 
influential point the results changed again, to a third form.
Thefore I tried to use robust regression, with rlm. My questions are:
1 - how do I get the test of bias of the robust regression? In the S-Plus 
guide (p. 338), it is said that this test should be used to choose between 
the robust regression and the other one. Is this correct?
2 - when I run an anova, comparing the complete rlm model to a more simple 
model, I get no values for F nor P. How do I get these?
3 - may I use robust regression for count response variables, taking the 
log of the data?

Thank you all!

Carlos Sperber, Carla Galbiati & Carla Ribas

Carlos Frankl Sperber
Ecology - Orthoptera: Grylloidea

Departamento de Biologia Geral
Universidade Federal de Vi?osa
3.570-000 - Vi?osa - MG
Brazil

tel: +55 (31) 3899 2556
fax: +55 (31) 3899 2549
E-mail: sperber at ufv.br


-------------- next part --------------

---




From ligges at statistik.uni-dortmund.de  Thu Oct  9 13:24:07 2003
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Thu, 09 Oct 2003 13:24:07 +0200
Subject: [R] Source package installation for WinXPpro
In-Reply-To: <15b.25dc51d5.2cb6979a@aol.com>
References: <15b.25dc51d5.2cb6979a@aol.com>
Message-ID: <3F854557.3090807@statistik.uni-dortmund.de>

[Please do not cross-post to several mailing lists]



Phguardiol at aol.com wrote:

 > Hi
 > I d like to compile the source code from last Affy package for 
WinXPpro under
 > R18beta
 > however the doc is pretty poor regarding this process...

...\gnuwin32\readme.packages and ...\gnuwin32\install do provide a great 
description.


 > I went to FAQ R Win then to http://www.stats.ox.ac.uk/pub/Rtools/ and
 > downloaded in The essentials, tools and Perl 5.8.0, then MingW from 
www.mingw.org and
 > I wonder if I need any other stuff from this page ?

Please read the files mentioned above and follow the instructions given 
therein exactly.


 > I m not sure but I dont
 > think I need the cross-conpilers ?

No, given you are not going to cross-compile ...


 > what about the others below ? Then I
 > installed MingW and Perl.
 > Then what am i suposed to do....?
 > If I click on Rcmd I dont get any windows to open or just for a second...

Rcmd is supposed to be used within the windows shell.

Uwe Ligges


 > thanks for your help
 > Philippe
 >
 > 	[[alternative HTML version deleted]]
 >
 > ______________________________________________
 > R-help at stat.math.ethz.ch mailing list
 > https://www.stat.math.ethz.ch/mailman/listinfo/r-help



From daniele.frison at email.it  Thu Oct  9 13:26:57 2003
From: daniele.frison at email.it (daniele.frison)
Date: Thu,  9 Oct 2003 13:26:57 +0200
Subject: [R] robust standard error
Message-ID: <HMHMGX$EA5C8C5164A56EF080DAB586AC40C56E@email.it>

How can I do a 
Multiple Linear Regression Models
 with robust standard error?
Thank in advance.
D.F.



--
Email.it, the professional e-mail, gratis per te: http://www.email.it/f

Sponsor:
Vieni a visitare il Garden Center Peraga. Seimila metri quadrati di esposizione per servire una clientela competente ed esigente.
Clicca qui: http://adv.email.it/cgi-bin/foclick.cgi?mid=1480&d=9-10



From vito.muggeo at giustizia.it  Thu Oct  9 13:44:39 2003
From: vito.muggeo at giustizia.it (Vito Muggeo)
Date: Thu, 9 Oct 2003 13:44:39 +0200
Subject: R: [R] simulate binary data from a logistic regression model
References: <200310090938.LAA07190@server.psico.univ.trieste.it>
Message-ID: <01fb01c38e5a$bc02b240$5c13070a@PROCGEN>

You can use the followings:

lp<- -5+..... #linear predictor
y<-rbinom(length(lp), size, plogis(lp))

Note that size means your "denominator" in proportions to be simulated. For
instance, if you want binary data use size=1.


best,
vito



----- Original Message -----
From: Michele Grassi <grassi at psico.univ.trieste.it>
To: <r-help at stat.math.ethz.ch>
Sent: Thursday, October 09, 2003 11:38 AM
Subject: [R] simulate binary data from a logistic regression model


> Hi.
> How can i simulate a binary data set from a logistic
> regression model?I need to manipulate parameters and so
> obtain my set of data.
> I want to show the improve in analyzing binary data
> with GLM(binomial) model instead of classical ANOVA or
> NON-MODELS procedures(relative risk-odds ratio-Pearson
> test of godness of fit...)
> Can you say me what is the right function to use?
> Do you know any interesting simulation in the web?
>
> Thank you.
> Michele.
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help



From jmacdon at med.umich.edu  Thu Oct  9 13:47:35 2003
From: jmacdon at med.umich.edu (James MacDonald)
Date: Thu, 09 Oct 2003 07:47:35 -0400
Subject: [R] Source package installation for WinXPpro
Message-ID: <sf851296.058@med-gwia-02a.med.umich.edu>

You need to make sure that all the tools are in your path, with the R
tools before perl and mingw. Set this by right clicking My Computer,
then properties, advanced, environment variables, then edit the path in
the system variables window. Once you have done that, you should be able
to download the sources (put them in a folder where the path doesn't
contain spaces. In addition, it is best if R is in a path that doesn't
contain spaces (so C:\Program Files\Rw1080 is not ideal. I usually
install in C:\Rw1080, but do what you like.)

Next, open a command prompt. Start menu, run, type cmd, enter. Then cd
to the folder where the affy.tar.gz resides and type 'tar xvfz affy*gz'
without the quotes. After everything is untarred and unzipped, cd to
Rw1080\bin and type 'rcmd install <path to affy folder>'. Windows is
case insensitive (mostly), so you don't need to capitalize. You should
then see a bunch of stuff happening, hopefully none of which contains an
error message.

If something doesn't work, re-read Brian Ripley's page on compiling. You
can also get good information by downloading the R-1.8.1 tarball and
looking for the readme-packages file.

HTH,

Jim




James W. MacDonald
Affymetrix and cDNA Microarray Core
University of Michigan Cancer Center
1500 E. Medical Center Drive
7410 CCGC
Ann Arbor MI 48109
734-647-5623
>>> <Phguardiol at aol.com> 10/09/03 06:53 AM >>>
Hi
I d like to compile the source code from last Affy package for WinXPpro
under 
R18beta
however the doc is pretty poor regarding this process...
I went to FAQ R Win then to http://www.stats.ox.ac.uk/pub/Rtools/ and 
downloaded in The essentials, tools and Perl 5.8.0, then MingW from
www.mingw.org and 
I wonder if I need any other stuff from this page ? I m not sure but I
dont 
think I need the cross-conpilers ? what about the others below ? Then I 
installed MingW and Perl.
Then what am i suposed to do....?
If I click on Rcmd I dont get any windows to open or just for a
second...
thanks for your help
Philippe

	[[alternative HTML version deleted]]

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://www.stat.math.ethz.ch/mailman/listinfo/r-help



From ripley at stats.ox.ac.uk  Thu Oct  9 13:50:51 2003
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Thu, 9 Oct 2003 12:50:51 +0100 (BST)
Subject: [R] Source package installation for WinXPpro
In-Reply-To: <15b.25dc51d5.2cb6979a@aol.com>
Message-ID: <Pine.LNX.4.44.0310091249150.10786-100000@gannet.stats>

On Thu, 9 Oct 2003 Phguardiol at aol.com wrote:

> Hi
> I d like to compile the source code from last Affy package for WinXPpro under 
> R18beta
> however the doc is pretty poor regarding this process...

Or possibly you have failed to find the documentation.

> I went to FAQ R Win then to http://www.stats.ox.ac.uk/pub/Rtools/ and 
> downloaded in The essentials, tools and Perl 5.8.0, then MingW from www.mingw.org and 
> I wonder if I need any other stuff from this page ? I m not sure but I dont 
> think I need the cross-conpilers ? what about the others below ? Then I 
> installed MingW and Perl.
> Then what am i suposed to do....?

Read the documentation, starting with file readme.packages in the R 
distribution.

> If I click on Rcmd I dont get any windows to open or just for a second...

And where in the documentation does it tell you to do that?

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From mt at michaelltaylor.com  Thu Oct  9 13:59:33 2003
From: mt at michaelltaylor.com (michaell taylor)
Date: Thu, 09 Oct 2003 11:59:33 -0000
Subject: [R] run R under unix
In-Reply-To: <3F84F7A4.7060808@indigoindustrial.co.nz>
References: <Sea2-F5p7IpGwC6RZT200003497@hotmail.com> 
	<3F84F7A4.7060808@indigoindustrial.co.nz>
Message-ID: <1065700816.20483.102.camel@xeon>

One can run R 'txt', script files thusly:

1. create the txt file (foo.txt) script.
2. at a command prompt type :
	R --vanilla < foo.txt > foo.results

The file 'foo.results' will now have all the output that you normally
would see on the screen.  This is actually quite useful in that you can
move around freely within foo.results (using some sort of text editor) -
unlike results written to a screen.

This will only run while you are logged into the machine, however.  The
best way to run a script without having to be logged in is through a
batch processor. Indeed, for jobs running for days - your system
administrator will be thrilled that it runs on the batch processor
instead of interactively.  The method for doing this depends on your
particular unix machine configuration, but a common method flow like
this:

1. Place the line:

R --vanilla < foo.txt > foo.results

into a file named foo.batch.  No other text should be in the file. Make
this file executable via

> chmod 755 foo.batch

Then at the command line:

> at -f foo.batch now

or perhaps,
> batch -f foo.batch

If this does not work, ask your system administrator how to set up a
batch process.

The advantage of the batch process is 1) you need not be logged in, 2)
your job will take a lower priority than interactive jobs.

Michaell

On Thu, 2003-10-09 at 01:52, Jason Turner wrote:
> Zhen Pang wrote:
> ...
> > I now want to run the code under unix. However, I do not know how to run 
> > this code in txt file under unix. I just log in to the unix and enter 
> > the R, what should I do next?
> > 
> > One more question is: if I log off my computer when R is running under 
> > unix (i.e., disconnect my computer from server), will I get the result 
> > when I log in my computer next time?
> ...
> 
> You'll lose it if you run R in the normal, interactive way.  Running it 
> in the background will allow you to log out and still have it running, but!
> 
> 1) If you're not the only person using this machine, you learn the 
> command "nice" before you begin.
> 2) I'm not certain you'll be able to produce jpeg or png graphics when 
> backgrounded; your backgrounded task needs access to the windowing 
> system for graphics rendering, and local security policy might prohibit 
> this.
> 3) Save early, save often.  You probably already know that, but it bears 
> repeating.
> 
> Here are some suggested steps to run your simulation in background mode. 
>   Unfortunately, the exact commands will depend on which version of Unix 
> you're using, and what command shells are available.  Consult your local 
> expert.
> 
> 1) transfer the text file of commands to the unix machine.  FTP, using 
> ASCII mode is safest.
> 2) log onto the Unix machine.
> 3) run sh, ksh, or bash.  (the syntax for what follows is different for 
> the C shell, and I don't know it).
> 4) using a stripped-down version of your script which will complete in a 
> short time (say, a minute or two), just to check that things work, type
> 
> nohup nice 15 R < my.small.script >my.output 2>&1 &
> 
> (again, learn what "nice" means before you use it.  This may not be 
> suitable, and it's impossible for me to tell from here if it is).
> 
> I know that's not the full answer, but only someone who knows the local 
> setup can give you that answer.
> 
> Cheers
> 
> Jason
> -- 
> Indigo Industrial Controls Ltd.
> http://www.indigoindustrial.co.nz
> 64-21-343-545
> jasont at indigoindustrial.co.nz
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help



From tord.snall at ebc.uu.se  Thu Oct  9 14:30:57 2003
From: tord.snall at ebc.uu.se (Tord Snall)
Date: Thu, 09 Oct 2003 14:30:57 +0200
Subject: Under dispersion; Was: [R] binomial glm warnings revisited
In-Reply-To: <x2pth7jz2d.fsf@biostat.ku.dk>
References: <3F844F5D.40806@pdf.com>
	<3.0.6.32.20031008174724.00c59560@mail.anst.uu.se>
	<3F844F5D.40806@pdf.com>
Message-ID: <3.0.6.32.20031009143057.00c6e020@mail.anst.uu.se>

Dear all,

>
>> >I have this problem with my data. In a GLM, I have 269 zeroes and
>> >only 1 one:
>
>
>During profiling, you may be pushing one of the parameter near the
>extremes and get a model where the fitted p's are very close to 0/1.

I just want to clarify that the warning was given already when I fitted the
glm():

> dbh<- glm(MPext ~ dbh, maxit = 100, family = "binomial", data = valkdat)
Warning message: 
fitted probabilities numerically 0 or 1 occurred in: (if (

(As you can see I had to increase maxit for th algorithm to converge.)

A summary:
summary(dbh)
Coefficients:
            Estimate Std. Error z value Pr(>|z|)
(Intercept)   0.1659     3.8781   0.043    0.966
dbh          -0.5872     0.5320  -1.104    0.270
    Null deviance: 13.1931  on 269  degrees of freedom
Residual deviance:  9.9168  on 268  degrees of freedom
AIC: 13.917

> drop1(dbh, test = "Chisq")
       Df Deviance     AIC     LRT Pr(Chi)  
<none>      9.9168 13.9168                  
dbh     1  13.1931 15.1931  3.2763 0.07029 .

And then CI:
confint(dbh)
Waiting for profiling to be done...
                2.5 %      97.5 %
(Intercept) -6.458119 10.12380773
dbh         -2.253015 -0.05047997
There were 17 warnings (use warnings() to see them)

BUT, note the under dispersion. I GUESS it is because I have surveyed a
moss on marked trees at three occations (with two years in between). The
response 1 means that the moss has disappeared, and dbh is tree diameter.
(This corresponds to revisitng patients who has a disease, and whose weight
is unchanged between the visits. H0: weight does not affect tha chance of
recovery from the disease)

Here is a version with quasibinomial:

> dbh<- glm(MPext ~ dbh, maxit = 100, family = "quasibinomial", data =
valkdat)

Note, no warning.

> summary(dbh)
            Estimate Std. Error t value Pr(>|t|)  
(Intercept)   0.1659     1.7179   0.097   0.9231  
dbh          -0.5872     0.2357  -2.491   0.0133 *
(Dispersion parameter for quasibinomial family taken to be 0.1962275)
    Null deviance: 13.1931  on 269  degrees of freedom
Residual deviance:  9.9168  on 268  degrees of freedom
AIC: NA
Number of Fisher Scoring iterations: 11

> confint(dbh)
Waiting for profiling to be done...
                2.5 %     97.5 %
(Intercept) -2.970644  3.9019555
dbh         -1.158646 -0.2131936

> drop1(dbh, test = "Chisq")
       Df Deviance scaled dev.   Pr(Chi)    
<none>      9.9168                          
dbh     1  13.1931     16.6966 4.386e-05 ***

Note, no warning.

I guess that this quasibinomial model is more reliable than the binomial.
Now I can trust the SE of the Estim. too, can't I? 

(Under dispersion has not been discussed on the list except for a reply by
Prof. Ripley on a Poisson model question.)


>That's not necessarily a sign of unreliability -- the procedure is to
>set one parameter to a sequence of fixed values and optimize over the
>other, and it might just be the case that the optimizations have been
>wandering a bit far from the optimum. (I'd actually be more suspicious
>about the fact that the name of the predictor suddenly changed....)

:D 

>
>However, if you have only one "1" you are effectively asking whether
>one observation has a different mean than the other 269, and you have
>to consider the sensitivity to the distribution of the predictor. As
>far as I can see, you end up with the test of the null hypothesis
>beta==0 being essentially equivalent to a two sample t test between
>the mean of the "0" group and that of the "1" group, so with only one
>observation in one of the groups, the normal approximation of the test
>hinges quite strongly on a normal distribution of the predictor
>itself.

Thanks for this interesting point of view.


Sincerely,
Tord

>
>-- 
>   O__  ---- Peter Dalgaard             Blegdamsvej 3  
>  c/ /'_ --- Dept. of Biostatistics     2200 Cph. N   
> (*) \(*) -- University of Copenhagen   Denmark      Ph: (+45) 35327918
>~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)             FAX: (+45) 35327907
>

-----------------------------------------------------------------------
Tord Sn?ll
Avd. f v?xtekologi, Evolutionsbiologiskt centrum, Uppsala universitet
Dept. of Plant Ecology, Evolutionary Biology Centre, Uppsala University
Villav?gen 14			
SE-752 36 Uppsala, Sweden
Tel: 018-471 28 82 (int +46 18 471 28 82) (work)
Tel: 018-25 71 33 (int +46 18 25 71 33) (home)
Fax: 018-55 34 19 (int +46 18 55 34 19) (work)
E-mail: Tord.Snall at ebc.uu.se
Check this: http://www.vaxtbio.uu.se/resfold/snall.htm!



From kjetil at entelnet.bo  Thu Oct  9 14:45:04 2003
From: kjetil at entelnet.bo (kjetil@entelnet.bo)
Date: Thu, 09 Oct 2003 08:45:04 -0400
Subject: [R] simulate binary data from a logistic regression model
In-Reply-To: <200310090938.LAA07190@server.psico.univ.trieste.it>
Message-ID: <3F852010.29321.35ED1D@localhost>

On 9 Oct 2003 at 11:38, Michele Grassi wrote:

Here is one way of doing it:

> x <- rnorm(1000)
> beta <- 1
> p <- 1/(1+exp(-beta*x))
> o <- order(x)
> plot( x[o], p[o], ylim=c(0,1), type="l")
> y <- rbinom(1000, 1, prob=p)
> model <- glm(y ~ x , family=binomial)
> summary(model)
.
.
.

> B <- 1000 # number of simulation replications
> coefs <- matrix(0, B, 2)
> for (i in 1:B) {
+    coefs[i, ] <- coef(glm(rbinom(1000,1,prob=p) ~ x, 
family=binomial))
+ }
> hist( coefs[,1])
> hist(coefs[,2])
> plot(coefs)



Kjetil Halvorsen

Hi.
How can i simulate a binary data set from a logistic 
regression model?I need to manipulate parameters and so 
obtain my set of data.
I want to show the improve in analyzing binary data 
with GLM(binomial) model instead of classical ANOVA or 
NON-MODELS procedures(relative risk-odds ratio-Pearson 
test of godness of fit...)
Can you say me what is the right function to use?
Do you know any interesting simulation in the web?

Thank you.
Michele.

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://www.stat.math.ethz.ch/mailman/listinfo/r-help



From ripley at stats.ox.ac.uk  Thu Oct  9 14:52:34 2003
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Thu, 9 Oct 2003 13:52:34 +0100 (BST)
Subject: [R] Correlations
In-Reply-To: <200310091112.11427.bobbie@tzv.fal.de>
Message-ID: <Pine.LNX.4.44.0310091350001.12686-100000@gannet.stats>

On Thu, 9 Oct 2003, Bobbie van der Westhuizen wrote:

> Hi I am trying out R for the first time and it seems to work nice.
> 
> I also try it out on a LINUX platform aswell as on Windows. I downloaded the 
> new version of R for windows (version 8.1.0) but something look not right in 
> the calculations of correlations between traits when I use the next command:
> 
> >cor(fc,use="pairwise")
> 
> The correlation return between the same trait (that must be 1.000) is not 1 
> and sometimes it is 0.37?? When I run the same scrip with the same data in 
> Linux the correlation is correctly, 1.000. And when I run the following scrip 
> in 1.8.0 for windows it returns the correct correlation of 1.000.
> 
> >cor.test(BIRHTMASS,BIRHTMASS,na.rm=T,method="pearson")
> 
> Could there be a bug in this version of Windows?

Given that R 1.8.0 (sic) for Windows is not yet on CRAN, there is at least
a bug in your description, and undoubtedly there are bugs in Windows.

If you want to discuss a bug in R, please consult the FAQ and provide 
proper version information and a reproducible example.

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From p.dalgaard at biostat.ku.dk  Thu Oct  9 15:01:37 2003
From: p.dalgaard at biostat.ku.dk (Peter Dalgaard BSA)
Date: Thu, 09 Oct 2003 13:01:37 -0000
Subject: Under dispersion; Was: [R] binomial glm warnings revisited
In-Reply-To: <3.0.6.32.20031009143057.00c6e020@mail.anst.uu.se>
References: <3F844F5D.40806@pdf.com>
	<3.0.6.32.20031008174724.00c59560@mail.anst.uu.se>
	<3F844F5D.40806@pdf.com>
	<3.0.6.32.20031009143057.00c6e020@mail.anst.uu.se>
Message-ID: <x2ad8a36lj.fsf@biostat.ku.dk>

Tord Snall <tord.snall at ebc.uu.se> writes:

>     Null deviance: 13.1931  on 269  degrees of freedom
> Residual deviance:  9.9168  on 268  degrees of freedom
> AIC: 13.917
...

> BUT, note the under dispersion. I GUESS it is because I have surveyed a
> moss on marked trees at three occations (with two years in between). The
> response 1 means that the moss has disappeared, and dbh is tree diameter.
> (This corresponds to revisitng patients who has a disease, and whose weight
> is unchanged between the visits. H0: weight does not affect tha chance of
> recovery from the disease)

Don't trust deviances as measures of dispersion with binary data! 

> Here is a version with quasibinomial:
> 
...
> 
> Note, no warning.
> 
> I guess that this quasibinomial model is more reliable than the binomial.
> Now I can trust the SE of the Estim. too, can't I? 

No. Neither nor.
 
With binary data, the deviance is purely a function of the fitted
parameters. It is the difference in -2 log L between a "perfect fit"
and the observed fit. A perfect fit has a zero prob. where the obs is
"0" and probability 1 where it is "1", and L == 1 identically in that
case. Now consider the likelihood for the "complete toss-up" i.e.
intercept and slope both equal to 0 so all probabilities are 0.5. The
likelihood in that case is 0.5^269, i.e. a constant. Take logarithms
and notice that the model deviance plus the change in deviance from
the model to the "toss-up" model is constant (2*269*log(2) to be
precise). So what appears to be a measure of residual error is
really just a measure of how far the fitted probabilities are from
0.5!


-- 
   O__  ---- Peter Dalgaard             Blegdamsvej 3  
  c/ /'_ --- Dept. of Biostatistics     2200 Cph. N   
 (*) \(*) -- University of Copenhagen   Denmark      Ph: (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)             FAX: (+45) 35327907



From feh3k at spamcop.net  Thu Oct  9 15:03:48 2003
From: feh3k at spamcop.net (Frank E Harrell Jr)
Date: Thu, 9 Oct 2003 09:03:48 -0400
Subject: [R] robust standard error
In-Reply-To: <HMHMGX$EA5C8C5164A56EF080DAB586AC40C56E@email.it>
References: <HMHMGX$EA5C8C5164A56EF080DAB586AC40C56E@email.it>
Message-ID: <20031009090348.24ba3548.feh3k@spamcop.net>

On Thu,  9 Oct 2003 13:26:57 +0200
"daniele\.frison" <daniele.frison at email.it> wrote:

> How can I do a 
> Multiple Linear Regression Models
>  with robust standard error?
> Thank in advance.
> D.F.
> 
> 

One way:

install.packages('Hmisc'); install.packages('Design')
library(Design)
f <- ols(y ~ x1 + ...., x=T, y=T)
g <- robcov(f)  # or bootcov to use bootstrap s.e.'s

---
Frank E Harrell Jr    Professor and Chair            School of Medicine
                      Department of Biostatistics    Vanderbilt University



From feh3k at spamcop.net  Thu Oct  9 15:04:18 2003
From: feh3k at spamcop.net (Frank E Harrell Jr)
Date: Thu, 9 Oct 2003 09:04:18 -0400
Subject: [R] simulate binary data from a logistic regression model
In-Reply-To: <200310090938.LAA07190@server.psico.univ.trieste.it>
References: <200310090938.LAA07190@server.psico.univ.trieste.it>
Message-ID: <20031009090418.1e6f9540.feh3k@spamcop.net>

On Thu, 9 Oct 2003 11:38:20 +0200 (MEST)
Michele Grassi <grassi at psico.univ.trieste.it> wrote:

> Hi.
> How can i simulate a binary data set from a logistic 
> regression model?I need to manipulate parameters and so 
> obtain my set of data.
> I want to show the improve in analyzing binary data 
> with GLM(binomial) model instead of classical ANOVA or 
> NON-MODELS procedures(relative risk-odds ratio-Pearson 
> test of godness of fit...)
> Can you say me what is the right function to use?
> Do you know any interesting simulation in the web?
> 
> Thank you.
> Michele.
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help

This is from the Overview help file from the Design package:

    n <- 1000    # define sample size
    set.seed(17) # so can reproduce the results
    treat <- factor(sample(c('a','b','c'), n, T))
    num.diseases <- sample(0:4, n, T)
    age <- rnorm(n, 50, 10)
    cholesterol <- rnorm(n, 200, 25)
    weight <- rnorm(n, 150, 20)
    sex <- factor(sample(c('female','male'), n, T))

    # Specify population model for log odds that Y=1
    L <- .1*(num.diseases-2) + .045*(age-50) +
      (log(cholesterol - 10)-5.2)*(-2*(treat=='a') +
          3.5*(treat=='b')+2*(treat=='c'))
    # Simulate binary y to have Prob(y=1) = 1/[1+exp(-L)]
    y <- ifelse(runif(n) < plogis(L), 1, 0)

But note that it's no longer necessary to demonstrate that logistic regression works better than ordinary regression when the response is binary.

---
Frank E Harrell Jr    Professor and Chair            School of Medicine
                      Department of Biostatistics    Vanderbilt University



From polzehl at wias-berlin.de  Thu Oct  9 15:11:05 2003
From: polzehl at wias-berlin.de (=?iso-8859-1?Q?J=F6rg?= Polzehl)
Date: Thu, 09 Oct 2003 15:11:05 +0200
Subject: [R] Partly successful build on Alpha/Tru64 UNIX 5.1
Message-ID: <5.2.1.1.0.20031009144911.07550048@mail-service.wias-berlin.de>

Dear all,

      I'd like to share my experiences in building R 1.8.0 on an 
Alpha/Tru64 UNIX 5.1
machine.

I used the following modifications in file  config.site:

CC=cc
F77=f77
CXX=c++
MAKE=gmake

with these settings I was able to successfully run

./configure MAKE=gmake

I had to manually unpack the files in    src/library/recommended
to get

gmake

running through the end (otherwise gmake complained about a
missing method for survival). Everything worked fine then.

gmake check

produces three errors in    arith.R .
All of these errors are due to an occurrence of  a  -0 instead of 0.
Unfortunately gmake check stops here, with  gmake check FORCE=FORCE
yielding the same result.

gmake install

runs without problems.

Best regards,

J?rg Polzehl

Dr. J?rg Polzehl
WIAS, Mohrenstr. 39
D-10117 Berlin, Germany
email: polzehl at wias-berlin.de
Tel. +49 30/20372 481
Fax: +49 30/2044975
WWW: http://www.wias-berlin.de/people/polzehl



From bates at stat.wisc.edu  Thu Oct  9 15:32:03 2003
From: bates at stat.wisc.edu (Douglas Bates)
Date: Thu, 09 Oct 2003 13:32:03 -0000
Subject: [R] anonymous access to the R CVS archive
Message-ID: <6rwubed0dx.fsf@bates4.stat.wisc.edu>

Thanks to John Eaton of the Octave project we now have anonymous CVS
access to (a copy of) the R archive.  See http://anoncvs.r-project.org/

_______________________________________________
R-announce at stat.math.ethz.ch mailing list
https://www.stat.math.ethz.ch/mailman/listinfo/r-announce



From bates at stat.wisc.edu  Thu Oct  9 15:34:20 2003
From: bates at stat.wisc.edu (Douglas Bates)
Date: Thu, 09 Oct 2003 13:34:20 -0000
Subject: [R] nlme & lme for complex mixed ANOVAs
In-Reply-To: <875DC9F4-FA03-11D7-B30F-003065D59C7A@vanderbilt.edu>
References: <875DC9F4-FA03-11D7-B30F-003065D59C7A@vanderbilt.edu>
Message-ID: <6rfzi2cz5b.fsf@bates4.stat.wisc.edu>

The short answer is "yes".

The longer answer is the lme function in the nlme package uses a
compact representation of the model matrices for a mixed-effects
model.  It is a sparse matrix representation that is specific to cases
of strictly nested random effects when there is more that one level of
random effects.  In your case it seems that there will only be random
effects for animal and the sparse matrix representation would apply.

You may find that the version of lme in the experimental lme4 package
is faster than the version in nlme.

David Airey <david.airey at vanderbilt.edu> writes:

> I downloaded R for the first time yesterday, in the hopes that I might
> deal more effectively with a complex repeated measures experimental
> design involving inbred strains of laboratory mice. The design below,
> somewhat simplified, cannot be computed with standard ANOVA, because
> something called the X'X matrix is too large. The design has the
> following factors:
> 
> 
> Between-subject factors (levels):
> 	inbred mouse strain (20, "twenty")
> 	sex (2)
> Animals:
> 	10 per sex*strain combination (400 total)
> Within-subject factors:
> 	drug (3)
> 	trial set (4)
> 	stimulus characteristic 1 (2)
> 	stimulus characteristic 2 (2)
> 
> My question for the R community is, does R have in nlme or lme the
> ability to compute this problem on a typical desktop PC? In Stata, for
> instance, which has a matrix size limit of 11,000, the problem above
> will not fit, using standard ANOVA. Matrix size can be determined by
> listing all the terms of the full model and multiplying the levels of
> each factor withing a term and summing all terms. I said simplified in
> the first paragraph above, because if I include the day of drug
> challenge, the model oversteps the number of within-subject factors
> allowed. So, surprisingly to me, Stata/SE, which I bought for large
> data sets, is too small! Not that I don't like Stata, but I am annoyed
> that I must find another tool to use. I understand that SAS Proc Mixed
> will compute the problem, because it may handle the covariance matrix
> in some kind of piecemeal fashion (perhaps by animal but I've no
> confirmation of this, except that it zips through a comparable data
> set on someone else's computer). However, I am running Apple OS X and
> don't have SAS on my machine. I don't really understand what's going
> on underneath these programs respective hoods, but my question here is
> whether R computes mixed models in such a way as to require a matrix
> size like Stata or like SAS, when mixed models of this size are
> presented?
> 
> 
> Sincerely,
> 
> -Dave
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help

-- 
Douglas Bates                            bates at stat.wisc.edu
Statistics Department                    608/262-2598
University of Wisconsin - Madison        http://www.stat.wisc.edu/~bates/



From tord.snall at ebc.uu.se  Thu Oct  9 16:05:29 2003
From: tord.snall at ebc.uu.se (Tord Snall)
Date: Thu, 09 Oct 2003 16:05:29 +0200
Subject: Under dispersion; Was: [R] binomial glm warnings revisited
In-Reply-To: <x2ad8a36lj.fsf@biostat.ku.dk>
References: <3.0.6.32.20031009143057.00c6e020@mail.anst.uu.se>
	<3F844F5D.40806@pdf.com>
	<3.0.6.32.20031008174724.00c59560@mail.anst.uu.se>
	<3F844F5D.40806@pdf.com>
	<3.0.6.32.20031009143057.00c6e020@mail.anst.uu.se>
Message-ID: <3.0.6.32.20031009160529.00c6e020@mail.anst.uu.se>

Dear all,

>> BUT, note the under dispersion. I GUESS it is because I have surveyed a
>> moss on marked trees at three occations (with two years in between). The
>> response 1 means that the moss has disappeared, and dbh is tree diameter.
>> (This corresponds to revisitng patients who has a disease, and whose weight
>> is unchanged between the visits. H0: weight does not affect tha chance of
>> recovery from the disease)
>
>Don't trust deviances as measures of dispersion with binary data! 
>



From christoff_pale at yahoo.com  Thu Oct  9 16:39:58 2003
From: christoff_pale at yahoo.com (christoff pale)
Date: Thu, 9 Oct 2003 07:39:58 -0700 (PDT)
Subject: [R] startup file and lambda
Message-ID: <20031009143958.18131.qmail@web40806.mail.yahoo.com>

Hi,s there a .Rrc file? so that when R starts up
it automatically loads this file?
I am interested in putting in some matlab-like
functions names I am very use to.
for example: ans=.Last.value 
etc



From andy_liaw at merck.com  Thu Oct  9 16:50:59 2003
From: andy_liaw at merck.com (Liaw, Andy)
Date: Thu, 09 Oct 2003 10:50:59 -0400
Subject: [R] startup file and lambda
Message-ID: <3A822319EB35174CA3714066D590DCD50205CC5C@usrymx25.merck.com>

See ?Startup.

Andy

> -----Original Message-----
> From: christoff pale [mailto:christoff_pale at yahoo.com] 
> Sent: Thursday, October 09, 2003 10:40 AM
> To: r-help at stat.math.ethz.ch
> Subject: [R] startup file and lambda
> 
> 
> Hi,s there a .Rrc file? so that when R starts up
> it automatically loads this file?
> I am interested in putting in some matlab-like
> functions names I am very use to.
> for example: ans=.Last.value 
> etc
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list 
> https://www.stat.math.ethz.ch/mailman/listinfo> /r-help
>



From Sinnwell.Jason at mayo.edu  Thu Oct  9 16:52:09 2003
From: Sinnwell.Jason at mayo.edu (Jason Sinnwell)
Date: Thu, 9 Oct 2003 09:52:09 -0500 (CDT)
Subject: [R] dyn.load error with C file
Message-ID: <200310091452.h99Eq9523125@rocky.mayo.edu>

Re-sending this from 10/7.  Please help! I'm really clueless how to fix this:

System: Solaris 7,     Software: R_1.7.1 for unix

> R CMD INSTALL -l /Rdir/library haplo.stats_1.1.0.tar.gz

-works like a charm-- no syntax errors, etc


But problems are when I'm loading the library within R.
 
> library(haplo.stats, lib.loc="/people/biostat3/sinnwell/Rdir/library")
> Error in dyn.load(x, as.logical(local), as.logical(now)) : 
>         unable to load shared library 
> "/people/biostat3/sinnwell/Rdir/library/haplo.stats/libs/haplo.stats.so":
>   ld.so.1: /opt/tools/R/current-version/lib/R/bin/R.bin: fatal: relocation 
> error: file 
> /people/biostat3/sinnwell/Rdir/library/haplo.stats/libs/haplo.stats.so: symbol 
> errmsg: referenced symbol not found
> Error in library(haplo.stats, lib.loc = 
> "/people/biostat3/sinnwell/Rdir/library") : 
>         .First.lib failed

Within my package I have three C files, one of which I believe is causing the 
dynamic library error below.  
When I leave out this one C file when building the package, though remaining 
R-functions depend on it, the library loads beautifully.  We use all Calloc and 
Free for double-compatibility with S, so memory allocation is fine.  Is there 
some issue with C and R that I'm not aware of?


> FYI:  
> We have the whole haplo.stats package working in S, INCLUDING that C-file
> Also, zzz.R for haplo.stats package:
> 
> .First.lib <- function(lib, pkg) {
>    library.dynam("haplo.stats", pkg, lib)
> }

Has anyone dealt with anything similar to this?  I see some similar Fortran 
error messages...
 
Thank you for your help,
 
Jason 
 
-------------------------
Jason P. Sinnwell, M.S.
Mayo Clinic, Rochester
Health Sciences Research
Division of Biostatistics
507.284.3270



From p.pagel at gsf.de  Thu Oct  9 17:00:57 2003
From: p.pagel at gsf.de (Philipp Pagel)
Date: Thu, 9 Oct 2003 17:00:57 +0200
Subject: [R] startup file and lambda
In-Reply-To: <20031009143958.18131.qmail@web40806.mail.yahoo.com>
References: <20031009143958.18131.qmail@web40806.mail.yahoo.com>
Message-ID: <20031009150057.GA4885@porcupine.gsf.de>

On Thu, Oct 09, 2003 at 07:39:58AM -0700, christoff pale wrote:
> Hi,s there a .Rrc file? so that when R starts up
> it automatically loads this file?

?Startup

will tell you everything you want to know - especially bout the
.Rprofile file.

cu
	Philipp

-- 
Dr. Philipp Pagel                                Tel.  +49-89-3187-3675
Institute for Bioinformatics / MIPS              Fax.  +49-89-3187-3585
GSF - National Research Center for Environment and Health
Ingolstaedter Landstrasse 1
85764 Neuherberg, Germany



From Weiming.Zhang at uchsc.edu  Thu Oct  9 17:37:37 2003
From: Weiming.Zhang at uchsc.edu (Weiming Zhang)
Date: Thu, 09 Oct 2003 15:37:37 -0000
Subject: [R] upgrading R
Message-ID: <1065713904.13119.9.camel@molecule.uchsc.edu>

Hi,

I have installed a lot of extra packages for R 1.7.1. If I install R
1.8.0, will I have to reinstall all those packages? Is there a way that
I can upgrading R without losing old packages?

Thank you.

wz



From solares at unsl.edu.ar  Thu Oct  9 17:42:21 2003
From: solares at unsl.edu.ar (solares@unsl.edu.ar)
Date: Thu, 9 Oct 2003 12:42:21 -0300 (ART)
Subject: [R] curious mistake in tkradiobutton
Message-ID: <39918.170.210.173.216.1065714141.squirrel@inter14.unsl.edu.ar>

Hello, the following code produces an error when executing it, it is a code 
that produces 6 checkbutton that at 
the beginning are empty, when selecting the first checkbox he says that  
doesn't know the second 
variable tcl, he say: 

[1] "1"
Error in structure(.External("dotTcl", ..., PACKAGE = "tcltk"), class 
= "tclObj") : 
        [tcl] can't read "::RTcl2": no such variable.

when six variables tcl has been created in the array b and they exist:

> b
[1] "::RTcl1" "::RTcl2" "::RTcl3" "::RTcl4" "::RTcl5" "::RTcl6"

the script is(I'm use R 1.7.1 for win):

library(tcltk)
tt<-tktoplevel()
f<-tkframe(tt)
tkpack(f)
j<-6
nbre<-c("c1","c2","c3","c4","c5","c6")
b<-c()
i<-1
 while (i<=j){
  aux<-paste("b",i,sep="")
  aux<-tclVar(init=0)
  b<-c(b,as.character(aux))
  tclvalue(b[i])<-0
  i<-i+1
 }
 i<-1
 while (i<=j){
 t<-tkcheckbutton(f,text=nbre[i],variable=eval(b
[i]),relief="raised",command=function()ver())
 tkpack(t)
 i<-i+1
 }
 
 ver<-function(){
 k<-1
 while (k<=j){
  print(tclvalue(b[k]))
  k<-k+1
  }
  }

 any help will be welcome. Thank you Ruben



From tord.snall at ebc.uu.se  Thu Oct  9 17:54:09 2003
From: tord.snall at ebc.uu.se (Tord Snall)
Date: Thu, 09 Oct 2003 17:54:09 +0200
Subject: Thanks for: Re: Under dispersion; Was: [R] binomial glm
	warnings revisited
In-Reply-To: <x2ad8a36lj.fsf@biostat.ku.dk>
References: <3.0.6.32.20031009143057.00c6e020@mail.anst.uu.se>
	<3F844F5D.40806@pdf.com>
	<3.0.6.32.20031008174724.00c59560@mail.anst.uu.se>
	<3F844F5D.40806@pdf.com>
	<3.0.6.32.20031009143057.00c6e020@mail.anst.uu.se>
Message-ID: <3.0.6.32.20031009175409.00c6e020@mail.anst.uu.se>

Dear Peter and Spencer,

I guess that you are busy with other things now. 

I just want to tell that I appreciated your help.

Thanks very much!


Sincerely,
Tord



At 15:04 2003-10-09 +0200, Peter Dalgaard BSA wrote:
>Tord Snall <tord.snall at ebc.uu.se> writes:
>
>>     Null deviance: 13.1931  on 269  degrees of freedom
>> Residual deviance:  9.9168  on 268  degrees of freedom
>> AIC: 13.917
>.
>
>> BUT, note the under dispersion. I GUESS it is because I have surveyed a
>> moss on marked trees at three occations (with two years in between). The
>> response 1 means that the moss has disappeared, and dbh is tree diameter.
>> (This corresponds to revisitng patients who has a disease, and whose weight
>> is unchanged between the visits. H0: weight does not affect tha chance of
>> recovery from the disease)
>
>Don't trust deviances as measures of dispersion with binary data! 
>
>> Here is a version with quasibinomial:
>> 
>.
>> 
>> Note, no warning.
>> 
>> I guess that this quasibinomial model is more reliable than the binomial.
>> Now I can trust the SE of the Estim. too, can't I? 
>
>No. Neither nor.
> 
>With binary data, the deviance is purely a function of the fitted
>parameters. It is the difference in -2 log L between a "perfect fit"
>and the observed fit. A perfect fit has a zero prob. where the obs is
>"0" and probability 1 where it is "1", and L == 1 identically in that
>case. Now consider the likelihood for the "complete toss-up" i.e.
>intercept and slope both equal to 0 so all probabilities are 0.5. The
>likelihood in that case is 0.5^269, i.e. a constant. Take logarithms
>and notice that the model deviance plus the change in deviance from
>the model to the "toss-up" model is constant (2*269*log(2) to be
>precise). So what appears to be a measure of residual error is
>really just a measure of how far the fitted probabilities are from
>0.5!
>
>
>-- 
>   O__  ---- Peter Dalgaard             Blegdamsvej 3  
>  c/ /'_ --- Dept. of Biostatistics     2200 Cph. N   
> (*) \(*) -- University of Copenhagen   Denmark      Ph: (+45) 35327918
>~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)             FAX: (+45) 35327907
>

-----------------------------------------------------------------------
Tord Sn?ll
Avd. f v?xtekologi, Evolutionsbiologiskt centrum, Uppsala universitet
Dept. of Plant Ecology, Evolutionary Biology Centre, Uppsala University
Villav?gen 14			
SE-752 36 Uppsala, Sweden
Tel: 018-471 28 82 (int +46 18 471 28 82) (work)
Tel: 018-25 71 33 (int +46 18 25 71 33) (home)
Fax: 018-55 34 19 (int +46 18 55 34 19) (work)
E-mail: Tord.Snall at ebc.uu.se
Check this: http://www.vaxtbio.uu.se/resfold/snall.htm!



From mark.lee at orbisuk.com  Thu Oct  9 18:02:25 2003
From: mark.lee at orbisuk.com (Mark Lee)
Date: Thu, 09 Oct 2003 17:02:25 +0100
Subject: [R] Getting rows from a dataframe
Message-ID: <E1A7dFB-0005pP-00@orion.orbis>

Sorry if this is a silly question. I'm trying to extract all elements
of a dataframe at a particular row. I can find no mention of this in
any documentation and it may be naivety of dataframe on my part as I'm
very green at this. Thankyou,

Mark



From eric.esposito at gazdefrance.com  Thu Oct  9 17:39:31 2003
From: eric.esposito at gazdefrance.com (Eric ESPOSITO)
Date: Thu, 9 Oct 2003 17:39:31 +0200
Subject: [R] plotting graphs with Rterm
Message-ID: <OF54D7AB48.88741EB1-ON41256DBA.005A45F9@notes.edfgdf.fr>

Hello,
I need to execute R code contained in  a file xxx.R from DOS.
The file is really simple, only:
> plot(rnorm(100))

When I launch Rterm from Dos command and then source the file xxx.R it
works, but I need to call the command from a DOS command file *.bat
Using Rterm --slave < xxx.R plots the graph in a postscript file Rplots.ps,
how can I do to get the graph in an R window.
Using a command windows() or win.graph() in the R file does not work any
more.

I have the same problem with tk widgets. If the file xxx.R is:
>library(tcltk)
>tt<-tktoplevel()
The widget does not appear on the screen.

Does anybody can help?
Regards,

Eric



From laura at env.leeds.ac.uk  Thu Oct  9 18:05:57 2003
From: laura at env.leeds.ac.uk (Laura Quinn)
Date: Thu, 9 Oct 2003 17:05:57 +0100 (BST)
Subject: [R] Fitting AR(p) models
Message-ID: <Pine.LNX.4.44.0310091654430.8728-100000@env-pc-phd13>

I am wanting to fit AR(p) models to column data for a series of matrices
I have. Rather than trying to compute the AR models "per column" I was
hoping to be able to do this per matrix (i.e. one AR model for each column
in said matrix). However, when i attempt this, if there is just one "NA"
value in any of the columns, the program refuses to compute AR models for
any of the columns.(I am using na.action=na.omit)

Is there a way I can force the calculation to provide AR models for
complete columns, or alternatively, a better way to deal with NA's to
provide an AR model, (eg via interpolation?) despite some missing values?

I hope this makes some sense,

thanks in advance,
Laura



From rajarshi at presidency.com  Thu Oct  9 18:11:36 2003
From: rajarshi at presidency.com (Rajarshi Guha)
Date: Thu, 09 Oct 2003 16:11:36 -0000
Subject: [R] Getting rows from a dataframe
In-Reply-To: <E1A7dFB-0005pP-00@orion.orbis>
References: <E1A7dFB-0005pP-00@orion.orbis>
Message-ID: <1065716142.29805.4.camel@ra.chem.psu.edu>

On Thu, 2003-10-09 at 12:02, Mark Lee wrote:
> Sorry if this is a silly question. I'm trying to extract all elements
> of a dataframe at a particular row. I can find no mention of this in
> any documentation and it may be naivety of dataframe on my part as I'm
> very green at this. Thankyou,

If d is your data.frame then

d[ 1, ]

will give all the columns coresponding to row 1 in a data.frame


> d <- read.table('data')
> class(d)
[1] "data.frame"
> x <- d[1,]
> class(x)
[1] "data.frame"
>

HTH
-------------------------------------------------------------------
Rajarshi Guha <rxg218 at psu.edu> <http://jijo.cjb.net>
GPG Fingerprint: 0CCA 8EE2 2EEB 25E2 AB04 06F7 1BB9 E634 9B87 56EE
-------------------------------------------------------------------
A red sign on the door of a physics professor: 
'If this sign is blue, you're going too fast.'



From apjaworski at mmm.com  Thu Oct  9 18:16:12 2003
From: apjaworski at mmm.com (apjaworski@mmm.com)
Date: Thu, 9 Oct 2003 11:16:12 -0500
Subject: [R] upgrading R
Message-ID: <OF8C7C8BC6.714207C5-ON86256DBA.0058DF4C-86256DBA.00595FF6@mmm.com>


I am not sure if this is a proper way, but here is what I did recently
installing consecutive alpha and beta releases of 1.8.0 on a Win2000
machine:
(1) I uninstalled previous version using the uninstall provided with R.
This leaves all the additional packages in the library folder
(2) I reinstalled the new version into the same folder structure.

The problem might be that some 1.7.1 packages might be different from
1.8.0, so it might be safer to reinstall them from CRAN.

Andy

__________________________________
Andy Jaworski
Engineering Systems Technology Center
3M Center, 518-1-01
St. Paul, MN 55144-1000
-----
E-mail: apjaworski at mmm.com
Tel:  (651) 733-6092
Fax:  (651) 736-3122


|---------+-------------------------------->
|         |           "Weiming Zhang"      |
|         |           <Weiming.Zhang at uchsc.|
|         |           edu>                 |
|         |           Sent by:             |
|         |           r-help-bounces at stat.m|
|         |           ath.ethz.ch          |
|         |                                |
|         |                                |
|         |           10/09/2003 10:38     |
|         |                                |
|---------+-------------------------------->
  >-----------------------------------------------------------------------------------------------------------------------------|
  |                                                                                                                             |
  |      To:       r-help at stat.math.ethz.ch                                                                                     |
  |      cc:                                                                                                                    |
  |      Subject:  [R] upgrading R                                                                                              |
  >-----------------------------------------------------------------------------------------------------------------------------|




Hi,

I have installed a lot of extra packages for R 1.7.1. If I install R
1.8.0, will I have to reinstall all those packages? Is there a way that
I can upgrading R without losing old packages?

Thank you.

wz

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://www.stat.math.ethz.ch/mailman/listinfo/r-help



From ripley at stats.ox.ac.uk  Thu Oct  9 18:17:23 2003
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Thu, 9 Oct 2003 17:17:23 +0100 (BST)
Subject: [R] dyn.load error with C file
In-Reply-To: <200310091452.h99Eq9523125@rocky.mayo.edu>
Message-ID: <Pine.LNX.4.44.0310091714010.12863-100000@gannet.stats>

On Thu, 9 Oct 2003, Jason Sinnwell wrote:

> Re-sending this from 10/7.  Please help! I'm really clueless how to fix this:

Your code calls the C entry point errmsg, and that is not in the R binary.  
So find out what library it is in and link against it.  NB: it does not
show up on our Solaris 8 system at a quick look.

I think the error message is pretty explicit, so maybe you need to take 
local advice about your C code.

> 
> System: Solaris 7,     Software: R_1.7.1 for unix
> 
> > R CMD INSTALL -l /Rdir/library haplo.stats_1.1.0.tar.gz
> 
> -works like a charm-- no syntax errors, etc
> 
> 
> But problems are when I'm loading the library within R.
>  
> > library(haplo.stats, lib.loc="/people/biostat3/sinnwell/Rdir/library")
> > Error in dyn.load(x, as.logical(local), as.logical(now)) : 
> >         unable to load shared library 
> > "/people/biostat3/sinnwell/Rdir/library/haplo.stats/libs/haplo.stats.so":
> >   ld.so.1: /opt/tools/R/current-version/lib/R/bin/R.bin: fatal: relocation 
> > error: file 
> > /people/biostat3/sinnwell/Rdir/library/haplo.stats/libs/haplo.stats.so: symbol 
> > errmsg: referenced symbol not found
> > Error in library(haplo.stats, lib.loc = 
> > "/people/biostat3/sinnwell/Rdir/library") : 
> >         .First.lib failed
> 
> Within my package I have three C files, one of which I believe is causing the 
> dynamic library error below.  
> When I leave out this one C file when building the package, though remaining 
> R-functions depend on it, the library loads beautifully.  We use all Calloc and 
> Free for double-compatibility with S, so memory allocation is fine.  Is there 
> some issue with C and R that I'm not aware of?
> 
> 
> > FYI:  
> > We have the whole haplo.stats package working in S, INCLUDING that C-file
> > Also, zzz.R for haplo.stats package:
> > 
> > .First.lib <- function(lib, pkg) {
> >    library.dynam("haplo.stats", pkg, lib)
> > }
> 
> Has anyone dealt with anything similar to this?  I see some similar Fortran 
> error messages...
>  
> Thank you for your help,
>  
> Jason 
>  
> -------------------------
> Jason P. Sinnwell, M.S.
> Mayo Clinic, Rochester
> Health Sciences Research
> Division of Biostatistics
> 507.284.3270
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> 
> 

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From gavin.simpson at ucl.ac.uk  Thu Oct  9 18:28:09 2003
From: gavin.simpson at ucl.ac.uk (Gavin Simpson)
Date: Thu, 09 Oct 2003 17:28:09 +0100
Subject: [R] Getting rows from a dataframe
In-Reply-To: <E1A7dFB-0005pP-00@orion.orbis>
References: <E1A7dFB-0005pP-00@orion.orbis>
Message-ID: <3F858C99.2020604@ucl.ac.uk>

Mark Lee wrote:

> Sorry if this is a silly question. I'm trying to extract all elements
> of a dataframe at a particular row. I can find no mention of this in
> any documentation and it may be naivety of dataframe on my part as I'm
> very green at this. Thankyou,
> 
> Mark
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> 
> 

dat <- data.frame(var1 = c(1:10), var2 = c(31:40), var3 = c(6:15))

dat[1, ] #extract 1st row

dat[1:4, ] #extract rows 1 through 4

dat[c(1,3,5,7,9), ] #extract named non-consecutive rows

dat[ ,1] # extract 1st column

... etc

It's in 'An Introduction to R' that should have come with your 
distribution, or in the manuals section of the r website:

http://www.r-project.org

HTH
-- 
%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%
Gavin Simpson                     [T] +44 (0)20 7679 5522
ENSIS Research Fellow             [F] +44 (0)20 7679 7565
ENSIS Ltd. & ECRC                 [E] gavin.simpson at ucl.ac.uk
UCL Department of Geography       [W] http://www.ucl.ac.uk/~ucfagls/cv/
26 Bedford Way                    [W] http://www.ucl.ac.uk/~ucfagls/
London.  WC1H 0AP.
%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%



From uth at zhwin.ch  Thu Oct  9 18:29:06 2003
From: uth at zhwin.ch (=?iso-8859-1?Q?=22Untern=E4hrer_Thomas=2C_uth=22?=)
Date: Thu, 9 Oct 2003 18:29:06 +0200
Subject: AW: [R] Getting rows from a dataframe
Message-ID: <53A181E56FB0694ABFD212F8AEDA7F6F1AB8A9@langouste.zhwin.ch>


Df[x, ]

Read 'an introduction to R' and 'FAQ'



-----Urspr?ngliche Nachricht-----
Von: Mark Lee [mailto:mark.lee at orbisuk.com] 
Gesendet: Donnerstag, 9. Oktober 2003 18:02
An: r-help at stat.math.ethz.ch
Betreff: [R] Getting rows from a dataframe


Sorry if this is a silly question. I'm trying to extract all elements of a dataframe at a particular row. I can find no mention of this in any documentation and it may be naivety of dataframe on my part as I'm very green at this. Thankyou,

Mark

______________________________________________
R-help at stat.math.ethz.ch mailing list https://www.stat.math.ethz.ch/mailman/listinfo/r-help



From mark.lee at orbisuk.com  Thu Oct  9 18:40:11 2003
From: mark.lee at orbisuk.com (Mark Lee)
Date: Thu, 9 Oct 2003 17:40:11 +0100
Subject: AW: [R] Getting rows from a dataframe
In-Reply-To: <53A181E56FB0694ABFD212F8AEDA7F6F1AB8A9@langouste.zhwin.ch>
References: <53A181E56FB0694ABFD212F8AEDA7F6F1AB8A9@langouste.zhwin.ch>
Message-ID: <16261.36715.772720.770069@orion.orbis>

I have this right on the desk in front of me. I have gone through most
of this actually and have been looking for the answer for several
weeks now before resorting to this. The only reference I've found to
this is on page 20 under array indexing but didn't see the relation to
dataframes. Thanks,

Mark



From hack at pedos.hr  Thu Oct  9 18:58:23 2003
From: hack at pedos.hr (Branimir K. Hackenberger)
Date: Thu, 9 Oct 2003 18:58:23 +0200
Subject: [R] R-OpenOffice.org Calc
Message-ID: <!~!UENERkVCMDkAAQACAAAAAAAAAAAAAAAAABgAAAAAAAAA/KN/oP+2mk+FvuiZg1iX78KAAAAQAAAAs2DmnfQDVkOHsUMGA9X0VwEAAAAA@knjiga.pedos.hr>


I have been very satisfied with R-Excel interface (DCOM). Few months ago
I have changed my OS to Linux-Mandrake, and now I am using
OpenOffice.org Calc as spreadsheet. I would like to know does exist some
R-OpenOffice.org interface or how is possible to use R-functions in
OpenOffice.org Calc?

Thanks a lot!



From ligges at statistik.uni-dortmund.de  Thu Oct  9 19:08:16 2003
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Thu, 09 Oct 2003 19:08:16 +0200
Subject: [R] plotting graphs with Rterm
In-Reply-To: <OF54D7AB48.88741EB1-ON41256DBA.005A45F9@notes.edfgdf.fr>
References: <OF54D7AB48.88741EB1-ON41256DBA.005A45F9@notes.edfgdf.fr>
Message-ID: <3F859600.7090009@statistik.uni-dortmund.de>

Eric ESPOSITO wrote:

> Hello,
> I need to execute R code contained in  a file xxx.R from DOS.
> The file is really simple, only:
> 
>>plot(rnorm(100))

You have to specify the windows() device explicitly, if you are in 
non-interactive mode:

  windows()
  plot(rnorm(100))


> When I launch Rterm from Dos command and then source the file xxx.R it
> works, but I need to call the command from a DOS command file *.bat
> Using Rterm --slave < xxx.R plots the graph in a postscript file Rplots.ps,
> how can I do to get the graph in an R window.
> Using a command windows() or win.graph() in the R file does not work any
> more.
> 
> I have the same problem with tk widgets. If the file xxx.R is:
> 
>>library(tcltk)
>>tt<-tktoplevel()
> 
> The widget does not appear on the screen.


It appears! I guess your R process has finished right after those lines 
and the window has been shut before you have seen it.

Uwe Ligges


> Does anybody can help?
> Regards,
> 
> Eric



From Ted.Harding at nessie.mcc.ac.uk  Thu Oct  9 19:03:37 2003
From: Ted.Harding at nessie.mcc.ac.uk ( (Ted Harding))
Date: Thu, 09 Oct 2003 18:03:37 +0100 (BST)
Subject: [R] Automatic re-looping after error
Message-ID: <XFMail.031009180337.Ted.Harding@nessie.mcc.ac.uk>

Hi Folks,
I'm seeking advice about how to resume an outer loop following
failure of a function within the loop (which issues an error
message).

Essentially, I'm repeating (simulating) a process which involves
random sampling, EM and MCMC. I'm walking on very edge of rather
thin ice -- data rather thinly spread over many parameters!

Occasionally some component of the loop fails, with an error message.
At this point, R quits the run altogether and returns to the command
prompt. I then have to restart the whole script.

All that's really needed to cope with the situation is for R to
drop that cycle of the loop, and resume with a new cycle.

However, I'm wondering how to set this up. I've had a look at
try(), and I'm not at all sure that it does what I would want.
What I'd really like is something (inside the loop) on the lines of

   on.error(maybe some parameters X) break

where X might specify what sort of error or what function it comes
from. Would setting

  options(error = break )

do it?

Any other suggestions?

With thanks,
Ted.


--------------------------------------------------------------------
E-Mail: (Ted Harding) <Ted.Harding at nessie.mcc.ac.uk>
Fax-to-email: +44 (0)870 167 1972
Date: 09-Oct-03                                       Time: 18:03:37
------------------------------ XFMail ------------------------------



From tplate at blackmesacapital.com  Thu Oct  9 19:11:22 2003
From: tplate at blackmesacapital.com (Tony Plate)
Date: Thu, 09 Oct 2003 11:11:22 -0600
Subject: AW: [R] Getting rows from a dataframe
In-Reply-To: <16261.36715.772720.770069@orion.orbis>
References: <53A181E56FB0694ABFD212F8AEDA7F6F1AB8A9@langouste.zhwin.ch>
	<53A181E56FB0694ABFD212F8AEDA7F6F1AB8A9@langouste.zhwin.ch>
Message-ID: <5.2.1.1.2.20031009110437.04231d08@mailhost.blackmesacapital.com>

If you're having so much trouble, perhaps it's because you want to get a 
vector result?  This requires a little more, and if so, perhaps one of the 
following provides what you are looking for:

 > x <- data.frame(a=1:3,b=4:6)
 > # row as a data frame
 > x[2,]
   a b
2 2 5
 > # row as a list
 > x[2,,drop=T]
$a
[1] 2

$b
[1] 5

 > # row as a vector
 > unlist(x[2,,drop=T])
a b
2 5
 > # row as a vector again
 > unlist(x[2,])
a b
2 5
 > # row as a matrix (if x contains any non-numeric columns, this will be a 
character matrix)
 > as.matrix(x[2,])
   a b
2 2 5
 > # row as a vector (if x contains any non-numeric columns, this will be a 
character vector)
 > as.matrix(x)[2,]
a b
2 5
 > # row as a numeric vector (non-numeric columns in x will be converted to 
numeric data, see ?data.matrix for how)
 > data.matrix(x[2,])
   a b
2 2 5
 >

Tony Plate

At Thursday 05:40 PM 10/9/2003 +0100, Mark Lee wrote:
>I have this right on the desk in front of me. I have gone through most
>of this actually and have been looking for the answer for several
>weeks now before resorting to this. The only reference I've found to
>this is on page 20 under array indexing but didn't see the relation to
>dataframes. Thanks,
>
>Mark
>
>______________________________________________
>R-help at stat.math.ethz.ch mailing list
>https://www.stat.math.ethz.ch/mailman/listinfo/r-help



From paulda at BATTELLE.ORG  Thu Oct  9 19:16:12 2003
From: paulda at BATTELLE.ORG (Paul, David  A)
Date: Thu, 09 Oct 2003 13:16:12 -0400
Subject: [R] Getting rows from a dataframe
Message-ID: <940250A9EB37A24CBE28D858EF07774967ABAA@ws-bco-mse3.milky-way.battelle.org>

Suppose foo.frame is your dataframe.  Use

foo.frame[j,]

to extract all of the elements in the jth row
of foo.frame.  Also, you should get two excellent
books that will help your learning process:

"The Basics of S and Splus", by Krause and Olsen,
Third Edition, Springer-Verlag

"Modern Applied Statistics with S", by Venables 
and Ripley, Fourth Edition, Springer-Verlag

Krause & Olsen's book is the more elementary
of the two.  Venables and Ripley will get you
to the point where you can productively use 
R/Splus for statistical analyses. Hope this helps.


Best,
  david paul


-----Original Message-----
From: Mark Lee [mailto:mark.lee at orbisuk.com] 
Sent: Thursday, October 09, 2003 12:02 PM
To: r-help at stat.math.ethz.ch
Subject: [R] Getting rows from a dataframe


Sorry if this is a silly question. I'm trying to extract all elements of a
dataframe at a particular row. I can find no mention of this in any
documentation and it may be naivety of dataframe on my part as I'm very
green at this. Thankyou,

Mark

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://www.stat.math.ethz.ch/mailman/listinfo/r-help



From spencer.graves at pdf.com  Thu Oct  9 20:01:38 2003
From: spencer.graves at pdf.com (Spencer Graves)
Date: Thu, 09 Oct 2003 11:01:38 -0700
Subject: [R] Automatic re-looping after error
In-Reply-To: <XFMail.031009180337.Ted.Harding@nessie.mcc.ac.uk>
References: <XFMail.031009180337.Ted.Harding@nessie.mcc.ac.uk>
Message-ID: <3F85A282.9050101@pdf.com>

Have you considered "try"? 

hope this helps.  spencer graves

(Ted Harding) wrote:

>Hi Folks,
>I'm seeking advice about how to resume an outer loop following
>failure of a function within the loop (which issues an error
>message).
>
>Essentially, I'm repeating (simulating) a process which involves
>random sampling, EM and MCMC. I'm walking on very edge of rather
>thin ice -- data rather thinly spread over many parameters!
>
>Occasionally some component of the loop fails, with an error message.
>At this point, R quits the run altogether and returns to the command
>prompt. I then have to restart the whole script.
>
>All that's really needed to cope with the situation is for R to
>drop that cycle of the loop, and resume with a new cycle.
>
>However, I'm wondering how to set this up. I've had a look at
>try(), and I'm not at all sure that it does what I would want.
>What I'd really like is something (inside the loop) on the lines of
>
>   on.error(maybe some parameters X) break
>
>where X might specify what sort of error or what function it comes
>from. Would setting
>
>  options(error = break )
>
>do it?
>
>Any other suggestions?
>
>With thanks,
>Ted.
>
>
>--------------------------------------------------------------------
>E-Mail: (Ted Harding) <Ted.Harding at nessie.mcc.ac.uk>
>Fax-to-email: +44 (0)870 167 1972
>Date: 09-Oct-03                                       Time: 18:03:37
>------------------------------ XFMail ------------------------------
>
>______________________________________________
>R-help at stat.math.ethz.ch mailing list
>https://www.stat.math.ethz.ch/mailman/listinfo/r-help
>  
>



From feh3k at spamcop.net  Thu Oct  9 20:02:37 2003
From: feh3k at spamcop.net (Frank E Harrell Jr)
Date: Thu, 9 Oct 2003 14:02:37 -0400
Subject: [R] New Department of Biostatistics, Multiple Job Openings
Message-ID: <20031009140237.36a56c85.feh3k@spamcop.net>

NEW DEPARTMENT OF BIOSTATISTICS
	
The School of Medicine at Vanderbilt University is pleased to announce
the creation of a new Department of Biostatistics.  Vanderbilt has
made a major new funding commitment to build a world-class department.
Chaired by Frank E. Harrell, Jr., the new department has exceptional
institutional support from a medical center that is currently 16th in
U.S. News and World Report's 2002 rankings of research-oriented
medical schools, and has seen a sharp rise in NIH and other grant
funding.  The Vanderbilt School of Medicine has a strong basic,
clinical, and health services research program.  The Dean and other
senior medical school faculty are committed to providing outstanding
collaborative support in biostatistics to clinical and basic
scientists and to developing a graduate program in biostatistics that
will train outstanding collaborative scientists and will focus on the
methods of modern applied statistics.

The department currently has 9 PhD faculty members and plans to double
in size over the next two years, continuing to add a significant
number of faculty and staff biostatisticians after that.  Besides a
graduate program, a statistical data coordinating center will be
created.  We are excited about the future and seek outstanding
biostatisticians who will help us to bring our goals to fruition.

POSITIONS AVAILABLE AT ALL LEVELS

There are openings at all levels: full professor, associate professor,
assistant professor, senior and junior M.S. biostatistician,
statistical programmers knowledgeable about tools such as R, S-Plus, and
LaTeX, and data managers.  We are especially seeking individuals with
expertise in clinical trials, health services research, Bayesian
methods, statistical methods in epidemiology, multicenter clinical
trial data management, clinical safety assessment for pharmaceutical
trials, and modern statistical computing.  Applicants for PhD or MS
biostatistician positions must have degrees in Biostatistics or have
graduate degrees in Statistics with significant experience in
clinical research or health services/outcomes research. 

Vanderbilt University is an independent, privately supported,
university with an enrollment of 10,800 undergraduate, graduate, and
professional students.  The medical center is located within the main
campus on 330 acres near downtown Nashville, Tennessee.  Faculty and
staff at the university receive excellent fringe benefits, including
health, dental, life, and disability insurance, a tax deferred
contributory retirement plan, and generous tuition assistance for
dependents.

Nashville is a pleasant metropolitan area of approximately 1.2 million
residents.  A wide variety of cultural activities are available,
especially for music lovers.  Nashville is famous as the home of
country and western music.  In addition, as a recording center for the
music industry, the city draws many excellent rock, jazz, and
classical artists.  Sports fans will appreciate the Tennessee Titans
(football), Nashville Predators (ice hockey), Nashville Sounds (AAA
baseball), and numerous intercollegiate sporting events.  There are
many parks in the area, including the largest in-city park in the
United States.  Nashville also provides easy access to recreational
areas for hiking, bicycling, water sports, and other outdoor
activities.

All candidates must enjoy collaborative research and possess excellent
written and verbal communication skills.  Send CV to
biostat at vanderbilt.edu or to Search Committee, Biostatistics, S-2323
MCN, Vanderbilt University, Nashville, TN 37232-2158. Vanderbilt
University is an equal opportunity/affirmative action employer; all
qualified persons are encouraged to apply.



From med at aghmed.fsnet.co.uk  Thu Oct  9 20:04:34 2003
From: med at aghmed.fsnet.co.uk (Michael Dewey)
Date: Thu, 09 Oct 2003 19:04:34 +0100
Subject: [R] Specifying suitable PC to run R
Message-ID: <5.2.1.1.0.20031009185506.009f8920@pop.freeserve.net>

If I am buying a PC where the most compute intensive task will be running R 
and I do not have unlimited resources what trade-offs should I make?
Specifically should I go for
1 - more memory, or
2 - faster processor, or
3 - something else?
If it makes a difference I shall be running Windows on it and I am thinking 
about getting a portable which I understand makes upgrading more difficult.

Extra background: the tasks I notice going slowly at the moment are fitting 
models with lme which have complex random effects and bootstrapping. By the 
standards of r-help posters I have small datasets (few thousand cases, few 
hundred variables). In order to facilitate working with colleagues I need 
to stick with windows even if linux would be more efficient


Michael Dewey
med at aghmed.fsnet.co.uk
http://www.aghmed.fsnet.co.uk/home.html



From zeileis at ci.tuwien.ac.at  Thu Oct  9 20:09:14 2003
From: zeileis at ci.tuwien.ac.at (Achim Zeileis)
Date: Thu, 9 Oct 2003 20:09:14 +0200
Subject: [R] Automatic re-looping after error
In-Reply-To: <XFMail.031009180337.Ted.Harding@nessie.mcc.ac.uk>
References: <XFMail.031009180337.Ted.Harding@nessie.mcc.ac.uk>
Message-ID: <200310091809.h99I9EPZ019548@thorin.ci.tuwien.ac.at>

On Thursday 09 October 2003 19:03, Ted Harding wrote:

> Hi Folks,
> I'm seeking advice about how to resume an outer loop following
> failure of a function within the loop (which issues an error
> message).
>
> Essentially, I'm repeating (simulating) a process which involves
> random sampling, EM and MCMC. I'm walking on very edge of rather
> thin ice -- data rather thinly spread over many parameters!
>
> Occasionally some component of the loop fails, with an error
> message. At this point, R quits the run altogether and returns to
> the command prompt. I then have to restart the whole script.
>
> All that's really needed to cope with the situation is for R to
> drop that cycle of the loop, and resume with a new cycle.
>
> However, I'm wondering how to set this up. I've had a look at
> try(), and I'm not at all sure that it does what I would want.
> What I'd really like is something (inside the loop) on the lines of
>
>    on.error(maybe some parameters X) break
>
> where X might specify what sort of error or what function it comes
> from. Would setting
>
>   options(error = break )
>
> do it?
>
> Any other suggestions?

Look at try().

hth,
Z

> With thanks,
> Ted.
>
>
> --------------------------------------------------------------------
> E-Mail: (Ted Harding) <Ted.Harding at nessie.mcc.ac.uk>
> Fax-to-email: +44 (0)870 167 1972
> Date: 09-Oct-03                                       Time: 18:03:37
> ------------------------------ XFMail ------------------------------
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help



From zeileis at ci.tuwien.ac.at  Thu Oct  9 20:11:28 2003
From: zeileis at ci.tuwien.ac.at (Achim Zeileis)
Date: Thu, 9 Oct 2003 20:11:28 +0200
Subject: [R] Automatic re-looping after error
In-Reply-To: <XFMail.031009180337.Ted.Harding@nessie.mcc.ac.uk>
References: <XFMail.031009180337.Ted.Harding@nessie.mcc.ac.uk>
Message-ID: <200310091811.h99IBSmF019580@thorin.ci.tuwien.ac.at>

On Thursday 09 October 2003 19:03, Ted Harding wrote:

> Hi Folks,
> I'm seeking advice about how to resume an outer loop following
> failure of a function within the loop (which issues an error
> message).
>
> Essentially, I'm repeating (simulating) a process which involves
> random sampling, EM and MCMC. I'm walking on very edge of rather
> thin ice -- data rather thinly spread over many parameters!
>
> Occasionally some component of the loop fails, with an error
> message. At this point, R quits the run altogether and returns to
> the command prompt. I then have to restart the whole script.
>
> All that's really needed to cope with the situation is for R to
> drop that cycle of the loop, and resume with a new cycle.
>
> However, I'm wondering how to set this up. I've had a look at
> try(), and I'm not at all sure that it does what I would want.
> What I'd really like is something (inside the loop) on the lines of
>
>    on.error(maybe some parameters X) break
>
> where X might specify what sort of error or what function it comes
> from. Would setting
>
>   options(error = break )
>
> do it?

Argh, didn't read carefully enough, sorry. Usually, I would expect 
from your description that try() would be good enough. You can "try()" 
the critical functions inside the loop and modify the actions inside 
the loop accordingly if something goes wrong...
Z

> Any other suggestions?
>
> With thanks,
> Ted.
>
>
> --------------------------------------------------------------------
> E-Mail: (Ted Harding) <Ted.Harding at nessie.mcc.ac.uk>
> Fax-to-email: +44 (0)870 167 1972
> Date: 09-Oct-03                                       Time: 18:03:37
> ------------------------------ XFMail ------------------------------
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help



From rpeng at jhsph.edu  Thu Oct  9 20:22:37 2003
From: rpeng at jhsph.edu (Roger D. Peng)
Date: Thu, 09 Oct 2003 14:22:37 -0400
Subject: [R] Specifying suitable PC to run R
In-Reply-To: <5.2.1.1.0.20031009185506.009f8920@pop.freeserve.net>
References: <5.2.1.1.0.20031009185506.009f8920@pop.freeserve.net>
Message-ID: <3F85A76D.2080807@jhsph.edu>

On Windows, I find that having as much memory as I can possibly afford 
makes a real difference with R.  Since I always end up having larger 
datasets/problems then I thought I'd have.  My general strategy is to 
maximize the amount of memory first -- if that doesn't work, then think 
about getting a faster processor.

-roger

Michael Dewey wrote:

> If I am buying a PC where the most compute intensive task will be 
> running R and I do not have unlimited resources what trade-offs should 
> I make?
> Specifically should I go for
> 1 - more memory, or
> 2 - faster processor, or
> 3 - something else?
> If it makes a difference I shall be running Windows on it and I am 
> thinking about getting a portable which I understand makes upgrading 
> more difficult.
>
> Extra background: the tasks I notice going slowly at the moment are 
> fitting models with lme which have complex random effects and 
> bootstrapping. By the standards of r-help posters I have small 
> datasets (few thousand cases, few hundred variables). In order to 
> facilitate working with colleagues I need to stick with windows even 
> if linux would be more efficient
>
>
> Michael Dewey
> med at aghmed.fsnet.co.uk
> http://www.aghmed.fsnet.co.uk/home.html
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
>



From nathanwands at hotmail.com  Thu Oct  9 20:31:59 2003
From: nathanwands at hotmail.com (Nathan Cooper)
Date: Thu, 09 Oct 2003 14:31:59 -0400
Subject: [R] Indexing Question
Message-ID: <Law11-F45lhRyZx4L2800019c21@hotmail.com>

I have an indexing question. I have a data set that looks like this:

>b
[[1]]
[1] 22 23 24 25 26

[[2]]
[1] 6 7 8 9 NA

etc. from [[1]] to [[1000]]

Then I need to use the sample function to take two samples from b[[1]] to 
b[[1000]] each separately. I thought something like 
"sample(na.omit(b[[1:1000]]),2,replace=TRUE)" would work but it doesn't. Is 
there a way to index this properly?? Thanks,

Nathan

_________________________________________________________________
Add MSN 8 Internet Software to your existing Internet access and enjoy 
patented spam protection and more.  Sign up now!



From B.Rowlingson at lancaster.ac.uk  Thu Oct  9 20:37:14 2003
From: B.Rowlingson at lancaster.ac.uk (Barry Rowlingson)
Date: Thu, 09 Oct 2003 19:37:14 +0100
Subject: [R] R-OpenOffice.org Calc
In-Reply-To: <!~!UENERkVCMDkAAQACAAAAAAAAAAAAAAAAABgAAAAAAAAA/KN/oP+2mk+FvuiZg1iX78KAAAAQAAAAs2DmnfQDVkOHsUMGA9X0VwEAAAAA@knjiga.pedos.hr>
References: <!~!UENERkVCMDkAAQACAAAAAAAAAAAAAAAAABgAAAAAAAAA/KN/oP+2mk+FvuiZg1iX78KAAAAQAAAAs2DmnfQDVkOHsUMGA9X0VwEAAAAA@knjiga.pedos.hr>
Message-ID: <3F85AADA.1020508@lancaster.ac.uk>

Branimir K. Hackenberger wrote:
> I have been very satisfied with R-Excel interface (DCOM). Few months ago
> I have changed my OS to Linux-Mandrake, and now I am using
> OpenOffice.org Calc as spreadsheet. I would like to know does exist some
> R-OpenOffice.org interface or how is possible to use R-functions in
> OpenOffice.org Calc?

  I think all you need to do is download the SDK:

http://www.openoffice.org/dev_docs/source/sdk/index.html

  then read and understand both the 900 page OpenOffice SDK manual, and 
everything there is to know about R internals. Then write an extension 
for OpenOffice Calc that uses UNO to communicate between Calc and the R 
library. You'll probably have to use C++.

  The spreadsheet section of the SDK doc starts at page 535, by the way...

  Or is there a quicker way?

  Baz



From tlumley at u.washington.edu  Thu Oct  9 20:51:18 2003
From: tlumley at u.washington.edu (Thomas Lumley)
Date: Thu, 9 Oct 2003 11:51:18 -0700 (PDT)
Subject: [R] Automatic re-looping after error
In-Reply-To: <XFMail.031009180337.Ted.Harding@nessie.mcc.ac.uk>
References: <XFMail.031009180337.Ted.Harding@nessie.mcc.ac.uk>
Message-ID: <Pine.A41.4.58.0310091150270.42270@homer40.u.washington.edu>

On Thu, 9 Oct 2003 Ted.Harding at nessie.mcc.ac.uk wrote:

> All that's really needed to cope with the situation is for R to
> drop that cycle of the loop, and resume with a new cycle.
>
> However, I'm wondering how to set this up. I've had a look at
> try(), and I'm not at all sure that it does what I would want.
> What I'd really like is something (inside the loop) on the lines of
>
>    on.error(maybe some parameters X) break
>
> where X might specify what sort of error or what function it comes
> from. Would setting
>
>   options(error = break )
>

I don't think so.  You may need to look at the new exception-handling code
(start with help(tryCatch)).

	-thomas



From p.dalgaard at biostat.ku.dk  Thu Oct  9 21:10:19 2003
From: p.dalgaard at biostat.ku.dk (Peter Dalgaard BSA)
Date: Thu, 09 Oct 2003 19:10:19 -0000
Subject: [R] Indexing Question
In-Reply-To: <Law11-F45lhRyZx4L2800019c21@hotmail.com>
References: <Law11-F45lhRyZx4L2800019c21@hotmail.com>
Message-ID: <x2u16itebm.fsf@biostat.ku.dk>

"Nathan Cooper" <nathanwands at hotmail.com> writes:

> I have an indexing question. I have a data set that looks like this:
> 
> >b
> [[1]]
> [1] 22 23 24 25 26
> 
> [[2]]
> [1] 6 7 8 9 NA
> 
> etc. from [[1]] to [[1000]]
> 
> Then I need to use the sample function to take two samples from b[[1]]
> to b[[1000]] each separately. I thought something like
> "sample(na.omit(b[[1:1000]]),2,replace=TRUE)" would work but it
> doesn't. Is there a way to index this properly?? Thanks,

Not really.

I'd try

lapply(lapply(b,na.omit),sample,2,replace=TRUE)

or

lapply(b,function(x)sample(na.omit(x),2,replace=TRUE))

-- 
   O__  ---- Peter Dalgaard             Blegdamsvej 3  
  c/ /'_ --- Dept. of Biostatistics     2200 Cph. N   
 (*) \(*) -- University of Copenhagen   Denmark      Ph: (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)             FAX: (+45) 35327907



From rpeng at jhsph.edu  Thu Oct  9 21:12:29 2003
From: rpeng at jhsph.edu (Roger D. Peng)
Date: Thu, 09 Oct 2003 15:12:29 -0400
Subject: [R] Indexing Question
In-Reply-To: <Law11-F45lhRyZx4L2800019c21@hotmail.com>
References: <Law11-F45lhRyZx4L2800019c21@hotmail.com>
Message-ID: <3F85B31D.3050604@jhsph.edu>

I think what you want is

lapply(b, function(x) sample(na.omit(x), 2, replace = TRUE))

-roger

Nathan Cooper wrote:

> I have an indexing question. I have a data set that looks like this:
>
>> b
>
> [[1]]
> [1] 22 23 24 25 26
>
> [[2]]
> [1] 6 7 8 9 NA
>
> etc. from [[1]] to [[1000]]
>
> Then I need to use the sample function to take two samples from b[[1]] 
> to b[[1000]] each separately. I thought something like 
> "sample(na.omit(b[[1:1000]]),2,replace=TRUE)" would work but it 
> doesn't. Is there a way to index this properly?? Thanks,
>
> Nathan
>
> _________________________________________________________________
> Add MSN 8 Internet Software to your existing Internet access and enjoy 
> patented spam protection and more.  Sign up now!
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
>



From james.holtman at convergys.com  Thu Oct  9 21:18:48 2003
From: james.holtman at convergys.com (james.holtman@convergys.com)
Date: Thu, 9 Oct 2003 15:18:48 -0400
Subject: [R] Specifying suitable PC to run R
Message-ID: <OF9212499B.C5B3F6CB-ON85256DBA.00693AF0@convergys.com>


If you are running Windows, do you have the Performance Monitor running?
This will help identify the reasons that programs are running slow.  Most
likely, you are low on memory and are paging a lot.  I alway have it
running and when I am running a large R script, if I am not using 100% of
the CPU, then I must be paging (assuming that I am not reading in my data).
You can also sprinkle the following function throughout your code to see
how much CPU and memory you are using.  I bracket all my major
computational sections with it:

 my.stats <-  function(text = "stats")
{
    cat(text, "-",sys.call(sys.parent())[[1]], ":",  proc.time()[1:3], " :
", round(
        memory.size()/2.^20., 1.), "MB\n")
    invisible(flush.console())
}

This prints out a message like:

> my.stats('Begin Reading')
Begin Reading - my.stats : 5.61 3.77 22309.67  :  18.7 MB

This says that I have used 5.61 CPU seconds of 'user' time, 3.77 CPU
seconds of 'system' time and the R session has been running for 22309
seconds (I always have one waiting for simple calculation) and I have
18.7MB of memory allocated to objects.

My first choice is get as much memory on your machine as you can; 1GB since
this the most that R can use.  I noticed a big difference in upgrading from
256M -> 512M.  I also watch the Performance Monitor and when memory gets
low and I want to run a large job, I restart R.  Most of my scripts are
setup to run R without saving any data in the .Rdata file.  If I need to
save a large object, I do it explicitly since memory is key performance
limiting factor and Windows is not that good at freeing up memory after you
have used a lot of it.

A faster CPU will also help, but it would be the second choice, since if
you are paging, most of your time is spent on data transfer and not
computation.
__________________________________________________________
James Holtman        "What is the problem you are trying to solve?"
Executive Consultant  --  Office of Technology, Convergys
james.holtman at convergys.com
(513) 723-2929


                                                                                                                   
                      Michael Dewey                                                                                
                      <med at aghmed.fsnet.co.        To:       r-help at stat.math.ethz.ch                              
                      uk>                          cc:                                                             
                      Sent by:                     Subject:  [R] Specifying suitable PC to run R                   
                      r-help-bounces at stat.m                                                                        
                      ath.ethz.ch                                                                                  
                                                                                                                   
                                                                                                                   
                      10/09/2003 14:04                                                                             
                                                                                                                   
                                                                                                                   




If I am buying a PC where the most compute intensive task will be running R

and I do not have unlimited resources what trade-offs should I make?
Specifically should I go for
1 - more memory, or
2 - faster processor, or
3 - something else?
If it makes a difference I shall be running Windows on it and I am thinking

about getting a portable which I understand makes upgrading more difficult.

Extra background: the tasks I notice going slowly at the moment are fitting

models with lme which have complex random effects and bootstrapping. By the

standards of r-help posters I have small datasets (few thousand cases, few
hundred variables). In order to facilitate working with colleagues I need
to stick with windows even if linux would be more efficient


Michael Dewey
med at aghmed.fsnet.co.uk
http://www.aghmed.fsnet.co.uk/home.html

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://www.stat.math.ethz.ch/mailman/listinfo/r-help




--
"NOTICE:  The information contained in this electronic mail ...{{dropped}}



From deepayan at stat.wisc.edu  Thu Oct  9 21:32:01 2003
From: deepayan at stat.wisc.edu (Deepayan Sarkar)
Date: Thu, 9 Oct 2003 14:32:01 -0500
Subject: [R] lattice/levelplot: panels with values can be empty
In-Reply-To: <20031009104025.GA3004@s1x.local>
References: <20031009104025.GA3004@s1x.local>
Message-ID: <200310091432.01764.deepayan@stat.wisc.edu>

On Thursday 09 October 2003 05:40, Wolfram Fischer wrote:
> I tried:
> 	library(lattice)
>
> 	F0  <- c( 'A', 'A', 'B', 'B' )
> 	F1  <- c(  1 ,  1 ,  1 ,  2 )
> 	F2  <- c(  8 ,  9 ,  8 ,  9 )
> 	VAL <- c(  20,  50,  10,  60 )
> 	df <- data.frame( F0, F1, F2, VAL )
>
> 	levelplot( VAL ~ F1 * F2 | F0, data=df )
>
> I got an empty field for F0 == 'A'
> and a colored field for F0 == 'B'.
> I expected two colored fields. - What can I do?

Hmm, this seems to be a bug triggered when some panel has only one unique x 
(or y) value. Can't think of a workaround, other than adding some dummy rows 
with VAL = NA, e.g.

F0  <- c( 'A', 'A', 'B', 'B', 'A')
F1  <- c(  1 ,  1 ,  1 ,  2 ,  2 )
F2  <- c(  8 ,  9 ,  8 ,  9 ,  8 )
VAL <- c(  20,  50,  10,  60, NA )
df <- data.frame( F0, F1, F2, VAL )

Deepayan



From jeaneid at chass.utoronto.ca  Thu Oct  9 22:06:12 2003
From: jeaneid at chass.utoronto.ca (Jean Eid)
Date: Thu, 9 Oct 2003 16:06:12 -0400
Subject: [R] subsetting objects
Message-ID: <001f01c38ea0$c96a8a20$ccb16480@Scorpion>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20031009/589aaa90/attachment.pl

From andrewr at uidaho.edu  Thu Oct  9 16:17:41 2003
From: andrewr at uidaho.edu (Andrew Robinson)
Date: Thu, 9 Oct 2003 07:17:41 -0700
Subject: [R] run R under unix
In-Reply-To: <200310091007.h99A7Hq5020434@stat.math.ethz.ch>
References: <200310091007.h99A7Hq5020434@stat.math.ethz.ch>
Message-ID: <200310090717.41201.andrewr@uidaho.edu>

Your Unix computer may also have "screen" installed.  This is a multiplexing 
terminal that allows you to place tasks in the background, log out of them, 
log back into them on different machines, maintain multiple command prompts, 
etc., all with minimal overhead - ideal for modems.

E.g. the following steps:

login
screen
R
source("mycode.R")
<start simulation>
Ctrl-a d
(screen is now detached; process is running in background)
exit

Then on a different machine

login
screen -r
(process is now running in foreground)
Ctrl-a d
(screen is now detached; process is running in background)
exit

You can create new screens, label them, obtain a menu, and use it to juggle 
numerous tasks - such as editing a source file and executing it efficiently.

Refinements: use source("mycode.R", echo=T) for information about the status 
of your simulation.  You can also easily write snippets of code to have the 
process email you when it's done, and run them via the "system" command. 
Locate "cat" commands judiciously through your code to update you on its 
progress.  

Andrew
-- 
Andrew Robinson                      Ph: 208 885 7115
Department of Forest Resources       Fa: 208 885 6226
University of Idaho                  E : andrewr at uidaho.edu
PO Box 441133                        W : http://www.uidaho.edu/~andrewr
Moscow ID 83843                      Or: http://www.biometrics.uidaho.edu
No statement above necessarily represents my employer's opinion.



From maj at stats.waikato.ac.nz  Thu Oct  9 22:34:43 2003
From: maj at stats.waikato.ac.nz (Murray Jorgensen)
Date: Fri, 10 Oct 2003 09:34:43 +1300
Subject: [R] detecting singular matrices
Message-ID: <3F85C663.3050703@stats.waikato.ac.nz>

My colleague runs R 1.7.1 under Windows XP. He remarks:

>> A
>     [,1] [,2] [,3]
> [1,]    1    2    3
> [2,]    4    5    6
> [3,]    7    8    9
>>
>> b
> [1] 1 2 3
>> solve(A,b)
> [1] -0.3333333  0.6666667  0.0000000
>> solve(A)
>            [,1]         [,2]        [,3]
> [1,] -4.5036e+15  9.00720e+15 -4.5036e+15
> [2,]  9.0072e+15 -1.80144e+16  9.0072e+15
> [3,] -4.5036e+15  9.00720e+15 -4.5036e+15
>> eigen(A)
> $values
> [1]  1.611684e+01 -1.116844e+00 -1.303678e-15
> 
> with a condition number of  1.236260e+16 I think I'd like it to tell me it's singular like it used to :) and I know it is singular 

Under 1.6.0 also on XP I get

 > solve(A,1:3)
Error in solve.default(A, 1:3) : singular matrix `a' in solve
 > solve(A)
Error in solve.default(A) : singular matrix `a' in solve
 > eigen(A)
$values
[1]  1.611684e+01 -1.116844e+00 -1.120190e-16

$vectors
            [,1]        [,2]       [,3]
[1,] -0.2719964 -0.76169140  0.3734378
[2,] -0.6159646 -0.08408654 -0.7468756
[3,] -0.9599328  0.59351831  0.3734378


Which seems to be preferable output.

Murray

-- 
Dr Murray Jorgensen      http://www.stats.waikato.ac.nz/Staff/maj.html
Department of Statistics, University of Waikato, Hamilton, New Zealand
Email: maj at waikato.ac.nz                                Fax 7 838 4155
Phone  +64 7 838 4773 wk    +64 7 849 6486 home    Mobile 021 1395 862



From PAlspach at hortresearch.co.nz  Thu Oct  9 22:53:33 2003
From: PAlspach at hortresearch.co.nz (Peter Alspach)
Date: Fri, 10 Oct 2003 09:53:33 +1300
Subject: [R] Scoping Rules: Summary
Message-ID: <sf8681c2.039@hrp3.palm.cri.nz>

Thanks to Andy Liaw, Roger Peng, Thomas Lumley, Brian Ripley and Peter
Dalgaard, all of whom addressed my questions or threads arising from
them.  The full messages were posted to the list so this is a brief
summary:

Andy Liaw explained the difference between lexical and dynamic scoping
and the rationale behind the choice of lexical scoping for R.  Roger
Peng showed how to modify fnB.  Brian Ripley suggested that it is
generally better to pass functions an object rather than just the name,
and warned of the dangers of using get() on the result of
deparse(substitute()).

Thanks all

Peter Alspach


Original question below:

I'm using R1.7.1 (Windows 2000) and having difficulty with scoping. 
I've studied the FAQ and on-line manuals and think I have identified
the
source of my difficulty, but cannot work out the solution.

For the purposes of illustration.  I have three functions as defined
below:

fnA <- function(my.x)
{
  list(first=as.character(substitute(my.x)), second=sqrt(my.x))
}

fnB <- function(AA)
{
  tmp.x <- get(AA$first)
  tmp.x^2
}

fnC <- function()
{
  x <- 1:2
  y <- fnA(x)
  z <- fnB(y)
  c(x,y,z)
}

fnA() has a vector as an argument and returns the name of the vector
and the square root of its elements in a list.  fn(B) takes the result
of fn(A) as its argument, gets the appropriate vector and computes the
square of its elements.  These work fine when called at the command
line.

fnC() defines a local vector x and calls fnA() which operates on this
vector.  Then fnB() is called, but it operates on a global vector x in
GlobalEnv (or returns an error is x doesn't exist there) - but I want
it
to operate on the local vector.

I think this is related to the enclosing environment of all three
functions being GlobalEnv (since they were created at the command
line),
but the parent environment of fnB() being different when invoked from
within fnC().

My questions:

1  Have I correctly understood the issue ?
2  How do I make fnB() operate on the local vector rather than the
global one ?
3  And, as an aside, I have used as.character(substitute(my.x)) to
pass
the name - but deparse(substitute(my.x)) also works.  Is there any
reason to prefer one over the other?

Thank you ...........



______________________________________________________
The contents of this e-mail are privileged and/or confidenti...{{dropped}}



From jasont at indigoindustrial.co.nz  Thu Oct  9 23:04:50 2003
From: jasont at indigoindustrial.co.nz (Jason Turner)
Date: Fri, 10 Oct 2003 10:04:50 +1300
Subject: [R] Fitting AR(p) models
In-Reply-To: <Pine.LNX.4.44.0310091654430.8728-100000@env-pc-phd13>
References: <Pine.LNX.4.44.0310091654430.8728-100000@env-pc-phd13>
Message-ID: <3F85CD72.1050206@indigoindustrial.co.nz>

Laura Quinn wrote:
> I am wanting to fit AR(p) models to column data for a series of matrices
> I have. Rather than trying to compute the AR models "per column" I was
> hoping to be able to do this per matrix (i.e. one AR model for each column
> in said matrix). However, when i attempt this, if there is just one "NA"
> value in any of the columns, the program refuses to compute AR models for
> any of the columns.(I am using na.action=na.omit)
> 
> Is there a way I can force the calculation to provide AR models for
> complete columns, or alternatively, a better way to deal with NA's to
> provide an AR model, (eg via interpolation?) despite some missing values?

Interpolation might not make sense for your data.  Can't tell from here. 
  Certainly it's quick, but it can also be pretty dirty - it 
over-weights those data regions.  You can get around this by 
interpolating, then selecting a lower resolution (window with a larger 
deltat), and build your model from that.

If the NAs are few and far between, some additional code to pick where 
the NAs are in each column, and build AR models from the longest 
continuous segment might be preferable.

Apart from that, there's not much else I can add.  Other takers?

Jason
-- 
Indigo Industrial Controls Ltd.
http://www.indigoindustrial.co.nz
64-21-343-545
jasont at indigoindustrial.co.nz



From ggrothendieck at myway.com  Thu Oct  9 23:08:51 2003
From: ggrothendieck at myway.com (Gabor Grothendieck)
Date: Thu,  9 Oct 2003 17:08:51 -0400 (EDT)
Subject: [R] subsetting objects
Message-ID: <20031009210851.6C49939E2@xmxpita.myway.com>



test <- data.frame(ID=c(1,1,1,2,2,2),data=1:6)
do.call("rbind", by(test,test$ID,function(x)x[2,]) )

 --- On Thu 10/09, Jean Eid < jeaneid at chass.utoronto.ca > wrote:
From: Jean Eid [mailto: jeaneid at chass.utoronto.ca]
To: r-help at stat.math.ethz.ch
Date: Thu, 9 Oct 2003 16:06:12 -0400
Subject: [R] subsetting objects

I want to be able to exctract a matrix from a data frame that contains repeated measurements for individuals. i.e I want to exctract the second observation for each individual in the study. <br>Is there a way of doing this? <br><br>I guess what I am trying to ask is if there as a multidimensional version of aggregate() as in aggregate(data, list(data[,"ID"]), function(x) x[2,])<br>but aggregate will only give back scalars. <br><br>Thank you, <br>Jean<br>	[[alternative HTML version deleted]]<br><br>______________________________________________<br>R-help at stat.math.ethz.ch mailing list<br>https://www.stat.math.ethz.ch/mailman/listinfo/r-help<br>

_______________________________________________
No banners. No pop-ups. No kidding.
Introducing My Way - http://www.myway.com



From tlumley at u.washington.edu  Thu Oct  9 23:17:35 2003
From: tlumley at u.washington.edu (Thomas Lumley)
Date: Thu, 9 Oct 2003 14:17:35 -0700 (PDT)
Subject: [R] detecting singular matrices
In-Reply-To: <3F85C663.3050703@stats.waikato.ac.nz>
References: <3F85C663.3050703@stats.waikato.ac.nz>
Message-ID: <Pine.A41.4.58.0310091412480.42270@homer40.u.washington.edu>

On Fri, 10 Oct 2003, Murray Jorgensen wrote:

> >
> > with a condition number of 1.236260e+16 I think I'd like it to tell me
> > it's singular like it used to :) and I know it is singular

This was found during code freeze (and I suggested to Martin that he file
a bug report on it then, so that we could say "Known Issue" when someone
reported it).

Unlike the old LINPACK code the LAPACK code reports singularity only when
it encounters an exactly zero pivot. This doesn't happen very often.
There is another, more complicated LAPACK routine that reports an
estimated condition number and reports when the matrix is singular to
within working precision, and we should probably use that one instead.
We need to investigate things like relative speed as well.

	-thomas



From arnab at myrealbox.com  Thu Oct  9 23:46:39 2003
From: arnab at myrealbox.com (Arnab mukherji)
Date: Thu, 09 Oct 2003 21:46:39 +0000
Subject: [R] RWinEdt patch
Message-ID: <1065735999.c70b88c0arnab@myrealbox.com>

Hi:

 I just tried to install the patch for RWinEdt - there is an automated package ('SWinRegistry' ) which with the rwinedt patch on the CRAN site have to be loaded as packages from R using the local zip drive option.

That works fine. The next step is to call the library(RwinEdt) and this does'nt work out ... the meesages i get are:

> install.packages(choose.files('',filters=Filters[c('zip','All'),]), .libPaths()[1], CRAN = NULL)

updating HTML package descriptions


> install.packages(choose.files('',filters=Filters[c('zip','All'),]), .libPaths()[1], CRAN = NULL)

updating HTML package descriptions


> install.packages(choose.files('',filters=Filters[c('zip','All'),]), .libPaths()[1], CRAN = NULL)

updating HTML package descriptions


> library(RWinEdt)
Error in testRversion(descfile) : This package has not been installed properly
 See the Note in ?library
In addition: Warning message: 
package RWinEdt was built under R version 1.7.1 

I am using windows office 2000 and R ver 1.7 and hence the final error message. I have tried it out on ver 1.7.1 too and i Get the same error.

Was wondering if anyone else encountered this problem

thanks

Arnab



From dmurdoch at pair.com  Fri Oct 10 03:23:12 2003
From: dmurdoch at pair.com (Duncan Murdoch)
Date: Thu, 09 Oct 2003 21:23:12 -0400
Subject: [R] R 1.8.0 Windows binaries uploaded
Message-ID: <md2covk1vhb9gqi2aqt8sapul1gb6e9u0u@4ax.com>

I've just uploaded the 1.8.0 binary build for Windows.  It should be
available on CRAN and the mirrors tomorrow.

Duncan Murdoch

_______________________________________________
R-announce at stat.math.ethz.ch mailing list
https://www.stat.math.ethz.ch/mailman/listinfo/r-announce



From Tom.Mulholland at health.wa.gov.au  Fri Oct 10 03:53:26 2003
From: Tom.Mulholland at health.wa.gov.au (Mulholland, Tom)
Date: Fri, 10 Oct 2003 09:53:26 +0800
Subject: [R] upgrading R
Message-ID: <74E242B6968AA0469B632C5A3EFC1EFD03D56FF2@nt207mesep.health.wa.gov.au>

This has been discussed with previous upgrades. One such discussion was
"How to update installed packages to a new version of R?".
http://finzi.psych.upenn.edu/R/Rhelp02/archive/8316.html  There are a
variety of methods to choose from. You may also learn about updating
packages directly from the net which can take care of the issues of
having to reinstall packages.

_________________________________________________
 
Tom Mulholland
Senior Policy Officer
WA Country Health Service
Tel: (08) 9222 4062
 
The contents of this e-mail transmission are confidential and may be
protected by professional privilege. The contents are intended only for
the named recipients of this e-mail. If you are not the intended
recipient, you are hereby notified that any use, reproduction,
disclosure or distribution of the information contained in this e-mail
is prohibited. Please notify the sender immediately.


-----Original Message-----
From: apjaworski at mmm.com [mailto:apjaworski at mmm.com] 
Sent: Friday, 10 October 2003 12:16 AM
To: Weiming Zhang
Cc: r-help at stat.math.ethz.ch
Subject: Re: [R] upgrading R



I am not sure if this is a proper way, but here is what I did recently
installing consecutive alpha and beta releases of 1.8.0 on a Win2000
machine:
(1) I uninstalled previous version using the uninstall provided with R.
This leaves all the additional packages in the library folder
(2) I reinstalled the new version into the same folder structure.

The problem might be that some 1.7.1 packages might be different from
1.8.0, so it might be safer to reinstall them from CRAN.

Andy

__________________________________
Andy Jaworski
Engineering Systems Technology Center
3M Center, 518-1-01
St. Paul, MN 55144-1000
-----
E-mail: apjaworski at mmm.com
Tel:  (651) 733-6092
Fax:  (651) 736-3122


|---------+-------------------------------->
|         |           "Weiming Zhang"      |
|         |           <Weiming.Zhang at uchsc.|
|         |           edu>                 |
|         |           Sent by:             |
|         |           r-help-bounces at stat.m|
|         |           ath.ethz.ch          |
|         |                                |
|         |                                |
|         |           10/09/2003 10:38     |
|         |                                |
|---------+-------------------------------->
 
>-----------------------------------------------------------------------
------------------------------------------------------|
  |
|
  |      To:       r-help at stat.math.ethz.ch
|
  |      cc:
|
  |      Subject:  [R] upgrading R
|
 
>-----------------------------------------------------------------------
------------------------------------------------------|




Hi,

I have installed a lot of extra packages for R 1.7.1. If I install R
1.8.0, will I have to reinstall all those packages? Is there a way that
I can upgrading R without losing old packages?

Thank you.

wz

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://www.stat.math.ethz.ch/mailman/listinfo/r-help

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://www.stat.math.ethz.ch/mailman/listinfo/r-help



From nusbj at hotmail.com  Fri Oct 10 06:49:29 2003
From: nusbj at hotmail.com (Zhen Pang)
Date: Fri, 10 Oct 2003 12:49:29 +0800
Subject: [R] command line limit under unix?
Message-ID: <Sea2-F56LGP6i4w3sgT0000229f@hotmail.com>

Dear all,

I have made my testing program to run successfully under unix in the 
background. However, my simulation work does not work. I read the 
foo.results file, I found it only have part of my code and not any output I 
want. Is there any line limit? My code is nearly 400 line. I can cut some of 
them, but I want to know whether there is any limit or exactly the number of 
limit is. Thanks.

Regards,

Pang Zhen



From jasont at indigoindustrial.co.nz  Fri Oct 10 07:15:59 2003
From: jasont at indigoindustrial.co.nz (Jason Turner)
Date: Fri, 10 Oct 2003 18:15:59 +1300
Subject: [R] command line limit under unix?
In-Reply-To: <Sea2-F56LGP6i4w3sgT0000229f@hotmail.com>
References: <Sea2-F56LGP6i4w3sgT0000229f@hotmail.com>
Message-ID: <3F86408F.5080402@indigoindustrial.co.nz>

Zhen Pang wrote:
> I have made my testing program to run successfully under unix in the 
> background. However, my simulation work does not work. I read the 
> foo.results file, I found it only have part of my code and not any 
> output I want. Is there any line limit? My code is nearly 400 line. I 
> can cut some of them, but I want to know whether there is any limit or 
> exactly the number of limit is. Thanks.

400 lines of input isn't large.   Were there any error messages?

Jason
-- 
Indigo Industrial Controls Ltd.
http://www.indigoindustrial.co.nz
64-21-343-545
jasont at indigoindustrial.co.nz



From rossini at blindglobe.net  Fri Oct 10 07:40:01 2003
From: rossini at blindglobe.net (A.J. Rossini)
Date: Thu, 09 Oct 2003 22:40:01 -0700
Subject: [R] R-OpenOffice.org Calc
In-Reply-To: <3F85AADA.1020508@lancaster.ac.uk> (Barry Rowlingson's message
	of "Thu, 09 Oct 2003 19:37:14 +0100")
References: <!~!UENERkVCMDkAAQACAAAAAAAAAAAAAAAAABgAAAAAAAAA/KN/oP+2mk+FvuiZg1iX78KAAAAQAAAAs2DmnfQDVkOHsUMGA9X0VwEAAAAA@knjiga.pedos.hr>
	<3F85AADA.1020508@lancaster.ac.uk>
Message-ID: <854qyhd51q.fsf@blindglobe.net>

Barry Rowlingson <B.Rowlingson at lancaster.ac.uk> writes:

> Branimir K. Hackenberger wrote:
>> I have been very satisfied with R-Excel interface (DCOM). Few months ago
>> I have changed my OS to Linux-Mandrake, and now I am using
>> OpenOffice.org Calc as spreadsheet. I would like to know does exist some
>> R-OpenOffice.org interface or how is possible to use R-functions in
>> OpenOffice.org Calc?
>
>   I think all you need to do is download the SDK:
>
> http://www.openoffice.org/dev_docs/source/sdk/index.html
>
>   then read and understand both the 900 page OpenOffice SDK manual,
>   and everything there is to know about R internals. Then write an
>   extension for OpenOffice Calc that uses UNO to communicate between
>   Calc and the R library. You'll probably have to use C++.
>
>   The spreadsheet section of the SDK doc starts at page 535, by the way...
>
>   Or is there a quicker way?

I think Harry M. (spelling escapes me this morning) posted about
working on a connection in the last few months.

That might help.

best,
-tony



-- 
rossini at u.washington.edu            http://www.analytics.washington.edu/ 
Biomedical and Health Informatics   University of Washington
Biostatistics, SCHARP/HVTN          Fred Hutchinson Cancer Research Center
UW (Tu/Th/F): 206-616-7630 FAX=206-543-3461 | Voicemail is unreliable
FHCRC  (M/W): 206-667-7025 FAX=206-667-4812 | use Email

CONFIDENTIALITY NOTICE: This e-mail message and any attachme...{{dropped}}



From ligges at statistik.uni-dortmund.de  Fri Oct 10 08:49:26 2003
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Fri, 10 Oct 2003 08:49:26 +0200
Subject: [R] command line limit under unix?
In-Reply-To: <Sea2-F56LGP6i4w3sgT0000229f@hotmail.com>
References: <Sea2-F56LGP6i4w3sgT0000229f@hotmail.com>
Message-ID: <3F865676.5030408@statistik.uni-dortmund.de>

Zhen Pang wrote:

> Dear all,
> 
> I have made my testing program to run successfully under unix in the 
> background. However, my simulation work does not work. I read the 
> foo.results file, I found it only have part of my code and not any 
> output I want. Is there any line limit? My code is nearly 400 line. I 
> can cut some of them, but I want to know whether there is any limit or 
> exactly the number of limit is. Thanks.

There are some buffer limits (at least on Windows). Use source() for the 
main part of those 400 lines.

Uwe Ligges



> Regards,
> 
> Pang Zhen
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help



From ligges at statistik.uni-dortmund.de  Fri Oct 10 09:19:03 2003
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Fri, 10 Oct 2003 09:19:03 +0200
Subject: [R] RWinEdt patch
In-Reply-To: <1065735999.c70b88c0arnab@myrealbox.com>
References: <1065735999.c70b88c0arnab@myrealbox.com>
Message-ID: <3F865D67.1010405@statistik.uni-dortmund.de>

Asking the RWinEdt maintainer directly seems to be more appropriate than 
asking on R-help.

There is no patch for the package RWinEdt available. Just the release 
1.5-0 of that package.


Arnab mukherji wrote:

> Hi:
> 
>  I just tried to install the patch for RWinEdt - there is an automated package ('SWinRegistry' ) which with the rwinedt patch on the CRAN site have to be loaded as packages from R using the local zip drive option.
> 
> That works fine. The next step is to call the library(RwinEdt) and this does'nt work out ... the meesages i get are:
> 
> 
>>install.packages(choose.files('',filters=Filters[c('zip','All'),]), .libPaths()[1], CRAN = NULL)
> 
> 
> updating HTML package descriptions
> 
> 
> 
>>install.packages(choose.files('',filters=Filters[c('zip','All'),]), .libPaths()[1], CRAN = NULL)
> 
> 
> updating HTML package descriptions
> 
> 
> 
>>install.packages(choose.files('',filters=Filters[c('zip','All'),]), .libPaths()[1], CRAN = NULL)
> 
> 
> updating HTML package descriptions
> 
> 
> 
>>library(RWinEdt)
> 
> Error in testRversion(descfile) : This package has not been installed properly
>  See the Note in ?library
> In addition: Warning message: 
> package RWinEdt was built under R version 1.7.1 
> 
> I am using windows office 2000 and R ver 1.7 and hence the final error message. I have tried it out on ver 1.7.1 too and i Get the same error.
> 
> Was wondering if anyone else encountered this problem

Yes. The problem probably is that the package SWinRegistry has not been 
installed properly.

Unfortunately, the source version is called 
http://www.omegahat.org/SWinRegistry/SWinRegistry_0.3-2.zip
(Attention: .zip instead of the usual .tar.gz) and the binary version
http://www.omegahat.org/SWinRegistry/SWinRegistry_0.3-2_binary.zip
This name is not valid for R's mechanism for installing packages and has 
to be renamed before installation, or unzip it manually.
I hope these filenames will be changed shortly.

So, for example do:
  - download the binary version
  - rename it to, e.g., SWinRegistry_0.3-2.zip (without the second "_")
  - Now, use the menu to install the package.

Duncan Temple Lang promised to change the names of source/binary 
packages according to the CRAN conventions, but he seems to be too busy 
these days. I'll update R-WinEdt's installation instructions with a new 
release in a couple of days.

Uwe Ligges



> thanks
> 
> Arnab
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help



From ripley at stats.ox.ac.uk  Fri Oct 10 09:22:19 2003
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Fri, 10 Oct 2003 08:22:19 +0100 (BST)
Subject: [R] Specifying suitable PC to run R
In-Reply-To: <OF9212499B.C5B3F6CB-ON85256DBA.00693AF0@convergys.com>
Message-ID: <Pine.LNX.4.44.0310100820140.13910-100000@gannet.stats>

On Thu, 9 Oct 2003 james.holtman at convergys.com wrote:

> My first choice is get as much memory on your machine as you can; 1GB since
> this the most that R can use. 

R can use up to 2Gb, but Windows is unlikely to give you that much -- 
1.7Gb has been achieved (see the CHANGES file, I believe).


-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From Rau at demogr.mpg.de  Fri Oct 10 09:49:09 2003
From: Rau at demogr.mpg.de (Rau, Roland)
Date: Fri, 10 Oct 2003 09:49:09 +0200
Subject: [R] Previous Commands, Summary
Message-ID: <3699CDBC4ED5D511BE6400306E1C0D8114CD47@hermes.demogr.mpg.de>

Dear all,

I don't know what was the actual remedy, but after installing readline and
ncurses and then re-installing R, everything works fine (=calling the
commands history with the cursor keys).
I would like to thank for the help provided by Jason Turner, Philippe
Glaziou, Peter Dalgaard and Brian Ripley.

Best,
Roland

> -----Original Message-----
> From:	Prof Brian Ripley [SMTP:ripley at stats.ox.ac.uk]
> Sent:	Thursday, October 09, 2003 10:49 AM
> To:	Philippe Glaziou
> Cc:	r-help at stat.math.ethz.ch
> Subject:	Re: [R] Previous Commands
> 
> On Thu, 9 Oct 2003, Philippe Glaziou wrote:
> 
> > Rau, Roland <Rau at demogr.mpg.de> wrote:
> > > yesterday I took the R-1.8.0-source file and compiled it on my
> > > own. As I am using Linux just for a couple of weeks, it was my
> > > first compiling session with ./configure, make, ....
> > > Everything went fine, except for one thing: if I want to look
> > > at the commands history by using the cursor keys, it does not
> > > work. Instead of displaying the previous commands, it returns
> > > something like "[[A".  Is this a common problem?  Does anyone
> > > have a solution?
> > 
> > 
> > You may need to install the ncurses dev libraries.
> 
> More likely it is the readline-del[ev] RPMs/.debs/.... that you need.
> Take a look at the R-admin manual (e.g. doc/html/R-admin.html in the 
> sources).  
> 
> -- 
> Brian D. Ripley,                  ripley at stats.ox.ac.uk
> Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
> University of Oxford,             Tel:  +44 1865 272861 (self)
> 1 South Parks Road,                     +44 1865 272866 (PA)
> Oxford OX1 3TG, UK                Fax:  +44 1865 272595
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help


+++++
This mail has been sent through the MPI for Demographic Research.  Should you receive a mail that is apparently from a MPI user without this text displayed, then the address has most likely been faked.   If you are uncertain about the validity of this message, please check the mail header or ask your system administrator for assistance.



From ripley at stats.ox.ac.uk  Fri Oct 10 10:02:57 2003
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Fri, 10 Oct 2003 09:02:57 +0100 (BST)
Subject: [R] command line limit under unix?
In-Reply-To: <Sea2-F56LGP6i4w3sgT0000229f@hotmail.com>
Message-ID: <Pine.LNX.4.44.0310100900140.18533-100000@gannet.stats>

On Fri, 10 Oct 2003, Zhen Pang wrote:

> Dear all,
> 
> I have made my testing program to run successfully under unix in the 
> background. However, my simulation work does not work. I read the 
> foo.results file, I found it only have part of my code and not any output I 
> want. Is there any line limit? My code is nearly 400 line. I can cut some of 
> them, but I want to know whether there is any limit or exactly the number of 
> limit is. Thanks.

If you are running a script by R CMD BATCH (or otherwise by redirectin) 
there is no limit on the number of lines (apart from file sizes on your 
OS) and I have used several tens of thousands of lines with auto-generated 
scripts.

There is a limit on the length of command lines, I believe 1024 chars on 
all implementations of R.

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From mmarques at inescporto.pt  Fri Oct 10 11:08:19 2003
From: mmarques at inescporto.pt (Mark Marques)
Date: Fri, 10 Oct 2003 10:08:19 +0100
Subject: [R] Lattice cloud() funtion bug Solved in Final version
Message-ID: <178684519390.20031010100819@power.inescn.pt>


  It seems that the bug in Lattice cloud() function is on the Win32 R1.8.0Beta only.
  So it is solved in the R1.8.0 Final ...



From maechler at stat.math.ethz.ch  Fri Oct 10 12:36:17 2003
From: maechler at stat.math.ethz.ch (Martin Maechler)
Date: Fri, 10 Oct 2003 12:36:17 +0200
Subject: [R] 2 questions regarding base-n and identifing digits + the
	source problem description
In-Reply-To: <FHEEJBDDCNPPNJEACDJAEEICDEAA.andrejk@zrc-sazu.si>
References: <20031009002019.188293990@xmxpita.myway.com>
	<FHEEJBDDCNPPNJEACDJAEEICDEAA.andrejk@zrc-sazu.si>
Message-ID: <16262.35745.391015.966609@gargle.gargle.HOWL>

>>>>> "Andrej" == Andrej Kveder <andrejk at zrc-sazu.si>
>>>>>     on Thu, 9 Oct 2003 10:09:22 +0200 writes:

    Andrej> Thanks for the suggestions..  The strsplit function
    Andrej> works great for the second question...  I will have
    Andrej> to come up with another solution to the first one,
    Andrej> since my problem would definetly involve a lot of
    Andrej> calculations and I think that current methods would
    Andrej> cause major computational congestion...  BUt here is
    Andrej> the outline of my problem... if anybody has an
    Andrej> idea...  In practice I start with 4 possible
    Andrej> correlations: -0.9, -0.1, 0.1, 0.9 and want to
    Andrej> construct all possible 3x3 correaltion matrices with
    Andrej> those values.  So I need to do all possible
    Andrej> permutations of a given length with replication. I
    Andrej> have a set of 4 distinct values and want to create
    Andrej> the permutations of length 3.  My idea was as
    Andrej> folows: specifing the sequence in base-4 form 0 to
    Andrej> 333 would account for all possible combinations.  I
    Andrej> checked some of the existing functions which were
    Andrej> either recursive and slow or without replication. I
    Andrej> think there was a thread on the list just a short
    Andrej> while ago.  If anybody has a hint to follow, i would
    Andrej> be more then glad to try it out...

So, what you really want is combinatorics and the are two
packages on CRAN that provide these.

Back to the original question: representing numbers in other
bases than 10 :

Several years ago, I had defined a "baseint" class (S3) and
methods for doing these base conversions -- for integers only.
I'm appending the R source file (baseint.R) with the
definitions.
As the header of the file says, I've always wanted to do this
in a much nicer way, but didn't get around to that.
actually, cbind.baseint() must have a bug.
In spite of all that it can be useful, e.g.

> (b1 <- baseint(0:10, 2))
 [1] 0{b}2    1{b}2    10{b}2   11{b}2   100{b}2  101{b}2  110{b}2  111{b}2 
 [9] 1000{b}2 1001{b}2 1010{b}2
> (o1 <- baseint(0:10, 8)) # octal : has special print form
 [1] 00  01  02  03  04  05  06  07  010 011 012
> (h1 <- baseint(0:17, 16))# hexadesimal
 [1] x0  x1  x2  x3  x4  x5  x6  x7  x8  x9  xa  xb  xc  xd  xe  xf  x10 x11
> c(b1) ## gives the char strings
 [1] "0"    "1"    "10"   "11"   "100"  "101"  "110"  "111"  "1000" "1001"
[11] "1010"
> c(baseint(1:64, 4))
 [1] "1"    "2"    "3"    "10"   "11"   "12"   "13"   "20"   "21"   "22"  
[11] "23"   "30"   "31"   "32"   "33"   "100"  "101"  "102"  "103"  "110" 
[21] "111"  "112"  "113"  "120"  "121"  "122"  "123"  "130"  "131"  "132" 
[31] "133"  "200"  "201"  "202"  "203"  "210"  "211"  "212"  "213"  "220" 
[41] "221"  "222"  "223"  "230"  "231"  "232"  "233"  "300"  "301"  "302" 
[51] "303"  "310"  "311"  "312"  "313"  "320"  "321"  "322"  "323"  "330" 
[61] "331"  "332"  "333"  "1000"
> 

-------------- next part --------------
An embedded and charset-unspecified text was scrubbed...
Name: baseint.R
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20031010/1c71727e/baseint.pl
-------------- next part --------------


Martin Maechler <maechler at stat.math.ethz.ch>	http://stat.ethz.ch/~maechler/
Seminar fuer Statistik, ETH-Zentrum  LEO C16	Leonhardstr. 27
ETH (Federal Inst. Technology)	8092 Zurich	SWITZERLAND
phone: x-41-1-632-3408		fax: ...-1228			<><

From ucgamdo at ucl.ac.uk  Fri Oct 10 13:43:12 2003
From: ucgamdo at ucl.ac.uk (ucgamdo@ucl.ac.uk)
Date: Fri, 10 Oct 2003 12:43:12 +0100
Subject: [R] fractal gallery
Message-ID: <3.0.5.32.20031010124312.007d81f0@pop-server.ucl.ac.uk>

Just in case anyone is curious, I have set up a small fractal gallery done
exclusively with R:

www.geocities.com/mariodosreis/Rfractals



From monica.palaseanu-lovejoy at stud.man.ac.uk  Fri Oct 10 13:57:01 2003
From: monica.palaseanu-lovejoy at stud.man.ac.uk (Monica Palaseanu-Lovejoy)
Date: Fri, 10 Oct 2003 12:57:01 +0100
Subject: [R] R(D)-COM stat conenctor for ArcGIS
Message-ID: <E1A7vtP-000PgM-D7@probity.mcc.ac.uk>


Hi everybody, 


  I heard about "R(D)-COM Stat connector" for ArcGIS, but i am not 
    sure what that is. I did a search in the archive but it seems i am 
    not getting anything back. 
    can anybody explain me what that is, and where i can find more 
    info about it? There is any possibility to run R from inside 
ArcGIS? 
    there is more than RArcInfo and Shapefile which can import 
these 
    kind of files in R environment?
    Sorry, but i have no clue ;-)
    Thanks a lot,

Monica



From bates at stat.wisc.edu  Fri Oct 10 14:59:40 2003
From: bates at stat.wisc.edu (Douglas Bates)
Date: 10 Oct 2003 07:59:40 -0500
Subject: [R] R 1.8.0 Windows binaries available on U.S. mirror
In-Reply-To: <md2covk1vhb9gqi2aqt8sapul1gb6e9u0u@4ax.com>
References: <md2covk1vhb9gqi2aqt8sapul1gb6e9u0u@4ax.com>
Message-ID: <6r4qyhtfib.fsf@bates4.stat.wisc.edu>

The Windows binaries are now available on the cran.us.r-project.org
mirror, the recommended download site for those in the U.S.A.

Duncan Murdoch <dmurdoch at pair.com> writes:

> I've just uploaded the 1.8.0 binary build for Windows.  It should be
> available on CRAN and the mirrors tomorrow.

_______________________________________________
R-announce at stat.math.ethz.ch mailing list
https://www.stat.math.ethz.ch/mailman/listinfo/r-announce



From david.whiting at ncl.ac.uk  Fri Oct 10 19:09:25 2003
From: david.whiting at ncl.ac.uk (David Whiting)
Date: 10 Oct 2003 17:09:25 +0000
Subject: [R] length() of vector after using strptime()
Message-ID: <m21xtl10l6.fsf@ncl.ac.uk>


Hi,

I am trying to parse a date (that is read in as a factor) and add it
to a dataframe.  The length of the parsed date is shorter than the
length of unparsed date and I therefore cannot add it to the dataframe:

> x
  [1] 20030807 20030807 20030807 20030808 20030809 20030809 20030809 20030809
  [9] 20030808 20030808 20030809 20030808 20030808 20030819 20030819 20030821
        .
        .
        .
[129] 20030808 20030805 20030726 20030810 20030805 20030811 20030816 20030818
[137] 20030811

> length(x)
[1] 137
> xParsed <- strptime(as.character(x), "%Y%m%d")
> 
> xParsed
  [1] "2003-08-07" "2003-08-07" "2003-08-07" "2003-08-08" "2003-08-09"
  [6] "2003-08-09" "2003-08-09" "2003-08-09" "2003-08-08" "2003-08-08"
        .
        .
        .
[131] "2003-07-26" "2003-08-10" "2003-08-05" "2003-08-11" "2003-08-16"
[136] "2003-08-18" "2003-08-11"
> length(xParsed)
[1] 9
> 
> dta$dateint <- xParsed
Error in "$<-.data.frame"(`*tmp*`, "dateint", value = xParsed) : 
	replacement has 9 rows, data has 137



platform i686-pc-linux-gnu
arch     i686             
os       linux-gnu        
system   i686, linux-gnu  
status                    
major    1                
minor    8.0              
year     2003             
month    10               
day      08               
language R                


Can someone tell me what I'm doing wrong?

Thanks.



-- 
David Whiting
Dar es Salaam, Tanzania



From YZhu at golder.com  Fri Oct 10 16:34:06 2003
From: YZhu at golder.com (Zhu, Yi)
Date: Fri, 10 Oct 2003 07:34:06 -0700
Subject: [R] fit a normal distribution?
Message-ID: <83D5829A495CFD4C97D49978033CE179253896@atlexchange.golder.com>


Hello Rs:

How can I fit a data set with normal distribution easily?

Thanks a lot!

Yi



From ripley at stats.ox.ac.uk  Fri Oct 10 16:37:26 2003
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Fri, 10 Oct 2003 15:37:26 +0100 (BST)
Subject: [R] length() of vector after using strptime()
In-Reply-To: <m21xtl10l6.fsf@ncl.ac.uk>
Message-ID: <Pine.LNX.4.44.0310101537020.1298-100000@gannet.stats>

You have a POSIXlt object, a list of length 9.

Try ?as.POSIXct.

On 10 Oct 2003, David Whiting wrote:

> 
> Hi,
> 
> I am trying to parse a date (that is read in as a factor) and add it
> to a dataframe.  The length of the parsed date is shorter than the
> length of unparsed date and I therefore cannot add it to the dataframe:
> 
> > x
>   [1] 20030807 20030807 20030807 20030808 20030809 20030809 20030809 20030809
>   [9] 20030808 20030808 20030809 20030808 20030808 20030819 20030819 20030821
>         .
>         .
>         .
> [129] 20030808 20030805 20030726 20030810 20030805 20030811 20030816 20030818
> [137] 20030811
> 
> > length(x)
> [1] 137
> > xParsed <- strptime(as.character(x), "%Y%m%d")
> > 
> > xParsed
>   [1] "2003-08-07" "2003-08-07" "2003-08-07" "2003-08-08" "2003-08-09"
>   [6] "2003-08-09" "2003-08-09" "2003-08-09" "2003-08-08" "2003-08-08"
>         .
>         .
>         .
> [131] "2003-07-26" "2003-08-10" "2003-08-05" "2003-08-11" "2003-08-16"
> [136] "2003-08-18" "2003-08-11"
> > length(xParsed)
> [1] 9
> > 
> > dta$dateint <- xParsed
> Error in "$<-.data.frame"(`*tmp*`, "dateint", value = xParsed) : 
> 	replacement has 9 rows, data has 137
> 
> 
> 
> platform i686-pc-linux-gnu
> arch     i686             
> os       linux-gnu        
> system   i686, linux-gnu  
> status                    
> major    1                
> minor    8.0              
> year     2003             
> month    10               
> day      08               
> language R                
> 
> 
> Can someone tell me what I'm doing wrong?
> 
> Thanks.
> 
> 
> 
> 

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From rlandis at middlebury.edu  Fri Oct 10 16:46:57 2003
From: rlandis at middlebury.edu (Landis, R Matthew)
Date: Fri, 10 Oct 2003 10:46:57 -0400
Subject: [R] predicted values from rq
Message-ID: <0FE98FA04927D411A48300D0B77CF9BB0E7A70F0@tiger.middlebury.edu>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20031010/d54a09e7/attachment.pl

From rlandis at middlebury.edu  Fri Oct 10 16:52:17 2003
From: rlandis at middlebury.edu (Landis, R Matthew)
Date: Fri, 10 Oct 2003 10:52:17 -0400
Subject: [R] interpretation of contr.poly coefficients?
Message-ID: <0FE98FA04927D411A48300D0B77CF9BB0E7A70F1@tiger.middlebury.edu>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20031010/28123c77/attachment.pl

From michael.watson at bbsrc.ac.uk  Fri Oct 10 17:11:01 2003
From: michael.watson at bbsrc.ac.uk (michael watson (IAH-C))
Date: Fri, 10 Oct 2003 16:11:01 +0100
Subject: [R] R sometimes dies from within Perl
Message-ID: <20B7EB075F2D4542AFFAF813E98ACD9301C00BD7@cl-exsrv1.irad.bbsrc.ac.uk>

Hi Guys

This is a combined Perl/R question, hopefully there are enough people out there who use both to help me.  I am using the latest versions of both on a SUSE Linux 8.2 machine.

I am wrapping up R in some very simple perl scripts.  My problem is that when I run a series of commands from within R, with a certain data set, one of the commands reports an error but the R process remains active.  

However, if I open a pipe within Perl (using 'open(R, "| R");') and then pipe the very same commands to R using 'print R "<commands>";', then when the error occurs, the R process dies and i get a message "Execution halted".

Can anyone think of a reason why R doesn't crash normally, but does when controlled from within perl?

Thanks
Mick



From rxg218 at psu.edu  Fri Oct 10 18:19:01 2003
From: rxg218 at psu.edu (Rajarshi Guha)
Date: 10 Oct 2003 12:19:01 -0400
Subject: [R] general genetic algorithm / simulated annealing framework
Message-ID: <1065802741.11148.25.camel@ra.chem.psu.edu>

Hi,
  we have some code that does variable selection with a genetic
algorithm or simulated annealing, using a linear regression routine or
neural network as the objective function. This code is a mixture of
fortran and C.

The code is more than 15 years old and I  am planning to rework it.
Though a C rewrite would be good for efficiency, I would like to
protoype it quickly in R and see how it performs.

I came across the subselect package which does variable selection. But
it has 3 fixed criterion (is it correct to call these objective
functions?).

Before I start from scratch is any code available that provides the
basic framework of a GA/SA into which I could basically plug an
objective function (which might actually be a regresion routine or a NN
routine)

Thanks

-------------------------------------------------------------------
Rajarshi Guha <rxg218 at psu.edu> <http://jijo.cjb.net>
GPG Fingerprint: 0CCA 8EE2 2EEB 25E2 AB04 06F7 1BB9 E634 9B87 56EE
-------------------------------------------------------------------
The way to love anything is to realize that it might be lost.



From bates at stat.wisc.edu  Fri Oct 10 19:03:09 2003
From: bates at stat.wisc.edu (Douglas Bates)
Date: 10 Oct 2003 12:03:09 -0500
Subject: [R] Debian (testing) packages for R-1.8.0
Message-ID: <6rd6d5rpo2.fsf@bates4.stat.wisc.edu>

Packages of R-1.8.0 for the Debian testing (or sarge) distribution are
now available on http://cran.us.r-project.org/bin/linux/debian and
should appear on other mirrors in a day or two.

_______________________________________________
R-announce at stat.math.ethz.ch mailing list
https://www.stat.math.ethz.ch/mailman/listinfo/r-announce



From p.pagel at gsf.de  Thu Oct  9 18:51:35 2003
From: p.pagel at gsf.de (Philipp Pagel)
Date: Thu, 9 Oct 2003 18:51:35 +0200
Subject: [R] Getting rows from a dataframe
In-Reply-To: <E1A7dFB-0005pP-00@orion.orbis>
References: <E1A7dFB-0005pP-00@orion.orbis>
Message-ID: <20031009165133.GB1492@porcupine.gsf.de>


> Sorry if this is a silly question. I'm trying to extract all elements
> of a dataframe at a particular row.

How about

df[12, ]

to get row number 12?

> I can find no mention of this in any documentation

You must have different documentation than me ....
Subsetting is described on the first couple of pages of pretty much
every introducory manual.

cu
	Philipp

-- 
Dr. Philipp Pagel                                Tel.  +49-89-3187-3675
Institute for Bioinformatics / MIPS              Fax.  +49-89-3187-3585
GSF - National Research Center for Environment and Health
Ingolstaedter Landstrasse 1
85764 Neuherberg, Germany



From mfero1 at comcast.net  Fri Oct 10 20:37:24 2003
From: mfero1 at comcast.net (Matthew Fero)
Date: Fri, 10 Oct 2003 11:37:24 -0700
Subject: [R] Multiple plots?
Message-ID: <CA5A4210-FB50-11D7-8F4A-0003931987E8@comcast.net>

Here's a beginner question:  How do I plot two different sets of points 
simultaneously, e.g. (Xapples, Yapples) and (Xoranges, Yoranges), 
preferably in different colors?

Matthew



From rpeng at jhsph.edu  Fri Oct 10 20:38:59 2003
From: rpeng at jhsph.edu (Roger D. Peng)
Date: Fri, 10 Oct 2003 14:38:59 -0400
Subject: [R] Multiple plots?
In-Reply-To: <CA5A4210-FB50-11D7-8F4A-0003931987E8@comcast.net>
References: <CA5A4210-FB50-11D7-8F4A-0003931987E8@comcast.net>
Message-ID: <3F86FCC3.8000605@jhsph.edu>

Look at ?points.  And you can set col = <color> for most plotting functions.

-roger

Matthew Fero wrote:

> Here's a beginner question:  How do I plot two different sets of 
> points simultaneously, e.g. (Xapples, Yapples) and (Xoranges, 
> Yoranges), preferably in different colors?
>
> Matthew
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
>



From dyang at nrcan.gc.ca  Fri Oct 10 21:09:58 2003
From: dyang at nrcan.gc.ca (Yang, Richard)
Date: Fri, 10 Oct 2003 15:09:58 -0400
Subject: [R] RE: Installing GLMMGibbs problems
Message-ID: <F0E0B899CB43D5118D220002A55113CF02F92800@s2-edm-r1.nofc.cfs.nrcan.gc.ca>

Thank Andy Liaw for his response to my enquiry. He suggested creating the
tar.gz source package at the parent directory of the fixed code using 
      R CMD build  GLMMGibbs
and then R CMD INSTALL GLMMGibbs to install the package. The package is now
installed and works fine.

Thanks Andy,

-- Richard 

My original question message:

> 
> Dear all;
> 
> Installing the GLMMGibbs package to my Solaris Unix box, I 
> got an compiling error:
> 
> 		ars.c:497:10: missing terminating " character
> 		ars.c: In function `dump_arse':
> 		ars.c:498: error: parse error before "mylesj"
> 
> 		.....
> 
> The compiling error was reported to the list on Jul 3, 2003. 
> According to Prof. Brain Ripley this is a known problem with 
> the package and gcc 3.3, and the maintainer has been informed ...
> 
> Because I was unable to reach the maintainer by email (my 
> email to mylesj at icrf.icnet.uk on Oct. 2, bounced), I saved 
> the GLMMGibbs package from the tmp directory, edited the 
> offensive lines in src/ars.c and saved the file. Following a 
> test compiling the edited ars.c without error, I re-tarred 
> and gzipped the saved GLMMGibbs package, stored the new 
> GLMMGibbs_0.5-1.tar.gz in a directory Rsources.
> 
> I expected to install the package from the saved directory 
> (similar to "install package(s) from local zip files..." on 
> W2K) and used the install.packages():
> 
> > install.packages("GLMMGibbs", 
> lib="/PATH/Rsources/R-1.7.1/library", destdir = "/PATH/Rsources")
> 
> Instead of using the local zip file from the Rsources 
> directory, R downloaded the GLMMGibbs_0.5-1.tar.gz from CRAN:
> 
> 	trying URL 
> `http://cran.r-project.org/src/contrib/GLMMGibbs_0.5-1.tar.gz'
> 	Content type `application/x-tar' length 346260 bytes
> 	opened URL
> 	.....
> 
> 	and produced the same compiling error. 
> 
> The question becomes how to install packages from local zip 
> files on Solaris or Linux platforms? 
> 
> In the absence of the original maintainer, could someone in R 
> team edit the source file ars.c in compliance with gcc 3.3? 
> 
> TIA,
> 
> Richard 
> 
> 
> 
>



From hstoll at latte.harvard.edu  Sat Oct 11 06:15:59 2003
From: hstoll at latte.harvard.edu (hstoll@latte.harvard.edu)
Date: Sat, 11 Oct 2003 00:15:59 -0400
Subject: [R] Simplex "Out of Bounds" Error
Message-ID: <1065845759.3f8783ffdd459@webmail.hmdc.harvard.edu>

I am running the following code (testing the use of the simplex function to 
determine if a point is in the convex hull of another set of points or not):

>a <- c(0, 0)
>A3 <-matrix(c(1,2,3,4,1,1), ncol = 2, byrow = T)
>b3 <-c(1.5, 3.5, 1)
>simplex(a = a, A3 = A3, b3 = b3)

and the following error message appears:
 Error in simplex1(out1$a[1:(n + m1 + m2)], out1$A[, 1:(n + m1 + m2)],  : 
        subscript out of bounds

Any advice on why this error is appearing and how to avoid it?  (Particularly 
the latter?)  In terms of the convex hull problem, this error seems to occur 
when the point in question (in the convex hull or not) is on the edge/surface 
of the convex hull.  But I'm not positive about this.  

Thanks in advance.

(I've noticed a few other posts of this problem, but never saw anyone reply.)



From feferraz at linux.ime.usp.br  Sat Oct 11 06:26:39 2003
From: feferraz at linux.ime.usp.br (Fernando Henrique Ferraz)
Date: Sat, 11 Oct 2003 01:26:39 -0300
Subject: [R] DBI Interface broken
Message-ID: <20031011042639.GA24192@gmx.de>


	Hi, I'm trying to use R's DBI interface but it appears to be broken. I am using R 1.8.0 on Linux, and have just installed DBI 0.1-6 through the command 'install.packages("DBI")'. 
	Installation went fine, but now when I try to do a simple dbConnect(), it won't do anything besides complaining about an internal error:

	> library(DBI)
	> dbConnect(anything)
	Error in dbConnect(anything) : couldn't find function ".valueClassTest"

	> dbConnect
	standardGeneric for "dbConnect" defined from package "DBI"
	  defined with value class: "DBIConnection"

	function (drv, ...) 
	.valueClassTest(standardGeneric("dbConnect"), "DBIConnection", 
	    "dbConnect")
	<environment: 0x91201a8>
	Methods may be defined for arguments: drv 

	Anyone has a clue on what might be going on? Is it DBI working for any of you using R 1.8.0?

	Thank you,
   

--
[]'s
Fernando Henrique Ferraz P. da Rosa



From jbsambatti at ucdavis.edu  Sat Oct 11 10:21:25 2003
From: jbsambatti at ucdavis.edu (Julianno Sambatti)
Date: Sat, 11 Oct 2003 01:21:25 -0700
Subject: [R] image usage
Message-ID: <E7D39636-FBC3-11D7-9352-0003939BFCD0@ucdavis.edu>

I am beginner in R and am trying to use the image function to create a 
grid, but would like to use an outer file as input. A file that 
contains the following array for example:

3	4	5
2	3	5
1	2	1

so that each cell has a color with different intensity. I am having 
problems to input the file and am not sure that image would do it  . 
Does anybody know how to do it or have another suggestion?

Thanks

Julianno



From ripley at stats.ox.ac.uk  Sat Oct 11 10:34:59 2003
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Sat, 11 Oct 2003 09:34:59 +0100 (BST)
Subject: [R] Simplex "Out of Bounds" Error
In-Reply-To: <1065845759.3f8783ffdd459@webmail.hmdc.harvard.edu>
Message-ID: <Pine.LNX.4.44.0310110932490.2185-100000@gannet.stats>

On Sat, 11 Oct 2003 hstoll at latte.harvard.edu wrote:

> I am running the following code (testing the use of the simplex function to 
> determine if a point is in the convex hull of another set of points or not):
> 
> >a <- c(0, 0)
> >A3 <-matrix(c(1,2,3,4,1,1), ncol = 2, byrow = T)
> >b3 <-c(1.5, 3.5, 1)
> >simplex(a = a, A3 = A3, b3 = b3)
> 
> and the following error message appears:
>  Error in simplex1(out1$a[1:(n + m1 + m2)], out1$A[, 1:(n + m1 + m2)],  : 
>         subscript out of bounds
> 
> Any advice on why this error is appearing and how to avoid it?  (Particularly 
> the latter?)  In terms of the convex hull problem, this error seems to occur 
> when the point in question (in the convex hull or not) is on the edge/surface 
> of the convex hull.  But I'm not positive about this.  
> 
> Thanks in advance.
> 
> (I've noticed a few other posts of this problem, but never saw anyone reply.)

I assume this is function simplex() in package boot, although you did not
say so.  If so, did you contact the author (and not the maintainer) who
may well not read R-help but is the person most likely to be able to 
supply a fix?

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From kris.nackaerts at agr.kuleuven.ac.be  Sat Oct 11 10:52:07 2003
From: kris.nackaerts at agr.kuleuven.ac.be (Kris Nackaerts)
Date: Sat, 11 Oct 2003 10:52:07 +0200
Subject: [R] image usage
In-Reply-To: <E7D39636-FBC3-11D7-9352-0003939BFCD0@ucdavis.edu>
References: <E7D39636-FBC3-11D7-9352-0003939BFCD0@ucdavis.edu>
Message-ID: <3F87C4B7.8090408@agr.kuleuven.ac.be>

Suppose your data is in image.txt, space delimited (one or more spaces),

then you could use:

myimage <- 
as.matrix(read.table('image.txt',skip=0,header=FALSE,sep="",strip.white=TRUE))to 


view the resulting image use:

image(myimage)

to get contours: contour(myimage)
etc.

-- 
------------------------------------------------------------------------

 http://perswww.kuleuven.ac.be/~u0027178/VCard/mycard.php?name=krisn

 http://gloveg.kuleuven.ac.be/

------------------------------------------------------------------------
 Minds are like parachutes, they only work when open



From ligges at statistik.uni-dortmund.de  Sat Oct 11 11:50:37 2003
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Sat, 11 Oct 2003 11:50:37 +0200
Subject: [R] DBI Interface broken
References: <20031011042639.GA24192@gmx.de>
Message-ID: <3F87D26D.D2CDAFA9@statistik.uni-dortmund.de>



Fernando Henrique Ferraz wrote:
> 
>         Hi, I'm trying to use R's DBI interface but it appears to be broken. I am using R 1.8.0 on Linux, and have just installed DBI 0.1-6 through the command 'install.packages("DBI")'.
>         Installation went fine, but now when I try to do a simple dbConnect(), it won't do anything besides complaining about an internal error:
> 
>         > library(DBI)
>         > dbConnect(anything)
>         Error in dbConnect(anything) : couldn't find function ".valueClassTest"
> 
>         > dbConnect
>         standardGeneric for "dbConnect" defined from package "DBI"
>           defined with value class: "DBIConnection"
> 
>         function (drv, ...)
>         .valueClassTest(standardGeneric("dbConnect"), "DBIConnection",
>             "dbConnect")
>         <environment: 0x91201a8>
>         Methods may be defined for arguments: drv
> 
>         Anyone has a clue on what might be going on? Is it DBI working for any of you using R 1.8.0?
> 
>         Thank you,

Why do you post twice, to R-help and to R-bugs? In order not to
cross-post I repeat my answer on your bug-report:

Package "methods" has a namespace from which .valueClassTest() is not
exported, but it's in there, try:
  methods:::.valueClassTest

I think .valueClassTest() is not intended to be called by the user, so
the bug seems to be in package DBI rather than in R, and the maintainer
of DBI (David A. James <dj at bell-labs.com>, in CC) will certainly fix it.

Uwe Ligges



From Ted.Harding at nessie.mcc.ac.uk  Sat Oct 11 11:52:34 2003
From: Ted.Harding at nessie.mcc.ac.uk ( (Ted Harding))
Date: Sat, 11 Oct 2003 10:52:34 +0100 (BST)
Subject: [R] Automatic re-looping after error
In-Reply-To: <Pine.A41.4.58.0310091150270.42270@homer40.u.washington.edu>
Message-ID: <XFMail.031011105234.Ted.Harding@nessie.mcc.ac.uk>

On 09-Oct-03 Thomas Lumley wrote:
> On Thu, 9 Oct 2003 Ted.Harding at nessie.mcc.ac.uk wrote:
> 
>> All that's really needed to cope with the situation is for R to
>> drop that cycle of the loop, and resume with a new cycle.
>>
>> However, I'm wondering how to set this up. I've had a look at
>> try(), and I'm not at all sure that it does what I would want.
>> What I'd really like is something (inside the loop) on the lines of
>>
>>    on.error(maybe some parameters X) break
>>
>> where X might specify what sort of error or what function it comes
>> from. Would setting
>>
>>   options(error = break )
>>
> 
> I don't think so.  You may need to look at the new exception-handling
> code (start with help(tryCatch)).

Thanks, Thomas.
I don't seem to have anything related to this in R-1.7.1 (16/06/03).
However, some web-searching finally tracked down

http://stat.ethz.ch/R-manual/R-devel/library/base/html/conditions.html

so what would be involved in making this stuff available? Upgrade to
current R? Install a beta-version?

Thanks again,
Ted.


--------------------------------------------------------------------
E-Mail: (Ted Harding) <Ted.Harding at nessie.mcc.ac.uk>
Fax-to-email: +44 (0)870 167 1972
Date: 11-Oct-03                                       Time: 10:52:34
------------------------------ XFMail ------------------------------



From rossini at blindglobe.net  Sat Oct 11 18:38:11 2003
From: rossini at blindglobe.net (A.J. Rossini)
Date: Sat, 11 Oct 2003 09:38:11 -0700
Subject: [R] Some teaching/training materials (ESS/ESS-Noweb-Sweave/SNOW)
Message-ID: <85brsnvifg.fsf@blindglobe.net>



http://www.analytics.washington.edu/~rossini/courses/cph-statcomp/

Lecture/Labs 1 and 2 are on For ESS, ESS-Noweb-Sweave.

Lecture/Lab 4 is on parallel computing with R

(each Lecture/Lab was just under 2 hours).  

Comments/corrections welcome, they were used last week here in
Copenhagen, so "most" of the bugs are out.

best,
-tony

p.s. Lecture/Lab 3 on visualization needs to be completely redone for
WWW work; it doesn't translate well (some might say, "at all").

-- 
rossini at u.washington.edu            http://www.analytics.washington.edu/ 
Biomedical and Health Informatics   University of Washington
Biostatistics, SCHARP/HVTN          Fred Hutchinson Cancer Research Center
UW (Tu/Th/F): 206-616-7630 FAX=206-543-3461 | Voicemail is unreliable
FHCRC  (M/W): 206-667-7025 FAX=206-667-4812 | use Email

CONFIDENTIALITY NOTICE: This e-mail message and any attachme...{{dropped}}



From rossini at blindglobe.net  Sat Oct 11 18:50:37 2003
From: rossini at blindglobe.net (A.J. Rossini)
Date: Sat, 11 Oct 2003 09:50:37 -0700
Subject: [R] Some R-related teaching/training materials
 (ESS/ESS-Noweb-Sweave/SNOW)
Message-ID: <85ad87vhuq.fsf@blindglobe.net>



http://www.analytics.washington.edu/~rossini/courses/cph-statcomp/

Lecture/Labs 1 and 2 are on For ESS, ESS-Noweb-Sweave.

Lecture/Lab 4 is on parallel computing with R

(each Lecture/Lab was just under 2 hours).  

Comments/corrections welcome, they were used last week here in
Copenhagen, so "most" of the bugs are out.

best,
-tony

p.s. Lecture/Lab 3 on visualization needs to be completely redone for
WWW work; it doesn't translate well (some might say, "at all").

-- 
rossini at u.washington.edu            http://www.analytics.washington.edu/ 
Biomedical and Health Informatics   University of Washington
Biostatistics, SCHARP/HVTN          Fred Hutchinson Cancer Research Center
UW (Tu/Th/F): 206-616-7630 FAX=206-543-3461 | Voicemail is unreliable
FHCRC  (M/W): 206-667-7025 FAX=206-667-4812 | use Email

CONFIDENTIALITY NOTICE: This e-mail message and any attachme...{{dropped}}



From guano at usp.br  Sat Oct 11 19:28:45 2003
From: guano at usp.br (Carlos Henrique Grohmann de Carvalho)
Date: Sat, 11 Oct 2003 14:28:45 -0300
Subject: [R] interpolation methods
Message-ID: <1065893325.3f883dcdb713f@webmail.usp.br>

Hello all,

I want to interpolate some data i prepared in GRASS, so I'm looking for what
interpolation methods are available for R. In GRASS I can do Splines with
tension and IDW. I'm looking for something like multiquadric equations. Is
there any function/package available fo rthis?

thanks all



-- 
+-------------------------------------------------+
   Carlos Henrique Grohmann de Carvalho - Guano  
    Geologist - MSc Student at IGc-USP - Brazil
       Linux User #89721  ICQ: 214752832
+-------------------------------------------------+



From rlee at fpcc.net  Sat Oct 11 19:43:17 2003
From: rlee at fpcc.net (rlee@fpcc.net)
Date: Sat, 11 Oct 2003 11:43:17 -0600 (MDT)
Subject: [R] Subclassing lm
Message-ID: <34249.64.24.105.152.1065894197.squirrel@webmail.fpcc.net>

I'd trying to subclass the "lm" class to produce a "mylm" class whose
instances behave like lm objects (are accepted by methods like summary.lm)
but have additional data or slots of my own design.

For starters:

setClass("mylm", "lm")

produces the somewhat cryptic:
 Warning message:
Old-style (``S3'') class "mylm" supplied as a superclass of "mylm", but no
automatic conversion will be peformed for S3 classes in:
.validDataPartClass(cl, name)

What does this imply?

-Rob



From kris.nackaerts at agr.kuleuven.ac.be  Sat Oct 11 21:27:10 2003
From: kris.nackaerts at agr.kuleuven.ac.be (Kris Nackaerts)
Date: Sat, 11 Oct 2003 21:27:10 +0200
Subject: [R] interpolation methods
In-Reply-To: <1065893325.3f883dcdb713f@webmail.usp.br>
References: <1065893325.3f883dcdb713f@webmail.usp.br>
Message-ID: <3F88598E.3020504@agr.kuleuven.ac.be>

Dear,

Have a look at:

*akima <http://cran.r-project.org/src/contrib/akima_0.3-4.tar.gz>: 
Interpolation of irregularly spaced data*

    Linear or cubic spline interpolation for irregular gridded data

Kris

>  
>


-- 
------------------------------------------------------------------------

 http://perswww.kuleuven.ac.be/~u0027178/VCard/mycard.php?name=krisn

 http://gloveg.kuleuven.ac.be/

------------------------------------------------------------------------
 Minds are like parachutes, they only work when open



From olau at fas.harvard.edu  Sat Oct 11 21:49:10 2003
From: olau at fas.harvard.edu (Olivia Lau)
Date: Sat, 11 Oct 2003 15:49:10 -0400 (EDT)
Subject: [R] boot statictic fn for dual estimation of 2 stats?  
Message-ID: <Pine.LNX.4.44.0310111520390.15772-100000@login1.fas.harvard.edu>

Hi, 

I am trying to use boot() to refit an ordinal logit (polr in MASS) model.  
(A very basic bootstrap which samples from the data frame without 
replacement and updates the model.)

I need to extract two statistics per run (the coefficients and zeta) and I 
tried concatenating them into a single vector after fitting, but I get the 
following error: 

Error in "[<-"(*tmp*, r, , value = statistic(data, i[r, ], ...)) : 
	number of items to replace is not a multiple of replacement length

This error goes away if I just return the coefficients, but I need zeta in 
order to calculate predicted probabilities for each outcome category (1:4 
in this case).   Alternatively, if boot() could return a vector of 
predicted probabilities for each categorical outcome, that would 
work as well, but I don't have a clue as to how to start programming this. 

Thanks, Olivia Lau

------------

Here's some sample data and code:  

cost <- c(4, 3, 2, 2, 2, 2, 2, 2, 1, 2, 1, 2, 2, 4, 3, 1, 2, 2, 1, 3, 2, 
1, 4, 1, 1, 2, 2, 2, 1, 3, 1, 1, 1, 1, 2, 1, 1, 1, 1, 2, 2, 2, 2, 2, 1, 1, 
2, 1, 1, 1, 2, 2, 1, 1, 1, 2, 2, 2, 2, 3, 1, 3, 3, 1, 2, 1, 3, 2, 2, 2, 
3, 1, 2, 1, 2, 2, 1, 2)

mil <- c(1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 
0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 
0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 
1, 0, 0, 0, 0, 1)

coop <-  c(4, 2, 1, 1, 1, 1, 2, 1, 2, 2, 1, 1, 3, 3, 3, 1, 4, 3, 1, 3, 4, 
1, 1, 1, 1, 3, 1, 1, 1, 4, 1, 1, 1, 1, 2, 3, 1, 1, 1, 2, 2, 2, 1, 2, 1, 1, 
1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 3, 3, 1, 3, 2, 3, 2, 3, 1, 1, 3, 2, 
2, 3, 2, 1, 4, 1, 3)

sanction <- data.frame(cost=cost, mil=mil, coop=coop)

est <- polr(as.factor(cost) ~ mil + coop, data = sanction)

bootfn <- function(data, i, object) {  
  d <- data[i,]
  fit <- update(object, data = d)
  c(fit$coef, fit$zeta)
}

res <- boot(sanction, bootfn, R = 10, object = est)



From rpeng at jhsph.edu  Sat Oct 11 22:18:07 2003
From: rpeng at jhsph.edu (Roger D. Peng)
Date: Sat, 11 Oct 2003 16:18:07 -0400
Subject: [R] boot statictic fn for dual estimation of 2 stats?
In-Reply-To: <Pine.LNX.4.44.0310111520390.15772-100000@login1.fas.harvard.edu>
References: <Pine.LNX.4.44.0310111520390.15772-100000@login1.fas.harvard.edu>
Message-ID: <3F88657F.4050301@jhsph.edu>

Occasionaly, the polr returns a zeta element of length equal to 2 rather 
than 3 (which I guess is what you're expecting).  You're bootfn function 
should always check to see that it's returning an object of the same 
length every time.

-roger

Olivia Lau wrote:
> Hi, 
> 
> I am trying to use boot() to refit an ordinal logit (polr in MASS) model.  
> (A very basic bootstrap which samples from the data frame without 
> replacement and updates the model.)
> 
> I need to extract two statistics per run (the coefficients and zeta) and I 
> tried concatenating them into a single vector after fitting, but I get the 
> following error: 
> 
> Error in "[<-"(*tmp*, r, , value = statistic(data, i[r, ], ...)) : 
> 	number of items to replace is not a multiple of replacement length
> 
> This error goes away if I just return the coefficients, but I need zeta in 
> order to calculate predicted probabilities for each outcome category (1:4 
> in this case).   Alternatively, if boot() could return a vector of 
> predicted probabilities for each categorical outcome, that would 
> work as well, but I don't have a clue as to how to start programming this. 
> 
> Thanks, Olivia Lau
> 
> ------------
> 
> Here's some sample data and code:  
> 
> cost <- c(4, 3, 2, 2, 2, 2, 2, 2, 1, 2, 1, 2, 2, 4, 3, 1, 2, 2, 1, 3, 2, 
> 1, 4, 1, 1, 2, 2, 2, 1, 3, 1, 1, 1, 1, 2, 1, 1, 1, 1, 2, 2, 2, 2, 2, 1, 1, 
> 2, 1, 1, 1, 2, 2, 1, 1, 1, 2, 2, 2, 2, 3, 1, 3, 3, 1, 2, 1, 3, 2, 2, 2, 
> 3, 1, 2, 1, 2, 2, 1, 2)
> 
> mil <- c(1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 
> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 
> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 
> 1, 0, 0, 0, 0, 1)
> 
> coop <-  c(4, 2, 1, 1, 1, 1, 2, 1, 2, 2, 1, 1, 3, 3, 3, 1, 4, 3, 1, 3, 4, 
> 1, 1, 1, 1, 3, 1, 1, 1, 4, 1, 1, 1, 1, 2, 3, 1, 1, 1, 2, 2, 2, 1, 2, 1, 1, 
> 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 3, 3, 1, 3, 2, 3, 2, 3, 1, 1, 3, 2, 
> 2, 3, 2, 1, 4, 1, 3)
> 
> sanction <- data.frame(cost=cost, mil=mil, coop=coop)
> 
> est <- polr(as.factor(cost) ~ mil + coop, data = sanction)
> 
> bootfn <- function(data, i, object) {  
>   d <- data[i,]
>   fit <- update(object, data = d)
>   c(fit$coef, fit$zeta)
> }
> 
> res <- boot(sanction, bootfn, R = 10, object = est)
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
>



From mn216 at columbia.edu  Sat Oct 11 22:31:29 2003
From: mn216 at columbia.edu (Murad Nayal)
Date: Sat, 11 Oct 2003 16:31:29 -0400
Subject: [R] multiple character matching within a string
Message-ID: <3F8868A1.7F6BE484@columbia.edu>



Hello all,

I need to count the number of times certain characters occur in a
string. The only way I have found so far to accomplish this is by using
strsplit i.e.

my.string <- "DDDRRHIH"
my.char   <- "D"
num.char <- -1 + length(unlist(strsplit(my.string,my.char)))

now you probably won't be surprised if I say that this has proven to be
extremely slow (I am not sure exactly why though, is it because strsplit
creates new list for every call?). Is there an alternative way to do
this short of going to compiled code?

many thanks,





-- 
Murad Nayal M.D. Ph.D.
Department of Biochemistry and Molecular Biophysics
College of Physicians and Surgeons of Columbia University
630 West 168th Street. New York, NY 10032
Tel: 212-305-6884	Fax: 212-305-6926



From iryna at stat.tamu.edu  Sat Oct 11 23:27:10 2003
From: iryna at stat.tamu.edu (Iryna Lobach)
Date: Sat, 11 Oct 2003 16:27:10 -0500
Subject: [R] evaluating R expressions from C
Message-ID: <3F8875AE.1030001@stat.tamu.edu>

Hello!

I've looked at R help and previous postings, but I am not sure I 
completely understand the mechanism of evaluating R expressions from C++.

They have function

SEXP eval(SEXP expr, SEXP rho),

but I can't get it to work.

Could anyone who used it give any comments on how efficient it is and, 
if possible, give example of how to use it. My main problem is that I 
don't really understand how pass the name of  R-function and it's 
parameters.

Thank you,
Iryna



From v.demart at libero.it  Sun Oct 12 09:19:12 2003
From: v.demart at libero.it (v.demart@libero.it)
Date: Sun, 12 Oct 2003 09:19:12 +0200
Subject: [R] Integration between R & latex
Message-ID: <HMMV00$1D080D67DB9E5BFB1C89620EF0345764@libero.it>

As an R absolute beginner and an expert (very old) statistician and latex user,
I'm interested in using R to produce AUTOMAGICALLY tables in latex format. I
mean I would like to have the means to build an R procedure generating **FROM
INSIDE** a table or a graph to be inserted directly into latex.
I've read http://hesweb1.med.virginia.edu/biostat/s/doc/summary.pdf where the
author speaks of S (not R) and Latex integration via a Hmisc library which
allows to make automatic, calculated tables in latex.
Unfortunately this library doesn't seem to be present in R (I tried to follow
the example issuing a "library(Hmisc)" and then, plainly, "library()" but
nothing with the same functions appeared).

Any help on how to set up that integration? And, where can I find
documentation/examples on that?

Thanks

Vittorio



From christoph.bier at web.de  Sun Oct 12 10:02:02 2003
From: christoph.bier at web.de (Christoph Bier)
Date: Sun, 12 Oct 2003 10:02:02 +0200
Subject: [R] Integration between R & latex
In-Reply-To: <HMMV00$1D080D67DB9E5BFB1C89620EF0345764@libero.it>
References: <HMMV00$1D080D67DB9E5BFB1C89620EF0345764@libero.it>
Message-ID: <3F890A7A.6060606@web.de>

v.demart at libero.it schrieb:
> As an R absolute beginner and an expert (very old)
> statistician and latex user, I'm interested in using R to
> produce AUTOMAGICALLY tables in latex format. I mean I
> would like to have the means to build an R procedure
> generating **FROM INSIDE** a table or a graph to be
> inserted directly into latex. I've read
> http://hesweb1.med.virginia.edu/biostat/s/doc/summary.pdf
> where the author speaks of S (not R) and Latex integration
> via a Hmisc library which allows to make automatic,
> calculated tables in latex. Unfortunately this library
> doesn't seem to be present in R (I tried to follow the
> example issuing a "library(Hmisc)" and then, plainly,
> "library()" but nothing with the same functions appeared).
> 
> Any help on how to set up that integration? And, where can
> I find documentation/examples on that?

A Google Groups search matches many postings,
e.g. this one pan.2003.10.09.14.08.58.922000 at NIETZOLEUKskynet.be:

"I don't know how well gnuplot integrates
with LaTeX, but R certainly does. If you
use the Sweave-package (delivered by default
with R), you can put your R code right into
your LaTeX document.
Process your document once with R and then
with LaTeX and that's it: a nice paper with
the results of your analyses. If you _really_
want to speed up things, you can use R from
within your favorite emacs editor (install
ESS mode, i.e. emacs speaks statistics)."

At the moment I don't have any further
information because I'm also at the beginning
of integrating R into LaTeX. But from the
german newsgroup de.comp.text.tex I know, that
it's possible and even easy. Reading the
following document was suggested to me:
http://www.ci.tuwien.ac.at/~leisch/Sweave/Sweave-manual-20020507.pdf 

And LaTeX-tables could really be done with
library(hmisc). Maybe you have to install the
hmisc-package. It isn't yet in my machine, too.
How to install a package describes the FAQ
(http://cran.r-project.org/doc/FAQ/R-FAQ.html):

5.2 How can add-on packages be installed?

HTH

Greetings,

Christoph
-- 
Christoph Bier, Dipl.Oecotroph., Email: bier at wiz.uni-kassel.de
Universitaet Kassel, FG Oekologische Lebensmittelqualitaet und
Ernaehrungskultur \\ Postfach 12 52 \\ 37202 Witzenhausen
Tel.: +49 (0) 55 42 / 98 -17 21, Fax: -17 13



From Detlef.Steuer at unibw-hamburg.de  Sun Oct 12 10:01:40 2003
From: Detlef.Steuer at unibw-hamburg.de (Detlef Steuer)
Date: Sun, 12 Oct 2003 10:01:40 +0200
Subject: [R] Integration between R & latex
In-Reply-To: <HMMV00$1D080D67DB9E5BFB1C89620EF0345764@libero.it>
References: <HMMV00$1D080D67DB9E5BFB1C89620EF0345764@libero.it>
Message-ID: <20031012100140.6b741f5f.steuer@unibw-hamburg.de>

Hi!

Most probably you did not install Hmisc on your system.
It is  not included in R-base.
You find it on CRAN. (cran.r-project.org -> source for contributed packages)

After installing it for your system the examples may well work.

Have a nice sunday,

detlef

On Sun, 12 Oct 2003 09:19:12 +0200
"v\.demart\@libero\.it" <v.demart at libero.it> wrote:

> As an R absolute beginner and an expert (very old) statistician and latex user,
> I'm interested in using R to produce AUTOMAGICALLY tables in latex format. I
> mean I would like to have the means to build an R procedure generating **FROM
> INSIDE** a table or a graph to be inserted directly into latex.
> I've read http://hesweb1.med.virginia.edu/biostat/s/doc/summary.pdf where the
> author speaks of S (not R) and Latex integration via a Hmisc library which
> allows to make automatic, calculated tables in latex.
> Unfortunately this library doesn't seem to be present in R (I tried to follow
> the example issuing a "library(Hmisc)" and then, plainly, "library()" but
> nothing with the same functions appeared).
> 
> Any help on how to set up that integration? And, where can I find
> documentation/examples on that?
> 
> Thanks
> 
> Vittorio
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> 


-- 
"The whole problem with the world is that fools and fanatics are always
 so certain of themselves, but wiser people so full of doubts." Russell

Detlef Steuer --- http://fawn.unibw-hamburg.de/steuer.html
***** Encrypted mail preferred *****



From christoph.bier at web.de  Sun Oct 12 10:33:17 2003
From: christoph.bier at web.de (Christoph Bier)
Date: Sun, 12 Oct 2003 10:33:17 +0200
Subject: [R] Integration between R & latex
In-Reply-To: <20031012100140.6b741f5f.steuer@unibw-hamburg.de>
References: <HMMV00$1D080D67DB9E5BFB1C89620EF0345764@libero.it>
	<20031012100140.6b741f5f.steuer@unibw-hamburg.de>
Message-ID: <3F8911CD.9000808@web.de>

Detlef Steuer schrieb:
> Hi!
> 
> Most probably you did not install Hmisc on your system. It
> is  not included in R-base. You find it on CRAN.
> (cran.r-project.org -> source for contributed packages)
> 
> After installing it for your system the examples may well
> work.

Hi,

i just tried to install Hmisc with

 > install.packages(Hmisc,installWithVers = true)

but get the following error:

Error in unique(pkgs) : Object "Hmisc" not found

It searches in
http://cran.r-project.org/src/contrib/PACKAGES
where Hmisc *is* listed. Did *I* make a mistake
or is there something else wrong?

Greetings,

Christoph
-- 
Christoph Bier, Dipl.Oecotroph., Email: bier at wiz.uni-kassel.de
Universitaet Kassel, FG Oekologische Lebensmittelqualitaet und
Ernaehrungskultur \\ Postfach 12 52 \\ 37202 Witzenhausen
Tel.: +49 (0) 55 42 / 98 -17 21, Fax: -17 13



From glaziou at pasteur-kh.org  Sun Oct 12 10:41:13 2003
From: glaziou at pasteur-kh.org (Philippe Glaziou)
Date: Sun, 12 Oct 2003 15:41:13 +0700
Subject: [R] Integration between R & latex
In-Reply-To: <3F8911CD.9000808@web.de>
References: <HMMV00$1D080D67DB9E5BFB1C89620EF0345764@libero.it>
	<20031012100140.6b741f5f.steuer@unibw-hamburg.de>
	<3F8911CD.9000808@web.de>
Message-ID: <20031012084113.GA26498@pasteur-kh.org>

Christoph Bier <christoph.bier at web.de> wrote:
> i just tried to install Hmisc with
> 
> > install.packages(Hmisc,installWithVers = true)
> 
> but get the following error:
> 
> Error in unique(pkgs) : Object "Hmisc" not found
> 
> It searches in
> http://cran.r-project.org/src/contrib/PACKAGES
> where Hmisc *is* listed. Did *I* make a mistake
> or is there something else wrong?


You may need to quote the Hmisc object's name:

> install.packages("Hmisc")

-- 
Philippe



From christoph.bier at web.de  Sun Oct 12 11:02:33 2003
From: christoph.bier at web.de (Christoph Bier)
Date: Sun, 12 Oct 2003 11:02:33 +0200
Subject: [R] Integration between R & latex
In-Reply-To: <20031012084113.GA26498@pasteur-kh.org>
References: <HMMV00$1D080D67DB9E5BFB1C89620EF0345764@libero.it>	<20031012100140.6b741f5f.steuer@unibw-hamburg.de>	<3F8911CD.9000808@web.de>
	<20031012084113.GA26498@pasteur-kh.org>
Message-ID: <3F8918A9.9040405@web.de>

Philippe Glaziou schrieb:

[...]

> You may need to quote the Hmisc object's name:
> 
> 
>>install.packages("Hmisc")

Yes, that's it -- nearly: I had also to
quote "installWithVers = true".

Thanks!

Greetings,

Christoph
-- 
Christoph Bier, Dipl.Oecotroph., Email: bier at wiz.uni-kassel.de
Universitaet Kassel, FG Oekologische Lebensmittelqualitaet und
Ernaehrungskultur \\ Postfach 12 52 \\ 37202 Witzenhausen
Tel.: +49 (0) 55 42 / 98 -17 21, Fax: -17 13



From ripley at stats.ox.ac.uk  Sun Oct 12 11:05:26 2003
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Sun, 12 Oct 2003 10:05:26 +0100 (BST)
Subject: [R] Integration between R & latex
In-Reply-To: <20031012084113.GA26498@pasteur-kh.org>
Message-ID: <Pine.LNX.4.44.0310121003550.15759-100000@gannet.stats>

On Sun, 12 Oct 2003, Philippe Glaziou wrote:

> Christoph Bier <christoph.bier at web.de> wrote:
> > i just tried to install Hmisc with
> > 
> > > install.packages(Hmisc,installWithVers = true)
> > 
> > but get the following error:
> > 
> > Error in unique(pkgs) : Object "Hmisc" not found
> > 
> > It searches in
> > http://cran.r-project.org/src/contrib/PACKAGES
> > where Hmisc *is* listed. Did *I* make a mistake
> > or is there something else wrong?
> 
> 
> You may need to quote the Hmisc object's name:
> 
> > install.packages("Hmisc")
> 

I also suggest leaving out installWithVers = true (which should be TRUE or 
FALSE) until you need it (unlikely any time soon).

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From rossini at blindglobe.net  Sun Oct 12 12:11:18 2003
From: rossini at blindglobe.net (A.J. Rossini)
Date: Sun, 12 Oct 2003 03:11:18 -0700
Subject: [R] Integration between R & latex
In-Reply-To: <HMMV00$1D080D67DB9E5BFB1C89620EF0345764@libero.it>
	(v.demart@libero.it's
	message of "Sun, 12 Oct 2003 09:19:12 +0200")
References: <HMMV00$1D080D67DB9E5BFB1C89620EF0345764@libero.it>
Message-ID: <85pth2kbp5.fsf@blindglobe.net>


See also 
    http://www.analytics.washington.edu/~rossini/course/cph-statcomp/

and lab 2, for a "walk-through" example (with sample files) of using
Sweave (both figures and tables get included).



"v\.demart\@libero\.it" <v.demart at libero.it> writes:

> As an R absolute beginner and an expert (very old) statistician and latex user,
> I'm interested in using R to produce AUTOMAGICALLY tables in latex format. I
> mean I would like to have the means to build an R procedure generating **FROM
> INSIDE** a table or a graph to be inserted directly into latex.
> I've read http://hesweb1.med.virginia.edu/biostat/s/doc/summary.pdf where the
> author speaks of S (not R) and Latex integration via a Hmisc library which
> allows to make automatic, calculated tables in latex.
> Unfortunately this library doesn't seem to be present in R (I tried to follow
> the example issuing a "library(Hmisc)" and then, plainly, "library()" but
> nothing with the same functions appeared).
>
> Any help on how to set up that integration? And, where can I find
> documentation/examples on that?
>
> Thanks
>
> Vittorio
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
>

-- 
rossini at u.washington.edu            http://www.analytics.washington.edu/ 
Biomedical and Health Informatics   University of Washington
Biostatistics, SCHARP/HVTN          Fred Hutchinson Cancer Research Center
UW (Tu/Th/F): 206-616-7630 FAX=206-543-3461 | Voicemail is unreliable
FHCRC  (M/W): 206-667-7025 FAX=206-667-4812 | use Email

CONFIDENTIALITY NOTICE: This e-mail message and any attachme...{{dropped}}



From tura at centroin.com.br  Sun Oct 12 12:13:51 2003
From: tura at centroin.com.br (Bernardo Rangel Tura)
Date: Sun, 12 Oct 2003 07:13:51 -0300
Subject: [R] stepAIC problem
In-Reply-To: <002c01c38b3e$a4292920$0b01a8c0@hirotohome>
References: <002c01c38b3e$a4292920$0b01a8c0@hirotohome>
Message-ID: <6.0.0.22.2.20031012071210.029dc100@pop.centroin.com.br>

At 09:46 05-10-2003, Hiroto Miyoshi wrote:
>Dear R-users
>
>I have a probelm running stepAIC in R1.7.1
>(...)
>--small example
>> library(MASS)
>> x1<-runif(100)
>> x2<-runif(100)
>> x3<-runif(100)
>> x4<-runif(100)
>> x5<-runif(100)
>> y<-x1+x2+x3+runif(100)
>> t<-data.frame(y=y,x1=x1,x2=x2,x3=x3,x4=x4,x5=x5)
>> x<-lm(y~x1+x2+x3+x4+x5,data=t)
>> stepAIC(x)
>Start:  AIC= -247.61
> y ~ x1 + x2 + x3 + x4 + x5
>
>       Df Sum of Sq      RSS      AIC
>- x5    1 3.747e-06    7.456 -249.608
>- x4    1     0.026    7.483 -249.254
><none>                 7.456 -247.609
>- x1    1     4.866   12.322 -199.375
>- x2    1     8.182   15.639 -175.543
>- x3    1     8.597   16.054 -172.922
>Error in as.data.frame.default(data) : can't coerce function into a
>data.frame

Hiroto,

In My computer with R 1.7.1 e R 1.8.0 (win 98 SE) don?t have this error... 


[]s
Tura



From christoph.bier at web.de  Sun Oct 12 12:57:07 2003
From: christoph.bier at web.de (Christoph Bier)
Date: Sun, 12 Oct 2003 12:57:07 +0200
Subject: [R] Integration between R & latex
In-Reply-To: <Pine.LNX.4.44.0310121100450.15823-100000@gannet.stats>
References: <Pine.LNX.4.44.0310121100450.15823-100000@gannet.stats>
Message-ID: <3F893383.70103@web.de>

Prof Brian Ripley schrieb:
> On Sun, 12 Oct 2003, Christoph Bier wrote:

[...]

>> Ok, I'll keep that in mind. But may I ask why you suggest
>> leaving it out?
> 
> 
> That's the wrong question: you need to say why you intend
> to include it. Givne that you don't know the difference
> between true and TRUE, I guess you are a naive R user and
> this would be a needless complication. (Maybe 0.1% of R
> users make use it.)

Yes, I'm a quite new R user and yet I don't
know the difference between true and TRUE.
But I read on the manuals.

Greetings,

Christoph
-- 
Christoph Bier, Dipl.Oecotroph., Email: bier at wiz.uni-kassel.de
Universitaet Kassel, FG Oekologische Lebensmittelqualitaet und
Ernaehrungskultur \\ Postfach 12 52 \\ 37202 Witzenhausen
Tel.: +49 (0) 55 42 / 98 -17 21, Fax: -17 13



From rossini at blindglobe.net  Sun Oct 12 14:12:03 2003
From: rossini at blindglobe.net (A.J. Rossini)
Date: Sun, 12 Oct 2003 05:12:03 -0700
Subject: [R] Integration between R & latex
In-Reply-To: <HMMV00$1D080D67DB9E5BFB1C89620EF0345764@libero.it>
	(v.demart@libero.it's
	message of "Sun, 12 Oct 2003 09:19:12 +0200")
References: <HMMV00$1D080D67DB9E5BFB1C89620EF0345764@libero.it>
Message-ID: <85zng6irjg.fsf@blindglobe.net>

Whoops.  Should be courses, not course, below.


See also 
    http://www.analytics.washington.edu/~rossini/course/cph-statcomp/

and lab 2, for a "walk-through" example (with sample files) of using
Sweave (both figures and tables get included).



"v\.demart\@libero\.it" <v.demart at libero.it> writes:

> As an R absolute beginner and an expert (very old) statistician and latex user,
> I'm interested in using R to produce AUTOMAGICALLY tables in latex format. I
> mean I would like to have the means to build an R procedure generating **FROM
> INSIDE** a table or a graph to be inserted directly into latex.
> I've read http://hesweb1.med.virginia.edu/biostat/s/doc/summary.pdf where the
> author speaks of S (not R) and Latex integration via a Hmisc library which
> allows to make automatic, calculated tables in latex.
> Unfortunately this library doesn't seem to be present in R (I tried to follow
> the example issuing a "library(Hmisc)" and then, plainly, "library()" but
> nothing with the same functions appeared).
>
> Any help on how to set up that integration? And, where can I find
> documentation/examples on that?
>
> Thanks
>
> Vittorio
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
>

-- 
rossini at u.washington.edu            http://www.analytics.washington.edu/ 
Biomedical and Health Informatics   University of Washington
Biostatistics, SCHARP/HVTN          Fred Hutchinson Cancer Research Center
UW (Tu/Th/F): 206-616-7630 FAX=206-543-3461 | Voicemail is unreliable
FHCRC  (M/W): 206-667-7025 FAX=206-667-4812 | use Email

CONFIDENTIALITY NOTICE: This e-mail message and any attachme...{{dropped}}



From Ted.Harding at nessie.mcc.ac.uk  Sun Oct 12 14:44:27 2003
From: Ted.Harding at nessie.mcc.ac.uk ( (Ted Harding))
Date: Sun, 12 Oct 2003 13:44:27 +0100 (BST)
Subject: [R] Automatic re-looping after error
In-Reply-To: <Pine.A41.4.58.0310091150270.42270@homer40.u.washington.edu>
Message-ID: <XFMail.031012134427.Ted.Harding@nessie.mcc.ac.uk>

On 12-Oct-03 Andy Liaw wrote:
> It's in R-1.8.0, released October 8th.

On 09-Oct-03 Thomas Lumley wrote:
> On Thu, 9 Oct 2003 Ted.Harding at nessie.mcc.ac.uk wrote:
> 
>> All that's really needed to cope with the situation is for R to
>> drop that cycle of the loop, and resume with a new cycle.
>>
>> However, I'm wondering how to set this up. I've had a look at
>> try(), and I'm not at all sure that it does what I would want.
>> What I'd really like is something (inside the loop) on the lines of
>>
>>    on.error(maybe some parameters X) break
>>
>> where X might specify what sort of error or what function it comes
>> from. Would setting
>>
>>   options(error = break )
>>
> 
> I don't think so.  You may need to look at the new exception-handling
> code (start with help(tryCatch)).

I've now installed R-1.8.0 which does include 'tryCatch' and related
things, but from the look of it I'll have to study it a bit before I see
how it all works!

Meanwhile, thanks to others (Spencer Graves, Achim Zeileis, as well as
Thomas Lumley and Andy Liaw) for suggestions. Though I'd already looked
at "try", I thought I'd have another go. It turns out I'd been muddled
about testing the result of 'try' in the right way.

In fact, if 'myfun(...)' might fail in a loop, then

  result <- try(myfun(...));
  if(class(result)=="try-error") next ;

will have the effect of breaking out of the current cycle of the
loop and starting a new cycle. Otherwise 'result' will be a valid
returned value from 'myfun'. This is exactly what I had wanted.

Thanks to all for the help!
Ted.


--------------------------------------------------------------------
E-Mail: (Ted Harding) <Ted.Harding at nessie.mcc.ac.uk>
Fax-to-email: +44 (0)870 167 1972
Date: 12-Oct-03                                       Time: 13:44:27
------------------------------ XFMail ------------------------------



From kjetil at entelnet.bo  Sun Oct 12 15:55:00 2003
From: kjetil at entelnet.bo (kjetil@entelnet.bo)
Date: Sun, 12 Oct 2003 09:55:00 -0400
Subject: [R] Rd problems --- followup
Message-ID: <3F8924F4.2848.8D0A52@localhost>

I should'nt have sent the last mail so fast. 

Same problem with

\eqn{u_j = a_j + b x + c x^2, \quad  j=1, \ldots, r-1}
        {u[j] = a[j] + b*x + c*x^2 j = 1,\dots,r-1}

I thought the problem in the first case could have to do with
the use use of \mbox{}  (with the braces) within the arguments 
of \eqn, but here there are none braces in the arguments of \eqn{}{}.

Another case giving the same problem is 

 \eqn{(k(n-1))\times (k(n-1))}
        {( k*(n-1) ) X ( k*(n-1) )}


Kjetil Halvorsen



From kjetil at entelnet.bo  Sun Oct 12 15:55:00 2003
From: kjetil at entelnet.bo (kjetil@entelnet.bo)
Date: Sun, 12 Oct 2003 09:55:00 -0400
Subject: [R] Rd problems
Message-ID: <3F8924F4.3351.8D09F5@localhost>

Hola!

I have the following in a .Rd file:

 \eqn{\mbox{coef} = c(\mbox{coef}[1],\ldots, \mbox{coef}[n]) }
        {coef = c(coef[1], coef[2], \dots, coef[n])}

However, both arguments come out in the latex file!

Whats happening?

Kjetil Halvorsen



From Timur.Elzhov at jinr.ru  Sun Oct 12 17:04:05 2003
From: Timur.Elzhov at jinr.ru (Timur Elzhov)
Date: Sun, 12 Oct 2003 19:04:05 +0400
Subject: [R] R graphics + non-R langs
Message-ID: <20031012150405.GA13060@nf034.jinr.ru>

Hello, dear R experts!

One of the first reasons I began to use R, was its beautiful
plotting capabilities. I found them very nice and simple. But
at the last time I consider another languages (Perl, Python) for
coding my scientific applications. So for now, in order to plot graphics
I have to write data to the disk, launch R separately, and run R script
just to plot my data.

I gave a glance at the other scientific plotting libraries (plplot,
pgplot, dislin) - all of them support API of many languages.
The question is: what about R graphics? Is it so hard to bind it with
any non-R languages?

Thank you!

--
WBR,
Timur V. Elzhov



From ligges at statistik.uni-dortmund.de  Sun Oct 12 17:32:13 2003
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Sun, 12 Oct 2003 17:32:13 +0200
Subject: [R] R graphics + non-R langs
In-Reply-To: <20031012150405.GA13060@nf034.jinr.ru>
References: <20031012150405.GA13060@nf034.jinr.ru>
Message-ID: <3F8973FD.50302@statistik.uni-dortmund.de>

Timur Elzhov wrote:

> Hello, dear R experts!
> 
> One of the first reasons I began to use R, was its beautiful
> plotting capabilities. I found them very nice and simple. But
> at the last time I consider another languages (Perl, Python) for
> coding my scientific applications. So for now, in order to plot graphics
> I have to write data to the disk, launch R separately, and run R script
> just to plot my data.
> 
> I gave a glance at the other scientific plotting libraries (plplot,
> pgplot, dislin) - all of them support API of many languages.
> The question is: what about R graphics? Is it so hard to bind it with
> any non-R languages?

You are looking for the the packages RSPython and RSPerl from the 
Omegahat project http://www.omegahat.org/

Uwe Ligges



From borgulya at gyer2.sote.hu  Sun Oct 12 17:47:30 2003
From: borgulya at gyer2.sote.hu (Gabor Borgulya)
Date: 12 Oct 2003 17:47:30 +0200
Subject: [R] R graphics + non-R langs
In-Reply-To: <20031012150405.GA13060@nf034.jinr.ru>
References: <20031012150405.GA13060@nf034.jinr.ru>
Message-ID: <1065973649.2012.9.camel@catv-d5de952e.bp04catv.broadband.hu>

2003-10-12, v keltez?ssel Timur Elzhov ezt ?rta:
> at the last time I consider another languages (Perl, Python) for

> The question is: what about R graphics? Is it so hard to bind it with
> any non-R languages?

Try RPy (R from Python)!
http://rpy.sourceforge.net/

G?bor



From tlumley at u.washington.edu  Sun Oct 12 17:46:37 2003
From: tlumley at u.washington.edu (Thomas Lumley)
Date: Sun, 12 Oct 2003 08:46:37 -0700 (PDT)
Subject: [R] Automatic re-looping after error
In-Reply-To: <XFMail.031011105234.Ted.Harding@nessie.mcc.ac.uk>
References: <XFMail.031011105234.Ted.Harding@nessie.mcc.ac.uk>
Message-ID: <Pine.A41.4.58.0310120846130.249570@homer01.u.washington.edu>

On Sat, 11 Oct 2003 Ted.Harding at nessie.mcc.ac.uk wrote:
> Thanks, Thomas.
> I don't seem to have anything related to this in R-1.7.1 (16/06/03).
> However, some web-searching finally tracked down
>
> http://stat.ethz.ch/R-manual/R-devel/library/base/html/conditions.html
>
> so what would be involved in making this stuff available? Upgrade to
> current R? Install a beta-version?
>

Update to the current R.  It was introduced in 1.8.0

	-thomas



From christoph.bier at web.de  Sun Oct 12 19:57:41 2003
From: christoph.bier at web.de (Christoph Bier)
Date: Sun, 12 Oct 2003 19:57:41 +0200
Subject: [R] Integration between R & latex
In-Reply-To: <3F893A09.2020707@pburns.seanet.com>
References: <Pine.LNX.4.44.0310121100450.15823-100000@gannet.stats>
	<3F893383.70103@web.de> <3F893A09.2020707@pburns.seanet.com>
Message-ID: <3F899615.2060008@web.de>

Patrick Burns schrieb:
> You may be interested in "A Guide for the Unwilling S
> User" and chapter 1 of S Poetry.

I already found this Guide but didn't felt
addressed, because using R is my choice,
so I'm not unwilling. Maybe a misestimation.
Before I made the decision to migrate to R
I read a book about S and S-Plus in german
and some introductions found in the web.
At the moment I'm waiting for "Introductory
Statistics with R" by Peter Dalgaard. I hope
our bookseller is "fast" enough, because I'm
pressed for time. Who's not ... =)?
    Nevertheless thanks for the hint, I will
have a look at this guide!

Greetings,

Christoph
-- 
Christoph Bier, Dipl.Oecotroph., Email: bier at wiz.uni-kassel.de
Universitaet Kassel, FG Oekologische Lebensmittelqualitaet und
Ernaehrungskultur \\ Postfach 12 52 \\ 37202 Witzenhausen
Tel.: +49 (0) 55 42 / 98 -17 21, Fax: -17 13



From kjetil at entelnet.bo  Sun Oct 12 20:21:06 2003
From: kjetil at entelnet.bo (kjetil@entelnet.bo)
Date: Sun, 12 Oct 2003 14:21:06 -0400
Subject: [R] Rd problems
Message-ID: <3F896352.20279.180A992@localhost>

I am running 
Rcmd check          (Windows XP, rw1080 from cran)
on a new package. 

This reports "undocumented code objects" for 14 functions, 
which all have their .Rd files!

What might be happening?

Kjetil Halvorsen



From ripley at stats.ox.ac.uk  Sun Oct 12 20:21:33 2003
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Sun, 12 Oct 2003 19:21:33 +0100 (BST)
Subject: [R] Rd problems --- followup
In-Reply-To: <3F8924F4.2848.8D0A52@localhost>
Message-ID: <Pine.LNX.4.44.0310121917240.16457-100000@gannet.stats>

I don't think you are allowed a line break in }{ : you certainly are
not in other circumstances.  Please try it without, as your example works 
for me without the (probably) illegal line break.


On Sun, 12 Oct 2003 kjetil at entelnet.bo wrote:

> I should'nt have sent the last mail so fast. 
> 
> Same problem with
> 
> \eqn{u_j = a_j + b x + c x^2, \quad  j=1, \ldots, r-1}
>         {u[j] = a[j] + b*x + c*x^2 j = 1,\dots,r-1}
> 
> I thought the problem in the first case could have to do with
> the use use of \mbox{}  (with the braces) within the arguments 
> of \eqn, but here there are none braces in the arguments of \eqn{}{}.
> 
> Another case giving the same problem is 
> 
>  \eqn{(k(n-1))\times (k(n-1))}
>         {( k*(n-1) ) X ( k*(n-1) )}
> 
> 
> Kjetil Halvorsen

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From ligges at statistik.uni-dortmund.de  Sun Oct 12 21:45:17 2003
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Sun, 12 Oct 2003 21:45:17 +0200
Subject: [R] Rd problems
References: <3F896352.20279.180A992@localhost>
Message-ID: <3F89AF4D.A5B78AB@statistik.uni-dortmund.de>

kjetil at entelnet.bo wrote:
> 
> I am running
> Rcmd check          (Windows XP, rw1080 from cran)
> on a new package.
> 
> This reports "undocumented code objects" for 14 functions,
> which all have their .Rd files!
> 
> What might be happening?

1) You forgot to set an \alias{} (most probable)
2) There is another error in the Rd files 
3) There is a bug in R (less probable)

At first check points 1-2) from above, after that repeat the complete
output of Rcmd check and provide a minimal version of one of your Rd
files which does not work.

Uwe Ligges


> Kjetil Halvorsen
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help



From rpeng at jhsph.edu  Sun Oct 12 21:52:24 2003
From: rpeng at jhsph.edu (Roger D. Peng)
Date: Sun, 12 Oct 2003 15:52:24 -0400
Subject: [R] Rd problems
In-Reply-To: <3F896352.20279.180A992@localhost>
References: <3F896352.20279.180A992@localhost>
Message-ID: <3F89B0F8.2010703@jhsph.edu>

Do the functions all have proper \alias entries?

-roger

kjetil at entelnet.bo wrote:
> I am running 
> Rcmd check          (Windows XP, rw1080 from cran)
> on a new package. 
> 
> This reports "undocumented code objects" for 14 functions, 
> which all have their .Rd files!
> 
> What might be happening?
> 
> Kjetil Halvorsen
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
>



From ligges at statistik.uni-dortmund.de  Sun Oct 12 21:58:06 2003
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Sun, 12 Oct 2003 21:58:06 +0200
Subject: [R] evaluating R expressions from C
References: <3F8875AE.1030001@stat.tamu.edu>
Message-ID: <3F89B24E.A5F5E84E@statistik.uni-dortmund.de>



Iryna Lobach wrote:
> 
> Hello!
> 
> I've looked at R help and previous postings, but I am not sure I
> completely understand the mechanism of evaluating R expressions from C++.

C or C++ (the latter is a bit more tricky)?
Have you read the manual "Writing R Extensions"? 


> They have function
> 
> SEXP eval(SEXP expr, SEXP rho),
> 
> but I can't get it to work.
> 
> Could anyone who used it give any comments on how efficient it is and,
> if possible, give example of how to use it. My main problem is that I
> don't really understand how pass the name of  R-function and it's
> parameters.

See Section 4.9, "Evaluating R expressions from C", of the manual
"Writing R Extension". There is an example just after the line you gave
above.

Re efficiency: In most circumstances it is faster than in R itself (if
not, nobody would like to do it in C!). As always, it depends on your
problem ...

Uwe Ligges



> Thank you,
> Iryna
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help



From ligges at statistik.uni-dortmund.de  Sun Oct 12 22:21:13 2003
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Sun, 12 Oct 2003 22:21:13 +0200
Subject: [R] Subclassing lm
References: <34249.64.24.105.152.1065894197.squirrel@webmail.fpcc.net>
Message-ID: <3F89B7B9.7CA1CA84@statistik.uni-dortmund.de>

rlee at fpcc.net wrote:
> 
> I'd trying to subclass the "lm" class to produce a "mylm" class whose
> instances behave like lm objects (are accepted by methods like summary.lm)
> but have additional data or slots of my own design.
> 
> For starters:
> 
> setClass("mylm", "lm")
> 
> produces the somewhat cryptic:
>  Warning message:
> Old-style (``S3'') class "mylm" supplied as a superclass of "mylm", but no
> automatic conversion will be peformed for S3 classes in:
> .validDataPartClass(cl, name)
> 
> What does this imply?
> 

I've never mixed S3 and S4 methods, so excuse my ignorance here (and I'm
only answering because I haven't seen another reply).

Since there is no formal definition of a representation of "lm" (an S3
class!): Does it really make sense for you to extend it with S4 methods?
If not, I'd rather use the "old" construct to tell the object about
inheritance:
class(anymylmobject) <- c("mylm", "lm")

Anyway, what you are looking for is 
  ?setOldClass
in my expectation, as in:
  setOldClass(c("mylm", "lm"))
Reading that help file carefully might help. 
In particular, you can get a bit of S4 behaviour (e.g. you can defining
S4 Classes and Methods), but you'll "just" get a virtual class, so the
requested definition of formal slots will be still a problem.

Uwe Ligges



From s.mcclatchie at niwa.co.nz  Sun Oct 12 23:25:48 2003
From: s.mcclatchie at niwa.co.nz (Sam McClatchie)
Date: Mon, 13 Oct 2003 10:25:48 +1300
Subject: [R] plot/ layout/ overlay problem
Message-ID: <3F89C6DC.6060508@niwa.co.nz>

System info:
Red Hat 9.0
R Version 1.7.0
ESS 5.1.21
Emacs 21.2.1
-------------------
Colleagues

I have a small problem with positioning overlays using layout.

  ## Purpose: to plot temperature and salinity profiles
   ## as a multipanel figure, and
   ## overlay the the mixed
   ## layer depths.

Here is the code skeleton:
   --------------------
   nf <- layout(matrix(c(1,2,3,4,5,6,7,8,9,10,11,
                         12,13,14,15,16,17,18,19,20,21,22),
                       2,11,byrow=TRUE))

   par.old <- par(mai=c(1,0.5,1,0))
   for (i in 1:22){
     ## plot temperature profiles (solid line)
     plot(...

     ## plot sigma t profiles (dashed line)
     par("new"=TRUE)
     plot( ...
   }

   ## overlay the mixed layer depths
   nf <- layout(matrix(c(1,2),
                       2,1,byrow=TRUE))

   par("new"=TRUE)
   test.x <- c(0,1,2,3,4,5,6,7,8,9,10,11)
   test.y <- c(90,100,110,200,200,250,300,350,330,370,330,370)
   plot(test.x,test.y, ylim=c(1000,0), xlim=c(0,11),type='b',
        axes=F,lty=3)

   par(par.old)
-----------------------
The problem is that the second overlay comes up in the bottom panel 
(where the last of the 22 panels ended).

The question is: how do I get the second overlay to start in the top 
panel where the first of the 22 panels began?

Best fishes

Sam
-- 
Sam McClatchie, Research scientist (fisheries acoustics))))))))))
NIWA (National Institute of Water & Atmospheric Research Ltd)
PO Box 14 901, Kilbirnie, Wellington, New Zealand
s.mcclatchie at niwa.cri.nz
Research home page <http://www.smcc.150m.com/>
                          /\
               >><xX(&>
                       /// \\\
                      //// \\\\
                     ///  <%)Xx><<
                    /////  \\\\\\
              ><(((@>
        ><(((%>     ..>><xX(?>O<?)Xx><<



From p.murrell at auckland.ac.nz  Mon Oct 13 00:16:30 2003
From: p.murrell at auckland.ac.nz (Paul Murrell)
Date: Mon, 13 Oct 2003 11:16:30 +1300
Subject: [R] plot/ layout/ overlay problem
References: <3F89C6DC.6060508@niwa.co.nz>
Message-ID: <3F89D2BE.1050600@stat.auckland.ac.nz>

Hi


Sam McClatchie wrote:
> System info:
> Red Hat 9.0
> R Version 1.7.0
> ESS 5.1.21
> Emacs 21.2.1
> -------------------
> Colleagues
> 
> I have a small problem with positioning overlays using layout.
> 
>  ## Purpose: to plot temperature and salinity profiles
>   ## as a multipanel figure, and
>   ## overlay the the mixed
>   ## layer depths.
> 
> Here is the code skeleton:
>   --------------------
>   nf <- layout(matrix(c(1,2,3,4,5,6,7,8,9,10,11,
>                         12,13,14,15,16,17,18,19,20,21,22),
>                       2,11,byrow=TRUE))
> 
>   par.old <- par(mai=c(1,0.5,1,0))
>   for (i in 1:22){
>     ## plot temperature profiles (solid line)
>     plot(...
> 
>     ## plot sigma t profiles (dashed line)
>     par("new"=TRUE)
>     plot( ...
>   }
> 
>   ## overlay the mixed layer depths
>   nf <- layout(matrix(c(1,2),
>                       2,1,byrow=TRUE))
> 
>   par("new"=TRUE)
>   test.x <- c(0,1,2,3,4,5,6,7,8,9,10,11)
>   test.y <- c(90,100,110,200,200,250,300,350,330,370,330,370)
>   plot(test.x,test.y, ylim=c(1000,0), xlim=c(0,11),type='b',
>        axes=F,lty=3)
> 
>   par(par.old)
> -----------------------
> The problem is that the second overlay comes up in the bottom panel 
> (where the last of the 22 panels ended).
> 
> The question is: how do I get the second overlay to start in the top 
> panel where the first of the 22 panels began?


The problem is that when you try to do your "second overlay" you do not 
end up calling plot.new() so it does not move to the next plotting 
region.  After the layout() call you are actually sitting in the last 
layout plotting region (number 2 in this case -- this is done so that 
the next plot.new() call will "wrap" to the first plotting region). 
This is why you end up drawing in the bottom panel.

A very nasty hack to get what you want is to change your layout so that 
the "last" plotting region is actually the one you want, as follows ...

   nf <- layout(matrix(c(2,1),
                       2,1,byrow=TRUE))


That is a very nasty way to do it, plus I wonder how well your second 
overlay lines up with the individual plots(?).  I think we could achieve 
something much nicer using some of grid's features (there is now a 
package called gridBase on CRAN for combining grid with base plots).  I 
would be very interested to hear more about what you are trying to do 
and would like to help achieve it -- please contact me directly if you 
are interested.

Paul
-- 
Dr Paul Murrell
Department of Statistics
The University of Auckland
Private Bag 92019
Auckland
New Zealand
64 9 3737599 x85392
paul at stat.auckland.ac.nz
http://www.stat.auckland.ac.nz/~paul/



From wang at galton.uchicago.edu  Sat Oct 11 20:35:06 2003
From: wang at galton.uchicago.edu (Yong Wang)
Date: Sat, 11 Oct 2003 13:35:06 -0500 (CDT)
Subject: [R] how to seperate R's input and output screen?
In-Reply-To: <200310101004.h9AA3CqE023249@stat.math.ethz.ch>
Message-ID: <Pine.GSO.4.05.10310111332100.10258-100000@aitken.uchicago.edu>

Dear all
can anybady tell me how to seperate R's input and output screen?I mean
just like SAS or some others, the commands does not mixed with results.
thank you



From jimenez at ce.berkeley.edu  Mon Oct 13 03:36:49 2003
From: jimenez at ce.berkeley.edu (Rafael Jimenez)
Date: Sun, 12 Oct 2003 18:36:49 -0700 (PDT)
Subject: [R] Use of outer
Message-ID: <Pine.LNX.4.58.0310121825080.16096@jimenez.ce.berkeley.edu>

Dear list,

I have been trying to use the function outer to make contour plots of a
cost function for regression analysis with no success. (see commented
code below).

Could anybody give me some advice on what I am doing wrong, or what is
relevant for me to read?

Thanks,
Rafael

## ============================================= CODE
## Create vectors and matrix
Y <- c(0.5116403,2.4055245,1.6596707,1.8286057,2.6119199)

col.1 <- c(0.8997692,0.8216292,0.6449104,0.8179743,0.6602276)
col.2 <- c(0.01175669,0.89389797,0.19913807,0.29872301,0.66144258)

X <- cbind(col.1, col.2)

## Define function
## Give default values so that it "takes two arguments"
J <- function (x1, x2, X.val=X, Y.val=Y) {
    t(Y.val-X.val[,1]*x1-X.val[,2]*x2) %*% (Y.val-X.val[,1]*x1-X.val[,2]*x2)
}

## Get grid to plot
theta.1 <- array(seq(-1, 1, length=10))
theta.2 <- theta.1

## How to do this with outer?
z.matrix <- outer(theta.1, theta.2, "J")

## Plot
contour(theta.1, theta.2, z.matrix)



From roxburg.david at qx.net  Mon Oct 13 03:45:08 2003
From: roxburg.david at qx.net (David Allen)
Date: Sun, 12 Oct 2003 21:45:08 -0400
Subject: [R] regression
Message-ID: <6.0.0.22.2.20031012213503.01abf398@mail.qx.net>

After calling function lm one can use the as.matrix function on anova to 
get the numbers
out of an analysis of variance table and output latex code for a nicely 
formatted table.

I would like to do a similar thing with regression coefficients, standard 
errors, and
p-values, etc. I have not been able to. Is it possible?

David Allen



From spencer.graves at pdf.com  Mon Oct 13 04:43:57 2003
From: spencer.graves at pdf.com (Spencer Graves)
Date: Sun, 12 Oct 2003 19:43:57 -0700
Subject: [R] Use of outer
In-Reply-To: <Pine.LNX.4.58.0310121825080.16096@jimenez.ce.berkeley.edu>
References: <Pine.LNX.4.58.0310121825080.16096@jimenez.ce.berkeley.edu>
Message-ID: <3F8A116D.7010301@pdf.com>

I get

 > var(as.vector(z.matrix))
[1] 0

I got sensible results from the following: 

 z.matrix <- outer(theta.1, theta.2, "+")
 >
 > ## Plot
 > contour(theta.1, theta.2, z.matrix)

hope this helps.  spencer graves

Rafael Jimenez wrote:

>Dear list,
>
>I have been trying to use the function outer to make contour plots of a
>cost function for regression analysis with no success. (see commented
>code below).
>
>Could anybody give me some advice on what I am doing wrong, or what is
>relevant for me to read?
>
>Thanks,
>Rafael
>
>## ============================================= CODE
>## Create vectors and matrix
>Y <- c(0.5116403,2.4055245,1.6596707,1.8286057,2.6119199)
>
>col.1 <- c(0.8997692,0.8216292,0.6449104,0.8179743,0.6602276)
>col.2 <- c(0.01175669,0.89389797,0.19913807,0.29872301,0.66144258)
>
>X <- cbind(col.1, col.2)
>
>## Define function
>## Give default values so that it "takes two arguments"
>J <- function (x1, x2, X.val=X, Y.val=Y) {
>    t(Y.val-X.val[,1]*x1-X.val[,2]*x2) %*% (Y.val-X.val[,1]*x1-X.val[,2]*x2)
>}
>
>## Get grid to plot
>theta.1 <- array(seq(-1, 1, length=10))
>theta.2 <- theta.1
>
>## How to do this with outer?
>z.matrix <- outer(theta.1, theta.2, "J")
>
>## Plot
>contour(theta.1, theta.2, z.matrix)
>
>______________________________________________
>R-help at stat.math.ethz.ch mailing list
>https://www.stat.math.ethz.ch/mailman/listinfo/r-help
>  
>



From spencer.graves at pdf.com  Mon Oct 13 04:56:23 2003
From: spencer.graves at pdf.com (Spencer Graves)
Date: Sun, 12 Oct 2003 19:56:23 -0700
Subject: [R] regression
In-Reply-To: <6.0.0.22.2.20031012213503.01abf398@mail.qx.net>
References: <6.0.0.22.2.20031012213503.01abf398@mail.qx.net>
Message-ID: <3F8A1457.8010403@pdf.com>

It can help you and help us if you provide a toy example that 
illustrates the problem.  Consider the following: 

df1 <- data.frame(x=1:6, y=rep(1:3, 2))
fit <- lm(y~x, df1)

In previous consideration of problems of this nature, I learned to 
consider "summary" and "attributes(summary(fit))":  : 

 > Sum <- summary(fit)
 > attributes(Sum)
$names
 [1] "call"          "terms"         "residuals"     "coefficients"
 [5] "sigma"         "df"            "r.squared"     "adj.r.squared"
 [9] "fstatistic"    "cov.unscaled"

After trying several things, I discovered the following: 

 > as.matrix(coefficients(Sum))
             Estimate Std. Error  t value  Pr(>|t|)
(Intercept) 1.2000000  0.8176622 1.467599 0.2161194
x           0.2285714  0.2099563 1.088662 0.3375019

Does this answer your question? 
spencer graves

David Allen wrote:

> After calling function lm one can use the as.matrix function on anova 
> to get the numbers
> out of an analysis of variance table and output latex code for a 
> nicely formatted table.
>
> I would like to do a similar thing with regression coefficients, 
> standard errors, and
> p-values, etc. I have not been able to. Is it possible?
>
> David Allen
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help



From jimenez at ce.berkeley.edu  Mon Oct 13 05:50:06 2003
From: jimenez at ce.berkeley.edu (Rafael Jimenez)
Date: Sun, 12 Oct 2003 20:50:06 -0700 (PDT)
Subject: [R] Use of outer
In-Reply-To: <3F8A116D.7010301@pdf.com>
References: <Pine.LNX.4.58.0310121825080.16096@jimenez.ce.berkeley.edu>
	<3F8A116D.7010301@pdf.com>
Message-ID: <Pine.LNX.4.58.0310122044420.16096@jimenez.ce.berkeley.edu>


On Sun, 12 Oct 2003, Spencer Graves wrote:

> I get
>
>  > var(as.vector(z.matrix))
> [1] 0
>
> I got sensible results from the following:
>
>  z.matrix <- outer(theta.1, theta.2, "+")
>  >
>  > ## Plot
>  > contour(theta.1, theta.2, z.matrix)
>
> hope this helps.  spencer graves

Thanks for the pointer.

A constant matrix is obtained with my call to 'outer'. However, the call
to outer seems to work fine when a function that takes two arguments
only is used. I guess the problem is in my definition of the function
'J', which depends on four arguments, but I couldn't figure out how to
make it work.

-Rafael

[...]
> >## ============================================= CODE
> >## Create vectors and matrix
> >Y <- c(0.5116403,2.4055245,1.6596707,1.8286057,2.6119199)
> >
> >col.1 <- c(0.8997692,0.8216292,0.6449104,0.8179743,0.6602276)
> >col.2 <- c(0.01175669,0.89389797,0.19913807,0.29872301,0.66144258)
> >
> >X <- cbind(col.1, col.2)
> >
> >## Define function
> >## Give default values so that it "takes two arguments"
> >J <- function (x1, x2, X.val=X, Y.val=Y) {
> >    t(Y.val-X.val[,1]*x1-X.val[,2]*x2) %*% (Y.val-X.val[,1]*x1-X.val[,2]*x2)
> >}
> >
> >## Get grid to plot
> >theta.1 <- array(seq(-1, 1, length=10))
> >theta.2 <- theta.1
> >
> >## How to do this with outer?
> >z.matrix <- outer(theta.1, theta.2, "J")
> >
> >## Plot
> >contour(theta.1, theta.2, z.matrix)
> >
> >______________________________________________
> >R-help at stat.math.ethz.ch mailing list
> >https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> >
> >
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
>



From maechler at stat.math.ethz.ch  Mon Oct 13 08:41:21 2003
From: maechler at stat.math.ethz.ch (Martin Maechler)
Date: Mon, 13 Oct 2003 08:41:21 +0200
Subject: [R] R sometimes dies from within Perl
In-Reply-To: <20B7EB075F2D4542AFFAF813E98ACD9301C00BD7@cl-exsrv1.irad.bbsrc.ac.uk>
References: <20B7EB075F2D4542AFFAF813E98ACD9301C00BD7@cl-exsrv1.irad.bbsrc.ac.uk>
Message-ID: <16266.18705.415436.305771@gargle.gargle.HOWL>

>>>>> "michael" == michael watson (IAH-C) <michael.watson at bbsrc.ac.uk>
>>>>>     on Fri, 10 Oct 2003 16:11:01 +0100 writes:

    michael> Hi Guys This is a combined Perl/R question,
    michael> hopefully there are enough people out there who use
    michael> both to help me.  I am using the latest versions of
    michael> both on a SUSE Linux 8.2 machine.

    michael> I am wrapping up R in some very simple perl
    michael> scripts.  My problem is that when I run a series of
    michael> commands from within R, with a certain data set,
    michael> one of the commands reports an error but the R
    michael> process remains active.

    michael> However, if I open a pipe within Perl (using
    michael> 'open(R, "| R");') and then pipe the very same
    michael> commands to R using 'print R "<commands>";', then
    michael> when the error occurs, the R process dies and i get
    michael> a message "Execution halted".

    michael> Can anyone think of a reason why R doesn't crash
    michael> normally, but does when controlled from within
    michael> perl?

it doesn't crash, but it stops ("gracefully") because your code
produced an error.

This is the difference between "batch" and interactive mode.
In batch mode, R stops on errors {very reasonably in most
cases!}, in interactive mode it doesn't.

In a shell :
	 echo 'interactive()' | R --slave
gives 
[1] FALSE

You can use try(...) for things that produce errors (but make
sure to deal with  the result of try(.) in error cases), or
since 1.8.0 and inside functions even more generally
tryCatch() and "friends".

Regards,
-- 
Martin Maechler <maechler at stat.math.ethz.ch>	http://stat.ethz.ch/~maechler/
Seminar fuer Statistik, ETH-Zentrum  LEO C16	Leonhardstr. 27
ETH (Federal Inst. Technology)	8092 Zurich	SWITZERLAND
phone: x-41-1-632-3408		fax: ...-1228			<><



From maechler at stat.math.ethz.ch  Mon Oct 13 08:45:18 2003
From: maechler at stat.math.ethz.ch (Martin Maechler)
Date: Mon, 13 Oct 2003 08:45:18 +0200
Subject: [R] Rd problems
In-Reply-To: <3F8924F4.3351.8D09F5@localhost>
References: <3F8924F4.3351.8D09F5@localhost>
Message-ID: <16266.18942.112041.300104@gargle.gargle.HOWL>

>>>>> "kjetil" == kjetil halvorsen <kjetil at entelnet.bo>
>>>>>     on Sun, 12 Oct 2003 09:55:00 -0400 writes:

    kjetil> Hola!  I have the following in a .Rd file:

    kjetil> \eqn{\mbox{coef} = c(\mbox{coef}[1],\ldots, \mbox{coef}[n]) }
    kjetil>       {coef = c(coef[1], coef[2], \dots, coef[n])}

    kjetil> However, both arguments come out in the latex file!

    kjetil> Whats happening?

\eqn comes in a 1-argument and 2-argument version.
If you want the 2-argument version, you cannot put spaces
between the ending "}" of the 1st arg and the starting "{" of
the 2nd one.

Instead of the above, 
use
	 \eqn{\mbox{coef} = c(\mbox{coef}[1],\ldots, \mbox{coef}[n]) }{%
              coef = c(coef[1], coef[2], \dots, coef[n])}

(note the comment "%" after the opening "{" )
Martin



From mkondrin at hppi.troitsk.ru  Mon Oct 13 14:12:59 2003
From: mkondrin at hppi.troitsk.ru (M.Kondrin)
Date: Mon, 13 Oct 2003 12:12:59 +0000
Subject: [R] DBI Interface broken (revisited)
Message-ID: <3F8A96CB.4040009@hppi.troitsk.ru>

In R-1.8.0 there is a problem connecting to databases through DBI 
interface (I have noticed it too in RMySQL). It seems that this is a 
fault of "methods" package because adding a line 
"exports(.valueClassTest)" to the end of NAMESPACE file in methods 
library tree fixes the problem.



From ripley at stats.ox.ac.uk  Mon Oct 13 10:28:19 2003
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Mon, 13 Oct 2003 09:28:19 +0100 (BST)
Subject: [R] DBI Interface broken (revisited)
In-Reply-To: <3F8A96CB.4040009@hppi.troitsk.ru>
Message-ID: <Pine.LNX.4.44.0310130928050.10451-100000@gannet.stats>

Have you tried the current R-patched?

On Mon, 13 Oct 2003, M.Kondrin wrote:

> In R-1.8.0 there is a problem connecting to databases through DBI 
> interface (I have noticed it too in RMySQL). It seems that this is a 
> fault of "methods" package because adding a line 
> "exports(.valueClassTest)" to the end of NAMESPACE file in methods 
> library tree fixes the problem.
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> 
> 

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From mkondrin at hppi.troitsk.ru  Mon Oct 13 15:04:44 2003
From: mkondrin at hppi.troitsk.ru (M.Kondrin)
Date: Mon, 13 Oct 2003 13:04:44 +0000
Subject: [R] DBI Interface broken (revisited)
References: <Pine.LNX.4.44.0310130928050.10451-100000@gannet.stats>
Message-ID: <3F8AA2EC.6010008@hppi.troitsk.ru>

Prof Brian Ripley wrote:

>Have you tried the current R-patched?
>
>On Mon, 13 Oct 2003, M.Kondrin wrote:
>
>  
>
>>In R-1.8.0 there is a problem connecting to databases through DBI 
>>interface (I have noticed it too in RMySQL). It seems that this is a 
>>fault of "methods" package because adding a line 
>>"exports(.valueClassTest)" to the end of NAMESPACE file in methods 
>>library tree fixes the problem.
>>
>>______________________________________________
>>R-help at stat.math.ethz.ch mailing list
>>https://www.stat.math.ethz.ch/mailman/listinfo/r-help
>>
>>
>>    
>>
>
>  
>
I have downloaded this realease (Oct 08):

http://cran.us.r-project.org/src/base/R-1.8.0.tgz <http://cran.us.r-project.org/src/base/R-1.8.0.tgz>
I have not found any R-1.8.0.patch files there, so I am not sure I have understood what you mean.



From maechler at stat.math.ethz.ch  Mon Oct 13 11:14:40 2003
From: maechler at stat.math.ethz.ch (Martin Maechler)
Date: Mon, 13 Oct 2003 11:14:40 +0200
Subject: [R] Automatic re-looping after error
In-Reply-To: <XFMail.031012134427.Ted.Harding@nessie.mcc.ac.uk>
References: <Pine.A41.4.58.0310091150270.42270@homer40.u.washington.edu>
	<XFMail.031012134427.Ted.Harding@nessie.mcc.ac.uk>
Message-ID: <16266.27904.813151.342911@gargle.gargle.HOWL>

>>>>> "Ted" == Ted Harding <Ted.Harding at nessie.mcc.ac.uk>
>>>>>     on Sun, 12 Oct 2003 13:44:27 +0100 (BST) writes:

 <.....>

    Ted> I've now installed R-1.8.0 which does include
    Ted> 'tryCatch' and related things, but from the look of it
    Ted> I'll have to study it a bit before I see how it all
    Ted> works!

    Ted> Meanwhile, thanks to others (Spencer Graves, Achim
    Ted> Zeileis, as well as Thomas Lumley and Andy Liaw) for
    Ted> suggestions. Though I'd already looked at "try", I
    Ted> thought I'd have another go. It turns out I'd been
    Ted> muddled about testing the result of 'try' in the right
    Ted> way.

    Ted> In fact, if 'myfun(...)' might fail in a loop, then

    Ted>   result <- try(myfun(...));
    Ted>   if(class(result)=="try-error") next ;

    Ted> will have the effect of breaking out of the current
    Ted> cycle of the loop and starting a new cycle. Otherwise
    Ted> 'result' will be a valid returned value from
    Ted> 'myfun'. This is exactly what I had wanted.

very good.

slightly more recommendable code is

   result <- try(myfun(...))
   if(inherits(result, "try-error")) next 


- using  inherits(obj, cls) is more robust than 
  class(obj) == cls because it also works when  `obj' has more
  than one class (e.g. a `glm' object) or when `obj' has no
  class {i.e. when `methods' is not attached or in older R versions}.

- no trailing ";"

Martin Maechler <maechler at stat.math.ethz.ch>	http://stat.ethz.ch/~maechler/
Seminar fuer Statistik, ETH-Zentrum  LEO C16	Leonhardstr. 27
ETH (Federal Inst. Technology)	8092 Zurich	SWITZERLAND
phone: x-41-1-632-3408		fax: ...-1228			<><



From Simon.Fear at synequanon.com  Mon Oct 13 11:15:49 2003
From: Simon.Fear at synequanon.com (Simon Fear)
Date: Mon, 13 Oct 2003 10:15:49 +0100
Subject: [R] how to seperate R's input and output screen?
Message-ID: <6C8A8033ABC1E3468048ABC4F13CE572AC0E1C@synequanon01>

I use this template for all my script files:

{ # start source code group (for pretty echoing)

# put your code in here

} # end source code group

If you have echo on, this makes all the code appear first in
the {} group, then all the output. With echo off, the code
is not echoed at all, and can be written directly to an
"output file" using sink. I suggest you check out ?source (see 
in particular the arguments echo and max.deparse.length) 
and ?sink.


> -----Original Message-----
> From: Yong Wang [mailto:wang at galton.uchicago.edu]
> Sent: 11 October 2003 19:35
> To: r-help at stat.math.ethz.ch
> Subject: [R] how to seperate R's input and output screen?
> 
> 
> Security Warning:
> If you are not sure an attachment is safe to open please contact 
> Andy on x234. There are 0 attachments with this message.
> ________________________________________________________________
> 
> Dear all
> can anybady tell me how to seperate R's input and output screen?I mean
> just like SAS or some others, the commands does not mixed 
> with results.
> thank you
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
>
 

Simon Fear
Senior Statistician
Syne qua non Ltd
Tel: +44 (0) 1379 644449
Fax: +44 (0) 1379 644445
email: Simon.Fear at synequanon.com
web: http://www.synequanon.com
 
Number of attachments included with this message: 0
 
This message (and any associated files) is confidential and\...{{dropped}}



From ripley at stats.ox.ac.uk  Mon Oct 13 11:42:30 2003
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Mon, 13 Oct 2003 10:42:30 +0100 (BST)
Subject: [R] DBI Interface broken (revisited)
In-Reply-To: <3F8AA2EC.6010008@hppi.troitsk.ru>
Message-ID: <Pine.LNX.4.44.0310131040570.1124-100000@gannet.stats>

On Mon, 13 Oct 2003, M.Kondrin wrote:

> Prof Brian Ripley wrote:
> 
> >Have you tried the current R-patched?
> >
> >On Mon, 13 Oct 2003, M.Kondrin wrote:
> >
> >  
> >
> >>In R-1.8.0 there is a problem connecting to databases through DBI 
> >>interface (I have noticed it too in RMySQL). It seems that this is a 
> >>fault of "methods" package because adding a line 
> >>"exports(.valueClassTest)" to the end of NAMESPACE file in methods 
> >>library tree fixes the problem.
> >>
> >>______________________________________________
> >>R-help at stat.math.ethz.ch mailing list
> >>https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> >>
> >>
> >>    
> >>
> >
> >  
> >
> I have downloaded this realease (Oct 08):
> 
> http://cran.us.r-project.org/src/base/R-1.8.0.tgz <http://cran.us.r-project.org/src/base/R-1.8.0.tgz>
> I have not found any R-1.8.0.patch files there, so I am not sure I have understood what you mean. 

Look in the FAQ Q2.4 to understand the standard terminology for R
versions.

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From Virgilio.Gomez at uv.es  Mon Oct 13 14:01:08 2003
From: Virgilio.Gomez at uv.es (Virgilio =?ISO-8859-1?Q?G=F3mez?= Rubio)
Date: Mon, 13 Oct 2003 14:01:08 +0200
Subject: [R] Pre-release of package DCluster available
Message-ID: <1066046467.6737.37.camel@chomsky.estadi.uv.es>

Hi,

I have just put the source code (and a zip file for Window$ users) of
package DCluster in my website:

http://matheron.estadi.uv.es/~virgil/Rpackages/DCluster/

DCluster is a package that contains routines for the detection of
spatial clusters of diseases (Openshaw's GAM, Besag and Newell, 
Kulldorff and Nagarwalla, Stone's Test and others).

A full description can be found here:

http://www.ci.tuwien.ac.at/Conferences/DSC-2003/Proceedings/GomezRubioEtAl.pdf

The purpose of this pre-release is to get feedback from the users. I'm
sure there are still bugs in the code (please, use it with care), so I
hope to debug them from your comments!!

Best regards,

-- 
             Virgilio G?mez Rubio

Grup d'Estad?stica espacial i temporal 
en Epidemiologia i medi ambient 

Dpto. Estad?stica e I. O. - Facultat de Matem?tiques
Avda. Vicent A. Estell?s, 1 - 46100 Burjassot
Valencia - SPAIN

http://matheron.uv.es/~virgil

TLF: 00 34 96 354 43 62 - FAX: 00 34 96 354 47 35



From Ted.Harding at nessie.mcc.ac.uk  Mon Oct 13 11:50:16 2003
From: Ted.Harding at nessie.mcc.ac.uk ( (Ted Harding))
Date: Mon, 13 Oct 2003 10:50:16 +0100 (BST)
Subject: [R] Automatic re-looping after error
In-Reply-To: <16266.27904.813151.342911@gargle.gargle.HOWL>
Message-ID: <XFMail.031013105016.Ted.Harding@nessie.mcc.ac.uk>

On 13-Oct-03 Martin Maechler wrote:
> slightly more recommendable code is
> 
>    result <- try(myfun(...))
>    if(inherits(result, "try-error")) next 
> 
> 
> - using  inherits(obj, cls) is more robust than 
>   class(obj) == cls because it also works when  `obj' has more
>   than one class (e.g. a `glm' object) or when `obj' has no
>   class {i.e. when `methods' is not attached or in older R versions}.

Thanks! (I had been worried about exactly this kind of issue, though in
fact it worked in the case I had.)

> - no trailing ";"

... Old C (and awk) habits die hard ...

Ted.


--------------------------------------------------------------------
E-Mail: (Ted Harding) <Ted.Harding at nessie.mcc.ac.uk>
Fax-to-email: +44 (0)870 167 1972
Date: 13-Oct-03                                       Time: 10:50:16
------------------------------ XFMail ------------------------------



From Virgilio.Gomez at uv.es  Mon Oct 13 14:09:28 2003
From: Virgilio.Gomez at uv.es (Virgilio =?ISO-8859-1?Q?G=F3mez?= Rubio)
Date: Mon, 13 Oct 2003 14:09:28 +0200
Subject: [R] Re:  R(D)-COM stat conenctor for ArcGIS
Message-ID: <1066046968.6897.45.camel@chomsky.estadi.uv.es>

Hi,


>  I heard about "R(D)-COM Stat connector" for ArcGIS, but i am not 
>    sure what that is. I did a search in the archive but it seems i am 
>    not getting anything back. 
>    can anybody explain me what that is, and where i can find more 
>    info about it? There is any possibility to run R from inside 
>ArcGIS? 

Yes, it is possible. I have made some trivial tests (1+1 :D) and it
worked as expected (=2). But, definitively, you can use R from within
ArcGIS. I am planning, in the near future, to try to write something
to link ArcGIS and R (mostly like the R-Excel interface). But first
I have to learn a bit more about ArcObjects...

So far, I know that R(D)-COM has been usually by other. Take a look
at 
http://www.gisig.it/coastgis/programma/abstract/astad.htm


>    there is more than RArcInfo and Shapefile which can import 
>these 
>    kind of files in R environment?

Roger S. Bivand has set up this great web site about R and spatial
statistics and data analysis:

http://sal.agecon.uiuc.edu/csiss/Rgeo/index.html


Hope it works,

-- 
             Virgilio G?mez Rubio

Grup d'Estad?stica espacial i temporal 
en Epidemiologia i medi ambient 

Dpto. Estad?stica e I. O. - Facultat de Matem?tiques
Avda. Vicent A. Estell?s, 1 - 46100 Burjassot
Valencia - SPAIN

http://matheron.uv.es/~virgil

TLF: 00 34 96 354 43 62 - FAX: 00 34 96 354 47 35



From mkondrin at hppi.troitsk.ru  Mon Oct 13 18:13:19 2003
From: mkondrin at hppi.troitsk.ru (M.Kondrin)
Date: Mon, 13 Oct 2003 16:13:19 +0000
Subject: [R] DBI Interface broken (revisited)
References: <Pine.LNX.4.44.0310131040570.1124-100000@gannet.stats>
Message-ID: <3F8ACF1F.7000306@hppi.troitsk.ru>

Prof Brian Ripley wrote:

>On Mon, 13 Oct 2003, M.Kondrin wrote:
>
>  
>
>>Prof Brian Ripley wrote:
>>
>>    
>>
>>>Have you tried the current R-patched?
>>>
>>>On Mon, 13 Oct 2003, M.Kondrin wrote:
>>>
>>> 
>>>
>>>      
>>>
>>>>In R-1.8.0 there is a problem connecting to databases through DBI 
>>>>interface (I have noticed it too in RMySQL). It seems that this is a 
>>>>fault of "methods" package because adding a line 
>>>>"exports(.valueClassTest)" to the end of NAMESPACE file in methods 
>>>>library tree fixes the problem.
>>>>
>>>>______________________________________________
>>>>R-help at stat.math.ethz.ch mailing list
>>>>https://www.stat.math.ethz.ch/mailman/listinfo/r-help
>>>>
>>>>
>>>>   
>>>>
>>>>        
>>>>
>>> 
>>>
>>>      
>>>
>>I have downloaded this realease (Oct 08):
>>
>>http://cran.us.r-project.org/src/base/R-1.8.0.tgz <http://cran.us.r-project.org/src/base/R-1.8.0.tgz>
>>I have not found any R-1.8.0.patch files there, so I am not sure I have understood what you mean. 
>>    
>>
>
>Look in the FAQ Q2.4 to understand the standard terminology for R
>versions.
>
>  
>
OK, it was patched yesterday.



From temiz at deprem.gov.tr  Mon Oct 13 14:40:55 2003
From: temiz at deprem.gov.tr (temiz)
Date: Mon, 13 Oct 2003 15:40:55 +0300
Subject: [R] inverse sinus in degrees
Message-ID: <3F8A9D57.2090302@deprem.gov.tr>

Hello

I would like to know how I can get inverse sinus in degrees.
sin(90) >>>1 ; inv_fun_sin(1) >>>90.
what should "inv_fun_sin" be in R ?

thanks in advance

Ahmet Temiz
General Directory of Disaster Affairs
Ankara TURKEY



______________________________________
Inflex - installed on mailserver for domain @deprem.gov.tr
Queries to: postmaster at deprem.gov.tr

______________________________________
The views and opinions expressed in this e-mail message are ...{{dropped}}



From ripley at stats.ox.ac.uk  Mon Oct 13 15:02:16 2003
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Mon, 13 Oct 2003 14:02:16 +0100 (BST)
Subject: [R] inverse sinus in degrees
In-Reply-To: <3F8A9D57.2090302@deprem.gov.tr>
Message-ID: <Pine.LNX.4.44.0310131400390.1110-100000@gannet.stats>

asin(), but both sin() and asin() are in radians so you appear to want

180/pi * asin(x)

On Mon, 13 Oct 2003, temiz wrote:

> Hello
> 
> I would like to know how I can get inverse sinus in degrees.
> sin(90) >>>1 ; inv_fun_sin(1) >>>90.
> what should "inv_fun_sin" be in R ?
> 
> thanks in advance
> 
> Ahmet Temiz
> General Directory of Disaster Affairs
> Ankara TURKEY

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From jfox at mcmaster.ca  Mon Oct 13 15:40:45 2003
From: jfox at mcmaster.ca (John Fox)
Date: Mon, 13 Oct 2003 09:40:45 -0400
Subject: [R] how to seperate R's input and output screen?
In-Reply-To: <Pine.GSO.4.05.10310111332100.10258-100000@aitken.uchicago. edu>
References: <200310101004.h9AA3CqE023249@stat.math.ethz.ch>
Message-ID: <5.1.0.14.2.20031013093034.01fb7d60@127.0.0.1>

Dear Yong Wang,

At 01:35 PM 10/11/2003 -0500, Yong Wang wrote:
>Dear all
>can anybady tell me how to seperate R's input and output screen?I mean
>just like SAS or some others, the commands does not mixed with results.
>thank you

You can create a split screen of this sort with the Emacs or Xemacs editor 
along with the ESS package (although the commands will appear interspersed 
with the output as well as in an editor window -- which seems desirable). 
See <http://www.analytics.washington.edu/Zope/wikis/ess/> for information 
on ESS. I have some information for Windows users at 
<http://socserv.socsci.mcmaster.ca/jfox/Books/Companion/ESS/index.html>. 
You could also use an independent editor for commands, such as WinEdt under 
Windows (see the section on editing support in 
<http://cran.r-project.org/other-software.html>).

I hope that this helps,
  John
-----------------------------------------------------
John Fox
Department of Sociology
McMaster University
Hamilton, Ontario, Canada L8S 4M4
email: jfox at mcmaster.ca
phone: 905-525-9140x23604
web: www.socsci.mcmaster.ca/jfox



From temiz at deprem.gov.tr  Mon Oct 13 16:03:47 2003
From: temiz at deprem.gov.tr (orkun)
Date: Mon, 13 Oct 2003 17:03:47 +0300
Subject: [R] inverse sinus in degrees
In-Reply-To: <Pine.LNX.4.44.0310131400390.1110-100000@gannet.stats>
References: <Pine.LNX.4.44.0310131400390.1110-100000@gannet.stats>
Message-ID: <3F8AB0C3.2000101@deprem.gov.tr>

>
>
>>Hello
>>
>>I would like to know how I can get inverse sinus in degrees.
>>sin(90) >>>1 ; inv_fun_sin(1) >>>90.
>>what should "inv_fun_sin" be in R ?
>>
>>thanks in advance
>>
>>Ahmet Temiz
>>General Directory of Disaster Affairs
>>Ankara TURKEY
>>    
>>
>
>  
>
thank you all people for your help


______________________________________
Inflex - installed on mailserver for domain @deprem.gov.tr
Queries to: postmaster at deprem.gov.tr

______________________________________
The views and opinions expressed in this e-mail message are ...{{dropped}}



From simon at versailles.inra.fr  Mon Oct 13 16:18:59 2003
From: simon at versailles.inra.fr (Adeline Simon)
Date: Mon, 13 Oct 2003 16:18:59 +0200
Subject: [R] R & html & "template approach"
Message-ID: <5.1.0.14.1.20031013150541.00a07ca0@versailles.inra.fr>

Hello,
I have a question about R and html, especially about what I call a 
"template approach".

I use CGIwithR.
This is OK to decode html form data, but, instead of writing html output in 
the R code (with tag, untag ... functions), I'm wondering if there is a way 
to this with a "template approach". This kind of approach should permit to 
open an existing html page from the R code and give to this html page all 
the variables defined in the current namespace of R.
I do this in python with the WriteProcessor module and find it very easy 
and useful.

Is anyone know if there is something like that with R ? I didn't find it in 
any documentation...

Thanks.
Adeline.


********************************************************
Adeline Brunet-Simon
Analyse fonctionnelle du genome de Botrytis cinerea
Unite PMDV
INRA - Route de Saint-Cyr
78026 Versailles
Tel : 01.30.83.32.17
Fax : 01.30.83.31.95
E-mail : simon at versailles.inra.fr

http://www-pmdv.versailles.inra.fr/



From martinol at ensam.inra.fr  Mon Oct 13 16:30:37 2003
From: martinol at ensam.inra.fr (Martin Olivier)
Date: Mon, 13 Oct 2003 16:30:37 +0200
Subject: [R] help with gsub and grep functions
Message-ID: <3F8AB70D.7030903@ensam.inra.fr>

Hi all,

Let Names a vector of chatacters. For example,

 > Names
[1] "g 604 be-0 -p1 (602 matches)" "g 606 Phli-0 -p2 (517 matches)"
[3] "g 608 alu-0  (659 matches)"

I try to use gsub or grep functions for two problems :

1. First, I would like to delete all the characters between parentheses.
[1] "g 604 be-0 -p1" "g 606 be-0 -p2"
[3] "g 608 be-0 -p3"

2. And, I would like to extract the characters between parentheses
[1] "602 matches" "517 matches"
[3] "659 matches"



Any idea?

Best regards,
Olivier

-- 

-------------------------------------------------------------
Martin Olivier
INRA - Unit? prot?omique           LIRMM - IFA/MAB
2, Place Viala                     161, rue Ada
34060 Montpellier C?dex 1          34392 Montpellier C?dex 5	

Tel : 04 99 61 27 01               Tel : O4 67 41 86 71
martinol at ensam.inra.fr             martin at lirmm.fr



From Pascal.Niklaus at unibas.ch  Mon Oct 13 17:11:07 2003
From: Pascal.Niklaus at unibas.ch (Pascal A. Niklaus)
Date: Mon, 13 Oct 2003 17:11:07 +0200
Subject: [R] Rotate a plot, and subplot 
Message-ID: <3F8AC08B.4020407@unibas.ch>

Hi all,

Is there a way to rotate a plot, e.g. a histogram, by a certain angle 
(90/180/270 degress)? I spent hours trying to figure out how this is 
done, but without success.

Also, I'm looking for an equivalent to the S-Plus "subplot" command to 
insert a kind of "thumbnail" graphic into a bigger one. How is this best 
done in R?

Thanks for your help

Pascal



From jfox at mcmaster.ca  Mon Oct 13 17:26:25 2003
From: jfox at mcmaster.ca (John Fox)
Date: Mon, 13 Oct 2003 11:26:25 -0400
Subject: [R] help with gsub and grep functions
In-Reply-To: <3F8AB70D.7030903@ensam.inra.fr>
Message-ID: <5.1.0.14.2.20031013112320.01ff4b10@127.0.0.1>

Dear Oliver,

I believe that the following will give you what you want:

At 04:30 PM 10/13/2003 +0200, Martin Olivier wrote:
>Hi all,
>
>Let Names a vector of chatacters. For example,
>
> > Names
>[1] "g 604 be-0 -p1 (602 matches)" "g 606 Phli-0 -p2 (517 matches)"
>[3] "g 608 alu-0  (659 matches)"
>
>I try to use gsub or grep functions for two problems :
>
>1. First, I would like to delete all the characters between parentheses.
>[1] "g 604 be-0 -p1" "g 606 be-0 -p2"
>[3] "g 608 be-0 -p3"

gsub(" *$", "", gsub("\\(.*\\)$", "", Names))  # also deletes trailing blanks


>2. And, I would like to extract the characters between parentheses
>[1] "602 matches" "517 matches"
>[3] "659 matches"
>

posn <- regexpr("\\(.*\\)$", Names)
substring(Names, first=posn+1, last=posn+attr(posn,"match.length")-2)


>Any idea?
>
>Best regards,
>Olivier

I hope that this helps,
  John

-----------------------------------------------------
John Fox
Department of Sociology
McMaster University
Hamilton, Ontario, Canada L8S 4M4
email: jfox at mcmaster.ca
phone: 905-525-9140x23604
web: www.socsci.mcmaster.ca/jfox



From Simon.Fear at synequanon.com  Mon Oct 13 17:29:00 2003
From: Simon.Fear at synequanon.com (Simon Fear)
Date: Mon, 13 Oct 2003 16:29:00 +0100
Subject: [R] help with gsub and grep functions
Message-ID: <6C8A8033ABC1E3468048ABC4F13CE572AC0E21@synequanon01>

Well, this works for the first one:

> sub(" \\([A-Za-z0-9_ ]*\\)", "", Names)

and from there the second one is fairly obvious I hope.

QUESTION: having recently been using Source Edit I wanted
to write [\\w]* instead of [A-Za-z0-9_ ]* but that doesn't
seem to work in R. ?grep points to
ftp://ftp.csx.cam.ac.uk/pub/software/programming/pcre/
but I can't access that (server/gateway restriction). So,
could anyone tell me exactly what is allowed in R regular
expressions? A URL to the POSIX standards would be useful
too.

In fact it would be even more useful if R's particular choice
of RE syntax, together with R's multiple backslashes, was given 
somewhere in the R help itself ... yes I will write it if someone 
gives me the info or points me in the right direction ...


> -----Original Message-----
> From: Martin Olivier [mailto:martinol at ensam.inra.fr]
> Sent: 13 October 2003 15:31
> To: r-help
> Subject: [R] help with gsub and grep functions
> 
> 
> Security Warning:
> If you are not sure an attachment is safe to open please contact 
> Andy on x234. There are 0 attachments with this message.
> ________________________________________________________________
> 
> Hi all,
> 
> Let Names a vector of chatacters. For example,
> 
>  > Names
> [1] "g 604 be-0 -p1 (602 matches)" "g 606 Phli-0 -p2 (517 matches)"
> [3] "g 608 alu-0  (659 matches)"
> 
> I try to use gsub or grep functions for two problems :
> 
> 1. First, I would like to delete all the characters between 
> parentheses.
> [1] "g 604 be-0 -p1" "g 606 be-0 -p2"
> [3] "g 608 be-0 -p3"
> 
> 2. And, I would like to extract the characters between parentheses
> [1] "602 matches" "517 matches"
> [3] "659 matches"
> 
> 
> 
> Any idea?
> 
> Best regards,
> Olivier
> 
> -- 
> 
> -------------------------------------------------------------
> Martin Olivier
> INRA - Unit? prot?omique           LIRMM - IFA/MAB
> 2, Place Viala                     161, rue Ada
> 34060 Montpellier C?dex 1          34392 Montpellier C?dex 5	
> 
> Tel : 04 99 61 27 01               Tel : O4 67 41 86 71
> martinol at ensam.inra.fr             martin at lirmm.fr
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
>
 

Simon Fear
Senior Statistician
Syne qua non Ltd
Tel: +44 (0) 1379 644449
Fax: +44 (0) 1379 644445
email: Simon.Fear at synequanon.com
web: http://www.synequanon.com
 
Number of attachments included with this message: 0
 
This message (and any associated files) is confidential and\...{{dropped}}



From mailinglist.wegmann at web.de  Mon Oct 13 17:30:17 2003
From: mailinglist.wegmann at web.de (Martin Wegmann)
Date: Mon, 13 Oct 2003 17:30:17 +0200
Subject: [R] NA's from GRASS-package
Message-ID: <200310131730.17528.mailinglist.wegmann@web.de>

Hello, 

I run R inside GRASS and tried to obtain values of a raster file inside GRASS 
but this raster image contains only a few relevant pixel and approx. 1.5 
Mill. NA's, I did

G <- gmeta()
sites1 <- rast.get(G, "sites1")
df.sites1 <- data.frame(east(G), north(G), sites1, na.action=na.omit)
Error in as.data.frame.default(x[[i]], optional = TRUE) :
        can't coerce function into a data.frame

executing it without na.action works fine but with 1.5 Mill. redundant values. 

any advice how I receive the relevant 100 values without the NA's?

thanks in advance cheers Martin



From jschum at bgc-jena.mpg.de  Mon Oct 13 16:41:55 2003
From: jschum at bgc-jena.mpg.de (Jens Schumacher)
Date: Mon, 13 Oct 2003 16:41:55 +0200
Subject: [R] Initial Simplex in Nelder-Mead algorithm
Message-ID: <3F8AB9B3.40693CE6@bgc-jena.mpg.de>

Hi all,

in the optim() function there is no control over the size of the initial
simplex for the Nelder-Mead algorithm. Two years ago Ben Bolker
suggested to include an additional control parameter to allow more
flexibility in this respect. Did anybody implement such a modified
version.

Thanks in advance

Jens
--

--------------------------------------------
Dr. Jens Schumacher
Max-Planck-Institut f. Biogeochemie
Winzerlaer Str. 10
D-07745 Jena
Germany

Tel: +49 (0)3641/576181
Fax: +49 (0)3641/577100
email: jens.schumacher at bgc-jena.mpg.de



From ripley at stats.ox.ac.uk  Mon Oct 13 17:57:22 2003
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Mon, 13 Oct 2003 16:57:22 +0100 (BST)
Subject: [R] NA's from GRASS-package
In-Reply-To: <200310131730.17528.mailinglist.wegmann@web.de>
Message-ID: <Pine.LNX.4.44.0310131656050.1428-100000@gannet.stats>

?data.frame will show na.action is not an argument to data.frame.

na.omit(data.frame(east(G), north(G), sites1)) might be what you want.

On Mon, 13 Oct 2003, Martin Wegmann wrote:

> I run R inside GRASS and tried to obtain values of a raster file inside GRASS 
> but this raster image contains only a few relevant pixel and approx. 1.5 
> Mill. NA's, I did
> 
> G <- gmeta()
> sites1 <- rast.get(G, "sites1")
> df.sites1 <- data.frame(east(G), north(G), sites1, na.action=na.omit)
> Error in as.data.frame.default(x[[i]], optional = TRUE) :
>         can't coerce function into a data.frame
> 
> executing it without na.action works fine but with 1.5 Mill. redundant values. 
> 
> any advice how I receive the relevant 100 values without the NA's?


-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From CMiller at PICR.man.ac.uk  Mon Oct 13 17:50:52 2003
From: CMiller at PICR.man.ac.uk (Crispin Miller)
Date: Mon, 13 Oct 2003 16:50:52 +0100
Subject: [R] Setting PNG sizes
Message-ID: <BAA35444B19AD940997ED02A6996AAE0B5C1FA@sanmail.picr.man.ac.uk>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20031013/155f6538/attachment.pl

From jeff_hamann at hamanndonald.com  Mon Oct 13 18:22:02 2003
From: jeff_hamann at hamanndonald.com (Jeff D. Hamann)
Date: Mon, 13 Oct 2003 09:22:02 -0700
Subject: [R] colnames from submatrix?
References: <200310131006.h9DA3BqL008949@stat.math.ethz.ch>
Message-ID: <007801c391a6$22a2d040$0a00a8c0@rodan>

Hi R-Wizards:

I've looking through the R docs and have yet to find what I'm looking for
and have tried a few intermediate steps to now avail yet and rather than
spend another few hours looking for the solution, I figured I would post a
message.

I have a matrix (actually a set of them) that I want to pull all the names
of the non-zero columns into a vector/list for further processing:

             d0            d1            d2 s0 s1 s2 s3
 [1,] -4.4721360 -447.29878969 -436.18978037  0  0  0  0
 [2,]  0.2236068  -25.83121172  -29.21571675  0  0  0  0
 [3,]  0.2236068    0.13009088   42.49338060  0  0  0  0
 [4,]  0.2236068    0.17155235    0.04891280  0  0  0  0
 [5,]  0.2236068   -0.08027479   -0.14749423  0  0  0  0
 [6,]  0.2236068   -0.02394758   -0.12845820  0  0  0  0
 [7,]  0.2236068    0.03838012   -0.15270556  0  0  0  0
 [8,]  0.2236068    0.18150155   -0.17073267  0  0  0  0
 [9,]  0.2236068   -0.14047328   -0.11013806  0  0  0  0
[10,]  0.2236068   -0.34247697   -0.05627832  0  0  0  0
[11,]  0.2236068   -0.27058719    0.31379811  0  0  0  0
[12,]  0.2236068   -0.04930450    0.41093718  0  0  0  0
[13,]  0.2236068    0.10968920    0.32996356  0  0  0  0
[14,]  0.2236068   -0.05104658    0.08743581  0  0  0  0
[15,]  0.2236068   -0.19184524   -0.29548841  0  0  0  0
[16,]  0.2236068   -0.06285400   -0.26123732  0  0  0  0
[17,]  0.2236068   -0.52558878   -0.34821174  0  0  0  0
[18,]  0.2236068    0.15258305   -0.10895070  0  0  0  0
[19,]  0.2236068    0.22044668   -0.21442723  0  0  0  0
[20,]  0.2236068    0.51934865   -0.41193918  0  0  0  0


such that I only end up with a list of names (d0,d1,d2). I've been using the
qr( x )$rank to get the number of non-zero columns, but I'm not sure how to
only return the names of the non-zero columns as

> colnames( qr( attr( eval( nlsystemols$eq[[1]]$deriv ), "gradient" ) )$qr )

will return

> [1] "d0" "d1" "d2" "s0" "s1" "s2" "s3"

Is/Are there one of those great S or R function/shortcuts that will return a
submatrix(?) or a partioned matrix such that when I call colnames() I get:

> [1] "d0" "d1" "d2"

Thanks,
Jeff.



From ripley at stats.ox.ac.uk  Mon Oct 13 18:09:24 2003
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Mon, 13 Oct 2003 17:09:24 +0100 (BST)
Subject: [R] help with gsub and grep functions
In-Reply-To: <6C8A8033ABC1E3468048ABC4F13CE572AC0E21@synequanon01>
Message-ID: <Pine.LNX.4.44.0310131704480.1508-100000@gannet.stats>

On Mon, 13 Oct 2003, Simon Fear wrote:

> Well, this works for the first one:
> 
> > sub(" \\([A-Za-z0-9_ ]*\\)", "", Names)
> 
> and from there the second one is fairly obvious I hope.
> 
> QUESTION: having recently been using Source Edit I wanted
> to write [\\w]* instead of [A-Za-z0-9_ ]* but that doesn't

space is not in \w ... so try

> sub(" \\([\\w ]*\\)", "", Names, perl=TRUE)
[1] "g 604 be-0 -p1"

However

> sub(".*\\((.*)\\)", "\\1", Names)

picks out the parenthesized part, and

sub("\\((.*)\\)", "", Names)

omits it.


> seem to work in R. ?grep points to
> ftp://ftp.csx.cam.ac.uk/pub/software/programming/pcre/
> but I can't access that (server/gateway restriction). So,

Works here.

> could anyone tell me exactly what is allowed in R regular
> expressions? A URL to the POSIX standards would be useful
> too.
> 
> In fact it would be even more useful if R's particular choice
> of RE syntax, together with R's multiple backslashes, was given 
> somewhere in the R help itself ... yes I will write it if someone 
> gives me the info or points me in the right direction ...
> 
> 
> > -----Original Message-----
> > From: Martin Olivier [mailto:martinol at ensam.inra.fr]
> > Sent: 13 October 2003 15:31
> > To: r-help
> > Subject: [R] help with gsub and grep functions
> > 
> > 
> > Security Warning:
> > If you are not sure an attachment is safe to open please contact 
> > Andy on x234. There are 0 attachments with this message.
> > ________________________________________________________________
> > 
> > Hi all,
> > 
> > Let Names a vector of chatacters. For example,
> > 
> >  > Names
> > [1] "g 604 be-0 -p1 (602 matches)" "g 606 Phli-0 -p2 (517 matches)"
> > [3] "g 608 alu-0  (659 matches)"
> > 
> > I try to use gsub or grep functions for two problems :
> > 
> > 1. First, I would like to delete all the characters between 
> > parentheses.
> > [1] "g 604 be-0 -p1" "g 606 be-0 -p2"
> > [3] "g 608 be-0 -p3"
> > 
> > 2. And, I would like to extract the characters between parentheses
> > [1] "602 matches" "517 matches"
> > [3] "659 matches"
> > 
> > 
> > 
> > Any idea?
> > 
> > Best regards,
> > Olivier
> > 
> > -- 
> > 
> > -------------------------------------------------------------
> > Martin Olivier
> > INRA - Unit? prot?omique           LIRMM - IFA/MAB
> > 2, Place Viala                     161, rue Ada
> > 34060 Montpellier C?dex 1          34392 Montpellier C?dex 5	
> > 
> > Tel : 04 99 61 27 01               Tel : O4 67 41 86 71
> > martinol at ensam.inra.fr             martin at lirmm.fr
> > 
> > ______________________________________________
> > R-help at stat.math.ethz.ch mailing list
> > https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> >
>  
> 
> Simon Fear
> Senior Statistician
> Syne qua non Ltd
> Tel: +44 (0) 1379 644449
> Fax: +44 (0) 1379 644445
> email: Simon.Fear at synequanon.com
> web: http://www.synequanon.com
>  
> Number of attachments included with this message: 0
>  
> This message (and any associated files) is confidential and\...{{dropped}}
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> 
> 

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From ahenningsen at email.uni-kiel.de  Mon Oct 13 18:25:53 2003
From: ahenningsen at email.uni-kiel.de (Arne Henningsen)
Date: Mon, 13 Oct 2003 18:25:53 +0200
Subject: [R] help with gsub and grep functions
In-Reply-To: <3F8AB70D.7030903@ensam.inra.fr>
References: <3F8AB70D.7030903@ensam.inra.fr>
Message-ID: <200310131825.53541.ahenningsen@email.uni-kiel.de>

On Monday 13 October 2003 16:30, Martin Olivier wrote:
> Hi all,
>
> Let Names a vector of chatacters. For example,
>
>  > Names
>
> [1] "g 604 be-0 -p1 (602 matches)" "g 606 Phli-0 -p2 (517 matches)"
> [3] "g 608 alu-0  (659 matches)"
>
> I try to use gsub or grep functions for two problems :
>
> 1. First, I would like to delete all the characters between parentheses.
> [1] "g 604 be-0 -p1" "g 606 be-0 -p2"
> [3] "g 608 be-0 -p3"
>
> 2. And, I would like to extract the characters between parentheses
> [1] "602 matches" "517 matches"
> [3] "659 matches"
>
>
> Any idea?
> Best regards,
> Olivier

There might be a better solution, but the following commands do what you want 
(at least in the 3 cases that you showed above):

   sub(" [(].*","",Names)
   sub("[\)]+","",sub("[^(]*[\(]","",Names))

Arne

-- 
Arne Henningsen
Department of Agricultural Economics
Christian-Albrechts-University Kiel
24098 Kiel, Germany
Tel: +49-431-880-4445
Fax: +49-431-880-1397 
ahenningsen at email.uni-kiel.de
http://www.uni-kiel.de/agrarpol/ahenningsen/



From spencer.graves at pdf.com  Mon Oct 13 18:35:00 2003
From: spencer.graves at pdf.com (Spencer Graves)
Date: Mon, 13 Oct 2003 09:35:00 -0700
Subject: [R] colnames from submatrix?
In-Reply-To: <007801c391a6$22a2d040$0a00a8c0@rodan>
References: <200310131006.h9DA3BqL008949@stat.math.ethz.ch>
	<007801c391a6$22a2d040$0a00a8c0@rodan>
Message-ID: <3F8AD434.9090404@pdf.com>

Does the following meet your requirements: 

 > DF <- data.frame(a=1:2, b=c(0,0))
 > DFzeros <- sapply(DF, function(x)all(x==0))
 > names(DF)[!DFzeros]
[1] "a"

hope this helps.  spencer graves

Jeff D. Hamann wrote:

>Hi R-Wizards:
>
>I've looking through the R docs and have yet to find what I'm looking for
>and have tried a few intermediate steps to now avail yet and rather than
>spend another few hours looking for the solution, I figured I would post a
>message.
>
>I have a matrix (actually a set of them) that I want to pull all the names
>of the non-zero columns into a vector/list for further processing:
>
>             d0            d1            d2 s0 s1 s2 s3
> [1,] -4.4721360 -447.29878969 -436.18978037  0  0  0  0
> [2,]  0.2236068  -25.83121172  -29.21571675  0  0  0  0
> [3,]  0.2236068    0.13009088   42.49338060  0  0  0  0
> [4,]  0.2236068    0.17155235    0.04891280  0  0  0  0
> [5,]  0.2236068   -0.08027479   -0.14749423  0  0  0  0
> [6,]  0.2236068   -0.02394758   -0.12845820  0  0  0  0
> [7,]  0.2236068    0.03838012   -0.15270556  0  0  0  0
> [8,]  0.2236068    0.18150155   -0.17073267  0  0  0  0
> [9,]  0.2236068   -0.14047328   -0.11013806  0  0  0  0
>[10,]  0.2236068   -0.34247697   -0.05627832  0  0  0  0
>[11,]  0.2236068   -0.27058719    0.31379811  0  0  0  0
>[12,]  0.2236068   -0.04930450    0.41093718  0  0  0  0
>[13,]  0.2236068    0.10968920    0.32996356  0  0  0  0
>[14,]  0.2236068   -0.05104658    0.08743581  0  0  0  0
>[15,]  0.2236068   -0.19184524   -0.29548841  0  0  0  0
>[16,]  0.2236068   -0.06285400   -0.26123732  0  0  0  0
>[17,]  0.2236068   -0.52558878   -0.34821174  0  0  0  0
>[18,]  0.2236068    0.15258305   -0.10895070  0  0  0  0
>[19,]  0.2236068    0.22044668   -0.21442723  0  0  0  0
>[20,]  0.2236068    0.51934865   -0.41193918  0  0  0  0
>
>
>such that I only end up with a list of names (d0,d1,d2). I've been using the
>qr( x )$rank to get the number of non-zero columns, but I'm not sure how to
>only return the names of the non-zero columns as
>
>  
>
>>colnames( qr( attr( eval( nlsystemols$eq[[1]]$deriv ), "gradient" ) )$qr )
>>    
>>
>
>will return
>
>  
>
>>[1] "d0" "d1" "d2" "s0" "s1" "s2" "s3"
>>    
>>
>
>Is/Are there one of those great S or R function/shortcuts that will return a
>submatrix(?) or a partioned matrix such that when I call colnames() I get:
>
>  
>
>>[1] "d0" "d1" "d2"
>>    
>>
>
>Thanks,
>Jeff.
>
>______________________________________________
>R-help at stat.math.ethz.ch mailing list
>https://www.stat.math.ethz.ch/mailman/listinfo/r-help
>  
>



From andy_liaw at merck.com  Mon Oct 13 18:41:58 2003
From: andy_liaw at merck.com (Liaw, Andy)
Date: Mon, 13 Oct 2003 12:41:58 -0400
Subject: [R] colnames from submatrix?
Message-ID: <3A822319EB35174CA3714066D590DCD50205CC75@usrymx25.merck.com>

If the columns you want to exclude can only contain all 0s (and not some
other values such as 1s), you can do something like:

mat[, colSums(mat != 0) > 0]

to extract the non-zero columns of the matrix.  "mat != 0" gives you a
matrix of TRUEs and FALSEs, and colSums() of this matrix tells you how many
non-zero elements are in the columns.  You just want columns with at least
one non-zero entry.

HTH,
Andy

> -----Original Message-----
> From: Jeff D. Hamann [mailto:jeff_hamann at hamanndonald.com] 
> Sent: Monday, October 13, 2003 12:22 PM
> To: r-help at stat.math.ethz.ch
> Subject: [R] colnames from submatrix?
> 
> 
> Hi R-Wizards:
> 
> I've looking through the R docs and have yet to find what I'm 
> looking for and have tried a few intermediate steps to now 
> avail yet and rather than spend another few hours looking for 
> the solution, I figured I would post a message.
> 
> I have a matrix (actually a set of them) that I want to pull 
> all the names of the non-zero columns into a vector/list for 
> further processing:
> 
>              d0            d1            d2 s0 s1 s2 s3
>  [1,] -4.4721360 -447.29878969 -436.18978037  0  0  0  0
>  [2,]  0.2236068  -25.83121172  -29.21571675  0  0  0  0
>  [3,]  0.2236068    0.13009088   42.49338060  0  0  0  0
>  [4,]  0.2236068    0.17155235    0.04891280  0  0  0  0
>  [5,]  0.2236068   -0.08027479   -0.14749423  0  0  0  0
>  [6,]  0.2236068   -0.02394758   -0.12845820  0  0  0  0
>  [7,]  0.2236068    0.03838012   -0.15270556  0  0  0  0
>  [8,]  0.2236068    0.18150155   -0.17073267  0  0  0  0
>  [9,]  0.2236068   -0.14047328   -0.11013806  0  0  0  0
> [10,]  0.2236068   -0.34247697   -0.05627832  0  0  0  0
> [11,]  0.2236068   -0.27058719    0.31379811  0  0  0  0
> [12,]  0.2236068   -0.04930450    0.41093718  0  0  0  0
> [13,]  0.2236068    0.10968920    0.32996356  0  0  0  0
> [14,]  0.2236068   -0.05104658    0.08743581  0  0  0  0
> [15,]  0.2236068   -0.19184524   -0.29548841  0  0  0  0
> [16,]  0.2236068   -0.06285400   -0.26123732  0  0  0  0
> [17,]  0.2236068   -0.52558878   -0.34821174  0  0  0  0
> [18,]  0.2236068    0.15258305   -0.10895070  0  0  0  0
> [19,]  0.2236068    0.22044668   -0.21442723  0  0  0  0
> [20,]  0.2236068    0.51934865   -0.41193918  0  0  0  0
> 
> 
> such that I only end up with a list of names (d0,d1,d2). I've 
> been using the qr( x )$rank to get the number of non-zero 
> columns, but I'm not sure how to only return the names of the 
> non-zero columns as
> 
> > colnames( qr( attr( eval( nlsystemols$eq[[1]]$deriv ), "gradient" ) 
> > )$qr )
> 
> will return
> 
> > [1] "d0" "d1" "d2" "s0" "s1" "s2" "s3"
> 
> Is/Are there one of those great S or R function/shortcuts 
> that will return a
> submatrix(?) or a partioned matrix such that when I call 
> colnames() I get:
> 
> > [1] "d0" "d1" "d2"
> 
> Thanks,
> Jeff.
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list 
> https://www.stat.math.ethz.ch/mailman/listinfo> /r-help
>



From cmoffet at nwrc.ars.usda.gov  Mon Oct 13 21:02:02 2003
From: cmoffet at nwrc.ars.usda.gov (Corey Moffet)
Date: Mon, 13 Oct 2003 13:02:02 -0600
Subject: [R] extracting quoted text from character string
Message-ID: <3.0.6.32.20031013130202.0107f338@nwrc.ars.usda.gov>

Hello all,

I am trying to solve a problem, and my solution is rather ugly and not very
general.  The posts for "[R] help with gsub and grep functions" seemed
relevent
and gave me hope for a more refined and more general solution.

The Problem:

line <- "'this text has spaces' 'thisNot' 3 4 5 6 7 8 9 10"
bad.line <- "'this text has spaces' thisNot 3 4 5 6 7 8 9 10"

The desired result of a process on 'line' or "bad.line":

> parts <- some.function(line)

> parts
 [1] "this text has spaces"
 [2] "thisNot"
 [3] "3"
 [4] "4"
 [5] "5"
 [6] "6"
 [7] "7"
 [8] "8"
 [9] "9"
[10] "10"

Current function to obtain a solution for "line" but not "bad.line":

"some.function" <- function(line, quote.char = "'") {
   quoted <- unlist(strsplit(line, quote.char))
   quoted <- quoted[quoted != ""]
   first <- quoted[1]
   second <- quoted[3]
   last <- quoted[4]
   last.parts <-unlist(strsplit(last, " "))
   last.parts <- last.parts[last.parts != ""]
   out <- c(first, second, last.parts)
   return(out)
}

This solution is not very good because the text parts of "line" are not 
required to be enclosed in quotations unless it has a space.  All the files
I currently have to process have the first two pieces enclose in "'".  But
it is future files that I worry about.  Is there an existing function that
I have overlooked that splits strings, ignoring the delimiter when it is
enclosed in quotes?  I know that I can do some testing on the length of
"quoted" in function "some.function" but it seems there should be a more
elegent way of doing this type of thing. Any suggestions?

With best wishes and kind regards I am

Sincerely,

Corey A. Moffet, Ph.D.
Support Scientist

University of Idaho
Northwest Watershed Research Center
800 Park Blvd, Plaza IV, Suite 105
Boise, ID 83712-7716

Voice: (208) 422-0718
FAX:   (208) 334-1502



From brian_haag at yahoo.com  Mon Oct 13 21:03:23 2003
From: brian_haag at yahoo.com (Brian J. Haag)
Date: Mon, 13 Oct 2003 12:03:23 -0700 (PDT)
Subject: [R] conditional less than
Message-ID: <20031013190323.48404.qmail@web14006.mail.yahoo.com>

I'm sure this is a total noob question, but half an
hour of searching bore no fruit: How do you select a
subset of a vector by negative value?  If I try

> hist(sp$p[sp$r>0.01])   # all is well
> hist(sp$p[sp$r<-0.01])  # this obviously causes
issues

Also, putting -0.01 in parentheses didn't help.

Thanks in advance --

b



From cmoffet at nwrc.ars.usda.gov  Mon Oct 13 21:19:10 2003
From: cmoffet at nwrc.ars.usda.gov (Corey Moffet)
Date: Mon, 13 Oct 2003 13:19:10 -0600
Subject: [R] conditional less than
In-Reply-To: <20031013190323.48404.qmail@web14006.mail.yahoo.com>
Message-ID: <3.0.6.32.20031013131910.0107f338@nwrc.ars.usda.gov>

x <- rnorm(100)

x[x < -1]
x[x<(-1)]
# both work for me!

x[x<-1]
# as you note gives problems. 

At 12:03 PM 10/13/2003 -0700, Brian J. Haag wrote:
>I'm sure this is a total noob question, but half an
>hour of searching bore no fruit: How do you select a
>subset of a vector by negative value?  If I try
>
>> hist(sp$p[sp$r>0.01])   # all is well
>> hist(sp$p[sp$r<-0.01])  # this obviously causes
>issues
>
>Also, putting -0.01 in parentheses didn't help.
>
>Thanks in advance --
>
>b
>
>______________________________________________
>R-help at stat.math.ethz.ch mailing list
>https://www.stat.math.ethz.ch/mailman/listinfo/r-help
>
With best wishes and kind regards I am

Sincerely,

Corey A. Moffet, Ph.D.
Support Scientist

University of Idaho
Northwest Watershed Research Center
800 Park Blvd, Plaza IV, Suite 105
Boise, ID 83712-7716

Voice: (208) 422-0718
FAX:   (208) 334-1502



From hb at maths.lth.se  Mon Oct 13 21:22:01 2003
From: hb at maths.lth.se (Henrik Bengtsson)
Date: Mon, 13 Oct 2003 21:22:01 +0200
Subject: [R] conditional less than
In-Reply-To: <20031013190323.48404.qmail@web14006.mail.yahoo.com>
Message-ID: <000101c391bf$5ece8640$e502eb82@maths.lth.se>

Add a spaces!

hist(sp$p[sp$r > 0.01])   
hist(sp$p[sp$r < -0.01])  

compare with

hist(sp$p[sp$r <- 0.01])  

which is what you tried.

Cheers.

Henrik Bengtsson
Lund University


> -----Original Message-----
> From: r-help-bounces at stat.math.ethz.ch 
> [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Brian J. Haag
> Sent: den 13 oktober 2003 21:03
> To: r-help at stat.math.ethz.ch
> Subject: [R] conditional less than
> 
> 
> I'm sure this is a total noob question, but half an
> hour of searching bore no fruit: How do you select a
> subset of a vector by negative value?  If I try
> 
> > hist(sp$p[sp$r>0.01])   # all is well
> > hist(sp$p[sp$r<-0.01])  # this obviously causes
> issues
> 
> Also, putting -0.01 in parentheses didn't help.
> 
> Thanks in advance --
> 
> b
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list 
> https://www.stat.math.ethz.ch/mailma> n/listinfo/r-help
> 
>



From spencer.graves at pdf.com  Mon Oct 13 21:30:30 2003
From: spencer.graves at pdf.com (Spencer Graves)
Date: Mon, 13 Oct 2003 12:30:30 -0700
Subject: [R] conditional less than
In-Reply-To: <20031013190323.48404.qmail@web14006.mail.yahoo.com>
References: <20031013190323.48404.qmail@web14006.mail.yahoo.com>
Message-ID: <3F8AFD56.3@pdf.com>

Putting -0.01 in parenthesis works for me.  Consider the following: 

 > DF <- data.frame(a=c(-2,2), b=3:4)
 > DF[DF$a<(-1),]
   a b
1 -2 3

However, if I first do it without parentheses, then I've changed DF 
before I attempt to subset it: 

 DF[DF$a<-0.01,]
[1] a b
<0 rows> (or 0-length row.names)
 > DF
     a b
1 0.01 3
2 0.01 4

This can create other problems, because now length(DF$a) == 1 (even 
though it printed as length 2): 

 > DF[DF$a<-1,]
     a b
1 0.01 3

hope this helps.  spencer graves

Brian J. Haag wrote:

>I'm sure this is a total noob question, but half an
>hour of searching bore no fruit: How do you select a
>subset of a vector by negative value?  If I try
>
>  
>
>>hist(sp$p[sp$r>0.01])   # all is well
>>hist(sp$p[sp$r<-0.01])  # this obviously causes
>>    
>>
>issues
>
>Also, putting -0.01 in parentheses didn't help.
>
>Thanks in advance --
>
>b
>
>______________________________________________
>R-help at stat.math.ethz.ch mailing list
>https://www.stat.math.ethz.ch/mailman/listinfo/r-help
>  
>



From brian_haag at yahoo.com  Mon Oct 13 21:33:50 2003
From: brian_haag at yahoo.com (Brian J. Haag)
Date: Mon, 13 Oct 2003 12:33:50 -0700 (PDT)
Subject: [R] conditional less than
In-Reply-To: <3F8AFD56.3@pdf.com>
Message-ID: <20031013193350.74016.qmail@web14005.mail.yahoo.com>

Thanks to all for the help ... I rather immediately
realized (*blush*) that I had run it incorrectly the
first time and blown up my series, so of course
nothing else worked either.  Parentheses is all that
was needed.

Sorry for the wasted bandwidth.

b



From vasileios_p at yahoo.gr  Mon Oct 13 21:58:34 2003
From: vasileios_p at yahoo.gr (=?iso-8859-7?q?vasilis=20pappas?=)
Date: Mon, 13 Oct 2003 20:58:34 +0100 (BST)
Subject: [R] contigency tables
Message-ID: <20031013195834.60783.qmail@web12907.mail.yahoo.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20031013/6522233d/attachment.pl

From p.murrell at auckland.ac.nz  Mon Oct 13 22:19:16 2003
From: p.murrell at auckland.ac.nz (Paul Murrell)
Date: Tue, 14 Oct 2003 09:19:16 +1300
Subject: [R] Rotate a plot, and subplot
References: <3F8AC08B.4020407@unibas.ch>
Message-ID: <3F8B08C4.2050309@stat.auckland.ac.nz>

Hi


Pascal A. Niklaus wrote:
> Hi all,
> 
> Is there a way to rotate a plot, e.g. a histogram, by a certain angle 
> (90/180/270 degress)? I spent hours trying to figure out how this is 
> done, but without success.


Some plots provide an argument (e.g., the horiz argument in barplot), 
but there is no general rotation transformation available for all base 
plots.

Viewports in the grid package provide arbitrary rotations, and these can 
be used with lattice plots.  This may take a bit of digesting, but since 
you're prepared to spend hours on it :) ... here's a relatively simple 
example:

myhist <- histogram(rnorm(50))
# Make two square regions side by side
push.viewport(viewport(layout=grid.layout(1, 2, respect=TRUE)))
# Go to the left region
push.viewport(viewport(layout.pos.col=1))
# Draw the histogram in normal orientation
print(myhist, newpage=FALSE)
pop.viewport()
# Go to the right region then rotate 90 degrees
push.viewport(viewport(layout.pos.col=2), viewport(angle=90))
# Draw the histogram (rotated 90 degrees)
print(myhist, newpage=FALSE)
pop.viewport(3)


That only works because I made the regions square (so a region is the 
same size when it is rotated 90 degrees).  Here's a slightly more 
complex example which will work for non-square regions ...

# Create a region in the left half of the page
push.viewport(viewport(x=0, width=0.5, just="left"))
# Draw the histogram in normal orientation
print(myhist, newpage=FALSE)
pop.viewport()
# Create a region in the right half of the page
# which is rotated 90 degrees
push.viewport(viewport(x=0.75,
                        # Make the rotated width the same as
                        # the height of the page
                        width=grid.convert(unit(1, "npc"), "npc",
                          "y", "dimension", "x", "dimension"),
                        # Make the rotated height half the
                        # width of the page
                        height=grid.convert(unit(0.5, "npc"), "npc",
                          "x", "dimension", "y", "dimension"),
                        angle=90))
# Draw the histogram (rotated 90 degrees)
print(myhist, newpage=FALSE)
pop.viewport()


> Also, I'm looking for an equivalent to the S-Plus "subplot" command to 
> insert a kind of "thumbnail" graphic into a bigger one. How is this best 
> done in R?


You can use par(fig) and/or par(plt) and par(new) to do some things, but 
these are not terribly helpful for positioning subplots within another plot.

Again, grid viewports can be used to create subregions with a lot of 
flexibility.  There is a package on CRAN called gridBase which allows 
you to create a region using a grid viewport, then draw a standard plot 
in that region (with a few restrictions).  There is a vignette with the 
package that tries to explain how it works -- the last example in there 
embeds some piecharts as subplots within a scatterplot.

Hope that helps.  Feel free to contact me directly if you have further 
questions;  I'd be interested to hear more about what you are trying to 
achieve.

Paul
-- 
Dr Paul Murrell
Department of Statistics
The University of Auckland
Private Bag 92019
Auckland
New Zealand
64 9 3737599 x85392
paul at stat.auckland.ac.nz
http://www.stat.auckland.ac.nz/~paul/



From zeileis at ci.tuwien.ac.at  Mon Oct 13 23:20:08 2003
From: zeileis at ci.tuwien.ac.at (Achim Zeileis)
Date: Mon, 13 Oct 2003 23:20:08 +0200
Subject: [R] contigency tables
In-Reply-To: <20031013195834.60783.qmail@web12907.mail.yahoo.com>
References: <20031013195834.60783.qmail@web12907.mail.yahoo.com>
Message-ID: <200310132120.h9DLK8bc014682@thorin.ci.tuwien.ac.at>

On Monday 13 October 2003 21:58, vasilis pappas wrote:

> Hello everybody,
> Can anyone tell me how I could analyze data that are at a contigency
> table form? I already found function cfa in the cfa package but I
> still don't understand how I could use this function in order to
> elaborate a contigency table. Every answer is welcome!

I haven't looked at the cfa package, but the usual approach to analyze 
contingency tables would usually involve the functions:
  o table(), ftable(), xtabs() to generate contingency tables
  o chisq.test(), loglin(), loglm() to perform tests and fit
    loglinear models (the latter function is in the package MASS)
See also the links on the respective help pages.

hth,
Z

>
>
> ---------------------------------
>
> ????????? ??? ?????? ???@yahoo.gr
>
> 	[[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help



From AdamL at spc.int  Tue Oct 14 00:19:34 2003
From: AdamL at spc.int (Adam Langley)
Date: Tue, 14 Oct 2003 09:19:34 +1100
Subject: [R] opening a WinZip file
Message-ID: <27DF1E0087070642B128C853E74FCCAF03C5891F@tazar.spc.int>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20031014/d86ad668/attachment.pl

From ggrothendieck at myway.com  Tue Oct 14 01:54:01 2003
From: ggrothendieck at myway.com (Gabor Grothendieck)
Date: Mon, 13 Oct 2003 19:54:01 -0400 (EDT)
Subject: [R] help with gsub and grep functions
Message-ID: <20031013235401.E85DD39DC@xmxpita.myway.com>


If you split the strings using strsplit:

s <- strsplit(Names," *[()]")  # remove space * if trailing space OK

Then the two results are:

sapply(s,"[",-2)
sapply(s,"[",2)


---

Date: Mon, 13 Oct 2003 16:30:37 +0200 
From: Martin Olivier <martinol at ensam.inra.fr>
Subject: [R] help with gsub and grep functions 
 
Hi all,

Let Names a vector of chatacters. For example,

> Names
[1] "g 604 be-0 -p1 (602 matches)" "g 606 Phli-0 -p2 (517 matches)"
[3] "g 608 alu-0 (659 matches)"

I try to use gsub or grep functions for two problems :

1. First, I would like to delete all the characters between parentheses.
[1] "g 604 be-0 -p1" "g 606 be-0 -p2"
[3] "g 608 be-0 -p3"

2. And, I would like to extract the characters between parentheses
[1] "602 matches" "517 matches"
[3] "659 matches"

Any idea?

_______________________________________________
No banners. No pop-ups. No kidding.
Introducing My Way - http://www.myway.com



From ken_lee at tynesys.com  Tue Oct 14 03:04:34 2003
From: ken_lee at tynesys.com (Ken Lee)
Date: Tue, 14 Oct 2003 09:04:34 +0800
Subject: [R] What's the message "locale not supported by C library" ?
In-Reply-To: <85zng6irjg.fsf@blindglobe.net>
Message-ID: <FFEKIEFDONDECJDODGDJIEOKCKAA.ken_lee@tynesys.com>

Dear all,
      As I run a png(...) function I got the message "locale not supported by C library" and then core dump at unix system.
What's I can do for this trouble?

 Thanks
Ken



From ggrothendieck at myway.com  Tue Oct 14 04:00:02 2003
From: ggrothendieck at myway.com (Gabor Grothendieck)
Date: Mon, 13 Oct 2003 22:00:02 -0400 (EDT)
Subject: [R] extracting quoted text from character string
Message-ID: <20031014020002.867983980@xmxpita.myway.com>



The first line in the body of the function splits the input, s, 
using the separator and makes it a list.

The second line and third lines define a regexp which matches 
leading and trailing whitespace and define a logical vector which
selects the odd positioned elements.

Since we know that the odd positioned elements are not between
quotes, in the fourth line we remove any leading and 
trailing whitespace from them and split them on whitespace.

In the fifth line we convert s from list to vector.

test <- function(s,sep="'") {
   s <- lapply(strsplit(s,sep)[[1]],c)
   re <- "^[[:space:]]+|[[:space:]]+$"
   odd <- seq(along=s)%%2 == 1
   s[odd] <- strsplit( gsub(re,"",unlist(s)[odd]), "[[:space:]]+" )
   unlist(s)
}

test(line)
test(bad.line)



---
From: Corey Moffet <cmoffet at nwrc.ars.usda.gov>
 
Hello all,

I am trying to solve a problem, and my solution is rather ugly and not very
general. The posts for "[R] help with gsub and grep functions" seemed
relevent
and gave me hope for a more refined and more general solution.

The Problem:

line <- "'this text has spaces' 'thisNot' 3 4 5 6 7 8 9 10"
bad.line <- "'this text has spaces' thisNot 3 4 5 6 7 8 9 10"

The desired result of a process on 'line' or "bad.line":

> parts <- some.function(line)

> parts
[1] "this text has spaces"
[2] "thisNot"
[3] "3"
[4] "4"
[5] "5"
[6] "6"
[7] "7"
[8] "8"
[9] "9"
[10] "10"

Current function to obtain a solution for "line" but not "bad.line":

"some.function" <- function(line, quote.char = "'") {
quoted <- unlist(strsplit(line, quote.char))
quoted <- quoted[quoted != ""]
first <- quoted[1]
second <- quoted[3]
last <- quoted[4]
last.parts <-unlist(strsplit(last, " "))
last.parts <- last.parts[last.parts != ""]
out <- c(first, second, last.parts)
return(out)
}

This solution is not very good because the text parts of "line" are not 
required to be enclosed in quotations unless it has a space. All the files
I currently have to process have the first two pieces enclose in "'". But
it is future files that I worry about. Is there an existing function that
I have overlooked that splits strings, ignoring the delimiter when it is
enclosed in quotes? I know that I can do some testing on the length of
"quoted" in function "some.function" but it seems there should be a more
elegent way of doing this type of thing. Any suggestions?

With best wishes and kind regards I am



_______________________________________________
No banners. No pop-ups. No kidding.
Introducing My Way - http://www.myway.com



From Paul.Sorenson at vision-bio.com  Tue Oct 14 08:00:24 2003
From: Paul.Sorenson at vision-bio.com (Paul Sorenson)
Date: Tue, 14 Oct 2003 16:00:24 +1000
Subject: [R] RPy for windows and python 2.3
Message-ID: <5E06BFED29594F4C9C5EBE230DE320C62739AA@ewok.vsl.com.au>

Does anyone have an RPy installer or some way of getting RPy to work with python 2.3 on windows? I am using R 1.7.1.

I have tried compiling with several different compilers from the source and the best I got was parser errors in header files with mingw32.

paul



From v.demart at libero.it  Tue Oct 14 09:20:34 2003
From: v.demart at libero.it (v.demart@libero.it)
Date: Tue, 14 Oct 2003 09:20:34 +0200
Subject: [R] Organized examples for newbyes
Message-ID: <HMQKEA$97FDBA7E24B0D729E490032316D0C6B1@libero.it>

I'm learning R from scratch on my linux box, being deeply biased at work by those graphical programs, nice to look but often poor in content and almost generally limited, running under M$ Windows. 

I wonder if someone out there can suggest to an absolute beginner as I am where to find a collection of examples of R-code (or Splus, as I understand) to play with in order to learn R quicker.

The collection should:
1) be in English, or French, or, of course, Italian,
2) be concentrated in one or at most a handful of sites,  
3) contain examples of gradually increasing complexity.

Such a collection could be extremely helpful to convince other people at work to move to R.

Thanks & Ciao

Vittorio



From ripley at stats.ox.ac.uk  Tue Oct 14 09:53:36 2003
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Tue, 14 Oct 2003 08:53:36 +0100 (BST)
Subject: [R] Organized examples for newbyes
In-Reply-To: <HMQKEA$97FDBA7E24B0D729E490032316D0C6B1@libero.it>
Message-ID: <Pine.LNX.4.44.0310140848190.3627-100000@gannet.stats>

Look at the collection of books in the FAQ, Q2.7.  For you, I would add

Laboratorio di statistica con R 
by S. M. Iacus and G. Masarotto, McGraw-Hill, ISBN 88-386-6084-0

which my Italian student found very helpful.

I don't think examples without explanations would be anything like 
as helpful.

On Tue, 14 Oct 2003, v.demart at libero.it wrote:

> I'm learning R from scratch on my linux box, being deeply biased at work
> by those graphical programs, nice to look but often poor in content and
> almost generally limited, running under M$ Windows.
> 
> I wonder if someone out there can suggest to an absolute beginner as I
> am where to find a collection of examples of R-code (or Splus, as I
> understand) to play with in order to learn R quicker.
> 
> The collection should:
> 1) be in English, or French, or, of course, Italian,
> 2) be concentrated in one or at most a handful of sites,  
> 3) contain examples of gradually increasing complexity.
> 
> Such a collection could be extremely helpful to convince other people at
> work to move to R.
> 
> Thanks & Ciao
> 
> Vittorio

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From johannes.huesing at medizin.uni-essen.de  Tue Oct 14 09:56:37 2003
From: johannes.huesing at medizin.uni-essen.de (=?iso-8859-1?Q?=22H=FCsing=2C_Johannes=22?=)
Date: Tue, 14 Oct 2003 09:56:37 +0200
Subject: [R] Organized examples for newbyes
Message-ID: <B3A80C9C13928B45B2FCE4C43656363A0194E2A2@mail-srv02.master.medizin.uni-essen.de>

> I wonder if someone out there can suggest to an absolute 
> beginner as I am where to find a collection of examples of 
> R-code (or Splus, as I understand) to play with in order to 
> learn R quicker.

How about the examples provided in the help to
various functions? Let's see:

> 
> The collection should:
> 1) be in English, or French, or, of course, Italian,

in English (check),

> 2) be concentrated in one or at most a handful of sites,  

concentrated on your box (check),

> 3) contain examples of gradually increasing complexity.

ok, the examples are at varying complexity, not necessarily
increasing. Yet many people gain a lot of insight when looking
at the examples.



From tobias.verbeke at bivv.be  Tue Oct 14 10:05:47 2003
From: tobias.verbeke at bivv.be (tobias.verbeke@bivv.be)
Date: Tue, 14 Oct 2003 10:05:47 +0200
Subject: Betr.: [R] Organized examples for newbyes
In-Reply-To: <HMQKEA$97FDBA7E24B0D729E490032316D0C6B1@libero.it>
Message-ID: <OF200A8C48.116A89CE-ONC1256DBF.002B8D3A-C1256DBF.002C5EE3@BIVV.BE>







r-help-bounces at stat.math.ethz.ch wrote on 14/10/2003 09:20:34:

> I'm learning R from scratch on my linux box, being deeply biased at
> work by those graphical programs, nice to look but often poor in
> content and almost generally limited, running under M$ Windows.
>
> I wonder if someone out there can suggest to an absolute beginner as
> I am where to find a collection of examples of R-code (or Splus, as
> I understand) to play with in order to learn R quicker.
>
> The collection should:
> 1) be in English, or French, or, of course, Italian,
> 2) be concentrated in one or at most a handful of sites,
> 3) contain examples of gradually increasing complexity.
>
This site will not respond to all of the conditions,
but I learned a lot from it.

http://www.ku.edu/~pauljohn/R/Rtips.html

Do not forget to explore all of the contributed docs
you can find on CRAN (section Documentation, Contributed).

HTH,

Tobias



From John.Marsland at CommerzbankIB.com  Tue Oct 14 10:09:20 2003
From: John.Marsland at CommerzbankIB.com (Marsland, John)
Date: Tue, 14 Oct 2003 09:09:20 +0100
Subject: [R] opening a WinZip file
Message-ID: <8CBAA121CEB4D5118CB200508BB2BBEF0317E9E2@xmx8lonib.lonib.commerzbank.com>

I use the following to read files from inside a .zip file directly:

z <- unz("C:/temp/my.zip", "/the/full/path/my.csv", open="r")
mydata <-
read.csv(z,skip=4,header=F,as.is=T,colClasses=c("character","character"))
close(z)

obviously you should use whatever options are appropriate with read.csv
but the important thing is to reference the full saved path of the file
within the zip file.

This works fine, but there is an intermitent "bug" about seek not working on
this connection - if you re-run it alwys goes away.

Regards,

John
> -----Original Message-----
> From: Adam Langley [mailto:AdamL at spc.int]
> Sent: 13 October 2003 23:20
> To: r-help at stat.math.ethz.ch
> Subject: [R] opening a WinZip file
> 
> 
> Hello
> 
> I am trying to read a large number of WinZip files from 
> inside R (v 1.7.1,
> options()$unzip = internal). I have unsuccessfully tried to 
> unpack the files
> and write to an external directory so that I could read the 
> text files using
> zip.unpack. Reading some of the archives suggests that this 
> is for .ZIP
> format files only. 
> 
> I have also unsuccessfully tried to use gzfile to open a 
> connection and then
> read.
> 
> Any suggestions?
> 
> Thanks in advance.
> 
> Adam
> 
> ------------------------------------------------------------------
> Adam Langley
> Principal Fisheries Scientist 
> Oceanic Fisheries Programme
> Secretariat of the Pacific Community
> BP D5, 98848 Noumea Cedex, New Caledonia 
> Tel:	+687 262 000 (Ext. 192)
> Fax:	+687 263 818
> 
> 
> 
> 
> ---
> 
> 
> 
>  
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> 


********************************************************************** 
This is a commercial communication from Commerzbank AG.\ \ T...{{dropped}}



From mkondrin at hppi.troitsk.ru  Tue Oct 14 14:35:00 2003
From: mkondrin at hppi.troitsk.ru (M.Kondrin)
Date: Tue, 14 Oct 2003 12:35:00 +0000
Subject: [R] What's the message "locale not supported by C library" ?
References: <FFEKIEFDONDECJDODGDJIEOKCKAA.ken_lee@tynesys.com>
Message-ID: <3F8BED74.4010505@hppi.troitsk.ru>

Ken Lee wrote:

>Dear all,
>      As I run a png(...) function I got the message "locale not supported by C library" and then core dump at unix system.
>What's I can do for this trouble?
>
> Thanks
>Ken
>
>______________________________________________
>R-help at stat.math.ethz.ch mailing list
>https://www.stat.math.ethz.ch/mailman/listinfo/r-help
>
>
>  
>
This is a standard error message of Xlib indicating what it doesn't 
understand system locale (command locale). You should tweak your system 
settings or (as a workaround) you can set locale to default posix one 
(export LC_ALL=POSIX) before calling R (for example add this line to R 
script) - although in this case you will have only ASCII charset



From alessandro.semeria at cramont.it  Tue Oct 14 10:24:11 2003
From: alessandro.semeria at cramont.it (alessandro.semeria@cramont.it)
Date: Tue, 14 Oct 2003 10:24:11 +0200
Subject: [R] Organized examples for newbyes
Message-ID: <OF58796E31.BD477F59-ONC1256DBF.002CA9EE@tomware.it>


Look at the CRAN  closest to you, I think was
http://cran.at.r-project.org/, on the section
Documentation/Contributed:
- "Il linguaggio R: concetti introduttivi ed esempi" by Vito M. R. Muggeo
(30 pages for beginners in Italian).
- "Simple R" by John Verzani (data sets, various PDF, PS and a browsable
HTML version are available at the Simple R homepage).
After this lectures and exercises I think you should able to find alone
more specific doc.
Best!
A.S.

----------------------------

Alessandro Semeria
Models and Simulations Laboratory
Montecatini Environmental Research Center (Edison Group),
Via Ciro Menotti 48,
48023 Marina di Ravenna (RA), Italy
Tel. +39 544 536811
Fax. +39 544 538663
E-mail: alessandro.semeria at cramont.it



From arc at arcriswell.com  Tue Oct 14 10:48:08 2003
From: arc at arcriswell.com (Andrew Criswell)
Date: Tue, 14 Oct 2003 15:48:08 +0700
Subject: [R] Problems with arrow keys
References: <1066046467.6737.37.camel@chomsky.estadi.uv.es>
Message-ID: <3F8BB848.1040805@arcriswell.com>

Hello:

I just built R-1.8.0 on my Linux-Mandrake 9.0 using the commands,

./configure --enable-R-shlib
make
make install

It all seems to have gone fine.  But when I open R and use the arrow 
keys to try to go back to previous lines, it doesn't work.  What I get 
instead is something like.

 > ^[[A^[[A^[[A^[[A^[[A

I didn't have this problem when I installed R-1.7.1 as rpm.  I could 
make use of the arrow keys.

Help!

ANDREW



From michael.watson at bbsrc.ac.uk  Tue Oct 14 10:56:27 2003
From: michael.watson at bbsrc.ac.uk (michael watson (IAH-C))
Date: Tue, 14 Oct 2003 09:56:27 +0100
Subject: [R] Number Format
Message-ID: <20B7EB075F2D4542AFFAF813E98ACD9301C00BE2@cl-exsrv1.irad.bbsrc.ac.uk>

Hi

A very simple question on number formats.  After some calculations, some variables I have come out looking like this:

-1.892972e+00

Now apart from the fact that the "e+00" is completely redundant, I would rather have the number represented without the e bit.  I want to export data like this to a text file and would rather have:

-1.892972

than

-1.892972e+00

I have tried options(digits = 12) or something similar but that doesn't seem to help.  How can I get R to export this data without the e+00?

Thanks
Mick



From alessandro.semeria at cramont.it  Tue Oct 14 11:44:12 2003
From: alessandro.semeria at cramont.it (alessandro.semeria@cramont.it)
Date: Tue, 14 Oct 2003 11:44:12 +0200
Subject: [R] Number Format
Message-ID: <OFA72E7A8F.0CC6244A-ONC1256DBF.003529FA@tomware.it>


Probably you have set options(scipen=some negative number),
try with options(scipen=3).
options(digits=#) for the number of digit.
Best
A.S.

----------------------------

Alessandro Semeria
Models and Simulations Laboratory
Montecatini Environmental Research Center (Edison Group),
Via Ciro Menotti 48,
48023 Marina di Ravenna (RA), Italy
Tel. +39 544 536811
Fax. +39 544 538663
E-mail: alessandro.semeria at cramont.it



From ripley at stats.ox.ac.uk  Tue Oct 14 11:48:27 2003
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Tue, 14 Oct 2003 10:48:27 +0100 (BST)
Subject: [R] Number Format
In-Reply-To: <20B7EB075F2D4542AFFAF813E98ACD9301C00BE2@cl-exsrv1.irad.bbsrc.ac.uk>
Message-ID: <Pine.LNX.4.44.0310141046270.4557-100000@gannet.stats>

On Tue, 14 Oct 2003, michael watson (IAH-C) wrote:

> A very simple question on number formats.  After some calculations, some
> variables I have come out looking like this:
> 
> -1.892972e+00
> 
> Now apart from the fact that the "e+00" is completely redundant, I would
> rather have the number represented without the e bit.  I want to export
> data like this to a text file and would rather have:
> 
> -1.892972
> 
> than
> 
> -1.892972e+00
> 
> I have tried options(digits = 12) or something similar but that doesn't
> seem to help.  How can I get R to export this data without the e+00?

I think you need to tell us how you got it to output that way!  I get

> -1.892972e+00
[1] -1.892972

So I suspect this is part of a set of numbers, all in scientific format.
You can control that with options(scipen=) in R 1.8.0: see its help page.

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From Ted.Harding at nessie.mcc.ac.uk  Tue Oct 14 12:31:02 2003
From: Ted.Harding at nessie.mcc.ac.uk ( (Ted Harding))
Date: Tue, 14 Oct 2003 11:31:02 +0100 (BST)
Subject: [R] Number Format
In-Reply-To: <20B7EB075F2D4542AFFAF813E98ACD9301C00BE2@cl-exsrv1.irad.bbsrc.ac.uk>
Message-ID: <XFMail.031014113102.Ted.Harding@nessie.mcc.ac.uk>

On 14-Oct-03 michael watson (IAH-C) wrote:
> A very simple question on number formats.  After some calculations,
> some variables I have come out looking like this:
> [...]
> -1.892972e+00
> 
> I have tried options(digits = 12) or something similar but that doesn't
> seem to help.  How can I get R to export this data without the e+00?

Try formatC:

  pnorm(-6.1)
  [1] 5.303423e-10

  formatC(pnorm(-6.1),format="f",digits=15)
  [1] "0.000000000530342"

Ted.


--------------------------------------------------------------------
E-Mail: (Ted Harding) <Ted.Harding at nessie.mcc.ac.uk>
Fax-to-email: +44 (0)870 167 1972
Date: 14-Oct-03                                       Time: 11:31:02
------------------------------ XFMail ------------------------------



From glaziou at pasteur-kh.org  Tue Oct 14 13:37:02 2003
From: glaziou at pasteur-kh.org (Philippe Glaziou)
Date: Tue, 14 Oct 2003 18:37:02 +0700
Subject: [R] Problems with arrow keys
In-Reply-To: <3F8BB848.1040805@arcriswell.com>
References: <1066046467.6737.37.camel@chomsky.estadi.uv.es>
	<3F8BB848.1040805@arcriswell.com>
Message-ID: <20031014113702.GB1969@pasteur-kh.org>

Andrew Criswell <arc at arcriswell.com> wrote:
> I just built R-1.8.0 on my Linux-Mandrake 9.0 using the commands,
> 
> ./configure --enable-R-shlib
> make
> make install
> 
> It all seems to have gone fine.  But when I open R and use the arrow 
> keys to try to go back to previous lines, it doesn't work.  What I get 
> instead is something like.
> 
> > ^[[A^[[A^[[A^[[A^[[A
> 
> I didn't have this problem when I installed R-1.7.1 as rpm.  I could 
> make use of the arrow keys.


This was answered recently. Check the archives.

-- 
Philippe



From chrysopa at insecta.ufv.br  Tue Oct 14 11:14:34 2003
From: chrysopa at insecta.ufv.br (Ronaldo Reis Jr.)
Date: Tue, 14 Oct 2003 07:14:34 -0200
Subject: [R] different results depending of variable position.
Message-ID: <200310140714.34219.chrysopa@insecta.ufv.br>

Hi,

I make an analysis and depending of the order of the variables, the 
significance change, look.

m1 <- glm((infec/ntot)~idade+sexo+peso,family=binomial,weights=ntot)
> anova(m1,test="F")
Analysis of Deviance Table

Model: binomial, link: logit

Response: (infec/ntot)

Terms added sequentially (first to last)


      Df Deviance Resid. Df Resid. Dev       F    Pr(>F)    
NULL                     80     83.234                      
idade  1    1.302        79     81.932  1.3020 0.2538510    
sexo   1    9.137        78     72.796  9.1366 0.0025055 ** 
peso   1   12.937        77     59.859 12.9373 0.0003221 ***
---
Signif. codes:  0 `***' 0.001 `**' 0.01 `*' 0.05 `.' 0.1 ` ' 1 
> m1 <- glm((infec/ntot)~sexo+peso+idade,family=binomial,weights=ntot)
> anova(m1,test="F")
Analysis of Deviance Table

Model: binomial, link: logit

Response: (infec/ntot)

Terms added sequentially (first to last)


      Df Deviance Resid. Df Resid. Dev       F    Pr(>F)    
NULL                     80     83.234                      
sexo   1    8.278        79     74.956  8.2780 0.0040128 ** 
peso   1   11.171        78     63.785 11.1711 0.0008308 ***
idade  1    3.927        77     59.859  3.9268 0.0475237 *  
---
Signif. codes:  0 `***' 0.001 `**' 0.01 `*' 0.05 `.' 0.1 ` ' 1 
> m1 <- glm((infec/ntot)~peso+idade+sexo,family=binomial,weights=ntot)
> anova(m1,test="F")
Analysis of Deviance Table

Model: binomial, link: logit

Response: (infec/ntot)

Terms added sequentially (first to last)


      Df Deviance Resid. Df Resid. Dev       F    Pr(>F)    
NULL                     80     83.234                      
peso   1   15.162        79     68.072 15.1623 9.865e-05 ***
idade  1    2.773        78     65.299  2.7731   0.09586 .  
sexo   1    5.440        77     59.859  5.4405   0.01968 *  
---
Signif. codes:  0 `***' 0.001 `**' 0.01 `*' 0.05 `.' 0.1 ` ' 1 
> m1 <- glm((infec/ntot)~idade+peso+sexo,family=binomial,weights=ntot)
> anova(m1,test="F")
Analysis of Deviance Table

Model: binomial, link: logit

Response: (infec/ntot)

Terms added sequentially (first to last)


      Df Deviance Resid. Df Resid. Dev       F    Pr(>F)    
NULL                     80     83.234                      
idade  1    1.302        79     81.932  1.3020   0.25385    
peso   1   16.633        78     65.299 16.6334 4.534e-05 ***
sexo   1    5.440        77     59.859  5.4405   0.01968 *  
---
Signif. codes:  0 `***' 0.001 `**' 0.01 `*' 0.05 `.' 0.1 ` ' 1 
> 

Why this?

How the best method to select the model (with idade or without idade)? AIC?

Thanks
Ronaldo
-- 

Entre dois pecados, eu sempre escolho o que ainda n?o cometi

--Mae West
--
|>   // | \\   [***********************************]
|   ( ?   ? )  [Ronaldo Reis J?nior                ]
|>      V      [UFV/DBA-Entomologia                ]
|    /     \   [36571-000 Vi?osa - MG              ]
|>  /(.''`.)\  [Fone: 31-3899-2532                 ]
|  /(: :'  :)\ [chrysopa at insecta.ufv.br            ]
|>/ (`. `'` ) \[ICQ#: 5692561 | LinuxUser#: 205366 ]
|    ( `-  )   [***********************************]
|>>  _/   \_Powered by GNU/Debian Woody/Sarge



From kjetil at entelnet.bo  Tue Oct 14 14:16:00 2003
From: kjetil at entelnet.bo (kjetil@entelnet.bo)
Date: Tue, 14 Oct 2003 08:16:00 -0400
Subject: [R] contigency tables
In-Reply-To: <20031013195834.60783.qmail@web12907.mail.yahoo.com>
Message-ID: <3F8BB0C0.22020.14030C@localhost>

On 13 Oct 2003 at 20:58, vasilis pappas wrote:

(you should use a mailer which wraps long lines)

Maybe you coulf be more specific? What do you want to do with the 
contingency table. Test for independence with chisq.test(), 
but of course it is often more interesting to describe 
deviations from independence!

There are correspondende analysis, for example 
corresp() from MASS, and sopme other packages to
(look at ade4). There are multiple correspondence analysis, 
mca from MASS. 

There are liglinear models, use help.search("loglinear"). 

If some body knows what cfa does, please inform me to!!

Kjetil Halvorsen


> Hello everybody,
> Can anyone tell me how I could analyze data that are at a contigency table form? 
I already found function cfa in the cfa package but I still don't understand how I could use this function in order to 
elaborate a contigency table. Every answer is welcome!
> 
> 
> 
> ---------------------------------
> 
> ????????? ??? ?????? ???@yahoo.gr
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help



From andy_liaw at merck.com  Tue Oct 14 14:18:00 2003
From: andy_liaw at merck.com (Liaw, Andy)
Date: Tue, 14 Oct 2003 08:18:00 -0400
Subject: [R] different results depending of variable position.
Message-ID: <3A822319EB35174CA3714066D590DCD50205CC84@usrymx25.merck.com>

> From: Ronaldo Reis Jr. [mailto:chrysopa at insecta.ufv.br] 
> 
> Hi,
> 
> I make an analysis and depending of the order of the variables, the 
> significance change, look.
 
[output of glm fits omitted] 

> Why this?

Because, as the output says:

> Terms added sequentially (first to last)

When the predictors are not orthogonal (i.e., correlation=0), the question
"is variable X significant" depends on the model that's being fitted.  The
significance test that anova() performs the following comparisons, assuming
X1 through X4 are the variables:

Y ~ 1   vs.  Y ~ X1
Y ~ X1  vs.  Y ~ X1 + X2
Y ~ X1 + X2  vs.  Y ~ X1 + X2 + X3
Y ~ X1 + X2 + X3  vs.  Y ~ X1 + X2 + X3 + X4

> How the best method to select the model (with idade or 
> without idade)? AIC?

That also depends on what you are looking for in the model.  If you are
looking for interpretation, probably the answer is not to select models, as
that could lead to bias in the coefficients of the selected model.

HTH,
Andy

 
> Thanks
> Ronaldo
> -- 
> 
> Entre dois pecados, eu sempre escolho o que ainda n?o cometi
> 
> --Mae West
> --
> |>   // | \\   [***********************************]
> |   ( ?   ? )  [Ronaldo Reis J?nior                ]
> |>      V      [UFV/DBA-Entomologia                ]
> |    /     \   [36571-000 Vi?osa - MG              ]
> |>  /(.''`.)\  [Fone: 31-3899-2532                 ]
> |  /(: :'  :)\ [chrysopa at insecta.ufv.br            ]
> |>/ (`. `'` ) \[ICQ#: 5692561 | LinuxUser#: 205366 ]
> |    ( `-  )   [***********************************]
> |>>  _/   \_Powered by GNU/Debian Woody/Sarge
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list 
> https://www.stat.math.ethz.ch/mailman/listinfo> /r-help
>



From p.dalgaard at biostat.ku.dk  Tue Oct 14 14:37:49 2003
From: p.dalgaard at biostat.ku.dk (Peter Dalgaard BSA)
Date: 14 Oct 2003 14:37:49 +0200
Subject: [R] different results depending of variable position.
In-Reply-To: <200310140714.34219.chrysopa@insecta.ufv.br>
References: <200310140714.34219.chrysopa@insecta.ufv.br>
Message-ID: <x2ad843sgy.fsf@biostat.ku.dk>

"Ronaldo Reis Jr." <chrysopa at insecta.ufv.br> writes:

> Hi,
> 
> I make an analysis and depending of the order of the variables, the 
> significance change, look.
> 
> m1 <- glm((infec/ntot)~idade+sexo+peso,family=binomial,weights=ntot)
> > anova(m1,test="F")
> Analysis of Deviance Table
> 
> Model: binomial, link: logit
> 
> Response: (infec/ntot)
> 
> Terms added sequentially (first to last)
> 
...
> 
> Why this?

Because terms are added sequentially.  Adding a variable to a model
means different things depending on what else is in the model. Most
textbooks on regression will explain this.

> How the best method to select the model (with idade or without idade)? AIC?

I'd suggest looking at a direct comparison:

m1 <- glm((infec/ntot)~idade+sexo+peso,family=binomial,weights=ntot)
m2 <- glm((infec/ntot)~sexo+peso,family=binomial,weights=ntot)
anova(m1,m2,test="F")

        -p

-- 
   O__  ---- Peter Dalgaard             Blegdamsvej 3  
  c/ /'_ --- Dept. of Biostatistics     2200 Cph. N   
 (*) \(*) -- University of Copenhagen   Denmark      Ph: (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)             FAX: (+45) 35327907



From rlandis at middlebury.edu  Tue Oct 14 14:58:42 2003
From: rlandis at middlebury.edu (Landis, R Matthew)
Date: Tue, 14 Oct 2003 08:58:42 -0400
Subject: [R] predicted values from rq
Message-ID: <0FE98FA04927D411A48300D0B77CF9BB0E7A7103@tiger.middlebury.edu>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20031014/ec94e16a/attachment.pl

From rpeng at jhsph.edu  Tue Oct 14 15:05:18 2003
From: rpeng at jhsph.edu (Roger D. Peng)
Date: Tue, 14 Oct 2003 09:05:18 -0400
Subject: [R] Problems with arrow keys
In-Reply-To: <3F8BB848.1040805@arcriswell.com>
References: <1066046467.6737.37.camel@chomsky.estadi.uv.es>
	<3F8BB848.1040805@arcriswell.com>
Message-ID: <3F8BF48E.2060803@jhsph.edu>

It would seem that you don't have (the development libraries for) 
readline installed.  You need these when compiling from source.  On 
Redhat, the rpm is called readline-devel, but I'm not sure of the naming 
for Mandrake.  Check to see if something like this can be installed and 
then recompile.

-roger

Andrew Criswell wrote:

> Hello:
>
> I just built R-1.8.0 on my Linux-Mandrake 9.0 using the commands,
>
> ./configure --enable-R-shlib
> make
> make install
>
> It all seems to have gone fine.  But when I open R and use the arrow 
> keys to try to go back to previous lines, it doesn't work.  What I get 
> instead is something like.
>
> > ^[[A^[[A^[[A^[[A^[[A
>
> I didn't have this problem when I installed R-1.7.1 as rpm.  I could 
> make use of the arrow keys.
>
> Help!
>
> ANDREW
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
>



From roger at ysidro.econ.uiuc.edu  Tue Oct 14 15:24:51 2003
From: roger at ysidro.econ.uiuc.edu (Roger Koenker)
Date: Tue, 14 Oct 2003 08:24:51 -0500 (CDT)
Subject: [R] predicted values from rq
In-Reply-To: <0FE98FA04927D411A48300D0B77CF9BB0E7A7103@tiger.middlebury.edu>
Message-ID: <Pine.SOL.4.30.0310140813290.4176-100000@ysidro.econ.uiuc.edu>

Matthew,

I intended to respond, but this got lost in the shuffle last weekend.
You are correct that there is no predict method for rq...this is
something that is on the todo list, but it required some thought
about what form this should take.  When I do this sort of thing
I use your generate predictions "by hand" using something like this

	X <- model.matrix(yourModelFormulaGoesHere)
	pred <- X %*% fit$coef[,1]

This presumes that you are using the default algorithm "br".
This is usually in a loop over the quantiles that I'm interested in.
I hope that this helps.

Roger

PS.  Generally help inquiries about packages are directed to maintainers
in the first instance and then to R-help in desperation.  This probably
helps explain there was no response.


url:	www.econ.uiuc.edu/~roger/my.html	Roger Koenker
email	rkoenker at uiuc.edu			Department of Economics
vox: 	217-333-4558				University of Illinois
fax:   	217-244-6678				Champaign, IL 61820

On Tue, 14 Oct 2003, Landis, R Matthew wrote:

> Dear statistics and R experts,
>
> I'm reposting the following message - I orginally posted it last Friday, and
> generated exactly no response.  I assume it got lost over the weekend
> (either that or it is just unbelievably obvious!).
>
> I would like to plot the predicted values from a quantile regression
> analysis (quantreg v.3.31; R v.1.7.1) so that I can evaluate the fit.
>
> I fit the model using something like:  fit.rq <- rq(growth ~ ht*spp*light,
> tau = 0.9)
>
> My response variable is tree growth (continuous), and my predictor variables
> are height (continuous), species (factor with 3 levels), and light (ordered
> factor with 3 levels).  I'd like to look at the relationship between growth
> and height separately for each combination of species and light.
>
> I would simply use 'predict()' as in lm, but unfortunately, there does not
> seem to be a predict method for rq objects (unless I am missing something?).
> I know how to extract coefficients from the rq object, and I've calculated
> predicted values by simply adding the relevant coefficients for a particular
> level of spp*light as:  intercept <- fit.rq$coef[1,1] + fit.rq$coef[3,1] +
> ...; slope <- fit.rq$coef[2,1] + fit.rq$coef[5,1] +...)  but this is slow
> and awkward to do for each of the 9 levels (not to mention different values
> of tau).  Plus, I would eventually like to do some non-linear fits, and then
> it will be even worse.  I'm sure there must be a way to do this with a
> matrix of coefficients, if only my poor memory of linear algebra didn't
> prevent me from seeing it.
>
> Is there a simple straightforward way to generate the predicted values
> without having to manually add up all the relevant coefficients for each
> level?  Or, even better, is predict.rq out there somewhere that I haven't
> found?  The help on rq objects does refer to it, but ?predict.rq doesn't
> turn up anything.
>
> Thanks for any help,
> Matt
>
> R. Matthew Landis, Ph.D.
> Dept. Biology
> Middlebury College
> Middlebury VT 05753
>
> tel. 802/443.3484
> fax.802/443.2072
>
>
> 	[[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
>



From spencer.graves at pdf.com  Tue Oct 14 15:27:12 2003
From: spencer.graves at pdf.com (Spencer Graves)
Date: Tue, 14 Oct 2003 06:27:12 -0700
Subject: [R] predicted values from rq
In-Reply-To: <0FE98FA04927D411A48300D0B77CF9BB0E7A7103@tiger.middlebury.edu>
References: <0FE98FA04927D411A48300D0B77CF9BB0E7A7103@tiger.middlebury.edu>
Message-ID: <3F8BF9B0.9000505@pdf.com>

I'm not familiar with "rq".  By searching "www.r-project.org" -> search 
-> "R site search" for "quantreg", I found documentation.  This referred 
to Koenker, R. W., et al., apparently at the Econometrics at the 
University of Illinois.  Have you attempted to contact Koenker? 

hope this helps.  spencer graves

Landis, R Matthew wrote:

>Dear statistics and R experts,
> 
>I'm reposting the following message - I orginally posted it last Friday, and
>generated exactly no response.  I assume it got lost over the weekend
>(either that or it is just unbelievably obvious!).
> 
>I would like to plot the predicted values from a quantile regression
>analysis (quantreg v.3.31; R v.1.7.1) so that I can evaluate the fit.  
> 
>I fit the model using something like:  fit.rq <- rq(growth ~ ht*spp*light,
>tau = 0.9)
> 
>My response variable is tree growth (continuous), and my predictor variables
>are height (continuous), species (factor with 3 levels), and light (ordered
>factor with 3 levels).  I'd like to look at the relationship between growth
>and height separately for each combination of species and light.  
> 
>I would simply use 'predict()' as in lm, but unfortunately, there does not
>seem to be a predict method for rq objects (unless I am missing something?).
>I know how to extract coefficients from the rq object, and I've calculated
>predicted values by simply adding the relevant coefficients for a particular
>level of spp*light as:  intercept <- fit.rq$coef[1,1] + fit.rq$coef[3,1] +
>...; slope <- fit.rq$coef[2,1] + fit.rq$coef[5,1] +...)  but this is slow
>and awkward to do for each of the 9 levels (not to mention different values
>of tau).  Plus, I would eventually like to do some non-linear fits, and then
>it will be even worse.  I'm sure there must be a way to do this with a
>matrix of coefficients, if only my poor memory of linear algebra didn't
>prevent me from seeing it.
> 
>Is there a simple straightforward way to generate the predicted values
>without having to manually add up all the relevant coefficients for each
>level?  Or, even better, is predict.rq out there somewhere that I haven't
>found?  The help on rq objects does refer to it, but ?predict.rq doesn't
>turn up anything.
> 
>Thanks for any help,
>Matt
> 
>R. Matthew Landis, Ph.D.
>Dept. Biology
>Middlebury College
>Middlebury VT 05753
> 
>tel. 802/443.3484
>fax.802/443.2072
> 
>
>	[[alternative HTML version deleted]]
>
>______________________________________________
>R-help at stat.math.ethz.ch mailing list
>https://www.stat.math.ethz.ch/mailman/listinfo/r-help
>  
>



From ggrothendieck at myway.com  Tue Oct 14 15:45:50 2003
From: ggrothendieck at myway.com (Gabor Grothendieck)
Date: Tue, 14 Oct 2003 09:45:50 -0400 (EDT)
Subject: [R] Organized examples for newbyes
Message-ID: <20031014134550.E33893A10@xmxpita.myway.com>



I have found Paul Johnson's examples very useful:

   http://gifi.stat.ucla.edu/R/doc/statsRus.html


 
Date: Tue, 14 Oct 2003 09:20:34 +0200 
From: v.demart at libero.it <v.demart at libero.it>
 
I'm learning R from scratch on my linux box, being deeply biased at work by those graphical programs, nice to look but often poor in content and almost generally limited, running under M$ Windows. 

I wonder if someone out there can suggest to an absolute beginner as I am where to find a collection of examples of R-code (or Splus, as I understand) to play with in order to learn R quicker.

The collection should:
1) be in English, or French, or, of course, Italian,
2) be concentrated in one or at most a handful of sites, 
3) contain examples of gradually increasing complexity.

Such a collection could be extremely helpful to convince other people at work to move to R.

Thanks & Ciao

Vittorio


_______________________________________________
No banners. No pop-ups. No kidding.
Introducing My Way - http://www.myway.com



From chrysopa at insecta.ufv.br  Tue Oct 14 14:46:42 2003
From: chrysopa at insecta.ufv.br (Ronaldo Reis Jr.)
Date: Tue, 14 Oct 2003 10:46:42 -0200
Subject: [R] [OFF] Dataset for extra Crawley Chapter
Message-ID: <200310141046.42305.chrysopa@insecta.ufv.br>

Hi,

anybody have the dataset used in Gamma Errors chapter of the Crawley's books 
(An Introduction to Data Analysis using S-Plus).

specifically the functionalresponse and the Density datasets.

Thanks
Ronaldo
-- 
For every problem there is one solution which is simple, neat, and wrong.
-- H. L. Mencken
--
|>   // | \\   [***********************************]
|   ( ?   ? )  [Ronaldo Reis J?nior                ]
|>      V      [UFV/DBA-Entomologia                ]
|    /     \   [36571-000 Vi?osa - MG              ]
|>  /(.''`.)\  [Fone: 31-3899-2532                 ]
|  /(: :'  :)\ [chrysopa at insecta.ufv.br            ]
|>/ (`. `'` ) \[ICQ#: 5692561 | LinuxUser#: 205366 ]
|    ( `-  )   [***********************************]
|>>  _/   \_Powered by GNU/Debian Woody/Sarge



From dyang at nrcan.gc.ca  Tue Oct 14 16:01:38 2003
From: dyang at nrcan.gc.ca (Yang, Richard)
Date: Tue, 14 Oct 2003 10:01:38 -0400
Subject: [R] [OFF] Dataset for extra Crawley Chapter
Message-ID: <F0E0B899CB43D5118D220002A55113CF02F92803@s2-edm-r1.nofc.cfs.nrcan.gc.ca>

Ronaldo;

	You can download all datasets used in Crawley's book from his web
site. 

-- Richard

> -----Original Message-----
> From: Ronaldo Reis Jr. [mailto:chrysopa at insecta.ufv.br]
> Sent: Tuesday, October 14, 2003 6:47 AM
> To: R-Help
> Subject: [R] [OFF] Dataset for extra Crawley Chapter
> 
> 
> Hi,
> 
> anybody have the dataset used in Gamma Errors chapter of the 
> Crawley's books 
> (An Introduction to Data Analysis using S-Plus).
> 
> specifically the functionalresponse and the Density datasets.
> 
> Thanks
> Ronaldo
> -- 
> For every problem there is one solution which is simple, 
> neat, and wrong.
> -- H. L. Mencken
> --
> |>   // | \\   [***********************************]
> |   ( ?   ? )  [Ronaldo Reis J?nior                ]
> |>      V      [UFV/DBA-Entomologia                ]
> |    /     \   [36571-000 Vi?osa - MG              ]
> |>  /(.''`.)\  [Fone: 31-3899-2532                 ]
> |  /(: :'  :)\ [chrysopa at insecta.ufv.br            ]
> |>/ (`. `'` ) \[ICQ#: 5692561 | LinuxUser#: 205366 ]
> |    ( `-  )   [***********************************]
> |>>  _/   \_Powered by GNU/Debian Woody/Sarge
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
>



From baud-bovy.gabriel at hsr.it  Tue Oct 14 12:51:03 2003
From: baud-bovy.gabriel at hsr.it (Gabriel Baud-Bovy)
Date: Tue, 14 Oct 2003 12:51:03 +0200
Subject: [R] "Turtle world" graphics in R
Message-ID: <5.2.1.1.1.20031014122851.03952d00@mail.hsr.it>

I would greatly appreciate some idea about the best approach to make a
graphical interface of for a turtle world in R.

In the turtle environment, the user controls movements of a turtle
through the command line (e.g. forward(10), turn(5), etc.).  The turtle should
be displayed in the new position and heading after each command.
Also, the turtle's path and, possibly, objects (squares, barriers, ...) with
which turtle might interact should be displayed.

Thank you,

Gabriel

PS: Right now, I am using basic plotting R functions to plot the turtle
and its path. Before each turtle's movement, I "erase" the turtle be redrawing
it with the same color as the background but that does not look good as it
also erases everything ese. I tought of using large matrix (e.g 1000x1000) as
a representation of the turtle world and displaying it with contour() but 
it is slow
to update after each user command.

PPS: I did ask a similar question 1 month ago but did not get any answer. 
Probably
that my question was too long... I just would like to know if I am missing 
possibilites.
--------------------------------------------------------------------
Gabriel Baud-Bovy
Assistant Professor
UHSR University
via Olgettina, 58	tel:  (+39) 02 2643 4839
20132 Milan, Italy	fax: (+39) 02 2643 4892



From B.Rowlingson at lancaster.ac.uk  Tue Oct 14 16:27:59 2003
From: B.Rowlingson at lancaster.ac.uk (Barry Rowlingson)
Date: Tue, 14 Oct 2003 15:27:59 +0100
Subject: [R] "Turtle world" graphics in R
In-Reply-To: <5.2.1.1.1.20031014122851.03952d00@mail.hsr.it>
References: <5.2.1.1.1.20031014122851.03952d00@mail.hsr.it>
Message-ID: <3F8C07EF.3000600@lancaster.ac.uk>

Gabriel Baud-Bovy wrote:

> I "erase" the turtle be 
> redrawing
> it with the same color as the background but that does not look good as it
> also erases everything ese. I tought of using large matrix (e.g 
> 1000x1000) as
> a representation of the turtle world and displaying it with contour() 
> but it is slow
> to update after each user command.
> 

  R's graphics aren't really designed for moving graphics, or undrawing 
things.

  Perhaps you can use the tcltk library, you could draw on a tk canvas. 
This would be much more suited to turtle graphics.

  If you want to get the turtle track back into R once the turtle has 
finished its wanderings then you can probably read the path from the Tk 
canvas widget into R using the tcltk library.

  Or you may want to use python, instead of R:
http://www.python.org/doc/current/lib/module-turtle.html

  Or you may want to use Perl instead or R:
http://home.hccnet.nl/kees.moerman/turtle.html

  Both these systems use Tk as their graphics output.

  I wrote some turtle-graphics subroutines for the ZX Spectrum about 20 
years ago :)

  Baz



From ggrothendieck at myway.com  Tue Oct 14 16:47:25 2003
From: ggrothendieck at myway.com (Gabor Grothendieck)
Date: Tue, 14 Oct 2003 10:47:25 -0400 (EDT)
Subject: [R] Web Site Suggestion Regarding R Source
Message-ID: <20031014144725.AF9143988@xmxpita.myway.com>



In the Lua language they have put the entire source on the net in 
such a way that you can easily browse it.  Go to:

   http://www.lua.org/source/5.0/

to see what I mean.

One of the nice things about R is that if you want more info on 
a function than in the help page you can just type the name of 
the function at the R prompt and you get its source, at least if 
it itself was written in R.  Methods complicate this a bit but its
still relatively easy to get the source.

Unfortunately, for someone trying to find out what a function 
does, the real work may be done in C in which case you come to a 
dead end.  Admittedly its possible to get the source but 
this requires a bit of work (downloading large files, figuring
out rsync) and some hunting around.

If the R source were placed on the net in a similar fashion to Lua 
I think this would make it more accessible and help people help
themselves by making it easier to answer many questions 
themselves that they currently have to go to rhelp for.



_______________________________________________
No banners. No pop-ups. No kidding.
Introducing My Way - http://www.myway.com



From P.Lemmens at nici.kun.nl  Tue Oct 14 17:10:03 2003
From: P.Lemmens at nici.kun.nl (Paul Lemmens)
Date: Tue, 14 Oct 2003 17:10:03 +0200
Subject: is.na(v)<-b (was: Re: [R] Beginner's query - segmentation fault)
In-Reply-To: <6C8A8033ABC1E3468048ABC4F13CE572AC0E11@synequanon01>
References: <6C8A8033ABC1E3468048ABC4F13CE572AC0E11@synequanon01>
Message-ID: <32153296.1066151403@lemmens.socsci.kun.nl>

By accident I'm also toying around with NA's, so I started reading up on 
this thread but failed to find a 'concluding' remark or advice. As a naive 
R user I would have loved to see a comment "do it like this".

The prevailing opinion seemed to be that is.na() might be better (safer) 
but x <- NA is much clearer to understand. Can I relatively safely use the 
easy form, or is it better to remember (the hard way) the safer version? 
Has the discussion continued privately or just stopped here?

Personally I still find the fragments below (taken from the thread) very 
counter intuitive, not to say scary.

x <- 1:10
is.na(x) <- 1:5

and

is.na(x) <- FALSE


It's very hard to understand what happens (as layman) because the 
assignment seems to reverse in meaning in the first example (actually 
taking indices 1:5 of x and assigning those the value NA) whereas in the 
second case it's not obvious what happens to x: will it get the value FALSE 
or will the original value remain(*).

IMHO the <- NA construct is much easier to understand and should be made 
safe in all possible situations (whatever the underlying safety problem or 
other difficulties might be).


kind regards,
Paul

(*) Such a remark will probably lead to some kind of reprimand because it's 
probably somewhere within the 10e6 manual pages but I'm trying my luck here.


-- 
Paul Lemmens
NICI, University of Nijmegen              ASCII Ribbon Campaign /"\
Montessorilaan 3 (B.01.03)                    Against HTML Mail \ /
NL-6525 HR Nijmegen                                              X
The Netherlands                                                 / \
Phonenumber    +31-24-3612648
Fax            +31-24-3616066



From rossini at blindglobe.net  Tue Oct 14 17:23:16 2003
From: rossini at blindglobe.net (A.J. Rossini)
Date: Tue, 14 Oct 2003 08:23:16 -0700
Subject: [R] Web Site Suggestion Regarding R Source
In-Reply-To: <20031014144725.AF9143988@xmxpita.myway.com> (Gabor
	Grothendieck's message of "Tue, 14 Oct 2003 10:47:25 -0400 (EDT)")
References: <20031014144725.AF9143988@xmxpita.myway.com>
Message-ID: <8565iru9ln.fsf@blindglobe.net>


You want it, you got it:

R's source (via CVS, but you can look at the tags) on the WWW, via:

    http://anoncvs.r-project.org/cgi-bin/cvsweb.cgi/

Unfortunately, it's cvsweb and not viewcvs (couldn't easily figure out
how to use the tags to find 1.8.0's branch, for example)



"Gabor Grothendieck" <ggrothendieck at myway.com> writes:

> In the Lua language they have put the entire source on the net in 
> such a way that you can easily browse it.  Go to:
>
>    http://www.lua.org/source/5.0/
>
> to see what I mean.
>
> One of the nice things about R is that if you want more info on 
> a function than in the help page you can just type the name of 
> the function at the R prompt and you get its source, at least if 
> it itself was written in R.  Methods complicate this a bit but its
> still relatively easy to get the source.
>
> Unfortunately, for someone trying to find out what a function 
> does, the real work may be done in C in which case you come to a 
> dead end.  Admittedly its possible to get the source but 
> this requires a bit of work (downloading large files, figuring
> out rsync) and some hunting around.
>
> If the R source were placed on the net in a similar fashion to Lua 
> I think this would make it more accessible and help people help
> themselves by making it easier to answer many questions 
> themselves that they currently have to go to rhelp for.
>
>
>
> _______________________________________________
> No banners. No pop-ups. No kidding.
> Introducing My Way - http://www.myway.com
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
>

-- 
rossini at u.washington.edu            http://www.analytics.washington.edu/ 
Biomedical and Health Informatics   University of Washington
Biostatistics, SCHARP/HVTN          Fred Hutchinson Cancer Research Center
UW (Tu/Th/F): 206-616-7630 FAX=206-543-3461 | Voicemail is unreliable
FHCRC  (M/W): 206-667-7025 FAX=206-667-4812 | use Email

CONFIDENTIALITY NOTICE: This e-mail message and any attachme...{{dropped}}



From rksh at soc.soton.ac.uk  Tue Oct 14 17:40:24 2003
From: rksh at soc.soton.ac.uk (Robin Hankin)
Date: Tue, 14 Oct 2003 16:40:24 +0100
Subject: [R] mapply() gives seg fault
Message-ID: <a06002007bbb1c91c2d44@[139.166.242.29]>

Hello everybody.

I've been experimenting with mapply().  Does anyone else have problems with:

R> mapply(rep,times=1:4, MoreArgs=42)

(I get a seg fault).

robin



R> R.version
          _
platform powerpc-apple-darwin6.6
arch     powerpc
os       darwin6.6
system   powerpc, darwin6.6
status   beta
major    1
minor    8.0
year     2003
month    10
day      02
language R

>



From laura at env.leeds.ac.uk  Tue Oct 14 17:41:23 2003
From: laura at env.leeds.ac.uk (Laura Quinn)
Date: Tue, 14 Oct 2003 16:41:23 +0100 (BST)
Subject: [R] Naive Q re applying AR(p) models to matrix data
Message-ID: <Pine.LNX.4.44.0310141640350.28985-100000@env-pc-phd13>


As a total beginner still, I am having great difficulty trying to apply
ar models to column data in a matrix.

As each matrix has 20 columns (and some of these contain NA data) it is
obviously a long, slow process to calculate ar models for each column
individually, though I am (probably extremely stupidly) struggling to see
how I am doing this wrong.

I have tried using:

for (i in 1:20)
apply(mydata,c(,i),ar(mydata[!is.na(mydata)],order.max=3))

and

apply(mydata,ar(mydata[!is.na(mydata)],order.max=3))

and

apply(mydata,c(,1:20),ar(mydata[,1:20][!is.na(mydata[,1:20])],order.max=3))

and none of these work - can someone tell me what I am doing wrong??

Thank you in advance, and excuse me for my stupidity..
Laura



From rossini at blindglobe.net  Tue Oct 14 17:45:29 2003
From: rossini at blindglobe.net (A.J. Rossini)
Date: Tue, 14 Oct 2003 08:45:29 -0700
Subject: [R] Web Site Suggestion Regarding R Source
In-Reply-To: <8565iru9ln.fsf@blindglobe.net> (A. J. Rossini's message of
	"Tue, 14 Oct 2003 08:23:16 -0700")
References: <20031014144725.AF9143988@xmxpita.myway.com>
	<8565iru9ln.fsf@blindglobe.net>
Message-ID: <85oewjsu06.fsf@blindglobe.net>

rossini at blindglobe.net (A.J. Rossini) writes:

> You want it, you got it:
>
> R's source (via CVS, but you can look at the tags) on the WWW, via:
>
>     http://anoncvs.r-project.org/cgi-bin/cvsweb.cgi/
>
> Unfortunately, it's cvsweb and not viewcvs (couldn't easily figure out
> how to use the tags to find 1.8.0's branch, for example)

Easily is the key word.  I found them.

best,
-tony


-- 
rossini at u.washington.edu            http://www.analytics.washington.edu/ 
Biomedical and Health Informatics   University of Washington
Biostatistics, SCHARP/HVTN          Fred Hutchinson Cancer Research Center
UW (Tu/Th/F): 206-616-7630 FAX=206-543-3461 | Voicemail is unreliable
FHCRC  (M/W): 206-667-7025 FAX=206-667-4812 | use Email

CONFIDENTIALITY NOTICE: This e-mail message and any attachme...{{dropped}}



From tlumley at u.washington.edu  Tue Oct 14 17:49:50 2003
From: tlumley at u.washington.edu (Thomas Lumley)
Date: Tue, 14 Oct 2003 08:49:50 -0700 (PDT)
Subject: [R] mapply() gives seg fault
In-Reply-To: <a06002007bbb1c91c2d44@[139.166.242.29]>
References: <a06002007bbb1c91c2d44@[139.166.242.29]>
Message-ID: <Pine.A41.4.58.0310140848160.36704@homer15.u.washington.edu>

On Tue, 14 Oct 2003, Robin Hankin wrote:

> Hello everybody.
>
> I've been experimenting with mapply().  Does anyone else have problems with:
>
> R> mapply(rep,times=1:4, MoreArgs=42)
>
> (I get a seg fault).
>

Yes, thanks for reporting this.

You should get an error message instead: the correct syntax would be
mapply(rep, times=1:4, MoreArgs=list(42))

It looks as though just coercing to a list would probably be sufficient.

	-thomas



From rksh at soc.soton.ac.uk  Tue Oct 14 17:54:51 2003
From: rksh at soc.soton.ac.uk (Robin Hankin)
Date: Tue, 14 Oct 2003 16:54:51 +0100
Subject: [R] mapply() gives seg fault
In-Reply-To: <Pine.A41.4.58.0310140848160.36704@homer15.u.washington.edu>
References: <a06002007bbb1c91c2d44@[139.166.242.29]>
	<Pine.A41.4.58.0310140848160.36704@homer15.u.washington.edu>
Message-ID: <a0600200bbbb1cc13df05@[139.166.242.29]>

Hello again

thanks for this.  What I should have asked is, should one always report
repeatable seg faults (even if functions are called with 
inappropriate arguments)?

[the emacs bug report FAQ says "yes" but I don't think there is a 
similar policy for R].

best


rksh


>On Tue, 14 Oct 2003, Robin Hankin wrote:
>
>>  Hello everybody.
>>
>>  I've been experimenting with mapply().  Does anyone else have problems with:
>>
>>  R> mapply(rep,times=1:4, MoreArgs=42)
>>
>>  (I get a seg fault).
>>
>
>Yes, thanks for reporting this.
>
>You should get an error message instead: the correct syntax would be
>mapply(rep, times=1:4, MoreArgs=list(42))
>
>It looks as though just coercing to a list would probably be sufficient.
>
>	-thomas



From rpeng at jhsph.edu  Tue Oct 14 18:04:35 2003
From: rpeng at jhsph.edu (Roger D. Peng)
Date: Tue, 14 Oct 2003 12:04:35 -0400
Subject: [R] mapply() gives seg fault
In-Reply-To: <a06002007bbb1c91c2d44@[139.166.242.29]>
References: <a06002007bbb1c91c2d44@[139.166.242.29]>
Message-ID: <3F8C1E93.7030903@jhsph.edu>

I get this too on the released version.  I guess the problem is that the 
value passed to MoreArgs is not a list.  Maybe,

mapply(rep, times = 1:4, MoreArgs = list(42))

produces what you want?  At any rate, R shouldn't segfault so it looks 
like bug somewhere.

-roger

Robin Hankin wrote:

> Hello everybody.
>
> I've been experimenting with mapply().  Does anyone else have problems 
> with:
>
> R> mapply(rep,times=1:4, MoreArgs=42)
>
> (I get a seg fault).
>
> robin
>
>
>
> R> R.version
>          _
> platform powerpc-apple-darwin6.6
> arch     powerpc
> os       darwin6.6
> system   powerpc, darwin6.6
> status   beta
> major    1
> minor    8.0
> year     2003
> month    10
> day      02
> language R
>
>>
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
>



From p.dalgaard at biostat.ku.dk  Tue Oct 14 18:23:22 2003
From: p.dalgaard at biostat.ku.dk (Peter Dalgaard BSA)
Date: 14 Oct 2003 18:23:22 +0200
Subject: [R] mapply() gives seg fault
In-Reply-To: <a06002007bbb1c91c2d44@[139.166.242.29]>
References: <a06002007bbb1c91c2d44@[139.166.242.29]>
Message-ID: <x2oewj3i11.fsf@biostat.ku.dk>

Robin Hankin <rksh at soc.soton.ac.uk> writes:

> Hello everybody.
> 
> I've been experimenting with mapply().  Does anyone else have problems with:
> 
> R> mapply(rep,times=1:4, MoreArgs=42)
> 
> (I get a seg fault).

So do I. It's a bug, but I suppose list(42) is what  you need.

-- 
   O__  ---- Peter Dalgaard             Blegdamsvej 3  
  c/ /'_ --- Dept. of Biostatistics     2200 Cph. N   
 (*) \(*) -- University of Copenhagen   Denmark      Ph: (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)             FAX: (+45) 35327907



From zeileis at ci.tuwien.ac.at  Tue Oct 14 19:21:39 2003
From: zeileis at ci.tuwien.ac.at (Achim Zeileis)
Date: Tue, 14 Oct 2003 19:21:39 +0200
Subject: [R] Naive Q re applying AR(p) models to matrix data
In-Reply-To: <Pine.LNX.4.44.0310141640350.28985-100000@env-pc-phd13>
References: <Pine.LNX.4.44.0310141640350.28985-100000@env-pc-phd13>
Message-ID: <200310141721.h9EHLeCo008953@thorin.ci.tuwien.ac.at>

On Tuesday 14 October 2003 17:41, Laura Quinn wrote:

> As a total beginner still, I am having great difficulty trying to
> apply ar models to column data in a matrix.
>
> As each matrix has 20 columns (and some of these contain NA data) it
> is obviously a long, slow process to calculate ar models for each
> column individually, though I am (probably extremely stupidly)
> struggling to see how I am doing this wrong.
>
> I have tried using:
>
> for (i in 1:20)
> apply(mydata,c(,i),ar(mydata[!is.na(mydata)],order.max=3))
>
> and
>
> apply(mydata,ar(mydata[!is.na(mydata)],order.max=3))
>
> and
>
> apply(mydata,c(,1:20),ar(mydata[,1:20][!is.na(mydata[,1:20])],order.
>max=3))
>
> and none of these work - can someone tell me what I am doing wrong??

Try

   apply(mydata, 2, function(x) ar(x[!is.na(x)]))

and then look at

  help(apply)

to understand what you have done wrong.

hth,
Z

> Thank you in advance, and excuse me for my stupidity..
> Laura
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help



From bfbraum at fas.harvard.edu  Tue Oct 14 19:42:21 2003
From: bfbraum at fas.harvard.edu (Bear F. Braumoeller)
Date: Tue, 14 Oct 2003 13:42:21 -0400
Subject: [R] [R-pkgs] Updated package: Boolean v1.03
Message-ID: <C3ACF3F0-FE6D-11D7-BC96-000A95A672D6@fas.harvard.edu>

Version 1.03 of the R package boolean has been uploaded to CRAN and is 
now available.

boolean implements partial-observability logit and probit models for 
testing Boolean hypotheses.  It permits researchers to model the 
probability of the occurrence of a given outcome as a complex function 
of the probabilities that other outcomes will occur (or other 
conditions will be fulfilled).  For example, if p(Y) = p(A) * p(B) 
(that is, A and B jointly produce Y, and the absence of either 
precludes it), the boolean routine models p(A) and p(B) as logit or 
probit curves and p(Y) as their product.  Similarly, if A or B produces 
Y, p(Y) = 1 - [(1-p(A)) * (1-p(B))] becomes the functional form, again 
with p(A) and p(B) estimated as logit/probit curves.  Arbitrarily 
convoluted combinations -- e.g., (A and B) or (C and D) produces Y; or, 
p(Y) = 1 - [1-p(A)*p(B)] * [1-p(C)*p(D)] -- can be estimated, and 
dependence of probabilities can be modeled by including some of the 
same independent variables in the separate logit or probit equations.  
The derivation is in Braumoeller, "Causal Complexity and the Study of 
Politics," Political Analysis 11(3), 209-233.

Version 1.03 is a service update which ensures that all documentation, 
in particular the documentation of S4 methods objects, is compatible 
with R 1.8.  Previous versions will produce warnings upon compilation.


Bear F. Braumoeller
Assistant Professor
Department of Government
Harvard University
http://www.people.fas.harvard.edu/~bfbraum

_______________________________________________
R-packages mailing list
R-packages at stat.math.ethz.ch
https://www.stat.math.ethz.ch/mailman/listinfo/r-packages



From tlumley at u.washington.edu  Tue Oct 14 19:55:10 2003
From: tlumley at u.washington.edu (Thomas Lumley)
Date: Tue, 14 Oct 2003 10:55:10 -0700 (PDT)
Subject: [R] mapply() gives seg fault
In-Reply-To: <a0600200bbbb1cc13df05@[139.166.242.29]>
References: <a06002007bbb1c91c2d44@[139.166.242.29]>
	<Pine.A41.4.58.0310140848160.36704@homer15.u.washington.edu>
	<a0600200bbbb1cc13df05@[139.166.242.29]>
Message-ID: <Pine.A41.4.58.0310141049530.55772@homer07.u.washington.edu>

On Tue, 14 Oct 2003, Robin Hankin wrote:

> Hello again
>
> thanks for this.  What I should have asked is, should one always report
> repeatable seg faults (even if functions are called with
> inappropriate arguments)?
>

Yes.  Reproducible segfaults that occur `in the wild' are fairly important
bugs even if they result from errors.

Segfaults from trying to break the system are a much lower priority, but
even they should probably be reported.

The FAQ (9.1) says
	If R executes an illegal instruction, or dies with an operating system
	error message that indicates a problem in the program (as opposed to
	something like "disk full"), then it is certainly a bug.
and goes on to explain that the only time this doesn't apply is when you
call compiled code with the wrong arguments or call your own compiled
code.

	-thomas



From krcabrer at unalmed.edu.co  Tue Oct 14 20:25:29 2003
From: krcabrer at unalmed.edu.co (Kenneth Cabrera)
Date: Tue, 14 Oct 2003 13:25:29 -0500
Subject: [R] Trasparent graphs?
Message-ID: <oprw1pgrv4faouaq@200.24.8.4>

Hi R-users and R-experts:

Is it posible to make transparent graphics on R?
I mean, one graphic with colors over another one
but both visible with transparent options?

Thank you for your help

Kenneth

--



From tblackw at umich.edu  Tue Oct 14 20:37:29 2003
From: tblackw at umich.edu (Thomas W Blackwell)
Date: Tue, 14 Oct 2003 14:37:29 -0400 (EDT)
Subject: [R] Trasparent graphs?
In-Reply-To: <oprw1pgrv4faouaq@200.24.8.4>
References: <oprw1pgrv4faouaq@200.24.8.4>
Message-ID: <Pine.SOL.4.58.0310141431450.16162@robotron.gpcc.itd.umich.edu>

Kenneth  -

Using base package graphics,  use  plot()  on the first call,
then either  points() or lines()  on subsequent calls to build
up a single plot, layer by layer.  Each call can use an argument
col=... .   However, the colors themselves are not transparent,
AFAIK, so that where two symbols are plotted on top of each other,
only the top layer shows.  If you need to give the appearance of
color mixing, you will have to identify which points coincide,
and plot those again using a third color.  I don't use lattice
graphics, but I presume the situation is much the same.

-  tom blackwell  -  u michigan medical school  -  ann arbor  -

On Tue, 14 Oct 2003, Kenneth Cabrera wrote:

> Hi R-users and R-experts:
>
> Is it posible to make transparent graphics on R?
> I mean, one graphic with colors over another one
> but both visible with transparent options?
>
> Thank you for your help
>
> Kenneth



From krcabrer at unalmed.edu.co  Tue Oct 14 20:45:38 2003
From: krcabrer at unalmed.edu.co (Kenneth Cabrera)
Date: Tue, 14 Oct 2003 13:45:38 -0500
Subject: [R] Trasparent graphs?
In-Reply-To: <Pine.SOL.4.58.0310141431450.16162@robotron.gpcc.itd.umich.edu>
References: <oprw1pgrv4faouaq@200.24.8.4>
	<Pine.SOL.4.58.0310141431450.16162@robotron.gpcc.itd.umich.edu>
Message-ID: <oprw1qecq3faouaq@200.24.8.4>

Yes, Dr. Blackwell:

What I mean is the second part:

> If you need to give the appearance of
> color mixing, you will have to identify which points coincide,
> and plot those again using a third color.  I don't use lattice
> graphics, but I presume the situation is much the same.

Is it possible with pixmap library or image options?

Thank you again for your help.

Kenneth



From matthew_wiener at merck.com  Tue Oct 14 22:08:08 2003
From: matthew_wiener at merck.com (Wiener, Matthew)
Date: Tue, 14 Oct 2003 16:08:08 -0400
Subject: [R] 64-bit R on AIX?
Message-ID: <AEBD81486231A343B1813FE62D3352250369A43A@usrymx15.merck.com>

R-helpers --

Has anyone gotten R to compile as a 64-bit application on AIX 5L (or any
other version)?

(I don't have the details of the errors here, as I'm not the person actually
doing the compiling.  I might be able to get them if someone wants to see
them in particular.)

Thanks for any help.

Matthew Wiener
RY84-202
Applied Computer Science & Mathematics Dept.
Merck Research Labs
126 E. Lincoln Ave.
Rahway, NJ 07065
732-594-5303



From penglish at hydro.washington.edu  Tue Oct 14 22:29:50 2003
From: penglish at hydro.washington.edu (Paul English)
Date: Tue, 14 Oct 2003 13:29:50 -0700 (PDT)
Subject: [R] Trouble installing RandomFields package
In-Reply-To: <200310111001.h9BA1Tq7008590@stat.math.ethz.ch>
Message-ID: <20031014132710.I35898-100000@dynamic.hydro.washington.edu>


Hi,
	I'm having some difficulty installing the RandomFields package in
R 1.7.0 under FreeBSD. I've installed the geoR package with no
difficulties.

Following are the errors that I get:

* Installing *source* package 'RandomFields' ...
** libs
c++ -I/usr/local/lib/R/include  -I/usr/local/include -mieee-fp  -fPIC  -O
-pipe -c MPP.cc -o MPP.o
c++ -I/usr/local/lib/R/include  -I/usr/local/include -mieee-fp  -fPIC  -O
-pipe -c MPPFcts.cc -o MPPFcts.o
c++ -I/usr/local/lib/R/include  -I/usr/local/include -mieee-fp  -fPIC  -O
-pipe -c RFCovFcts.cc -o RFCovFcts.o
c++ -I/usr/local/lib/R/include  -I/usr/local/include -mieee-fp  -fPIC  -O
-pipe -c RFcircembed.cc -o RFcircembed.o
c++ -I/usr/local/lib/R/include  -I/usr/local/include -mieee-fp  -fPIC  -O
-pipe -c RFdirect.cc -o RFdirect.o
c++ -I/usr/local/lib/R/include  -I/usr/local/include -mieee-fp  -fPIC  -O
-pipe -c RFsimu.cc -o RFsimu.o
In file included from RFsimu.cc:41:
/usr/include/sys/timeb.h:47: syntax error before `;'
*** Error code 1

Stop in /tmp/R.INSTALL.36022/RandomFields/src.
ERROR: compilation failed for package 'RandomFields'



I have no idea how there could possibly be a syntax error in the system
include files, since just about everything else under the sun requires
them for compilation anyway.

Thanks for your help,
Paul



From B.Rowlingson at lancaster.ac.uk  Tue Oct 14 23:07:01 2003
From: B.Rowlingson at lancaster.ac.uk (Barry Rowlingson)
Date: Tue, 14 Oct 2003 22:07:01 +0100
Subject: [R] Trasparent graphs?
In-Reply-To: <oprw1qecq3faouaq@200.24.8.4>
References: <oprw1pgrv4faouaq@200.24.8.4>	<Pine.SOL.4.58.0310141431450.16162@robotron.gpcc.itd.umich.edu>
	<oprw1qecq3faouaq@200.24.8.4>
Message-ID: <3F8C6575.3040703@lancaster.ac.uk>

Kenneth Cabrera wrote:

> Is it possible with pixmap library or image options?
> 
> Thank you again for your help.
> 
>

  If you have NA values in a matrix, they will be transparent when you 
use image() with the add=T option.

Example:

 > #make a matrix with all numbers, and show it:

 > z=matrix(outer(1:10,1:10,"*"),10,10)
 > image(z)

 > # now set anything over 50 to be NA
 > z[z>50] = NA

 > # plot some points, then add the image, showing the NA region to be 
see-thru:
 > plot(runif(1000),runif(1000))
 > image(z,add=T)

  Note that you cant have partial transparency - its all or nothing.

Baz



From p.murrell at auckland.ac.nz  Tue Oct 14 23:38:12 2003
From: p.murrell at auckland.ac.nz (Paul Murrell)
Date: Wed, 15 Oct 2003 10:38:12 +1300
Subject: [R] Trasparent graphs?
References: <oprw1pgrv4faouaq@200.24.8.4>
Message-ID: <3F8C6CC4.5030500@stat.auckland.ac.nz>

Hi


Kenneth Cabrera wrote:
> Hi R-users and R-experts:
> 
> Is it posible to make transparent graphics on R?
> I mean, one graphic with colors over another one
> but both visible with transparent options?


In general this is not possible, but ...

If you are prepared to work with grid graphics, and produce SVG output, 
and use a very experimental package, then take a look at the gridSVG 
package (http://www.stat.auckland.ac.nz/~paul/index.html).  The package 
has a vignette with a very simple example of transparency and there are 
a couple more examples in some talk slides also linked off the page 
given above.  The package has some serious deficiencies, but I have seen 
one reasonably complex real usage of it.

A general alpha transparency graphical parameter may be added to R in 
the near future, but even then this will not be respected by all output 
formats.

Paul
-- 
Dr Paul Murrell
Department of Statistics
The University of Auckland
Private Bag 92019
Auckland
New Zealand
64 9 3737599 x85392
paul at stat.auckland.ac.nz
http://www.stat.auckland.ac.nz/~paul/



From jmiyamot at u.washington.edu  Tue Oct 14 23:40:44 2003
From: jmiyamot at u.washington.edu (John Miyamoto)
Date: Tue, 14 Oct 2003 14:40:44 -0700 (PDT)
Subject: [R] Job notice at the University of Washington
Message-ID: <Pine.A41.4.58.0310141437350.913508@mead12.u.washington.edu>

UNIVERSITY OF WASHINGTON

FACULTY POSITION IN QUANTITATIVE PSYCHOLOGY

The Department of Psychology seeks to fill a position in Quantitative
Psychology at the tenure-track assistant professor level.  In exceptional
circumstances, appointment at the Associate Professor or Professor level
may be considered for candidates who offer extraordinary opportunities to
further the University's commitments to mentoring under represented
students in the sciences.

Applicants are expected to have expertise and a passion for teaching that
encompasses a number of the following methods: hierarchical linear
modeling, structural equation modeling, latent variable modeling, growth
modeling and other longitudinal statistical models. The candidate must
also have an active, high quality research program that applies these
methods in a substantive area of psychology.

To apply, send a detailed statement of research and teaching interests, a
curriculum vitae, not more than three reprints or preprints, evidence of
teaching effectiveness, and at least three letters of recommendation to:
Quantitative Psychology Search Committee, Department of Psychology,
University of Washington, Box 351525, Seattle, WA 98195-1525.
Applications will be reviewed as they are received.  Those received by
December 15, 2003, will receive full consideration.  Ph.D. required by
date of appointment.  The University of Washington is building a
culturally diverse faculty and strongly encourages applications from
female and minority candidates.  The University is an Equal
Opportunity/Affirmative Action employer.

--------------------------------------------------------------------
John Miyamoto, Dept. of Psychology, Box 351525
University of Washington, Seattle, WA 98195-1525
Phone 206-543-0805, Fax 206-685-3157, Email jmiyamot at u.washington.edu
Homepage http://faculty.washington.edu/jmiyamot/



From gerifalte28 at hotmail.com  Wed Oct 15 02:33:28 2003
From: gerifalte28 at hotmail.com (Francisco Vergara)
Date: Wed, 15 Oct 2003 00:33:28 +0000
Subject: [R] (no subject)
Message-ID: <Law14-F39JGFyVjHWIg00008b15@hotmail.com>

Dear Dr. Bates

I replied to your email before but apparently it didn't make it so I am 
replying again.  I would really appreciate if you could send me an example 
on how you suggest to build a cell means model for fixed effects using the 
NLME library.  I am not sure whether you suggest to create a separate factor 
for each unique combination of levels of a factor or whether you suggest to 
create a factor for each level of the original factors.  Wouldnt the first 
case generate collinearity problems under some situations?

Many thanks again for your help

Francisco


>From: Douglas Bates <bates at stat.wisc.edu>
>To: "Francisco Vergara" <gerifalte28 at hotmail.com>
>Subject: Re: [R] (no subject)
>Date: 08 Oct 2003 19:21:13 -0500
>
>"Francisco Vergara" <gerifalte28 at hotmail.com> writes:
>
> > I want to specify the contrasts to build a cell means model on LME
> > when there are several fixed effect  as factors in the model and also
> > interactions between them.  Can anybody give me a hint on how to do
> > accomplish this? How can I override the default "Contr.Treatment" for
> > linear models?  I tried removing the intercept but this will only give
> > me the mean values just for the first factor included in the model but
> > then will use "contrast treatment" for all the other factors.
>
>You can't do what you want to do by defining interactions.
>
>If you really want a cell means model then construct a new factor
>with one level for each unique combination of your fixed factors and
>use this constructed factor and - 1 in the formula.
>
>That's what a cell means model is.  There is one parameter for each
>unique combination of factor levels from the original design.
>
>Let me know if this answer is too vague and I will provide an example.
>
>--
>Douglas Bates                            bates at stat.wisc.edu
>Statistics Department                    608/262-2598
>University of Wisconsin - Madison        http://www.stat.wisc.edu/~bates/

_________________________________________________________________
See when your friends are online with MSN Messenger 6.0. Download it now



From seanpor at acm.org  Wed Oct 15 11:06:36 2003
From: seanpor at acm.org (Sean O'Riordain)
Date: Wed, 15 Oct 2003 10:06:36 +0100
Subject: [R] r-ish ? how can i improve my code?
Message-ID: <3F8D0E1C.9060508@acm.org>

Hi Folks,

I'm trying to learn R.  One of my intentions is to do some Monte-Carlo 
type modelling of road "accidents".

Below, to simplify things, I've appended a little program which does a 
'monte-carlo' type simulation.  However, it is written in a way which 
seems a bit un-natural in R.  Could someone help me make this a bit more 
R-ish please?

Or is there a completely different approach I should be taking?

Many thanks in advance,
Sean O'Riordain
seanpor AT acm.org


--------------------------------------------
n <- 900; # number of valid items required...

x <- numeric(n);
y <- numeric(n);
z <- numeric(n);

c <- 1;  # current 'array' pointer
tc <- 0; # total items actually looked at...

while (c <= n) {
     x[c] = runif(1, 0, 1);
     y[c] = runif(1, 0, 1);

     z[c] = sqrt(x[c]^2 + y[c]^2);
     if (z[c] < 1)
         c <- c + 1;
     tc <- tc + 1;
}

print("'overwork' ratio");
print(tc/(c-1));
plot(x,y);



From Simon.Fear at synequanon.com  Wed Oct 15 11:08:41 2003
From: Simon.Fear at synequanon.com (Simon Fear)
Date: Wed, 15 Oct 2003 10:08:41 +0100
Subject: is.na(v)<-b (was: Re: [R] Beginner's query - segmentation fault)
Message-ID: <6C8A8033ABC1E3468048ABC4F13CE572AC0E2B@synequanon01>

I think the thread ended up with several people (not only me)
feeling certain they didn't like `is.na<-` but with the 
developers defending it and me not really understanding
why.

Uwe Ligges was going to come up with an example of
`<- NA` going wrong (sorry Brian R, I mean behaving
unexpectedly), but never did, and I think the problem
has been fixed. It was apparently a problem with assigning
NAs to an existing factor, but the code for `[<-.factor`
looks pretty robust to me [not that I'm at all qualified to say
that, be warned]. Interestingly, at some point both methods
for `is.na<-` perform this operation: x[value] <- NA. Ahem.

By the way, `is.na(x) <- FALSE` will leave x unchanged (including
leaving it as NA ! how bad is that ?!)

> -----Original Message-----
> From: Paul Lemmens [mailto:P.Lemmens at nici.kun.nl]
> Sent: 14 October 2003 16:10
> To: r-help at stat.math.ethz.ch
> Subject: RE: is.na(v)<-b (was: Re: [R] Beginner's query - segmentation
> fault)
> 
> 
> Security Warning:
> If you are not sure an attachment is safe to open please contact 
> Andy on x234. There are 0 attachments with this message.
> ________________________________________________________________
> 
> By accident I'm also toying around with NA's, so I started 
> reading up on
> this thread but failed to find a 'concluding' remark or advice. As a
> naive 
> R user I would have loved to see a comment "do it like this".
> 
> The prevailing opinion seemed to be that is.na() might be 
> better (safer)
> but x <- NA is much clearer to understand. Can I relatively safely use
> the 
> easy form, or is it better to remember (the hard way) the 
> safer version?
> Has the discussion continued privately or just stopped here?
> 
> Personally I still find the fragments below (taken from the 
> thread) very
> counter intuitive, not to say scary.
> 
> x <- 1:10
> is.na(x) <- 1:5
> 
> and
> 
> is.na(x) <- FALSE
> 
> 
> It's very hard to understand what happens (as layman) because the 
> assignment seems to reverse in meaning in the first example (actually 
> taking indices 1:5 of x and assigning those the value NA) 
> whereas in the
> second case it's not obvious what happens to x: will it get the value
> FALSE 
> or will the original value remain(*).
> 
> IMHO the <- NA construct is much easier to understand and 
> should be made
> safe in all possible situations (whatever the underlying 
> safety problem
> or 
> other difficulties might be).
> 
> 
> kind regards,
> Paul
> 
> (*) Such a remark will probably lead to some kind of reprimand because
> it's 
> probably somewhere within the 10e6 manual pages but I'm trying my luck
> here.
> 
> 
> -- 
> Paul Lemmens
> NICI, University of Nijmegen              ASCII Ribbon Campaign /"\
> Montessorilaan 3 (B.01.03)                    Against HTML Mail \ /
> NL-6525 HR Nijmegen                                              X
> The Netherlands                                                 / \
> Phonenumber    +31-24-3612648
> Fax            +31-24-3616066
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
>
 

Simon Fear
Senior Statistician
Syne qua non Ltd
Tel: +44 (0) 1379 644449
Fax: +44 (0) 1379 644445
email: Simon.Fear at synequanon.com
web: http://www.synequanon.com
 
Number of attachments included with this message: 0
 
This message (and any associated files) is confidential and\...{{dropped}}



From spencer.graves at pdf.com  Wed Oct 15 11:40:24 2003
From: spencer.graves at pdf.com (Spencer Graves)
Date: Wed, 15 Oct 2003 02:40:24 -0700
Subject: [R] r-ish ? how can i improve my code?
In-Reply-To: <3F8D0E1C.9060508@acm.org>
References: <3F8D0E1C.9060508@acm.org>
Message-ID: <3F8D1608.1070501@pdf.com>

      1.  I suggest you avoid using "c" as a loop index, as it conflicts 
with the name of a function.  R is smart enough to figure out the 
difference in some cases but not in all. 

      2. How about the following: 

n <- 900
x <- runif(n)
y <- runif(n)
z <- sqrt(x^2+y^2)
print(list(Overwork.ratio = n/sum(z<1)))
plot(x, y)
theta <- seq(0, pi/2, length=31)
lines(sin(theta), cos(theta))
######################

      This won't give you n numbers z < 1.  If you need exactly that, 
what about first generating, say, 2*n, then either throwing away the 
excess or generating another 2*n if you don't have enough?  You can use 
a recursive function for this, which would almost never recurse with 2*n 
but might with 1.1*n. 

      hope this helps.  spencer graves

Sean O'Riordain wrote:

> Hi Folks,
>
> I'm trying to learn R.  One of my intentions is to do some Monte-Carlo 
> type modelling of road "accidents".
>
> Below, to simplify things, I've appended a little program which does a 
> 'monte-carlo' type simulation.  However, it is written in a way which 
> seems a bit un-natural in R.  Could someone help me make this a bit 
> more R-ish please?
>
> Or is there a completely different approach I should be taking?
>
> Many thanks in advance,
> Sean O'Riordain
> seanpor AT acm.org
>
>
> --------------------------------------------
> n <- 900; # number of valid items required...
>
> x <- numeric(n);
> y <- numeric(n);
> z <- numeric(n);
>
> c <- 1;  # current 'array' pointer
> tc <- 0; # total items actually looked at...
>
> while (c <= n) {
>     x[c] = runif(1, 0, 1);
>     y[c] = runif(1, 0, 1);
>
>     z[c] = sqrt(x[c]^2 + y[c]^2);
>     if (z[c] < 1)
>         c <- c + 1;
>     tc <- tc + 1;
> }
>
> print("'overwork' ratio");
> print(tc/(c-1));
> plot(x,y);
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help



From pburns at pburns.seanet.com  Wed Oct 15 11:42:59 2003
From: pburns at pburns.seanet.com (Patrick Burns)
Date: Wed, 15 Oct 2003 10:42:59 +0100
Subject: [R] r-ish ? how can i improve my code?
References: <3F8D0E1C.9060508@acm.org>
Message-ID: <3F8D16A3.5090807@pburns.seanet.com>

For this particular problem you can probably use
polar coordinates.

But something similar to your code could be:

x <- runif(900)
y <- runif(900)
z <- sqrt(x^2 + y^2)
okay <- z < 1
while(any(!okay)) {
    n.bad <- sum(!okay)
    x[!okay] <- runif(n.bad)
    y[!okay] <- runif(n.bad)
    z <- sqrt(x^2 + y^2)  # restricting to !okay may or may not be useful
    okay <- z < 1
}



Patrick Burns

Burns Statistics
patrick at burns-stat.com
+44 (0)20 8525 0696
http://www.burns-stat.com
(home of S Poetry and "A Guide for the Unwilling S User")

Sean O'Riordain wrote:

> Hi Folks,
>
> I'm trying to learn R.  One of my intentions is to do some Monte-Carlo 
> type modelling of road "accidents".
>
> Below, to simplify things, I've appended a little program which does a 
> 'monte-carlo' type simulation.  However, it is written in a way which 
> seems a bit un-natural in R.  Could someone help me make this a bit 
> more R-ish please?
>
> Or is there a completely different approach I should be taking?
>
> Many thanks in advance,
> Sean O'Riordain
> seanpor AT acm.org
>
>
> --------------------------------------------
> n <- 900; # number of valid items required...
>
> x <- numeric(n);
> y <- numeric(n);
> z <- numeric(n);
>
> c <- 1;  # current 'array' pointer
> tc <- 0; # total items actually looked at...
>
> while (c <= n) {
>     x[c] = runif(1, 0, 1);
>     y[c] = runif(1, 0, 1);
>
>     z[c] = sqrt(x[c]^2 + y[c]^2);
>     if (z[c] < 1)
>         c <- c + 1;
>     tc <- tc + 1;
> }
>
> print("'overwork' ratio");
> print(tc/(c-1));
> plot(x,y);
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
>
>



From p.pagel at gsf.de  Wed Oct 15 11:58:56 2003
From: p.pagel at gsf.de (Philipp Pagel)
Date: Wed, 15 Oct 2003 11:58:56 +0200
Subject: [R] r-ish ? how can i improve my code?
In-Reply-To: <3F8D0E1C.9060508@acm.org>
References: <3F8D0E1C.9060508@acm.org>
Message-ID: <20031015095856.GA2414@porcupine.gsf.de>

On Wed, Oct 15, 2003 at 10:06:36AM +0100, Sean O'Riordain wrote:


> n <- 900; # number of valid items required...
> 
> x <- numeric(n);
> y <- numeric(n);
> z <- numeric(n);
> 
> c <- 1;  # current 'array' pointer
> tc <- 0; # total items actually looked at...
> 
> while (c <= n) {
>     x[c] = runif(1, 0, 1);
>     y[c] = runif(1, 0, 1);
> 
>     z[c] = sqrt(x[c]^2 + y[c]^2);
>     if (z[c] < 1)
>         c <- c + 1;
>     tc <- tc + 1;
> }
> 
> print("'overwork' ratio");
> print(tc/(c-1));
> plot(x,y);


If I read this correctly you want to find the frequency of the event
"sqrt(x^2 + y^2) < 1" where 0 <= x, y <= 1  
right?

How about this:

n <- 1000
z <- sqrt(runif(n,0,1)^2 + runif(n,0,1)^2)
ratio <- (length(z)-1) / (length( z[z<1] ))

The main difference of course is that I uses a fixed number of "total
items" rather than "valid items". If that's a problem and you need
exactly 900 "valid items" things get a bit more complicated...

cu
	Philipp

-- 
Dr. Philipp Pagel                                Tel.  +49-89-3187-3675
Institute for Bioinformatics / MIPS              Fax.  +49-89-3187-3585
GSF - National Research Center for Environment and Health
Ingolstaedter Landstrasse 1
85764 Neuherberg, Germany



From seanpor at acm.org  Wed Oct 15 12:45:25 2003
From: seanpor at acm.org (Sean O'Riordain)
Date: Wed, 15 Oct 2003 11:45:25 +0100
Subject: [R] r-ish ? how can i improve my code?
In-Reply-To: <3F8D16A3.5090807@pburns.seanet.com>
References: <3F8D0E1C.9060508@acm.org> <3F8D16A3.5090807@pburns.seanet.com>
Message-ID: <3F8D2545.8000807@acm.org>

Hi Patrick, Spencer,

Thanks for that!  Both your solutions are MUCH quicker!

afaik overwork_ratio = 4/pi # what a hard way to calculate this :-)

The real problem I'm actually working on is a little more complicated 
:-) - 6 'random' variables, and 18 dependant variables (at last 
count)... currently only 4 computed terms in the 'if() c<-c+1' 
statement...  I just created the x,y area problem as a simple example of 
the style I was using.

In the past I've worked on other 'accident' related problems and one 
difficulty was in getting enough final 'z' values to be statistically 
significant as there was an enormous rejection ratio - but it was 
running on a 486 written in compiled turbo pascal and it would take days 
of work...

I'll have a go at recoding using your techniques.

Many Thanks,
Sean


Patrick Burns wrote:
> For this particular problem you can probably use
> polar coordinates.
> 
> But something similar to your code could be:
> 
> x <- runif(900)
> y <- runif(900)
> z <- sqrt(x^2 + y^2)
> okay <- z < 1
> while(any(!okay)) {
>    n.bad <- sum(!okay)
>    x[!okay] <- runif(n.bad)
>    y[!okay] <- runif(n.bad)
>    z <- sqrt(x^2 + y^2)  # restricting to !okay may or may not be useful
>    okay <- z < 1
> }
> 
> 
> 
> Patrick Burns
> 
> Burns Statistics
> patrick at burns-stat.com
> +44 (0)20 8525 0696
> http://www.burns-stat.com
> (home of S Poetry and "A Guide for the Unwilling S User")
> 
> Sean O'Riordain wrote:
> 
>> Hi Folks,
>>
>> I'm trying to learn R.  One of my intentions is to do some Monte-Carlo 
>> type modelling of road "accidents".
>>
>> Below, to simplify things, I've appended a little program which does a 
>> 'monte-carlo' type simulation.  However, it is written in a way which 
>> seems a bit un-natural in R.  Could someone help me make this a bit 
>> more R-ish please?
>>
>> Or is there a completely different approach I should be taking?
>>
>> Many thanks in advance,
>> Sean O'Riordain
>> seanpor AT acm.org
>>
>>
>> --------------------------------------------
>> n <- 900; # number of valid items required...
>>
>> x <- numeric(n);
>> y <- numeric(n);
>> z <- numeric(n);
>>
>> c <- 1;  # current 'array' pointer
>> tc <- 0; # total items actually looked at...
>>
>> while (c <= n) {
>>     x[c] = runif(1, 0, 1);
>>     y[c] = runif(1, 0, 1);
>>
>>     z[c] = sqrt(x[c]^2 + y[c]^2);
>>     if (z[c] < 1)
>>         c <- c + 1;
>>     tc <- tc + 1;
>> }
>>
>> print("'overwork' ratio");
>> print(tc/(c-1));
>> plot(x,y);
>>
>> ______________________________________________
>> R-help at stat.math.ethz.ch mailing list
>> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
>>
>>
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> 
>



From mikewhite.diu at tiscali.co.uk  Wed Oct 15 12:46:46 2003
From: mikewhite.diu at tiscali.co.uk (Mike White)
Date: Wed, 15 Oct 2003 11:46:46 +0100
Subject: Fw: [R] SIMCA algorithm implementation
Message-ID: <001201c39309$a2012a80$be67e150@FSSFQCV7BGDVED>

I have used PCA for data classification by visual examination of the 3D
scatter plot of the first 3 principal components.  I now want to use the
results to predict the class for new data.  I have used predict.princomp to
predict the scores and then visualise the results on a 3D scatter plot using
the rgl library.  However, is there an R function that will fit the new data
to the class assignments derived from PCA?  I think this is similar to what
the SIMCA algoirthm does.

Thanks
Mike

----- Original Message -----
From: "Mike White" <mikewhite.diu at tiscali.co.uk>
To: <R-help at stat.math.ethz.ch>
Sent: Wednesday, October 08, 2003 9:25 AM
Subject: Re: [R] SIMCA algorithm implementation


> Dear All
> Is there a SIMCA (Soft Independent Modelling Class Analogy) implementation
> on R or does anyone know if is it possible to replicate the SIMCA
algorithm
> using existing R functions?
>
> Thanks
> Mike White
>



From P.Lemmens at nici.kun.nl  Wed Oct 15 13:05:07 2003
From: P.Lemmens at nici.kun.nl (Paul Lemmens)
Date: Wed, 15 Oct 2003 13:05:07 +0200
Subject: is.na(v)<-b (was: Re: [R] Beginner's query - segmentation fault)
In-Reply-To: <6C8A8033ABC1E3468048ABC4F13CE572AC0E2B@synequanon01>
References: <6C8A8033ABC1E3468048ABC4F13CE572AC0E2B@synequanon01>
Message-ID: <17404593.1066223107@lemmens.socsci.kun.nl>

Hello Simon,

--On woensdag 15 oktober 2003 10:08 +0100 Simon Fear 
<Simon.Fear at synequanon.com> wrote:

> By the way, `is.na(x) <- FALSE` will leave x unchanged (including
> leaving it as NA ! how bad is that ?!)
>
Twilight Zone (Golden Earring). But with that remark I'm getting off topic, 
so thank you for your summary. I've already memorized the is.na() 
construct, so I should be safe for the time being :>

kind regards,
Paul



-- 
Paul Lemmens
NICI, University of Nijmegen              ASCII Ribbon Campaign /"\
Montessorilaan 3 (B.01.03)                    Against HTML Mail \ /
NL-6525 HR Nijmegen                                              X
The Netherlands                                                 / \
Phonenumber    +31-24-3612648
Fax            +31-24-3616066



From chrysopa at insecta.ufv.br  Wed Oct 15 12:09:45 2003
From: chrysopa at insecta.ufv.br (Ronaldo Reis Jr.)
Date: Wed, 15 Oct 2003 08:09:45 -0200
Subject: [R] [OFF] Dataset for extra Crawley Chapter
In-Reply-To: <F0E0B899CB43D5118D220002A55113CF02F92803@s2-edm-r1.nofc.cfs.nrcan.gc.ca>
References: <F0E0B899CB43D5118D220002A55113CF02F92803@s2-edm-r1.nofc.cfs.nrcan.gc.ca>
Message-ID: <200310150809.45372.chrysopa@insecta.ufv.br>

Em Ter 14 Out 2003 12:01, Yang, Richard escreveu:
> Ronaldo;
>
> 	You can download all datasets used in Crawley's book from his web
> site.
>
> -- Richard
>

In the web site dont have the dataset for extra chapters, only for the printed 
book's chapters.

Thanks

Ronaldo

-- 
Todo mundo precisa crer em algo. Creio que vou tomar outra cerveja.
                -- Grouxo Marx
--
|>   // | \\   [***********************************]
|   ( ?   ? )  [Ronaldo Reis J?nior                ]
|>      V      [UFV/DBA-Entomologia                ]
|    /     \   [36571-000 Vi?osa - MG              ]
|>  /(.''`.)\  [Fone: 31-3899-2532                 ]
|  /(: :'  :)\ [chrysopa at insecta.ufv.br            ]
|>/ (`. `'` ) \[ICQ#: 5692561 | LinuxUser#: 205366 ]
|    ( `-  )   [***********************************]
|>>  _/   \_Powered by GNU/Debian Woody/Sarge



From rksh at soc.soton.ac.uk  Wed Oct 15 13:16:57 2003
From: rksh at soc.soton.ac.uk (Robin Hankin)
Date: Wed, 15 Oct 2003 12:16:57 +0100
Subject: [R] mapply() gives seg fault
In-Reply-To: <Pine.A41.4.58.0310141049530.55772@homer07.u.washington.edu>
References: <a06002007bbb1c91c2d44@[139.166.242.29]>
	<Pine.A41.4.58.0310140848160.36704@homer15.u.washington.edu>
	<a0600200bbbb1cc13df05@[139.166.242.29]>
	<Pine.A41.4.58.0310141049530.55772@homer07.u.washington.edu>
Message-ID: <a0600200cbbb2d685526d@[139.166.242.29]>

Thanks for this everyone.

I didn't know that a seg fault was an operating system error message 
(in fact, I don't really know what a seg fault is... I'm not a C 
programmer, just plain Mr Joe Q Public, R user).

I would say the FAQ makes sense only if you know what kind of error 
messages operating systems are likely to report.  Mentioning seg 
faults  explicitly  in FAQ-9.1 might be helpful for nonprogrammers 
like me.   Something like "a seg fault is always a bug" would be good.

best wishes

robin


>On Tue, 14 Oct 2003, Robin Hankin wrote:
>
>>  Hello again
>>
>>  thanks for this.  What I should have asked is, should one always report
>>  repeatable seg faults (even if functions are called with
>>  inappropriate arguments)?
>>
>
>Yes.  Reproducible segfaults that occur `in the wild' are fairly important
>bugs even if they result from errors.
>
>Segfaults from trying to break the system are a much lower priority, but
>even they should probably be reported.
>
>The FAQ (9.1) says
>	If R executes an illegal instruction, or dies with an operating system
>	error message that indicates a problem in the program (as opposed to
>	something like "disk full"), then it is certainly a bug.
>and goes on to explain that the only time this doesn't apply is when you
>call compiled code with the wrong arguments or call your own compiled
>code.
>
>	-thomas
>
>______________________________________________
>R-help at stat.math.ethz.ch mailing list
>https://www.stat.math.ethz.ch/mailman/listinfo/r-help



From hi_ono2001 at ybb.ne.jp  Wed Oct 15 14:18:28 2003
From: hi_ono2001 at ybb.ne.jp (Hisaji Ono)
Date: Wed, 15 Oct 2003 21:18:28 +0900
Subject: [R] Where is R_PHP_Online available?
References: <Pine.LNX.4.21.0306130233350.23727-100000@paradise.stat.tku.edu.tw>
Message-ID: <028701c39316$8533e480$818001db@webgis>

Hi.

 Currently, I couldn't access following sites.

  Where is R_PHP_Online available?

>
> R_PHP_Online is a PHP CGI web interface to run R programs online, with the
> capability to show graphics online.
>
> You can get current version of R_PHP_Online from
>
> http://steve-chen.net/R_PHP/
>
> or
>
> http://steve.stat.tku.edu.tw/R_PHP/
>
>



From sway at tanox.com  Wed Oct 15 14:57:26 2003
From: sway at tanox.com (Shawn Way)
Date: Wed, 15 Oct 2003 07:57:26 -0500
Subject: [R] Design and Hmisc
Message-ID: <2F3262756375D411B0CC00B0D049775D0138013B@westpark>


I'm looking for design and hmisc version 2.0 for R 1.8 for windows.  I've
found design 2.0 in the downloads for R1.7 but not hmisc.  

I've also checked Dr. Harrell's site and it only goes to 1.6 for windows.

Any thoughts?


Shawn Way



From steve at paradise.stat.tku.edu.tw  Wed Oct 15 15:34:23 2003
From: steve at paradise.stat.tku.edu.tw (Steve Chen)
Date: Wed, 15 Oct 2003 21:34:23 +0800 (CST)
Subject: [R] Re: Where is R_PHP_Online available?
In-Reply-To: <028701c39316$8533e480$818001db@webgis>
Message-ID: <Pine.LNX.4.21.0310152132150.26718-100000@paradise.stat.tku.edu.tw>


Hi All,

I re-organized my site few days ago and forgot to
keep the directory of R_PHP_Online.

It's now back again and you can find it at

http://steve-chen.net/R_PHP 

Steve

On Wed, 15 Oct 2003, Hisaji Ono wrote:

> Hi.
> 
>  Currently, I couldn't access following sites.
> 
>   Where is R_PHP_Online available?
> 
> >
> > R_PHP_Online is a PHP CGI web interface to run R programs online, with the
> > capability to show graphics online.
> >
> > You can get current version of R_PHP_Online from
> >
> > http://steve-chen.net/R_PHP/
> >
> > or
> >
> > http://steve.stat.tku.edu.tw/R_PHP/
> >
> >
>



From bates at stat.wisc.edu  Wed Oct 15 15:40:33 2003
From: bates at stat.wisc.edu (Douglas Bates)
Date: 15 Oct 2003 08:40:33 -0500
Subject: [R] Example of cell means model
In-Reply-To: <Law14-F53S4sCvXihqv00010aa5@hotmail.com>
References: <Law14-F53S4sCvXihqv00010aa5@hotmail.com>
Message-ID: <6r1xte62lq.fsf@bates4.stat.wisc.edu>

This is an example from chapter 11 of the 6th edition of Devore's
engineering statistics text.  It happens to be a balanced data set in
two factors but the calculations will also work for unbalanced data.

I create a factor called 'cell' from the text representation of the
Variety level and the Density level using '/' as the separator
character. The coefficients for the linear model 'Yield ~ cell - 1'
are exactly the cell means.

> library(Devore6)
> data(xmp11.07)
> str(xmp11.07)
`data.frame':	36 obs. of  3 variables:
 $ Yield  : num  10.5 9.2 7.9 12.8 11.2 13.3 12.1 12.6 14 10.8 ...
 $ Variety: Factor w/ 3 levels "1","2","3": 1 1 1 1 1 1 1 1 1 1 ...
 $ Density: Factor w/ 4 levels "1","2","3","4": 1 1 1 2 2 2 3 3 3 4 ...
> xmp11.07$cell = with(xmp11.07, factor(paste(Variety, Density, sep = '/')))
> xtabs(~ Variety + Density, xmp11.07)
       Density
Variety 1 2 3 4
      1 3 3 3 3
      2 3 3 3 3
      3 3 3 3 3
> means = xtabs(Yield ~ Variety+Density, xmp11.07)/xtabs(~ Variety + Density, xmp11.07)
> means
       Density
Variety 1         2         3         4        
      1  9.200000 12.433333 12.900000 10.800000
      2  8.933333 12.633333 14.500000 12.766667
      3 16.300000 18.100000 19.933333 18.166667
> coef(fm1 <- lm(Yield ~ cell - 1, xmp11.07))
  cell1/1   cell1/2   cell1/3   cell1/4   cell2/1   cell2/2   cell2/3   cell2/4 
 9.200000 12.433333 12.900000 10.800000  8.933333 12.633333 14.500000 12.766667 
  cell3/1   cell3/2   cell3/3   cell3/4 
16.300000 18.100000 19.933333 18.166667 

The residual sum of squares for this model is the same as that for the
model with crossed terms

> deviance(fm1)
[1] 38.04
> deviance(fm2 <- lm(Yield ~ Variety * Density, xmp11.07))
[1] 38.04

but the coefficients are quite different because they represent a
different parameterization.

> coef(fm2)
      (Intercept)          Variety2          Variety3          Density2 
       9.20000000       -0.26666667        7.10000000        3.23333333 
         Density3          Density4 Variety2:Density2 Variety3:Density2 
       3.70000000        1.60000000        0.46666667       -1.43333333 
Variety2:Density3 Variety3:Density3 Variety2:Density4 Variety3:Density4 
       1.86666667       -0.06666667        2.23333333        0.26666667 

I hope this answers your question.  Sorry for the delay in
responding to you.

"Francisco Vergara" <gerifalte28 at hotmail.com> writes:

> Many thanks for your reply.  An example would be very helpful to make
> sure that I understand correctly what you described.



From hi_ono2001 at ybb.ne.jp  Wed Oct 15 15:43:01 2003
From: hi_ono2001 at ybb.ne.jp (Hisaji Ono)
Date: Wed, 15 Oct 2003 22:43:01 +0900
Subject: [R] Where is R_PHP_Online available?
References: <Pine.LNX.4.21.0306130233350.23727-100000@paradise.stat.tku.edu.tw>
	<028701c39316$8533e480$818001db@webgis> <3F8D3DB1.4050408@acm.org>
Message-ID: <02ab01c39322$40c884c0$818001db@webgis>

Thanks!

----- Original Message ----- 
From: "Sean O'Riordain" <seanpor at acm.org>
To: "Hisaji Ono" <hi_ono2001 at ybb.ne.jp>
Sent: Wednesday, October 15, 2003 9:29 PM
Subject: Re: [R] Where is R_PHP_Online available?


> Hi!
> 
> Try going to http://steve-chen.net/ and folling the link for "R" at the 
> top of the page.
>



From ligges at statistik.uni-dortmund.de  Wed Oct 15 15:47:59 2003
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Wed, 15 Oct 2003 15:47:59 +0200
Subject: [R] Design and Hmisc
In-Reply-To: <2F3262756375D411B0CC00B0D049775D0138013B@westpark>
References: <2F3262756375D411B0CC00B0D049775D0138013B@westpark>
Message-ID: <3F8D500F.4020201@statistik.uni-dortmund.de>

Shawn Way wrote:

> I'm looking for design and hmisc version 2.0 for R 1.8 for windows.  I've
> found design 2.0 in the downloads for R1.7 but not hmisc.  
> 
> I've also checked Dr. Harrell's site and it only goes to 1.6 for windows.
> 
> Any thoughts?

Yes.

The ReadMe at CRAN/bin/windows/contrib/1.8 (for R-1.8.x) tells us:

'Packages that do not compile out of the box or do not pass
"Rcmd check" with "OK" or at least a "WARNING" will *not* be
published. This "Status", i.e. result of "Rcmd check", is listed in
file "Status". Possible values are "OK", "WARN", and "ERROR".
Corresponding check.log files can be found in subdirectory ./check.'

And you'll find that both packages produce errors:

For Hmisc:

* checking Hmisc-manual.tex ... ERROR
LaTeX errors when creating DVI version.
This typically indicates Rd problems.


And therefore Design has:

* checking package dependencies ... ERROR
Packages required but not available:
   Hmisc


Frank E Harrell (in CC), the maintainer of those packages, might want to 
fix it the problem.
Frank, does Hmisc pass make check on another OS? Can you fix it (easily)?

Uwe Ligges


> 
> Shawn Way
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help



From ripley at stats.ox.ac.uk  Wed Oct 15 15:49:57 2003
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Wed, 15 Oct 2003 14:49:57 +0100 (BST)
Subject: [R] Design and Hmisc
In-Reply-To: <2F3262756375D411B0CC00B0D049775D0138013B@westpark>
Message-ID: <Pine.LNX.4.44.0310151439100.1447-100000@gannet.stats>

Try reading the ReadMe = @ReadMe file in the /bin/windows/contrib/1.8 
directory!  That points you at the check or last directories.

Hmisc is failing Rcmd check under Windows, and Design depends on Hmisc.
There is a compiled Hmisc in 1.7/last you could try.

On Wed, 15 Oct 2003, Shawn Way wrote:

> 
> I'm looking for design and hmisc version 2.0 for R 1.8 for windows.  I've
> found design 2.0 in the downloads for R1.7 but not hmisc.  
> 
> I've also checked Dr. Harrell's site and it only goes to 1.6 for windows.
> 
> Any thoughts?

Uwe has provided you with complete diagnostic information: please make use 
of it.

You can of course compile from the sources yourself.

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From tblackw at umich.edu  Wed Oct 15 16:53:29 2003
From: tblackw at umich.edu (Thomas W Blackwell)
Date: Wed, 15 Oct 2003 10:53:29 -0400 (EDT)
Subject: Fw: [R] SIMCA algorithm implementation
In-Reply-To: <001201c39309$a2012a80$be67e150@FSSFQCV7BGDVED>
References: <001201c39309$a2012a80$be67e150@FSSFQCV7BGDVED>
Message-ID: <Pine.SOL.4.58.0310151031140.2133@zektor.gpcc.itd.umich.edu>

Mike  -

For predicting class membership, I would use either  lda() or
qda()  from the MASS package.  See the Venables and Ripley
book for detailed description of the methods.  You'll have
to rely on your own references for what the 'SIMCA' algorithm
actually does.  I've never heard of it.  Sounds like some
sort of discriminant analysis to me.  Maybe its authors
would like to contribute an implementation of it to R.

-  tom blackwell  -  u michigan medical school  -  ann arbor  -

On Wed, 15 Oct 2003, Mike White wrote:

> I have used PCA for data classification by visual examination of the 3D
> scatter plot of the first 3 principal components.  I now want to use the
> results to predict the class for new data.  I have used predict.princomp to
> predict the scores and then visualise the results on a 3D scatter plot using
> the rgl library.  However, is there an R function that will fit the new data
> to the class assignments derived from PCA?  I think this is similar to what
> the SIMCA algoirthm does.
>
> Thanks
> Mike
>
> ----- Original Message -----
> From: "Mike White" <mikewhite.diu at tiscali.co.uk>
> To: <R-help at stat.math.ethz.ch>
> Sent: Wednesday, October 08, 2003 9:25 AM
> Subject: Re: [R] SIMCA algorithm implementation
>
>
> > Dear All
> > Is there a SIMCA (Soft Independent Modelling Class Analogy) implementation
> > on R or does anyone know if is it possible to replicate the SIMCA
> algorithm
> > using existing R functions?
> >
> > Thanks
> > Mike White
>



From baud-bovy.gabriel at hsr.it  Wed Oct 15 17:10:40 2003
From: baud-bovy.gabriel at hsr.it (Gabriel Baud-Bovy)
Date: Wed, 15 Oct 2003 17:10:40 +0200
Subject: [R] tkcanvas/bitmap for Turtle World
Message-ID: <5.2.1.1.1.20031015161944.00b82b10@mail.hsr.it>

To represent a turtle inside a canvas (tcltk package),  I have a serie of 
bitmaps
representing the turtle heading in different directions and I would like
to display the one that corresponds to the current direction of the turtle
(function below).

I have created with Paint a bitmap representing the turtle in BMP format
and succeeded displaying it on canvas after converting it into XBM format
with XBM-BMP utility by Allen Lam :).

However, I have a problem with the rotated versions (rotation done in Adobe
Illustrator and file saved as BMP). When I try to convert the rotated BMP 
into XBM,
the turtle is not centered anymore, its size changes and background becomes 
black.
I know that is not easy to rotate bitmaps but there should be better solution
than Adobe Illustrator (part of the problem might be to know exactly how to 
save it
to keep the same size and appearance).

- does canvas in tcltk support other bitmap formats than BMP on a PC 
(Windows) ?
- is there an utility to rotate bitmap in BMP format (2 colors) working on 
the PC?
(found some for UNIX/LINUX but none on PC).
- is there a simpler way?

Sorry if the question is not exactly R but I am learning to use canvas widget
from tcltk package  (thanks to Barry Rowlings and Damon Wischik for their
replies to my previous post).

Gabriel

Here is my function:

setTurtle<-function(turtle,x,y,angle) {
     if(angle!=turtle$angle) {
        path<-"@c:/Docume~1/Owner/MyDocu~1/MyWork~1/MILAN/UNIHSR/R/turtle_bitmap/"
        # select bitmap file (turtle000.xbm, turtle045.xbm, ...)	
        heading<-(45*(angle*180/pi+22.5)%/%45)%%360
        file<-paste("turtle",paste(rep(0,3-nchar(heading)),collapse=""),heading,".xbm",sep="")
        # redraw
        tkdelete(turtle$canvas,turtle$turtle)
        turtle$turtle<-tkcreate(turtle$canvas,"bitmap",x,y,anchor="center",bitmap=paste(path,file,sep=""))
        turtle$angle <- angle
     }
     if(x!=turtle$x || y!=turtle$y) {
        tkmove(turtle$canvas,turtle$turtle,x-turtle$x,y-turtle$y)
        turtle$x <- x
        turtle$y <- y
     }
     turtle
}
--------------------------------------------------------------------
Gabriel Baud-Bovy
Assistant Professor
UHSR University
via Olgettina, 58	tel:  (+39) 02 2643 4839
20132 Milan, Italy	fax: (+39) 02 2643 4892



From mfbattle at artsci.wustl.edu  Wed Oct 15 17:28:29 2003
From: mfbattle at artsci.wustl.edu (Martin F. Battle)
Date: Wed, 15 Oct 2003 10:28:29 -0500 (CDT)
Subject: [R] nlme and multinomial choice
Message-ID: <Pine.GSO.4.58.0310151026590.8683@ascc.artsci.wustl.edu>

I am trying to estimate a multinomial choice model with three levels of
data.  I am wondering if I can do this in R, as the nlme command does not
seem to be able to do this?

Thank you,

Martin Battle

--------------------------------------

  Martin Battle
  Ph.D. Student in Political Science
  Washington University in St. Louis
  Department of Political Science
  Box 1063
  St. Louis MO 63130

  When people agree with me I always
  feel that I must be wrong.
  Oscar Wilde



From Jason.L.Higbee at stls.frb.org  Wed Oct 15 18:02:46 2003
From: Jason.L.Higbee at stls.frb.org (Jason.L.Higbee@stls.frb.org)
Date: Wed, 15 Oct 2003 11:02:46 -0500
Subject: R-WinEdt, [R] 1.8, deprecating warning 
Message-ID: <20031015160249.7432285E70@p3fed1.frb.org>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20031015/af89a9e4/attachment.pl

From jmacdon at med.umich.edu  Wed Oct 15 18:15:42 2003
From: jmacdon at med.umich.edu (James MacDonald)
Date: Wed, 15 Oct 2003 12:15:42 -0400
Subject: R-WinEdt, [R] 1.8, deprecating warning
Message-ID: <sf8d3a86.058@med-gwia-02a.med.umich.edu>

1.) You should not be concerned. I am sure the package maintainer will
fix this soon enough.

1a.) Deprecating means it will not be used in the future. For instance
<- and _ were equivalent in the past, but now _ is deprecated, and can
be used for purposes other than assignment.

2.) You could fix it yourself by changing all the return arguments, but
it will be fixed in due time anyway. Basically, if a function ends with
something like return(blah, blah1, blah2) you could change to
return(list(blah=blah, blah1=blah1, blah2=blah2)) and you wouldn't see
the warnings any more. However, this is just something the package
maintainer will have to deal with.

HTH,

Jim



James W. MacDonald
Affymetrix and cDNA Microarray Core
University of Michigan Cancer Center
1500 E. Medical Center Drive
7410 CCGC
Ann Arbor MI 48109
734-647-5623

>>> <Jason.L.Higbee at stls.frb.org> 10/15/03 12:02PM >>>
When I load R-WinEdt (>library(RWinEdt), I get the warning:

Warning message: 
multi-argument returns are deprecated in: return(InstallRoot, 
RWinEdtInstalled) 

I have upgraded to R 1. 8 on Windows, by copying non-base libraries
into 
the 1.8 library folder and updating the help.
I also reinstalled R-WinEdt from the zip file as detailed in RWinEdt 
ReadMe file using the recommend (A) installation and still get the same

warning.
I have a few questions:

1) Should I be concerned about this warning? 

1a) What is deprecating?

2) Is there a way fix it so that the warning message is fixed and
RWinEdt 
loads properly?

2a) I read the help file on the return function which mentions, "Prior
to 
R 1.8.0, 'value' could be a series of non-empty 
expressions separated by commas"  Would the fix be to find the part of

some code that is executing (whatever and where ever that might be) 
when 
the editor loads and change it to some thing like
">return(InstallRoot); 
return(RWinEdtInstalled)"?


Thanks for you comments and assistance,

Jason Higbee
Research Analyst
Federal Reserve Bank of St. Louis
E: jason.l.higbee at stls.frg.org 
	[[alternative HTML version deleted]]

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://www.stat.math.ethz.ch/mailman/listinfo/r-help



From andy_liaw at merck.com  Wed Oct 15 18:15:48 2003
From: andy_liaw at merck.com (Liaw, Andy)
Date: Wed, 15 Oct 2003 12:15:48 -0400
Subject: Fw: [R] SIMCA algorithm implementation
Message-ID: <3A822319EB35174CA3714066D590DCD50205CCA1@usrymx25.merck.com>

SIMCA, I believe, is Svante Wold's invention, and extensively used in the
chemometrics area (analysis of data from analytic chemistry).  My vague
impression of what it does is PCA in the individual classes.  I have not
been able to locate a detail description of the algorithm.  (I'd appreciate
it very much if some one can help.)  One feature of the methodology that
distinguish it from others, I believe, is its ability to classify new
observations as belonging to classes other than what appeared in the
training data.

Contribution from it's author?  Fat chance.  Svante sells (stand-alone)
SIMCA at a premium price.

Best,
Andy


> -----Original Message-----
> From: Thomas W Blackwell [mailto:tblackw at umich.edu] 
> Sent: Wednesday, October 15, 2003 10:53 AM
> To: Mike White
> Cc: R-help at stat.math.ethz.ch
> Subject: Re: Fw: [R] SIMCA algorithm implementation
> 
> 
> Mike  -
> 
> For predicting class membership, I would use either  lda() or
> qda()  from the MASS package.  See the Venables and Ripley
> book for detailed description of the methods.  You'll have
> to rely on your own references for what the 'SIMCA' algorithm 
> actually does.  I've never heard of it.  Sounds like some 
> sort of discriminant analysis to me.  Maybe its authors would 
> like to contribute an implementation of it to R.
> 
> -  tom blackwell  -  u michigan medical school  -  ann arbor  -
> 
> On Wed, 15 Oct 2003, Mike White wrote:
> 
> > I have used PCA for data classification by visual 
> examination of the 
> > 3D scatter plot of the first 3 principal components.  I now want to 
> > use the results to predict the class for new data.  I have used 
> > predict.princomp to predict the scores and then visualise 
> the results 
> > on a 3D scatter plot using the rgl library.  However, is there an R 
> > function that will fit the new data to the class 
> assignments derived 
> > from PCA?  I think this is similar to what the SIMCA algoirthm does.
> >
> > Thanks
> > Mike
> >
> > ----- Original Message -----
> > From: "Mike White" <mikewhite.diu at tiscali.co.uk>
> > To: <R-help at stat.math.ethz.ch>
> > Sent: Wednesday, October 08, 2003 9:25 AM
> > Subject: Re: [R] SIMCA algorithm implementation
> >
> >
> > > Dear All
> > > Is there a SIMCA (Soft Independent Modelling Class Analogy) 
> > > implementation on R or does anyone know if is it possible to 
> > > replicate the SIMCA
> > algorithm
> > > using existing R functions?
> > >
> > > Thanks
> > > Mike White
> >
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list 
> https://www.stat.math.ethz.ch/mailman/listinfo> /r-help
>



From ligges at statistik.uni-dortmund.de  Wed Oct 15 18:22:08 2003
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Wed, 15 Oct 2003 18:22:08 +0200
Subject: R-WinEdt, [R] 1.8, deprecating warning
In-Reply-To: <20031015160249.7432285E70@p3fed1.frb.org>
References: <20031015160249.7432285E70@p3fed1.frb.org>
Message-ID: <3F8D7430.6000408@statistik.uni-dortmund.de>

Jason.L.Higbee at stls.frb.org wrote:

> When I load R-WinEdt (>library(RWinEdt), I get the warning:
> 
> Warning message: 
> multi-argument returns are deprecated in: return(InstallRoot, 
> RWinEdtInstalled) 
> 
> I have upgraded to R 1. 8 on Windows, by copying non-base libraries into 
> the 1.8 library folder and updating the help.
> I also reinstalled R-WinEdt from the zip file as detailed in RWinEdt 
> ReadMe file using the recommend (A) installation and still get the same 
> warning.
> I have a few questions:
> 
> 1) Should I be concerned about this warning? 


> 1a) What is deprecating?

See ?Deprecated:
"These functions are provided for compatibility with older versions of R 
only, and may be defunct as soon as of the next release."


> 2) Is there a way fix it so that the warning message is fixed and RWinEdt 
> loads properly?

Yes: Use the recent version of R-WinEdt in package RWinEdt_1.5-1.zip


> 2a) I read the help file on the return function which mentions, "Prior to 
> R 1.8.0, 'value' could be a series of non-empty 
> expressions separated by commas"  Would the fix be to find the part of 
> some code that is executing (whatever and where ever that might be)  when 
> the editor loads and change it to some thing like ">return(InstallRoot); 
> return(RWinEdtInstalled)"?

No, the fix is to change it to
   return(list(InstallRoot = InstallRoot,
     RWinEdtInstalled = WinEdtInstalled))
which already has been done for the recent version.


Uwe Ligges


> 
> Thanks for you comments and assistance,
> 
> Jason Higbee
> Research Analyst
> Federal Reserve Bank of St. Louis
> E: jason.l.higbee at stls.frg.org
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help



From alex_s_42 at yahoo.com  Wed Oct 15 18:32:12 2003
From: alex_s_42 at yahoo.com (Alexander Sirotkin [at Yahoo])
Date: Wed, 15 Oct 2003 09:32:12 -0700 (PDT)
Subject: [R] aov and non-categorical variables
Message-ID: <20031015163212.74491.qmail@web14904.mail.yahoo.com>

It is unclear to me how aov() handles non-categorical
variables.

I mean it works and produces results that I would
expect, but I was under impression that ANOVA is only
defined for categorical variables.

In addition, help(aov) says that it "call to 'lm' for
each  stratum", which  I presume means that it calls
to lm() for every group of the categorical variable,
however I don't quite understand what this means for
non-categorical variable.

Thanks



From alobo at ija.csic.es  Wed Oct 15 18:47:25 2003
From: alobo at ija.csic.es (Agustin Lobo)
Date: Wed, 15 Oct 2003 18:47:25 +0200 (MET DST)
Subject: [R] Subseting in a 3D array
Message-ID: <Pine.OSF.3.91.1031015183809.8877V-100000@paleo.ija.csic.es>


Hi!

I have a 3d array:
> dim(ib5km15.dbc)
[1] 190 241  19

and a set of positions to extract:
> ib5km.lincol.random[1:3,]
     [,1] [,2]
[1,]   78   70
[2,]   29  213
[3,]  180   22

Geting the values of a 2D array
for that set of positions would
be:

> ima <- ib5km15.dbc[,,1]
> ima[ib5km.lincol.random[1:10,]]

but don't find the way for the case
of the 3D array:

> ib5km15.dbc[ib5km.lincol.random[1:10,],]
Error in ib5km15.dbc[ib5km.lincol.random[1:10, ], ] :
        incorrect number of dimensions

Could anyone suggest the way of subseting
the 3D array to get a vector of z values
for each position recorded in ib5km.lincol.random?
(avoiding the use of for loops).

Thanks

Agus

Dr. Agustin Lobo
Instituto de Ciencias de la Tierra (CSIC)
Lluis Sole Sabaris s/n
08028 Barcelona SPAIN
tel 34 93409 5410
fax 34 93411 0012
alobo at ija.csic.es



From hi_ono2001 at ybb.ne.jp  Wed Oct 15 18:52:18 2003
From: hi_ono2001 at ybb.ne.jp (Hisaji Ono)
Date: Thu, 16 Oct 2003 01:52:18 +0900
Subject: [R] SOM library for R
Message-ID: <031b01c3933c$b19b9470$818001db@webgis>

Hi.

 Three years ago, I've read the question of availability of SOM library for
R using Kohonen's SOM_PAK in this mailing-list. This answer was no
availability. And no package dealing with SOM in CRAN.

 Is this situation same?

 Could you tell me availability this library?


 Best Regards.



From P.Lemmens at nici.kun.nl  Wed Oct 15 19:01:37 2003
From: P.Lemmens at nici.kun.nl (Paul Lemmens)
Date: Wed, 15 Oct 2003 19:01:37 +0200
Subject: [R] Subseting in a 3D array
In-Reply-To: <Pine.OSF.3.91.1031015183809.8877V-100000@paleo.ija.csic.es>
References: <Pine.OSF.3.91.1031015183809.8877V-100000@paleo.ija.csic.es>
Message-ID: <5880836.1066244497@[192.168.1.7]>

Hoi Agustin,

--On woensdag 15 oktober 2003 18:47 +0200 Agustin Lobo <alobo at ija.csic.es> 
wrote:

> Could anyone suggest the way of subseting
> the 3D array to get a vector of z values
> for each position recorded in ib5km.lincol.random?
> (avoiding the use of for loops).
>
Is section 5.3 from the Introduction to R (p21) helpfull?

regards,
Paul


-- 
Paul Lemmens
NICI, University of Nijmegen              ASCII Ribbon Campaign /"\
Montessorilaan 3 (B.01.03)                    Against HTML Mail \ /
NL-6525 HR Nijmegen                                              X
The Netherlands                                                 / \
Phonenumber    +31-24-3612648
Fax            +31-24-3616066



From tplate at blackmesacapital.com  Wed Oct 15 19:04:48 2003
From: tplate at blackmesacapital.com (Tony Plate)
Date: Wed, 15 Oct 2003 11:04:48 -0600
Subject: [R] Subseting in a 3D array
In-Reply-To: <Pine.OSF.3.91.1031015183809.8877V-100000@paleo.ija.csic.es
 >
Message-ID: <5.2.1.1.2.20031015110227.03712628@mailhost.blackmesacapital.com>

One way would be:

 > apply(ib5km.lincol.random[1:3,], 1, function(i) ib5km15.dbc[i[1],i[2],])

(untested)

-- Tony Plate

At Wednesday 06:47 PM 10/15/2003 +0200, Agustin Lobo wrote:

>Hi!
>
>I have a 3d array:
> > dim(ib5km15.dbc)
>[1] 190 241  19
>
>and a set of positions to extract:
> > ib5km.lincol.random[1:3,]
>      [,1] [,2]
>[1,]   78   70
>[2,]   29  213
>[3,]  180   22
>
>Geting the values of a 2D array
>for that set of positions would
>be:
>
> > ima <- ib5km15.dbc[,,1]
> > ima[ib5km.lincol.random[1:10,]]
>
>but don't find the way for the case
>of the 3D array:
>
> > ib5km15.dbc[ib5km.lincol.random[1:10,],]
>Error in ib5km15.dbc[ib5km.lincol.random[1:10, ], ] :
>         incorrect number of dimensions
>
>Could anyone suggest the way of subseting
>the 3D array to get a vector of z values
>for each position recorded in ib5km.lincol.random?
>(avoiding the use of for loops).
>
>Thanks
>
>Agus
>
>Dr. Agustin Lobo
>Instituto de Ciencias de la Tierra (CSIC)
>Lluis Sole Sabaris s/n
>08028 Barcelona SPAIN
>tel 34 93409 5410
>fax 34 93411 0012
>alobo at ija.csic.es
>
>______________________________________________
>R-help at stat.math.ethz.ch mailing list
>https://www.stat.math.ethz.ch/mailman/listinfo/r-help



From TyagiAnupam at aol.com  Wed Oct 15 19:10:32 2003
From: TyagiAnupam at aol.com (TyagiAnupam@aol.com)
Date: Wed, 15 Oct 2003 13:10:32 EDT
Subject: [R] help with aggregate.survey.design
Message-ID: <1d8.129179bf.2cbed988@aol.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20031015/06056e46/attachment.pl

From andy_liaw at merck.com  Wed Oct 15 19:17:25 2003
From: andy_liaw at merck.com (Liaw, Andy)
Date: Wed, 15 Oct 2003 13:17:25 -0400
Subject: [R] SOM library for R
Message-ID: <3A822319EB35174CA3714066D590DCD50205CCA4@usrymx25.merck.com>

Doing help.search("SOM") in R gives

batchSOM(class)          Self-Organizing Maps: Batch Algorithm
SOM(class)               Self-Organizing Maps: Online Algorithm
somgrid(class)           Plot SOM Fits

so you ought to have it in your (recent enough) version of R.  The "class"
package is part of the VR bundle, which is shipped with R.

Andy


> -----Original Message-----
> From: Hisaji Ono [mailto:hi_ono2001 at ybb.ne.jp] 
> Sent: Wednesday, October 15, 2003 12:52 PM
> To: r-help at stat.math.ethz.ch
> Subject: [R] SOM library for R
> 
> 
> Hi.
> 
>  Three years ago, I've read the question of availability of 
> SOM library for R using Kohonen's SOM_PAK in this 
> mailing-list. This answer was no availability. And no package 
> dealing with SOM in CRAN.
> 
>  Is this situation same?
> 
>  Could you tell me availability this library?
> 
> 
>  Best Regards.
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list 
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
>



From hec.villafuerte at telgua.com.gt  Wed Oct 15 21:20:26 2003
From: hec.villafuerte at telgua.com.gt (=?ISO-8859-1?Q?=22H=E9ctor_Villafuerte_D=2E=22?=)
Date: Wed, 15 Oct 2003 11:20:26 -0800
Subject: [R] Problems Building RMySQL in Windows
Message-ID: <3F8D9DFA.9030509@telgua.com.gt>

Hi all,
Unfortunately I must use winXP at my job, so I'm trying install
from source RMySQL. Here are some details about this problem:
- R 1.7.1
- RMySQL 0.5-2
- mysql  4.1.0-alpha-max-nt
I followed the instructions posted in 
http://www.biostat.jhsph.edu/~kbroman/Rintro/Rwinpack.html
and the following messages appeared. BTW, thanks in advance for your 
help and comments.


C:\Program Files\R\rw1071\src\gnuwin32>make pkg-RMySQL

---------- Making package RMySQL ------------

   **********************************************
   WARNING: this package has a configure script
         It probably needs manual configuration
   **********************************************

  installing inst files
  adding build stamp to DESCRIPTION
  making DLL ...
making RS-DBI.d from RS-DBI.c
making RS-MySQL.d from RS-MySQL.c
gcc  -Ic:/mysql/include  -IC:/PROGRA~1/R/rw1071/src/include -Wall -O2   
-c RS-DBI.c -o RS-DBI.o
gcc  -Ic:/mysql/include  -IC:/PROGRA~1/R/rw1071/src/include -Wall -O2   
-c RS-MySQL.c -o RS-MySQL.o
ar cr RMySQL.a *.o
ranlib RMySQL.a
windres --include-dir C:/PROGRA~1/R/rw1071/src/include  -i RMySQL_res.rc 
-o RMySQL_res.o
gcc  --shared -s  -o RMySQL.dll RMySQL.def RMySQL.a RMySQL_res.o  
-LC:/PROGRA~1/R/rw1071/src/gnuwin32 -Lc:/mysql/lib/opt
 -lmysql -liberty -lg2c -lR
RMySQL.a(RS-MySQL.o)(.text+0x216):RS-MySQL.c: undefined reference to 
`_mysql_get_client_info'
RMySQL.a(RS-MySQL.o)(.text+0x7f9):RS-MySQL.c: undefined reference to 
`_mysql_init'
RMySQL.a(RS-MySQL.o)(.text+0x817):RS-MySQL.c: undefined reference to 
`_mysql_options'
RMySQL.a(RS-MySQL.o)(.text+0x879):RS-MySQL.c: undefined reference to 
`_mysql_options'
RMySQL.a(RS-MySQL.o)(.text+0x8e4):RS-MySQL.c: undefined reference to 
`_load_defaults'
RMySQL.a(RS-MySQL.o)(.text+0xb39):RS-MySQL.c: undefined reference to 
`_mysql_real_connect'
RMySQL.a(RS-MySQL.o)(.text+0xc3d):RS-MySQL.c: undefined reference to 
`_mysql_close'
RMySQL.a(RS-MySQL.o)(.text+0xd31):RS-MySQL.c: undefined reference to 
`_mysql_options'
RMySQL.a(RS-MySQL.o)(.text+0xde1):RS-MySQL.c: undefined reference to 
`_mysql_close'
RMySQL.a(RS-MySQL.o)(.text+0xf89):RS-MySQL.c: undefined reference to 
`_mysql_query'
RMySQL.a(RS-MySQL.o)(.text+0xfa4):RS-MySQL.c: undefined reference to 
`_mysql_use_result'
RMySQL.a(RS-MySQL.o)(.text+0xfbe):RS-MySQL.c: undefined reference to 
`_mysql_field_count'
RMySQL.a(RS-MySQL.o)(.text+0x106a):RS-MySQL.c: undefined reference to 
`_mysql_affected_rows'
RMySQL.a(RS-MySQL.o)(.text+0x10de):RS-MySQL.c: undefined reference to 
`_mysql_error'
RMySQL.a(RS-MySQL.o)(.text+0x116f):RS-MySQL.c: undefined reference to 
`_mysql_fetch_fields'
RMySQL.a(RS-MySQL.o)(.text+0x1188):RS-MySQL.c: undefined reference to 
`_mysql_field_count'
RMySQL.a(RS-MySQL.o)(.text+0x155c):RS-MySQL.c: undefined reference to 
`_mysql_fetch_row'
RMySQL.a(RS-MySQL.o)(.text+0x1576):RS-MySQL.c: undefined reference to 
`_mysql_fetch_lengths'
RMySQL.a(RS-MySQL.o)(.text+0x17c3):RS-MySQL.c: undefined reference to 
`_mysql_errno'
RMySQL.a(RS-MySQL.o)(.text+0x19e4):RS-MySQL.c: undefined reference to 
`_mysql_errno'
RMySQL.a(RS-MySQL.o)(.text+0x19ef):RS-MySQL.c: undefined reference to 
`_mysql_error'
RMySQL.a(RS-MySQL.o)(.text+0x1a64):RS-MySQL.c: undefined reference to 
`_mysql_fetch_row'
RMySQL.a(RS-MySQL.o)(.text+0x1a74):RS-MySQL.c: undefined reference to 
`_mysql_free_result'
RMySQL.a(RS-MySQL.o)(.text+0x1d1b):RS-MySQL.c: undefined reference to 
`_mysql_get_client_info'
RMySQL.a(RS-MySQL.o)(.text+0x1f4e):RS-MySQL.c: undefined reference to 
`_mysql_get_host_info'
RMySQL.a(RS-MySQL.o)(.text+0x1f78):RS-MySQL.c: undefined reference to 
`_mysql_get_server_info'
RMySQL.a(RS-MySQL.o)(.text+0x1fa5):RS-MySQL.c: undefined reference to 
`_mysql_get_proto_info'
RMySQL.a(RS-MySQL.o)(.text+0x1fb6):RS-MySQL.c: undefined reference to 
`_mysql_thread_id'
RMySQL.a(RS-MySQL.o)(.text+0x2c32):RS-MySQL.c: undefined reference to 
`_mysql_fetch_lengths'
RMySQL.a(RS-MySQL.o)(.text+0x2c59):RS-MySQL.c: undefined reference to 
`_mysql_errno'
RMySQL.a(RS-MySQL.o)(.text+0x2eda):RS-MySQL.c: undefined reference to 
`_mysql_fetch_row'
collect2: ld returned 1 exit status
make[2]: *** [RMySQL.dll] Error 1
make[1]: *** [src/RMySQL.dll] Error 2
make: *** [pkg-RMySQL] Error 2

C:\Program Files\R\rw1071\src\gnuwin32>



From ripley at stats.ox.ac.uk  Wed Oct 15 19:30:06 2003
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Wed, 15 Oct 2003 18:30:06 +0100 (BST)
Subject: [R] SOM library for R
In-Reply-To: <031b01c3933c$b19b9470$818001db@webgis>
Message-ID: <Pine.LNX.4.44.0310151828070.1201-100000@gannet.stats>

On Thu, 16 Oct 2003, Hisaji Ono wrote:

> Hi.
> 
>  Three years ago, I've read the question of availability of SOM library for
> R using Kohonen's SOM_PAK in this mailing-list. This answer was no
> availability. And no package dealing with SOM in CRAN.

There are two SOM methods in package class (bundled with R) and a package
som (formerly GeneSOM) on CRAN.

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From ripley at stats.ox.ac.uk  Wed Oct 15 19:42:32 2003
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Wed, 15 Oct 2003 18:42:32 +0100 (BST)
Subject: [R] Subseting in a 3D array
In-Reply-To: <5.2.1.1.2.20031015110227.03712628@mailhost.blackmesacapital.com>
Message-ID: <Pine.LNX.4.44.0310151836120.1201-100000@gannet.stats>

That does use a for loop inside apply.

I would make use of dim shuffling: you have an array indexed by (x,y,z)
and you want (I presume) a matrix indexed by ((x,y), z) for specified
pairs (x,y).

X <- ib5km15.dbc
dim(X) <- c(190*241, 19)
X[ib5km.lincol.random %*% c(1, 240) - 240, ]

should be something close to what you want if I have read the runes 
correctly.


On Wed, 15 Oct 2003, Tony Plate wrote:

> One way would be:
> 
>  > apply(ib5km.lincol.random[1:3,], 1, function(i) ib5km15.dbc[i[1],i[2],])
> 
> (untested)
> 
> -- Tony Plate
> 
> At Wednesday 06:47 PM 10/15/2003 +0200, Agustin Lobo wrote:
> 
> >Hi!
> >
> >I have a 3d array:
> > > dim(ib5km15.dbc)
> >[1] 190 241  19
> >
> >and a set of positions to extract:
> > > ib5km.lincol.random[1:3,]
> >      [,1] [,2]
> >[1,]   78   70
> >[2,]   29  213
> >[3,]  180   22
> >
> >Geting the values of a 2D array
> >for that set of positions would
> >be:
> >
> > > ima <- ib5km15.dbc[,,1]
> > > ima[ib5km.lincol.random[1:10,]]
> >
> >but don't find the way for the case
> >of the 3D array:
> >
> > > ib5km15.dbc[ib5km.lincol.random[1:10,],]
> >Error in ib5km15.dbc[ib5km.lincol.random[1:10, ], ] :
> >         incorrect number of dimensions
> >
> >Could anyone suggest the way of subseting
> >the 3D array to get a vector of z values
> >for each position recorded in ib5km.lincol.random?
> >(avoiding the use of for loops).
> >
> >Thanks
> >
> >Agus
> >
> >Dr. Agustin Lobo
> >Instituto de Ciencias de la Tierra (CSIC)
> >Lluis Sole Sabaris s/n
> >08028 Barcelona SPAIN
> >tel 34 93409 5410
> >fax 34 93411 0012
> >alobo at ija.csic.es
> >
> >______________________________________________
> >R-help at stat.math.ethz.ch mailing list
> >https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> 
> 

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From ripley at stats.ox.ac.uk  Wed Oct 15 19:51:47 2003
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Wed, 15 Oct 2003 18:51:47 +0100 (BST)
Subject: [R] Problems Building RMySQL in Windows
In-Reply-To: <3F8D9DFA.9030509@telgua.com.gt>
Message-ID: <Pine.LNX.4.44.0310151843190.1201-100000@gannet.stats>

On Wed, 15 Oct 2003, "H?ctor Villafuerte D." wrote:

> Hi all,
> Unfortunately I must use winXP at my job, so I'm trying install
> from source RMySQL. Here are some details about this problem:
> - R 1.7.1
> - RMySQL 0.5-2
> - mysql  4.1.0-alpha-max-nt
> I followed the instructions posted in 
> http://www.biostat.jhsph.edu/~kbroman/Rintro/Rwinpack.html
> and the following messages appeared. BTW, thanks in advance for your 
> help and comments.

It hasn't found the mysql entry points: have you built an import
library for the mysql client and is it in c:/mysql/lib/opt, for example.

Error messages very similar to these are covered 
in the README.windows file in the RMySQL package.

Please read the documentation in the RMySQL package and in the current
R for Windows relase (that web page is long out of date and I believe was 
never as accurate as the official documentation).

> 
> 
> C:\Program Files\R\rw1071\src\gnuwin32>make pkg-RMySQL
> 
> ---------- Making package RMySQL ------------
> 
>    **********************************************
>    WARNING: this package has a configure script
>          It probably needs manual configuration
>    **********************************************
> 
>   installing inst files
>   adding build stamp to DESCRIPTION
>   making DLL ...
> making RS-DBI.d from RS-DBI.c
> making RS-MySQL.d from RS-MySQL.c
> gcc  -Ic:/mysql/include  -IC:/PROGRA~1/R/rw1071/src/include -Wall -O2   
> -c RS-DBI.c -o RS-DBI.o
> gcc  -Ic:/mysql/include  -IC:/PROGRA~1/R/rw1071/src/include -Wall -O2   
> -c RS-MySQL.c -o RS-MySQL.o
> ar cr RMySQL.a *.o
> ranlib RMySQL.a
> windres --include-dir C:/PROGRA~1/R/rw1071/src/include  -i RMySQL_res.rc 
> -o RMySQL_res.o
> gcc  --shared -s  -o RMySQL.dll RMySQL.def RMySQL.a RMySQL_res.o  
> -LC:/PROGRA~1/R/rw1071/src/gnuwin32 -Lc:/mysql/lib/opt
>  -lmysql -liberty -lg2c -lR
> RMySQL.a(RS-MySQL.o)(.text+0x216):RS-MySQL.c: undefined reference to 
> `_mysql_get_client_info'
> RMySQL.a(RS-MySQL.o)(.text+0x7f9):RS-MySQL.c: undefined reference to 
> `_mysql_init'
> RMySQL.a(RS-MySQL.o)(.text+0x817):RS-MySQL.c: undefined reference to 
> `_mysql_options'
> RMySQL.a(RS-MySQL.o)(.text+0x879):RS-MySQL.c: undefined reference to 
> `_mysql_options'
> RMySQL.a(RS-MySQL.o)(.text+0x8e4):RS-MySQL.c: undefined reference to 
> `_load_defaults'
> RMySQL.a(RS-MySQL.o)(.text+0xb39):RS-MySQL.c: undefined reference to 
> `_mysql_real_connect'
> RMySQL.a(RS-MySQL.o)(.text+0xc3d):RS-MySQL.c: undefined reference to 
> `_mysql_close'
> RMySQL.a(RS-MySQL.o)(.text+0xd31):RS-MySQL.c: undefined reference to 
> `_mysql_options'
> RMySQL.a(RS-MySQL.o)(.text+0xde1):RS-MySQL.c: undefined reference to 
> `_mysql_close'
> RMySQL.a(RS-MySQL.o)(.text+0xf89):RS-MySQL.c: undefined reference to 
> `_mysql_query'
> RMySQL.a(RS-MySQL.o)(.text+0xfa4):RS-MySQL.c: undefined reference to 
> `_mysql_use_result'
> RMySQL.a(RS-MySQL.o)(.text+0xfbe):RS-MySQL.c: undefined reference to 
> `_mysql_field_count'
> RMySQL.a(RS-MySQL.o)(.text+0x106a):RS-MySQL.c: undefined reference to 
> `_mysql_affected_rows'
> RMySQL.a(RS-MySQL.o)(.text+0x10de):RS-MySQL.c: undefined reference to 
> `_mysql_error'
> RMySQL.a(RS-MySQL.o)(.text+0x116f):RS-MySQL.c: undefined reference to 
> `_mysql_fetch_fields'
> RMySQL.a(RS-MySQL.o)(.text+0x1188):RS-MySQL.c: undefined reference to 
> `_mysql_field_count'
> RMySQL.a(RS-MySQL.o)(.text+0x155c):RS-MySQL.c: undefined reference to 
> `_mysql_fetch_row'
> RMySQL.a(RS-MySQL.o)(.text+0x1576):RS-MySQL.c: undefined reference to 
> `_mysql_fetch_lengths'
> RMySQL.a(RS-MySQL.o)(.text+0x17c3):RS-MySQL.c: undefined reference to 
> `_mysql_errno'
> RMySQL.a(RS-MySQL.o)(.text+0x19e4):RS-MySQL.c: undefined reference to 
> `_mysql_errno'
> RMySQL.a(RS-MySQL.o)(.text+0x19ef):RS-MySQL.c: undefined reference to 
> `_mysql_error'
> RMySQL.a(RS-MySQL.o)(.text+0x1a64):RS-MySQL.c: undefined reference to 
> `_mysql_fetch_row'
> RMySQL.a(RS-MySQL.o)(.text+0x1a74):RS-MySQL.c: undefined reference to 
> `_mysql_free_result'
> RMySQL.a(RS-MySQL.o)(.text+0x1d1b):RS-MySQL.c: undefined reference to 
> `_mysql_get_client_info'
> RMySQL.a(RS-MySQL.o)(.text+0x1f4e):RS-MySQL.c: undefined reference to 
> `_mysql_get_host_info'
> RMySQL.a(RS-MySQL.o)(.text+0x1f78):RS-MySQL.c: undefined reference to 
> `_mysql_get_server_info'
> RMySQL.a(RS-MySQL.o)(.text+0x1fa5):RS-MySQL.c: undefined reference to 
> `_mysql_get_proto_info'
> RMySQL.a(RS-MySQL.o)(.text+0x1fb6):RS-MySQL.c: undefined reference to 
> `_mysql_thread_id'
> RMySQL.a(RS-MySQL.o)(.text+0x2c32):RS-MySQL.c: undefined reference to 
> `_mysql_fetch_lengths'
> RMySQL.a(RS-MySQL.o)(.text+0x2c59):RS-MySQL.c: undefined reference to 
> `_mysql_errno'
> RMySQL.a(RS-MySQL.o)(.text+0x2eda):RS-MySQL.c: undefined reference to 
> `_mysql_fetch_row'
> collect2: ld returned 1 exit status
> make[2]: *** [RMySQL.dll] Error 1
> make[1]: *** [src/RMySQL.dll] Error 2
> make: *** [pkg-RMySQL] Error 2
> 
> C:\Program Files\R\rw1071\src\gnuwin32>
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> 
> 

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From kjetil at entelnet.bo  Wed Oct 15 20:12:55 2003
From: kjetil at entelnet.bo (kjetil@entelnet.bo)
Date: Wed, 15 Oct 2003 14:12:55 -0400
Subject: [R] aov and non-categorical variables
In-Reply-To: <20031015163212.74491.qmail@web14904.mail.yahoo.com>
Message-ID: <3F8D55E7.6349.32825B@localhost>

On 15 Oct 2003 at 9:32, Alexander Sirotkin [at Yahoo] wrote:

> It is unclear to me how aov() handles non-categorical
> variables.

aov is an interface to lm, so it can estimate every model lm
can, the difference is that it produces the results (summary)
in the classical way for anova.

> 
> I mean it works and produces results that I would
> expect, but I was under impression that ANOVA is only
> defined for categorical variables.
> 
> In addition, help(aov) says that it "call to 'lm' for
> each  stratum", which  I presume means that it calls
> to lm() for every group of the categorical variable, 

No. With anova you can also define "error strata" using 
Error() as part of the formula, lm() cannot do that. If you don't use 
Error() in the formula, lm() is called only once. 

Kjetil Halvorsen

> however I don't quite understand what this means for
> non-categorical variable.
> 
> Thanks
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help



From hec.villafuerte at telgua.com.gt  Wed Oct 15 22:16:04 2003
From: hec.villafuerte at telgua.com.gt (=?ISO-8859-1?Q?=22H=E9ctor_Villafuerte_D=2E=22?=)
Date: Wed, 15 Oct 2003 12:16:04 -0800
Subject: [R] Problems Building RMySQL in Windows
In-Reply-To: <Pine.LNX.4.44.0310151843190.1201-100000@gannet.stats>
References: <Pine.LNX.4.44.0310151843190.1201-100000@gannet.stats>
Message-ID: <3F8DAB04.9090404@telgua.com.gt>

Thanks Prof.,
I forgot to mention that I had already read README.windows.
So I downloaded reimp.exe and did:
        C:\mysql\lib\opt>reimp.exe libmysql.lib
And the output was:
        liblibmysql.a
which I renamed to: libmysql.a
And then I got the errors of my previous mail.

I've just found the "R Installation and Administration" manual (oops)
so I'll try to find some answers in there.
Thanks again.
Hector

Prof Brian Ripley wrote:

>It hasn't found the mysql entry points: have you built an import
>library for the mysql client and is it in c:/mysql/lib/opt, for example.
>
>Error messages very similar to these are covered 
>in the README.windows file in the RMySQL package.
>
>Please read the documentation in the RMySQL package and in the current
>R for Windows relase (that web page is long out of date and I believe was 
>never as accurate as the official documentation).
>
>  
>



From gerifalte28 at hotmail.com  Wed Oct 15 20:17:03 2003
From: gerifalte28 at hotmail.com (Francisco Vergara)
Date: Wed, 15 Oct 2003 18:17:03 +0000
Subject: [R] Example of cell means model
Message-ID: <Law14-F32Ky8VBDUfdf0000a788@hotmail.com>

Thanks a lot for your reply. This helps a lot!
Just to confirm, using lm this model will give me the mean yield value for 
each cell in the two way array.  Now if I want to obtain the mean
of group means (like a SS type III approach) using LME (since I have random 
effects in the model) how can I parametrize this?
I could definitivelly use xtabs in a two-way case but in my case I have 2 
other (continuous) covariates that are potential confounders in
the model so I need to keep them to obtain the corrected means.
I added a continuous variable (NewVar) to the dataset Newxmp11.07 and 
obtaned a model with the covariate (fm4) and another without it (fm3)

>Newxmp11.07<-fix(xmp11.07)
>str(Newxmp11.07)
`data.frame':   36 obs. of  4 variables:
$ Yield  : num  10.5 9.2 7.9 12.8 11.2 13.3 12.1 12.6 14 10.8 ...
$ Variety: Factor w/ 3 levels "1","2","3": 1 1 1 1 1 1 1 1 1 1 ...
$ Density: Factor w/ 4 levels "1","2","3","4": 1 1 1 2 2 2 3 3 3 4 ...
$ NewVar : num  10 9 7 12 11 11 12 11 15 16 ...

>fm3<-gls(Yield~-1+Variety + Density, xmp11.07)
>fm4<-gls(Yield~-1+Variety + Density+NewVar, Newxmp11.07)
>fm3
Coefficients:
Variety1  Variety2  Variety3  Density2  Density3  Density4
8.922222  9.797222 15.713889  2.911111  4.300000  2.433333

Degrees of freedom: 36 total; 30 residual
Residual standard error: 1.239243

>fm4
Coefficients:
   Variety1    Variety2    Variety3    Density2    Density3    Density4
8.75757265  9.70589316 15.43347009  2.88152564  4.32186752  2.40117521
     NewVar
0.01157692

Degrees of freedom: 36 total; 29 residual
Residual standard error: 1.252991

fm4 gives me the mean of the group means for all the varieties but 
apparently it gives me the treatment contrasts for the densities.  If I 
change the order of the factors in the model specification I get

>coef(fm5<-gls(Yield~-1+ Density+Variety+NewVar, Newxmp11.07))
   Density1    Density2    Density3    Density4    Variety2    Variety3
8.75757265 11.63909829 13.07944017 11.15874786  0.94832051  6.67589744
     NewVar
0.01157692

This, just like fm4 will include the original intercept value in Density 1 
which is not the actual density 1 mean.  What am I missing? I am sorry if 
these questions are very basic but I want to make sure that I understand 
what I am doing. I guess that this is the price that I am paying for having 
used in the past packages like SAS where you just ask for lsmeans and the 
software will give you a "black box" answer!

Best regards

Francisco


>From: Douglas Bates <bates at stat.wisc.edu>
>To: "Francisco Vergara" <gerifalte28 at hotmail.com>
>CC: r-help at r-project.org
>Subject: Re: [R] Example of cell means model
>Date: 15 Oct 2003 08:40:33 -0500
>
>This is an example from chapter 11 of the 6th edition of Devore's
>engineering statistics text.  It happens to be a balanced data set in
>two factors but the calculations will also work for unbalanced data.
>
>I create a factor called 'cell' from the text representation of the
>Variety level and the Density level using '/' as the separator
>character. The coefficients for the linear model 'Yield ~ cell - 1'
>are exactly the cell means.
>
> > library(Devore6)
> > data(xmp11.07)
> > str(xmp11.07)
>`data.frame':	36 obs. of  3 variables:
>  $ Yield  : num  10.5 9.2 7.9 12.8 11.2 13.3 12.1 12.6 14 10.8 ...
>  $ Variety: Factor w/ 3 levels "1","2","3": 1 1 1 1 1 1 1 1 1 1 ...
>  $ Density: Factor w/ 4 levels "1","2","3","4": 1 1 1 2 2 2 3 3 3 4 ...
> > xmp11.07$cell = with(xmp11.07, factor(paste(Variety, Density, sep = 
>'/')))
> > xtabs(~ Variety + Density, xmp11.07)
>        Density
>Variety 1 2 3 4
>       1 3 3 3 3
>       2 3 3 3 3
>       3 3 3 3 3
> > means = xtabs(Yield ~ Variety+Density, xmp11.07)/xtabs(~ Variety + 
>Density, xmp11.07)
> > means
>        Density
>Variety 1         2         3         4
>       1  9.200000 12.433333 12.900000 10.800000
>       2  8.933333 12.633333 14.500000 12.766667
>       3 16.300000 18.100000 19.933333 18.166667
> > coef(fm1 <- lm(Yield ~ cell - 1, xmp11.07))
>   cell1/1   cell1/2   cell1/3   cell1/4   cell2/1   cell2/2   cell2/3   
>cell2/4
>  9.200000 12.433333 12.900000 10.800000  8.933333 12.633333 14.500000 
>12.766667
>   cell3/1   cell3/2   cell3/3   cell3/4
>16.300000 18.100000 19.933333 18.166667
>
>The residual sum of squares for this model is the same as that for the
>model with crossed terms
>
> > deviance(fm1)
>[1] 38.04
> > deviance(fm2 <- lm(Yield ~ Variety * Density, xmp11.07))
>[1] 38.04
>
>but the coefficients are quite different because they represent a
>different parameterization.
>
> > coef(fm2)
>       (Intercept)          Variety2          Variety3          Density2
>        9.20000000       -0.26666667        7.10000000        3.23333333
>          Density3          Density4 Variety2:Density2 Variety3:Density2
>        3.70000000        1.60000000        0.46666667       -1.43333333
>Variety2:Density3 Variety3:Density3 Variety2:Density4 Variety3:Density4
>        1.86666667       -0.06666667        2.23333333        0.26666667
>
>I hope this answers your question.  Sorry for the delay in
>responding to you.
>
>"Francisco Vergara" <gerifalte28 at hotmail.com> writes:
>
> > Many thanks for your reply.  An example would be very helpful to make
> > sure that I understand correctly what you described.

_________________________________________________________________
Surf and talk on the phone at the same time with broadband Internet access. 
Get high-speed for as low as $29.95/month (depending on the local service 
providers in your area).  https://broadband.msn.com



From emb7 at st-andrews.ac.uk  Wed Oct 15 20:49:44 2003
From: emb7 at st-andrews.ac.uk (Martin Biuw)
Date: Wed, 15 Oct 2003 19:49:44 +0100
Subject: [R] (no subject)
Message-ID: <oprw3k86kp8xmvrg@gatty.st-and.ac.uk>

Hello,

Does anyone know of any Stochastic Dynamic Programming functions/packages 
for R?

Thanks!

Martin

 -- Martin Biuw
Sea Mammal Research Unit
Gatty Marine Laboratory, University of St Andrews
St Andrews, Fife KY16 8PA
Scotland
Ph: +44-(0)1334-462637
Fax: +44-(0)1334-462632
Web: http://smub.st.and.ac.uk



From paul.schwarz at oregonstate.edu  Wed Oct 15 21:09:11 2003
From: paul.schwarz at oregonstate.edu (Schwarz, Paul)
Date: Wed, 15 Oct 2003 12:09:11 -0700
Subject: [R] help with legend()
Message-ID: <8E46EB3BC001414AA6CDB57C5E551F8D12F209@thuja>



I am converting some S-PLUS scripts that I use for creating manuscript
figures to R so that I can take advantage of the plotmath capabilities.
In my S-PLUS scripts I like to use the key() function for adding legends
to plots, and I have a couple of questions regarding using the legend()
function in R.

1) is there a way to specify different colors for the legend vector of
text values?

2) is there a way to reverse the order of the legend items so that the
text values precede the symbols?


Thanks for your time and patience.

-Paul

Paul A. Schwarz, Ph.D.
Department of Forest Science
342 Richardson Hall
Oregon State University
Corvallis, Oregon 97331-5752
 
paul.schwarz at oregonstate.edu
 
(541) 737-8481 (office)
(541) 737-1393 (fax)



From Virgilio.Gomez at uv.es  Wed Oct 15 21:27:33 2003
From: Virgilio.Gomez at uv.es (Virgilio =?ISO-8859-1?Q?G=F3mez?= Rubio)
Date: Wed, 15 Oct 2003 21:27:33 +0200
Subject: [R] Windows binaries for DCluster updated
In-Reply-To: <200310151003.h9FA3Vq7002468@stat.math.ethz.ch>
References: <200310151003.h9FA3Vq7002468@stat.math.ethz.ch>
Message-ID: <1066246053.1281.4.camel@chomsky.estadi.uv.es>

Hi,

As Frank M. Howell noticed (and probably other users), the Windows 
binaries for DCluster I put in my web page are not working... I have
compiled the source code
again and know it does. Please, download it again, and sorry for the
inconvenience.

The URL is http://matheron.uv.es/~virgil/Rpackages/DCluster

DCluster is a package that implements some methods for the detection of
spatial clusters of diseases.

Best regards,

Virgilio



From jkawczak at math.uncc.edu  Wed Oct 15 22:11:02 2003
From: jkawczak at math.uncc.edu (Janusz Kawczak)
Date: Wed, 15 Oct 2003 16:11:02 -0400
Subject: [R] Solaris 2.9
Message-ID: <000001c39358$7535c890$33a80f98@jk12pc>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20031015/ec1448ea/attachment.pl

From hi_ono2001 at ybb.ne.jp  Wed Oct 15 22:15:47 2003
From: hi_ono2001 at ybb.ne.jp (Hisaji Ono)
Date: Thu, 16 Oct 2003 05:15:47 +0900
Subject: [R] Re: [R-sig-Geo] Windows binaries for DCluster updated
References: <200310151003.h9FA3Vq7002468@stat.math.ethz.ch>
	<1066246053.1281.4.camel@chomsky.estadi.uv.es>
Message-ID: <039901c39359$1f0bdf30$818001db@webgis>

Hi.

 DCluster is very nice tool, especially Stan Openshaw's GAM.

 DCluster's GAM's testing function is different from original GAM whose
testing function used Monte Carlo method.

 Could you tell me how I can execute original GAM using DCluster?

 And can I execute GAM/K using DCluster?

 Regards.

----- Original Message ----- 
From: "Virgilio G$BN(Bez Rubio" <Virgilio.Gomez at uv.es>
To: <r-sig-geo at stat.math.ethz.ch>; <r-help at stat.math.ethz.ch>
Sent: Thursday, October 16, 2003 4:27 AM
Subject: [R-sig-Geo] Windows binaries for DCluster updated


> Hi,
>
> As Frank M. Howell noticed (and probably other users), the Windows
> binaries for DCluster I put in my web page are not working... I have
> compiled the source code
> again and know it does. Please, download it again, and sorry for the
> inconvenience.
>
> The URL is http://matheron.uv.es/~virgil/Rpackages/DCluster
>
> DCluster is a package that implements some methods for the detection of
> spatial clusters of diseases.



From hec.villafuerte at telgua.com.gt  Thu Oct 16 00:17:41 2003
From: hec.villafuerte at telgua.com.gt (=?ISO-8859-1?Q?=22H=E9ctor_Villafuerte_D=2E=22?=)
Date: Wed, 15 Oct 2003 14:17:41 -0800
Subject: [R] Problems Building RMySQL in Windows
In-Reply-To: <20031015143438.A22585@jessie.research.bell-labs.com>
References: <3F8D9DFA.9030509@telgua.com.gt>
	<20031015143438.A22585@jessie.research.bell-labs.com>
Message-ID: <3F8DC785.10901@telgua.com.gt>

David James wrote:

>Hi,
>
>I'm attaching a Windows binary version of RMySQL.  It was compiled
>against MySQL 2.23.56 (recall that you must also install the DBI
>package). Let me know if it works.
>
>Hope this helps,
>
>--
>David
>

Thanks David,
I tried the binaries you sent me. Here's the warning it gave me:

    > library(RMySQL)
    Warning message:
    DLL attempted to change FPU control word from 8001f to 9001f

Then I tried to do this:

    > con <- dbConnect(dbDriver("MySQL"), dbname = "test")

but it crashed R.
I got back to the sources then (but now using RCMD.EXE)... but again
it seems I'm missing something important:

    C:\Program Files\R\rw1071\bin>rcmd INSTALL ..\src\library\RMySQL

    make: *** No rule to make target `Files/R/rw1071/src/library'.  Stop.
    *** Installation of RMySQL failed ***

    C:\Program Files\R\rw1071\bin>

Thank you so much for your help.



From ramasamya at gis.a-star.edu.sg  Wed Oct 15 22:22:22 2003
From: ramasamya at gis.a-star.edu.sg (Adaikalavan RAMASAMY)
Date: Thu, 16 Oct 2003 04:22:22 +0800
Subject: [R] Solaris 2.9
Message-ID: <6D9E9B9DF347EF4385F6271C64FB8D56075FD0@BIONIC.biopolis.one-north.com>

What was the command that caused the problem (reproducible example please).
 
What was the error message ?
 
Try the same command with R 1.7.1 and see if you get the same error.

	-----Original Message----- 
	From: Janusz Kawczak [mailto:jkawczak at math.uncc.edu] 
	Sent: Thu 10/16/2003 4:11 AM 
	To: R-help at stat.math.ethz.ch 
	Cc: 
	Subject: [R] Solaris 2.9
	
	

	I've compiled the 1.8.0 R on Solaris 2.9 and when trying to use
	plot command I get the Bus error and a core dump. Anybody experienced
	something like this? If yes, what should I patch?
	Janusz.
	
	
	        [[alternative HTML version deleted]]
	
	______________________________________________
	R-help at stat.math.ethz.ch mailing list
	https://www.stat.math.ethz.ch/mailman/listinfo/r-help



From Virgilio.Gomez at uv.es  Wed Oct 15 22:49:47 2003
From: Virgilio.Gomez at uv.es (Virgilio =?ISO-8859-1?Q?G=F3mez?= Rubio)
Date: Wed, 15 Oct 2003 22:49:47 +0200
Subject: [R] Re: [R-sig-Geo] Windows binaries for DCluster updated
In-Reply-To: <039901c39359$1f0bdf30$818001db@webgis>
References: <200310151003.h9FA3Vq7002468@stat.math.ethz.ch>
	<1066246053.1281.4.camel@chomsky.estadi.uv.es>
	<039901c39359$1f0bdf30$818001db@webgis>
Message-ID: <1066250987.1286.32.camel@chomsky.estadi.uv.es>

Hi,


>  DCluster is very nice tool, especially Stan Openshaw's GAM.

Thank you!

> 
>  DCluster's GAM's testing function is different from original GAM whose
> testing function used Monte Carlo method.
> 
>  Could you tell me how I can execute original GAM using DCluster?

Well, I think you can write your own function to do that and pass it to
opgam as argument 'iscluster'. The default value is
'opgam.iscluster.default':

opgam.iscluster.default<-function (data, idx, idxorder, alpha, ...)
{
    localO <- sum(data$Observed[idx])
    localE <- sum(data$Expected[idx])
    if (localE == 0)
        return(c(localO, NA, NA))
    localP <- sum(data$Population[idx])
    pvalue <- ppois(localO, localE, lower.tail = FALSE)
    return(c(localO, alpha > pvalue, pvalue))
}

What you can do is to change the line where the pvalue is calculated
and, instead of using 'ppois' use a function that performs a Monte Carlo
test. Probably you will not need localR nor localP if you use Monte
Carlo.

So, the function could be simething like:

montecarlo.iscluster<-function(data, idx, idxorder, alpha, ...)
{
	localO <- sum(data$Observed[idx])
	pvalue <- return_pvalue_from_monte_carlo_simulations(data, localO) 
	return(c(localO, alpha > pvalue, pvalue))
}

where 'return_pvalue_from_monte_carlo_simulations' performs the M.C.
simulations and compare localO with the values obtained to compute
the pvalue.

I hope it's clear... if not, just tell me. :D

>  And can I execute GAM/K using DCluster?

Not at the moment, but 'opgam' returns the coordinates of the centres of
the balls that were significant, so I guess that you can use other R
packages to calculate a density from this table.

I have also written some code (not released yet) to calculate how many
times a centroid is included in these circles, in case you are
interested. I know it's not GAM/K but it can be useful for some
purposes...

Best regards,

-- 
             Virgilio G?mez Rubio

Grup d'Estad?stica espacial i temporal 
en Epidemiologia i medi ambient 

Dpto. Estad?stica e I. O. - Facultat de Matem?tiques
Avda. Vicent A. Estell?s, 1 - 46100 Burjassot
Valencia - SPAIN

http://matheron.uv.es/~virgil

TLF: 00 34 96 354 43 62 - FAX: 00 34 96 354 47 35



From pgilbert at bank-banque-canada.ca  Wed Oct 15 23:05:38 2003
From: pgilbert at bank-banque-canada.ca (Paul Gilbert)
Date: Wed, 15 Oct 2003 17:05:38 -0400
Subject: [R] Solaris 2.9
In-Reply-To: <000001c39358$7535c890$33a80f98@jk12pc>
References: <000001c39358$7535c890$33a80f98@jk12pc>
Message-ID: <3F8DB6A2.60703@bankofcanada.ca>

I've experienced that problem on Solaris 2.8 with gcc prior to gcc 3.2.3.

Paul Gilbert

Janusz Kawczak wrote:

>I've compiled the 1.8.0 R on Solaris 2.9 and when trying to use
>plot command I get the Bus error and a core dump. Anybody experienced
>something like this? If yes, what should I patch?
>Janusz. 
> 
>
>	[[alternative HTML version deleted]]
>
>______________________________________________
>R-help at stat.math.ethz.ch mailing list
>https://www.stat.math.ethz.ch/mailman/listinfo/r-help
>
>  
>



From Tokuyasu at cc.ucsf.edu  Wed Oct 15 23:35:36 2003
From: Tokuyasu at cc.ucsf.edu (Tokuyasu, Taku)
Date: Wed, 15 Oct 2003 14:35:36 -0700
Subject: [R] nnet:  Too many weights?
Message-ID: <112CB72C222BD2118FFF00A0C9D18C6404C1B358@cc.ucsf.edu>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20031015/c1ece98e/attachment.pl

From alex_s_42 at yahoo.com  Thu Oct 16 00:53:29 2003
From: alex_s_42 at yahoo.com (Alexander Sirotkin [at Yahoo])
Date: Wed, 15 Oct 2003 15:53:29 -0700 (PDT)
Subject: [R] aov and non-categorical variables
In-Reply-To: <3F8D55E7.6349.32825B@localhost>
Message-ID: <20031015225329.64186.qmail@web14907.mail.yahoo.com>

Thanks. One more question, if you don't mind.

If  instead of aov(), I call lm() directly it fits a
linear regression model and if it encounters
categorical variable it does what needs to be done in
this case - defines a new indicator variable for each
level of categorical var.

However, if I call aov() with the same data
(categorical and numeric) I don't see all these
indicator variables in the ANOVA table. It is unclear
to me how the ANOVA table with lots of inidcator
variables produced by lm() is transferred into the
ANOVA table of aov().

Also, after you mention the Error() term in aov() I
tried to find some explaination about it in R manuals,
and did not find any. Do you know where the meaning of
Error() in aov() is documented ?

Thanks.

--- kjetil at entelnet.bo wrote:
> On 15 Oct 2003 at 9:32, Alexander Sirotkin [at
> Yahoo] wrote:
> 
> > It is unclear to me how aov() handles
> non-categorical
> > variables.
> 
> aov is an interface to lm, so it can estimate every
> model lm
> can, the difference is that it produces the results
> (summary)
> in the classical way for anova.
> 
> > 
> > I mean it works and produces results that I would
> > expect, but I was under impression that ANOVA is
> only
> > defined for categorical variables.
> > 
> > In addition, help(aov) says that it "call to 'lm'
> for
> > each  stratum", which  I presume means that it
> calls
> > to lm() for every group of the categorical
> variable, 
> 
> No. With anova you can also define "error strata"
> using 
> Error() as part of the formula, lm() cannot do that.
> If you don't use 
> Error() in the formula, lm() is called only once. 
> 
> Kjetil Halvorsen
> 
> > however I don't quite understand what this means
> for
> > non-categorical variable.
> > 
> > Thanks
> > 
> > ______________________________________________
> > R-help at stat.math.ethz.ch mailing list
> >
>
https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> 
>



From kjetil at entelnet.bo  Thu Oct 16 01:03:41 2003
From: kjetil at entelnet.bo (kjetil@entelnet.bo)
Date: Wed, 15 Oct 2003 19:03:41 -0400
Subject: [R] nnet:  Too many weights?
In-Reply-To: <112CB72C222BD2118FFF00A0C9D18C6404C1B358@cc.ucsf.edu>
Message-ID: <3F8D9A0D.10440.852534@localhost>

On 15 Oct 2003 at 14:35, Tokuyasu, Taku wrote:

There is an argument to nnet setting the maximum number of weights. 
Default is 1000. I have successfully used this. 
Try
?nnet and read carefully!

Kjetil Halvorsen

> I am using library(nnet) to train up an ANN with what I believe is a
> moderately sized dataset, but R is complaining about too many weights:
> 
> ---
> > nn.1 <- nnet(t(data), targets, size = 4, rang = 0.1, decay = 5e-4, maxit =
> 200)
> Error in nnet.default(t(data), targets, size = 4, rang = 0.1, decay = 5e-04,
> : 
>         Too many (1614) weights
> > dim(targets)
> [1] 146   2
> > dim(data)  ## Note I'm using the transpose as input
> [1] 400 146
> ---
> 
> Is there a way around this?  Pointers to relevant docs/code or the source of
> the problem would be greatly appreciated.
> 
> Thanks,
> 
> _Taku
> 
> ---
> Taku A. Tokuyasu, PhD
> UCSF Cancer Center, Box 0128
> San Francisco, CA 94143-0128
> Tel: (415) 514-1530 Fax: (415) 502-3179
> Email: tokuyasu at cc.ucsf.edu
> 
> 
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help



From hec.villafuerte at telgua.com.gt  Thu Oct 16 03:19:56 2003
From: hec.villafuerte at telgua.com.gt (=?ISO-8859-1?Q?=22H=E9ctor_Villafuerte_D=2E=22?=)
Date: Wed, 15 Oct 2003 17:19:56 -0800
Subject: [R] Problems Building RMySQL in Windows
Message-ID: <3F8DF23C.3070403@telgua.com.gt>

David James wrote:

>"H?ctor Villafuerte D." wrote:
>  
>
>>Thanks David,
>>I tried the binaries you sent me. Here's the warning it gave me:
>>
>>    > library(RMySQL)
>>    Warning message:
>>    DLL attempted to change FPU control word from 8001f to 9001f
>>    
>>
>
>I have seen this, but it has not caused any problem to me, so my
>reaction is that it may be ok to ignore.
>
>  
>
>>Then I tried to do this:
>>
>>    > con <- dbConnect(dbDriver("MySQL"), dbname = "test")
>>
>>but it crashed R.
>>    
>>
>
>I've seen this too.  Typically it is a *.dll conflict between the
>version of the MySQL library included and used for compiling RMySQL
>(MySQL 3.23.56) and the runtime MySQL library you use (say, 4.0.x).
>If this is the problem, you can easily fix it by making sure your
>PATH variable picks the *.dll used in the RMySQL package first
>(ahead of the 4.0.x *.dll).
>
>Hope this helps,
>
>--
>David
>  
>

Here is a  copy of my PATH environment variable:
C:\Perl\bin\;%SystemRoot%\system32;%SystemRoot%;%SystemRoot%\System32\Wbem;
"C:\Program Files\NMapWin\bin";c:\cygwin\bin;C:\Python23;
"C:\Program Files\R\rw1071\library\RMySQL\libs";C:\mysql\bin

And yes... running dbConnect still crashes R...
Thanks again for your help.



From canty at math.mcmaster.ca  Thu Oct 16 01:50:47 2003
From: canty at math.mcmaster.ca (Angelo Canty)
Date: Wed, 15 Oct 2003 19:50:47 -0400
Subject: [R] Strange scope problem
Message-ID: <3F8DDD57.A74B9FEC@icarus.math.mcmaster.ca>

Hi,

I have come across the following problem which seems to be a scoping
issue but I'm at a loss to see why this is so or to find a good
workaround.

Suppose I have a function to get a prediction after model selection
using the step function.

step.pred <- function(dat, x0) {
  fit.model <- step(lm(y~., data=dat), trace=F)
  predict(fit.model, x0, se.fit=T)
}

This function works sometimes for example

set.seed(1)
X.1 <- data.frame(x1=rnorm(20), x2=rnorm(20), x3=rnorm(20))
y.1 <- 5+as.matrix(X.1[,1:2])%*%matrix(c(1,1))+rnorm(20)
Xy.1 <- data.frame(X.1,y=y.1)
x0.1 <- data.frame(x1=-1,x2=-1, x3=-1)
step.pred(Xy.1, x0.1)

$fit
[1] 3.359540

$se.fit
[1] 0.523629

$df
[1] 16

$residual.scale
[1] 1.093526

but most often it crashes as in

set.seed(2)
X.2 <- data.frame(x1=rnorm(20), x2=rnorm(20), x3=rnorm(20))
y.2 <- 5+as.matrix(X.2[,1:2])%*%matrix(c(1,1))+rnorm(20)
Xy.2 <- data.frame(X.2,y=y.2)
x0.2 <- data.frame(x1=-1,x2=-1, x3=-1)
step.pred(Xy.2, x0.2)
Error in model.frame.default(formula = y ~ x1 + x2, data = dat,
drop.unused.levels = TRUE) : 
        Object "dat" not found

The difference seems to be that for the first dataset, step retains
all three variables whereas for the second it drops one of them.

> step(lm(y~.,data=Xy.1), trace=F)

Call:
lm(formula = y ~ x1 + x2 + x3, data = Xy.1)

Coefficients:
(Intercept)           x1           x2           x3  
     4.8347       0.8937       1.0331      -0.4516  

> step(lm(y~.,data=Xy.2), trace=F)

Call:
lm(formula = y ~ x1 + x2, data = Xy.2)

Coefficients:
(Intercept)           x1           x2  
     5.0802       0.9763       1.1369  


One possible workaround is to explicitely assign the local variable
dat in the .GlobalEnv as in

step.pred1 <- function(dat, x0) {
  assign("dat",dat, envir=.GlobalEnv)
  fit.model <- step(lm(y~., data=dat), trace=F)
  predict(fit.model, x0, se.fit=T)
}

I don't like this method since it would overwrite anything else called
dat in .GlobalEnv.  I realize that I could give it an obscure name but
the potential for damage still remains.  Am I missing something obvious
here?  If not, is it possible to work around this problem in such a way
that .GlobalEnv does not need to be touched?

In S-Plus I would use 
assign("dat",dat, frame=1)
which works but that is not available (AFAIK) in R.  Is there
something similar that I can use?

I am using R 1.6.1 for Unix on a Sun Workstation. I know that I need
to upgrade but our sysadmin doesn't regard it as priority!  

Thanks for any help you can give for this.
Angelo

------------------------------------------------------------------
|   Angelo J. Canty                Email: cantya at mcmaster.ca     |
|   Mathematics and Statistics     Phone: (905) 525-9140 x 27079 |
|   McMaster University            Fax  : (905) 522-0935         |
|   1280 Main St. W.                                             |
|   Hamilton ON L8S 4K1                                          |



From andy_liaw at merck.com  Thu Oct 16 03:37:59 2003
From: andy_liaw at merck.com (Liaw, Andy)
Date: Wed, 15 Oct 2003 21:37:59 -0400
Subject: [R] aov and non-categorical variables
Message-ID: <3A822319EB35174CA3714066D590DCD50205CCAE@usrymx25.merck.com>

> From: Alexander Sirotkin [at Yahoo] [mailto:alex_s_42 at yahoo.com] 
> 
> Thanks. One more question, if you don't mind.
> 
> If  instead of aov(), I call lm() directly it fits a
> linear regression model and if it encounters
> categorical variable it does what needs to be done in
> this case - defines a new indicator variable for each
> level of categorical var.

What ANOVA table are you talking about; i.e., from which function?  There is
no anova() method for aov objects, so you will see identical result for
anova(fit) whether `fit' is fitted by direct call to lm() or aov().  The
difference is the the output of summary.  For an aov object, summary() just
prints the ANOVA table, which gives the same answer as anova().  For an lm
object, summary() prints the coefficients, se's and the associated t-tests
for each term (or contrasts, for categorical variables).  That's not ANOVA
table.

BTW (for the developers), the labels for the terms shown below (output of
summary.lm) look rather confusing:

> summary(fit2)

Call:
lm(formula = y ~ x + x2)

Residuals:
    Min      1Q  Median      3Q     Max 
-1.2779 -0.7437  0.3228  0.7196  0.8628 

Coefficients:
            Estimate Std. Error t value Pr(>|t|)  
(Intercept)   0.8287     0.4990   1.661   0.1353  
x2           -0.6625     0.6817  -0.972   0.3596  
x3           -1.6203     0.6843  -2.368   0.0454 *
x2            0.5110     0.2150   2.377   0.0448 *
---
Signif. codes:  0 `***' 0.001 `**' 0.01 `*' 0.05 `.' 0.1 ` ' 1 

Residual standard error: 0.9604 on 8 degrees of freedom
Multiple R-Squared: 0.5587,     Adjusted R-squared: 0.3933 
F-statistic: 3.377 on 3 and 8 DF,  p-value: 0.07493 

Notice `x2' appear twice!  Is there a better way to label the contrasts so
as to avoid this confusion?

Andy

 
> However, if I call aov() with the same data
> (categorical and numeric) I don't see all these
> indicator variables in the ANOVA table. It is unclear
> to me how the ANOVA table with lots of inidcator
> variables produced by lm() is transferred into the
> ANOVA table of aov().
> 
> Also, after you mention the Error() term in aov() I
> tried to find some explaination about it in R manuals,
> and did not find any. Do you know where the meaning of
> Error() in aov() is documented ?
> 
> Thanks.
> 
> --- kjetil at entelnet.bo wrote:
> > On 15 Oct 2003 at 9:32, Alexander Sirotkin [at
> > Yahoo] wrote:
> > 
> > > It is unclear to me how aov() handles
> > non-categorical
> > > variables.
> > 
> > aov is an interface to lm, so it can estimate every
> > model lm
> > can, the difference is that it produces the results
> > (summary)
> > in the classical way for anova.
> > 
> > > 
> > > I mean it works and produces results that I would
> > > expect, but I was under impression that ANOVA is
> > only
> > > defined for categorical variables.
> > > 
> > > In addition, help(aov) says that it "call to 'lm'
> > for
> > > each  stratum", which  I presume means that it
> > calls
> > > to lm() for every group of the categorical
> > variable,
> > 
> > No. With anova you can also define "error strata"
> > using
> > Error() as part of the formula, lm() cannot do that.
> > If you don't use 
> > Error() in the formula, lm() is called only once. 
> > 
> > Kjetil Halvorsen
> > 
> > > however I don't quite understand what this means
> > for
> > > non-categorical variable.
> > > 
> > > Thanks
> > > 
> > > ______________________________________________
> > > R-help at stat.math.ethz.ch mailing list
> > >
> >
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> > 
> >
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list 
> https://www.stat.math.ethz.ch/mailman/listinfo> /r-help
>



From tlumley at u.washington.edu  Thu Oct 16 03:38:24 2003
From: tlumley at u.washington.edu (Thomas Lumley)
Date: Wed, 15 Oct 2003 18:38:24 -0700 (PDT)
Subject: [R] help with aggregate.survey.design
In-Reply-To: <1d8.129179bf.2cbed988@aol.com>
References: <1d8.129179bf.2cbed988@aol.com>
Message-ID: <Pine.A41.4.58.0310151836520.98214@homer04.u.washington.edu>

On Wed, 15 Oct 2003 TyagiAnupam at aol.com wrote:

> I am trying to modify aggregate.data.frame to create an aggregate method for
> survey design objects.  I am running into problems because survey design
> objects are lists, with the variables and other design information stored in
> separate dataframes, or objects of other classes, in this list. *Apply and split
> functions do not seem to work on the design objects. How do I approach this,
> without having to rewrite *apply and split (and what would that involve?)?  I
> thought of creating lots of subsets, using "subset", but that does not seem to be
> a good approach.
>

I think the right approach is to extract the `variables' component of the
survey.design, use aggregate on it, and then work out how to attach the
right design metadata (weights, clusters, &c) to it.

This is not completely trivial, or I would have done it already.

	-thomas



From nortonsm at verizon.net  Thu Oct 16 05:58:18 2003
From: nortonsm at verizon.net (Scott Norton)
Date: Wed, 15 Oct 2003 23:58:18 -0400
Subject: [R] indexing a particular element in a list of vectors
Message-ID: <002101c39399$bc5bfb90$6501a8c0@scott>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20031015/564f49c4/attachment.pl

From Simon.Blomberg at anu.edu.au  Thu Oct 16 06:38:02 2003
From: Simon.Blomberg at anu.edu.au (Simon Blomberg)
Date: Thu, 16 Oct 2003 14:38:02 +1000
Subject: [R] indexing a particular element in a list of vectors
Message-ID: <7A3A13F416B40842BD2C1753E044B35901228103@CASEVS02.cas.anu.edu.au>

How about:

lst <- list(c("a", "b", "c"), c("d", "e", "f"))

> sapply(lst, function (x) x[1])
[1] "a" "d"

Cheers,

Simon.

Simon Blomberg, PhD
Depression & Anxiety Consumer Research Unit
Centre for Mental Health Research
Australian National University
http://www.anu.edu.au/cmhr/
Simon.Blomberg at anu.edu.au  +61 (2) 6125 3379


> -----Original Message-----
> From: Scott Norton [mailto:nortonsm at verizon.net]
> Sent: Thursday, 16 October 2003 1:58 PM
> To: r-help at stat.math.ethz.ch
> Subject: [R] indexing a particular element in a list of vectors
> 
> 
> I have a "list" of character vectors.  I'm trying to see if 
> there is a way
> (in a single line, without a loop) to pull out the first 
> element of all the
> vectors contained in the list.
> 
>  
> 
> listOfVectors[1:length(listOfVectors][1]
> 
>  
> 
> doesn't work.
> 
>  
> 
> ==========================
> 
> If you want more details..
> 
> Here is my listOfVectors which is called "uuu"
> 
> > uuu[order(uuu)]
> 
> [[1]]
> 
> [1] "pt1pg"      "multi.expr"
> 
>  
> 
> [[2]]
> 
> [1] "1ng"        "ml"         "fluor.expr"
> 
>  
> 
> [[3]]
> 
> [1] "1pg"        "ml"         "fluor.expr"
> 
>  
> 
> [[4]]
> 
> [1] "10ng"       "ml"         "fluor.expr"
> 
>  
> 
> [[5]]
> 
> [1] "10pg"       "ml"         "fluor.expr"
> 
>  
> 
> I'm basically interested in getting the following elements:
> 
> "pt1pg", "1ng", "1pg", etc. as a list (or character vector)
> 
>  
> 
> Note, this might have an obvious solution but I claim Newbie status!
> 
>  
> 
> Thanks in advance!
> 
> -Scott
> 
>  
> 
>  
> 
> Scott Norton, Ph.D.
> 
> Engineering Manager
> 
> Nanoplex Technologies, Inc.
> 
> 2375 Garcia Ave.
> 
> Mountain View, CA 94043
> 
> www.nanoplextech.com
> 
>  
> 
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
>



From ozric at web.de  Thu Oct 16 07:22:20 2003
From: ozric at web.de (Christian Schulz)
Date: Thu, 16 Oct 2003 07:22:20 +0200
Subject: [R] Problems Building RMySQL in Windows
References: <3F8DF23C.3070403@telgua.com.gt>
Message-ID: <003501c393a5$79d9a720$4404ebd9@pc>

I get the same,

library(RMySQL)
    Warning message:
    DLL attempted to change FPU control word from 8001f to 9001f

...but until now this never occur in a crash and i develope
a complex direct-marketing-system what get's and put's dynamically
~ 100.000 cases and 50 ~ variables  with this tools!

I experiment something, how i get runing  mysql 4.1.0.alpha!

(1.)  Install with the reimport description and the different path settings
mysql4.0 and RMySQL.
Now, when this works it is possible to unzip the  mysql-4.1.0-alpha.zip and
overwrite
your Mysql4.0 installation.

(2.) Doing the remip libmysql.lib etc. direct for the 4.1.0-alpha occur for
me in problems!?

<MySQLConnection:(620,0)>
  User: root
  Host: localhost
  Dbname: test
  Connection type: localhost via TCP/IP
  MySQL server version:  4.1.0-alpha-max-nt
  MySQL client version:  4.0.15
  MySQL protocol version:  10
  MySQL server thread id:  5
  No resultSet available

regards,christian




----- Original Message -----
From: "H?ctor Villafuerte D." <hec.villafuerte at telgua.com.gt>
To: <r-help at stat.math.ethz.ch>
Sent: Thursday, October 16, 2003 3:19 AM
Subject: Re: [R] Problems Building RMySQL in Windows


David James wrote:

>"H?ctor Villafuerte D." wrote:
>
>
>>Thanks David,
>>I tried the binaries you sent me. Here's the warning it gave me:
>>
>>    > library(RMySQL)
>>    Warning message:
>>    DLL attempted to change FPU control word from 8001f to 9001f
>>
>>
>
>I have seen this, but it has not caused any problem to me, so my
>reaction is that it may be ok to ignore.
>
>
>
>>Then I tried to do this:
>>
>>    > con <- dbConnect(dbDriver("MySQL"), dbname = "test")
>>
>>but it crashed R.
>>
>>
>
>I've seen this too.  Typically it is a *.dll conflict between the
>version of the MySQL library included and used for compiling RMySQL
>(MySQL 3.23.56) and the runtime MySQL library you use (say, 4.0.x).
>If this is the problem, you can easily fix it by making sure your
>PATH variable picks the *.dll used in the RMySQL package first
>(ahead of the 4.0.x *.dll).
>
>Hope this helps,
>
>--
>David
>
>

Here is a  copy of my PATH environment variable:
C:\Perl\bin\;%SystemRoot%\system32;%SystemRoot%;%SystemRoot%\System32\Wbem;
"C:\Program Files\NMapWin\bin";c:\cygwin\bin;C:\Python23;
"C:\Program Files\R\rw1071\library\RMySQL\libs";C:\mysql\bin

And yes... running dbConnect still crashes R...
Thanks again for your help.

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://www.stat.math.ethz.ch/mailman/listinfo/r-help



From jlandgr1 at gwdg.de  Thu Oct 16 07:43:00 2003
From: jlandgr1 at gwdg.de (jobst landgrebe)
Date: Thu, 16 Oct 2003 07:43:00 +0200
Subject: [R] fit.contrast and groupedData-derived lme-objects: inheritance
	problem
Message-ID: <E1AA0ua-0004qA-QC@mailer.gwdg.de>

Dear List,

I have to use a groupedData-object as input for lme and run fit.contrast 
from gregmisc on it like this:

myData <- groupedData(y ~ 1 | group, subdata)

...               

lmeRes  <-      lme(y ~ dye + treatment, myData, random =
                                pdBlocked(list(~slide-1, ~animal-1), pdClass =
                                "pdIdent"),na.action=na.omit)         

myContrasts <- fit.contrast(lmeRes, "treatment", c(1,-1,0))

This works fine in interactive mode on a single dataset, but NOT within an
lapply over a list of groupedData-objects, where fit.contrast does not handle
the lme-class-object "lmeRes" properly: either it cannot inherit "myData"
anymore,
or (if I use a for-loop as a workaround) it computes the contrasts for
the first lme-object for all objects (which I find totally confusing).
Other functions working with the lme-object (line anova) work perfectly OK and
deal properly with the inheritance of groupedData via the lme-object.

So I guess there is some grouedData-related bug in fit.contrast.

How can I proceed?
1. Does anyone know how to extract contrastst from an lme-model avoiding
fit.contrast?
2. Can someone tell me how to hack fit.contrast to make it work properly with
groupedData-derived lme-objects?

Thanks in advance,

Jobst



Dr. Jobst Landgrebe
Universitaet Goettingen
Zentrum Biochemie
Humboldtallee 23
37073 Goettingen
Germany

tel: 0551/39-2316 oder -2223
fax: 0551/39-5960
web: http://www.microarrays.med.uni-goettingen.de
mail: jlandgr1 at gwdg.de



From ripley at stats.ox.ac.uk  Thu Oct 16 08:28:50 2003
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Thu, 16 Oct 2003 07:28:50 +0100 (BST)
Subject: [R] Solaris 2.9
In-Reply-To: <3F8DB6A2.60703@bankofcanada.ca>
Message-ID: <Pine.LNX.4.44.0310160727480.2066-100000@gannet.stats>

Specifically gcc 3.2.1 and 3.2.2: that _is_ described in the R-admin 
manual.

On Wed, 15 Oct 2003, Paul Gilbert wrote:

> I've experienced that problem on Solaris 2.8 with gcc prior to gcc 3.2.3.
> 
> Paul Gilbert
> 
> Janusz Kawczak wrote:
> 
> >I've compiled the 1.8.0 R on Solaris 2.9 and when trying to use
> >plot command I get the Bus error and a core dump. Anybody experienced
> >something like this? If yes, what should I patch?

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From adorio at math.upd.edu.ph  Thu Oct 16 17:11:33 2003
From: adorio at math.upd.edu.ph (Ernie Adorio)
Date: Thu, 16 Oct 2003 23:11:33 +0800
Subject: [R] How to flip image?
Message-ID: <200310162311.33472.adorio@math.upd.edu.ph>

I am displaying some image of a person's head stored as a matrix, using 

  image(Face, col=gray((0:255)/255))

but the image is flipped, the nose is above the eyes. How can I 
correct  this without  making changes to the matrix Face?
If not possible, is there any built-in R command to reverse the rows of a 
matrix?

Ernie Adorio
Math Department
U.P. Diliman



From temiz at deprem.gov.tr  Thu Oct 16 09:18:14 2003
From: temiz at deprem.gov.tr (temiz)
Date: Thu, 16 Oct 2003 10:18:14 +0300
Subject: [R] Interpolation of azimuth values
Message-ID: <3F8E4636.9060007@deprem.gov.tr>


Hello

I will make an interpolation of data which represents azimuth direction
( angle from north in clockwise direction) values.
But there is a problem.
Say, for instance, while 1 and 359 indicate somewhat same direction, 
interpolation puts  values
in the range from 1 to 359.  What can I do to solve the problem ?

Anything you offer ?


thanks in advance


Ahmet Temiz
General Directory of Disaster Affairs
Ankara TURKEY



______________________________________
Inflex - installed on mailserver for domain @deprem.gov.tr
Queries to: postmaster at deprem.gov.tr

______________________________________
The views and opinions expressed in this e-mail message are ...{{dropped}}



From hans.gardfjell at eg.umu.se  Thu Oct 16 09:19:57 2003
From: hans.gardfjell at eg.umu.se (Hans Gardfjell)
Date: Thu, 16 Oct 2003 09:19:57 +0200
Subject: Fw: [R] SIMCA algorithm implementation
Message-ID: <3F8E469D.3040309@eg.umu.se>

>/ Dear All
/>/ Is there a SIMCA (Soft Independent Modelling Class Analogy) implementation
/>/ on R or does anyone know if is it possible to replicate the SIMCA
/algorithm
>/ using existing R functions?
/>/
/>/ Thanks
/>/ Mike White
/>/
/
I haven't seen any implementation in R, but check out the 'chemiometrics' home page here at
Ume? University, http://www.acc.umu.se/%7Etnkjtg/chemometrics/publications.html

Hans Gardfjell
Dept of Ecology and Environmental science
Ume? University, Sweden



From Isabelle.ZABALZA-MEZGHANI at ifp.fr  Thu Oct 16 09:38:01 2003
From: Isabelle.ZABALZA-MEZGHANI at ifp.fr (ZABALZA-MEZGHANI Isabelle)
Date: Thu, 16 Oct 2003 09:38:01 +0200
Subject: [R] Managing memory on R
Message-ID: <488C02265C6AD611BF200002A542182F03F2A793@irnts22.ifp.fr>

Hi,

I have a problem with the memory size within R. I would like to know if
there is any may for getting back free memory during a R session. I've tried
with rm and gc but it still craches. I am working on windows R1.3.1.
I know I have big datasets, but during my study, some results are temporary,
and I would like to get back the memory allocated to this temporary object
as soon as I remove them. But, as I 've said the garbage collector seems to
have no effect on my memory management.

Please, I wait for help,

Regards,

Isabelle Zabalza-Mezghani



From ripley at stats.ox.ac.uk  Thu Oct 16 09:34:55 2003
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Thu, 16 Oct 2003 08:34:55 +0100 (BST)
Subject: [R] Interpolation of azimuth values
In-Reply-To: <3F8E4636.9060007@deprem.gov.tr>
Message-ID: <Pine.LNX.4.44.0310160833570.4263-100000@gannet.stats>

On Thu, 16 Oct 2003, temiz wrote:

> I will make an interpolation of data which represents azimuth direction
> ( angle from north in clockwise direction) values.
> But there is a problem.
> Say, for instance, while 1 and 359 indicate somewhat same direction, 
> interpolation puts  values
> in the range from 1 to 359.  What can I do to solve the problem ?
> 
> Anything you offer ?

The usual solution is to extend the data periodically, that is to put 
copies shifted by one rotation on each side.

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From kwan022 at stat.auckland.ac.nz  Thu Oct 16 09:48:47 2003
From: kwan022 at stat.auckland.ac.nz (Ko-Kang Kevin Wang)
Date: Thu, 16 Oct 2003 20:48:47 +1300 (NZDT)
Subject: [R] Managing memory on R
In-Reply-To: <488C02265C6AD611BF200002A542182F03F2A793@irnts22.ifp.fr>
Message-ID: <Pine.LNX.4.44.0310162048190.24452-100000@stat54.stat.auckland.ac.nz>

On Thu, 16 Oct 2003, ZABALZA-MEZGHANI Isabelle wrote:

> I have a problem with the memory size within R. I would like to know if
> there is any may for getting back free memory during a R session. I've tried
> with rm and gc but it still craches. I am working on windows R1.3.1.

Perhaps trying to upgrade your R version first?  R 1.3.1 is kind of old 
now..

-- 
Cheers,

Kevin

---------------------------------------------------------------
Ko-Kang Kevin Wang
Master of Science (MSc) Student
SLC Tutor and Lab Demonstrator
Department of Statistics
University of Auckland
New Zealand
Homepage: http://www.stat.auckland.ac.nz/~kwan022
Ph: 373-7599
    x88475 (City)
    x88480 (Tamaki)



From bhx2 at mevik.net  Thu Oct 16 09:57:10 2003
From: bhx2 at mevik.net (=?iso-8859-1?q?Bj=F8rn-Helge_Mevik?=)
Date: Thu, 16 Oct 2003 09:57:10 +0200
Subject: [R] How to flip image?
In-Reply-To: <200310162311.33472.adorio@math.upd.edu.ph> (Ernie Adorio's
	message of "Thu, 16 Oct 2003 23:11:33 +0800")
References: <200310162311.33472.adorio@math.upd.edu.ph>
Message-ID: <7oy8vl399l.fsf@foo.nemo-project.org>

Ernie Adorio <adorio at math.upd.edu.ph> writes:

> If not possible, is there any built-in R command to reverse the rows of a 
> matrix?

How about Face[nrow(Face):1, ] ?

-- 
Bj?rn-Helge Mevik



From d.orme at ic.ac.uk  Wed Oct 15 16:04:14 2003
From: d.orme at ic.ac.uk (David Orme)
Date: Wed, 15 Oct 2003 15:04:14 +0100
Subject: [R] RODBC on Mac OSX pointers?
Message-ID: <5EA00400-FFB2-11D7-9181-000393DC1748@ic.ac.uk>

Hi,

I'm trying to get to grips with accessing my postgresql tables in R 
using RODBC. I have Mac OS 10.2.8 (Darwin 6.8) installed and R 1.8.0 
installed from source and I have updated RODBC to 1.0-4.

On the ODBC manager end, I am using OpenLink software's iODBC 
Administrator and I've set up a User DSN called "PostGRES"  - odbctest 
suggests that this is functioning:

> [doibook:~] dorme% /usr/bin/odbctest
> iODBC Demonstration program
> This program shows an interactive SQL processor
>
> Enter ODBC connect string (? shows list): dsn=PostGRES
>
> Have a nice day.
> [doibook:~] dorme%

However, when I try to access the connection from R I get the following:

> > test <- odbcConnect(dsn="PostGRES", uid="dorme", pwd="secret", 
> case="postgresql")
> Warning messages:
> 1: [RODBC] ERROR: state IM002, code 0, message [unixODBC][Driver 
> Manager]Data so
> urce name not found, and no default driver specified
> 2: ODBC connection failed in: odbcDriverConnect(paste("DSN=", dsn, 
> ";UID=", uid,
>  ";PWD=", pwd,

Any suggestions? My suspicion is that RODBC requires unixODBC and 
doesn't know about iOBDC, but in which case I wouldn't expect to see a 
Driver Manager error code.

Thanks,
David


---------------------------------------
Dr. David Orme

Department of Biological Sciences
Imperial College London
Silwood Park Campus
Ascot, Berkshire SL5 7PY UK.

Tel: +44 (0)20 759 42358
Fax: +44 (0)20 759 42339
e-mail: d.orme at imperial.ac.uk



From P.Lemmens at nici.kun.nl  Thu Oct 16 10:46:28 2003
From: P.Lemmens at nici.kun.nl (Paul Lemmens)
Date: Thu, 16 Oct 2003 10:46:28 +0200
Subject: [R] Cbind warning message
Message-ID: <11811053.1066301188@[192.168.1.7]>

Hello!

I'm not grasping why cbind (in the code below) warns that

Warning message:
number of rows of result
        is not a multiple of vector length (arg 2) in: cbind(z, p)

when I do

sections <- function(length, parts)
{
	p <- 1:parts
	q <- length %/% parts
	z <- array(p, dim=c(parts,q))

	r <- length %% parts
	if ( r > 0 )
	{
		p[r+1:length(p)] <- NA
		z <- cbind(z,p)
	}

	z <- na.omit(as.vector(t(z)))
}

and then

sections(32,5) -> a

As I see it, rows in result are 5 and the vector length of p (which is 5) 
is a multiple of 5.

kind regards,
Paul


-- 
Paul Lemmens
NICI, University of Nijmegen              ASCII Ribbon Campaign /"\
Montessorilaan 3 (B.01.03)                    Against HTML Mail \ /
NL-6525 HR Nijmegen                                              X
The Netherlands                                                 / \
Phonenumber    +31-24-3612648
Fax            +31-24-3616066



From ligges at statistik.uni-dortmund.de  Thu Oct 16 10:52:58 2003
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Thu, 16 Oct 2003 10:52:58 +0200
Subject: [R] Strange scope problem
In-Reply-To: <3F8DDD57.A74B9FEC@icarus.math.mcmaster.ca>
References: <3F8DDD57.A74B9FEC@icarus.math.mcmaster.ca>
Message-ID: <3F8E5C6A.3090102@statistik.uni-dortmund.de>

Angelo Canty wrote:

> Hi,
> 
> I have come across the following problem which seems to be a scoping
> issue but I'm at a loss to see why this is so or to find a good
> workaround.
> 
> Suppose I have a function to get a prediction after model selection
> using the step function.
> 
> step.pred <- function(dat, x0) {
>   fit.model <- step(lm(y~., data=dat), trace=F)
>   predict(fit.model, x0, se.fit=T)
> }
> 
> This function works sometimes for example
> 
> set.seed(1)
> X.1 <- data.frame(x1=rnorm(20), x2=rnorm(20), x3=rnorm(20))
> y.1 <- 5+as.matrix(X.1[,1:2])%*%matrix(c(1,1))+rnorm(20)
> Xy.1 <- data.frame(X.1,y=y.1)
> x0.1 <- data.frame(x1=-1,x2=-1, x3=-1)
> step.pred(Xy.1, x0.1)
> 
> $fit
> [1] 3.359540
> 
> $se.fit
> [1] 0.523629
> 
> $df
> [1] 16
> 
> $residual.scale
> [1] 1.093526
> 
> but most often it crashes as in

It does not crash. Your outdated version reports an error. R-1.8.0 does 
not. So please upgrade!

Uwe Ligges


> set.seed(2)
> X.2 <- data.frame(x1=rnorm(20), x2=rnorm(20), x3=rnorm(20))
> y.2 <- 5+as.matrix(X.2[,1:2])%*%matrix(c(1,1))+rnorm(20)
> Xy.2 <- data.frame(X.2,y=y.2)
> x0.2 <- data.frame(x1=-1,x2=-1, x3=-1)
> step.pred(Xy.2, x0.2)
> Error in model.frame.default(formula = y ~ x1 + x2, data = dat,
> drop.unused.levels = TRUE) : 
>         Object "dat" not found
> 
> The difference seems to be that for the first dataset, step retains
> all three variables whereas for the second it drops one of them.
> 
> 
>>step(lm(y~.,data=Xy.1), trace=F)
> 
> 
> Call:
> lm(formula = y ~ x1 + x2 + x3, data = Xy.1)
> 
> Coefficients:
> (Intercept)           x1           x2           x3  
>      4.8347       0.8937       1.0331      -0.4516  
> 
> 
>>step(lm(y~.,data=Xy.2), trace=F)
> 
> 
> Call:
> lm(formula = y ~ x1 + x2, data = Xy.2)
> 
> Coefficients:
> (Intercept)           x1           x2  
>      5.0802       0.9763       1.1369  
> 
> 
> One possible workaround is to explicitely assign the local variable
> dat in the .GlobalEnv as in
> 
> step.pred1 <- function(dat, x0) {
>   assign("dat",dat, envir=.GlobalEnv)
>   fit.model <- step(lm(y~., data=dat), trace=F)
>   predict(fit.model, x0, se.fit=T)
> }
> 
> I don't like this method since it would overwrite anything else called
> dat in .GlobalEnv.  I realize that I could give it an obscure name but
> the potential for damage still remains.  Am I missing something obvious
> here?  If not, is it possible to work around this problem in such a way
> that .GlobalEnv does not need to be touched?
> 
> In S-Plus I would use 
> assign("dat",dat, frame=1)
> which works but that is not available (AFAIK) in R.  Is there
> something similar that I can use?
> 
> I am using R 1.6.1 for Unix on a Sun Workstation. I know that I need
> to upgrade but our sysadmin doesn't regard it as priority!  
 >
> Thanks for any help you can give for this.
> Angelo
> 
> ------------------------------------------------------------------
> |   Angelo J. Canty                Email: cantya at mcmaster.ca     |
> |   Mathematics and Statistics     Phone: (905) 525-9140 x 27079 |
> |   McMaster University            Fax  : (905) 522-0935         |
> |   1280 Main St. W.                                             |
> |   Hamilton ON L8S 4K1                                          |
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help



From p.dalgaard at biostat.ku.dk  Thu Oct 16 11:00:11 2003
From: p.dalgaard at biostat.ku.dk (Peter Dalgaard BSA)
Date: 16 Oct 2003 11:00:11 +0200
Subject: [R] Cbind warning message
In-Reply-To: <11811053.1066301188@[192.168.1.7]>
References: <11811053.1066301188@[192.168.1.7]>
Message-ID: <x2he29v9pg.fsf@biostat.ku.dk>

Paul Lemmens <P.Lemmens at nici.kun.nl> writes:

> Hello!
> 
> I'm not grasping why cbind (in the code below) warns that
> 
> Warning message:
> number of rows of result
>         is not a multiple of vector length (arg 2) in: cbind(z, p)
> 
> when I do
> 
> sections <- function(length, parts)
> {
> 	p <- 1:parts
> 	q <- length %/% parts
> 	z <- array(p, dim=c(parts,q))
> 
> 	r <- length %% parts
> 	if ( r > 0 )
> 	{
> 		p[r+1:length(p)] <- NA
> 		z <- cbind(z,p)
> 	}
> 
> 	z <- na.omit(as.vector(t(z)))
> }
> 
> and then
> 
> sections(32,5) -> a
> 
> As I see it, rows in result are 5 and the vector length of p (which is
> 5) is a multiple of 5.

Did you intend
   p[(r+1):length(p)] <- NA
?
-- 
   O__  ---- Peter Dalgaard             Blegdamsvej 3  
  c/ /'_ --- Dept. of Biostatistics     2200 Cph. N   
 (*) \(*) -- University of Copenhagen   Denmark      Ph: (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)             FAX: (+45) 35327907



From ripley at stats.ox.ac.uk  Thu Oct 16 10:58:51 2003
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Thu, 16 Oct 2003 09:58:51 +0100 (BST)
Subject: [R] RODBC on Mac OSX pointers?
In-Reply-To: <5EA00400-FFB2-11D7-9181-000393DC1748@ic.ac.uk>
Message-ID: <Pine.LNX.4.44.0310160953380.4650-100000@gannet.stats>

RODBC has been used with iODBC under Linux, so your suspicion is wrong 
(and contradicted by the RODBC documentation).

Nevertheless, your error message shows that your build is trying to use 
unixODBC, so I suspect you built it against the wrong ODBC driver manager.

On Wed, 15 Oct 2003, David Orme wrote:

> Hi,
> 
> I'm trying to get to grips with accessing my postgresql tables in R 
> using RODBC. I have Mac OS 10.2.8 (Darwin 6.8) installed and R 1.8.0 
> installed from source and I have updated RODBC to 1.0-4.
> 
> On the ODBC manager end, I am using OpenLink software's iODBC 
> Administrator and I've set up a User DSN called "PostGRES"  - odbctest 
> suggests that this is functioning:
> 
> > [doibook:~] dorme% /usr/bin/odbctest
> > iODBC Demonstration program
> > This program shows an interactive SQL processor
> >
> > Enter ODBC connect string (? shows list): dsn=PostGRES
> >
> > Have a nice day.
> > [doibook:~] dorme%
> 
> However, when I try to access the connection from R I get the following:
> 
> > > test <- odbcConnect(dsn="PostGRES", uid="dorme", pwd="secret", 
> > case="postgresql")
> > Warning messages:
> > 1: [RODBC] ERROR: state IM002, code 0, message [unixODBC][Driver 
> > Manager]Data so
> > urce name not found, and no default driver specified
> > 2: ODBC connection failed in: odbcDriverConnect(paste("DSN=", dsn, 
> > ";UID=", uid,
> >  ";PWD=", pwd,
> 
> Any suggestions? My suspicion is that RODBC requires unixODBC and 
> doesn't know about iOBDC, but in which case I wouldn't expect to see a 
> Driver Manager error code.
> 
> Thanks,
> David
> 
> 
> ---------------------------------------
> Dr. David Orme
> 
> Department of Biological Sciences
> Imperial College London
> Silwood Park Campus
> Ascot, Berkshire SL5 7PY UK.
> 
> Tel: +44 (0)20 759 42358
> Fax: +44 (0)20 759 42339
> e-mail: d.orme at imperial.ac.uk
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> 
> 

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From adorio at math.upd.edu.ph  Thu Oct 16 19:15:28 2003
From: adorio at math.upd.edu.ph (Ernie Adorio)
Date: Fri, 17 Oct 2003 01:15:28 +0800
Subject: [R] How to flip image?
In-Reply-To: <7oy8vl399l.fsf@foo.nemo-project.org>
References: <200310162311.33472.adorio@math.upd.edu.ph>
	<7oy8vl399l.fsf@foo.nemo-project.org>
Message-ID: <200310170115.28896.adorio@math.upd.edu.ph>

On Thursday 16 October 2003 03:57 pm, Bj?rn-Helge Mevik wrote:
> Ernie Adorio <adorio at math.upd.edu.ph> writes:
> > If not possible, is there any built-in R command to reverse the rows of a
> > matrix?
>
> How about Face[nrow(Face):1, ] ?

Many thanks!  Have to use Face[, ncol(Face):1] for normal view.

Ernie Adorio



From bhx2 at mevik.net  Thu Oct 16 11:14:13 2003
From: bhx2 at mevik.net (=?iso-8859-1?q?Bj=F8rn-Helge_Mevik?=)
Date: Thu, 16 Oct 2003 11:14:13 +0200
Subject: [R] princomp with more coloumns than rows: why not?
Message-ID: <7ou16935p6.fsf@foo.nemo-project.org>

As of R 1.7.0, princomp no longer accept matrices with more coloumns
than rows.  I'm curious:  Why was this decision made?

I work a lot with data where more coloumns than rows is more of a rule
than an exception (for instance spectroscopic data).  To me, princomp
have two advantages above prcomp: 1) It has a predict method, and 2)
it has a biplot method.

A biplot method shouldn't be too difficult to implement (I believe
I've seen one on R-help).

A predict method seems to be more difficult, because the prcomp object
doesn't include the means that need to be subtracted from the new
data.  Would it break conformance with S to let prcomp return the
means as well?

-- 
Sincerely,
Bj?rn-Helge Mevik



From hb at maths.lth.se  Thu Oct 16 11:26:34 2003
From: hb at maths.lth.se (Henrik Bengtsson)
Date: Thu, 16 Oct 2003 11:26:34 +0200
Subject: [R] How to flip image?
In-Reply-To: <7oy8vl399l.fsf@foo.nemo-project.org>
Message-ID: <002201c393c7$9a01a580$e502eb82@maths.lth.se>

Yes, you need to follow Bj?rn-Helge advice and manipulate the matrix
that is given to image. As this creates a copy of your first matrix,
this may be expensive if your matrix is large. I do not know of another
way. Unfortunately you can *not* use the argument 'x' and 'y' of image()
to flip and mirror the image produced; these arguments must given with
increasing values.

BTW, do you really want to flip your image (upside down) or you would
like to rotate it 180 degrees. If the latter, see
http://www.maths.lth.se/help/R/image/ for some image() example code
related to this. Try also

  source("http://www.maths.lth.se/help/R/image/image.R")

Best wishes

Henrik Bengtsson
Lund University

> -----Original Message-----
> From: r-help-bounces at stat.math.ethz.ch 
> [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of 
> Bj?rn-Helge Mevik
> Sent: den 16 oktober 2003 09:57
> To: r-help at stat.math.ethz.ch
> Subject: Re: [R] How to flip image?
> 
> 
> Ernie Adorio <adorio at math.upd.edu.ph> writes:
> 
> > If not possible, is there any built-in R command to reverse 
> the rows 
> > of a
> > matrix?
> 
> How about Face[nrow(Face):1, ] ?
> 
> -- 
> Bj?rn-Helge Mevik
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list 
> https://www.stat.math.ethz.ch/mailma> n/listinfo/r-help
> 
>



From mrufino at icm.csic.es  Thu Oct 16 12:15:31 2003
From: mrufino at icm.csic.es (Marta Rufino)
Date: Thu, 16 Oct 2003 12:15:31 +0200
Subject: [R] plot discrimnant analysis
Message-ID: <5.2.1.1.1.20031016120146.00c1bcf8@cucafera.icm.csic.es>

Hello,

Does anyone knows how to do the plots from discriminant analysis (lda and qda)?

Is there any computed function to do the stepwise procedure?

thank you in advance
Marta



From pburns at pburns.seanet.com  Thu Oct 16 12:32:22 2003
From: pburns at pburns.seanet.com (Patrick Burns)
Date: Thu, 16 Oct 2003 11:32:22 +0100
Subject: [R] [OFF]  a data game
Message-ID: <3F8E73B6.5000502@pburns.seanet.com>

Burns Statistics recently finished the "Technical Analysis
Challenge" which was a study of the ability to tell real
stock price series from random ones.  Some people seem
to be finding it a fun game to play (including how best
to cheat).

There are 100 series of 500 points each.  Each of these
series has 4 possible extensions of 50 points each -- one
is real the other 3 are random.

On the Burns Statistic website are:
* plots of the data
* the actual data
* the answers
* the results from the few that actually made submissions
* a report on the results
* the advertisement for R related to the challenge

Patrick Burns

Burns Statistics
patrick at burns-stat.com
+44 (0)20 8525 0696
http://www.burns-stat.com
(home of S Poetry and "A Guide for the Unwilling S User")



From ripley at stats.ox.ac.uk  Thu Oct 16 12:41:56 2003
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Thu, 16 Oct 2003 11:41:56 +0100 (BST)
Subject: [R] plot discrimnant analysis
In-Reply-To: <5.2.1.1.1.20031016120146.00c1bcf8@cucafera.icm.csic.es>
Message-ID: <Pine.LNX.4.44.0310161136300.4840-100000@gannet.stats>

On Thu, 16 Oct 2003, Marta Rufino wrote:

> Hello,
> 
> Does anyone knows how to do the plots from discriminant analysis (lda and qda)?

There are plot methods supplied for lda with examples in the MASS scripts.
You could have tried
> methods(class="lda")
[1] coef.lda*        model.frame.lda* pairs.lda*       plot.lda*
[5] predict.lda*     print.lda*

    Non-visible functions are asterisked

and see also ldahist() (referenced from ?plot.lda).

I don't know what `the plots' from qda are, but there is _a_ plot in the
MASS scripts.

> Is there any computed function to do the stepwise procedure?

Which procedure?
There is no implementation of the stepwise LDA as supplied by SPSS, say.
The main problem would be computing signifcance levels for the test 
statistics (and also the very strong distributional assumptions needed).

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From christian.schulz at questico.de  Thu Oct 16 13:03:03 2003
From: christian.schulz at questico.de (Christian Schulz)
Date: Thu, 16 Oct 2003 13:03:03 +0200
Subject: [R] ROracle Win2K
Message-ID: <JAEELBHBOPKJDMMCNHKMOEGJCAAA.christian.schulz@questico.de>

I have success with compiling and installation on
R-1.7.1 with ROracle_0.5-0.tar.gz.tar

But starting the first tests i get this error ?

>>
>>library(DBI)
>>library(ROracle)
>>
>>drv <- dbDriver("Oracle")
Error in .Call("RS_Ora_init", config.params, reload, PACKAGE = .OraPkgName)
:
        .Call function name not in load table

Perhaps it is necessary copy some *.dll from Oracle
to  /library/ROracle/libs


Thanks for any help!!!
Christian



From christoph.bier at web.de  Thu Oct 16 13:31:21 2003
From: christoph.bier at web.de (Christoph Bier)
Date: Thu, 16 Oct 2003 13:31:21 +0200
Subject: [R] summary with names
Message-ID: <3F8E8189.2020808@web.de>

Hi,

how can I tell 'summary' to print the name of the summarised 
variable? This is probably an awkward newbie question but I 
didn't find an answer in the Docus, the FAQ and maillist archive.
    I want a summary for about 250 variables and realise it 
the following way (I know, that I shouldn't use iterations 
that way in R; but at the moment it's the easiest way for me):

for(i in fb.12.unt[varA1:varZ9]){print (summary(i, na.rm=t))}

It works fine, but I don't know which summary corresponds to 
which variable, because the variable names are not printed. 
Can somebody give me a hint?

TIA

Regards,

Christoph
-- 
Christoph Bier, Dipl.Oecotroph., Email: bier at wiz.uni-kassel.de
Universitaet Kassel, FG Oekologische Lebensmittelqualitaet und
Ernaehrungskultur \\ Postfach 12 52 \\ 37202 Witzenhausen
Tel.: +49 (0) 55 42 / 98 -17 21, Fax: -17 13



From ramasamya at gis.a-star.edu.sg  Thu Oct 16 13:42:16 2003
From: ramasamya at gis.a-star.edu.sg (Adaikalavan RAMASAMY)
Date: Thu, 16 Oct 2003 19:42:16 +0800
Subject: [R] Statisticians - Genome Institute of Singapore
Message-ID: <6D9E9B9DF347EF4385F6271C64FB8D56075FD9@BIONIC.biopolis.one-north.com>

Genome Institute of Singapore (GIS) 

Biostatistics Group

The Genome Institute of Singapore ( http://www.gis.a-star.edu.sg <http://www.gis.a-star.edu.sg> ) is looking for statisticians who are interested in a wide range of biomedical probems.  

We are especially interested in hearing from senior researchers interested in leading their own group. 

The institute is well-funded and engaged in the study of molecular biology using technologies that allow biological systems to be interrogated on a genome-wide scale.  Technology platforms include microarrays, FTMS/proteomics, sequencing and genotyping. Biological domains include cancer, infectious disease, pharmacology, cell signaling, stem cell biology, population and human genetics, renal and liver diseases, and comparative biology.

Statisticians working at the GIS are required to perform high-quality research on methodological issues arising from the analysis of data generated by the above (and other) platforms, as well as to participating in biological research using them.  How time is divided between these depends on individual interests and abilities.

We particularly welcome individuals interested in the analysis of gene expression and in statistical genetics. While experience working with genomic data is an advantage, we welcome talented and enthusiastic people who wish to develop their careers in this field.

To apply, please send a CV and a cover letter to 

        gisrecruit at gis.a-star.edu.sg



From d.orme at imperial.ac.uk  Thu Oct 16 13:48:23 2003
From: d.orme at imperial.ac.uk (David Orme)
Date: Thu, 16 Oct 2003 12:48:23 +0100
Subject: [R] RODBC on Mac OSX pointers?
In-Reply-To: <Pine.LNX.4.44.0310160953380.4650-100000@gannet.stats>
Message-ID: <A58C6C28-FFCE-11D7-9181-000393DC1748@ic.ac.uk>

Thanks for the help - I installed RODBC from within R using 
install.packages() over the network so I missed/ignored the README 
within the source directory.

I can't work out how to specify the driver manager in the INSTALL 
command. I can set the INSTALL to look in the right search path for the 
driver manager library using --configure-vars='LIBS=-L/usr/lib' but I 
can't work out how to set the install to use iODBC rather than 
unixODBC. My /usr/lib directory does not contain any unixODBC libraries 
but does contain libiodbc.2.1.6dylib and links to it.

David




On Thursday, October 16, 2003, at 09:58  am, Prof Brian Ripley wrote:

> RODBC has been used with iODBC under Linux, so your suspicion is wrong
> (and contradicted by the RODBC documentation).
>
> Nevertheless, your error message shows that your build is trying to use
> unixODBC, so I suspect you built it against the wrong ODBC driver 
> manager.
>
> On Wed, 15 Oct 2003, David Orme wrote:
>
>> Hi,
>>
>> I'm trying to get to grips with accessing my postgresql tables in R
>> using RODBC. I have Mac OS 10.2.8 (Darwin 6.8) installed and R 1.8.0
>> installed from source and I have updated RODBC to 1.0-4.
>>
>> On the ODBC manager end, I am using OpenLink software's iODBC
>> Administrator and I've set up a User DSN called "PostGRES"  - odbctest
>> suggests that this is functioning:
>>
>>> [doibook:~] dorme% /usr/bin/odbctest
>>> iODBC Demonstration program
>>> This program shows an interactive SQL processor
>>>
>>> Enter ODBC connect string (? shows list): dsn=PostGRES
>>>
>>> Have a nice day.
>>> [doibook:~] dorme%
>>
>> However, when I try to access the connection from R I get the 
>> following:
>>
>>>> test <- odbcConnect(dsn="PostGRES", uid="dorme", pwd="secret",
>>> case="postgresql")
>>> Warning messages:
>>> 1: [RODBC] ERROR: state IM002, code 0, message [unixODBC][Driver
>>> Manager]Data so
>>> urce name not found, and no default driver specified
>>> 2: ODBC connection failed in: odbcDriverConnect(paste("DSN=", dsn,
>>> ";UID=", uid,
>>>  ";PWD=", pwd,
>>
>> Any suggestions? My suspicion is that RODBC requires unixODBC and
>> doesn't know about iOBDC, but in which case I wouldn't expect to see a
>> Driver Manager error code.
>>
>> Thanks,
>> David
>>
>>
>> ---------------------------------------
>> Dr. David Orme
>>
>> Department of Biological Sciences
>> Imperial College London
>> Silwood Park Campus
>> Ascot, Berkshire SL5 7PY UK.
>>
>> Tel: +44 (0)20 759 42358
>> Fax: +44 (0)20 759 42339
>> e-mail: d.orme at imperial.ac.uk
>>
>> ______________________________________________
>> R-help at stat.math.ethz.ch mailing list
>> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
>>
>>
>
> -- 
> Brian D. Ripley,                  ripley at stats.ox.ac.uk
> Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
> University of Oxford,             Tel:  +44 1865 272861 (self)
> 1 South Parks Road,                     +44 1865 272866 (PA)
> Oxford OX1 3TG, UK                Fax:  +44 1865 272595
>



From ligges at statistik.uni-dortmund.de  Thu Oct 16 13:53:08 2003
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Thu, 16 Oct 2003 13:53:08 +0200
Subject: [R] summary with names
In-Reply-To: <3F8E8189.2020808@web.de>
References: <3F8E8189.2020808@web.de>
Message-ID: <3F8E86A4.2020007@statistik.uni-dortmund.de>

Christoph Bier wrote:
> Hi,
> 
> how can I tell 'summary' to print the name of the summarised variable? 
> This is probably an awkward newbie question but I didn't find an answer 
> in the Docus, the FAQ and maillist archive.
>    I want a summary for about 250 variables and realise it the following 
> way (I know, that I shouldn't use iterations that way in R; but at the 
> moment it's the easiest way for me):
> 
> for(i in fb.12.unt[varA1:varZ9]){print (summary(i, na.rm=t))}

Given fb.12.unt is a list or data.frame,
   lapply(fb.12.unt, summary, na.rm = TRUE)
or
   sapply(fb.12.unt, summary, na.rm = TRUE)
might do what you want.

BTW: "na.rm=t" is wrong anyway ...


Uwe Ligges


> It works fine, but I don't know which summary corresponds to which 
> variable, because the variable names are not printed. Can somebody give 
> me a hint?
> 
> TIA
> 
> Regards,
> 
> Christoph



From glaziou at pasteur-kh.org  Thu Oct 16 13:56:41 2003
From: glaziou at pasteur-kh.org (Philippe Glaziou)
Date: Thu, 16 Oct 2003 18:56:41 +0700
Subject: [R] summary with names
In-Reply-To: <3F8E8189.2020808@web.de>
References: <3F8E8189.2020808@web.de>
Message-ID: <20031016115641.GM1969@pasteur-kh.org>

Christoph Bier <christoph.bier at web.de> wrote:
> how can I tell 'summary' to print the name of the summarised 
> variable? This is probably an awkward newbie question but I 
> didn't find an answer in the Docus, the FAQ and maillist archive.
>    I want a summary for about 250 variables and realise it 
> the following way (I know, that I shouldn't use iterations 
> that way in R; but at the moment it's the easiest way for me):
> 
> for(i in fb.12.unt[varA1:varZ9]){print (summary(i, na.rm=t))}
>
> It works fine, but I don't know which summary corresponds to 
           ^^^^

This surprises me.


> which variable, because the variable names are not printed. 
> Can somebody give me a hint?


Can you give us an example of summary not giving variable names?

> df<-data.frame(var1=c(1,2,3),var2=c(4,5,6),factor1=c('a','a','b'))
> summary(df)
      var1          var2     factor1
 Min.   :1.0   Min.   :4.0   a:2    
 1st Qu.:1.5   1st Qu.:4.5   b:1    
 Median :2.0   Median :5.0          
 Mean   :2.0   Mean   :5.0          
 3rd Qu.:2.5   3rd Qu.:5.5          
 Max.   :3.0   Max.   :6.0          

> summary(df[1:2])
      var1          var2    
 Min.   :1.0   Min.   :4.0  
 1st Qu.:1.5   1st Qu.:4.5  
 Median :2.0   Median :5.0  
 Mean   :2.0   Mean   :5.0  
 3rd Qu.:2.5   3rd Qu.:5.5  
 Max.   :3.0   Max.   :6.0  

-- 
Philippe



From andy_liaw at merck.com  Thu Oct 16 14:05:56 2003
From: andy_liaw at merck.com (Liaw, Andy)
Date: Thu, 16 Oct 2003 08:05:56 -0400
Subject: [R] princomp with more coloumns than rows: why not?
Message-ID: <3A822319EB35174CA3714066D590DCD50205CCB6@usrymx25.merck.com>

In the `Detail' section of ?princomp:

princomp only handles so-called Q-mode PCA, that is feature extraction of
variables. If a data matrix is supplied (possibly via a formula) it is
required that there are at least as many units as variables. For R-mode PCA
use prcomp. 


Andy

> -----Original Message-----
> From: bhx2 at mevik.net [mailto:bhx2 at mevik.net] 
> Sent: Thursday, October 16, 2003 5:14 AM
> To: r-help at stat.math.ethz.ch
> Subject: [R] princomp with more coloumns than rows: why not?
> 
> 
> As of R 1.7.0, princomp no longer accept matrices with more 
> coloumns than rows.  I'm curious:  Why was this decision made?
> 
> I work a lot with data where more coloumns than rows is more 
> of a rule than an exception (for instance spectroscopic 
> data).  To me, princomp have two advantages above prcomp: 1) 
> It has a predict method, and 2) it has a biplot method.
> 
> A biplot method shouldn't be too difficult to implement (I 
> believe I've seen one on R-help).
> 
> A predict method seems to be more difficult, because the 
> prcomp object doesn't include the means that need to be 
> subtracted from the new data.  Would it break conformance 
> with S to let prcomp return the means as well?
> 
> -- 
> Sincerely,
> Bj?rn-Helge Mevik
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list 
> https://www.stat.math.ethz.ch/mailman/listinfo> /r-help
>



From rpeng at jhsph.edu  Thu Oct 16 14:18:18 2003
From: rpeng at jhsph.edu (Roger D. Peng)
Date: Thu, 16 Oct 2003 08:18:18 -0400
Subject: [R] Strange scope problem
In-Reply-To: <3F8DDD57.A74B9FEC@icarus.math.mcmaster.ca>
References: <3F8DDD57.A74B9FEC@icarus.math.mcmaster.ca>
Message-ID: <3F8E8C8A.2040101@jhsph.edu>

I believe this was fixed in 1.7.0, although I'm not sure of all the 
details.  The NEWS file says

    o   step(), add1.default() and drop1.default() now work somewhat
        better if called from a function.


-roger

Angelo Canty wrote:

>Hi,
>
>I have come across the following problem which seems to be a scoping
>issue but I'm at a loss to see why this is so or to find a good
>workaround.
>
>Suppose I have a function to get a prediction after model selection
>using the step function.
>
>step.pred <- function(dat, x0) {
>  fit.model <- step(lm(y~., data=dat), trace=F)
>  predict(fit.model, x0, se.fit=T)
>}
>
>This function works sometimes for example
>
>set.seed(1)
>X.1 <- data.frame(x1=rnorm(20), x2=rnorm(20), x3=rnorm(20))
>y.1 <- 5+as.matrix(X.1[,1:2])%*%matrix(c(1,1))+rnorm(20)
>Xy.1 <- data.frame(X.1,y=y.1)
>x0.1 <- data.frame(x1=-1,x2=-1, x3=-1)
>step.pred(Xy.1, x0.1)
>
>$fit
>[1] 3.359540
>
>$se.fit
>[1] 0.523629
>
>$df
>[1] 16
>
>$residual.scale
>[1] 1.093526
>
>but most often it crashes as in
>
>set.seed(2)
>X.2 <- data.frame(x1=rnorm(20), x2=rnorm(20), x3=rnorm(20))
>y.2 <- 5+as.matrix(X.2[,1:2])%*%matrix(c(1,1))+rnorm(20)
>Xy.2 <- data.frame(X.2,y=y.2)
>x0.2 <- data.frame(x1=-1,x2=-1, x3=-1)
>step.pred(Xy.2, x0.2)
>Error in model.frame.default(formula = y ~ x1 + x2, data = dat,
>drop.unused.levels = TRUE) : 
>        Object "dat" not found
>
>The difference seems to be that for the first dataset, step retains
>all three variables whereas for the second it drops one of them.
>
>  
>
>>step(lm(y~.,data=Xy.1), trace=F)
>>    
>>
>
>Call:
>lm(formula = y ~ x1 + x2 + x3, data = Xy.1)
>
>Coefficients:
>(Intercept)           x1           x2           x3  
>     4.8347       0.8937       1.0331      -0.4516  
>
>  
>
>>step(lm(y~.,data=Xy.2), trace=F)
>>    
>>
>
>Call:
>lm(formula = y ~ x1 + x2, data = Xy.2)
>
>Coefficients:
>(Intercept)           x1           x2  
>     5.0802       0.9763       1.1369  
>
>
>One possible workaround is to explicitely assign the local variable
>dat in the .GlobalEnv as in
>
>step.pred1 <- function(dat, x0) {
>  assign("dat",dat, envir=.GlobalEnv)
>  fit.model <- step(lm(y~., data=dat), trace=F)
>  predict(fit.model, x0, se.fit=T)
>}
>
>I don't like this method since it would overwrite anything else called
>dat in .GlobalEnv.  I realize that I could give it an obscure name but
>the potential for damage still remains.  Am I missing something obvious
>here?  If not, is it possible to work around this problem in such a way
>that .GlobalEnv does not need to be touched?
>
>In S-Plus I would use 
>assign("dat",dat, frame=1)
>which works but that is not available (AFAIK) in R.  Is there
>something similar that I can use?
>
>I am using R 1.6.1 for Unix on a Sun Workstation. I know that I need
>to upgrade but our sysadmin doesn't regard it as priority!  
>
>Thanks for any help you can give for this.
>Angelo
>
>------------------------------------------------------------------
>|   Angelo J. Canty                Email: cantya at mcmaster.ca     |
>|   Mathematics and Statistics     Phone: (905) 525-9140 x 27079 |
>|   McMaster University            Fax  : (905) 522-0935         |
>|   1280 Main St. W.                                             |
>|   Hamilton ON L8S 4K1                                          |
>
>______________________________________________
>R-help at stat.math.ethz.ch mailing list
>https://www.stat.math.ethz.ch/mailman/listinfo/r-help
>
>  
>



From rksh at soc.soton.ac.uk  Thu Oct 16 14:53:06 2003
From: rksh at soc.soton.ac.uk (Robin Hankin)
Date: Thu, 16 Oct 2003 13:53:06 +0100
Subject: [R] apply() question
Message-ID: <a06002002bbb4447b150d@[139.166.242.29]>

Hi

I have a function that returns an array with four columns but the
number of rows differs with the calling argument.  I want to use
something like sapply() to rbind() the outputs together.

Following toy example illustrates my problem:

f  <- function(i) {
   options(warn= -1)
   r <- ceiling(sqrt(i))
   return(matrix(1:3,r,4))
}

Thus sapply(1:5,f) is a list with five elements.

R> (a <- sapply(1:5,f))
[[1]]
      [,1] [,2] [,3] [,4]
[1,]    1    2    3    1

#{...DELETED...}

[[5]]
      [,1] [,2] [,3] [,4]
[1,]    1    1    1    1
[2,]    2    2    2    2
[3,]    3    3    3    3

R>

what I want is the equivalent of

R> rbind(a[[1]],a[[2]],a[[3]],a[[4]],a[[5]])

but with an arbitrary upper limit and without any loops.  Anyone?


Obligatory attempt:
R> string<-paste("jj<-rbind(a[[1]] ",paste(",a[[",2:5,"]]",collapse=" "),")")
R> eval(parse(text=string))



From canty at math.mcmaster.ca  Thu Oct 16 14:58:25 2003
From: canty at math.mcmaster.ca (Angelo Canty)
Date: Thu, 16 Oct 2003 08:58:25 -0400 (EDT)
Subject: [R] Strange scope problem
In-Reply-To: <3F8DDD57.A74B9FEC@icarus.math.mcmaster.ca>
Message-ID: <Pine.SOL.4.44.0310160851080.5460-100000@icarus.math.mcmaster.ca>

Thanks to everyone who replied to my message last night and sorry
for any confusion in my use of the term "crashes".  I meant that
the function returns an error not that the program crashes.  Anyway
it seems that this problem was fixed in release 1.7.0 so it's back
to annoying the sysadmin until he updates to R 1.8.0

Thanks also to those who pointed out that I could use a local version
of R.  This is true but the problem actually arose in the context of
a student's project and she is limited by disk quotas etc. I
suspect that it would not be so feasible for her to run a her own
local version of R but I will explore the option.

Thanks,
Angelo

------------------------------------------------------------------
|   Angelo J. Canty                Email: cantya at mcmaster.ca     |
|   Mathematics and Statistics     Phone: (905) 525-9140 x 27079 |
|   McMaster University            Fax  : (905) 522-0935         |
|   1280 Main St. W.                                             |
|   Hamilton ON L8S 4K1                                          |
------------------------------------------------------------------

On Wed, 15 Oct 2003, Angelo Canty wrote:

> Hi,
>
> I have come across the following problem which seems to be a scoping
> issue but I'm at a loss to see why this is so or to find a good
> workaround.
>
> Suppose I have a function to get a prediction after model selection
> using the step function.
>
> step.pred <- function(dat, x0) {
>   fit.model <- step(lm(y~., data=dat), trace=F)
>   predict(fit.model, x0, se.fit=T)
> }
>
> This function works sometimes for example
>
> set.seed(1)
> X.1 <- data.frame(x1=rnorm(20), x2=rnorm(20), x3=rnorm(20))
> y.1 <- 5+as.matrix(X.1[,1:2])%*%matrix(c(1,1))+rnorm(20)
> Xy.1 <- data.frame(X.1,y=y.1)
> x0.1 <- data.frame(x1=-1,x2=-1, x3=-1)
> step.pred(Xy.1, x0.1)
>
> $fit
> [1] 3.359540
>
> $se.fit
> [1] 0.523629
>
> $df
> [1] 16
>
> $residual.scale
> [1] 1.093526
>
> but most often it crashes as in
>
> set.seed(2)
> X.2 <- data.frame(x1=rnorm(20), x2=rnorm(20), x3=rnorm(20))
> y.2 <- 5+as.matrix(X.2[,1:2])%*%matrix(c(1,1))+rnorm(20)
> Xy.2 <- data.frame(X.2,y=y.2)
> x0.2 <- data.frame(x1=-1,x2=-1, x3=-1)
> step.pred(Xy.2, x0.2)
> Error in model.frame.default(formula = y ~ x1 + x2, data = dat,
> drop.unused.levels = TRUE) :
>         Object "dat" not found
>
> The difference seems to be that for the first dataset, step retains
> all three variables whereas for the second it drops one of them.
>
> > step(lm(y~.,data=Xy.1), trace=F)
>
> Call:
> lm(formula = y ~ x1 + x2 + x3, data = Xy.1)
>
> Coefficients:
> (Intercept)           x1           x2           x3
>      4.8347       0.8937       1.0331      -0.4516
>
> > step(lm(y~.,data=Xy.2), trace=F)
>
> Call:
> lm(formula = y ~ x1 + x2, data = Xy.2)
>
> Coefficients:
> (Intercept)           x1           x2
>      5.0802       0.9763       1.1369
>
>
> One possible workaround is to explicitely assign the local variable
> dat in the .GlobalEnv as in
>
> step.pred1 <- function(dat, x0) {
>   assign("dat",dat, envir=.GlobalEnv)
>   fit.model <- step(lm(y~., data=dat), trace=F)
>   predict(fit.model, x0, se.fit=T)
> }
>
> I don't like this method since it would overwrite anything else called
> dat in .GlobalEnv.  I realize that I could give it an obscure name but
> the potential for damage still remains.  Am I missing something obvious
> here?  If not, is it possible to work around this problem in such a way
> that .GlobalEnv does not need to be touched?
>
> In S-Plus I would use
> assign("dat",dat, frame=1)
> which works but that is not available (AFAIK) in R.  Is there
> something similar that I can use?
>
> I am using R 1.6.1 for Unix on a Sun Workstation. I know that I need
> to upgrade but our sysadmin doesn't regard it as priority!
>
> Thanks for any help you can give for this.
> Angelo
>
> ------------------------------------------------------------------
> |   Angelo J. Canty                Email: cantya at mcmaster.ca     |
> |   Mathematics and Statistics     Phone: (905) 525-9140 x 27079 |
> |   McMaster University            Fax  : (905) 522-0935         |
> |   1280 Main St. W.                                             |
> |   Hamilton ON L8S 4K1                                          |
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
>



From bolker at zoo.ufl.edu  Thu Oct 16 15:13:02 2003
From: bolker at zoo.ufl.edu (Ben Bolker)
Date: Thu, 16 Oct 2003 09:13:02 -0400 (EDT)
Subject: [R] apply() question
In-Reply-To: <a06002002bbb4447b150d@[139.166.242.29]>
Message-ID: <Pine.LNX.4.44.0310160912530.1000-100000@bolker.zoo.ufl.edu>


do.call("rbind",list)

On Thu, 16 Oct 2003, Robin Hankin wrote:

> Hi
> 
> I have a function that returns an array with four columns but the
> number of rows differs with the calling argument.  I want to use
> something like sapply() to rbind() the outputs together.
> 
> Following toy example illustrates my problem:
> 
> f  <- function(i) {
>    options(warn= -1)
>    r <- ceiling(sqrt(i))
>    return(matrix(1:3,r,4))
> }
> 
> Thus sapply(1:5,f) is a list with five elements.
> 
> R> (a <- sapply(1:5,f))
> [[1]]
>       [,1] [,2] [,3] [,4]
> [1,]    1    2    3    1
> 
> #{...DELETED...}
> 
> [[5]]
>       [,1] [,2] [,3] [,4]
> [1,]    1    1    1    1
> [2,]    2    2    2    2
> [3,]    3    3    3    3
> 
> R>
> 
> what I want is the equivalent of
> 
> R> rbind(a[[1]],a[[2]],a[[3]],a[[4]],a[[5]])
> 
> but with an arbitrary upper limit and without any loops.  Anyone?
> 
> 
> Obligatory attempt:
> R> string<-paste("jj<-rbind(a[[1]] ",paste(",a[[",2:5,"]]",collapse=" "),")")
> R> eval(parse(text=string))
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> 

-- 
620B Bartram Hall                            bolker at zoo.ufl.edu
Zoology Department, University of Florida    http://www.zoo.ufl.edu/bolker
Box 118525                                   (ph)  352-392-5697
Gainesville, FL 32611-8525                   (fax) 352-392-3704



From pburns at pburns.seanet.com  Thu Oct 16 15:16:24 2003
From: pburns at pburns.seanet.com (Patrick Burns)
Date: Thu, 16 Oct 2003 14:16:24 +0100
Subject: [R] apply() question
References: <a06002002bbb4447b150d@[139.166.242.29]>
Message-ID: <3F8E9A28.7040307@pburns.seanet.com>

do.call("rbind", your.list)

Patrick Burns

Burns Statistics
patrick at burns-stat.com
+44 (0)20 8525 0696
http://www.burns-stat.com
(home of S Poetry and "A Guide for the Unwilling S User")

Robin Hankin wrote:

> Hi
>
> I have a function that returns an array with four columns but the
> number of rows differs with the calling argument.  I want to use
> something like sapply() to rbind() the outputs together.
>
> Following toy example illustrates my problem:
>
> f  <- function(i) {
>   options(warn= -1)
>   r <- ceiling(sqrt(i))
>   return(matrix(1:3,r,4))
> }
>
> Thus sapply(1:5,f) is a list with five elements.
>
> R> (a <- sapply(1:5,f))
> [[1]]
>      [,1] [,2] [,3] [,4]
> [1,]    1    2    3    1
>
> #{...DELETED...}
>
> [[5]]
>      [,1] [,2] [,3] [,4]
> [1,]    1    1    1    1
> [2,]    2    2    2    2
> [3,]    3    3    3    3
>
> R>
>
> what I want is the equivalent of
>
> R> rbind(a[[1]],a[[2]],a[[3]],a[[4]],a[[5]])
>
> but with an arbitrary upper limit and without any loops.  Anyone?
>
>
> Obligatory attempt:
> R> string<-paste("jj<-rbind(a[[1]] ",paste(",a[[",2:5,"]]",collapse=" 
> "),")")
> R> eval(parse(text=string))
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
>
>



From Arne.Muller at aventis.com  Thu Oct 16 15:21:05 2003
From: Arne.Muller at aventis.com (Arne.Muller@aventis.com)
Date: Thu, 16 Oct 2003 15:21:05 +0200
Subject: [R] A data frame of data frames
Message-ID: <C80ECAFA2ACC1B45BE45D133ED660ADE410B17@crbsmxsusr04.pharma.aventis.com>

Hello,

I'm trying to set up the flowwing data structure in R:

A data frame with 7,000 rows and 4 colums. The rownames have some special
meaning (they are names of genes). The 1st column per row is itself a data
frame, and columns 2 to 4 will keep numeric values.

The data frame contained in the 1st column will have 54 rows (with special
names) and 4 colums (1st col is a response, cols 2- 4 are factors). Each of
these data frames with the response/factors will be fed into an 3way linear
model for anova. The other colums of the 1st data will hold the p-values.

Basically running 7,000 anovas is very quick but the reformating of the data
so that it is suitable for the anova takes a long time (45 minutes). So I'd
just like to keep the generated data structure as a persistent R object.

I haven't managed to store the 2nd data frame in the 1st colum of the 1st
data frame.

>From other languages such as C I'd know how to setup this kind of data
structure (pointers), but I get stuck in R (I guess I'm still struggling the
way the R philosophy on how to present data structures).

Do you've any suggestions on how to do this?

	kind regards,

	Arne



From bates at stat.wisc.edu  Thu Oct 16 15:38:35 2003
From: bates at stat.wisc.edu (Douglas Bates)
Date: 16 Oct 2003 08:38:35 -0500
Subject: [R] Example of cell means model
In-Reply-To: <Law14-F32Ky8VBDUfdf0000a788@hotmail.com>
References: <Law14-F32Ky8VBDUfdf0000a788@hotmail.com>
Message-ID: <6rvfqppajo.fsf@bates4.stat.wisc.edu>

"Francisco Vergara" <gerifalte28 at hotmail.com> writes:

> Thanks a lot for your reply. This helps a lot!
> Just to confirm, using lm this model will give me the mean yield value
> for each cell in the two way array.  Now if I want to obtain the mean
> of group means (like a SS type III approach) using LME (since I have
> random effects in the model) how can I parametrize this?

Didn't I just answer that?

> I could definitivelly use xtabs in a two-way case but in my case I
> have 2 other (continuous) covariates that are potential confounders in
> the model so I need to keep them to obtain the corrected means.
> I added a continuous variable (NewVar) to the dataset Newxmp11.07 and
> obtaned a model with the covariate (fm4) and another without it (fm3)

I think you misinterpreted what I wrote.  I used xtabs to show an
alternative, direct calculation of the cell means.  The purpose was to
to compare the result with the coefficients in the model fit with the
new factor constructed as I described.

> >Newxmp11.07<-fix(xmp11.07)
> >str(Newxmp11.07)
> `data.frame':   36 obs. of  4 variables:
> $ Yield  : num  10.5 9.2 7.9 12.8 11.2 13.3 12.1 12.6 14 10.8 ...
> $ Variety: Factor w/ 3 levels "1","2","3": 1 1 1 1 1 1 1 1 1 1 ...
> $ Density: Factor w/ 4 levels "1","2","3","4": 1 1 1 2 2 2 3 3 3 4 ...
> $ NewVar : num  10 9 7 12 11 11 12 11 15 16 ...
> 
> >fm3<-gls(Yield~-1+Variety + Density, xmp11.07)
> >fm4<-gls(Yield~-1+Variety + Density+NewVar, Newxmp11.07)

Why are you using gls for these models?

Why do you not generate a factor to achieve the cell means model that
(apparently) you want?

> >fm3
> Coefficients:
> Variety1  Variety2  Variety3  Density2  Density3  Density4
> 8.922222  9.797222 15.713889  2.911111  4.300000  2.433333
> 
> Degrees of freedom: 36 total; 30 residual
> Residual standard error: 1.239243
> 
> >fm4
> Coefficients:
>    Variety1    Variety2    Variety3    Density2    Density3    Density4
> 8.75757265  9.70589316 15.43347009  2.88152564  4.32186752  2.40117521
>      NewVar
> 0.01157692
> 
> Degrees of freedom: 36 total; 29 residual
> Residual standard error: 1.252991
> 
> fm4 gives me the mean of the group means for all the varieties but
> apparently it gives me the treatment contrasts for the densities.  If
> I change the order of the factors in the model specification I get

Please read what I wrote.

> >coef(fm5<-gls(Yield~-1+ Density+Variety+NewVar, Newxmp11.07))
>    Density1    Density2    Density3    Density4    Variety2    Variety3
> 8.75757265 11.63909829 13.07944017 11.15874786  0.94832051  6.67589744
>      NewVar
> 0.01157692
> 
> This, just like fm4 will include the original intercept value in
> Density 1 which is not the actual density 1 mean.  What am I missing? 
> I am sorry if these questions are very basic but I want to make sure
> that I understand what I am doing. I guess that this is the price that
> I am paying for having used in the past packages like SAS where you
> just ask for lsmeans and the software will give you a "black box"
> answer!

Yes.  I think it would be interesting to ask people who use the
results from lsmeans to explain what the results represent.  My guess
is that less than 1% of the people who use lsmeans know what they in
fact are.

I won't be able to continue this conversation.  I am very busy right
putting new facilities into R.  Perhaps others on the list will be
able to respond to your questions.



From rksh at soc.soton.ac.uk  Thu Oct 16 15:39:34 2003
From: rksh at soc.soton.ac.uk (Robin Hankin)
Date: Thu, 16 Oct 2003 14:39:34 +0100
Subject: [R] apply() question and help(do.call)
In-Reply-To: <3F8E9A28.7040307@pburns.seanet.com>
References: <a06002002bbb4447b150d@[139.166.242.29]>
	<3F8E9A28.7040307@pburns.seanet.com>
Message-ID: <a06002006bbb44d963772@[139.166.242.29]>

many very helpful people wrote:

>do.call("rbind", list)


Thank you very much everybody.  I wondered what the deal was WRT do.call().
Could we add this or a similarly educational example to the help(do.call) page?

rksh



From John.Marsland at CommerzbankIB.com  Thu Oct 16 15:54:54 2003
From: John.Marsland at CommerzbankIB.com (Marsland, John)
Date: Thu, 16 Oct 2003 14:54:54 +0100
Subject: [R] monitoring a real-time event stream in R
Message-ID: <8CBAA121CEB4D5118CB200508BB2BBEF0317E9F2@xmx8lonib.lonib.commerzbank.com>


I have a real-time event stream which I need to monitor in R. I have not
done any work in this area before and would welcome any input from anybody
who has experience in this area.

I have some code to monitor this event stream in C++. I was proposing to
extend this to include <Rdefines.h> and use the .Call() function.

My idea is to create an R variable through SEXP, PROTECT it and return
control of the main thread to R and establish a separate thread updating the
R variable on events from my datastream ... is R thread-safe? ... are there
any obvious flaws in this strategy?

Any thoughts about how to use the GNU thread library would also be useful.

Does anybody have a better ideas?

Regards,

John


********************************************************************** 
This is a commercial communication from Commerzbank AG.\ \ T...{{dropped}}



From Maciej.Hoffman-Wecker at evotecoai.com  Thu Oct 16 16:02:02 2003
From: Maciej.Hoffman-Wecker at evotecoai.com (Maciej.Hoffman-Wecker@evotecoai.com)
Date: Thu, 16 Oct 2003 16:02:02 +0200
Subject: [R] make error R-1.8.0 on SuSE Linux 7.3 (i386)
Message-ID: <OF1604727C.E400F3D8-ONC1256DC0.00485BBE@evotecoai.com>

My first Problem is a minor one. Maybe some knows a reson. Pleas give me a 
hint off-line, as it seems to be something stupid.

1. I sent my first mail to r-help at lists.R-project.org and got 

Delivery Failure Report 

Your document:  make error R-1.8.0 on SuSE Linux 7.3 (i386)
was not delivered to:   r-help at lists.R-project.org
because:        Delivery time expired

I already had this problem earlier and last time r-help at stat.math.ethz.ch 
worked.

2. But now the real issue:

I can't make R-1.8.0 on our SuSE Linux 7.3 (i386). 

make reports:

make[3]: Entering directory 
`/yphome/hoffman/projects/rsources/R-1.8.0/doc/html'
make[3]: Leaving directory 
`/yphome/hoffman/projects/rsources/R-1.8.0/doc/html'
make[2]: Leaving directory 
`/yphome/hoffman/projects/rsources/R-1.8.0/doc/manual'
make[1]: Leaving directory `/yphome/hoffman/projects/rsources/R-1.8.0/doc'
make[1]: Entering directory 
`/yphome/hoffman/projects/rsources/R-1.8.0/src/library'
building all R object docs (text, HTML, LaTeX, examples)
make[2]: Entering directory 
`/yphome/hoffman/projects/rsources/R-1.8.0/src/library'
cat: ../../library/*/CONTENTS: No such file or directory
make[2]: *** [DOCS] Error 1
make[2]: Leaving directory 
`/yphome/hoffman/projects/rsources/R-1.8.0/src/library'
make[1]: *** [docs] Error 2
make[1]: Leaving directory 
`/yphome/hoffman/projects/rsources/R-1.8.0/src/library'
make: [docs] Error 2 (ignored)


and a make check reports:

make[1]: Entering directory 
`/yphome/hoffman/projects/rsources/R-1.8.0/tests'
make[2]: Entering directory 
`/yphome/hoffman/projects/rsources/R-1.8.0/tests'
make[3]: Entering directory 
`/yphome/hoffman/projects/rsources/R-1.8.0/tests/Examples'
make[4]: Entering directory 
`/yphome/hoffman/projects/rsources/R-1.8.0/tests/Examples'
make[4]: Leaving directory 
`/yphome/hoffman/projects/rsources/R-1.8.0/tests/Examples'
make[4]: Entering directory 
`/yphome/hoffman/projects/rsources/R-1.8.0/tests/Examples'
collecting examples for package 'base' ...
make[5]: Entering directory 
`/yphome/hoffman/projects/rsources/R-1.8.0/src/library'
cat: ../../library/*/CONTENTS: No such file or directory
make[5]: *** [DOCS] Error 1
make[5]: Leaving directory 
`/yphome/hoffman/projects/rsources/R-1.8.0/src/library'
file ../../library/base/R-ex cannot be opened at 
../../share/perl/massage-Examples.pl line 107.
running code in 'base-Ex.R' ...make[4]: *** [base-Ex.Rout] Error 1
make[4]: Leaving directory 
`/yphome/hoffman/projects/rsources/R-1.8.0/tests/Examples'
make[3]: *** [test-Examples-Base] Error 2
make[3]: Leaving directory 
`/yphome/hoffman/projects/rsources/R-1.8.0/tests/Examples'
make[2]: *** [test-Examples] Error 2
make[2]: Leaving directory 
`/yphome/hoffman/projects/rsources/R-1.8.0/tests'
make[1]: *** [test-all-basics] Error 1
make[1]: Leaving directory 
`/yphome/hoffman/projects/rsources/R-1.8.0/tests'
make: *** [check] Error 2

This looks like nothing serious - as it seems only to concern the 
documentation - but it's too serious for me to figure it out.

I tried to start R-1.8.0/bin/R anyway. It starts, but after "Type 'q()' to quit R." it crashes with a "Segmentation fault (core 
dumped)".

Hope for some help. Thanks in advance.

maciej


Maciej Hoffman-Wecker
Evotec OAI AG
Discovery Informatics
Schnackenburgallee 114
22525 Hamburg
http://www.evotecoai.com
Phone +49-40-5 60 81-2 31
Fax      +49-40-5 60 81-2 22
E-mail  maciej.hoffman-wecker at evotecoai.com



From tlumley at u.washington.edu  Thu Oct 16 16:01:50 2003
From: tlumley at u.washington.edu (Thomas Lumley)
Date: Thu, 16 Oct 2003 07:01:50 -0700 (PDT)
Subject: [R] summary with names
In-Reply-To: <3F8E8189.2020808@web.de>
References: <3F8E8189.2020808@web.de>
Message-ID: <Pine.A41.4.58.0310160700130.130712@homer08.u.washington.edu>

On Thu, 16 Oct 2003, Christoph Bier wrote:

> Hi,
>
> how can I tell 'summary' to print the name of the summarised
> variable? This is probably an awkward newbie question but I
> didn't find an answer in the Docus, the FAQ and maillist archive.
>     I want a summary for about 250 variables and realise it
> the following way (I know, that I shouldn't use iterations
> that way in R; but at the moment it's the easiest way for me):
>
> for(i in fb.12.unt[varA1:varZ9]){print (summary(i, na.rm=t))}
>
> It works fine, but I don't know which summary corresponds to
> which variable, because the variable names are not printed.
> Can somebody give me a hint?

In this context you will have to print the names yourself.  summary()
doesn't know the names, it only knows the contents of i.

summary(a.data.frame) can print the names becuase the names are part of
the data frame.

As people have already pointed out there are some other strange things
about the command.

	-thomas



From tlumley at u.washington.edu  Thu Oct 16 16:06:16 2003
From: tlumley at u.washington.edu (Thomas Lumley)
Date: Thu, 16 Oct 2003 07:06:16 -0700 (PDT)
Subject: [R] apply() question
In-Reply-To: <a06002002bbb4447b150d@[139.166.242.29]>
References: <a06002002bbb4447b150d@[139.166.242.29]>
Message-ID: <Pine.A41.4.58.0310160702440.130712@homer08.u.washington.edu>

On Thu, 16 Oct 2003, Robin Hankin wrote:
>
> what I want is the equivalent of
>
> R> rbind(a[[1]],a[[2]],a[[3]],a[[4]],a[[5]])
>

Note that while do.call("rbind",a) is the natural solution it may in fact
be faster to create a matrix and use a loop to put things in it.

rval<-matrix(ncol=4, nrow=sum(sapply(a,nrow)))
i<-1
for(ai in a){
	r<-nrow(ai)
	rval[i+1:r,]<-ai
	i<- i+r
}


	-thomas



From tlumley at u.washington.edu  Thu Oct 16 16:10:06 2003
From: tlumley at u.washington.edu (Thomas Lumley)
Date: Thu, 16 Oct 2003 07:10:06 -0700 (PDT)
Subject: [R] A data frame of data frames
In-Reply-To: <C80ECAFA2ACC1B45BE45D133ED660ADE410B17@crbsmxsusr04.pharma.aventis.com>
References: <C80ECAFA2ACC1B45BE45D133ED660ADE410B17@crbsmxsusr04.pharma.aventis.com>
Message-ID: <Pine.A41.4.58.0310160707510.130712@homer08.u.washington.edu>

On Thu, 16 Oct 2003 Arne.Muller at aventis.com wrote:

> Hello,
>
> I'm trying to set up the flowwing data structure in R:
>
> A data frame with 7,000 rows and 4 colums. The rownames have some special
> meaning (they are names of genes). The 1st column per row is itself a data
> frame, and columns 2 to 4 will keep numeric values.
>
> The data frame contained in the 1st column will have 54 rows (with special
> names) and 4 colums (1st col is a response, cols 2- 4 are factors). Each of
> these data frames with the response/factors will be fed into an 3way linear
> model for anova. The other colums of the 1st data will hold the p-values.

I would use a list with the genes in a matrix and the other variables in a
dataframe.

(In fact, I would try to use the data structures that the bioconductor
project has developed for this problem.)

	-thomas



From hess at stat.colostate.edu  Thu Oct 16 16:13:21 2003
From: hess at stat.colostate.edu (Ann Hess)
Date: Thu, 16 Oct 2003 08:13:21 -0600 (MDT)
Subject: [R] .Random.seed
Message-ID: <Pine.GSO.4.58.0310160808570.12394@liberator.stat.colostate.edu>

I am writing a function for the purposes of a simulation.  Due to memory
problems, the function sometimes crashes.  In order to get around this
problem, I would like to include to be able to save the "last" seed, so I
can pick up with the next run of the simulation after a "crash".  I am
having trouble understanding what is going on with .Random.seed!

For each run of the following function, a random uniform and the current
.Random.seed should be printed:

test<-function(runs,seed){
   .Random.seed<-seed
   for (i in 1:runs) {
print(i)
print(runif(1,0,1))
print(.Random.seed)}
   return(.Random.seed}

Consider the following input/output:
>RNGkind(kind="Marsaglia-Multicarry")
> set.seed(20391)
> seed1<-.Random.seed
> seed1
[1]         401 -1607331462  -462081869
> test(2,seed1)
[1] 1
[1] 0.4188851
[1]         401 -1607331462  -462081869
[1] 2
[1] 0.7713649
[1]         401 -1607331462  -462081869
[1]         401 -1607331462  -462081869
> seed1
[1]         401 -1607331462  -462081869
> test(2,seed1)
[1] 1
[1] 0.7293294
[1]         401 -1607331462  -462081869
[1] 2
[1] 0.8266798
[1]         401 -1607331462  -462081869
[1]         401 -1607331462  -462081869


The output from each call of the function seems to suggest that
.Random.seed is not changing (although different random uniforms are
generated each time).  The second call of the function doesn't match the
first call even though the same "seed" is used.

Can anyone explain what is happening here?  My goal is to save the "last"
seed so that I can use it to generate the next run of a simulation (after
a crash).

Thanks in advance!



From gerifalte28 at hotmail.com  Thu Oct 16 16:41:17 2003
From: gerifalte28 at hotmail.com (Francisco Vergara)
Date: Thu, 16 Oct 2003 14:41:17 +0000
Subject: [R] Example of cell means model
Message-ID: <LAW14-F9h3LqPg33I6N0000c2e2@hotmail.com>

Many thansk to Dr. Bates to take time follow on this conversation.  Could 
anybody follow up on the question on what the SAS lsmeans function exactly 
does and how to reproduce that on R?

Thanks

Francisco

>From: Douglas Bates <bates at stat.wisc.edu>
>To: "Francisco Vergara" <gerifalte28 at hotmail.com>
>CC: r-help at r-project.org
>Subject: Re: [R] Example of cell means model
>Date: 16 Oct 2003 08:38:35 -0500
>
>"Francisco Vergara" <gerifalte28 at hotmail.com> writes:
>
> > Thanks a lot for your reply. This helps a lot!
> > Just to confirm, using lm this model will give me the mean yield value
> > for each cell in the two way array.  Now if I want to obtain the mean
> > of group means (like a SS type III approach) using LME (since I have
> > random effects in the model) how can I parametrize this?
>
>Didn't I just answer that?
>
> > I could definitivelly use xtabs in a two-way case but in my case I
> > have 2 other (continuous) covariates that are potential confounders in
> > the model so I need to keep them to obtain the corrected means.
> > I added a continuous variable (NewVar) to the dataset Newxmp11.07 and
> > obtaned a model with the covariate (fm4) and another without it (fm3)
>
>I think you misinterpreted what I wrote.  I used xtabs to show an
>alternative, direct calculation of the cell means.  The purpose was to
>to compare the result with the coefficients in the model fit with the
>new factor constructed as I described.
>
> > >Newxmp11.07<-fix(xmp11.07)
> > >str(Newxmp11.07)
> > `data.frame':   36 obs. of  4 variables:
> > $ Yield  : num  10.5 9.2 7.9 12.8 11.2 13.3 12.1 12.6 14 10.8 ...
> > $ Variety: Factor w/ 3 levels "1","2","3": 1 1 1 1 1 1 1 1 1 1 ...
> > $ Density: Factor w/ 4 levels "1","2","3","4": 1 1 1 2 2 2 3 3 3 4 ...
> > $ NewVar : num  10 9 7 12 11 11 12 11 15 16 ...
> >
> > >fm3<-gls(Yield~-1+Variety + Density, xmp11.07)
> > >fm4<-gls(Yield~-1+Variety + Density+NewVar, Newxmp11.07)
>
>Why are you using gls for these models?
>
>Why do you not generate a factor to achieve the cell means model that
>(apparently) you want?
>
> > >fm3
> > Coefficients:
> > Variety1  Variety2  Variety3  Density2  Density3  Density4
> > 8.922222  9.797222 15.713889  2.911111  4.300000  2.433333
> >
> > Degrees of freedom: 36 total; 30 residual
> > Residual standard error: 1.239243
> >
> > >fm4
> > Coefficients:
> >    Variety1    Variety2    Variety3    Density2    Density3    Density4
> > 8.75757265  9.70589316 15.43347009  2.88152564  4.32186752  2.40117521
> >      NewVar
> > 0.01157692
> >
> > Degrees of freedom: 36 total; 29 residual
> > Residual standard error: 1.252991
> >
> > fm4 gives me the mean of the group means for all the varieties but
> > apparently it gives me the treatment contrasts for the densities.  If
> > I change the order of the factors in the model specification I get
>
>Please read what I wrote.
>
> > >coef(fm5<-gls(Yield~-1+ Density+Variety+NewVar, Newxmp11.07))
> >    Density1    Density2    Density3    Density4    Variety2    Variety3
> > 8.75757265 11.63909829 13.07944017 11.15874786  0.94832051  6.67589744
> >      NewVar
> > 0.01157692
> >
> > This, just like fm4 will include the original intercept value in
> > Density 1 which is not the actual density 1 mean.  What am I missing?
> > I am sorry if these questions are very basic but I want to make sure
> > that I understand what I am doing. I guess that this is the price that
> > I am paying for having used in the past packages like SAS where you
> > just ask for lsmeans and the software will give you a "black box"
> > answer!
>
>Yes.  I think it would be interesting to ask people who use the
>results from lsmeans to explain what the results represent.  My guess
>is that less than 1% of the people who use lsmeans know what they in
>fact are.
>
>I won't be able to continue this conversation.  I am very busy right
>putting new facilities into R.  Perhaps others on the list will be
>able to respond to your questions.
>

_________________________________________________________________
Try MSN Messenger 6.0 with integrated webcam functionality!



From stefan at rannsoknir.is  Thu Oct 16 17:03:59 2003
From: stefan at rannsoknir.is (=?iso-8859-1?Q?Stef=E1n_Hrafn_J=F3nsson?=)
Date: Thu, 16 Oct 2003 15:03:59 -0000 (GMT)
Subject: [R] tick marks and barchart
Message-ID: <51226.213.213.153.238.1066316639.squirrel@vefpostur.rhea.is>


Dear R community.

I have two problems with figures. First deals with short vector on the
x-axis and the second with two-panel barchart.
1) For demonstration I create the following pseudo data for three years,
2001:2003. The indicated plot looks fine except for the number of tick
marks on the x-axis. I get seq(2001,2003,0.5). I want three and only three
tick marks to indicate we have measure once a year not two times each
year. (Having year 2001.5 is not that nice anyway). I tried
as.factor(2001:2003) but this did not do what I want. I have considered
having no labels and plot the year with text(y=-b,x=(2001,2002,2003),
(2001:2003)  ) -b being some value less than 0. A simpler version is
preferred.
2) For the second problem I want to use the same data but create a
barchart with two bars (Group A and group B) for 2001, same two groups for
2002 and same two for 2003. Group A would have blue bars and Group B  red
bars.
Would I use barchart() or panel.barchart()?  Looking in help(barchart) I
find that I need to define a formula.  What would the x, y and g1 be in my
case?

All help highly appreciated


Thanks,
Stefan


demo1 <- matrix(nrow=3, ncol =2, log(c(7,3,2,4,5,6))/log(7) , dimnames
                               =list(as.character(2001:2003),
			        c("Group A","Group B")) )

windows()
par(lab=c(3, 6,7) ,las=1  )

plot(x=(2001:2003), y=demo1[,1]*100,type = "l",
    lwd=3,ylim=c(0,100), xlab = "" , ylab="%")
lines(x=(2001:2003), y=demo1[,2]*100,
    lwd=3, xlab = "Year" )



From sundar.dorai-raj at pdf.com  Thu Oct 16 16:45:14 2003
From: sundar.dorai-raj at pdf.com (Sundar Dorai-Raj)
Date: Thu, 16 Oct 2003 09:45:14 -0500
Subject: [R] .Random.seed
In-Reply-To: <Pine.GSO.4.58.0310160808570.12394@liberator.stat.colostate.edu>
References: <Pine.GSO.4.58.0310160808570.12394@liberator.stat.colostate.edu>
Message-ID: <3F8EAEFA.5070407@pdf.com>

This is a scoping issue. I think what you want in your example is

test <-  function(runs, seed) {
   # this is a local copy of ".Random.seed"
   .Random.seed <- seed
   # need to make a global copy
   assign(".Random.seed", .Random.seed, 1)
   for (i in 1:runs) {
     print(i)
     print(runif(1,0,1))
     print(.Random.seed)
   }
   return(.Random.seed)
}

RNGkind(kind="Marsaglia-Multicarry")
set.seed(20391)
seed1 <- .Random.seed
seed1
test(2, seed1)
test(2, seed1)


Hope this helps,

-sundar

Ann Hess wrote:
> I am writing a function for the purposes of a simulation.  Due to memory
> problems, the function sometimes crashes.  In order to get around this
> problem, I would like to include to be able to save the "last" seed, so I
> can pick up with the next run of the simulation after a "crash".  I am
> having trouble understanding what is going on with .Random.seed!
> 
> For each run of the following function, a random uniform and the current
> .Random.seed should be printed:
> 
> test<-function(runs,seed){
>    .Random.seed<-seed
>    for (i in 1:runs) {
> print(i)
> print(runif(1,0,1))
> print(.Random.seed)}
>    return(.Random.seed}
> 
> Consider the following input/output:
> 
>>RNGkind(kind="Marsaglia-Multicarry")
>>set.seed(20391)
>>seed1<-.Random.seed
>>seed1
> 
> [1]         401 -1607331462  -462081869
> 
>>test(2,seed1)
> 
> [1] 1
> [1] 0.4188851
> [1]         401 -1607331462  -462081869
> [1] 2
> [1] 0.7713649
> [1]         401 -1607331462  -462081869
> [1]         401 -1607331462  -462081869
> 
>>seed1
> 
> [1]         401 -1607331462  -462081869
> 
>>test(2,seed1)
> 
> [1] 1
> [1] 0.7293294
> [1]         401 -1607331462  -462081869
> [1] 2
> [1] 0.8266798
> [1]         401 -1607331462  -462081869
> [1]         401 -1607331462  -462081869
> 
> 
> The output from each call of the function seems to suggest that
> .Random.seed is not changing (although different random uniforms are
> generated each time).  The second call of the function doesn't match the
> first call even though the same "seed" is used.
> 
> Can anyone explain what is happening here?  My goal is to save the "last"
> seed so that I can use it to generate the next run of a simulation (after
> a crash).
> 
> Thanks in advance!
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help



From hec.villafuerte at telgua.com.gt  Thu Oct 16 18:51:20 2003
From: hec.villafuerte at telgua.com.gt (=?ISO-8859-1?Q?=22H=E9ctor_Villafuerte_D=2E=22?=)
Date: Thu, 16 Oct 2003 08:51:20 -0800
Subject: [R] Problems Building RMySQL in Windows
In-Reply-To: <m23cdttsbu.fsf@ncl.ac.uk>
References: <Pine.LNX.4.44.0310151843190.1201-100000@gannet.stats>
	<3F8DAB04.9090404@telgua.com.gt> <m23cdttsbu.fsf@ncl.ac.uk>
Message-ID: <3F8ECC88.6000404@telgua.com.gt>

David Whiting wrote:

>Can you use RODBC instead?  I use it all the time and find it works
>very well.  I installed it some time ago and have forgotten the exact
>details but I remember that it was easy to do following the
>instructions.  I also can't remember why I choose RODBC instead of
>RMySQL and whether there are advantages of RMySQL over RODBC.
>  
>

Ok, I think I'll try RODBC. For what I have read of ODBC's I pressume 
RODBC will be
easier to use, but slower (since it is a higher layer of abstraction... 
or something like that).
I'll let you know if I succeed (to be honest, I'll let you know if I 
fail too!)
Thanks,
Hector



From david_m_potter at groton.pfizer.com  Thu Oct 16 17:55:29 2003
From: david_m_potter at groton.pfizer.com (Potter, David M)
Date: Thu, 16 Oct 2003 11:55:29 -0400
Subject: [R] retrieving coefficients from nls() when it fails to converge
Message-ID: <F3BC043CCE69D611A8BC0002A58F0A7005BA77AF@groexmb06.pfizer.com>

Hi --

I am fitting four-parameter logistic regression models using nls(),
typically within the body of a loop.  Sometimes a fit fails to converge and
yet I would like to pull off the coefficients from the last iteration and
use them to compute backfitted (i.e., inverse prediction) values.

Note: To ignore the fit completely, but have the loop continue, I have
successfully used try(), but in this case the fits are sufficient for
getting backfitted values (even though the coefficients are poorly
estimated), so I need the actual coefficients.

R information:

platform i386-pc-mingw32
arch     i386           
os       mingw32        
system   i386, mingw32  
status                  
major    1              
minor    7.1            
year     2003           
month    06             
day      16             
language R  

Thanks for your help.

David

__
David M. Potter
Groton Nonclinical Statistics
Pfizer Global Research and Development



LEGAL NOTICE\ Unless expressly stated otherwise, this messag...{{dropped}}



From nortonsm at verizon.net  Thu Oct 16 18:02:07 2003
From: nortonsm at verizon.net (Scott Norton)
Date: Thu, 16 Oct 2003 12:02:07 -0400
Subject: [R] returning dynamic variable names from function
Message-ID: <000001c393fe$da145e10$6501a8c0@scott>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20031016/f46cea54/attachment.pl

From root at cdv.dhs.org  Thu Oct 16 18:10:46 2003
From: root at cdv.dhs.org (root@cdv.dhs.org)
Date: Thu, 16 Oct 2003 19:10:46 +0300
Subject: [R] Calling R from C/C++
Message-ID: <200310161910.46436.root@cdv.dhs.org>

Hi !
I'm totally strange to R. And started downloading/reading dox because my gf. 
is using it in her university project. 
Now the problem sounds like this:
I need to make a module/lib that will allow to call easaly R code/functions 
from C++ (C++ Builder 6). Is it possible without using any intermediate 
things like R-DCOM ?

Please help !

P.S.
Sorry ... no time for RTFM. Any idea/hint is apreciated !



From hec.villafuerte at telgua.com.gt  Thu Oct 16 20:18:30 2003
From: hec.villafuerte at telgua.com.gt (=?ISO-8859-1?Q?=22H=E9ctor_Villafuerte_D=2E=22?=)
Date: Thu, 16 Oct 2003 10:18:30 -0800
Subject: [R] Problems Building RMySQL in Windows
In-Reply-To: <m23cdttsbu.fsf@ncl.ac.uk>
References: <Pine.LNX.4.44.0310151843190.1201-100000@gannet.stats>
	<3F8DAB04.9090404@telgua.com.gt> <m23cdttsbu.fsf@ncl.ac.uk>
Message-ID: <3F8EE0F6.5020203@telgua.com.gt>

David Whiting wrote:

>Can you use RODBC instead?  I use it all the time and find it works
>very well.  I installed it some time ago and have forgotten the exact
>details but I remember that it was easy to do following the
>instructions.  I also can't remember why I choose RODBC instead of
>RMySQL and whether there are advantages of RMySQL over RODBC.
>  
>

Great! Using RODBC is really easy!
Would someone please comment on the pros and cons of RODBC compared with 
RMySQL?
Thanks in advance.
Hector



From B.Rowlingson at lancaster.ac.uk  Thu Oct 16 18:32:39 2003
From: B.Rowlingson at lancaster.ac.uk (Barry Rowlingson)
Date: Thu, 16 Oct 2003 17:32:39 +0100
Subject: [R] Calling R from C/C++
In-Reply-To: <200310161910.46436.root@cdv.dhs.org>
References: <200310161910.46436.root@cdv.dhs.org>
Message-ID: <3F8EC827.2030701@lancaster.ac.uk>

root at cdv.dhs.org wrote:

> Now the problem sounds like this:
> I need to make a module/lib that will allow to call easaly R code/functions 
> from C++ (C++ Builder 6). Is it possible without using any intermediate 
> things like R-DCOM ?

> P.S.
> Sorry ... no time for RTFM. Any idea/hint is apreciated !

  This is all documented in TFM. Those who WTFM dont want to have to 
WTFM again on the mailing list. RTFM.

  Here, I'll save you a search and point you to the PDF version of 
'Writing R extensions':

http://cran.r-project.org/doc/manuals/R-exts.pdf

  Other manuals may be needed - for example R may not like your C++ 
compiler. www.r-project.org is your friend.

Baz



From rpeng at jhsph.edu  Thu Oct 16 18:31:48 2003
From: rpeng at jhsph.edu (Roger D. Peng)
Date: Thu, 16 Oct 2003 12:31:48 -0400
Subject: [R] returning dynamic variable names from function
In-Reply-To: <000001c393fe$da145e10$6501a8c0@scott>
References: <000001c393fe$da145e10$6501a8c0@scott>
Message-ID: <3F8EC7F4.7000804@jhsph.edu>

One thing you could do the assign() directly into the global workspace, i.e.

assign(dfNames[[i]],ReadFileCreateDF(fullFileNames[[i]]), envir = globalenv())

-roger



Scott Norton wrote:

>Within a function I'm assigning dynamic variable names and values to them
>using the "assign" function.  I want to pass back the results but am
>uncertain how to do this.  
>
> 
>
>Basically, my function reads a number of data files and uses the filename of
>each file as the variable name for a list-to-become-dataframe.  I want then
>to pass all these lists back, but again, the names of the lists to pass back
>have been "assign-ed" to the filename.  See code snippet below, especially
>the "*** WHAT DO I PUT HERE ***" part! 
>
> 
>
>Thanks in advance for any help!!!
>
>
>
>-Scott
>
> 
>
> 
>
>CODE SNIPPET BELOW:
>
> 
>
>loadFiles <- function()
>
>{
>
> 
>
> fullFileNames <- choose.files(filters = c("Fluor file (*.data,*.Dat)",
>"*.dat;*.Dat"))
>
> numFiles <- length(fullFileNames)
>
> fileNames <- basename(fullFileNames)   # removes the all the paths from the
>full filenames
>
> splitNames <- strsplit(fileNames,"_")    # create a "list" of strings
>separated by the "_" character
>
> 
>
> dfNames <- sapply(splitNames,"[",1)
>
> # or I could use  "sapply(splitNames, function (x) x[1])"   OR EVEN
>"unlist(lapply(splitNames, function(x)x[1]))"   -- [Thanks to Andy Liaw,
>Simon Blomberg, Gabor Grothendieck, 
>
> #James Holtman and Robert Keefe for their helpful responses on this
>question - See posting on returning the first elements of a list of vectors]
>
> # 
>
> 
>
> for (i in 1:numFiles)
>
>     {
>
>     assign(dfNames[[i]],ReadFileCreateDF(fullFileNames[[i]]))
># we use "assign" since we want the quoted string in dfNames to be assigned
>the value of the function call
>
>               # the ReadFileCreateList is a function that reads an
>individual file and sends back a number of parameters in a list
>
>     }
>
> return(list( *** WHAT DO I PUT HERE***))
>
> }
>
> 
>
> 
>
>Scott Norton, Ph.D.
>
>Engineering Manager
>
>Nanoplex Technologies, Inc.
>
>2375 Garcia Ave.
>
>Mountain View, CA 94043
>
>www.nanoplextech.com
>
> 
>
>
>	[[alternative HTML version deleted]]
>
>______________________________________________
>R-help at stat.math.ethz.ch mailing list
>https://www.stat.math.ethz.ch/mailman/listinfo/r-help
>
>  
>



From ripley at stats.ox.ac.uk  Thu Oct 16 18:50:31 2003
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Thu, 16 Oct 2003 17:50:31 +0100 (BST)
Subject: [R] .Random.seed
In-Reply-To: <Pine.GSO.4.58.0310160808570.12394@liberator.stat.colostate.edu>
Message-ID: <Pine.LNX.4.44.0310161747470.8659-100000@gannet.stats>

>From the NEWS for 1.8.0

    o   .Random.seed is only looked for and stored in the user's
        workspace.  Previously the first place a variable of that name
        was found on the search path was used.

Note that a local variable in a function was never used, so you are 
printing out a local variable .Random.seed which had nothing to do with 
random-number generation.

You need to save the value of .Random.seed from .GlobalEnv.


On Thu, 16 Oct 2003, Ann Hess wrote:

> I am writing a function for the purposes of a simulation.  Due to memory
> problems, the function sometimes crashes.  In order to get around this
> problem, I would like to include to be able to save the "last" seed, so I
> can pick up with the next run of the simulation after a "crash".  I am
> having trouble understanding what is going on with .Random.seed!
> 
> For each run of the following function, a random uniform and the current
> .Random.seed should be printed:
> 
> test<-function(runs,seed){
>    .Random.seed<-seed
>    for (i in 1:runs) {
> print(i)
> print(runif(1,0,1))
> print(.Random.seed)}
>    return(.Random.seed}
> 
> Consider the following input/output:
> >RNGkind(kind="Marsaglia-Multicarry")
> > set.seed(20391)
> > seed1<-.Random.seed
> > seed1
> [1]         401 -1607331462  -462081869
> > test(2,seed1)
> [1] 1
> [1] 0.4188851
> [1]         401 -1607331462  -462081869
> [1] 2
> [1] 0.7713649
> [1]         401 -1607331462  -462081869
> [1]         401 -1607331462  -462081869
> > seed1
> [1]         401 -1607331462  -462081869
> > test(2,seed1)
> [1] 1
> [1] 0.7293294
> [1]         401 -1607331462  -462081869
> [1] 2
> [1] 0.8266798
> [1]         401 -1607331462  -462081869
> [1]         401 -1607331462  -462081869
> 
> 
> The output from each call of the function seems to suggest that
> .Random.seed is not changing (although different random uniforms are
> generated each time).  The second call of the function doesn't match the
> first call even though the same "seed" is used.
> 
> Can anyone explain what is happening here?  My goal is to save the "last"
> seed so that I can use it to generate the next run of a simulation (after
> a crash).
> 
> Thanks in advance!
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> 
> 

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From christoph.bier at web.de  Thu Oct 16 19:02:25 2003
From: christoph.bier at web.de (Christoph Bier)
Date: Thu, 16 Oct 2003 19:02:25 +0200
Subject: AW: [R] summary with names
In-Reply-To: <87F1BE4E9ED108429DB4C9B9838356219B31@samos.mlab.iao.fhg.de>
References: <87F1BE4E9ED108429DB4C9B9838356219B31@samos.mlab.iao.fhg.de>
Message-ID: <3F8ECF21.60101@web.de>

As I read this, my answers to the other guys don't seem to be 
delievered to the list, although I sent them nearly 4 hours 
ago. I fear, I only sent them to the posters and not to the 
list ... I try to correct this! Sorry!

Axel Benz schrieb:
> Which name do you want?
> varA1

This one resp. the corresponding one in this way (varA2 etc.).

> or
> fb.12.unt[varA1]
> or even then value of varA1????

This question tells me, that you know more than me ;-). Who 
wonders ...? There are 98 values for varA1. Did I use wrong 
terms, that 98 values are surprisingly?

> Your question looks like you made a mistake in your code (just a
> suggestion, of course I can be wrong (that's on thing why I don't like R
> too much: You can more or less write what you want, It's allway correct,
> even if it makes no semantical sense at all)).

Hm, how can I enlighten this? There's a data.frame (fb.12.unt) 
with 302 variables (columns) and 98 values for each (rows). 
Sorry, if I use the wrong terms!! The data.frame was imported 
from SPSS-Data. Some of these variables belong together 
(altogether ~250) and for those I want summaries in extra files.

> What is: fb.12.unt[varA1:varZ9]
> You treat it like a vector, but it is already indexed by a vector, which
> is a sequence from the 
> value of varA1 to the value of varZ9.

Ok, here I has to capitulate ;-/.
    A vector is a datastructure, that concatenates several 
objects of the same kind to one object, e.g. a variable?!?
A data frame allows the combination of different data types in 
a matrix, right? Whereas a matrix has a rectangular structure 
with vectors resp. variables in columns?!
    With this understanding "fb.12.unt" is a data frame and 
"varA1:varZ9" is a sequence of vectors resp. variables. These 
vectors consist of 98 single values resulting in 98 rows of 
the data frame.
    So, "fb.12.unt[varA1]" is a vector, that I want to be 
summarised. And I want "fb.12.unt[varA2]" etc. summarised, too.
    Hoo, I hope I could clarify a little bit =).

> This is really complicated and hard to follow! That's why I believe it's
> not what you want.

After some investigation, I have to say, that you may be right 
(but don't give too much on this statement, because I'm quite 
confused at the moment). What I want is a summary of each of 
the variables/columns. I could also do

 > attach(fb.12.unt)
 > summary(varA1)    # varA1 is a variable from fb.12.unt

for each variable (varA2 etc.) or

 > detach(fb.12.unt)
 > summary(fb.12.unt["varA1"])

for each variable.

> What ist fb.12.unt?
> If it is a data.frame:
> for (i in colnames(fb.12.unt)){print i;...}

I couldn't check this code, because I don't know how to 
replace the ellipsis ("...").

> If it is a vector:
> for (i in 1:length(fb.12.unt)){print i;...}
> If it is a matrix, it could be:
> for (i in 1:ncol(fb.12.unt)){print i;...}
> (or nrow instead of ncol)
> 
> By the way: The only reason NOT to use this relatively easy-to-read
> iterations is performance and with 250 loops this reason does not count.

Ok. I thought, it wasn't "elegant", too =).

[...]

Thanks for your answer, that was first confusing and then 
therefore clarifying. I hope /my/ answer isn't too confusing.

Best regards,

Christoph
-- 
Christoph Bier, Dipl.Oecotroph., Email: bier at wiz.uni-kassel.de
Universitaet Kassel, FG Oekologische Lebensmittelqualitaet und
Ernaehrungskultur \\ Postfach 12 52 \\ 37202 Witzenhausen
Tel.: +49 (0) 55 42 / 98 -17 21, Fax: -17 13



From christoph.bier at web.de  Thu Oct 16 19:12:25 2003
From: christoph.bier at web.de (Christoph Bier)
Date: Thu, 16 Oct 2003 19:12:25 +0200
Subject: [R] summary with names
In-Reply-To: <20031016115641.GM1969@pasteur-kh.org>
References: <3F8E8189.2020808@web.de> <20031016115641.GM1969@pasteur-kh.org>
Message-ID: <3F8ED179.7010807@web.de>

Philippe Glaziou schrieb:

[...]

> Can you give us an example of summary not giving variable names?

I could send you my data frame, but that's surely not what you
want =). Your following example prints the value names on my
machine, too. But my data frame does not. And the summary
isn't printed among one another but abreast. Sorry, can't tell
you why. But I will try to create a simple example.

[example]

Regards,

Christoph
-- 
Christoph Bier, Dipl.Oecotroph., Email: bier at wiz.uni-kassel.de
Universitaet Kassel, FG Oekologische Lebensmittelqualitaet und
Ernaehrungskultur \\ Postfach 12 52 \\ 37202 Witzenhausen
Tel.: +49 (0) 55 42 / 98 -17 21, Fax: -17 13



From christoph.bier at web.de  Thu Oct 16 19:12:46 2003
From: christoph.bier at web.de (Christoph Bier)
Date: Thu, 16 Oct 2003 19:12:46 +0200
Subject: [R] summary with names
In-Reply-To: <3F8E86C1.2070305@pburns.seanet.com>
References: <3F8E8189.2020808@web.de> <3F8E86C1.2070305@pburns.seanet.com>
Message-ID: <3F8ED18E.60704@web.de>

Patrick Burns schrieb:
> I think you mean na.rm=T by the way.

Yes, you're right!

> Is
> 
> for(i in whatever) {cat(i, "\n"); print(summary(i, na.rm=TRUE))}
> 
> what you want?

No, it doesn't print the value names. I can't tell you why.
Philippe wonders, too. In his example the value names are
printed. But not with my data frame.

Regards,

Christoph
-- 
Christoph Bier, Dipl.Oecotroph., Email: bier at wiz.uni-kassel.de
Universitaet Kassel, FG Oekologische Lebensmittelqualitaet und
Ernaehrungskultur \\ Postfach 12 52 \\ 37202 Witzenhausen
Tel.: +49 (0) 55 42 / 98 -17 21, Fax: -17 13



From christoph.bier at web.de  Thu Oct 16 19:13:04 2003
From: christoph.bier at web.de (Christoph Bier)
Date: Thu, 16 Oct 2003 19:13:04 +0200
Subject: [R] summary with names
In-Reply-To: <3F8E86A4.2020007@statistik.uni-dortmund.de>
References: <3F8E8189.2020808@web.de>
	<3F8E86A4.2020007@statistik.uni-dortmund.de>
Message-ID: <3F8ED1A0.7090605@web.de>

Uwe Ligges schrieb:
> Christoph Bier wrote:
> 
>> Hi,
>>
>> how can I tell 'summary' to print the name of the summarised variable? 
>> This is probably an awkward newbie question but I didn't find an 
>> answer in the Docus, the FAQ and maillist archive.
>>    I want a summary for about 250 variables and realise it the 
>> following way (I know, that I shouldn't use iterations that way in R; 
>> but at the moment it's the easiest way for me):
>>
>> for(i in fb.12.unt[varA1:varZ9]){print (summary(i, na.rm=t))}
> 
> 
> Given fb.12.unt is a list or data.frame,
>   lapply(fb.12.unt, summary, na.rm = TRUE)
> or
>   sapply(fb.12.unt, summary, na.rm = TRUE)
> might do what you want.

Yes, it does :-)! Thanks a lot.

> BTW: "na.rm=t" is wrong anyway ...

Sure, actually I used "na.rm=T" -- but not "TRUE". My alleged
knowledge about this is based on a book about S and S-Plus.
I'm still waiting for "An Introductory in R" as I wrote in a
mail some days ago. I can not read Docus on a monitor, I need
paper in my hand =).

Best regards,

Christoph
-- 
Christoph Bier, Dipl.Oecotroph., Email: bier at wiz.uni-kassel.de
Universitaet Kassel, FG Oekologische Lebensmittelqualitaet und
Ernaehrungskultur \\ Postfach 12 52 \\ 37202 Witzenhausen
Tel.: +49 (0) 55 42 / 98 -17 21, Fax: -17 13



From spencer.graves at pdf.com  Thu Oct 16 19:33:16 2003
From: spencer.graves at pdf.com (Spencer Graves)
Date: Thu, 16 Oct 2003 10:33:16 -0700
Subject: [R] retrieving coefficients from nls() when it fails to converge
In-Reply-To: <F3BC043CCE69D611A8BC0002A58F0A7005BA77AF@groexmb06.pfizer.com>
References: <F3BC043CCE69D611A8BC0002A58F0A7005BA77AF@groexmb06.pfizer.com>
Message-ID: <3F8ED65C.4090505@pdf.com>

      Have you considered "optim"?  That has produced something for me 
when I got nothing from "nls".  You may wish to call "optim" a second 
time with the output from the first;  you might actually get different 
answers.  Please specify "hessian=TRUE", as follows: 

      fit0 <- optim(..., hessian=TRUE)

      Then "eigen(fit$hessian, symmetric=TRUE)" may tell you why "nls" 
did not converge. 

      The standard advice in such cases is to reconsider what you really 
want and then fewer parameters and / or reparameterize;  see e.g., Bates 
and Watts (1988) Nonlinear Regression Analysis and Its Applications 
(Wiley). 

      hope this helps.  spencer graves

Potter, David M wrote:

>Hi --
>
>I am fitting four-parameter logistic regression models using nls(),
>typically within the body of a loop.  Sometimes a fit fails to converge and
>yet I would like to pull off the coefficients from the last iteration and
>use them to compute backfitted (i.e., inverse prediction) values.
>
>Note: To ignore the fit completely, but have the loop continue, I have
>successfully used try(), but in this case the fits are sufficient for
>getting backfitted values (even though the coefficients are poorly
>estimated), so I need the actual coefficients.
>
>R information:
>
>platform i386-pc-mingw32
>arch     i386           
>os       mingw32        
>system   i386, mingw32  
>status                  
>major    1              
>minor    7.1            
>year     2003           
>month    06             
>day      16             
>language R  
>
>Thanks for your help.
>
>David
>
>__
>David M. Potter
>Groton Nonclinical Statistics
>Pfizer Global Research and Development
>
>
>
>LEGAL NOTICE\ Unless expressly stated otherwise, this messag...{{dropped}}
>
>______________________________________________
>R-help at stat.math.ethz.ch mailing list
>https://www.stat.math.ethz.ch/mailman/listinfo/r-help
>  
>



From spencer.graves at pdf.com  Thu Oct 16 19:34:51 2003
From: spencer.graves at pdf.com (Spencer Graves)
Date: Thu, 16 Oct 2003 10:34:51 -0700
Subject: [R] princomp with more coloumns than rows: why not?
In-Reply-To: <3A822319EB35174CA3714066D590DCD50205CCB6@usrymx25.merck.com>
References: <3A822319EB35174CA3714066D590DCD50205CCB6@usrymx25.merck.com>
Message-ID: <3F8ED6BB.9090007@pdf.com>

Have you considered "svd"? 

hope this helps.  spencer graves

Liaw, Andy wrote:

>In the `Detail' section of ?princomp:
>
>princomp only handles so-called Q-mode PCA, that is feature extraction of
>variables. If a data matrix is supplied (possibly via a formula) it is
>required that there are at least as many units as variables. For R-mode PCA
>use prcomp. 
>
>
>Andy
>
>  
>
>>-----Original Message-----
>>From: bhx2 at mevik.net [mailto:bhx2 at mevik.net] 
>>Sent: Thursday, October 16, 2003 5:14 AM
>>To: r-help at stat.math.ethz.ch
>>Subject: [R] princomp with more coloumns than rows: why not?
>>
>>
>>As of R 1.7.0, princomp no longer accept matrices with more 
>>coloumns than rows.  I'm curious:  Why was this decision made?
>>
>>I work a lot with data where more coloumns than rows is more 
>>of a rule than an exception (for instance spectroscopic 
>>data).  To me, princomp have two advantages above prcomp: 1) 
>>It has a predict method, and 2) it has a biplot method.
>>
>>A biplot method shouldn't be too difficult to implement (I 
>>believe I've seen one on R-help).
>>
>>A predict method seems to be more difficult, because the 
>>prcomp object doesn't include the means that need to be 
>>subtracted from the new data.  Would it break conformance 
>>with S to let prcomp return the means as well?
>>
>>-- 
>>Sincerely,
>>Bj?rn-Helge Mevik
>>
>>______________________________________________
>>R-help at stat.math.ethz.ch mailing list 
>>https://www.stat.math.ethz.ch/mailman/listinfo> /r-help
>>
>>    
>>
>
>______________________________________________
>R-help at stat.math.ethz.ch mailing list
>https://www.stat.math.ethz.ch/mailman/listinfo/r-help
>  
>



From tlumley at u.washington.edu  Thu Oct 16 20:17:38 2003
From: tlumley at u.washington.edu (Thomas Lumley)
Date: Thu, 16 Oct 2003 11:17:38 -0700 (PDT)
Subject: [R] returning dynamic variable names from function
In-Reply-To: <000001c393fe$da145e10$6501a8c0@scott>
References: <000001c393fe$da145e10$6501a8c0@scott>
Message-ID: <Pine.A41.4.58.0310161106490.78616@homer24.u.washington.edu>

On Thu, 16 Oct 2003, Scott Norton wrote:

> Within a function I'm assigning dynamic variable names and values to them
> using the "assign" function.  I want to pass back the results but am
> uncertain how to do this.
>
>
>
> Basically, my function reads a number of data files and uses the filename of
> each file as the variable name for a list-to-become-dataframe.  I want then
> to pass all these lists back, but again, the names of the lists to pass back
> have been "assign-ed" to the filename.  See code snippet below, especially
> the "*** WHAT DO I PUT HERE ***" part!


This looks like a compromise between two strategies, either of which would
work on its own

1/ return a list of dataframes with tags based on the filenames
2/ create new variables with names based on the filenames.

The former approach is easy. The latter requires assigning into a
non-local environment, which is normally considered harmful but here is
the whole point of the command (as with R's load() function). You might
want to specify where the variables are to be created (as load() does).
I've written it below so that the default location is the calling
environment, and you would use loadFiles2(where=.GlobalEnv) to load into
the global environment.


So either

loadFiles1 <- function() {

  fullFileNames <- choose.files(filters = c("Fluor file (*.data,*.Dat)",
 "*.dat;*.Dat"))

  numFiles <- length(fullFileNames)

  fileNames <- basename(fullFileNames)
  splitNames <- strsplit(fileNames,"_")
  dfNames <- sapply(splitNames,"[",1)

  rval <-lapply(fullFileNames, ReadFileCreateDF)
  names(rval) <- dfNames

  return(rval)
 }

or

 loadFiles2 <- function(where=parent.frame()) {
    fullFileNames <- choose.files(filters = c("Fluor file (*.data,*.Dat)",
   "*.dat;*.Dat"))

  numFiles <- length(fullFileNames)

  fileNames <- basename(fullFileNames)
  splitNames <- strsplit(fileNames,"_")
  dfNames <- sapply(splitNames,"[",1)

  for (i in 1:numFiles)
     assign(dfNames[i], ReadFileCreateDF[i], envir=where)

  invisible(NULL)
 }



From pjabardo at ipt.br  Thu Oct 16 19:31:44 2003
From: pjabardo at ipt.br (Paulo Jabardo)
Date: 16 Oct 2003 15:31:44 -0200
Subject: [R] Too long to open a socket connection
Message-ID: <1066325505.931.17.camel@vazao17>

Hello Guys!

I've been doing data aquisition of a pressure measurement system that
communicates using TCP/IP protocol. R's socket connection has been a
blessing for aquiring this data. I've been doing this data aquisition on
Linux (where I do most of the development) and Windows. It works fine on
both systems but there is only a slight problem (not really a problem,
something weird). When I'm running windows, it takes too long to open
the connection:

con <- socketConnection(host="191.30.5.176", port=23,
                         blocking=TRUE, open="+ab")

It takes more than 1 minute to execute this code under Windows (and I've
tested several different machines) even though when I use telnet I can
establish the connection immediately. Under Linux, this code executes
very fast.

As I said, after openning the connection the program executes just fine
so this is not more than an annoyance.
By the way in version 1.6.2, the first time I attempted to open the
connection, there was always an error problem (on the next attempts I
just had the speed issue) that was solved in versions 1.7.0, 1.7.1 and
1.8.0 but the speed issue continues the same.


I hope we can fix this problem.
Thanks guys

Paulo Jabardo
Engineer
IPT - Institute for Technological Research
Sao Paulo - BRAZIL



From jasont at indigoindustrial.co.nz  Thu Oct 16 22:22:05 2003
From: jasont at indigoindustrial.co.nz (Jason Turner)
Date: Fri, 17 Oct 2003 09:22:05 +1300
Subject: [R] Managing memory on R
In-Reply-To: <488C02265C6AD611BF200002A542182F03F2A793@irnts22.ifp.fr>
References: <488C02265C6AD611BF200002A542182F03F2A793@irnts22.ifp.fr>
Message-ID: <3F8EFDED.6080003@indigoindustrial.co.nz>

ZABALZA-MEZGHANI Isabelle wrote:

> Hi,
> 
> I have a problem with the memory size within R. I would like to know if
> there is any may for getting back free memory during a R session. I've tried
> with rm and gc but it still craches. I am working on windows R1.3.1.
> I know I have big datasets, but during my study, some results are temporary,
> and I would like to get back the memory allocated to this temporary object
> as soon as I remove them. But, as I 've said the garbage collector seems to
> have no effect on my memory management.
> 

1) Update R.  1.3.1 is over two years old.

2) What do you mean by "crashes"?  Is the behaviour the same under the 
latest version of R?

3) R can release memory, but it can't force the operating system to 
start using it again.  gc() only speeds up the release of memory by R; 
it doesn't change what the operating system does.

4) Windows is reported to have more trouble with the management of large 
memory than Unix or Unix-like systems.  Have you tried the same under 
Solaris / FreeBSD / Linux?

Hope that helps

Jason

-- 
Indigo Industrial Controls Ltd.
http://www.indigoindustrial.co.nz
64-21-343-545
jasont at indigoindustrial.co.nz



From alex_s_42 at yahoo.com  Fri Oct 17 00:59:08 2003
From: alex_s_42 at yahoo.com (Alexander Sirotkin [at Yahoo])
Date: Thu, 16 Oct 2003 15:59:08 -0700 (PDT)
Subject: [R] R memory and CPU requirements
Message-ID: <20031016225908.70750.qmail@web14910.mail.yahoo.com>

Thanks for all the help on my previous questions.

One more (hopefully last one) : I've been very
surprised when I tried to fit a model (using aov())
for a sample of size 200 and 10 variables and their
interactions. 

It turns out that even 2GB of RAM is not anough for
aov() with this sample size, which does not seem so
big for me. Am I doing something wrong or this is
considered a normal memory requirements ? 

Frankly, I just don't have an access to a machine with
more then 2GB of RAM so I'm not sure how I should
attack this problem.

10x.

P.S. When I reduced sample size to 50 2GB RAM was
enough, but aov() kept working for all night and has
not finished yet.



From GPetris at uark.edu  Fri Oct 17 01:09:55 2003
From: GPetris at uark.edu (Giovanni Petris)
Date: Thu, 16 Oct 2003 18:09:55 -0500 (CDT)
Subject: [R] Improving efficiency in "outer"-like calculation
Message-ID: <200310162309.h9GN9tDg010920@definetti.uark.edu>


Hello,

I am doing mcmc=10000 simulations from a posterior distribution of the parameters
of a mixture of K=6 normal densities.
I have mcmc by K matrices simMeans, simVars and simWeights containing
the simulation output: one row for each simulation, one column for
each normal component of the mixture. 
One thing I would like to do is a plot of the posterior predictive
density. In order to do that I define a vector x of points at which I
want to evaluate this density. And then I use the following commands:

pred <- colMeans(apply(structure(dnorm(rep(x,each=mcmc*K), mean=simMeans, sd=sqrt(simVars))*
                                 rep(simWeights,length(x)),dim=c(mcmc,K,length(x))),c(1,3),sum))
lines(x,pred)

Everything works, but it is very slow. Does anybody have suggestions
to do the same thing in a more efficient way?

Thanks in advance,
Giovanni Petris

-- 

 __________________________________________________
[                                                  ]
[ Giovanni Petris                 GPetris at uark.edu ]
[ Department of Mathematical Sciences              ]
[ University of Arkansas - Fayetteville, AR 72701  ]
[ Ph: (479) 575-6324, 575-8630 (fax)               ]
[ http://definetti.uark.edu/~gpetris/              ]
[__________________________________________________]



From deepayan at stat.wisc.edu  Fri Oct 17 00:55:58 2003
From: deepayan at stat.wisc.edu (Deepayan Sarkar)
Date: Thu, 16 Oct 2003 17:55:58 -0500
Subject: [R] R memory and CPU requirements
In-Reply-To: <20031016225908.70750.qmail@web14910.mail.yahoo.com>
References: <20031016225908.70750.qmail@web14910.mail.yahoo.com>
Message-ID: <200310161755.58905.deepayan@stat.wisc.edu>

On Thursday 16 October 2003 17:59, Alexander Sirotkin \[at Yahoo\] wrote:
> Thanks for all the help on my previous questions.
>
> One more (hopefully last one) : I've been very
> surprised when I tried to fit a model (using aov())
> for a sample of size 200 and 10 variables and their
> interactions.

That doesn't really say much. How many of these variables are factors ? How 
many levels do they have ? And what is the order of the interaction ? (Note 
that for 10 numeric variables, if you allow all interactions, then there will 
be a 100 terms in your model. This increases for factors.)

In other words, how big is your model matrix ? (See ?model.matrix)

Deepayan



From ok at cs.otago.ac.nz  Fri Oct 17 01:40:34 2003
From: ok at cs.otago.ac.nz (Richard A. O'Keefe)
Date: Fri, 17 Oct 2003 12:40:34 +1300 (NZDT)
Subject: [R] indexing a particular element in a list of vectors
Message-ID: <200310162340.h9GNeY0Y353403@atlas.otago.ac.nz>

"Scott Norton" <nortonsm at verizon.net> wrote:
	I have a "list" of character vectors.  I'm trying to see if
	there is a way (in a single line, without a loop) to pull out
	the first element of all the vectors contained in the list.
	
You have a list.
You want to do something to each element.
See ?lapply

> u <- c("Fee","fie","foe","fum")
> v <- c("Ping","pong","diplomacy")
> w <- c("Hi","fi")
> x <- list(a=u, b=v, c=w)
> lapply(x, function (cv) cv[1])
$a
[1] "Fee"

$b
[1] "Ping"

$c
[1] "Hi"

If you want the result as a character vector, see ?sapply

> sapply(x, function (cv) cv[1])
     a      b      c 
 "Fee" "Ping"   "Hi"



From alex_s_42 at yahoo.com  Fri Oct 17 02:03:15 2003
From: alex_s_42 at yahoo.com (Alexander Sirotkin [at Yahoo])
Date: Thu, 16 Oct 2003 17:03:15 -0700 (PDT)
Subject: [R] R memory and CPU requirements
In-Reply-To: <200310161755.58905.deepayan@stat.wisc.edu>
Message-ID: <20031017000315.73776.qmail@web14903.mail.yahoo.com>


--- Deepayan Sarkar <deepayan at stat.wisc.edu> wrote:
> On Thursday 16 October 2003 17:59, Alexander
> Sirotkin \[at Yahoo\] wrote:
> > Thanks for all the help on my previous questions.
> >
> > One more (hopefully last one) : I've been very
> > surprised when I tried to fit a model (using
> aov())
> > for a sample of size 200 and 10 variables and
> their
> > interactions.
> 
> That doesn't really say much. How many of these
> variables are factors ? How 
> many levels do they have ? And what is the order of
> the interaction ? (Note 
> that for 10 numeric variables, if you allow all
> interactions, then there will 
> be a 100 terms in your model. This increases for
> factors.)
> 
> In other words, how big is your model matrix ? (See
> ?model.matrix)
> 
> Deepayan
> 


I see... 

Unfortunately, model.matrix() ran out of memory :)
I have 10 variables, 6 of which are factor, 2 of which

have quite a lot of levels (about 40). And I would
like 
to allow all interactions.

I understand your point about categorical variables,
but 
still - this does not seem like too much data to me.


I remmeber fitting all kinds of models (mostly
decision 
trees) for much, much larger data sets.



From jawegelin at ucdavis.edu  Fri Oct 17 02:23:41 2003
From: jawegelin at ucdavis.edu (Jacob Wegelin)
Date: Thu, 16 Oct 2003 17:23:41 -0700 (PDT)
Subject: [R] as.matrix does not turn data frame into character matrix
Message-ID: <Pine.OSX.4.53.0310161529010.1412@biostat5.ucdavis.edu>


The as.matrix function behaves in a puzzling manner.  The help file says:
"`as.matrix' is a generic function. The method for data frames will
     convert any non-numeric column into a character vector using
     `format' and so return a character matrix."
But this does not appear to be the case in the following example. Instead,
as.matrix turns a data.frame into a list, not a character matrix, which
wreaks havoc with my old code.

junk<-
structure(list(SUBNUM = structure(c(3, 4, 5, 7, 6), class = "factor",
.Label = c("01",
"02", "03", "04", "07", "08", "09", "10", "11", "12", "13", "16",
"17", "18", "21", "22", "23", "24", "25", "26", "27", "28")),
    AGE = c(7, 7, 10, 8, 5), DIAGNOSI = c(1, 1, 1, 1, 1), cortrange =
structure(list(
        "03" = 19.0674, "04" = 40.3009, "07" = 37.0205, "09" = 8.84131,
        "08" = 10.9855), .Names = c("03", "04", "07", "09", "08"
    )), logcortrange = structure(list("03" = 1.90097866386896,
        "04" = 2.75040785570225, "07" = 3.15633470025647, "09" =
2.56744094585387,
        "08" = 2.84160608522206), .Names = c("03", "04", "07",
    "09", "08"))), .Names = c("SUBNUM", "AGE", "DIAGNOSI", "cortrange",
"logcortrange"), row.names = c("03", "04", "07", "09", "08"), class =
"data.frame")

> junk
   SUBNUM AGE DIAGNOSI cortrange logcortrange
03     03   7        1   19.0674     1.900979
04     04   7        1   40.3009     2.750408
07     07  10        1   37.0205     3.156335
09     09   8        1   8.84131     2.567441
08     08   5        1   10.9855     2.841606

> jank<-as.matrix(junk)
> jank
   SUBNUM AGE DIAGNOSI cortrange logcortrange
03 "03"   7   1        19.0674   1.900979
04 "04"   7   1        40.3009   2.750408
07 "07"   10  1        37.0205   3.156335
09 "09"   8   1        8.84131   2.567441
08 "08"   5   1        10.9855   2.841606

Notice that the first column is character, whereas the other columns are plain numeric!
This is *not* a matrix of character.

> dput(jank)
structure(list("03", "04", "07", "09", "08", 7, 7, 10, 8, 5,
    1, 1, 1, 1, 1, 19.0674, 40.3009, 37.0205, 8.84131, 10.9855,
    1.90097866386896, 2.75040785570225, 3.15633470025647, 2.56744094585387,
    2.84160608522206), .Dim = c(5, 5), .Dimnames = list(c("03",
"04", "07", "09", "08"), c("SUBNUM", "AGE", "DIAGNOSI", "cortrange",
"logcortrange")))

Is this a bug?

One result of this:

> cbind(dimnames(jank)[[1]], jank)
Error in cbind(...) : cannot create a matrix from these types

(I'm using version 7.0, because the links for downloading version 7.1 are dead today:)

> version
         _
platform i386-pc-mingw32
arch     i386
os       mingw32
system   i386, mingw32
status
major    1
minor    7.0
year     2003
month    04
day      16
language R

Thanks for any information.

Jake



From johannesson1 at llnl.gov  Fri Oct 17 03:10:28 2003
From: johannesson1 at llnl.gov (Gardar Johannesson)
Date: Thu, 16 Oct 2003 18:10:28 -0700
Subject: [R] Improving efficiency in "outer"-like calculation
In-Reply-To: <200310162309.h9GN9tDg010920@definetti.uark.edu>
Message-ID: <5.0.0.25.2.20031016180213.00ae2838@poptop.llnl.gov>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20031016/4a3bfa0c/attachment.pl

From jfox at mcmaster.ca  Fri Oct 17 04:04:10 2003
From: jfox at mcmaster.ca (John Fox)
Date: Thu, 16 Oct 2003 22:04:10 -0400
Subject: [R] R memory and CPU requirements
In-Reply-To: <20031017000315.73776.qmail@web14903.mail.yahoo.com>
References: <200310161755.58905.deepayan@stat.wisc.edu>
Message-ID: <5.1.0.14.2.20031016220153.01fb92b0@127.0.0.1>

Dear Alexander,

If I understand you correctly, you have a sample of 200 observations. Even 
if you had only two factors with 40 levels each, the main effects and 
interactions of these factors would require about 1600 degrees of freedom 
-- that is, more than the number of observations. This doesn't make a whole 
lot of sense.

I hope that this helps,
  John

At 05:03 PM 10/16/2003 -0700, Alexander Sirotkin \[at Yahoo\] wrote:

>--- Deepayan Sarkar <deepayan at stat.wisc.edu> wrote:
> > On Thursday 16 October 2003 17:59, Alexander
> > Sirotkin \[at Yahoo\] wrote:
> > > Thanks for all the help on my previous questions.
> > >
> > > One more (hopefully last one) : I've been very
> > > surprised when I tried to fit a model (using
> > aov())
> > > for a sample of size 200 and 10 variables and
> > their
> > > interactions.
> >
> > That doesn't really say much. How many of these
> > variables are factors ? How
> > many levels do they have ? And what is the order of
> > the interaction ? (Note
> > that for 10 numeric variables, if you allow all
> > interactions, then there will
> > be a 100 terms in your model. This increases for
> > factors.)
> >
> > In other words, how big is your model matrix ? (See
> > ?model.matrix)
> >
> > Deepayan
> >
>
>
>I see...
>
>Unfortunately, model.matrix() ran out of memory :)
>I have 10 variables, 6 of which are factor, 2 of which
>
>have quite a lot of levels (about 40). And I would
>like
>to allow all interactions.
>
>I understand your point about categorical variables,
>but
>still - this does not seem like too much data to me.
>
>
>I remmeber fitting all kinds of models (mostly
>decision
>trees) for much, much larger data sets.
>
>______________________________________________
>R-help at stat.math.ethz.ch mailing list
>https://www.stat.math.ethz.ch/mailman/listinfo/r-help

-----------------------------------------------------
John Fox
Department of Sociology
McMaster University
Hamilton, Ontario, Canada L8S 4M4
email: jfox at mcmaster.ca
phone: 905-525-9140x23604
web: www.socsci.mcmaster.ca/jfox



From adorio at math.upd.edu.ph  Fri Oct 17 22:08:30 2003
From: adorio at math.upd.edu.ph (Ernie Adorio)
Date: Sat, 18 Oct 2003 04:08:30 +0800
Subject: [R] don't display rulers in image() command and script file input
Message-ID: <200310180408.30755.adorio@math.upd.edu.ph>

Dear R experts,

1. How can I turn off the display of rulers in image() command?

2. Rather than typing my commands at the command line, how can I  input a file 
contents aside from doing a copy and paste operation?

Thanks in advance,

Ernesto Adorio
Math Department
University of the Philippines
Diliman



From ligges at statistik.uni-dortmund.de  Fri Oct 17 08:34:09 2003
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Fri, 17 Oct 2003 08:34:09 +0200
Subject: [R] don't display rulers in image() command and script file input
In-Reply-To: <200310180408.30755.adorio@math.upd.edu.ph>
References: <200310180408.30755.adorio@math.upd.edu.ph>
Message-ID: <3F8F8D61.4060209@statistik.uni-dortmund.de>

Ernie Adorio wrote:
> Dear R experts,
> 
> 1. How can I turn off the display of rulers in image() command?

image(..., axes = FALSE)


> 2. Rather than typing my commands at the command line, how can I  input a file 
> contents aside from doing a copy and paste operation?

source()


Uwe Ligges

> Thanks in advance,
> 
> Ernesto Adorio
> Math Department
> University of the Philippines
> Diliman
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help



From ripley at stats.ox.ac.uk  Fri Oct 17 08:45:22 2003
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Fri, 17 Oct 2003 07:45:22 +0100 (BST)
Subject: [R] as.matrix does not turn data frame into character matrix
In-Reply-To: <Pine.OSX.4.53.0310161529010.1412@biostat5.ucdavis.edu>
Message-ID: <Pine.LNX.4.44.0310170732520.9699-100000@gannet.stats>

At least in R 1.8.0 it is a list matrix:

> class(jank)
[1] "matrix"
> typeof(jank)
[1] "list"

The help page is not quite correct, as it does not mention what happens 
when you create a data frame with a *list* as a column.

How did you create this data frame?  Columns cortrange and logcortrange
are surely intended to be numeric columns, and it is pretty moot point if
it is a valid data frame (it should not be possible to create it with
data.frame, for example, and do.call("data.frame", unclass(junk)) fails).

Please note that R 1.8.0 is current: we are nowhere near` 7.0 or `7.0'.
A lot of error-checking/correction in the data.frame area was added in 
1.8.0.


On Thu, 16 Oct 2003, Jacob Wegelin wrote:

> 
> The as.matrix function behaves in a puzzling manner.  The help file says:
> "`as.matrix' is a generic function. The method for data frames will
>      convert any non-numeric column into a character vector using
>      `format' and so return a character matrix."
> But this does not appear to be the case in the following example. Instead,
> as.matrix turns a data.frame into a list, not a character matrix, which
> wreaks havoc with my old code.
> 
> junk<-
> structure(list(SUBNUM = structure(c(3, 4, 5, 7, 6), class = "factor",
> .Label = c("01",
> "02", "03", "04", "07", "08", "09", "10", "11", "12", "13", "16",
> "17", "18", "21", "22", "23", "24", "25", "26", "27", "28")),
>     AGE = c(7, 7, 10, 8, 5), DIAGNOSI = c(1, 1, 1, 1, 1), cortrange =
> structure(list(
>         "03" = 19.0674, "04" = 40.3009, "07" = 37.0205, "09" = 8.84131,
>         "08" = 10.9855), .Names = c("03", "04", "07", "09", "08"
>     )), logcortrange = structure(list("03" = 1.90097866386896,
>         "04" = 2.75040785570225, "07" = 3.15633470025647, "09" =
> 2.56744094585387,
>         "08" = 2.84160608522206), .Names = c("03", "04", "07",
>     "09", "08"))), .Names = c("SUBNUM", "AGE", "DIAGNOSI", "cortrange",
> "logcortrange"), row.names = c("03", "04", "07", "09", "08"), class =
> "data.frame")
> 
> > junk
>    SUBNUM AGE DIAGNOSI cortrange logcortrange
> 03     03   7        1   19.0674     1.900979
> 04     04   7        1   40.3009     2.750408
> 07     07  10        1   37.0205     3.156335
> 09     09   8        1   8.84131     2.567441
> 08     08   5        1   10.9855     2.841606
> 
> > jank<-as.matrix(junk)
> > jank
>    SUBNUM AGE DIAGNOSI cortrange logcortrange
> 03 "03"   7   1        19.0674   1.900979
> 04 "04"   7   1        40.3009   2.750408
> 07 "07"   10  1        37.0205   3.156335
> 09 "09"   8   1        8.84131   2.567441
> 08 "08"   5   1        10.9855   2.841606
> 
> Notice that the first column is character, whereas the other columns are plain numeric!
> This is *not* a matrix of character.
> 
> > dput(jank)
> structure(list("03", "04", "07", "09", "08", 7, 7, 10, 8, 5,
>     1, 1, 1, 1, 1, 19.0674, 40.3009, 37.0205, 8.84131, 10.9855,
>     1.90097866386896, 2.75040785570225, 3.15633470025647, 2.56744094585387,
>     2.84160608522206), .Dim = c(5, 5), .Dimnames = list(c("03",
> "04", "07", "09", "08"), c("SUBNUM", "AGE", "DIAGNOSI", "cortrange",
> "logcortrange")))
> 
> Is this a bug?
> 
> One result of this:
> 
> > cbind(dimnames(jank)[[1]], jank)
> Error in cbind(...) : cannot create a matrix from these types
> 
> (I'm using version 7.0, because the links for downloading version 7.1 are dead today:)
> 
> > version
>          _
> platform i386-pc-mingw32
> arch     i386
> os       mingw32
> system   i386, mingw32
> status
> major    1
> minor    7.0
> year     2003
> month    04
> day      16
> language R
> 
> Thanks for any information.
> 
> Jake
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> 
> 

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From Roger.Bivand at nhh.no  Fri Oct 17 08:52:48 2003
From: Roger.Bivand at nhh.no (Roger Bivand)
Date: Fri, 17 Oct 2003 08:52:48 +0200 (CEST)
Subject: [R] don't display rulers in image() command and script file input
In-Reply-To: <200310180408.30755.adorio@math.upd.edu.ph>
Message-ID: <Pine.LNX.4.44.0310170843020.18546-100000@reclus.nhh.no>

On Sat, 18 Oct 2003, Ernie Adorio wrote:

> Dear R experts,
> 
> 1. How can I turn off the display of rulers in image() command?

Set axes=FALSE in image(). In this case - as in many others - running 
example() - here example(image) - on the function does point in the right 
direction. The function help files, and especially their examples, are 
places where much wisdom is to be found!

> 
> 2. Rather than typing my commands at the command line, how can I input a
> file contents aside from doing a copy and paste operation?
> 

source() the file. On Windows, there is a menu item "Source R code" under
"File", but sourcing from the command line lets you change arguments. On
Windows you may find source(file.choose(), echo=TRUE) useful - but perhaps
turn off buffered output in the "Misc" menu first - otherwise the console
stays blank until the whole sourced file is completed. On other systems
console output is not usually buffered.

Roger Bivand

> Thanks in advance,
> 
> Ernesto Adorio
> Math Department
> University of the Philippines
> Diliman
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> 

-- 
Roger Bivand
Economic Geography Section, Department of Economics, Norwegian School of
Economics and Business Administration, Breiviksveien 40, N-5045 Bergen,
Norway. voice: +47 55 95 93 55; fax +47 55 95 93 93
e-mail: Roger.Bivand at nhh.no



From maechler at stat.math.ethz.ch  Fri Oct 17 09:20:26 2003
From: maechler at stat.math.ethz.ch (Martin Maechler)
Date: Fri, 17 Oct 2003 09:20:26 +0200
Subject: [R] help with legend()
In-Reply-To: <8E46EB3BC001414AA6CDB57C5E551F8D12F209@thuja>
References: <8E46EB3BC001414AA6CDB57C5E551F8D12F209@thuja>
Message-ID: <16271.38970.646500.488205@gargle.gargle.HOWL>

>>>>> "PaulSch" == Schwarz, Paul <paul.schwarz at oregonstate.edu>
>>>>>     on Wed, 15 Oct 2003 12:09:11 -0700 writes:

    PaulSch> I am converting some S-PLUS scripts that I use for
    PaulSch> creating manuscript figures to R so that I can take
    PaulSch> advantage of the plotmath capabilities.  In my
    PaulSch> S-PLUS scripts I like to use the key() function for
    PaulSch> adding legends to plots, 

AFAIK  key() in S+ is from the trellis library section.
The corresponding R package, trellis, has
a draw.key() function that may work similarly to S-plus' key() 
{Deepayan ?}.

    PaulSch> and I have a couple of
    PaulSch> questions regarding using the legend() function in
    PaulSch> R.

    PaulSch> 1) is there a way to specify different colors for
    PaulSch> the legend vector of text values?

not yet in legend() -- but see below

    PaulSch> 2) is there a way to reverse the order of the
    PaulSch> legend items so that the text values precede the
    PaulSch> symbols?

not yet in legend()   --- but it's an open source project living
		       from "community support" ...

Can S+ key() do these two things?
If yes, how do you specify it there
{this sounds as if I was willing to consider adding these wished
 features to legend .... }

    PaulSch> Thanks for your time and patience.

You're welcome,
Martin



From p.dalgaard at biostat.ku.dk  Fri Oct 17 09:47:33 2003
From: p.dalgaard at biostat.ku.dk (Peter Dalgaard)
Date: 17 Oct 2003 09:47:33 +0200
Subject: [R] indexing a particular element in a list of vectors
In-Reply-To: <200310162340.h9GNeY0Y353403@atlas.otago.ac.nz>
References: <200310162340.h9GNeY0Y353403@atlas.otago.ac.nz>
Message-ID: <x2ekxcnw4q.fsf@biostat.ku.dk>

"Richard A. O'Keefe" <ok at cs.otago.ac.nz> writes:

> "Scott Norton" <nortonsm at verizon.net> wrote:
> 	I have a "list" of character vectors.  I'm trying to see if
> 	there is a way (in a single line, without a loop) to pull out
> 	the first element of all the vectors contained in the list.
> 	
> You have a list.
> You want to do something to each element.
> See ?lapply
> 
> > u <- c("Fee","fie","foe","fum")
> > v <- c("Ping","pong","diplomacy")
> > w <- c("Hi","fi")
> > x <- list(a=u, b=v, c=w)
> > lapply(x, function (cv) cv[1])
...
> If you want the result as a character vector, see ?sapply
> 
> > sapply(x, function (cv) cv[1])
>      a      b      c 
>  "Fee" "Ping"   "Hi"

Or even

> sapply(x, "[", 1)
     a      b      c
 "Fee" "Ping"   "Hi"

(same thing with lapply)

-- 
   O__  ---- Peter Dalgaard             Blegdamsvej 3  
  c/ /'_ --- Dept. of Biostatistics     2200 Cph. N   
 (*) \(*) -- University of Copenhagen   Denmark      Ph: (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)             FAX: (+45) 35327907



From M.Mamin at intershop.de  Fri Oct 17 10:11:07 2003
From: M.Mamin at intershop.de (Marc Mamin)
Date: Fri, 17 Oct 2003 10:11:07 +0200
Subject: [R] plot with dates on x axis,
	how to fix the number of days betwenn tick marks ?
Message-ID: <770E451830D96B4D84747B54665DA1B202DAF14C@jena03.net.j.ad.intershop.net>

Hi,

Following plot is displaying fine, (starting arount the 10. september),
except that the xaxp parameter has no effect.

I'd like to have a tickmark every 7 days...

plot(timeline,
   subset(myd, TYPE=="A")$list1,
   ylim=c(100*floor(min(subset(myd, TYPE=="A")$list1)/100-1),
        100*ceiling(max(subset(myd, TYPE=="A")$list1)/100+1)),
   xlim=c(1063500000, Sys.time()),
   xaxp=c(1063500000, Sys.time(),7*24*3600),
   type="o", col="blue",ylab="")


Thanks for your hints.


Marc Mamin



From gmm at ds.unifi.it  Fri Oct 17 09:11:56 2003
From: gmm at ds.unifi.it (Giovanni Marchetti)
Date: Fri, 17 Oct 2003 10:11:56 +0300
Subject: [R] Problems with crossprod
Message-ID: <200310171011.59461.gmm@ds.unifi.it>

Dear R-users, 

I found a strange problem 
working with products of two matrices, say: 
 
a <- A[i, ] ; crossprod(a)

where i is a set of integers selecting rows. When i is empty 
the result is in a sense random.

After some trials the right answer 
(a matrix of zeros) appears.

--------------- Illustration --------------------
R : Copyright 2003, The R Development Core Team
Version 1.8.0  (2003-10-08)

> A  <-matrix(0, 5, 5)
> i <- c()
> a <- A[i, ] ; crossprod(a)
              [,1]          [,2]          [,3]          [,4] [,5]
[1,] 6.578187e-313           NaN           NaN           NaN  NaN
[2,]           NaN 1.273197e-313           NaN 1.485397e-313  NaN
[3,]           NaN 4.243992e-313 2.121996e-314           NaN  NaN
[4,]           NaN 1.697597e-313           NaN 4.880590e-313  NaN
[5,] 5.941588e-313           NaN           NaN 1.697597e-313  NaN
> a <- A[i, ] ; crossprod(a)
              [,1]          [,2]          [,3]          [,4]          [,5]
[1,] 2.121996e-314 5.729389e-313           NaN           NaN           NaN
[2,]           NaN           NaN           NaN           NaN 1.909796e-313
[3,] 2.970794e-313           NaN           NaN           NaN           NaN
[4,]           NaN           NaN           NaN 8.487983e-314           NaN
[5,]           NaN 6.365987e-313 2.546395e-313           NaN           NaN
> a <- A[i, ] ; crossprod(a)
              [,1]          [,2] [,3]          [,4]          [,5]
[1,]           NaN 1.485397e-313  NaN           NaN 2.970794e-313
[2,] 3.182994e-313           NaN  NaN 1.060998e-313           NaN
[3,]           NaN           NaN  NaN 1.697597e-313 2.737375e-312
[4,]           NaN           NaN  NaN           NaN  2.048394e+10
[5,]           NaN           NaN  NaN           NaN 2.970794e-313
> a <- A[i, ] ; crossprod(a)
              [,1]        [,2] [,3] [,4] [,5]
[1,] 1.591383e-266 20489834629    0    0    0
[2,] 5.031994e-266           0    0    0    0
[3,] 1.591205e-266           0    0    0    0
[4,] 1.264128e-267           0    0    0    0
[5,] 1.037656e-311           0    0    0    0
> a <- A[i, ] ; crossprod(a)
     [,1] [,2] [,3] [,4] [,5]
[1,]    0    0    0    0    0
[2,]    0    0    0    0    0
[3,]    0    0    0    0    0
[4,]    0    0    0    0    0
[5,]    0    0    0    0    0
--------------- End of illustration------------

The same problem does not appear using the matrix product:

> a <- A[i, ] ; t(a) %*% a
     [,1] [,2] [,3] [,4] [,5]
[1,]    0    0    0    0    0
[2,]    0    0    0    0    0
[3,]    0    0    0    0    0
[4,]    0    0    0    0    0
[5,]    0    0    0    0    0

Note that Splus 6 returns an error message:

> a <- A[i, ] ; crossprod(a)

Problem in .Fortran.ok.Internal(if(cmplx) "zcrossp1"..: subroutine dcrossp1: 
Argument 1 has zero length


Thank you, 

Giovanni



From alex_s_42 at yahoo.com  Fri Oct 17 10:29:31 2003
From: alex_s_42 at yahoo.com (Alexander Sirotkin [at Yahoo])
Date: Fri, 17 Oct 2003 01:29:31 -0700 (PDT)
Subject: [R] R memory and CPU requirements
In-Reply-To: <5.1.0.14.2.20031016220153.01fb92b0@127.0.0.1>
Message-ID: <20031017082931.17634.qmail@web14901.mail.yahoo.com>

I agree completely. 

In fact, I have about 5000 observations, which should
be enough. 
I was using 200 samples because of RAM limitations and
 I'm afraid to think about what amount of RAM I'll
need to fit an aov() for such data.


--- John Fox <jfox at mcmaster.ca> wrote:
> Dear Alexander,
> 
> If I understand you correctly, you have a sample of
> 200 observations. Even 
> if you had only two factors with 40 levels each, the
> main effects and 
> interactions of these factors would require about
> 1600 degrees of freedom 
> -- that is, more than the number of observations.
> This doesn't make a whole 
> lot of sense.
> 
> I hope that this helps,
>   John
> 
> At 05:03 PM 10/16/2003 -0700, Alexander Sirotkin
> \[at Yahoo\] wrote:
> 
> >--- Deepayan Sarkar <deepayan at stat.wisc.edu> wrote:
> > > On Thursday 16 October 2003 17:59, Alexander
> > > Sirotkin \[at Yahoo\] wrote:
> > > > Thanks for all the help on my previous
> questions.
> > > >
> > > > One more (hopefully last one) : I've been very
> > > > surprised when I tried to fit a model (using
> > > aov())
> > > > for a sample of size 200 and 10 variables and
> > > their
> > > > interactions.
> > >
> > > That doesn't really say much. How many of these
> > > variables are factors ? How
> > > many levels do they have ? And what is the order
> of
> > > the interaction ? (Note
> > > that for 10 numeric variables, if you allow all
> > > interactions, then there will
> > > be a 100 terms in your model. This increases for
> > > factors.)
> > >
> > > In other words, how big is your model matrix ?
> (See
> > > ?model.matrix)
> > >
> > > Deepayan
> > >
> >
> >
> >I see...
> >
> >Unfortunately, model.matrix() ran out of memory :)
> >I have 10 variables, 6 of which are factor, 2 of
> which
> >
> >have quite a lot of levels (about 40). And I would
> >like
> >to allow all interactions.
> >
> >I understand your point about categorical
> variables,
> >but
> >still - this does not seem like too much data to
> me.
> >
> >
> >I remmeber fitting all kinds of models (mostly
> >decision
> >trees) for much, much larger data sets.
> >
> >______________________________________________
> >R-help at stat.math.ethz.ch mailing list
>
>https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> 
>
-----------------------------------------------------
> John Fox
> Department of Sociology
> McMaster University
> Hamilton, Ontario, Canada L8S 4M4
> email: jfox at mcmaster.ca
> phone: 905-525-9140x23604
> web: www.socsci.mcmaster.ca/jfox
>
-----------------------------------------------------
>



From alex_s_42 at yahoo.com  Fri Oct 17 10:33:59 2003
From: alex_s_42 at yahoo.com (Alexander Sirotkin [at Yahoo])
Date: Fri, 17 Oct 2003 01:33:59 -0700 (PDT)
Subject: [R] R memory and CPU requirements
In-Reply-To: <200310162151.17938.deepayan@stat.wisc.edu>
Message-ID: <20031017083359.953.qmail@web14914.mail.yahoo.com>


--- Deepayan Sarkar <deepayan at stat.wisc.edu> wrote:
> On Thursday 16 October 2003 19:03, Alexander
> Sirotkin \[at Yahoo\] wrote:
> 
> > > > Thanks for all the help on my previous
> questions.
> > > >
> > > > One more (hopefully last one) : I've been very
> > > > surprised when I tried to fit a model (using
> > > > aov())
> > > > for a sample of size 200 and 10 variables and
> > > > their interactions.
> > >
> > > That doesn't really say much. How many of these
> > > variables are factors ? How
> > > many levels do they have ? And what is the order
> of
> > > the interaction ? (Note
> > > that for 10 numeric variables, if you allow all
> > > interactions, then there will
> > > be a 100 terms in your model. This increases for
> > > factors.)
> > >
> > > In other words, how big is your model matrix ?
> (See
> > > ?model.matrix)
> > >
> > > Deepayan
> >
> > I see...
> >
> > Unfortunately, model.matrix() ran out of memory :)
> > I have 10 variables, 6 of which are factor, 2 of
> which
> >
> > have quite a lot of levels (about 40). And I would
> > like to allow all interactions.
> >
> > I understand your point about categorical
> variables,
> > but still - this does not seem like too much data
> to me.
> 
> That's one way to look at it. You don't have enough
> data for the model you are 
> trying to fit. The usual approach under these
> circumstances is to try 
> 'simpler' models.
> 
> Please try to understand what you are trying to do
> (in this case by reading an 
> introductory linear model text) before blindly
> applying a methodology.
> 
> Deepayan
> 
> 


I did study ANOVA and I do have enough observations.
200 was only a random sample of more then 5000 which I
think should be enough. However, I'm afraid to even
think about amount of RAM I will need with R to fit a
model for this data.



From Ivar.Herfindal at bio.ntnu.no  Fri Oct 17 10:54:17 2003
From: Ivar.Herfindal at bio.ntnu.no (Ivar Herfindal)
Date: Fri, 17 Oct 2003 10:54:17 +0200
Subject: [R] sort charcters in W2K and NT
Message-ID: <oprw6i0rmundboo6@mail.bio.ntnu.no>

Hello.

I have a problem using sort() in windows 2000 and windows NT 4.0, running R 
1.8.0 on both. I want to sort a vector of characters names, where I have 
used "Scandinavian" letters, like '?', '?', and '?' (for those who cannot 
display these letters this question seems rather meaningless, i guess). 
Windows 2000 sorts the vector like I am used to from other software, with 
'?' as the last letter in the alphabet, while windows NT has "?" just after 
"A", and "?" following "O".

Is there a way to solve this problem (other than replace the Scandinavian 
letters)?

A short example:
sort(c('a','p','?'))
# on windows 2000:
[1] "a" "p" "?"

# on windows NT
[1] "a" "?" "p"

Thanks in advance

Ivar Herfindal

On windows 2000:
> version
         _              platform i386-pc-mingw32
arch     i386           os       mingw32        system   i386, mingw32  
status                  major    1              minor    8.0            
year     2003           month    10             day      08             
language R
>

On windows NT:
> version
         _              platform i386-pc-mingw32
arch     i386           os       mingw32        system   i386, mingw32  
status                  major    1              minor    8.0            
year     2003           month    10             day      08             
language R
>



From cristian at biometria.univr.it  Fri Oct 17 10:53:31 2003
From: cristian at biometria.univr.it (cristian@biometria.univr.it)
Date: Fri, 17 Oct 2003 10:53:31 +0200
Subject: [R] Query: colouring graph
Message-ID: <1066380811.3f8fae0bd94b7@biometria.univr.it>


Hi!

How can I fill with colors a portion of a graph (e.g.: I want fill in red the
area within two confidence intervals)?

Thank you very much
Cristian


~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
Cristian Pattaro
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
Unit of Epidemiology & Medical Statistics
University of Verona

Tel +39 045 8027668
fax +39 045 505357

cristian at biometria.univr.it
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~


-------------------------------------------------
Biometria - biometria.univr.it



From spencer.graves at pdf.com  Fri Oct 17 11:24:37 2003
From: spencer.graves at pdf.com (Spencer Graves)
Date: Fri, 17 Oct 2003 02:24:37 -0700
Subject: [R] Problems with crossprod
In-Reply-To: <200310171011.59461.gmm@ds.unifi.it>
References: <200310171011.59461.gmm@ds.unifi.it>
Message-ID: <3F8FB555.7020707@pdf.com>

I still have R 1.7.1, and the problem appears there as well: 

 > a <- A[i, ] ; crossprod(a)
              [,1]          [,2]          [,3]          [,4]          [,5]
[1,] 1.195616e-301 7.042305e-302 9.563047e-302 2.281448e-302 2.198017e-302
[2,] 6.905419e-302 1.204915e-301 3.382433e-302 2.398701e-302 2.358828e-302
[3,] 1.194968e-301 7.039991e-302 1.384628e-302 2.446584e-302 4.507199e-302
[4,] 7.046400e-302 1.204416e-301 2.444003e-302 2.357136e-302 2.446228e-302
[5,] 1.205363e-301 1.204367e-301 2.445605e-302 3.979963e-302 7.861951e-302
 > a <- A[i, ] ; crossprod(a)
     [,1] [,2] [,3] [,4] [,5]
[1,]    0    0    0    0    0
[2,]    0    0    0    0    0
[3,]    0    0    0    0    0
[4,]    0    0    0    0    0
[5,]    0    0    0    0    0
 > a <- A[i, ] ; crossprod(a)
     [,1] [,2] [,3] [,4] [,5]
[1,]    0    0    0    0    0
[2,]    0    0    0    0    0
[3,]    0    0    0    0    0
[4,]    0    0    0    0    0
[5,]    0    0    0    0    0
 > a <- A[i, ] ; crossprod(a)
     [,1] [,2] [,3] [,4] [,5]
[1,]    0    0    0    0    0
[2,]    0    0    0    0    0
[3,]    0    0    0    0    0
[4,]    0    0    0    0    0
[5,]    0    0    0    0    0
 > a <- A[i, ] ; crossprod(a)
     [,1] [,2]          [,3]          [,4] [,5]
[1,]    0    0 5.092790e-313 4.372522e-111    0
[2,]    0    0 5.708624e-307  0.000000e+00    0
[3,]  NaN    0 4.516565e-300  0.000000e+00    0
[4,]    0    0 8.997262e-312  0.000000e+00    0
[5,]    0    0 1.086462e-311  0.000000e+00    0
 >
hope this helps.  spencer graves

Giovanni Marchetti wrote:

>Dear R-users, 
>
>I found a strange problem 
>working with products of two matrices, say: 
> 
>a <- A[i, ] ; crossprod(a)
>
>where i is a set of integers selecting rows. When i is empty 
>the result is in a sense random.
>
>After some trials the right answer 
>(a matrix of zeros) appears.
>
>--------------- Illustration --------------------
>R : Copyright 2003, The R Development Core Team
>Version 1.8.0  (2003-10-08)
>
>  
>
>>A  <-matrix(0, 5, 5)
>>i <- c()
>>a <- A[i, ] ; crossprod(a)
>>    
>>
>              [,1]          [,2]          [,3]          [,4] [,5]
>[1,] 6.578187e-313           NaN           NaN           NaN  NaN
>[2,]           NaN 1.273197e-313           NaN 1.485397e-313  NaN
>[3,]           NaN 4.243992e-313 2.121996e-314           NaN  NaN
>[4,]           NaN 1.697597e-313           NaN 4.880590e-313  NaN
>[5,] 5.941588e-313           NaN           NaN 1.697597e-313  NaN
>  
>
>>a <- A[i, ] ; crossprod(a)
>>    
>>
>              [,1]          [,2]          [,3]          [,4]          [,5]
>[1,] 2.121996e-314 5.729389e-313           NaN           NaN           NaN
>[2,]           NaN           NaN           NaN           NaN 1.909796e-313
>[3,] 2.970794e-313           NaN           NaN           NaN           NaN
>[4,]           NaN           NaN           NaN 8.487983e-314           NaN
>[5,]           NaN 6.365987e-313 2.546395e-313           NaN           NaN
>  
>
>>a <- A[i, ] ; crossprod(a)
>>    
>>
>              [,1]          [,2] [,3]          [,4]          [,5]
>[1,]           NaN 1.485397e-313  NaN           NaN 2.970794e-313
>[2,] 3.182994e-313           NaN  NaN 1.060998e-313           NaN
>[3,]           NaN           NaN  NaN 1.697597e-313 2.737375e-312
>[4,]           NaN           NaN  NaN           NaN  2.048394e+10
>[5,]           NaN           NaN  NaN           NaN 2.970794e-313
>  
>
>>a <- A[i, ] ; crossprod(a)
>>    
>>
>              [,1]        [,2] [,3] [,4] [,5]
>[1,] 1.591383e-266 20489834629    0    0    0
>[2,] 5.031994e-266           0    0    0    0
>[3,] 1.591205e-266           0    0    0    0
>[4,] 1.264128e-267           0    0    0    0
>[5,] 1.037656e-311           0    0    0    0
>  
>
>>a <- A[i, ] ; crossprod(a)
>>    
>>
>     [,1] [,2] [,3] [,4] [,5]
>[1,]    0    0    0    0    0
>[2,]    0    0    0    0    0
>[3,]    0    0    0    0    0
>[4,]    0    0    0    0    0
>[5,]    0    0    0    0    0
>--------------- End of illustration------------
>
>The same problem does not appear using the matrix product:
>
>  
>
>>a <- A[i, ] ; t(a) %*% a
>>    
>>
>     [,1] [,2] [,3] [,4] [,5]
>[1,]    0    0    0    0    0
>[2,]    0    0    0    0    0
>[3,]    0    0    0    0    0
>[4,]    0    0    0    0    0
>[5,]    0    0    0    0    0
>
>Note that Splus 6 returns an error message:
>
>  
>
>>a <- A[i, ] ; crossprod(a)
>>    
>>
>
>Problem in .Fortran.ok.Internal(if(cmplx) "zcrossp1"..: subroutine dcrossp1: 
>Argument 1 has zero length
>
>
>Thank you, 
>
>Giovanni
>
>______________________________________________
>R-help at stat.math.ethz.ch mailing list
>https://www.stat.math.ethz.ch/mailman/listinfo/r-help
>  
>



From B.Rowlingson at lancaster.ac.uk  Fri Oct 17 11:28:09 2003
From: B.Rowlingson at lancaster.ac.uk (Barry Rowlingson)
Date: Fri, 17 Oct 2003 10:28:09 +0100
Subject: [R] Query: colouring graph
In-Reply-To: <1066380811.3f8fae0bd94b7@biometria.univr.it>
References: <1066380811.3f8fae0bd94b7@biometria.univr.it>
Message-ID: <3F8FB629.9000001@lancaster.ac.uk>

cristian at biometria.univr.it wrote:
> Hi!
> 
> How can I fill with colors a portion of a graph (e.g.: I want fill in red the
> area within two confidence intervals)?

  You can construct the coordinates of the polygon that fills this 
region, then use 'polygon' to fill it. Here:

# first set up the plot - we want the density over the polygon,
# so make a blank plot of the right size:

x <- seq(-3,3,len=100)
plot(x,dnorm(x),type='n')

# a little function that draws a filled polygon between limits under
# dnorm(x) The polygon has to go from the axis, up, along the curve,
# then back down again.

fillDnorm <- function(low,high,col="red",n=100){
   x <- seq(low,high,len=n)
   y <- dnorm(x)
   x <- c(x[1],x,x[length(x)])
   y <- c(0,y,0)
   polygon(x,y,col=col,border=NA)
}

# fill between -2 and -1
  fillDnorm(-2,-1)

# now add the density
  lines(x,dnorm(x))

  Tweak as required.

Baz



From martinol at ensam.inra.fr  Fri Oct 17 11:32:12 2003
From: martinol at ensam.inra.fr (Martin Olivier)
Date: Fri, 17 Oct 2003 11:32:12 +0200
Subject: [R] heatmap function
Message-ID: <3F8FB71C.9030304@ensam.inra.fr>

Hi all,

By default, the heatmap function gives an image with a dendrogram added 
to the
left side and to the top. Is it possible to only add the dendrogram to 
the left side
and  let the order of the columns unchanged ?

I tried
heatmap(mat, col=rbg,Rowv=res.hclust$order,Colv=1:dim(mat)[[2]]).
In this case, the order of the columns are unchanged but a dendrogram
is added to the top. How can I avoid it?

Thanks,
Oiliver

-- 

-------------------------------------------------------------
Martin Olivier
INRA - Unit? prot?omique           LIRMM - IFA/MAB
2, Place Viala                     161, rue Ada
34060 Montpellier C?dex 1          34392 Montpellier C?dex 5	

Tel : 04 99 61 27 01               Tel : O4 67 41 86 71
martinol at ensam.inra.fr             martin at lirmm.fr



From ligges at statistik.uni-dortmund.de  Fri Oct 17 11:58:49 2003
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Fri, 17 Oct 2003 11:58:49 +0200
Subject: [R] sort charcters in W2K and NT
In-Reply-To: <oprw6i0rmundboo6@mail.bio.ntnu.no>
References: <oprw6i0rmundboo6@mail.bio.ntnu.no>
Message-ID: <3F8FBD59.7070909@statistik.uni-dortmund.de>

Ivar Herfindal wrote:

> Hello.
> 
> I have a problem using sort() in windows 2000 and windows NT 4.0, 
> running R 1.8.0 on both. I want to sort a vector of characters names, 
> where I have used "Scandinavian" letters, like '?', '?', and '?' (for 
> those who cannot display these letters this question seems rather 
> meaningless, i guess). Windows 2000 sorts the vector like I am used to 
> from other software, with '?' as the last letter in the alphabet, while 
> windows NT has "?" just after "A", and "?" following "O".
> 
> Is there a way to solve this problem (other than replace the 
> Scandinavian letters)?
> 
> A short example:
> sort(c('a','p','?'))
> # on windows 2000:
> [1] "a" "p" "?"
> 
> # on windows NT
> [1] "a" "?" "p"
> 
> Thanks in advance

?sort tells us:

"The sort order for character vectors will depend on the collating 
sequence of the locale in use: see Comparison."

and ?Comparison points you to ?locales which gives an example:

   Sys.setlocale("LC_COLLATE", "C")  # turn off locale-specific sorting

Uwe Ligges


> Ivar Herfindal
> 
> On windows 2000:
> 
>> version
> 
>         _              platform i386-pc-mingw32
> arch     i386           os       mingw32        system   i386, mingw32  
> status                  major    1              minor    8.0            
> year     2003           month    10             day      08             
> language R
> 
>>
> 
> On windows NT:
> 
>> version
> 
>         _              platform i386-pc-mingw32
> arch     i386           os       mingw32        system   i386, mingw32  
> status                  major    1              minor    8.0            
> year     2003           month    10             day      08             
> language R
> 
>>
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help



From Simon.Fear at synequanon.com  Fri Oct 17 12:09:10 2003
From: Simon.Fear at synequanon.com (Simon Fear)
Date: Fri, 17 Oct 2003 11:09:10 +0100
Subject: [R] indexing a particular element in a list of vectors
Message-ID: <6C8A8033ABC1E3468048ABC4F13CE572AC0E39@synequanon01>

Or

do.call("cbind",x)[1,]

which of course makes a whole new copy of x and 
gives you a nasty warning as well, but does not
use a conceptual `for` loop. Which I think was the
original question, to which AFAIK the answer is no, there is no
easy subscripting construct such as x[[1:3]][1] that will do 
what was asked.

> -----Original Message-----
> From: Peter Dalgaard [mailto:p.dalgaard at biostat.ku.dk]
> Sent: 17 October 2003 08:48
> To: Richard A. O'Keefe
> Cc: nortonsm at verizon.net; r-help at stat.math.ethz.ch
> Subject: Re: [R] indexing a particular element in a list of vectors
> 
> 
> Security Warning:
> If you are not sure an attachment is safe to open please contact 
> Andy on x234. There are 0 attachments with this message.
> ________________________________________________________________
> 
> "Richard A. O'Keefe" <ok at cs.otago.ac.nz> writes:
> 
> > "Scott Norton" <nortonsm at verizon.net> wrote:
> > 	I have a "list" of character vectors.  I'm trying to see if
> > 	there is a way (in a single line, without a loop) to pull out
> > 	the first element of all the vectors contained in the list.
> > 	
> > You have a list.
> > You want to do something to each element.
> > See ?lapply
> > 
> > > u <- c("Fee","fie","foe","fum")
> > > v <- c("Ping","pong","diplomacy")
> > > w <- c("Hi","fi")
> > > x <- list(a=u, b=v, c=w)
> > > lapply(x, function (cv) cv[1])
> ...
> > If you want the result as a character vector, see ?sapply
> > 
> > > sapply(x, function (cv) cv[1])
> >      a      b      c 
> >  "Fee" "Ping"   "Hi"
> 
> Or even
> 
> > sapply(x, "[", 1)
>      a      b      c
>  "Fee" "Ping"   "Hi"
> 
> (same thing with lapply)
> 
> -- 
>    O__  ---- Peter Dalgaard             Blegdamsvej 3  
>   c/ /'_ --- Dept. of Biostatistics     2200 Cph. N   
>  (*) \(*) -- University of Copenhagen   Denmark      Ph: 
> (+45) 35327918
> ~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)             FAX: 
> (+45) 35327907
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
>
 

Simon Fear
Senior Statistician
Syne qua non Ltd
Tel: +44 (0) 1379 644449
Fax: +44 (0) 1379 644445
email: Simon.Fear at synequanon.com
web: http://www.synequanon.com
 
Number of attachments included with this message: 0
 
This message (and any associated files) is confidential and\...{{dropped}}



From Arne.Muller at aventis.com  Fri Oct 17 12:11:30 2003
From: Arne.Muller at aventis.com (Arne.Muller@aventis.com)
Date: Fri, 17 Oct 2003 12:11:30 +0200
Subject: [R] sub data frame by expression
Message-ID: <C80ECAFA2ACC1B45BE45D133ED660ADE410B1A@crbsmxsusr04.pharma.aventis.com>

Hi All,

I've the following data frame with 54 rows and 4 colums:

> x                  
                  Ratio  Dose Time Batch
R.010mM.04h.NEW    0.02 010mM  04h   NEW
R.010mM.04h.NEW.1  0.07 010mM  04h   NEW
...
R.010mM.24h.NEW.2  0.06 010mM  24h   NEW
R.010mM.04h.OLD    0.19 010mM  04h   OLD
...
R.010mM.04h.OLD.1  0.49 010mM  04h   OLD
R.100mM.24h.OLD    0.40 100mM  24h   OLD

I'd like to create a sub data frame containing all rows where Batch == "OLD"
and keeping the 4 colums. Assume that I don't know the order of the rows
(otherwise I could just do something like x[1:20,]).

I've tried x[x$Batch == 'OLD'] or x[x[,4] == 'OLD'] but it generates errors.
So I assume I've still not realy understood the philosophy of indexing ...
:-(

What's the easiest way to do this, any suggestions?

	thanks a lot for you help,

	Arne



From v.demart at libero.it  Fri Oct 17 12:21:30 2003
From: v.demart at libero.it (v.demart@libero.it)
Date: Fri, 17 Oct 2003 12:21:30 +0200
Subject: [R] Strange behaviour
Message-ID: <HMWCRU$33742BA4ECD9AE7C0D98A69A00C5D01F@libero.it>

As an absolute beginner I'm reading and practicing with the Verzani doc to learn R.

Now, being an expert latex user who wants to integrate graphical capabilities if R and latex, using the "Simple" library and the simple.scatterplot examples I had a go at:
1)   Including the resulting graph into a doc.snw then compiled through sweave & latex;
2) Produce the graph  in pdf format directly (using pdf(filename) at the very beginning and before issuing the "simple.scatterplot " command) and including it into the latex file via \includegraphics.

In both cases the graph is a set of numbered rectangles. Investigating into the pdf files generated by R I found that they are made of 2 pages: the first contains those nasty rectangles while the second the "right" graph that should be inserted.

The same result comes out if I try the "layout" command example in the help of the same package.

My question is: is there a way to tell R to produce the second page only as a final pdf file?
If not, any suggestion for a way out.

Ciao
Vittorio



From Arne.Muller at aventis.com  Fri Oct 17 12:28:04 2003
From: Arne.Muller at aventis.com (Arne.Muller@aventis.com)
Date: Fri, 17 Oct 2003 12:28:04 +0200
Subject: [R] sub data frame by expression
Message-ID: <C80ECAFA2ACC1B45BE45D133ED660ADECDCC4E@crbsmxsusr04.pharma.aventis.com>

Sorry, I just figured it out: x[x$Batch == 'OLD',] instead of x[x$Batch ==
'OLD']. I didn't know this has to be in the same format then x[1:20,] where I
already used the comma.

	sorry for posting the previous message ...

	Arne


> -----Original Message-----
> From: r-help-bounces at stat.math.ethz.ch
> [mailto:r-help-bounces at stat.math.ethz.ch]On Behalf Of
> Arne.Muller at aventis.com
> Sent: 17 October 2003 12:12
> To: r-help at stat.math.ethz.ch
> Subject: [R] sub data frame by expression
> 
> 
> Hi All,
> 
> I've the following data frame with 54 rows and 4 colums:
> 
> > x                  
>                   Ratio  Dose Time Batch
> R.010mM.04h.NEW    0.02 010mM  04h   NEW
> R.010mM.04h.NEW.1  0.07 010mM  04h   NEW
> ...
> R.010mM.24h.NEW.2  0.06 010mM  24h   NEW
> R.010mM.04h.OLD    0.19 010mM  04h   OLD
> ...
> R.010mM.04h.OLD.1  0.49 010mM  04h   OLD
> R.100mM.24h.OLD    0.40 100mM  24h   OLD
> 
> I'd like to create a sub data frame containing all rows where 
> Batch == "OLD"
> and keeping the 4 colums. Assume that I don't know the order 
> of the rows
> (otherwise I could just do something like x[1:20,]).
> 
> I've tried x[x$Batch == 'OLD'] or x[x[,4] == 'OLD'] but it 
> generates errors.
> So I assume I've still not realy understood the philosophy of 
> indexing ...
> :-(
> 
> What's the easiest way to do this, any suggestions?
> 
> 	thanks a lot for you help,
> 
> 	Arne
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
>



From ripley at stats.ox.ac.uk  Fri Oct 17 12:37:52 2003
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Fri, 17 Oct 2003 11:37:52 +0100 (BST)
Subject: [R] sub data frame by expression
In-Reply-To: <C80ECAFA2ACC1B45BE45D133ED660ADE410B1A@crbsmxsusr04.pharma.aventis.com>
Message-ID: <Pine.LNX.4.44.0310171136440.10048-100000@gannet.stats>

On Fri, 17 Oct 2003 Arne.Muller at aventis.com wrote:

> I've the following data frame with 54 rows and 4 colums:
> 
> > x                  
>                   Ratio  Dose Time Batch
> R.010mM.04h.NEW    0.02 010mM  04h   NEW
> R.010mM.04h.NEW.1  0.07 010mM  04h   NEW
> ...
> R.010mM.24h.NEW.2  0.06 010mM  24h   NEW
> R.010mM.04h.OLD    0.19 010mM  04h   OLD
> ...
> R.010mM.04h.OLD.1  0.49 010mM  04h   OLD
> R.100mM.24h.OLD    0.40 100mM  24h   OLD
> 
> I'd like to create a sub data frame containing all rows where Batch == "OLD"
> and keeping the 4 colums. Assume that I don't know the order of the rows
> (otherwise I could just do something like x[1:20,]).
> 
> I've tried x[x$Batch == 'OLD'] or x[x[,4] == 'OLD'] but it generates errors.

That subsets columns, not rows. Try x[x$Batch == "OLD",]

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From P.Lemmens at nici.kun.nl  Fri Oct 17 12:40:58 2003
From: P.Lemmens at nici.kun.nl (Paul Lemmens)
Date: Fri, 17 Oct 2003 12:40:58 +0200
Subject: [R] sub data frame by expression
In-Reply-To: <C80ECAFA2ACC1B45BE45D133ED660ADE410B1A@crbsmxsusr04.pharma.aventis.com>
References: <C80ECAFA2ACC1B45BE45D133ED660ADE410B1A@crbsmxsusr04.pharma.aven
	tis.com>
Message-ID: <16368703.1066394458@lemmens.socsci.kun.nl>

Hoi Arne.Muller at aventis.com,

--On vrijdag 17 oktober 2003 12:11 +0200 Arne.Muller at aventis.com wrote:

> I'd like to create a sub data frame containing all rows where Batch ==
> "OLD" and keeping the 4 colums. Assume that I don't know the order of the
> rows

subset(x, Batch == 'Old') or something like that!?



-- 
Paul Lemmens
NICI, University of Nijmegen              ASCII Ribbon Campaign /"\
Montessorilaan 3 (B.01.03)                    Against HTML Mail \ /
NL-6525 HR Nijmegen                                              X
The Netherlands                                                 / \
Phonenumber    +31-24-3612648
Fax            +31-24-3616066



From f.harrell at vanderbilt.edu  Fri Oct 17 13:38:11 2003
From: f.harrell at vanderbilt.edu (Frank E Harrell Jr)
Date: Fri, 17 Oct 2003 07:38:11 -0400
Subject: [R] Design and Hmisc
In-Reply-To: <3F8D500F.4020201@statistik.uni-dortmund.de>
References: <2F3262756375D411B0CC00B0D049775D0138013B@westpark>
	<3F8D500F.4020201@statistik.uni-dortmund.de>
Message-ID: <20031017073811.465f05dc.f.harrell@vanderbilt.edu>

On Wed, 15 Oct 2003 15:47:59 +0200
Uwe Ligges <ligges at statistik.uni-dortmund.de> wrote:

> Shawn Way wrote:
> 
> > I'm looking for design and hmisc version 2.0 for R 1.8 for windows.  I've
> > found design 2.0 in the downloads for R1.7 but not hmisc.  
> > 
> > I've also checked Dr. Harrell's site and it only goes to 1.6 for windows.
> > 
> > Any thoughts?
> 
> Yes.
> 
> The ReadMe at CRAN/bin/windows/contrib/1.8 (for R-1.8.x) tells us:
> 
> 'Packages that do not compile out of the box or do not pass
> "Rcmd check" with "OK" or at least a "WARNING" will *not* be
> published. This "Status", i.e. result of "Rcmd check", is listed in
> file "Status". Possible values are "OK", "WARN", and "ERROR".
> Corresponding check.log files can be found in subdirectory ./check.'
> 
> Uwe Ligges
> 
> 
> > 
> > Shawn Way

Thanks to Uwe, there is now a new version of Hmisc for Windows on CRAN.  The error in the help file for sas.get which prevented the Windows version from passing Rcmd check has been fixed.
---
Frank E Harrell Jr    Professor and Chair            School of Medicine
                      Department of Biostatistics    Vanderbilt University



From Arne.Muller at aventis.com  Fri Oct 17 13:49:43 2003
From: Arne.Muller at aventis.com (Arne.Muller@aventis.com)
Date: Fri, 17 Oct 2003 13:49:43 +0200
Subject: [R] sub data frame by expression
Message-ID: <C80ECAFA2ACC1B45BE45D133ED660ADECDCC52@crbsmxsusr04.pharma.aventis.com>

Hi,

thanks for your replies regarding the problem to select a sub data frame by
expression. I start getting an understanding on how indexing works in R.

	thanks for your replies,

	Arne

> -----Original Message-----
> From: Prof Brian Ripley [mailto:ripley at stats.ox.ac.uk]
> Sent: 17 October 2003 12:38
> To: Muller, Arne PH/FR
> Cc: r-help at stat.math.ethz.ch
> Subject: Re: [R] sub data frame by expression
> 
> 
> On Fri, 17 Oct 2003 Arne.Muller at aventis.com wrote:
> 
> > I've the following data frame with 54 rows and 4 colums:
> > 
> > > x                  
> >                   Ratio  Dose Time Batch
> > R.010mM.04h.NEW    0.02 010mM  04h   NEW
> > R.010mM.04h.NEW.1  0.07 010mM  04h   NEW
> > ...
> > R.010mM.24h.NEW.2  0.06 010mM  24h   NEW
> > R.010mM.04h.OLD    0.19 010mM  04h   OLD
> > ...
> > R.010mM.04h.OLD.1  0.49 010mM  04h   OLD
> > R.100mM.24h.OLD    0.40 100mM  24h   OLD
> > 
> > I'd like to create a sub data frame containing all rows 
> where Batch == "OLD"
> > and keeping the 4 colums. Assume that I don't know the 
> order of the rows
> > (otherwise I could just do something like x[1:20,]).
> > 
> > I've tried x[x$Batch == 'OLD'] or x[x[,4] == 'OLD'] but it 
> generates errors.
> 
> That subsets columns, not rows. Try x[x$Batch == "OLD",]
> 
> -- 
> Brian D. Ripley,                  ripley at stats.ox.ac.uk
> Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
> University of Oxford,             Tel:  +44 1865 272861 (self)
> 1 South Parks Road,                     +44 1865 272866 (PA)
> Oxford OX1 3TG, UK                Fax:  +44 1865 272595
> 
>



From andy_liaw at merck.com  Fri Oct 17 14:36:44 2003
From: andy_liaw at merck.com (Liaw, Andy)
Date: Fri, 17 Oct 2003 08:36:44 -0400
Subject: [R] R memory and CPU requirements
Message-ID: <3A822319EB35174CA3714066D590DCD50205CCCD@usrymx25.merck.com>

A couple of comments:

o  Methods such as decision trees do not need to expand factors into columns
of 1df contrasts, so the memory requirement is vastly different.  The models
produced is also very, very different.

o  Why would you want "all possible interactions" of 10 variables, 6 of
which are factors?  How do you intend to interpret, e.g., the 6-factor
interaction?  What can you conclude about a significant 10-variable
interaction?  What is your ultimate goal for this exercise?  Answer to that
should help you decide on more reasonable models to fit.

o  One thing to try is fit the ANOVA model "by hand" by computing cell means
and examine them.  This avoids creating the huge design matrix that's mostly
0s.

HTH,
Andy

> -----Original Message-----
> From: Alexander Sirotkin [at Yahoo] [mailto:alex_s_42 at yahoo.com] 
> Sent: Friday, October 17, 2003 4:30 AM
> To: John Fox
> Cc: r-help at stat.math.ethz.ch
> Subject: Re: [R] R memory and CPU requirements
> 
> 
> I agree completely. 
> 
> In fact, I have about 5000 observations, which should
> be enough. 
> I was using 200 samples because of RAM limitations and
>  I'm afraid to think about what amount of RAM I'll
> need to fit an aov() for such data.
> 
> 
> --- John Fox <jfox at mcmaster.ca> wrote:
> > Dear Alexander,
> > 
> > If I understand you correctly, you have a sample of
> > 200 observations. Even
> > if you had only two factors with 40 levels each, the
> > main effects and 
> > interactions of these factors would require about
> > 1600 degrees of freedom 
> > -- that is, more than the number of observations.
> > This doesn't make a whole 
> > lot of sense.
> > 
> > I hope that this helps,
> >   John
> > 
> > At 05:03 PM 10/16/2003 -0700, Alexander Sirotkin
> > \[at Yahoo\] wrote:
> > 
> > >--- Deepayan Sarkar <deepayan at stat.wisc.edu> wrote:
> > > > On Thursday 16 October 2003 17:59, Alexander
> > > > Sirotkin \[at Yahoo\] wrote:
> > > > > Thanks for all the help on my previous
> > questions.
> > > > >
> > > > > One more (hopefully last one) : I've been very 
> surprised when I 
> > > > > tried to fit a model (using
> > > > aov())
> > > > > for a sample of size 200 and 10 variables and
> > > > their
> > > > > interactions.
> > > >
> > > > That doesn't really say much. How many of these
> > > > variables are factors ? How
> > > > many levels do they have ? And what is the order
> > of
> > > > the interaction ? (Note
> > > > that for 10 numeric variables, if you allow all 
> interactions, then 
> > > > there will be a 100 terms in your model. This increases for
> > > > factors.)
> > > >
> > > > In other words, how big is your model matrix ?
> > (See
> > > > ?model.matrix)
> > > >
> > > > Deepayan
> > > >
> > >
> > >
> > >I see...
> > >
> > >Unfortunately, model.matrix() ran out of memory :)
> > >I have 10 variables, 6 of which are factor, 2 of
> > which
> > >
> > >have quite a lot of levels (about 40). And I would
> > >like
> > >to allow all interactions.
> > >
> > >I understand your point about categorical
> > variables,
> > >but
> > >still - this does not seem like too much data to
> > me.
> > >
> > >
> > >I remmeber fitting all kinds of models (mostly
> > >decision
> > >trees) for much, much larger data sets.
> > >
> > >______________________________________________
> > >R-help at stat.math.ethz.ch mailing list
> >
> >https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> > 
> >
> -----------------------------------------------------
> > John Fox
> > Department of Sociology
> > McMaster University
> > Hamilton, Ontario, Canada L8S 4M4
> > email: jfox at mcmaster.ca
> > phone: 905-525-9140x23604
> > web: www.socsci.mcmaster.ca/jfox
> >
> -----------------------------------------------------
> >
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list 
> https://www.stat.math.ethz.ch/mailman/listinfo> /r-help
>



From jfox at mcmaster.ca  Fri Oct 17 14:48:50 2003
From: jfox at mcmaster.ca (John Fox)
Date: Fri, 17 Oct 2003 08:48:50 -0400
Subject: [R] R memory and CPU requirements
In-Reply-To: <20031017082931.17634.qmail@web14901.mail.yahoo.com>
References: <5.1.0.14.2.20031016220153.01fb92b0@127.0.0.1>
Message-ID: <5.1.0.14.2.20031017083516.01fa5bc0@127.0.0.1>

Dear Alexander,


At 01:29 AM 10/17/2003 -0700, Alexander Sirotkin \[at Yahoo\] wrote:
>I agree completely.
>
>In fact, I have about 5000 observations, which should
>be enough.
>I was using 200 samples because of RAM limitations and
>  I'm afraid to think about what amount of RAM I'll
>need to fit an aov() for such data.
>


OK -- I didn't realize that you have 5000 observations. Perhaps I didn't 
read some of the earlier messages carefully enough.

At the risk of getting you to repeat information that you've already 
provided, how many degrees of freedom are there in the model that you're 
trying to fit? I can create a 5000 by 5000 model matrix on my relatively 
anemic Windows machine, and surely (unless there's some specification 
error) your model should have many fewer df than that if it includes just 
the main effects and two-way interactions (or by all interactions, do you 
mean higher-order interactions as well?).

Perhaps providing the following information would help: What is the model 
formula? Which variables are factors? How many levels does each factor have?

Regards,
  John

>--- John Fox <jfox at mcmaster.ca> wrote:
> > Dear Alexander,
> >
> > If I understand you correctly, you have a sample of
> > 200 observations. Even
> > if you had only two factors with 40 levels each, the
> > main effects and
> > interactions of these factors would require about
> > 1600 degrees of freedom
> > -- that is, more than the number of observations.
> > This doesn't make a whole
> > lot of sense.
> >
> > I hope that this helps,
> >   John
> >
> > At 05:03 PM 10/16/2003 -0700, Alexander Sirotkin
> > \[at Yahoo\] wrote:
> >
> > >--- Deepayan Sarkar <deepayan at stat.wisc.edu> wrote:
> > > > On Thursday 16 October 2003 17:59, Alexander
> > > > Sirotkin \[at Yahoo\] wrote:
> > > > > Thanks for all the help on my previous
> > questions.
> > > > >
> > > > > One more (hopefully last one) : I've been very
> > > > > surprised when I tried to fit a model (using
> > > > aov())
> > > > > for a sample of size 200 and 10 variables and
> > > > their
> > > > > interactions.
> > > >
> > > > That doesn't really say much. How many of these
> > > > variables are factors ? How
> > > > many levels do they have ? And what is the order
> > of
> > > > the interaction ? (Note
> > > > that for 10 numeric variables, if you allow all
> > > > interactions, then there will
> > > > be a 100 terms in your model. This increases for
> > > > factors.)
> > > >
> > > > In other words, how big is your model matrix ?
> > (See
> > > > ?model.matrix)
> > > >
> > > > Deepayan
> > > >
> > >
> > >
> > >I see...
> > >
> > >Unfortunately, model.matrix() ran out of memory :)
> > >I have 10 variables, 6 of which are factor, 2 of
> > which
> > >
> > >have quite a lot of levels (about 40). And I would
> > >like
> > >to allow all interactions.
> > >
> > >I understand your point about categorical
> > variables,
> > >but
> > >still - this does not seem like too much data to
> > me.
> > >
> > >
> > >I remmeber fitting all kinds of models (mostly
> > >decision
> > >trees) for much, much larger data sets.
> > >
> > >______________________________________________
> > >R-help at stat.math.ethz.ch mailing list
> >
> >https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> >
> >
>-----------------------------------------------------
> > John Fox
> > Department of Sociology
> > McMaster University
> > Hamilton, Ontario, Canada L8S 4M4
> > email: jfox at mcmaster.ca
> > phone: 905-525-9140x23604
> > web: www.socsci.mcmaster.ca/jfox
> >
>-----------------------------------------------------
> >
>
>
>__________________________________
>Do you Yahoo!?

>http://shopping.yahoo.com

-----------------------------------------------------
John Fox
Department of Sociology
McMaster University
Hamilton, Ontario, Canada L8S 4M4
email: jfox at mcmaster.ca
phone: 905-525-9140x23604
web: www.socsci.mcmaster.ca/jfox



From andy_liaw at merck.com  Fri Oct 17 14:45:36 2003
From: andy_liaw at merck.com (Liaw, Andy)
Date: Fri, 17 Oct 2003 08:45:36 -0400
Subject: [R] RE: [S] Dynamic Memory Allocation in R
Message-ID: <3A822319EB35174CA3714066D590DCD50205CCCE@usrymx25.merck.com>

> From: Gamal Abdel-Azim [mailto:gamal at crinet.com] 
> 
> While trying to expand the memory/object size in R, I noticed 
> that R might be using 
> only heap memory. Is this true? Are all objects in R created 
> in the heap not 
> allocated? It's not logical that this is the case!! Otherwise 
> the whole R project would 
> be a total waste of time and resources.
> 
> If I am wrong please inform me. How to increase memory.size 
> in R? Is there a way 
> similar to options(object.size=size) in S-Plus. Notice that 
> the command R --max- vsize=N targets the heap memory!
> 
> I have recently installed R on a Linux machine (3GB RAM and 
> sufficiently large HD).
> 
> Sorry for posting to Splus not R. But if R works only in the 
> heap, I may not need to 
> subscribe to R-news at all.

But only the R folks could give you the definitive answer to this question!

Andy
 
> Thank You
> 
> --
> 
> --------------------------------------------------------------------
> This message was distributed by 
> s-news at lists.biostat.wustl.edu.  To unsubscribe send e-mail 
> to s-news-request at lists.biostat.wustl.edu with the BODY of 
> the message:  unsubscribe s-news
>



From kjetil at entelnet.bo  Fri Oct 17 15:07:01 2003
From: kjetil at entelnet.bo (kjetil@entelnet.bo)
Date: Fri, 17 Oct 2003 09:07:01 -0400
Subject: [R] R memory and CPU requirements
In-Reply-To: <20031017083359.953.qmail@web14914.mail.yahoo.com>
References: <200310162151.17938.deepayan@stat.wisc.edu>
Message-ID: <3F8FB135.30489.5387AA@localhost>

On 17 Oct 2003 at 1:33, Alexander Sirotkin [at Yahoo] wrote:

You mentioned in an earlier post that at least one of your factors 
have 40 levels. If you use the default contrast, contrast.traetment, 
the design matrix for this factor will be dominated by zeros. Maybe 
you shoukd look at tha CRAN package SparseM, which have 
function slm for linear models with sparse matrices?

(I did'nt try this , but it could be worthwile)

Still, I don't think it makes much sense to start with a model with 
all the interactions in!

Kjetil Halvorsen

> 
> --- Deepayan Sarkar <deepayan at stat.wisc.edu> wrote:
> > On Thursday 16 October 2003 19:03, Alexander
> > Sirotkin \[at Yahoo\] wrote:
> > 
> > > > > Thanks for all the help on my previous
> > questions.
> > > > >
> > > > > One more (hopefully last one) : I've been very
> > > > > surprised when I tried to fit a model (using
> > > > > aov())
> > > > > for a sample of size 200 and 10 variables and
> > > > > their interactions.
> > > >
> > > > That doesn't really say much. How many of these
> > > > variables are factors ? How
> > > > many levels do they have ? And what is the order
> > of
> > > > the interaction ? (Note
> > > > that for 10 numeric variables, if you allow all
> > > > interactions, then there will
> > > > be a 100 terms in your model. This increases for
> > > > factors.)
> > > >
> > > > In other words, how big is your model matrix ?
> > (See
> > > > ?model.matrix)
> > > >
> > > > Deepayan
> > >
> > > I see...
> > >
> > > Unfortunately, model.matrix() ran out of memory :)
> > > I have 10 variables, 6 of which are factor, 2 of
> > which
> > >
> > > have quite a lot of levels (about 40). And I would
> > > like to allow all interactions.
> > >
> > > I understand your point about categorical
> > variables,
> > > but still - this does not seem like too much data
> > to me.
> > 
> > That's one way to look at it. You don't have enough
> > data for the model you are 
> > trying to fit. The usual approach under these
> > circumstances is to try 
> > 'simpler' models.
> > 
> > Please try to understand what you are trying to do
> > (in this case by reading an 
> > introductory linear model text) before blindly
> > applying a methodology.
> > 
> > Deepayan
> > 
> > 
> 
> 
> I did study ANOVA and I do have enough observations.
> 200 was only a random sample of more then 5000 which I
> think should be enough. However, I'm afraid to even
> think about amount of RAM I will need with R to fit a
> model for this data.
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help



From andy_liaw at merck.com  Fri Oct 17 15:10:16 2003
From: andy_liaw at merck.com (Liaw, Andy)
Date: Fri, 17 Oct 2003 09:10:16 -0400
Subject: [R] heatmap function
Message-ID: <3A822319EB35174CA3714066D590DCD50205CCD1@usrymx25.merck.com>

One of the good thing about R (and S in general, I guess) is that if a
function does mostly what you want, except for some small things, you can
just make another copy of it, change the name, and make the desired changes
to the new function (provided the changes you need to make isn't in the
compiled code, but R is Open Source...).

In this case, you should be able to strip out the code in heatmap() that
plot the top dendrogram w/o much problem.  While your at it, you might want
to change the layout() so as not to leave the blank space on top.

HTH,
Andy

> -----Original Message-----
> From: Martin Olivier [mailto:martinol at ensam.inra.fr] 
> Sent: Friday, October 17, 2003 5:32 AM
> To: r-help
> Subject: [R] heatmap function
> 
> 
> Hi all,
> 
> By default, the heatmap function gives an image with a 
> dendrogram added 
> to the
> left side and to the top. Is it possible to only add the 
> dendrogram to 
> the left side
> and  let the order of the columns unchanged ?
> 
> I tried
> heatmap(mat, col=rbg,Rowv=res.hclust$order,Colv=1:dim(mat)[[2]]).
> In this case, the order of the columns are unchanged but a 
> dendrogram is added to the top. How can I avoid it?
> 
> Thanks,
> Oiliver
> 
> -- 
> 
> -------------------------------------------------------------
> Martin Olivier
> INRA - Unit? prot?omique           LIRMM - IFA/MAB
> 2, Place Viala                     161, rue Ada
> 34060 Montpellier C?dex 1          34392 Montpellier C?dex 5	
> 
> Tel : 04 99 61 27 01               Tel : O4 67 41 86 71
> martinol at ensam.inra.fr             martin at lirmm.fr
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list 
> https://www.stat.math.ethz.ch/mailman/listinfo> /r-help
>



From sway at tanox.com  Fri Oct 17 15:16:56 2003
From: sway at tanox.com (Shawn Way)
Date: Fri, 17 Oct 2003 08:16:56 -0500
Subject: [R] Design and Hmisc
Message-ID: <2F3262756375D411B0CC00B0D049775D01380151@westpark>

Thank you very much...


Shawn Way

-----Original Message-----
From: Frank E Harrell Jr [mailto:f.harrell at vanderbilt.edu] 
Sent: Friday, October 17, 2003 6:38 AM
To: Uwe Ligges
Cc: r-help at stat.math.ethz.ch; sway at tanox.com
Subject: Re: [R] Design and Hmisc


On Wed, 15 Oct 2003 15:47:59 +0200
Uwe Ligges <ligges at statistik.uni-dortmund.de> wrote:

> Shawn Way wrote:
> 
> > I'm looking for design and hmisc version 2.0 for R 1.8 for windows.  
> > I've found design 2.0 in the downloads for R1.7 but not hmisc.
> > 
> > I've also checked Dr. Harrell's site and it only goes to 1.6 for 
> > windows.
> > 
> > Any thoughts?
> 
> Yes.
> 
> The ReadMe at CRAN/bin/windows/contrib/1.8 (for R-1.8.x) tells us:
> 
> 'Packages that do not compile out of the box or do not pass "Rcmd 
> check" with "OK" or at least a "WARNING" will *not* be published. This 
> "Status", i.e. result of "Rcmd check", is listed in file "Status". 
> Possible values are "OK", "WARN", and "ERROR". Corresponding check.log 
> files can be found in subdirectory ./check.'
> 
> Uwe Ligges
> 
> 
> > 
> > Shawn Way

Thanks to Uwe, there is now a new version of Hmisc for Windows on CRAN.  The
error in the help file for sas.get which prevented the Windows version from
passing Rcmd check has been fixed.
---
Frank E Harrell Jr    Professor and Chair            School of Medicine
                      Department of Biostatistics    Vanderbilt University

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://www.stat.math.ethz.ch/mailman/listinfo/r-help



From jetches at iwh.on.ca  Fri Oct 17 15:21:38 2003
From: jetches at iwh.on.ca (Jacob Etches)
Date: Fri, 17 Oct 2003 09:21:38 -0400
Subject: [R] datetime data and plotting
Message-ID: <C91DF477A51618409DB4B2FC262215DE0854C8@iwhmail01.iwh.on.ca>

If I take the following simple data:

YEAR MONTH DAY WEIGHT.KG
2003 10 6 1.2
2003 10 12 1.2
2003 10 16 1.3

and format the date data and plot it:

dates <- strptime(paste(DAY,MONTH,YEAR),"%d%m%Y")
plot(c(min(dates),max(dates)),c(0,max(WEIGHT.KG)),
 xlab="Date",ylab="Weight (kg)",type="n")
 lines(dates,WEIGHT.KG)
 points(dates,WEIGHT.KG)

I find that the data points are all plotted at (x-1,y),
where x is in days.  Have I requested this behaviour 
accidentally?  I'm using R-1.8 on OS X.

Printing the dates object looks correct, and simple 
manipulations such as max(dates)-min(dates) behave 
normally.

Jacob Etches

Doctoral candidate
Dept of Public Health Sciences
University of Toronto


From andy_liaw at merck.com  Fri Oct 17 14:59:18 2003
From: andy_liaw at merck.com (Liaw, Andy)
Date: Fri, 17 Oct 2003 08:59:18 -0400
Subject: [R] Problems with crossprod
Message-ID: <3A822319EB35174CA3714066D590DCD50205CCD0@usrymx25.merck.com>

Somehow R creates `a' as a matrix with 0 rows and 5 columns.  I don't know
how crossprod() or other linear algebra functions deals with such a
degenerate matrix.

I'd suggest R Core to add checks for strictly positive dimensions in such
functions.

(Also, I find it strange that A[1,] is a vector, but A[numeric(0),] is a 0x5
matrix...)

Andy

> From: Giovanni Marchetti [mailto:gmm at ds.unifi.it] 
> 
> Dear R-users, 
> 
> I found a strange problem 
> working with products of two matrices, say: 
>  
> a <- A[i, ] ; crossprod(a)
> 
> where i is a set of integers selecting rows. When i is empty 
> the result is in a sense random.
> 
> After some trials the right answer 
> (a matrix of zeros) appears.
> 
> --------------- Illustration --------------------
> R : Copyright 2003, The R Development Core Team
> Version 1.8.0  (2003-10-08)
> 
> > A  <-matrix(0, 5, 5)
> > i <- c()
> > a <- A[i, ] ; crossprod(a)
>               [,1]          [,2]          [,3]          [,4] [,5]
> [1,] 6.578187e-313           NaN           NaN           NaN  NaN
> [2,]           NaN 1.273197e-313           NaN 1.485397e-313  NaN
> [3,]           NaN 4.243992e-313 2.121996e-314           NaN  NaN
> [4,]           NaN 1.697597e-313           NaN 4.880590e-313  NaN
> [5,] 5.941588e-313           NaN           NaN 1.697597e-313  NaN
> > a <- A[i, ] ; crossprod(a)
>               [,1]          [,2]          [,3]          [,4]  
>         [,5]
> [1,] 2.121996e-314 5.729389e-313           NaN           NaN  
>          NaN
> [2,]           NaN           NaN           NaN           NaN 
> 1.909796e-313
> [3,] 2.970794e-313           NaN           NaN           NaN  
>          NaN
> [4,]           NaN           NaN           NaN 8.487983e-314  
>          NaN
> [5,]           NaN 6.365987e-313 2.546395e-313           NaN  
>          NaN
> > a <- A[i, ] ; crossprod(a)
>               [,1]          [,2] [,3]          [,4]          [,5]
> [1,]           NaN 1.485397e-313  NaN           NaN 2.970794e-313
> [2,] 3.182994e-313           NaN  NaN 1.060998e-313           NaN
> [3,]           NaN           NaN  NaN 1.697597e-313 2.737375e-312
> [4,]           NaN           NaN  NaN           NaN  2.048394e+10
> [5,]           NaN           NaN  NaN           NaN 2.970794e-313
> > a <- A[i, ] ; crossprod(a)
>               [,1]        [,2] [,3] [,4] [,5]
> [1,] 1.591383e-266 20489834629    0    0    0
> [2,] 5.031994e-266           0    0    0    0
> [3,] 1.591205e-266           0    0    0    0
> [4,] 1.264128e-267           0    0    0    0
> [5,] 1.037656e-311           0    0    0    0
> > a <- A[i, ] ; crossprod(a)
>      [,1] [,2] [,3] [,4] [,5]
> [1,]    0    0    0    0    0
> [2,]    0    0    0    0    0
> [3,]    0    0    0    0    0
> [4,]    0    0    0    0    0
> [5,]    0    0    0    0    0
> --------------- End of illustration------------
> 
> The same problem does not appear using the matrix product:
> 
> > a <- A[i, ] ; t(a) %*% a
>      [,1] [,2] [,3] [,4] [,5]
> [1,]    0    0    0    0    0
> [2,]    0    0    0    0    0
> [3,]    0    0    0    0    0
> [4,]    0    0    0    0    0
> [5,]    0    0    0    0    0
> 
> Note that Splus 6 returns an error message:
> 
> > a <- A[i, ] ; crossprod(a)
> 
> Problem in .Fortran.ok.Internal(if(cmplx) "zcrossp1"..: 
> subroutine dcrossp1: 
> Argument 1 has zero length
> 
> 
> Thank you, 
> 
> Giovanni
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list 
> https://www.stat.math.ethz.ch/mailman/listinfo> /r-help
>



From Ivar.Herfindal at bio.ntnu.no  Fri Oct 17 15:49:06 2003
From: Ivar.Herfindal at bio.ntnu.no (Ivar Herfindal)
Date: Fri, 17 Oct 2003 15:49:06 +0200
Subject: [R] sort charcters in W2K and NT
In-Reply-To: <3F8FBD59.7070909@statistik.uni-dortmund.de>
References: <oprw6i0rmundboo6@mail.bio.ntnu.no>
	<3F8FBD59.7070909@statistik.uni-dortmund.de>
Message-ID: <oprw6wn4ntndboo6@mail.bio.ntnu.no>

On Fri, 17 Oct 2003 11:58:49 +0200, Uwe Ligges <ligges at statistik.uni- 
dortmund.de> wrote:

> Ivar Herfindal wrote:
>
>> Hello.
>>
>> I have a problem using sort() in windows 2000 and windows NT 4.0, 
>> running R 1.8.0 on both. I want to sort a vector of characters names, 
>> where I have used "Scandinavian" letters, like '?', '?', and '?' (for 
>> those who cannot display these letters this question seems rather 
>> meaningless, i guess). Windows 2000 sorts the vector like I am used to 
>> from other software, with '?' as the last letter in the alphabet, while 
>> windows NT has "?" just after "A", and "?" following "O".
>>
>> Is there a way to solve this problem (other than replace the 
>> Scandinavian letters)?
>>
>> A short example:
>> sort(c('a','p','?'))
>> # on windows 2000:
>> [1] "a" "p" "?"
>>
>> # on windows NT
>> [1] "a" "?" "p"
>>
>> Thanks in advance
>
> ?sort tells us:
>
> "The sort order for character vectors will depend on the collating 
> sequence of the locale in use: see Comparison."
>
> and ?Comparison points you to ?locales which gives an example:
>
> Sys.setlocale("LC_COLLATE", "C")  # turn off locale-specific sorting
>
> Uwe Ligges
>

Thanks for the help, it worked great. However, it appers that using the 
Sys.setlocale("LC_COLLATE", "C") makes R sort the vector in a new way, 
different from the two mentioned above. But since R sorts character vectors 
at same manner on both W2K and Window NT, after writing 
Sys.setlocale("LC_COLLATE", "C"), it is sufficient for me.

Ivar Herfindal

>
>> Ivar Herfindal
>>
>> On windows 2000:
>>
>>> version
>>
>> _              platform i386-pc-mingw32
>> arch     i386           os       mingw32        system   i386, mingw32  
>> status                  major    1              minor    8.0            
>> year     2003           month    10             day      08             
>> language R
>>
>>>
>>
>> On windows NT:
>>
>>> version
>>
>> _              platform i386-pc-mingw32
>> arch     i386           os       mingw32        system   i386, mingw32  
>> status                  major    1              minor    8.0            
>> year     2003           month    10             day      08             
>> language R
>>
>>>
>>
>> ______________________________________________
>> R-help at stat.math.ethz.ch mailing list
>> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
>
>
>



From tlumley at u.washington.edu  Fri Oct 17 15:49:54 2003
From: tlumley at u.washington.edu (Thomas Lumley)
Date: Fri, 17 Oct 2003 06:49:54 -0700 (PDT)
Subject: [R] R memory and CPU requirements
In-Reply-To: <20031017083359.953.qmail@web14914.mail.yahoo.com>
References: <20031017083359.953.qmail@web14914.mail.yahoo.com>
Message-ID: <Pine.A41.4.58.0310170647260.180024@homer07.u.washington.edu>

On Fri, 17 Oct 2003, Alexander Sirotkin [at Yahoo] wrote:

>
> I did study ANOVA and I do have enough observations.
> 200 was only a random sample of more then 5000 which I
> think should be enough. However, I'm afraid to even
> think about amount of RAM I will need with R to fit a
> model for this data.
>

The memory requirements depend on the size of the design matrix. If the
number of columns in the design matrix doesn't increase then 5000
observations won't be much worse.  If it does increase then you have the
same problem of sparseness even with 5000 observations.

There must be *something* strange about your model. I routinely fit
regression models with 6000 observations and a dozen or so variables in R,
in much less than 2Gb of RAM.

	-thomas

Thomas Lumley			Assoc. Professor, Biostatistics
tlumley at u.washington.edu	University of Washington, Seattle



From tlumley at u.washington.edu  Fri Oct 17 15:54:22 2003
From: tlumley at u.washington.edu (Thomas Lumley)
Date: Fri, 17 Oct 2003 06:54:22 -0700 (PDT)
Subject: [R] Problems with crossprod
In-Reply-To: <3A822319EB35174CA3714066D590DCD50205CCD0@usrymx25.merck.com>
References: <3A822319EB35174CA3714066D590DCD50205CCD0@usrymx25.merck.com>
Message-ID: <Pine.A41.4.58.0310170653060.180024@homer07.u.washington.edu>

On Fri, 17 Oct 2003, Liaw, Andy wrote:

> Somehow R creates `a' as a matrix with 0 rows and 5 columns.  I don't know
> how crossprod() or other linear algebra functions deals with such a
> degenerate matrix.
>
> I'd suggest R Core to add checks for strictly positive dimensions in such
> functions.

Yes. There usually are, which  is why the matrix multiplication version
works

> (Also, I find it strange that A[1,] is a vector, but A[numeric(0),] is a 0x5
> matrix...)
>

Why?  A[1,] is a 1x5 matrix, ie, a column vector, so it makes sense for it
to decay to a vector.  A[numeric(0),] is not a vector, so it stays a
matrix.

	-thomas



From ripley at stats.ox.ac.uk  Fri Oct 17 16:02:28 2003
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Fri, 17 Oct 2003 15:02:28 +0100 (BST)
Subject: [R] sort charcters in W2K and NT
In-Reply-To: <oprw6wn4ntndboo6@mail.bio.ntnu.no>
Message-ID: <Pine.LNX.4.44.0310171501220.1203-100000@gannet.stats>

You can set any locale you like, and I suspect your machines are in 
different locales (I believe older versions of Windows, including NT4, had 
limited support for locales).

On Fri, 17 Oct 2003, Ivar Herfindal wrote:

> On Fri, 17 Oct 2003 11:58:49 +0200, Uwe Ligges <ligges at statistik.uni- 
> dortmund.de> wrote:
> 
> > Ivar Herfindal wrote:
> >
> >> Hello.
> >>
> >> I have a problem using sort() in windows 2000 and windows NT 4.0, 
> >> running R 1.8.0 on both. I want to sort a vector of characters names, 
> >> where I have used "Scandinavian" letters, like '?', '?', and '?' (for 
> >> those who cannot display these letters this question seems rather 
> >> meaningless, i guess). Windows 2000 sorts the vector like I am used to 
> >> from other software, with '?' as the last letter in the alphabet, while 
> >> windows NT has "?" just after "A", and "?" following "O".
> >>
> >> Is there a way to solve this problem (other than replace the 
> >> Scandinavian letters)?
> >>
> >> A short example:
> >> sort(c('a','p','?'))
> >> # on windows 2000:
> >> [1] "a" "p" "?"
> >>
> >> # on windows NT
> >> [1] "a" "?" "p"
> >>
> >> Thanks in advance
> >
> > ?sort tells us:
> >
> > "The sort order for character vectors will depend on the collating 
> > sequence of the locale in use: see Comparison."
> >
> > and ?Comparison points you to ?locales which gives an example:
> >
> > Sys.setlocale("LC_COLLATE", "C")  # turn off locale-specific sorting
> >
> > Uwe Ligges
> >
> 
> Thanks for the help, it worked great. However, it appers that using the 
> Sys.setlocale("LC_COLLATE", "C") makes R sort the vector in a new way, 
> different from the two mentioned above. But since R sorts character vectors 
> at same manner on both W2K and Window NT, after writing 
> Sys.setlocale("LC_COLLATE", "C"), it is sufficient for me.
> 
> Ivar Herfindal
> 
> >
> >> Ivar Herfindal
> >>
> >> On windows 2000:
> >>
> >>> version
> >>
> >> _              platform i386-pc-mingw32
> >> arch     i386           os       mingw32        system   i386, mingw32  
> >> status                  major    1              minor    8.0            
> >> year     2003           month    10             day      08             
> >> language R
> >>
> >>>
> >>
> >> On windows NT:
> >>
> >>> version
> >>
> >> _              platform i386-pc-mingw32
> >> arch     i386           os       mingw32        system   i386, mingw32  
> >> status                  major    1              minor    8.0            
> >> year     2003           month    10             day      08             
> >> language R
> >>
> >>>
> >>
> >> ______________________________________________
> >> R-help at stat.math.ethz.ch mailing list
> >> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> >
> >
> >
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> 
> 

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From tlumley at u.washington.edu  Fri Oct 17 16:02:47 2003
From: tlumley at u.washington.edu (Thomas Lumley)
Date: Fri, 17 Oct 2003 07:02:47 -0700 (PDT)
Subject: [R] RE: [S] Dynamic Memory Allocation in R
In-Reply-To: <3A822319EB35174CA3714066D590DCD50205CCCE@usrymx25.merck.com>
References: <3A822319EB35174CA3714066D590DCD50205CCCE@usrymx25.merck.com>
Message-ID: <Pine.A41.4.58.0310170655030.180024@homer07.u.washington.edu>

On Fri, 17 Oct 2003, Liaw, Andy wrote:

> > From: Gamal Abdel-Azim [mailto:gamal at crinet.com]
> >
> > While trying to expand the memory/object size in R, I noticed
> > that R might be using
> > only heap memory. Is this true? Are all objects in R created
> > in the heap not
> > allocated?

To the extent that this means anything it is true. All R objects are
stored in memory controlled by the R process, in two heaps.

The "g.data" package allows objects to be stored on disk and only loaded
as necessary.

>>		 It's not logical that this is the case!! Otherwise
> > the whole R project would
> > be a total waste of time and resources.

There appear to be some missing steps in this reasoning.

> > If I am wrong please inform me. How to increase memory.size
> > in R? Is there a way
> > similar to options(object.size=size) in S-Plus. Notice that
> > the command R --max- vsize=N targets the heap memory!

The --max-vsize option exists to *reduce* the maximum memory use, not to
increase it (at least under Unix).  You have access to all the memory your
operating system will give you.

> >
> > I have recently installed R on a Linux machine (3GB RAM and
> > sufficiently large HD).

Then you should be able to access nearly 4Gb in R, which is enough for
quite a lot of purposes.

> > Sorry for posting to Splus not R. But if R works only in the
> > heap, I may not need to
> > subscribe to R-news at all.

 !

	-thomas



From ripley at stats.ox.ac.uk  Fri Oct 17 16:20:59 2003
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Fri, 17 Oct 2003 15:20:59 +0100 (BST)
Subject: [R] Problems with crossprod
In-Reply-To: <3A822319EB35174CA3714066D590DCD50205CCD0@usrymx25.merck.com>
Message-ID: <Pine.LNX.4.44.0310171510310.1203-100000@gannet.stats>

On Fri, 17 Oct 2003, Liaw, Andy wrote:

> Somehow R creates `a' as a matrix with 0 rows and 5 columns.  I don't know
> how crossprod() or other linear algebra functions deals with such a
> degenerate matrix.
> 
> I'd suggest R Core to add checks for strictly positive dimensions in such
> functions.

Rather, we do try to ensure they give the right answers for 0 extents.
The code has lines like

    if (nrx > 0 && ncx > 0 && nry > 0 && ncy > 0) {
        F77_CALL(dgemm)(transa, transb, &ncx, &ncy, &nrx, &one,
			x, &nrx, y, &nry, &zero, z, &ncx);
    }

and so does nothing for 0 extents!  The corresponding matprod code does, 
so this seems a simple oversight that will be fixed shortly.

> (Also, I find it strange that A[1,] is a vector, but A[numeric(0),] is a 0x5
> matrix...)

That's the point of drop = TRUE (the default): it drops extents of length 
1 (and not 0).


> 
> Andy
> 
> > From: Giovanni Marchetti [mailto:gmm at ds.unifi.it] 
> > 
> > Dear R-users, 
> > 
> > I found a strange problem 
> > working with products of two matrices, say: 
> >  
> > a <- A[i, ] ; crossprod(a)
> > 
> > where i is a set of integers selecting rows. When i is empty 
> > the result is in a sense random.
> > 
> > After some trials the right answer 
> > (a matrix of zeros) appears.
> > 
> > --------------- Illustration --------------------
> > R : Copyright 2003, The R Development Core Team
> > Version 1.8.0  (2003-10-08)
> > 
> > > A  <-matrix(0, 5, 5)
> > > i <- c()
> > > a <- A[i, ] ; crossprod(a)
> >               [,1]          [,2]          [,3]          [,4] [,5]
> > [1,] 6.578187e-313           NaN           NaN           NaN  NaN
> > [2,]           NaN 1.273197e-313           NaN 1.485397e-313  NaN
> > [3,]           NaN 4.243992e-313 2.121996e-314           NaN  NaN
> > [4,]           NaN 1.697597e-313           NaN 4.880590e-313  NaN
> > [5,] 5.941588e-313           NaN           NaN 1.697597e-313  NaN
> > > a <- A[i, ] ; crossprod(a)
> >               [,1]          [,2]          [,3]          [,4]  
> >         [,5]
> > [1,] 2.121996e-314 5.729389e-313           NaN           NaN  
> >          NaN
> > [2,]           NaN           NaN           NaN           NaN 
> > 1.909796e-313
> > [3,] 2.970794e-313           NaN           NaN           NaN  
> >          NaN
> > [4,]           NaN           NaN           NaN 8.487983e-314  
> >          NaN
> > [5,]           NaN 6.365987e-313 2.546395e-313           NaN  
> >          NaN
> > > a <- A[i, ] ; crossprod(a)
> >               [,1]          [,2] [,3]          [,4]          [,5]
> > [1,]           NaN 1.485397e-313  NaN           NaN 2.970794e-313
> > [2,] 3.182994e-313           NaN  NaN 1.060998e-313           NaN
> > [3,]           NaN           NaN  NaN 1.697597e-313 2.737375e-312
> > [4,]           NaN           NaN  NaN           NaN  2.048394e+10
> > [5,]           NaN           NaN  NaN           NaN 2.970794e-313
> > > a <- A[i, ] ; crossprod(a)
> >               [,1]        [,2] [,3] [,4] [,5]
> > [1,] 1.591383e-266 20489834629    0    0    0
> > [2,] 5.031994e-266           0    0    0    0
> > [3,] 1.591205e-266           0    0    0    0
> > [4,] 1.264128e-267           0    0    0    0
> > [5,] 1.037656e-311           0    0    0    0
> > > a <- A[i, ] ; crossprod(a)
> >      [,1] [,2] [,3] [,4] [,5]
> > [1,]    0    0    0    0    0
> > [2,]    0    0    0    0    0
> > [3,]    0    0    0    0    0
> > [4,]    0    0    0    0    0
> > [5,]    0    0    0    0    0
> > --------------- End of illustration------------
> > 
> > The same problem does not appear using the matrix product:
> > 
> > > a <- A[i, ] ; t(a) %*% a
> >      [,1] [,2] [,3] [,4] [,5]
> > [1,]    0    0    0    0    0
> > [2,]    0    0    0    0    0
> > [3,]    0    0    0    0    0
> > [4,]    0    0    0    0    0
> > [5,]    0    0    0    0    0
> > 
> > Note that Splus 6 returns an error message:
> > 
> > > a <- A[i, ] ; crossprod(a)
> > 
> > Problem in .Fortran.ok.Internal(if(cmplx) "zcrossp1"..: 
> > subroutine dcrossp1: 
> > Argument 1 has zero length
> > 
> > 
> > Thank you, 
> > 
> > Giovanni
> > 
> > ______________________________________________
> > R-help at stat.math.ethz.ch mailing list 
> > https://www.stat.math.ethz.ch/mailman/listinfo> /r-help
> >
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> 
> 

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From deepayan at stat.wisc.edu  Fri Oct 17 16:04:33 2003
From: deepayan at stat.wisc.edu (Deepayan Sarkar)
Date: Fri, 17 Oct 2003 09:04:33 -0500
Subject: [R] help with legend()
In-Reply-To: <16271.38970.646500.488205@gargle.gargle.HOWL>
References: <8E46EB3BC001414AA6CDB57C5E551F8D12F209@thuja>
	<16271.38970.646500.488205@gargle.gargle.HOWL>
Message-ID: <200310170904.34303.deepayan@stat.wisc.edu>

On Friday 17 October 2003 02:20, Martin Maechler wrote:
> >>>>> "PaulSch" == Schwarz, Paul <paul.schwarz at oregonstate.edu>
> >>>>>     on Wed, 15 Oct 2003 12:09:11 -0700 writes:
>
>     PaulSch> I am converting some S-PLUS scripts that I use for
>     PaulSch> creating manuscript figures to R so that I can take
>     PaulSch> advantage of the plotmath capabilities.  In my
>     PaulSch> S-PLUS scripts I like to use the key() function for
>     PaulSch> adding legends to plots,
>
> AFAIK  key() in S+ is from the trellis library section.
> The corresponding R package, trellis, has
                               ^^^^^^^
lattice, actually :-)

> a draw.key() function that may work similarly to S-plus' key()
> {Deepayan ?}.

That's correct. Of course, the S-PLUS key() works wih non-trellis graphs as 
well, whereas draw.key() will produce a grid object and hence work with grid 
graphics only. (I haven't checked Paul's new gridBase package, that may 
enable using this for base graphics as well.)

>     PaulSch> and I have a couple of
>     PaulSch> questions regarding using the legend() function in
>     PaulSch> R.
>
>     PaulSch> 1) is there a way to specify different colors for
>     PaulSch> the legend vector of text values?
>
> not yet in legend() -- but see below
>
>     PaulSch> 2) is there a way to reverse the order of the
>     PaulSch> legend items so that the text values precede the
>     PaulSch> symbols?
>
> not yet in legend()   --- but it's an open source project living
> 		       from "community support" ...
>
> Can S+ key() do these two things?
> If yes, how do you specify it there
> {this sounds as if I was willing to consider adding these wished
>  features to legend .... }

key() is a bit weird, in that it allows multiple arguments of the same name 
(as long as the names are text, points, lines and rectangles). The order of 
the arguments control the order of column types.

For example, 

key(text = list(letters[1:5], col = 1:5), 
    points = list(col = 1:5),
    text = list(letters[6:10]))

will produce a column of text followed by points and then text again (with the 
first two columns in different color).

Deepayan



From debene at unimc.it  Fri Oct 17 16:36:47 2003
From: debene at unimc.it (Luca De Benedictis)
Date: Fri, 17 Oct 2003 16:36:47 +0200
Subject: [R] correlation matrix in Hmisc
Message-ID: <3F8FFE7F.7060502@unimc.it>

Dear all,
I am trying to compute a  matrix of Pearson's `r' or Spearman's `rho' 
rank correlation coefficients using rcorr (Hmisc) the following way:

 > mx<-rcorr(x, type="spearman")[1]

but then ...

 > is.matrix(mx)
[1] FALSE

Even if  I use as.matrix the result is not better.
What can I do?

Thank you all

Luca



From feh3k at spamcop.net  Fri Oct 17 16:47:10 2003
From: feh3k at spamcop.net (Frank E Harrell Jr)
Date: Fri, 17 Oct 2003 10:47:10 -0400
Subject: [R] correlation matrix in Hmisc
In-Reply-To: <3F8FFE7F.7060502@unimc.it>
References: <3F8FFE7F.7060502@unimc.it>
Message-ID: <20031017104710.03c9ba8e.feh3k@spamcop.net>

On Fri, 17 Oct 2003 16:36:47 +0200
Luca De Benedictis <debene at unimc.it> wrote:

> Dear all,
> I am trying to compute a  matrix of Pearson's `r' or Spearman's `rho' 
> rank correlation coefficients using rcorr (Hmisc) the following way:
> 
>  > mx<-rcorr(x, type="spearman")[1]

Instead of [1] use $r.  Or use the new cor() function builtin to R 1.8.

Frank

> 
> but then ...
> 
>  > is.matrix(mx)
> [1] FALSE
> 
> Even if  I use as.matrix the result is not better.
> What can I do?
> 
> Thank you all
> 
> Luca
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help


---
Frank E Harrell Jr    Professor and Chair            School of Medicine
                      Department of Biostatistics    Vanderbilt University



From deepayan at stat.wisc.edu  Fri Oct 17 16:53:38 2003
From: deepayan at stat.wisc.edu (Deepayan Sarkar)
Date: Fri, 17 Oct 2003 09:53:38 -0500
Subject: [R] R memory and CPU requirements
In-Reply-To: <20031017083359.953.qmail@web14914.mail.yahoo.com>
References: <20031017083359.953.qmail@web14914.mail.yahoo.com>
Message-ID: <200310170953.38600.deepayan@stat.wisc.edu>

On Friday 17 October 2003 03:33, Alexander Sirotkin \[at Yahoo\] wrote:

> > > > > One more (hopefully last one) : I've been very
> > > > > surprised when I tried to fit a model (using
> > > > > aov())
> > > > > for a sample of size 200 and 10 variables and
> > > > > their interactions.
> > > >
> > > > That doesn't really say much. How many of these
> > > > variables are factors ? How
> > > > many levels do they have ? And what is the order
> > > > of the interaction ? (Note
> > > > that for 10 numeric variables, if you allow all
> > > > interactions, then there will
> > > > be a 100 terms in your model. This increases for
> > > > factors.)
> > > >
> > > > In other words, how big is your model matrix ?
> > >
> > > I see...
> > >
> > > Unfortunately, model.matrix() ran out of memory :)
> > > I have 10 variables, 6 of which are factor, 2 of
> > which
> > > have quite a lot of levels (about 40). And I would
> > > like to allow all interactions.
> > >
> > > I understand your point about categorical
> >
> > variables,
> >
> > > but still - this does not seem like too much data
> >
> > to me.
> >
> > That's one way to look at it. You don't have enough
> > data for the model you are
> > trying to fit. The usual approach under these
> > circumstances is to try
> > 'simpler' models.
> >
> > Please try to understand what you are trying to do
> > (in this case by reading an
> > introductory linear model text) before blindly
> > applying a methodology.
> >
> > Deepayan
>
> I did study ANOVA and I do have enough observations.
> 200 was only a random sample of more then 5000 which I
> think should be enough. However, I'm afraid to even
> think about amount of RAM I will need with R to fit a
> model for this data.

Let's see. You have 10 variables, 6 of which are factors, 2 of which have at 
least 40 levels, and you want all interactions. Let's conservatively estimate 
that all the other four factors have only 2 levels. 

> x1 = gl(40, 1, 1)
> x2 = gl(40, 1, 1)
> x3 = gl(2, 1, 1)
> x4 = gl(2, 1, 1)
> x5 = gl(2, 1, 1)
> x6 = gl(2, 1, 1)

> dim(model.matrix(~ x1 * x2 * x3 * x4 * x5 * x6))
[1]     1 25600

This was for one data point, increasing that would only increase the number of 
rows, the columns would be the same. And of course, this is just for 6-way 
interactions, and the least possible given the information you have given us 
about your model. In actual fact, your model matrix will have many many more 
columns.

I hope you realize that the number of columns in the model matrix is the 
number of parameters you are trying to estimate. If your sample size is less 
than this number (and 5000 is way less), then there will be infinitely many 
solutions to this problem, each of which will fit your data perfectly. Do you 
really want such an answer ? Assuming that you find one, what are you going 
to do with it ?

I have no idea what made you choose such an high order model, but as Andy has 
said, you really should try to figure out what exactly your goals are before 
proceeding. If you believe that your data can really not be modeled 
reasonably by anything simpler, you probably should not use a linear model at 
all. 

Hope that helps,

Deepayan



From phgrosjean at sciviews.org  Fri Oct 17 17:39:15 2003
From: phgrosjean at sciviews.org (Philippe Grosjean)
Date: Fri, 17 Oct 2003 17:39:15 +0200
Subject: [R] Problems Building RMySQL in Windows
In-Reply-To: <3F8EE0F6.5020203@telgua.com.gt>
Message-ID: <MABBLJDICACNFOLGIHJOOEKADNAA.phgrosjean@sciviews.org>

Regarding the very few tests I did (RMySQL versus RODBC using a MySQL ODBC
driver, but I do not remember details here), RMySQL is faster. It should be
great, if you need to access a MySQL database from R, to try both and decide
by yourself. If you do that, I am very interested by the results.
Best,

Philippe Grosjean

-----Original Message-----
From: r-help-bounces at stat.math.ethz.ch
[mailto:r-help-bounces at stat.math.ethz.ch]On Behalf Of Hector Villafuerte
D.
Sent: Thursday, 16 October, 2003 20:19
To: r-help at stat.math.ethz.ch
Subject: Re: [R] Problems Building RMySQL in Windows


David Whiting wrote:

>Can you use RODBC instead?  I use it all the time and find it works
>very well.  I installed it some time ago and have forgotten the exact
>details but I remember that it was easy to do following the
>instructions.  I also can't remember why I choose RODBC instead of
>RMySQL and whether there are advantages of RMySQL over RODBC.
>
>

Great! Using RODBC is really easy!
Would someone please comment on the pros and cons of RODBC compared with
RMySQL?
Thanks in advance.
Hector

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://www.stat.math.ethz.ch/mailman/listinfo/r-help



From CMiller at PICR.man.ac.uk  Fri Oct 17 17:43:44 2003
From: CMiller at PICR.man.ac.uk (Crispin Miller)
Date: Fri, 17 Oct 2003 16:43:44 +0100
Subject: [R] environments
Message-ID: <BAA35444B19AD940997ED02A6996AAE0B5C226@sanmail.picr.man.ac.uk>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20031017/a0417347/attachment.pl

From hec.villafuerte at telgua.com.gt  Fri Oct 17 20:00:22 2003
From: hec.villafuerte at telgua.com.gt (=?ISO-8859-1?Q?=22H=E9ctor_Villafuerte_D=2E=22?=)
Date: Fri, 17 Oct 2003 10:00:22 -0800
Subject: [R] Problems Building RMySQL in Windows
In-Reply-To: <MABBLJDICACNFOLGIHJOOEKADNAA.phgrosjean@sciviews.org>
References: <MABBLJDICACNFOLGIHJOOEKADNAA.phgrosjean@sciviews.org>
Message-ID: <3F902E36.5070805@telgua.com.gt>

Philippe Grosjean wrote:

>Regarding the very few tests I did (RMySQL versus RODBC using a MySQL ODBC
>driver, but I do not remember details here), RMySQL is faster. It should be
>great, if you need to access a MySQL database from R, to try both and decide
>by yourself. If you do that, I am very interested by the results.
>
I would gladly do such comparison.... but I was unable to install RMySQL 
successfully (it keeps crashing R).
I think I'll have to stick with RODBC.
Thank you all, anyway.



From brahm at alum.mit.edu  Fri Oct 17 18:04:04 2003
From: brahm at alum.mit.edu (David Brahm)
Date: Fri, 17 Oct 2003 12:04:04 -0400
Subject: [R] [R-pkgs] Updated package: g.data v1.4
Message-ID: <16272.4852.615349.178004@arbres1a.fmr.com>

Version 1.4 of package "g.data" is available on CRAN.  This upgrade is
necessary for it to work under R-1.8.0, and is fully backward compatible.

Description: Create and maintain delayed-data packages (DDP's).  Data stored in
  a DDP are available on demand, but do not take up memory until requested.
  You attach a DDP with g.data.attach(), then read from it and assign to it in
  a manner similar to S-Plus, except that you must run g.data.save() to
  actually commit to disk.

Thanks very much to <brian at dumbaz.com> for pointing out the incompatibility.
(Sorry, Brian, a direct reply to you bounced.)  "g.data" basically creates mock
packages (DDP's) to contain the data, and in R-1.8.0 a package needs a
DESCRIPTION file to be recognized by .find.package().  Note you will need
temporary write access to any existing (pre-1.4) DDP's, as g.data.attach() will
try to create a DESCRIPTION file for any DDP that doesn't already have one.
-- 
                              -- David Brahm (brahm at alum.mit.edu)

_______________________________________________
R-packages mailing list
R-packages at stat.math.ethz.ch
https://www.stat.math.ethz.ch/mailman/listinfo/r-packages



From ripley at stats.ox.ac.uk  Fri Oct 17 18:05:33 2003
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Fri, 17 Oct 2003 17:05:33 +0100 (BST)
Subject: [R] datetime data and plotting
In-Reply-To: <C91DF477A51618409DB4B2FC262215DE0854C8@iwhmail01.iwh.on.ca>
Message-ID: <Pine.LNX.4.44.0310171700500.28312-100000@gannet.stats>

I am not seeing this on Linux.  The x axis marks are at midnight GMT, 
hence 1am BST on my system.

On Fri, 17 Oct 2003, Jacob Etches wrote:

> If I take the following simple data:
> 
> YEAR MONTH DAY WEIGHT.KG
> 2003 10 6 1.2
> 2003 10 12 1.2
> 2003 10 16 1.3
> 
> and format the date data and plot it:
> 
> dates <- strptime(paste(DAY,MONTH,YEAR),"%d%m%Y")
> plot(c(min(dates),max(dates)),c(0,max(WEIGHT.KG)),
>  xlab="Date",ylab="Weight (kg)",type="n")
>  lines(dates,WEIGHT.KG)
>  points(dates,WEIGHT.KG)
> 
> I find that the data points are all plotted at (x-1,y),
> where x is in days.  Have I requested this behaviour 
> accidentally?  I'm using R-1.8 on OS X.
> 
> Printing the dates object looks correct, and simple 
> manipulations such as max(dates)-min(dates) behave 
> normally.
> 
> Jacob Etches
> 
> Doctoral candidate
> Dept of Public Health Sciences
> University of Toronto
> 
> 

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From comm at 263.net  Fri Oct 17 18:09:08 2003
From: comm at 263.net (Jean Sun)
Date: Sat, 18 Oct 2003 0:9:8 +0800
Subject: [R] about parameter fitting of Gld(Generalized Lambda Distribution)
Message-ID: <20031017160835.5257834062@smtp.263.net>

Currently, I am intrested in parameter fitting of "Generalized Lambda Distribution".And I have found two packages in R related to Gld,Davies and gld. What's a pity that no method in Davies deals with fitting of gld,and "starship" used in package:gld is quite time-consuming when sample size is large.

So i wonder if there is "method of moments" or "least square" implementation available at now.

or

if u know the "method of moments"(or moment matching), i think u must know how to solve a optimization problem of two parameters,involing multiple "beta" functions.plz give me some hints. Thanks in advance.

Regards,
Jean Sun



From ligges at statistik.uni-dortmund.de  Fri Oct 17 18:13:08 2003
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Fri, 17 Oct 2003 18:13:08 +0200
Subject: [R] environments
In-Reply-To: <BAA35444B19AD940997ED02A6996AAE0B5C226@sanmail.picr.man.ac.uk>
References: <BAA35444B19AD940997ED02A6996AAE0B5C226@sanmail.picr.man.ac.uk>
Message-ID: <3F901514.4010804@statistik.uni-dortmund.de>

Crispin Miller wrote:
> Hi,
> I have a string representing an environment:
> 
> "bob"
> 
> And an environment
> 
>>bob
> 
> <environment: 0x3901234ac>
> How do write a function that takes the string and returns the
> environment?

  get("bob")

Uwe Ligges

> Crispin
>  
> --------------------------------------------------------
> 
>  
> This email is confidential and intended solely for the use o...{{dropped}}
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help



From ripley at stats.ox.ac.uk  Fri Oct 17 18:07:49 2003
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Fri, 17 Oct 2003 17:07:49 +0100 (BST)
Subject: [R] environments
In-Reply-To: <BAA35444B19AD940997ED02A6996AAE0B5C226@sanmail.picr.man.ac.uk>
Message-ID: <Pine.LNX.4.44.0310171706470.28312-100000@gannet.stats>

Is get("bob") what you are looking for?

It is the usual way to go from the name of an R object (as a character 
string) to the actual object.

On Fri, 17 Oct 2003, Crispin Miller wrote:

> Hi,
> I have a string representing an environment:
> 
> "bob"
> 
> And an environment
> > bob
> <environment: 0x3901234ac>
> How do write a function that takes the string and returns the
> environment?

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From deepayan at stat.wisc.edu  Fri Oct 17 17:53:40 2003
From: deepayan at stat.wisc.edu (Deepayan Sarkar)
Date: Fri, 17 Oct 2003 10:53:40 -0500
Subject: [R] tick marks and barchart
In-Reply-To: <51226.213.213.153.238.1066316639.squirrel@vefpostur.rhea.is>
References: <51226.213.213.153.238.1066316639.squirrel@vefpostur.rhea.is>
Message-ID: <200310171053.40087.deepayan@stat.wisc.edu>

On Thursday 16 October 2003 10:03, Stef?n Hrafn J?nsson wrote:
> Dear R community.
>
> I have two problems with figures. First deals with short vector on the
> x-axis and the second with two-panel barchart.
> 1) For demonstration I create the following pseudo data for three years,
> 2001:2003. The indicated plot looks fine except for the number of tick
> marks on the x-axis. I get seq(2001,2003,0.5). I want three and only three
> tick marks to indicate we have measure once a year not two times each
> year. (Having year 2001.5 is not that nice anyway). I tried
> as.factor(2001:2003) but this did not do what I want. I have considered
> having no labels and plot the year with text(y=-b,x=(2001,2002,2003),
> (2001:2003)  ) -b being some value less than 0. A simpler version is
> preferred.

You can suppress the axes during the first plot() call and then construct them 
manually:

##---------------
demo1 <- matrix(nrow=3, ncol =2, log(c(7,3,2,4,5,6))/log(7) , dimnames
                               =list(as.character(2001:2003),
                                c("Group A","Group B")) )

par(lab=c(3, 6,7) ,las=1  )

plot(x = (2001:2003), y = demo1[,1]*100, type = "l",
     lwd=3,ylim=c(0,100), xlab = "" , ylab="%",
     axes = FALSE, frame.plot = TRUE)

axis(2)
axis(1, at = 2001:2003)

lines(x=(2001:2003), y=demo1[,2]*100,
    lwd=3, xlab = "Year" )

##-----------------

A slightly better approach (not for your problem, but for what you are trying 
to do) would be to use matplot instead:

##----------------

matplot(x = 2001:2003, demo1 * 100, type = "l", lwd=3,
        ylim = c(0, 100), xlab = "", ylab="%",
        axes = FALSE, frame.plot = TRUE)

axis(2)
axis(1, at = 2001:2003)

##----------------

> 2) For the second problem I want to use the same data but create a
> barchart with two bars (Group A and group B) for 2001, same two groups for
> 2002 and same two for 2003. Group A would have blue bars and Group B  red
> bars.
> Would I use barchart() or panel.barchart()?  Looking in help(barchart) I
> find that I need to define a formula.  What would the x, y and g1 be in my
> case?

barchart() is the trellis/lattice function for drawing barcharts, the 
corresponding base function is barplot. In this case, what you want should be 
doable with

barplot(t(demo1), beside = TRUE)

HTH,

Deepayan



From search-awareness at word-of-mouth-connection.com  Fri Oct 17 21:23:33 2003
From: search-awareness at word-of-mouth-connection.com (search-awareness@word-of-mouth-connection.com)
Date: Fri, 17 Oct 2003 12:23:33 -0700
Subject: [R] Someone just searched for word-of-mouth information about:
	r-help@lists.r-project.org
Message-ID: <200310171923.h9HJNXid001831@peter1.word-of-mouth-connection.com>


WordofMouthConnection.com Search Awareness System

This email is a website-generated message, but it is not spam.

An acquaintance of yours recently conducted a search on your email address in our online community, WordofMouthConnection.com. It could be a friend, a family member, co-worker, business associate, or someone else who's interested in learning more about you.

Why are we sending you this email?

When people find out others are talking about them -- whether it is good or bad -- they want to know. At WordofMouthConnection.com, we feel responsible to alert people so they have an opportunity to find out what is being said. 

Click here to view all "Word-of-Mouth Connections" in our system regarding this email address:

http://womc.net/pass.php?a=search&b=5&c=r-help at lists.r-project.org

How did we find you?

When your acquaintance searched for connections at our website he/she provided us with your email address.

What can you do?

First, we'd like to invite you to visit our website at www.WordofMouthConnection.com and learn about our service.  Registration is free. Not only can you find out if any WordofMouthConnection.com Connections have been submitted about you, but you can connect to others as well to research word-of-mouth information about your friends, co-workers, family, etc.

We are solely interested in fostering a community of willing people to promote information exchange. If you decide you're not interested, you can choose to block future emails from us. 

To add your email address to our Do Not Email List use the following link:

http://womc.net/pass.php?a=donotemail&b=r-help at lists.r-project.org&c=true

*Important - "Word-of-Mouth Connections" are only for the purpose of identifying the connection subject.  What you see is what you get, in terms of search results.  WordofMouthConnection.com is an online community that helps connect people so that they can talk at greater length and in greater detail. None of the information that is exchanged resides within the website itself.

If you have any questions or comments please email us at:

http://womc.net/pass.php?a=contact

Sincerely,
WordofMouthConnection.com



From maechler at stat.math.ethz.ch  Fri Oct 17 18:26:11 2003
From: maechler at stat.math.ethz.ch (Martin Maechler)
Date: Fri, 17 Oct 2003 18:26:11 +0200
Subject: [R] heatmap function
In-Reply-To: <3A822319EB35174CA3714066D590DCD50205CCD1@usrymx25.merck.com>
References: <3A822319EB35174CA3714066D590DCD50205CCD1@usrymx25.merck.com>
Message-ID: <16272.6179.371966.654404@gargle.gargle.HOWL>

>>>>> "AndyL" == Liaw, Andy <andy_liaw at merck.com>
>>>>>     on Fri, 17 Oct 2003 09:10:16 -0400 writes:

    AndyL> One of the good thing about R (and S in general, I
    AndyL> guess) is that if a function does mostly what you
    AndyL> want, except for some small things, you can just make
    AndyL> another copy of it, change the name, and make the
    AndyL> desired changes to the new function (provided the
    AndyL> changes you need to make isn't in the compiled code,
    AndyL> but R is Open Source...).

    AndyL> In this case, you should be able to strip out the
    AndyL> code in heatmap() that plot the top dendrogram w/o
    AndyL> much problem.  While your at it, you might want to
    AndyL> change the layout() so as not to leave the blank
    AndyL> space on top.

Yes, thanks Andy.

heatmap() has already been improved quite a bit for R 1.8.0
(and particularly the dendrogram reordering which lead to bad
 drawings has been fixed, the drawings are now fine).

But I have received many suggestions (from Gregory Warnes,
notably, and Art Owen, and others) that just didn't make it
anymore in time before feature freeze.

The above {an option for *dis*allowing one or the other
dendrogram} has been among the wishes, and is reasonable.

heatmap() being a relatively new function in R, and a "high
level" one (i.e. typically not used as basic building bloc for
other functions), not even usable as a sub-plot in other plots
because it relies on layout(), but also widely used
in some contexts I'd vote for being allowed to add features to
it even before the next major release of R.



    >> -----Original Message----- From: Martin Olivier
    >> [mailto:martinol at ensam.inra.fr] Sent: Friday, October 17,
    >> 2003 5:32 AM To: r-help Subject: [R] heatmap function
    >> 
    >> 
    >> Hi all,
    >> 
    >> By default, the heatmap function gives an image with a
    >> dendrogram added to the left side and to the top. Is it
    >> possible to only add the dendrogram to the left side and
    >> let the order of the columns unchanged ?
    >> 
    >> I tried heatmap(mat,
    >> col=rbg,Rowv=res.hclust$order,Colv=1:dim(mat)[[2]]).  In
    >> this case, the order of the columns are unchanged but a
    >> dendrogram is added to the top. How can I avoid it?
    >> 
    >> Thanks, Oiliver



From GPetris at uark.edu  Fri Oct 17 18:50:29 2003
From: GPetris at uark.edu (Giovanni Petris)
Date: Fri, 17 Oct 2003 11:50:29 -0500 (CDT)
Subject: [R] Modifying dim attribute of elements of a list
Message-ID: <200310171650.h9HGoTR4011617@definetti.uark.edu>


I am creating lists of vectors withing a loop. I also would like to
change the dim attribute to the vectors in order to make them
matrices. 

I have tried the following, but it doesn't work...

> sim <- c('simMeans','simVars','simWeights')
> indexTable <- table(modelIndex)
> for (i in sim) {
+     assign(tmp <- paste(i,'By',sep=''),split(get(i), modelIndex))
+     lapply(seq(along=indexTable),function(j) attr(get(tmp)[[j]],'dim') <<- c(indexTable[j],K))
+ }
Error in FUN(X[[1]], ...) : couldn't find function "get<-"
In addition: Warning message: 
argument lengths differ in: split(x, f) 

Any suggestions will be appreciated. 
Thanks



From search-awareness at word-of-mouth-connection.com  Fri Oct 17 21:51:06 2003
From: search-awareness at word-of-mouth-connection.com (search-awareness@word-of-mouth-connection.com)
Date: Fri, 17 Oct 2003 12:51:06 -0700
Subject: [R] Someone just searched for word-of-mouth information about:
	r-help@lists.r-project.org
Message-ID: <200310171951.h9HJp6UG002053@peter1.word-of-mouth-connection.com>


WordofMouthConnection.com Search Awareness System

This email is a website-generated message, but it is not spam.

An acquaintance of yours recently conducted a search on your email address in our online community, WordofMouthConnection.com. It could be a friend, a family member, co-worker, business associate, or someone else who's interested in learning more about you.

Why are we sending you this email?

When people find out others are talking about them -- whether it is good or bad -- they want to know. At WordofMouthConnection.com, we feel responsible to alert people so they have an opportunity to find out what is being said. 

Click here to view all "Word-of-Mouth Connections" in our system regarding this email address:

http://womc.net/pass.php?a=search&b=5&c=r-help at lists.r-project.org

How did we find you?

When your acquaintance searched for connections at our website he/she provided us with your email address.

What can you do?

First, we'd like to invite you to visit our website at www.WordofMouthConnection.com and learn about our service.  Registration is free. Not only can you find out if any WordofMouthConnection.com Connections have been submitted about you, but you can connect to others as well to research word-of-mouth information about your friends, co-workers, family, etc.

We are solely interested in fostering a community of willing people to promote information exchange. If you decide you're not interested, you can choose to block future emails from us. 

To add your email address to our Do Not Email List use the following link:

http://womc.net/pass.php?a=donotemail&b=r-help at lists.r-project.org&c=true

*Important - "Word-of-Mouth Connections" are only for the purpose of identifying the connection subject.  What you see is what you get, in terms of search results.  WordofMouthConnection.com is an online community that helps connect people so that they can talk at greater length and in greater detail. None of the information that is exchanged resides within the website itself.

If you have any questions or comments please email us at:

http://womc.net/pass.php?a=contact

Sincerely,
WordofMouthConnection.com



From search-awareness at word-of-mouth-connection.com  Fri Oct 17 21:52:20 2003
From: search-awareness at word-of-mouth-connection.com (search-awareness@word-of-mouth-connection.com)
Date: Fri, 17 Oct 2003 12:52:20 -0700
Subject: [R] Someone just searched for word-of-mouth information about:
	r-help@lists.r-project.org
Message-ID: <200310171952.h9HJqKV3002088@peter1.word-of-mouth-connection.com>


WordofMouthConnection.com Search Awareness System

This email is a website-generated message, but it is not spam.

An acquaintance of yours recently conducted a search on your email address in our online community, WordofMouthConnection.com. It could be a friend, a family member, co-worker, business associate, or someone else who's interested in learning more about you.

Why are we sending you this email?

When people find out others are talking about them -- whether it is good or bad -- they want to know. At WordofMouthConnection.com, we feel responsible to alert people so they have an opportunity to find out what is being said. 

Click here to view all "Word-of-Mouth Connections" in our system regarding this email address:

http://womc.net/pass.php?a=search&b=5&c=r-help at lists.r-project.org

How did we find you?

When your acquaintance searched for connections at our website he/she provided us with your email address.

What can you do?

First, we'd like to invite you to visit our website at www.WordofMouthConnection.com and learn about our service.  Registration is free. Not only can you find out if any WordofMouthConnection.com Connections have been submitted about you, but you can connect to others as well to research word-of-mouth information about your friends, co-workers, family, etc.

We are solely interested in fostering a community of willing people to promote information exchange. If you decide you're not interested, you can choose to block future emails from us. 

To add your email address to our Do Not Email List use the following link:

http://womc.net/pass.php?a=donotemail&b=r-help at lists.r-project.org&c=true

*Important - "Word-of-Mouth Connections" are only for the purpose of identifying the connection subject.  What you see is what you get, in terms of search results.  WordofMouthConnection.com is an online community that helps connect people so that they can talk at greater length and in greater detail. None of the information that is exchanged resides within the website itself.

If you have any questions or comments please email us at:

http://womc.net/pass.php?a=contact

Sincerely,
WordofMouthConnection.com



From JRynak at biopure.com  Fri Oct 17 18:59:37 2003
From: JRynak at biopure.com (Rynak, John)
Date: Fri, 17 Oct 2003 12:59:37 -0400
Subject: [R] Opening - Director Biostatistics - Cambridge MA
Message-ID: <7798F8AC473C6E498F8041A2F8409C9303516307@biopuremail.corp.biopure.com>

We are currently looking for a Director Biostatistics and Data Management in
our Cambridge MA facility. 

We are looking for 7+ years of Statistical Analysis in a Biotech /
Pharmaceutical environment with Phase 2 and Phase 3 clinical trial data.
Experience with clinical protocol design, coordination of clinical database
design requirements, statistical planning and preparation of regulatory
submissions. Experience in management of clinical statistical contractors
and CRO. Phd / MS in Bio-Statistics or Statistics, and demonstrated SAS
programming skills.

Biopure Corporation is a leading developer, manufacturer and supplier of a
new class of pharmaceuticals, called oxygen therapeutics, which are
intravenously administered to deliver oxygen to the body's tissues as a
sterile alternative to red blood cell transfusion. Biopure's oxygen
therapeutics possess unique attributes that address many of the medical and
logistical issues associated with red blood cell transfusions. 

If you or any of your associates are interested in our Cambridge, MA
openings please forward a resume to jrynak at biopure.com or fax to
617-234-6505.  

Thank you for your interest in Biopure.

John Rynak
Biopure Corporation
617-234-6835



From ggrothendieck at myway.com  Fri Oct 17 19:16:00 2003
From: ggrothendieck at myway.com (Gabor Grothendieck)
Date: Fri, 17 Oct 2003 13:16:00 -0400 (EDT)
Subject: [R] datetime data and plotting
Message-ID: <20031017171600.6C42D39B8@xmxpita.myway.com>



The problem is related to time zones.  The easiest way to
handle this is to avoid using POSIXt and use chron instead
so you don't have to worry about them.

require(chron)
day <- 6:16
dts <- dates(paste("10", day, "03", sep="/"))
plot(dts,day)
abline(v=dts)

 
---
From: Jacob Etches <jetches at iwh.on.ca>
 
If I take the following simple data:

YEAR MONTH DAY WEIGHT.KG
2003 10 6 1.2
2003 10 12 1.2
2003 10 16 1.3

and format the date data and plot it:

dates <- strptime(paste(DAY,MONTH,YEAR),"%d%m%Y")
plot(c(min(dates),max(dates)),c(0,max(WEIGHT.KG)),
xlab="Date",ylab="Weight (kg)",type="n")
lines(dates,WEIGHT.KG)
points(dates,WEIGHT.KG)

I find that the data points are all plotted at (x-1,y),
where x is in days. Have I requested this behaviour 
accidentally? I'm using R-1.8 on OS X.

Printing the dates object looks correct, and simple 
manipulations such as max(dates)-min(dates) behave 
normally.

Jacob Etches

Doctoral candidate
Dept of Public Health Sciences
University of Toronto



_______________________________________________
No banners. No pop-ups. No kidding.
Introducing My Way - http://www.myway.com



From jasont at indigoindustrial.co.nz  Fri Oct 17 19:32:00 2003
From: jasont at indigoindustrial.co.nz (Jason Turner)
Date: Sat, 18 Oct 2003 06:32:00 +1300
Subject: [R] don't display rulers in image() command and script file input
In-Reply-To: <200310180408.30755.adorio@math.upd.edu.ph>
References: <200310180408.30755.adorio@math.upd.edu.ph>
Message-ID: <3F902790.2020905@indigoindustrial.co.nz>

Ernie Adorio wrote:

> Dear R experts,
> 
> 1. How can I turn off the display of rulers in image() command?

Are rulers the same as axes?  If so, try
image(...,axes = FALSE)

See example(image) for detials.

> 
> 2. Rather than typing my commands at the command line, how can I  input a file 
> contents aside from doing a copy and paste operation?

?source

Also, see the FAQ, 7.18.

Cheers

Jason
-- 
Indigo Industrial Controls Ltd.
http://www.indigoindustrial.co.nz
64-21-343-545
jasont at indigoindustrial.co.nz



From edd at debian.org  Fri Oct 17 19:46:17 2003
From: edd at debian.org (Dirk Eddelbuettel)
Date: Fri, 17 Oct 2003 12:46:17 -0500
Subject: [R] datetime data and plotting
In-Reply-To: <20031017171600.6C42D39B8@xmxpita.myway.com>
References: <20031017171600.6C42D39B8@xmxpita.myway.com>
Message-ID: <20031017174617.GA9209@sonny.eddelbuettel.com>

On Fri, Oct 17, 2003 at 01:16:00PM -0400, Gabor Grothendieck wrote:
> 
> 
> The problem is related to time zones.  The easiest way to
> handle this is to avoid using POSIXt and use chron instead
> so you don't have to worry about them.
> 
> require(chron)
> day <- 6:16
> dts <- dates(paste("10", day, "03", sep="/"))
> plot(dts,day)
> abline(v=dts)

I don't think I'd call that easiest. Jacob simply did not specify hour,
minute and second for a display where it mattered (mostly because he only
plotted 3 points, with 300 it would have close to impossible to discern).  

One way to address this would be to give a hour and minute as in 

  dates <- strptime(paste(DAY,MONTH,YEAR,"23:59"),"%d %m %Y %H:%M")

(where I also adjust the format string for the space paste() adds).
The three plot commands can also be combined into

  plot(dates,WEIGHT.KG,
       ylim=c(0,max(WEIGHT.KG)),
       xlab="Date",ylab="Weight (kg)",type="o")
       
Hth, Dirk

 
> ---
> From: Jacob Etches <jetches at iwh.on.ca>
>  
> If I take the following simple data:
> 
> YEAR MONTH DAY WEIGHT.KG
> 2003 10 6 1.2
> 2003 10 12 1.2
> 2003 10 16 1.3
> 
> and format the date data and plot it:
> 
> dates <- strptime(paste(DAY,MONTH,YEAR),"%d%m%Y")
> plot(c(min(dates),max(dates)),c(0,max(WEIGHT.KG)),
> xlab="Date",ylab="Weight (kg)",type="n")
> lines(dates,WEIGHT.KG)
> points(dates,WEIGHT.KG)
> 
> I find that the data points are all plotted at (x-1,y),
> where x is in days. Have I requested this behaviour 
> accidentally? I'm using R-1.8 on OS X.
> 
> Printing the dates object looks correct, and simple 
> manipulations such as max(dates)-min(dates) behave 
> normally.
> 
> Jacob Etches
> 
> Doctoral candidate
> Dept of Public Health Sciences
> University of Toronto
> 
> 
> 
> _______________________________________________
> No banners. No pop-ups. No kidding.
> Introducing My Way - http://www.myway.com
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> 

-- 
Those are my principles, and if you don't like them... well, I have others.
                                                -- Groucho Marx



From pavlicov at stat.ohio-state.edu  Fri Oct 17 19:59:11 2003
From: pavlicov at stat.ohio-state.edu (Martina Pavlicova)
Date: Fri, 17 Oct 2003 13:59:11 -0400 (EDT)
Subject: [R] Lilliefors Test
Message-ID: <Pine.GSO.4.58.0310171341260.12534@spatial.stat.ohio-state.edu>


Hello everybody,

I would like to perform a test for normality (without specifying the
mean a variance) on the sample data (80 observations). I found that
Lilliefors test is appropriate. Does anybody have it programmed already,
or is there a function for this test in R?

Thank you very much,

Martina Pavlicova
--------------------------------------------------------------------------
Department of Statistics             Office Phone: (614) 292-1567
1958 Neil Avenue, 304E Cockins Hall  FAX: (614) 292-2096
The Ohio State University            E-mail: pavlicov at stat.ohio-state.edu
Columbus, OH 43210-1247              www.stat.ohio-state.edu/~pavlicov



From kjetil at entelnet.bo  Fri Oct 17 20:00:36 2003
From: kjetil at entelnet.bo (kjetil@entelnet.bo)
Date: Fri, 17 Oct 2003 14:00:36 -0400
Subject: [R] Rd problems
In-Reply-To: <16266.18942.112041.300104@gargle.gargle.HOWL>
References: <3F8924F4.3351.8D09F5@localhost>
Message-ID: <3F8FF604.3086.16050DF@localhost>

On 13 Oct 2003 at 8:45, Martin Maechler wrote:

> >>>>> "kjetil" == kjetil halvorsen <kjetil at entelnet.bo>
> >>>>>     on Sun, 12 Oct 2003 09:55:00 -0400 writes:
> 
>     kjetil> Hola!  I have the following in a .Rd file:
> 
>     kjetil> \eqn{\mbox{coef} = c(\mbox{coef}[1],\ldots, \mbox{coef}[n]) }
>     kjetil>       {coef = c(coef[1], coef[2], \dots, coef[n])}
> 
>     kjetil> However, both arguments come out in the latex file!
> 
>     kjetil> Whats happening?
> 
> \eqn comes in a 1-argument and 2-argument version.
> If you want the 2-argument version, you cannot put spaces
> between the ending "}" of the 1st arg and the starting "{" of
> the 2nd one.
> 
> Instead of the above, 
> use
> 	 \eqn{\mbox{coef} = c(\mbox{coef}[1],\ldots, \mbox{coef}[n]) }{%
>               coef = c(coef[1], coef[2], \dots, coef[n])}
> 
> (note the comment "%" after the opening "{" )

Thanks!, but I did'nt get this to work with the % trick, I had to put 
everything on one line as Brian Ripley said. By the way , 
\deqn{}
        {}
works fine.

Kjetil Halvorsen


> Martin



From ggrothendieck at myway.com  Fri Oct 17 20:20:50 2003
From: ggrothendieck at myway.com (Gabor Grothendieck)
Date: Fri, 17 Oct 2003 14:20:50 -0400 (EDT)
Subject: [R] datetime data and plotting
Message-ID: <20031017182050.43C253997@xmxpita.myway.com>


> From: Dirk Eddelbuettel <edd at debian.org>
>  
> On Fri, Oct 17, 2003 at 01:16:00PM -0400, Gabor Grothendieck wrote:
> > 
> > 
> > The problem is related to time zones. The easiest way to
> > handle this is to avoid using POSIXt and use chron instead
> > so you don't have to worry about them.
> > 
> > require(chron)
> > day <- 6:16
> > dts <- dates(paste("10", day, "03", sep="/"))
> > plot(dts,day)
> > abline(v=dts)
> 
> I don't think I'd call that easiest. Jacob simply did not specify hour,
> minute and second for a display where it mattered (mostly because he only
> plotted 3 points, with 300 it would have close to impossible to discern). 
> 
> One way to address this would be to give a hour and minute as in 
> 
> dates <- strptime(paste(DAY,MONTH,YEAR,"23:59"),"%d %m %Y %H:%M")
> 
> (where I also adjust the format string for the space paste() adds).
> The three plot commands can also be combined into
> 
> plot(dates,WEIGHT.KG,
> ylim=c(0,max(WEIGHT.KG)),
> xlab="Date",ylab="Weight (kg)",type="o")

Unfortunately, that solution will not work in all time zones.  
For example, to get dates to line up in my time zone 
(Eastern Daylight Time) I would have to do this:

  day <- 6:16
  dts <- strptime(paste(day,10,2003,"20:00"),"%d %m %Y %H:%M") 
  plot(dts,day)
  abline(v=as.POSIXct(dts))

Its currently daylight savings time where I am but we are soon 
going to change to standard time for the winter which will force 
this to change shortly so even the above is not sufficient.

Time zones are not part of the problem yet POSIXt forces this 
extraneous complication on you.  chron has no time zones in the 
first place and therefore allows you to work in the natural frame 
of the problem, avoiding subtle problems like this.

This sort of thing has been discussed a number of times and I
had previously suggested that chron be moved to the base or else that
a timezone-less version of POSIXt be added to the base.  See:
https://stat.ethz.ch/pipermail/r-devel/2003-August/027269.html

(I am using R 1.7.1 on Windows 2000.)



_______________________________________________
No banners. No pop-ups. No kidding.
Introducing My Way - http://www.myway.com



From macq at llnl.gov  Fri Oct 17 20:30:02 2003
From: macq at llnl.gov (Don MacQueen)
Date: Fri, 17 Oct 2003 11:30:02 -0700
Subject: [R] datetime data and plotting
In-Reply-To: <C91DF477A51618409DB4B2FC262215DE0854C8@iwhmail01.iwh.on.ca>
References: <C91DF477A51618409DB4B2FC262215DE0854C8@iwhmail01.iwh.on.ca>
Message-ID: <p05210608bbb5dd29509d@[128.115.153.6]>

I do see the described behavior, on three systems, linux R 1.8.0, Mac 
OS X R 1.8.0, and Solaris R 1.7.1.
Plot 1 is different than plot 2; in plot 1 the points are offset to 
the left of the axis tick marks.

datet <- as.POSIXct(dates)

## 1
plot(datet,WEIGHT.KG)

## 2
plot(datet,WEIGHT.KG,xaxt='n')
axis.POSIXct(1,at=datet)

To investigate a bit, I made a copy of axis.POSIXct and modified it 
slightly to return the value of "at" that it calculates. I get this:
"2003-10-06 17:00:00 PDT" "2003-10-08 17:00:00 PDT"
"2003-10-10 17:00:00 PDT" "2003-10-12 17:00:00 PDT"
"2003-10-14 17:00:00 PDT"

These are equal to midnight GMT, since my systems are currently in 
PDT, i.e. GMT-7.


>  version
          _                       
platform powerpc-apple-darwin6.7.5
arch     powerpc                 
os       darwin6.7.5             
system   powerpc, darwin6.7.5    
status   Patched                 
major    1                       
minor    8.0                     
year     2003                    
month    10                      
day      13                      
language R                       


>  version
          _               
platform i686-pc-linux-gnu
arch     i686            
os       linux-gnu       
system   i686, linux-gnu 
status   Patched         
major    1               
minor    8.0             
year     2003            
month    10              
day      16
language R

>  version
          _                  
platform sparc-sun-solaris2.7
arch     sparc              
os       solaris2.7         
system   sparc, solaris2.7  
status                      
major    1                  
minor    7.1                
year     2003               
month    06                 
day      16                 
language R                  


-Don

At 9:21 AM -0400 10/17/03, Jacob Etches wrote:
>If I take the following simple data:
>
>YEAR MONTH DAY WEIGHT.KG
>2003 10 6 1.2
>2003 10 12 1.2
>2003 10 16 1.3
>
>and format the date data and plot it:
>
>dates <- strptime(paste(DAY,MONTH,YEAR),"%d%m%Y")
>plot(c(min(dates),max(dates)),c(0,max(WEIGHT.KG)),
>  xlab="Date",ylab="Weight (kg)",type="n")
>  lines(dates,WEIGHT.KG)
>  points(dates,WEIGHT.KG)
>
>I find that the data points are all plotted at (x-1,y),
>where x is in days.  Have I requested this behaviour
>accidentally?  I'm using R-1.8 on OS X.
>
>Printing the dates object looks correct, and simple
>manipulations such as max(dates)-min(dates) behave
>normally.
>
>Jacob Etches
>
>Doctoral candidate
>Dept of Public Health Sciences
>University of Toronto
>
>
>______________________________________________
>R-help at stat.math.ethz.ch mailing list
>https://www.stat.math.ethz.ch/mailman/listinfo/r-help


-- 
--------------------------------------
Don MacQueen
Environmental Protection Department
Lawrence Livermore National Laboratory
Livermore, CA, USA



From GPetris at uark.edu  Fri Oct 17 20:39:28 2003
From: GPetris at uark.edu (Giovanni Petris)
Date: Fri, 17 Oct 2003 13:39:28 -0500 (CDT)
Subject: [R] Modifying dim attribute of elements of a list
In-Reply-To: <200310171650.h9HGoTR4011617@definetti.uark.edu> (message from
	Giovanni Petris on Fri, 17 Oct 2003 11:50:29 -0500 (CDT))
References: <200310171650.h9HGoTR4011617@definetti.uark.edu>
Message-ID: <200310171839.h9HIdS6Z011776@definetti.uark.edu>


I've found the solution to my problem (see below), although it looks
somewhat ugly: 

sim <- c('simMeans','simVars','simWeights')
indexTable <- table(modelIndex)
for (i in sim) {
    assign(tmp <- paste(i,'By',sep=''),split(get(i), modelIndex))
    lapply(seq(along=indexTable), function(j)
           eval(parse(text=paste('dim(',tmp,'[[',j,']])','<<-','c(indexTable[',j,'],K)'))))
}

> Date: Fri, 17 Oct 2003 11:50:29 -0500 (CDT)
> From: Giovanni Petris <GPetris at uark.edu>
> Sender: r-help-bounces at stat.math.ethz.ch
> Precedence: list
> 
> 
> I am creating lists of vectors withing a loop. I also would like to
> change the dim attribute to the vectors in order to make them
> matrices. 
> 
> I have tried the following, but it doesn't work...
> 
> > sim <- c('simMeans','simVars','simWeights')
> > indexTable <- table(modelIndex)
> > for (i in sim) {
> +     assign(tmp <- paste(i,'By',sep=''),split(get(i), modelIndex))
> +     lapply(seq(along=indexTable),function(j) attr(get(tmp)[[j]],'dim') <<- c(indexTable[j],K))
> + }
> Error in FUN(X[[1]], ...) : couldn't find function "get<-"
> In addition: Warning message: 
> argument lengths differ in: split(x, f) 
> 
> Any suggestions will be appreciated. 
> Thanks
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> 
> 
>



From Dipti.Kamdar at sun.com  Fri Oct 17 21:07:48 2003
From: Dipti.Kamdar at sun.com (Dipti Kamdar)
Date: Fri, 17 Oct 2003 12:07:48 -0700
Subject: [R] gcc for SuSE
Message-ID: <3F903E04.FE3DFE4B@Sun.COM>

Hi,
I wish to compile the R source on SuSE,
but am unable to find the gcc for it.
Can anyone send me a pointer of where
they got it from.
Thanks,
Dipti

-- 
--------------------------------------------------------------------
    /\       Dipti Kamdar                        
   \\ \      Solution Engineering     
  \ \\ /     Customer Advocacy & Solutions   
 / \/ / /           
/ /   \//\   Sun Microsystems, Inc.
\//\   / /   Phone:(650) 786-8907 / Internal: x88907
 / / /\ /    Email: dipti.kamdar at sun.com
  / \\ \     
   \ \\      
    \/



From kjetil at entelnet.bo  Fri Oct 17 21:28:40 2003
From: kjetil at entelnet.bo (kjetil@entelnet.bo)
Date: Fri, 17 Oct 2003 15:28:40 -0400
Subject: [R] Rd problems
In-Reply-To: <3F89AF4D.A5B78AB@statistik.uni-dortmund.de>
Message-ID: <3F900AA8.22989.1B0F056@localhost>

On 12 Oct 2003 at 21:45, Uwe Ligges wrote:

> kjetil at entelnet.bo wrote:
> > 
> > I am running
> > Rcmd check          (Windows XP, rw1080 from cran)
> > on a new package.
> > 
> > This reports "undocumented code objects" for 14 functions,
> > which all have their .Rd files!
> > 
> > What might be happening?
> 
> 1) You forgot to set an \alias{} (most probable)
> 2) There is another error in the Rd files 
> 3) There is a bug in R (less probable)
> 

It is 3). It was caused by one unmatched brace, but the Rcmd check 
did not comply about unmatched braces. The first few lines of the 
file had the structure
\name{aname}
\alias{anothername}
}   % this is the unmatced brace

.
.
.

Kjetil Halvorsen


> At first check points 1-2) from above, after that repeat the complete
> output of Rcmd check and provide a minimal version of one of your Rd
> files which does not work.
> 
> Uwe Ligges
> 
> 
> > Kjetil Halvorsen
> > 
> > ______________________________________________
> > R-help at stat.math.ethz.ch mailing list
> > https://www.stat.math.ethz.ch/mailman/listinfo/r-help



From kjetil at entelnet.bo  Fri Oct 17 21:32:55 2003
From: kjetil at entelnet.bo (kjetil@entelnet.bo)
Date: Fri, 17 Oct 2003 15:32:55 -0400
Subject: [R] Lilliefors Test
In-Reply-To: <Pine.GSO.4.58.0310171341260.12534@spatial.stat.ohio-state.edu>
Message-ID: <3F900BA7.19970.1B4D364@localhost>

On 17 Oct 2003 at 13:59, Martina Pavlicova wrote:

There is shapiro.test in package ctest, which have much better power 
properties than Lillefors test. So there is no need to have 
Lilliefors test in R, except for archeological interest. 

Kjetil Halvorsen

> 
> Hello everybody,
> 
> I would like to perform a test for normality (without specifying the
> mean a variance) on the sample data (80 observations). I found that
> Lilliefors test is appropriate. Does anybody have it programmed already,
> or is there a function for this test in R?
> 
> Thank you very much,
> 
> Martina Pavlicova
> --------------------------------------------------------------------------
> Department of Statistics             Office Phone: (614) 292-1567
> 1958 Neil Avenue, 304E Cockins Hall  FAX: (614) 292-2096
> The Ohio State University            E-mail: pavlicov at stat.ohio-state.edu
> Columbus, OH 43210-1247              www.stat.ohio-state.edu/~pavlicov
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help



From ripley at stats.ox.ac.uk  Fri Oct 17 21:47:17 2003
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Fri, 17 Oct 2003 20:47:17 +0100 (BST)
Subject: [R] datetime data and plotting
In-Reply-To: <p05210608bbb5dd29509d@[128.115.153.6]>
Message-ID: <Pine.LNX.4.44.0310172045200.28576-100000@gannet.stats>

So someone forgot to specify the timezone, if the current one was not 
wanted.

However, I don't see how timezones can account for a 24hour difference 
as originally reported.

On Fri, 17 Oct 2003, Don MacQueen wrote:

> I do see the described behavior, on three systems, linux R 1.8.0, Mac 
> OS X R 1.8.0, and Solaris R 1.7.1.
> Plot 1 is different than plot 2; in plot 1 the points are offset to 
> the left of the axis tick marks.
> 
> datet <- as.POSIXct(dates)
> 
> ## 1
> plot(datet,WEIGHT.KG)
> 
> ## 2
> plot(datet,WEIGHT.KG,xaxt='n')
> axis.POSIXct(1,at=datet)
> 
> To investigate a bit, I made a copy of axis.POSIXct and modified it 
> slightly to return the value of "at" that it calculates. I get this:
> "2003-10-06 17:00:00 PDT" "2003-10-08 17:00:00 PDT"
> "2003-10-10 17:00:00 PDT" "2003-10-12 17:00:00 PDT"
> "2003-10-14 17:00:00 PDT"

Have you heard of debug()?

> These are equal to midnight GMT, since my systems are currently in 
> PDT, i.e. GMT-7.

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From ripley at stats.ox.ac.uk  Fri Oct 17 21:49:42 2003
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Fri, 17 Oct 2003 20:49:42 +0100 (BST)
Subject: [R] Rd problems
In-Reply-To: <3F900AA8.22989.1B0F056@localhost>
Message-ID: <Pine.LNX.4.44.0310172047580.28576-100000@gannet.stats>

On Fri, 17 Oct 2003 kjetil at entelnet.bo wrote:

> On 12 Oct 2003 at 21:45, Uwe Ligges wrote:
> 
> > kjetil at entelnet.bo wrote:
> > > 
> > > I am running
> > > Rcmd check          (Windows XP, rw1080 from cran)
> > > on a new package.
> > > 
> > > This reports "undocumented code objects" for 14 functions,
> > > which all have their .Rd files!
> > > 
> > > What might be happening?
> > 
> > 1) You forgot to set an \alias{} (most probable)
> > 2) There is another error in the Rd files 
> > 3) There is a bug in R (less probable)
> > 
> 
> It is 3). It was caused by one unmatched brace, but the Rcmd check 
> did not comply about unmatched braces. The first few lines of the 
> file had the structure
> \name{aname}
> \alias{anothername}
> }   % this is the unmatced brace

That's 2) not 3).

Not catching _your_ errors is not a bug (and given that .Rd does not have
a parser, is inevitable).

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From tplate at blackmesacapital.com  Fri Oct 17 21:52:29 2003
From: tplate at blackmesacapital.com (Tony Plate)
Date: Fri, 17 Oct 2003 13:52:29 -0600
Subject: [R] datetime data and plotting
In-Reply-To: <20031017182050.43C253997@xmxpita.myway.com>
Message-ID: <5.2.1.1.2.20031017135009.040745c8@mailhost.blackmesacapital.com>

At Friday 02:20 PM 10/17/2003 -0400, Gabor Grothendieck wrote:
>[material deleted]
>Time zones are not part of the problem yet POSIXt forces this
>extraneous complication on you.  chron has no time zones in the
>first place and therefore allows you to work in the natural frame
>of the problem, avoiding subtle problems like this.
>
>This sort of thing has been discussed a number of times and I
>had previously suggested that chron be moved to the base or else that
>a timezone-less version of POSIXt be added to the base.  See:
>https://stat.ethz.ch/pipermail/r-devel/2003-August/027269.html

I also see the usefulness of a time-zone-free time/date class, but why
does "chron" need to be moved to the base to be useful here?

-- Tony Plate



From ggrothendieck at myway.com  Fri Oct 17 22:10:35 2003
From: ggrothendieck at myway.com (Gabor Grothendieck)
Date: Fri, 17 Oct 2003 16:10:35 -0400 (EDT)
Subject: [R] datetime data and plotting
Message-ID: <20031017201035.7BF9E39B5@xmxpita.myway.com>


From: Tony Plate <tplate at blackmesacapital.com>
> I also see the usefulness of a time-zone-free time/date class, 
> but why does "chron" need to be moved to the base to be useful here?

Because other software makes use of times in the base.  Package
writers figure that what is in the base is the most available 
and used so that is what they use.  Thus classes in the base 
get propagated throughout the libraries too.


_______________________________________________
No banners. No pop-ups. No kidding.
Introducing My Way - http://www.myway.com



From ligges at statistik.uni-dortmund.de  Fri Oct 17 22:36:16 2003
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Fri, 17 Oct 2003 22:36:16 +0200
Subject: [R] gcc for SuSE
References: <3F903E04.FE3DFE4B@Sun.COM>
Message-ID: <3F9052C0.92BDFCD@statistik.uni-dortmund.de>

Dipti Kamdar wrote:
> 
> Hi,
> I wish to compile the R source on SuSE,
> but am unable to find the gcc for it.
> Can anyone send me a pointer of where
> they got it from.
> Thanks,
> Dipti

Why do you think installing gcc is a topic related to R-help?

You can install gcc from rpms that are on the SuSE CD/DVD. You can use
YAST to look for rpms.

Uwe Ligges



From alex_s_42 at yahoo.com  Fri Oct 17 22:37:31 2003
From: alex_s_42 at yahoo.com (Alexander Sirotkin [at Yahoo])
Date: Fri, 17 Oct 2003 13:37:31 -0700 (PDT)
Subject: [R] R memory and CPU requirements
In-Reply-To: <5.1.0.14.2.20031017083516.01fa5bc0@127.0.0.1>
Message-ID: <20031017203731.53368.qmail@web14911.mail.yahoo.com>

Thanks for all the responses. 

After re-examining my data I came to realize that
second order interactions would be enough in my
particular case. With second order instructions I
managed to fit a model with less then 512MB RAM.

Thanks to everybody.


--- John Fox <jfox at mcmaster.ca> wrote:
> Dear Alexander,
> 
> 
> At 01:29 AM 10/17/2003 -0700, Alexander Sirotkin
> \[at Yahoo\] wrote:
> >I agree completely.
> >
> >In fact, I have about 5000 observations, which
> should
> >be enough.
> >I was using 200 samples because of RAM limitations
> and
> >  I'm afraid to think about what amount of RAM I'll
> >need to fit an aov() for such data.
> >
> 
> 
> OK -- I didn't realize that you have 5000
> observations. Perhaps I didn't 
> read some of the earlier messages carefully enough.
> 
> At the risk of getting you to repeat information
> that you've already 
> provided, how many degrees of freedom are there in
> the model that you're 
> trying to fit? I can create a 5000 by 5000 model
> matrix on my relatively 
> anemic Windows machine, and surely (unless there's
> some specification 
> error) your model should have many fewer df than
> that if it includes just 
> the main effects and two-way interactions (or by all
> interactions, do you 
> mean higher-order interactions as well?).
> 
> Perhaps providing the following information would
> help: What is the model 
> formula? Which variables are factors? How many
> levels does each factor have?
> 
> Regards,
>   John
> 
> >--- John Fox <jfox at mcmaster.ca> wrote:
> > > Dear Alexander,
> > >
> > > If I understand you correctly, you have a sample
> of
> > > 200 observations. Even
> > > if you had only two factors with 40 levels each,
> the
> > > main effects and
> > > interactions of these factors would require
> about
> > > 1600 degrees of freedom
> > > -- that is, more than the number of
> observations.
> > > This doesn't make a whole
> > > lot of sense.
> > >
> > > I hope that this helps,
> > >   John
> > >
> > > At 05:03 PM 10/16/2003 -0700, Alexander Sirotkin
> > > \[at Yahoo\] wrote:
> > >
> > > >--- Deepayan Sarkar <deepayan at stat.wisc.edu>
> wrote:
> > > > > On Thursday 16 October 2003 17:59, Alexander
> > > > > Sirotkin \[at Yahoo\] wrote:
> > > > > > Thanks for all the help on my previous
> > > questions.
> > > > > >
> > > > > > One more (hopefully last one) : I've been
> very
> > > > > > surprised when I tried to fit a model
> (using
> > > > > aov())
> > > > > > for a sample of size 200 and 10 variables
> and
> > > > > their
> > > > > > interactions.
> > > > >
> > > > > That doesn't really say much. How many of
> these
> > > > > variables are factors ? How
> > > > > many levels do they have ? And what is the
> order
> > > of
> > > > > the interaction ? (Note
> > > > > that for 10 numeric variables, if you allow
> all
> > > > > interactions, then there will
> > > > > be a 100 terms in your model. This increases
> for
> > > > > factors.)
> > > > >
> > > > > In other words, how big is your model matrix
> ?
> > > (See
> > > > > ?model.matrix)
> > > > >
> > > > > Deepayan
> > > > >
> > > >
> > > >
> > > >I see...
> > > >
> > > >Unfortunately, model.matrix() ran out of memory
> :)
> > > >I have 10 variables, 6 of which are factor, 2
> of
> > > which
> > > >
> > > >have quite a lot of levels (about 40). And I
> would
> > > >like
> > > >to allow all interactions.
> > > >
> > > >I understand your point about categorical
> > > variables,
> > > >but
> > > >still - this does not seem like too much data
> to
> > > me.
> > > >
> > > >
> > > >I remmeber fitting all kinds of models (mostly
> > > >decision
> > > >trees) for much, much larger data sets.
> > > >
> > > >______________________________________________
> > > >R-help at stat.math.ethz.ch mailing list
> > >
> >
>
>https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> > >
> > >
>
>-----------------------------------------------------
> > > John Fox
> > > Department of Sociology
> > > McMaster University
> > > Hamilton, Ontario, Canada L8S 4M4
> > > email: jfox at mcmaster.ca
> > > phone: 905-525-9140x23604
> > > web: www.socsci.mcmaster.ca/jfox
> > >
>
>-----------------------------------------------------
> > >
> >
> >
> >__________________________________
> >Do you Yahoo!?

> search
> >http://shopping.yahoo.com
> 
>
-----------------------------------------------------
> John Fox
> Department of Sociology
> McMaster University
> Hamilton, Ontario, Canada L8S 4M4
> email: jfox at mcmaster.ca
> phone: 905-525-9140x23604
> web: www.socsci.mcmaster.ca/jfox
>
-----------------------------------------------------
>



From jebacklund at dow.com  Fri Oct 17 18:30:07 2003
From: jebacklund at dow.com (Backlund, Jan Erik (JE))
Date: Fri, 17 Oct 2003 12:30:07 -0400
Subject: [R] Behavior of [[ in S vs. R
Message-ID: <822EC14BF42CC742AE6B6E2931C941523A4C5F@USINDMDOWM001.dow.com>

I am confused by the following difference in the behavior of R and S. Any
clarification would be greatly appreciated.
Jan Erik Backlund
Dow AgroSciences, LLC.
jebacklund at dow.com


R : Copyright 2003, The R Development Core Team
Version 1.8.0  (2003-10-08)
> sw
             Fertility Agriculture Examination Education Catholic
Courtelary        80.2        17.0          15        12     9.96
Delemont          83.1        45.1           6         9    84.84
Franches-Mnt      92.5        39.7           5         5    93.40
Moutier           85.8        36.5          12         7    33.77
Neuveville        76.9        43.5          17        15     5.16
> sw[2,1]
[1] 83.1
> sw[[2,1]]
[1] 17
> 

and the corresponding behaviour in S
S-PLUS : Copyright (c) 1988, 2001 Insightful Corp.
S : Copyright Lucent Technologies, Inc.
Professional Edition Version 6.0.3 Release 2 for Microsoft Windows : 2001 
> sw
             Fertility Agriculture Examination Education Catholic 
  Courtelary      80.2        17.0          15        12     9.96
    Delemont      83.1        45.1           6         9    84.84
Franches-Mnt      92.5        39.7           5         5    93.40
     Moutier      85.8        36.5          12         7    33.77
  Neuveville      76.9        43.5          17        15     5.16
> sw[2,1]
[1] 83.1
> sw[[2,1]]
[1] 83.1



From tlumley at u.washington.edu  Fri Oct 17 23:03:03 2003
From: tlumley at u.washington.edu (Thomas Lumley)
Date: Fri, 17 Oct 2003 14:03:03 -0700 (PDT)
Subject: [R] Behavior of [[ in S vs. R
In-Reply-To: <822EC14BF42CC742AE6B6E2931C941523A4C5F@USINDMDOWM001.dow.com>
References: <822EC14BF42CC742AE6B6E2931C941523A4C5F@USINDMDOWM001.dow.com>
Message-ID: <Pine.A41.4.58.0310171400290.53714@homer27.u.washington.edu>

On Fri, 17 Oct 2003, Backlund, Jan Erik (JE) wrote:

> I am confused by the following difference in the behavior of R and S. Any
> clarification would be greatly appreciated.

sw[[2,1]] in R is short for sw[[2]][[1]], which in the case of a data
frame is sw[1,2], as your example shows.

	-thomas


> Jan Erik Backlund
> Dow AgroSciences, LLC.
> jebacklund at dow.com
>
>
> R : Copyright 2003, The R Development Core Team
> Version 1.8.0  (2003-10-08)
> > sw
>              Fertility Agriculture Examination Education Catholic
> Courtelary        80.2        17.0          15        12     9.96
> Delemont          83.1        45.1           6         9    84.84
> Franches-Mnt      92.5        39.7           5         5    93.40
> Moutier           85.8        36.5          12         7    33.77
> Neuveville        76.9        43.5          17        15     5.16
> > sw[2,1]
> [1] 83.1
> > sw[[2,1]]
> [1] 17
> >
>
> and the corresponding behaviour in S
> S-PLUS : Copyright (c) 1988, 2001 Insightful Corp.
> S : Copyright Lucent Technologies, Inc.
> Professional Edition Version 6.0.3 Release 2 for Microsoft Windows : 2001
> > sw
>              Fertility Agriculture Examination Education Catholic
>   Courtelary      80.2        17.0          15        12     9.96
>     Delemont      83.1        45.1           6         9    84.84
> Franches-Mnt      92.5        39.7           5         5    93.40
>      Moutier      85.8        36.5          12         7    33.77
>   Neuveville      76.9        43.5          17        15     5.16
> > sw[2,1]
> [1] 83.1
> > sw[[2,1]]
> [1] 83.1
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
>

Thomas Lumley			Assoc. Professor, Biostatistics
tlumley at u.washington.edu	University of Washington, Seattle



From GPetris at uark.edu  Fri Oct 17 23:03:35 2003
From: GPetris at uark.edu (Giovanni Petris)
Date: Fri, 17 Oct 2003 16:03:35 -0500 (CDT)
Subject: [R] Behavior of [[ in S vs. R
In-Reply-To: <822EC14BF42CC742AE6B6E2931C941523A4C5F@USINDMDOWM001.dow.com>
	(jebacklund@dow.com)
References: <822EC14BF42CC742AE6B6E2931C941523A4C5F@USINDMDOWM001.dow.com>
Message-ID: <200310172103.h9HL3ZYg011930@definetti.uark.edu>


See ?[[
"[[" operates recursively, so 
sw[[2,1]] is the same as sw[[2]][[1]] 

Giovanni

> Date: Fri, 17 Oct 2003 12:30:07 -0400
> From: "Backlund, Jan Erik (JE)" <jebacklund at dow.com>
> Sender: r-help-bounces at stat.math.ethz.ch
> Cc: 
> Precedence: list
> 
> I am confused by the following difference in the behavior of R and S. Any
> clarification would be greatly appreciated.
> Jan Erik Backlund
> Dow AgroSciences, LLC.
> jebacklund at dow.com
> 
> 
> R : Copyright 2003, The R Development Core Team
> Version 1.8.0  (2003-10-08)
> > sw
>              Fertility Agriculture Examination Education Catholic
> Courtelary        80.2        17.0          15        12     9.96
> Delemont          83.1        45.1           6         9    84.84
> Franches-Mnt      92.5        39.7           5         5    93.40
> Moutier           85.8        36.5          12         7    33.77
> Neuveville        76.9        43.5          17        15     5.16
> > sw[2,1]
> [1] 83.1
> > sw[[2,1]]
> [1] 17
> > 
> 
> and the corresponding behaviour in S
> S-PLUS : Copyright (c) 1988, 2001 Insightful Corp.
> S : Copyright Lucent Technologies, Inc.
> Professional Edition Version 6.0.3 Release 2 for Microsoft Windows : 2001 
> > sw
>              Fertility Agriculture Examination Education Catholic 
>   Courtelary      80.2        17.0          15        12     9.96
>     Delemont      83.1        45.1           6         9    84.84
> Franches-Mnt      92.5        39.7           5         5    93.40
>      Moutier      85.8        36.5          12         7    33.77
>   Neuveville      76.9        43.5          17        15     5.16
> > sw[2,1]
> [1] 83.1
> > sw[[2,1]]
> [1] 83.1
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> 
> 
> 

-- 

 __________________________________________________
[                                                  ]
[ Giovanni Petris                 GPetris at uark.edu ]
[ Department of Mathematical Sciences              ]
[ University of Arkansas - Fayetteville, AR 72701  ]
[ Ph: (479) 575-6324, 575-8630 (fax)               ]
[ http://definetti.uark.edu/~gpetris/              ]
[__________________________________________________]



From ggrothendieck at myway.com  Sat Oct 18 01:46:06 2003
From: ggrothendieck at myway.com (Gabor Grothendieck)
Date: Fri, 17 Oct 2003 19:46:06 -0400 (EDT)
Subject: [R] datetime data and plotting
Message-ID: <20031017234606.DDD323987@xmxpita.myway.com>



Yes.  The timezone is not the whole problem.

What one would really like is that plot understands that it is
being given daily data and acts accordingly, in the same
way that plot already understands that its being given a factor
object or a dendogram, etc. and produces the right plot.

The OO way would be that the data describes itself, "telling" 
plot what it is being given so that plot can make the right choice.

---

Date: Fri, 17 Oct 2003 20:47:17 +0100 (BST) 
From: Prof Brian Ripley <ripley at stats.ox.ac.uk>
[ Add to Address Book | Block Address | Report as Spam ] 
To: Don MacQueen <macq at llnl.gov> 
Cc: Jacob Etches <jetches at iwh.on.ca>, <r-help at stat.math.ethz.ch> 
Subject: Re: [R] datetime data and plotting 

 
 
So someone forgot to specify the timezone, if the current one was not 
wanted.

However, I don't see how timezones can account for a 24hour difference 
as originally reported.

On Fri, 17 Oct 2003, Don MacQueen wrote:

> I do see the described behavior, on three systems, linux R 1.8.0, Mac 
> OS X R 1.8.0, and Solaris R 1.7.1.
> Plot 1 is different than plot 2; in plot 1 the points are offset to 
> the left of the axis tick marks.
> 
> datet <- as.POSIXct(dates)
> 
> ## 1
> plot(datet,WEIGHT.KG)
> 
> ## 2
> plot(datet,WEIGHT.KG,xaxt='n')
> axis.POSIXct(1,at=datet)
> 
> To investigate a bit, I made a copy of axis.POSIXct and modified it 
> slightly to return the value of "at" that it calculates. I get this:
> "2003-10-06 17:00:00 PDT" "2003-10-08 17:00:00 PDT"
> "2003-10-10 17:00:00 PDT" "2003-10-12 17:00:00 PDT"
> "2003-10-14 17:00:00 PDT"

Have you heard of debug()?

> These are equal to midnight GMT, since my systems are currently in 
> PDT, i.e. GMT-7.

-- 
Brian D. Ripley, ripley at stats.ox.ac.uk
Professor of Applied Statistics, http://www.stats.ox.ac.uk/~ripley/
University of Oxford, Tel: +44 1865 272861 (self)
1 South Parks Road, +44 1865 272866 (PA)
Oxford OX1 3TG, UK Fax: +44 1865 272595



_______________________________________________
No banners. No pop-ups. No kidding.
Introducing My Way - http://www.myway.com



From jeff_hamann at hamanndonald.com  Sat Oct 18 01:54:04 2003
From: jeff_hamann at hamanndonald.com (Jeff D. Hamann)
Date: Fri, 17 Oct 2003 16:54:04 -0700
Subject: [R] nlm, hessian, and derivatives in obj function?
Message-ID: <001701c39509$f58eda10$0a00a8c0@rodan>

I've been working on a new package and I have a few questions regarding the
behaviour of the nlm function. I've been (for better or worse) using the nlm
function to fit a linear model without suppling the hessian or gradient
attributes in the objective function. I'm curious as to why the nlm requires
31 iterations (for the linear model), and then it doesn't work when I try to
add the derivative information. I know using nlm for a linear model isn't
the "optimal" method, but I would like to make sure the parameter estimates
and the se's are matching before I attempt more difficult problems.

rm(list=ls(all=TRUE))
print( "running nlsystemfit models test at end...")
data( kmenta )
attach( kmenta )
##demand2 <- q ~ d0 + d1 * p + d2 * d
supply2 <- q ~ s0 + s1 * p + s2 * f + s3 * a
##system2 <- list( demand2, supply2 )
##labels <- list( "Demand", "Supply" )
##inst <- ~ d + f + a
##sv2 <- c(d0=3,s2=2.123,d2=4,s0=-2.123,s3=4.234,d1=4.234,s1=0.234)
sv2 <- c(s0=-2.123,s1=0.234,s2=2.123,s3=4.234)

obj <- function( s, eqn, data, parmnames )
{
  ## get the values of the parameters
  for( i in 1:length( parmnames ) )
    {
      name <- names( parmnames )[i]
      val <- s[i]
      storage.mode( val ) <-  "double"
      assign( name, val )
    }

  lhs <- as.matrix( eval( as.formula( eqn )[[2]] ) )
  rhs <- as.matrix( eval( as.formula( eqn )[[3]] ) )
  resid <- crossprod( lhs - rhs )

  ## just how does this work...
  attr( obj, "value" ) <- resid
  attr( obj, "gradient" ) <- attr( eval( deriv3( eqn, names(
parmnames ) ) ), "gradient" )

}

res <- nlm( obj, sv2, hessian=T, eqn=supply2, data=kmenta, parmnames=sv2,
check.analyticals=T)

I haven't been able to get nlm to function as I keep getting the following
error message:

Error in nlm(obj, sv2, hessian = T, eqn = supply2, data = kmenta, parmnames
= sv2,  :
 invalid function value in 'nlm' optimizer


If I perform the fit without the derivative information, I get the correct
estimates,

$minimum
[1] 92.55106

$estimate
[1] 58.2754312  0.1603666  0.2481333  0.2483023

$gradient
[1] 8.552542e-08 9.087699e-06 5.716032e-06 2.163105e-06

$hessian
         [,1]       [,2]     [,3]     [,4]
[1,]   40.000   4000.762   3865.0   420.00
[2,] 4000.762 401486.918 386045.8 42007.76
[3,] 3865.000 386045.812 379593.1 39762.40
[4,]  420.000  42007.764  39762.4  5740.00

$code
[1] 1

$iterations
[1] 31

I was under the impression that you could also obtain the se of the
parameter estimates using the sqrt( diag( res$hessian ) ), but I haven't
been able to reproduce the se computed by the Jacobian

se <- sqrt( mse * diag( solve( crossprod( J ) ) ) )    # gives the correct
results...
hse <- sqrt( ( res$minimum / 8 ) * diag( solve( res$hessian ) ) )   # gives
similar results, but why 8?

I've tried to put the functionality to include the jacobian and hessian in
the objective function for nlm without success as I don't know what the form
of the functions will be ahead of time.

and get the se from the sqrt( diag( hessian ) ), but it's nowhere close?

Jeff.

---
Jeff D. Hamann
Hamann, Donald and Associates, Inc.
PO Box 1421
Corvallis, Oregon USA 97339-1421
(office) 541-754-1428
(cell) 541-740-5988
jeff_hamann at hamanndonald.com
www.hamanndonald.com



From mli at access4less.net  Sat Oct 18 02:53:40 2003
From: mli at access4less.net (mli@access4less.net)
Date: Fri, 17 Oct 2003 20:53:40 -0400
Subject: [R] cor function in R 1.8.0
Message-ID: <3f908f14.2ca.7ee8.721958211@access4less.net>

Dear R users,

Does anyone know why the following two ways to calculate
correlation variance give different answers? I also obtain
different answers when I use, say, "spearman" method in
cor(). The problem does not happen in R 1.7.1 ("pearson"
correlation only, of course in R 1.7.1).

> set.seed(1234)
> x <- matrix(rnorm(10*5),10,5)
> y1 <- cor(x)
> y2 <- cor(x, use="pair")
> y1;y2
           [,1]        [,2]       [,3]        [,4]       
[,5]
[1,]  1.0000000 -0.17528322 -0.5528785 -0.33876389
-0.49755947
[2,] -0.1752832  1.00000000 -0.2776360 -0.04840035 
0.05265522
[3,] -0.5528785 -0.27763602  1.0000000  0.16272829 
0.38392034
[4,] -0.3387639 -0.04840035  0.1627283  1.00000000 
0.85404798
[5,] -0.4975595  0.05265522  0.3839203  0.85404798 
1.00000000
           [,1]        [,2]       [,3]        [,4]       
[,5]
[1,]  1.0000000 -0.17348156 -0.5523156 -0.33585411
-0.48292994
[2,] -0.1734816  0.99965819 -0.2743654 -0.04417098 
0.05661364
[3,] -0.5523156 -0.27436539  0.9990913  0.16439438 
0.38457068
[4,] -0.3358541 -0.04417098  0.1643944  0.99862845 
0.85389126
[5,] -0.4829299  0.05661364  0.3845707  0.85389126 
0.99985356

Thanks,

Ming-Chung Li



From ripley at stats.ox.ac.uk  Sat Oct 18 11:08:37 2003
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Sat, 18 Oct 2003 10:08:37 +0100 (BST)
Subject: [R] Behavior of [[ in S vs. R
In-Reply-To: <Pine.A41.4.58.0310171400290.53714@homer27.u.washington.edu>
Message-ID: <Pine.LNX.4.44.0310181004510.29617-100000@gannet.stats>

On Fri, 17 Oct 2003, Thomas Lumley wrote:

> On Fri, 17 Oct 2003, Backlund, Jan Erik (JE) wrote:
>
> > I am confused by the following difference in the behavior of R and S. Any
> > clarification would be greatly appreciated.
>
> sw[[2,1]] in R is short for sw[[2]][[1]], which in the case of a data
> frame is sw[1,2], as your example shows.

For a data frame only, that is.  In S-PLUS it is equivalent to
sw[[1]][[2]]. Neither system documents exactly what they do, AFAICS.
S-PLUS 6.1 says `[[ is designed to subscript lists'.  R does say

     When '[' and '[[' are used with two indices they act like indexing
     a matrix:  '[[' can only be used to select one element.

which suggests that the R difference is unintentional.

You will find that sw[[c(2,1)]] works the same in both systems.

R needs to either document more clearly what it does or change its
behaviour to conform to the (naive reading of the) documentation (and I
would prefer the second).

[...]

Example of `for data frame only'

> foo <- list(a=list(A=1, B=2), b=list(C=3, D=4))
> foo[[2]][[1]]
[1] 3
> foo[[2,1]]
Error in foo[[2, 1]] : incorrect number of subscripts
> foo[[c(2,1)]]
[1] 3

although ?"[[" does show x[[i, j, ...]] as a usage.


-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From Arne.Muller at aventis.com  Sat Oct 18 12:33:45 2003
From: Arne.Muller at aventis.com (Arne.Muller@aventis.com)
Date: Sat, 18 Oct 2003 12:33:45 +0200
Subject: [R] why does data frame subset return vector
Message-ID: <C80ECAFA2ACC1B45BE45D133ED660ADECDCC5B@crbsmxsusr04.pharma.aventis.com>

Hello,

I've a weired problem with a data frame. Basically it should be just one
column with
specific names coming from a data file (the file contains 2 rows, one should
be
the for the rownames of the data frame the other contains numeric values).

> df.rr <- read.table("RR_anova.txt", header=T, comment.char="", row.names=1)
> df.rr[c(1,2,3),]
[1] 1.11e-16 1.11e-16 1.11e-16

Why are the rownames not displayed?

The data file itself look slike this:
> df.rr <- read.table("RR_anova.txt", header=T, comment.char="")
> df.rr[c(1,2,3),]
            QUAL   PVALUE
1    AJ224120_at 1.11e-16
2 rc_AA893000_at 1.11e-16
3 rc_AA946368_at 1.11e-16

and assigning the rownames explicitely works as I'd expect:   
> rownames(df.rr) <- df.rr$'QUAL'
> df.rr[c(1,2,3),]
                         QUAL   PVALUE
AJ224120_at       AJ224120_at 1.11e-16
rc_AA893000_at rc_AA893000_at 1.11e-16
rc_AA946368_at rc_AA946368_at 1.11e-16

Ok, now they are displayed, but it's a duplication to keep the "QUAL" colum.

below I create the a new data frame to skip the "QUAL" column, since it is
already
a rowname.
> df.rr2 <- data.frame(PVALUE=df.rr, row.names=1)
> df.rr2[1:4,]
[1] 1.11e-16 1.11e-16 1.11e-16 1.11e-16

However, the rowname is still there ..., you just cannot see it:
> df.rr2["AJ224120_at",]
[1] 1.11e-16

The code below shows that "sub-setting" the df.rr data frame in deed creates
a
vector rather than a data frame whereas sub-setting the 2 column data frame
returns
a new data frame (as I'd expect).
 
> df.rr[1:4,]
[1] 1.11e-16 1.11e-16 1.11e-16 1.11e-16
> is.vector(df.rr[1:4,])
[1] TRUE
> is.data.frame(df.rr[1:4,])
[1] FALSE
> df.rr <- read.table("CLO_RR_anova.txt", header=T, comment.char="")
> is.data.frame(df.rr[1:4,])
[1] TRUE

Any explanation is appreciated. There must be a good reason for this I guess
... . On
the other hand is there a way to fore the subset of the 1 colum data frame to
be
dataframe itself? I'd just like to see the rownames displayed, that's it ...

	thanks alot for your help,

	Arne



From p.dalgaard at biostat.ku.dk  Sat Oct 18 13:53:53 2003
From: p.dalgaard at biostat.ku.dk (Peter Dalgaard)
Date: 18 Oct 2003 13:53:53 +0200
Subject: [R] Behavior of [[ in S vs. R
In-Reply-To: <Pine.LNX.4.44.0310181004510.29617-100000@gannet.stats>
References: <Pine.LNX.4.44.0310181004510.29617-100000@gannet.stats>
Message-ID: <x265imiwxa.fsf@biostat.ku.dk>

Prof Brian Ripley <ripley at stats.ox.ac.uk> writes:

> which suggests that the R difference is unintentional.

It was. While inserting the unclass-eliminating code, I seem to have
switched ..1 and ..2 around:

-- src/library/base/R/dataframe.R      25 Mar 2002 17:48:30 -0000   1.51
+++ src/library/base/R/dataframe.R      5 May 2002 22:34:53 -0000   1.52
@@ -404,11 +404,11 @@
     ## explicitly. Also will check for wrong number or empty args
     if(nargs() < 3)
        (function(x, i)
-        if(is.matrix(i))
-        as.matrix(x)[[i]]
-        else unclass(x)[[i]])(x, ...)
-    else (function(x, i, j)
-         x[[j]][[i]])(unclass(x), ...)
+         if(is.matrix(i))
+         as.matrix(x)[[i]]
+         else .subset2(x,i))(x, ...)
+    else
+        .subset2(.subset2(x,..1),..2)
 }


-- 
   O__  ---- Peter Dalgaard             Blegdamsvej 3  
  c/ /'_ --- Dept. of Biostatistics     2200 Cph. N   
 (*) \(*) -- University of Copenhagen   Denmark      Ph: (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)             FAX: (+45) 35327907



From ripley at stats.ox.ac.uk  Sat Oct 18 14:47:01 2003
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Sat, 18 Oct 2003 13:47:01 +0100 (BST)
Subject: [R] why does data frame subset return vector
In-Reply-To: <C80ECAFA2ACC1B45BE45D133ED660ADECDCC5B@crbsmxsusr04.pharma.aventis.com>
Message-ID: <Pine.LNX.4.44.0310181339500.30067-100000@gannet.stats>

Do look at the discussion of the drop argument in ?"[.data.frame".
It is all in TFM, and there is nothing `wiered' about it.

On Sat, 18 Oct 2003 Arne.Muller at aventis.com wrote:

> I've a weired problem with a data frame. Basically it should be just one
> column with
> specific names coming from a data file (the file contains 2 rows, one should
> be
> the for the rownames of the data frame the other contains numeric values).
> 
> > df.rr <- read.table("RR_anova.txt", header=T, comment.char="", row.names=1)
> > df.rr[c(1,2,3),]
> [1] 1.11e-16 1.11e-16 1.11e-16
> 
> Why are the rownames not displayed?

Because data frames have row names (with a space) and vectors do not.

[...]

> Any explanation is appreciated. There must be a good reason for this I guess
> ... . On
> the other hand is there a way to fore the subset of the 1 colum data frame to
> be
> dataframe itself? I'd just like to see the rownames displayed, that's it ...

The help page will tell you how.

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From maechler at stat.math.ethz.ch  Sat Oct 18 15:54:30 2003
From: maechler at stat.math.ethz.ch (Martin Maechler)
Date: Sat, 18 Oct 2003 15:54:30 +0200
Subject: [R] cor function in R 1.8.0
In-Reply-To: <3f908f14.2ca.7ee8.721958211@access4less.net>
References: <3f908f14.2ca.7ee8.721958211@access4less.net>
Message-ID: <16273.17942.257863.24299@gargle.gargle.HOWL>

>>>>> "mli" == mli  <mli at access4less.net>
>>>>>     on Fri, 17 Oct 2003 20:53:40 -0400 writes:

    mli> Dear R users, Does anyone know why the following two
    mli> ways to calculate correlation variance give different
    mli> answers? I also obtain different answers when I use,
    mli> say, "spearman" method in cor(). The problem does not
    mli> happen in R 1.7.1 ("pearson" correlation only, of
    mli> course in R 1.7.1).


Problem confirmed --- a bug, clearly.
Thank you for your feedback!



From alberts at iwi201.iwinet.rug.nl  Sat Oct 18 17:57:46 2003
From: alberts at iwi201.iwinet.rug.nl (alberts@iwi201.iwinet.rug.nl)
Date: Sat, 18 Oct 2003 17:57:46 +0200
Subject: [R] integer notation 1,000,000 instead of 1000000
Message-ID: <1066492666.3f9162fa48fa1@webmail.iwinet.rug.nl>

Hello,

How do I write one million as 1,000,000 instead of 1000000?
(cannot find it)

regards, R. Alberts

----------------------------------------------------------------
This message was sent using IMP, the Internet Messaging Program.



From ligges at statistik.uni-dortmund.de  Sat Oct 18 18:15:41 2003
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Sat, 18 Oct 2003 18:15:41 +0200
Subject: [R] integer notation 1,000,000 instead of 1000000
In-Reply-To: <1066492666.3f9162fa48fa1@webmail.iwinet.rug.nl>
References: <1066492666.3f9162fa48fa1@webmail.iwinet.rug.nl>
Message-ID: <3F91672D.2060905@statistik.uni-dortmund.de>

alberts at iwi201.iwinet.rug.nl wrote:
> Hello,
> 
> How do I write one million as 1,000,000 instead of 1000000?
> (cannot find it)
> 
> regards, R. Alberts

See ?fromatC :

  formatC(1000000, format = "fg", big.mark = ",")

Uwe Ligges



> ----------------------------------------------------------------
> This message was sent using IMP, the Internet Messaging Program.
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help



From MSchwartz at medanalytics.com  Sat Oct 18 18:19:00 2003
From: MSchwartz at medanalytics.com (Marc Schwartz)
Date: Sat, 18 Oct 2003 11:19:00 -0500
Subject: [R] integer notation 1,000,000 instead of 1000000
In-Reply-To: <1066492666.3f9162fa48fa1@webmail.iwinet.rug.nl>
References: <1066492666.3f9162fa48fa1@webmail.iwinet.rug.nl>
Message-ID: <1066493940.22403.18.camel@localhost.localdomain>

On Sat, 2003-10-18 at 10:57, alberts at iwi201.iwinet.rug.nl wrote:
> Hello,
> 
> How do I write one million as 1,000,000 instead of 1000000?
> (cannot find it)
> 
> regards, R. Alberts


formatC(1000000, format = "d", big.mark = ",")
[1] "1,000,000"

See ?formatC

HTH,

Marc Schwartz



From Mike.Prager at noaa.gov  Sat Oct 18 20:14:14 2003
From: Mike.Prager at noaa.gov (Mike Prager)
Date: Sat, 18 Oct 2003 14:14:14 -0400
Subject: [R] Oceanographic lattice plots?
Message-ID: <6.0.0.22.0.20031018140413.01b88508@hermes.nos.noaa.gov>

R 1.8.0 on Windows XP Professional.  A huge THANK YOU to the R Team for 
this marvelous software.

I am making lattice plots of oceanographic data.  The usual layout does not 
conform to plotting conventions that marine scientists use when depth is 
the independent variable.  Under those conventions, plots are made with the 
origin at the upper left, depth on the vertical axis (increasing as it goes 
down), and the dependent variable on the horizontal axis (increasing to the 
right).

That convention has implications not just in how axes are labeled and set 
up, but also when using smoothing routines such as panel.lowess(), because 
the smoothed values are on the horizontal axis, not the vertical axis.

Before I start looking at and modifying the R code that makes up the 
relevant routines, I wonder if any reader has already developed R routines 
for this purpose?


-- 
Michael Prager, Ph.D.
NOAA Beaufort Laboratory
Beaufort, North Carolina  28516
http://shrimp.ccfhrb.noaa.gov/~mprager/
***



From deepayan at stat.wisc.edu  Sat Oct 18 20:29:14 2003
From: deepayan at stat.wisc.edu (Deepayan Sarkar)
Date: Sat, 18 Oct 2003 13:29:14 -0500
Subject: [R] Oceanographic lattice plots?
In-Reply-To: <6.0.0.22.0.20031018140413.01b88508@hermes.nos.noaa.gov>
References: <6.0.0.22.0.20031018140413.01b88508@hermes.nos.noaa.gov>
Message-ID: <200310181329.14450.deepayan@stat.wisc.edu>

On Saturday 18 October 2003 13:14, Mike Prager wrote:
> R 1.8.0 on Windows XP Professional.  A huge THANK YOU to the R Team for
> this marvelous software.
>
> I am making lattice plots of oceanographic data.  The usual layout does not
> conform to plotting conventions that marine scientists use when depth is
> the independent variable.  Under those conventions, plots are made with the
> origin at the upper left, depth on the vertical axis (increasing as it goes
> down), and the dependent variable on the horizontal axis (increasing to the
> right).

In case you decide to work on this yourself, this might be useful: 

Ideally, specifications like 

xyplot(depth ~ x, ylim = c(10, 0)) 

should reverse the y-axis direction. As pointed out some time back, this 
doesn't work in lattice currently, but that should be fixed in the future (it 
already works in my development version).

> That convention has implications not just in how axes are labeled and set
> up, but also when using smoothing routines such as panel.lowess(), because
> the smoothed values are on the horizontal axis, not the vertical axis.

These should be easy to modify.

Deepayan

> Before I start looking at and modifying the R code that makes up the
> relevant routines, I wonder if any reader has already developed R routines
> for this purpose?



From tblackw at umich.edu  Sat Oct 18 21:37:04 2003
From: tblackw at umich.edu (Thomas W Blackwell)
Date: Sat, 18 Oct 2003 15:37:04 -0400 (EDT)
Subject: [R] nlm, hessian, and derivatives in obj function?
In-Reply-To: <001701c39509$f58eda10$0a00a8c0@rodan>
References: <001701c39509$f58eda10$0a00a8c0@rodan>
Message-ID: <Pine.SOL.4.58.0310181531430.6307@robotron.gpcc.itd.umich.edu>

Jeff  -

The function  obj()  which you define below is just
a bit peculiar, since inside the function it assigns
attributes to an object 'obj' with the same name as
the *function* but which has not previously been
defined inside the function.  Is this really what
you intended ?  I'm not enough of a guru to figure
out what R will do with this syntax.

-  tom blackwell  -  u michigan medical school  -  ann arbor  -

On Fri, 17 Oct 2003, Jeff D. Hamann wrote:

> I've been working on a new package and I have a few questions regarding the
> behaviour of the nlm function. I've been (for better or worse) using the nlm
> function to fit a linear model without suppling the hessian or gradient
> attributes in the objective function. I'm curious as to why the nlm requires
> 31 iterations (for the linear model), and then it doesn't work when I try to
> add the derivative information. I know using nlm for a linear model isn't
> the "optimal" method, but I would like to make sure the parameter estimates
> and the se's are matching before I attempt more difficult problems.
>
> rm(list=ls(all=TRUE))
> print( "running nlsystemfit models test at end...")
> data( kmenta )
> attach( kmenta )
> ##demand2 <- q ~ d0 + d1 * p + d2 * d
> supply2 <- q ~ s0 + s1 * p + s2 * f + s3 * a
> ##system2 <- list( demand2, supply2 )
> ##labels <- list( "Demand", "Supply" )
> ##inst <- ~ d + f + a
> ##sv2 <- c(d0=3,s2=2.123,d2=4,s0=-2.123,s3=4.234,d1=4.234,s1=0.234)
> sv2 <- c(s0=-2.123,s1=0.234,s2=2.123,s3=4.234)
>
> obj <- function( s, eqn, data, parmnames )
> {
>   ## get the values of the parameters
>   for( i in 1:length( parmnames ) )
>     {
>       name <- names( parmnames )[i]
>       val <- s[i]
>       storage.mode( val ) <-  "double"
>       assign( name, val )
>     }
>
>   lhs <- as.matrix( eval( as.formula( eqn )[[2]] ) )
>   rhs <- as.matrix( eval( as.formula( eqn )[[3]] ) )
>   resid <- crossprod( lhs - rhs )
>
>   ## just how does this work...
>   attr( obj, "value" ) <- resid
>   attr( obj, "gradient" ) <- attr( eval( deriv3( eqn, names(
> parmnames ) ) ), "gradient" )
>
> }
>
> res <- nlm( obj, sv2, hessian=T, eqn=supply2, data=kmenta, parmnames=sv2,
> check.analyticals=T)
>
> I haven't been able to get nlm to function as I keep getting the following
> error message:
>
> Error in nlm(obj, sv2, hessian = T, eqn = supply2, data = kmenta, parmnames
> = sv2,  :
>  invalid function value in 'nlm' optimizer
>
>
>
> I was under the impression that you could also obtain the se of the
> parameter estimates using the sqrt( diag( res$hessian ) ), but I haven't
> been able to reproduce the se computed by the Jacobian
>
> se <- sqrt( mse * diag( solve( crossprod( J ) ) ) )    # gives the correct
> results...
> hse <- sqrt( ( res$minimum / 8 ) * diag( solve( res$hessian ) ) )   # gives
> similar results, but why 8?
>
> I've tried to put the functionality to include the jacobian and hessian in
> the objective function for nlm without success as I don't know what the form
> of the functions will be ahead of time.
>
> and get the se from the sqrt( diag( hessian ) ), but it's nowhere close?
>
> Jeff.
>
> ---
> Jeff D. Hamann
> Hamann, Donald and Associates, Inc.
> PO Box 1421
> Corvallis, Oregon USA 97339-1421
> (office) 541-754-1428
> (cell) 541-740-5988
> jeff_hamann at hamanndonald.com
> www.hamanndonald.com



From j.haessler at comcast.net  Sat Oct 18 23:16:39 2003
From: j.haessler at comcast.net (Jeffrey Haessler)
Date: Sat, 18 Oct 2003 14:16:39 -0700
Subject: [R] (no subject)
Message-ID: <NGBBIIGCJKBFMFLBEPAHMEODCAAA.j.haessler@comcast.net>



From KLee at igeotech.com  Sun Oct 19 03:20:23 2003
From: KLee at igeotech.com (Kuantsai Lee)
Date: Sat, 18 Oct 2003 18:20:23 -0700
Subject: [R] Bagplot
Message-ID: <5.2.0.9.0.20031018180953.035489b8@mail.zoper.com>

Has anyone ported the Bagplot function by Rousseeuw, Ruts, and Tukey from S 
to R? The S function comprises a script and a FORTRAN function. I assume 
porting is relatively uncomplicated, but since I have not done any porting 
before I would not want to invest the effort if a port is readily available.



From paul at datavore.com  Sun Oct 19 16:02:35 2003
From: paul at datavore.com (Paul Meagher)
Date: Sun, 19 Oct 2003 11:02:35 -0300
Subject: [R] Markov chain resources and questions
References: <6.0.0.22.0.20031018140413.01b88508@hermes.nos.noaa.gov>
	<200310181329.14450.deepayan@stat.wisc.edu>
Message-ID: <002001c39649$a5f0b320$f07afea9@computer>

Can someone give me a pointer to where I should be looking for markov chain
resources in R?

Longer term, I am also interested in the question of whether explanatory
variables can coupled to a probability transition matrix to assist in
predicting the next state that a an object/system will go into.  I'm
imagining that this gets kind of ugly when you have nominal data (i.e., the
next state) that you are trying to predict using a transition matrix and you
want to try to boost your predictive power by incorporating other regressor
variables.  Can this be done, how, and is there something in R that does
this?

Another issue is how to assess the potential usefulness of the probability
transition matrix and the corresponding frequency matrix.  If your frequency
matrix only has a few observations in each cell then it is not too useful
for predictive purposes.  Also, if the probabilties are close to .50 in all
the cells, again it is not useful because it is not "informative" about the
next state.  Is their research that speaks to the issue of assessing the
"utility" or "informativeness" of the transition matrix for predictive
purposes.

Regards,
Paul Meagher



From spencer.graves at pdf.com  Sun Oct 19 16:46:04 2003
From: spencer.graves at pdf.com (Spencer Graves)
Date: Sun, 19 Oct 2003 07:46:04 -0700
Subject: [R] Markov chain resources and questions
In-Reply-To: <002001c39649$a5f0b320$f07afea9@computer>
References: <6.0.0.22.0.20031018140413.01b88508@hermes.nos.noaa.gov>	<200310181329.14450.deepayan@stat.wisc.edu>
	<002001c39649$a5f0b320$f07afea9@computer>
Message-ID: <3F92A3AC.4020802@pdf.com>

I just got several hits from "www.r-project.org" -> search -> "R site 
search" for "Markov chain", "Markov chain estimation", etc.  Have you 
tried that? 

help this helps.  spencer graves

Paul Meagher wrote:

>Can someone give me a pointer to where I should be looking for markov chain
>resources in R?
>
>Longer term, I am also interested in the question of whether explanatory
>variables can coupled to a probability transition matrix to assist in
>predicting the next state that a an object/system will go into.  I'm
>imagining that this gets kind of ugly when you have nominal data (i.e., the
>next state) that you are trying to predict using a transition matrix and you
>want to try to boost your predictive power by incorporating other regressor
>variables.  Can this be done, how, and is there something in R that does
>this?
>
>Another issue is how to assess the potential usefulness of the probability
>transition matrix and the corresponding frequency matrix.  If your frequency
>matrix only has a few observations in each cell then it is not too useful
>for predictive purposes.  Also, if the probabilties are close to .50 in all
>the cells, again it is not useful because it is not "informative" about the
>next state.  Is their research that speaks to the issue of assessing the
>"utility" or "informativeness" of the transition matrix for predictive
>purposes.
>
>Regards,
>Paul Meagher
>
>______________________________________________
>R-help at stat.math.ethz.ch mailing list
>https://www.stat.math.ethz.ch/mailman/listinfo/r-help
>  
>



From kjetil at entelnet.bo  Sun Oct 19 17:00:35 2003
From: kjetil at entelnet.bo (kjetil@entelnet.bo)
Date: Sun, 19 Oct 2003 11:00:35 -0400
Subject: [R] Markov chain resources and questions
In-Reply-To: <002001c39649$a5f0b320$f07afea9@computer>
Message-ID: <3F926ED3.18560.EAA19C@localhost>

On 19 Oct 2003 at 11:02, Paul Meagher wrote:

You could look at chapter 5 of Jim Lindset's online document

"The statistical analysis of stochastic processes in Time", at his 
website, www.luc.ac.be/~jlindsey
At this site there is also a collection of R functions for his
examples.

Kjetil Halvorsen

> Can someone give me a pointer to where I should be looking for markov chain
> resources in R?
> 
> Longer term, I am also interested in the question of whether explanatory
> variables can coupled to a probability transition matrix to assist in
> predicting the next state that a an object/system will go into.  I'm
> imagining that this gets kind of ugly when you have nominal data (i.e., the
> next state) that you are trying to predict using a transition matrix and you
> want to try to boost your predictive power by incorporating other regressor
> variables.  Can this be done, how, and is there something in R that does
> this?
> 
> Another issue is how to assess the potential usefulness of the probability
> transition matrix and the corresponding frequency matrix.  If your frequency
> matrix only has a few observations in each cell then it is not too useful
> for predictive purposes.  Also, if the probabilties are close to .50 in all
> the cells, again it is not useful because it is not "informative" about the
> next state.  Is their research that speaks to the issue of assessing the
> "utility" or "informativeness" of the transition matrix for predictive
> purposes.
> 
> Regards,
> Paul Meagher
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help



From paulda at BATTELLE.ORG  Sun Oct 19 18:28:17 2003
From: paulda at BATTELLE.ORG (Paul, David  A)
Date: Sun, 19 Oct 2003 12:28:17 -0400
Subject: [R] problem with win.metafile( )
Message-ID: <940250A9EB37A24CBE28D858EF07774967ABD5@ws-bco-mse3.milky-way.battelle.org>

R1.8.0, Win2k:

When I paste the code


win.metafile(file = "//.../plot1.wmf",
	width = 8.5, height = 6.25)
lset( list( background = list(col = "white")))	
xyplot( y ~ x | ID, data = Group1,
	scales = list(alternating = FALSE),
	ylim = c(.75,y.max),
	panel = function(x, y, panel.number, ... )
		{
			panel.superpose(x = Group1$TIME, y = Group1$y,
					subscripts = TRUE, groups =
Group1$ID,
					type = 'l', col = gray(.6))
			sup.sym <- trellis.par.get("superpose.symbol")
			panel.xyplot(x, y, type = 'b', col = "black", 
						lwd = 4, cex = 1.5, ...)
			panel.loess(x = Group1$TIME, y = Group1$y, 
			col = gray(.2), lwd = 3, lty = 5, span = 1/3)
		}
        )
dev.off()


into R1.8.0 I get the following messages:

...... 

> lset( list( background = list(col = "white")))
Error in get(x, envir, mode, inherits) : 
variable "win.metafile://.../plot1.wmf" was not found

...... 

> xyplot( ...... )
Error in get(x, envir, mode, inherits) : 
variable "win.metafile://.../plot1.wmf" was not found
> 
> dev.off()
null device 
          1


If I change the first command in this script to

trellis.device(postscript, file = "//.../plot1.ps", color = TRUE)

I get no errors.  I also get no errors if I run the above script
in R1.7.1.  Are there any known issues with win.metafile( ) in R1.8.0?
Should I reinstall?  Any help would be appreciated.


-david paul



From paulda at BATTELLE.ORG  Sun Oct 19 18:49:08 2003
From: paulda at BATTELLE.ORG (Paul, David  A)
Date: Sun, 19 Oct 2003 12:49:08 -0400
Subject: [R] problem with win.metafile( ): traceback()
Message-ID: <940250A9EB37A24CBE28D858EF07774967ABD6@ws-bco-mse3.milky-way.battelle.org>

For the first error message:

> win.metafile(file = "//.../plot1.wmf",
+ width = 8.5, height = 6.25)

> lset( list( background = list(col = "white")))
Error in get(x, envir, mode, inherits) : 
variable "win.metafile://.../plot1.wmf" was not found

> traceback()
4: get(device)
3: trellis.device(device = .Device, new = FALSE)
2: trellis.par.get(item)
1: lset(list(background = list(col = "white")))


For the second error message:

> xyplot( y ~ x | ID, data = Group1,
+ scales = list(alternating = FALSE),
+ ylim = c(.75,y.max),
+ panel = function(x, y, panel.number, ... )
+ {
+ panel.superpose(x = Group1$TIME, y = Group1$y,
+ subscripts = TRUE, groups = Group1$ID,
+ type = 'l', col = gray(.6))
+ 
+ sup.sym <- trellis.par.get("superpose.symbol")
+ panel.xyplot(x, y, type = 'b', col = "black", 
+ lwd = 4, cex = 1.5, ...)
+ 
+ #panel.loess(x = Group1$TIME, y = Group1$y, 
+ col = gray(.2), lwd = 3, lty = 5, span = 1/3)
+ }
+         )
Error in get(x, envir, mode, inherits) : 
variable "win.metafile://.../plot1.wmf" was not found

> traceback()
6: get(device)
5: trellis.device(device = .Device, new = FALSE)
4: trellis.par.get("add.text")
3: trellis.skeleton(aspect = "fill", strip = TRUE, panel = function (x, 
       y, panel.number, ...) 
   {
       panel.superpose(x = Group1$TIME, y = Group1$y, 
           subscripts = TRUE, groups = Group1$ID, type = "l", col =
gray(0.6))
       sup.sym <- trellis.par.get("superpose.symbol")
       panel.xyplot(x, y, type = "b", col = "black", lwd = 4, cex = 1.5, 
           ...)
   }
2: do.call("trellis.skeleton", c(list(aspect = aspect, strip = strip, 
       panel = panel), dots))
1: xyplot(y ~ x | ID, data = Group1, scales = list(alternating = FALSE), 
       ylim = c(0.75, y.max), 
       panel = function(x, y, panel.number, ...) {
           panel.superpose(x = Group1$TIME, y = Group1$y, 
               subscripts = TRUE, groups = Group1$ID, type = "l", 
               col = gray(0.6))
           sup.sym <- trellis.par.get("superpose.symbol")
           panel.xyplot(x, y, type = "b", col = "black", lwd = 4, 
               cex = 1.5, ...)
       })


-----Original Message-----
From: Paul, David A 
Sent: Sunday, October 19, 2003 12:28 PM
To: 'r-help at stat.math.ethz.ch'
Subject: [R] problem with win.metafile( )


R1.8.0, Win2k:

When I paste the code


win.metafile(file = "//.../plot1.wmf",
	width = 8.5, height = 6.25)
lset( list( background = list(col = "white")))	
xyplot( y ~ x | ID, data = Group1,
	scales = list(alternating = FALSE),
	ylim = c(.75,y.max),
	panel = function(x, y, panel.number, ... )
		{
			panel.superpose(x = Group1$TIME, y = Group1$y,
					subscripts = TRUE, groups =
Group1$ID,
					type = 'l', col = gray(.6))
			sup.sym <- trellis.par.get("superpose.symbol")
			panel.xyplot(x, y, type = 'b', col = "black", 
						lwd = 4, cex = 1.5, ...)
			panel.loess(x = Group1$TIME, y = Group1$y, 
			col = gray(.2), lwd = 3, lty = 5, span = 1/3)
		}
        )
dev.off()


into R1.8.0 I get the following messages:

...... 

> lset( list( background = list(col = "white")))
Error in get(x, envir, mode, inherits) : 
variable "win.metafile://.../plot1.wmf" was not found

...... 

> xyplot( ...... )
Error in get(x, envir, mode, inherits) : 
variable "win.metafile://.../plot1.wmf" was not found
> 
> dev.off()
null device 
          1


If I change the first command in this script to

trellis.device(postscript, file = "//.../plot1.ps", color = TRUE)

I get no errors.  I also get no errors if I run the above script in R1.7.1.
Are there any known issues with win.metafile( ) in R1.8.0? Should I
reinstall?  Any help would be appreciated.


-david paul

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://www.stat.math.ethz.ch/mailman/listinfo/r-help



From deepayan at stat.wisc.edu  Sun Oct 19 18:48:08 2003
From: deepayan at stat.wisc.edu (Deepayan Sarkar)
Date: Sun, 19 Oct 2003 11:48:08 -0500
Subject: [R] problem with win.metafile( ): traceback()
In-Reply-To: <940250A9EB37A24CBE28D858EF07774967ABD6@ws-bco-mse3.milky-way.battelle.org>
References: <940250A9EB37A24CBE28D858EF07774967ABD6@ws-bco-mse3.milky-way.battelle.org>
Message-ID: <200310191148.08579.deepayan@stat.wisc.edu>

On Sunday 19 October 2003 11:49, Paul, David A wrote:
> For the first error message:
> > win.metafile(file = "//.../plot1.wmf",
> + width = 8.5, height = 6.25)

Could you check what the value of the .Device variable (and .Devices as well) 
is at this point ? And not that it should matter, but what happens if you use

trellis.device(win.metafile, file = "//.../plot1.wmf",
               width = 8.5, height = 6.25)

Deepayan


> > lset( list( background = list(col = "white")))
>
> Error in get(x, envir, mode, inherits) :
> variable "win.metafile://.../plot1.wmf" was not found
>
> > traceback()
>
> 4: get(device)
> 3: trellis.device(device = .Device, new = FALSE)
> 2: trellis.par.get(item)
> 1: lset(list(background = list(col = "white")))



From p.murrell at auckland.ac.nz  Sun Oct 19 22:13:12 2003
From: p.murrell at auckland.ac.nz (Paul Murrell)
Date: Mon, 20 Oct 2003 09:13:12 +1300
Subject: [R] Strange behaviour
References: <HMWCRU$33742BA4ECD9AE7C0D98A69A00C5D01F@libero.it>
Message-ID: <3F92F058.8000108@stat.auckland.ac.nz>

Hi


v.demart at libero.it wrote:
> As an absolute beginner I'm reading and practicing with the Verzani doc to learn R.
> 
> Now, being an expert latex user who wants to integrate graphical capabilities if R and latex, using the "Simple" library and the simple.scatterplot examples I had a go at:
> 1)   Including the resulting graph into a doc.snw then compiled through sweave & latex;
> 2) Produce the graph  in pdf format directly (using pdf(filename) at the very beginning and before issuing the "simple.scatterplot " command) and including it into the latex file via \includegraphics.
> 
> In both cases the graph is a set of numbered rectangles. Investigating into the pdf files generated by R I found that they are made of 2 pages: the first contains those nasty rectangles while the second the "right" graph that should be inserted.
> 
> The same result comes out if I try the "layout" command example in the help of the same package.
> 
> My question is: is there a way to tell R to produce the second page only as a final pdf file?
> If not, any suggestion for a way out.


The "nasty rectangles" are the output of the layout.show() function. 
This function draws a simple diagram (consisting of nasty rectangles) to 
indicate the regions that a call to layout() has set up.  It is designed 
to help users to understand what on earth the layout() function is 
doing.  (It is NOT a necessary part of setting up an arrangement of 
plots using the layout() function.)

I suspect that the author of "simpleR" may have accidentally left the 
layout.show() call in simple.scatterplot() when copying the example from 
the layout() help file (apologies to John Verzani if this is an unfair 
diagnosis).

So the immediate solution to your problem is to remove the line ...

     layout.show(nf)

... from simple.scatterplot().  The output should then be a single page 
which should "include" ok in latex.

The larger problem of how to get at individual pages of output is 
probably best solved using something like the "onefile" argument to 
devices.  For example, look at the files produced by ...

     pdf(onefile=FALSE)
     example(layout)

... and at the help page for pdf() to see more about how to do this.

Hope that helps

Paul
-- 
Dr Paul Murrell
Department of Statistics
The University of Auckland
Private Bag 92019
Auckland
New Zealand
64 9 3737599 x85392
paul at stat.auckland.ac.nz
http://www.stat.auckland.ac.nz/~paul/



From s.mcclatchie at niwa.co.nz  Sun Oct 19 23:13:44 2003
From: s.mcclatchie at niwa.co.nz (Sam McClatchie)
Date: Mon, 20 Oct 2003 10:13:44 +1300
Subject: [R] Oceanographic lattice plots
Message-ID: <3F92FE88.2040205@niwa.co.nz>

System info:
Red Hat 9.0
R Version 1.8.0
ESS 5.1.21
Emacs 21.2.1
-------------------

Hello

I've been working with Paul Murrell here in New Zealand to develop plots 
of temperature and density profiles at 22 stations spanning a frontal 
zxone, and then overlaying the isothermal and mixed layer depths 
(similar to Kara et al. 2003 JGR Oceans 108(C3) pg. 24-5, figure 2).

Paul Murrell has a package called gridBase which works with R 1.8.0 that 
does the job nicely.

I've put the plot on a publicly accessible ftp site
<ftp.niwa.co.nz/incoming/mcclatchie/misc/ocean.plot.pdf>

Here is the code used to generate the plot that Paul developed with my 
collaboration.
---------------------------------------
panel.t.s.profiles.mld.paul.alternate <- function()
{
   ## Purpose:
   ## ----------------------------------------------------------------------
   ## Arguments:
   ## ----------------------------------------------------------------------
   ## Author: Paul Murrell & Sam McClatchie, Date: 16 Oct 2003, 09:43


require(gridBase)

## zt, p, zsigma, and lat are in the workspace

## ILD data generated by "calculate.MLD.and.ILD(0.8)" etc
#ILD values are in the workspace
#convert to positive values
ILD0.2 <- -ILD0.2
ILD0.2 <- matrix(c(ILD0.2,NA,NA),ncol=8,byrow=T)
ILD0.2.vec <- c(ILD0.2[1,],ILD0.2[2,],ILD0.2[2,])

ILD0.5 <- -ILD0.5
ILD0.5 <- matrix(c(ILD0.5,NA,NA),ncol=8,byrow=T)
ILD0.5.vec <- c(ILD0.5[1,],ILD0.5[2,],ILD0.5[2,])

ILD0.8 <- -ILD0.8
ILD0.8 <- matrix(c(ILD0.8,NA,NA),ncol=8,byrow=T)
ILD0.8.vec <- c(ILD0.8[1,],ILD0.8[2,],ILD0.8[2,])

## dummy MLD data
MLD0.5 <- ILD0.5*0.75
MLD0.5.vec <- c(MLD0.5[1,],MLD0.5[2,],MLD0.5[2,])

nrow <- 3
ncol <- 8
grid.newpage()
par(xpd=NA, col.axis="grey")
# Use grid to make a layout of rows and columns
# Each "row" consists of a plot region with a label area on top
# (i.e., a Trellis-like arrangement) hence the "nrow*2".
# The label area is 1 line high, the plot areas
# consume the remaining height.
# Maybe you could add extra rows and cols to this layout to
# create small gaps between each plot
# This layout is within a viewport which leaves margins for axes
# and labels
push.viewport(viewport(x=unit(4, "lines"),
                        y=unit(4, "lines"),
                        width=unit(1, "npc") - unit(6, "lines"),
                        height=unit(1, "npc") - unit(6, "lines"),
                        just=c("left", "bottom"),
                        layout=grid.layout(nrow*2, ncol,
                          heights=unit(rep(c(1, 1), nrow),
                            rep(c("lines", "null"), nrow)))))
for (i in 1:nrow) {
   for (j in 1:ncol) {
     index <- (i-1)*ncol + j
     evenrow <- i %% 2 == 0
     evencol <- j %% 2 == 0
     if (index < 23) {
       # Go to plot region i, j
       # Set the yscale for doing the overlay later
       push.viewport(viewport(layout.pos.row=i*2, layout.pos.col=j,
                              yscale=c(400, 0)))
       grid.rect(gp=gpar(col="grey"))
       # Draw first plot
       # Here's where we use gridBase to put a plot into a grid viewport
       # The par(plt=gridPLT()) makes the plotting region line up with
       # the current grid viewport (pushed two lines ago)
       par(plt=gridPLT(), new=TRUE, cex=0.8)

       plot(zt[,index],-p[,1], ylim=c(400,0), xlim=c(6,15), type='l',
            xlab="",ylab="", axes=FALSE, xpd=FALSE)
       # Draw axes (only do some)
       if (j == 1 && !evenrow)
         axis(2, col="grey")
       if (i == nrow && !evencol)
         axis(1, col="grey")
       par(new=TRUE)
       # Draw second plot
       plot(zsigmat[,index],-p[,1], ylim=c(400,0), type='l',lty=2,
            xlab="",
            ylab="",
            xlim=c(26.1,27.4),
            axes=FALSE, xpd=FALSE)

       # Draw axes
       if ((j == ncol || index == 22) && evenrow)
         axis(4, col="grey")
       pop.viewport()
       # Draw the latitude labels
       push.viewport(viewport(layout.pos.row=i*2 - 1, layout.pos.col=j,
                              gp=gpar(col="grey", fill="light grey")))
       grid.rect()
       grid.text(round(lat[index,],digits=2), gp=gpar(col="white"))
       # Draw top axes
       if (i == 1 && evencol) {
         par(plt=gridPLT())
         axis(3, col="grey")
       }
       pop.viewport()
     }
   }
}
       # Overlay mixed layer depths lines
       # Here we use some grid functions to do drawing
       # The 0.5 means "half way across the region",
       # the "native" means that the value is relative
       # to the yscale we set up when we created the viewport
for (i in 1:nrow) {
   for (j in 1:ncol) {
     index <- (i-1)*ncol + j
     if (index < 23) {
       push.viewport(viewport(layout.pos.row=i*2, layout.pos.col=j,
                              yscale=c(400, 0)))
       # Do a move.to in the first column and a line.to otherwise
       if (j == 1)
         grid.move.to(0.5, unit(MLD0.5[i, j], "native"))
       else
         grid.line.to(0.5, unit(MLD0.5[i, j], "native"),
                      gp=gpar(lty="dotted"))
       pop.viewport()
     }
   }
}

# Draw the overlay points last to overwrite the overlay lines
for (i in 1:nrow) {
   for (j in 1:ncol) {
     index <- (i-1)*ncol + j
     if (index < 23) {
       push.viewport(viewport(layout.pos.row=i*2, layout.pos.col=j,
                              yscale=c(400, 0)))
       # Overlay mixed layer depths points
       grid.points(0.5, unit(ILD0.5[i, j], "native"), pch=21,
                   size=unit(3, "mm"),
                   gp=gpar(fill=NULL))

        grid.points(0.5, unit(ILD0.2[i, j], "native"), pch=21,
                   size=unit(3, "mm"),
                   gp=gpar(fill=NULL))

        grid.points(0.5, unit(ILD0.8[i, j], "native"), pch=21,
                   size=unit(3, "mm"),
                   gp=gpar(fill=NULL))

        grid.points(0.5, unit(MLD0.5[i, j], "native"), pch=21,
                   size=unit(3, "mm"),
                   gp=gpar(fill="grey"))
       pop.viewport()
     }
   }
}
# Do some overall axis labels
push.viewport(viewport(layout.pos.row=nrow*2))
grid.text(expression(paste("temp", .^o,"C", sep="")),
           y=unit(-3, "lines"))
pop.viewport()
push.viewport(viewport(layout.pos.col=1))
grid.text("depth m", x=unit(-3, "lines"), rot=90)
pop.viewport()
pop.viewport()
}
------------------

I hope that some of this may be of use to you. It would be nice if you 
could let both myself and Paul see the result of efforts.

Cheers

Sam
-- 
Sam McClatchie, Research scientist (fisheries acoustics))))))))))
NIWA (National Institute of Water & Atmospheric Research Ltd)
PO Box 14 901, Kilbirnie, Wellington, New Zealand
s.mcclatchie at niwa.cri.nz
Research home page <http://www.smcc.150m.com/>
                          /\
               >><xX(&>
                       /// \\\
                      //// \\\\
                     ///  <%)Xx><<
                    /////  \\\\\\
              ><(((@>
        ><(((%>     ..>><xX(?>O<?)Xx><<



From mailinglist.wegmann at web.de  Sun Oct 19 23:31:01 2003
From: mailinglist.wegmann at web.de (Martin Wegmann)
Date: Sun, 19 Oct 2003 23:31:01 +0200
Subject: [R] lattice error
Message-ID: <200310192331.01925.mailinglist.wegmann@web.de>

Hello, 

I tried to open lattice, but I get the following error:

> library(lattice)
Error in loadNamespace(i, c(lib.loc, .libPaths()), keep.source) :
        package `grid' does not have a name space
Error in library(lattice) : package/namespace load failed
>

I retyped it after loading grid but the same message appeared. 

could it be that it is caused by a recently done update.packages() ?

any idea what might cause this problem?

using

> R.version.string
[1] "R version 1.7.1, 2003-06-16"


thanks in advance, Martin



From goedman at mac.com  Mon Oct 20 00:09:04 2003
From: goedman at mac.com (Rob J Goedman)
Date: Sun, 19 Oct 2003 15:09:04 -0700
Subject: [R] lattice error
In-Reply-To: <200310192331.01925.mailinglist.wegmann@web.de>
Message-ID: <DA0C29BE-0280-11D8-AA8D-000393DC5238@mac.com>

I started to notice this when I moved to 1.8.0 and e.g. load MASS.

Rob

On Sunday, October 19, 2003, at 02:31 PM, Martin Wegmann wrote:

> Hello,
>
> I tried to open lattice, but I get the following error:
>
>> library(lattice)
> Error in loadNamespace(i, c(lib.loc, .libPaths()), keep.source) :
>         package `grid' does not have a name space
> Error in library(lattice) : package/namespace load failed
>>
>
> I retyped it after loading grid but the same message appeared.
>
> could it be that it is caused by a recently done update.packages() ?
>
> any idea what might cause this problem?
>
> using
>
>> R.version.string
> [1] "R version 1.7.1, 2003-06-16"
>
>
> thanks in advance, Martin
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help



From bw at northbranchlogic.com  Mon Oct 20 00:28:30 2003
From: bw at northbranchlogic.com (Barnet Wagman)
Date: Sun, 19 Oct 2003 17:28:30 -0500
Subject: [R] Running RMySQL with SuSE 8.2?
Message-ID: <3F93100E.2060203@northbranchlogic.com>

Since there doesn't appear to be an RMySQL rpm for SuSE 8.*,  does 
anyone know if the 7.3 version will work with the SuSE 8.2 rpms of R and 
DBI?

The package installs without complaint, but when I try to run

    con <- dbConnect(dbDriver("MySQL"),dbname="test")

I get the error

    Error in dbConnect(dbDriver("MySQL")) : couldn't find function 
".valueClassTest"

(This is my first attempt to access a an rdms from R, so I could be 
doing something else wrong.)

Any ideas as what might be generating this error, or as to combinations 
of rpms that will work under SuSE 8.2 would be appreciated. (I took a 
stab at compiling RMySQL from src, but I don't have MySQL src installed 
and I rather not get involved in this if I can avoid it.)

Thanks,

Barnet Wagman



From p.murrell at auckland.ac.nz  Mon Oct 20 00:44:08 2003
From: p.murrell at auckland.ac.nz (Paul Murrell)
Date: Mon, 20 Oct 2003 11:44:08 +1300
Subject: [R] help with legend()
References: <8E46EB3BC001414AA6CDB57C5E551F8D12F209@thuja>	<16271.38970.646500.488205@gargle.gargle.HOWL>
	<200310170904.34303.deepayan@stat.wisc.edu>
Message-ID: <3F9313B8.2030806@stat.auckland.ac.nz>

Hi


Deepayan Sarkar wrote:
> On Friday 17 October 2003 02:20, Martin Maechler wrote:
> 
>>>>>>>"PaulSch" == Schwarz, Paul <paul.schwarz at oregonstate.edu>
>>>>>>>    on Wed, 15 Oct 2003 12:09:11 -0700 writes:
>>>>>>
>>    PaulSch> I am converting some S-PLUS scripts that I use for
>>    PaulSch> creating manuscript figures to R so that I can take
>>    PaulSch> advantage of the plotmath capabilities.  In my
>>    PaulSch> S-PLUS scripts I like to use the key() function for
>>    PaulSch> adding legends to plots,
>>
>>AFAIK  key() in S+ is from the trellis library section.
>>The corresponding R package, trellis, has
> 
>                                ^^^^^^^
> lattice, actually :-)
> 
> 
>>a draw.key() function that may work similarly to S-plus' key()
>>{Deepayan ?}.
> 
> 
> That's correct. Of course, the S-PLUS key() works wih non-trellis graphs as 
> well, whereas draw.key() will produce a grid object and hence work with grid 
> graphics only. (I haven't checked Paul's new gridBase package, that may 
> enable using this for base graphics as well.)


gridBase makes it possible, although it takes a little bit of work.
Here's a simple example.

## First a standard base plot (mangled example from lattice):

data(OrchardSprays)
attach(OrchardSprays)
tmt <- sort(as.numeric(treatment))
dec <- decrease[order(as.numeric(treatment))]
row <- rowpos[order(as.numeric(treatment))]
plot(tmt, dec, type="n")
for (i in unique(row)) {
   subset <- row == i
   lines(tmt[subset], dec[subset], col=i)
}

## Now load lattice (to produce the key) and gridBase (to combine the
## lattice key with the base plot):

library(lattice)
library(gridBase)

## Align grid viewports with base plot:

par(new=TRUE)
vps <- baseViewports()
push.viewport(vps$inner, vps$figure, vps$plot)

## Create lattice key:

key <- draw.key(list(lines = list(col=1:8),
                      text =
                    list(lab=as.character(unique(OrchardSprays$rowpos))),
                      columns = 4, title = "Row position",
                      background=par("bg"),
                      border=TRUE))

## Use a grid viewport to position the key 3mm in from the top-left
## corner of the plot (NOTE this doesn't quite work properly -- the
## width and height of the key are not calculated correctly
## [Deepayan: It's an error in grid and I'm working on a fix]):

push.viewport(viewport(x=unit(3, "mm"), y=unit(1, "npc") - unit(3,"mm"),
                        width=unit(1, "grobwidth", key),
                        height=unit(1, "grobheight", key),
                        just=c("left", "top")))
grid.draw(key)
# This just shows where the viewport is
# and shows how it is too big for the key
grid.rect(gp=gpar(col="grey"))

## Clean up:

pop.viewport(4)

Paul
-- 
Dr Paul Murrell
Department of Statistics
The University of Auckland
Private Bag 92019
Auckland
New Zealand
64 9 3737599 x85392
paul at stat.auckland.ac.nz
http://www.stat.auckland.ac.nz/~paul/



From deepayan at stat.wisc.edu  Mon Oct 20 00:21:34 2003
From: deepayan at stat.wisc.edu (Deepayan Sarkar)
Date: Sun, 19 Oct 2003 17:21:34 -0500
Subject: [R] lattice error
In-Reply-To: <200310192331.01925.mailinglist.wegmann@web.de>
References: <200310192331.01925.mailinglist.wegmann@web.de>
Message-ID: <200310191721.34703.deepayan@stat.wisc.edu>

On Sunday 19 October 2003 16:31, Martin Wegmann wrote:
> Hello,
>
> I tried to open lattice, but I get the following error:
> > library(lattice)
>
> Error in loadNamespace(i, c(lib.loc, .libPaths()), keep.source) :
>         package `grid' does not have a name space
> Error in library(lattice) : package/namespace load failed
>
>
> I retyped it after loading grid but the same message appeared.
>
> could it be that it is caused by a recently done update.packages() ?

Yes. You haven't upgraded R to 1.8.0, but are trying to use a version of 
lattice meant to work with 1.8.0 and above. (It would probably have worked if 
grid had been updated as well, but I think that won't happen due to some 
internal restructuring.)

Deepayan

> any idea what might cause this problem?
>
> using
>
> > R.version.string
>
> [1] "R version 1.7.1, 2003-06-16"
>
>
> thanks in advance, Martin
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help



From ekr at rtfm.com  Mon Oct 20 02:00:14 2003
From: ekr at rtfm.com (Eric Rescorla)
Date: Sun, 19 Oct 2003 17:00:14 -0700
Subject: [R] Fitting a Weibull/NaNs
Message-ID: <20031020000014.A46267141@sierra.rtfm.com>

I'm trying to fit a Weibull distribution to some data via maximum
likelihood estimation. I'm following the procedure described by Doug
Bates in his "Using Open Source Software to Teach Mathematical
Statistics" but I keep getting warnings about NaNs being converted to
maximum positive value:

> llfunc <- function (x) { -sum(dweibull(AM,shape=x[1],scale=x[2], log=TRUE))}
> mle <- nlm(llfunc,c(shape=1.5,scale=40), hessian=TRUE)
Warning messages: 
1: NaNs produced in: dweibull(x, shape, scale, log) 
2: NA/Inf replaced by maximum positive value 
3: NaNs produced in: dweibull(x, shape, scale, log) 
4: NA/Inf replaced by maximum positive value 
> 

Can someone offer some advice here?

Thanks,
-Ekr

--
[Eric Rescorla                                   ekr at rtfm.com]
                      http://www.rtfm.com/



From spencer.graves at pdf.com  Mon Oct 20 02:11:48 2003
From: spencer.graves at pdf.com (Spencer Graves)
Date: Sun, 19 Oct 2003 17:11:48 -0700
Subject: [R] Fitting a Weibull/NaNs
In-Reply-To: <20031020000014.A46267141@sierra.rtfm.com>
References: <20031020000014.A46267141@sierra.rtfm.com>
Message-ID: <3F932844.1080708@pdf.com>

      I have not used "nlm", but that happens routinely with function 
minimizers trying to test negative values for one or more component of 
x.  My standard approach to something like this is to parameterize 
"llfunc" in terms of log(shape) and log(scale), as follows: 

llfunc <- function (x) { -sum(dweibull(AM,shape=exp(x[1]),scale=exp(x[2]), log=TRUE))}

      Have you tried this?  If no, I suspect the warnings will disappear 
when you try this.  If not, I suggest you rewrite "llfunc" to store 
"nlglk <- (-sum(...))" and then print out x whenever nlglk is NA or Inf 
or Nan. 

      hope this helps.  spencer graves

Eric Rescorla wrote:

>I'm trying to fit a Weibull distribution to some data via maximum
>likelihood estimation. I'm following the procedure described by Doug
>Bates in his "Using Open Source Software to Teach Mathematical
>Statistics" but I keep getting warnings about NaNs being converted to
>maximum positive value:
>
>  
>
>>llfunc <- function (x) { -sum(dweibull(AM,shape=x[1],scale=x[2], log=TRUE))}
>>mle <- nlm(llfunc,c(shape=1.5,scale=40), hessian=TRUE)
>>    
>>
>Warning messages: 
>1: NaNs produced in: dweibull(x, shape, scale, log) 
>2: NA/Inf replaced by maximum positive value 
>3: NaNs produced in: dweibull(x, shape, scale, log) 
>4: NA/Inf replaced by maximum positive value 
>  
>
>
>Can someone offer some advice here?
>
>Thanks,
>-Ekr
>
>--
>[Eric Rescorla                                   ekr at rtfm.com]
>                      http://www.rtfm.com/
>
>______________________________________________
>R-help at stat.math.ethz.ch mailing list
>https://www.stat.math.ethz.ch/mailman/listinfo/r-help
>  
>



From Wanzare at HCJP.com  Mon Oct 20 02:24:03 2003
From: Wanzare at HCJP.com (Manoj - Hachibushu Capital)
Date: Mon, 20 Oct 2003 09:24:03 +0900
Subject: [R] Processing logic for Huge Data set
Message-ID: <1CBA12F2D414914989C723D196B287DC040455@jp-svr-ex1.HCJP.COM>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20031020/e05e3ea3/attachment.pl

From ekr at rtfm.com  Mon Oct 20 02:56:15 2003
From: ekr at rtfm.com (Eric Rescorla)
Date: 19 Oct 2003 17:56:15 -0700
Subject: [R] Fitting a Weibull/NaNs
In-Reply-To: <3F932844.1080708@pdf.com>
References: <20031020000014.A46267141@sierra.rtfm.com>
	<3F932844.1080708@pdf.com>
Message-ID: <kjfzhog21c.fsf@romeo.rtfm.com>

Spencer Graves <spencer.graves at pdf.com> writes:
>       I have not used "nlm", but that happens routinely with function
>       minimizers trying to test negative values for one or more
>       component of x.  My standard approach to something like this is
>       to parameterize "llfunc" in terms of log(shape) and log(scale),
>       as follows: llfunc <- function (x) {
>       -sum(dweibull(AM,shape=exp(x[1]),scale=exp(x[2]), log=TRUE))}
>
>       Have you tried this?  If no, I suspect the warnings will
>       disappear when you try this.

This works. I've got some more questions, though:

(1) Does it introduce bias to work with the logs like this?

(2) My original data set had zero values. I added .5 experimentally,
    which is how I got to this data set. This procedure doesn't work
    on the original data set.

    Instead I get (with the numbers below being the values
    that caused problems):

[1] 0.41 3.70 1.00
[1] 0.41 3.70 1.00
[1] 0.410001 3.700000 1.000000
[1] 0.410000 3.700004 1.000000
[1] 0.410000 3.700000 1.000001
Warning messages: 
1: NA/Inf replaced by maximum positive value 
2: NA/Inf replaced by maximum positive value 
3: NA/Inf replaced by maximum positive value 
4: NA/Inf replaced by maximum positive value 

Thanks,
-Ekr

-- 
[Eric Rescorla                                   ekr at rtfm.com]
                http://www.rtfm.com/



From mail at joeconway.com  Mon Oct 20 04:11:03 2003
From: mail at joeconway.com (Joe Conway)
Date: Sun, 19 Oct 2003 19:11:03 -0700
Subject: [R] Processing logic for Huge Data set
In-Reply-To: <1CBA12F2D414914989C723D196B287DC040455@jp-svr-ex1.HCJP.COM>
References: <1CBA12F2D414914989C723D196B287DC040455@jp-svr-ex1.HCJP.COM>
Message-ID: <3F934437.1040003@joeconway.com>

Manoj - Hachibushu Capital wrote:
>             I am new to R. I am trying to process this huge data set of
> matrix containing four columns, say x1, x2, x3, x4 and n number of rows.
> 
> I want to aggregate the matrix by x1 and perform statistic based on
> columns x2, x3, x4.

Someone will probably give you a way to do this directly in R, but if 
your data set is truly huge, at least one option is to use a PostgreSQL 
database for the data, and define a custom aggregate using PL/R. For a 
simple example, see:
   http://www.joeconway.com/plr/doc/plr-aggregate-funcs.html

HTH,

Joe



From spencer.graves at pdf.com  Mon Oct 20 04:20:28 2003
From: spencer.graves at pdf.com (Spencer Graves)
Date: Sun, 19 Oct 2003 19:20:28 -0700
Subject: [R] Fitting a Weibull/NaNs
In-Reply-To: <kjfzhog21c.fsf@romeo.rtfm.com>
References: <20031020000014.A46267141@sierra.rtfm.com>	<3F932844.1080708@pdf.com>
	<kjfzhog21c.fsf@romeo.rtfm.com>
Message-ID: <3F93466C.8030803@pdf.com>

      If the algorithm works properly, you should get exactly the same 
answer using a linear or a log scale for the parameters. 

      The bigger question is not bias but the accuracy of a normal 
approximation for confidence intervals and regions.  I have evaluated 
this by making contour plots of the log(likelihood).  I use "outer" to 
compute this over an appropriate grid of the parameters.  Then I use 
"contour" [or "image" with "contour(..., add=TRUE)"] to see the result.  
After I get a picture, I may specify the levels, using, e.g., 
2*log(likelihood ratio) is approximately chi-square with 2 degrees of 
freedom.  The normality assumption says that the contours should be 
close to elliptical.  I've also fit log(likelihood) to a parabola in the 
parameters, possibly after deleting points beyond the 0.001 level for 
chi-square(2).  If I get a good fit, I'm happy.  If not, I try a 
different parameterization. 

      When I've done this, I've found that I tend to get more nearly 
normal contours by throwing the constraint to (-Inf) than leaving it at 
0, i.e., by

      Bates and Watts (1988) Nonlinear Regression Analysis and Its 
Applications (Wiley) explain that parameter effects curvature seems to 
be vastly greater than the "intrinsic curvature" of the nonlinear 
manifold, onto which a response vector is projected by nonlinear least 
square.  This is different from maximum likelihood, but I believe that 
this principle would still likely apply. 

      Does this make sense? 
      spencer graves

         p.s.  I don't understand what you are saying about "0.41 3.70 
1.00" below.  You are giving me a set of three numbers when you are 
trying to estimate two parameters and getting NAs, Inf's and NaNs.  I 
don't understand.  Are you printing out "x" when the log(likelihood) is 
NA, NaN or Inf?  If yes, is one component of "x" <= 0? 

Eric Rescorla wrote:

>Spencer Graves <spencer.graves at pdf.com> writes:
>  
>
>>      I have not used "nlm", but that happens routinely with function
>>      minimizers trying to test negative values for one or more
>>      component of x.  My standard approach to something like this is
>>      to parameterize "llfunc" in terms of log(shape) and log(scale),
>>      as follows: llfunc <- function (x) {
>>      -sum(dweibull(AM,shape=exp(x[1]),scale=exp(x[2]), log=TRUE))}
>>
>>      Have you tried this?  If no, I suspect the warnings will
>>      disappear when you try this.
>>    
>>
>
>This works. I've got some more questions, though:
>
>(1) Does it introduce bias to work with the logs like this?
>
>(2) My original data set had zero values. I added .5 experimentally,
>    which is how I got to this data set. This procedure doesn't work
>    on the original data set.
>
>    Instead I get (with the numbers below being the values
>    that caused problems):
>
>[1] 0.41 3.70 1.00
>[1] 0.41 3.70 1.00
>[1] 0.410001 3.700000 1.000000
>[1] 0.410000 3.700004 1.000000
>[1] 0.410000 3.700000 1.000001
>Warning messages: 
>1: NA/Inf replaced by maximum positive value 
>2: NA/Inf replaced by maximum positive value 
>3: NA/Inf replaced by maximum positive value 
>4: NA/Inf replaced by maximum positive value 
>
>Thanks,
>-Ekr
>
>  
>



From TyagiAnupam at aol.com  Mon Oct 20 05:33:43 2003
From: TyagiAnupam at aol.com (TyagiAnupam@aol.com)
Date: Sun, 19 Oct 2003 23:33:43 EDT
Subject: [R] Processing logic for Huge Data set
Message-ID: <fb.485111a2.2cc4b197@aol.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20031019/b8ba1d77/attachment.pl

From ekr at rtfm.com  Mon Oct 20 06:18:09 2003
From: ekr at rtfm.com (Eric Rescorla)
Date: 19 Oct 2003 21:18:09 -0700
Subject: [R] Fitting a Weibull/NaNs
In-Reply-To: <3F93466C.8030803@pdf.com>
References: <20031020000014.A46267141@sierra.rtfm.com>
	<3F932844.1080708@pdf.com> <kjfzhog21c.fsf@romeo.rtfm.com>
	<3F93466C.8030803@pdf.com>
Message-ID: <kjad7wfsou.fsf@romeo.rtfm.com>

Spencer Graves <spencer.graves at pdf.com> writes:
>       Bates and Watts (1988) Nonlinear Regression Analysis and Its
>       Applications (Wiley) explain that parameter effects curvature
>       seems to be vastly greater than the "intrinsic curvature" of the
>       nonlinear manifold, onto which a response vector is projected by
>       nonlinear least square.  This is different from maximum
>       likelihood, but I believe that this principle would still likely
>       apply.     Does this make sense?     spencer graves
Some :)

>          p.s.  I don't understand what you are saying about "0.41 3.70
>          1.00" below.  You are giving me a set of three numbers when
>          you are trying to estimate two parameters and getting NAs,
>          Inf's and NaNs.  I don't understand.  Are you printing out
>          "x" when the log(likelihood) is NA, NaN or Inf?  If yes, is
>          one component of "x" <= 0? Eric Rescorla wrote:
Doh! Typographical error to R. I had the "hessian=TRUE" clause inside
the c(). Doesn't make any difference for the results, though.

I'm doing the following:

> llfunc <-
+   function (zzz) {
+     tmp <- -sum(dweibull(d$Age.Month,shape=exp(zzz[1]),scale=exp(zzz[2]), log=TRUE))
+      if(is.infinite(tmp) | is.na(tmp)) { print(zzz);}
+     tmp
+ 
+   }
> mle <- nlm(llfunc,c(shape=.37,scale=4.0), hessian=TRUE)
[1] 0.37 4.00
[1] 0.37 4.00
[1] 0.370001 4.000000
[1] 0.370000 4.000004
[1] 0.3701 4.0000
[1] 0.3700 4.0004
[1] 0.3702 4.0000
[1] 0.3701 4.0004
[1] 0.3700 4.0008
Warning messages: 
1: NA/Inf replaced by maximum positive value 
2: NA/Inf replaced by maximum positive value 
3: NA/Inf replaced by maximum positive value 
4: NA/Inf replaced by maximum positive value 
5: NA/Inf replaced by maximum positive value 
6: NA/Inf replaced by maximum positive value 
7: NA/Inf replaced by maximum positive value 
8: NA/Inf replaced by maximum positive value 

I'm a little vague on how this is supposed to work, but when
I just compute 

        -sum(dweibull(d$Age.Month,shape=1.5,scale=40,log=TRUE))

I get "Inf". 

The problem seems to be that some of the values of d$Age.Month are 0
and since the Weibull always has a value of 0 at 0, the log likelihood
comes out insane. (I'm getting 0 values due to quantization error). 
OTOH when I remove the 0 values it works great, but that seems
kind of ad hoc. Is there some standard fix for this? 

Thanks much,
-Ekr


-- 
[Eric Rescorla                                   ekr at rtfm.com]
                http://www.rtfm.com/



From Ted.Harding at nessie.mcc.ac.uk  Mon Oct 20 09:26:30 2003
From: Ted.Harding at nessie.mcc.ac.uk ( (Ted Harding))
Date: Mon, 20 Oct 2003 08:26:30 +0100 (BST)
Subject: [R] win.metafile and Linux
Message-ID: <XFMail.031020030346.Ted.Harding@nessie.mcc.ac.uk>

Hi Folks,

I see that people have been discussing the win.metafile device
on the list since before 2000.

Yet I have never seen this on a Linux distribution of R.

Is this because the device works by making calls (GPL calls of
course!) to a proprietary Windows library?

In that case I can understand that it would be far from kosher to
implement it on Linux. But I'd like confirmation.

I must say that being able to produce Windows metafiles on Linux
would help to bridge the gap between Linux and Windows: often one
needs to produce a graphic which will be imported into a Windows
application, but while PNG and the like are quite nice they don't
have the merit of being a vector format, and don't scale well.

Thanks,
Ted.


--------------------------------------------------------------------
E-Mail: (Ted Harding) <Ted.Harding at nessie.mcc.ac.uk>
Fax-to-email: +44 (0)870 167 1972
Date: 20-Oct-03                                       Time: 03:03:45
------------------------------ XFMail ------------------------------



From rossini at blindglobe.net  Mon Oct 20 09:43:31 2003
From: rossini at blindglobe.net (A.J. Rossini)
Date: Mon, 20 Oct 2003 00:43:31 -0700
Subject: [R] win.metafile and Linux
In-Reply-To: <XFMail.031020030346.Ted.Harding@nessie.mcc.ac.uk>
	(Ted.Harding@nessie.mcc.ac.uk's
	message of "Mon, 20 Oct 2003 08:26:30 +0100 (BST)")
References: <XFMail.031020030346.Ted.Harding@nessie.mcc.ac.uk>
Message-ID: <85oewce4m4.fsf@blindglobe.net>

(Ted Harding) <Ted.Harding at nessie.mcc.ac.uk> writes:

> Hi Folks,
>
> I see that people have been discussing the win.metafile device
> on the list since before 2000.
>
> Yet I have never seen this on a Linux distribution of R.
>
> Is this because the device works by making calls (GPL calls of
> course!) to a proprietary Windows library?
>
> In that case I can understand that it would be far from kosher to
> implement it on Linux. But I'd like confirmation.
>
> I must say that being able to produce Windows metafiles on Linux
> would help to bridge the gap between Linux and Windows: often one
> needs to produce a graphic which will be imported into a Windows
> application, but while PNG and the like are quite nice they don't
> have the merit of being a vector format, and don't scale well.
>

This has been discussed before; search the mailing list archives.
There are libraries which sort-of work (used to be poorly, not sure
what the exact status is now).  No one has wanted to put in the time
for finishing a sort-of working result up to now.

Note that the same did hold true for PICT format on Macs.  Not sure if
it still does, though.

best,
-tony

-- 
rossini at u.washington.edu            http://www.analytics.washington.edu/ 
Biomedical and Health Informatics   University of Washington
Biostatistics, SCHARP/HVTN          Fred Hutchinson Cancer Research Center
UW (Tu/Th/F): 206-616-7630 FAX=206-543-3461 | Voicemail is unreliable
FHCRC  (M/W): 206-667-7025 FAX=206-667-4812 | use Email

CONFIDENTIALITY NOTICE: This e-mail message and any attachme...{{dropped}}



From ripley at stats.ox.ac.uk  Mon Oct 20 10:05:23 2003
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Mon, 20 Oct 2003 09:05:23 +0100 (BST)
Subject: [R] win.metafile and Linux
In-Reply-To: <XFMail.031020030346.Ted.Harding@nessie.mcc.ac.uk>
Message-ID: <Pine.LNX.4.44.0310200854150.20687-100000@gannet.stats>

On Mon, 20 Oct 2003 Ted.Harding at nessie.mcc.ac.uk wrote:

> Hi Folks,
> 
> I see that people have been discussing the win.metafile device
> on the list since before 2000.
> 
> Yet I have never seen this on a Linux distribution of R.
> 
> Is this because the device works by making calls (GPL calls of
> course!) to a proprietary Windows library?

More precisely, it makes calls to the underlying OS: metafiles are
part of Windows.

> In that case I can understand that it would be far from kosher to
> implement it on Linux. But I'd like confirmation.

It's more a question of having the means to do so.  There have been 
projects to produce bitmap metafiles on Linux, but that's not what you 
want.  Look in the archives for discussion, and the Windows TODO on
developer.r-project.org says

The ability to write WMF on Unix. (This means vector WMF, not the
bitmaps that <tt>epstool</tt> and <tt>GSView</tt> make.  One possible
approach (suggested by Ben Bolker) would be to use <a
href="http://sourceforge.net/projects/libemf/"><tt>libEMF</tt></a>.)

Also, current versions of S-PLUS (on Linux/Unix) have wmf.graph() which
demonstrates that it should be feasible to do so.

> I must say that being able to produce Windows metafiles on Linux
> would help to bridge the gap between Linux and Windows: often one
> needs to produce a graphic which will be imported into a Windows
> application, but while PNG and the like are quite nice they don't
> have the merit of being a vector format, and don't scale well.

It's like reading/writing Excel files directly from R: it could be done
but no one has wanted to do it enough to contribute code to do so. It's
unclear if this would be a good use of scarce resources, as people who
want to use Windows applications by definition have access to Windows and
so could re-run scripts to generate plots using R under Windows (which is
what I do when forced to supply .wmf figures).

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From maechler at stat.math.ethz.ch  Mon Oct 20 10:22:37 2003
From: maechler at stat.math.ethz.ch (Martin Maechler)
Date: Mon, 20 Oct 2003 10:22:37 +0200
Subject: [R] Bagplot
In-Reply-To: <5.2.0.9.0.20031018180953.035489b8@mail.zoper.com>
References: <5.2.0.9.0.20031018180953.035489b8@mail.zoper.com>
Message-ID: <16275.39757.867706.919738@gargle.gargle.HOWL>

>>>>> "Kuantsai" == Kuantsai Lee <KLee at igeotech.com>
>>>>>     on Sat, 18 Oct 2003 18:20:23 -0700 writes:

    Kuantsai> Has anyone ported the Bagplot function by
    Kuantsai> Rousseeuw, Ruts, and Tukey from S to R? The S
    Kuantsai> function comprises a script and a FORTRAN
    Kuantsai> function. I assume porting is relatively
    Kuantsai> uncomplicated, but since I have not done any
    Kuantsai> porting before I would not want to invest the
    Kuantsai> effort if a port is readily available.

Yes, Christian Keller did a semi-port, which I had ``finished''
long ago.
When starting to experiment with it I found that it produced
segfaults (in the Fortran code) semi-stochastically and I never
found the time to figuring out why they happened.  I can have a look and provide
my "bagplot" package as source package -- but not via CRAN as
long the problems mentioned remain.
Can you build R package from the sources?

Regards,
Martin Maechler <maechler at stat.math.ethz.ch>	http://stat.ethz.ch/~maechler/
Seminar fuer Statistik, ETH-Zentrum  LEO C16	Leonhardstr. 27
ETH (Federal Inst. Technology)	8092 Zurich	SWITZERLAND
phone: x-41-1-632-3408		fax: ...-1228			<><



From bhx2 at mevik.net  Mon Oct 20 11:14:58 2003
From: bhx2 at mevik.net (=?iso-8859-1?q?Bj=F8rn-Helge_Mevik?=)
Date: Mon, 20 Oct 2003 11:14:58 +0200
Subject: [R] princomp with more coloumns than rows: why not?
In-Reply-To: <3F8ED6BB.9090007@pdf.com> (Spencer Graves's message of "Thu,
	16 Oct 2003 10:34:51 -0700")
References: <3A822319EB35174CA3714066D590DCD50205CCB6@usrymx25.merck.com>
	<3F8ED6BB.9090007@pdf.com>
Message-ID: <7ooewcb78t.fsf@foo.nemo-project.org>

Thanks for good suggestions for alternatives to princomp!

My original question, though, was /why/ it was decided to disallow
more coloumns than rows in princomp.  (And also whether it would be
possible to augment the result from prcomp with the coloumn means.)

-- 
Bj?rn-Helge Mevik



From michael.watson at bbsrc.ac.uk  Mon Oct 20 11:49:02 2003
From: michael.watson at bbsrc.ac.uk (michael watson (IAH-C))
Date: Mon, 20 Oct 2003 10:49:02 +0100
Subject: [R] How to upgrade R
Message-ID: <20B7EB075F2D4542AFFAF813E98ACD9301C00C1E@cl-exsrv1.irad.bbsrc.ac.uk>

Hi

I wondered what the standard procedure was for upgrading from one version of R to the next.  I currently have R 1.7.1 and want the latest release, R 1.8.0.  I am running SUSE linux 8.2.  The main thing is that I want to keep all of the libraries that I have installed for R 1.7.1 without having to re-install them.

I tried the rpm but that didn't really work :-(

I then tried installing from source and that worked.  I then copied my library/ folder across to the new installation, but now I get lots of error messages along the lines of:

Error in grep("\\w", platform) : 5 arguments passed to "grep" which requires 6.

So, my question being, how do I upgrade from R version 1.7.1 to R 1.8.0 and keep all of my libraries intact?

Thanks
Mick



From ucgamdo at ucl.ac.uk  Mon Oct 20 12:40:36 2003
From: ucgamdo at ucl.ac.uk (ucgamdo@ucl.ac.uk)
Date: Mon, 20 Oct 2003 11:40:36 +0100
Subject: [R] Re: win.metafiles in linux and R
Message-ID: <3.0.5.32.20031020114036.007de770@pop-server.ucl.ac.uk>

I posted a similar query some months ago. Some people answered stating that
win.metafiles were not available for R in linux. However, I was suggested
to use the package RSvgDevice, which allows transforming any graphic device
into Scalable Vector Graphics format, this file can then be opened with
sodipodi, and really impresive stuff can be achieved. I also noticed that
OpenOffice for linux has an option to open win.metafiles, so it seems that
they are in theory available for linux!



From adorio at math.upd.edu.ph  Tue Oct 21 08:47:03 2003
From: adorio at math.upd.edu.ph (Ernie Adorio)
Date: Tue, 21 Oct 2003 14:47:03 +0800
Subject: [R] interactive prompts
Message-ID: <200310211447.03799.adorio@math.upd.edu.ph>


Am using R on a Linux box and am currently writing an interactive R script.

1. How do I ask a user to press any key to continue ? I used a system call to 
read but this only works if the Enter key is pressed:
  print("Press any key to continue")
  system("read")

2. How do I get a string input from the user? Would like to see an R function,
say askget():

  delay  =  askget("Enter delay in seconds")
  system(paste( "sleep ", delay))

TIA,

Ernie Adorio
Math Department
University of the Philippines
Diliman, Quezon City



From kimm at pet.auh.dk  Mon Oct 20 12:55:56 2003
From: kimm at pet.auh.dk (Kim Mouridsen)
Date: Mon, 20 Oct 2003 12:55:56 +0200
Subject: [R] dev.print in R 1.7.1
Message-ID: <001001c396f8$bd38d4e0$ce65030a@pckim>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20031020/596f55f2/attachment.pl

From baron at psych.upenn.edu  Mon Oct 20 13:06:51 2003
From: baron at psych.upenn.edu (Jonathan Baron)
Date: Mon, 20 Oct 2003 07:06:51 -0400
Subject: [R] interactive prompts
In-Reply-To: <200310211447.03799.adorio@math.upd.edu.ph>
References: <200310211447.03799.adorio@math.upd.edu.ph>
Message-ID: <20031020110651.GA24201@mail2.sas.upenn.edu>

On 10/21/03 14:47, Ernie Adorio wrote:
>
>Am using R on a Linux box and am currently writing an interactive R script.
>
>1. How do I ask a user to press any key to continue ? I used a system call to 
>read but this only works if the Enter key is pressed:
>  print("Press any key to continue")
>  system("read")
>
>2. How do I get a string input from the user? Would like to see an R function,
>say askget():
>
>  delay  =  askget("Enter delay in seconds")
>  system(paste( "sleep ", delay))

readline()

-- 
Jonathan Baron, Professor of Psychology, University of Pennsylvania
Home page:            http://www.sas.upenn.edu/~baron
R page:               http://finzi.psych.upenn.edu/



From kwan022 at stat.auckland.ac.nz  Mon Oct 20 13:10:25 2003
From: kwan022 at stat.auckland.ac.nz (Ko-Kang Kevin Wang)
Date: Tue, 21 Oct 2003 00:10:25 +1300 (NZDT)
Subject: [R] interactive prompts
In-Reply-To: <200310211447.03799.adorio@math.upd.edu.ph>
Message-ID: <Pine.LNX.4.44.0310210009520.6725-100000@stat55.stat.auckland.ac.nz>

On Tue, 21 Oct 2003, Ernie Adorio wrote:

> Am using R on a Linux box and am currently writing an interactive R script.
> 
> 1. How do I ask a user to press any key to continue ? I used a system call to 
> read but this only works if the Enter key is pressed:
>   print("Press any key to continue")
>   system("read")

  par(ask = TRUE)
will do what you want.

-- 
Cheers,

Kevin

---------------------------------------------------------------
Ko-Kang Kevin Wang
Master of Science (MSc) Student
SLC Tutor and Lab Demonstrator
Department of Statistics
University of Auckland
New Zealand
Homepage: http://www.stat.auckland.ac.nz/~kwan022
Ph: 373-7599
    x88475 (City)
    x88480 (Tamaki)



From ripley at stats.ox.ac.uk  Mon Oct 20 13:15:46 2003
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Mon, 20 Oct 2003 12:15:46 +0100 (BST)
Subject: [R] Re: win.metafiles in linux and R
In-Reply-To: <3.0.5.32.20031020114036.007de770@pop-server.ucl.ac.uk>
Message-ID: <Pine.LNX.4.44.0310201206280.1219-100000@gannet.stats>

On Mon, 20 Oct 2003 ucgamdo at ucl.ac.uk wrote:

> I posted a similar query some months ago. Some people answered stating that
> win.metafiles were not available for R in linux. However, I was suggested
> to use the package RSvgDevice, which allows transforming any graphic device
> into Scalable Vector Graphics format, this file can then be opened with
> sodipodi, and really impresive stuff can be achieved. I also noticed that
> OpenOffice for linux has an option to open win.metafiles, so it seems that
> they are in theory available for linux!

Code to *read* them is more widely available than to *write* them.
Can sodipodi output wmf/emf?

Note that libEMF's help page says

  It is also possible now to generate EMF files from PSTOEDIT on POSIX 
  systems. Therefore, if your graphics code only outputs PostScript, you 
  can now easily convert it to EMF.

but that is only true on Windows (unfortunately).

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From ripley at stats.ox.ac.uk  Mon Oct 20 13:23:55 2003
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Mon, 20 Oct 2003 12:23:55 +0100 (BST)
Subject: [R] interactive prompts
In-Reply-To: <200310211447.03799.adorio@math.upd.edu.ph>
Message-ID: <Pine.LNX.4.44.0310201217420.1219-100000@gannet.stats>

On Tue, 21 Oct 2003, Ernie Adorio wrote:

> 
> Am using R on a Linux box and am currently writing an interactive R script.
> 
> 1. How do I ask a user to press any key to continue ? I used a system call to 
> read but this only works if the Enter key is pressed:
>   print("Press any key to continue")
>   system("read")

You seem over-fond of the system() command!  Try

cat("Press enter key to continue\n")
foo <- readLines(stdin(), 1)

In general R does not have a character-by-character interface with a 
keyboard.

> 2. How do I get a string input from the user? Would like to see an R function,
> say askget():
> 
>   delay  =  askget("Enter delay in seconds")
>   system(paste( "sleep ", delay))

cat("Enter delay in seconds:\n")
Delay <- scan("", numeric(1))
Sys.sleep(Delay)

will do it (there are other more elegant ways such as

testit <- function()
{
  cat("Enter delay in seconds: ")
  Delay <- as.numeric(readLines(stdin(), 1))
  if(is.na(Delay)) stop("non-numeric input")
  Sys.sleep(Delay)
}

)


-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From Roger.Bivand at nhh.no  Mon Oct 20 13:24:18 2003
From: Roger.Bivand at nhh.no (Roger Bivand)
Date: Mon, 20 Oct 2003 13:24:18 +0200 (CEST)
Subject: [R] interactive prompts
In-Reply-To: <200310211447.03799.adorio@math.upd.edu.ph>
Message-ID: <Pine.LNX.4.44.0310201318580.31878-100000@reclus.nhh.no>

On Tue, 21 Oct 2003, Ernie Adorio wrote:

> 
> Am using R on a Linux box and am currently writing an interactive R script.
> 
> 1. How do I ask a user to press any key to continue ? I used a system call to 
> read but this only works if the Enter key is pressed:
>   print("Press any key to continue")
>   system("read")

Maybe just readline("Press Enter to continue")? Why give the user the 
choice?

> 
> 2. How do I get a string input from the user? Would like to see an R function,
> say askget():
> 
>   delay  =  askget("Enter delay in seconds")
>   system(paste( "sleep ", delay))

readline(), but the return value needs to be cast from a character vector
of length one to what you need, because you can't depend on the user to
only give admissible values. Although your example is just that,
Sys.sleep()  gets you there directly.


> 
> TIA,
> 
> Ernie Adorio
> Math Department
> University of the Philippines
> Diliman, Quezon City
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> 

-- 
Roger Bivand
Economic Geography Section, Department of Economics, Norwegian School of
Economics and Business Administration, Breiviksveien 40, N-5045 Bergen,
Norway. voice: +47 55 95 93 55; fax +47 55 95 93 93
e-mail: Roger.Bivand at nhh.no



From ripley at stats.ox.ac.uk  Mon Oct 20 13:29:10 2003
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Mon, 20 Oct 2003 12:29:10 +0100 (BST)
Subject: [R] dev.print in R 1.7.1
In-Reply-To: <001001c396f8$bd38d4e0$ce65030a@pckim>
Message-ID: <Pine.LNX.4.44.0310201224410.1219-100000@gannet.stats>

I can't help you with 1.7.1: it is obselete.  But since you are apparently
using Windows, there are menus in 1.8.0 to do what you want, as well as
the command savePlot() (AFAIK there never was a function save.plot).

On Mon, 20 Oct 2003, Kim Mouridsen wrote:

> Dear R experts
> 
>  
> 
> How do I save a plot to a file in a specified format, f.ex ?png??
> 
>  
> 
> Apparently ?save.plot? no longer exists, so I tried instead
> 
>  
> 
> dev.print(file="H:\\jesperf\\data1image",device=png())

That should be device=png, not png(), as the help page says.

> However no file is created and ? much worse ? no graphics is produced
> (on screen or file) if I run
> 
> f.ex qqnorm afterwards.

Try dev.list() and dev.off()?

> What am I doing wrong and how do I get R to print graphics on the screen
> as ususal?

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From haynesm at cfr.nichd.nih.gov  Mon Oct 20 13:35:37 2003
From: haynesm at cfr.nichd.nih.gov (Haynes, Maurice (NIH/NICHD))
Date: Mon, 20 Oct 2003 07:35:37 -0400
Subject: [R] Lilliefors Test
Message-ID: <6000BB14AFA9A741BC2315A598837ED5693A3E@nihexchange4.nih.gov>

Is it not true that the Shapiro Wilks test implemented in the package
ctest requires the assumption that the population variance of the
variable is known?

Is it also not true that the Lilliefors is not a test of normality
as such, but is rather a correction of the p-value for the
Kolmogorov Smirnov test?

Thanks,

Maurice Haynes
National Institute of Child Health and Human Development
Child and Family Research Section
6705 Rockledge Drive
Bethesda, MD  20892
Voice: 301-496-8180
Fax: 301-496-2766
E-Mail: mh192j at nih.gov



| -----Original Message-----
| From: kjetil at entelnet.bo [mailto:kjetil at entelnet.bo]
| Sent: Friday, October 17, 2003 3:33 PM
| To: R HELP; Martina Pavlicova
| Subject: Re: [R] Lilliefors Test
| 
| 
| On 17 Oct 2003 at 13:59, Martina Pavlicova wrote:
| 
| There is shapiro.test in package ctest, which have much better power 
| properties than Lillefors test. So there is no need to have 
| Lilliefors test in R, except for archeological interest. 
| 
| Kjetil Halvorsen
| 
| > 
| > Hello everybody,
| > 
| > I would like to perform a test for normality (without specifying the
| > mean a variance) on the sample data (80 observations). I found that
| > Lilliefors test is appropriate. Does anybody have it 
| programmed already,
| > or is there a function for this test in R?
| > 
| > Thank you very much,
| > 
| > Martina Pavlicova
| > 
| --------------------------------------------------------------
| ------------
| > Department of Statistics             Office Phone: (614) 292-1567
| > 1958 Neil Avenue, 304E Cockins Hall  FAX: (614) 292-2096
| > The Ohio State University            E-mail: 
| pavlicov at stat.ohio-state.edu
| > Columbus, OH 43210-1247              
| www.stat.ohio-state.edu/~pavlicov
| > 
| > ______________________________________________
| > R-help at stat.math.ethz.ch mailing list
| > https://www.stat.math.ethz.ch/mailman/listinfo/r-help
| 
| ______________________________________________
| R-help at stat.math.ethz.ch mailing list
| https://www.stat.math.ethz.ch/mailman/listinfo/r-help
|



From kwan022 at stat.auckland.ac.nz  Mon Oct 20 13:50:20 2003
From: kwan022 at stat.auckland.ac.nz (Ko-Kang Kevin Wang)
Date: Tue, 21 Oct 2003 00:50:20 +1300 (NZDT)
Subject: [R] interactive prompts
In-Reply-To: <Pine.LNX.4.44.0310210009520.6725-100000@stat55.stat.auckland.ac.nz>
Message-ID: <Pine.LNX.4.44.0310210049340.6725-100000@stat55.stat.auckland.ac.nz>

On Tue, 21 Oct 2003, Ko-Kang Kevin Wang wrote:

> On Tue, 21 Oct 2003, Ernie Adorio wrote:
> 
> > Am using R on a Linux box and am currently writing an interactive R script.
> > 
> > 1. How do I ask a user to press any key to continue ? I used a system call to 
> > read but this only works if the Enter key is pressed:
> >   print("Press any key to continue")
> >   system("read")
> 
>   par(ask = TRUE)
> will do what you want.

Sorry, that will only work if you are plotting graphs.  I didn't quite 
read your question properly *_*.

-- 
Cheers,

Kevin

---------------------------------------------------------------
Ko-Kang Kevin Wang
Master of Science (MSc) Student
SLC Tutor and Lab Demonstrator
Department of Statistics
University of Auckland
New Zealand
Homepage: http://www.stat.auckland.ac.nz/~kwan022
Ph: 373-7599
    x88475 (City)
    x88480 (Tamaki)



From Torsten.Hothorn at rzmail.uni-erlangen.de  Mon Oct 20 13:51:16 2003
From: Torsten.Hothorn at rzmail.uni-erlangen.de (Torsten Hothorn)
Date: Mon, 20 Oct 2003 13:51:16 +0200 (CEST)
Subject: [R] How to upgrade R
In-Reply-To: <20B7EB075F2D4542AFFAF813E98ACD9301C00C1E@cl-exsrv1.irad.bbsrc.ac.uk>
References: <20B7EB075F2D4542AFFAF813E98ACD9301C00C1E@cl-exsrv1.irad.bbsrc.ac.uk>
Message-ID: <Pine.LNX.4.51.0310201345380.20097@artemis.imbe.med.uni-erlangen.de>

On Mon, 20 Oct 2003, michael watson (IAH-C) wrote:

> Hi
>
> I wondered what the standard procedure was for upgrading from one version of R to the next.  I currently have R 1.7.1 and want the latest release, R 1.8.0.  I am running SUSE linux 8.2.  The main thing is that I want to keep all of the libraries that I have installed for R 1.7.1 without having to re-install them.
>
> I tried the rpm but that didn't really work :-(
>
> I then tried installing from source and that worked.  I then copied my library/ folder across to the new installation, but now I get lots of error messages along the lines of:
>
> Error in grep("\\w", platform) : 5 arguments passed to "grep" which requires 6.
>
> So, my question being, how do I upgrade from R version 1.7.1 to R 1.8.0 and keep all of my libraries intact?
>

You should not install your packages to the library path of R itself but
to something like `$HOME/lib/R' and tell R about it (via the R_LIBS
environment variable, see section 4 of R Installation and Administration).

Note that in contrast recommended packages may be installed in R's install
directory and that is what's happens by default.

Best,

Torsten


> Thanks
> Mick
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
>
>



From B.Rowlingson at lancaster.ac.uk  Mon Oct 20 14:01:45 2003
From: B.Rowlingson at lancaster.ac.uk (Barry Rowlingson)
Date: Mon, 20 Oct 2003 13:01:45 +0100
Subject: [R] interactive prompts
In-Reply-To: <Pine.LNX.4.44.0310201318580.31878-100000@reclus.nhh.no>
References: <Pine.LNX.4.44.0310201318580.31878-100000@reclus.nhh.no>
Message-ID: <3F93CEA9.7090608@lancaster.ac.uk>


>>Am using R on a Linux box and am currently writing an interactive R script.
>>
>>1. How do I ask a user to press any key to continue ? I used a system call to 
>>read but this only works if the Enter key is pressed:
>>  print("Press any key to continue")
>>  system("read")

  If you want to do something prettier, you can use the tcltk library 
and have little dialog boxes, input boxes, menus, etc.

  If you do, then could you also write some nice documentation for how 
to do it in R, since the R docs refer you to the Tcl/Tk docs, and you 
end up having to back-translate everything to R. Or is there a nice 
R-TclTk guide somewhere that I've not found yet?

Baz



From purvis at mbi.ufl.edu  Mon Oct 20 14:03:13 2003
From: purvis at mbi.ufl.edu (Purvis Bedenbaugh)
Date: Mon, 20 Oct 2003 08:03:13 -0400
Subject: [R] R - S compatibility table
Message-ID: <450B582867097B4E8B19AE7F947C3BEE59E4D0@mbi-00.mbi.ufl.edu>

Hello all - I've just recently been exploring R for the
first time. After noticing a few things that have changed
from S to R, I started looking for an R-S compatibility table
but didn't find it. Is such a table out there ? Where ?

Thanks much,

Purvis Bedenbaugh
purvis at mbi.ufl.edu

Examples:

'stdev' is now 'sd'  - is it exactly the same computation ?
couldn't find a built-in for error.bar()
syntax that is an error in R: param(thisframe,"b") <- value

> version
         _               
platform mips-sgi-irix6.5
arch     mips            
os       irix6.5         
system   mips, irix6.5   
status                   
major    1               
minor    8.0             
year     2003            
month    10              
day      08              
language R               
>



From dmurdoch at pair.com  Mon Oct 20 14:01:43 2003
From: dmurdoch at pair.com (Duncan Murdoch)
Date: Mon, 20 Oct 2003 08:01:43 -0400
Subject: [R] Fitting a Weibull/NaNs
In-Reply-To: <kjad7wfsou.fsf@romeo.rtfm.com>
References: <20031020000014.A46267141@sierra.rtfm.com>
	<3F932844.1080708@pdf.com> <kjfzhog21c.fsf@romeo.rtfm.com>
	<3F93466C.8030803@pdf.com> <kjad7wfsou.fsf@romeo.rtfm.com>
Message-ID: <iej7pvoqtk38i0a3ue95rv8d7acfdpb43l@4ax.com>

On 19 Oct 2003 21:18:09 -0700, you wrote:

>The problem seems to be that some of the values of d$Age.Month are 0
>and since the Weibull always has a value of 0 at 0, the log likelihood
>comes out insane. (I'm getting 0 values due to quantization error). 
>OTOH when I remove the 0 values it works great, but that seems
>kind of ad hoc. Is there some standard fix for this? 

A standard fix is to replace zeros with some small positive number,
but this isn't entirely satisfactory.  It's definitely worthwhile
doing a sensitivity test:  e.g. do you get essentially the same answer
using 0.01 and 0.0001 for the replacement value?  If not, you might
want to question the use of a model that predicts zero density in an
area where you've got observations.

Duncan Murdoch



From andy_liaw at merck.com  Mon Oct 20 14:10:40 2003
From: andy_liaw at merck.com (Liaw, Andy)
Date: Mon, 20 Oct 2003 08:10:40 -0400
Subject: [R] Processing logic for Huge Data set
Message-ID: <3A822319EB35174CA3714066D590DCD50205CCE6@usrymx25.merck.com>

> From: TyagiAnupam at aol.com [mailto:TyagiAnupam at aol.com] 
> 
> Loops are time consuming in R. Try one of the apply functions 
> for vectorized 
> calculations, like "apply", "lapply","sapply" or "tapply". 
> Also see help for 
> "split".

Have you actually compared for loop with apply, in terms of timing?  Have
you looked at the R code for apply()?  It has:

    <...>
    if (length(d.call) < 2) {
        if (length(dn.call)) 
            dimnames(newX) <- c(dn.call, list(NULL))
        for (i in 1:d2) ans[[i]] <- FUN(newX[, i], ...)
    }
    else for (i in 1:d2) ans[[i]] <- FUN(array(newX[, i], d.call, 
        dn.call), ...)
    <...>

Notice the for loop there!  While what you said about apply and for loop
might be true for (older version of) Splus, it's not true for R.

lapply() does do the looping at the C level.  sapply and tapply uses lapply,
so they can be faster than for loop at the R level.

Andy


> 
> In a message dated 10/19/03 5:25:51 PM Pacific Daylight Time, 
> Wanzare at HCJP.com writes:
> 
> > Hello All,
> >       I am new to R. I am trying to process this huge data set of 
> > matrix containing four columns, say x1, x2, x3, x4 and n number of 
> > rows.
> > 
> > I want to aggregate the matrix by x1 and perform statistic based on 
> > columns x2, x3, x4. I tried aggregate function but it gave 
> me memory 
> > allocation error (which I am not surprised), so I ended up 
> performing 
> > a for loop based on x1 and subsetting the matrix based on 
> x1. However 
> > I have a hunch that their should be a less expensive way of 
> doing this 
> > processing.  Any ideas or tips to optimize this processing 
> logic would 
> > be greatly appreciated.
> > 
> 
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list 
> https://www.stat.math.ethz.ch/mailman/listinfo> /r-help
>



From andy_liaw at merck.com  Mon Oct 20 14:18:45 2003
From: andy_liaw at merck.com (Liaw, Andy)
Date: Mon, 20 Oct 2003 08:18:45 -0400
Subject: [R] How to upgrade R
Message-ID: <3A822319EB35174CA3714066D590DCD50205CCE7@usrymx25.merck.com>

What I do is to separate packages that shipped with R separate from other
contributed packages from CRAN, so that when I upgrade, I can wipe clean the
old R and re-install while keeping all other packages in place.

What's not clear to me is a good way of keeping two versions of R
simultaneously (for ease of transition).  Can anyone suggest a good strategy
for doing that on *nix?

Best,
Andy

> -----Original Message-----
> From: michael watson (IAH-C) [mailto:michael.watson at bbsrc.ac.uk] 
> Sent: Monday, October 20, 2003 5:49 AM
> To: r-help
> Subject: [R] How to upgrade R
> 
> 
> Hi
> 
> I wondered what the standard procedure was for upgrading from 
> one version of R to the next.  I currently have R 1.7.1 and 
> want the latest release, R 1.8.0.  I am running SUSE linux 
> 8.2.  The main thing is that I want to keep all of the 
> libraries that I have installed for R 1.7.1 without having to 
> re-install them.
> 
> I tried the rpm but that didn't really work :-(
> 
> I then tried installing from source and that worked.  I then 
> copied my library/ folder across to the new installation, but 
> now I get lots of error messages along the lines of:
> 
> Error in grep("\\w", platform) : 5 arguments passed to "grep" 
> which requires 6.
> 
> So, my question being, how do I upgrade from R version 1.7.1 
> to R 1.8.0 and keep all of my libraries intact?
> 
> Thanks
> Mick
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list 
> https://www.stat.math.ethz.ch/mailman/listinfo> /r-help
>



From B.Rowlingson at lancaster.ac.uk  Mon Oct 20 14:20:34 2003
From: B.Rowlingson at lancaster.ac.uk (Barry Rowlingson)
Date: Mon, 20 Oct 2003 13:20:34 +0100
Subject: [R] interactive prompts - tcltk examples
In-Reply-To: <Pine.LNX.4.44.0310201318580.31878-100000@reclus.nhh.no>
References: <Pine.LNX.4.44.0310201318580.31878-100000@reclus.nhh.no>
Message-ID: <3F93D312.9000108@lancaster.ac.uk>

I've found a nice page of tcltk examples:

http://bioinf.wehi.edu.au/~wettenhall/RTclTkExamples/

  so no excuse now for giving R users nice UIs. Or making horrendously 
bad ones, of course.

  I've just been a collaborator on a grant application knocked back 
partly because the funders thought non-technical users might not be able 
to use our R code, so I'm looking at UI issues a bit these days. Its 
either that or telling funding bodies that we'll be writing Excel macros....

Baz



From pingping.zheng at lancaster.ac.uk  Mon Oct 20 14:35:21 2003
From: pingping.zheng at lancaster.ac.uk (Pingping Zheng)
Date: Mon, 20 Oct 2003 13:35:21 +0100
Subject: [R] presentation of spatial-temporal point processes
Message-ID: <3F93D689.6070700@lancs.ac.uk>

Hello all,

Would anybody tell me how to present spatial-temoral point processes in R,
for example, I'd like to plot the spatial points in the sequence of 
their time domain?

Cheers

-- 
Pingping Zheng
Department of Mathematics and Statistics
Fylde College
Lancaster University
Lancaster LA1 4YF
UK



From temiz at deprem.gov.tr  Mon Oct 20 14:48:44 2003
From: temiz at deprem.gov.tr (temiz)
Date: Mon, 20 Oct 2003 15:48:44 +0300
Subject: [R] interpolation of the vector
Message-ID: <3F93D9AC.60806@deprem.gov.tr>

Hello

library(fields)
 x<-cbind(dt1$V1,dt1$V2)  # V1:east , V2=north coordinates of points
 Y<-cbind(dt1$V4,dt1$V5)  # V4,V5  cos and sines components
 fit2 <- Krig(x,Y,cov.function=exp.cov, scale.type="range") : this gives 
"Error in if (abs(f2 - f1) < tol) { : missing value where TRUE/FALSE needed"

what should I have done ?

is interpolation of x,y components as a vector ok ?
or
what would you suggest for interpolation of the vector Y ?

I will appreciate , if you tell me what I have to do

Ahmet Temiz
General Directory of Disaster Affairs
Ankara TURKEY



______________________________________
Inflex - installed on mailserver for domain @deprem.gov.tr
Queries to: postmaster at deprem.gov.tr

______________________________________
The views and opinions expressed in this e-mail message are ...{{dropped}}



From ripley at stats.ox.ac.uk  Mon Oct 20 14:47:09 2003
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Mon, 20 Oct 2003 13:47:09 +0100 (BST)
Subject: [R] R - S compatibility table
In-Reply-To: <450B582867097B4E8B19AE7F947C3BEE59E4D0@mbi-00.mbi.ufl.edu>
Message-ID: <Pine.LNX.4.44.0310201344140.1145-100000@gannet.stats>

One comment: the first two of those are not in S, but rather in S-PLUS.
So a big question is compatibility with what?  (There are quite large 
compatibility differences between versions of S-PLUS too.)

There are comments on differences in the FAQ, but they are not 
comprehensive.

On Mon, 20 Oct 2003, Purvis Bedenbaugh wrote:

> Hello all - I've just recently been exploring R for the
> first time. After noticing a few things that have changed
> from S to R, I started looking for an R-S compatibility table
> but didn't find it. Is such a table out there ? Where ?
> 
> Thanks much,
> 
> Purvis Bedenbaugh
> purvis at mbi.ufl.edu
> 
> Examples:
> 
> 'stdev' is now 'sd'  - is it exactly the same computation ?
> couldn't find a built-in for error.bar()
> syntax that is an error in R: param(thisframe,"b") <- value

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From ripley at stats.ox.ac.uk  Mon Oct 20 14:52:02 2003
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Mon, 20 Oct 2003 13:52:02 +0100 (BST)
Subject: [R] How to upgrade R
In-Reply-To: <3A822319EB35174CA3714066D590DCD50205CCE7@usrymx25.merck.com>
Message-ID: <Pine.LNX.4.44.0310201347370.1145-100000@gannet.stats>

On Mon, 20 Oct 2003, Liaw, Andy wrote:

> What I do is to separate packages that shipped with R separate from other
> contributed packages from CRAN, so that when I upgrade, I can wipe clean the
> old R and re-install while keeping all other packages in place.
> 
> What's not clear to me is a good way of keeping two versions of R
> simultaneously (for ease of transition).  Can anyone suggest a good strategy
> for doing that on *nix?

What I do is to have them installed in different dirs, and have links to 
the bin/R files in my ~/bin directory, called names like R1.7.1, Rpat, 
Rdev.  And I have 

gannet% cat ~/.Renviron
R_LIBS=/users/ripley/R/library

so all of them can see the contributed packages I have installed (which is 
fine except where those get to be version-specific when I normally install 
in the main library tree).

> > -----Original Message-----
> > From: michael watson (IAH-C) [mailto:michael.watson at bbsrc.ac.uk] 
> > Sent: Monday, October 20, 2003 5:49 AM
> > To: r-help
> > Subject: [R] How to upgrade R
> > 
> > 
> > Hi
> > 
> > I wondered what the standard procedure was for upgrading from 
> > one version of R to the next.  I currently have R 1.7.1 and 
> > want the latest release, R 1.8.0.  I am running SUSE linux 
> > 8.2.  The main thing is that I want to keep all of the 
> > libraries that I have installed for R 1.7.1 without having to 
> > re-install them.

That may not be a good idea.  At least those using saved images need to be 
reinstalled because of changes in the methods package, and the help 
translations have been improved.

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From v_bill_pikounis at merck.com  Mon Oct 20 14:57:49 2003
From: v_bill_pikounis at merck.com (Pikounis, Bill)
Date: Mon, 20 Oct 2003 08:57:49 -0400
Subject: [R] Re: win.metafiles in linux and R
Message-ID: <CFBD404F5E0C9547B4E10B7BDC3DFA2F0197D787@usrymx18.merck.com>

Dear all:

Professor Ripley commented:
> Note that libEMF's help page says
> 
>   It is also possible now to generate EMF files from PSTOEDIT 
> on POSIX 
>   systems. Therefore, if your graphics code only outputs 
> PostScript, you 
>   can now easily convert it to EMF.
> 
> but that is only true on Windows (unfortunately).
> 

Actually I have had some luck combining libEMF and pstoedit *on Linux* as
well; i.e., generate a postscript file thru R on Linux, and them apply
pstoedit to convert it to EMF (on Linux).  It is not perfect though, to be
sure; for instance I had to apply a rotation argument so that when I
transferred the resultant EMF file to Windows and then imported it into Word
it would be right-side up. Plus, I only tried some basic examples, like
plot(1:10) and a couple of snippets from example(xyplot). 

So yes, in theory a win.metafile() function is possible in R under Linux.
But as has been pointed out, (1) There are many more important things for
the R-core developers to concentrate on, and (2) all contributions are
welcome.  

Note for example, that at the libEMF site
(http://sourceforge.net/projects/libemf), the libEMF author has not done
anything with it for nearly 2 years, not even to incorporate 2 (gcc 3
related) patches that (see Bugs, and Patches) even I as a complete novice in
C could apply to get libEMF working with pstoedit
(http://www.pstoedit.net/pstoedit) under Linux Mandrake 9.1. 

The on-hold status of libEMF suggests to me that there has been no clamor
for the libEMF author to incorporate these simple patches, nor update the
library for widespread use.  

Thanks,
Bill

----------------------------------------
Bill Pikounis, Ph.D.

Biometrics Research Department
Merck Research Laboratories
PO Box 2000, MailDrop RY33-300  
126 E. Lincoln Avenue
Rahway, New Jersey 07065-0900
USA

Phone: 732 594 3913
Fax: 732 594 1565


> -----Original Message-----
> From: Prof Brian Ripley [mailto:ripley at stats.ox.ac.uk] 
> Sent: Monday, October 20, 2003 7:16 AM
> To: ucgamdo at ucl.ac.uk
> Cc: Ted.Harding at nessie.mcc.ac.uk; r-help at stat.math.ethz.ch
> Subject: Re: [R] Re: win.metafiles in linux and R
> 
> 
> On Mon, 20 Oct 2003 ucgamdo at ucl.ac.uk wrote:
> 
> > I posted a similar query some months ago. Some people 
> answered stating that
> > win.metafiles were not available for R in linux. However, I 
> was suggested
> > to use the package RSvgDevice, which allows transforming 
> any graphic device
> > into Scalable Vector Graphics format, this file can then be 
> opened with
> > sodipodi, and really impresive stuff can be achieved. I 
> also noticed that
> > OpenOffice for linux has an option to open win.metafiles, 
> so it seems that
> > they are in theory available for linux!
> 
> Code to *read* them is more widely available than to *write* them.
> Can sodipodi output wmf/emf?
> 
> Note that libEMF's help page says
> 
>   It is also possible now to generate EMF files from PSTOEDIT 
> on POSIX 
>   systems. Therefore, if your graphics code only outputs 
> PostScript, you 
>   can now easily convert it to EMF.
> 
> but that is only true on Windows (unfortunately).
> 
> -- 
> Brian D. Ripley,                  ripley at stats.ox.ac.uk
> Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
> University of Oxford,             Tel:  +44 1865 272861 (self)
> 1 South Parks Road,                     +44 1865 272866 (PA)
> Oxford OX1 3TG, UK                Fax:  +44 1865 272595
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
>



From tleong at sph.emory.edu  Mon Oct 20 15:08:26 2003
From: tleong at sph.emory.edu (Traci Lyn Leong)
Date: Mon, 20 Oct 2003 09:08:26 -0400
Subject: [R] calculating K-function (Kest) in spatstat
Message-ID: <1066655306.3f93de4a75dfa@www.sph.emory.edu>


Dear R-helpers,

After putting my somewhat sparse (21 points) data into a 'ppp' format, I am 
unable to calculate the K function using Kest.  I get the error "some 'x' not 
counted; maybe 'breaks' do not span range of 'x' ".  So I tried to calculate 
the K function using the swedishpines dataset. Of course Kest(swedishpines) 
works fine but when i break it down to Kest(ppp
(swedishpines$x,swedishpines$y)), I get the same spanning error.

Has anyone else encountered this problem or knows how to get around it?

Thank you.

Traci Leong



From tlumley at u.washington.edu  Mon Oct 20 15:56:56 2003
From: tlumley at u.washington.edu (Thomas Lumley)
Date: Mon, 20 Oct 2003 06:56:56 -0700 (PDT)
Subject: [R] win.metafile and Linux
In-Reply-To: <85oewce4m4.fsf@blindglobe.net>
References: <XFMail.031020030346.Ted.Harding@nessie.mcc.ac.uk>
	<85oewce4m4.fsf@blindglobe.net>
Message-ID: <Pine.A41.4.58.0310200655530.99736@homer04.u.washington.edu>

On Mon, 20 Oct 2003, A.J. Rossini wrote:
>
> Note that the same did hold true for PICT format on Macs.  Not sure if
> it still does, though.
>

It's been rendered less relevant on Macs by the use of PDF in OS X.

	-thomas



From tlumley at u.washington.edu  Mon Oct 20 16:03:42 2003
From: tlumley at u.washington.edu (Thomas Lumley)
Date: Mon, 20 Oct 2003 07:03:42 -0700 (PDT)
Subject: [R] dev.print in R 1.7.1
In-Reply-To: <Pine.LNX.4.44.0310201224410.1219-100000@gannet.stats>
References: <Pine.LNX.4.44.0310201224410.1219-100000@gannet.stats>
Message-ID: <Pine.A41.4.58.0310200659060.99736@homer04.u.washington.edu>

On Mon, 20 Oct 2003, Prof Brian Ripley wrote:

> I can't help you with 1.7.1: it is obselete.  But since you are apparently
> using Windows, there are menus in 1.8.0 to do what you want, as well as
> the command savePlot() (AFAIK there never was a function save.plot).
>

There was, back in prehistory.  Not this century, though.

	-thomas



From ferreol at crpgl.lu  Mon Oct 20 16:04:02 2003
From: ferreol at crpgl.lu (Martial Ferreol)
Date: Mon, 20 Oct 2003 16:04:02 +0200
Subject: [R] MRPP
Message-ID: <OFDC7FA5BC.DC50BD66-ONC1256DC5.004A3EBB-C1256DC5.004D4476@crpgl.lu>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20031020/533f9b86/attachment.pl

From Carlisle.Thacker at noaa.gov  Mon Oct 20 15:48:49 2003
From: Carlisle.Thacker at noaa.gov (W. C. Thacker)
Date: Mon, 20 Oct 2003 09:48:49 -0400
Subject: [R] Re: Oceanographic lattice plots?
References: <200310191002.h9JA1qq8010226@stat.math.ethz.ch>
Message-ID: <3F93E7C1.1D7DE9F1@noaa.gov>

Mike,

If depth is positive downward, you can simply plot its negative.  Fir
example, suppose you have a dataframe with ctd data:

> ctd[1,]
            id p      t      s 
104613 5993512 2 25.248 36.238 
> unique(ctd$id)
 [1] 5993512 3319148 3358648 3358538 3317809 3317562 3304786 3300555
3313863
[10] 3317901 3249908 3249987

If you are not fussy about the axis labels indicating negative values
of pressure, you can plot them like this:

xyplot(-p~t|factor(id),
  data=ctd,
  type="l",
  xlab="temperature (C)",
  ylab="pressure (dbar)")

If you want to get rid of the minus signs:

xyplot(-p~t|factor(id),
  data=ctd,type="l",
  xlab="temperature (C)",
  ylab="pressure (dbar)",
scales=list(y=list(at=seq(-3000,0,by=500),
               labels=format(-seq(-3000,0,by=500)))))

Hope this helps.

Carlisle
-- 

William Carlisle Thacker                            
                                                    
Atlantic Oceanographic and Meteorological Laboratory
4301 Rickenbacker Causeway, Miami, Florida 33149 USA
Office: (305) 361-4323           Fax: (305) 361-4392

"Too many have dispensed with generosity 
     in order to practice charity."     Albert Camus

> Subject: [R] Oceanographic lattice plots?
> Date: Sat, 18 Oct 2003 14:14:14 -0400
> From: "Mike Prager" <Mike.Prager at noaa.gov>
> To: R Help List <r-help at stat.math.ethz.ch>
> CC: Michael Prager <Mike.Prager at noaa.gov>
> 
> R 1.8.0 on Windows XP Professional.  A huge THANK YOU to the R Team for
> this marvelous software.
> 
> I am making lattice plots of oceanographic data.  The usual layout does not
> conform to plotting conventions that marine scientists use when depth is
> the independent variable.  Under those conventions, plots are made with the
> origin at the upper left, depth on the vertical axis (increasing as it goes
> down), and the dependent variable on the horizontal axis (increasing to the
> right).
> 
> That convention has implications not just in how axes are labeled and set
> up, but also when using smoothing routines such as panel.lowess(), because
> the smoothed values are on the horizontal axis, not the vertical axis.
> 
> Before I start looking at and modifying the R code that makes up the
> relevant routines, I wonder if any reader has already developed R routines
> for this purpose?
> 
> --
> Michael Prager, Ph.D.
> NOAA Beaufort Laboratory
> Beaufort, North Carolina  28516
> http://shrimp.ccfhrb.noaa.gov/~mprager/
> ***
> 
>   ----------------------------------------------------------------------
> 
> Subject: Re: [R] Oceanographic lattice plots?
> Date: Sat, 18 Oct 2003 13:29:14 -0500
> From: Deepayan Sarkar <deepayan at stat.wisc.edu>
> To: "Mike Prager" <Mike.Prager at noaa.gov>,
>      R Help List <r-help at stat.math.ethz.ch>
> CC: Michael Prager <Mike.Prager at noaa.gov>
> References: <6.0.0.22.0.20031018140413.01b88508 at hermes.nos.noaa.gov>
> 
> On Saturday 18 October 2003 13:14, Mike Prager wrote:
> > R 1.8.0 on Windows XP Professional.  A huge THANK YOU to the R Team for
> > this marvelous software.
> >
> > I am making lattice plots of oceanographic data.  The usual layout does not
> > conform to plotting conventions that marine scientists use when depth is
> > the independent variable.  Under those conventions, plots are made with the
> > origin at the upper left, depth on the vertical axis (increasing as it goes
> > down), and the dependent variable on the horizontal axis (increasing to the
> > right).
> 
> In case you decide to work on this yourself, this might be useful:
> 
> Ideally, specifications like
> 
> xyplot(depth ~ x, ylim = c(10, 0))
> 
> should reverse the y-axis direction. As pointed out some time back, this
> doesn't work in lattice currently, but that should be fixed in the future (it
> already works in my development version).
> 
> > That convention has implications not just in how axes are labeled and set
> > up, but also when using smoothing routines such as panel.lowess(), because
> > the smoothed values are on the horizontal axis, not the vertical axis.
> 
> These should be easy to modify.
> 
> Deepayan
> 
> > Before I start looking at and modifying the R code that makes up the
> > relevant routines, I wonder if any reader has already developed R routines
> > for this purpose?
> 
>   ----------------------------------------------------------------------



From maechler at stat.math.ethz.ch  Mon Oct 20 16:13:18 2003
From: maechler at stat.math.ethz.ch (Martin Maechler)
Date: Mon, 20 Oct 2003 16:13:18 +0200
Subject: [R] R - S compatibility table
In-Reply-To: <450B582867097B4E8B19AE7F947C3BEE59E4D0@mbi-00.mbi.ufl.edu>
References: <450B582867097B4E8B19AE7F947C3BEE59E4D0@mbi-00.mbi.ufl.edu>
Message-ID: <16275.60798.306794.636237@gargle.gargle.HOWL>

>>>>> "Purvis" == Purvis Bedenbaugh <purvis at mbi.ufl.edu>
>>>>>     on Mon, 20 Oct 2003 08:03:13 -0400 writes:

    Purvis> Hello all - I've just recently been exploring R for
    Purvis> the first time. After noticing a few things that
    Purvis> have changed from S to R, I started looking for an
    Purvis> R-S compatibility table but didn't find it. Is such
    Purvis> a table out there ? Where ?

    Purvis> Thanks much,

    Purvis> Purvis Bedenbaugh purvis at mbi.ufl.edu

    Purvis> Examples:

    Purvis> 'stdev' is now 'sd' 

"now" is good.  sd() has been in R for about 7 years, whereas
stdev() was introduced into S+ only for version 6.x.. in about 2000.

    Purvis>  - is it exactly the same computation ?  

no, it isn't (in more than one sense)...

    Purvis> couldn't find a built-in for error.bar() 

"built-in" is not a good notion in any version of S, but much
less in R : R comes with 28 (standard + recommended) packages of
which only a few are attached by default, and CRAN has over 200
contributed packages; Bioconductor has another several dozen,
and there are more outside the official repositories above.
Several of the CRAN packages have  error.bar  incantations..

    Purvis> syntax that is an error in R: param(thisframe,"b") <- value

It's not a syntax error;  just, there's no  "param<-" function
in the currently attached R packages (and not in other R
packages I seem to have around)...

Prof. Brian Ripley has given you more hints already.

-- 
Martin Maechler <maechler at stat.math.ethz.ch>	http://stat.ethz.ch/~maechler/
Seminar fuer Statistik, ETH-Zentrum  LEO C16	Leonhardstr. 27
ETH (Federal Inst. Technology)	8092 Zurich	SWITZERLAND
phone: x-41-1-632-3408		fax: ...-1228			<><



From TyagiAnupam at aol.com  Mon Oct 20 16:46:43 2003
From: TyagiAnupam at aol.com (TyagiAnupam@aol.com)
Date: Mon, 20 Oct 2003 10:46:43 EDT
Subject: [R] Processing logic for Huge Data set
Message-ID: <1a8.1b0c8b12.2cc54f53@aol.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20031020/551bc69f/attachment.pl

From KLee at igeotech.com  Mon Oct 20 17:12:49 2003
From: KLee at igeotech.com (Kuantsai Lee)
Date: Mon, 20 Oct 2003 08:12:49 -0700
Subject: [R] Bagplot
In-Reply-To: <16275.39757.867706.919738@gargle.gargle.HOWL>
References: <5.2.0.9.0.20031018180953.035489b8@mail.zoper.com>
	<5.2.0.9.0.20031018180953.035489b8@mail.zoper.com>
Message-ID: <5.2.0.9.0.20031020081049.00b3dc40@mail.zoper.com>


   At 10:22 AM 10/20/2003 +0200, Martin Maechler wrote:

     >>>>> "Kuantsai" == Kuantsai Lee <KLee at igeotech.com>
     >>>>>     on Sat, 18 Oct 2003 18:20:23 -0700 writes:
         Kuantsai> Has anyone ported the Bagplot function by
         Kuantsai> Rousseeuw, Ruts, and Tukey from S to R? The S
         Kuantsai> function comprises a script and a FORTRAN
         Kuantsai> function. I assume porting is relatively
         Kuantsai> uncomplicated, but since I have not done any
         Kuantsai> porting before I would not want to invest the
         Kuantsai> effort if a port is readily available.
     Yes, Christian Keller did a semi-port, which I had ``finished''
     long ago.
     When starting to experiment with it I found that it produced
     segfaults (in the Fortran code) semi-stochastically and I never
     found the time to figuring out why they happened.  I can have a
     look and provide
     my "bagplot" package as source package -- but not via CRAN as
     long the problems mentioned remain.
     Can you build R package from the sources?
     Regards,
     Martin Maechler
     <maechler at stat.math.ethz.ch>    [1]http://stat.ethz.ch/~maechler/
     Seminar fuer Statistik, ETH-Zentrum  LEO C16    Leonhardstr. 27
     ETH (Federal Inst. Technology)  8092 Zurich     SWITZERLAND
     phone: x-41-1-632-3408          fax: ...-1228                   <><

   I have not built R package from the sources before, but there is
   always a first in everything. Send me the source package and I will
   give it a try.

References

   1. http://stat.ethz.ch/~maechler/


From Ted.Harding at nessie.mcc.ac.uk  Mon Oct 20 17:15:46 2003
From: Ted.Harding at nessie.mcc.ac.uk ( (Ted Harding))
Date: Mon, 20 Oct 2003 16:15:46 +0100 (BST)
Subject: [R] interactive prompts - tcltk examples
In-Reply-To: <3F93D312.9000108@lancaster.ac.uk>
Message-ID: <XFMail.031020161546.Ted.Harding@nessie.mcc.ac.uk>

On 20-Oct-03 Barry Rowlingson wrote:
>   I've just been a collaborator on a grant application knocked back 
> partly because the funders thought non-technical users might not be
> able to use our R code, so I'm looking at UI issues a bit these days.
> Its either that or telling funding bodies that we'll be writing Excel
> macros....
> 
> Baz

Dear Baz,

I'm not clear about what you mean here. Are you proposing to lie,
or are you proposing to abandon hope?

:)
Ted.


--------------------------------------------------------------------
E-Mail: (Ted Harding) <Ted.Harding at nessie.mcc.ac.uk>
Fax-to-email: +44 (0)870 167 1972
Date: 20-Oct-03                                       Time: 16:15:46
------------------------------ XFMail ------------------------------



From Ted.Harding at nessie.mcc.ac.uk  Mon Oct 20 17:09:56 2003
From: Ted.Harding at nessie.mcc.ac.uk ( (Ted Harding))
Date: Mon, 20 Oct 2003 16:09:56 +0100 (BST)
Subject: [R] presentation of spatial-temporal point processes
In-Reply-To: <3F93D689.6070700@lancs.ac.uk>
Message-ID: <XFMail.031020160956.Ted.Harding@nessie.mcc.ac.uk>

On 20-Oct-03 Pingping Zheng wrote:
> Hello all,
> 
> Would anybody tell me how to present spatial-temoral point processes in
> R, for example, I'd like to plot the spatial points in the sequence of 
> their time domain?

Perhaps you need a "movie"? You can use different colours to show the
progression of values at the spatial points as well.

This should be possible without cruel and unusual effort using a 'for'
loop and 'plot' with axes pre-set. The main thing you need to tune
is the delay between frames, so that you create the desired impression.

(I've done this sort of thing, once on a CP/M machine to display the
progress of Johs. Schmidt's Atlantic eel investigations, once using
matlab on DOS to show earthquake patterns, with magnitudes, evolving
over time).

By the way, Postscript being a nice programming language, it is
possible to create a "movie" in a single PS file. The only thing I've
not found built-in to PS is a 'delay' or 'sleep' function.

In PDF, if you are able to include PDFmarks, you can be even fancier,
since there is a PDFmark which causes successive images to merge
smoothly into each other.

The ImageMagick suite of programs includes 'animate' which allows you
to display a "movie" based on a sequence of separate image files.

You might consider have each succesive image also containing "ghosts"
(with succesively increasing ghostliness) of the last few, say 2 or 3,
images.

Best wishes,
Ted.


--------------------------------------------------------------------
E-Mail: (Ted Harding) <Ted.Harding at nessie.mcc.ac.uk>
Fax-to-email: +44 (0)870 167 1972
Date: 20-Oct-03                                       Time: 16:09:56
------------------------------ XFMail ------------------------------



From mark.laubach at yale.edu  Mon Oct 20 17:24:24 2003
From: mark.laubach at yale.edu (Mark Laubach)
Date: Mon, 20 Oct 2003 11:24:24 -0400
Subject: [R] POSTDOCTORAL POSITION: REAL-TIME DECODING FOR BRAIN-MACHINE
 INTERFACES
Message-ID: <3F93FE28.4020803@yale.edu>

A postdoctoral position is available in January, 2003 in the
laboratory of Dr Mark Laubach at the John B Pierce Laboratory, a
non-profit research institute affiliated with Yale University. The
position is part of a collaboration with Drs Jon Kaas and Troy Hackett
and their colleagues at Vanderbilt University and is supported by
DARPA.

This individual will develop novel methods for on-line analyses of
neuronal ensemble data (e.g., adaptive methods for spike sorting and
decoding analyses of spike and field potential data) using a cluster
of Linux workstations. In addition, putative neuronal codes in
auditory, prefrontal, and motor areas of the cerebral cortex will be
investigated using neuronal ensemble recording and microstimulation
methods in collaboration with neurophysiologists in our group.

The position requires expertise in methods for statistical pattern
recognition (e.g.,  random forest, SVMs), functional data analysis,
and scientific computing under Linux. Our lab makes heavy use of
Matlab, R, and Python, so knowledge of these tools is also necessary.

Interested individuals should submit a letter of application, CV,
copies of publications, and the names of three references to:

Mark Laubach, PhD
The John B. Pierce Laboratory Inc.
290 Congress Avenue
New Haven, CT 06519
http://spikelab.jbpierce.org

Inquires: laubach at jbpierce.org

The review of applications will begin November 1, 2003, and will
continue until the position is filled.

E.O.E.



From B.Rowlingson at lancaster.ac.uk  Mon Oct 20 17:32:27 2003
From: B.Rowlingson at lancaster.ac.uk (Barry Rowlingson)
Date: Mon, 20 Oct 2003 16:32:27 +0100
Subject: [R] presentation of spatial-temporal point processes
In-Reply-To: <XFMail.031020160956.Ted.Harding@nessie.mcc.ac.uk>
References: <XFMail.031020160956.Ted.Harding@nessie.mcc.ac.uk>
Message-ID: <3F94000B.3080004@lancaster.ac.uk>

(Ted Harding) wrote:

> Perhaps you need a "movie"? You can use different colours to show the
> progression of values at the spatial points as well.

Something I've used in the past to visualise space-time datasets is 
Xgobi (or Ggobi). Feed it a dataframe of x, y, t, numbers then set up 
two plots:

  * an x-y plot - the "map"
  * an x-t or y-t plot - it doesn't matter as long as 't' is on one axis.

  Now make a long, thin brush so that when placed on the x-t plot it 
highlights a small amount of t-space but all the x-space. This will then 
select all the points in a small time-space.

  With linked plots, you can either set transient highlighting and see 
points come-and-go in the map as you drag the brush on the time-plot, or 
persistent highlighting and then start at t=0 and drag up to t=tmax, 
showing the birth of points in time building up the final pattern.

Baz

[pingping: I'll install this on your machine and give you a demo if you 
want]



From pavlicov at stat.ohio-state.edu  Mon Oct 20 17:51:44 2003
From: pavlicov at stat.ohio-state.edu (Martina Pavlicova)
Date: Mon, 20 Oct 2003 11:51:44 -0400 (EDT)
Subject: [R] Lilliefors Test
In-Reply-To: <6000BB14AFA9A741BC2315A598837ED5693A3E@nihexchange4.nih.gov>
References: <6000BB14AFA9A741BC2315A598837ED5693A3E@nihexchange4.nih.gov>
Message-ID: <Pine.GSO.4.58.0310201147480.29767@spatial.stat.ohio-state.edu>


> Is it not true that the Shapiro Wilks test implemented in the package
> ctest requires the assumption that the population variance of the
> variable is known?

I am not sure about this one...

> Is it also not true that the Lilliefors is not a test of normality
> as such, but is rather a correction of the p-value for the
> Kolmogorov Smirnov test?

Yes, that is right. Here is the code:

null.hyp <- function(x) {
	## one observation from the null distribution
  mu <- mean(x)
  sigma <- sd(x)
  foo <- ks.test(x=x, y="pnorm", mean = mu, sd = sigma)
  sim <- foo$statistic
  sim
}

create.null <- function(n=n, rep=100000){
	## creates a sample of length 'rep' from null distribution
  null <- numeric(rep)
  for(i in 1:rep){
    null[i] <- null.hyp(rnorm(n))
  }
  null
}

Lilliefors.test <- function(sample=data, rep){
  n <- length(sample)
  null.dist <- create.null(n, rep)
  res <- list()
  res$statistic <- null.hyp(sample)
  res$p.value <- sum(null.dist>res$statistic)/rep
  res
}


Thanks,

Martina Pavlicova

--------------------------------------------------------------------------
Department of Statistics             Office Phone: (614) 292-1567
1958 Neil Avenue, 304E Cockins Hall  FAX: (614) 292-2096
The Ohio State University            E-mail: pavlicov at stat.ohio-state.edu
Columbus, OH 43210-1247              www.stat.ohio-state.edu/~pavlicov

> | -----Original Message-----
> | From: kjetil at entelnet.bo [mailto:kjetil at entelnet.bo]
> | Sent: Friday, October 17, 2003 3:33 PM
> | To: R HELP; Martina Pavlicova
> | Subject: Re: [R] Lilliefors Test
> |
> |
> | On 17 Oct 2003 at 13:59, Martina Pavlicova wrote:
> |
> | There is shapiro.test in package ctest, which have much better power
> | properties than Lillefors test. So there is no need to have
> | Lilliefors test in R, except for archeological interest.
> |
> | Kjetil Halvorsen
> |
> | >
> | > Hello everybody,
> | >
> | > I would like to perform a test for normality (without specifying the
> | > mean a variance) on the sample data (80 observations). I found that
> | > Lilliefors test is appropriate. Does anybody have it
> | programmed already,
> | > or is there a function for this test in R?
> | >
> | > Thank you very much,
> | >
> | > Martina Pavlicova
> | >



From HBaize at buttecounty.net  Mon Oct 20 17:57:28 2003
From: HBaize at buttecounty.net (Baize, Harold)
Date: Mon, 20 Oct 2003 08:57:28 -0700
Subject: [R] Request for introductory document
Message-ID: <7B33963AB700D711A107000802A38DC26BDF3F@bcismailchico.buttecounty.net>


After 25 years of using SPSS syntax 
I am having some problems adapting to 
R. It would be a huge help for people 
like myself to have an introductory 
help document, like "R for SPSS Users". 
Such a document would jump start SPSS users 
by covering the use of the foreign function 
to read ".sav" files, then show equivalent R 
syntax for the most common SPSS procedures. 

Once I achieve a degree of proficiency with 
R I will attempt to draft the SPSS to R 
help document, but maybe someone out there 
is already able to make it.  

Much of my difficulty as an R novice is 
confusion over data types required by 
functions. Some accept vectors, others 
matrices or data.frames. An SPSS to R 
help document would spell out where the 
".sav" data files, read in as data.frames, 
would need to extract vectors or define 
matrices to apply common functions such 
as aov, t.test, and glm. 

One of the first things I wanted to do 
with R was create bar charts with 95% CI. 
I was very pleased to find barplot2 (in 
gregmisc). Unfortunately I was completely 
stymied when the example used faked CIs! 
The SPSS to R document would walk users 
through such a task, assuming a data.frame 
read in from a ".sav" file.  

A big thank you to all who contribute 
to this great software.

Harold Baize
Butte County Department of Behavior Health 
Youth Services



From mailinglist2_wegmann at web.de  Mon Oct 20 18:31:09 2003
From: mailinglist2_wegmann at web.de (Martin Wegmann)
Date: Mon, 20 Oct 2003 18:31:09 +0200
Subject: [R] R 1.8 for debian
Message-ID: <200310201831.09324.mailinglist2_wegmann@web.de>

Hello, 

I start running into problems with 1.7. due to packages which are only 
supported by 1.8., therefore I tried to get 1.8. but synaptic only shows up 
with 1.7.. Are there already debian 1.8 packages?

Another question would be, how I can keep all my previously installed 
packages. 
will they be kept in '/usr/local/lib/R/site-library or does the new R 
overwrite these addional packages?

cheers Martin



From Rau at demogr.mpg.de  Mon Oct 20 18:58:03 2003
From: Rau at demogr.mpg.de (Rau, Roland)
Date: Mon, 20 Oct 2003 18:58:03 +0200
Subject: [R] R 1.8 for debian
Message-ID: <3699CDBC4ED5D511BE6400306E1C0D81030A0414@hermes.demogr.mpg.de>

Hi Martin,

> Another question would be, how I can keep all my previously installed 
> packages. 
> will they be kept in '/usr/local/lib/R/site-library or does the new R 
> overwrite these addional packages?
> 
> cheers Martin
> 
	don't know about your first question. But your second question has
been already adressed and answered today on this list. Maybe you should read
today's previous messages.

	Best,
	Roland



+++++
This mail has been sent through the MPI for Demographic Research.  Should you receive a mail that is apparently from a MPI user without this text displayed, then the address has most likely been faked.   If you are uncertain about the validity of this message, please check the mail header or ask your system administrator for assistance.



From edd at debian.org  Mon Oct 20 19:23:27 2003
From: edd at debian.org (Dirk Eddelbuettel)
Date: Mon, 20 Oct 2003 12:23:27 -0500
Subject: [R] R 1.8 for debian
In-Reply-To: <200310201831.09324.mailinglist2_wegmann@web.de>
References: <200310201831.09324.mailinglist2_wegmann@web.de>
Message-ID: <20031020172327.GA7795@sonny.eddelbuettel.com>

On Mon, Oct 20, 2003 at 06:31:09PM +0200, Martin Wegmann wrote:
> I start running into problems with 1.7. due to packages which are only 
> supported by 1.8., therefore I tried to get 1.8. but synaptic only shows up 
> with 1.7.. Are there already debian 1.8 packages?

Sure, got uploaded the day of the 1.8 release. You may want to learn how to
point apt at different archives. Debian has 1.8 in unstable; CRAN has it in
testing (thanks to Doug Bates) and even in stable (thanks to Korbinian
Strimmer)

This general question about where to get Debian packages is also answered in
the FAQ with the relevant info about how to connect apt to the CRAN archives.

Dirk

-- 
Those are my principles, and if you don't like them... well, I have others.
                                                -- Groucho Marx



From john.gavin at ubs.com  Mon Oct 20 19:35:43 2003
From: john.gavin at ubs.com (john.gavin@ubs.com)
Date: Mon, 20 Oct 2003 18:35:43 +0100
Subject: [R] controling x-labels in xyplot (lattice) when x is POSIX object
Message-ID: <012821F286ED1D4ABDC72F9E1DD63D0C01FF963F@NLDNC003PEX1.ubsgs.ubsgroup.net>

Hi,

V1.8.0 seems to allow DateTimeClasses as the x argument in xyplots (lattice).
For example:

x <- seq.POSIXt(strptime("2003/01/01", format = "%Y/%m/%d"),
                strptime("2003/10/01", format = "%Y/%m/%d"), by = "month")
y <- rnorm(length(x))
dat <- data.frame(x= x, y = y)
xyplot(y ~ x, data = dat, type = "b")

However, the labelling for the x-axis is not what I want.
(I see only one tick mark and one label ('Oct').)
What is the recommended way to relabel the x-axis?
Ideally, I want to see several months (3-6) labelled along the x-axis.

Previously, I used 'calculateAxisComponents' to massage the labels manually
but that function (which I realise was internal to lattice) is no longer available.

I am on Windows XP, R 1.8.0.

Regards,

John.

John Gavin <john.gavin at ubs.com>,
Quantitative Risk Models and Statistics,
UBS Investment Bank, 6th floor, 
100 Liverpool St., London EC2M 2RH, UK.
Phone +44 (0) 207 567 4289
Fax   +44 (0) 207 568 5352

Visit our website at http://www.ubs.com

This message contains confidential information and is intend...{{dropped}}



From laura at env.leeds.ac.uk  Mon Oct 20 19:41:29 2003
From: laura at env.leeds.ac.uk (Laura Quinn)
Date: Mon, 20 Oct 2003 18:41:29 +0100 (BST)
Subject: [R] selecting subsets of data from matrix
Message-ID: <Pine.LNX.4.44.0310201837060.23087-100000@env-pc-phd13>

Probably a stupid question, but I don't seem to be able to find the answer
I'm looking for from any of the R literature. Basically I have a matrix
with several thousand rows and 20 columns(weather stations) of wind
direction data.

I am wanting to extract a matrix which contains data for all columns
conditional on column 20 having a value of _either_ less than 45 or
greater than 315. (ie I want to extract a matrix which contains wind direction
data for all columns {weather stations} when there is a prevailing
northerly wind for one of the stations).

I have tried a few different methods of doing this, none with any success,
can anyone please advise?

Thanks in advance!



From bolker at zoo.ufl.edu  Mon Oct 20 20:05:34 2003
From: bolker at zoo.ufl.edu (Ben Bolker)
Date: Mon, 20 Oct 2003 14:05:34 -0400 (EDT)
Subject: [R] selecting subsets of data from matrix
In-Reply-To: <Pine.LNX.4.44.0310201837060.23087-100000@env-pc-phd13>
Message-ID: <Pine.LNX.4.44.0310201405010.4553-100000@bolker.zoo.ufl.edu>


 x[x[,20]<45 | x[,20]>315, ]

On Mon, 20 Oct 2003, Laura Quinn wrote:

> Probably a stupid question, but I don't seem to be able to find the answer
> I'm looking for from any of the R literature. Basically I have a matrix
> with several thousand rows and 20 columns(weather stations) of wind
> direction data.
> 
> I am wanting to extract a matrix which contains data for all columns
> conditional on column 20 having a value of _either_ less than 45 or
> greater than 315. (ie I want to extract a matrix which contains wind direction
> data for all columns {weather stations} when there is a prevailing
> northerly wind for one of the stations).
> 
> I have tried a few different methods of doing this, none with any success,
> can anyone please advise?
> 
> Thanks in advance!
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> 

-- 
620B Bartram Hall                            bolker at zoo.ufl.edu
Zoology Department, University of Florida    http://www.zoo.ufl.edu/bolker
Box 118525                                   (ph)  352-392-5697
Gainesville, FL 32611-8525                   (fax) 352-392-3704



From p.dalgaard at biostat.ku.dk  Mon Oct 20 20:01:04 2003
From: p.dalgaard at biostat.ku.dk (Peter Dalgaard)
Date: 20 Oct 2003 20:01:04 +0200
Subject: [R] Lilliefors Test
In-Reply-To: <Pine.GSO.4.58.0310201147480.29767@spatial.stat.ohio-state.edu>
References: <6000BB14AFA9A741BC2315A598837ED5693A3E@nihexchange4.nih.gov>
	<Pine.GSO.4.58.0310201147480.29767@spatial.stat.ohio-state.edu>
Message-ID: <x2ad7vss9r.fsf@biostat.ku.dk>

Martina Pavlicova <pavlicov at stat.ohio-state.edu> writes:

> > Is it not true that the Shapiro Wilks test implemented in the package
> > ctest requires the assumption that the population variance of the
> > variable is known?
> 
> I am not sure about this one...

No, that is false. Shapiro-Wilks basically measures the correlation in
a qqnorm() plot (too far below 1 and the distribution is deemed
non-normal) which is a  scale- and position-invariant measure.

-- 
   O__  ---- Peter Dalgaard             Blegdamsvej 3  
  c/ /'_ --- Dept. of Biostatistics     2200 Cph. N   
 (*) \(*) -- University of Copenhagen   Denmark      Ph: (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)             FAX: (+45) 35327907



From deepayan at stat.wisc.edu  Mon Oct 20 19:58:36 2003
From: deepayan at stat.wisc.edu (Deepayan Sarkar)
Date: Mon, 20 Oct 2003 12:58:36 -0500
Subject: [R] controling x-labels in xyplot (lattice) when x is POSIX object
In-Reply-To: <012821F286ED1D4ABDC72F9E1DD63D0C01FF963F@NLDNC003PEX1.ubsgs.ubsgroup.net>
References: <012821F286ED1D4ABDC72F9E1DD63D0C01FF963F@NLDNC003PEX1.ubsgs.ubsgroup.net>
Message-ID: <200310201258.36327.deepayan@stat.wisc.edu>

On Monday 20 October 2003 12:35, john.gavin at ubs.com wrote:
> Hi,
> 
> V1.8.0 seems to allow DateTimeClasses as the x argument in xyplots 
(lattice).
> For example:
> 
> x <- seq.POSIXt(strptime("2003/01/01", format = "%Y/%m/%d"),
>                 strptime("2003/10/01", format = "%Y/%m/%d"), by = "month")
> y <- rnorm(length(x))
> dat <- data.frame(x= x, y = y)
> xyplot(y ~ x, data = dat, type = "b")
> 
> However, the labelling for the x-axis is not what I want.
> (I see only one tick mark and one label ('Oct').)
> What is the recommended way to relabel the x-axis?
> Ideally, I want to see several months (3-6) labelled along the x-axis.

A final resort could always be to explicitly specify the at and labels 
components in scales.

> Previously, I used 'calculateAxisComponents' to massage the labels manually
> but that function (which I realise was internal to lattice) is no longer 
> available.

It's still there, but not exported (in the NAMESPACE sense). You will find it 
in the source, and perhaps be able to use it to calculate your own 'at' and 
'labels' externally.

The DateTime handling you see in 1.8.0 was actually present in earlier 
versions of lattice, only it didn't work when relation="same" (the default) 
because of a bug. In a (hopefully near) future version the axis labels 
handling will be restructured, and in particular will allow the user to 
specify the format (as in axis.POSIXct) explicitly in the high level xyplot 
call, which might make this slightly more flexible.

HTH,

Deepayan



From cmoffet at nwrc.ars.usda.gov  Mon Oct 20 19:59:13 2003
From: cmoffet at nwrc.ars.usda.gov (Corey Moffet)
Date: Mon, 20 Oct 2003 11:59:13 -0600
Subject: [R] selecting subsets of data from matrix
In-Reply-To: <Pine.LNX.4.44.0310201837060.23087-100000@env-pc-phd13>
Message-ID: <3.0.6.32.20031020115913.00f8d2e8@nwrc.ars.usda.gov>

try:

mat[(mat[,20] > 315 | mat[,20] < 45),]


At 06:41 PM 10/20/2003 +0100, Laura Quinn wrote:
>Probably a stupid question, but I don't seem to be able to find the answer
>I'm looking for from any of the R literature. Basically I have a matrix
>with several thousand rows and 20 columns(weather stations) of wind
>direction data.
>
>I am wanting to extract a matrix which contains data for all columns
>conditional on column 20 having a value of _either_ less than 45 or
>greater than 315. (ie I want to extract a matrix which contains wind
direction
>data for all columns {weather stations} when there is a prevailing
>northerly wind for one of the stations).
>
>I have tried a few different methods of doing this, none with any success,
>can anyone please advise?
>
>Thanks in advance!
>
>______________________________________________
>R-help at stat.math.ethz.ch mailing list
>https://www.stat.math.ethz.ch/mailman/listinfo/r-help
>
With best wishes and kind regards I am

Sincerely,

Corey A. Moffet, Ph.D.
Support Scientist

University of Idaho
Northwest Watershed Research Center
800 Park Blvd, Plaza IV, Suite 105
Boise, ID 83712-7716

Voice: (208) 422-0718
FAX:   (208) 334-1502



From bates at stat.wisc.edu  Mon Oct 20 20:00:49 2003
From: bates at stat.wisc.edu (Douglas Bates)
Date: 20 Oct 2003 13:00:49 -0500
Subject: [R] selecting subsets of data from matrix
In-Reply-To: <Pine.LNX.4.44.0310201837060.23087-100000@env-pc-phd13>
References: <Pine.LNX.4.44.0310201837060.23087-100000@env-pc-phd13>
Message-ID: <6rptgrdc1a.fsf@bates4.stat.wisc.edu>

Laura Quinn <laura at env.leeds.ac.uk> writes:

> Probably a stupid question, but I don't seem to be able to find the answer
> I'm looking for from any of the R literature. Basically I have a matrix
> with several thousand rows and 20 columns(weather stations) of wind
> direction data.

Is it a matrix or a data frame?

> I am wanting to extract a matrix which contains data for all columns
> conditional on column 20 having a value of _either_ less than 45 or
> greater than 315. (ie I want to extract a matrix which contains wind
> direction data for all columns {weather stations} when there is a
> prevailing northerly wind for one of the stations).

> I have tried a few different methods of doing this, none with any success,
> can anyone please advise?

If x is a matrix they you want

  mysubset <- x[x[,20] < 45 | x[,20] > 315,]

If x is a data frame and column 20 is named col20 then you can use

  mysubset <- subset(x, col20 < 45 | col20 >315)



From laura at env.leeds.ac.uk  Mon Oct 20 20:06:03 2003
From: laura at env.leeds.ac.uk (Laura Quinn)
Date: Mon, 20 Oct 2003 19:06:03 +0100 (BST)
Subject: [R] selecting subsets of data from matrix
In-Reply-To: <6rptgrdc1a.fsf@bates4.stat.wisc.edu>
Message-ID: <Pine.LNX.4.44.0310201903220.23087-100000@env-pc-phd13>

Thanks for your help everyone,

My data is a matrix. However if i use the command:

x[(x[,20] > 315 | x[,20] < 45), ]

and then request a summary, I get a warning message saying that a large
number of the row names have been duplicated - I don't understand this?

On 20 Oct 2003, Douglas Bates wrote:

> Laura Quinn <laura at env.leeds.ac.uk> writes:
>
> > Probably a stupid question, but I don't seem to be able to find the answer
> > I'm looking for from any of the R literature. Basically I have a matrix
> > with several thousand rows and 20 columns(weather stations) of wind
> > direction data.
>
> Is it a matrix or a data frame?
>
> > I am wanting to extract a matrix which contains data for all columns
> > conditional on column 20 having a value of _either_ less than 45 or
> > greater than 315. (ie I want to extract a matrix which contains wind
> > direction data for all columns {weather stations} when there is a
> > prevailing northerly wind for one of the stations).
>
> > I have tried a few different methods of doing this, none with any success,
> > can anyone please advise?
>
> If x is a matrix they you want
>
>   mysubset <- x[x[,20] < 45 | x[,20] > 315,]
>
> If x is a data frame and column 20 is named col20 then you can use
>
>   mysubset <- subset(x, col20 < 45 | col20 >315)
>



From laurent at cbs.dtu.dk  Mon Oct 20 20:10:23 2003
From: laurent at cbs.dtu.dk (Laurent Gautier)
Date: Mon, 20 Oct 2003 20:10:23 +0200
Subject: [R] 'optim' and extra argument to the objective function
Message-ID: <20031020181023.GB4080718@genome.cbs.dtu.dk>


Hello,


I'd like to use optim, and give extra arguments to the objective
function. The man page says that the '...' should let one do it,
but I have a hard time to understand how.

Example:

x <- 1:10
y <- rnorm(10)
cost.f <- function(par, x, y) {
 A <- par[1]
 cost <- sum( (log(A*x) - log(y))^2)
 return(cost)
}

optim(3, cost.f, x, y)
## returns:
Error in pmatch(x, table, duplicates.ok) : 
	argument is not of mode character
## uh, uh... may the problem is with the argument matching ?
optim(3, cost.f, method="BFGS", x, y)
## returns:
Error in log(A * x) : Argument "x" is missing, with no default
In addition: Warning message: 
bounds can only be used with method L-BFGS-B in: optim(3, cost.f, method = "BFGS", x, y) 

Any suggestion ?


L.


PS: this was done with both R-1.7.1-patched and R-1.8.0-patched for Linux.



From info at lepetitmasdile.com  Mon Oct 20 20:09:17 2003
From: info at lepetitmasdile.com (Le Petit Mas d`Ile)
Date: Mon, 20 Oct 2003 20:09:17 +0200
Subject: [R] Search for a text string and write position related data to a
Message-ID: <000c01c39735$47f2a110$75190950@Peter>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20031020/4d04e2f0/attachment.pl

From kjetil at entelnet.bo  Mon Oct 20 20:17:42 2003
From: kjetil at entelnet.bo (kjetil@entelnet.bo)
Date: Mon, 20 Oct 2003 14:17:42 -0400
Subject: [R] Lilliefors Test
In-Reply-To: <6000BB14AFA9A741BC2315A598837ED5693A3E@nihexchange4.nih.gov>
Message-ID: <3F93EE86.5363.19B934@localhost>

On 20 Oct 2003 at 7:35, Haynes, Maurice (NIH/NICHD) wrote:

> Is it not true that the Shapiro Wilks test implemented in the package
> ctest requires the assumption that the population variance of the
> variable is known?

AFAIK, there is not. And there is no such argument to shapiro.test, 
so if this was a requirement for the correctness of shapiro.test, the 
implementation in ctest is wrong, which I doubt. Furthermore, from my 
limited reading about the Shapiro-Wilk test there is no reason to 
beleave this is a requirement. 


> 
> Is it also not true that the Lilliefors is not a test of normality
> as such, but is rather a correction of the p-value for the
> Kolmogorov Smirnov test?

Well, yes, but it is still a (bad) test of normality, 
isnt it?

Kjetil Halvorsen

> 
> Thanks,
> 
> Maurice Haynes
> National Institute of Child Health and Human Development
> Child and Family Research Section
> 6705 Rockledge Drive
> Bethesda, MD  20892
> Voice: 301-496-8180
> Fax: 301-496-2766
> E-Mail: mh192j at nih.gov
> 
> 
> 
> | -----Original Message-----
> | From: kjetil at entelnet.bo [mailto:kjetil at entelnet.bo]
> | Sent: Friday, October 17, 2003 3:33 PM
> | To: R HELP; Martina Pavlicova
> | Subject: Re: [R] Lilliefors Test
> | 
> | 
> | On 17 Oct 2003 at 13:59, Martina Pavlicova wrote:
> | 
> | There is shapiro.test in package ctest, which have much better power 
> | properties than Lillefors test. So there is no need to have 
> | Lilliefors test in R, except for archeological interest. 
> | 
> | Kjetil Halvorsen
> | 
> | > 
> | > Hello everybody,
> | > 
> | > I would like to perform a test for normality (without specifying the
> | > mean a variance) on the sample data (80 observations). I found that
> | > Lilliefors test is appropriate. Does anybody have it 
> | programmed already,
> | > or is there a function for this test in R?
> | > 
> | > Thank you very much,
> | > 
> | > Martina Pavlicova
> | > 
> | --------------------------------------------------------------
> | ------------
> | > Department of Statistics             Office Phone: (614) 292-1567
> | > 1958 Neil Avenue, 304E Cockins Hall  FAX: (614) 292-2096
> | > The Ohio State University            E-mail: 
> | pavlicov at stat.ohio-state.edu
> | > Columbus, OH 43210-1247              
> | www.stat.ohio-state.edu/~pavlicov
> | > 
> | > ______________________________________________
> | > R-help at stat.math.ethz.ch mailing list
> | > https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> | 
> | ______________________________________________
> | R-help at stat.math.ethz.ch mailing list
> | https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> |



From Ted.Harding at nessie.mcc.ac.uk  Mon Oct 20 20:22:22 2003
From: Ted.Harding at nessie.mcc.ac.uk ( (Ted Harding))
Date: Mon, 20 Oct 2003 19:22:22 +0100 (BST)
Subject: [R] selecting subsets of data from matrix
In-Reply-To: <Pine.LNX.4.44.0310201837060.23087-100000@env-pc-phd13>
Message-ID: <XFMail.031020192018.Ted.Harding@nessie.mcc.ac.uk>

On 20-Oct-03 Laura Quinn wrote:
> Probably a stupid question, but I don't seem to be able to find the
> answer I'm looking for from any of the R literature. Basically I have
> a matrix with several thousand rows and 20 columns(weather stations)
> of wind direction data.
> 
> I am wanting to extract a matrix which contains data for all columns
> conditional on column 20 having a value of _either_ less than 45 or
> greater than 315. (ie I want to extract a matrix which contains wind
> direction data for all columns {weather stations} when there is a
> prevailing northerly wind for one of the stations).
> 
> I have tried a few different methods of doing this, none with any
> success,
> can anyone please advise?

I'd normally do this kind of thing by setting up an index variable.
Say your matrix is Winds.

  iDir <- (Winds[,20]<45)|(Winds[,20]>315)
  I.want <- Winds[iDir,]

This gives you every row of Winds for which the element in col 20
satisfies the condition.

Of course you can do it in one line with

  I.want <- Winds[(Winds[,20]<45)|(Winds[,20]>315),]

but the advantage of the other is that iDir is there, and much easier to
type, if you need it later.

By the way, if it should happen that there are missing values (coded
as NA), then you may get unwanted results unless you deal with the NAs
explicitly:

  iDir <- ((Winds[,20]<45)|(Winds[,20]>315))&(!is.na(Winds[,20]))

This excludes cases of NA in col 20 (i.e. you only get those cases
where it is definitely known that col20 < 45 or col20 > 315). E.g.:

  x<-(1:20);x[c(3,7,11,15,19)]<-NA; x
    [1]  1  2 NA  4  5  6 NA  8  9 10 NA 12 13 14 NA 16 17 18 NA 20
  x[x>15]
    [1] NA NA NA NA 16 17 18 NA 20

(Not only do you get the x-values satisfying the condition; you get
all the NAs as well).

  x[x>15&(!is.na(x))]
    [1] 16 17 18 20

Ted.



From rxg218 at psu.edu  Mon Oct 20 20:36:44 2003
From: rxg218 at psu.edu (Rajarshi Guha)
Date: 20 Oct 2003 14:36:44 -0400
Subject: [R] changing the the column header in a data.frame
Message-ID: <1066675004.5560.0.camel@ra.chem.psu.edu>

Hi,
  is there any way I can change the column header in a data.frame?

Thanks,

-------------------------------------------------------------------
Rajarshi Guha <rxg218 at psu.edu> <http://jijo.cjb.net>
GPG Fingerprint: 0CCA 8EE2 2EEB 25E2 AB04 06F7 1BB9 E634 9B87 56EE
-------------------------------------------------------------------
Eureka!
-- Archimedes



From spencer.graves at pdf.com  Mon Oct 20 20:48:03 2003
From: spencer.graves at pdf.com (Spencer Graves)
Date: Mon, 20 Oct 2003 11:48:03 -0700
Subject: [R] 'optim' and extra argument to the objective function
In-Reply-To: <20031020181023.GB4080718@genome.cbs.dtu.dk>
References: <20031020181023.GB4080718@genome.cbs.dtu.dk>
Message-ID: <3F942DE3.6040903@pdf.com>

Have you tried the following: 

optim(3, cost.f, x=x, y=y)

      In the form given below, R interprets you call as equivalent to 
the following: 

optim(par=3, fn=cost.f, gr=x, method=y)

      Arguments will get passed via "..." when they have names that are 
different from names in the official argument list. 

hope this helps.  spencer graves

Laurent Gautier wrote:

>Hello,
>
>
>I'd like to use optim, and give extra arguments to the objective
>function. The man page says that the '...' should let one do it,
>but I have a hard time to understand how.
>
>Example:
>
>x <- 1:10
>y <- rnorm(10)
>cost.f <- function(par, x, y) {
> A <- par[1]
> cost <- sum( (log(A*x) - log(y))^2)
> return(cost)
>}
>
>optim(3, cost.f, x, y)
>## returns:
>Error in pmatch(x, table, duplicates.ok) : 
>	argument is not of mode character
>## uh, uh... may the problem is with the argument matching ?
>optim(3, cost.f, method="BFGS", x, y)
>## returns:
>Error in log(A * x) : Argument "x" is missing, with no default
>In addition: Warning message: 
>bounds can only be used with method L-BFGS-B in: optim(3, cost.f, method = "BFGS", x, y) 
>
>Any suggestion ?
>
>
>L.
>
>
>PS: this was done with both R-1.7.1-patched and R-1.8.0-patched for Linux.
>
>______________________________________________
>R-help at stat.math.ethz.ch mailing list
>https://www.stat.math.ethz.ch/mailman/listinfo/r-help
>  
>



From jmacdon at med.umich.edu  Mon Oct 20 21:03:12 2003
From: jmacdon at med.umich.edu (James MacDonald)
Date: Mon, 20 Oct 2003 15:03:12 -0400
Subject: [R] changing the the column header in a data.frame
Message-ID: <sf93f949.037@med-gwia-02a.med.umich.edu>

If you data.frame is called df, then

names(df) <- c(new names here)

You can also do individual names

names(df)[1] <- "new name for first column"

HTH

Jim



James W. MacDonald
Affymetrix and cDNA Microarray Core
University of Michigan Cancer Center
1500 E. Medical Center Drive
7410 CCGC
Ann Arbor MI 48109
734-647-5623

>>> Rajarshi Guha <rxg218 at psu.edu> 10/20/03 02:36PM >>>
Hi,
  is there any way I can change the column header in a data.frame?

Thanks,

-------------------------------------------------------------------
Rajarshi Guha <rxg218 at psu.edu> <http://jijo.cjb.net>
GPG Fingerprint: 0CCA 8EE2 2EEB 25E2 AB04 06F7 1BB9 E634 9B87 56EE
-------------------------------------------------------------------
Eureka!
-- Archimedes

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://www.stat.math.ethz.ch/mailman/listinfo/r-help



From tlumley at u.washington.edu  Mon Oct 20 20:53:13 2003
From: tlumley at u.washington.edu (Thomas Lumley)
Date: Mon, 20 Oct 2003 11:53:13 -0700 (PDT)
Subject: [R] 'optim' and extra argument to the objective function
In-Reply-To: <20031020181023.GB4080718@genome.cbs.dtu.dk>
References: <20031020181023.GB4080718@genome.cbs.dtu.dk>
Message-ID: <Pine.A41.4.58.0310201152000.84428@homer10.u.washington.edu>

On Mon, 20 Oct 2003, Laurent Gautier wrote:

>
> Hello,
>
>
> I'd like to use optim, and give extra arguments to the objective
> function. The man page says that the '...' should let one do it,
> but I have a hard time to understand how.
>
> Example:
>
> x <- 1:10
> y <- rnorm(10)
> cost.f <- function(par, x, y) {
>  A <- par[1]
>  cost <- sum( (log(A*x) - log(y))^2)
>  return(cost)
> }
>
> optim(3, cost.f, x, y)


You need
  optim(3, cost.f, x=x, y=y)

As always, if you don't specify a tag for an argument it gets matched
positionally, in this case to the gr and method arguments of optim().

	-thomas



From TyagiAnupam at aol.com  Mon Oct 20 21:02:23 2003
From: TyagiAnupam at aol.com (TyagiAnupam@aol.com)
Date: Mon, 20 Oct 2003 15:02:23 EDT
Subject: [R] Matrix of Index Variables
Message-ID: <164.272b028b.2cc58b3f@aol.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20031020/1ec753ca/attachment.pl

From TyagiAnupam at aol.com  Mon Oct 20 21:02:23 2003
From: TyagiAnupam at aol.com (TyagiAnupam@aol.com)
Date: Mon, 20 Oct 2003 15:02:23 EDT
Subject: [R] Matrix of Index Variables
Message-ID: <164.272b028b.2cc58b3f@aol.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20031020/1ec753ca/attachment-0001.pl

From ripley at stats.ox.ac.uk  Mon Oct 20 21:01:16 2003
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Mon, 20 Oct 2003 20:01:16 +0100 (BST)
Subject: [R] 'optim' and extra argument to the objective function
In-Reply-To: <20031020181023.GB4080718@genome.cbs.dtu.dk>
Message-ID: <Pine.LNX.4.44.0310201957330.1759-100000@gannet.stats>

On Mon, 20 Oct 2003, Laurent Gautier wrote:

> I'd like to use optim, and give extra arguments to the objective
> function. The man page says that the '...' should let one do it,
> but I have a hard time to understand how.

Try reading up on argument matching, then.  E.g. in `S Programming'
(see the R FAQ).

> Example:
> 
> x <- 1:10
> y <- rnorm(10)
> cost.f <- function(par, x, y) {
>  A <- par[1]
>  cost <- sum( (log(A*x) - log(y))^2)
>  return(cost)
> }
> 
> optim(3, cost.f, x, y)
> ## returns:
> Error in pmatch(x, table, duplicates.ok) : 
> 	argument is not of mode character
> ## uh, uh... may the problem is with the argument matching ?
> optim(3, cost.f, method="BFGS", x, y)
> ## returns:
> Error in log(A * x) : Argument "x" is missing, with no default
> In addition: Warning message: 
> bounds can only be used with method L-BFGS-B in: optim(3, cost.f, method = "BFGS", x, y) 
> 
> Any suggestion ?

optim(3, cost.f, method="BFGS", x=x, y=y)

except that some component of y is negative with high probability ....

You do have to *name* arguments in ... -- that's standard S/R syntax.

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From spencer.graves at pdf.com  Mon Oct 20 21:09:18 2003
From: spencer.graves at pdf.com (Spencer Graves)
Date: Mon, 20 Oct 2003 12:09:18 -0700
Subject: [R] changing the the column header in a data.frame
In-Reply-To: <1066675004.5560.0.camel@ra.chem.psu.edu>
References: <1066675004.5560.0.camel@ra.chem.psu.edu>
Message-ID: <3F9432DE.7090202@pdf.com>

Consider the following: 

DF <- data.frame(a=1:2, b=3:4)
names(DF) <- letters[1:2]
DF
dimnames(DF)[[2]][2] <- "z"
DF

      This example illustrates two different ways to do what I 
understood from your question.  Answered? 

      spencer graves


Rajarshi Guha wrote:

>Hi,
>  is there any way I can change the column header in a data.frame?
>
>Thanks,
>
>-------------------------------------------------------------------
>Rajarshi Guha <rxg218 at psu.edu> <http://jijo.cjb.net>
>GPG Fingerprint: 0CCA 8EE2 2EEB 25E2 AB04 06F7 1BB9 E634 9B87 56EE
>-------------------------------------------------------------------
>Eureka!
>-- Archimedes
>
>______________________________________________
>R-help at stat.math.ethz.ch mailing list
>https://www.stat.math.ethz.ch/mailman/listinfo/r-help
>  
>



From paulda at BATTELLE.ORG  Mon Oct 20 21:59:42 2003
From: paulda at BATTELLE.ORG (Paul, David  A)
Date: Mon, 20 Oct 2003 15:59:42 -0400
Subject: [R] problem with win.metafile( ): traceback()
Message-ID: <940250A9EB37A24CBE28D858EF07774967ABDC@ws-bco-mse3.milky-way.battelle.org>

In R1.8.0 for Windows, immediately after I 
execute win.metafile(...) and lset(...) I get the 
following values for .Device and .Devices:


> .Device
[1] "win.metafile://.../plot1.wmf"

> .Devices
[[1]]
[1] "null device"

[[2]]
[1] "win.metafile://.../plot1.wmf"


However, when I change to

> trellis.device(win.metafile, 
+   file = "//.../plot1.wmf", 
+   width = 8.5, height = 6.25)
>
> lset( list( background = list(col = "white")))

I get _no_ errors, and xyplot(...) creates the
appropriate windows metafile.

-david paul


-----Original Message-----
From: Deepayan Sarkar [mailto:deepayan at stat.wisc.edu] 
Sent: Sunday, October 19, 2003 12:48 PM
To: Paul, David A; 'r-help at stat.math.ethz.ch'; 'Deepayan Sarkar'
Subject: Re: [R] problem with win.metafile( ): traceback()


On Sunday 19 October 2003 11:49, Paul, David A wrote:
> For the first error message:
> > win.metafile(file = "//.../plot1.wmf",
> + width = 8.5, height = 6.25)

Could you check what the value of the .Device variable (and .Devices as
well) 
is at this point ? And not that it should matter, but what happens if you
use

trellis.device(win.metafile, file = "//.../plot1.wmf",
               width = 8.5, height = 6.25)

Deepayan


> > lset( list( background = list(col = "white")))
>
> Error in get(x, envir, mode, inherits) :
> variable "win.metafile://.../plot1.wmf" was not found
>
> > traceback()
>
> 4: get(device)
> 3: trellis.device(device = .Device, new = FALSE)
> 2: trellis.par.get(item)
> 1: lset(list(background = list(col = "white")))



From mailinglist2_wegmann at web.de  Mon Oct 20 22:06:46 2003
From: mailinglist2_wegmann at web.de (Martin Wegmann)
Date: Mon, 20 Oct 2003 22:06:46 +0200
Subject: [R] R 1.8 for debian
In-Reply-To: <3699CDBC4ED5D511BE6400306E1C0D81030A0414@hermes.demogr.mpg.de>
References: <3699CDBC4ED5D511BE6400306E1C0D81030A0414@hermes.demogr.mpg.de>
Message-ID: <200310202206.46894.mailinglist2_wegmann@web.de>

On Monday 20 October 2003 18:58, Rau, Roland wrote:
> Hi Martin,
>
> > Another question would be, how I can keep all my previously installed
> > packages.
> > will they be kept in '/usr/local/lib/R/site-library or does the new R
> > overwrite these addional packages?
> >
> > cheers Martin
>
> 	don't know about your first question. But your second question has
> been already adressed and answered today on this list. Maybe you should
> read today's previous messages.

thanks for this information, I did not receive any R-list mails recently - 
excessive bouncing of mails due to spam probably caused again a 
"non-delivery" status for this address   - I will check it in the archive, 
regards Martin



From paul.schwarz at oregonstate.edu  Mon Oct 20 22:23:32 2003
From: paul.schwarz at oregonstate.edu (Schwarz, Paul)
Date: Mon, 20 Oct 2003 13:23:32 -0700
Subject: [R] win.metafile() problem -- possible bug?
Message-ID: <8E46EB3BC001414AA6CDB57C5E551F8D12F20C@thuja>


As a followup to a previous posting regarding the win.metafile()
function, I've been having some problems as well.  In my case, I've been
converting some scripts from S-SPLUS to R that create graphs in order to
take advantage of R's plotmath capabilities.

In the sample code shown below, the axis labels display the units for
the data.  In particular, the units for the y axis are "grams carbon per
meter squared per month" or "gC m-2 m-1".  I have noticed that using the
following code, if one prints directly from the graph window, the plot
prints correctly with the proper spacing between the "gC", "m-2", and
"m-1".  However, when I use either the "copy as metafile" feature of the
Windows RGui or add a direct call to win.metafile(), the subsequent .wmf
file does not have spaces between the components of the axis labels.

#win.metafile(filename= "Rplot%02d.emf", height=11, width=8.5,
pointsize=12)
par(mar= c(5,4,4,5)+0.1)

x <- 1:10
y <- 0.8*x+1
plot(x, y, type="n", axes=F, xlab="", ylab="")
box()
points(x, y, pch=1, col=1, cex=1.2)
axis(side=2)
mtext(side=2, line=2.5, expression(paste("monthly (gC ", m^-2, " ",
m^-1, ")")))
axis(side=1, at=1:12)

par(new=T)#, xaxs="d")
plot(x, y, type="n", axes=F, xlab="", ylab="")
y <- 0.5*x-1
points(x, y, pch=16, col=1, cex=1.2)
axis(side=4)
mtext(side=4, line=3, expression(paste("cumulative (gC ", m^-2, ")")))


I have been working with R 1.8.0 on a computer runing Windows XP.
However, I've also tried the code under R 1.7.1, and the behavior
appears to be the same.  Also, the "correct" spacing when printing the
graph window directly does not appear to be related to what kind of
print driver is used because both PostScript and PCL drivers print
correctly.  Moreover, I've also tried to paste the metafile into a
program other than MS Word, but the results are the same as with MS
Word.  So I am guessing that the lack of spaces between the axis label
components is related to the creation of the windows metafile.

A workaround is that an extra space can be added in the call to
expression(), but it certainly would be nice not to need two different
statements that depended on whether the graph was printed directly or
saved as a metafile.

So could this be a bug, or is it some sort of known limitation when
creating metafiles?

Thanks for your time and consideration,

-Paul Schwarz



From deepayan at stat.wisc.edu  Mon Oct 20 22:04:16 2003
From: deepayan at stat.wisc.edu (Deepayan Sarkar)
Date: Mon, 20 Oct 2003 15:04:16 -0500
Subject: [R] problem with win.metafile( ): traceback()
In-Reply-To: <940250A9EB37A24CBE28D858EF07774967ABDC@ws-bco-mse3.milky-way.battelle.org>
References: <940250A9EB37A24CBE28D858EF07774967ABDC@ws-bco-mse3.milky-way.battelle.org>
Message-ID: <200310201504.16814.deepayan@stat.wisc.edu>


OK, I see the problem, and it should be fixed in the next release. Use 
trellis.device() till then. Thanks,

Deepayan

On Monday 20 October 2003 14:59, Paul, David A wrote:
> In R1.8.0 for Windows, immediately after I
> execute win.metafile(...) and lset(...) I get the
>
> following values for .Device and .Devices:
> > .Device
>
> [1] "win.metafile://.../plot1.wmf"
>
> > .Devices
>
> [[1]]
> [1] "null device"
>
> [[2]]
> [1] "win.metafile://.../plot1.wmf"
>
>
> However, when I change to
>
> > trellis.device(win.metafile,
>
> +   file = "//.../plot1.wmf",
> +   width = 8.5, height = 6.25)
>
> > lset( list( background = list(col = "white")))
>
> I get _no_ errors, and xyplot(...) creates the
> appropriate windows metafile.
>
> -david paul
>
>
> -----Original Message-----
> From: Deepayan Sarkar [mailto:deepayan at stat.wisc.edu]
> Sent: Sunday, October 19, 2003 12:48 PM
> To: Paul, David A; 'r-help at stat.math.ethz.ch'; 'Deepayan Sarkar'
> Subject: Re: [R] problem with win.metafile( ): traceback()
>
> On Sunday 19 October 2003 11:49, Paul, David A wrote:
> > For the first error message:
> > > win.metafile(file = "//.../plot1.wmf",
> >
> > + width = 8.5, height = 6.25)
>
> Could you check what the value of the .Device variable (and .Devices as
> well)
> is at this point ? And not that it should matter, but what happens if you
> use
>
> trellis.device(win.metafile, file = "//.../plot1.wmf",
>                width = 8.5, height = 6.25)
>
> Deepayan
>
> > > lset( list( background = list(col = "white")))
> >
> > Error in get(x, envir, mode, inherits) :
> > variable "win.metafile://.../plot1.wmf" was not found
> >
> > > traceback()
> >
> > 4: get(device)
> > 3: trellis.device(device = .Device, new = FALSE)
> > 2: trellis.par.get(item)
> > 1: lset(list(background = list(col = "white")))



From Ted.Harding at nessie.mcc.ac.uk  Mon Oct 20 21:38:01 2003
From: Ted.Harding at nessie.mcc.ac.uk ( (Ted Harding))
Date: Mon, 20 Oct 2003 20:38:01 +0100 (BST)
Subject: [R] "aliases" for R constructs?
Message-ID: <XFMail.031020203801.Ted.Harding@nessie.mcc.ac.uk>

Hi Folks,

My recent response to Laura Quinn's query about matrix subsetting
reminded of a question.

I wrote:

  iDir <- ((Winds[,20]<45)|(Winds[,20]>315))&(!is.na(Winds[,20]))

Now, I find "!is.na" a bit awkward to type, so I might prefer to
type it as "nis.na".

While it is possible to define

  nis.na <- function(x){ !is.na(x) }

this involves a slight bloat of overhead in execution since achieving
!is.na involves an extra layer of function call. So is it possible
to define an "alias" so that

  nis.na(x)

is _exactly_ equivalent to

  !is.na(x)

?

This is not quite the same as defining a simple alias for the name of
a function or other object, though I'd be very interested to know if
this can be done too.

I'm thinking of something which would work like the 'define'
mechanism in C (and some other languages), where

  #define nis.na !is.na

would cause simple substitution of "!is.na" for "nis.na" on input
(and you can have parametrised defines too).

With thanks,
Ted.



--------------------------------------------------------------------
E-Mail: (Ted Harding) <Ted.Harding at nessie.mcc.ac.uk>
Fax-to-email: +44 (0)870 167 1972
Date: 20-Oct-03                                       Time: 20:38:01
------------------------------ XFMail ------------------------------



From jasont at indigoindustrial.co.nz  Mon Oct 20 23:04:17 2003
From: jasont at indigoindustrial.co.nz (Jason Turner)
Date: Tue, 21 Oct 2003 10:04:17 +1300
Subject: [R] selecting subsets of data from matrix
In-Reply-To: <Pine.LNX.4.44.0310201903220.23087-100000@env-pc-phd13>
References: <Pine.LNX.4.44.0310201903220.23087-100000@env-pc-phd13>
Message-ID: <3F944DD1.4000701@indigoindustrial.co.nz>

Laura Quinn wrote:

> Thanks for your help everyone,
> 
> My data is a matrix. However if i use the command:
> 
> x[(x[,20] > 315 | x[,20] < 45), ]
> 
> and then request a summary, I get a warning message saying that a large
> number of the row names have been duplicated - I don't understand this?

If x is the original matrix, and subx is the subseted matrix
(x[(x[,20] > 315 | x[,20] < 45), ])  ...

Do you get this warning message when you type summary(x)?

If you're not explicitly using the rownames for anything other than 
labeling, it can be safely ignored.  Duplicate rownames are a bit 
strange to R, but they're not illegal in matricies.

Cheers

Jason
-- 
Indigo Industrial Controls Ltd.
http://www.indigoindustrial.co.nz
64-21-343-545
jasont at indigoindustrial.co.nz



From jasont at indigoindustrial.co.nz  Mon Oct 20 23:08:46 2003
From: jasont at indigoindustrial.co.nz (Jason Turner)
Date: Tue, 21 Oct 2003 10:08:46 +1300
Subject: [R] Matrix of Index Variables
In-Reply-To: <164.272b028b.2cc58b3f@aol.com>
References: <164.272b028b.2cc58b3f@aol.com>
Message-ID: <3F944EDE.1010505@indigoindustrial.co.nz>

TyagiAnupam at aol.com wrote:

> What is a good way to create a matrix of index variables based on all 
> possible combinations of a list of factors in a data-frame, say list(age, sex)? "age" 
> and "sex" are numeric and factor variables in a dataframe, with 99 and 2 
> values, respectively.
> I would like to use these for subsetting the data-frame, apply functions to 
> subset and collecting the results.
> 

Rather than creating a new index, I think you want the function "by()". 
  Something like

foo <- by(mydata,list(age=mydata$age, sex=mydata$sex), 
your.summary.function)

Cheers

Jason
-- 
Indigo Industrial Controls Ltd.
http://www.indigoindustrial.co.nz
64-21-343-545
jasont at indigoindustrial.co.nz



From rxg218 at psu.edu  Mon Oct 20 23:17:59 2003
From: rxg218 at psu.edu (Rajarshi Guha)
Date: 20 Oct 2003 17:17:59 -0400
Subject: [R] nnet behaving oddly
Message-ID: <1066684679.5744.8.camel@ra.chem.psu.edu>

Hi,
  I was trying to use the nnet library and am not sure of whats going
on. I am calling the nnet function as:

 n <- nnet(x,y,size=3,subset=sets[[1]], maxit=200)

Where x is a 272x4 matrix of observations (examples) and y is a 272x1
matrix of target values. However when I look at nnet$residuals they are
off by two orders of magnitude (compared to the output from neural
network code that I already have). Looking at nnet$fitted.values shows
all the values to be 1 (whereas my target values range from 0 to 150).

Am I making an obvious mistake in the way I'm calling the function? Is
the fact that n$fitted.values is all 1's indicating that the NN is doing
a classification? If so how can I make it do quantitation?

The man page mentions that if the response is a factor then it defaults
to quantitation. However my y matrix just contain numbers - so it
should'nt be doing classification.

Any pointers would be appreciated.

Thanks,

-------------------------------------------------------------------
Rajarshi Guha <rxg218 at psu.edu> <http://jijo.cjb.net>
GPG Fingerprint: 0CCA 8EE2 2EEB 25E2 AB04 06F7 1BB9 E634 9B87 56EE
-------------------------------------------------------------------
Psychology is merely producing habits out of rats.



From ok at cs.otago.ac.nz  Mon Oct 20 23:46:57 2003
From: ok at cs.otago.ac.nz (Richard A. O'Keefe)
Date: Tue, 21 Oct 2003 10:46:57 +1300 (NZDT)
Subject: [R] dev.print in R 1.7.1
Message-ID: <200310202146.h9KLkvKm406438@atlas.otago.ac.nz>

Prof Brian Ripley <ripley at stats.ox.ac.uk> wrote:
	(AFAIK there never was a function save.plot).
	
In R 1.7.1 ?save.plot does in fact list save.plot among the ?Defunct
functions.  In the text, it says

     The new function `dev.print()' should now be used for saving plots
     to a file or printing them.

?dev.print explains
  device: A device function (e.g., `x11', `postscript', ...)
which unfortunately suggests that the argument should be a string,
but the example
     dev.print(device=postscript, ...)
makes all clear.

R on-line documentation is really _very_ good.



From nathoo at cs.sfu.ca  Mon Oct 20 23:51:37 2003
From: nathoo at cs.sfu.ca (Farouk Nathoo)
Date: Mon, 20 Oct 2003 14:51:37 -0700 (PDT)
Subject: [R] memory error / iterative procedure
In-Reply-To: <8E46EB3BC001414AA6CDB57C5E551F8D12F20C@thuja>
Message-ID: <Pine.GSO.4.10.10310201401560.25881-100000@stawlmihq>

Dear R experts,


I have been trying to run an iterative procedure in R and am having
some sort of memory build up problem. I am using R1.8.0 on Windows XP.
One single iteration of my procedure is coded into a function. This
function creates an extremely large matrix of simulated values (actually
calls WinBugs which returns simulations), does some calculations with it
and returns a single number as a result.  After this one step I no longer
need this large matrix but it seems to be stored in memory anyhow. The
code is something like:


	parameter <- 0 # initial value
	for  (i in 1:1000)
		{
		parameter <- one.step(parameter, data)
		mem <- memory.size()
		cat(parameter," ", mem,"\n")
		}


I output the memory.size() at each iteration and this grows and grows 
until I run out of memory and get an allocation error. When this happens,
I record the last parameter value, quit R, start R again and rerun the
procedure starting with this most recent value. I'd rather not do it this
way! I have increased the memory limit using the memory.limit() function
and this helps a bit. 

My Questions:

1. Is there any way to free the memory after each iteration since 
I really don't need anything other than the most recent parameter value?

2. If I run this code on the same machine but using the Linux OS will i
have the same problem?

3. Would I be able to avoid this problem if I ran the loop in some other
language like Perl or C and called the R function to do one iteration at
each step.  

I have noticed several postings about this sort of thing in the archives
but I'm still a bit unclear. Any help is greatly appreciated.

Thanks,
Farouk



From tlumley at u.washington.edu  Tue Oct 21 00:02:44 2003
From: tlumley at u.washington.edu (Thomas Lumley)
Date: Mon, 20 Oct 2003 15:02:44 -0700 (PDT)
Subject: [R] "aliases" for R constructs?
In-Reply-To: <XFMail.031020203801.Ted.Harding@nessie.mcc.ac.uk>
References: <XFMail.031020203801.Ted.Harding@nessie.mcc.ac.uk>
Message-ID: <Pine.A41.4.58.0310201458140.84428@homer10.u.washington.edu>

On Mon, 20 Oct 2003 Ted.Harding at nessie.mcc.ac.uk wrote:

> Hi Folks,
>
> My recent response to Laura Quinn's query about matrix subsetting
> reminded of a question.
>
> I wrote:
>
>   iDir <- ((Winds[,20]<45)|(Winds[,20]>315))&(!is.na(Winds[,20]))
>
> Now, I find "!is.na" a bit awkward to type, so I might prefer to
> type it as "nis.na".
>
> While it is possible to define
>
>   nis.na <- function(x){ !is.na(x) }
>
> this involves a slight bloat of overhead in execution since achieving
> !is.na involves an extra layer of function call. So is it possible
> to define an "alias" so that
>
>   nis.na(x)
>
> is _exactly_ equivalent to
>
>   !is.na(x)
>

No, R doesn't have macros.  As R is interpreted, a macro wouldn't
necessarily save you any execution time -- Lisp macros, IIRC, are only
expanded at compile-time.

You can get most of the programming advantages of macros with substitute
and eval, but this is presumably slower rather than faster.


	-thomas



From p.dalgaard at biostat.ku.dk  Tue Oct 21 00:21:43 2003
From: p.dalgaard at biostat.ku.dk (Peter Dalgaard)
Date: 21 Oct 2003 00:21:43 +0200
Subject: [R] dev.print in R 1.7.1
In-Reply-To: <200310202146.h9KLkvKm406438@atlas.otago.ac.nz>
References: <200310202146.h9KLkvKm406438@atlas.otago.ac.nz>
Message-ID: <x23cdnsg7c.fsf@biostat.ku.dk>

"Richard A. O'Keefe" <ok at cs.otago.ac.nz> writes:

> ?dev.print explains
>   device: A device function (e.g., `x11', `postscript', ...)
> which unfortunately suggests that the argument should be a string,

Mmmno. It suggests quite specifically that it should be a function. In
the plaintext help files, `...' is used for marking code samples.
(\code{...} in the documentation sources)

> but the example
>      dev.print(device=postscript, ...)
> makes all clear.


-- 
   O__  ---- Peter Dalgaard             Blegdamsvej 3  
  c/ /'_ --- Dept. of Biostatistics     2200 Cph. N   
 (*) \(*) -- University of Copenhagen   Denmark      Ph: (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)             FAX: (+45) 35327907



From ggrothendieck at myway.com  Tue Oct 21 01:31:42 2003
From: ggrothendieck at myway.com (Gabor Grothendieck)
Date: Mon, 20 Oct 2003 19:31:42 -0400 (EDT)
Subject: [R] aliases
Message-ID: <20031020233142.61059396E@xmxpita.myway.com>



How about:

   nis.na <- complete.cases
 
---

From: <Ted.Harding at nessie.mcc.ac.uk>

Hi Folks,

My recent response to Laura Quinn's query about matrix subsetting
reminded of a question.

I wrote:

iDir <- ((Winds[,20]<45)|(Winds[,20]>315))&(!is.na(Winds[,20]))

Now, I find "!is.na" a bit awkward to type, so I might prefer to
type it as "nis.na".

While it is possible to define

nis.na <- function(x){ !is.na(x) }

this involves a slight bloat of overhead in execution since achieving
!is.na involves an extra layer of function call. So is it possible
to define an "alias" so that

nis.na(x)

is _exactly_ equivalent to

!is.na(x)

?

This is not quite the same as defining a simple alias for the name of
a function or other object, though I'd be very interested to know if
this can be done too.

I'm thinking of something which would work like the 'define'
mechanism in C (and some other languages), where

#define nis.na !is.na

would cause simple substitution of "!is.na" for "nis.na" on input
(and you can have parametrised defines too).

With thanks,
Ted.



--------------------------------------------------------------------
E-Mail: (Ted Harding) <Ted.Harding at nessie.mcc.ac.uk>
Fax-to-email: +44 (0)870 167 1972
Date: 20-Oct-03 Time: 20:38:01
------------------------------ XFMail ------------------------------



_______________________________________________
No banners. No pop-ups. No kidding.
Introducing My Way - http://www.myway.com



From hammour at msn.com  Tue Oct 21 01:50:52 2003
From: hammour at msn.com (Ahmad)
Date: Mon, 20 Oct 2003 19:50:52 -0400
Subject: [R] compiling R 1.8.0 on Mandrake 9.2
Message-ID: <BAY3-DAV155JiLyNfHg00000cf5@hotmail.com>

hi,
I compiled R 1.8.0 on mandrake 9.2 dowload eddition.
I got tools necessary to compile it, but what I find that the function
"link.html.help()" is not found. There could be more functions not found.
Anybody give some advice on this.
Thank you for your help.
Ahmad Abu Hammour



From paul.livingstone at aerostructures.com.au  Tue Oct 21 02:02:16 2003
From: paul.livingstone at aerostructures.com.au (Paul Livingstone)
Date: Tue, 21 Oct 2003 10:02:16 +1000
Subject: [R] select text using only the keyboard
Message-ID: <GJEJLMOAOIMMJLBHDACOMELCCFAA.paul.livingstone@aerostructures.com.au>

I am using R 1.6.1 on Windows.  I usually write/edit code in the R console,
and when I get it working right, copy and paste it to a text file for later
use.  In Splus for Windows, this could be done easily using just the
keyboard, with up-arrow, shift-end or shift-home, ctrl-c, etc.  

But in R, the shift key doesn't select text.  The only way I can select text
in R is with the mouse.  Does anyone know how to select text in R using only
the keyboard?

Many thanks,
Paul.

Paul Livingstone
Statistical Analyst
AeroStructures?
Level 14, 222 Kingsway
South Melbourne, Vic, 3205
Phone: 03 9694 1083
Mobile: 0418 121 530
Fax: 03 9696 8195
Email: paul.livingstone at aerostructures.com.au
Web: www.aerostructures.com.au


From nathoo at cs.sfu.ca  Tue Oct 21 03:12:59 2003
From: nathoo at cs.sfu.ca (Farouk Nathoo)
Date: Mon, 20 Oct 2003 18:12:59 -0700 (PDT)
Subject: [R] memory error / iterative procedure
Message-ID: <Pine.GSO.4.10.10310201811200.28742-100000@stawlmihq>


Dear R experts,


I have been trying to run an iterative procedure in R and am having
some sort of memory build up problem. I am using R1.8.0 on Windows XP.
One single iteration of my procedure is coded into a function. This
function creates an extremely large matrix of simulated values (actually
calls WinBugs which returns simulations), does some calculations with it
and returns a single number as a result.  After this one step I no longer
need this large matrix but it seems to be stored in memory anyhow. The
code is something like:


        parameter <- 0 # initial value
   for  (i in 1:1000)
                {
                parameter <- one.step(parameter, data)
                mem <- memory.size()
                cat(parameter," ", mem,"\n")
                }


I output the memory.size() at each iteration and this grows and grows
until I run out of memory and get an allocation error. When this happens,
I record the last parameter value, quit R, start R again and rerun the
procedure starting with this most recent value. I'd rather not do it this
way! I have increased the memory limit using the memory.limit() function
and this helps a bit.

My Questions:

1. Is there any way to free the memory after each iteration since
I really don't need anything other than the most recent parameter value?

2. If I run this code on the same machine but using the Linux OS will i
have the same problem?

3. Would I be able to avoid this problem if I ran the loop in some other
language like Perl or C and called the R function to do one iteration at

I have noticed several postings about this sort of thing in the archives
but I'm still a bit unclear. Any help is greatly appreciated.

Thanks,
Farouk



From gerifalte28 at hotmail.com  Tue Oct 21 03:29:18 2003
From: gerifalte28 at hotmail.com (Francisco Vergara)
Date: Tue, 21 Oct 2003 01:29:18 +0000
Subject: [R] Polynomial lags
Message-ID: <Law14-F54XsL1c00goZ0001477b@hotmail.com>

Does anybody know if there is a built in fuction to use create polynomial 
distributed lags (sometimed called Almon lag) on linear models?

Thanks

Francisco

_________________________________________________________________
See when your friends are online with MSN Messenger 6.0. Download it now



From glaziou at pasteur-kh.org  Tue Oct 21 03:31:35 2003
From: glaziou at pasteur-kh.org (Philippe Glaziou)
Date: Tue, 21 Oct 2003 08:31:35 +0700
Subject: [R] R 1.8 for debian
In-Reply-To: <20031020172327.GA7795@sonny.eddelbuettel.com>
References: <200310201831.09324.mailinglist2_wegmann@web.de>
	<20031020172327.GA7795@sonny.eddelbuettel.com>
Message-ID: <20031021013135.GC13276@pasteur-kh.org>

Dirk Eddelbuettel <edd at debian.org> wrote:
 Sure, got uploaded the day of the 1.8 release. You may want to
> learn how to point apt at different archives. Debian has 1.8 in
> unstable; CRAN has it in testing (thanks to Doug Bates) and
> even in stable (thanks to Korbinian Strimmer)


Please, note also that if your debian box has the compilers and
header files required to compile R from source, then building
your own deb packages should be easy because the developers
included all relevant debian files in the sources.  There is
little need to tweak /etc/apt/sources.list to get the latest
vintage of debianized R, nor to wait if you need non-intel
binaries. 

  sudo apt-get install fakeroot  # if not already installed
  tar xvzf R-1.8.0.tgz
  cd R-1.8.0
  dpkg-buildpackage -rfakeroot   # now is time to have a coffee...
  cd ..
  sudo dpkg -i r-*               # this will install everything


-- 
Philippe



From hamish_nospam at yahoo.com  Tue Oct 21 03:57:54 2003
From: hamish_nospam at yahoo.com (Hamish)
Date: Tue, 21 Oct 2003 14:57:54 +1300
Subject: [R] Re: [STATSGRASS] Interpolation of azimuth values
In-Reply-To: <3F8E4636.9060007@deprem.gov.tr>
References: <3F8E4636.9060007@deprem.gov.tr>
Message-ID: <20031021145754.74a3d9f8.hamish_nospam@yahoo.com>

> I will make an interpolation of data which represents azimuth
> direction( angle from north in clockwise direction) values.
> But there is a problem.
> Say, for instance, while 1 and 359 indicate somewhat same direction, 
> interpolation puts  values
> in the range from 1 to 359.  What can I do to solve the problem ?


You might try breaking it down to x,y components on a circle of radius
1, or take the real and imaginary parts. Bin/average the results and
then convert back to polar angles.

Note if you are comparing vectors it is important to include magnitude
in the calculation and otherwise think about the reality of the answer.
e.g. A strong east-west dominant wind flow through a valley, which may
be 15 km/hr east half the time and 15 km/hr west the other half may
average to a weak north-south net, which tells you nothing about the
'usual' conditions. Sorry, that example isn't very illustrative of my
point.


R CircStats's circ.mean.R does this:
http://cran.r-project.org/src/contrib/PACKAGES.html#CircStats

circ.mean <- function(x) {
        sinr <- sum(sin(x))
        cosr <- sum(cos(x))
        circmean <- atan(sinr, cosr)
        circmean



or a Matlab example:

TH=wind.dirns;
R=wind.velos;
[X,Y] = pol2cart(TH,R);
[TH1,R1] = cart2pol(mean(X),mean(Y));
wind_mean.dirn = TH1 * (180/pi)
wind_mean.velo = mean(wind.velos);



Hamish



From edd at debian.org  Tue Oct 21 04:11:31 2003
From: edd at debian.org (Dirk Eddelbuettel)
Date: Mon, 20 Oct 2003 21:11:31 -0500
Subject: [R] R 1.8 for debian
In-Reply-To: <20031021013135.GC13276@pasteur-kh.org>
References: <200310201831.09324.mailinglist2_wegmann@web.de>
	<20031020172327.GA7795@sonny.eddelbuettel.com>
	<20031021013135.GC13276@pasteur-kh.org>
Message-ID: <20031021021131.GA12249@sonny.eddelbuettel.com>

On Tue, Oct 21, 2003 at 08:31:35AM +0700, Philippe Glaziou wrote:
> Dirk Eddelbuettel <edd at debian.org> wrote:
>  Sure, got uploaded the day of the 1.8 release. You may want to
> > learn how to point apt at different archives. Debian has 1.8 in
> > unstable; CRAN has it in testing (thanks to Doug Bates) and
> > even in stable (thanks to Korbinian Strimmer)
> 
> 
> Please, note also that if your debian box has the compilers and
> header files required to compile R from source, then building
> your own deb packages should be easy because the developers
> included all relevant debian files in the sources.  There is

Yes, but you're coming close to confusing two things here. R upstream has a
debian/ dir, yes, but it has a time lag that occurs because Doug and I only
synchronise every few months between my Debian sources and what is in CVS.
For example, a fix I just put into 1.8.0-2 (concerning the Atlas/Blas
interaction) may not get into CVS until 1.8.1.

> little need to tweak /etc/apt/sources.list to get the latest
> vintage of debianized R, nor to wait if you need non-intel
> binaries. 
> 
>   sudo apt-get install fakeroot  # if not already installed
>   tar xvzf R-1.8.0.tgz
>   cd R-1.8.0
>   dpkg-buildpackage -rfakeroot   # now is time to have a coffee...
>   cd ..
>   sudo dpkg -i r-*               # this will install everything
> 

But if you grab the package sources from Debian, rather than CRAN, and
preferably from unstable, then your advise is spot-on. Locally building .deb
package is very easy, and a good way to customise if that is desired.  For
the rest, there's always apt-get :)  Thanks for pointing that out.

Dirk

-- 
Those are my principles, and if you don't like them... well, I have others.
                                                -- Groucho Marx



From nusbj at hotmail.com  Tue Oct 21 06:33:22 2003
From: nusbj at hotmail.com (Zhen Pang)
Date: Tue, 21 Oct 2003 12:33:22 +0800
Subject: [R] run R under linux
Message-ID: <Sea2-F5qWcH8A1aWSXA00003059@hotmail.com>

Dear all,

Our department uses the linux system and we are not allowed to submit job 
directly. We must make a batch to submit through "qmon".

so, I make a foo.sh file, which only contains one line: nohup R --vanilla < 
foo.txt > foo.results

foo is all my codes. It is a simulation of 200 times. I set the seed at the 
beginning. It is to estimate the success probability, which is very small, 
so my samples may be all 0s. I have tried, if the samples are all 0s, one 
parameter tends to be infinity, and one NA comes.

I want to do 200 simulations. I found during the fisrt 128 simulations, some 
parameters may be NAs, since I use if (abs(aold-anew)<1e-5) {print (anew)  
break} to break the one estimation. Some anew is not printed. I think NA 
comes. But for the 129th simulation, one matrix is singular and my program 
ends.

I want to know how to resume my program with the seeds saved, and do like 
continueing the 130th one without break. If possible, the results of the 
first 128 simulations can be saved and combine with the remaining 
simulation.

One more question is: in my code, I write the estimation of 200 simulations 
as a matrix aaa to a txt file. After the if (abs(aold-anew)<1e-5) {print 
(anew)  break}, I assign aaa[i]<- anew (where i is the number of 
simulation).  But I found although some anew is not printed, which I assume 
NA comes, the resulting matrix does not have NAs, they all be numbers. Why?


Regards,

Zhen


>From: michaell taylor <mt at michaelltaylor.com>
>To: R-help at stat.math.ethz.ch
>CC: nusbj at hotmail.com
>Subject: Re: [R] run R under unix
>Date: 09 Oct 2003 08:00:15 -0400
>
>One can run R 'txt', script files thusly:
>
>1. create the txt file (foo.txt) script.
>2. at a command prompt type :
>	R --vanilla < foo.txt > foo.results
>
>The file 'foo.results' will now have all the output that you normally
>would see on the screen.  This is actually quite useful in that you can
>move around freely within foo.results (using some sort of text editor) -
>unlike results written to a screen.
>
>This will only run while you are logged into the machine, however.  The
>best way to run a script without having to be logged in is through a
>batch processor. Indeed, for jobs running for days - your system
>administrator will be thrilled that it runs on the batch processor
>instead of interactively.  The method for doing this depends on your
>particular unix machine configuration, but a common method flow like
>this:
>
>1. Place the line:
>
>R --vanilla < foo.txt > foo.results
>
>into a file named foo.batch.  No other text should be in the file. Make
>this file executable via
>
> > chmod 755 foo.batch
>
>Then at the command line:
>
> > at -f foo.batch now
>
>or perhaps,
> > batch -f foo.batch
>
>If this does not work, ask your system administrator how to set up a
>batch process.
>
>The advantage of the batch process is 1) you need not be logged in, 2)
>your job will take a lower priority than interactive jobs.
>
>Michaell
>
>On Thu, 2003-10-09 at 01:52, Jason Turner wrote:
> > Zhen Pang wrote:
> > ...
> > > I now want to run the code under unix. However, I do not know how to 
>run
> > > this code in txt file under unix. I just log in to the unix and enter
> > > the R, what should I do next?
> > >
> > > One more question is: if I log off my computer when R is running under
> > > unix (i.e., disconnect my computer from server), will I get the result
> > > when I log in my computer next time?
> > ...
> >
> > You'll lose it if you run R in the normal, interactive way.  Running it
> > in the background will allow you to log out and still have it running, 
>but!
> >
> > 1) If you're not the only person using this machine, you learn the
> > command "nice" before you begin.
> > 2) I'm not certain you'll be able to produce jpeg or png graphics when
> > backgrounded; your backgrounded task needs access to the windowing
> > system for graphics rendering, and local security policy might prohibit
> > this.
> > 3) Save early, save often.  You probably already know that, but it bears
> > repeating.
> >
> > Here are some suggested steps to run your simulation in background mode.
> >   Unfortunately, the exact commands will depend on which version of Unix
> > you're using, and what command shells are available.  Consult your local
> > expert.
> >
> > 1) transfer the text file of commands to the unix machine.  FTP, using
> > ASCII mode is safest.
> > 2) log onto the Unix machine.
> > 3) run sh, ksh, or bash.  (the syntax for what follows is different for
> > the C shell, and I don't know it).
> > 4) using a stripped-down version of your script which will complete in a
> > short time (say, a minute or two), just to check that things work, type
> >
> > nohup nice 15 R < my.small.script >my.output 2>&1 &
> >
> > (again, learn what "nice" means before you use it.  This may not be
> > suitable, and it's impossible for me to tell from here if it is).
> >
> > I know that's not the full answer, but only someone who knows the local
> > setup can give you that answer.
> >
> > Cheers
> >
> > Jason
> > --
> > Indigo Industrial Controls Ltd.
> > http://www.indigoindustrial.co.nz
> > 64-21-343-545
> > jasont at indigoindustrial.co.nz
> >
> > ______________________________________________
> > R-help at stat.math.ethz.ch mailing list
> > https://www.stat.math.ethz.ch/mailman/listinfo/r-help
>
>______________________________________________
>R-help at stat.math.ethz.ch mailing list
>https://www.stat.math.ethz.ch/mailman/listinfo/r-help

_________________________________________________________________
Find love on MSN Personals http://personals.msn.com.sg/



From glaziou at pasteur-kh.org  Tue Oct 21 07:14:21 2003
From: glaziou at pasteur-kh.org (Philippe Glaziou)
Date: Tue, 21 Oct 2003 12:14:21 +0700
Subject: [R] R 1.8 for debian
In-Reply-To: <20031021021131.GA12249@sonny.eddelbuettel.com>
References: <200310201831.09324.mailinglist2_wegmann@web.de>
	<20031020172327.GA7795@sonny.eddelbuettel.com>
	<20031021013135.GC13276@pasteur-kh.org>
	<20031021021131.GA12249@sonny.eddelbuettel.com>
Message-ID: <20031021051421.GG13276@pasteur-kh.org>

Dirk Eddelbuettel <edd at debian.org> wrote:
> Yes, but you're coming close to confusing two things here. R
> upstream has a debian/ dir, yes, but it has a time lag that
> occurs because Doug and I only synchronise every few months
> between my Debian sources and what is in CVS.  For example, a
> fix I just put into 1.8.0-2 (concerning the Atlas/Blas
> interaction) may not get into CVS until 1.8.1.


Thanks a lot for pointing this out. 

-- 
Philippe



From bw at northbranchlogic.com  Tue Oct 21 07:37:55 2003
From: bw at northbranchlogic.com (Barnet Wagman)
Date: Tue, 21 Oct 2003 00:37:55 -0500
Subject: [R] Running RMySQL with SuSE 8.2?
In-Reply-To: <20031020082754.C9295@jessie.research.bell-labs.com>
References: <3F93100E.2060203@northbranchlogic.com>
	<20031020082754.C9295@jessie.research.bell-labs.com>
Message-ID: <3F94C633.1060108@northbranchlogic.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20031021/4c06b15b/attachment.pl

From ripley at stats.ox.ac.uk  Tue Oct 21 08:43:01 2003
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Tue, 21 Oct 2003 07:43:01 +0100 (BST)
Subject: [R] nnet behaves as documented (was nnet behaving oddly)
In-Reply-To: <1066684679.5744.8.camel@ra.chem.psu.edu>
Message-ID: <Pine.LNX.4.44.0310210735460.2577-100000@gannet.stats>

On 20 Oct 2003, Rajarshi Guha wrote:

> Hi,
>   I was trying to use the nnet library and am not sure of whats going
> on. I am calling the nnet function as:
> 
>  n <- nnet(x,y,size=3,subset=sets[[1]], maxit=200)
> 
> Where x is a 272x4 matrix of observations (examples) and y is a 272x1
> matrix of target values. However when I look at nnet$residuals they are
> off by two orders of magnitude (compared to the output from neural
> network code that I already have). Looking at nnet$fitted.values shows
> all the values to be 1 (whereas my target values range from 0 to 150).
> 
> Am I making an obvious mistake in the way I'm calling the function? Is
> the fact that n$fitted.values is all 1's indicating that the NN is doing
> a classification? If so how can I make it do quantitation?

Yes, so please do read the help page accurately.

> The man page mentions that if the response is a factor then it defaults
> to quantitation. However my y matrix just contain numbers - so it
> should'nt be doing classification.

That's incorrect reading of the help page, which actually says

     If the response in 'formula' is a factor, an appropriate
     classification network is constructed; this has one output and
     entropy fit if the number of levels is two, and a number of
     outputs equal to the number of classes and a softmax output stage
     for more levels.  If the response is not a factor, it is passed on
     unchanged to 'nnet.default'.

and you did not give a formula.

As someone else said recently. those who write the manuals don't expect to 
either read them for you nor re-write them here, so please show more 
consideration for their work.

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From ripley at stats.ox.ac.uk  Tue Oct 21 08:51:14 2003
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Tue, 21 Oct 2003 07:51:14 +0100 (BST)
Subject: [R] select text using only the keyboard
In-Reply-To: <GJEJLMOAOIMMJLBHDACOMELCCFAA.paul.livingstone@aerostructures.com.au>
Message-ID: <Pine.LNX.4.44.0310210750180.2577-100000@gannet.stats>

On Tue, 21 Oct 2003, Paul Livingstone wrote:

> I am using R 1.6.1 on Windows.  I usually write/edit code in the R console,
> and when I get it working right, copy and paste it to a text file for later
> use.  In Splus for Windows, this could be done easily using just the
> keyboard, with up-arrow, shift-end or shift-home, ctrl-c, etc.  
> 
> But in R, the shift key doesn't select text.  The only way I can select text
> in R is with the mouse.  Does anyone know how to select text in R using only
> the keyboard?

No.

If you think this would be a useful contribution, please send a patch 
against the current R sources (1.9.0-to-be, not 1.6.1).

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From p.dalgaard at biostat.ku.dk  Tue Oct 21 08:58:02 2003
From: p.dalgaard at biostat.ku.dk (Peter Dalgaard)
Date: 21 Oct 2003 08:58:02 +0200
Subject: [R] memory error / iterative procedure
In-Reply-To: <Pine.GSO.4.10.10310201811200.28742-100000@stawlmihq>
References: <Pine.GSO.4.10.10310201811200.28742-100000@stawlmihq>
Message-ID: <x24qy3dqmd.fsf@biostat.ku.dk>

Farouk Nathoo <nathoo at cs.sfu.ca> writes:

>         parameter <- 0 # initial value
>    for  (i in 1:1000)
>                 {
>                 parameter <- one.step(parameter, data)
>                 mem <- memory.size()
>                 cat(parameter," ", mem,"\n")
>                 }
> 
> 
> I output the memory.size() at each iteration and this grows and grows
> until I run out of memory and get an allocation error. When this happens,
> I record the last parameter value, quit R, start R again and rerun the
> procedure starting with this most recent value. I'd rather not do it this
> way! I have increased the memory limit using the memory.limit() function
> and this helps a bit.
> 
> My Questions:
> 
> 1. Is there any way to free the memory after each iteration since
> I really don't need anything other than the most recent parameter value?

That generally shouldn't be necessary, unless you're doing something
in one.step() that causes R objects to hang around after each
iteration. A typical mistake is to have an attach() inside the loop
and end up with 1000 copies of the entire data set on the search
path... 
 
> 2. If I run this code on the same machine but using the Linux OS will i
> have the same problem?

Most likely, yes.
 
> 3. Would I be able to avoid this problem if I ran the loop in some other
> language like Perl or C and called the R function to do one iteration at

Probably not.

-- 
   O__  ---- Peter Dalgaard             Blegdamsvej 3  
  c/ /'_ --- Dept. of Biostatistics     2200 Cph. N   
 (*) \(*) -- University of Copenhagen   Denmark      Ph: (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)             FAX: (+45) 35327907



From maechler at stat.math.ethz.ch  Tue Oct 21 10:05:53 2003
From: maechler at stat.math.ethz.ch (Martin Maechler)
Date: Tue, 21 Oct 2003 10:05:53 +0200
Subject: [R] controling x-labels in xyplot (lattice) when x is POSIX object
In-Reply-To: <200310201258.36327.deepayan@stat.wisc.edu>
References: <012821F286ED1D4ABDC72F9E1DD63D0C01FF963F@NLDNC003PEX1.ubsgs.ubsgroup.net>
	<200310201258.36327.deepayan@stat.wisc.edu>
Message-ID: <16276.59617.631697.734787@gargle.gargle.HOWL>

>>>>> "Deepayan" == Deepayan Sarkar <deepayan at stat.wisc.edu>
>>>>>     on Mon, 20 Oct 2003 12:58:36 -0500 writes:

    Deepayan> On Monday 20 October 2003 12:35,
    Deepayan> john.gavin at ubs.com wrote:

   <.....>

    >> Previously, I used 'calculateAxisComponents' to massage
    >> the labels manually but that function (which I realise
    >> was internal to lattice) is no longer available.

    Deepayan> It's still there, but not exported (in the
    Deepayan> NAMESPACE sense). You will find it in the source,
    Deepayan> and perhaps be able to use it to calculate your
    Deepayan> own 'at' and 'labels' externally.

and you can now (R 1.8.x) use the ":::" operator to access internal
symbols, i.e., use the function as

lattice:::calculateAxisComponents(...)

    Deepayan> <.........>

Martin



From susanabird at yahoo.com.au  Tue Oct 21 10:22:52 2003
From: susanabird at yahoo.com.au (=?iso-8859-1?q?Susana=20Bird?=)
Date: Tue, 21 Oct 2003 18:22:52 +1000 (EST)
Subject: [R] Jarque-Bera Test
Message-ID: <20031021082252.81152.qmail@web20502.mail.yahoo.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20031021/548cbdb5/attachment.pl

From Bernhard.Pfaff at drkw.com  Tue Oct 21 10:29:17 2003
From: Bernhard.Pfaff at drkw.com (Pfaff, Bernhard)
Date: Tue, 21 Oct 2003 10:29:17 +0200
Subject: [R] Jarque-Bera Test
Message-ID: <18D602BD42B7E24EB810D6454A58DB9004730672@ibfftce505.is.de.dresdnerkb.com>

> Dear all,
>  
> i have the question about the using of Jarque-Bera Test by 
> using R. The question is that I do not have in my package 
> "ts" this test and can not obtain any information in the 
> help-file. Could you help my? Where could I download the 
> package and which one, to use the Jarque-Bera Test?

see package "tseries"


>  
> Thank You,
> Susan
> 
> 


--------------------------------------------------------------------------------
The information contained herein is confidential and is intended solely for the
addressee. Access by any other party is unauthorised without the express
written permission of the sender. If you are not the intended recipient, please
contact the sender either via the company switchboard on +44 (0)20 7623 8000, or
via e-mail return. If you have received this e-mail in error or wish to read our
e-mail disclaimer statement and monitoring policy, please refer to 
http://www.drkw.com/disc/email/ or contact the sender.



From tobias.verbeke at bivv.be  Tue Oct 21 10:58:38 2003
From: tobias.verbeke at bivv.be (tobias.verbeke@bivv.be)
Date: Tue, 21 Oct 2003 10:58:38 +0200
Subject: [R] weighted correlations and NAs
Message-ID: <OFC60E2123.F143B1D8-ONC1256DC6.00306E57-C1256DC6.0031398C@BIVV.BE>





Dear list,

Is there a way to obtain a matrix of weighted
correlations in the presence of missing values ?
cor() can deal with NAs but cov.wt() apparently
can't. Is there any package that offers such a
function, e.g. one that uses all complete pairs
of observations ?

Thanks in advance,

Tobias



From mkondrin at hppi.troitsk.ru  Tue Oct 21 15:06:06 2003
From: mkondrin at hppi.troitsk.ru (M.Kondrin)
Date: Tue, 21 Oct 2003 13:06:06 +0000
Subject: [R] Running RMySQL with SuSE 8.2?
References: <3F93100E.2060203@northbranchlogic.com>	<20031020082754.C9295@jessie.research.bell-labs.com>
	<3F94C633.1060108@northbranchlogic.com>
Message-ID: <3F952F3E.1070005@hppi.troitsk.ru>

Barnet Wagman wrote:

>Do you know where I find a patched version, or where I find the patch an 
>instructions on how to install it?  (I didn't see anything about this on 
>CRAN.)  I'm running R-base-1.8.0-1.i386.rpm (the most recent binary 
>available for SuSE) and it appears to have the 'valueClass' problem.
>
>Thanks,
>
>Barnet
>
>David James wrote:
>
>  
>
>>However, 
>>there is a problem in the released version of R 1.8.0 that affects
>>the DBI and other packages (has something to do with methods
>>that use the "valueClass" argument in the setGeneric/setMethod functions).
>>In this case one needs to use the R-patched version.
>>
>>    
>>
>
>
>  
>
>>Hope this helps,
>>
>>--
>>David
>>
>>Barnet Wagman wrote:
>> 
>>
>>    
>>
>>>Since there doesn't appear to be an RMySQL rpm for SuSE 8.*,  does 
>>>anyone know if the 7.3 version will work with the SuSE 8.2 rpms of R and 
>>>DBI?
>>>
>>>The package installs without complaint, but when I try to run
>>>
>>>   con <- dbConnect(dbDriver("MySQL"),dbname="test")
>>>
>>>I get the error
>>>
>>>   Error in dbConnect(dbDriver("MySQL")) : couldn't find function 
>>>".valueClassTest"
>>>
>>>(This is my first attempt to access a an rdms from R, so I could be 
>>>doing something else wrong.)
>>>
>>>Any ideas as what might be generating this error, or as to combinations 
>>>of rpms that will work under SuSE 8.2 would be appreciated. (I took a 
>>>stab at compiling RMySQL from src, but I don't have MySQL src installed 
>>>and I rather not get involved in this if I can avoid it.)
>>>
>>>Thanks,
>>>
>>>Barnet Wagman
>>>
>>>______________________________________________
>>>R-help at stat.math.ethz.ch mailing list
>>>https://www.stat.math.ethz.ch/mailman/listinfo/r-help
>>>   
>>>
>>>      
>>>
>> 
>>
>>    
>>
>
>	[[alternative HTML version deleted]]
>
>______________________________________________
>R-help at stat.math.ethz.ch mailing list
>https://www.stat.math.ethz.ch/mailman/listinfo/r-help
>
>  
>
There was a discussion about it just after R-1.8.0 was released. Patched 
version may be found at CRAN (as usual). For example 
ftp://ftp.stat.math.ethz.ch/Software/R/R-patched_2003-10-*. You can 
either compile it from scratch or fix your available installation just 
adding a line exports(.valueClassTest) to the end of NAMESPACE file in 
the methods library (this works for me).



From spencer.graves at pdf.com  Tue Oct 21 11:10:53 2003
From: spencer.graves at pdf.com (Spencer Graves)
Date: Tue, 21 Oct 2003 02:10:53 -0700
Subject: [R] Polynomial lags
In-Reply-To: <Law14-F54XsL1c00goZ0001477b@hotmail.com>
References: <Law14-F54XsL1c00goZ0001477b@hotmail.com>
Message-ID: <3F94F81D.9030505@pdf.com>

Have you checked "www.r-project.org" -> search -> "R site search"?  I 
just got 15 hits for "polynomial lag".  If you haven't already tried 
this, I'd guess that some of these hits (though certainly not all) might 
help you. 

hope this helps.  spencer graves

Francisco Vergara wrote:

> Does anybody know if there is a built in fuction to use create 
> polynomial distributed lags (sometimed called Almon lag) on linear 
> models?
>
> Thanks
>
> Francisco
>
> _________________________________________________________________

>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help



From jasont at indigoindustrial.co.nz  Tue Oct 21 11:17:59 2003
From: jasont at indigoindustrial.co.nz (Jason Turner)
Date: Tue, 21 Oct 2003 22:17:59 +1300
Subject: [R] run R under linux
In-Reply-To: <Sea2-F5qWcH8A1aWSXA00003059@hotmail.com>
References: <Sea2-F5qWcH8A1aWSXA00003059@hotmail.com>
Message-ID: <3F94F9C7.60506@indigoindustrial.co.nz>

Zhen Pang wrote:
> I want to do 200 simulations. I found during the fisrt 128 simulations, 
> some parameters may be NAs, since I use if (abs(aold-anew)<1e-5) {print 
> (anew)  break} to break the one estimation. 
...
  > I want to know how to resume my program with the seeds saved, and do
> like continueing the 130th one without break. If possible, the results 
> of the first 128 simulations can be saved and combine with the remaining 
> simulation.

help(try)

Cheers

Jason
-- 
Indigo Industrial Controls Ltd.
http://www.indigoindustrial.co.nz
64-21-343-545
jasont at indigoindustrial.co.nz



From grassi at psico.univ.trieste.it  Tue Oct 21 11:23:53 2003
From: grassi at psico.univ.trieste.it (Michele Grassi)
Date: Tue, 21 Oct 2003 11:23:53 +0200 (MEST)
Subject: [R] BEGINNER: please help me to write my  VERY simple function
Message-ID: <200310210923.LAA10484@server.psico.univ.trieste.it>

Hi.
1)I have two variables: call a<-c(e.g.0,3,6,7...)
                           b<-c(e.g.6,8,3,4...)
I want to create a third vector z wich contain the 
pairs values z<-c(0,6,3,8,6,3,7,4....and so on for each 
pairs (a,b)).
There is a specific function?
How can i write my own function?

2)When i try to write a function and then i save it 
like "function.R" file, i try to retrieve it with 
source comand. As result i obtain an error 
message "error in parse: sintax error on line...". I 
apply deparse() and i see an incorrect parsing: how 
avoid unwanted parsing?
Thanks.
Michele.



From jasont at indigoindustrial.co.nz  Tue Oct 21 11:52:33 2003
From: jasont at indigoindustrial.co.nz (Jason Turner)
Date: Tue, 21 Oct 2003 22:52:33 +1300
Subject: [R] BEGINNER: please help me to write my  VERY simple function
In-Reply-To: <200310210923.LAA10484@server.psico.univ.trieste.it>
References: <200310210923.LAA10484@server.psico.univ.trieste.it>
Message-ID: <3F9501E1.2000707@indigoindustrial.co.nz>

Michele Grassi wrote:

> Hi.
> 1)I have two variables: call a<-c(e.g.0,3,6,7...)
>                            b<-c(e.g.6,8,3,4...)
> I want to create a third vector z wich contain the 
> pairs values z<-c(0,6,3,8,6,3,7,4....and so on for each 
> pairs (a,b)).
> There is a specific function?
> How can i write my own function?
> 

For that, you don't need to.

help(expand.grid)


> 2)When i try to write a function and then i save it 
> like "function.R" file, i try to retrieve it with 
> source comand. As result i obtain an error 
> message "error in parse: sintax error on line...". I 
> apply deparse() and i see an incorrect parsing: how 
> avoid unwanted parsing?

I'm really not sure what you're trying to do with deparse here, but I 
don't think it's supposed to do what you meant.  The error message is 
what should be attended to - fix that with your favorite text editor.

Cheers

Jason
-- 
Indigo Industrial Controls Ltd.
http://www.indigoindustrial.co.nz
64-21-343-545
jasont at indigoindustrial.co.nz



From s-plus at wiwi.uni-bielefeld.de  Tue Oct 21 12:12:25 2003
From: s-plus at wiwi.uni-bielefeld.de (Peter Wolf)
Date: Tue, 21 Oct 2003 12:12:25 +0200
Subject: [R] BEGINNER: please help me to write my  VERY simple function
References: <200310210923.LAA10484@server.psico.univ.trieste.it>
Message-ID: <3F950689.7010905@wiwi.uni-bielefeld.de>

Michele Grassi wrote:

>Hi.
>1)I have two variables: call a<-c(e.g.0,3,6,7...)
>                           b<-c(e.g.6,8,3,4...)
>I want to create a third vector z wich contain the 
>pairs values z<-c(0,6,3,8,6,3,7,4....and so on for each 
>pairs (a,b)).
>There is a specific function?
>How can i write my own function?
>
>2)When i try to write a function and then i save it 
>like "function.R" file, i try to retrieve it with 
>source comand. As result i obtain an error 
>message "error in parse: sintax error on line...". I 
>apply deparse() and i see an incorrect parsing: how 
>avoid unwanted parsing?
>Thanks.
>Michele.
>
>______________________________________________
>R-help at stat.math.ethz.ch mailing list
>https://www.stat.math.ethz.ch/mailman/listinfo/r-help
>  
>
1) a simple solution to the problem:
     define (for example):
     gen.pairs <- function(x,y){
          z<-as.vector(rbind(x,y))
          z
     }
    try:
  >   x<-1:10
  >   y<-11:20
  >  gen.pairs(x,y)
 [1]  1 11  2 12  3 13  4 14  5 15  6 16  7 17  8 18  9 19 10 20

2) a) you can write the definition to a file and then use  source this file.
          If the file name is   myfun.R  
              > source("myfun.R")
           will work. But only if there are no errors in the definition
     b) you can type in the code of the definition after the R prompt ">"
     c) you can type in :
              > gen.pairs <- function(x,y) { z }
          and complete the definition by using edit:
              > edit(gen.pairs)
      However, syntax errors are not allowed!   

Peter



From christoph.bier at web.de  Tue Oct 21 12:22:23 2003
From: christoph.bier at web.de (Christoph Bier)
Date: Tue, 21 Oct 2003 12:22:23 +0200
Subject: [R] Graphics overview
Message-ID: <3F9508DF.4000905@web.de>

Hi,

is there an graphics overview, where the graphic capabitlities 
of R are shown with the corresponding code? I already tested 
'demo(graphics)', that isn't that comprehensive, 
'demo(image)', 'demo(lattice)', searched the Mailarchive, 
googled and the FAQ keeps silent, too.
    For example, I know how a special graphic I need should 
look like, but I don't know how to realise it. I even don't 
know how to describe it =).
    Another example, much more simpler (I hope): I want to get 
the sum of the values in a plot above the columns. Like this:

|       3
|  2    _
|  _   | |
| | |  | |
|_|_|__|_|__
    A    B

    RTMFs are welcome =/. But I read 'help(plot)' (plot is 
what I actually use for the graphic above?) and 'help(par)', 
searched my introduction to S and S-Plus and I'm still waiting 
for "Introductory Statistics with R" (P. Dalgaard), that is 
not deliverable at the moment.

TIA

Regards,

Christoph

_______________________
? Data is a data.frame with A and B being the sums of the 
characteristic values (not numeric) of one variable.
-- 
Christoph Bier, Dipl.Oecotroph., Email: bier at wiz.uni-kassel.de
Universitaet Kassel, FG Oekologische Lebensmittelqualitaet und
Ernaehrungskultur \\ Postfach 12 52 \\ 37202 Witzenhausen
Tel.: +49 (0) 55 42 / 98 -17 21, Fax: -17 13



From ripley at stats.ox.ac.uk  Tue Oct 21 12:50:13 2003
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Tue, 21 Oct 2003 11:50:13 +0100 (BST)
Subject: [R] Graphics overview
In-Reply-To: <3F9508DF.4000905@web.de>
Message-ID: <Pine.LNX.4.44.0310211146330.1168-100000@gannet.stats>

On Tue, 21 Oct 2003, Christoph Bier wrote:

> Hi,
> 
> is there an graphics overview, where the graphic capabitlities 
> of R are shown with the corresponding code? I already tested 
> 'demo(graphics)', that isn't that comprehensive, 
> 'demo(image)', 'demo(lattice)', searched the Mailarchive, 
> googled and the FAQ keeps silent, too.
>     For example, I know how a special graphic I need should 
> look like, but I don't know how to realise it. I even don't 
> know how to describe it =).

Chapter 4 of MASS (the book) is a pretty comprehensive set of examples, 
but given that there are lots of plots associated with e.g. multivariate 
analysis (try chapter 11 of MASS) and time series (try chapter 14 of MASS) 
the scope is enormous.

>     Another example, much more simpler (I hope): I want to get 
> the sum of the values in a plot above the columns. Like this:
> 
> |       3
> |  2    _
> |  _   | |
> | | |  | |
> |_|_|__|_|__
>     A    B
> 
>     RTMFs are welcome =/. But I read 'help(plot)' (plot is 
> what I actually use for the graphic above?) and 'help(par)', 

It looks like a barplot to me.

> searched my introduction to S and S-Plus and I'm still waiting 
> for "Introductory Statistics with R" (P. Dalgaard), that is 
> not deliverable at the moment.

There is an example of that in the MASS package script ch04.R
The means to do it are described in `An Introduction to R' (and 
elsewhere).

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From lucas.gonzalez at canarias.org  Tue Oct 21 12:55:09 2003
From: lucas.gonzalez at canarias.org (Lucas Gonzalez Santa Cruz)
Date: Tue, 21 Oct 2003 11:55:09 +0100
Subject: [R] report generator a la epiinfo
Message-ID: <eb509e9a17.e9a17eb509@canarias.org>

Hi

I'd like to use R in epidemiology and disease surveillance.

In EpiInfo you can have a script (.pgm) which calls a predefined report
(.rpt), where a table is calculated and values picked from that table
and placed where the author of the report wants them, with text around
those values. (Please see example below.)

I've looked at manuals, faq, mail-search and google. The closest is an
"R Report Generator" email that looked as if it wasn't followed after a
couple of years.

##The script might have something like this:
read.epiinfo("oswego.rec")
report("oswego.rpt", output="oswego.txt")

##The predefined report might have this:
#{ill}
Exactly {"YES"} people fell ill, and {"NO"} people didn't.
We don't know about the remaining [({}-{"YES"}-{"NO"})*100/{}] percent.
#{icecream ill}
We are specifically interested in the number of people who chose vanilla
and didn't fall ill (all {"VANILLA", "YES"} of them).

Is there anyway to do this with R? Any direction I should look into?

Thanks in advance.

Lucas



From Bernhard.Pfaff at drkw.com  Tue Oct 21 13:04:18 2003
From: Bernhard.Pfaff at drkw.com (Pfaff, Bernhard)
Date: Tue, 21 Oct 2003 13:04:18 +0200
Subject: [R] png() and/or jpeg(): line missing by using box(which="outer")
Message-ID: <18D602BD42B7E24EB810D6454A58DB9004730675@ibfftce505.is.de.dresdnerkb.com>

Dear R list,

I do encounter the following problem by generating either a png-file
(example below) or a jpeg-file: 
By employing 'box(which="outer")' a box is drawn, except for the right line.
If I generate the plot without the 'box(which="outer")', a line at the
bottom in the graphics file still appears. However, both plots are displayed
correctly in the R Graphics Device Window, i.e  with a box including the
right side or one without any lines at the outer margins of the plot. Now, I
want either a file - including the right side of box or one that has none on
all sides. 

test <- rnorm(100)
par(mar=c(6,4,6,4), oma=c(1,1,1,1))
png("test1.png")
plot(test)
grid()
box(which="outer")
box(which="plot")
dev.off()

png("test2.png")
plot(test)
grid()
box(which="plot")
dev.off()

Incidentally, both functions are calling .Internal(devga(....)). I have not
encountered this problem with version R 1.7.1 (for which I used the binary
distribution on CRAN). Now, I have source compiled R 1.8.0. Although,
everything passed 'make check', I am wondering if it could be possible that
'devga.c' or any other necessary file for running png() or jpeg() have not
been compiled 'correctly', or do I have simply to adjust a par()-argument?  

Any pointers or help is appreciated.
 

Bernhard


platform: "i386-pc-mingw32"
arch: "i386"
os: "mingw32"
system: "i386, mingw32"
major: "1"
minor: "8.0"
Windows NT 5.0



--------------------------------------------------------------------------------
The information contained herein is confidential and is intended solely for the
addressee. Access by any other party is unauthorised without the express
written permission of the sender. If you are not the intended recipient, please
contact the sender either via the company switchboard on +44 (0)20 7623 8000, or
via e-mail return. If you have received this e-mail in error or wish to read our
e-mail disclaimer statement and monitoring policy, please refer to 
http://www.drkw.com/disc/email/ or contact the sender.



From Ted.Harding at nessie.mcc.ac.uk  Tue Oct 21 12:27:02 2003
From: Ted.Harding at nessie.mcc.ac.uk ( (Ted Harding))
Date: Tue, 21 Oct 2003 11:27:02 +0100 (BST)
Subject: [R] BEGINNER: please help me to write my  VERY simple functi
In-Reply-To: <200310210923.LAA10484@server.psico.univ.trieste.it>
Message-ID: <XFMail.031021112702.Ted.Harding@nessie.mcc.ac.uk>

On 21-Oct-03 Michele Grassi wrote:
> Hi.
> 1)I have two variables: call a<-c(e.g.0,3,6,7...)
>                            b<-c(e.g.6,8,3,4...)
> I want to create a third vector z wich contain the 
> pairs values z<-c(0,6,3,8,6,3,7,4....and so on for each 
> pairs (a,b)).
> There is a specific function?
> How can i write my own function?

The following will do what you want, though I don't know whether
there is a simpler way to do it.

  z <- rbind(a,b) ; z <- as.vector(z)

For example:

> a <- c(1,3,5,7) ; b <- c(2,4,6,8)
> z <- rbind(a,b) ; z <- as.vector(z) ; z
[1] 1 2 3 4 5 6 7 8

Best wishes,
Ted.



--------------------------------------------------------------------
E-Mail: (Ted Harding) <Ted.Harding at nessie.mcc.ac.uk>
Fax-to-email: +44 (0)870 167 1972
Date: 21-Oct-03                                       Time: 11:27:02
------------------------------ XFMail ------------------------------



From baron at psych.upenn.edu  Tue Oct 21 13:12:45 2003
From: baron at psych.upenn.edu (Jonathan Baron)
Date: Tue, 21 Oct 2003 07:12:45 -0400
Subject: [R] Graphics overview
In-Reply-To: <3F9508DF.4000905@web.de>
References: <3F9508DF.4000905@web.de>
Message-ID: <20031021111245.GA7128@mail2.sas.upenn.edu>

On 10/21/03 12:22, Christoph Bier wrote:
>Hi,
>
>is there an graphics overview, where the graphic capabitlities 
>of R are shown with the corresponding code?

A very elementary overview like this is in our "Notes on R for
psychology experiments and questionnaires," in CRAN "contributed
documents" and in my R page below.  We expanded it a bit from the
even-more elementary version that was there before August.
-- 
Jonathan Baron, Professor of Psychology, University of Pennsylvania
Home page:            http://www.sas.upenn.edu/~baron
R page:               http://finzi.psych.upenn.edu/



From ripley at stats.ox.ac.uk  Tue Oct 21 13:32:05 2003
From: ripley at stats.ox.ac.uk (Prof Brian D Ripley)
Date: Tue, 21 Oct 2003 12:32:05 +0100 (GMT Daylight Time)
Subject: [R] png() and/or jpeg(): line missing by using box(which="outer")
In-Reply-To: <18D602BD42B7E24EB810D6454A58DB9004730675@ibfftce505.is.de.dresdnerkb.com>
Message-ID: <Pine.WNT.4.44.0310211230030.2336-100000@gannet.stats.ox.ac.uk>

It is probably a bug: does it happen when you copy from the screen in png?
I would expect not, hence that may be a workaround for you.

When I have both time and access to a Windows machine I may be able it take
a closer look: meanwhile you do have access and have the source code so
please investigate it yourself and submit a patch.

On Tue, 21 Oct 2003, Pfaff, Bernhard wrote:

> Dear R list,
>
> I do encounter the following problem by generating either a png-file
> (example below) or a jpeg-file:
> By employing 'box(which="outer")' a box is drawn, except for the right line.
> If I generate the plot without the 'box(which="outer")', a line at the
> bottom in the graphics file still appears. However, both plots are displayed
> correctly in the R Graphics Device Window, i.e  with a box including the
> right side or one without any lines at the outer margins of the plot. Now, I
> want either a file - including the right side of box or one that has none on
> all sides.
>
> test <- rnorm(100)
> par(mar=c(6,4,6,4), oma=c(1,1,1,1))
> png("test1.png")
> plot(test)
> grid()
> box(which="outer")
> box(which="plot")
> dev.off()
>
> png("test2.png")
> plot(test)
> grid()
> box(which="plot")
> dev.off()
>
> Incidentally, both functions are calling .Internal(devga(....)). I have not
> encountered this problem with version R 1.7.1 (for which I used the binary
> distribution on CRAN). Now, I have source compiled R 1.8.0. Although,
> everything passed 'make check', I am wondering if it could be possible that
> 'devga.c' or any other necessary file for running png() or jpeg() have not
> been compiled 'correctly', or do I have simply to adjust a par()-argument?
>
> Any pointers or help is appreciated.
>
>
> Bernhard
>
>
> platform: "i386-pc-mingw32"
> arch: "i386"
> os: "mingw32"
> system: "i386, mingw32"
> major: "1"
> minor: "8.0"
> Windows NT 5.0
>
>
>
> --------------------------------------------------------------------------------
> The information contained herein is confidential and is intended solely for the
> addressee. Access by any other party is unauthorised without the express
> written permission of the sender. If you are not the intended recipient, please
> contact the sender either via the company switchboard on +44 (0)20 7623 8000, or
> via e-mail return. If you have received this e-mail in error or wish to read our
> e-mail disclaimer statement and monitoring policy, please refer to
> http://www.drkw.com/disc/email/ or contact the sender.
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
>
>

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272860 (secr)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From nusbj at hotmail.com  Tue Oct 21 13:42:50 2003
From: nusbj at hotmail.com (Zhen Pang)
Date: Tue, 21 Oct 2003 19:42:50 +0800
Subject: [R] run R under linux
Message-ID: <Sea2-F65113teT3i6wO0003082f@hotmail.com>

We are not allowed to submit job directly, so I never type R to use R, just 
make a batch. How can I use try() to correct my codes? In the interactive 
mode, I know how to continue, but now I never enter the R window, where to 
find my results and save seed to continue?



>From: Jason Turner <jasont at indigoindustrial.co.nz>
>To: Zhen Pang <nusbj at hotmail.com>
>CC: R-help at stat.math.ethz.ch
>Subject: Re: [R] run R under linux
>Date: Tue, 21 Oct 2003 22:17:59 +1300
>
>Zhen Pang wrote:
>>I want to do 200 simulations. I found during the fisrt 128 simulations, 
>>some parameters may be NAs, since I use if (abs(aold-anew)<1e-5) {print 
>>(anew)  break} to break the one estimation.
>...
>  > I want to know how to resume my program with the seeds saved, and do
>>like continueing the 130th one without break. If possible, the results of 
>>the first 128 simulations can be saved and combine with the remaining 
>>simulation.
>
>help(try)
>
>Cheers
>
>Jason
>--
>Indigo Industrial Controls Ltd.
>http://www.indigoindustrial.co.nz
>64-21-343-545
>jasont at indigoindustrial.co.nz
>
>______________________________________________
>R-help at stat.math.ethz.ch mailing list
>https://www.stat.math.ethz.ch/mailman/listinfo/r-help



From v.demart at libero.it  Tue Oct 21 15:56:10 2003
From: v.demart at libero.it (Vittorio)
Date: Tue, 21 Oct 2003 13:56:10 +0000
Subject: [R] Strange behaviour
In-Reply-To: <3F92F058.8000108@stat.auckland.ac.nz>
Message-ID: <20031021135610.GA7406@tin.it>

Paul Murrell [r-help] <20/10/03 09:13 +1300>:
> Hi
>..................................................... 
> The "nasty rectangles" are the output of the layout.show() function. 
> This function draws a simple diagram (consisting of nasty rectangles) to 
> indicate the regions that a call to layout() has set up.  It is designed 
> to help users to understand what on earth the layout() function is 
> doing.  (It is NOT a necessary part of setting up an arrangement of 
> plots using the layout() function.)
> 
> I suspect that the author of "simpleR" may have accidentally left the 
> layout.show() call in simple.scatterplot() when copying the example from 
> the layout() help file (apologies to John Verzani if this is an unfair 
> diagnosis).
> 
> So the immediate solution to your problem is to remove the line ...
> 
>     layout.show(nf)
> 
> ... from simple.scatterplot().  The output should then be a single page 
> which should "include" ok in latex.
> 
> The larger problem of how to get at individual pages of output is 
> probably best solved using something like the "onefile" argument to 
> devices.  For example, look at the files produced by ...
> 
>     pdf(onefile=FALSE)
>     example(layout)
> 
> ... and at the help page for pdf() to see more about how to do this.
> 
> Hope that helps
>...............................

Yes, Paul, definitely it helps. Thanks!

I obtained what I wanted. 

Now, I want to control the output of the pdf() command making it write
a specific file chosen by me and not the system. After reading the
help page for the pdf, I was unable to do it.

E.g. I issued
 
onefile<-FALSE
pdf(file=ifelse(onefile,,"vic.pdf")
example(layout)


And I obtained a 5-pages vic.pdf with page 1-4 full of "nasty
rectangles" of any kind and page 5 with the right picture.

Please help

Ciao from Rome - Vittorio



From Subramanian_Karthikeyan at hc-sc.gc.ca  Tue Oct 21 14:01:15 2003
From: Subramanian_Karthikeyan at hc-sc.gc.ca (Subramanian Karthikeyan)
Date: Tue, 21 Oct 2003 08:01:15 -0400
Subject: [R] Type III Sum of Squares Calculation
Message-ID: <OF97DB0065.E6C6DDFB-ON85256DC6.004184CB@hc-sc.gc.ca>

HI All:

Can anyone give me the formulae/steps for calculating the type III sum of
squares for an unbalanced 2-way ANOVA design?  Eg. we are looking at 8
treatments x 4 doses, with unequal numbers of replications within the
groups.  I really need the stepwise calculation, as I would try to put it
in my own code (possibly in Visual Basic) to automate the task.

Thanks very much.

Karth.



From ripley at stats.ox.ac.uk  Tue Oct 21 14:14:37 2003
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Tue, 21 Oct 2003 13:14:37 +0100 (BST)
Subject: [R] Strange behaviour
In-Reply-To: <20031021135610.GA7406@tin.it>
Message-ID: <Pine.LNX.4.44.0310211308510.3865-100000@gannet.stats>

On Tue, 21 Oct 2003, Vittorio wrote:

> Paul Murrell [r-help] <20/10/03 09:13 +1300>:
> > Hi
> >..................................................... 
> > The "nasty rectangles" are the output of the layout.show() function. 
> > This function draws a simple diagram (consisting of nasty rectangles) to 
> > indicate the regions that a call to layout() has set up.  It is designed 
> > to help users to understand what on earth the layout() function is 
> > doing.  (It is NOT a necessary part of setting up an arrangement of 
> > plots using the layout() function.)
> > 
> > I suspect that the author of "simpleR" may have accidentally left the 
> > layout.show() call in simple.scatterplot() when copying the example from 
> > the layout() help file (apologies to John Verzani if this is an unfair 
> > diagnosis).
> > 
> > So the immediate solution to your problem is to remove the line ...
> > 
> >     layout.show(nf)
> > 
> > ... from simple.scatterplot().  The output should then be a single page 
> > which should "include" ok in latex.
> > 
> > The larger problem of how to get at individual pages of output is 
> > probably best solved using something like the "onefile" argument to 
> > devices.  For example, look at the files produced by ...
> > 
> >     pdf(onefile=FALSE)
> >     example(layout)
> > 
> > ... and at the help page for pdf() to see more about how to do this.
> > 
> > Hope that helps
> >...............................
> 
> Yes, Paul, definitely it helps. Thanks!
> 
> I obtained what I wanted. 
> 
> Now, I want to control the output of the pdf() command making it write
> a specific file chosen by me and not the system. After reading the
> help page for the pdf, I was unable to do it.
> 
> E.g. I issued
>  
> onefile<-FALSE
> pdf(file=ifelse(onefile,,"vic.pdf")
That's an error: it has a missing argument and a missing parenthesis.
> example(layout)

Note:

1) onefile is no longer set as an argument to pdf().

2) When you set onefile=FALSE, you will only get the last plot in your
file unless you give a file name of the type described on the help page.

3) *You* plotted the `nasty rectangles', so why are you aruprised you got 
them in the file?  If you don't want them, don't plot them!

> And I obtained a 5-pages vic.pdf with page 1-4 full of "nasty
> rectangles" of any kind and page 5 with the right picture.
> 
> Please help

Please follow more carefully the help you have already been given.

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From cdeclercq at nordnet.fr  Tue Oct 21 15:21:07 2003
From: cdeclercq at nordnet.fr (Christophe Declercq)
Date: Tue, 21 Oct 2003 14:21:07 +0100
Subject: [R] report generator a la epiinfo
In-Reply-To: <eb509e9a17.e9a17eb509@canarias.org>
Message-ID: <NGBBKLJCOLPAFMJIEMHCMEBMCMAA.cdeclercq@nordnet.fr>

Hi, Lucas

You should try Sweave in the 'tools' package (see
http://www.ci.tuwien.ac.at/~leisch/Sweave/).

You will have to get a TeX/LaTeX distribution and learn a little of LaTeX
but it is worth the effort.

I frequently use R with Sweave on EpiData files (http://www.epidata.dk/)
with great success.

Hope it helps.

Christophe
--
Christophe DECLERCQ, MD
Observatoire R?gional de la Sant? Nord-Pas-de-Calais
13, rue Faidherbe 59046 LILLE Cedex FRANCE
Phone +33 3 20 15 49 24
Fax   +33 3 20 55 92 30
E-mail c.declercq at orsnpdc.org


> -----Message d'origine-----
> De : r-help-bounces at stat.math.ethz.ch
> [mailto:r-help-bounces at stat.math.ethz.ch]De la part de Lucas Gonzalez
> Santa Cruz
> Envoy? : mardi 21 octobre 2003 11:55
> ? : r-help at stat.math.ethz.ch
> Objet : [R] report generator a la epiinfo
>
>
> Hi
>
> I'd like to use R in epidemiology and disease surveillance.
>
> In EpiInfo you can have a script (.pgm) which calls a predefined report
> (.rpt), where a table is calculated and values picked from that table
> and placed where the author of the report wants them, with text around
> those values. (Please see example below.)
>
> I've looked at manuals, faq, mail-search and google. The closest is an
> "R Report Generator" email that looked as if it wasn't followed after a
> couple of years.
>
> ##The script might have something like this:
> read.epiinfo("oswego.rec")
> report("oswego.rpt", output="oswego.txt")
>
> ##The predefined report might have this:
> #{ill}
> Exactly {"YES"} people fell ill, and {"NO"} people didn't.
> We don't know about the remaining [({}-{"YES"}-{"NO"})*100/{}] percent.
> #{icecream ill}
> We are specifically interested in the number of people who chose vanilla
> and didn't fall ill (all {"VANILLA", "YES"} of them).
>
> Is there anyway to do this with R? Any direction I should look into?
>
> Thanks in advance.
>
> Lucas
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
>



From christoph.bier at web.de  Tue Oct 21 14:28:34 2003
From: christoph.bier at web.de (Christoph Bier)
Date: Tue, 21 Oct 2003 14:28:34 +0200
Subject: [R] Graphics overview
In-Reply-To: <Pine.LNX.4.44.0310211146330.1168-100000@gannet.stats>
References: <Pine.LNX.4.44.0310211146330.1168-100000@gannet.stats>
Message-ID: <3F952672.5000302@web.de>

Prof Brian Ripley wrote:

> Chapter 4 of MASS (the book) is a pretty comprehensive set of examples, 
> but given that there are lots of plots associated with e.g. multivariate 
> analysis (try chapter 11 of MASS) and time series (try chapter 14 of MASS) 
> the scope is enormous.

I ordered it right now in our library.

> 
>>    Another example, much more simpler (I hope): I want to get 
>>the sum of the values in a plot above the columns. Like this:
>>
>>|       3
>>|  2    _
>>|  _   | |
>>| | |  | |
>>|_|_|__|_|__
>>    A    B
>>
>>    RTMFs are welcome =/. But I read 'help(plot)' (plot is 
>>what I actually use for the graphic above?) and 'help(par)', 
> 
> 
> It looks like a barplot to me.

It's realised (without the sum of the values above the 
coliumns) via

 > attach(data.frame)
 > plot(variable.from.data.frame)

...

>>searched my introduction to S and S-Plus and I'm still waiting 
>>for "Introductory Statistics with R" (P. Dalgaard), that is 
>>not deliverable at the moment.
> 
> 
> There is an example of that in the MASS package script ch04.R
> The means to do it are described in `An Introduction to R' (and 
> elsewhere).

I had a look at this script on my machine and have a print 
version of "An Introduction to R". Maybe I find out what to do 
with such scripts.

Thanks for your answer!

Best regards,

Christoph
-- 
Christoph Bier, Dipl.Oecotroph., Email: bier at wiz.uni-kassel.de
Universitaet Kassel, FG Oekologische Lebensmittelqualitaet und
Ernaehrungskultur \\ Postfach 12 52 \\ 37202 Witzenhausen
Tel.: +49 (0) 55 42 / 98 -17 21, Fax: -17 13



From p.dalgaard at biostat.ku.dk  Tue Oct 21 14:33:05 2003
From: p.dalgaard at biostat.ku.dk (Peter Dalgaard)
Date: 21 Oct 2003 14:33:05 +0200
Subject: [R] Graphics overview
In-Reply-To: <Pine.LNX.4.44.0310211146330.1168-100000@gannet.stats>
References: <Pine.LNX.4.44.0310211146330.1168-100000@gannet.stats>
Message-ID: <x21xt6ixdq.fsf@biostat.ku.dk>

Prof Brian Ripley <ripley at stats.ox.ac.uk> writes:

> On Tue, 21 Oct 2003, Christoph Bier wrote:
> 
> > Hi,
> > 
> > is there an graphics overview, where the graphic capabitlities 
> > of R are shown with the corresponding code? I already tested 
> > 'demo(graphics)', that isn't that comprehensive, 
> > 'demo(image)', 'demo(lattice)', searched the Mailarchive, 
> > googled and the FAQ keeps silent, too.
> >     For example, I know how a special graphic I need should 
> > look like, but I don't know how to realise it. I even don't 
> > know how to describe it =).
> 
> Chapter 4 of MASS (the book) is a pretty comprehensive set of examples, 
> but given that there are lots of plots associated with e.g. multivariate 
> analysis (try chapter 11 of MASS) and time series (try chapter 14 of MASS) 
> the scope is enormous.
> 
> >     Another example, much more simpler (I hope): I want to get 
> > the sum of the values in a plot above the columns. Like this:
> > 
> > |       3
> > |  2    _
> > |  _   | |
> > | | |  | |
> > |_|_|__|_|__
> >     A    B
> > 
> >     RTMFs are welcome =/. But I read 'help(plot)' (plot is 
> > what I actually use for the graphic above?) and 'help(par)', 

(Read the muckin' *what*?? ;-) )
 
> It looks like a barplot to me.
> 
> > searched my introduction to S and S-Plus and I'm still waiting 
> > for "Introductory Statistics with R" (P. Dalgaard), that is 
> > not deliverable at the moment.

That book tries rather hard to show only the basic procedure and
not to do fancy things, so this is not explicitly covered in there. It
does describe barplot() and text(), though. (Odd, BTW, www.springer.de
says it ships within 3 days).
 
> There is an example of that in the MASS package script ch04.R
> The means to do it are described in `An Introduction to R' (and 
> elsewhere).

Also, try 

par(ask=T); example(barplot)

The fourth example is fairly close to what you want to do (colSums
instead of colMeans should place the numbers at the end of the
columns). 

-- 
   O__  ---- Peter Dalgaard             Blegdamsvej 3  
  c/ /'_ --- Dept. of Biostatistics     2200 Cph. N   
 (*) \(*) -- University of Copenhagen   Denmark      Ph: (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)             FAX: (+45) 35327907



From christoph.bier at web.de  Tue Oct 21 14:38:33 2003
From: christoph.bier at web.de (Christoph Bier)
Date: Tue, 21 Oct 2003 14:38:33 +0200
Subject: [R] Graphics overview
In-Reply-To: <20031021111245.GA7128@mail2.sas.upenn.edu>
References: <3F9508DF.4000905@web.de>
	<20031021111245.GA7128@mail2.sas.upenn.edu>
Message-ID: <3F9528C9.4030702@web.de>

Jonathan Baron schrieb:
> On 10/21/03 12:22, Christoph Bier wrote:
> 
>>Hi,
>>
>>is there an graphics overview, where the graphic capabitlities 
>>of R are shown with the corresponding code?
> 
> 
> A very elementary overview like this is in our "Notes on R for
> psychology experiments and questionnaires," in CRAN "contributed
> documents" and in my R page below.  We expanded it a bit from the
> even-more elementary version that was there before August.

I can't find any graphics neither in the document nor on your 
webpage. Maybe a missunderstanding what I'm looking for.

Thanks anyway!

Best regards,

Christoph
-- 
Christoph Bier, Dipl.Oecotroph., Email: bier at wiz.uni-kassel.de
Universitaet Kassel, FG Oekologische Lebensmittelqualitaet und
Ernaehrungskultur \\ Postfach 12 52 \\ 37202 Witzenhausen
Tel.: +49 (0) 55 42 / 98 -17 21, Fax: -17 13



From csaintje at univ-lr.fr  Tue Oct 21 14:53:37 2003
From: csaintje at univ-lr.fr (Christophe Saint-Jean)
Date: Tue, 21 Oct 2003 14:53:37 +0200
Subject: [R] R and Arcgis through VBA
Message-ID: <3F952C51.9080707@univ-lr.fr>

Dear R experts,
  I am trying to use R with Arcgis Desktop 8.1.
  When i try to add a "StatConnectorGraphicsDevice" control to my form, VBA returns an specified error and nothing else.
  Does anybody has a successful experience with Arcgis and R ?
Thanks,
Christophe Saint-Jean.



From baron at psych.upenn.edu  Tue Oct 21 14:57:12 2003
From: baron at psych.upenn.edu (Jonathan Baron)
Date: Tue, 21 Oct 2003 08:57:12 -0400
Subject: [R] Graphics overview
In-Reply-To: <3F9528C9.4030702@web.de>
References: <3F9508DF.4000905@web.de>
	<20031021111245.GA7128@mail2.sas.upenn.edu>
	<3F9528C9.4030702@web.de>
Message-ID: <20031021125712.GA19758@mail2.sas.upenn.edu>

On 10/21/03 14:38, Christoph Bier wrote:
>Jonathan Baron schrieb:
>> A very elementary overview like this is in our "Notes on R for
>> psychology experiments and questionnaires," in CRAN "contributed
>> documents" and in my R page below.  We expanded it a bit from the
>> even-more elementary version that was there before August.
>
>I can't find any graphics neither in the document nor on your 
>webpage. Maybe a missunderstanding what I'm looking for.

Perhaps.  I'm sorry.  I was referring to the chapter on
graphics.  Specifically
http://www.psych.upenn.edu/~baron/rpsych/rpsych.html#SECTION00060000000000000000

Although this isn't what you wanted, it might be useful to
someone else who wants a "graphics overview" (the title of your
post).

Jon
-- 
Jonathan Baron, Professor of Psychology, University of Pennsylvania
Home page:            http://www.sas.upenn.edu/~baron
R page:               http://finzi.psych.upenn.edu/



From christoph.bier at web.de  Tue Oct 21 15:03:21 2003
From: christoph.bier at web.de (Christoph Bier)
Date: Tue, 21 Oct 2003 15:03:21 +0200
Subject: [R] Graphics overview
In-Reply-To: <x21xt6ixdq.fsf@biostat.ku.dk>
References: <Pine.LNX.4.44.0310211146330.1168-100000@gannet.stats>
	<x21xt6ixdq.fsf@biostat.ku.dk>
Message-ID: <3F952E99.1010908@web.de>

Peter Dalgaard wrote:
>>On Tue, 21 Oct 2003, Christoph Bier wrote:

[...]

>>>    RTMFs are welcome =/. But I read 'help(plot)' (plot is 
>>>what I actually use for the graphic above?) and 'help(par)', 
> 
> 
> (Read the muckin' *what*?? ;-) )

Oops :-D

[...]

> (Odd, BTW, www.springer.de says it ships within 3 days).

And our local book store said, that it's not deliverable. So
my colleague tried www.amazon.de, that says it ships within
11--12 days. We are still waiting ...

> par(ask=T); example(barplot)

Nice!

> The fourth example is fairly close to what you want to do (colSums
> instead of colMeans should place the numbers at the end of the
> columns). 

Yes, it is, thanks! But it seems only to work with arrays as
VADeaths. I don't have an array but a data.frame. And mainly
I'm a naive newbie, that gets more confused the more he wants
from R =(.

Best regards,

Christoph
-- 
Christoph Bier, Dipl.Oecotroph., Email: bier at wiz.uni-kassel.de
Universitaet Kassel, FG Oekologische Lebensmittelqualitaet und
Ernaehrungskultur \\ Postfach 12 52 \\ 37202 Witzenhausen
Tel.: +49 (0) 55 42 / 98 -17 21, Fax: -17 13



From andy_liaw at merck.com  Tue Oct 21 15:07:13 2003
From: andy_liaw at merck.com (Liaw, Andy)
Date: Tue, 21 Oct 2003 09:07:13 -0400
Subject: [R] nnet behaving oddly
Message-ID: <3A822319EB35174CA3714066D590DCD50205CD06@usrymx25.merck.com>

> From: Rajarshi Guha [mailto:rxg218 at psu.edu] 
> 
> Hi,
>   I was trying to use the nnet library and am not sure of 
> whats going on. I am calling the nnet function as:
> 
>  n <- nnet(x,y,size=3,subset=sets[[1]], maxit=200)

Please give us output of something like:

str(x)
summary(y)

Also, I believe the subset argument is only meant for calls via 
formula; e.g.,

nnet(y ~ x, ...)

and needs to be a vector of logicals the same length as the number
of rows in x, indicating which rows to include in the fitting.
Please also tell us what sets[[1]] is.

Andy
 
> Where x is a 272x4 matrix of observations (examples) and y is 
> a 272x1 matrix of target values. However when I look at 
> nnet$residuals they are off by two orders of magnitude 
> (compared to the output from neural network code that I 
> already have). Looking at nnet$fitted.values shows all the 
> values to be 1 (whereas my target values range from 0 to 150).
> 
> Am I making an obvious mistake in the way I'm calling the 
> function? Is the fact that n$fitted.values is all 1's 
> indicating that the NN is doing a classification? If so how 
> can I make it do quantitation?
> 
> The man page mentions that if the response is a factor then 
> it defaults to quantitation. However my y matrix just contain 
> numbers - so it should'nt be doing classification.
> 
> Any pointers would be appreciated.
> 
> Thanks,
> 
> -------------------------------------------------------------------
> Rajarshi Guha <rxg218 at psu.edu> <http://jijo.cjb.net>
> GPG Fingerprint: 0CCA 8EE2 2EEB 25E2 AB04 06F7 1BB9 E634 9B87 56EE
> -------------------------------------------------------------------
> Psychology is merely producing habits out of rats.
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list 
> https://www.stat.math.ethz.ch/mailman/listinfo> /r-help
>



From jfox at mcmaster.ca  Tue Oct 21 15:13:09 2003
From: jfox at mcmaster.ca (John Fox)
Date: Tue, 21 Oct 2003 09:13:09 -0400
Subject: [R] Type III Sum of Squares Calculation
In-Reply-To: <OF97DB0065.E6C6DDFB-ON85256DC6.004184CB@hc-sc.gc.ca>
Message-ID: <5.1.0.14.2.20031021091045.01fc0390@127.0.0.1>

Dear Karth,

The Anova function in the car package can calculate "type-III" sums of 
squares, though it doesn't do so by default. Be careful with the contrast 
coding or you will get nonsense (and you might want to think about whether 
you really want type-III SSs).

I hope that this helps,
  John

At 08:01 AM 10/21/2003 -0400, Subramanian Karthikeyan wrote:
>HI All:
>
>Can anyone give me the formulae/steps for calculating the type III sum of
>squares for an unbalanced 2-way ANOVA design?  Eg. we are looking at 8
>treatments x 4 doses, with unequal numbers of replications within the
>groups.  I really need the stepwise calculation, as I would try to put it
>in my own code (possibly in Visual Basic) to automate the task.

-----------------------------------------------------
John Fox
Department of Sociology
McMaster University
Hamilton, Ontario, Canada L8S 4M4
email: jfox at mcmaster.ca
phone: 905-525-9140x23604
web: www.socsci.mcmaster.ca/jfox



From Roger.Bivand at nhh.no  Tue Oct 21 15:18:09 2003
From: Roger.Bivand at nhh.no (Roger Bivand)
Date: Tue, 21 Oct 2003 15:18:09 +0200 (CEST)
Subject: [R] R and Arcgis through VBA
In-Reply-To: <3F952C51.9080707@univ-lr.fr>
Message-ID: <Pine.LNX.4.44.0310211511450.16119-100000@reclus.nhh.no>

Christophe:

I suggest moving this discussion from r-help to r-sig-geo, there will be a 
higher density of people there who may have had successful experiences. 
There are so many different configuration issues that might impact this, 
that r-help is too broad a forum. I have also CC-ed this to the R(D)COM 
list, which is also relevant. Can you first confirm that you can use the 
StatConnector with say Excel?

Roger

On Tue, 21 Oct 2003, Christophe Saint-Jean wrote:

> Dear R experts,
>   I am trying to use R with Arcgis Desktop 8.1.
>   When i try to add a "StatConnectorGraphicsDevice" control to my form,
> VBA returns an specified error and nothing else.
>   Does anybody has a successful experience with Arcgis and R ?
> Thanks,
> Christophe Saint-Jean.
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> 

-- 
Roger Bivand
Economic Geography Section, Department of Economics, Norwegian School of
Economics and Business Administration, Breiviksveien 40, N-5045 Bergen,
Norway. voice: +47 55 95 93 55; fax +47 55 95 93 93
e-mail: Roger.Bivand at nhh.no



From krcabrer at unalmed.edu.co  Tue Oct 21 15:35:08 2003
From: krcabrer at unalmed.edu.co (Kenneth Cabrera)
Date: Tue, 21 Oct 2003 08:35:08 -0500
Subject: [R] Indicator Kriging
Message-ID: <oprxeaouj4faouaq@200.24.8.4>

Hi R-Users

Is there any package in R that work Indicator Kriging?
Or somebody is working in a link between GSLIB library in
FORTRAN for R?

Thank you very much for your help.
--



From claus-peter.gwiggner at eurocontrol.int  Tue Oct 21 15:52:48 2003
From: claus-peter.gwiggner at eurocontrol.int (GWIGGNER Claus-Peter (EXT))
Date: Tue, 21 Oct 2003 15:52:48 +0200
Subject: [R] Lines between coordinates
Message-ID: <CD4F7F9A2B7DD41194B900508BB0B8DD05408714@agnfr02.eurocontrol.fr>

Hello,

Given x1, ..., xn and y1, ..., yn I'd like to draw n lines between xi,yi.
The xi, yi shoulfd be 2-D coordinates.
 
What is an elegant solution?
Thanks.

____

This message and any files transmitted with it are legally p...{{dropped}}



From andy_liaw at merck.com  Tue Oct 21 15:57:29 2003
From: andy_liaw at merck.com (Liaw, Andy)
Date: Tue, 21 Oct 2003 09:57:29 -0400
Subject: [R] Lines between coordinates
Message-ID: <3A822319EB35174CA3714066D590DCD50205CD09@usrymx25.merck.com>

I suspect you are looking for segments().

Andy

> From: GWIGGNER Claus-Peter (EXT) 
> 
> Hello,
> 
> Given x1, ..., xn and y1, ..., yn I'd like to draw n lines 
> between xi,yi. The xi, yi shoulfd be 2-D coordinates.
>  
> What is an elegant solution?
> Thanks.
> 
> ____
> 
> This message and any files transmitted with it are legally 
> p...{{dropped}}
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list 
> https://www.stat.math.ethz.ch/mailman/listinfo> /r-help
>



From tlumley at u.washington.edu  Tue Oct 21 16:39:00 2003
From: tlumley at u.washington.edu (Thomas Lumley)
Date: Tue, 21 Oct 2003 07:39:00 -0700 (PDT)
Subject: [R] Polynomial lags
In-Reply-To: <3F94F81D.9030505@pdf.com>
References: <Law14-F54XsL1c00goZ0001477b@hotmail.com>
	<3F94F81D.9030505@pdf.com>
Message-ID: <Pine.A41.4.58.0310210732200.22232@homer30.u.washington.edu>

On Tue, 21 Oct 2003, Spencer Graves wrote:

> Have you checked "www.r-project.org" -> search -> "R site search"?  I
> just got 15 hits for "polynomial lag".  If you haven't already tried
> this, I'd guess that some of these hits (though certainly not all) might
> help you.
>

Only one of these is about polynomial distributed lag models, and it's an
apparently unsuccessful request for information.

I have code
	http://faculty.washington.edu/tlumley/pdl.R
and documentation
	http://faculty.washington.edu/tlumley/pdlglm.html
for a version for generalised linear models.

Note that I have not used this for some years, so it may run into problems
with changes in R.  Also note that it is just a generalised linear model
-- it doesn't do anything about residual autocorrelation (which isn't a
problem in the application I was working on).


	-thomas



From roger at ysidro.econ.uiuc.edu  Tue Oct 21 16:57:10 2003
From: roger at ysidro.econ.uiuc.edu (Roger Koenker)
Date: Tue, 21 Oct 2003 09:57:10 -0500 (CDT)
Subject: [R] Polynomial lags
In-Reply-To: <Pine.A41.4.58.0310210732200.22232@homer30.u.washington.edu>
Message-ID: <Pine.SOL.4.30.0310210951300.16018-100000@ysidro.econ.uiuc.edu>

For what it is worth, I would have thought that expressing
the lag coefficients in a B-spline expansion would be preferable
to going back to Almon approach. This would give a relatively
simple lm() application.


url:	www.econ.uiuc.edu/~roger/my.html	Roger Koenker
email	rkoenker at uiuc.edu			Department of Economics
vox: 	217-333-4558				University of Illinois
fax:   	217-244-6678				Champaign, IL 61820

On Tue, 21 Oct 2003, Thomas Lumley wrote:

> On Tue, 21 Oct 2003, Spencer Graves wrote:
>
> > Have you checked "www.r-project.org" -> search -> "R site search"?  I
> > just got 15 hits for "polynomial lag".  If you haven't already tried
> > this, I'd guess that some of these hits (though certainly not all) might
> > help you.
> >
>
> Only one of these is about polynomial distributed lag models, and it's an
> apparently unsuccessful request for information.
>
> I have code
> 	http://faculty.washington.edu/tlumley/pdl.R
> and documentation
> 	http://faculty.washington.edu/tlumley/pdlglm.html
> for a version for generalised linear models.
>
> Note that I have not used this for some years, so it may run into problems
> with changes in R.  Also note that it is just a generalised linear model
> -- it doesn't do anything about residual autocorrelation (which isn't a
> problem in the application I was working on).
>
>
> 	-thomas
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
>



From tord.snall at ebc.uu.se  Tue Oct 21 17:22:36 2003
From: tord.snall at ebc.uu.se (Tord Snall)
Date: Tue, 21 Oct 2003 17:22:36 +0200
Subject: [R] code efficiency, extr. info from list
Message-ID: <3.0.6.32.20031021172236.00ca55e8@mail.anst.uu.se>

Dear all, 
I try extracting information from a list with several levels, but I would
be happy for recommendation on writing more efficient code:

> h0<- seq(0,100, by = 20); expo<- seq(0.1, 0.5, l = 5)
> grid<- expand.grid(h0, expo)
> test<- apply(grid, 1, pcp, point.data = as.points(dat[,c("x","y")]),
poly.data = studyarea)

> test[1]
$"1"
$"1"$par
          s2          rho 
1.815343e-06 2.358788e-02 

$"1"$value
[1] 144.346

$"1"$counts
function gradient 
      65       NA 

$"1"$convergence
[1] 0

$"1"$message
NULL

I want to put the results together:
val<- c(test[[1]]$value, test[[2]]$value, test[[3]]$value, test[[4]]$value...)
s2<- c(test[[1]]$par[1], test[[2]]$par[1], test[[3]]$par[1],
test[[4]]$par[1]...)
rho<- ...
funct<- ....
grad<- 
.

useful.df<- as.data.frame(cbind(val, s2....), F)

However, as you can see 
> dim(grid)
[1] 30  2

the call rows 

val<- c(test[[1]]$value, test[[2]]$value, test[[3]]$value,
test[[4]]$value.......)
etc.

will be long.

I would thus be happy for help with writing this code more efficient (and I
know will benefit from this knowing how to do this in the future).


Thanks in advance!

Sincerely,
Tord

-----------------------------------------------------------------------
Tord Sn?ll
Avd. f v?xtekologi, Evolutionsbiologiskt centrum, Uppsala universitet
Dept. of Plant Ecology, Evolutionary Biology Centre, Uppsala University
Villav?gen 14			
SE-752 36 Uppsala, Sweden
Tel: 018-471 28 82 (int +46 18 471 28 82) (work)
Tel: 018-25 71 33 (int +46 18 25 71 33) (home)
Fax: 018-55 34 19 (int +46 18 55 34 19) (work)
E-mail: Tord.Snall at ebc.uu.se
Check this: http://www.vaxtbio.uu.se/resfold/snall.htm!



From john.gavin at ubs.com  Tue Oct 21 17:44:35 2003
From: john.gavin at ubs.com (john.gavin@ubs.com)
Date: Tue, 21 Oct 2003 16:44:35 +0100
Subject: [R] summary - controling x-labels in xyplot (lattice) when x is
	POSIX object
Message-ID: <012821F286ED1D4ABDC72F9E1DD63D0C01FF964E@NLDNC003PEX1.ubsgs.ubsgroup.net>

Hi,

The solution to my problem is to use 
lattice:::calculateAxisComponents to calculate appropriate labels
for the time axis in trellis plots.

# For example, given

x <- seq.POSIXt(strptime("2003/01/01", format = "%Y/%m/%d"),
                strptime("2003/10/01", format = "%Y/%m/%d"), by = "month")
y <- rnorm(length(x))
dat <- data.frame(x= x, y = y)

# the code 

xyplot(y ~ x, data = dat, type = "b")

# could be replaced with

labels <- lattice:::calculateAxisComponents(x)
labels$at <- ISOdate(1970,01,01) + as.numeric(x)
xyplot(y ~ x, data = dat, type = "b",
       scales = list(x = list(at = labels$at, labels = labels$labels)))

# to get the effect that I want.

This is essentially what I used to do (< 1.8.0) 
but the ':::' operator is now required.
Also, the 'at' component must be of class "POSIXt" rather than numeric,
as was the case before.

Thanks to Deepayan Sarkar <deepayan at stat.wisc.edu> and
Martin Maechler <maechler at stat.math.ethz.ch>.

Regards,

John.

John Gavin <john.gavin at ubs.com>,
Quantitative Risk Models and Statistics,
UBS Investment Bank, 6th floor, 
100 Liverpool St., London EC2M 2RH, UK.
Phone +44 (0) 207 567 4289
Fax   +44 (0) 207 568 5352

Date: Mon, 20 Oct 2003 18:35:43 +0100
From: <john.gavin at ubs.com>
Subject: [R] controling x-labels in xyplot (lattice) when x is POSIX object
To: <r-help at stat.math.ethz.ch>

Hi,

V1.8.0 seems to allow DateTimeClasses as the x argument in xyplots (lattice).
For example:

x <- seq.POSIXt(strptime("2003/01/01", format = "%Y/%m/%d"),
                strptime("2003/10/01", format = "%Y/%m/%d"), by = "month")
y <- rnorm(length(x))
dat <- data.frame(x= x, y = y)
xyplot(y ~ x, data = dat, type = "b")

However, the labelling for the x-axis is not what I want.
(I see only one tick mark and one label ('Oct').)
What is the recommended way to relabel the x-axis?
Ideally, I want to see several months (3-6) labelled along the x-axis.

Previously, I used 'calculateAxisComponents' to massage the labels manually
but that function (which I realise was internal to lattice) is no longer available.

I am on Windows XP, R 1.8.0.

Regards,

John.


Visit our website at http://www.ubs.com

This message contains confidential information and is intend...{{dropped}}



From rksh at soc.soton.ac.uk  Tue Oct 21 17:48:27 2003
From: rksh at soc.soton.ac.uk (Robin Hankin)
Date: Tue, 21 Oct 2003 16:48:27 +0100
Subject: [R] do.call() and aperm()
Message-ID: <a06002003bbbb058f49db@[139.166.242.29]>

Hi everyone

I've been playing with do.call() but I'm having problems understanding it.

I have a list of "n" elements, each one of which is "d" dimensional
[actually an n-by-n-by ... by-n array].  Neither n nor d is known in
advance.  I want to bind the elements together in a higher-dimensional
array.

Toy example follows with d=n=3.

f <- function(n){array(n,c(3,3,3))}
x <-  sapply(1:3,f,simplify=FALSE)

Then what I want is

ans <- abind(x[[1]] , x[[2]] , x[[3]]  , along=4)

[abind() is defined in library(abind)].

Note that dim(ans) is c(3,3,3,3), as required.

PROBLEM: how do I do tell do.call() that I want to give abind() the
extra argument along=4 (in general, I want
along=length(dim(x[[1]]))+1)?


Oblig Attempt:

jj <- function(...){abind(... , along=4)}
do.call("jj" , x)

This works, because I know that d=3 (and therefore use along=4), but
it doesn't generalize easily to arbitrary d.  I'm clearly missing
something basic.  Anyone?



From anielsen at math.ku.dk  Tue Oct 21 17:51:37 2003
From: anielsen at math.ku.dk (Anders Nielsen)
Date: Tue, 21 Oct 2003 17:51:37 +0200 (CEST)
Subject: [R] code efficiency, extr. info from list
In-Reply-To: <3.0.6.32.20031021172236.00ca55e8@mail.anst.uu.se>
Message-ID: <Pine.LNX.4.40.0310211749350.25403-100000@shannon.math.ku.dk>


Try using lapply()

For instance like:

val<-unlist(lapply(test, function(x)x$value))

You can also extend this by having your function return
everything you need from the list.

Cheers,

Anders.


On Tue, 21 Oct 2003, Tord Snall wrote:

> Dear all,
> I try extracting information from a list with several levels, but I would
> be happy for recommendation on writing more efficient code:
>
> > h0<- seq(0,100, by = 20); expo<- seq(0.1, 0.5, l = 5)
> > grid<- expand.grid(h0, expo)
> > test<- apply(grid, 1, pcp, point.data = as.points(dat[,c("x","y")]),
> poly.data = studyarea)
>
> > test[1]
> $"1"
> $"1"$par
>           s2          rho
> 1.815343e-06 2.358788e-02
>
> $"1"$value
> [1] 144.346
>
> $"1"$counts
> function gradient
>       65       NA
>
> $"1"$convergence
> [1] 0
>
> $"1"$message
> NULL
>
> I want to put the results together:
> val<- c(test[[1]]$value, test[[2]]$value, test[[3]]$value, test[[4]]$value...)
> s2<- c(test[[1]]$par[1], test[[2]]$par[1], test[[3]]$par[1],
> test[[4]]$par[1]...)
> rho<- ...
> funct<- ....
> grad<-
> .
>
> useful.df<- as.data.frame(cbind(val, s2....), F)
>
> However, as you can see
> > dim(grid)
> [1] 30  2
>
> the call rows
>
> val<- c(test[[1]]$value, test[[2]]$value, test[[3]]$value,
> test[[4]]$value.......)
> etc.
>
> will be long.
>
> I would thus be happy for help with writing this code more efficient (and I
> know will benefit from this knowing how to do this in the future).
>
>
> Thanks in advance!
>
> Sincerely,
> Tord
>
> -----------------------------------------------------------------------
> Tord Sn?ll
> Avd. f v?xtekologi, Evolutionsbiologiskt centrum, Uppsala universitet
> Dept. of Plant Ecology, Evolutionary Biology Centre, Uppsala University
> Villav?gen 14
> SE-752 36 Uppsala, Sweden
> Tel: 018-471 28 82 (int +46 18 471 28 82) (work)
> Tel: 018-25 71 33 (int +46 18 25 71 33) (home)
> Fax: 018-55 34 19 (int +46 18 55 34 19) (work)
> E-mail: Tord.Snall at ebc.uu.se
> Check this: http://www.vaxtbio.uu.se/resfold/snall.htm!
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
>
>



From bill.shipley at usherbrooke.ca  Tue Oct 21 17:58:35 2003
From: bill.shipley at usherbrooke.ca (Bill Shipley)
Date: Tue, 21 Oct 2003 11:58:35 -0400
Subject: [R] explaining curious result of aov
Message-ID: <01b901c397ec$2ee12950$8d1ad284@BIO041>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20031021/6966c1a5/attachment.pl

From tplate at blackmesacapital.com  Tue Oct 21 18:10:55 2003
From: tplate at blackmesacapital.com (Tony Plate)
Date: Tue, 21 Oct 2003 10:10:55 -0600
Subject: [R] do.call() and aperm()
In-Reply-To: <a06002003bbbb058f49db@[139.166.242.29]>
Message-ID: <5.2.1.1.2.20031021100813.040b08a8@mailhost.blackmesacapital.com>

 > do.call("abind" c(list.of.arrays, list(along=4)))

This reminds me that I had been meaning to submit an enhancement of abind() 
that allows the first argument to be a list of arrays so that you could 
simply do abind(list.of.arrays, along=4), as I find this is a very common 
pattern.

-- Tony Plate


At Tuesday 04:48 PM 10/21/2003 +0100, Robin Hankin wrote:
>Hi everyone
>
>I've been playing with do.call() but I'm having problems understanding it.
>
>I have a list of "n" elements, each one of which is "d" dimensional
>[actually an n-by-n-by ... by-n array].  Neither n nor d is known in
>advance.  I want to bind the elements together in a higher-dimensional
>array.
>
>Toy example follows with d=n=3.
>
>f <- function(n){array(n,c(3,3,3))}
>x <-  sapply(1:3,f,simplify=FALSE)
>
>Then what I want is
>
>ans <- abind(x[[1]] , x[[2]] , x[[3]]  , along=4)
>
>[abind() is defined in library(abind)].
>
>Note that dim(ans) is c(3,3,3,3), as required.
>
>PROBLEM: how do I do tell do.call() that I want to give abind() the
>extra argument along=4 (in general, I want
>along=length(dim(x[[1]]))+1)?
>
>
>Oblig Attempt:
>
>jj <- function(...){abind(... , along=4)}
>do.call("jj" , x)
>
>This works, because I know that d=3 (and therefore use along=4), but
>it doesn't generalize easily to arbitrary d.  I'm clearly missing
>something basic.  Anyone?
>
>______________________________________________
>R-help at stat.math.ethz.ch mailing list
>https://www.stat.math.ethz.ch/mailman/listinfo/r-help



From tplate at blackmesacapital.com  Tue Oct 21 18:17:05 2003
From: tplate at blackmesacapital.com (Tony Plate)
Date: Tue, 21 Oct 2003 10:17:05 -0600
Subject: [R] do.call() and aperm()
In-Reply-To: <a06002003bbbb058f49db@[139.166.242.29]>
Message-ID: <5.2.1.1.2.20031021101329.040e2400@mailhost.blackmesacapital.com>

I've also been thinking about how to specify that 'along' should be 
length(dim)+1.  At the moment one can specify any number from 0 up to 
length(dim)+1, but as you point out you have to spell out length(dim)+1 as 
the value for the along argument.  It would possible to make abind() 
automatically calculate along=length(dim)+1 when given along=NA, or 
along=-1, or along="+1".  Any preferences?

-- Tony Plate


At Tuesday 04:48 PM 10/21/2003 +0100, Robin Hankin wrote:
>Hi everyone
>
>I've been playing with do.call() but I'm having problems understanding it.
>
>I have a list of "n" elements, each one of which is "d" dimensional
>[actually an n-by-n-by ... by-n array].  Neither n nor d is known in
>advance.  I want to bind the elements together in a higher-dimensional
>array.
>
>Toy example follows with d=n=3.
>
>f <- function(n){array(n,c(3,3,3))}
>x <-  sapply(1:3,f,simplify=FALSE)
>
>Then what I want is
>
>ans <- abind(x[[1]] , x[[2]] , x[[3]]  , along=4)
>
>[abind() is defined in library(abind)].
>
>Note that dim(ans) is c(3,3,3,3), as required.
>
>PROBLEM: how do I do tell do.call() that I want to give abind() the
>extra argument along=4 (in general, I want
>along=length(dim(x[[1]]))+1)?
>
>
>Oblig Attempt:
>
>jj <- function(...){abind(... , along=4)}
>do.call("jj" , x)
>
>This works, because I know that d=3 (and therefore use along=4), but
>it doesn't generalize easily to arbitrary d.  I'm clearly missing
>something basic.  Anyone?
>
>______________________________________________
>R-help at stat.math.ethz.ch mailing list
>https://www.stat.math.ethz.ch/mailman/listinfo/r-help



From ripley at stats.ox.ac.uk  Tue Oct 21 18:33:39 2003
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Tue, 21 Oct 2003 17:33:39 +0100 (BST)
Subject: [R] explaining curious result of aov
In-Reply-To: <01b901c397ec$2ee12950$8d1ad284@BIO041>
Message-ID: <Pine.LNX.4.44.0310211730500.4342-100000@gannet.stats>

The ANOVA assumes equal variances in the groups.  Suppose groups 5 and 6 
had much lower variances than groups 1 to 4, and group 6 had a different 
mean from the other 5 (which were about equal)?

Given how small the groups apperat to be, this could happen.

On Tue, 21 Oct 2003, Bill Shipley wrote:

> Hello.  I have come across a curious result that I cannot explain.
> Hopefully, someone can explain this.  I am doing a 1-way ANOVA with 6
> groups (example: summary(aov(y~A)) with A having 6 levels).  I get an F
> of 0.899 with 5 and 15 df (p=0.51).  I then do the same analysis but
> using data only corresponding to groups 5 and 6.  This is, of course,
> equivalent to a t-test.  I now get an F of 142.3 with 1 and 3 degrees of
> freedom and a null probability of 0.001.  I know that multiple
> comparisons changes the model-wise error rate, but even if I did all 15
> comparisons of the 6 groups, the Bonferroni correction to a 5% alpha is
> 0.003, yet the Bonferroni correction gives conservative rejection
> levels.
> 
> How can such a result occur?  Any clues would be helpful.

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From p.dalgaard at biostat.ku.dk  Tue Oct 21 18:41:07 2003
From: p.dalgaard at biostat.ku.dk (Peter Dalgaard)
Date: 21 Oct 2003 18:41:07 +0200
Subject: [R] explaining curious result of aov
In-Reply-To: <01b901c397ec$2ee12950$8d1ad284@BIO041>
References: <01b901c397ec$2ee12950$8d1ad284@BIO041>
Message-ID: <x2r816fsrg.fsf@biostat.ku.dk>

"Bill Shipley" <bill.shipley at usherbrooke.ca> writes:

> Hello.  I have come across a curious result that I cannot explain.
> Hopefully, someone can explain this.  I am doing a 1-way ANOVA with 6
> groups (example: summary(aov(y~A)) with A having 6 levels).  I get an F
> of 0.899 with 5 and 15 df (p=0.51).  I then do the same analysis but
> using data only corresponding to groups 5 and 6.  This is, of course,
> equivalent to a t-test.  I now get an F of 142.3 with 1 and 3 degrees of
> freedom and a null probability of 0.001.  I know that multiple
> comparisons changes the model-wise error rate, but even if I did all 15
> comparisons of the 6 groups, the Bonferroni correction to a 5% alpha is
> 0.003, yet the Bonferroni correction gives conservative rejection
> levels.
> 
> How can such a result occur?  Any clues would be helpful.

It's a question of assumptions. 

Notice first that you have some very small groups there. Comparing two
groups with 3df means that there are five observations in all,
presumably two in one group and three in the other (although it could
be 4-1).

The joint F test assumes that all the groups have a similar
(theoretical) SD, whereas the two group comparison only assumes that
those two groups are similar.

Suppose one of the other groups had a huge SD; then a joint comparison
would clearly lose power if the actual differences were between some
of the groups with a smaller SD. 

On the other hand, the test on 3df is extremely dependent on
distributional assumptions, and if data are non-normally distributed,
there may be an increased probability of getting a very small variance
(quantization can do that, e.g.) and thus a falsely significant
result.

I.e. I'd take a closer look at the SD's for the 6 groups and perhaps
make a dotplot.

-- 
   O__  ---- Peter Dalgaard             Blegdamsvej 3  
  c/ /'_ --- Dept. of Biostatistics     2200 Cph. N   
 (*) \(*) -- University of Copenhagen   Denmark      Ph: (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)             FAX: (+45) 35327907



From s-plus at wiwi.uni-bielefeld.de  Tue Oct 21 18:37:41 2003
From: s-plus at wiwi.uni-bielefeld.de (Peter Wolf)
Date: Tue, 21 Oct 2003 18:37:41 +0200
Subject: [R] report generator a la epiinfo
References: <eb509e9a17.e9a17eb509@canarias.org>
Message-ID: <3F9560D5.2000308@wiwi.uni-bielefeld.de>

Lucas Gonzalez Santa Cruz wrote:

>Hi
>
>I'd like to use R in epidemiology and disease surveillance.
>
>In EpiInfo you can have a script (.pgm) which calls a predefined report
>(.rpt), where a table is calculated and values picked from that table
>and placed where the author of the report wants them, with text around
>those values. (Please see example below.)
>
>I've looked at manuals, faq, mail-search and google. The closest is an
>"R Report Generator" email that looked as if it wasn't followed after a
>couple of years.
>
>##The script might have something like this:
>read.epiinfo("oswego.rec")
>report("oswego.rpt", output="oswego.txt")
>
>##The predefined report might have this:
>#{ill}
>Exactly {"YES"} people fell ill, and {"NO"} people didn't.
>We don't know about the remaining [({}-{"YES"}-{"NO"})*100/{}] percent.
>#{icecream ill}
>We are specifically interested in the number of people who chose vanilla
>and didn't fall ill (all {"VANILLA", "YES"} of them).
>
>Is there anyway to do this with R? Any direction I should look into?
>
>Thanks in advance.
>
>Lucas
>
>______________________________________________
>R-help at stat.math.ethz.ch mailing list
>https://www.stat.math.ethz.ch/mailman/listinfo/r-help
>  
>
One way is to use Sweave, another one is given by my R function ff.
ff allows you to substitute expressions by evaluated results in a raw 
report.

See: 
http://www.wiwi.uni-bielefeld.de/~wolf/software/R-wtools/formfill/ff.html
or http://www.wiwi.uni-bielefeld.de/~wolf/software/R-wtools/formfill/ff.rd

Peter Wolf



From tord.snall at ebc.uu.se  Tue Oct 21 18:59:56 2003
From: tord.snall at ebc.uu.se (Tord =?iso-8859-1?b?U27kbGw=?=)
Date: Tue, 21 Oct 2003 18:59:56 +0200
Subject: [R] code efficiency, extr. info from list
Message-ID: <1066755596.3f95660c6b237@webmail.anst.uu.se>

Dear Anders and Paulo,

Thanks very much for your recommendations! 

I did it this way:

test2 <- unlist(lapply(test, function(x)
                cbind(x$par[1], x$par[2], x$value, x$conv)))
m<- as.data.frame(matrix(test2, nrow = dim(grid)[1], ncol = 4, byrow = T))
names(m) <- c("s2", "rho", "value", "conv")

but I sure there are better ways....


Sincerely,
Tord



From Ted.Harding at nessie.mcc.ac.uk  Tue Oct 21 18:56:53 2003
From: Ted.Harding at nessie.mcc.ac.uk ( (Ted Harding))
Date: Tue, 21 Oct 2003 17:56:53 +0100 (BST)
Subject: [R] explaining curious result of aov
In-Reply-To: <01b901c397ec$2ee12950$8d1ad284@BIO041>
Message-ID: <XFMail.031021175653.Ted.Harding@nessie.mcc.ac.uk>

On 21-Oct-03 Bill Shipley wrote:
> Hello.  I have come across a curious result that I cannot explain.
> Hopefully, someone can explain this.  I am doing a 1-way ANOVA with 6
> groups (example: summary(aov(y~A)) with A having 6 levels).  I get an F
> of 0.899 with 5 and 15 df (p=0.51).  I then do the same analysis but
> using data only corresponding to groups 5 and 6.  This is, of course,
> equivalent to a t-test.  I now get an F of 142.3 with 1 and 3 degrees
> of freedom and a null probability of 0.001.  I know that multiple
> comparisons changes the model-wise error rate, but even if I did all 15
> comparisons of the 6 groups, the Bonferroni correction to a 5% alpha is
> 0.003, yet the Bonferroni correction gives conservative rejection
> levels.
> 
> How can such a result occur?  Any clues would be helpful.

It's not obvious from your description. However, one possibility (which
I very strongly suspect) is apparent heterogeneity of variance, coupled
with paucity of data.

To wit: The denominator in F is the residual sum of squares (divided by
its degrees of freedom -- 15 in your first case, 3 in your second).

If the data in groups 5 and 6 are very close to their group means,
the group means themselves being more widely separated, then you can
indeed get a large F. The very moderate F that you get from the full
set of groups is quite compatible with the extreme result from the
two-group analysis if the data happen to be more widely spread about
their group means than they happen to be in G5+G6. This is the
"heterogeneity of variance" side of it.

Your denominator df = 3 for the two-group case indicates that you
only have 5 data values altogether in these two groups. Your df = 15
for the six-group case indicates that you have only 21 data all told.
At an average of 3.5 data per group you have a very thin data set.
Your 2.5 data per group in G5+G6 is even thinner. I would be very
cautious about interpreting the results in such a case.

Perhaps if you told us more about your data we could give a more
focussed diagnosis.

Best wishes,
Ted.


--------------------------------------------------------------------
E-Mail: (Ted Harding) <Ted.Harding at nessie.mcc.ac.uk>
Fax-to-email: +44 (0)870 167 1972
Date: 21-Oct-03                                       Time: 17:56:53
------------------------------ XFMail ------------------------------



From ggrothendieck at myway.com  Tue Oct 21 19:22:28 2003
From: ggrothendieck at myway.com (Gabor Grothendieck)
Date: Tue, 21 Oct 2003 13:22:28 -0400 (EDT)
Subject: [R] do.call() and aperm()
Message-ID: <20031021172228.E0F9F39BD@xmxpita.myway.com>



I suggest following APL as that is a well thought out system.
In APL terms there are two operations here called:

- catenation. In abind, this occurs when along = 1,2,...,length(dim)
- lamination.  In abind, this occurs when along = length(dim) + 1

however, the latter is really only one case of lamination in 
which the added dimension comes at the end.  To do it in full
generality would require that one can add the new dimension
at any spot including before the first, between the first and
the second, ..., after the last.

In APL notation, if along has a fractional part then the new
dimension is placed between floor(along) and ceiling(along).
Thus along=1.1 would put the new dimension between the first
and second.  The actual value of the fractional part is not material.

---
From: Tony Plate <tplate at blackmesacapital.com>
 
I've also been thinking about how to specify that 'along' should be 
length(dim)+1. At the moment one can specify any number from 0 up to 
length(dim)+1, but as you point out you have to spell out length(dim)+1 as 
the value for the along argument. It would possible to make abind() 
automatically calculate along=length(dim)+1 when given along=NA, or 
along=-1, or along="+1". Any preferences?

-- Tony Plate



_______________________________________________
No banners. No pop-ups. No kidding.
Introducing My Way - http://www.myway.com



From ggrothendieck at myway.com  Tue Oct 21 19:30:07 2003
From: ggrothendieck at myway.com (Gabor Grothendieck)
Date: Tue, 21 Oct 2003 13:30:07 -0400 (EDT)
Subject: [R] do.call() and aperm()
Message-ID: <20031021173007.9A6B639D1@xmxpita.myway.com>


Please ignore this email.  I just reread the abind
documentation and it already does this!




 --- On Tue 10/21, Gabor Grothendieck < ggrothendieck at myway.com > wrote:
From: Gabor Grothendieck [mailto: ggrothendieck at myway.com]
To: tplate at blackmesacapital.com, rksh at soc.soton.ac.uk, r-help at stat.math.ethz.ch
Date: Tue, 21 Oct 2003 13:22:28 -0400 (EDT)
Subject: Re: [R] do.call() and aperm()

<br><br>I suggest following APL as that is a well thought out system.<br>In APL terms there are two operations here called:<br><br>- catenation. In abind, this occurs when along = 1,2,...,length(dim)<br>- lamination.  In abind, this occurs when along = length(dim) + 1<br><br>however, the latter is really only one case of lamination in <br>which the added dimension comes at the end.  To do it in full<br>generality would require that one can add the new dimension<br>at any spot including before the first, between the first and<br>the second, ..., after the last.<br><br>In APL notation, if along has a fractional part then the new<br>dimension is placed between floor(along) and ceiling(along).<br>Thus along=1.1 would put the new dimension between the first<br>and second.  The actual value of the fractional part is not material.<br><br>---<br>From: Tony Plate <tplate at blackmesacapital.com><br> <br>I've also been thinking about how to specify that 'along' should be <br>length(dim)+1. At the moment one can specify any number from 0 up to <br>length(dim)+1, but as you point out you have to spell out length(dim)+1 as <br>the value for the along argument. It would possible to make abind() <br>automatically calculate along=length(dim)+1 when given along=NA, or <br>along=-1, or along="+1". Any preferences?<br><br>-- Tony Plate<br><br><br><br>_______________________________________________<br>No banners. No pop-ups. No kidding.<br>Introducing My Way - http://www.myway.com<br><br>______________________________________________<br>R-help at stat.math.ethz.ch mailing list<br>https://www.stat.math.ethz.ch/mailman/listinfo/r-help<br>

_______________________________________________
No banners. No pop-ups. No kidding.
Introducing My Way - http://www.myway.com



From tplate at blackmesacapital.com  Tue Oct 21 19:38:37 2003
From: tplate at blackmesacapital.com (Tony Plate)
Date: Tue, 21 Oct 2003 11:38:37 -0600
Subject: [R] do.call() and aperm()
In-Reply-To: <20031021172228.E0F9F39BD@xmxpita.myway.com>
Message-ID: <5.2.1.1.2.20031021113525.040f9a70@mailhost.blackmesacapital.com>

Thanks, I appreciate knowing that.

abind() can currently take a fractional value for along, and behaves as per 
your description of 'catenation' in APL.

Does APL supply any hints as to what sort of value to give 'along' to tell 
abind() to perform 'lamination'?

-- Tony Plate

At Tuesday 01:22 PM 10/21/2003 -0400, Gabor Grothendieck wrote:


>I suggest following APL as that is a well thought out system.
>In APL terms there are two operations here called:
>
>- catenation. In abind, this occurs when along = 1,2,...,length(dim)
>- lamination.  In abind, this occurs when along = length(dim) + 1
>
>however, the latter is really only one case of lamination in
>which the added dimension comes at the end.  To do it in full
>generality would require that one can add the new dimension
>at any spot including before the first, between the first and
>the second, ..., after the last.
>
>In APL notation, if along has a fractional part then the new
>dimension is placed between floor(along) and ceiling(along).
>Thus along=1.1 would put the new dimension between the first
>and second.  The actual value of the fractional part is not material.
>
>---
>From: Tony Plate <tplate at blackmesacapital.com>
>
>I've also been thinking about how to specify that 'along' should be
>length(dim)+1. At the moment one can specify any number from 0 up to
>length(dim)+1, but as you point out you have to spell out length(dim)+1 as
>the value for the along argument. It would possible to make abind()
>automatically calculate along=length(dim)+1 when given along=NA, or
>along=-1, or along="+1". Any preferences?
>
>-- Tony Plate
>
>
>
>_______________________________________________
>No banners. No pop-ups. No kidding.
>Introducing My Way - http://www.myway.com



From bill.shipley at usherbrooke.ca  Tue Oct 21 20:48:14 2003
From: bill.shipley at usherbrooke.ca (Bill Shipley)
Date: Tue, 21 Oct 2003 14:48:14 -0400
Subject: [R] summary of "explaining curious results of aov"
Message-ID: <023001c39803$e23b5950$8d1ad284@BIO041>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20031021/bc84ad46/attachment.pl

From jsb0027 at unt.edu  Tue Oct 21 21:05:53 2003
From: jsb0027 at unt.edu (Jim Battista)
Date: Tue, 21 Oct 2003 14:05:53 -0500 (Central Daylight Time)
Subject: [R] lme mildly blowing up
Message-ID: <Pine.WNT.4.56.0310211350580.736@pc92524>


I'm running a hierarchical linear model of legislative committee
representativeness (so I have committees in chambers) using lme.  It's a
simple random-intercept-as-outcome model.

When I run it, everything converges and I get results like this, trimmed
for brevity.  The following are the group(chamber)-level variables.  The
dependent variable is bounded between zero and one.

                  Value Std.Error  DF    t-value p-value
effpty       0.29241814 0.1709523   5  1.7105246  0.1479
pay         -0.00395368 0.0051280   5 -0.7710054  0.4755
totalsta    -0.10386395 0.1466623   5 -0.7081842  0.5105
days         0.24975346 0.1626935   5  1.5351167  0.1853

Random effects:
 Formula: ~1 | stateid
        (Intercept)  Residual
StdDev:  0.01543434 0.3163471

BUT the intervals around the random effects are

sd((Intercept)) 2.440728e-08 0.01543434 9760.155

Which is obviously nonsense.

Now, I know some of what's going on here.  The model is overparameterized,
and I should be dropping some group-level variables.  And if I do that,
everything is kosher, and none of these variables matter there either.
OTOH, I can also get everything to come out apparently-kosher if I
estimate on a theoretically-relevant reduced dataset -- that is, if I drop
some observations (for committees nobody would ever care about), it
behaves again.

What I'm wondering is:

(1)  Is the model basically running home to OLS-or-very-close-to-it?  If
     I estimate the same model using stata's xtreg, it returns a sigma_u
     of zero, and if I estimate it with HLM, it generates a bad tau and
     tries again with one that is positive but weensy.  Are the algorithms
     in lme doing the same thing here, or something closely analogous?
     Generating an impermissible negative that gets truncated to zero, or
     substituting a very small positive number for it, or generating that
     very small number directly?
(2)  Assuming that the model is degenerating into OLS or something an
     epsilon away from OLS, can I still make the following inference?
	(a)  The std errors on group-level variables are underestimated,
	     since I'm running OLS(-like) on grouped data
	(b)  Therefore if they're not significant here, I can treat them
             as not significant (assuming I've checked for / dealt with
             collinearity problems, etc).

Thanks much,

Jim

James S. Coleman Battista
Dept. of Political Science, Univ. of North Texas
battista at unt.edu    (940)565-4960
-----
Pinky, are you pondering what I'm pondering?
I think so, but where will we find an open tattoo parlor at this time
of night?



From hi_ono2001 at ybb.ne.jp  Tue Oct 21 21:38:57 2003
From: hi_ono2001 at ybb.ne.jp (Hisaji Ono)
Date: Wed, 22 Oct 2003 04:38:57 +0900
Subject: [R] SOM library for R
References: <3A822319EB35174CA3714066D590DCD50205CCA4@usrymx25.merck.com>
Message-ID: <019201c3980a$f810eea0$818001db@webgis>

Thank you for your responses, Sean, Professor Ripley, Liaw.

 Sorry for my thanks.

 I'll try to use "SOM" in both "class" and "(Gene)som".

However, I couldn't know how to draw 2D hexagon maps using these packages.

 Could you give any suggestion?

 Thanks.



From p.murrell at auckland.ac.nz  Tue Oct 21 21:51:21 2003
From: p.murrell at auckland.ac.nz (Paul Murrell)
Date: Wed, 22 Oct 2003 08:51:21 +1300
Subject: [R] Strange behaviour
References: <20031021135610.GA7406@tin.it>
Message-ID: <3F958E39.2010702@stat.auckland.ac.nz>

Hi


Vittorio wrote:
> Paul Murrell [r-help] <20/10/03 09:13 +1300>:
> 
>>Hi
>>..................................................... 
>>The "nasty rectangles" are the output of the layout.show() function. 
>>This function draws a simple diagram (consisting of nasty rectangles) to 
>>indicate the regions that a call to layout() has set up.  It is designed 
>>to help users to understand what on earth the layout() function is 
>>doing.  (It is NOT a necessary part of setting up an arrangement of 
>>plots using the layout() function.)
>>
>>I suspect that the author of "simpleR" may have accidentally left the 
>>layout.show() call in simple.scatterplot() when copying the example from 
>>the layout() help file (apologies to John Verzani if this is an unfair 
>>diagnosis).
>>
>>So the immediate solution to your problem is to remove the line ...
>>
>>    layout.show(nf)
>>
>>... from simple.scatterplot().  The output should then be a single page 
>>which should "include" ok in latex.
>>
>>The larger problem of how to get at individual pages of output is 
>>probably best solved using something like the "onefile" argument to 
>>devices.  For example, look at the files produced by ...
>>
>>    pdf(onefile=FALSE)
>>    example(layout)
>>
>>... and at the help page for pdf() to see more about how to do this.
>>
>>Hope that helps
>>...............................
> 
> 
> Yes, Paul, definitely it helps. Thanks!
> 
> I obtained what I wanted. 
> 
> Now, I want to control the output of the pdf() command making it write
> a specific file chosen by me and not the system. After reading the
> help page for the pdf, I was unable to do it.
> 
> E.g. I issued
>  
> onefile<-FALSE
> pdf(file=ifelse(onefile,,"vic.pdf")
> example(layout)
> 
> 
> And I obtained a 5-pages vic.pdf with page 1-4 full of "nasty
> rectangles" of any kind and page 5 with the right picture.


You need something like ...

     pdf("yourname%03d.pdf", onefile=FALSE)
     example(layout)

Paul
-- 
Dr Paul Murrell
Department of Statistics
The University of Auckland
Private Bag 92019
Auckland
New Zealand
64 9 3737599 x85392
paul at stat.auckland.ac.nz
http://www.stat.auckland.ac.nz/~paul/



From KKelley at nd.edu  Tue Oct 21 22:42:11 2003
From: KKelley at nd.edu (Ken Kelley)
Date: Tue, 21 Oct 2003 15:42:11 -0500
Subject: [R] Denominator Degrees of Freedom in lme() -- Adjusting and
 Understanding Them
Message-ID: <5.1.0.14.2.20031021135131.03067e70@imap-k.nd.edu>

Hello all.

I was wondering if there is any way to adjust the denominator degrees of 
freedom in lme(). It seems to me that there is only one method that can be 
used. As has been pointed out previously on the list, the denominator 
degrees of freedom given by lme() do not match those given by SAS Proc 
Mixed or HLM5. Proc Mixed, for example, offers five different options for 
computing the denominator degrees of freedom. Is there anyway to make such 
specifications in lme(), so that the degrees of freedom will correspond 
with the output given from Proc Mixed.

I've looked at Pinheiro and Bates' Mixed-Effects Models book (especially p. 
91), but I still don't quite understand the method used for determining the 
degrees of freedom in lme(). When analyzing longitudinal data with the 
straight-line growth model (intercept and slope both have fixed and random 
effects), the degrees of freedom seem to be N*T-N-1, where N is total 
sample size and T is the number of timepoints (at least when data are 
balanced). In the Pinheiro and Bates book (p. 91), the degrees of freedom 
are given as m_i-(m_1-1+pi), where m_i is the number of groups at the ith 
level, m_0=1 if an intercept is included and p_i is the sum of the degrees 
of freedom corresponding to the terms estimated. I'm not sure how the 
N*T-N-1 matches up with the formula given on page 91. It seems to me the 
number of "groups" (i.e., m_i) would be equal to N, the number of 
individuals (note that this is what is given as the "number of groups" in 
the summary of the lme() object.). However, as more occasions of 
measurements are added, the number of degrees of freedom gets larger, 
making it seems as though m_i represents the total number of observations, 
not the "number of groups."

For example, if N=2 and T=3, you end up with 3 degrees of freedom using 
lme(). Increasing T to 10 has not changed the number of groups (i.e., N 
still equals 2), but the degrees of freedom increases to 17. In such a 
situation SAS Proc Mixed would still have 1 degree of freedom (N-1) 
regardless of T, as the number of "groups" have not changed (just the 
number of observations per group have changed).

Any insight into understanding the denominator degrees of freedom for the 
fixed effects would be appreciated. Since the degrees of freedom given by 
lme() can be made to be arbitrarily larger than those given by PROC MIXED 
(i.e., by having an arbitrarily large number of measurement occasions for 
each individual), and since the degrees of freedom affect the standard 
errors, then the hypothesis tests, then the p values, the differences 
between the methods is surprising. It seems one of the methods would be 
better than the other since they can potentially be so much different.

Thanks and have a good one,
Ken
P.S. I have posted this to both the R and Multilevel Modeling list.



From brahm at alum.mit.edu  Tue Oct 21 22:49:17 2003
From: brahm at alum.mit.edu (David Brahm)
Date: Tue, 21 Oct 2003 16:49:17 -0400
Subject: [R] R - S compatibility table
References: <450B582867097B4E8B19AE7F947C3BEE59E4D0@mbi-00.mbi.ufl.edu>
Message-ID: <16277.39885.941647.500214@arbres1a.fmr.com>

Purvis Bedenbaugh <purvis at mbi.ufl.edu> wrote:
> I started looking for an R-S compatibility table but didn't find it.
> Examples:
>   'stdev' is now 'sd'  - is it exactly the same computation ?
>   couldn't find a built-in for error.bar()
>   syntax that is an error in R: param(thisframe,"b") <- value

It's a moving target!  I wrote such a list in October 2001, and revised it a
year later, but I haven't updated it since then (I no longer use S-Plus).  Some
people (e.g. Paul Gilbert) replied that they also had lists which didn't
overlap much with mine, suggesting we were each seeing just a small part of the
puzzle.  So maintaining a complete and current list would be quite a challenge.

Paul also mentioned to me a mailing list "R-sig-S" on the topic:
  <https://www.stat.math.ethz.ch/mailman/listinfo/r-sig-s>
but it seems dead since May 2001.

That said, here's my year-old list.  Note "S" means "S-Plus 6.1.2" (not "The S
Language") and "R" probably means "R-1.6.0".

                     ***   R vs. S (DB 10/28/02)  ***

Language differences:
- Scoping rules differ.  In R, functions see the functions they're in.  Try:
    f1 <- function() {x <- 1; f2 <- function() print(x); f2()};  f1()
- Data must be loaded explicitly in R, can be attach()'ed in S.
    Addressed by my contributed package "g.data".
- R has a character-type NA, so LETTERS[c(NA,2)] = c(NA,"B") not c("","B")
- paste("a","b", sep="|", sep=".") is an error in R; ok in S.
- for() loops more efficient in R.

Graphics differences:
- Log scale indicated in S with par(xaxt)=="l", in R with par("xlog")==T.
- R has cex.main, col.lab, font.axis, etc.  Thus title("Hi", cex=4) fails.
- R has plotmath and Hershey vector fonts.
- R has palette(rainbow(10)) to define colors (both screen and printer).

Functions missing from R:
- unpaste, slice.index, colVars

Functions missing from S:
- strsplit, sub, gsub, chartr, formatC

Functions that work differently:
- system() has no "input" argument in R.
- substring(s,"x") <- "X" only works in S, but R has s <- gsub("x","X",s).
- scan expects numbers by default in R.
- which(<numeric>) converts to logical in S, is an error in R.
- The NULL returned by if(F){...} is invisible in R, visible in S.
- The NULL returned by return() is visible in R, invisible in S.
- Args to "var" differ, and R has "cov".  S na.method="a" ~ R use="p".
- var (or cov) drops dimensions in S, not R.
- cut allows labels=F in R, not in S (also left.include=T becomes right=F).
- Last argument of a replacement function must be named "value" in R.
- tapply(1:3, c("a","b","a"), sum) is a 1D-array in R, a vector in S.
- probability distribution fcn's have arg "log.x" in R (ref: Spencer Graves)

-- 
                              -- David Brahm (brahm at alum.mit.edu)



From brahm at alum.mit.edu  Tue Oct 21 23:00:30 2003
From: brahm at alum.mit.edu (David Brahm)
Date: Tue, 21 Oct 2003 17:00:30 -0400
Subject: [R] How to upgrade R
References: <3A822319EB35174CA3714066D590DCD50205CCE7@usrymx25.merck.com>
Message-ID: <16277.40558.859241.948048@arbres1a.fmr.com>

Andy Liaw <andy_liaw at merck.com> wrote:

> What's not clear to me is a good way of keeping two versions of R
> simultaneously (for ease of transition).  Can anyone suggest a good strategy
> for doing that on *nix?

I'm not sure what you mean, but I'll tell you what we do.  We have built
/res/R/R-1.6.2, /res/R/R-1.7.1, /res/R/R-1.8.0, etc. (note we never run "make
install").  Anyone who doesn't want to upgrade (e.g. frozen production code)
just hard-codes one of those paths.  Everyone else uses a symbolic link
/res/R/R, which right now points to R-1.7.1 but will change to R-1.8.0 when we
feel we're "ready".

Just to add another layer of complication, actually we have another symbolic
link /res/R/Rdb, which I ("db") use, because I like to upgrade faster.  So Rdb
currently points to R-1.8.0.  Symbolic links are your friend.
-- 
                              -- David Brahm (brahm at alum.mit.edu)



From tlumley at u.washington.edu  Tue Oct 21 23:03:52 2003
From: tlumley at u.washington.edu (Thomas Lumley)
Date: Tue, 21 Oct 2003 14:03:52 -0700 (PDT)
Subject: [R] do.call() and aperm()
In-Reply-To: <a06002003bbbb058f49db@[139.166.242.29]>
References: <a06002003bbbb058f49db@[139.166.242.29]>
Message-ID: <Pine.A41.4.58.0310211401151.138038@homer10.u.washington.edu>

On Tue, 21 Oct 2003, Robin Hankin wrote:

> Hi everyone
>
> I've been playing with do.call() but I'm having problems understanding it.
>
> I have a list of "n" elements, each one of which is "d" dimensional
> [actually an n-by-n-by ... by-n array].  Neither n nor d is known in
> advance.  I want to bind the elements together in a higher-dimensional
> array.
>
> Toy example follows with d=n=3.
>
> f <- function(n){array(n,c(3,3,3))}
> x <-  sapply(1:3,f,simplify=FALSE)
>
> Then what I want is
>
> ans <- abind(x[[1]] , x[[2]] , x[[3]]  , along=4)
>
> [abind() is defined in library(abind)].
>
> Note that dim(ans) is c(3,3,3,3), as required.
>
> PROBLEM: how do I do tell do.call() that I want to give abind() the
> extra argument along=4 (in general, I want
> along=length(dim(x[[1]]))+1)?
>
>
> Oblig Attempt:
>
> jj <- function(...){abind(... , along=4)}
> do.call("jj" , x)
>
> This works, because I know that d=3 (and therefore use along=4), but
> it doesn't generalize easily to arbitrary d.  I'm clearly missing
> something basic.  Anyone?
>

If I have understood correctly, then
  d<-length(dim(x[[1]]))
  do.call("abind",c(x,along=d+1))
should work.


	-thomas



From andy_liaw at merck.com  Tue Oct 21 23:14:43 2003
From: andy_liaw at merck.com (Liaw, Andy)
Date: Tue, 21 Oct 2003 17:14:43 -0400
Subject: [R] How to upgrade R
Message-ID: <3A822319EB35174CA3714066D590DCD50205CD12@usrymx25.merck.com>

It's actually easier than I thought (and perhaps than you described).  What
Prof. Ripley suggested, combined with the options to the configure script,
should make this fairly straight-forward, even with "make install".

Cheers,
Andy

> From: David Brahm [mailto:brahm at alum.mit.edu] 
> 
> Andy Liaw <andy_liaw at merck.com> wrote:
> 
> > What's not clear to me is a good way of keeping two versions of R 
> > simultaneously (for ease of transition).  Can anyone suggest a good 
> > strategy for doing that on *nix?
> 
> I'm not sure what you mean, but I'll tell you what we do.  We 
> have built /res/R/R-1.6.2, /res/R/R-1.7.1, /res/R/R-1.8.0, 
> etc. (note we never run "make install").  Anyone who doesn't 
> want to upgrade (e.g. frozen production code) just hard-codes 
> one of those paths.  Everyone else uses a symbolic link 
> /res/R/R, which right now points to R-1.7.1 but will change 
> to R-1.8.0 when we feel we're "ready".
> 
> Just to add another layer of complication, actually we have 
> another symbolic link /res/R/Rdb, which I ("db") use, because 
> I like to upgrade faster.  So Rdb currently points to 
> R-1.8.0.  Symbolic links are your friend.
> -- 
>                               -- David Brahm (brahm at alum.mit.edu)
>



From jasont at indigoindustrial.co.nz  Tue Oct 21 23:15:54 2003
From: jasont at indigoindustrial.co.nz (Jason Turner)
Date: Wed, 22 Oct 2003 10:15:54 +1300
Subject: [R] run R under linux
In-Reply-To: <Sea2-F65113teT3i6wO0003082f@hotmail.com>
References: <Sea2-F65113teT3i6wO0003082f@hotmail.com>
Message-ID: <3F95A20A.3050800@indigoindustrial.co.nz>

Zhen Pang wrote:

> We are not allowed to submit job directly, so I never type R to use R, 
> just make a batch. How can I use try() to correct my codes? In the 
> interactive mode, I know how to continue, but now I never enter the R 
> window, where to find my results and save seed to continue?
> 

Like you'd program any exception handling.  A toy example to get you 
started might look like this:

(somewhere inside your script file)
...
results <- vector(length=200,mode="numeric") #or whatever you use
set.seed(123)
...
errors <- list()

for(ii in 1:200) {
   foo <- try(some.simulation.thingy(your.params))
   if(inherits(foo,"try-error")) { #something bombed
     results[ii] <- NA
     errors[[as.character(ii)]] <- foo
   } else { #it worked
     results[ii] <- foo
   }
}
...
save(results,errors,file="results+errors.RData")
...
The end.

Play with that, and see if you get some useful ideas.

Cheers

Jason
-- 
Indigo Industrial Controls Ltd.
http://www.indigoindustrial.co.nz
64-21-343-545
jasont at indigoindustrial.co.nz



From bw at northbranchlogic.com  Tue Oct 21 23:32:58 2003
From: bw at northbranchlogic.com (Barnet Wagman)
Date: Tue, 21 Oct 2003 16:32:58 -0500
Subject: [R] Patches for DBI/RMySQL "valueClass" problem?
Message-ID: <3F95A60A.4060003@northbranchlogic.com>

According David Jame's response to my earlier question, there is a 
problem with setGeneric.setMethod in R 1.8.0 that affects DBI and RMySQL. 

Is there a fix for this?  David Jame's refers to an 'R-patched version' 
but I haven't seen anything like this on  CRAN.  Would going back to an 
older version of R solve the problem?

Thanks,

Barnet Wagman


"David James wrote:

However, there is a problem in the released version of R 1.8.0 that affects
the DBI and other packages (has something to do with methods
that use the "valueClass" argument in the setGeneric/setMethod functions).
In this case one needs to use the R-patched version. "



From bates at stat.wisc.edu  Tue Oct 21 23:54:31 2003
From: bates at stat.wisc.edu (Douglas Bates)
Date: 21 Oct 2003 16:54:31 -0500
Subject: [R] Denominator Degrees of Freedom in lme() -- Adjusting and
	Understanding Them
In-Reply-To: <5.1.0.14.2.20031021135131.03067e70@imap-k.nd.edu>
References: <5.1.0.14.2.20031021135131.03067e70@imap-k.nd.edu>
Message-ID: <6rad7u9rzc.fsf@bates4.stat.wisc.edu>

Contributions of code to provide alternative calculations of
denominator degrees of freedom are welcome :-)

I think it would be good to bear in mind that the use of the t and F
distributions for models with mixed effects is already an
approximation.  If your design is such that you end up with a very few
denominator degrees of freedom then the whole question of whether you
should be using F or t distributions in the first place becomes
problematic.   If the number of denominator degrees of freedom is
moderate than the distinction between alternative methods becomes
unimportant.

-- 
Douglas Bates                            bates at stat.wisc.edu
Statistics Department                    608/262-2598
University of Wisconsin - Madison        http://www.stat.wisc.edu/~bates/



From bates at stat.wisc.edu  Tue Oct 21 23:57:25 2003
From: bates at stat.wisc.edu (Douglas Bates)
Date: 21 Oct 2003 16:57:25 -0500
Subject: [R] R - S compatibility table
In-Reply-To: <16277.39885.941647.500214@arbres1a.fmr.com>
References: <450B582867097B4E8B19AE7F947C3BEE59E4D0@mbi-00.mbi.ufl.edu>
	<16277.39885.941647.500214@arbres1a.fmr.com>
Message-ID: <6r4qy29rui.fsf@bates4.stat.wisc.edu>

David Brahm  <brahm at alum.mit.edu> writes:

> Language differences:
> - Scoping rules differ.  In R, functions see the functions they're in.  Try:
>     f1 <- function() {x <- 1; f2 <- function() print(x); f2()};  f1()

It may be more accurate to say "the functions they're defined in".
It's lexical scoping, not dynamic scoping.



From apv at capital.net  Wed Oct 22 01:19:40 2003
From: apv at capital.net (Arend P. van der Veen)
Date: 21 Oct 2003 19:19:40 -0400
Subject: [R] Custom Device
Message-ID: <1066778380.3411.3.camel@redtail.mydomain.home>

Hi,

Can anyone point in the direction of how to write a custom output device
in R.  I currently generate output using PS and JPEG but need to produce
output in our own vector graphics language.

Thanks in advance,
Arend van der Veen



From hec.villafuerte at telgua.com.gt  Wed Oct 22 03:29:27 2003
From: hec.villafuerte at telgua.com.gt (=?ISO-8859-1?Q?=22H=E9ctor_Villafuerte_D=2E=22?=)
Date: Tue, 21 Oct 2003 17:29:27 -0800
Subject: [R] R-help login page error
Message-ID: <3F95DD77.8040805@telgua.com.gt>

Hi all,
I'm trying to access my account at
https://www.stat.math.ethz.ch/mailman/options/r-help
but the following appears:
*Error: */Authentication failed./

Yes, I've already checked my password. Is it a problem
with mailman or something?
Thanks in advance.
Hector



From p.murrell at auckland.ac.nz  Wed Oct 22 01:35:31 2003
From: p.murrell at auckland.ac.nz (Paul Murrell)
Date: Wed, 22 Oct 2003 12:35:31 +1300
Subject: [R] Custom Device
References: <1066778380.3411.3.camel@redtail.mydomain.home>
Message-ID: <3F95C2C3.9020703@stat.auckland.ac.nz>

Hi


Arend P. van der Veen wrote:
> Hi,
> 
> Can anyone point in the direction of how to write a custom output device
> in R.  I currently generate output using PS and JPEG but need to produce
> output in our own vector graphics language.


Look at ...

R/src/include/R_ext/GraphicsDevice.h

... for the in-progress API and ...

R/src/modules/X11/devX11.c

... for a device template to model yourself on.

And if you decide to go ahead with something, keep in touch because this 
stuff will be undergoing changes in the near future.

Paul
-- 
Dr Paul Murrell
Department of Statistics
The University of Auckland
Private Bag 92019
Auckland
New Zealand
64 9 3737599 x85392
paul at stat.auckland.ac.nz
http://www.stat.auckland.ac.nz/~paul/



From gerifalte28 at hotmail.com  Wed Oct 22 01:42:18 2003
From: gerifalte28 at hotmail.com (Francisco Vergara)
Date: Tue, 21 Oct 2003 23:42:18 +0000
Subject: [R] Polynomial lags
Message-ID: <Law14-F792fFh28D3W3000172d2@hotmail.com>

Thanks to Thomas, Roger and Spencer on your reply!

I am trying to replicate/validate the results from the "black box" 
Econometrics software Eviews, compared to R and EViews uses the Almon 
approach.  I will try the code kindly offered by Thomas.
I would be interested in the way to apply the B-spline expansion in lm()

Thanks!

Francisco


>From: Roger Koenker <roger at ysidro.econ.uiuc.edu>
>To: Thomas Lumley <tlumley at u.washington.edu>
>CC: r-help <r-help at stat.math.ethz.ch>
>Subject: Re: [R] Polynomial lags
>Date: Tue, 21 Oct 2003 09:57:10 -0500 (CDT)
>
>For what it is worth, I would have thought that expressing
>the lag coefficients in a B-spline expansion would be preferable
>to going back to Almon approach. This would give a relatively
>simple lm() application.
>
>
>url:	www.econ.uiuc.edu/~roger/my.html	Roger Koenker
>email	rkoenker at uiuc.edu			Department of Economics
>vox: 	217-333-4558				University of Illinois
>fax:   	217-244-6678				Champaign, IL 61820
>
>On Tue, 21 Oct 2003, Thomas Lumley wrote:
>
> > On Tue, 21 Oct 2003, Spencer Graves wrote:
> >
> > > Have you checked "www.r-project.org" -> search -> "R site search"?  I
> > > just got 15 hits for "polynomial lag".  If you haven't already tried
> > > this, I'd guess that some of these hits (though certainly not all) 
>might
> > > help you.
> > >
> >
> > Only one of these is about polynomial distributed lag models, and it's 
>an
> > apparently unsuccessful request for information.
> >
> > I have code
> > 	http://faculty.washington.edu/tlumley/pdl.R
> > and documentation
> > 	http://faculty.washington.edu/tlumley/pdlglm.html
> > for a version for generalised linear models.
> >
> > Note that I have not used this for some years, so it may run into 
>problems
> > with changes in R.  Also note that it is just a generalised linear model
> > -- it doesn't do anything about residual autocorrelation (which isn't a
> > problem in the application I was working on).
> >
> >
> > 	-thomas
> >
> > ______________________________________________
> > R-help at stat.math.ethz.ch mailing list
> > https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> >
>
>______________________________________________
>R-help at stat.math.ethz.ch mailing list
>https://www.stat.math.ethz.ch/mailman/listinfo/r-help

_________________________________________________________________
Surf and talk on the phone at the same time with broadband Internet access. 
Get high-speed for as low as $29.95/month (depending on the local service 
providers in your area).  https://broadband.msn.com



From spencer.graves at pdf.com  Wed Oct 22 02:22:07 2003
From: spencer.graves at pdf.com (Spencer Graves)
Date: Tue, 21 Oct 2003 17:22:07 -0700
Subject: [R] Denominator Degrees of Freedom in lme() -- Adjusting and
	Understanding Them
In-Reply-To: <6rad7u9rzc.fsf@bates4.stat.wisc.edu>
References: <5.1.0.14.2.20031021135131.03067e70@imap-k.nd.edu>
	<6rad7u9rzc.fsf@bates4.stat.wisc.edu>
Message-ID: <3F95CDAF.7010800@pdf.com>

Prof. Bates may be able to give us more recent references on this, but 
the best literature I know on this is Pinhiero and Bates (2000) 
Mixed-Effects Models in S and S-Plus (Springer, sec. 2.4).  This 
includes description of a "simulate.lme" function, which you can use to 
generate random numbers according to a given assumed model and then 
compare some results with a reference distribution.  Something like this 
could be used to answer your question of what is the correct number of 
degrees of freedom to use for any particular model. 

hope this helps.  spencer graves

Douglas Bates wrote:

>Contributions of code to provide alternative calculations of
>denominator degrees of freedom are welcome :-)
>
>I think it would be good to bear in mind that the use of the t and F
>distributions for models with mixed effects is already an
>approximation.  If your design is such that you end up with a very few
>denominator degrees of freedom then the whole question of whether you
>should be using F or t distributions in the first place becomes
>problematic.   If the number of denominator degrees of freedom is
>moderate than the distinction between alternative methods becomes
>unimportant.
>
>  
>



From ggrothendieck at myway.com  Wed Oct 22 02:31:16 2003
From: ggrothendieck at myway.com (Gabor Grothendieck)
Date: Tue, 21 Oct 2003 20:31:16 -0400 (EDT)
Subject: [R] Excel to R
Message-ID: <20031022003116.8E52E39D3@xmxpita.myway.com>



I have Excel files containing data that I would like to move to R.
They are in the standard form of a one row header followed by 
rows of data, one record per row EXCEPT that there are a few
rows of comments before the header.  The number of rows of comments
varies.  For Excel files of this form without comments I have had
success with:

require(RODBC)
z <- odbcConnectExcel("C:/myspread.xls")
z.df <- sqlFetch(z,"Sheet1")
close(z)

but the comments interfere with this.  

I don't want to manually delete the rows but want the entire
process from Excel file to R to be automatic.

I can accomplish this with a free utility, Baird's dataload that 
I found on the net.  This will convert the Excel files to text 
and then the text can be processed using R to locate the start of 
the header and only process the remainder of the file.  (There is
also another free utility called xlhtml that I don't use, but could 
have, that does this too.) Thus at this point I have an 
adequate automated solution.

Nevertheless, I was wondering, for sake of interest, if there is 
some solution in R that does not involve such an external program
such as dataload or xlhtml.

Thanks.

(I am using Windows 2000.)

_______________________________________________
No banners. No pop-ups. No kidding.
Introducing My Way - http://www.myway.com



From andy_liaw at merck.com  Wed Oct 22 03:25:26 2003
From: andy_liaw at merck.com (Liaw, Andy)
Date: Tue, 21 Oct 2003 21:25:26 -0400
Subject: [R] run R under linux
Message-ID: <3A822319EB35174CA3714066D590DCD50205CD13@usrymx25.merck.com>

> From: Jason Turner [mailto:jasont at indigoindustrial.co.nz] 
> 
> Zhen Pang wrote:
> 
> > We are not allowed to submit job directly, so I never type 
> R to use R,
> > just make a batch. How can I use try() to correct my codes? In the 
> > interactive mode, I know how to continue, but now I never 
> enter the R 
> > window, where to find my results and save seed to continue?
> > 
> 
> Like you'd program any exception handling.  A toy example to get you 
> started might look like this:

Or see ?tryCatch in R-1.8.0...

Andy
>



From edd at debian.org  Wed Oct 22 04:24:10 2003
From: edd at debian.org (Dirk Eddelbuettel)
Date: Tue, 21 Oct 2003 21:24:10 -0500
Subject: [R] Excel to R
In-Reply-To: <20031022003116.8E52E39D3@xmxpita.myway.com>
References: <20031022003116.8E52E39D3@xmxpita.myway.com>
Message-ID: <20031022022410.GA3799@sonny.eddelbuettel.com>

On Tue, Oct 21, 2003 at 08:31:16PM -0400, Gabor Grothendieck wrote:
> 
> 
> I have Excel files containing data that I would like to move to R.
> They are in the standard form of a one row header followed by 
> rows of data, one record per row EXCEPT that there are a few
> rows of comments before the header.  The number of rows of comments
> varies.  For Excel files of this form without comments I have had
> success with:
> 
> require(RODBC)
> z <- odbcConnectExcel("C:/myspread.xls")
> z.df <- sqlFetch(z,"Sheet1")
> close(z)
> 
> but the comments interfere with this.  
> 
> I don't want to manually delete the rows but want the entire
> process from Excel file to R to be automatic.
> 
> I can accomplish this with a free utility, Baird's dataload that 
> I found on the net.  This will convert the Excel files to text 
> and then the text can be processed using R to locate the start of 
> the header and only process the remainder of the file.  (There is
> also another free utility called xlhtml that I don't use, but could 
> have, that does this too.) Thus at this point I have an 
> adequate automated solution.

There is also Spreadsheet::ParseExcel, which comes with a simple xls2csv
which I once extended, and posted here (or maybe only to BDR following a
discussion here).  Being Perl, it can easily be automated, and will cope
with your comment lines. If I recall, ActiveState provides this as well for
win* platforms.

> Nevertheless, I was wondering, for sake of interest, if there is 
> some solution in R that does not involve such an external program
> such as dataload or xlhtml.

There are a few candidates for a cross-platform solution:

- GNU Gretl (an econometric program with a nice Gnome GUI, and a win32 port)
  has code for this, taken from a C program xls2csv as well as from Gnumeric. 
  I had planned to look into this for R, but never got around to it.
  
- Gnumeric just added a standalone tool 'ssconvert', this may compile on
  Windows.  
  
- Also, OpenOffice has code for this which one could extract, but I am not
  familiar with the details.
  
Someone just has to sit down and do it. Typically the person with the
greatest urge wins. As I nowadays get all my data directly from databases
systems, I will probably not be the one.

Hth, Dirk

-- 
Those are my principles, and if you don't like them... well, I have others.
                                                -- Groucho Marx



From yyh101 at psu.edu  Wed Oct 22 08:37:39 2003
From: yyh101 at psu.edu (yyh)
Date: Wed, 22 Oct 2003 02:37:39 -0400
Subject: [R] questions about axis
Message-ID: <001c01c39866$fcf95270$91f3cb82@FIRSTNOTEBOOK>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20031022/4cd683a2/attachment.pl

From kwan022 at stat.auckland.ac.nz  Wed Oct 22 09:04:39 2003
From: kwan022 at stat.auckland.ac.nz (Ko-Kang Kevin Wang)
Date: Wed, 22 Oct 2003 20:04:39 +1300 (NZDT)
Subject: [R] questions about axis
In-Reply-To: <001c01c39866$fcf95270$91f3cb82@FIRSTNOTEBOOK>
Message-ID: <Pine.LNX.4.44.0310221956080.7996-100000@stat61.stat.auckland.ac.nz>

Hi,

On Wed, 22 Oct 2003, yyh wrote:

> I have difficulties to handle axis. I want to draw axis label such that axis has range of [-0.4,0.4] with intervel 0.2 for x and y axis.
> Some part of range do not have data points. Thus, plot does not show whole range. How can I enforce plot to depict the whole range regardless of existence of data points ?

You can draw the plot from first principle.  For example:
  x = seq(-0.4, 0.4, length = 50)
  y = runif(50, -0.2, 0.2)
  plot.new()
  plot.window(xlim = c(-0.4, 0.4), ylim = c(-0.4, 0.4))
  axis(1, at = seq(-0.4, 0.4, by = 0.2))
  axis(2, at = seq(-0.4, 0.4, by = 0.2))
  points(x, y)
  box()

> Another problem is that when I depict axis labels, some labels are overlapped because interval is very small. In this case, I'd like to put one of label into insde the box which is drawn by plot. 
> How can I do this ?

Try:
  title(xlab = "This is my x-label", line = -2)
 
Of course, this is just a silly example, with 50 uniform random numbers 
between -0.2 and 0.2.  But you get the idea...

-- 
Cheers,

Kevin

---------------------------------------------------------------
Ko-Kang Kevin Wang
Master of Science (MSc) Student
SLC Tutor and Lab Demonstrator
Department of Statistics
University of Auckland
New Zealand
Homepage: http://www.stat.auckland.ac.nz/~kwan022
Ph: 373-7599
    x88475 (City)
    x88480 (Tamaki)



From nusbj at hotmail.com  Wed Oct 22 09:05:53 2003
From: nusbj at hotmail.com (Zhen Pang)
Date: Wed, 22 Oct 2003 15:05:53 +0800
Subject: [R] run R under linux
Message-ID: <Sea2-F15FrvG5aVqQ1B000314ca@hotmail.com>


Thank you for your help. But try() seems to only allow for an expression. My 
simulation have serveral expressions which possibly have problem. Is there 
any possibility to include them all in the try()? If not, I seem to use 
several try().


>From: Jason Turner <jasont at indigoindustrial.co.nz>
>To: Zhen Pang <nusbj at hotmail.com>
>CC: R-help at stat.math.ethz.ch
>Subject: Re: [R] run R under linux
>Date: Wed, 22 Oct 2003 10:15:54 +1300
>
>Zhen Pang wrote:
>
>>We are not allowed to submit job directly, so I never type R to use R, 
>>just make a batch. How can I use try() to correct my codes? In the 
>>interactive mode, I know how to continue, but now I never enter the R 
>>window, where to find my results and save seed to continue?
>>
>
>Like you'd program any exception handling.  A toy example to get you 
>started might look like this:
>
>(somewhere inside your script file)
>...
>results <- vector(length=200,mode="numeric") #or whatever you use
>set.seed(123)
>...
>errors <- list()
>
>for(ii in 1:200) {
>   foo <- try(some.simulation.thingy(your.params))
>   if(inherits(foo,"try-error")) { #something bombed
>     results[ii] <- NA
>     errors[[as.character(ii)]] <- foo
>   } else { #it worked
>     results[ii] <- foo
>   }
>}
>...
>save(results,errors,file="results+errors.RData")
>...
>The end.
>
>Play with that, and see if you get some useful ideas.
>
>Cheers
>
>Jason
>--
>Indigo Industrial Controls Ltd.
>http://www.indigoindustrial.co.nz
>64-21-343-545
>jasont at indigoindustrial.co.nz
>
>______________________________________________
>R-help at stat.math.ethz.ch mailing list
>https://www.stat.math.ethz.ch/mailman/listinfo/r-help



From a.letertre at invs.sante.fr  Wed Oct 22 09:18:01 2003
From: a.letertre at invs.sante.fr (LE TERTRE Alain)
Date: Wed, 22 Oct 2003 09:18:01 +0200
Subject: [R] Automatically updating GLM object
Message-ID: <E205E1E28C581B4AB24D569AC43F2FE7AA5659@exchange.invs>


Dear all,
i generated several GLM objects, named myobject1 to myobject25.
Now i'd like to update both of them.
So i tried:

for(ii in 1:25) {
     assign(paste("myobject.updated", ii, sep=""), update( myobject[ii]
,.~ + VAR2))
    }

Doesn't work
I also tried to get all the names in a vector and
update(names.myobject[ii],.~ + VAR2)
Stiil doesn't work

Any ideas
thanks

O__ ---- Alain Le Tertre
 c/ /'_ --- Institut de Veille Sanitaire (InVS)
(*) \(*) -- 12 rue du val d'Osne
~~~~~~~~~~ - 94415 Saint Maurice cedex FRANCE
Voice: 33 1 41 79 67 50 Fax: 33 1 41 79 67 68
email: a.letertre at invs.sante.fr



From ripley at stats.ox.ac.uk  Wed Oct 22 09:18:54 2003
From: ripley at stats.ox.ac.uk (Prof Brian D Ripley)
Date: Wed, 22 Oct 2003 08:18:54 +0100 (BST)
Subject: [R] run R under linux
In-Reply-To: <Sea2-F15FrvG5aVqQ1B000314ca@hotmail.com>
Message-ID: <Pine.GSO.4.44.0310220815500.21251-100000@auk.stats>

On Wed, 22 Oct 2003, Zhen Pang wrote:

> Thank you for your help. But try() seems to only allow for an expression. My
> simulation have serveral expressions which possibly have problem. Is there
> any possibility to include them all in the try()? If not, I seem to use
> several try().

{line1
line2
line3}

*is* a single expression.

As in the example on the help page for try, you can also wrap the
computation in a function call, and that is usually a good idea.

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272860 (secr)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From christoph.bier at web.de  Wed Oct 22 09:35:04 2003
From: christoph.bier at web.de (Christoph Bier)
Date: Wed, 22 Oct 2003 09:35:04 +0200
Subject: [R] Graphics overview
In-Reply-To: <3F952E99.1010908@web.de>
References: <Pine.LNX.4.44.0310211146330.1168-100000@gannet.stats>	<x21xt6ixdq.fsf@biostat.ku.dk>
	<3F952E99.1010908@web.de>
Message-ID: <3F963328.9000608@web.de>

Christoph Bier schrieb:

[...]

> Yes, it is, thanks! But it seems only to work with arrays as

No, it also works with data.frames as help(colSums) told me.

-- 
Christoph Bier, Dipl.Oecotroph., Email: bier at wiz.uni-kassel.de
Universitaet Kassel, FG Oekologische Lebensmittelqualitaet und
Ernaehrungskultur \\ Postfach 12 52 \\ 37202 Witzenhausen
Tel.: +49 (0) 55 42 / 98 -17 21, Fax: -17 13



From ripley at stats.ox.ac.uk  Wed Oct 22 09:56:27 2003
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Wed, 22 Oct 2003 08:56:27 +0100 (BST)
Subject: [R] Automatically updating GLM object
In-Reply-To: <E205E1E28C581B4AB24D569AC43F2FE7AA5659@exchange.invs>
Message-ID: <Pine.LNX.4.44.0310220853370.5263-100000@gannet.stats>

What does

	Doesn't work

mean here?

Note that myobject[ii] is unlikely to be what you want, and perhaps it 
should be

get(paste("myobject", ii, sep=""))

but the error messages you surely got should have been informative.

On Wed, 22 Oct 2003, LE TERTRE Alain wrote:

> 
> Dear all,
> i generated several GLM objects, named myobject1 to myobject25.
> Now i'd like to update both of them.
> So i tried:
> 
> for(ii in 1:25) {
>      assign(paste("myobject.updated", ii, sep=""), update( myobject[ii]
> ,.~ + VAR2))
>     }
> 
> Doesn't work
> I also tried to get all the names in a vector and
> update(names.myobject[ii],.~ + VAR2)
> Stiil doesn't work

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From a.letertre at invs.sante.fr  Wed Oct 22 10:17:29 2003
From: a.letertre at invs.sante.fr (LE TERTRE Alain)
Date: Wed, 22 Oct 2003 10:17:29 +0200
Subject: [R] Automatically updating GLM object
Message-ID: <E205E1E28C581B4AB24D569AC43F2FE727F3AF@exchange.invs>

Sure that doesn't work is not very informative.

The error message is "Error in update.default(mod.init, new.form, data = mydata, na.action = na.omit) : 
        need an object with call component"
That's because he's looking at a character object (the name) and not at the object itself.

But your solution is what i wanted.

Thanks both for the solution and your ability to read further an obscure doesn't work

O__ ---- Alain Le Tertre
 c/ /'_ --- Institut de Veille Sanitaire (InVS)
(*) \(*) -- 12 rue du val d'Osne
~~~~~~~~~~ - 94415 Saint Maurice cedex FRANCE
Voice: 33 1 41 79 67 50 Fax: 33 1 41 79 67 68
email: a.letertre at invs.sante.fr



> -----Message d'origine-----
> De : Prof Brian Ripley [mailto:ripley at stats.ox.ac.uk] 
> Envoy? : mercredi 22 octobre 2003 09:56
> ? : LE TERTRE Alain
> Cc : r-help at stat.math.ethz.ch
> Objet : Re: [R] Automatically updating GLM object
> 
> 
> What does
> 
> 	Doesn't work
> 
> mean here?
> 
> Note that myobject[ii] is unlikely to be what you want, and 
> perhaps it 
> should be
> 
> get(paste("myobject", ii, sep=""))
> 
> but the error messages you surely got should have been informative.
> 
> On Wed, 22 Oct 2003, LE TERTRE Alain wrote:
> 
> > 
> > Dear all,
> > i generated several GLM objects, named myobject1 to myobject25. Now 
> > i'd like to update both of them. So i tried:
> > 
> > for(ii in 1:25) {
> >      assign(paste("myobject.updated", ii, sep=""), update( 
> > myobject[ii] ,.~ + VAR2))
> >     }
> > 
> > Doesn't work
> > I also tried to get all the names in a vector and 
> > update(names.myobject[ii],.~ + VAR2) Stiil doesn't work
> 
> -- 
> Brian D. Ripley,                  ripley at stats.ox.ac.uk
> Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
> University of Oxford,             Tel:  +44 1865 272861 (self)
> 1 South Parks Road,                     +44 1865 272866 (PA)
> Oxford OX1 3TG, UK                Fax:  +44 1865 272595
> 
>



From a.letertre at invs.sante.fr  Wed Oct 22 10:17:57 2003
From: a.letertre at invs.sante.fr (LE TERTRE Alain)
Date: Wed, 22 Oct 2003 10:17:57 +0200
Subject: [R] Automatically updating GLM object
Message-ID: <E205E1E28C581B4AB24D569AC43F2FE727F3AF@exchange.invs>

Sure that doesn't work is not very informative.

The error message is "Error in update.default(mod.init, new.form, data = mydata, na.action = na.omit) : 
        need an object with call component"
That's because he's looking at a character object (the name) and not at the object itself.

But your solution is what i wanted.

Thanks both for the solution and your ability to read further an obscure doesn't work

O__ ---- Alain Le Tertre
 c/ /'_ --- Institut de Veille Sanitaire (InVS)
(*) \(*) -- 12 rue du val d'Osne
~~~~~~~~~~ - 94415 Saint Maurice cedex FRANCE
Voice: 33 1 41 79 67 50 Fax: 33 1 41 79 67 68
email: a.letertre at invs.sante.fr



> -----Message d'origine-----
> De : Prof Brian Ripley [mailto:ripley at stats.ox.ac.uk] 
> Envoy? : mercredi 22 octobre 2003 09:56
> ? : LE TERTRE Alain
> Cc : r-help at stat.math.ethz.ch
> Objet : Re: [R] Automatically updating GLM object
> 
> 
> What does
> 
> 	Doesn't work
> 
> mean here?
> 
> Note that myobject[ii] is unlikely to be what you want, and 
> perhaps it 
> should be
> 
> get(paste("myobject", ii, sep=""))
> 
> but the error messages you surely got should have been informative.
> 
> On Wed, 22 Oct 2003, LE TERTRE Alain wrote:
> 
> > 
> > Dear all,
> > i generated several GLM objects, named myobject1 to myobject25. Now 
> > i'd like to update both of them. So i tried:
> > 
> > for(ii in 1:25) {
> >      assign(paste("myobject.updated", ii, sep=""), update( 
> > myobject[ii] ,.~ + VAR2))
> >     }
> > 
> > Doesn't work
> > I also tried to get all the names in a vector and 
> > update(names.myobject[ii],.~ + VAR2) Stiil doesn't work
> 
> -- 
> Brian D. Ripley,                  ripley at stats.ox.ac.uk
> Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
> University of Oxford,             Tel:  +44 1865 272861 (self)
> 1 South Parks Road,                     +44 1865 272866 (PA)
> Oxford OX1 3TG, UK                Fax:  +44 1865 272595
> 
>



From JonesW at kssg.com  Wed Oct 22 10:22:00 2003
From: JonesW at kssg.com (Wayne Jones)
Date: Wed, 22 Oct 2003 09:22:00 +0100
Subject: [R]: Prediction interval for a Gaussian family log-link model
Message-ID: <6B5A9304046AD411BD0200508BDFB6CB021F0E96@gimli.middleearth.kssg.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20031022/b5557fb2/attachment.pl

From e.pebesma at geog.uu.nl  Wed Oct 22 10:34:22 2003
From: e.pebesma at geog.uu.nl (Edzer J. Pebesma)
Date: Wed, 22 Oct 2003 10:34:22 +0200
Subject: [R] Indicator Kriging
Message-ID: <3F96410E.9010506@geog.uu.nl>

Indicator kriging is nothing but ordinary or simple
kriging on binary (0/1) data; there's quite a list of packages
that can do kriging: gstat, geoR, sgeostat, fields,
spatial, ...

Be aware that indicator kriging outcomes only estimate
probabilities and may lie outside [0,1]

To my knowledge there's no package that ports GSLIB to R.
--
Edzer



From matthew.dowle at citigroup.com  Wed Oct 22 10:52:14 2003
From: matthew.dowle at citigroup.com (Dowle, Matthew [EQRE])
Date: Wed, 22 Oct 2003 09:52:14 +0100
Subject: [R] RSPerl on Windows
Message-ID: <2A1F656DA222DC4582290979562E92A018522D@exchuk37.eur.nsroot.net>


Dear All,

RSPerl looks like a great package. Does anyone have it working on Windows? I have seen references to problems building it on Windows, and suggestions to use R(D)COM instead, therefore I was wondering what the current state of play is.

Many thanks,
Matthew



From Pascal.Niklaus at unibas.ch  Wed Oct 22 11:50:54 2003
From: Pascal.Niklaus at unibas.ch (Pascal A. Niklaus)
Date: Wed, 22 Oct 2003 11:50:54 +0200
Subject: [R] plotmath question: y'
Message-ID: <3F9652FE.2010303@unibas.ch>

Hi all,

I wonder how to correctly write the following expression (it's the axis 
label in a plot command):

    ylab=expression(y' == y - bar(y) )

Somehow the single quote in y' is causing the problems, I guess because 
it is interpreted as a quote...
Does it have to be escaped? But how?

Thanks for your help

Pascal



From sophie at pierroton.inra.fr  Wed Oct 22 11:59:34 2003
From: sophie at pierroton.inra.fr (Sophie Gerber)
Date: Wed, 22 Oct 2003 11:59:34 +0200
Subject: [R] non linear regression with R
Message-ID: <3F965506.6030008@pierroton.inra.fr>

Dear Colleagues,

I have x, y data (pollen and seed dispersal from oaks !) that I would 
like to fit with a function which look like this:

p(a,b,x,y)=b/(2*pi*a?gamma(2/b))*exp(-(square_root(x?+y?)/a)power(b))

I am looking for a and b values that fit my data at best.
Can someone give me hints to perform such an analysis with R ?

Thanks a lot

Sophie


     Sophie Gerber                      gerber at pierroton.inra.fr
      INRA - UMR BIOGECO
      69 route d'Arcachon               tel (33) (0)5 57 12 28 30
      33612 Cestas cedex                fax (33) (0)5 57 12 28 81
       http://www.pierroton.inra.fr/genetics/Perso/Sophie/
        page_sophie_english.html



From p.dalgaard at biostat.ku.dk  Wed Oct 22 12:07:21 2003
From: p.dalgaard at biostat.ku.dk (Peter Dalgaard)
Date: 22 Oct 2003 12:07:21 +0200
Subject: [R]: Prediction interval for a Gaussian family log-link model
In-Reply-To: <6B5A9304046AD411BD0200508BDFB6CB021F0E96@gimli.middleearth.kssg.com>
References: <6B5A9304046AD411BD0200508BDFB6CB021F0E96@gimli.middleearth.kssg.com>
Message-ID: <x2d6cpr3fq.fsf@biostat.ku.dk>

Wayne Jones <JonesW at kssg.com> writes:

> Hi there fellow R-users, 
> 
> Can anyone tell me how to build  a prediction interval for a gaussian
> log-link model for the reponse variable??
> I can find the standard error of the predictions but I cant seem to find the
> prediction interval. Is there a way I can calculate the 
> prediction interval from the standard errors??
> 
> Here's the example: 
> 
> logX<-rnorm(100)
> logY<--2-0.5*logX+rnorm(100,0,0.4)
> Y<-exp(logY)
> my.glm.mod<-glm(Y~logX,family=gaussian(link="log"))
> predict(my.glm.mod,type="response",se.fit=TRUE)

Er, that's not the right model for those data! 

Y <- exp(-2-0.5*logX)+rnorm(100,0,0.4)

would be a log-link gaussian model (but a smaller SD would be more
interesting). Assuming that this is what you intended,

logX<-sort(rnorm(100)) 
Y <- exp(-2-0.5*logX)+rnorm(100,0,0.04)
my.glm.mod<-glm(Y~logX,family=gaussian(link="log"), etastart=pmax(Y,0))
pp <- predict(my.glm.mod,type="response",se.fit=TRUE)
bounds <- pp$fit + outer(sqrt(pp$residual.scale^2+pp$se.fit^2),
                         qt(c(.05,.95),100-2))

plot(logX,Y)
matlines(logX,bounds)


BTW, we seem to have a problem with the gaussian(log) starting values
if the observations are negative...

Notice that this *only* works for gaussian responses and relies on the
approximate normality of the predicted values. In general you have to
compute the convolution of the error distribution with the
distribution of the fitted value (possibly approximate). With discrete
response variables, all bets are off: It is not at all clear what a
prediction interval for binary response means.

If you really intended a lognormal distribution for Y, and a loglinear
relation with logX, just fit a linear model for log(Y) and transform
everything back.

-- 
   O__  ---- Peter Dalgaard             Blegdamsvej 3  
  c/ /'_ --- Dept. of Biostatistics     2200 Cph. N   
 (*) \(*) -- University of Copenhagen   Denmark      Ph: (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)             FAX: (+45) 35327907



From p.dalgaard at biostat.ku.dk  Wed Oct 22 12:13:29 2003
From: p.dalgaard at biostat.ku.dk (Peter Dalgaard)
Date: 22 Oct 2003 12:13:29 +0200
Subject: [R] plotmath question: y'
In-Reply-To: <3F9652FE.2010303@unibas.ch>
References: <3F9652FE.2010303@unibas.ch>
Message-ID: <x28yndr35i.fsf@biostat.ku.dk>

"Pascal A. Niklaus" <Pascal.Niklaus at unibas.ch> writes:

> Hi all,
> 
> I wonder how to correctly write the following expression (it's the
> axis label in a plot command):
> 
>     ylab=expression(y' == y - bar(y) )
> 
> Somehow the single quote in y' is causing the problems, I guess
> because it is interpreted as a quote...
> Does it have to be escaped? But how?

y*minute or y*"'" should do it. 

-- 
   O__  ---- Peter Dalgaard             Blegdamsvej 3  
  c/ /'_ --- Dept. of Biostatistics     2200 Cph. N   
 (*) \(*) -- University of Copenhagen   Denmark      Ph: (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)             FAX: (+45) 35327907



From ligges at statistik.uni-dortmund.de  Wed Oct 22 12:12:47 2003
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Wed, 22 Oct 2003 12:12:47 +0200
Subject: [R] plotmath question: y'
In-Reply-To: <3F9652FE.2010303@unibas.ch>
References: <3F9652FE.2010303@unibas.ch>
Message-ID: <3F96581F.6060103@statistik.uni-dortmund.de>

Pascal A. Niklaus wrote:

> Hi all,
> 
> I wonder how to correctly write the following expression (it's the axis 
> label in a plot command):
> 
>    ylab=expression(y' == y - bar(y) )

   expression(y * "'"  == y - bar(y))

Uwe Ligges


> Somehow the single quote in y' is causing the problems, I guess because 
> it is interpreted as a quote...
> Does it have to be escaped? But how?
> 
> Thanks for your help
> 
> Pascal
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help



From Simon.Fear at synequanon.com  Wed Oct 22 12:16:28 2003
From: Simon.Fear at synequanon.com (Simon Fear)
Date: Wed, 22 Oct 2003 11:16:28 +0100
Subject: [R] select text using only the keyboard
Message-ID: <6C8A8033ABC1E3468048ABC4F13CE572AC0E49@synequanon01>

I have a different wish: I want to be able to mouseclick in
the middle of a line to get the cursor there (as in SPlus).

While I appreciate that to get my wish I should just write
a little patch, I estimate it would take me about 2 years 
to reach the point where I was capable of it, assuming I 
did nothing else, and I would certainly have to understand 
Windows, which in previous brushes I have found to be 
very bad for the brain.

I can live without it.

Paul - in the meantime you might type savehistory() and then 
edit .Rhistory in your favourite keyboard-friendly editor.

> -----Original Message-----
> From: Prof Brian Ripley [mailto:ripley at stats.ox.ac.uk]
> Sent: 21 October 2003 07:51
> To: Paul Livingstone
> Cc: R-Help
> Subject: Re: [R] select text using only the keyboard
> 
> 
> Security Warning:
> If you are not sure an attachment is safe to open please contact 
> Andy on x234. There are 0 attachments with this message.
> ________________________________________________________________
> 
> On Tue, 21 Oct 2003, Paul Livingstone wrote:
> 
> > I am using R 1.6.1 on Windows.  I usually write/edit code in the R
> console,
> > and when I get it working right, copy and paste it to a 
> text file for
> later
> > use.  In Splus for Windows, this could be done easily using just the
> > keyboard, with up-arrow, shift-end or shift-home, ctrl-c, etc.  
> > 
> > But in R, the shift key doesn't select text.  The only way I can
> select text
> > in R is with the mouse.  Does anyone know how to select text in R
> using only
> > the keyboard?
> 
> No.
> 
> If you think this would be a useful contribution, please send a patch 
> against the current R sources (1.9.0-to-be, not 1.6.1).
> 
> -- 
> Brian D. Ripley,                  ripley at stats.ox.ac.uk
> Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
> University of Oxford,             Tel:  +44 1865 272861 (self)
> 1 South Parks Road,                     +44 1865 272866 (PA)
> Oxford OX1 3TG, UK                Fax:  +44 1865 272595
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
>  
 
Simon Fear 
Senior Statistician 
Syne qua non Ltd 
Tel: +44 (0) 1379 644449 
Fax: +44 (0) 1379 644445 
email: Simon.Fear at synequanon.com 
web: http://www.synequanon.com 
  
Number of attachments included with this message: 0 
  
This message (and any associated files) is confidential and\...{{dropped}}



From si-opz at dsv.su.se  Wed Oct 22 12:57:43 2003
From: si-opz at dsv.su.se (Orlando Zacarias)
Date: Wed, 22 Oct 2003 12:57:43 +0200
Subject: [R] adjacency matrix
Message-ID: <000a01c3988b$5221f8a0$94a1ed82@dsv.su.se>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20031022/b49c1845/attachment.pl

From Simon.Fear at synequanon.com  Wed Oct 22 13:07:16 2003
From: Simon.Fear at synequanon.com (Simon Fear)
Date: Wed, 22 Oct 2003 12:07:16 +0100
Subject: [R] "aliases" for R constructs?
Message-ID: <6C8A8033ABC1E3468048ABC4F13CE572AC0E4B@synequanon01>

Some related suggestions:

1. instead of dataFrame[condition, ], use 

>>>>>>>>>>>  subset()  <<<<<<<<<<<

which has an inbuilt test for !is.na(condition). This function is 
not documented in the Introduction or cross-referenced 
by ?"[", so should be posted to this list at least once a week 
until everybody knows about it. (Sorry Martin M. that I still 
have not done what I promised re documentation. The ink has
faded so much on the sticky reminder that I can't read it
any more.)

2. consider using an editor with definable keyboard macros.
There's nothing quicker, or easier to debug, than plain old,
bottom level !is.na, but you can get there by typing F1 if you 
set it up that way.

3. I have a very old brown-covered book describing macros in
the S language (version 1? I don't have it to hand to check). 
When and why did they disappear? (Hoping John Chambers 
is reading this.)

> -----Original Message-----
> From: Thomas Lumley [mailto:tlumley at u.washington.edu]
> Sent: 20 October 2003 23:03
> To: Ted.Harding at nessie.mcc.ac.uk
> Cc: r-help at stat.math.ethz.ch
> Subject: Re: [R] "aliases" for R constructs?
> 
> 
> Security Warning:
> If you are not sure an attachment is safe to open please contact 
> Andy on x234. There are 0 attachments with this message.
> ________________________________________________________________
> 
> On Mon, 20 Oct 2003 Ted.Harding at nessie.mcc.ac.uk wrote:
> 
> > Hi Folks,
> >
> > My recent response to Laura Quinn's query about matrix subsetting
> > reminded of a question.
> >
> > I wrote:
> >
> >   iDir <- ((Winds[,20]<45)|(Winds[,20]>315))&(!is.na(Winds[,20]))
> >
> > Now, I find "!is.na" a bit awkward to type, so I might prefer to
> > type it as "nis.na".
> >
> > While it is possible to define
> >
> >   nis.na <- function(x){ !is.na(x) }
> >
> > this involves a slight bloat of overhead in execution since 
> achieving
> > !is.na involves an extra layer of function call. So is it possible
> > to define an "alias" so that
> >
> >   nis.na(x)
> >
> > is _exactly_ equivalent to
> >
> >   !is.na(x)
> >
> 
> No, R doesn't have macros.  As R is interpreted, a macro wouldn't
> necessarily save you any execution time -- Lisp macros, IIRC, are only
> expanded at compile-time.
> 
> You can get most of the programming advantages of macros with 
> substitute
> and eval, but this is presumably slower rather than faster.
> 
> 
> 	-thomas
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
>  
 
Simon Fear 
Senior Statistician 
Syne qua non Ltd 
Tel: +44 (0) 1379 644449 
Fax: +44 (0) 1379 644445 
email: Simon.Fear at synequanon.com 
web: http://www.synequanon.com 
  
Number of attachments included with this message: 0 
  
This message (and any associated files) is confidential and\...{{dropped}}



From anwhite at worldnet.att.net  Wed Oct 22 13:08:42 2003
From: anwhite at worldnet.att.net (Andrew White)
Date: Wed, 22 Oct 2003 01:08:42 -1000
Subject: [R] Help with STL function in R compared to S-Plus
Message-ID: <18CA02BB-0480-11D8-8FF9-000393861E24@worldnet.att.net>

I am trying to understand the nuances of STL (seasonal trend 
decomposition with loess) based on William Cleveland's (and others?) 
original development. I do not understand the specification or use of 
"frequency components" or equivalent "low-pass filter" components in 
the stl() function.

I have run the stl() function on a standard example data (co2) in both 
S-Plus and in R version 1.8 and the stacked-panel plots are different. 
The R version shows only rawdata, seasonal, (long-term) trend and then 
remainder (residual).  The S-Plus version of the plot shows in addition 
plots for the specified "frequency components". Why ?

In both versions of stl() the user specifies a univariate time series, 
and the length of window for a seasonal component, a trend component 
and one or more frequency (or low-pass filter) component(s).

Using the data "co2" as example, the S-Plus specification is something 
like:
   stl(co2, ss.window=17, fc.window=c(101,25), fc.degree=c(1,2)))

The near-equivalent specification of stl in R is something like:
   stl(co2, s.window=17, l.window=c(101,25), l.degree=c(1,2))

The user has the option of selecting multiple frequency components in 
S-Plus asa concatenation of values, as:  fc.window =c(101,25)  as the 
S-Plus stl help page  example shows. It is NOT clear if such a 
specification actually works in the R implementation of stl(), although 
summary() of the stl output object shows that the values are entered 
and stored in the routine.

Can anyone help explain the use and "tuning" of "frequency components" 
in S-Plus version of STL and the parallel "low-pass filter" components 
in R ? the RESULTS of specified frequency components show up in the 
stl.plot in S-Plus as additional plot panels, whereas the low-pass 
filter results do not! Only the raw-data, then seasonal, then trend, 
and finally residuals plots are stacked up. This is what confuses me 
the most: why the same routine from Wm. Cleveland and same named 
function would show 2 different results and plot formats in S-Plus vs R.

Thanks very much for any help with this matter.
Aloha from Hawaii, Andy White

email: anwhite at att.net; andrew_white at hmsa.com



From spencer.graves at pdf.com  Wed Oct 22 13:30:59 2003
From: spencer.graves at pdf.com (Spencer Graves)
Date: Wed, 22 Oct 2003 04:30:59 -0700
Subject: [R] non linear regression with R
In-Reply-To: <3F965506.6030008@pierroton.inra.fr>
References: <3F965506.6030008@pierroton.inra.fr>
Message-ID: <3F966A73.7090406@pdf.com>

      Have you considered "nls" or "optim"? 

      My way of handling this kind of problem is to assume that x and / 
or y follow some probability distribution with parameters a and b.  Then 
I write a function to compute deviance = (-2*log(likelihood)) = 
(-2*log(probability density for x and / or y given a and b)) and feed it 
to "optim" to minimize this "deviance".  Or if the response variable is 
a nonlinear model plus independent normal errors with constant variance, 
I may use "nls". 

      hope this helps.  spencer graves

Sophie Gerber wrote:

> Dear Colleagues,
>
> I have x, y data (pollen and seed dispersal from oaks !) that I would 
> like to fit with a function which look like this:
>
> p(a,b,x,y)=b/(2*pi*a?gamma(2/b))*exp(-(square_root(x?+y?)/a)power(b))
>
> I am looking for a and b values that fit my data at best.
> Can someone give me hints to perform such an analysis with R ?
>
> Thanks a lot
>
> Sophie
>
>
>     Sophie Gerber                      gerber at pierroton.inra.fr
>      INRA - UMR BIOGECO
>      69 route d'Arcachon               tel (33) (0)5 57 12 28 30
>      33612 Cestas cedex                fax (33) (0)5 57 12 28 81
>       http://www.pierroton.inra.fr/genetics/Perso/Sophie/
>        page_sophie_english.html
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help



From bolker at zoo.ufl.edu  Wed Oct 22 13:44:22 2003
From: bolker at zoo.ufl.edu (Ben Bolker)
Date: Wed, 22 Oct 2003 07:44:22 -0400 (EDT)
Subject: [R] non linear regression with R
In-Reply-To: <3F965506.6030008@pierroton.inra.fr>
Message-ID: <Pine.LNX.4.44.0310220738220.7595-100000@bolker.zoo.ufl.edu>


  Your function is not quite complete -- you need to specify an error 
distribution such as Poisson or negative binomial (see literature from the 
past decade by J. Clark, Muller-Landau, Ribbens and others)
  Assuming Poisson errors, if x and y are locations and
d is observed density:

f <- function(p) {
  a <- p[1]
  b <- p[2]
  r <- sqrt(x^2+y^2)
  exp.dens = b/(2*pi*a^2*gamma(2/b))*exp(-(r/a)^b)
  lik = -sum(dpois(d,exp.dens,log=TRUE))
}

optim(fn=f,par=(a vector of reasonable starting values))

  I see from my e-mail that Spencer Graves has just replied to this 
message too so I'll go see what he wrote ...




On Wed, 22 Oct 2003, Sophie Gerber wrote:

> Dear Colleagues,
> 
> I have x, y data (pollen and seed dispersal from oaks !) that I would 
> like to fit with a function which look like this:
> 
> p(a,b,x,y)=b/(2*pi*a?gamma(2/b))*exp(-(square_root(x?+y?)/a)power(b))
> 
> I am looking for a and b values that fit my data at best.
> Can someone give me hints to perform such an analysis with R ?
> 
> Thanks a lot
> 
> Sophie
> 
> 
>      Sophie Gerber                      gerber at pierroton.inra.fr
>       INRA - UMR BIOGECO
>       69 route d'Arcachon               tel (33) (0)5 57 12 28 30
>       33612 Cestas cedex                fax (33) (0)5 57 12 28 81
>        http://www.pierroton.inra.fr/genetics/Perso/Sophie/
>         page_sophie_english.html
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> 

-- 
620B Bartram Hall                            bolker at zoo.ufl.edu
Zoology Department, University of Florida    http://www.zoo.ufl.edu/bolker
Box 118525                                   (ph)  352-392-5697
Gainesville, FL 32611-8525                   (fax) 352-392-3704



From sbarbar at gwdg.de  Wed Oct 22 13:41:23 2003
From: sbarbar at gwdg.de (Salvatore Barbaro)
Date: Wed, 22 Oct 2003 13:41:23 +0200
Subject: [R] Latex and R
Message-ID: <3F968903.30230.4A4BE5@localhost>

Hi all!

Is there a possibility to read Latex-formulas directly into R, e.g. 
to read c=\frac{a}{b} such that R knows that c<- a/b is 
meant? I wish to generate some numerical examples of a 
theoretical model with R (and also to generate some plots) and 
I find it exhausting to "translate" my model into a R-code.

Thanks in advance

Salva



Thanks in advancesalvatore barbaro
department of public economics
platz der g?ttinger sieben 3
37073 g?ttingen
tel.: +49 551 3919704
fax:  +49 551 39 7353
http://www.gwdg.de/~sbarbar



From pburns at pburns.seanet.com  Wed Oct 22 14:15:20 2003
From: pburns at pburns.seanet.com (Patrick Burns)
Date: Wed, 22 Oct 2003 13:15:20 +0100
Subject: [R] "aliases" for R constructs?
References: <6C8A8033ABC1E3468048ABC4F13CE572AC0E4B@synequanon01>
Message-ID: <3F9674D8.40900@pburns.seanet.com>



Simon Fear wrote:

>
>3. I have a very old brown-covered book describing macros in
>the S language (version 1? I don't have it to hand to check). 
>When and why did they disappear? (Hoping John Chambers 
>is reading this.)
>  
>
Pretty much the same reason that you don't grind your flour
by hand with a stone.

Patrick Burns

Burns Statistics
patrick at burns-stat.com
+44 (0)20 8525 0696
http://www.burns-stat.com
(home of S Poetry and "A Guide for the Unwilling S User")

>  
>



From ripley at stats.ox.ac.uk  Wed Oct 22 14:37:45 2003
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Wed, 22 Oct 2003 13:37:45 +0100 (BST)
Subject: [R] Help with STL function in R compared to S-Plus
In-Reply-To: <18CA02BB-0480-11D8-8FF9-000393861E24@worldnet.att.net>
Message-ID: <Pine.LNX.4.44.0310221335070.9970-100000@gannet.stats>

On Wed, 22 Oct 2003, Andrew White wrote:

> I am trying to understand the nuances of STL (seasonal trend 
> decomposition with loess) based on William Cleveland's (and others?) 
> original development. I do not understand the specification or use of 
> "frequency components" or equivalent "low-pass filter" components in 
> the stl() function.

Have you read the original paper? -- I found it helped.

> I have run the stl() function on a standard example data (co2) in both 
> S-Plus and in R version 1.8 and the stacked-panel plots are different. 
> The R version shows only rawdata, seasonal, (long-term) trend and then 
> remainder (residual).  The S-Plus version of the plot shows in addition 
> plots for the specified "frequency components". Why ?

Not the same function!

> In both versions of stl() the user specifies a univariate time series, 
> and the length of window for a seasonal component, a trend component 
> and one or more frequency (or low-pass filter) component(s).
> 
> Using the data "co2" as example, the S-Plus specification is something 
> like:
>    stl(co2, ss.window=17, fc.window=c(101,25), fc.degree=c(1,2)))
> 
> The near-equivalent specification of stl in R is something like:
>    stl(co2, s.window=17, l.window=c(101,25), l.degree=c(1,2))
> 
> The user has the option of selecting multiple frequency components in 
> S-Plus asa concatenation of values, as:  fc.window =c(101,25)  as the 
> S-Plus stl help page  example shows. It is NOT clear if such a 
> specification actually works in the R implementation of stl(), although 
> summary() of the stl output object shows that the values are entered 
> and stored in the routine.
> 
> Can anyone help explain the use and "tuning" of "frequency components" 
> in S-Plus version of STL and the parallel "low-pass filter" components 
> in R ? the RESULTS of specified frequency components show up in the 
> stl.plot in S-Plus as additional plot panels, whereas the low-pass 
> filter results do not! Only the raw-data, then seasonal, then trend, 
> and finally residuals plots are stacked up. This is what confuses me 
> the most: why the same routine from Wm. Cleveland and same named 
> function would show 2 different results and plot formats in S-Plus vs R.

It's not the same routine.  The R one is the one taken from the R 
reference, and I don't know what exactly is in S-PLUS but it is not the 
same code.

> Thanks very much for any help with this matter.
> Aloha from Hawaii, Andy White

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From v.demart at libero.it  Wed Oct 22 15:09:43 2003
From: v.demart at libero.it (v.demart@libero.it)
Date: Wed, 22 Oct 2003 15:09:43 +0200
Subject: [R] High frequency time-series
Message-ID: <HN5TW7$32133594405753796349FA89552F5358@libero.it>

Having to collect hourly electricity loads and quarter-of-an-hour electricity production data for some years I think that the tidiest way of doing it is to resort to ts but I don't know how to define such a frequency starting from a set date.

Leafing through r-help mail archives I've found this *ALMOST* satisfactory message:
==========================================================
.........
 > I have a series of hourly rainfall and quarterly flow 
> measurements (i.e. 4 times an hour) of a catchment
........
> Maybe time series are easier, but in 
> 
> ts(data = NA, start = X,... 
> 
> X should be a number or a vector. how does this coresponds to a 
> data and hour (e.g. april 26,2002, 15:00:00)? 


If your observations are equidistant, e.g. you've got 24 hourly 
measurements per day, you could do something like this for the above 
example: 


R> rain <- ts(rain, start = c(26, 15), freq = 24) 
R> flow <- ts(flow, start = c(26, 15), freq = 96) 
...........
============================================================

But how does R know that we are speaking of a timeseries starting from April 26, 2002 and not, say, Feb 26, 2000?
There's some piece of info missing in the answer.

Am I correct?
Please help.

Ciao from Rome

Vittorio



From dj at research.bell-labs.com  Wed Oct 22 15:17:33 2003
From: dj at research.bell-labs.com (David James)
Date: Wed, 22 Oct 2003 09:17:33 -0400
Subject: [R] Patches for DBI/RMySQL "valueClass" problem?
In-Reply-To: <3F95A60A.4060003@northbranchlogic.com>;
	from bw@northbranchlogic.com on Tue, Oct 21, 2003 at 04:32:58PM
	-0500
References: <3F95A60A.4060003@northbranchlogic.com>
Message-ID: <20031022091733.A17696@jessie.research.bell-labs.com>

Hi,

I think yesterday's email from M.Kondrin's <mkondrin at hppi.troitsk.ru> 
summarizes two alternatives (1 and 2 below); of course you can also 
use R-1.7.1.  To recap, I think you have 3 easy choices (at least):  

(1) install R-patched from 
    ftp://ftp.stat.math.ethz.ch/Software/R/R-patched.tar.gz

(2) Or add the line 
        export(.valueClassTest) 
    to the file $R_HOME/src/library/methods/NAMESPACE
    (where $R_HOME is the directory where you installed R-1.8.0)

(3) Or revert back to R 1.7.1
 
Hope this helps,

--
David

Barnet Wagman wrote:
> According David Jame's response to my earlier question, there is a 
> problem with setGeneric.setMethod in R 1.8.0 that affects DBI and RMySQL. 
> 
> Is there a fix for this?  David Jame's refers to an 'R-patched version' 
> but I haven't seen anything like this on  CRAN.  Would going back to an 
> older version of R solve the problem?
> 
> Thanks,
> 
> Barnet Wagman
> 
> 
> "David James wrote:
> 
> However, there is a problem in the released version of R 1.8.0 that affects
> the DBI and other packages (has something to do with methods
> that use the "valueClass" argument in the setGeneric/setMethod functions).
> In this case one needs to use the R-patched version. "
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help



From cdeclercq at nordnet.fr  Wed Oct 22 16:19:11 2003
From: cdeclercq at nordnet.fr (Christophe Declercq)
Date: Wed, 22 Oct 2003 15:19:11 +0100
Subject: [R] adjacency matrix
In-Reply-To: <000a01c3988b$5221f8a0$94a1ed82@dsv.su.se>
Message-ID: <NGBBKLJCOLPAFMJIEMHCEECKCMAA.cdeclercq@nordnet.fr>

Your question is not really clear because:

- I don't know what 'convert.r' do
- I don't know where 'transf.cgm' is (BTW, 'cgm' is not the usual extension
for shapefiles which is 'shp'). Check if it is really in your current
directory.

but, yes, if what you want is to import shapefiles in R, it's easy. Look at
the 'maptools' package on CRAN (there is also a 'shapefiles' package).

And if you want to build an adjacency matrix for a map of polygons (as I
guess from the header of you mail), look at the 'spdep' package, too. It is
then possible to write a text file with your data as a list readable by
WinBUGS.

(I send a copy of my answer to the the R-SIG-GEO mailing list which can be
of interest to you).

Hope it helps.

Christophe
--
Christophe DECLERCQ, MD
Observatoire R?gional de la Sant? Nord-Pas-de-Calais
13, rue Faidherbe 59046 LILLE Cedex FRANCE
Phone +33 3 20 15 49 24
Fax   +33 3 20 55 92 30
E-mail c.declercq at orsnpdc.org



> -----Message d'origine-----
> De : r-help-bounces at stat.math.ethz.ch
> [mailto:r-help-bounces at stat.math.ethz.ch]De la part de Orlando Zacarias
> Envoy? : mercredi 22 octobre 2003 11:58
> ? : r-help at stat.math.ethz.ch
> Objet : [R] adjacency matrix
>
>
> Dear R experts,
> I am new to the list and R software. I need to convert arcview
> file to Winbugs having R has middle package. Got from friends
> that it is possible following the steps:
> 1. Converting arcview shapefile to "cgm clear text file"
> 2. Downloading "convert.r" to into R
>     2.1 use source("convert.r")
>      2.2 convert("filename_cgm") without the extension.
> But that doesn't work, given the following errors:
> ----------------------------
> R : Copyright 2003, The R Development Core Team
> Version 1.8.0  (2003-10-08)
> R is free software and comes with ABSOLUTELY NO WARRANTY.
> You are welcome to redistribute it under certain conditions.
> Type 'license()' or 'licence()' for distribution details.
> R is a collaborative project with many contributors.
> Type 'contributors()' for more information.
> Type 'demo()' for some demos, 'help()' for on-line help, or
> 'help.start()' for a HTML browser interface to help.
> Type 'q()' to quit R.
> [Previously saved workspace restored]
> > source("convert.r")
> > convert("transf")
> Error in file(file, "r") : unable to open connection
> In addition: Warning message:
> cannot open file `transf.cgm'
> >
> -----------------------------------
> Have i done some mistakes?Or i am missing some files?
> Please assist.
> Rgs,
> Orlando.
>
>
>
> 	[[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
>



From Roger.Bivand at nhh.no  Wed Oct 22 15:13:51 2003
From: Roger.Bivand at nhh.no (Roger Bivand)
Date: Wed, 22 Oct 2003 15:13:51 +0200 (CEST)
Subject: [R] adjacency matrix
In-Reply-To: <000a01c3988b$5221f8a0$94a1ed82@dsv.su.se>
Message-ID: <Pine.LNX.4.44.0310221509110.13509-100000@reclus.nhh.no>

On Wed, 22 Oct 2003, Orlando Zacarias wrote:

> Dear R experts,
> I am new to the list and R software. I need to convert arcview file to Winbugs having R has middle package. Got from friends that it is possible following the steps:
> 1. Converting arcview shapefile to "cgm clear text file"
> 2. Downloading "convert.r" to into R
>     2.1 use source("convert.r")
>      2.2 convert("filename_cgm") without the extension.
> But that doesn't work, given the following errors:
> ----------------------------
> R : Copyright 2003, The R Development Core Team
> Version 1.8.0  (2003-10-08)
> R is free software and comes with ABSOLUTELY NO WARRANTY.
> You are welcome to redistribute it under certain conditions.
> Type 'license()' or 'licence()' for distribution details.
> R is a collaborative project with many contributors.
> Type 'contributors()' for more information.
> Type 'demo()' for some demos, 'help()' for on-line help, or
> 'help.start()' for a HTML browser interface to help.
> Type 'q()' to quit R.
> [Previously saved workspace restored]
> > source("convert.r")
> > convert("transf")
> Error in file(file, "r") : unable to open connection
> In addition: Warning message: 
> cannot open file `transf.cgm' 
> > 
> -----------------------------------

This is quite a "narrow" question based on other people knowing that you 
are refering to:

http://www.biostat.umn.edu/~yuecui/

and the file you have downloaded from there.

The most typical cause of files not being found is when you are not in the 
correct working directory. Use:

> getwd()

to access the directory name, and 

> list.files()

to see if the name of your *.cgm file is returned. The key comment in the 
web page you must have used is:

"Open Splus or R under the folder in which convert.s  is saved"

but even if you start in another folder, you can change the R working 
directory using

> setwd("where_I_want_to_be")

or from the File menu.


> Have i done some mistakes?Or i am missing some files?
> Please assist.
> Rgs,
> Orlando.
> 
> 
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> 

-- 
Roger Bivand
Economic Geography Section, Department of Economics, Norwegian School of
Economics and Business Administration, Breiviksveien 40, N-5045 Bergen,
Norway. voice: +47 55 95 93 55; fax +47 55 95 93 93
e-mail: Roger.Bivand at nhh.no



From duncan at research.bell-labs.com  Wed Oct 22 15:46:38 2003
From: duncan at research.bell-labs.com (Duncan Temple Lang)
Date: Wed, 22 Oct 2003 09:46:38 -0400
Subject: [R] Excel to R
In-Reply-To: <20031022003116.8E52E39D3@xmxpita.myway.com>;
	from ggrothendieck@myway.com on Tue, Oct 21, 2003 at 08:31:16PM
	-0400
References: <20031022003116.8E52E39D3@xmxpita.myway.com>
Message-ID: <20031022094637.A19357@jessie.research.bell-labs.com>


Indeed, keeping the computations in R and working directly with the
Excel view of the cells is desirable for generality. It minimizes the
loss in information from translating to simpler forms such as CSV,
strings, etc.

>From within R on Windows, one can use DCOM to create an Excel
application, load the file and then read each of the worksheets in the
workbook cell by cell and obtain all sorts of information about each
cell.

The RDCOMClient package on the Omegahat Web site
(http://www.omegahat.org/RDCOMClient) and Thomas Baier's rcom package
provide the DCOM facilities from R.  David James has already done some
work to process Excel Worksheet and Range objects using the
RDCOMClient and create R data frames. And there is C code in the GGobi
distribution which does something very similar.

 D.

Gabor Grothendieck wrote:
> 
> 
> I have Excel files containing data that I would like to move to R.
> They are in the standard form of a one row header followed by 
> rows of data, one record per row EXCEPT that there are a few
> rows of comments before the header.  The number of rows of comments
> varies.  For Excel files of this form without comments I have had
> success with:
> 
> require(RODBC)
> z <- odbcConnectExcel("C:/myspread.xls")
> z.df <- sqlFetch(z,"Sheet1")
> close(z)
> 
> but the comments interfere with this.  
> 
> I don't want to manually delete the rows but want the entire
> process from Excel file to R to be automatic.
> 
> I can accomplish this with a free utility, Baird's dataload that 
> I found on the net.  This will convert the Excel files to text 
> and then the text can be processed using R to locate the start of 
> the header and only process the remainder of the file.  (There is
> also another free utility called xlhtml that I don't use, but could 
> have, that does this too.) Thus at this point I have an 
> adequate automated solution.
> 
> Nevertheless, I was wondering, for sake of interest, if there is 
> some solution in R that does not involve such an external program
> such as dataload or xlhtml.
> 
> Thanks.
> 
> (I am using Windows 2000.)
> 
> _______________________________________________
> No banners. No pop-ups. No kidding.
> Introducing My Way - http://www.myway.com
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help

-- 
_______________________________________________________________

Duncan Temple Lang                duncan at research.bell-labs.com
Bell Labs, Lucent Technologies    office: (908)582-3217
700 Mountain Avenue, Room 2C-259  fax:    (908)582-3340
Murray Hill, NJ  07974-2070       
         http://cm.bell-labs.com/stat/duncan



From mailinglist2_wegmann at web.de  Wed Oct 22 16:09:04 2003
From: mailinglist2_wegmann at web.de (Martin Wegmann)
Date: Wed, 22 Oct 2003 16:09:04 +0200
Subject: [R] 0-only-rows in correspondence analysis
Message-ID: <200310221609.04129.mailinglist2_wegmann@web.de>

Hello, 

I am looking for a workaround of "species empty" plots in a correspondence 
analysis. 

I tried to do a community structure analysis with a ca via decorana(), ca() 
and CAIV(), but none of them allow 0 only-rows.

I have species (abs./pres) in columns and sites in rows

	sp1 sp1 sp3 ...
plot1   0    1     1
plot2   0    0    0
plot3   1    0     1
.....

but on some plots no species could be found, therefore only 0 appear. I know 
that this isn't interesting for a community analysis but I am interested in 
the contributions of variables to the factors for a further analysis with 
env. variables where it is interesting that there are no species found. 

i thought about adding a "dummy" species with only presence on all sites, that 
would solve the 0-only-row problem but I am not sure if that would bias the 
outcome of a ca seriously.
 
Or is there a subcommand to allow 0 rows in a ca which I haven't seen?

thnaks Martin



From tlumley at u.washington.edu  Wed Oct 22 16:36:52 2003
From: tlumley at u.washington.edu (Thomas Lumley)
Date: Wed, 22 Oct 2003 07:36:52 -0700 (PDT)
Subject: [R] "aliases" for R constructs?
In-Reply-To: <3F9674D8.40900@pburns.seanet.com>
References: <6C8A8033ABC1E3468048ABC4F13CE572AC0E4B@synequanon01>
	<3F9674D8.40900@pburns.seanet.com>
Message-ID: <Pine.A41.4.58.0310220730590.211112@homer05.u.washington.edu>

On Wed, 22 Oct 2003, Patrick Burns wrote:

>
>
> Simon Fear wrote:
>
> >
> >3. I have a very old brown-covered book describing macros in
> >the S language (version 1? I don't have it to hand to check).
> >When and why did they disappear? (Hoping John Chambers
> >is reading this.)
> >
> >
> Pretty much the same reason that you don't grind your flour
> by hand with a stone.


Well, many very civilised languages have macros (not the #define things,
but ones that work on parsed expressions).  Lazy evaluation lets us
do most of the same things, but not all.   If we had a compiler there
would probably be more interest in macros, as then there would be a
distinction between compile-time and run-time evaluation.


	-thomas



From bw at northbranchlogic.com  Wed Oct 22 16:40:40 2003
From: bw at northbranchlogic.com (Barnet Wagman)
Date: Wed, 22 Oct 2003 09:40:40 -0500
Subject: [R] Patches for DBI/RMySQL "valueClass" problem?
In-Reply-To: <20031022091733.A17696@jessie.research.bell-labs.com>
References: <3F95A60A.4060003@northbranchlogic.com>
	<20031022091733.A17696@jessie.research.bell-labs.com>
Message-ID: <3F9696E8.7010706@northbranchlogic.com>

Thank you.  The copy of M.Kondrin's email that I received was truncated, 
so I don't know what he suggested.  This should get me going.

Regards,

Barnet Wagman

David James wrote:

>Hi,
>
>I think yesterday's email from M.Kondrin's <mkondrin at hppi.troitsk.ru> 
>summarizes two alternatives (1 and 2 below); of course you can also 
>use R-1.7.1.  To recap, I think you have 3 easy choices (at least):  
>
>(1) install R-patched from 
>    ftp://ftp.stat.math.ethz.ch/Software/R/R-patched.tar.gz
>
>(2) Or add the line 
>        export(.valueClassTest) 
>    to the file $R_HOME/src/library/methods/NAMESPACE
>    (where $R_HOME is the directory where you installed R-1.8.0)
>
>(3) Or revert back to R 1.7.1
> 
>Hope this helps,
>
>--
>David
>
>Barnet Wagman wrote:
>  
>
>>According David Jame's response to my earlier question, there is a 
>>problem with setGeneric.setMethod in R 1.8.0 that affects DBI and RMySQL. 
>>
>>Is there a fix for this?  David Jame's refers to an 'R-patched version' 
>>but I haven't seen anything like this on  CRAN.  Would going back to an 
>>older version of R solve the problem?
>>
>>Thanks,
>>
>>Barnet Wagman
>>
>>
>>"David James wrote:
>>
>>However, there is a problem in the released version of R 1.8.0 that affects
>>the DBI and other packages (has something to do with methods
>>that use the "valueClass" argument in the setGeneric/setMethod functions).
>>In this case one needs to use the R-patched version. "
>>
>>______________________________________________
>>R-help at stat.math.ethz.ch mailing list
>>https://www.stat.math.ethz.ch/mailman/listinfo/r-help
>>    
>>
>
>
>  
>



From Giles.Heywood at CommerzbankIB.com  Wed Oct 22 16:53:06 2003
From: Giles.Heywood at CommerzbankIB.com (Heywood, Giles)
Date: Wed, 22 Oct 2003 15:53:06 +0100
Subject: [R] High frequency time-series
Message-ID: <8CBAA121CEB4D5118CB200508BB2BBEF05BF7331@xmx8lonib.lonib.commerzbank.com>

You may find the irregular time-series (its) package on CRAN 
helpful.  

If your raw data were in a csv file thus:

				x
april 26 2002 15:00:00	1.1
april 26 2002 15:15:00	1.2
april 26 2002 15:30:00	1.3
april 26 2002 15:45:00	1.4

Then you could read it in thus:

require("its")
its.format("%B %d %Y %H:%M:%S")
x <- its(readcsvIts("c:/temp/mydata.csv"))

If on the other hand you already have the times in POSIX
form, it is slightly simpler e.g.
 
x <- its(xmat,POSIXdate)

 Giles

> -----Original Message-----
> From: v.demart at libero.it [mailto:v.demart at libero.it]
> Sent: 22 October 2003 14:10
> To: r-help r-help
> Subject: [R] High frequency time-series
> 
> 
> Having to collect hourly electricity loads and 
> quarter-of-an-hour electricity production data for some years 
> I think that the tidiest way of doing it is to resort to ts 
> but I don't know how to define such a frequency starting from 
> a set date.
> 
> Leafing through r-help mail archives I've found this *ALMOST* 
> satisfactory message:
> ==========================================================
> .........
>  > I have a series of hourly rainfall and quarterly flow 
> > measurements (i.e. 4 times an hour) of a catchment
> ........
> > Maybe time series are easier, but in 
> > 
> > ts(data = NA, start = X,... 
> > 
> > X should be a number or a vector. how does this coresponds to a 
> > data and hour (e.g. april 26,2002, 15:00:00)? 
> 
> 
> If your observations are equidistant, e.g. you've got 24 hourly 
> measurements per day, you could do something like this for the above 
> example: 
> 
> 
> R> rain <- ts(rain, start = c(26, 15), freq = 24) 
> R> flow <- ts(flow, start = c(26, 15), freq = 96) 
> ...........
> ============================================================
> 
> But how does R know that we are speaking of a timeseries 
> starting from April 26, 2002 and not, say, Feb 26, 2000?
> There's some piece of info missing in the answer.
> 
> Am I correct?
> Please help.
> 
> Ciao from Rome
> 
> Vittorio
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> 


********************************************************************** 
This is a commercial communication from Commerzbank AG.\ \ T...{{dropped}}



From zeileis at ci.tuwien.ac.at  Wed Oct 22 17:06:01 2003
From: zeileis at ci.tuwien.ac.at (Achim Zeileis)
Date: Wed, 22 Oct 2003 17:06:01 +0200
Subject: [R] High frequency time-series
In-Reply-To: <HN5TW7$32133594405753796349FA89552F5358@libero.it>
References: <HN5TW7$32133594405753796349FA89552F5358@libero.it>
Message-ID: <200310221506.h9MF62ZN022916@thorin.ci.tuwien.ac.at>

On Wednesday 22 October 2003 15:09, v.demart at libero.it wrote:

> Having to collect hourly electricity loads and quarter-of-an-hour
> electricity production data for some years I think that the tidiest
> way of doing it is to resort to ts but I don't know how to define
> such a frequency starting from a set date.
>
> Leafing through r-help mail archives I've found this *ALMOST*
> satisfactory message:
> ==========================================================
> .........
>
>  > I have a series of hourly rainfall and quarterly flow
> >
> > measurements (i.e. 4 times an hour) of a catchment
>
> ........
>
> > Maybe time series are easier, but in
> >
> > ts(data = NA, start = X,...
> >
> > X should be a number or a vector. how does this coresponds to a
> > data and hour (e.g. april 26,2002, 15:00:00)?
>
> If your observations are equidistant, e.g. you've got 24 hourly
> measurements per day, you could do something like this for the above
> example:
>
>
> R> rain <- ts(rain, start = c(26, 15), freq = 24)
> R> flow <- ts(flow, start = c(26, 15), freq = 96)
> ...........
> ============================================================
>
> But how does R know that we are speaking of a timeseries starting
> from April 26, 2002 and not, say, Feb 26, 2000? There's some piece
> of info missing in the answer.

Two possibilities:
  - ts(): you can just have a single number which counts the 
    time unit. So you have to do that in a unique way. Either
    by convention or e.g. by augmenting the ts object by an
    additional attribute which gives the start date or something
    in that direction.
  - irts(): this is a function for definin irregularly spaced
    time series and it's available in the package "tseries". The
    time attribute is a vector of POSIXct times. This is more
    flexible in some sense, but requires the storage of more
    information. Furthermore, you have more functions around which
    can compute objects of interest based on "ts" object than on
    "irts" objects.

hth,
Z

> Am I correct?
> Please help.
>
> Ciao from Rome
>
> Vittorio
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help



From dray at biomserv.univ-lyon1.fr  Wed Oct 22 17:55:14 2003
From: dray at biomserv.univ-lyon1.fr (Stephane DRAY)
Date: Wed, 22 Oct 2003 11:55:14 -0400
Subject: [R] 0-only-rows in correspondence analysis
In-Reply-To: <200310221609.04129.mailinglist2_wegmann@web.de>
Message-ID: <5.2.1.1.0.20031022113532.01fb02b8@biomserv.univ-lyon1.fr>

CA is based on relative composition and not on absolute value. So, empty 
site do not give any information. In a computational point of view, empty 
row or column do not allow to compute row / col weights. that is why most 
function do not work with empty row or column. dudi.coa (ade4 package) 
allows empty rows or columns. They are simply put at the centroid (and so 
have no weight in the analysis). CA do not take into acccount the 
information given by empty site !!! If you want that the information given 
by empty site are taken into account, you should prefer an other analysis 
(e.g. pca).

Sincerely,


At 10:09 22/10/2003, you wrote:
>Hello,
>
>I am looking for a workaround of "species empty" plots in a correspondence
>analysis.
>
>I tried to do a community structure analysis with a ca via decorana(), ca()
>and CAIV(), but none of them allow 0 only-rows.
>
>I have species (abs./pres) in columns and sites in rows
>
>         sp1 sp1 sp3 ...
>plot1   0    1     1
>plot2   0    0    0
>plot3   1    0     1
>.....
>
>but on some plots no species could be found, therefore only 0 appear. I know
>that this isn't interesting for a community analysis but I am interested in
>the contributions of variables to the factors for a further analysis with
>env. variables where it is interesting that there are no species found.
>
>i thought about adding a "dummy" species with only presence on all sites, 
>that
>would solve the 0-only-row problem but I am not sure if that would bias the
>outcome of a ca seriously.
>
>Or is there a subcommand to allow 0 rows in a ca which I haven't seen?
>
>thnaks Martin
>
>______________________________________________
>R-help at stat.math.ethz.ch mailing list
>https://www.stat.math.ethz.ch/mailman/listinfo/r-help



From GPetris at uark.edu  Wed Oct 22 17:59:38 2003
From: GPetris at uark.edu (Giovanni Petris)
Date: Wed, 22 Oct 2003 10:59:38 -0500 (CDT)
Subject: [R] comments and Sweave
Message-ID: <200310221559.h9MFxcMN017858@definetti.uark.edu>


I am using Sweave to produce handouts for teaching. 
Is there a way of making Sweave keep the comments following the `#' in
the code chuncks?

Thanks,
Giovanni

-- 

 __________________________________________________
[                                                  ]
[ Giovanni Petris                 GPetris at uark.edu ]
[ Department of Mathematical Sciences              ]
[ University of Arkansas - Fayetteville, AR 72701  ]
[ Ph: (479) 575-6324, 575-8630 (fax)               ]
[ http://definetti.uark.edu/~gpetris/              ]
[__________________________________________________]



From vigoyal at eden.rutgers.edu  Wed Oct 22 18:33:21 2003
From: vigoyal at eden.rutgers.edu (vishal goyal)
Date: Wed, 22 Oct 2003 12:33:21 -0400 (EDT)
Subject: [R] Weighted Clustering
Message-ID: <Pine.SOL.4.44.0310221229380.18355-100000@er7.rutgers.edu>

Hi,

I have a data set(say 2-d demands of a product (say flow-rate vs
concentration)) and with each demand is the weightage (like a probability)
of that demand occuring. Is there a way to cluster this demand-data
(deterministic or probabilistic(if possible)) which also incorporates the
weights (just multiplying distances with weights gives poor and mixed
clusters) while clustering (Something like a Facility Location problem).
I would appreciate any advice.

Regards
Vishal Goyal








--------------------------------------------------------------------------
"Simplicity is the ultimate sophistication"
---------------------------------------------------------------------------

Vishal Goyal, Graduate Student
Department of Chemical and Biochemical Engineering
Rutgers - The State University of New Jersey
98 Brett Road, Piscataway, NJ 08854
tel: 732-445-7061 (O)
email: vigoyal at eden.rutgers.edu



From apv at capital.net  Wed Oct 22 18:37:16 2003
From: apv at capital.net (Arend P. van der Veen)
Date: 22 Oct 2003 12:37:16 -0400
Subject: [R] Scheme and R
Message-ID: <1066840636.4067.5.camel@redtail.mydomain.home>

I have been using a product called rpy to allow me to access R from
Python.  I was wondering if anybody know if a similar product for
Scheme.

Thanks,
Arend



From rn001 at cebas.csic.es  Wed Oct 22 18:19:44 2003
From: rn001 at cebas.csic.es (javier garcia - CEBAS)
Date: Wed, 22 Oct 2003 18:19:44 +0200
Subject: [R] output in a list
Message-ID: <200310221601.h9MG0TR22354@natura.cebas.csic.es>

Hello. 

I've got a very short question.

I've got a vector with about 800 numbers; and I would like to put them in a 
file, but I need them to be written just one value in each row. Is this 
possible?

Best regards

Javier



From bates at stat.wisc.edu  Wed Oct 22 19:30:36 2003
From: bates at stat.wisc.edu (Douglas Bates)
Date: 22 Oct 2003 12:30:36 -0500
Subject: [R] output in a list
In-Reply-To: <200310221601.h9MG0TR22354@natura.cebas.csic.es>
References: <200310221601.h9MG0TR22354@natura.cebas.csic.es>
Message-ID: <6rznftdvsz.fsf@bates4.stat.wisc.edu>

javier garcia - CEBAS <rn001 at cebas.csic.es> writes:

> I've got a very short question.
> 
> I've got a vector with about 800 numbers; and I would like to put them in a 
> file, but I need them to be written just one value in each row. Is this 
> possible?

write(myvec, file = 'myfile.txt', ncol = 1)



From wolski at molgen.mpg.de  Wed Oct 22 19:35:44 2003
From: wolski at molgen.mpg.de (Eryk Wolski)
Date: Wed, 22 Oct 2003 19:35:44 +0200 (MET DST)
Subject: [R] INDEX file building packages?
Message-ID: <Pine.OSF.4.31.0310221843510.11977-100000@harry.molgen.mpg.de>

Hi!
Have build a package. I do not create by myself an INDEX file. If I got it
right it is optional. If no INDEX are there it is created during the
installation process.(So I understand the manual.)   I call R CMD INSTALL
mytest .. But no INDEX file
are generated. Hence no function list are displayed when calling
help(package="mytest"). There are some warnings during the compilation of
the Rd files but no errors. The html documentation and platform specific
documentation stuff are generated from Rd files.



Eryk.



From ripley at stats.ox.ac.uk  Wed Oct 22 19:37:22 2003
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Wed, 22 Oct 2003 18:37:22 +0100 (BST)
Subject: [R] output in a list
In-Reply-To: <200310221601.h9MG0TR22354@natura.cebas.csic.es>
Message-ID: <Pine.LNX.4.44.0310221836340.1054-100000@gannet.stats>

On Wed, 22 Oct 2003, javier garcia - CEBAS wrote:

> Hello. 
> 
> I've got a very short question.
> 
> I've got a vector with about 800 numbers; and I would like to put them in a 
> file, but I need them to be written just one value in each row. Is this 
> possible?

Yes.

?write tells you one way.

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From Subramanian_Karthikeyan at hc-sc.gc.ca  Wed Oct 22 19:31:30 2003
From: Subramanian_Karthikeyan at hc-sc.gc.ca (Subramanian Karthikeyan)
Date: Wed, 22 Oct 2003 13:31:30 -0400
Subject: [R] providing a variable as a parameter in a function
Message-ID: <OF49B0666F.04F14268-ON85256DC7.005EA0E0@hc-sc.gc.ca>

>From a data frame, how do we extract a specific column name, and plug that
into a command (eg. for Anova as shown below)

> df = read.delim("mydata.txt")
> y = colnames(df)
> r = ncol(x)

Lets say that in the data frame column 1 contains treatments, column 2
contains doses, and columns 3, 4, 5 etc. are different responses, and I
want to run separate 2-way anovas for each response, i.e. my first anova
will be done using col 1: Treatment, col 2: Dose and Col 3: a response,
second anova will be done using treatment (col1), dose (col2) and another
response (col 4) and so on.

I could use a loop to automate the task.

> for (i in 3:r) {
+ mod = lm(y[i]~Trt*Dose, data = x, contrasts = list(Trt = contr.sum, Dose
= contr.sum))
+ Anova(mod, type = "III")
+ }

The problem is when I directly plug in y[3] for my response variables name,
it gives me an error

Error in model.frame(formula, rownames, variables, varnames, extras,
extranames,  :
        invalid variable type

This is likely because the lm() function wants the actual column name,
rather than a variable containing the column name.

Can someone advice?

Thanks,
Karth.



From Subramanian_Karthikeyan at hc-sc.gc.ca  Wed Oct 22 19:49:01 2003
From: Subramanian_Karthikeyan at hc-sc.gc.ca (Subramanian Karthikeyan)
Date: Wed, 22 Oct 2003 13:49:01 -0400
Subject: [R] passing a variable (containing the value of the argument) to a
	function
Message-ID: <OFCF2D1E81.3A7B0B08-ON85256DC7.00613B92@hc-sc.gc.ca>

My previous question put in a simpler  way:

How would I pass a value of a variable to a function such as

lm(Effect1~Trt*Dose, data = x, contrasts = list(Trt = contr.sum, Dose =
contr.sum))?

Here, 'Effect' is a column name in my data matrix, and I want "Effect1" to
be replaced by "Effect2" and so on (my other column names in the data
frame) for successive anova calculations. So I am storing the column names
as an array, and passing the array as a parameter to the lm() function.

Thanks,


----- Forwarded by Subramanian Karthikeyan/HC-SC/GC/CA on 2003-10-22 01:47
PM -----
                                                                                                                                       
                      Subramanian                                                                                                      
                      Karthikeyan              To:       r-help at stat.math.ethz.ch                                                      
                                               cc:                                                                                     
                      2003-10-22 01:31         Subject:  providing a variable as a parameter in a function                             
                      PM                                                                                                               
                                                                                                                                       
                                                                                                                                       



>From a data frame, how do we extract a specific column name, and plug that
into a command (eg. for Anova as shown below)

> df = read.delim("mydata.txt")
> y = colnames(df)
> r = ncol(x)

Lets say that in the data frame column 1 contains treatments, column 2
contains doses, and columns 3, 4, 5 etc. are different responses, and I
want to run separate 2-way anovas for each response, i.e. my first anova
will be done using col 1: Treatment, col 2: Dose and Col 3: a response,
second anova will be done using treatment (col1), dose (col2) and another
response (col 4) and so on.

I could use a loop to automate the task.

> for (i in 3:r) {
+ mod = lm(y[i]~Trt*Dose, data = x, contrasts = list(Trt = contr.sum, Dose
= contr.sum))
+ Anova(mod, type = "III")
+ }

The problem is when I directly plug in y[3] for my response variables name,
it gives me an error

Error in model.frame(formula, rownames, variables, varnames, extras,
extranames,  :
        invalid variable type

This is likely because the lm() function wants the actual column name,
rather than a variable containing the column name.

Can someone advice?

Thanks,
Karth.



From kwan022 at stat.auckland.ac.nz  Wed Oct 22 20:08:39 2003
From: kwan022 at stat.auckland.ac.nz (Ko-Kang Kevin Wang)
Date: Thu, 23 Oct 2003 07:08:39 +1300 (NZDT)
Subject: [R] comments and Sweave
In-Reply-To: <200310221559.h9MFxcMN017858@definetti.uark.edu>
Message-ID: <Pine.LNX.4.44.0310230707390.11035-100000@stat61.stat.auckland.ac.nz>

Hi,

I asked exactly the same question to the author of Sweave privately last 
week.  Friedrich's reply was that it is not possible at the moment, as R 
parser discard the comments.  But he's working on it ;-D

On Wed, 22 Oct 2003, Giovanni Petris wrote:

> Date: Wed, 22 Oct 2003 10:59:38 -0500 (CDT)
> From: Giovanni Petris <GPetris at uark.edu>
> To: r-help at stat.math.ethz.ch
> Subject: [R] comments and Sweave
> 
> 
> I am using Sweave to produce handouts for teaching. 
> Is there a way of making Sweave keep the comments following the `#' in
> the code chuncks?
> 
> Thanks,
> Giovanni
> 
> 

-- 
Cheers,

Kevin

---------------------------------------------------------------
Ko-Kang Kevin Wang
Master of Science (MSc) Student
SLC Tutor and Lab Demonstrator
Department of Statistics
University of Auckland
New Zealand
Homepage: http://www.stat.auckland.ac.nz/~kwan022
Ph: 373-7599
    x88475 (City)
    x88480 (Tamaki)



From Roger.Bivand at nhh.no  Wed Oct 22 20:11:10 2003
From: Roger.Bivand at nhh.no (Roger Bivand)
Date: Wed, 22 Oct 2003 20:11:10 +0200 (CEST)
Subject: [R] INDEX file building packages?
In-Reply-To: <Pine.OSF.4.31.0310221843510.11977-100000@harry.molgen.mpg.de>
Message-ID: <Pine.LNX.4.44.0310222008500.11336-100000@reclus.nhh.no>

On Wed, 22 Oct 2003, Eryk Wolski wrote:

> Hi!
> Have build a package. I do not create by myself an INDEX file. If I got it
> right it is optional. If no INDEX are there it is created during the
> installation process.(So I understand the manual.)   I call R CMD INSTALL
> mytest .. But no INDEX file
> are generated. Hence no function list are displayed when calling
> help(package="mytest"). There are some warnings during the compilation of
> the Rd files but no errors. The html documentation and platform specific
> documentation stuff are generated from Rd files.
> 

R CMD build --force

should do it.

> 
> 
> Eryk.
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> 

-- 
Roger Bivand
Economic Geography Section, Department of Economics, Norwegian School of
Economics and Business Administration, Breiviksveien 40, N-5045 Bergen,
Norway. voice: +47 55 95 93 55; fax +47 55 95 93 93
e-mail: Roger.Bivand at nhh.no



From jfox at mcmaster.ca  Wed Oct 22 20:24:32 2003
From: jfox at mcmaster.ca (John Fox)
Date: Wed, 22 Oct 2003 14:24:32 -0400
Subject: [R] providing a variable as a parameter in a function
In-Reply-To: <OF49B0666F.04F14268-ON85256DC7.005EA0E0@hc-sc.gc.ca>
Message-ID: <5.0.2.1.0.20031022142026.00af2648@127.0.0.1>

Dear Subramanian,

How about this:

         for (y in df[, 3:5]) {
                 mod = lm(y ~ Trt*Dose, data = x, contrasts = list(Trt = 
contr.sum, Dose = contr.sum))
                 Anova(mod, type = "III")
                 }

Does that give you what you want?

John



At 01:31 PM 10/22/2003 -0400, Subramanian Karthikeyan wrote:
> >From a data frame, how do we extract a specific column name, and plug that
>into a command (eg. for Anova as shown below)
>
> > df = read.delim("mydata.txt")
> > y = colnames(df)
> > r = ncol(x)
>
>Lets say that in the data frame column 1 contains treatments, column 2
>contains doses, and columns 3, 4, 5 etc. are different responses, and I
>want to run separate 2-way anovas for each response, i.e. my first anova
>will be done using col 1: Treatment, col 2: Dose and Col 3: a response,
>second anova will be done using treatment (col1), dose (col2) and another
>response (col 4) and so on.
>
>I could use a loop to automate the task.
>
> > for (i in 3:r) {
>+ mod = lm(y[i]~Trt*Dose, data = x, contrasts = list(Trt = contr.sum, Dose
>= contr.sum))
>+ Anova(mod, type = "III")
>+ }
>
>The problem is when I directly plug in y[3] for my response variables name,
>it gives me an error
>
>Error in model.frame(formula, rownames, variables, varnames, extras,
>extranames,  :
>         invalid variable type
>
>This is likely because the lm() function wants the actual column name,
>rather than a variable containing the column name.
>
>Can someone advice?
>
>Thanks,
>Karth.
>
>______________________________________________
>R-help at stat.math.ethz.ch mailing list
>https://www.stat.math.ethz.ch/mailman/listinfo/r-help

____________________________
John Fox
Department of Sociology
McMaster University
email: jfox at mcmaster.ca
web: http://www.socsci.mcmaster.ca/jfox



From jasont at indigoindustrial.co.nz  Wed Oct 22 20:30:37 2003
From: jasont at indigoindustrial.co.nz (Jason Turner)
Date: Thu, 23 Oct 2003 07:30:37 +1300
Subject: [R] select text using only the keyboard
In-Reply-To: <6C8A8033ABC1E3468048ABC4F13CE572AC0E49@synequanon01>
References: <6C8A8033ABC1E3468048ABC4F13CE572AC0E49@synequanon01>
Message-ID: <3F96CCCD.9000309@indigoindustrial.co.nz>

Simon Fear wrote:
> I have a different wish: I want to be able to mouseclick in
> the middle of a line to get the cursor there (as in SPlus).
> 
> While I appreciate that to get my wish I should just write
> a little patch, I estimate it would take me about 2 years 
> to reach the point where I was capable of it, assuming I 
> did nothing else, and I would certainly have to understand 
> Windows, which in previous brushes I have found to be 
> very bad for the brain.

Or you could get a copy of (X)Emacs, and use ESS.  Total install and 
learning warm-up time should be in the order of a week at most.  It does 
all these nice things, and much more.  In my short experience with it, 
XEmacs plays nicer with ESS under Windows than GNU Emacs - YMMV.

Cheers

Jason
-- 
Indigo Industrial Controls Ltd.
http://www.indigoindustrial.co.nz
64-21-343-545
jasont at indigoindustrial.co.nz



From mdalphin at amgen.com  Wed Oct 22 21:05:44 2003
From: mdalphin at amgen.com (Mark Dalphin)
Date: Wed, 22 Oct 2003 12:05:44 -0700
Subject: [R] How to reformat data from database into data.frame?
Message-ID: <200310221205.44043.mdalphin@amgen.com>

I'm trying to find a clever way to re-map data from a database
query into a data.frame.

Querying a database often returns a table (data.frame) like this:

GeneID	 MethodID   Value
6	 1	    123
6	 2	    456
6	 3	    987
7	 1	    234
7	 3	    432
8	 2	    190
8	 3	    34
8	 1	    864	    

Note that GeneID=7 doesn't have a value for MethodID=2.

Note that GeneID=8 doesn't have the MethodID in any particular order
and, in fact, there doesn't need to be an order to the GeneID, although
I can force ordering as part of my SQL query.

I want to reformat this into a data.frame with multiple columns:

       Value	   Value     Value
GeneID Method1	   Method2   Method3
6      123	   456	     987
7      234	   NA	     432
8      864	   190	     34

Is there an elegant way to do this type of reformating in R?

Thanks.
Mark

-- 
Mark Dalphin                          email: mdalphin at amgen.com
Mail Stop: 29-2-A                     phone: +1-805-447-4951 (work)
One Amgen Center Drive                       +1-805-375-0680 (home)
Thousand Oaks, CA 91320                 fax: +1-805-499-9955 (work)



From Subramanian_Karthikeyan at hc-sc.gc.ca  Wed Oct 22 21:15:13 2003
From: Subramanian_Karthikeyan at hc-sc.gc.ca (Subramanian Karthikeyan)
Date: Wed, 22 Oct 2003 15:15:13 -0400
Subject: [R] providing a variable as a parameter in a function
Message-ID: <OF9FDF09B1.79814244-ON85256DC7.00693F2B@hc-sc.gc.ca>


Thanks John,

Your solution worked..

After i posted my message I tried sth else which also worked...

I tried this:

for (i in 3:cnum) {
      fla = as.formula(paste((cnom[i],"~","Trt*Dose"))
      mod = lm(fla, data = x, contrasts = list(Trt = contr.sum, Dose =
contr.sum))
      an = Anova(mod,type = "III")
      print(an)
      }

but yours is a succint and nice solution.

Thanks again,
Karth.



                                                                                                                                       
                      John Fox                                                                                                         
                      <jfox at mcmaster.ca        To:       "Subramanian Karthikeyan" <Subramanian_Karthikeyan at hc-sc.gc.ca>               
                      >                        cc:       r-help at stat.math.ethz.ch                                                      
                                               Subject:  Re: [R] providing a variable as a parameter in a function                     
                      2003-10-22 02:24                                                                                                 
                      PM                                                                                                               
                                                                                                                                       
                                                                                                                                       




Dear Subramanian,

How about this:

         for (y in df[, 3:5]) {
                 mod = lm(y ~ Trt*Dose, data = x, contrasts = list(Trt =
contr.sum, Dose = contr.sum))
                 Anova(mod, type = "III")
                 }

Does that give you what you want?

John



At 01:31 PM 10/22/2003 -0400, Subramanian Karthikeyan wrote:
> >From a data frame, how do we extract a specific column name, and plug
that
>into a command (eg. for Anova as shown below)
>
> > df = read.delim("mydata.txt")
> > y = colnames(df)
> > r = ncol(x)
>
>Lets say that in the data frame column 1 contains treatments, column 2
>contains doses, and columns 3, 4, 5 etc. are different responses, and I
>want to run separate 2-way anovas for each response, i.e. my first anova
>will be done using col 1: Treatment, col 2: Dose and Col 3: a response,
>second anova will be done using treatment (col1), dose (col2) and another
>response (col 4) and so on.
>
>I could use a loop to automate the task.
>
> > for (i in 3:r) {
>+ mod = lm(y[i]~Trt*Dose, data = x, contrasts = list(Trt = contr.sum, Dose
>= contr.sum))
>+ Anova(mod, type = "III")
>+ }
>
>The problem is when I directly plug in y[3] for my response variables
name,
>it gives me an error
>
>Error in model.frame(formula, rownames, variables, varnames, extras,
>extranames,  :
>         invalid variable type
>
>This is likely because the lm() function wants the actual column name,
>rather than a variable containing the column name.
>
>Can someone advice?
>
>Thanks,
>Karth.
>
>______________________________________________
>R-help at stat.math.ethz.ch mailing list
>https://www.stat.math.ethz.ch/mailman/listinfo/r-help

____________________________
John Fox
Department of Sociology
McMaster University
email: jfox at mcmaster.ca
web: http://www.socsci.mcmaster.ca/jfox



From p.dalgaard at biostat.ku.dk  Wed Oct 22 21:58:36 2003
From: p.dalgaard at biostat.ku.dk (Peter Dalgaard)
Date: 22 Oct 2003 21:58:36 +0200
Subject: [R] passing a variable (containing the value of the argument) to
	a function
In-Reply-To: <OFCF2D1E81.3A7B0B08-ON85256DC7.00613B92@hc-sc.gc.ca>
References: <OFCF2D1E81.3A7B0B08-ON85256DC7.00613B92@hc-sc.gc.ca>
Message-ID: <x2wuax2ger.fsf@biostat.ku.dk>

"Subramanian Karthikeyan" <Subramanian_Karthikeyan at hc-sc.gc.ca> writes:

> My previous question put in a simpler  way:
> 
> How would I pass a value of a variable to a function such as
> 
> lm(Effect1~Trt*Dose, data = x, contrasts = list(Trt = contr.sum, Dose =
> contr.sum))?
> 
> Here, 'Effect' is a column name in my data matrix, and I want "Effect1" to
> be replaced by "Effect2" and so on (my other column names in the data
> frame) for successive anova calculations. So I am storing the column names
> as an array, and passing the array as a parameter to the lm() function.

A canonical trick is

for (myname in names(myframe)){
  mycall <- substitute(lm(myvar~etc.etc.....),list(myvar=as.name(myname)))
  myfit <- eval(mycall)
  print(summary(myfit))
}

-- 
   O__  ---- Peter Dalgaard             Blegdamsvej 3  
  c/ /'_ --- Dept. of Biostatistics     2200 Cph. N   
 (*) \(*) -- University of Copenhagen   Denmark      Ph: (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)             FAX: (+45) 35327907



From p.dalgaard at biostat.ku.dk  Wed Oct 22 22:02:38 2003
From: p.dalgaard at biostat.ku.dk (Peter Dalgaard)
Date: 22 Oct 2003 22:02:38 +0200
Subject: [R] select text using only the keyboard
In-Reply-To: <6C8A8033ABC1E3468048ABC4F13CE572AC0E49@synequanon01>
References: <6C8A8033ABC1E3468048ABC4F13CE572AC0E49@synequanon01>
Message-ID: <x2smll2g81.fsf@biostat.ku.dk>

"Simon Fear" <Simon.Fear at synequanon.com> writes:

> While I appreciate that to get my wish I should just write
> a little patch, I estimate it would take me about 2 years 
> to reach the point where I was capable of it, assuming I 
> did nothing else, and I would certainly have to understand 
> Windows, which in previous brushes I have found to be 
> very bad for the brain.

Now that actually goes for most members of R core as well, so the
question is really whose brain you'd want to preserve.
 
> I can live without it.

Well *you* said it!  ;-)

-- 
   O__  ---- Peter Dalgaard             Blegdamsvej 3  
  c/ /'_ --- Dept. of Biostatistics     2200 Cph. N   
 (*) \(*) -- University of Copenhagen   Denmark      Ph: (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)             FAX: (+45) 35327907



From ggrothendieck at myway.com  Wed Oct 22 21:59:47 2003
From: ggrothendieck at myway.com (Gabor Grothendieck)
Date: Wed, 22 Oct 2003 15:59:47 -0400 (EDT)
Subject: [R] How to reformat data from database into data.frame?
Message-ID: <20031022195947.B0CE139C3@xmxpita.myway.com>



Assuming your input data frame is genes:

  reshape(genes,idvar="GeneID",timevar="MethodID",direction="wide")


 --- On Wed 10/22, Mark Dalphin < mdalphin at amgen.com > wrote:
From: Mark Dalphin [mailto: mdalphin at amgen.com]
To: r-help at stat.math.ethz.ch
Date: Wed, 22 Oct 2003 12:05:44 -0700
Subject: [R] How to reformat data from database into data.frame?

I'm trying to find a clever way to re-map data from a database<br>query into a data.frame.<br><br>Querying a database often returns a table (data.frame) like this:<br><br>GeneID	 MethodID   Value<br>6	 1	    123<br>6	 2	    456<br>6	 3	    987<br>7	 1	    234<br>7	 3	    432<br>8	 2	    190<br>8	 3	    34<br>8	 1	    864	    <br><br>Note that GeneID=7 doesn't have a value for MethodID=2.<br><br>Note that GeneID=8 doesn't have the MethodID in any particular order<br>and, in fact, there doesn't need to be an order to the GeneID, although<br>I can force ordering as part of my SQL query.<br><br>I want to reformat this into a data.frame with multiple columns:<br><br>       Value	   Value     Value<br>GeneID Method1	   Method2   Method3<br>6      123	   456	     987<br>7      234	   NA	     432<br>8      864	   190	     34<br><br>Is there an elegant way to do this type of reformating in R?<br><br>Thanks.<br>Mark<br><br>-- <br>Mark Dalphin                          email: mdalphin at amgen.com<br>Mail Stop: 29-2-A                     phone: +1-805-447-4951 (work)<br>One Amgen Center Drive                       +1-805-375-0680 (home)<br>Thousand Oaks, CA 91320                 fax: +1-805-499-9955 (work)<br><br>______________________________________________<br>R-help at stat.math.ethz.ch mailing list<br>https://www.stat.math.ethz.ch/mailman/listinfo/r-help<br>

_______________________________________________
No banners. No pop-ups. No kidding.
Introducing My Way - http://www.myway.com



From spencer.graves at pdf.com  Wed Oct 22 22:23:28 2003
From: spencer.graves at pdf.com (Spencer Graves)
Date: Wed, 22 Oct 2003 13:23:28 -0700
Subject: [R] passing a variable (containing the value of the argument)
	to	a function
In-Reply-To: <x2wuax2ger.fsf@biostat.ku.dk>
References: <OFCF2D1E81.3A7B0B08-ON85256DC7.00613B92@hc-sc.gc.ca>
	<x2wuax2ger.fsf@biostat.ku.dk>
Message-ID: <3F96E740.5070709@pdf.com>

Hi, Peter: 

      How does that compare with the following: 

for (myname in names(myframe)[1:4]){
  mdl <- formula(paste(myname, "~ etc.etc"))
  myfit <- lm(mdl, data=myframe)
  print(summary(myfit))
}

      Or: 

for (myname in names(myframe)[1:4]){
  lm.txt <- paste("lm(", myname, "~ etc.etc, data=myframe)")
   myfit <- eval(parse(text=lm.txt))
  print(summary(myfit))
}

      You are teaching me new uses of "substitute", and I just wonder 
about the relative advantages and disadvantages of the different 
approaches. 

      Thanks,
      spencer graves

Peter Dalgaard wrote:

>"Subramanian Karthikeyan" <Subramanian_Karthikeyan at hc-sc.gc.ca> writes:
>
>  
>
>>My previous question put in a simpler  way:
>>
>>How would I pass a value of a variable to a function such as
>>
>>lm(Effect1~Trt*Dose, data = x, contrasts = list(Trt = contr.sum, Dose =
>>contr.sum))?
>>
>>Here, 'Effect' is a column name in my data matrix, and I want "Effect1" to
>>be replaced by "Effect2" and so on (my other column names in the data
>>frame) for successive anova calculations. So I am storing the column names
>>as an array, and passing the array as a parameter to the lm() function.
>>    
>>
>
>A canonical trick is
>
>for (myname in names(myframe)){
>  mycall <- substitute(lm(myvar~etc.etc.....),list(myvar=as.name(myname)))
>  myfit <- eval(mycall)
>  print(summary(myfit))
>}
>
>  
>



From p.dalgaard at biostat.ku.dk  Wed Oct 22 22:35:44 2003
From: p.dalgaard at biostat.ku.dk (Peter Dalgaard)
Date: 22 Oct 2003 22:35:44 +0200
Subject: [R] passing a variable (containing the value of the argument)
	to	a function
In-Reply-To: <3F96E740.5070709@pdf.com>
References: <OFCF2D1E81.3A7B0B08-ON85256DC7.00613B92@hc-sc.gc.ca>
	<x2wuax2ger.fsf@biostat.ku.dk> <3F96E740.5070709@pdf.com>
Message-ID: <x2k76x2eov.fsf@biostat.ku.dk>

Spencer Graves <spencer.graves at pdf.com> writes:

> Hi, Peter:     How does that compare with the following: for (myname
> in names(myframe)[1:4]){
>   mdl <- formula(paste(myname, "~ etc.etc"))
>   myfit <- lm(mdl, data=myframe)
>   print(summary(myfit))
> }
> 
>       Or: for (myname in names(myframe)[1:4]){
>   lm.txt <- paste("lm(", myname, "~ etc.etc, data=myframe)")
>    myfit <- eval(parse(text=lm.txt))
>   print(summary(myfit))
> }
> 
>       You are teaching me new uses of "substitute", and I just wonder
> about the relative advantages and disadvantages of the different
> approaches.     Thanks,
>       spencer graves

Those variants should work (and similar code is all over the place in
the modelling functions). I just tend to prefer to avoid going via the
textual representation. There are a couple of devils lurking in there,
in particular if a data frame has variables with "funny names" -
spaces or special characters inside, for example.

To wit:

> myname <- "foo bar"
> lm.txt <- paste("lm(", myname, "~ etc.etc, data=myframe)")
> lm.txt
[1] "lm( foo bar ~ etc.etc, data=myframe)"
> parse(text=lm.txt)
Error in parse(file, n, text, prompt) : parse error


-- 
   O__  ---- Peter Dalgaard             Blegdamsvej 3  
  c/ /'_ --- Dept. of Biostatistics     2200 Cph. N   
 (*) \(*) -- University of Copenhagen   Denmark      Ph: (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)             FAX: (+45) 35327907



From spencer.graves at pdf.com  Wed Oct 22 22:37:53 2003
From: spencer.graves at pdf.com (Spencer Graves)
Date: Wed, 22 Oct 2003 13:37:53 -0700
Subject: [R] passing a variable (containing the value of the argument)
	to	a function
In-Reply-To: <x2k76x2eov.fsf@biostat.ku.dk>
References: <OFCF2D1E81.3A7B0B08-ON85256DC7.00613B92@hc-sc.gc.ca>	<x2wuax2ger.fsf@biostat.ku.dk>
	<3F96E740.5070709@pdf.com> <x2k76x2eov.fsf@biostat.ku.dk>
Message-ID: <3F96EAA1.1050600@pdf.com>

Hi, Peter:  I'll have to study your example and alternatives.  spencer 
graves

Peter Dalgaard wrote:

>Spencer Graves <spencer.graves at pdf.com> writes:
>
>  
>
>>Hi, Peter:     How does that compare with the following: for (myname
>>in names(myframe)[1:4]){
>>  mdl <- formula(paste(myname, "~ etc.etc"))
>>  myfit <- lm(mdl, data=myframe)
>>  print(summary(myfit))
>>}
>>
>>      Or: for (myname in names(myframe)[1:4]){
>>  lm.txt <- paste("lm(", myname, "~ etc.etc, data=myframe)")
>>   myfit <- eval(parse(text=lm.txt))
>>  print(summary(myfit))
>>}
>>
>>      You are teaching me new uses of "substitute", and I just wonder
>>about the relative advantages and disadvantages of the different
>>approaches.     Thanks,
>>      spencer graves
>>    
>>
>
>Those variants should work (and similar code is all over the place in
>the modelling functions). I just tend to prefer to avoid going via the
>textual representation. There are a couple of devils lurking in there,
>in particular if a data frame has variables with "funny names" -
>spaces or special characters inside, for example.
>
>To wit:
>
>  
>
>>myname <- "foo bar"
>>lm.txt <- paste("lm(", myname, "~ etc.etc, data=myframe)")
>>lm.txt
>>    
>>
>[1] "lm( foo bar ~ etc.etc, data=myframe)"
>  
>
>>parse(text=lm.txt)
>>    
>>
>Error in parse(file, n, text, prompt) : parse error
>
>
>  
>



From John.Fieberg at dnr.state.mn.us  Wed Oct 22 23:40:40 2003
From: John.Fieberg at dnr.state.mn.us (John Fieberg)
Date: Wed, 22 Oct 2003 16:40:40 -0500
Subject: [R] 2 D non-parametric density estimation
Message-ID: <sf96b349.017@co5.dnr.state.mn.us>

I have spatial data in 2 dimensions - say (x,y).  The correlation
between x and y is fairly substantial.  My goal is to use a
non-parametric approach to estimate the multivariate density describing
the spatial locations.  Ultimately, I would like to use this estimated
density to determine the area associated with a 95% probability contour
for the data.

Given the strong correlation between x and y, I have not been real
happy w/ the results obtained using kernel density estimators with
separate smoothing parameters for the x and y directions - e.g., bkde2D
(KernSmooth library), sm (sm library), kde2d (MASS library).   It seems
to me that a better alternative would be to transform the data to have
~0 correlation, estimate the density, then transform back to the
original scale.  Does this seem reasonable for this sort of problem? 
Has anyone written code in R to do this sort of thing?

I also attempted to explore local likelihood fitting (using locfit
library).  I liked the look of the estimated densities, but found it
difficult to obtain predictions at an arbitrary set of grid points (as
needed to determine a 95% probability contour).  Does anyone have
examples using locfit w/ the "ev" option or predict.locfit in order to
obtain local likelihood density estimates at an arbitrary set of grid
points?  

Any suggestions would be greatly appreciated!

John

John Fieberg, Ph.D.
Wildlife Biometrician, MN DNR
5463-C W. Broadway
Forest Lake, MN 55434
Phone: (651) 296-2704



From tblackw at umich.edu  Thu Oct 23 01:18:16 2003
From: tblackw at umich.edu (Thomas W Blackwell)
Date: Wed, 22 Oct 2003 19:18:16 -0400 (EDT)
Subject: [R] 2 D non-parametric density estimation
In-Reply-To: <sf96b349.017@co5.dnr.state.mn.us>
References: <sf96b349.017@co5.dnr.state.mn.us>
Message-ID: <Pine.SOL.4.58.0310221908110.7183@timepilot.gpcc.itd.umich.edu>

John  -

My recollection is that Adrian Raftery's contributed package 'mclust'
does kernel density estimation as well.  Not sure whether it does what
you need.  Take a look at it on CRAN.  Ah..I see that the description
which shows up on Jon Baron's search page is not encouraging.  Give it
a try, anyway.  That description does not do it justice.

-  tom blackwell  -  u michigan medical school  -  ann arbor  -

On Wed, 22 Oct 2003, John Fieberg wrote:

> I have spatial data in 2 dimensions - say (x,y).  The correlation
> between x and y is fairly substantial.  My goal is to use a
> non-parametric approach to estimate the multivariate density describing
> the spatial locations.  Ultimately, I would like to use this estimated
> density to determine the area associated with a 95% probability contour
> for the data.
>
> Given the strong correlation between x and y, I have not been real
> happy w/ the results obtained using kernel density estimators with
> separate smoothing parameters for the x and y directions - e.g., bkde2D
> (KernSmooth library), sm (sm library), kde2d (MASS library).   It seems
> to me that a better alternative would be to transform the data to have
> ~0 correlation, estimate the density, then transform back to the
> original scale.  Does this seem reasonable for this sort of problem?
> Has anyone written code in R to do this sort of thing?
>
> I also attempted to explore local likelihood fitting (using locfit
> library).  I liked the look of the estimated densities, but found it
> difficult to obtain predictions at an arbitrary set of grid points (as
> needed to determine a 95% probability contour).  Does anyone have
> examples using locfit w/ the "ev" option or predict.locfit in order to
> obtain local likelihood density estimates at an arbitrary set of grid
> points?
>
> Any suggestions would be greatly appreciated!
>
> John
>
> John Fieberg, Ph.D.
> Wildlife Biometrician, MN DNR
> 5463-C W. Broadway
> Forest Lake, MN 55434
> Phone: (651) 296-2704
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
>



From mn216 at columbia.edu  Thu Oct 23 08:00:46 2003
From: mn216 at columbia.edu (Murad Nayal)
Date: Thu, 23 Oct 2003 02:00:46 -0400
Subject: [R] anova model refinement/clustering question
Message-ID: <3F976E8E.FBB51568@columbia.edu>



Hi,

I am trying to refine models of a continuous response variable and a
number of categorical predictor variables. I know of some model
refinement tools available in R that help in the selection of model
terms like dropterm and addterm from MASS etc. However, I would also
like to try to refine the model by 'coalescing' some levels of some of
the predictor factors. Is there a standard procedure / R-functions that
will allow me to do this.

This might be naive but I thought that one way to do this is to perform
a pairwise comparison between all levels, say using tukeyHSD, and
coalesce levels that do not have a statistically significant difference
in the average of the response variable between them. so in a way this
becomes a clustering problem. is there a relatively easy way to do this
in R, say short of trying to figure out how to make the relevant
tukeyHSD output look like a dist object and trick hclust into using it. 

I am somewhat of an amateur in the field (and R) and I am probably
making that obvious. any guidance to the 'right' path to approach this
(privately or on the list) is really appreciated.

many thanks
Murad



-- 
Murad Nayal M.D. Ph.D.
Department of Biochemistry and Molecular Biophysics
College of Physicians and Surgeons of Columbia University
630 West 168th Street. New York, NY 10032
Tel: 212-305-6884	Fax: 212-305-6926



From maechler at stat.math.ethz.ch  Thu Oct 23 08:29:30 2003
From: maechler at stat.math.ethz.ch (Martin Maechler)
Date: Thu, 23 Oct 2003 08:29:30 +0200
Subject: [R] do.call() and aperm()
In-Reply-To: <5.2.1.1.2.20031021101329.040e2400@mailhost.blackmesacapital.com>
References: <a06002003bbbb058f49db@[139.166.242.29]>
	<5.2.1.1.2.20031021101329.040e2400@mailhost.blackmesacapital.com>
Message-ID: <16279.30026.241991.237507@gargle.gargle.HOWL>

>>>>> "Tony" == Tony Plate <tplate at blackmesacapital.com>
>>>>>     on Tue, 21 Oct 2003 10:17:05 -0600 writes:

    Tony> I've also been thinking about how to specify that 'along' should be 
    Tony> length(dim)+1.  At the moment one can specify any number from 0 up to 
    Tony> length(dim)+1, but as you point out you have to spell out length(dim)+1 as 
    Tony> the value for the along argument.  It would possible to make abind() 
    Tony> automatically calculate along=length(dim)+1 when given along=NA, or 
    Tony> along=-1, or along="+1".  Any preferences?

I'd take a string, as your last proposal does,  (since I think
such use should not be confoundable with `integer - use' -- in
order to catch programming errors of the caller).

Martin Maechler <maechler at stat.math.ethz.ch>	http://stat.ethz.ch/~maechler/
Seminar fuer Statistik, ETH-Zentrum  LEO C16	Leonhardstr. 27
ETH (Federal Inst. Technology)	8092 Zurich	SWITZERLAND
phone: x-41-1-632-3408		fax: ...-1228			<><



From bw at northbranchlogic.com  Thu Oct 23 08:38:06 2003
From: bw at northbranchlogic.com (Barnet Wagman)
Date: Thu, 23 Oct 2003 01:38:06 -0500
Subject: [R] Can you create a MySQL database with RMySQL?
Message-ID: <3F97774E.9080104@northbranchlogic.com>

Is it possible to create a database in MySQL via RMySQL?

Also, is the format for the authorization field 
'userName/password at databasename'?  I saw an example like this somewhere 
in the documenation, but I haven't found the actual specification.

Thanks,

Barnet Wagman



From maechler at stat.math.ethz.ch  Thu Oct 23 08:40:15 2003
From: maechler at stat.math.ethz.ch (Martin Maechler)
Date: Thu, 23 Oct 2003 08:40:15 +0200
Subject: [R] R-help login page error
In-Reply-To: <3F95DD77.8040805@telgua.com.gt>
References: <3F95DD77.8040805@telgua.com.gt>
Message-ID: <16279.30671.511203.580364@gargle.gargle.HOWL>

>>>>> "H?ctor" == H?ctor Villafuerte D <hec.villafuerte at telgua.com.gt>
>>>>>     on Tue, 21 Oct 2003 17:29:27 -0800 writes:

    H?ctor> Hi all,
    H?ctor> I'm trying to access my account at
    H?ctor> https://www.stat.math.ethz.ch/mailman/options/r-help
    H?ctor> but the following appears:
    H?ctor> *Error: */Authentication failed./

    H?ctor> Yes, I've already checked my password. Is it a problem
    H?ctor> with mailman or something?

this could well be the case.
Recently, the web server has been moved (from an older Sun-Solaris
to a new Intel-Linux box) and we have checked quite a few things
that did work fine.

OTOH, mailman authentication has been a problem from time to
time, and we've never found the time and energy to track the
problem completely, as it seemed to only exist "randomly".  As a
matter of fact, I had hoped that the mailman upgrade (from 2.1.2
to 2.1.3) about 10 days ago might solve some of these
authentication problems.  I'm vaguely guessing that the problem
could also be on your side (some proxy/firewall/... feature)...

However if we want to follow this up, please do NOT use R-help!
(you might use R-help-owner at ....)

Martin Maechler <maechler at stat.math.ethz.ch>	http://stat.ethz.ch/~maechler/
Seminar fuer Statistik, ETH-Zentrum  LEO C16	Leonhardstr. 27
ETH (Federal Inst. Technology)	8092 Zurich	SWITZERLAND
phone: x-41-1-632-3408		fax: ...-1228			<><



From Vincent.Spiesser at univ-tlse1.fr  Thu Oct 23 11:04:31 2003
From: Vincent.Spiesser at univ-tlse1.fr (Vincent Spiesser)
Date: Thu, 23 Oct 2003 11:04:31 +0200
Subject: [R] multivariate ARMA analysis with R
Message-ID: <4.2.0.58.20031023110134.00a3d4c0@mail.univ-tlse1.fr>

Hi,
I try to make a multivariate ARMA analysis.
Function arima (from ts packages only accept univariate argument.

Does an R function exist for multivariate analysis ?

Thanks
Vincent Spiesser



From ripley at stats.ox.ac.uk  Thu Oct 23 12:11:53 2003
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Thu, 23 Oct 2003 11:11:53 +0100 (BST)
Subject: [R] multivariate ARMA analysis with R
In-Reply-To: <4.2.0.58.20031023110134.00a3d4c0@mail.univ-tlse1.fr>
Message-ID: <Pine.LNX.4.44.0310231109090.27437-100000@gannet.stats>

Look in package bundle DSE for multiple ARMA processes.

(I would say multivariate and multiple time series are different things, 
but guess multiple is what you want.)


On Thu, 23 Oct 2003, Vincent Spiesser wrote:

> Hi,
> I try to make a multivariate ARMA analysis.
> Function arima (from ts packages only accept univariate argument.
> 
> Does an R function exist for multivariate analysis ?

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From christian.schulz at questico.de  Thu Oct 23 12:31:01 2003
From: christian.schulz at questico.de (Christian Schulz)
Date: Thu, 23 Oct 2003 12:31:01 +0200
Subject: AW: [R] Can you create a MySQL database with RMySQL?
In-Reply-To: <3F97774E.9080104@northbranchlogic.com>
Message-ID: <JAEELBHBOPKJDMMCNHKMGEKACAAA.christian.schulz@questico.de>


>>library(DBI)
>>library(RMySQL)
>>drv <- dbDriver("MySQL")
>>con <- dbConnect(drv,"test","root","")
>>dbListTables(con)
[1] "dmjanuar"  "dmjuli"    "dmmai"     "model0"    "ndata"     "newtable"
[7] "uebergang" "weka"
>>mysqlQuickSQL(con, "Create table newTable( attr1 int, attr2 int)")
>>dbListTables(con)
[1] "dmjanuar"  "dmjuli"    "dmmai"     "model0"    "ndata"     "newtable"
[7] "uebergang" "weka"
>>
>>dbListFields(con,"newTable")
[1] "attr1" "attr2"
>>

regards,christian


-----Urspr?ngliche Nachricht-----
Von: r-help-bounces at stat.math.ethz.ch
[mailto:r-help-bounces at stat.math.ethz.ch]Im Auftrag von Barnet Wagman
Gesendet: Donnerstag, 23. Oktober 2003 08:38
An: R-help at stat.math.ethz.ch
Betreff: [R] Can you create a MySQL database with RMySQL?


Is it possible to create a database in MySQL via RMySQL?

Also, is the format for the authorization field
'userName/password at databasename'?  I saw an example like this somewhere
in the documenation, but I haven't found the actual specification.

Thanks,

Barnet Wagman

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://www.stat.math.ethz.ch/mailman/listinfo/r-help



From ahenningsen at email.uni-kiel.de  Thu Oct 23 13:05:30 2003
From: ahenningsen at email.uni-kiel.de (Arne Henningsen)
Date: Thu, 23 Oct 2003 13:05:30 +0200
Subject: [R] nlm, hessian, and derivatives in obj function?
In-Reply-To: <001701c39509$f58eda10$0a00a8c0@rodan>
References: <001701c39509$f58eda10$0a00a8c0@rodan>
Message-ID: <200310231305.30741.ahenningsen@email.uni-kiel.de>

Hi,

I don't know much about non-linear models, but there is another possibility to 
fit these models: 

1) get some starting values for the parameters
2) take the derivatives of the model with respect to the parameters at the 
point of the starting values of the parameters
3) perform a linear estimation of this linearized model (using systemfit) to 
get new parameter estimates
4) got to step 2) and take these new parameter estimates in place of the 
starting values
5) iterate this until the parameters stay stable from one to the next 
iteration

This has three advantages:
1) It is not much work to write these function since systemfit already exists
2) If the model is linear in parameters, it is identical to the linearized 
model and, thus, the first iteration leads directly to the optimum
3) You get get the SEs from the last iteration of systemfit

Does this approach also have disadvantages (e.g. non-convergence of parameters 
in many cases)?

Best wishes,
Arne


On Saturday 18 October 2003 01:54, Jeff D. Hamann wrote:
> I've been working on a new package and I have a few questions regarding the
> behaviour of the nlm function. I've been (for better or worse) using the
> nlm function to fit a linear model without suppling the hessian or gradient
> attributes in the objective function. I'm curious as to why the nlm
> requires 31 iterations (for the linear model), and then it doesn't work
> when I try to add the derivative information. I know using nlm for a linear
> model isn't the "optimal" method, but I would like to make sure the
> parameter estimates and the se's are matching before I attempt more
> difficult problems.
>
> rm(list=ls(all=TRUE))
> print( "running nlsystemfit models test at end...")
> data( kmenta )
> attach( kmenta )
> ##demand2 <- q ~ d0 + d1 * p + d2 * d
> supply2 <- q ~ s0 + s1 * p + s2 * f + s3 * a
> ##system2 <- list( demand2, supply2 )
> ##labels <- list( "Demand", "Supply" )
> ##inst <- ~ d + f + a
> ##sv2 <- c(d0=3,s2=2.123,d2=4,s0=-2.123,s3=4.234,d1=4.234,s1=0.234)
> sv2 <- c(s0=-2.123,s1=0.234,s2=2.123,s3=4.234)
>
> obj <- function( s, eqn, data, parmnames )
> {
>   ## get the values of the parameters
>   for( i in 1:length( parmnames ) )
>     {
>       name <- names( parmnames )[i]
>       val <- s[i]
>       storage.mode( val ) <-  "double"
>       assign( name, val )
>     }
>
>   lhs <- as.matrix( eval( as.formula( eqn )[[2]] ) )
>   rhs <- as.matrix( eval( as.formula( eqn )[[3]] ) )
>   resid <- crossprod( lhs - rhs )
>
>   ## just how does this work...
>   attr( obj, "value" ) <- resid
>   attr( obj, "gradient" ) <- attr( eval( deriv3( eqn, names(
> parmnames ) ) ), "gradient" )
>
> }
>
> res <- nlm( obj, sv2, hessian=T, eqn=supply2, data=kmenta, parmnames=sv2,
> check.analyticals=T)
>
> I haven't been able to get nlm to function as I keep getting the following
> error message:
>
> Error in nlm(obj, sv2, hessian = T, eqn = supply2, data = kmenta, parmnames
> = sv2,  :
>  invalid function value in 'nlm' optimizer
>
>
> If I perform the fit without the derivative information, I get the correct
> estimates,
>
> $minimum
> [1] 92.55106
>
> $estimate
> [1] 58.2754312  0.1603666  0.2481333  0.2483023
>
> $gradient
> [1] 8.552542e-08 9.087699e-06 5.716032e-06 2.163105e-06
>
> $hessian
>          [,1]       [,2]     [,3]     [,4]
> [1,]   40.000   4000.762   3865.0   420.00
> [2,] 4000.762 401486.918 386045.8 42007.76
> [3,] 3865.000 386045.812 379593.1 39762.40
> [4,]  420.000  42007.764  39762.4  5740.00
>
> $code
> [1] 1
>
> $iterations
> [1] 31
>
> I was under the impression that you could also obtain the se of the
> parameter estimates using the sqrt( diag( res$hessian ) ), but I haven't
> been able to reproduce the se computed by the Jacobian
>
> se <- sqrt( mse * diag( solve( crossprod( J ) ) ) )    # gives the correct
> results...
> hse <- sqrt( ( res$minimum / 8 ) * diag( solve( res$hessian ) ) )   # gives
> similar results, but why 8?
>
> I've tried to put the functionality to include the jacobian and hessian in
> the objective function for nlm without success as I don't know what the
> form of the functions will be ahead of time.
>
> and get the se from the sqrt( diag( hessian ) ), but it's nowhere close?
>
> Jeff.
>
> ---
> Jeff D. Hamann
> Hamann, Donald and Associates, Inc.
> PO Box 1421
> Corvallis, Oregon USA 97339-1421
> (office) 541-754-1428
> (cell) 541-740-5988
> jeff_hamann at hamanndonald.com
> www.hamanndonald.com
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help

-- 
Arne Henningsen
Department of Agricultural Economics
Christian-Albrechts-University Kiel
24098 Kiel, Germany
Tel: +49-431-880-4445
Fax: +49-431-880-1397 
ahenningsen at email.uni-kiel.de
http://www.uni-kiel.de/agrarpol/ahenningsen/



From oehl_list at gmx.de  Thu Oct 23 13:36:48 2003
From: oehl_list at gmx.de (Jens =?ISO-8859-1?Q?Oehlschl=E4gel?=)
Date: Thu, 23 Oct 2003 13:36:48 +0200 (MEST)
Subject: [R] GIS re-mapping / polygon overlap
Message-ID: <32412.1066909008@www59.gmx.net>


In Germany the Unemployment Agency uses a sectioning of the german map that
is different from the usual Administrative Boundaries. 
Some demographic data are available in Administrative Boundaries only, some
in Unemployment Boundaries only.

I would like to generate estimates in one boundary system of data availabe
in the other boundary system, and would appreciate advice concerning the
following questions:

- can someone provide mapping functions between german Administrative
Boundaries (Kreise, Gemeinden, ...) and Unemployment Boundaries (Arbeits?mter)
OR
  - can someone provide polygon data of Unemployment Boundaries and/or
Administrative Boundaries
  AND
    - is there a general function in R available to map demographic data
between two boundary systems
    OR
    - is there an efficient R function that can determine the amount of area
overlap between two general polygons (beyond gpclib)

I am willing to share developed code for this problem (that's why I would
prefer gpl code over gpclib, which is not really free)

Thanks for any help


Jens Oehlschl?gel


-- 
NEU F?R ALLE - GMX MediaCenter - f?r Fotos, Musik, Dateien...
Fotoalbum, File Sharing, MMS, Multimedia-Gru?, GMX FotoService

Jetzt kostenlos anmelden unter http://www.gmx.net

+++ GMX - die erste Adresse f?r Mail, Message, More! +++



From petzoldt at rcs.urz.tu-dresden.de  Thu Oct 23 13:47:56 2003
From: petzoldt at rcs.urz.tu-dresden.de (Thomas Petzoldt)
Date: Thu, 23 Oct 2003 13:47:56 +0200
Subject: [R] OOP like handling of lists?
Message-ID: <3F97BFEC.2030302@rcs.urz.tu-dresden.de>

Hello,

I am writing a package with a collection of several models. In order to 
allow users to play interactively with the models (in contrast to 
hacking lengthy scripts), I want to put all what is needed to run a 
particular model into a single list object for each model.

Then there will be a collection of functions to run the model or to 
modify parameters, time steps, integration method ..., which should 
*work on the list itself* or make a copy of it.

An example:

the model object may have the name "lvmodel" (see below). so it can be 
simulated and plotted simply using:

lvmodel <- simulate(lvmodel)
plot(lvmodel)

Parameters (and other stuff) may be modified with:

getParams(lvmodel)
lvmodel <- setParms(lvmodel, list(k1=0.5))

... and then simulated and plotted again.

The problem however is, that for functions which modify list elements an 
assignement to a new (or the same) variable MUST exist.

I want to write simply:

setParms(lvmodel, list(k1=0.5))

and not

lvmodel <- setParms(lvmodel, list(k1=0.5))

or at least get a warning, if the assignement is missing. I don't want 
to break the R philosophy of function parameter handling. On the other 
side the full OOP-approach in R-News 1(2002)3 of Chambers and Lang 
works, but may be a little bit to complicated to explain it to my 
collegues and students. So, is there an alternative to do such things, 
which I may have overlooked?

Thank you!

Thomas Petzoldt


#####################################################################
#A simplified working example
#####################################################################

library(odesolve)

## The differential equation model ##################################
lvmodel<-list(
     equations = function(t, x, p) {
       dx1.dt <-   p["k1"] * x[1] - p["k2"] * x[1] * x[2]
       dx2.dt <- - p["k3"] * x[2] + p["k2"] * x[1] * x[2]
       list(c(dx1.dt, dx2.dt))
     },
     parms  = c(k1=0.2, k2=0.2, k3=0.2),
     xstart = c(prey=0.5, predator=1)
     # and some more elements ...
)
class(lvmodel) <- "odemodel"

## Getting and setting parameters ###################################

getParms <- function(model) {
   model$parms
}

setParms <- function(model, parmlist) {
   for (i in 1:length(parmlist)) {
     model$parms[names(parmlist[i])] <- parmlist[[i]]
   }
   invisible(model)
}

## Simulation #######################################################

simulate <- function(model, ...) {
   times <- seq(0, 100, 0.1)
   res <- lsoda(model$xstart, times, model$equation, model$parms, ...)
   model$out <- as.data.frame(res)
   model
}

## Plotting

plot.odemodel <- function(model) {
     oldpar <- par(no.readonly=TRUE)
     par(mfrow=c(2, 1))
     nam <- names(model$out)
     for (i in 2:ncol(model$out)) {
       plot(model$out[[1]], model$out[[i]],
            type="l", xlab=nam[1], ylab=nam[i])
     }
     par(oldpar)
}

#### MAIN PROGRAM #########

lvmodel <- simulate(lvmodel)
plot(lvmodel)

getParms(lvmodel)
lvmodel <- setParms(lvmodel, list(k1=0.5))
plot(simulate(lvmodel))



From mailinglist2_wegmann at web.de  Thu Oct 23 13:55:29 2003
From: mailinglist2_wegmann at web.de (Martin Wegmann)
Date: Thu, 23 Oct 2003 13:55:29 +0200
Subject: [R] GIS re-mapping / polygon overlap
In-Reply-To: <32412.1066909008@www59.gmx.net>
References: <32412.1066909008@www59.gmx.net>
Message-ID: <200310231355.29396.mailinglist2_wegmann@web.de>

On Thursday 23 October 2003 13:36, Jens Oehlschl?gel wrote:
> In Germany the Unemployment Agency uses a sectioning of the german map that
> is different from the usual Administrative Boundaries.
> Some demographic data are available in Administrative Boundaries only, some
> in Unemployment Boundaries only.
>
> I would like to generate estimates in one boundary system of data availabe
> in the other boundary system, and would appreciate advice concerning the
> following questions:
>
> - can someone provide mapping functions between german Administrative
> Boundaries (Kreise, Gemeinden, ...) and Unemployment Boundaries
> (Arbeits?mter) OR
>   - can someone provide polygon data of Unemployment Boundaries and/or
> Administrative Boundaries
>   AND
>     - is there a general function in R available to map demographic data
> between two boundary systems
>     OR
>     - is there an efficient R function that can determine the amount of
> area overlap between two general polygons (beyond gpclib)
>
> I am willing to share developed code for this problem (that's why I would
> prefer gpl code over gpclib, which is not really free)
>
> Thanks for any help
>
>
> Jens Oehlschl?gel


I am not sure, but it sounds more that you are kooking for a GPL GIS like 
GRASS http://grass.itc.it , have a look at the 5.3/7 development branch (I am 
not sure if it is already usable but 5.0.3 is close to official release), 
they have such things as amount overlap and other gis commands.
Thuban could be worth a try as well,http://freegis.org/index.de.html

cheers Martin



From solares at unsl.edu.ar  Thu Oct 23 14:29:51 2003
From: solares at unsl.edu.ar (solares@unsl.edu.ar)
Date: Thu, 23 Oct 2003 09:29:51 -0300 (ART)
Subject: [R] R command for input/output with parallel port
Message-ID: <47022.170.210.173.216.1066912191.squirrel@inter14.unsl.edu.ar>

Hello, my question is,Exist a command for to make input from parallel port
, i have a sensor connected to parallel port, and i need have the data in 
real time,i need obtain the data and put in .txt file. Thank Ruben



From pburns at pburns.seanet.com  Thu Oct 23 14:19:43 2003
From: pburns at pburns.seanet.com (Patrick Burns)
Date: Thu, 23 Oct 2003 13:19:43 +0100
Subject: [R] OOP like handling of lists?
References: <3F97BFEC.2030302@rcs.urz.tu-dresden.de>
Message-ID: <3F97C75F.3030400@pburns.seanet.com>

I think what you want to write is:

setParms(lvmodel) <- list(k1=0.5)

which means that you need to define an assignment function.
If you are using S3 style OOP (which you appear to be), then
do something along the lines of:

 'setParms<-' <- function(x, value) UseMethod('setParms<-')
 'setParms<-.odemodel' <- function(x, value) {x$parms <- value; x}

Aside:  You might want to use "on.exit" to reset par in the plot
function.

Patrick Burns

Burns Statistics
patrick at burns-stat.com
+44 (0)20 8525 0696
http://www.burns-stat.com
(home of S Poetry and "A Guide for the Unwilling S User")




Thomas Petzoldt wrote:

> Hello,
>
> I am writing a package with a collection of several models. In order 
> to allow users to play interactively with the models (in contrast to 
> hacking lengthy scripts), I want to put all what is needed to run a 
> particular model into a single list object for each model.
>
> Then there will be a collection of functions to run the model or to 
> modify parameters, time steps, integration method ..., which should 
> *work on the list itself* or make a copy of it.
>
> An example:
>
> the model object may have the name "lvmodel" (see below). so it can be 
> simulated and plotted simply using:
>
> lvmodel <- simulate(lvmodel)
> plot(lvmodel)
>
> Parameters (and other stuff) may be modified with:
>
> getParams(lvmodel)
> lvmodel <- setParms(lvmodel, list(k1=0.5))
>
> ... and then simulated and plotted again.
>
> The problem however is, that for functions which modify list elements 
> an assignement to a new (or the same) variable MUST exist.
>
> I want to write simply:
>
> setParms(lvmodel, list(k1=0.5))
>
> and not
>
> lvmodel <- setParms(lvmodel, list(k1=0.5))
>
> or at least get a warning, if the assignement is missing. I don't want 
> to break the R philosophy of function parameter handling. On the other 
> side the full OOP-approach in R-News 1(2002)3 of Chambers and Lang 
> works, but may be a little bit to complicated to explain it to my 
> collegues and students. So, is there an alternative to do such things, 
> which I may have overlooked?
>
> Thank you!
>
> Thomas Petzoldt
>
>
> #####################################################################
> #A simplified working example
> #####################################################################
>
> library(odesolve)
>
> ## The differential equation model ##################################
> lvmodel<-list(
>     equations = function(t, x, p) {
>       dx1.dt <-   p["k1"] * x[1] - p["k2"] * x[1] * x[2]
>       dx2.dt <- - p["k3"] * x[2] + p["k2"] * x[1] * x[2]
>       list(c(dx1.dt, dx2.dt))
>     },
>     parms  = c(k1=0.2, k2=0.2, k3=0.2),
>     xstart = c(prey=0.5, predator=1)
>     # and some more elements ...
> )
> class(lvmodel) <- "odemodel"
>
> ## Getting and setting parameters ###################################
>
> getParms <- function(model) {
>   model$parms
> }
>
> setParms <- function(model, parmlist) {
>   for (i in 1:length(parmlist)) {
>     model$parms[names(parmlist[i])] <- parmlist[[i]]
>   }
>   invisible(model)
> }
>
> ## Simulation #######################################################
>
> simulate <- function(model, ...) {
>   times <- seq(0, 100, 0.1)
>   res <- lsoda(model$xstart, times, model$equation, model$parms, ...)
>   model$out <- as.data.frame(res)
>   model
> }
>
> ## Plotting
>
> plot.odemodel <- function(model) {
>     oldpar <- par(no.readonly=TRUE)
>     par(mfrow=c(2, 1))
>     nam <- names(model$out)
>     for (i in 2:ncol(model$out)) {
>       plot(model$out[[1]], model$out[[i]],
>            type="l", xlab=nam[1], ylab=nam[i])
>     }
>     par(oldpar)
> }
>
> #### MAIN PROGRAM #########
>
> lvmodel <- simulate(lvmodel)
> plot(lvmodel)
>
> getParms(lvmodel)
> lvmodel <- setParms(lvmodel, list(k1=0.5))
> plot(simulate(lvmodel))
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
>
>



From fm3a004 at math.uni-hamburg.de  Thu Oct 23 15:25:20 2003
From: fm3a004 at math.uni-hamburg.de (Christian Hennig)
Date: Thu, 23 Oct 2003 15:25:20 +0200 (MET DST)
Subject: [R] Weighted Clustering
In-Reply-To: <Pine.SOL.4.44.0310221229380.18355-100000@er7.rutgers.edu>
Message-ID: <Pine.GSO.3.95q.1031023152046.14383V-100000@sun12.math.uni-hamburg.de>

Hi,

the target functions of k-means clusterung and of normal mixture model
clustering (in library mclust) should work with weighted data points as
well. This is, however, only a theoretical suggestion, because as far as I
know, it is not implemented in R, and the R-functions for kmeans and 
model based clustering call C and fortran code, which I think is
not too easy to adapt to your problem.
If I would be in your situation, I would presumably write new code for a
k-means algorithm (there are some, which are very easy to implement) with
weighted data.

Best,
Christian

On Wed, 22 Oct 2003, vishal goyal wrote:

> Hi,
> 
> I have a data set(say 2-d demands of a product (say flow-rate vs
> concentration)) and with each demand is the weightage (like a probability)
> of that demand occuring. Is there a way to cluster this demand-data
> (deterministic or probabilistic(if possible)) which also incorporates the
> weights (just multiplying distances with weights gives poor and mixed
> clusters) while clustering (Something like a Facility Location problem).
> I would appreciate any advice.
> 
> Regards
> Vishal Goyal
> 
> 
> 
> 
> 
> 
> 
> 
> --------------------------------------------------------------------------
> "Simplicity is the ultimate sophistication"
> ---------------------------------------------------------------------------
> 
> Vishal Goyal, Graduate Student
> Department of Chemical and Biochemical Engineering
> Rutgers - The State University of New Jersey
> 98 Brett Road, Piscataway, NJ 08854
> tel: 732-445-7061 (O)
> email: vigoyal at eden.rutgers.edu
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> 

***********************************************************************
Christian Hennig
Fachbereich Mathematik-SPST/ZMS, Universitaet Hamburg
hennig at math.uni-hamburg.de, http://www.math.uni-hamburg.de/home/hennig/
#######################################################################
ich empfehle www.boag-online.de



From Subramanian_Karthikeyan at hc-sc.gc.ca  Thu Oct 23 15:29:46 2003
From: Subramanian_Karthikeyan at hc-sc.gc.ca (Subramanian Karthikeyan)
Date: Thu, 23 Oct 2003 09:29:46 -0400
Subject: [R] Writing and running a R program
Message-ID: <OF61C70719.C7632BD4-ON85256DC8.0049A1F3@hc-sc.gc.ca>

Is there a way I can combine multiple lines of R commands (see below) into
a little code snippet or a program in a text file, and run it in R to do my
analysis?

sink("mysink.txt")
for (......) {
code for creating a dataframe from supplied data
code for doing anova from selected data
}

Thanks very much.

Karth.



From anna at email.arc.nasa.gov  Thu Oct 23 15:47:46 2003
From: anna at email.arc.nasa.gov (Anna Pryor)
Date: Thu, 23 Oct 2003 06:47:46 -0700
Subject: [R] repeating colors in graph
Message-ID: <200310230647.46685.anna@email.arc.nasa.gov>

I have a plot with 13 lines and R repeats colors.  Is there any way to make R 
not repeat colors, or is this just a fundamental limitation on R's part to 
say have only 8 colors that it can use?

Anna



From ahenningsen at email.uni-kiel.de  Thu Oct 23 15:54:22 2003
From: ahenningsen at email.uni-kiel.de (Arne Henningsen)
Date: Thu, 23 Oct 2003 15:54:22 +0200
Subject: [R] Writing and running a R program
In-Reply-To: <OF61C70719.C7632BD4-ON85256DC8.0049A1F3@hc-sc.gc.ca>
References: <OF61C70719.C7632BD4-ON85256DC8.0049A1F3@hc-sc.gc.ca>
Message-ID: <200310231554.22854.ahenningsen@email.uni-kiel.de>

source("mycode.R")

On Thursday 23 October 2003 15:29, Subramanian Karthikeyan wrote:
> Is there a way I can combine multiple lines of R commands (see below) into
> a little code snippet or a program in a text file, and run it in R to do my
> analysis?
>
> sink("mysink.txt")
> for (......) {
> code for creating a dataframe from supplied data
> code for doing anova from selected data
> }
>
> Thanks very much.
>
> Karth.
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help

-- 
Arne Henningsen
Department of Agricultural Economics
Christian-Albrechts-University Kiel
24098 Kiel, Germany
Tel: +49-431-880-4445
Fax: +49-431-880-1397 
ahenningsen at email.uni-kiel.de
http://www.uni-kiel.de/agrarpol/ahenningsen/



From ligges at statistik.uni-dortmund.de  Thu Oct 23 16:02:43 2003
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Thu, 23 Oct 2003 16:02:43 +0200
Subject: [R] repeating colors in graph
In-Reply-To: <200310230647.46685.anna@email.arc.nasa.gov>
References: <200310230647.46685.anna@email.arc.nasa.gov>
Message-ID: <3F97DF83.1090303@statistik.uni-dortmund.de>

Anna Pryor wrote:
> I have a plot with 13 lines and R repeats colors.  Is there any way to make R 
> not repeat colors, or is this just a fundamental limitation on R's part to 
> say have only 8 colors that it can use?

See ?colors and the other help topics linked in its "See Also" Section.

Uwe Ligges



> Anna
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help



From ligges at statistik.uni-dortmund.de  Thu Oct 23 16:03:37 2003
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Thu, 23 Oct 2003 16:03:37 +0200
Subject: [R] Writing and running a R program
In-Reply-To: <OF61C70719.C7632BD4-ON85256DC8.0049A1F3@hc-sc.gc.ca>
References: <OF61C70719.C7632BD4-ON85256DC8.0049A1F3@hc-sc.gc.ca>
Message-ID: <3F97DFB9.9080803@statistik.uni-dortmund.de>

Subramanian Karthikeyan wrote:

> Is there a way I can combine multiple lines of R commands (see below) into
> a little code snippet or a program in a text file, and run it in R to do my
> analysis?
> 
> sink("mysink.txt")
> for (......) {
> code for creating a dataframe from supplied data
> code for doing anova from selected data
> }
> 
> Thanks very much.
> 
> Karth.
> 

I guess you are looking for ?source.

Uwe Ligges



From emb7 at st-andrews.ac.uk  Thu Oct 23 16:14:21 2003
From: emb7 at st-andrews.ac.uk (Martin Biuw)
Date: Thu, 23 Oct 2003 15:14:21 +0100
Subject: [R] Stochastic dynamic programming
Message-ID: <oprxh1t7iw8xmvrg@gatty.st-and.ac.uk>

Hello all,
Does anyone know of any already written functions for carrying out 
stochastic dynamic programming in R?

Thanks!

Martin

-- 
Martin Biuw
Sea Mammal Research Unit
Gatty Marine Laboratory, University of St Andrews
St Andrews, Fife KY16 8PA
Scotland
Ph: +44-(0)1334-462637
Fax: +44-(0)1334-462632
Web: http://smub.st.and.ac.uk



From paolo.radaelli at unimib.it  Thu Oct 23 16:19:32 2003
From: paolo.radaelli at unimib.it (Radaelli Paolo - Dottorati di Ricerca)
Date: Thu, 23 Oct 2003 16:19:32 +0200
Subject: [R] Quantreg Package
Message-ID: <00e701c39970$adb2f800$6e788495@dimequant.unimib.it>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20031023/cb35d33f/attachment.pl

From p.pagel at gsf.de  Thu Oct 23 16:20:02 2003
From: p.pagel at gsf.de (Philipp Pagel)
Date: Thu, 23 Oct 2003 16:20:02 +0200
Subject: [R] Writing and running a R program
In-Reply-To: <OF61C70719.C7632BD4-ON85256DC8.0049A1F3@hc-sc.gc.ca>
References: <OF61C70719.C7632BD4-ON85256DC8.0049A1F3@hc-sc.gc.ca>
Message-ID: <20031023142002.GA4928@porcupine.gsf.de>


On Thu, Oct 23, 2003 at 09:29:46AM -0400, Subramanian Karthikeyan wrote:
> Is there a way I can combine multiple lines of R commands (see below) into
> a little code snippet or a program in a text file, and run it in R to do my
> analysis?

It's quite easy: Write your R code with your favorite text editor and run
it in R using source().

cu
	Philipp

-- 
Dr. Philipp Pagel                                Tel.  +49-89-3187-3675
Institute for Bioinformatics / MIPS              Fax.  +49-89-3187-3585
GSF - National Research Center for Environment and Health
Ingolstaedter Landstrasse 1
85764 Neuherberg, Germany



From roger at ysidro.econ.uiuc.edu  Thu Oct 23 16:49:29 2003
From: roger at ysidro.econ.uiuc.edu (Roger Koenker)
Date: Thu, 23 Oct 2003 09:49:29 -0500 (CDT)
Subject: [R] Quantreg Package
In-Reply-To: <00e701c39970$adb2f800$6e788495@dimequant.unimib.it>
Message-ID: <Pine.SOL.4.30.0310230944110.19787-100000@ysidro.econ.uiuc.edu>

I'm not sure why quantreg isn't in the windows binaries on CRAN,
perhaps Uwe can explain this...if there is a problem I'd be happy
to try to help, but I don't have access to a windows machine myself.

url:	www.econ.uiuc.edu/~roger/my.html	Roger Koenker
email	rkoenker at uiuc.edu			Department of Economics
vox: 	217-333-4558				University of Illinois
fax:   	217-244-6678				Champaign, IL 61820

On Thu, 23 Oct 2003, Radaelli Paolo - Dottorati di Ricerca wrote:

> I've just installed R 1.0.8 (for Windows) and I tried to install the package Quantreg directly from Cran but it's not in the list of downlodable packages.
> I tried also downloading the zip file and then install it but there is an error.
> How can I do it?
> Thank you
> 	[[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
>



From ligges at statistik.uni-dortmund.de  Thu Oct 23 16:44:49 2003
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Thu, 23 Oct 2003 16:44:49 +0200
Subject: [R] Quantreg Package
In-Reply-To: <00e701c39970$adb2f800$6e788495@dimequant.unimib.it>
References: <00e701c39970$adb2f800$6e788495@dimequant.unimib.it>
Message-ID: <3F97E961.7030709@statistik.uni-dortmund.de>

Radaelli Paolo - Dottorati di Ricerca wrote:

> I've just installed R 1.0.8 (for Windows) and I tried to install the package Quantreg directly from Cran but it's not in the list of downlodable packages.
> I tried also downloading the zip file and then install it but there is an error.
> How can I do it?
> Thank you
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help

CRAN/bin/windows/contrib/1.8/ReadMe tells you that Windows binaries of 
those packages not passing Rcmd check are not published on CRAN. And the 
corresponding Status file and check-log tells you quantreg is among 
those packages.

So you have to compile from source yourself.

Uwe Ligges



From tlumley at u.washington.edu  Thu Oct 23 16:48:24 2003
From: tlumley at u.washington.edu (Thomas Lumley)
Date: Thu, 23 Oct 2003 07:48:24 -0700 (PDT)
Subject: [R] repeating colors in graph
In-Reply-To: <200310230647.46685.anna@email.arc.nasa.gov>
References: <200310230647.46685.anna@email.arc.nasa.gov>
Message-ID: <Pine.A41.4.58.0310230745041.60014@homer06.u.washington.edu>

On Thu, 23 Oct 2003, Anna Pryor wrote:

> I have a plot with 13 lines and R repeats colors.  Is there any way to make R
> not repeat colors, or is this just a fundamental limitation on R's part to
> say have only 8 colors that it can use?

You can change the color palette with the palette() function, supplying as
long a vector of colors as you want.

Alternatively, you can specify colors explicitly rather than as numbers
(either by name or by rgb values)

You may have difficulty coming up with 13 colors that are clearly
distinguishable when applied to narrow lines.

See the help pages for color() and palette(), and perhaps the RColorBrewer
package.


	-thomas



From zhuw at mail.smu.edu  Thu Oct 23 16:51:22 2003
From: zhuw at mail.smu.edu (zhu wang)
Date: 23 Oct 2003 09:51:22 -0500
Subject: [R] generic algorithm
Message-ID: <1066920682.1882.12.camel@zwang.stat.smu.edu>

Dear all,

Is there any generic algorithm code for optimization implemented in R? I
searched without success.

Thanks,
-- 
Zhu Wang

Statistical Science Department
Southern Methodist University
Phone: (214)768-2453
Fax: (214)768-4035
Email: zhuw at mail.smu.edu



From jc at or.psychology.dal.ca  Thu Oct 23 16:51:52 2003
From: jc at or.psychology.dal.ca (John Christie)
Date: Thu, 23 Oct 2003 11:51:52 -0300
Subject: [R] disappointed (card model)
Message-ID: <700C050C-0568-11D8-9549-000A9566473A@or.psychology.dal.ca>

	I was very disappointed to find that there are no R packages for 
blackjack or at least a model for cards.  Anyone know of one?



From paolo.radaelli at unimib.it  Thu Oct 23 17:02:41 2003
From: paolo.radaelli at unimib.it (Radaelli Paolo - Dottorati di Ricerca)
Date: Thu, 23 Oct 2003 17:02:41 +0200
Subject: [R] Quantreg Package
References: <00e701c39970$adb2f800$6e788495@dimequant.unimib.it>
	<3F97E961.7030709@statistik.uni-dortmund.de>
Message-ID: <011e01c39976$c29fa960$6e788495@dimequant.unimib.it>


I saw the read-me but I didn't undersstand wich is the problem. I only know
that in a previous version of R I installed on my pc it was all ok. So know
I have to download the extensions files and then compile them on my own ?
Thank you
Paolo Radaelli

----- Original Message ----- 
From: "Uwe Ligges" <ligges at statistik.uni-dortmund.de>
To: "Radaelli Paolo - Dottorati di Ricerca" <paolo.radaelli at unimib.it>
Cc: <r-help at stat.math.ethz.ch>
Sent: Thursday, October 23, 2003 4:44 PM
Subject: Re: [R] Quantreg Package


> Radaelli Paolo - Dottorati di Ricerca wrote:
>
> > I've just installed R 1.0.8 (for Windows) and I tried to install the
package Quantreg directly from Cran but it's not in the list of downlodable
packages.
> > I tried also downloading the zip file and then install it but there is
an error.
> > How can I do it?
> > Thank you
> > [[alternative HTML version deleted]]
> >
> > ______________________________________________
> > R-help at stat.math.ethz.ch mailing list
> > https://www.stat.math.ethz.ch/mailman/listinfo/r-help
>
> CRAN/bin/windows/contrib/1.8/ReadMe tells you that Windows binaries of
> those packages not passing Rcmd check are not published on CRAN. And the
> corresponding Status file and check-log tells you quantreg is among
> those packages.
>
> So you have to compile from source yourself.
>
> Uwe Ligges
>



From Giovanni_Millo at generali.com  Thu Oct 23 17:03:20 2003
From: Giovanni_Millo at generali.com (Millo Giovanni)
Date: Thu, 23 Oct 2003 17:03:20 +0200
Subject: [R] estimating probit models
Message-ID: <74F2D4ED68558643B63A6CC21746040D01A073A7@BEMAILEXTS1.ad.generali.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20031023/1d39befc/attachment.pl

From seanpor at acm.org  Thu Oct 23 17:08:04 2003
From: seanpor at acm.org (Sean O'Riordain)
Date: Thu, 23 Oct 2003 16:08:04 +0100
Subject: [R] generic algorithm (optimize)
In-Reply-To: <1066920682.1882.12.camel@zwang.stat.smu.edu>
References: <1066920682.1882.12.camel@zwang.stat.smu.edu>
Message-ID: <3F97EED4.4030009@acm.org>

help.search("optimize")

optim(base)             General-purpose Optimization
optimize(base)          One Dimensional Optimization
olvq1(class)            Optimized Learning Vector Quantization 1
lmeScale(nlme)          Scale for lme Optimization


zhu wang wrote:
> Dear all,
> 
> Is there any generic algorithm code for optimization implemented in R? I
> searched without success.
> 
> Thanks,



From mkondrin at hppi.troitsk.ru  Fri Oct 24 04:13:35 2003
From: mkondrin at hppi.troitsk.ru (M.Kondrin)
Date: Thu, 23 Oct 2003 19:13:35 -0700
Subject: [R] generic algorithm
In-Reply-To: <1066920682.1882.12.camel@zwang.stat.smu.edu>
References: <1066920682.1882.12.camel@zwang.stat.smu.edu>
Message-ID: <3F988ACF.6000701@hppi.troitsk.ru>

zhu wang wrote:
> Dear all,
> 
> Is there any generic algorithm code for optimization implemented in R? I
> searched without success.
> 
> Thanks,
?optim



From zhuw at mail.smu.edu  Thu Oct 23 17:11:28 2003
From: zhuw at mail.smu.edu (zhu wang)
Date: 23 Oct 2003 10:11:28 -0500
Subject: [R] Re: generic algorithm
In-Reply-To: <1066920682.1882.12.camel@zwang.stat.smu.edu>
References: <1066920682.1882.12.camel@zwang.stat.smu.edu>
Message-ID: <1066921888.1882.16.camel@zwang.stat.smu.edu>

Sorry, it is my bad. 

I mean genetic algorithm. It seems genoud {rgenoud} is the one I am
looking for.

On Thu, 2003-10-23 at 09:51, zhu wang wrote:
> Dear all,
> 
> Is there any generic algorithm code for optimization implemented in R? I
> searched without success.
> 
> Thanks,
-- 
Zhu Wang

Statistical Science Department
Southern Methodist University
Phone: (214)768-2453
Fax: (214)768-4035
Email: zhuw at mail.smu.edu



From mkondrin at hppi.troitsk.ru  Fri Oct 24 04:17:56 2003
From: mkondrin at hppi.troitsk.ru (M.Kondrin)
Date: Thu, 23 Oct 2003 19:17:56 -0700
Subject: [R] nlm, hessian, and derivatives in obj function?
In-Reply-To: <200310231305.30741.ahenningsen@email.uni-kiel.de>
References: <001701c39509$f58eda10$0a00a8c0@rodan>
	<200310231305.30741.ahenningsen@email.uni-kiel.de>
Message-ID: <3F988BD4.2060800@hppi.troitsk.ru>

Arne Henningsen wrote:
> Hi,
> 
> I don't know much about non-linear models, but there is another possibility to 
> fit these models: 
> 
> 1) get some starting values for the parameters
> 2) take the derivatives of the model with respect to the parameters at the 
> point of the starting values of the parameters
> 3) perform a linear estimation of this linearized model (using systemfit) to 
> get new parameter estimates
> 4) got to step 2) and take these new parameter estimates in place of the 
> starting values
> 5) iterate this until the parameters stay stable from one to the next 
> iteration
> 
> This has three advantages:
> 1) It is not much work to write these function since systemfit already exists
> 2) If the model is linear in parameters, it is identical to the linearized 
> model and, thus, the first iteration leads directly to the optimum
> 3) You get get the SEs from the last iteration of systemfit
> 
> Does this approach also have disadvantages (e.g. non-convergence of parameters 
> in many cases)?
> 
> Best wishes,
> Arne
> 
> 
OK you have reinvented gradient descent method. It has one really big 
disadvantage - it has a rather poor convergence if function has a long 
"valleys". This method finds this valley fast enough, but it takes a 
long time to find a minima along this valley - successive iterations 
jumps from one wall to another of the valley. To solve this problem many 
methods was invented which somewhat modify search path to deviate it 
from gradient line (conjugate gradient methods, Levenberg-Marquardt) 
using estimation of hessian matrix.



From alessandro.semeria at cramont.it  Thu Oct 23 17:24:42 2003
From: alessandro.semeria at cramont.it (alessandro.semeria@cramont.it)
Date: Thu, 23 Oct 2003 17:24:42 +0200
Subject: [R] generic algorithm
Message-ID: <OFBFC4A10E.36158ED4-ONC1256DC8.0053C0C2@tomware.it>


Are you sure?
look at "nlm" on base pkg and at pkgs "nls" "nlme" ...
all pkgs to minimize something optimize
or better, what signify optimize for you?
Best

A.S.

----------------------------

Alessandro Semeria
Models and Simulations Laboratory
Montecatini Environmental Research Center (Edison Group),
Via Ciro Menotti 48,
48023 Marina di Ravenna (RA), Italy
Tel. +39 544 536811
Fax. +39 544 538663
E-mail: alessandro.semeria at cramont.it



From ligges at statistik.uni-dortmund.de  Thu Oct 23 17:28:01 2003
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Thu, 23 Oct 2003 17:28:01 +0200
Subject: [R] Quantreg Package
In-Reply-To: <011e01c39976$c29fa960$6e788495@dimequant.unimib.it>
References: <00e701c39970$adb2f800$6e788495@dimequant.unimib.it>
	<3F97E961.7030709@statistik.uni-dortmund.de>
	<011e01c39976$c29fa960$6e788495@dimequant.unimib.it>
Message-ID: <3F97F381.8030809@statistik.uni-dortmund.de>

Radaelli Paolo - Dottorati di Ricerca wrote:

> I saw the read-me but I didn't undersstand wich is the problem. I only know
> that in a previous version of R I installed on my pc it was all ok. 

Yes, but on the recent version it is *not* OK. 
http://cran.r-project.org/bin/windows/contrib/1.8/check/quantreg-check.log 
tells you:

  [...]
  * checking examples ... ERROR
  Running examples in quantreg-Ex.R failed.

BTW: Efforts have been made to upload these check logs to CRAN in order 
to provide you with this information, so please read those files!


Looking closer (as a hint for Roger), R *crashes* when running the 
examples in ?boot.rq.


> So know I have to download the extensions files and then compile them on my own ?

You can download the source package and compile from source (I don't 
know of any "extensions files").
Attention: The problem not passing Rcmd check remains (almost certain).

Uwe Ligges



> Thank you
> Paolo Radaelli
> 
> ----- Original Message ----- 
> From: "Uwe Ligges" <ligges at statistik.uni-dortmund.de>
> To: "Radaelli Paolo - Dottorati di Ricerca" <paolo.radaelli at unimib.it>
> Cc: <r-help at stat.math.ethz.ch>
> Sent: Thursday, October 23, 2003 4:44 PM
> Subject: Re: [R] Quantreg Package
> 
> 
> 
>>Radaelli Paolo - Dottorati di Ricerca wrote:
>>
>>
>>>I've just installed R 1.0.8 (for Windows) and I tried to install the
> 
> package Quantreg directly from Cran but it's not in the list of downlodable
> packages.
> 
>>>I tried also downloading the zip file and then install it but there is
> 
> an error.
> 
>>>How can I do it?
>>>Thank you
>>>[[alternative HTML version deleted]]
>>>
>>>______________________________________________
>>>R-help at stat.math.ethz.ch mailing list
>>>https://www.stat.math.ethz.ch/mailman/listinfo/r-help
>>
>>CRAN/bin/windows/contrib/1.8/ReadMe tells you that Windows binaries of
>>those packages not passing Rcmd check are not published on CRAN. And the
>>corresponding Status file and check-log tells you quantreg is among
>>those packages.
>>
>>So you have to compile from source yourself.
>>
>>Uwe Ligges
>>
> 
>



From mkondrin at hppi.troitsk.ru  Fri Oct 24 04:33:05 2003
From: mkondrin at hppi.troitsk.ru (M.Kondrin)
Date: Thu, 23 Oct 2003 19:33:05 -0700
Subject: [R] OOP like handling of lists?
In-Reply-To: <3F97BFEC.2030302@rcs.urz.tu-dresden.de>
References: <3F97BFEC.2030302@rcs.urz.tu-dresden.de>
Message-ID: <3F988F61.4010003@hppi.troitsk.ru>

Thomas Petzoldt wrote:
> Hello,
> 
> I am writing a package with a collection of several models. In order to 
> allow users to play interactively with the models (in contrast to 
> hacking lengthy scripts), I want to put all what is needed to run a 
> particular model into a single list object for each model.
> 
There is a great introduction to OO programming in R:
John M. Chambers and Duncan Temple Lang. Object-oriented programming in 
R. R News, 1(3):17-19, September 2001.
There was described a more generic model of object-oriented programming 
in R without UseMethod. Think about your object as a function (not a 
list), and about object's methods and fields as a variables in this 
function environment.



From alessandro.semeria at cramont.it  Thu Oct 23 17:38:27 2003
From: alessandro.semeria at cramont.it (alessandro.semeria@cramont.it)
Date: Thu, 23 Oct 2003 17:38:27 +0200
Subject: [R] Re: generic algorithm
Message-ID: <OFD9A9F928.CCBD7DDA-ONC1256DC8.0055E1C4@tomware.it>


Look at pkg gafit
Bye
A.S.

----------------------------

Alessandro Semeria
Models and Simulations Laboratory
Montecatini Environmental Research Center (Edison Group),
Via Ciro Menotti 48,
48023 Marina di Ravenna (RA), Italy
Tel. +39 544 536811
Fax. +39 544 538663
E-mail: alessandro.semeria at cramont.it



From vigoyal at eden.rutgers.edu  Thu Oct 23 17:41:23 2003
From: vigoyal at eden.rutgers.edu (vishal goyal)
Date: Thu, 23 Oct 2003 11:41:23 -0400 (EDT)
Subject: [R] Weighted Clustering
In-Reply-To: <Pine.GSO.3.95q.1031023152046.14383V-100000@sun12.math.uni-hamburg.de>
Message-ID: <Pine.SOL.4.44.0310231135320.26615-100000@er6.rutgers.edu>

Hi,

I have read some of the papers on weighted clustering but those weigths
are with respect to variables (say we are clustering height measured in
cms and weights in tons etc) rather than each point itself and hence they
just multiply the distances with the weights. In my case, it is something
like a facility location. Each point has a weight (probability of its
requirement) (all the variables have equal weightage) and i want to
cluster and also move the mean to the point of highest weightage. I know
in deterministic case, this is exactly the facility location problem but i
was hoping to have some simpler-algorithm in probabilitsic case.

Or, did i mis-understand the theory and there is something other than just
multpying the variables with the weights. I would appreciate any reference
or pointer.

Regards
Vishal


On Thu, 23 Oct 2003, Christian
Hennig wrote:

> Hi,
>
> the target functions of k-means clusterung and of normal mixture model
> clustering (in library mclust) should work with weighted data points as
> well. This is, however, only a theoretical suggestion, because as far as I
> know, it is not implemented in R, and the R-functions for kmeans and
> model based clustering call C and fortran code, which I think is
> not too easy to adapt to your problem.
> If I would be in your situation, I would presumably write new code for a
> k-means algorithm (there are some, which are very easy to implement) with
> weighted data.
>
> Best,
> Christian
>
> On Wed, 22 Oct 2003, vishal goyal wrote:
>
> > Hi,
> >
> > I have a data set(say 2-d demands of a product (say flow-rate vs
> > concentration)) and with each demand is the weightage (like a probability)
> > of that demand occuring. Is there a way to cluster this demand-data
> > (deterministic or probabilistic(if possible)) which also incorporates the
> > weights (just multiplying distances with weights gives poor and mixed
> > clusters) while clustering (Something like a Facility Location problem).
> > I would appreciate any advice.
> >
> > Regards
> > Vishal Goyal
> >
> >
> >
> >
> >
> >
> >
> >
> > --------------------------------------------------------------------------
> > "Simplicity is the ultimate sophistication"
> > ---------------------------------------------------------------------------
> >
> > Vishal Goyal, Graduate Student
> > Department of Chemical and Biochemical Engineering
> > Rutgers - The State University of New Jersey
> > 98 Brett Road, Piscataway, NJ 08854
> > tel: 732-445-7061 (O)
> > email: vigoyal at eden.rutgers.edu
> >
> > ______________________________________________
> > R-help at stat.math.ethz.ch mailing list
> > https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> >
>
> ***********************************************************************
> Christian Hennig
> Fachbereich Mathematik-SPST/ZMS, Universitaet Hamburg
> hennig at math.uni-hamburg.de, http://www.math.uni-hamburg.de/home/hennig/
> #######################################################################
> ich empfehle www.boag-online.de
>
>








--------------------------------------------------------------------------
"Simplicity is the ultimate sophistication"
---------------------------------------------------------------------------

Vishal Goyal, Graduate Student
Department of Chemical and Biochemical Engineering
Rutgers - The State University of New Jersey
98 Brett Road, Piscataway, NJ 08854
tel: 732-445-7061 (O)
email: vigoyal at eden.rutgers.edu



From fm3a004 at math.uni-hamburg.de  Thu Oct 23 18:03:27 2003
From: fm3a004 at math.uni-hamburg.de (Christian Hennig)
Date: Thu, 23 Oct 2003 18:03:27 +0200 (MET DST)
Subject: [R] Weighted Clustering
In-Reply-To: <Pine.SOL.4.44.0310231135320.26615-100000@er6.rutgers.edu>
Message-ID: <Pine.GSO.3.95q.1031023175400.14383X-100000@sun12.math.uni-hamburg.de>

Hi,

sorry, I do not have any references, it was only an intuitive idea (but
there might be references somewhere out in space).
The idea is that you can indeed weight the points. For this you have to
multiply the contribution of each point to the target function by the
weight, and you have to compute every case number for a cluster as the
sum of the weights (i.e., the means become weighted means). Some thought
about the target function should tell you where you need a sum of weights
instead of a case number.
ML estimation for normal mixture models does exactly this, by weighting
the points according to the a posteriori probability of membership in a
class. But these weights could also be multiplied by a fixed weight for a
point. (If the weights would be fractions with the same
denominator, this should lead to about the same clustering as if you 
would have
so many replications of each point as the numerator says; that's the
motivation.)

Best,
Christian 
 

On Thu, 23 Oct 2003, vishal goyal wrote:

> Hi,
> 
> I have read some of the papers on weighted clustering but those weigths
> are with respect to variables (say we are clustering height measured in
> cms and weights in tons etc) rather than each point itself and hence they
> just multiply the distances with the weights. In my case, it is something
> like a facility location. Each point has a weight (probability of its
> requirement) (all the variables have equal weightage) and i want to
> cluster and also move the mean to the point of highest weightage. I know
> in deterministic case, this is exactly the facility location problem but i
> was hoping to have some simpler-algorithm in probabilitsic case.
> 
> Or, did i mis-understand the theory and there is something other than just
> multpying the variables with the weights. I would appreciate any reference
> or pointer.
> 
> Regards
> Vishal
***********************************************************************
Christian Hennig
Fachbereich Mathematik-SPST/ZMS, Universitaet Hamburg
hennig at math.uni-hamburg.de, http://www.math.uni-hamburg.de/home/hennig/
#######################################################################
ich empfehle www.boag-online.de



From ashenfluff at yahoo.com  Thu Oct 23 18:09:29 2003
From: ashenfluff at yahoo.com (Eric Blair)
Date: Thu, 23 Oct 2003 09:09:29 -0700 (PDT)
Subject: [R] getAttrib segfaults in Windows
Message-ID: <20031023160929.41348.qmail@web60304.mail.yahoo.com>

Dear R people:

I've written a rather extensive simulation in R, much of which I've
ported to C.  I did all of this in Linux, where it works great. On a
Windows machine, it compiles without errors or warnings, but when it
runs, it segfaults.

Here are the exact lines:

void init(SEXP rho){
  	SEXP v      = findVar(install("vars"), rho);
	getAttrib(v,R_NamesSymbol);
}


It's called using
	vars<-data.frame(delta=.9)
	[...]
	.Call("init", as.environment("package:migration"))



It is the second line of init which fails. For example,

void init(SEXP rho){
  	SEXP v      = findVar(install("vars"), rho);
	VECTOR_ELT(v,0);
}
will return .9 like it's supposed to.

Any suggestions on what's going wrong?

Thanks,

B


=====
The author of this email does not endorse the following:



From H.F.Xie at westminster.ac.uk  Thu Oct 23 18:15:10 2003
From: H.F.Xie at westminster.ac.uk (Haifeng (Kevin) Xie)
Date: Thu, 23 Oct 2003 17:15:10 +0100
Subject: [R] Variance-covariance matrix for beta hat and b hat from lme
Message-ID: <00d601c39980$d543f490$be884aa1@cscs.wmin.ac.uk>

Dear all,

Given a LME model (following the notation of Pinheiro and Bates 2000)   y_i
= X_i*beta + Z_i*b_i + e_i, is it possible to extract the
variance-covariance matrix for the estimated beta_i hat and b_i hat from the
lme fitted object?

The reason for needing this is because I want to have interval prediction on
the predicted values (at level = 0:1). The "predict.lme" seems to provide
point estimates only. The predicted value for new observation of the i_th
subject, with E_i and F_i as regressor matrices for fixed- and
random-effects respectively, is
y_i hat = E_i * beta hat + F_i * b_i hat

which can also be written as
y_i hat = [ E_i, F_i ] * [ beta hat ', b_i hat' ]' , where prime denotes
transpose.

Therefore, the variance of y_i hat is
Var(y_i hat) = [ E_i, F_i ] * var( [ beta hat ', b_i hat' ]' ) * [ E_i,
F_i ]' ,

I'm not entire sure if this is the correct way to do, any suggestions will
be particularly welcome.

Many thanks.

Kevin Xie
Research Student
University of Westminster
London, UK



From B.Rowlingson at lancaster.ac.uk  Thu Oct 23 17:17:03 2003
From: B.Rowlingson at lancaster.ac.uk (Barry Rowlingson)
Date: Thu, 23 Oct 2003 16:17:03 +0100
Subject: [R] repeating colors in graph
In-Reply-To: <Pine.A41.4.58.0310230745041.60014@homer06.u.washington.edu>
References: <200310230647.46685.anna@email.arc.nasa.gov>
	<Pine.A41.4.58.0310230745041.60014@homer06.u.washington.edu>
Message-ID: <3F97F0EF.9050802@lancaster.ac.uk>

Thomas Lumley wrote:

> See the help pages for color() and palette(), and perhaps the RColorBrewer
> package.

Splus lets you define colour palettes in its Gui, using a nice little 
notation where you specify something like "black 8 white 6 red" to get a 
17-colour palette with 8 colours between balck and white and then 6 
colours up to red (linearly interpolating RGB values). Here's a little R 
function to do something similar:

smoothColours <-
function(...){
###
### usage: smoothColours(colour,[n|colour],...)
### like smoothColours("white",10,"black") 12 colours, white to black
###
   args <- list(...)
   r <- g <- b <- NULL
   while(length(args)>0){
     if(!is.character(args[[1]])){
       stop("bad args")
     }

     if(length(args)>1){
       if(is.numeric(args[[2]])){
         ## do interpolate:
         from <- col2rgb(args[[1]])
         too <- col2rgb(args[[3]])
         ## generate args[[2]] colours between specified colours:
         n <- args[[2]]+2 # add 2 for start and finish

         ## chop off last one since it will be added on the next iteration:
         r <- c(r,seq(from[1,],too[1,],length=n))
         i <- length(r)
         r <- r[-i]
         g <- c(g,seq(from[2,],too[2,],length=n))
         g <- g[-i]
         b <- c(b,seq(from[3,],too[3,],length=n))
         b <- b[-i]
         ## cut colour and n from list and back we go
         args <- args[-(1:2)]

       }else{
         ## insert colour, chop off 1
         cc <- col2rgb(args[[1]])
         r <- c(r,cc[1,])
         g <- c(g,cc[2,])
         b <- c(b,cc[3,])
         args <- args[-1]
       }
     }else{
       ## insert colour, chop off 1
       cc <- col2rgb(args[[1]])
       r <- c(r,cc[1,])
       g <- c(g,cc[2,])
       b <- c(b,cc[3,])
       args <- args[-1]
     }
    }
   rgb(r,g,b,max=255)
  }

  You can then do something like:
 
image(matrix(runif(100),10,10),col=smoothColours("red",5,"green",6,"blue"))

  You can use as many "colour",n,"colour" things as you like, as long as 
it starts and ends with a colour name. You can even do:

  smoothColours("red","green",10,"white")

  And you can use "#abcdef" notation too.

  Of course, linearly interpolating in RGB might not be the right thing 
to do...

Baz



From tlumley at u.washington.edu  Thu Oct 23 18:38:27 2003
From: tlumley at u.washington.edu (Thomas Lumley)
Date: Thu, 23 Oct 2003 09:38:27 -0700 (PDT)
Subject: [R] repeating colors in graph
In-Reply-To: <3F97F0EF.9050802@lancaster.ac.uk>
References: <200310230647.46685.anna@email.arc.nasa.gov>
	<Pine.A41.4.58.0310230745041.60014@homer06.u.washington.edu>
	<3F97F0EF.9050802@lancaster.ac.uk>
Message-ID: <Pine.A41.4.58.0310230934260.173338@homer04.u.washington.edu>

On Thu, 23 Oct 2003, Barry Rowlingson wrote:

> Thomas Lumley wrote:
>
> > See the help pages for color() and palette(), and perhaps the RColorBrewer
> > package.
>
> Splus lets you define colour palettes in its Gui, using a nice little
> notation where you specify something like "black 8 white 6 red" to get a
> 17-colour palette with 8 colours between balck and white and then 6
> colours up to red (linearly interpolating RGB values). Here's a little R
> function to do something similar:
>
<snip>
>
>   Of course, linearly interpolating in RGB might not be the right thing
> to do...
>

It isn't, but it works fairly well with well chosen colors (for example,
linearly interpolating the RColorBrewer sequential or diverging schemes
gives quite nice color ramps).  It's unlikely to work for producing
palettes for 13 different lines.


	-thomas



From hi_ono2001 at ybb.ne.jp  Thu Oct 23 18:41:37 2003
From: hi_ono2001 at ybb.ne.jp (Hisaji Ono)
Date: Fri, 24 Oct 2003 01:41:37 +0900
Subject: [R] generic algorithm
References: <1066920682.1882.12.camel@zwang.stat.smu.edu>
Message-ID: <009b01c39984$8716b150$818001db@webgis>

Hi.

 Here is GA. I've never used this.

  http://www.dst.unive.it/~claudio/R/index.html#ga

 But this can deal with GP(genetic programming)?

 Cheers.



From B.Rowlingson at lancaster.ac.uk  Thu Oct 23 18:48:24 2003
From: B.Rowlingson at lancaster.ac.uk (Barry Rowlingson)
Date: Thu, 23 Oct 2003 17:48:24 +0100
Subject: [R] repeating colors in graph
In-Reply-To: <Pine.A41.4.58.0310230934260.173338@homer04.u.washington.edu>
References: <200310230647.46685.anna@email.arc.nasa.gov>	<Pine.A41.4.58.0310230745041.60014@homer06.u.washington.edu>	<3F97F0EF.9050802@lancaster.ac.uk>
	<Pine.A41.4.58.0310230934260.173338@homer04.u.washington.edu>
Message-ID: <3F980658.4010308@lancaster.ac.uk>

Thomas Lumley wrote:

> It isn't, but it works fairly well with well chosen colors (for example,
> linearly interpolating the RColorBrewer sequential or diverging schemes
> gives quite nice color ramps).  It's unlikely to work for producing
> palettes for 13 different lines.

  Indeedy. Perhaps the original poster could also use the line type 
(lty) or line width (lwd) parameters to distinguish the lines, along 
with colour. Hey, it was all we had until a few years ago!

I'm sure when New Zealand wakes up Ross will want to mention his DSC2003 
paper on presentation graphics colours, draft paper here:

  http://www.ci.tuwien.ac.at/Conferences/DSC-2003/Drafts/Ihaka.pdf

Baz



From bates at stat.wisc.edu  Thu Oct 23 18:50:35 2003
From: bates at stat.wisc.edu (Douglas Bates)
Date: 23 Oct 2003 11:50:35 -0500
Subject: [R] Variance-covariance matrix for beta hat and b hat from lme
In-Reply-To: <00d601c39980$d543f490$be884aa1@cscs.wmin.ac.uk>
References: <00d601c39980$d543f490$be884aa1@cscs.wmin.ac.uk>
Message-ID: <6rad7rlwys.fsf@bates4.stat.wisc.edu>

"Haifeng \(Kevin\) Xie" <H.F.Xie at westminster.ac.uk> writes:

> Given a LME model (following the notation of Pinheiro and Bates 2000)   y_i
> = X_i*beta + Z_i*b_i + e_i, is it possible to extract the
> variance-covariance matrix for the estimated beta_i hat and b_i hat from the
> lme fitted object?

Not easily.  The pieces that you need are in the condensed linear
model structure and you may be able to extract them in R code but I
have not written any code to do that.

I am revising the internal representation of lme objects using S4
classes.  Saikat DebRoy and I have one representation in the lme4
package but will probably revise that.  Some recent work on
computational methods
        http://www.stat.wisc.edu/~bates/reports/MixedComp.pdf
has me convinced that even this representation should be reorganized
and simplified.

If you really want to delve into the old structures I can give you
some pointers (pun unintended) on where to look but beware that it's a
quagmire.  (Oops - not supposed to use that word in e-mail originating
in the U.S.A.  My regards to the NSA.)


-- 
Douglas Bates                            bates at stat.wisc.edu
Statistics Department                    608/262-2598
University of Wisconsin - Madison        http://www.stat.wisc.edu/~bates/



From yhuerta at msi.umn.edu  Thu Oct 23 18:52:10 2003
From: yhuerta at msi.umn.edu (Yectli A Huerta)
Date: Thu, 23 Oct 2003 11:52:10 -0500 (CDT)
Subject: [R] installation problems  Rgraphviz
Message-ID: <Pine.LNX.4.44.0310231150170.19308-100000@blizzard.msi.umn.edu>

i having issues installing Rgraphviz. i installed the prereqs
from the at&t bell labs. the latest. i'm using R 1.8. below are
my error messages. any help will be appreciated.

thanks,

--
Yectli

* Installing *source* package 'Rgraphviz' ...
** libs
`Rgraphviz.so' is up to date.
** R
** inst
** save image
Loading required package: graph
Error in bindingIsLocked(m1, where) : not an environment
Error in setClass("graphNEL", representation(nodes = "vector", edgeL = "list"),
 :    Error in contained classes ("graph") for class "graphNEL"; class
definition removed from "graph"
[1] FALSE
Loading required package: graph
Error in bindingIsLocked(m1, where) : not an environment
Error in setClass("graphNEL", representation(nodes = "vector", edgeL = "list"),
 :
        Error in contained classes ("graph") for class "graphNEL"; class
definition removed from "graph"
Error: Rgraphviz requires package graph
Execution halted
ERROR: execution of package source for 'Rgraphviz' failed
** Removing '/soft/local/lib/R/library/Rgraphviz'



From andy_liaw at merck.com  Thu Oct 23 18:52:10 2003
From: andy_liaw at merck.com (Liaw, Andy)
Date: Thu, 23 Oct 2003 12:52:10 -0400
Subject: [R] generic algorithm
Message-ID: <3A822319EB35174CA3714066D590DCD50205CD22@usrymx25.merck.com>

Could it be that you meant genetic (rather than "generic") algorithm?  If
so, from the package descriptions, the "gafit" and "seao" packages on CRAN
might be relevant.

HTH,
Andy

> -----Original Message-----
> From: zhu wang [mailto:zhuw at mail.smu.edu] 
> Sent: Thursday, October 23, 2003 10:51 AM
> To: r-help at stat.math.ethz.ch
> Subject: [R] generic algorithm
> 
> 
> Dear all,
> 
> Is there any generic algorithm code for optimization 
> implemented in R? I searched without success.
> 
> Thanks,
> -- 
> Zhu Wang
> 
> Statistical Science Department
> Southern Methodist University
> Phone: (214)768-2453
> Fax: (214)768-4035
> Email: zhuw at mail.smu.edu
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list 
> https://www.stat.math.ethz.ch/mailman/listinfo> /r-help
>



From darryl.humphreys at lehman.com  Thu Oct 23 18:48:51 2003
From: darryl.humphreys at lehman.com (Humphreys, Darryl)
Date: Thu, 23 Oct 2003 12:48:51 -0400
Subject: [R] Problem w/ SJava package
Message-ID: <5F84A09ECDD5D411973000508BE32470234CEFCC@exnyc07.lehman.com>

Hello,

Seems like it is almost working properly. I was able to run examples calling
R from Java successfully for the java code below. However, some commands,
such as "objects" when not commented out, give me the following error? If
you could point me to the correct place to find a solution, I would much
appreciate it. I am new to the R community and not sure where is the
appropriate place to look for this kind of help. I am running this on linux.

Thanks,
Darryl



Loading RInterpreter library

R : Copyright 2002, The R Development Core Team
Version 1.6.1  (2002-11-01)

R is free software and comes with ABSOLUTELY NO WARRANTY.
You are welcome to redistribute it under certain conditions.
Type `license()' or `licence()' for distribution details.

R is a collaborative project with many contributors.
Type `contributors()' for more information.

Type `demo()' for some demos, `help()' for on-line help, or
`help.start()' for a HTML browser interface to help.
Type `q()' to quit R.

done w/ evaluator constructor

Unexpected Signal : 11 occurred at PC=0x4039A074
Function=(null)+0x4039A074
Library=/opt/sunjdk/1.4.1_01-b01/jre/lib/i386/client/libjvm.so

NOTE: We are unable to locate the function name symbol for the error
      just occurred. Please refer to release documentation for possible
      reason and solutions.


Current Java thread:
	at org.omegahat.R.Java.REvaluator.call(Native Method)
	at org.omegahat.R.Java.REvaluator.call(REvaluator.java:137)
	at org.omegahat.R.Java.REvaluator.call(REvaluator.java:127)
	at org.omegahat.R.Java.REvaluator.call(REvaluator.java:118)
	at SJ.<init>(SJ.java:26)
	at SJ.main(SJ.java:39)

Dynamic libraries:
08048000-0804e000 r-xp 00000000 00:17 1402717
/opt/sunjdk/1.4.1_01-b01/bin/java
0804e000-0804f000 rw-p 00005000 00:17 1402717
/opt/sunjdk/1.4.1_01-b01/bin/java
40000000-40016000 r-xp 00000000 08:01 97248      /lib/ld-2.2.4.so
40016000-40017000 rw-p 00015000 08:01 97248      /lib/ld-2.2.4.so
40018000-4001b000 r--s 00000000 00:17 1352683
/opt/sunjdk/1.4.1_01-b01/jre/lib/ext/dnsns.jar
4001b000-4001c000 r--p 00000000 08:01 97167
/usr/lib/locale/en_US/LC_TIME
4001e000-4002b000 r-xp 00000000 08:01 210597     /lib/i686/libpthread-0.9.so
4002b000-4002f000 rw-p 0000c000 08:01 210597     /lib/i686/libpthread-0.9.so
4004f000-40052000 r-xp 00000000 08:01 97261      /lib/libdl-2.2.4.so
40052000-40053000 rw-p 00002000 08:01 97261      /lib/libdl-2.2.4.so
40053000-40186000 r-xp 00000000 08:01 210593     /lib/i686/libc-2.2.4.so
40186000-4018b000 rw-p 00132000 08:01 210593     /lib/i686/libc-2.2.4.so
4018f000-404a7000 r-xp 00000000 00:17 526106
/opt/sunjdk/1.4.1_01-b01/jre/lib/i386/client/libjvm.so
404a7000-4065b000 rw-p 00317000 00:17 526106
/opt/sunjdk/1.4.1_01-b01/jre/lib/i386/client/libjvm.so
4066b000-4067e000 r-xp 00000000 08:01 97266      /lib/libnsl-2.2.4.so
4067e000-4067f000 rw-p 00012000 08:01 97266      /lib/libnsl-2.2.4.so
40681000-406a3000 r-xp 00000000 08:01 210595     /lib/i686/libm-2.2.4.so
406a3000-406a4000 rw-p 00021000 08:01 210595     /lib/i686/libm-2.2.4.so
406a4000-406ad000 r-xp 00000000 00:17 526100
/opt/sunjdk/1.4.1_01-b01/jre/lib/i386/native_threads/libhpi.so
406ad000-406ae000 rw-p 00008000 00:17 526100
/opt/sunjdk/1.4.1_01-b01/jre/lib/i386/native_threads/libhpi.so
406af000-406bf000 r-xp 00000000 00:17 526110
/opt/sunjdk/1.4.1_01-b01/jre/lib/i386/libverify.so
406bf000-406c1000 rw-p 0000f000 00:17 526110
/opt/sunjdk/1.4.1_01-b01/jre/lib/i386/libverify.so
406c1000-406e2000 r-xp 00000000 00:17 526111
/opt/sunjdk/1.4.1_01-b01/jre/lib/i386/libjava.so
406e2000-406e4000 rw-p 00020000 00:17 526111
/opt/sunjdk/1.4.1_01-b01/jre/lib/i386/libjava.so
406e4000-406f9000 r-xp 00000000 00:17 2151585
/opt/sunjdk/1.4.1_01-b01/jre/lib/i386/libzip.so
406f9000-406fb000 rw-p 00014000 00:17 2151585
/opt/sunjdk/1.4.1_01-b01/jre/lib/i386/libzip.so
406fb000-41dc9000 r--s 00000000 00:17 2635968
/opt/sunjdk/1.4.1_01-b01/jre/lib/rt.jar
41e0c000-41e23000 r--s 00000000 00:17 1352686
/opt/sunjdk/1.4.1_01-b01/jre/lib/sunrsasign.jar
41e23000-41e94000 r--s 00000000 00:17 1352694
/opt/sunjdk/1.4.1_01-b01/jre/lib/jsse.jar
41e94000-41ea7000 r--s 00000000 00:17 1352687
/opt/sunjdk/1.4.1_01-b01/jre/lib/jce.jar
41ea7000-42163000 r--s 00000000 00:17 1437648
/opt/sunjdk/1.4.1_01-b01/jre/lib/charsets.jar
4c413000-4c43e000 r--p 00000000 08:01 16337
/usr/lib/locale/en_US/LC_CTYPE
4c43e000-4c448000 r-xp 00000000 08:01 97282      /lib/libnss_files-2.2.4.so
4c448000-4c449000 rw-p 00009000 08:01 97282      /lib/libnss_files-2.2.4.so
4c449000-4c453000 r-xp 00000000 08:01 97287      /lib/libnss_nis-2.2.4.so
4c453000-4c454000 rw-p 00009000 08:01 97287      /lib/libnss_nis-2.2.4.so
4c658000-4c675000 r--s 00000000 00:17 1352682
/opt/sunjdk/1.4.1_01-b01/jre/lib/ext/sunjce_provider.jar
4c675000-4c714000 r--s 00000000 00:17 1352684
/opt/sunjdk/1.4.1_01-b01/jre/lib/ext/localedata.jar
4c714000-4c722000 r--s 00000000 00:17 1352685
/opt/sunjdk/1.4.1_01-b01/jre/lib/ext/ldapsec.jar
4c722000-4c82d000 r--s 00000000 00:22 393209
/home/dahumphr/colt/colt.jar
4c82d000-4c8ac000 r--s 00000000 00:22 97437
/home/dahumphr/r/R-1.6.1/library/SJava/org/omegahat/Jars/Environment.jar
4c8ac000-4c8eb000 r--s 00000000 00:22 97438
/home/dahumphr/r/R-1.6.1/library/SJava/org/omegahat/Jars/antlr.jar
4c8eb000-4c8f9000 r--s 00000000 00:22 97439
/home/dahumphr/r/R-1.6.1/library/SJava/org/omegahat/Jars/jas.jar
4c8f9000-4c945000 r--s 00000000 00:22 97440
/home/dahumphr/r/R-1.6.1/library/SJava/org/omegahat/Jars/jhall.jar
4c945000-4c950000 r--s 00000000 00:22 97441
/home/dahumphr/r/R-1.6.1/library/SJava/org/omegahat/Jars/ROmegahatExamples.j
ar
4c950000-4c95b000 r-xp 00000000 00:22 89455
/home/dahumphr/r/R-1.6.1/library/SJava/libs/SJava.so
4c95b000-4c95c000 rw-p 0000a000 00:22 89455
/home/dahumphr/r/R-1.6.1/library/SJava/libs/SJava.so
4c95c000-4c963000 r-xp 00000000 00:22 93715
/home/dahumphr/r/R-1.6.1/library/SJava/libs/libRSNativeJava.so
4c963000-4c964000 rw-p 00006000 00:22 93715
/home/dahumphr/r/R-1.6.1/library/SJava/libs/libRSNativeJava.so
4c965000-4cacb000 r-xp 00000000 00:22 31993
/home/dahumphr/r/R-1.6.1/bin/libR.so
4cacb000-4cad6000 rw-p 00165000 00:22 31993
/home/dahumphr/r/R-1.6.1/bin/libR.so
4cb0a000-4cb10000 r--p 00000000 08:01 162013
/usr/lib/locale/en_US/LC_COLLATE
4cb10000-4cb31000 r-xp 00000000 08:01 340277     /usr/lib/libreadline.so.4.2
4cb31000-4cb36000 rw-p 00020000 08:01 340277     /usr/lib/libreadline.so.4.2
4cb36000-4cb6b000 r-xp 00000000 08:01 340190     /usr/lib/libncurses.so.5.2
4cb6b000-4cb74000 rw-p 00034000 08:01 340190     /usr/lib/libncurses.so.5.2
4cb77000-4cb7f000 r-xp 00000000 08:01 97547      /lib/libpcre.so.0.0.1
4cb7f000-4cb81000 rw-p 00007000 08:01 97547      /lib/libpcre.so.0.0.1
4cb81000-4cb8f000 r-xp 00000000 08:01 340094     /usr/lib/libbz2.so.1.0.0
4cb8f000-4cb91000 rw-p 0000d000 08:01 340094     /usr/lib/libbz2.so.1.0.0
4cb91000-4cb9d000 r-xp 00000000 08:01 340459     /usr/lib/libz.so.1.1.3
4cb9d000-4cb9f000 rw-p 0000b000 08:01 340459     /usr/lib/libz.so.1.1.3
4cba7000-4cbaf000 r-xp 00000000 00:22 32113
/home/dahumphr/r/R-1.6.1/library/ctest/libs/ctest.so
4cbaf000-4cbb0000 rw-p 00007000 00:22 32113
/home/dahumphr/r/R-1.6.1/library/ctest/libs/ctest.so

Local Time = Thu Oct 23 12:46:52 2003
Elapsed Time = 2
#
# HotSpot Virtual Machine Error : 11
# Error ID : 4F530E43505002E6
# Please report this error at
# http://java.sun.com/cgi-bin/bugreport.cgi
<http://java.sun.com/cgi-bin/bugreport.cgi> 
#
# Java VM: Java HotSpot(TM) Client VM (1.4.1_01-b01 mixed mode)
#
# An error report file has been saved as hs_err_pid10717.log.
# Please refer to the file for further information.
#
./RJava: line 66: 10717 Aborted                 ${JAVA} -classpath
${CLASSPATH}:/home/dahumphr ${OMEGA_PROPS} SJ

----------------------------------------------------------------------------
------------------------------------------------------------------
Code follows
----------------------------------------------------------------------------
------------------------------------------------------------------


import org.omegahat.R.Java.ROmegahatInterpreter;
import org.omegahat.R.Java.REvaluator;

public class SJ{

	public SJ(String[] args){

		ROmegahatInterpreter interp = new ROmegahatInterpreter(
			ROmegahatInterpreter.fixArgs(args),false);

		REvaluator e=new REvaluator();

		System.out.println("done w/ evaluator constructor");

		// String[] objects = (String[])e.call("objects");

		int[] seq;
		Object[] funArgs = new Object[2];
		funArgs[0]=new Integer(1);
		funArgs[1]=new Integer(10);
		seq=(int[])e.call("seq",funArgs);

		System.out.println("length "+seq.length);
		for(int i=0;i<seq.length;i++){
		    System.out.print(seq[i]+" ");
		}
		System.out.println();
		
	}


	public static void main(String[] args){

		SJ sj=new SJ(args);

	}

}


Darryl Humphreys, Ph.D.
Vice President
Lehman Brothers
745 7th Ave.
15th Floor
New York, NY
10019
(212) 526-2701



------------------------------------------------------------------------------
This message is intended only for the personal and confidential use of the
designated recipient(s) named above.  If you are not the intended recipient of
this message you are hereby notified that any review, dissemination,
distribution or copying of this message is strictly prohibited.  This
communication is for information purposes only and should not be regarded as
an offer to sell or as a solicitation of an offer to buy any financial
product, an official confirmation of any transaction, or as an official
statement of Lehman Brothers.  Email transmission cannot be guaranteed to be
secure or error-free.  Therefore, we do not represent that this information is
complete or accurate and it should not be relied upon as such.  All
information is subject to change without notice.



From tplate at blackmesacapital.com  Thu Oct 23 18:58:04 2003
From: tplate at blackmesacapital.com (Tony Plate)
Date: Thu, 23 Oct 2003 10:58:04 -0600
Subject: [R] what's going on here with substitute() ?
Message-ID: <5.2.1.1.2.20031023102017.040a5cc0@mailhost.blackmesacapital.com>

I was trying to create a function with a value computed at creation time, 
using substitute(), but I got results I don't understand:

 > this.is.R
Error: Object "this.is.R" not found
 > substitute(this.is.R <- function() X, 
list(X=!is.null(options("CRAN")[[1]])))
this.is.R <- function() TRUE
 > # the above expression as printed is what I want for the function definition
 > eval(substitute(this.is.R <- function() X, 
list(X=!is.null(options("CRAN")[[1]]))))
 > this.is.R
function() X
 > this.is.R()
[1] TRUE
 > X
Error: Object "X" not found
 > rm(this.is.R)
 > # Try again a slightly different way
 > substitute(this.is.R <- function() X, 
list(X=!is.null(options("CRAN")[[1]])))
this.is.R <- function() TRUE
 > .Last.value
this.is.R <- function() TRUE
 > eval(.Last.value)
 > this.is.R
function() X
 > this.is.R()
[1] TRUE
 > rm(this.is.R)
 >
Why is the body of the function "X" when I substituted a different 
expression for X? Also, given that the body of the function is X, how does 
the function evaluate to TRUE since X is not defined anywhere (except in a 
list that should have been discarded.)

This happens with both R 1.7.1 and R 1.8.0 (under Windows 2000).

(yes, I did discover the function is.R(), but I still want to discover 
what's going here.)

-- Tony Plate

PS.  In S-plus 6.1, things worked as I had expected:

 > substitute(this.is.R <- function() X, 
list(X=!is.null(options("CRAN")[[1]])))
this.is.R <- function()
F
 > eval(substitute(this.is.R <- function() X, 
list(X=!is.null(options("CRAN")[[1]]))))
function()
F
 > this.is.R
function()
F
 > this.is.R()
[1] F
 >



From jasont at indigoindustrial.co.nz  Thu Oct 23 19:32:41 2003
From: jasont at indigoindustrial.co.nz (Jason Turner)
Date: Fri, 24 Oct 2003 06:32:41 +1300
Subject: [R] multivariate ARMA analysis with R
In-Reply-To: <4.2.0.58.20031023110134.00a3d4c0@mail.univ-tlse1.fr>
References: <4.2.0.58.20031023110134.00a3d4c0@mail.univ-tlse1.fr>
Message-ID: <3F9810B9.8070501@indigoindustrial.co.nz>

Vincent Spiesser wrote:

> Hi,
> I try to make a multivariate ARMA analysis.
> Function arima (from ts packages only accept univariate argument.
> 
> Does an R function exist for multivariate analysis ?
> 
> Thanks
> Vincent Spiesser
> 

The dse bundle on CRAN has functions for VARX multivariate time series 
models.

Cheers

Jason

-- 
Indigo Industrial Controls Ltd.
http://www.indigoindustrial.co.nz
64-21-343-545
jasont at indigoindustrial.co.nz



From danlinyu at uwm.edu  Thu Oct 23 19:42:04 2003
From: danlinyu at uwm.edu (danlinyu@uwm.edu)
Date: Thu, 23 Oct 2003 12:42:04 -0500
Subject: [R-sig-Geo] RE: [R] adjacency matrix
In-Reply-To: <3F98084B.000006.01492@sida-opz.dsv.su.se>
References: <NGBBKLJCOLPAFMJIEMHCEECKCMAA.cdeclercq@nordnet.fr>
	<3F98084B.000006.01492@sida-opz.dsv.su.se>
Message-ID: <1066930924.3f9812ec22d2b@mail03.imt.uwm.edu>

Hi-
    As far as I know, some of the error messages results from the fact that you

either did not install the package or did not load the package.
    If you did not install the package, you can install from R's GUI (my 
experiences are based on the Windows Version), and load them by type library
(packagename). If you have already installed them, just type library
(packagename) and you will be good to go. For your specific case, after you 
started R, just type library(maptools), and you will be able to use all the 
functions such as read.shape, Map2poly, plotpolys, etc.

Quoting Orlando Zacarias <si-opz at dsv.su.se>:

> 

> Hi once again,

> Now i have been trying to run the R changed example from Maptools package
> manual, and errors occur underlined bellow. It is really troublesome....

> Please assist,

> Orlando.

> ---------------------------------------------------

> shp <- try(library(shapefiles))  

> Error in library(maptools) : There is no package called 'shapefiles'

> > if (class(shp) != "try-error") {

> + ShapeDir <- system.file("shapes", package="maptools")[1]

> + try1 <- read.shapefile(paste(ShapeDir, "shapes", sep="/"))

> + shppolys <- shape2poly(try1, as.character(try1$dbf$dbf$NEIGNO))

> + plotpolys(shppolys)

> + title(main="Polygons for Vermani from shapefiles package")

> + }

> > 

> > try2 <- read.shape(system.file("shapes/Vermani.shp",

> + package="maptools/man")[1])

> Error: couldn't find function "read.shape"

> > mappolys <- Map2poly(try2, as.character(try2$att.data$NEIGNO))

> Error: couldn't find function "Map2poly"

> > plotpolys(mappolys)

> Error: couldn't find function "plotpolys"

> > title(main="Polygons for Vermani from maptools package")

> Error in title(main = "Polygons for Vermani from maptools package") : 

>         plot.new has not been called yet

> > if (class(shp) != "try-error") {

> + plotpolys(shppolys)

> + plotpolys(mappolys, add=TRUE, border="red", lty="dotted")

> + title(main="Polygons for Vermani overplotted from both packages")

> + }

> ------------------------------------------------------



From cdeclercq at nordnet.fr  Thu Oct 23 19:43:35 2003
From: cdeclercq at nordnet.fr (Christophe Declercq)
Date: Thu, 23 Oct 2003 19:43:35 +0200
Subject: [R] adjacency matrix
References: <NGBBKLJCOLPAFMJIEMHCEECKCMAA.cdeclercq@nordnet.fr>
	<3F98084B.000006.01492@sida-opz.dsv.su.se>
Message-ID: <000c01c3998d$4cee4cf0$b999f9c1@Alfred>

> 
> ----- Original Message ----- 
> From: "Orlando Zacarias" <si-opz at dsv.su.se>
> To: <cdeclercq at nordnet.fr>
> Cc: <r-sig-geo at stat.math.ethz.ch>; <r-help at stat.math.ethz.ch>
> Sent: Thursday, October 23, 2003 6:56 PM
> Subject: RE: [R] adjacency matrix
> 
> 
> 
> Hi once again,
> Now i have been trying to run the R changed example from Maptools
> package manual, and errors occur underlined bellow. It is really
> troublesome....
> Please assist,
> Orlando.
> ---------------------------------------------------
> shp <- try(library(shapefiles))  
> Error in library(maptools) : There is no package called 'shapefiles'

The message is clear!

> > if (class(shp) != "try-error") {
> + ShapeDir <- system.file("shapes", package="maptools")[1]
> + try1 <- read.shapefile(paste(ShapeDir, "shapes", sep="/"))
> + shppolys <- shape2poly(try1, as.character(try1$dbf$dbf$NEIGNO))
> + plotpolys(shppolys)
> + title(main="Polygons for Vermani from shapefiles package")
> + }
> > 
> > try2 <- read.shape(system.file("shapes/Vermani.shp",
> + package="maptools/man")[1])
> Error: couldn't find function "read.shape"
> > mappolys <- Map2poly(try2, as.character(try2$att.data$NEIGNO))
> Error: couldn't find function "Map2poly"
> > plotpolys(mappolys)
> Error: couldn't find function "plotpolys"
> > title(main="Polygons for Vermani from maptools package")
> Error in title(main = "Polygons for Vermani from maptools package") : 
>         plot.new has not been called yet
> > if (class(shp) != "try-error") {
> + plotpolys(shppolys)
> + plotpolys(mappolys, add=TRUE, border="red", lty="dotted")
> + title(main="Polygons for Vermani overplotted from both packages")
> + }
> ------------------------------------------------------

Did you forget:
> library(maptools)
before that?

Christophe



From f0z6305 at labs.tamu.edu  Thu Oct 23 20:07:33 2003
From: f0z6305 at labs.tamu.edu (Feng Zhang)
Date: Thu, 23 Oct 2003 13:07:33 -0500
Subject: [R] What is the definition of curve continuity and smoothness
Message-ID: <001401c39990$889c2260$d9bfa543@f0z6305>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20031023/0be9a67e/attachment.pl

From anna at email.arc.nasa.gov  Thu Oct 23 20:19:10 2003
From: anna at email.arc.nasa.gov (Anna Pryor)
Date: Thu, 23 Oct 2003 11:19:10 -0700
Subject: [R] repeating colors in graph 2
Message-ID: <200310231119.10170.anna@email.arc.nasa.gov>

I've tried looking at ?colors and ?palette and if I'm understanding it 
correctly, I'm supposed to type in (for example) palette(rainbow(13)) before 
I type in my plot (of 13 lines) if I want 13 different colors.  But this does 
not work.  Other things that i have tried besides rainbow give me errors.   
Am I just doing something completely wrong?

Anna



From tlumley at u.washington.edu  Thu Oct 23 20:38:41 2003
From: tlumley at u.washington.edu (Thomas Lumley)
Date: Thu, 23 Oct 2003 11:38:41 -0700 (PDT)
Subject: [R] repeating colors in graph 2
In-Reply-To: <200310231119.10170.anna@email.arc.nasa.gov>
References: <200310231119.10170.anna@email.arc.nasa.gov>
Message-ID: <Pine.A41.4.58.0310231137390.173338@homer04.u.washington.edu>

On Thu, 23 Oct 2003, Anna Pryor wrote:

> I've tried looking at ?colors and ?palette and if I'm understanding it
> correctly, I'm supposed to type in (for example) palette(rainbow(13)) before
> I type in my plot (of 13 lines) if I want 13 different colors.  But this does
> not work.  Other things that i have tried besides rainbow give me errors.
> Am I just doing something completely wrong?
>

In what sense does it not work?  I get thirteen different colors.  They
aren't very different, but if you want thirteen easily distinguishable
colors you will have to choose them yourself rather than relying on
rainbow().

	-thomas



From jmacdon at med.umich.edu  Thu Oct 23 20:40:43 2003
From: jmacdon at med.umich.edu (James MacDonald)
Date: Thu, 23 Oct 2003 14:40:43 -0400
Subject: [R] repeating colors in graph 2
Message-ID: <sf97e887.050@med-gwia-01a.med.umich.edu>

I believe you want plot(blah blah, col=palette(rainbow(13)))

HTH,

Jim



James W. MacDonald
Affymetrix and cDNA Microarray Core
University of Michigan Cancer Center
1500 E. Medical Center Drive
7410 CCGC
Ann Arbor MI 48109
734-647-5623

>>> Anna Pryor <anna at email.arc.nasa.gov> 10/23/03 02:19PM >>>
I've tried looking at ?colors and ?palette and if I'm understanding it

correctly, I'm supposed to type in (for example) palette(rainbow(13))
before 
I type in my plot (of 13 lines) if I want 13 different colors.  But
this does 
not work.  Other things that i have tried besides rainbow give me
errors.   
Am I just doing something completely wrong?

Anna

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://www.stat.math.ethz.ch/mailman/listinfo/r-help



From p.dalgaard at biostat.ku.dk  Thu Oct 23 21:12:12 2003
From: p.dalgaard at biostat.ku.dk (Peter Dalgaard)
Date: 23 Oct 2003 21:12:12 +0200
Subject: [R] what's going on here with substitute() ?
In-Reply-To: <5.2.1.1.2.20031023102017.040a5cc0@mailhost.blackmesacapital.com>
References: <5.2.1.1.2.20031023102017.040a5cc0@mailhost.blackmesacapital.com>
Message-ID: <x2he1zzs37.fsf@biostat.ku.dk>

Tony Plate <tplate at blackmesacapital.com> writes:

> Why is the body of the function "X" when I substituted a different
> expression for X? Also, given that the body of the function is X, how
> does the function evaluate to TRUE since X is not defined anywhere
> (except in a list that should have been discarded.)
> 
> This happens with both R 1.7.1 and R 1.8.0 (under Windows 2000).
> 
> (yes, I did discover the function is.R(), but I still want to discover
> what's going here.)

I think you are defeating the keep.source mechanism. What you're
seeing is not the actual function but its source attribute. Try
attributes(this.is.R) <- NULL and see what happens.

-- 
   O__  ---- Peter Dalgaard             Blegdamsvej 3  
  c/ /'_ --- Dept. of Biostatistics     2200 Cph. N   
 (*) \(*) -- University of Copenhagen   Denmark      Ph: (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)             FAX: (+45) 35327907



From zeileis at ci.tuwien.ac.at  Thu Oct 23 21:51:04 2003
From: zeileis at ci.tuwien.ac.at (Achim Zeileis)
Date: Thu, 23 Oct 2003 21:51:04 +0200
Subject: pre-compiled win binaries (was: Re: [R] Quantreg Package)
In-Reply-To: <3F97F381.8030809@statistik.uni-dortmund.de>
References: <00e701c39970$adb2f800$6e788495@dimequant.unimib.it>
	<011e01c39976$c29fa960$6e788495@dimequant.unimib.it>
	<3F97F381.8030809@statistik.uni-dortmund.de>
Message-ID: <200310231951.h9NJp4G0025814@thorin.ci.tuwien.ac.at>

It happened for the second time in a week that a Windows binary for a 
CRAN package was not availabe (Hmisc and quantreg) although the 
package maintainer would have been willing to try to fix the problems 
that prevented automatic pre-compilation if he would have realized 
that there is a problem on Windows.
Of course, the information was provided on CRAN, but I thought that it 
might help to automatically notify the package maintainers if some 
conflicts occur on Windows. I would appreciate such a notification if 
one of my packages would have failed to compile...and it is not very 
unlikely that I wouldn't have discovered it myself.

I already asked Uwe privately and he would be willing to provide a 
notification but maybe there are concerns or objections from the 
maintainers?

best,
Z


On Thursday 23 October 2003 17:28, Uwe Ligges wrote:

> Radaelli Paolo - Dottorati di Ricerca wrote:
> > I saw the read-me but I didn't undersstand wich is the problem. I
> > only know that in a previous version of R I installed on my pc it
> > was all ok.
>
> Yes, but on the recent version it is *not* OK.
> http://cran.r-project.org/bin/windows/contrib/1.8/check/quantreg-che
>ck.log tells you:
>
>   [...]
>   * checking examples ... ERROR
>   Running examples in quantreg-Ex.R failed.
>
> BTW: Efforts have been made to upload these check logs to CRAN in
> order to provide you with this information, so please read those
> files!
>
>
> Looking closer (as a hint for Roger), R *crashes* when running the
> examples in ?boot.rq.
>
> > So know I have to download the extensions files and then compile
> > them on my own ?
>
> You can download the source package and compile from source (I don't
> know of any "extensions files").
> Attention: The problem not passing Rcmd check remains (almost
> certain).
>
> Uwe Ligges
>
> > Thank you
> > Paolo Radaelli
> >
> > ----- Original Message -----
> > From: "Uwe Ligges" <ligges at statistik.uni-dortmund.de>
> > To: "Radaelli Paolo - Dottorati di Ricerca"
> > <paolo.radaelli at unimib.it> Cc: <r-help at stat.math.ethz.ch>
> > Sent: Thursday, October 23, 2003 4:44 PM
> > Subject: Re: [R] Quantreg Package
> >
> >>Radaelli Paolo - Dottorati di Ricerca wrote:
> >>>I've just installed R 1.0.8 (for Windows) and I tried to install
> >>> the
> >
> > package Quantreg directly from Cran but it's not in the list of
> > downlodable packages.
> >
> >>>I tried also downloading the zip file and then install it but
> >>> there is
> >
> > an error.
> >
> >>>How can I do it?
> >>>Thank you
> >>>[[alternative HTML version deleted]]
> >>>
> >>>______________________________________________
> >>>R-help at stat.math.ethz.ch mailing list
> >>>https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> >>
> >>CRAN/bin/windows/contrib/1.8/ReadMe tells you that Windows
> >> binaries of those packages not passing Rcmd check are not
> >> published on CRAN. And the corresponding Status file and
> >> check-log tells you quantreg is among those packages.
> >>
> >>So you have to compile from source yourself.
> >>
> >>Uwe Ligges
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help



From roger at ysidro.econ.uiuc.edu  Thu Oct 23 22:06:54 2003
From: roger at ysidro.econ.uiuc.edu (Roger Koenker)
Date: Thu, 23 Oct 2003 15:06:54 -0500 (CDT)
Subject: pre-compiled win binaries (was: Re: [R] Quantreg Package)
In-Reply-To: <200310231951.h9NJp4G0025814@thorin.ci.tuwien.ac.at>
Message-ID: <Pine.SOL.4.30.0310231502260.19787-100000@ysidro.econ.uiuc.edu>


You are right, of course, it would be nice to have notification,
but I'm also sympathetic to Uwe's situation, and not everything
that could be automated, could be _easily_ automated within the
constraints imposed by the rest of world.  The lesson I've drawn
from this is that complaints will appear, and the check directory
does help explain problems.  My real difficulty is that I have
no good way to  explore windows specific problems.  But this is
just the flip side of saying what a great thing it is that
the windows binaries are usually appearing automagically without
any problems!

Roger


url:	www.econ.uiuc.edu/~roger/my.html	Roger Koenker
email	rkoenker at uiuc.edu			Department of Economics
vox: 	217-333-4558				University of Illinois
fax:   	217-244-6678				Champaign, IL 61820

On Thu, 23 Oct 2003, Achim Zeileis wrote:

> It happened for the second time in a week that a Windows binary for a
> CRAN package was not availabe (Hmisc and quantreg) although the
> package maintainer would have been willing to try to fix the problems
> that prevented automatic pre-compilation if he would have realized
> that there is a problem on Windows.
> Of course, the information was provided on CRAN, but I thought that it
> might help to automatically notify the package maintainers if some
> conflicts occur on Windows. I would appreciate such a notification if
> one of my packages would have failed to compile...and it is not very
> unlikely that I wouldn't have discovered it myself.
>
> I already asked Uwe privately and he would be willing to provide a
> notification but maybe there are concerns or objections from the
> maintainers?
>
> best,
> Z
>
>
> On Thursday 23 October 2003 17:28, Uwe Ligges wrote:
>
> > Radaelli Paolo - Dottorati di Ricerca wrote:
> > > I saw the read-me but I didn't undersstand wich is the problem. I
> > > only know that in a previous version of R I installed on my pc it
> > > was all ok.
> >
> > Yes, but on the recent version it is *not* OK.
> > http://cran.r-project.org/bin/windows/contrib/1.8/check/quantreg-che
> >ck.log tells you:
> >
> >   [...]
> >   * checking examples ... ERROR
> >   Running examples in quantreg-Ex.R failed.
> >
> > BTW: Efforts have been made to upload these check logs to CRAN in
> > order to provide you with this information, so please read those
> > files!
> >
> >
> > Looking closer (as a hint for Roger), R *crashes* when running the
> > examples in ?boot.rq.
> >
> > > So know I have to download the extensions files and then compile
> > > them on my own ?
> >
> > You can download the source package and compile from source (I don't
> > know of any "extensions files").
> > Attention: The problem not passing Rcmd check remains (almost
> > certain).
> >
> > Uwe Ligges
> >
> > > Thank you
> > > Paolo Radaelli
> > >
> > > ----- Original Message -----
> > > From: "Uwe Ligges" <ligges at statistik.uni-dortmund.de>
> > > To: "Radaelli Paolo - Dottorati di Ricerca"
> > > <paolo.radaelli at unimib.it> Cc: <r-help at stat.math.ethz.ch>
> > > Sent: Thursday, October 23, 2003 4:44 PM
> > > Subject: Re: [R] Quantreg Package
> > >
> > >>Radaelli Paolo - Dottorati di Ricerca wrote:
> > >>>I've just installed R 1.0.8 (for Windows) and I tried to install
> > >>> the
> > >
> > > package Quantreg directly from Cran but it's not in the list of
> > > downlodable packages.
> > >
> > >>>I tried also downloading the zip file and then install it but
> > >>> there is
> > >
> > > an error.
> > >
> > >>>How can I do it?
> > >>>Thank you
> > >>>[[alternative HTML version deleted]]
> > >>>
> > >>>______________________________________________
> > >>>R-help at stat.math.ethz.ch mailing list
> > >>>https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> > >>
> > >>CRAN/bin/windows/contrib/1.8/ReadMe tells you that Windows
> > >> binaries of those packages not passing Rcmd check are not
> > >> published on CRAN. And the corresponding Status file and
> > >> check-log tells you quantreg is among those packages.
> > >>
> > >>So you have to compile from source yourself.
> > >>
> > >>Uwe Ligges
> >
> > ______________________________________________
> > R-help at stat.math.ethz.ch mailing list
> > https://www.stat.math.ethz.ch/mailman/listinfo/r-help
>



From gcendoya at balcarce.inta.gov.ar  Thu Oct 23 18:42:44 2003
From: gcendoya at balcarce.inta.gov.ar (CENDOYA, Gabriela)
Date: Thu, 23 Oct 2003 14:42:44 -0200
Subject: [R] List of lm objects
Message-ID: <001901c39984$b0644540$b54a6cc8@gcendoya.balcarce.inta.gov.ar>

Hi R-Helpers:

I?m trying to fit the same linear model to a bunch of variables in a data
frame, so I was trying to adapt the codes John Fox, Spencer Graves and Peter
Dalgaard  proposed and discused yesterday on this e-mail list:

for (y in df[, 3:5]) {
mod = lm(y ~ Trt*Dose, data = x, contrasts = list(Trt =
contr.sum, Dose = contr.sum))
Anova(mod, type = "III")
} ##  by John Fox

or
for (myname in names(myframe)){
  mycall <- substitute(lm(myvar~etc.etc.....),list(myvar=as.name(myname)))
  myfit <- eval(mycall)
  print(summary(myfit))
} ## by Peter Dalgaard

But instead of printing summary or Anova results, I need to generate a list
containing all the lm() objects.
Is that possible? How?

Thanks, Gabriela.



From tplate at blackmesacapital.com  Thu Oct 23 22:18:51 2003
From: tplate at blackmesacapital.com (Tony Plate)
Date: Thu, 23 Oct 2003 14:18:51 -0600
Subject: [R] what's going on here with substitute() ?
In-Reply-To: <x2he1zzs37.fsf@biostat.ku.dk>
References: <5.2.1.1.2.20031023102017.040a5cc0@mailhost.blackmesacapital.com>
	<5.2.1.1.2.20031023102017.040a5cc0@mailhost.blackmesacapital.com>
Message-ID: <5.2.1.1.2.20031023133205.041383f0@mailhost.blackmesacapital.com>

Peter, thank you for the explanation.  This is indeed what is happening.

Might I suggest the following passage for inclusion in the help page for 
"function", and possibly also "body", in the DETAILS or WARNING section:

"Note that the text of the original function definition is saved as an 
attribute "source" on the function, and this is printed out when the 
function is printed.  Hence, if the function body is changed in some way 
other than by assigning a value via body() (which removes the "source" 
attribute), the printed form of the function may not be the same as the 
actual function body."

Something along these lines could also go in the help for "eval", though if 
it were only there it might be very difficult to find.

Here is a transcript that shows what is happening, with another suggestion 
following it.

 > eval(substitute(this.is.R <- function() X, 
list(X=!is.null(options("CRAN")[[1]]))))
 > this.is.R
function() X
 > body(this.is.R)
[1] TRUE
 > attributes(this.is.R)
$source
[1] "function() X"
 > attributes(this.is.R) <- NULL
 > this.is.R
function ()
TRUE
 > # the "source" attribute comes from function definition:
 > attributes(function() X)
$source
[1] "function() X"
 > # and seems to be added by "eval":
 > attr(eval(parse(text="function() TRUE")[[1]]), "source")
[1] "function() TRUE"
 >

 > # we can assign bogus "source"
 > attr(this.is.R, "source") <- "a totally bogus function body"
 > this.is.R
a totally bogus function body
 > # assigning to body() removes "source"
 > body(this.is.R) <- list(666)
 > this.is.R
function ()
666
 > attr(this.is.R, "source")
NULL
 >

An even better approach might be something that gave a warning on printing 
if the parsed "source" attribute was not identical to the language object 
being printed.  This would probably belong in the code for "case LANGSXP:" 
in the function PrintValueRec in main/print.c (if it were written in R, I 
could contribute a patch, but right now I don't have time to try to 
understand the C there.)  R code to do the test could be something like this:

 > f <- this.is.R
 > identical(f, eval(parse(text=attr(f, "source"))[[1]]))
[1] FALSE
 > f <- function() TRUE
 > identical(f, eval(parse(text=attr(f, "source"))[[1]]))
[1] TRUE
 >

-- Tony Plate



At Thursday 09:12 PM 10/23/2003 +0200, Peter Dalgaard wrote:
>Tony Plate <tplate at blackmesacapital.com> writes:
>
> > Why is the body of the function "X" when I substituted a different
> > expression for X? Also, given that the body of the function is X, how
> > does the function evaluate to TRUE since X is not defined anywhere
> > (except in a list that should have been discarded.)
> >
> > This happens with both R 1.7.1 and R 1.8.0 (under Windows 2000).
> >
> > (yes, I did discover the function is.R(), but I still want to discover
> > what's going here.)
>
>I think you are defeating the keep.source mechanism. What you're
>seeing is not the actual function but its source attribute. Try
>attributes(this.is.R) <- NULL and see what happens.
>
>--
>    O__  ---- Peter Dalgaard             Blegdamsvej 3
>   c/ /'_ --- Dept. of Biostatistics     2200 Cph. N
>  (*) \(*) -- University of Copenhagen   Denmark      Ph: (+45) 35327918
>~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)             FAX: (+45) 35327907



From p.dalgaard at biostat.ku.dk  Thu Oct 23 22:24:52 2003
From: p.dalgaard at biostat.ku.dk (Peter Dalgaard)
Date: 23 Oct 2003 22:24:52 +0200
Subject: [R] List of lm objects
In-Reply-To: <001901c39984$b0644540$b54a6cc8@gcendoya.balcarce.inta.gov.ar>
References: <001901c39984$b0644540$b54a6cc8@gcendoya.balcarce.inta.gov.ar>
Message-ID: <x28ynbzoq3.fsf@biostat.ku.dk>

"CENDOYA, Gabriela" <gcendoya at balcarce.inta.gov.ar> writes:

> for (myname in names(myframe)){
>   mycall <- substitute(lm(myvar~etc.etc.....),list(myvar=as.name(myname)))
>   myfit <- eval(mycall)
>   print(summary(myfit))
> } ## by Peter Dalgaard
> 
> But instead of printing summary or Anova results, I need to generate a list
> containing all the lm() objects.
> Is that possible? How?

Very easily:

f <- function(myname){
  mycall <- substitute(lm(myvar~etc.etc.....),list(myvar=as.name(myname)))
  myfit <- eval(mycall)
  print(summary(myfit))
}

lapply(names(myframe),f)

-- 
   O__  ---- Peter Dalgaard             Blegdamsvej 3  
  c/ /'_ --- Dept. of Biostatistics     2200 Cph. N   
 (*) \(*) -- University of Copenhagen   Denmark      Ph: (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)             FAX: (+45) 35327907



From tlumley at u.washington.edu  Thu Oct 23 22:26:17 2003
From: tlumley at u.washington.edu (Thomas Lumley)
Date: Thu, 23 Oct 2003 13:26:17 -0700 (PDT)
Subject: [R] repeating colors in graph 2
In-Reply-To: <sf97e887.050@med-gwia-01a.med.umich.edu>
References: <sf97e887.050@med-gwia-01a.med.umich.edu>
Message-ID: <Pine.A41.4.58.0310231325360.173338@homer04.u.washington.edu>

On Thu, 23 Oct 2003, James MacDonald wrote:

> I believe you want plot(blah blah, col=palette(rainbow(13)))
>

No.  palette() sets the default palette, so
eg
> palette(rainbow(13))
> plot(1:13,col=1:13)

will plot in the new palette.

	-thomas



From spencer.graves at pdf.com  Thu Oct 23 22:34:45 2003
From: spencer.graves at pdf.com (Spencer Graves)
Date: Thu, 23 Oct 2003 13:34:45 -0700
Subject: [R] List of lm objects
In-Reply-To: <001901c39984$b0644540$b54a6cc8@gcendoya.balcarce.inta.gov.ar>
References: <001901c39984$b0644540$b54a6cc8@gcendoya.balcarce.inta.gov.ar>
Message-ID: <3F983B65.5030903@pdf.com>

How about the following:

AList <- vector(mode="list", length=3)
for(i in 1:3){
y <- df[,i])
mod <- lm(y~Tr*Dose, data=x, contrasts=list(Tr=contr.sum, Dose=contr.sum))
AList[[i]] <- Anova(mod, type="III")
}

hope this helps. spencer graves

CENDOYA, Gabriela wrote:

>Hi R-Helpers:
>
>I?m trying to fit the same linear model to a bunch of variables in a data
>frame, so I was trying to adapt the codes John Fox, Spencer Graves and Peter
>Dalgaard  proposed and discused yesterday on this e-mail list:
>
>for (y in df[, 3:5]) {
>mod = lm(y ~ Trt*Dose, data = x, contrasts = list(Trt =
>contr.sum, Dose = contr.sum))
>Anova(mod, type = "III")
>} ##  by John Fox
>
>or
>for (myname in names(myframe)){
>  mycall <- substitute(lm(myvar~etc.etc.....),list(myvar=as.name(myname)))
>  myfit <- eval(mycall)
>  print(summary(myfit))
>} ## by Peter Dalgaard
>
>But instead of printing summary or Anova results, I need to generate a list
>containing all the lm() objects.
>Is that possible? How?
>
>Thanks, Gabriela.
>
>______________________________________________
>R-help at stat.math.ethz.ch mailing list
>https://www.stat.math.ethz.ch/mailman/listinfo/r-help
>  
>



From ihaka at stat.auckland.ac.nz  Thu Oct 23 23:13:13 2003
From: ihaka at stat.auckland.ac.nz (Ross Ihaka)
Date: Fri, 24 Oct 2003 10:13:13 +1300
Subject: [R] GIS re-mapping / polygon overlap
References: <32412.1066909008@www59.gmx.net>
Message-ID: <3F984469.8090305@stat.auckland.ac.nz>

Jens Oehlschl?gel wrote:

>   AND
>     - is there a general function in R available to map demographic data
> between two boundary systems
>     OR
>     - is there an efficient R function that can determine the amount of area
> overlap between two general polygons (beyond gpclib)
> 
> I am willing to share developed code for this problem (that's why I would
> prefer gpl code over gpclib, which is not really free)
> 
> Thanks for any help

There is pretty cool polygon intersection algorithm available at:

	http://davis.wpi.edu/~matt/courses/clipping/4th.html

with the code at:

	http://davis.wpi.edu/~matt/courses/clipping/code.html

You can pick up the paper the code is based on from kai hormann's home
page (currently):

	http://vcg.isti.cnr.it/~hormann/

The only problem with the algorithm is that it doesn't handle holes, but 
for area computations you could just join the holes to the boundary.

-- 
Ross Ihaka                         Email:  ihaka at stat.auckland.ac.nz
Department of Statistics           Phone:  (64-9) 373-7599 x 85054
University of Auckland             Fax:    (64-9) 373-7018
Private Bag 92019, Auckland
New Zealand



From p.dalgaard at biostat.ku.dk  Thu Oct 23 23:17:33 2003
From: p.dalgaard at biostat.ku.dk (Peter Dalgaard)
Date: 23 Oct 2003 23:17:33 +0200
Subject: [R] List of lm objects
In-Reply-To: <x28ynbzoq3.fsf@biostat.ku.dk>
References: <001901c39984$b0644540$b54a6cc8@gcendoya.balcarce.inta.gov.ar>
	<x28ynbzoq3.fsf@biostat.ku.dk>
Message-ID: <x24qxzzmaa.fsf@biostat.ku.dk>

Peter Dalgaard <p.dalgaard at biostat.ku.dk> writes:

> "CENDOYA, Gabriela" <gcendoya at balcarce.inta.gov.ar> writes:
> 
> > for (myname in names(myframe)){
> >   mycall <- substitute(lm(myvar~etc.etc.....),list(myvar=as.name(myname)))
> >   myfit <- eval(mycall)
> >   print(summary(myfit))
> > } ## by Peter Dalgaard
> > 
> > But instead of printing summary or Anova results, I need to generate a list
> > containing all the lm() objects.
> > Is that possible? How?
> 
> Very easily:
> 
> f <- function(myname){
>   mycall <- substitute(lm(myvar~etc.etc.....),list(myvar=as.name(myname)))
>   myfit <- eval(mycall)
>   print(summary(myfit))
> }
> 
> lapply(names(myframe),f)

Whoops. A bit too quick there: You want the lm objects, not summaries
and no printing, so make that

f <- function(myname){
  mycall <- substitute(lm(myvar~etc.etc.....),list(myvar=as.name(myname)))
  eval(mycall)
}
lapply(names(myframe),f)

Or, if you dont care about having the names substituted in the lm
calls (the list elements will get named appropriately)

lapply(myframe, function(y) lm(y~etc.etc.))

-- 
   O__  ---- Peter Dalgaard             Blegdamsvej 3  
  c/ /'_ --- Dept. of Biostatistics     2200 Cph. N   
 (*) \(*) -- University of Copenhagen   Denmark      Ph: (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)             FAX: (+45) 35327907



From hi_ono2001 at ybb.ne.jp  Thu Oct 23 23:33:20 2003
From: hi_ono2001 at ybb.ne.jp (Hisaji Ono)
Date: Fri, 24 Oct 2003 06:33:20 +0900
Subject: [R] GIS re-mapping / polygon overlap
References: <32412.1066909008@www59.gmx.net>
	<3F984469.8090305@stat.auckland.ac.nz>
Message-ID: <014c01c399ad$4780ef50$818001db@webgis>

Hi.

 How about GEOS available at.

 http://geos.refractions.net/

GEOS supports hole for clipping.

 I could build it in MiGW.

 However I've only executed demo codes.


----- Original Message ----- 
From: "Ross Ihaka" <ihaka at stat.auckland.ac.nz>
To: "Jens Oehlschl?gel" <oehl_list at gmx.de>
Cc: <r-help at stat.math.ethz.ch>
Sent: Friday, October 24, 2003 6:13 AM
Subject: Re: [R] GIS re-mapping / polygon overlap


> Jens Oehlschl?gel wrote:

> >     - is there an efficient R function that can determine the amount of
area
> > overlap between two general polygons (beyond gpclib)
> >
> > I am willing to share developed code for this problem (that's why I
would
> > prefer gpl code over gpclib, which is not really free)

>
> There is pretty cool polygon intersection algorithm available at:
>
> http://davis.wpi.edu/~matt/courses/clipping/4th.html
>
> with the code at:
>
> http://davis.wpi.edu/~matt/courses/clipping/code.html
>
> You can pick up the paper the code is based on from kai hormann's home
> page (currently):
>
> http://vcg.isti.cnr.it/~hormann/
>
> The only problem with the algorithm is that it doesn't handle holes, but
> for area computations you could just join the holes to the boundary.
>



From KKelley at nd.edu  Fri Oct 24 00:09:20 2003
From: KKelley at nd.edu (Ken Kelley)
Date: Thu, 23 Oct 2003 17:09:20 -0500
Subject: [R] Generating Data Sets -- Each with a Different Name
Message-ID: <5.1.0.14.2.20031023164706.00b28218@imap-k.nd.edu>

Hello all.

I was wondering if anyone had insight into how I might generate a large 
number of data sets/vectors, where each of the data sets/vectors have a 
different name?

For example, suppose I wanted to generate N data sets/vectors, each with a 
different name such as:

Random.Data.1 <- rnorm(10, 0, 1)
Random.Data.2 <- rnorm(10, 0, 1)
Random.Data.3 <- rnorm(10, 0, 1)
	.		.	
	.		.
	.		.
Random.Data.N <- rnorm(10, 0, 1)

Because I don't want to name each data set/vector myself, I want some sort 
of looping/automatic procedure to do it for me. However, I'm not sure how 
to do this.

What I want is something conceptually analogous to the following:

for(i in 1:N)
{
Random.Data.i <- rnorm(10, 0, 1)
}

Note the "i" in the "Random.Data.i" vector. This is the value I want to 
change for each iteration of the loop, so that I can have N data 
sets/vectors automatically generated for me with different names.

Does anyone know of a method where I can accomplish such a procedure.
Thanks for your thoughts,
Ken

P.S. For those of you wondering why I would ever want to do such a thing, 
it is because I need a large number of data sets to use in a Monte Carlo 
study with specialized software. If I could do the simulation all is R, 
then obviously I could loop the whole procedure. The program I am using has 
a limited Monte Carlo facility where each of the (say 10,000) data sets 
must be generated elsewhere.



From spencer.graves at pdf.com  Fri Oct 24 00:24:20 2003
From: spencer.graves at pdf.com (Spencer Graves)
Date: Thu, 23 Oct 2003 15:24:20 -0700
Subject: [R] Generating Data Sets -- Each with a Different Name
In-Reply-To: <5.1.0.14.2.20031023164706.00b28218@imap-k.nd.edu>
References: <5.1.0.14.2.20031023164706.00b28218@imap-k.nd.edu>
Message-ID: <3F985514.3090509@pdf.com>

      Have you considered using "paste(..., sep="") to generate either 
the object names or file names you want? 

      If you want objects in R, have you considered "assign"?  If you 
want files, then have you considered "write"? 

      hope this helps. 
      spencer graves

Ken Kelley wrote:

> Hello all.
>
> I was wondering if anyone had insight into how I might generate a 
> large number of data sets/vectors, where each of the data sets/vectors 
> have a different name?
>
> For example, suppose I wanted to generate N data sets/vectors, each 
> with a different name such as:
>
> Random.Data.1 <- rnorm(10, 0, 1)
> Random.Data.2 <- rnorm(10, 0, 1)
> Random.Data.3 <- rnorm(10, 0, 1)
>     .        .   
>     .        .
>     .        .
> Random.Data.N <- rnorm(10, 0, 1)
>
> Because I don't want to name each data set/vector myself, I want some 
> sort of looping/automatic procedure to do it for me. However, I'm not 
> sure how to do this.
>
> What I want is something conceptually analogous to the following:
>
> for(i in 1:N)
> {
> Random.Data.i <- rnorm(10, 0, 1)
> }
>
> Note the "i" in the "Random.Data.i" vector. This is the value I want 
> to change for each iteration of the loop, so that I can have N data 
> sets/vectors automatically generated for me with different names.
>
> Does anyone know of a method where I can accomplish such a procedure.
> Thanks for your thoughts,
> Ken
>
> P.S. For those of you wondering why I would ever want to do such a 
> thing, it is because I need a large number of data sets to use in a 
> Monte Carlo study with specialized software. If I could do the 
> simulation all is R, then obviously I could loop the whole procedure. 
> The program I am using has a limited Monte Carlo facility where each 
> of the (say 10,000) data sets must be generated elsewhere.
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help



From apjaworski at mmm.com  Fri Oct 24 00:47:21 2003
From: apjaworski at mmm.com (apjaworski@mmm.com)
Date: Thu, 23 Oct 2003 17:47:21 -0500
Subject: [R] Generating Data Sets -- Each with a Different Name
Message-ID: <OF73287EB3.1B1B5A06-ON86256DC8.007CD7C3-86256DC8.007D2F63@mmm.com>


You need paste and assign functions.  Paste will create a sequence of names
and assign will give values to the variables with these names.

This modification of your loop should work

for(i in 1:N)
{
assign(paste("Random.Data.", i, sep=''), rnorm(10, 0, 1))
}

Cheers,

Andy

__________________________________
Andy Jaworski
Engineering Systems Technology Center
3M Center, 518-1-01
St. Paul, MN 55144-1000
-----
E-mail: apjaworski at mmm.com
Tel:  (651) 733-6092
Fax:  (651) 736-3122


|---------+-------------------------------->
|         |           Ken Kelley           |
|         |           <KKelley at nd.edu>     |
|         |           Sent by:             |
|         |           r-help-bounces at stat.m|
|         |           ath.ethz.ch          |
|         |                                |
|         |                                |
|         |           10/23/2003 17:09     |
|         |                                |
|---------+-------------------------------->
  >-----------------------------------------------------------------------------------------------------------------------------|
  |                                                                                                                             |
  |      To:       r-help at stat.math.ethz.ch                                                                                     |
  |      cc:                                                                                                                    |
  |      Subject:  [R] Generating Data Sets -- Each with a Different Name                                                       |
  >-----------------------------------------------------------------------------------------------------------------------------|




Hello all.

I was wondering if anyone had insight into how I might generate a large
number of data sets/vectors, where each of the data sets/vectors have a
different name?

For example, suppose I wanted to generate N data sets/vectors, each with a
different name such as:

Random.Data.1 <- rnorm(10, 0, 1)
Random.Data.2 <- rnorm(10, 0, 1)
Random.Data.3 <- rnorm(10, 0, 1)
             .                       .
             .                       .
             .                       .
Random.Data.N <- rnorm(10, 0, 1)

Because I don't want to name each data set/vector myself, I want some sort
of looping/automatic procedure to do it for me. However, I'm not sure how
to do this.

What I want is something conceptually analogous to the following:

for(i in 1:N)
{
Random.Data.i <- rnorm(10, 0, 1)
}

Note the "i" in the "Random.Data.i" vector. This is the value I want to
change for each iteration of the loop, so that I can have N data
sets/vectors automatically generated for me with different names.

Does anyone know of a method where I can accomplish such a procedure.
Thanks for your thoughts,
Ken

P.S. For those of you wondering why I would ever want to do such a thing,
it is because I need a large number of data sets to use in a Monte Carlo
study with specialized software. If I could do the simulation all is R,
then obviously I could loop the whole procedure. The program I am using has

a limited Monte Carlo facility where each of the (say 10,000) data sets
must be generated elsewhere.

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://www.stat.math.ethz.ch/mailman/listinfo/r-help



From wfg at hotpennies.net  Thu Oct 23 17:29:20 2003
From: wfg at hotpennies.net (Hot Pennies)
Date: Thu, 23 Oct 2003 10:29:20 -0500
Subject: [R] IPFS Watch This St ock Trade
Message-ID: <56b5601c3997a$6dec7a20$66212c45@c2>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20031023/b8dd086d/attachment.pl

From ihaka at stat.auckland.ac.nz  Fri Oct 24 01:36:15 2003
From: ihaka at stat.auckland.ac.nz (Ross Ihaka)
Date: Fri, 24 Oct 2003 12:36:15 +1300
Subject: [R] GIS re-mapping / polygon overlap
References: <32412.1066909008@www59.gmx.net>	<3F984469.8090305@stat.auckland.ac.nz>
	<014c01c399ad$4780ef50$818001db@webgis>
Message-ID: <3F9865EF.40608@stat.auckland.ac.nz>

Hisaji Ono wrote:
> Hi.
> 
>  How about GEOS available at.
> 
>  http://geos.refractions.net/
> 
> GEOS supports hole for clipping.
> 
>  I could build it in MiGW.
> 
>  However I've only executed demo codes.

Very nice reference.  I will be taking a close look through it.

The Greiner-Hormann algorithm is still pretty interesting.
The code is very simple and fast.

-- 
Ross Ihaka                         Email:  ihaka at stat.auckland.ac.nz
Department of Statistics           Phone:  (64-9) 373-7599 x 85054
University of Auckland             Fax:    (64-9) 373-7018
Private Bag 92019, Auckland
New Zealand



From arrayprofile at yahoo.com  Fri Oct 24 02:40:16 2003
From: arrayprofile at yahoo.com (array chip)
Date: Thu, 23 Oct 2003 17:40:16 -0700 (PDT)
Subject: [R] linear discriminant analysis
Message-ID: <20031024004016.41663.qmail@web41206.mail.yahoo.com>

Hi all,

When I use "lda" for discriminant analysis, should I
normalized my data (variables) to mean 0, variance 1
before running "lda" if my variables might not be
exactly on the same scale? I have this question
because in principle component analysis, this is
indeed an issue where we can choose covariance matrix
or correlation matrix when running the analysis. Just
wondering if this is also an issue people should
consider when using discriminant analysis. 

second question: "coef(lda object)" returns
coefficients of the variables, are they standardized
coefficients (i.e. they should be applied to
standardized variables to calculate discrimnant
scores), or are they unstandardized coefficients (i.e.
they can be directly applied to the original variable
to compute the discrimnant scores)? SPSS can output
both coefficients, but it seems that "lda" can only
output one, I think they should be standardized
coefficeients, but not sure.

Thanks very much



From jiwarner at comcast.net  Fri Oct 24 06:21:43 2003
From: jiwarner at comcast.net (Janice Warner)
Date: Fri, 24 Oct 2003 00:21:43 -0400
Subject: [R] Scan() question
Message-ID: <000e01c399e6$55561ae0$6754fea9@TOSHIBAS503>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20031024/e4b671df/attachment.pl

From jbwu at pangea.stanford.edu  Fri Oct 24 06:22:16 2003
From: jbwu at pangea.stanford.edu (jbwu)
Date: Thu, 23 Oct 2003 21:22:16 -0700
Subject: [R] R -- for help
Message-ID: <5.1.0.14.2.20031023211754.03815c50@pangea.stanford.edu>

hi, guys:
I have a question on how to read data from file in which the length of each 
line is unfixed.

For instance, the data in sample.dat are:
1 2 3 4
5 6 7
8

I want to read this kind of data into a matrix as:
1 2 3 4
5 6 7 0
8 0 0 0

Thank you


Jianbing Wu



From Nicole.Baggette at pomona.edu  Fri Oct 24 06:24:29 2003
From: Nicole.Baggette at pomona.edu (Nicole Baggette)
Date: Thu, 23 Oct 2003 21:24:29 -0700
Subject: [R] Searching a Table
Message-ID: <A3483BF98372D411B7C50020482E1938067A4B68@excsrv-stu1.pomona.edu>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20031023/f015e4f5/attachment.pl

From jasont at indigoindustrial.co.nz  Fri Oct 24 07:13:09 2003
From: jasont at indigoindustrial.co.nz (Jason Turner)
Date: Fri, 24 Oct 2003 18:13:09 +1300
Subject: [R] Searching a Table
In-Reply-To: <A3483BF98372D411B7C50020482E1938067A4B68@excsrv-stu1.pomona.edu>
References: <A3483BF98372D411B7C50020482E1938067A4B68@excsrv-stu1.pomona.edu>
Message-ID: <3F98B4E5.2060604@indigoindustrial.co.nz>

Nicole Baggette wrote:

> Is there any way to search a column of a table?
> 
>  
> 
> I read in a table and I want to create a subset based on the criteria that a
> certain column (filled with words) has a word which starts with a certain
> letter. The only method I have seen that searches this way is apropos(), but
> that doesn't seem to search the location I want. If anyone can make a
> recommendation I would appreciate it.

help(grep)

To search for words that start with "W" or "w" (minus the quotes) in a 
data frame called "mydata", column 3,

grep("\\bw", mydata[,3], ignore.case=TRUE, perl=TRUE)

Which means - "\\b" - word boundary.
The "w" and ignore.case should be self-explanatory.
perl - use perl style regular expressions.

example:

foo <- c("now is The Time","for all good llamas","to come to the party")
foo

#find lines that contain words starting with "t" or "T"

grep("\\bt",foo,ignore.case=TRUE,perl=TRUE)

foo[grep("\\bt",foo,ignore.case=TRUE,perl=TRUE)]

Cheers

Jason
-- 
Indigo Industrial Controls Ltd.
http://www.indigoindustrial.co.nz
64-21-343-545
jasont at indigoindustrial.co.nz



From jasont at indigoindustrial.co.nz  Fri Oct 24 07:23:54 2003
From: jasont at indigoindustrial.co.nz (Jason Turner)
Date: Fri, 24 Oct 2003 18:23:54 +1300
Subject: [R] Scan() question
In-Reply-To: <000e01c399e6$55561ae0$6754fea9@TOSHIBAS503>
References: <000e01c399e6$55561ae0$6754fea9@TOSHIBAS503>
Message-ID: <3F98B76A.4010505@indigoindustrial.co.nz>

Janice Warner wrote:

> I am just beginning to use R 1.7.1 on Windows XP and can not do a simple 
 > scan of a .txt file with integer data separated by tabs.
> 
> 
> First I figured out that I need to open a connection to the file 
 >(which isn't mentioned in scan()).

It isn't mentioned because it isn't needed.

I'd try

read.csv(filename,sep="\t") # for tab-separated files


 > But now when I scan,  I get "Read 0 items".  What can I do?

Shouldn't happen quite like that.

I have a file on my XP box, in my "My Documents" folder, called 
"foo.txt".  It's a two-column text file, tab-separated.  First column is 
  the numbers 1 to 12.  Second column is some random numbers.

 > scan("c:/documents and settings/jason turner/my documents/foo.csv")
Read 24 items
  [1]  1.00000000  0.05905332  2.00000000  0.21829890  3.00000000 
0.07104709
  [7]  4.00000000  0.67140721  5.00000000  0.44471572  6.00000000 
0.75093844
[13]  7.00000000  0.40647603  8.00000000  0.85937071  9.00000000  0.31583605
[19] 10.00000000  0.77993103 11.00000000  0.55067598 12.00000000  0.25861385

Notice the "/" rather than the "\" as the directory delimiter.

Cheers

Jason

-- 
Indigo Industrial Controls Ltd.
http://www.indigoindustrial.co.nz
64-21-343-545
jasont at indigoindustrial.co.nz



From ripley at stats.ox.ac.uk  Fri Oct 24 08:11:32 2003
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Fri, 24 Oct 2003 07:11:32 +0100 (BST)
Subject: [R] R -- for help
In-Reply-To: <5.1.0.14.2.20031023211754.03815c50@pangea.stanford.edu>
Message-ID: <Pine.LNX.4.44.0310240709210.3699-100000@gannet.stats>

Look at the `fill' argument in ?read.table.

This fills with NAs, but you can convert those to 0's later (although
it may be better to do this in the analysis).

On Thu, 23 Oct 2003, jbwu wrote:

> hi, guys:
> I have a question on how to read data from file in which the length of each 
> line is unfixed.
> 
> For instance, the data in sample.dat are:
> 1 2 3 4
> 5 6 7
> 8
> 
> I want to read this kind of data into a matrix as:
> 1 2 3 4
> 5 6 7 0
> 8 0 0 0

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From wettenhall at wehi.edu.au  Fri Oct 24 08:54:03 2003
From: wettenhall at wehi.edu.au (James Wettenhall)
Date: Fri, 24 Oct 2003 16:54:03 +1000 (EST)
Subject: [R] select text using only the keyboard
Message-ID: <Pine.LNX.4.44.0310241641530.15442-100000@unix28.alpha.wehi.edu.au>

Simon,

> I have a different wish: I want to be able to mouseclick in
> the middle of a line to get the cursor there (as in SPlus).
>
> While I appreciate that to get my wish I should just write
> a little patch, I estimate it would take me about 2 years
> to reach the point where I was capable of it,

Depends on how much C programming you've done.  I could possibly 
write a patch in less than 2 years, but I have other things 
which are higher priority right now.  Here are some things you 
would need to look at:

Mouse-button event-handlers in GraphApp:
http://www.cs.usyd.edu.au/~graphapp/doc/manual/mouse.htm

Setting the cursor position/selection in a text window in 
GraphApp:
http://www.cs.usyd.edu.au/~loki/graphapp/manual/textedit.htm

Building R for Windows:
http://www.stats.ox.ac.uk/pub/Rtools/

Regards,
James



From bhx2 at mevik.net  Fri Oct 24 09:36:26 2003
From: bhx2 at mevik.net (=?iso-8859-1?q?Bj=F8rn-Helge_Mevik?=)
Date: Fri, 24 Oct 2003 09:36:26 +0200
Subject: [R] disappointed (card model)
In-Reply-To: <700C050C-0568-11D8-9549-000A9566473A@or.psychology.dal.ca> (John
	Christie's message of "Thu, 23 Oct 2003 11:51:52 -0300")
References: <700C050C-0568-11D8-9549-000A9566473A@or.psychology.dal.ca>
Message-ID: <7oekx3axz9.fsf@foo.nemo-project.org>

Don't be disappointed, be glad:  It gives you the opportunity to
contribute by writing one yourself!

(Remember, R is developed by volunteers.)

-- 
Sincerely,
Bj?rn-Helge Mevik



From johannes.huesing at medizin.uni-essen.de  Fri Oct 24 10:29:54 2003
From: johannes.huesing at medizin.uni-essen.de (=?iso-8859-1?Q?=22H=FCsing=2C_Johannes=22?=)
Date: Fri, 24 Oct 2003 10:29:54 +0200
Subject: [R] Scheme and R
Message-ID: <B3A80C9C13928B45B2FCE4C43656363A0194E2BB@mail-srv02.master.medizin.uni-essen.de>



> 
> I have been using a product called rpy to allow me to access R from
> Python.  I was wondering if anybody know if a similar product for
> Scheme.

The closest thing I know is a link to XLisp:
http://www.omegahat.org/RXLisp/index.html.

XLisp is syntactically much closer to Common Lisp than to Scheme,
but maybe you can live with that.

Regards


Johannes



From B.Rowlingson at lancaster.ac.uk  Fri Oct 24 11:00:59 2003
From: B.Rowlingson at lancaster.ac.uk (Barry Rowlingson)
Date: Fri, 24 Oct 2003 10:00:59 +0100
Subject: [R] disappointed (card model)
In-Reply-To: <7oekx3axz9.fsf@foo.nemo-project.org>
References: <700C050C-0568-11D8-9549-000A9566473A@or.psychology.dal.ca>
	<7oekx3axz9.fsf@foo.nemo-project.org>
Message-ID: <3F98EA4B.4070907@lancaster.ac.uk>

Bj?rn-Helge Mevik wrote:
> Don't be disappointed, be glad:  It gives you the opportunity to
> contribute by writing one yourself!
> 
> (Remember, R is developed by volunteers.)
> 

  Hmmmm how would this work?


 > myDeck <- deck() # create fresh deck of 52 cards

 > shuffle(myDeck)

 > myDeal <- deal(myDeck,hands=4,cards=2)  # this modifies myDeck, 
removing the dealt cards

 > myDeal[1]
[1] "Ace of Hearts" "King of Diamonds"

 > myDeal[2]
[1] "Three of Clubs" "Jack of Diamonds"

 > Flop <- deal(myDeck, cards=3, hands=1)[1]

 > Flop
[1] "Queen of Spades" "Jack of Clubs" "Ten of Diamonds"

 > bestHand(c(myDeal[1],Flop))
[1] "Straight to Ace"

 > bestHand(c(myDeal[2],Flop))
[1] "Pair of Jacks"

 > bestHand(c(myDeal[2],Flop)) > bestHand(c(myDeal[1],Flop))
[1] FALSE

- then I could write my Texas Hold'Em poker tactics in R, but I dont 
think my poker playing friends would be happy with my laptop at the 
poker table...

Baz



From rksh at soc.soton.ac.uk  Fri Oct 24 11:18:25 2003
From: rksh at soc.soton.ac.uk (Robin Hankin)
Date: Fri, 24 Oct 2003 10:18:25 +0100
Subject: [R] x[c(1,2,1)] <-  1:3
Message-ID: <a06002008bbbe9e5c18fe@[139.166.242.29]>

Hi everyone.

look at this:

x <- 1:4
x[c(1,2,1)] <- (1:3)
print(x[1])

I get 3, but isn't NA more appropriate? [1 would be as sensible].
FWIW, the equivalent Fortran 95 statement is flagged as an error.
R-intro, section 2.7, says that in such cases the assignment is
carried out "in order" which might support getting 3.

To my way of thinking, the concept of "in order" seems to violate the
usual strategy of considering vectors as whole entities---because in
this case we have to specify whether the assignment starts at
c(1,2,1)[1] and proceeds to c(1,2,1)[3], or starts at c(1,2,1)[3] and
proceeds to c(1,2,1)[1].  And the results are different!


What is the R position on this?


rksh



From Simon.Fear at synequanon.com  Fri Oct 24 11:31:35 2003
From: Simon.Fear at synequanon.com (Simon Fear)
Date: Fri, 24 Oct 2003 10:31:35 +0100
Subject: [R] x[c(1,2,1)] <-  1:3
Message-ID: <6C8A8033ABC1E3468048ABC4F13CE572AC0E53@synequanon01>

?order descending in means "order" assume always you Do

> -----Original Message-----
> From: Robin Hankin [mailto:rksh at soc.soton.ac.uk]
> Sent: 24 October 2003 10:18
> To: r-help at stat.math.ethz.ch
> Subject: [R] x[c(1,2,1)] <- 1:3
> 
> 
> Security Warning: 
> If you are not sure an attachment is safe to open please contact  
> Andy on x234. There are 0 attachments with this message. 
> ________________________________________________________________ 
>  
> Hi everyone.
> 
> look at this:
> 
> x <- 1:4
> x[c(1,2,1)] <- (1:3)
> print(x[1])
> 
> I get 3, but isn't NA more appropriate? [1 would be as sensible].
> FWIW, the equivalent Fortran 95 statement is flagged as an error.
> R-intro, section 2.7, says that in such cases the assignment is
> carried out "in order" which might support getting 3.
> 
> To my way of thinking, the concept of "in order" seems to violate the
> usual strategy of considering vectors as whole entities---because in
> this case we have to specify whether the assignment starts at
> c(1,2,1)[1] and proceeds to c(1,2,1)[3], or starts at c(1,2,1)[3] and
> proceeds to c(1,2,1)[1].  And the results are different!
> 
> 
> What is the R position on this?
> 
> 
> rksh
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
>  
 
Simon Fear 
Senior Statistician 
Syne qua non Ltd 
Tel: +44 (0) 1379 644449 
Fax: +44 (0) 1379 644445 
email: Simon.Fear at synequanon.com 
web: http://www.synequanon.com 
  
Number of attachments included with this message: 0 
  
This message (and any associated files) is confidential and\...{{dropped}}



From ivonefig at ipimar.pt  Fri Oct 24 11:50:57 2003
From: ivonefig at ipimar.pt (Ivone Figueiredo)
Date: Fri, 24 Oct 2003 10:50:57 +0100
Subject: [R] Nonlinear model Parameter estimation
Message-ID: <003c01c39a14$52bdf4f0$c9040a0a@Ivone1>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20031024/49d40b9a/attachment.pl

From ripley at stats.ox.ac.uk  Fri Oct 24 11:56:35 2003
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Fri, 24 Oct 2003 10:56:35 +0100 (BST)
Subject: [R] x[c(1,2,1)] <-  1:3
In-Reply-To: <a06002008bbbe9e5c18fe@[139.166.242.29]>
Message-ID: <Pine.LNX.4.44.0310241054410.15761-100000@gannet.stats>

On Fri, 24 Oct 2003, Robin Hankin wrote:

> Hi everyone.
> 
> look at this:
> 
> x <- 1:4
> x[c(1,2,1)] <- (1:3)
> print(x[1])
> 
> I get 3, but isn't NA more appropriate? [1 would be as sensible].
> FWIW, the equivalent Fortran 95 statement is flagged as an error.
> R-intro, section 2.7, says that in such cases the assignment is
> carried out "in order" which might support getting 3.
> 
> To my way of thinking, the concept of "in order" seems to violate the
> usual strategy of considering vectors as whole entities---because in
> this case we have to specify whether the assignment starts at
> c(1,2,1)[1] and proceeds to c(1,2,1)[3], or starts at c(1,2,1)[3] and
> proceeds to c(1,2,1)[1].  And the results are different!
> 
> 
> What is the R position on this?

That this is correct and works as documented!  It is occasionally useful.

R implements the S language, not Fortran 95, so consult the S references 
please.

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From vtas at uosis.mif.vu.lt  Fri Oct 24 12:13:21 2003
From: vtas at uosis.mif.vu.lt (vtas@uosis.mif.vu.lt)
Date: Fri, 24 Oct 2003 13:13:21 +0300 (EEST)
Subject: [R] installing packages not on CRAN
Message-ID: <2114.193.219.42.106.1066990401.squirrel@kedras.mif.vu.lt>

Hi,

how to use install.packages to install SimpleR package of J. Verzani?

using install.packages with the CRAN packages is simple:

install.packages("ineq",contriburl="http://cran.at.r-project.org/bin/windows/contrib/1.8",CRAN=NULL)

same with Simple package fails:

install.packages("Simple",contriburl="http://www.math.csi.cuny.edu/Statistics/R/simpleR",CRAN=NULL)

since there is no file named PACKAGES.
My solution is:

path<-paste(getwd(),"simple.zip", sep="/")
download.file("http://www.math.csi.cuny.edu/Statistics/R/simpleR/Simple_0.4.zip",path,mode="wb")
install.packages(path,CRAN=NULL)

Is there more elegant way to do this in Windows?

Vytautas Maniusis, Vilnius University, lithuania



From ripley at stats.ox.ac.uk  Fri Oct 24 12:42:15 2003
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Fri, 24 Oct 2003 11:42:15 +0100 (BST)
Subject: [R] installing packages not on CRAN
In-Reply-To: <2114.193.219.42.106.1066990401.squirrel@kedras.mif.vu.lt>
Message-ID: <Pine.LNX.4.44.0310241140510.17194-100000@gannet.stats>

On Fri, 24 Oct 2003 vtas at uosis.mif.vu.lt wrote:

> Hi,
> 
> how to use install.packages to install SimpleR package of J. Verzani?
> 
> using install.packages with the CRAN packages is simple:
> 
> install.packages("ineq",contriburl="http://cran.at.r-project.org/bin/windows/contrib/1.8",CRAN=NULL)
> 
> same with Simple package fails:
> 
> install.packages("Simple",contriburl="http://www.math.csi.cuny.edu/Statistics/R/simpleR",CRAN=NULL)
> 
> since there is no file named PACKAGES.
> My solution is:
> 
> path<-paste(getwd(),"simple.zip", sep="/")
> download.file("http://www.math.csi.cuny.edu/Statistics/R/simpleR/Simple_0.4.zip",path,mode="wb")
> install.packages(path,CRAN=NULL)
> 
> Is there more elegant way to do this in Windows?

Download the file and use the menu item to install it, if the above is too 
complicated for you.

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From Michael.Grottke at wiso.uni-erlangen.de  Fri Oct 24 12:27:50 2003
From: Michael.Grottke at wiso.uni-erlangen.de (Michael Grottke)
Date: Fri, 24 Oct 2003 12:27:50 +0200
Subject: [R] \mathcal symbols in R?
Message-ID: <3F98FEA6.2080103@wiso.uni-erlangen.de>

Hello,

Some time ago, I discovered the possibility of using mathematical 
symbols for axis labels etc. In order ensure consistency between text 
and graphics of some paper, I would like to include the calligraphic H 
(obtained in LaTeX via \mathcal{H}) in several diagrams. Is there any 
way to do so? Is it in general possible to use further mathematical 
fonts like \mathbb and \mathbf in R?

Best
   Michael

-- 

--------------------------------------------
Dr. Michael Grottke
University of Erlangen-Nuremberg
Chair of Statistics and Econometrics
Lange Gasse 20, D-90403 Nuernberg, Germany

Phone:  ++49-911-5302 276
Fax:    ++49-911-5302 277
E-mail: Michael.Grottke at wiso.uni-erlangen.de



From mailinglist2_wegmann at web.de  Fri Oct 24 13:12:56 2003
From: mailinglist2_wegmann at web.de (Martin Wegmann)
Date: Fri, 24 Oct 2003 13:12:56 +0200
Subject: [R] validation for GLM/GAM
Message-ID: <200310241312.56389.mailinglist2_wegmann@web.de>

Hello, 

I searched for a validation method for GLM/GAM inside R. 

So far I only found cv.glm() a k-fold cross-validation for GLM. 
But I wonder if there is something like ROC/AUC validation method for a GLM/
GAM model available?. Perhaps with bootsrapping afterwards to get confid. 
intervals, but not necessary.

any advice in which package I can find such a tool?

BTW I tried to install the ROC packages, but R prompts that this package is 
not available on cran. 

Thanks, Martin



From sp38 at st-andrews.ac.uk  Fri Oct 24 13:37:25 2003
From: sp38 at st-andrews.ac.uk (Simone Panigada)
Date: Fri, 24 Oct 2003 12:37:25 +0100
Subject: [R] questions please
Message-ID: <6.0.0.22.0.20031024123243.01b5b488@gatty.st-and.ac.uk>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20031024/e3753f13/attachment.pl

From krcabrer at unalmed.edu.co  Fri Oct 24 14:01:10 2003
From: krcabrer at unalmed.edu.co (Kenneth Cabrera)
Date: Fri, 24 Oct 2003 07:01:10 -0500
Subject: [R] geoR vector length error
In-Reply-To: <Pine.A41.4.44.0303250703480.94596-100000@homer36.u.washington.edu>
References: <Pine.A41.4.44.0303250703480.94596-100000@homer36.u.washington.edu>
Message-ID: <oprxjqb80kfaouaq@200.24.8.4>

Dear Dr Thomas

What should I do in this case?
How can I estimate the variogram using the full data set?

Thank you very much for your help.

Kenneth


> On Tue, 25 Mar 2003, Kris Nackaerts wrote:
>
>> Dear,
>>
>> we have a problem with geoR. We try to read an ASCII table (x,y,z) with 
>> 40000
>> lines. With read.geodata we get the error:
>>
>> Error in vector("double", length) : cannot allocate vector of length 
>> 799980000
>>
>> We can read the file without any problem with read.table, but trying to
>> convert it to the geodata class gets the same error.
>
> Looking at the source of as.geodata() it seems that it uses dist() to
> check for coincident points, and this will involve creating a vector of
> length approximately 8x10^8. You can't do that in R.
>
> 	-thomas
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help



--



From paulojus at est.ufpr.br  Fri Oct 24 14:11:06 2003
From: paulojus at est.ufpr.br (Paulo Justiniano Ribeiro Jr)
Date: Fri, 24 Oct 2003 10:11:06 -0200 (BRST)
Subject: [R] geoR vector length error
In-Reply-To: <oprxjqb80kfaouaq@200.24.8.4>
References: <Pine.A41.4.44.0303250703480.94596-100000@homer36.u.washington.edu>
	<oprxjqb80kfaouaq@200.24.8.4>
Message-ID: <Pine.LNX.4.56.0310241009550.29987@gauss.est.ufpr.br>

You can try to use geoR without using the geodata class.
Just read coords and data in separate objects and use the arguments coord
and data in  the variog function.

You may need more memory, anyway




On Fri, 24 Oct 2003, Kenneth Cabrera wrote:

> Dear Dr Thomas
>
> What should I do in this case?
> How can I estimate the variogram using the full data set?
>
> Thank you very much for your help.
>
> Kenneth
>
>
> > On Tue, 25 Mar 2003, Kris Nackaerts wrote:
> >
> >> Dear,
> >>
> >> we have a problem with geoR. We try to read an ASCII table (x,y,z) with
> >> 40000
> >> lines. With read.geodata we get the error:
> >>
> >> Error in vector("double", length) : cannot allocate vector of length
> >> 799980000
> >>
> >> We can read the file without any problem with read.table, but trying to
> >> convert it to the geodata class gets the same error.
> >
> > Looking at the source of as.geodata() it seems that it uses dist() to
> > check for coincident points, and this will involve creating a vector of
> > length approximately 8x10^8. You can't do that in R.
> >
> > 	-thomas
> >
> > ______________________________________________
> > R-help at stat.math.ethz.ch mailing list
> > https://www.stat.math.ethz.ch/mailman/listinfo/r-help
>
>
>
> --
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
>
>

Paulo Justiniano Ribeiro Jr
Departamento de Estat?stica
Universidade Federal do Paran?
Caixa Postal 19.081
CEP 81.531-990
Curitiba, PR  -  Brasil
Tel: (+55) 41 361 3471
Fax: (+55) 41 361 3141
e-mail: pj at est.ufpr.br
http://www.est.ufpr.br/~paulojus



From dmurdoch at pair.com  Fri Oct 24 14:12:03 2003
From: dmurdoch at pair.com (Duncan Murdoch)
Date: Fri, 24 Oct 2003 08:12:03 -0400
Subject: [R] questions please
In-Reply-To: <6.0.0.22.0.20031024123243.01b5b488@gatty.st-and.ac.uk>
References: <6.0.0.22.0.20031024123243.01b5b488@gatty.st-and.ac.uk>
Message-ID: <vb5ipv0m0vj07rdd4ldd1t9i082gpvlkv6@4ax.com>

On Fri, 24 Oct 2003 12:37:25 +0100, you wrote:

>Hi there,
>
>I am quite new to R and I would like to ask two questions:
>
>    *  is it possible to cancel the memory of the arrow up command?

Not within a session.  If you want to get rid of the history of
previous sessions, you should delete the .Rhistory file.  This file is
saved in the current directory when you shut down R; you can find out
what that is by using the "getwd()" function.  See "Invoking R" in the
Introduction to R manual for ways to control the starting directory.

Duncan Murdoch



From Friedrich.Leisch at ci.tuwien.ac.at  Fri Oct 24 14:17:34 2003
From: Friedrich.Leisch at ci.tuwien.ac.at (Friedrich.Leisch@ci.tuwien.ac.at)
Date: Fri, 24 Oct 2003 14:17:34 +0200
Subject: [R] CRAN down on Sunday, 26.10.2003
Message-ID: <16281.6238.405333.480843@galadriel.ci.tuwien.ac.at>


Due to construction work in a neighboring building there will be a
power outage in our offices for the complete day of

	Sunday, 26.10.2003

which means we will shut down all our computers some time tomorrow and
reboot them Monday morning (central European time).

This includes the server for

	www.R-project.org
	cran.R-project.org
	cran.at.R-project.org

Please a mirror site on this day (and do not flood us with "bug"
reports stating that CRAN is down :-).

Best,

-- 
-------------------------------------------------------------------
                        Friedrich Leisch 
Institut f?r Statistik                     Tel: (+43 1) 58801 10715
Technische Universit?t Wien                Fax: (+43 1) 58801 10798
Wiedner Hauptstra?e 8-10/1071      Friedrich.Leisch at ci.tuwien.ac.at
A-1040 Wien, Austria             http://www.ci.tuwien.ac.at/~leisch

_______________________________________________
R-announce at stat.math.ethz.ch mailing list
https://www.stat.math.ethz.ch/mailman/listinfo/r-announce



From jmacdon at med.umich.edu  Fri Oct 24 14:25:04 2003
From: jmacdon at med.umich.edu (James MacDonald)
Date: Fri, 24 Oct 2003 08:25:04 -0400
Subject: [R] repeating colors in graph 2
Message-ID: <sf98e1ec.033@med-gwia-02a.med.umich.edu>

Hmmm, I get the same results either way. Regardless of results, is there
a reason why my suggestion is 'invalid'?

TIA,

Jim



James W. MacDonald
Affymetrix and cDNA Microarray Core
University of Michigan Cancer Center
1500 E. Medical Center Drive
7410 CCGC
Ann Arbor MI 48109
734-647-5623

>>> Thomas Lumley <tlumley at u.washington.edu> 10/23/03 04:26PM >>>
On Thu, 23 Oct 2003, James MacDonald wrote:

> I believe you want plot(blah blah, col=palette(rainbow(13)))
>

No.  palette() sets the default palette, so
eg
> palette(rainbow(13))
> plot(1:13,col=1:13)

will plot in the new palette.

	-thomas



From gcendoya at balcarce.inta.gov.ar  Fri Oct 24 13:37:04 2003
From: gcendoya at balcarce.inta.gov.ar (CENDOYA, Gabriela)
Date: Fri, 24 Oct 2003 09:37:04 -0200
Subject: [R] List of lm objects
Message-ID: <001d01c39a23$263bec20$b54a6cc8@gcendoya.balcarce.inta.gov.ar>

Thank you!!!
all the answers were really useful!!! and taught me something different!!

Gabriela



From tlumley at u.washington.edu  Fri Oct 24 15:47:14 2003
From: tlumley at u.washington.edu (Thomas Lumley)
Date: Fri, 24 Oct 2003 06:47:14 -0700 (PDT)
Subject: [R] repeating colors in graph 2
In-Reply-To: <sf98e1ec.034@med-gwia-02a.med.umich.edu>
References: <sf98e1ec.034@med-gwia-02a.med.umich.edu>
Message-ID: <Pine.A41.4.58.0310240644180.35728@homer01.u.washington.edu>

On Fri, 24 Oct 2003, James MacDonald wrote:

> Hmmm, I get the same results either way. Regardless of results, is there
> a reason why my suggestion is 'invalid'?
>

Well, it does work, but palette() is designed to set or report the current
palette, so it makes more sense to do either
  palette(rainbow(13))
  plot(1:13, col=1:13)
or
  plot(1:13, col=rainbow(1:13))


	-thomas



From r.ghezzo at staff.mcgill.ca  Fri Oct 24 15:55:11 2003
From: r.ghezzo at staff.mcgill.ca (r.ghezzo)
Date: Fri, 24 Oct 2003 09:55:11 -0400
Subject: [R] problems with Affy
Message-ID: <3FFD600D@webmail.mcgill.ca>

I just downloaded Affy in R 1.8, Also downloaded 
Biobase,reposTools,tkWidgets,genefilter.
when called
> library(affy)
Error in bindingIsLocked(m2, where) : not an environment
Error in bindingIsLocked(m2, where) : not an environment
Error in getClass(Class, where = topenv(parent.frame())) :
        "MIAME" is not a defined class
Error in library(affy) : .First.lib failed
>
Is there any other package to install?
Thanks for any help
.
Heberto Ghezzo Ph.D.
Meakins-Christie Labs
McGill University
Montreal - Canada

R. Heberto Ghezzo Ph.D.
Meakins-Christie Labs
McGill University
Montreal - Que - Canada



From jmacdon at med.umich.edu  Fri Oct 24 16:00:32 2003
From: jmacdon at med.umich.edu (James MacDonald)
Date: Fri, 24 Oct 2003 10:00:32 -0400
Subject: [R] problems with Affy
Message-ID: <sf98f856.087@med-gwia-01a.med.umich.edu>

You need to use the devel versions of Bioconductor packages with
R-1.8.0

Jim



James W. MacDonald
Affymetrix and cDNA Microarray Core
University of Michigan Cancer Center
1500 E. Medical Center Drive
7410 CCGC
Ann Arbor MI 48109
734-647-5623

>>> "r.ghezzo" <r.ghezzo at staff.mcgill.ca> 10/24/03 09:55AM >>>
I just downloaded Affy in R 1.8, Also downloaded 
Biobase,reposTools,tkWidgets,genefilter.
when called
> library(affy)
Error in bindingIsLocked(m2, where) : not an environment
Error in bindingIsLocked(m2, where) : not an environment
Error in getClass(Class, where = topenv(parent.frame())) :
        "MIAME" is not a defined class
Error in library(affy) : .First.lib failed
>
Is there any other package to install?
Thanks for any help
.
Heberto Ghezzo Ph.D.
Meakins-Christie Labs
McGill University
Montreal - Canada

R. Heberto Ghezzo Ph.D.
Meakins-Christie Labs
McGill University
Montreal - Que - Canada

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://www.stat.math.ethz.ch/mailman/listinfo/r-help



From johannes.huesing at medizin.uni-essen.de  Fri Oct 24 16:14:25 2003
From: johannes.huesing at medizin.uni-essen.de (=?iso-8859-1?Q?=22H=FCsing=2C_Johannes=22?=)
Date: Fri, 24 Oct 2003 16:14:25 +0200
Subject: [R] problems with Affy
Message-ID: <B3A80C9C13928B45B2FCE4C43656363A0194E2BF@mail-srv02.master.medizin.uni-essen.de>

> I just downloaded Affy in R 1.8, Also downloaded 
> Biobase,reposTools,tkWidgets,genefilter.
> when called
> > library(affy)
> Error in bindingIsLocked(m2, where) : not an environment
> Error in bindingIsLocked(m2, where) : not an environment
> Error in getClass(Class, where = topenv(parent.frame())) :
>         "MIAME" is not a defined class
> Error in library(affy) : .First.lib failed
> >
> Is there any other package to install?
> Thanks for any help

Please refrain from posting your question to both r-help
and bioconductor. I tend to think it might be better 
suited for bioconductor.

Cheers 


Johannes



From zhuw at mail.smu.edu  Fri Oct 24 16:16:12 2003
From: zhuw at mail.smu.edu (zhu wang)
Date: 24 Oct 2003 09:16:12 -0500
Subject: [R] [summary] genetic algorithm
Message-ID: <1066958988.1882.35.camel@zwang.stat.smu.edu>

Thanks to Andy Liaw, Hisaji Ono,Alessandro Semeria, Patrick Burns, among
others.

There are three(?) packages on CRAN:

"gafit", "seao", "rgenoud". 

While "genopt" function from S Poetry also works. "rgenoud" and "genopt"
are for general optimization problems.

Cheers,

-- 
Zhu Wang

Statistical Science Department
Southern Methodist University
Phone: (214)768-2453
Fax: (214)768-4035
Email: zhuw at mail.smu.edu



From G.Caggiano at exeter.ac.uk  Fri Oct 24 16:18:02 2003
From: G.Caggiano at exeter.ac.uk (Giovanni Caggiano)
Date: Fri, 24 Oct 2003 15:18:02 +0100
Subject: [R] NLS estimation: Starting values
Message-ID: <5.0.2.1.0.20031024151217.00b24ec8@pop.exeter.ac.uk>

Dear All,

I am trying to fit to the data a nonlinear function, say z(tau;b) where tau 
is the independent variable and b is 3x1 vector of parameters. Following 
previous suggestions from the list, I am trying to find some "optimal" 
starting values for the three parameters by using the command 'optim' and 
then feed these values to nls for estimation. Does anybody know hot to use 
'optim' for this purpose?

Many thanks,
Giovanni



From rxg218 at psu.edu  Fri Oct 24 16:17:34 2003
From: rxg218 at psu.edu (Rajarshi Guha)
Date: 24 Oct 2003 10:17:34 -0400
Subject: [R] error in documentation
Message-ID: <1067005054.2579.1.camel@localhost.localdomain>

Hi,
  there appears to be an error in the documentation for fanny. In the
Details section, the equation representing the objective function has no
variable k in it, but the  two lines following it mention:

where n is the number of observations, k is the number of clusters, and
d(i,j) is the dissimilarity between i and j

-------------------------------------------------------------------
Rajarshi Guha <rxg218 at psu.edu> <http://jijo.cjb.net>
GPG Fingerprint: 0CCA 8EE2 2EEB 25E2 AB04 06F7 1BB9 E634 9B87 56EE
-------------------------------------------------------------------
After a number of decimal places, nobody gives a damn.



From ripley at stats.ox.ac.uk  Fri Oct 24 16:42:44 2003
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Fri, 24 Oct 2003 15:42:44 +0100 (BST)
Subject: [R] NLS estimation: Starting values
In-Reply-To: <5.0.2.1.0.20031024151217.00b24ec8@pop.exeter.ac.uk>
Message-ID: <Pine.LNX.4.44.0310241541560.1543-100000@gannet.stats>

optim() needs starting values too, but *may* be more robust than nls to 
their specification ....

On Fri, 24 Oct 2003, Giovanni Caggiano wrote:

> I am trying to fit to the data a nonlinear function, say z(tau;b) where tau 
> is the independent variable and b is 3x1 vector of parameters. Following 
> previous suggestions from the list, I am trying to find some "optimal" 
> starting values for the three parameters by using the command 'optim' and 
> then feed these values to nls for estimation. Does anybody know hot to use 
> 'optim' for this purpose?

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From Thomas.Bock at ptb.de  Fri Oct 24 13:22:31 2003
From: Thomas.Bock at ptb.de (Thomas Bock)
Date: Fri, 24 Oct 2003 13:22:31 +0200
Subject: [R] first value from nlm (non-finite value supplied by nlm)
Message-ID: <3F990B77.9090601@ptb.de>

Dear expeRts,

first of all I'd like to thank you for the
quick help on my last which() problem.

Here is another one I could not tackle:
I have data on an absorption measurement which I want to fit
with an voigt profile:

  fn.1 <- function(p){
    for (i1 in ilong){
      ff <- f[i1]
      ex[i1] <- exp(S*n*L*voigt(u,v,ff,p[1],p[2],p[3])[[1]])
    }
sum((t-ex)^2)
  }
out <- nlm(fn.1, p = c(fo, GG, GL), hessian = TRUE, steptol = 1e-5, 
iterlim = 1000)
foN <-  out$estimate[1]
GGN <-  out$estimate[2]
GLN <-  out$estimate[3]

This works fine but the my start value of S is to poor,
so I like to fit S in a second run, with the initial values from the 
first run
(two runs because I know that S as an parameter is an problem):

  fn.2 <- function(p){
    for (i1 in ilong){
      ex[i1] <- exp(p[1]*n*L*voigt(u,v,f[i1],p[2],p[3],GLN)[[1]])
          }
 sum((t-ex)^2)
}
out <-nlm(fn.2, p = c(S,foN,GGN), hessian = TRUE,
           steptol = 1e-5, iterlim = 1000,print.level=2)
SN <-  out$estimate[1]

The problem is now that the first value from nlm() is positive (1E-6 !?)
and this leeds to an Inf:

  iteration = 0
Step:
[1] 0 0 0
Parameter:
[1] -3.800000e-19  2.196660e+03  5.211179e-03
Function Value
[1] 0.5890603
Gradient:
[1]       Inf  11.23381 -23.61961 -
 
Error in nlm(fn.2, p = c(S, foN, GGN), hessian = TRUE, steptol = 1e-5,  :
    non-finite value supplied by nlm
In addition: Warning message:
NA/Inf replaced by maximum positive value

The number of parameters plays no role; same behaviour with p = c(S,GGN)

Can someone give a broad hint
             Thomas



From spencer.graves at pdf.com  Fri Oct 24 17:32:59 2003
From: spencer.graves at pdf.com (Spencer Graves)
Date: Fri, 24 Oct 2003 08:32:59 -0700
Subject: [R] first value from nlm (non-finite value supplied by nlm)
In-Reply-To: <3F990B77.9090601@ptb.de>
References: <3F990B77.9090601@ptb.de>
Message-ID: <3F99462B.8050100@pdf.com>

Have you tried fn.2(out$estimate), without the call to "nlm"?  There may 
be a problem with the definition of "fn.2", but I don't have time to 
check that for you, other than to note the "t" is the matrix transpose 
function.  In many but not all contexts, R is able to distinguish t as a 
function from t as a non-function.  However, I consider that skating on 
thin ice. 

hope this helps.  spencer graves

Thomas Bock wrote:

> Dear expeRts,
>
> first of all I'd like to thank you for the
> quick help on my last which() problem.
>
> Here is another one I could not tackle:
> I have data on an absorption measurement which I want to fit
> with an voigt profile:
>
>  fn.1 <- function(p){
>    for (i1 in ilong){
>      ff <- f[i1]
>      ex[i1] <- exp(S*n*L*voigt(u,v,ff,p[1],p[2],p[3])[[1]])
>    }
> sum((t-ex)^2)
>  }
> out <- nlm(fn.1, p = c(fo, GG, GL), hessian = TRUE, steptol = 1e-5, 
> iterlim = 1000)
> foN <-  out$estimate[1]
> GGN <-  out$estimate[2]
> GLN <-  out$estimate[3]
>
> This works fine but the my start value of S is to poor,
> so I like to fit S in a second run, with the initial values from the 
> first run
> (two runs because I know that S as an parameter is an problem):
>
>  fn.2 <- function(p){
>    for (i1 in ilong){
>      ex[i1] <- exp(p[1]*n*L*voigt(u,v,f[i1],p[2],p[3],GLN)[[1]])
>          }
> sum((t-ex)^2)
> }
> out <-nlm(fn.2, p = c(S,foN,GGN), hessian = TRUE,
>           steptol = 1e-5, iterlim = 1000,print.level=2)
> SN <-  out$estimate[1]
>
> The problem is now that the first value from nlm() is positive (1E-6 !?)
> and this leeds to an Inf:
>
>  iteration = 0
> Step:
> [1] 0 0 0
> Parameter:
> [1] -3.800000e-19  2.196660e+03  5.211179e-03
> Function Value
> [1] 0.5890603
> Gradient:
> [1]       Inf  11.23381 -23.61961 -
>
> Error in nlm(fn.2, p = c(S, foN, GGN), hessian = TRUE, steptol = 1e-5,  :
>    non-finite value supplied by nlm
> In addition: Warning message:
> NA/Inf replaced by maximum positive value
>
> The number of parameters plays no role; same behaviour with p = c(S,GGN)
>
> Can someone give a broad hint
>             Thomas
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help



From tplate at blackmesacapital.com  Fri Oct 24 17:42:55 2003
From: tplate at blackmesacapital.com (Tony Plate)
Date: Fri, 24 Oct 2003 09:42:55 -0600
Subject: [R] what's going on here with substitute() ?
Message-ID: <5.2.1.1.2.20031024094142.0409c200@mailhost.blackmesacapital.com>

[resent the next day, after not appearing on the list]

Peter, thank you for the explanation.  This is indeed what is happening.

Might I suggest the following passage for inclusion in the help page for 
"function", and possibly also "body", in the DETAILS or WARNING section:

"Note that the text of the original function definition is saved as an 
attribute "source" on the function, and this is printed out when the 
function is printed.  Hence, if the function body is changed in some way 
other than by assigning a value via body() (which removes the "source" 
attribute), the printed form of the function may not be the same as the 
actual function body."

Something along these lines could also go in the help for "eval", though if 
it were only there it might be very difficult to find if one were trying to 
look up puzzling behavior of a function.

Here is a transcript that shows what is happening, with another suggestion 
following it.

 > eval(substitute(this.is.R <- function() X, 
list(X=!is.null(options("CRAN")[[1]]))))
 > this.is.R
function() X
 > body(this.is.R)
[1] TRUE
 > attributes(this.is.R)
$source
[1] "function() X"
 > attributes(this.is.R) <- NULL
 > this.is.R
function ()
TRUE
 > # the "source" attribute comes from function definition:
 > attributes(function() X)
$source
[1] "function() X"
 > # and seems to be added by "eval":
 > attr(eval(parse(text="function() TRUE")[[1]]), "source")
[1] "function() TRUE"
 >

 > # we can assign bogus "source"
 > attr(this.is.R, "source") <- "a totally bogus function body"
 > this.is.R
a totally bogus function body
 > # assigning to body() removes "source"
 > body(this.is.R) <- list(666)
 > this.is.R
function ()
666
 > attr(this.is.R, "source")
NULL
 >

An even better approach might be something that gave a warning on printing 
if the parsed "source" attribute was not identical to the language object 
being printed.  This would probably belong in the code for "case LANGSXP:" 
in the function PrintValueRec in main/print.c (if it were written in R, I 
could contribute a patch, but right now I don't have time to try to 
understand the C there.)  R code to do the test could be something like this:

 > f <- this.is.R
 > identical(f, eval(parse(text=attr(f, "source"))[[1]]))
[1] FALSE
 > f <- function() TRUE
 > identical(f, eval(parse(text=attr(f, "source"))[[1]]))
[1] TRUE
 >

-- Tony Plate



At Thursday 09:12 PM 10/23/2003 +0200, Peter Dalgaard wrote:
>Tony Plate <tplate at blackmesacapital.com> writes:
>
> > Why is the body of the function "X" when I substituted a different
> > expression for X? Also, given that the body of the function is X, how
> > does the function evaluate to TRUE since X is not defined anywhere
> > (except in a list that should have been discarded.)
> >
> > This happens with both R 1.7.1 and R 1.8.0 (under Windows 2000).
> >
> > (yes, I did discover the function is.R(), but I still want to discover
> > what's going here.)
>
>I think you are defeating the keep.source mechanism. What you're
>seeing is not the actual function but its source attribute. Try
>attributes(this.is.R) <- NULL and see what happens.
>
>--
>    O__  ---- Peter Dalgaard             Blegdamsvej 3
>   c/ /'_ --- Dept. of Biostatistics     2200 Cph. N
>  (*) \(*) -- University of Copenhagen   Denmark      Ph: (+45) 35327918
>~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)             FAX: (+45) 35327907



From mrufino at icm.csic.es  Fri Oct 24 18:15:02 2003
From: mrufino at icm.csic.es (Marta Rufino)
Date: Fri, 24 Oct 2003 18:15:02 +0200
Subject: [R] MANOVA with random factor
Message-ID: <5.2.1.1.1.20031024181146.00c26988@cucafera.icm.csic.es>

Dear list members,

I would like to know how do I define a MANOVA with a random factor (block) 
and two fixed factors (repeated measures designs)?
What about if we want to see if a matrix is significantly different from 
zero? is there any nonparametric procedure available in R?


thank you very much
Marta



From mendola at dssm.unipa.it  Fri Oct 24 18:22:05 2003
From: mendola at dssm.unipa.it (mendola@dssm.unipa.it)
Date: Fri, 24 Oct 2003 18:22:05 +0200 (CEST)
Subject: [R] gee and geepack: different results?
Message-ID: <1186.147.163.47.99.1067012525.squirrel@dssm.unipa.it>

Hi, I downloaded both gee and geepack, and I am trying to understand the
differences between the two libraries.
I used the same data and estimated the same model, with a correlation
structure autoregressive of order 1. Surprisingly for me, I found very
different results. Coefficients are slightly different in value but
sometimes opposite in sign.
Moreover, the estimate of rho (correlation coefficient) given by gee is
0.5759 (see element 1.2 in the working correlation matrix) while the
estimate given by geese is 0.4519.
Could somebody explain me what happened?


Data are in  dati22, which is only a part of the data for the study I
intend to perform. Here what I did ,using first gee and then geese:

> str(dati22)
`data.frame':   47 obs. of  7 variables:
 $ TR    : Factor w/ 4 levels "7","8","9","11": 1 1 1 1 1 1 1 1 1 1 ...
 $ STAT  : Factor w/ 3 levels "1","2","3": 1 1 1 1 2 2 2 2 3 3 ...
 $ PIANTA: int  2 2 2 2 2 2 2 2 41 41 ...
 $ ANLEP : int  1999 1998 1999 1998 1999 1998 1999 1998 1999 1998 ...
 $ eta   : int  12 11 10 9 11 10 11 10 14 13 ...
 $ VCRE  : num  5.3 6.9 11 9.9 7.9 9.2 14.2 11.9 10.5 10 ...
 $ temp  : num  19.7 20.0 19.7 20.0 19.7 ...


> mio1.2<- gee(VCRE ~ TR + STAT + temp + eta, id = PIANTA, +
 + data = dati22,  family = Gamma, corstr = "AR-M", Mv = 1)

> summary(mio1.2)

 GEE:  GENERALIZED LINEAR MODELS FOR DEPENDENT DATA
 gee S-function, version 4.13 modified 98/01/27 (1998)

Model:
 Link:                      Reciprocal
 Variance to Mean Relation: Gamma
 Correlation Structure:     AR-M , M = 1

Call:
gee(formula = VCRE ~ TR + STAT + temp + eta, id = PIANTA, data = dati22,
    family = Gamma, corstr = "AR-M", Mv = 1)

Summary of Residuals:
       Min         1Q     Median         3Q        Max
-4.5154238 -2.0766622 -0.2153521  0.9418182  6.3871421


Coefficients:
                Estimate  Naive S.E.     Naive z Robust S.E.    Robust z
(Intercept)  0.411995251 0.174052364  2.36707644 0.131632816  3.12988253
TR8         -0.001154422 0.021191593 -0.05447549 0.011807163 -0.09777305
TR9          0.019559907 0.024379471  0.80231056 0.008993803  2.17482050
TR11        -0.041092894 0.021609580 -1.90160537 0.015384050 -2.67113620
STAT2        0.023886745 0.014219390  1.67987130 0.013543550  1.76369899
STAT3        0.045749728 0.016844262  2.71604237 0.012862504  3.55682917
temp        -0.020141682 0.008819798 -2.28368975 0.006851784 -2.93962600
eta          0.008021081 0.001465212  5.47434944 0.001129986  7.09838725

Estimated Scale Parameter:  0.1049601
Number of Iterations:  13

Working Correlation
       [,1]   [,2]   [,3]   [,4]   [,5]   [,6]   [,7]   [,8]
[1,] 1.0000 0.5758 0.3315 0.1909 0.1099 0.0633 0.0364 0.0210
[2,] 0.5758 1.0000 0.5758 0.3315 0.1909 0.1099 0.0633 0.0364
[3,] 0.3315 0.5758 1.0000 0.5758 0.3315 0.1909 0.1099 0.0633
[4,] 0.1909 0.3315 0.5758 1.0000 0.5758 0.3315 0.1909 0.1099
[5,] 0.1099 0.1909 0.3315 0.5758 1.0000 0.5758 0.3315 0.1909
[6,] 0.0633 0.1099 0.1909 0.3315 0.5758 1.0000 0.5758 0.3315
[7,] 0.0364 0.0633 0.1099 0.1909 0.3315 0.5758 1.0000 0.5758
[8,] 0.0210 0.0364 0.0633 0.1099 0.1909 0.3315 0.5758 1.0000

USING geese:

> mio1.2pack<-geese(VCRE ~ TR + STAT + temp + eta, id = PIANTA, data =
dati22, +
    + family = Gamma, corstr = "ar1")


Coefficients:
            estimate  san.se   wald        p
(Intercept)  0.44799 0.18279  6.007 1.43e-02
TR8          0.00444 0.00985  0.203 6.52e-01
TR9          0.02182 0.00632 11.923 5.54e-04
TR11        -0.03728 0.01407  7.024 8.04e-03
STAT2        0.02116 0.01476  2.056 1.52e-01
STAT3        0.04270 0.01245 11.758 6.06e-04
temp        -0.02233 0.00973  5.273 2.17e-02
eta          0.00865 0.00136 40.683 1.79e-10

Scale Model:
 Scale Link:                identity

 Estimated Scale Parameters:
            estimate san.se wald       p
(Intercept)    0.081 0.0287 7.98 0.00474

Correlation Model:
 Correlation Structure:     ar1
 Correlation Link:          identity

 Estimated Correlation Parameters:
      estimate san.se wald      p
alpha    0.452  0.272 2.77 0.0963

Returned Error Value:    0
Number of clusters:   6   Maximum cluster size: 8


Thanks, Daria


****************************

Daria Mendola
Department of Statistics and Matematics
"Silvio Vianelli"
University di Palermo
Viale delle Scienze - Edificio 13
90128 Palermo, Italy
phone  +39 091 6626210
fax  +39 091 485726
email: mendola at dssm.unipa.it



From flom at ndri.org  Fri Oct 24 18:40:00 2003
From: flom at ndri.org (Peter Flom)
Date: Fri, 24 Oct 2003 12:40:00 -0400
Subject: [R] predict for a model with a subset
Message-ID: <sf991dc6.075@MAIL.NDRI.ORG>

Hello

running R 1.7.1 on Windows 2000

I have a model

notmar1 <- glm(yprisx~age+harddrug+sex, subset = marcom == 0,
 family = quasipoisson)

and summary(notmar1) gives (as it should) 433 df for the null model

but when I run
predict(notmar1 <- glm(yprisx~age+harddrug+sex, subset = marcom == 0, 
family = quasipoisson))

I get preditions for 528 people (the full data set, not the subset)

How do I get predict to work on just the subset of people for whom the
model is estimated?

Thanks

Peter



From spencer.graves at pdf.com  Fri Oct 24 18:56:29 2003
From: spencer.graves at pdf.com (Spencer Graves)
Date: Fri, 24 Oct 2003 09:56:29 -0700
Subject: [R] predict for a model with a subset
In-Reply-To: <sf991dc6.075@MAIL.NDRI.ORG>
References: <sf991dc6.075@MAIL.NDRI.ORG>
Message-ID: <3F9959BD.7050004@pdf.com>

Have you tried the following: 

notmar1 <- glm(yprisx~age+harddrug+sex, subset = marcom == 0,
 family = quasipoisson), data=DF)

predict(notmar1, newdata=DF[DF$marcom==0,])

      I haven't tested this specific code, but I've gotten sensible 
results from constructs like this in the past. 

hope this helps.  spencer graves

Peter Flom wrote:

>Hello
>
>running R 1.7.1 on Windows 2000
>
>I have a model
>
>notmar1 <- glm(yprisx~age+harddrug+sex, subset = marcom == 0,
> family = quasipoisson)
>
>and summary(notmar1) gives (as it should) 433 df for the null model
>
>but when I run
>predict(notmar1 <- glm(yprisx~age+harddrug+sex, subset = marcom == 0, 
>family = quasipoisson))
>
>I get preditions for 528 people (the full data set, not the subset)
>
>How do I get predict to work on just the subset of people for whom the
>model is estimated?
>
>Thanks
>
>Peter
>
>______________________________________________
>R-help at stat.math.ethz.ch mailing list
>https://www.stat.math.ethz.ch/mailman/listinfo/r-help
>  
>



From gu4 at llnl.gov  Fri Oct 24 19:03:23 2003
From: gu4 at llnl.gov (Pauline Gu)
Date: Fri, 24 Oct 2003 10:03:23 -0700
Subject: [R] neural networks
In-Reply-To: <5.2.1.1.1.20031024181146.00c26988@cucafera.icm.csic.es>
Message-ID: <5.2.1.1.2.20031024093551.03f6e3d8@poptop.llnl.gov>

Hello, experts,

Does the number of input for training set need to be the same as the number 
of input for the data set that I am trying to predict from the nnet result 
of the training?

Thanks.

Pauline



From ripley at stats.ox.ac.uk  Fri Oct 24 19:18:51 2003
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Fri, 24 Oct 2003 18:18:51 +0100 (BST)
Subject: [R] predict for a model with a subset
In-Reply-To: <sf991dc6.075@MAIL.NDRI.ORG>
Message-ID: <Pine.LNX.4.44.0310241814030.4820-100000@gannet.stats>

example(predict.glm)
fit <- glm(SF ~ ldose, family=binomial, subset=sex=="M")
predict(fit)
[1] -2.8185550 -1.5596055 -0.3006561  0.9582933  2.2172427  3.4761922

which is just the males.

So, that example works correctly, and predict.glm as called by you should 
just return the fitted values, as above.

On Fri, 24 Oct 2003, Peter Flom wrote:

> running R 1.7.1 on Windows 2000
> 
> I have a model
> 
> notmar1 <- glm(yprisx~age+harddrug+sex, subset = marcom == 0,
>  family = quasipoisson)
> 
> and summary(notmar1) gives (as it should) 433 df for the null model
> 
> but when I run
> predict(notmar1 <- glm(yprisx~age+harddrug+sex, subset = marcom == 0, 
> family = quasipoisson))
> 
> I get preditions for 528 people (the full data set, not the subset)
> 
> How do I get predict to work on just the subset of people for whom the
> model is estimated?

Perhaps you can show us how you managed to break it?

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From ryszard.czerminski at pharma.novartis.com  Fri Oct 24 19:45:32 2003
From: ryszard.czerminski at pharma.novartis.com (ryszard.czerminski@pharma.novartis.com)
Date: Fri, 24 Oct 2003 13:45:32 -0400
Subject: [R] how to remove NaN columns ?
Message-ID: <OF5995D46A.D8D677C1-ON85256DC9.0060B9BF-85256DC9.0061A1AB@EU.novartis.net>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20031024/996b7279/attachment.pl

From andy_liaw at merck.com  Fri Oct 24 19:55:13 2003
From: andy_liaw at merck.com (Liaw, Andy)
Date: Fri, 24 Oct 2003 13:55:13 -0400
Subject: [R] how to remove NaN columns ?
Message-ID: <3A822319EB35174CA3714066D590DCD50205CD38@usrymx25.merck.com>

As an example:

> dat <- data.frame(x=1:5, y=NaN, z=5:1)
> dat
  x   y z
1 1 NaN 5
2 2 NaN 4
3 3 NaN 3
4 4 NaN 2
5 5 NaN 1
> bad <- sapply(dat, function(x) all(is.nan(x)))
> dat[,!bad]
  x z
1 1 5
2 2 4
3 3 3
4 4 2
5 5 1

HTH,
Andy

> -----Original Message-----
> From: ryszard.czerminski at pharma.novartis.com 
> [mailto:ryszard.czerminski at pharma.novartis.com] 
> Sent: Friday, October 24, 2003 1:46 PM
> To: r-help at stat.math.ethz.ch
> Subject: [R] how to remove NaN columns ?
> 
> 
> How can I remove columns with NaN entries ?
> 
> Here is my simple example:
> 
> > data <- read.csv("test.csv")
> > xdata <- data[3:length(data)]
> > xs <- lapply(xdata, function(x){(x - mean(x))/sqrt(var(x))}) x <- 
> > data.frame(xs) x
>                    C     D               E                F
> 1 -0.7071068 NaN -0.7071068 -0.7071068
> 2  0.7071068 NaN  0.7071068  0.7071068
> 
> I am sure it is possible to remove column D (with NaN's) in 
> some simple 
> fashion, using is.nan function
> without explicitly looping through, and I am sure I was able 
> to do it in 
> the past, but I cannot recall how.
> 
> Your help will be greatly appreciated.
> 
> Ryszard
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list 
> https://www.stat.math.ethz.ch/mailman/listinfo> /r-help
>



From flom at ndri.org  Fri Oct 24 20:03:39 2003
From: flom at ndri.org (Peter Flom)
Date: Fri, 24 Oct 2003 14:03:39 -0400
Subject: [R] Ooops, apologies, it was my error re predict with subset
Message-ID: <sf993148.097@MAIL.NDRI.ORG>

Sorry

The problem was with my eyes, not with R, predict works as it is
supposed to on a subset, I was misled by the labels, which looked like
they were giving the wrong thing.

Sorry again

Peter



From Joerg.Schaber at uv.es  Fri Oct 24 20:23:21 2003
From: Joerg.Schaber at uv.es (Joerg Schaber)
Date: Fri, 24 Oct 2003 20:23:21 +0200
Subject: [R] Oracle fetch problems
References: <200310171014.h9HA6HqZ018483@stat.math.ethz.ch>
Message-ID: <3F996E19.60306@uv.es>

Hi,

I use the ROracle interface in the R 64bit version.
When I fetch a large number of rows from the database I get alternating 
250 rows of what I want to see and than 250 rows junk and so on.
It does not matter whether I use the fetch command or dbGetquery or 
whatever. My fetch_default_rec = 500 and I can not change it by setting 
it explicitly in Oracle().
Any idea what's the problem here?

Thanks, joerg



From tlumley at u.washington.edu  Fri Oct 24 20:31:37 2003
From: tlumley at u.washington.edu (Thomas Lumley)
Date: Fri, 24 Oct 2003 11:31:37 -0700 (PDT)
Subject: [R] gee and geepack: different results?
In-Reply-To: <1186.147.163.47.99.1067012525.squirrel@dssm.unipa.it>
References: <1186.147.163.47.99.1067012525.squirrel@dssm.unipa.it>
Message-ID: <Pine.A41.4.58.0310241125100.6470@homer36.u.washington.edu>

On Fri, 24 Oct 2003 mendola at dssm.unipa.it wrote:

> Hi, I downloaded both gee and geepack, and I am trying to understand the
> differences between the two libraries.
> I used the same data and estimated the same model, with a correlation
> structure autoregressive of order 1. Surprisingly for me, I found very
> different results. Coefficients are slightly different in value but
> sometimes opposite in sign.
> Moreover, the estimate of rho (correlation coefficient) given by gee is
> 0.5759 (see element 1.2 in the working correlation matrix) while the
> estimate given by geese is 0.4519.
> Could somebody explain me what happened?

These results don't look all that different. The one that changed sign is
pretty close to zero.

The two packages are using different estimators for the correlation
parameter, and therefore different weights for the observations. This is a
widespread issue with GEE.

In reasonably large samples, if the AR-1 model is a good fit then the
different correlation estimators will give similar values, so the weights
will be similar and the regression coefficients will be similar.
Alternatively, if the mean model is a good fit then the coefficients will
be similar regardless of correlation model.  With a sample of size 6,
however, GEE really isn't advisable.  I'd probably try a linear mixed
model for log(VCRE) unless you have strong reasons for preferring to model
the reciprocal of the mean.

Note in particular that the standard errors and p-values can be quite
badly wrong with such a small sample size in GEE.

	-thomas

>
>
> Data are in  dati22, which is only a part of the data for the study I
> intend to perform. Here what I did ,using first gee and then geese:
>
> > str(dati22)
> `data.frame':   47 obs. of  7 variables:
>  $ TR    : Factor w/ 4 levels "7","8","9","11": 1 1 1 1 1 1 1 1 1 1 ...
>  $ STAT  : Factor w/ 3 levels "1","2","3": 1 1 1 1 2 2 2 2 3 3 ...
>  $ PIANTA: int  2 2 2 2 2 2 2 2 41 41 ...
>  $ ANLEP : int  1999 1998 1999 1998 1999 1998 1999 1998 1999 1998 ...
>  $ eta   : int  12 11 10 9 11 10 11 10 14 13 ...
>  $ VCRE  : num  5.3 6.9 11 9.9 7.9 9.2 14.2 11.9 10.5 10 ...
>  $ temp  : num  19.7 20.0 19.7 20.0 19.7 ...
>
>
> > mio1.2<- gee(VCRE ~ TR + STAT + temp + eta, id = PIANTA, +
>  + data = dati22,  family = Gamma, corstr = "AR-M", Mv = 1)
>
> > summary(mio1.2)
>
>  GEE:  GENERALIZED LINEAR MODELS FOR DEPENDENT DATA
>  gee S-function, version 4.13 modified 98/01/27 (1998)
>
> Model:
>  Link:                      Reciprocal
>  Variance to Mean Relation: Gamma
>  Correlation Structure:     AR-M , M = 1
>
> Call:
> gee(formula = VCRE ~ TR + STAT + temp + eta, id = PIANTA, data = dati22,
>     family = Gamma, corstr = "AR-M", Mv = 1)
>
> Summary of Residuals:
>        Min         1Q     Median         3Q        Max
> -4.5154238 -2.0766622 -0.2153521  0.9418182  6.3871421
>
>
> Coefficients:
>                 Estimate  Naive S.E.     Naive z Robust S.E.    Robust z
> (Intercept)  0.411995251 0.174052364  2.36707644 0.131632816  3.12988253
> TR8         -0.001154422 0.021191593 -0.05447549 0.011807163 -0.09777305
> TR9          0.019559907 0.024379471  0.80231056 0.008993803  2.17482050
> TR11        -0.041092894 0.021609580 -1.90160537 0.015384050 -2.67113620
> STAT2        0.023886745 0.014219390  1.67987130 0.013543550  1.76369899
> STAT3        0.045749728 0.016844262  2.71604237 0.012862504  3.55682917
> temp        -0.020141682 0.008819798 -2.28368975 0.006851784 -2.93962600
> eta          0.008021081 0.001465212  5.47434944 0.001129986  7.09838725
>
> Estimated Scale Parameter:  0.1049601
> Number of Iterations:  13
>
> Working Correlation
>        [,1]   [,2]   [,3]   [,4]   [,5]   [,6]   [,7]   [,8]
> [1,] 1.0000 0.5758 0.3315 0.1909 0.1099 0.0633 0.0364 0.0210
> [2,] 0.5758 1.0000 0.5758 0.3315 0.1909 0.1099 0.0633 0.0364
> [3,] 0.3315 0.5758 1.0000 0.5758 0.3315 0.1909 0.1099 0.0633
> [4,] 0.1909 0.3315 0.5758 1.0000 0.5758 0.3315 0.1909 0.1099
> [5,] 0.1099 0.1909 0.3315 0.5758 1.0000 0.5758 0.3315 0.1909
> [6,] 0.0633 0.1099 0.1909 0.3315 0.5758 1.0000 0.5758 0.3315
> [7,] 0.0364 0.0633 0.1099 0.1909 0.3315 0.5758 1.0000 0.5758
> [8,] 0.0210 0.0364 0.0633 0.1099 0.1909 0.3315 0.5758 1.0000
>
> USING geese:
>
> > mio1.2pack<-geese(VCRE ~ TR + STAT + temp + eta, id = PIANTA, data =
> dati22, +
>     + family = Gamma, corstr = "ar1")
>
>
> Coefficients:
>             estimate  san.se   wald        p
> (Intercept)  0.44799 0.18279  6.007 1.43e-02
> TR8          0.00444 0.00985  0.203 6.52e-01
> TR9          0.02182 0.00632 11.923 5.54e-04
> TR11        -0.03728 0.01407  7.024 8.04e-03
> STAT2        0.02116 0.01476  2.056 1.52e-01
> STAT3        0.04270 0.01245 11.758 6.06e-04
> temp        -0.02233 0.00973  5.273 2.17e-02
> eta          0.00865 0.00136 40.683 1.79e-10
>
> Scale Model:
>  Scale Link:                identity
>
>  Estimated Scale Parameters:
>             estimate san.se wald       p
> (Intercept)    0.081 0.0287 7.98 0.00474
>
> Correlation Model:
>  Correlation Structure:     ar1
>  Correlation Link:          identity
>
>  Estimated Correlation Parameters:
>       estimate san.se wald      p
> alpha    0.452  0.272 2.77 0.0963
>
> Returned Error Value:    0
> Number of clusters:   6   Maximum cluster size: 8
>
>
> Thanks, Daria
>
>
> ****************************
>
> Daria Mendola
> Department of Statistics and Matematics
> "Silvio Vianelli"
> University di Palermo
> Viale delle Scienze - Edificio 13
> 90128 Palermo, Italy
> phone  +39 091 6626210
> fax  +39 091 485726
> email: mendola at dssm.unipa.it
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
>

Thomas Lumley			Assoc. Professor, Biostatistics
tlumley at u.washington.edu	University of Washington, Seattle



From GPetris at uark.edu  Fri Oct 24 20:50:53 2003
From: GPetris at uark.edu (Giovanni Petris)
Date: Fri, 24 Oct 2003 13:50:53 -0500 (CDT)
Subject: [R] how to remove NaN columns ?
In-Reply-To: <OF5995D46A.D8D677C1-ON85256DC9.0060B9BF-85256DC9.0061A1AB@EU.novartis.net>
	(ryszard.czerminski@pharma.novartis.com)
References: <OF5995D46A.D8D677C1-ON85256DC9.0060B9BF-85256DC9.0061A1AB@EU.novartis.net>
Message-ID: <200310241850.h9OIorHU020122@definetti.uark.edu>


> > xs <- lapply(xdata, function(x){(x - mean(x))/sqrt(var(x))})

Incidentally, the function 'scale' does just that.

-- 

 __________________________________________________
[                                                  ]
[ Giovanni Petris                 GPetris at uark.edu ]
[ Department of Mathematical Sciences              ]
[ University of Arkansas - Fayetteville, AR 72701  ]
[ Ph: (479) 575-6324, 575-8630 (fax)               ]
[ http://definetti.uark.edu/~gpetris/              ]
[__________________________________________________]



From jasont at indigoindustrial.co.nz  Fri Oct 24 20:56:41 2003
From: jasont at indigoindustrial.co.nz (Jason Turner)
Date: Sat, 25 Oct 2003 07:56:41 +1300
Subject: [R] how to remove NaN columns ?
In-Reply-To: <OF5995D46A.D8D677C1-ON85256DC9.0060B9BF-85256DC9.0061A1AB@EU.novartis.net>
References: <OF5995D46A.D8D677C1-ON85256DC9.0060B9BF-85256DC9.0061A1AB@EU.novartis.net>
Message-ID: <3F9975E9.2090804@indigoindustrial.co.nz>

ryszard.czerminski at pharma.novartis.com wrote:
> How can I remove columns with NaN entries ?
> 
> Here is my simple example:
> 
> 
>>data <- read.csv("test.csv")
>>xdata <- data[3:length(data)]
>>xs <- lapply(xdata, function(x){(x - mean(x))/sqrt(var(x))})
>>x <- data.frame(xs)
>>x
> 
>                    C     D               E                F
> 1 -0.7071068 NaN -0.7071068 -0.7071068
> 2  0.7071068 NaN  0.7071068  0.7071068
> 
> I am sure it is possible to remove column D (with NaN's) in some simple 
> fashion, using is.nan function
> without explicitly looping through, and I am sure I was able to do it in 
> the past, but I cannot recall how.
> 

In addition to Andy's helpful suggestion, if your data is a matrix 
rather than a data.frame, you can use which() with arr.ind=TRUE.  For 
this example, Andy's suggestion is cleaner, however.

 > foo <- as.matrix(foo)
 > foo
            C   D          E          F
1 -0.7071068 NaN -0.7071068 -0.7071068
2  0.7071068 NaN  0.7071068  0.7071068
 > which(is.nan(foo))
[1] 3 4
 > which(is.nan(foo),arr.ind=TRUE)
   row col
1   1   2
2   2   2
 > unique(which(is.nan(foo),arr.ind=TRUE)[,2])
[1] 2
 >

-- 
Indigo Industrial Controls Ltd.
http://www.indigoindustrial.co.nz
64-21-343-545
jasont at indigoindustrial.co.nz



From Joerg.Schaber at uv.es  Fri Oct 24 20:56:45 2003
From: Joerg.Schaber at uv.es (Joerg Schaber)
Date: Fri, 24 Oct 2003 20:56:45 +0200
Subject: [R] Oracle fetch problems
References: <200310241013.h9OA3jqa028767@stat.math.ethz.ch>
Message-ID: <3F9975ED.5000800@uv.es>

Concerning my former message: I just found out that irrespective of the 
fetch size, I get always half of the fetch size good results/rows and 
the other half is corrupted. Strange, isn't?



From ryszard.czerminski at pharma.novartis.com  Fri Oct 24 21:15:53 2003
From: ryszard.czerminski at pharma.novartis.com (ryszard.czerminski@pharma.novartis.com)
Date: Fri, 24 Oct 2003 15:15:53 -0400
Subject: [R] How to avoid converting "_" to "." ?
Message-ID: <OF9841990A.71159E47-ON85256DC9.006970E1-85256DC9.0069E6F6@EU.novartis.net>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20031024/8dcc590a/attachment.pl

From ryszard.czerminski at pharma.novartis.com  Fri Oct 24 21:23:35 2003
From: ryszard.czerminski at pharma.novartis.com (ryszard.czerminski@pharma.novartis.com)
Date: Fri, 24 Oct 2003 15:23:35 -0400
Subject: [R] how to remove NaN columns ?
Message-ID: <OF5EAED5B4.3045DB6A-ON85256DC9.006A5106-85256DC9.006A9B73@EU.novartis.net>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20031024/c9c33d6d/attachment.pl

From jasont at indigoindustrial.co.nz  Fri Oct 24 21:36:16 2003
From: jasont at indigoindustrial.co.nz (Jason Turner)
Date: Sat, 25 Oct 2003 08:36:16 +1300
Subject: [R] questions please
In-Reply-To: <6.0.0.22.0.20031024123243.01b5b488@gatty.st-and.ac.uk>
References: <6.0.0.22.0.20031024123243.01b5b488@gatty.st-and.ac.uk>
Message-ID: <3F997F30.8060304@indigoindustrial.co.nz>

Simone Panigada wrote:
...
>     *  I am running a glm to model animal abundance relating it to research 
> effort and other variables. I have 10 years of data and I am treating year 
> as a factor, R is comparing each year with the first year my data sheet, 
> while I would like to compare each year with each other. Is there any 
> function in R to do this?
...

Short answer:

If your model was glm(critters ~ year, ...), change this to

your.model <- glm(critters ~ year - 1,...)

In other words, drop the intercept if you don't want to compare to a 
particular year.  A model that defines coefficients for each year *and* 
an overall intercept is over-defined; there's no additional information 
given by the extra term.

You might also benefit from the package multcomp (which requires 
mvtnorm) which has the function simint().

Long answer:

Get your hands on a copy of "Modern Applied Statistics with S" by 
Venables and Ripley, and read chapters 6 and 7 rather thoroughly.  This 
goes into the reasons behind the choice of comparing one year to others.

Cheers

Jason
-- 
Indigo Industrial Controls Ltd.
http://www.indigoindustrial.co.nz
64-21-343-545
jasont at indigoindustrial.co.nz



From jasont at indigoindustrial.co.nz  Fri Oct 24 21:42:33 2003
From: jasont at indigoindustrial.co.nz (Jason Turner)
Date: Sat, 25 Oct 2003 08:42:33 +1300
Subject: [R] Scan() question
In-Reply-To: <3F98B76A.4010505@indigoindustrial.co.nz>
References: <000e01c399e6$55561ae0$6754fea9@TOSHIBAS503>
	<3F98B76A.4010505@indigoindustrial.co.nz>
Message-ID: <3F9980A9.5080207@indigoindustrial.co.nz>

Oops.  The file name is obviously "foo.csv", not "foo.txt".  Sorry if 
that caused confusion.

Jason Turner wrote:
> I have a file on my XP box, in my "My Documents" folder, called 
> "foo.txt".  It's a two-column text file, tab-separated.  First column is 
>  the numbers 1 to 12.  Second column is some random numbers.
> 
>  > scan("c:/documents and settings/jason turner/my documents/foo.csv")

-- 
Indigo Industrial Controls Ltd.
http://www.indigoindustrial.co.nz
64-21-343-545
jasont at indigoindustrial.co.nz



From olau at fas.harvard.edu  Fri Oct 24 21:49:51 2003
From: olau at fas.harvard.edu (Olivia Lau)
Date: Fri, 24 Oct 2003 15:49:51 -0400 (EDT)
Subject: [R] browseURL() bug
In-Reply-To: <3F88657F.4050301@jhsph.edu>
Message-ID: <Pine.LNX.4.44.0310241545540.7610-100000@login1.fas.harvard.edu>

Hi, 

I'm getting the following warning:  

> browseURL("http://www.r-project.org/")
> No running window found.
Xlib:  extension "RENDER" missing on display ":1.0".

This also comes up with htmlhelp = TRUE.  Is there any way to supress the 
warning?  I'm calling browseURL under R 1.8 for linux, and I'm calling 
these commands within functions.  I tried invisible(), but it didn't get 
rid of it.

Thanks, 

Olivia Lau.



From spencer.graves at pdf.com  Fri Oct 24 21:57:10 2003
From: spencer.graves at pdf.com (Spencer Graves)
Date: Fri, 24 Oct 2003 12:57:10 -0700
Subject: [R] how to remove NaN columns ?
In-Reply-To: <OF5EAED5B4.3045DB6A-ON85256DC9.006A5106-85256DC9.006A9B73@EU.novartis.net>
References: <OF5EAED5B4.3045DB6A-ON85256DC9.006A5106-85256DC9.006A9B73@EU.novartis.net>
Message-ID: <3F998416.3060207@pdf.com>

Have you considered the following: 

 > x <- 1:11
 > xs <- scale(x)
 > xs
            [,1]
 [1,] -1.5075567
 [2,] -1.2060454
...
[11,]  1.5075567
attr(,"scaled:center")
[1] 6
attr(,"scaled:scale")
[1] 3.316625
 > attr(xs, "scaled:center")
[1] 6

hope this helps.  spencer graves

ryszard.czerminski at pharma.novartis.com wrote:

>Nice!
>
>I noticed that in generated structure it has two attributes 
>attr(,"scaled:center") and attr(,"scaled:scale")
>How can I access them ? 
>
>R
>
>
>
>
>
>Giovanni Petris <GPetris at uark.edu>
>10/24/2003 02:50 PM
>Please respond to Giovanni Petris
>
> 
>        To:     Ryszard Czerminski/PH/Novartis at PH
>        cc:     r-help at stat.math.ethz.ch
>        Subject:        Re: [R] how to remove NaN columns ?
>
>
>
>  
>
>>>xs <- lapply(xdata, function(x){(x - mean(x))/sqrt(var(x))})
>>>      
>>>
>
>Incidentally, the function 'scale' does just that.
>
>  
>



From ggrothendieck at myway.com  Fri Oct 24 22:57:31 2003
From: ggrothendieck at myway.com (Gabor Grothendieck)
Date: Fri, 24 Oct 2003 16:57:31 -0400 (EDT)
Subject: [R] repeating colors in graph 2
Message-ID: <20031024205731.B48A739B3@xmxpita.myway.com>


> plot(1:13, col=rainbow(1:13))

That should be

  plot(1:13, col=rainbow(13))


_______________________________________________
No banners. No pop-ups. No kidding.
Introducing My Way - http://www.myway.com



From p.dalgaard at biostat.ku.dk  Fri Oct 24 23:08:30 2003
From: p.dalgaard at biostat.ku.dk (Peter Dalgaard)
Date: 24 Oct 2003 23:08:30 +0200
Subject: [R] How to avoid converting "_" to "." ?
In-Reply-To: <OF9841990A.71159E47-ON85256DC9.006970E1-85256DC9.0069E6F6@EU.novartis.net>
References: <OF9841990A.71159E47-ON85256DC9.006970E1-85256DC9.0069E6F6@EU.novartis.net>
Message-ID: <x2llra2vjl.fsf@biostat.ku.dk>

ryszard.czerminski at pharma.novartis.com writes:

> It is minor thing, but how can I avoid converting "_" to "." ?
> 
> e.g. I have a data set "test.csv"
>  
> A,A_B,A_C,C,D
> X,11,0,13,14
> Y,21,0,23,24
> 
> and when I read it all underscores are converted to dots (:<)
> 
> > d <- read.csv("test.csv")
> > d
>   A A.B A.C  C  D
> 1 X  11   0 13 14
> 2 Y  21   0 23 24

Add check.names=FALSE (see ?read.csv).

-- 
   O__  ---- Peter Dalgaard             Blegdamsvej 3  
  c/ /'_ --- Dept. of Biostatistics     2200 Cph. N   
 (*) \(*) -- University of Copenhagen   Denmark      Ph: (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)             FAX: (+45) 35327907



From paulda at BATTELLE.ORG  Fri Oct 24 23:14:27 2003
From: paulda at BATTELLE.ORG (Paul, David  A)
Date: Fri, 24 Oct 2003 17:14:27 -0400
Subject: [R] possible win.metafile( ) problem?
Message-ID: <940250A9EB37A24CBE28D858EF07774967ABF5@ws-bco-mse3.milky-way.battelle.org>

R1.8.0, Win2k:

When I run the code

lset( list( background = list(col = "white")))
xyplot
( 
	y ~ TIME , data = foo.frame,
	scales = list(alternating = FALSE),
	ylim = c(.75,y.max),
	panel = function(x, y, panel.number, ... )
	{
	panel.superpose(x = foo.frame$TIME[foo.frame$group == 1],
		y = foo.frame$y[foo.frame$group == 1],
		subscripts = TRUE, 
		groups = foo.frame$subject.id[foo.frame$group == 1],
		type = 'b', 
		col = "black", 
		lwd = 4, 
		cex = 1.5)
			
	panel.superpose(x = foo.frame$TIME[foo.frame$group == 0],
		y = foo.frame$y[foo.frame$group == 0],
		subscripts = TRUE, 
		groups = foo.frame$subject.id[foo.frame$group == 0],
		type = 'l', 
		col = gray(.6))
			
	panel.loess(x = foo.frame$TIME, 
		y = foo.frame$y, col = gray(.2), 
		lwd = 3, 
		lty = 5, 
		span = 1/3)
	}
)

I get the desired graph on the screen.  Basically,
I am trying to display the time course of a set of test
subjects, where each of two subsets is displayed in a
different color, and a loess smoother is added to the plot
for reference.

When I add 

trellis.device(postscript, 
	file = "//.../foo_status.ps", 
	color = TRUE)

to the beginning of this little script, upon trying to open the
created file with Ghostview/Ghostscript, I get an error dialog 
box containing the error message in the attached text file and 
nothing is displayed.

If I change to

trellis.device(win.metafile, 
	file = "//.../foo_status.wmf", 
	width = 8.5, height = 6.25)

I am able to view the created file, but there are unwanted
vertical lines drawn at max(foo.frame$TIME).  Moreover, MSWord
has a hard time rendering this graph in a stable way, as it
is constantly redrawing it and freezing my screen.  The file
is only 35K, so this should not happen.  I tried the same
scripting options in R1.7.1 with the same results, which leads
me to believe that my xyplot(..) code could be in error.
OTOH, this is a fairly simple adaptation of some earlier
working code and the within-RGui graph appears to be fine and
doesn't have the unwanted vertical lines.  When I right-click
on this graph and try to paste it directly into MSWord, I
get the same "screen freeze" issues, and the vertical lines
appear in the window.

Any help would be greatly appreciated.


-david paul

-------------- next part --------------
An embedded and charset-unspecified text was scrubbed...
Name: Ghostscript Error Message.txt
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20031024/63969867/GhostscriptErrorMessage.txt

From andrewr at uidaho.edu  Fri Oct 24 23:13:05 2003
From: andrewr at uidaho.edu (Andrew Robinson)
Date: Fri, 24 Oct 2003 14:13:05 -0700
Subject: [R] Dataframes of marginal summaries
Message-ID: <200310241413.05879.andrewr@uidaho.edu>

Hi dear R-community,

I wonder if anyone has written code that will post-process the results of a 
tapply to create a dataframe that includes the outcome, the factor by which 
the function has been applied, and, if the factor is also an interaction, 
then also the levels of contributing factors?

>From this

outcome <- tapply(data, A.B, function)

I'd like to arrange something like this:

    A     B     A.B     outcome
1
2
3
4

I have the following example solution, which seems clumsy, relying on 
dissecting the labels of the tapply result.  I don't think I've harvested 
this from anyone else, but I apologize if I have.

=======================================================================

x <- factor(c(rep("B",4), rep("A",4)))
y <- factor(c(1,2,1,2,1,2,1,2))
z <- rnorm(8)
data <- as.data.frame(cbind(x, y, z))
data$x.y <- interaction(x,y)

my.test <- tapply(data$z, data$x.y, mean)

test <- as.data.frame(as.numeric(my.test))
names(test) <- "Mean"
nList <- function(list, n) list[[n]]
test$x <- factor(as.character(lapply(strsplit(row.names(my.test),
                                               "\\."), nList, n=1)))
test$y <- factor(as.character(lapply(strsplit(row.names(my.test),
                                               "\\."), nList, n=2)))
test$x.y <- factor(row.names(my.test))

test[,c(2:4,1)]

=======================================================================

Any suggestions to simplify would be much appreciated.

Andrew
-- 
Andrew Robinson                      Ph: 208 885 7115
Department of Forest Resources       Fa: 208 885 6226
University of Idaho                  E : andrewr at uidaho.edu
PO Box 441133                        W : http://www.uidaho.edu/~andrewr
Moscow ID 83843                      Or: http://www.biometrics.uidaho.edu
No statement above necessarily represents my employer's opinion.



From tlumley at u.washington.edu  Sat Oct 25 00:53:57 2003
From: tlumley at u.washington.edu (Thomas Lumley)
Date: Fri, 24 Oct 2003 15:53:57 -0700 (PDT)
Subject: [R] Dataframes of marginal summaries
In-Reply-To: <200310241413.05879.andrewr@uidaho.edu>
References: <200310241413.05879.andrewr@uidaho.edu>
Message-ID: <Pine.A41.4.58.0310241553310.81548@homer37.u.washington.edu>

On Fri, 24 Oct 2003, Andrew Robinson wrote:

> Hi dear R-community,
>
> I wonder if anyone has written code that will post-process the results of a
> tapply to create a dataframe that includes the outcome, the factor by which
> the function has been applied, and, if the factor is also an interaction,
> then also the levels of contributing factors?

aggregate() is close to this.

	-thomas



From ligges at statistik.uni-dortmund.de  Sat Oct 25 12:50:24 2003
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Sat, 25 Oct 2003 12:50:24 +0200
Subject: pre-compiled win binaries (was: Re: [R] Quantreg Package)
References: <Pine.SOL.4.30.0310231502260.19787-100000@ysidro.econ.uiuc.edu>
Message-ID: <3F9A5570.E9C792FA@statistik.uni-dortmund.de>

FYI: quantreg_3.32.tar.gz passes Rcmd check on Windows and the binary
will be on CRAN within a couple of days (I don't know when CRAN master
mirrors my archives again, because of the announced shutdown).

I will try to get automated e-mails to package maintainers of
"erroneous" (in Windows) packages into my scripts. An announcement to
all package maintainers will follow in a couple of days.

Uwe

Roger Koenker wrote:
> 
> You are right, of course, it would be nice to have notification,
> but I'm also sympathetic to Uwe's situation, and not everything
> that could be automated, could be _easily_ automated within the
> constraints imposed by the rest of world.  The lesson I've drawn
> from this is that complaints will appear, and the check directory
> does help explain problems.  My real difficulty is that I have
> no good way to  explore windows specific problems.  But this is
> just the flip side of saying what a great thing it is that
> the windows binaries are usually appearing automagically without
> any problems!
> 
> Roger
> 
> url:    www.econ.uiuc.edu/~roger/my.html        Roger Koenker
> email   rkoenker at uiuc.edu                       Department of Economics
> vox:    217-333-4558                            University of Illinois
> fax:    217-244-6678                            Champaign, IL 61820
> 
> On Thu, 23 Oct 2003, Achim Zeileis wrote:
> 
> > It happened for the second time in a week that a Windows binary for a
> > CRAN package was not availabe (Hmisc and quantreg) although the
> > package maintainer would have been willing to try to fix the problems
> > that prevented automatic pre-compilation if he would have realized
> > that there is a problem on Windows.
> > Of course, the information was provided on CRAN, but I thought that it
> > might help to automatically notify the package maintainers if some
> > conflicts occur on Windows. I would appreciate such a notification if
> > one of my packages would have failed to compile...and it is not very
> > unlikely that I wouldn't have discovered it myself.
> >
> > I already asked Uwe privately and he would be willing to provide a
> > notification but maybe there are concerns or objections from the
> > maintainers?
> >
> > best,
> > Z
> >
> >
> > On Thursday 23 October 2003 17:28, Uwe Ligges wrote:
> >
> > > Radaelli Paolo - Dottorati di Ricerca wrote:
> > > > I saw the read-me but I didn't undersstand wich is the problem. I
> > > > only know that in a previous version of R I installed on my pc it
> > > > was all ok.
> > >
> > > Yes, but on the recent version it is *not* OK.
> > > http://cran.r-project.org/bin/windows/contrib/1.8/check/quantreg-che
> > >ck.log tells you:
> > >
> > >   [...]
> > >   * checking examples ... ERROR
> > >   Running examples in quantreg-Ex.R failed.
> > >
> > > BTW: Efforts have been made to upload these check logs to CRAN in
> > > order to provide you with this information, so please read those
> > > files!
> > >
> > >
> > > Looking closer (as a hint for Roger), R *crashes* when running the
> > > examples in ?boot.rq.
> > >
> > > > So know I have to download the extensions files and then compile
> > > > them on my own ?
> > >
> > > You can download the source package and compile from source (I don't
> > > know of any "extensions files").
> > > Attention: The problem not passing Rcmd check remains (almost
> > > certain).
> > >
> > > Uwe Ligges
> > >
> > > > Thank you
> > > > Paolo Radaelli
> > > >
> > > > ----- Original Message -----
> > > > From: "Uwe Ligges" <ligges at statistik.uni-dortmund.de>
> > > > To: "Radaelli Paolo - Dottorati di Ricerca"
> > > > <paolo.radaelli at unimib.it> Cc: <r-help at stat.math.ethz.ch>
> > > > Sent: Thursday, October 23, 2003 4:44 PM
> > > > Subject: Re: [R] Quantreg Package
> > > >
> > > >>Radaelli Paolo - Dottorati di Ricerca wrote:
> > > >>>I've just installed R 1.0.8 (for Windows) and I tried to install
> > > >>> the
> > > >
> > > > package Quantreg directly from Cran but it's not in the list of
> > > > downlodable packages.
> > > >
> > > >>>I tried also downloading the zip file and then install it but
> > > >>> there is
> > > >
> > > > an error.
> > > >
> > > >>>How can I do it?
> > > >>>Thank you
> > > >>>[[alternative HTML version deleted]]
> > > >>>
> > > >>>______________________________________________
> > > >>>R-help at stat.math.ethz.ch mailing list
> > > >>>https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> > > >>
> > > >>CRAN/bin/windows/contrib/1.8/ReadMe tells you that Windows
> > > >> binaries of those packages not passing Rcmd check are not
> > > >> published on CRAN. And the corresponding Status file and
> > > >> check-log tells you quantreg is among those packages.
> > > >>
> > > >>So you have to compile from source yourself.
> > > >>
> > > >>Uwe Ligges
> > >
> > > ______________________________________________
> > > R-help at stat.math.ethz.ch mailing list
> > > https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> >



From maechler at stat.math.ethz.ch  Sat Oct 25 14:54:14 2003
From: maechler at stat.math.ethz.ch (Martin Maechler)
Date: Sat, 25 Oct 2003 14:54:14 +0200
Subject: [R] error in documentation
In-Reply-To: <1067005054.2579.1.camel@localhost.localdomain>
References: <1067005054.2579.1.camel@localhost.localdomain>
Message-ID: <16282.29302.49894.498357@gargle.gargle.HOWL>

>>>>> "Rajarshi" == Rajarshi Guha <rxg218 at psu.edu>
>>>>>     on 24 Oct 2003 10:17:34 -0400 writes:

    Rajarshi> Hi, there appears to be an error in the
    Rajarshi> documentation for fanny. In the Details section,
    Rajarshi> the equation representing the objective function
    Rajarshi> has no variable k in it, but the two lines
    Rajarshi> following it mention:

    Rajarshi> ... k is the number of clusters, ..........

You are correct -- note however that this is only for the text
version of the help page (not the nicely formatted  latex/ps/pdf
one which has real mathematical formulae) -- and which you can
get from {when you have latex installed on your computer}
    help(fanny, offline = TRUE)

The corrected "text-version" of the formula is

    SUM_[v=1..k] (SUM_(i,j) u(i,v)^2 u(j,v)^2 d(i,j)) / (2 SUM_j u(j,v)^2)}
       ^^^^^^^^

Martin Maechler <maechler at stat.math.ethz.ch>	http://stat.ethz.ch/~maechler/
Seminar fuer Statistik, ETH-Zentrum  LEO C16	Leonhardstr. 27
ETH (Federal Inst. Technology)	8092 Zurich	SWITZERLAND
phone: x-41-1-632-3408		fax: ...-1228			<><



From maechler at stat.math.ethz.ch  Sat Oct 25 15:00:16 2003
From: maechler at stat.math.ethz.ch (Martin Maechler)
Date: Sat, 25 Oct 2003 15:00:16 +0200
Subject: [R] neural networks
In-Reply-To: <5.2.1.1.2.20031024093551.03f6e3d8@poptop.llnl.gov>
References: <5.2.1.1.1.20031024181146.00c26988@cucafera.icm.csic.es>
	<5.2.1.1.2.20031024093551.03f6e3d8@poptop.llnl.gov>
Message-ID: <16282.29664.253465.971666@gargle.gargle.HOWL>

>>>>> "Pauline" == Pauline Gu <gu4 at llnl.gov>
>>>>>     on Fri, 24 Oct 2003 10:03:23 -0700 writes:

    Pauline> Hello, experts, Does the number of input for
    Pauline> training set need to be the same as the number of
    Pauline> input for the data set that I am trying to predict
    Pauline> from the nnet result of the training?

If `` the number of input '' is what we usually call 
   "number of (explanatory) variables",
then
   the answer is "yes".

If instead, you mean "number of cases", "number of
   observations", or "number of (ex|s)amples"
the answer is "no".

Martin Maechler <maechler at stat.math.ethz.ch>	http://stat.ethz.ch/~maechler/
Seminar fuer Statistik, ETH-Zentrum  LEO C16	Leonhardstr. 27
ETH (Federal Inst. Technology)	8092 Zurich	SWITZERLAND
phone: x-41-1-632-3408		fax: ...-1228			<><



From ligges at statistik.uni-dortmund.de  Sat Oct 25 15:31:53 2003
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Sat, 25 Oct 2003 15:31:53 +0200
Subject: [R] browseURL() bug
In-Reply-To: <Pine.LNX.4.44.0310241545540.7610-100000@login1.fas.harvard.edu>
References: <Pine.LNX.4.44.0310241545540.7610-100000@login1.fas.harvard.edu>
Message-ID: <3F9A7B49.6030006@statistik.uni-dortmund.de>

Olivia Lau wrote:

> Hi, 
> 
> I'm getting the following warning:  
> 
> 
>>browseURL("http://www.r-project.org/")
>>No running window found.
> 
> Xlib:  extension "RENDER" missing on display ":1.0".
> 
> This also comes up with htmlhelp = TRUE.  Is there any way to supress the 
> warning? 

No - at least not easily.

> I'm calling browseURL under R 1.8 for linux, and I'm calling 
> these commands within functions.  I tried invisible(), but it didn't get 
> rid of it.

The messages are from the Browser browseURL() tries to start via 
system(), hence only indirectly an R issue.

Uwe Ligges


> Thanks, 
> 
> Olivia Lau.
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help



From wolski at molgen.mpg.de  Sat Oct 25 16:26:36 2003
From: wolski at molgen.mpg.de (Eryk Wolski)
Date: Sat, 25 Oct 2003 16:26:36 +0200 (MET DST)
Subject: [R] Whats wrong with NextMethod("[[<-")?
Message-ID: <Pine.OSF.4.31.0310251623540.7140-100000@harry.molgen.mpg.de>

Hi!

Given a list object "myA" with several fields
i want to construct the assignment function "[[<-.myA"
which behaves differently dependent of the class of the arguments (value).

If value are a myB object it should append(assign) this massvector to the
data
field of myA object
if its any other object the assignement function should work as for any
list.


"[[<-.myA"<-function(mvl,name,value)
  {
    if(class(value)[1]!="myB")
      {
        NextMethod("[[<-"); #work as a list
      }
    else
      {
        mvl$data[[name]] <- value #work as myA
      }
    mvl
  }


test<-list(a=1,data=list(1))
class(test)<-c("myA","list")

duda<-"hello"
class(duda)<-"myB"

test[[2]]<-duda               #works
test[["newField"]]<-"bella"   #nothing happens!!!

The assignment test[["newField"]]<-"bella" has no effect. The list test
are not extended by the entry newField.
Looks to me like NextMethod("[[<-") does nothing.

Is it possible to do this what I try to do with S3?
If so what i do wrong?
If its not possible to do this in this way can anyone suggest
 a different solution to what i like to do?

Eryk



From paulda at BATTELLE.ORG  Sat Oct 25 16:57:10 2003
From: paulda at BATTELLE.ORG (Paul, David  A)
Date: Sat, 25 Oct 2003 10:57:10 -0400
Subject: [R] possible win.metafile( ) problem?
Message-ID: <940250A9EB37A24CBE28D858EF07774967ABF6@ws-bco-mse3.milky-way.battelle.org>

Sorry, I should have mentioned that
at the end of each script I have been using
dev.off().

-david paul



From hb at maths.lth.se  Sat Oct 25 17:23:42 2003
From: hb at maths.lth.se (Henrik Bengtsson)
Date: Sat, 25 Oct 2003 17:23:42 +0200
Subject: [R] Whats wrong with NextMethod("[[<-")?
In-Reply-To: <Pine.OSF.4.31.0310251623540.7140-100000@harry.molgen.mpg.de>
Message-ID: <005f01c39b0b$fa430e70$dd0040d5@maths.lth.se>

Hi. NextMethod("[[<-") do indeed call the "next" "[[<-"() method. The
thing is that it is also return the update object, which you are *not*
taking care of. You are just returning the same object again. So here is
what you want:

"[[<-.myA"<-function(mvl,name,value) {
  if(data.class(value) == "myB") {
    mvl$data[[name]] <- value
    mvl
  } else {
    NextMethod("[[<-")
  }
}

Also, it is better to use data.class(obj) than class(obj)[1]. 

Cheers

Henrik Bengtsson
Lund University


> -----Original Message-----
> From: r-help-bounces at stat.math.ethz.ch 
> [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Eryk Wolski
> Sent: den 25 oktober 2003 16:27
> To: r-help at stat.math.ethz.ch
> Subject: [R] Whats wrong with NextMethod("[[<-")?
> 
> 
> Hi!
> 
> Given a list object "myA" with several fields
> i want to construct the assignment function "[[<-.myA"
> which behaves differently dependent of the class of the 
> arguments (value).
> 
> If value are a myB object it should append(assign) this 
> massvector to the data field of myA object if its any other 
> object the assignement function should work as for any list.
> 
> 
> "[[<-.myA"<-function(mvl,name,value)
>   {
>     if(class(value)[1]!="myB")
>       {
>         NextMethod("[[<-"); #work as a list
>       }
>     else
>       {
>         mvl$data[[name]] <- value #work as myA
>       }
>     mvl
>   }
> 
> 
> test<-list(a=1,data=list(1))
> class(test)<-c("myA","list")
> 
> duda<-"hello"
> class(duda)<-"myB"
> 
> test[[2]]<-duda               #works
> test[["newField"]]<-"bella"   #nothing happens!!!
> 
> The assignment test[["newField"]]<-"bella" has no effect. The 
> list test are not extended by the entry newField. Looks to me 
> like NextMethod("[[<-") does nothing.
> 
> Is it possible to do this what I try to do with S3?
> If so what i do wrong?
> If its not possible to do this in this way can anyone suggest
>  a different solution to what i like to do?
> 
> Eryk
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list 
> https://www.stat.math.ethz.ch/mailma> n/listinfo/r-help
> 
>



From ripley at stats.ox.ac.uk  Sat Oct 25 18:01:07 2003
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Sat, 25 Oct 2003 17:01:07 +0100 (BST)
Subject: [R] Whats wrong with NextMethod("[[<-")?
In-Reply-To: <005f01c39b0b$fa430e70$dd0040d5@maths.lth.se>
Message-ID: <Pine.LNX.4.44.0310251656330.3272-100000@gannet.stats>

On Sat, 25 Oct 2003, Henrik Bengtsson wrote:

> Hi. NextMethod("[[<-") do indeed call the "next" "[[<-"() method. The
> thing is that it is also return the update object, which you are *not*
> taking care of. You are just returning the same object again. So here is
> what you want:
> 
> "[[<-.myA"<-function(mvl,name,value) {
>   if(data.class(value) == "myB") {
>     mvl$data[[name]] <- value
>     mvl
>   } else {
>     NextMethod("[[<-")
>   }
> }
> 
> Also, it is better to use data.class(obj) than class(obj)[1]. 

It's not.  First, the effect is the same here, and second data.class is 
really only for back-compatibility when fudging the difference between 
"numeric" and "integer".

It is almost certainly better to use inherits(value, "myB"), though, and
you should probably very rarely test the class (let alone data.class)
directly.

> 
> Cheers
> 
> Henrik Bengtsson
> Lund University
> 
> 
> > -----Original Message-----
> > From: r-help-bounces at stat.math.ethz.ch 
> > [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Eryk Wolski
> > Sent: den 25 oktober 2003 16:27
> > To: r-help at stat.math.ethz.ch
> > Subject: [R] Whats wrong with NextMethod("[[<-")?
> > 
> > 
> > Hi!
> > 
> > Given a list object "myA" with several fields
> > i want to construct the assignment function "[[<-.myA"
> > which behaves differently dependent of the class of the 
> > arguments (value).
> > 
> > If value are a myB object it should append(assign) this 
> > massvector to the data field of myA object if its any other 
> > object the assignement function should work as for any list.
> > 
> > 
> > "[[<-.myA"<-function(mvl,name,value)
> >   {
> >     if(class(value)[1]!="myB")
> >       {
> >         NextMethod("[[<-"); #work as a list
> >       }
> >     else
> >       {
> >         mvl$data[[name]] <- value #work as myA
> >       }
> >     mvl
> >   }
> > 
> > 
> > test<-list(a=1,data=list(1))
> > class(test)<-c("myA","list")
> > 
> > duda<-"hello"
> > class(duda)<-"myB"
> > 
> > test[[2]]<-duda               #works
> > test[["newField"]]<-"bella"   #nothing happens!!!
> > 
> > The assignment test[["newField"]]<-"bella" has no effect. The 
> > list test are not extended by the entry newField. Looks to me 
> > like NextMethod("[[<-") does nothing.
> > 
> > Is it possible to do this what I try to do with S3?
> > If so what i do wrong?
> > If its not possible to do this in this way can anyone suggest
> >  a different solution to what i like to do?
> > 
> > Eryk
> > 
> > ______________________________________________
> > R-help at stat.math.ethz.ch mailing list 
> > https://www.stat.math.ethz.ch/mailma> n/listinfo/r-help
> > 
> >
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> 
> 

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From andrejk at zrc-sazu.si  Sat Oct 25 18:48:09 2003
From: andrejk at zrc-sazu.si (Andrej Kveder)
Date: Sat, 25 Oct 2003 18:48:09 +0200
Subject: [R] memory optimization and use of recursion
Message-ID: <FHEEJBDDCNPPNJEACDJAGEKIDFAA.andrejk@zrc-sazu.si>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20031025/299ce6a0/attachment.pl

From faheem at email.unc.edu  Sat Oct 25 19:24:04 2003
From: faheem at email.unc.edu (Faheem Mitha)
Date: Sat, 25 Oct 2003 13:24:04 -0400 (EDT)
Subject: [R] compiling documentation into dvi form
Message-ID: <Pine.LNX.4.58.0310251310200.30903@Chrestomanci>


Dear People,

Perhaps I am missing something, but I;m trying to get the documentation
for the Rmpi package in a printable form. I've tried doing

faheem ~>R CMD Rd2dvi /usr/local/lib/R/site-library/Rmpi/
Hmm ... looks like a package
Converting Rd files to LaTeX ...
/usr/local/lib/R/site-library/Rmpi//man/Rmpi.Rd

but the result is basically just one page, which seems to include just one
command. What am I doing wrong, if anything?

BTW, the Rmpi package does not seem to be in CRAN. It can be found at
http://www.stats.uwo.ca/faculty/yu/Rmpi/

Thanks in advance.

                                                                 Faheem.



From hb at maths.lth.se  Sat Oct 25 19:29:11 2003
From: hb at maths.lth.se (Henrik Bengtsson)
Date: Sat, 25 Oct 2003 19:29:11 +0200
Subject: [R] Whats wrong with NextMethod("[[<-")?
In-Reply-To: <Pine.LNX.4.44.0310251656330.3272-100000@gannet.stats>
Message-ID: <000201c39b1d$821d21d0$410040d5@maths.lth.se>

> -----Original Message-----
> From: Prof Brian Ripley [mailto:ripley at stats.ox.ac.uk] 
> Sent: den 25 oktober 2003 18:01
> To: Henrik Bengtsson
> Cc: 'Eryk Wolski'; r-help at stat.math.ethz.ch
> Subject: RE: [R] Whats wrong with NextMethod("[[<-")?
> 
> 
> On Sat, 25 Oct 2003, Henrik Bengtsson wrote:
> 
> > Hi. NextMethod("[[<-") do indeed call the "next" "[[<-"() 
> method. The 
> > thing is that it is also return the update object, which 
> you are *not* 
> > taking care of. You are just returning the same object 
> again. So here 
> > is what you want:
> > 
> > "[[<-.myA"<-function(mvl,name,value) {
> >   if(data.class(value) == "myB") {
> >     mvl$data[[name]] <- value
> >     mvl
> >   } else {
> >     NextMethod("[[<-")
> >   }
> > }
> > 
> > Also, it is better to use data.class(obj) than class(obj)[1].
> 
> It's not.  First, the effect is the same here, and second 
> data.class is really only for back-compatibility when fudging the 
> difference between "numeric" and "integer".

I didn't really know that. Thanks for that comment.

> It is almost certainly better to use inherits(value, "myB"), 
> though, and you should probably very rarely test the class 
> (let alone data.class) directly.

Of course! ...and to extend what Brian Ripley write: you rarely want to
do an action *if and only if* an object is of a specific class and
another action if it is of a subclass or a superclass. This would in
some would be against why you have choose the class hierarchy that
chose. So inherits() should be used.

Henrik "too-little-sleep" Bengtsson

> > Cheers
> > 
> > Henrik Bengtsson
> > Lund University
> > 
> > 
> > > -----Original Message-----
> > > From: r-help-bounces at stat.math.ethz.ch
> > > [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Eryk Wolski
> > > Sent: den 25 oktober 2003 16:27
> > > To: r-help at stat.math.ethz.ch
> > > Subject: [R] Whats wrong with NextMethod("[[<-")?
> > > 
> > > 
> > > Hi!
> > > 
> > > Given a list object "myA" with several fields
> > > i want to construct the assignment function "[[<-.myA" 
> which behaves 
> > > differently dependent of the class of the arguments (value).
> > > 
> > > If value are a myB object it should append(assign) this
> > > massvector to the data field of myA object if its any other 
> > > object the assignement function should work as for any list.
> > > 
> > > 
> > > "[[<-.myA"<-function(mvl,name,value)
> > >   {
> > >     if(class(value)[1]!="myB")
> > >       {
> > >         NextMethod("[[<-"); #work as a list
> > >       }
> > >     else
> > >       {
> > >         mvl$data[[name]] <- value #work as myA
> > >       }
> > >     mvl
> > >   }
> > > 
> > > 
> > > test<-list(a=1,data=list(1))
> > > class(test)<-c("myA","list")
> > > 
> > > duda<-"hello"
> > > class(duda)<-"myB"
> > > 
> > > test[[2]]<-duda               #works
> > > test[["newField"]]<-"bella"   #nothing happens!!!
> > > 
> > > The assignment test[["newField"]]<-"bella" has no effect. The
> > > list test are not extended by the entry newField. Looks to me 
> > > like NextMethod("[[<-") does nothing.
> > > 
> > > Is it possible to do this what I try to do with S3?
> > > If so what i do wrong?
> > > If its not possible to do this in this way can anyone suggest  a 
> > > different solution to what i like to do?
> > > 
> > > Eryk
> > > 
> > > ______________________________________________
> > > R-help at stat.math.ethz.ch mailing list
> > > https://www.stat.math.ethz.ch/mailma> n/listinfo/r-help
> > > 
> > >
> > 
> > ______________________________________________
> > R-help at stat.math.ethz.ch mailing list 
> > https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> > 
> > 
> 
> -- 
> Brian D. Ripley,                  ripley at stats.ox.ac.uk
> Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
> University of Oxford,             Tel:  +44 1865 272861 (self)
> 1 South Parks Road,                     +44 1865 272866 (PA)
> Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From H.F.Xie at westminster.ac.uk  Sun Oct 26 02:49:10 2003
From: H.F.Xie at westminster.ac.uk (Haifeng (Kevin) Xie)
Date: Sun, 26 Oct 2003 01:49:10 -0000
Subject: [R] Variance-covariance matrix for beta hat and b hat from lme
References: <00d601c39980$d543f490$be884aa1@cscs.wmin.ac.uk>
	<6rad7rlwys.fsf@bates4.stat.wisc.edu>
Message-ID: <000f01c39b63$5adce740$6e4a4ed5@tiger>

Dear Prof. Bates,

Many thanks for your reply. I think I will resolve to manually reading off
the necessary information from the summary report produced by summary.lme
and calculate the variance-covariance matrix based on expressions given in
Laird & Ware (1982) and Henderson (1975). In particular I will make use of
the result that the covariance between beta hat and b_i hat's are zero, and
covariance between b_i hat's themselves are zero as well, i.e. the matrix
cov(beta hat, b hat) is block diagonal, where b hat = [b_1 hat; b_2 hat;
...; b_m hat]' , m is the number of groups.

Reference
Laird, N. M. and J. H. Ware (1982).  Random-effects models for longitudinal
data.  Biometrics 38, 963-974.
Henderson, C. R. (1975).  Best linear unbiased estimation and prediction
under a selection model.  Biometrics 31, 423-447.


Many thanks again.

Haifeng Xie
Research Student
University of Westminster
London, UK


----- Original Message ----- 
From: "Douglas Bates" <bates at stat.wisc.edu>
To: "Haifeng (Kevin) Xie" <H.F.Xie at westminster.ac.uk>
Cc: "R-help mailing list" <r-help at stat.math.ethz.ch>
Sent: Thursday, October 23, 2003 5:50 PM
Subject: Re: [R] Variance-covariance matrix for beta hat and b hat from lme


> "Haifeng \(Kevin\) Xie" <H.F.Xie at westminster.ac.uk> writes:
>
> > Given a LME model (following the notation of Pinheiro and Bates 2000)
y_i
> > = X_i*beta + Z_i*b_i + e_i, is it possible to extract the
> > variance-covariance matrix for the estimated beta_i hat and b_i hat from
the
> > lme fitted object?
>
> Not easily.  The pieces that you need are in the condensed linear
> model structure and you may be able to extract them in R code but I
> have not written any code to do that.
>
> I am revising the internal representation of lme objects using S4
> classes.  Saikat DebRoy and I have one representation in the lme4
> package but will probably revise that.  Some recent work on
> computational methods
>         http://www.stat.wisc.edu/~bates/reports/MixedComp.pdf
> has me convinced that even this representation should be reorganized
> and simplified.
>
> If you really want to delve into the old structures I can give you
> some pointers (pun unintended) on where to look but beware that it's a
> quagmire.  (Oops - not supposed to use that word in e-mail originating
> in the U.S.A.  My regards to the NSA.)
>
>
> -- 
> Douglas Bates                            bates at stat.wisc.edu
> Statistics Department                    608/262-2598
> University of Wisconsin - Madison        http://www.stat.wisc.edu/~bates/
>



From olau at fas.harvard.edu  Sun Oct 26 05:49:45 2003
From: olau at fas.harvard.edu (Olivia Lau)
Date: Sun, 26 Oct 2003 00:49:45 -0400
Subject: [R] commenting demos
Message-ID: <000901c39b7c$96d1d5d0$4e02a8c0@olau>

Hi,

Is there a way to make demo() print comments along with the code
and output?

For example, if I use something like readline("..."), demo
prints both readline("...") and ...; as far as I can tell, this
is also true of print() and cat().  I just want students to stop
and think about every command in my demo scripts.

Thanks,

Olivia.



From KLee at igeotech.com  Sun Oct 26 06:54:48 2003
From: KLee at igeotech.com (Kuantsai Lee)
Date: Sat, 25 Oct 2003 22:54:48 -0700
Subject: [R] possible win.metafile( ) problem?
In-Reply-To: <940250A9EB37A24CBE28D858EF07774967ABF5@ws-bco-mse3.milky-w
	ay.battelle.org>
Message-ID: <5.2.0.9.0.20031025224854.00bd58c0@mail.zoper.com>

Just a suggestion: After displaying the graph on the screen, have you tried 
saving the graph to file (from the menu, File -> Save As -> ...) to see if 
files saved this way behaved similarly?

At 05:14 PM 10/24/2003 -0400, Paul, David  A wrote:
>R1.8.0, Win2k:
>
>When I run the code
>
>lset( list( background = list(col = "white")))
>xyplot
>(
>         y ~ TIME , data = foo.frame,
>         scales = list(alternating = FALSE),
>         ylim = c(.75,y.max),
>         panel = function(x, y, panel.number, ... )
>         {
>         panel.superpose(x = foo.frame$TIME[foo.frame$group == 1],
>                 y = foo.frame$y[foo.frame$group == 1],
>                 subscripts = TRUE,
>                 groups = foo.frame$subject.id[foo.frame$group == 1],
>                 type = 'b',
>                 col = "black",
>                 lwd = 4,
>                 cex = 1.5)
>
>         panel.superpose(x = foo.frame$TIME[foo.frame$group == 0],
>                 y = foo.frame$y[foo.frame$group == 0],
>                 subscripts = TRUE,
>                 groups = foo.frame$subject.id[foo.frame$group == 0],
>                 type = 'l',
>                 col = gray(.6))
>
>         panel.loess(x = foo.frame$TIME,
>                 y = foo.frame$y, col = gray(.2),
>                 lwd = 3,
>                 lty = 5,
>                 span = 1/3)
>         }
>)
>
>I get the desired graph on the screen.  Basically,
>I am trying to display the time course of a set of test
>subjects, where each of two subsets is displayed in a
>different color, and a loess smoother is added to the plot
>for reference.
>
>When I add
>
>trellis.device(postscript,
>         file = "//.../foo_status.ps",
>         color = TRUE)
>
>to the beginning of this little script, upon trying to open the
>created file with Ghostview/Ghostscript, I get an error dialog
>box containing the error message in the attached text file and
>nothing is displayed.
>
>If I change to
>
>trellis.device(win.metafile,
>         file = "//.../foo_status.wmf",
>         width = 8.5, height = 6.25)
>
>I am able to view the created file, but there are unwanted
>vertical lines drawn at max(foo.frame$TIME).  Moreover, MSWord
>has a hard time rendering this graph in a stable way, as it
>is constantly redrawing it and freezing my screen.  The file
>is only 35K, so this should not happen.  I tried the same
>scripting options in R1.7.1 with the same results, which leads
>me to believe that my xyplot(..) code could be in error.
>OTOH, this is a fairly simple adaptation of some earlier
>working code and the within-RGui graph appears to be fine and
>doesn't have the unwanted vertical lines.  When I right-click
>on this graph and try to paste it directly into MSWord, I
>get the same "screen freeze" issues, and the vertical lines
>appear in the window.
>
>Any help would be greatly appreciated.
>
>
>-david paul
>
>
>______________________________________________
>R-help at stat.math.ethz.ch mailing list
>https://www.stat.math.ethz.ch/mailman/listinfo/r-help



From tomhopper at comcast.net  Sun Oct 26 21:12:33 2003
From: tomhopper at comcast.net (Tom Hopper)
Date: Sun, 26 Oct 2003 15:12:33 -0500
Subject: [R] Complete Newbie Q
In-Reply-To: <16282.29664.253465.971666@gargle.gargle.HOWL>
References: <5.2.1.1.1.20031024181146.00c26988@cucafera.icm.csic.es>
	<5.2.1.1.2.20031024093551.03f6e3d8@poptop.llnl.gov>
	<16282.29664.253465.971666@gargle.gargle.HOWL>
Message-ID: <BBF3B757-07F0-11D8-BB2E-003065410366@comcast.net>

-----BEGIN PGP MESSAGE-----
Version: GnuPG v1.2.3 (Darwin)

jA0EAgMCYon/X4MtApNgycC4LBI2TzRttoGFKXA7qYYfDYeCz83m+PloRQgnwt1k
tQgI3Dro/JUPhgnXLpvdlK6xDiioyW6FTbCniARPy61Bnr7AVW4+SKz2sDkuuKXT
Z2SEuKLcHV20+cxMJRzWpaRl7r0vBrSdUUuFim5bhEGS14szCiE/Y+G1oCivhfP/
AU16Z0fKPbHduG+oEQIqfK0ps838cguI5BG3FaAJ+T4leSO7+EboZe3meLUbaWty
BQ9pf9wGgCMtofOYc8a3ZpSeP9TGOab3waXe0uumisJ6U3p7tjO5L4BlGY690DbN
1Vm3ixhVDQQ7z9QOfBspEiLw0zo6sikFecC9Kla0PRMombOmJtbR+/150T5O2f3R
vv07vvjT5AssWs0DVbt4xjv4Pffp0Km7iVRlpe5bdEBH9ILORhgOJJYadlhnA5Ug
p73y1AwBTgpJgINpDyWjYUpzksESwGI3nDYKo7iPtBx7iGPxgYoijr7kJDk2yhSk
pYMSHRKnfoi0GA==
=hlJT
-----END PGP MESSAGE-----



From tomhopper at comcast.net  Sun Oct 26 22:04:34 2003
From: tomhopper at comcast.net (Tom Hopper)
Date: Sun, 26 Oct 2003 16:04:34 -0500
Subject: [R] Complete Newbie Q
In-Reply-To: <BBF3B757-07F0-11D8-BB2E-003065410366@comcast.net>
References: <5.2.1.1.1.20031024181146.00c26988@cucafera.icm.csic.es>
	<5.2.1.1.2.20031024093551.03f6e3d8@poptop.llnl.gov>
	<16282.29664.253465.971666@gargle.gargle.HOWL>
	<BBF3B757-07F0-11D8-BB2E-003065410366@comcast.net>
Message-ID: <006F40F9-07F8-11D8-BB2E-003065410366@comcast.net>

As the subject suggests, I'm new to R. I'm looking to move my 
statistical and graphing  from Excel to a more capable mathematical and 
graphing engine, and R looks to be a good choice.

My question is this: is there a quick-start guide or introductory 
tutorial for someone with my background? While there seems to be quite 
a lot of documentation on R, it all seems directed at people with a 
different background. It's too basic, or too advanced, but doesn't 
address someone reasonably familiar with statistics who just needs to 
change his work habits.

Thanks,

Tom

P.S. Sorry about that last message. I just upgraded my system, and seem 
to have inadvertently altered one of my email settings in the process.



From kwan022 at stat.auckland.ac.nz  Sun Oct 26 22:50:17 2003
From: kwan022 at stat.auckland.ac.nz (Ko-Kang Kevin Wang)
Date: Mon, 27 Oct 2003 10:50:17 +1300 (NZDT)
Subject: [R] Complete Newbie Q
In-Reply-To: <006F40F9-07F8-11D8-BB2E-003065410366@comcast.net>
Message-ID: <Pine.LNX.4.44.0310271045220.13780-100000@stat61.stat.auckland.ac.nz>

On Sun, 26 Oct 2003, Tom Hopper wrote:

> My question is this: is there a quick-start guide or introductory 
> tutorial for someone with my background? While there seems to be quite 
> a lot of documentation on R, it all seems directed at people with a 
> different background. It's too basic, or too advanced, but doesn't 
> address someone reasonably familiar with statistics who just needs to 
> change his work habits.

Not knowing your exact background, it is difficult to suggest one.  I'd 
say that An Introduction to R manual from CRAN is a good start.  
Alternatively, Venables and Ripley's Modern Applied Statistics with S is 
an excellent source -- perhaps the best in the field.  Peter Dalgaard's 
Introductory Statistics with R may also be a good choice.

At the moment, Steven Miller and I are planning on co-authoring a book on 
using R, targetted at experience statisticians who have little or no 
experience with R.  The current ideas are at 
http://www.stat.auckland.ac.nz/~kwan022/pub/R/RBook/ .  We are planning on 
finishing it by Feb/March 2004 -- which may be too late for you.  Once it 
is finished we will submit it to CRAN.

-- 
Cheers,

Kevin

---------------------------------------------------------------
Ko-Kang Kevin Wang
Master of Science (MSc) Student
SLC Tutor and Lab Demonstrator
Department of Statistics
University of Auckland
New Zealand
Homepage: http://www.stat.auckland.ac.nz/~kwan022
Ph: 373-7599
    x88475 (City)
    x88480 (Tamaki)



From baud-bovy.gabriel at hsr.it  Mon Oct 27 01:55:40 2003
From: baud-bovy.gabriel at hsr.it (Gabriel Baud-Bovy)
Date: Mon, 27 Oct 2003 01:55:40 +0100
Subject: [R] Difficulties with R.oo (static fields, etc.)
Message-ID: <5.2.1.1.1.20031026185540.00b3a170@mail.hsr.it>

I would like to use R.oo and tcltk to implement a Turtle World. I have 
encountered
many problems because:

1) I am not sure how to implement static fields with R.oo
2) I am not sure how to implement a constructor that would
     call a function only for the first instance of a class (i.e., to 
initialize
     value of static fields only once)
3) I am not sure how to remove/delete cleanly existing instances. In 
particular,
     I don't know how to reset tcktk when the last instance is deleted.

Here is the sort of behavior I would like to get:

t1<-TurtleBasic()   # a window (the Turtle world) with a turtle appears
forward(t1,50)       #  the turtle moves (turtle's state is also updated)
turn(t1,pi/2)
forward(t1,50)
t2<-TurtleBasic()  # a second turtle in the Turtle world appears
turn(t2,pi/2)  # second turtle moves
forward(t1,50)
delete(t1) # first turtle disappears
delete(t2) # second turtle and the world disappears

If possible, I would like to keep this syntax. In fact, I am using R.oo to
create mutable objects so as to avoid syntax like  tu<-forward(tu,10) which
would be more typical of a functional language like R. From the doc/help 
files of R.oo,
I believe that one could do it but I don't find many examples to guide me.

I pasted my current code with comments below. Thank you for your help.

Gabriel Baud-Bovy

#---------------------------------------------------------------------------------------------------------------------

Description of Turtle Basic Class:

private fields:
   .x, .y, .a (turtle position and heading)
   .turtle: ID of canvas item representing turtle
static fields:
   .canvas: tcltk canvas widget
   .top: tcktk toplevel widget
methods:
    TurtleBasic: constructor
    plot: display turtle in Turtle World
    forward, turn: move turtle
    delete: delete turtle (not implemented)

Note: My current code is very buggy:
  - Can't see turtle movements
  - Does not define method to delete turtles
  - I get an tcltk error when I try to recreate a turtle world after having 
tried to destroy it
    (for the moment, I need to detach tcltk to reset it)

#---------------------------------------------------------------------------------------------------------------------

library("R.oo")

setConstructorS3("TurtleBasic",function() {
   new<-extend(Object(),"TurtleBasic",.x=100,.y=100,.a=0,.turtle=NA)
   ## Create a new Turtle World if necessary
   require(tcltk) || stop("tcl/ library not available")
   if(!is.tkwin(new$.canvas)) { # check to see if it is first instance or not
     cat("Create Turtle World\n")
     top <- tktoplevel()
     tktitle(top) <- "Turtle World"
     canvas <- tkcanvas(top, relief="raised", width=200, height=200)
     tkpack(canvas, side="top", fill="both",expand="1")
     new$.canvas<-canvas 	 			# store canvas widget in static field
     new$.top<-top
   }
   plot.TurtleBasic(new)					# plot Turtle
   new
})

setMethodS3("plot","TurtleBasic",function(turtle) {
     ## delete old item (if it exists)
     if(is.tkwin(turtle$.canvas) && is.tclObj(turtle$.turtle)) 
tkdelete(turtle$.canvas,turtle$.turtle)
     ## a new canvas item representing the turtle is created
     ## (note: it's necessary to make new item because turtle's heading can 
change)
     x <- c(-10,10)
     y <- c(-10,10)
     aux<-   cos(turtle$.a)*x+sin(turtle$.a)*y + turtle$.x
     y  <-  -sin(turtle$.a)*x+cos(turtle$.a)*y + turtle$.y
     x  <- aux
     turtle$.turtle <- tkcreate(turtle$.canvas, "line", 
x[1],-y[1],x[2],-y[2],width=1)
})

setMethodS3("forward","TurtleBasic",function(turtle,length){
   aux       <- turtle$.x + length*cos(turtle$.a)
   turtle$.y <- turtle$.y + length*sin(turtle$.a)
   turtle$.x <- aux
   plot(turtle)
})

setMethodS3("turn","TurtleBasic",function(turtle,angle){
    turtle$.a<-turtle$.a+angle
    plot(turtle)
})


if(0) {

t1<-TurtleBasic()  # a window (the Turtle world) with a turtle appears
forward(t1,50)
turn(t1,pi/2)
forward(t1,50)
t2<-TurtleBasic()  # a second turtle in the Turtle world
turn(t2,pi/2)

# manually destroy first turtle (would be great to define some sort of 
method for it)
tkdelete(t1$.canvas,t1$.turtle)
rm(t1)
# destroy second turtle and world
tkdelete(t2$.canvas,t2$.turtle)
tkdestroy(t2$.top)
rm(t2)

}
--------------------------------------------------------------------
Gabriel Baud-Bovy
Assistant Professor
UHSR University
via Olgettina, 58	tel:  (+39) 02 2643 4839
20132 Milan, Italy	fax: (+39) 02 2643 4892



From Robert.King at newcastle.edu.au  Mon Oct 27 04:47:44 2003
From: Robert.King at newcastle.edu.au (Robert King)
Date: Mon, 27 Oct 2003 14:47:44 +1100
Subject: [R] R windows development environment via Novel Application	Launcher
Message-ID: <sf9d3023.063@MC-GWDOM2.newcastle.edu.au>

Hello List,

I'm preparing to teach a class on statistical computing, which will include a component where students develop an R package.  Has anybody tried putting the R windows requirements together and providing them in a Novel Application Launcher environment?  

We're currently providing R on its own in such an environment, with some problems still remaining with our implementation.  So, I'm interested in hearing from anyone with experience using the NAL for serving R to a windows lab.

Thanks in advance,
Robert King,
University of Newcastle



From seniorr at aracnet.com  Mon Oct 27 07:09:05 2003
From: seniorr at aracnet.com (Russell Senior)
Date: 26 Oct 2003 22:09:05 -0800
Subject: [R] variance component analysis for nested model
Message-ID: <86fzhfus8u.fsf@coulee.tdb.com>


Given a set of data:

> names(data)
[1] "city"     "house"     "visit"    "value"

I am looking for a way to compute the variance components of the
nested model (ie, visit 1 at house 2 at city 3 isn't related to visit
1 and house 2 at city 4), but different houses in the same city may be
related, and different visits to the same house are probably related.
I want to be able to compute how much of the total variance of "value"
is explained by each of these.  How can I do that in R?

Thanks!

-- 
Russell Senior         ``shtal latta wos ba padre u prett tu nashtonfi
seniorr at aracnet.com      mrlosh''  -- Bashgali Kafir for ``If you have
                         had diarrhoea many days you will surely die.''



From hb at maths.lth.se  Mon Oct 27 08:22:56 2003
From: hb at maths.lth.se (Henrik Bengtsson)
Date: Mon, 27 Oct 2003 08:22:56 +0100
Subject: [R] Difficulties with R.oo (static fields, etc.)
In-Reply-To: <5.2.1.1.1.20031026185540.00b3a170@mail.hsr.it>
Message-ID: <000801c39c5b$257e7f70$0c0040d5@maths.lth.se>

This is not really an rhelp question. I'll get back to you later today
with answers related to R.oo.

Henrik Bengtsson
The R.oo author

> -----Original Message-----
> From: r-help-bounces at stat.math.ethz.ch 
> [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of 
> Gabriel Baud-Bovy
> Sent: den 27 oktober 2003 01:56
> To: r-help at stat.math.ethz.ch
> Subject: [R] Difficulties with R.oo (static fields, etc.)
> 
> 
> I would like to use R.oo and tcltk to implement a Turtle 
> World. I have 
> encountered
> many problems because:
> 
> 1) I am not sure how to implement static fields with R.oo
> 2) I am not sure how to implement a constructor that would
>      call a function only for the first instance of a class (i.e., to 
> initialize
>      value of static fields only once)
> 3) I am not sure how to remove/delete cleanly existing instances. In 
> particular,
>      I don't know how to reset tcktk when the last instance 
> is deleted.
> 
> Here is the sort of behavior I would like to get:
> 
> t1<-TurtleBasic()   # a window (the Turtle world) with a 
> turtle appears
> forward(t1,50)       #  the turtle moves (turtle's state is 
> also updated)
> turn(t1,pi/2)
> forward(t1,50)
> t2<-TurtleBasic()  # a second turtle in the Turtle world appears
> turn(t2,pi/2)  # second turtle moves
> forward(t1,50)
> delete(t1) # first turtle disappears
> delete(t2) # second turtle and the world disappears
> 
> If possible, I would like to keep this syntax. In fact, I am 
> using R.oo to create mutable objects so as to avoid syntax 
> like  tu<-forward(tu,10) which would be more typical of a 
> functional language like R. From the doc/help 
> files of R.oo,
> I believe that one could do it but I don't find many examples 
> to guide me.
> 
> I pasted my current code with comments below. Thank you for your help.
> 
> Gabriel Baud-Bovy
> 
> #-------------------------------------------------------------
> --------------------------------------------------------
> 
> Description of Turtle Basic Class:
> 
> private fields:
>    .x, .y, .a (turtle position and heading)
>    .turtle: ID of canvas item representing turtle
> static fields:
>    .canvas: tcltk canvas widget
>    .top: tcktk toplevel widget
> methods:
>     TurtleBasic: constructor
>     plot: display turtle in Turtle World
>     forward, turn: move turtle
>     delete: delete turtle (not implemented)
> 
> Note: My current code is very buggy:
>   - Can't see turtle movements
>   - Does not define method to delete turtles
>   - I get an tcltk error when I try to recreate a turtle 
> world after having 
> tried to destroy it
>     (for the moment, I need to detach tcltk to reset it)
> 
> #-------------------------------------------------------------
> --------------------------------------------------------
> 
> library("R.oo")
> 
> setConstructorS3("TurtleBasic",function() {
>    new<-extend(Object(),"TurtleBasic",.x=100,.y=100,.a=0,.turtle=NA)
>    ## Create a new Turtle World if necessary
>    require(tcltk) || stop("tcl/ library not available")
>    if(!is.tkwin(new$.canvas)) { # check to see if it is first 
> instance or not
>      cat("Create Turtle World\n")
>      top <- tktoplevel()
>      tktitle(top) <- "Turtle World"
>      canvas <- tkcanvas(top, relief="raised", width=200, height=200)
>      tkpack(canvas, side="top", fill="both",expand="1")
>      new$.canvas<-canvas 	 			# store 
> canvas widget in static field
>      new$.top<-top
>    }
>    plot.TurtleBasic(new)					
> # plot Turtle
>    new
> })
> 
> setMethodS3("plot","TurtleBasic",function(turtle) {
>      ## delete old item (if it exists)
>      if(is.tkwin(turtle$.canvas) && is.tclObj(turtle$.turtle)) 
> tkdelete(turtle$.canvas,turtle$.turtle)
>      ## a new canvas item representing the turtle is created
>      ## (note: it's necessary to make new item because 
> turtle's heading can 
> change)
>      x <- c(-10,10)
>      y <- c(-10,10)
>      aux<-   cos(turtle$.a)*x+sin(turtle$.a)*y + turtle$.x
>      y  <-  -sin(turtle$.a)*x+cos(turtle$.a)*y + turtle$.y
>      x  <- aux
>      turtle$.turtle <- tkcreate(turtle$.canvas, "line", 
> x[1],-y[1],x[2],-y[2],width=1)
> })
> 
> setMethodS3("forward","TurtleBasic",function(turtle,length){
>    aux       <- turtle$.x + length*cos(turtle$.a)
>    turtle$.y <- turtle$.y + length*sin(turtle$.a)
>    turtle$.x <- aux
>    plot(turtle)
> })
> 
> setMethodS3("turn","TurtleBasic",function(turtle,angle){
>     turtle$.a<-turtle$.a+angle
>     plot(turtle)
> })
> 
> 
> if(0) {
> 
> t1<-TurtleBasic()  # a window (the Turtle world) with a turtle appears
> forward(t1,50)
> turn(t1,pi/2)
> forward(t1,50)
> t2<-TurtleBasic()  # a second turtle in the Turtle world
> turn(t2,pi/2)
> 
> # manually destroy first turtle (would be great to define 
> some sort of 
> method for it)
> tkdelete(t1$.canvas,t1$.turtle)
> rm(t1)
> # destroy second turtle and world
> tkdelete(t2$.canvas,t2$.turtle)
> tkdestroy(t2$.top)
> rm(t2)
> 
> }
> --------------------------------------------------------------------
> Gabriel Baud-Bovy
> Assistant Professor
> UHSR University
> via Olgettina, 58	tel:  (+39) 02 2643 4839
> 20132 Milan, Italy	fax: (+39) 02 2643 4892
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list 
> https://www.stat.math.ethz.ch/mailma> n/listinfo/r-help
> 
>



From Pascal.Niklaus at unibas.ch  Mon Oct 27 10:31:46 2003
From: Pascal.Niklaus at unibas.ch (Pascal A. Niklaus)
Date: Mon, 27 Oct 2003 10:31:46 +0100
Subject: [R] variance component analysis for nested model
In-Reply-To: <86fzhfus8u.fsf@coulee.tdb.com>
References: <86fzhfus8u.fsf@coulee.tdb.com>
Message-ID: <3F9CE602.3060503@unibas.ch>

lme should do the job (r1,r2,r3 are your random factors):

    library(nlme)
    y.lme <- lme(y ~ 1,random = ~ 1 | r1/r2/r3)
    summary(y.lme)

This is equivalent to a call to varcomp in S-Plus

Pascal

-- 

Dr. Pascal A. Niklaus
Institute of Botany
University of Basel
Sch?nbeinstrasse 6
CH-4056 Basel / Switzerland


Russell Senior wrote:

>Given a set of data:
>
>  
>
>>names(data)
>>    
>>
>[1] "city"     "house"     "visit"    "value"
>
>I am looking for a way to compute the variance components of the
>nested model (ie, visit 1 at house 2 at city 3 isn't related to visit
>1 and house 2 at city 4), but different houses in the same city may be
>related, and different visits to the same house are probably related.
>I want to be able to compute how much of the total variance of "value"
>is explained by each of these.  How can I do that in R?
>
>Thanks!
>
>
>  
>



From ripley at stats.ox.ac.uk  Mon Oct 27 09:26:46 2003
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Mon, 27 Oct 2003 08:26:46 +0000 (GMT)
Subject: [R] variance component analysis for nested model
In-Reply-To: <86fzhfus8u.fsf@coulee.tdb.com>
Message-ID: <Pine.LNX.4.44.0310270825170.32382-100000@gannet.stats>

On 26 Oct 2003, Russell Senior wrote:

> 
> Given a set of data:
> 
> > names(data)
> [1] "city"     "house"     "visit"    "value"
> 
> I am looking for a way to compute the variance components of the
> nested model (ie, visit 1 at house 2 at city 3 isn't related to visit
> 1 and house 2 at city 4), but different houses in the same city may be
> related, and different visits to the same house are probably related.
> I want to be able to compute how much of the total variance of "value"
> is explained by each of these.  How can I do that in R?

With lme or (if balanced) aov with an Error term.

There are lots of examples about, e.g. in the MASS and nlme scripts from 
the associated books.

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From wolfram at fischer-zim.ch  Mon Oct 27 10:50:02 2003
From: wolfram at fischer-zim.ch (Wolfram Fischer)
Date: Mon, 27 Oct 2003 10:50:02 +0100
Subject: [R] How can strheight be calculated in lattice/grid?
Message-ID: <20031027095002.GA3109@s1x.local>

If I have drawn a string with ``ltext( x, y, labels="first string" )''
how can a draw a second string just one line (or strheight("X")
below the first string regardless of the size and scales of the panel?

Thanks
Wolfram



From e.pebesma at geog.uu.nl  Mon Oct 27 10:56:57 2003
From: e.pebesma at geog.uu.nl (Edzer J. Pebesma)
Date: Mon, 27 Oct 2003 10:56:57 +0100
Subject: [R] Lattice: no grid name space
Message-ID: <3F9CEBE9.9070405@geog.uu.nl>

The following now occurs to me when I try to
load lattice (R 1.7.1, debian stable):

> library(lattice)
Error in loadNamespace(i, c(lib.loc, .libPaths()), keep.source) :
        package `grid' does not have a name space
Error in library(lattice) : package/namespace load failed

I did an update.packages() as root recently.
Any idea what's wrong?
--
Edzer



From ligges at statistik.uni-dortmund.de  Mon Oct 27 11:17:50 2003
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Mon, 27 Oct 2003 11:17:50 +0100
Subject: [R] Lattice: no grid name space
In-Reply-To: <3F9CEBE9.9070405@geog.uu.nl>
References: <3F9CEBE9.9070405@geog.uu.nl>
Message-ID: <3F9CF0CE.5080005@statistik.uni-dortmund.de>

Edzer J. Pebesma wrote:

> The following now occurs to me when I try to
> load lattice (R 1.7.1, debian stable):
> 
>> library(lattice)
> 
> Error in loadNamespace(i, c(lib.loc, .libPaths()), keep.source) :
>        package `grid' does not have a name space
> Error in library(lattice) : package/namespace load failed
> 
> I did an update.packages() as root recently.
> Any idea what's wrong?
> -- 
> Edzer

Yes. You got a version of lattice that is supposed to work with the grid 
version shipped with R-1.8.0 (and that one has got a namespace).

Uwe Ligges



From ripley at stats.ox.ac.uk  Mon Oct 27 11:19:55 2003
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Mon, 27 Oct 2003 10:19:55 +0000 (GMT)
Subject: [R] Lattice: no grid name space
In-Reply-To: <3F9CEBE9.9070405@geog.uu.nl>
Message-ID: <Pine.LNX.4.44.0310271016110.1187-100000@gannet.stats>

On Mon, 27 Oct 2003, Edzer J. Pebesma wrote:

> The following now occurs to me when I try to
> load lattice (R 1.7.1, debian stable):
> 
> > library(lattice)
> Error in loadNamespace(i, c(lib.loc, .libPaths()), keep.source) :
>         package `grid' does not have a name space
> Error in library(lattice) : package/namespace load failed
> 
> I did an update.packages() as root recently.
> Any idea what's wrong?

Yes, the current lattice requires 1.8.0.  Its Depends line is

Depends: R (>= 1.7.0), grid (>= 1.8.0), modreg

but only versions of R are checked by INSTALL (and that should be R 
(>=1.8.0) as grid is no longer available separately from R).

Time to upgrade to R 1.8.0?  If not, you need to find lattice_0.7.x in
CRAN's Archive section.

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From maechler at stat.math.ethz.ch  Mon Oct 27 11:25:14 2003
From: maechler at stat.math.ethz.ch (Martin Maechler)
Date: Mon, 27 Oct 2003 11:25:14 +0100
Subject: [R] Lattice: no grid name space
In-Reply-To: <3F9CEBE9.9070405@geog.uu.nl>
References: <3F9CEBE9.9070405@geog.uu.nl>
Message-ID: <16284.62090.113180.694077@gargle.gargle.HOWL>

>>>>> "Edzer" == Edzer J Pebesma <e.pebesma at geog.uu.nl>
>>>>>     on Mon, 27 Oct 2003 10:56:57 +0100 writes:

    Edzer> The following now occurs to me when I try to load
    Edzer> lattice (R 1.7.1, debian stable):

    >> library(lattice)
    Edzer> Error in loadNamespace(i, c(lib.loc, .libPaths()),
    Edzer> keep.source) : package `grid' does not have a name
    Edzer> space Error in library(lattice) : package/namespace
    Edzer> load failed

    Edzer> I did an update.packages() as root recently.  Any
    Edzer> idea what's wrong?  -- Edzer

I guess you have a too recent version of lattice for your "old"
version of R.



From vito.muggeo at giustizia.it  Mon Oct 27 11:55:18 2003
From: vito.muggeo at giustizia.it (Vito Muggeo)
Date: Mon, 27 Oct 2003 11:55:18 +0100
Subject: [R] using variables in obj$model
Message-ID: <014a01c39c78$d5b7b1a0$5c13070a@PROCGEN>

dear all,
for some reason I am intersted in updating a glm taking variables from its
model argument, namely:

> dati<-data.frame(y=runif(10),x=1:10)
> obj<-glm(y~x,data=dati)
> obj$model[,c("A","a:b")]<-cbind(rnorm(10),runif(10))
> names(obj$model)
[1] "y"   "x"   "A"   "a:b"
> update(obj,.~.+A,data=obj$model) #it works

Call:  glm(formula = y ~ x + A, data = obj$model)
............[SNIP]

> update(obj,.~.+A+a:b,data=obj$model) #it does not work
Error in eval(expr, envir, enclos) : Object "a" not found


Please, how can I solve this problem?
many thanks in avance,
best,
vito



From emmanuel.charpentier at sap.ap-hop-paris.fr  Mon Oct 27 12:23:13 2003
From: emmanuel.charpentier at sap.ap-hop-paris.fr (Emmanuel Charpentier)
Date: Mon, 27 Oct 2003 12:23:13 +0100
Subject: [R] Whitehead's group sequential procedures
Message-ID: <3F9D0021.6030401@sap.ap-hop-paris.fr>

Dear List,

I would like to know if there exist some R implementation of John 
Whitehead's procedures for the planning and analysis of group sequential 
clinical trials. His book is enlightenig but somewhat frustrating : it 
has a good basic exposition of his framework, but the technical details 
are sparse, and leave much work to do when one plans to reuimplement 
part of the procedures. For example, one has to work out the computation 
of test regions in non-trivial cases, the P-value and  confidence 
interval building, etc ... Given my abilities, I'd rather *not* do this ...

Did someone implement these procedures in R ? I am aware of a 
(commercial) implementation in S+, but Googling for an R implementation 
(and looking in the r-help archive) was ... well ... frustrating.

If no one did, and there is some interest, I might try to derive the 
necessary steps and reimplement this.

Sincerely,

					Emmanuel Charpentier

PS : please Cc: me your answers, since I'm not on the list and reading 
it through the archives.
-- 
Emmanuel Charpentier			Tel : +33-(0)1 40 27 35 98
Secr?tariat Scientifique du CEDIT	Fax : +33-(0)1 40 27 55 65
Assistance Publique - H?pitaux de Paris
3, Avenue Victoria, F-75100 Paris RP - France



From Joerg.Schaber at uv.es  Mon Oct 27 12:49:08 2003
From: Joerg.Schaber at uv.es (Joerg Schaber)
Date: Mon, 27 Oct 2003 12:49:08 +0100
Subject: [R] Oracle fetch problems
References: <200310271115.h9RB6ijY020243@hypatia.math.ethz.ch>
Message-ID: <3F9D0634.8000106@uv.es>

relating to my former messages concerning the strange fetch problems I 
have, I found out that 'fetch' fetches only every second row of the 
'native' SQL-results and fills up the rest of the rows with zeros.
Here a draft for the native SQL-query:

12:43:07 SQL>  SELECT OBS_DAY, OBS_YEAR, STAT_ID FROM WILD_PHENO_OBS 
WHERE PHASE_ID=7 AND OBS_YEAR BETWEEN 1951 AND 2000 order by stat_id, 
obs_year, obs_day;
PLEASE HIT RETURN TO CONTINUE

   OBS_DAY   OBS_YEAR    STAT_ID
---------- ---------- ----------
       133       1954   11110000
       140       1955   11110000
       147       1956   11110000
       133       1957   11110000
       146       1958   11110000
       118       1959   11110000
       132       1960   11110000
       120       1961   11110000
       138       1962   11110000
       144       1963   11110000

Here what fetches R:

 >  DBres <- dbSendQuery(DBcon, "SELECT OBS_DAY, OBS_YEAR, STAT_ID FROM 
WILD_PHENO_OBS WHERE PHASE_ID=7 AND OBS_YEAR BETWEEN 1951 AND 2000 order 
by stat_id, obs_year, obs_day")
 >  fetch(DBres,n=10)
  OBS_DAY OBS_YEAR  STAT_ID
0     140     1955 11110000
1     133     1957 11110000
2     118     1959 11110000
3     120     1961 11110000
4     144     1963 11110000
5       0        0 11110000
6       0        0 11110000
7       0        0 11110000
8       0        0 11110000
9       0        0 11110000
 >

Any idea what's the problem here?



From ripley at stats.ox.ac.uk  Mon Oct 27 13:03:44 2003
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Mon, 27 Oct 2003 12:03:44 +0000 (GMT)
Subject: [R] using variables in obj$model
In-Reply-To: <014a01c39c78$d5b7b1a0$5c13070a@PROCGEN>
Message-ID: <Pine.LNX.4.44.0310271202240.27788-100000@gannet.stats>

Model frames do not contain interactions, they contain variables.
`a:b' is not the name of a variable whereas A is.

On Mon, 27 Oct 2003, Vito Muggeo wrote:

> dear all,
> for some reason I am intersted in updating a glm taking variables from its
> model argument, namely:
> 
> > dati<-data.frame(y=runif(10),x=1:10)
> > obj<-glm(y~x,data=dati)
> > obj$model[,c("A","a:b")]<-cbind(rnorm(10),runif(10))
> > names(obj$model)
> [1] "y"   "x"   "A"   "a:b"
> > update(obj,.~.+A,data=obj$model) #it works
> 
> Call:  glm(formula = y ~ x + A, data = obj$model)
> ............[SNIP]
> 
> > update(obj,.~.+A+a:b,data=obj$model) #it does not work
> Error in eval(expr, envir, enclos) : Object "a" not found

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From ripley at stats.ox.ac.uk  Mon Oct 27 13:10:29 2003
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Mon, 27 Oct 2003 12:10:29 +0000 (GMT)
Subject: [R] Oracle fetch problems
In-Reply-To: <3F9D0634.8000106@uv.es>
Message-ID: <Pine.LNX.4.44.0310271204490.27788-100000@gannet.stats>

Could you at least tell us which R package(s) and versions you are using 
here?

You can access Oracle databases by ROracle and RODBC and perhaps other
ways.  If this is ROracle, it would be better to use the R-sig-DB list
(see https://www.stat.math.ethz.ch/mailman/listinfo) or talk directly to
the author.

On Mon, 27 Oct 2003, Joerg Schaber wrote:

> relating to my former messages concerning the strange fetch problems I 
> have, I found out that 'fetch' fetches only every second row of the 
> 'native' SQL-results and fills up the rest of the rows with zeros.
> Here a draft for the native SQL-query:
> 
> 12:43:07 SQL>  SELECT OBS_DAY, OBS_YEAR, STAT_ID FROM WILD_PHENO_OBS 
> WHERE PHASE_ID=7 AND OBS_YEAR BETWEEN 1951 AND 2000 order by stat_id, 
> obs_year, obs_day;
> PLEASE HIT RETURN TO CONTINUE
> 
>    OBS_DAY   OBS_YEAR    STAT_ID
> ---------- ---------- ----------
>        133       1954   11110000
>        140       1955   11110000
>        147       1956   11110000
>        133       1957   11110000
>        146       1958   11110000
>        118       1959   11110000
>        132       1960   11110000
>        120       1961   11110000
>        138       1962   11110000
>        144       1963   11110000
> 
> Here what fetches R:
> 
>  >  DBres <- dbSendQuery(DBcon, "SELECT OBS_DAY, OBS_YEAR, STAT_ID FROM 
> WILD_PHENO_OBS WHERE PHASE_ID=7 AND OBS_YEAR BETWEEN 1951 AND 2000 order 
> by stat_id, obs_year, obs_day")
>  >  fetch(DBres,n=10)
>   OBS_DAY OBS_YEAR  STAT_ID
> 0     140     1955 11110000
> 1     133     1957 11110000
> 2     118     1959 11110000
> 3     120     1961 11110000
> 4     144     1963 11110000
> 5       0        0 11110000
> 6       0        0 11110000
> 7       0        0 11110000
> 8       0        0 11110000
> 9       0        0 11110000
>  >
> 
> Any idea what's the problem here?
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> 
> 

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From parrinel at med.unibs.it  Mon Oct 27 13:36:52 2003
From: parrinel at med.unibs.it (parrinel@med.unibs.it)
Date: Mon, 27 Oct 2003 13:36:52 +0100
Subject: [R] I am looking for the barplot version of histbackback( Hmisc
	library ) TIA
Message-ID: <3F9D1F74.1879.F529B8@localhost>



From Thomas.Bock at ptb.de  Mon Oct 27 11:36:46 2003
From: Thomas.Bock at ptb.de (Thomas Bock)
Date: Mon, 27 Oct 2003 11:36:46 +0100
Subject: [R] (on) first value from nlm (non-finit value supplied by nlm)
Message-ID: <3F9CF53E.40202@ptb.de>

Dear R-helpers,

if someone is interested in (last night I dreamed):
  ...
SH <- -3.8
fn.2 <- function(p){
    for (i1 in ilong){
      ex[i1] <- (1E-19*p[1]*n*L*voigt(u,v,f[i1],foN,p[2],p[3])[[1]])
    }
 sum((log(tt)-ex)^2)
}
out.2 <-nlm(fn.2, p = c(SH,GG,GL), hessian = TRUE,
           steptol = 1e-6, iterlim = 1000,print.level=2)
SN <-  out.2$estimate[1]*(1E-19)
GGN <- out.2$estimate[2]
GLN <-  out.2$estimate[3]
...
which works:
  ...

iteration = 10
Parameter:
[1] -3.800003499  0.005181922  0.006242639
Function Value
[1] 1.068854
Gradient:
[1] -0.01561930  0.02665086 -0.01232618

Successive iterates within tolerance.
Current iterate is probably solution.
...

Another thing: 
Since I receive the R-help digest
I have this in my ~/.emacs:
 (custom-set-faces
'(font-lock-string-face ((t (:foreground "green3"))))
'(font-lock-keyword-face ((t (:foreground "#f939ff"))))
)
(defun highlight-R-help ()
  (interactive)
  (highlight-regexp "Message:" 'font-lock-keyword-face)
  (highlight-regexp "Date:" 'font-lock-string-face)
  (highlight-regexp "Subject:" 'font-lock-string-face)
  (highlight-regexp "From:" 'font-lock-string-face))

(defun unhighlight-R-help ()
  (interactive)
  (unhighlight-regexp  "Message:" )
  (unhighlight-regexp  "Date:" )
  (unhighlight-regexp  "Subject:")
  (unhighlight-regexp  "From:"))
;;;

regards
Thomas



From cristian at biometria.univr.it  Mon Oct 27 14:02:25 2003
From: cristian at biometria.univr.it (cristian@biometria.univr.it)
Date: Mon, 27 Oct 2003 14:02:25 +0100
Subject: [R] Query: IRR Confidence Intervals
Message-ID: <1067259745.3f9d1761d949c@biometria.univr.it>


Does anybody know a R routine to calculate exact Confidence Intervals for
Incidence Rate Ratio?

Thanks
Cristian


-------------------------------------------------
Biometria - biometria.univr.it



From H.RINNER at tirol.gv.at  Mon Oct 27 14:41:45 2003
From: H.RINNER at tirol.gv.at (RINNER Heinrich)
Date: Mon, 27 Oct 2003 14:41:45 +0100
Subject: AW: [R] Query: IRR Confidence Intervals
Message-ID: <C4D44AB4CB62D311BA6500041202E886031EE318@xms1.tirol.gv.at>

Hi Cristian,

I don't know about a R routine for exact CIs, but I found a function called
"ageadjust" using a gamma distribution approximation some time ago -take a
look at http://medepi.org/epitools/rfunctions/index.html.
That Webpage seems a bit outdated now, but the calculations given in the
ageadjust functions should still work, I think.

Maybe this helps;
regards
Heinrich.


> -----Urspr?ngliche Nachricht-----
> Von: cristian at biometria.univr.it [mailto:cristian at biometria.univr.it] 
> Gesendet: Montag, 27. Oktober 2003 14:02
> An: r-help at stat.math.ethz.ch
> Betreff: [R] Query: IRR Confidence Intervals
> 
> 
> 
> Does anybody know a R routine to calculate exact Confidence 
> Intervals for
> Incidence Rate Ratio?
> 
> Thanks
> Cristian
> 
> 
> -------------------------------------------------
> Biometria - biometria.univr.it
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
>



From rost at zf.jcu.cz  Mon Oct 27 14:47:04 2003
From: rost at zf.jcu.cz (Ing. Michael Rost)
Date: Mon, 27 Oct 2003 14:47:04 +0100
Subject: [R] Bioassays Yielding concentration-Mortality data
Message-ID: <004c01c39c90$ce185270$75a7d9a0@kmi07>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20031027/df6ff93a/attachment.pl

From maechler at stat.math.ethz.ch  Mon Oct 27 15:04:20 2003
From: maechler at stat.math.ethz.ch (Martin Maechler)
Date: Mon, 27 Oct 2003 15:04:20 +0100
Subject: [R] commenting demos
In-Reply-To: <000901c39b7c$96d1d5d0$4e02a8c0@olau>
References: <000901c39b7c$96d1d5d0$4e02a8c0@olau>
Message-ID: <16285.9700.407428.725616@gargle.gargle.HOWL>

>>>>> "Olivia" == Olivia Lau <olau at fas.harvard.edu>
>>>>>     on Sun, 26 Oct 2003 00:49:45 -0400 writes:

    Olivia> Hi, Is there a way to make demo() print comments
    Olivia> along with the code and output?

not so easily.  The same applies to  example(.).
Both demo(.) and example(.) end up calling source(.) which
itself relies on
 1)  parse(...)
 2)  eval(...)

parse(.) in R (currently) drops all comments, so they are "lost"
very early in source() and hence everything else that calls source().

Many have wanted a version of source() {and hence demo() and
example() and ....} that would be able to echo all comments; but
this is not really easy: Somehow you need a version of parse()
{think of multiline statements!} to do this properly.

    Olivia> For example, if I use something like
    Olivia> readline("..."), demo prints both readline("...")
    Olivia> and ...; as far as I can tell, this is also true of
    Olivia> print() and cat().  I just want students to stop and
    Olivia> think about every command in my demo scripts.

One way, not using demo() would be to ask them to read the R
script (e.g. in winedt or emacs) and *send* the commands to a
running R themselves.

But's that's a quite different approach {than demo()}.
Martin

Martin Maechler <maechler at stat.math.ethz.ch>	http://stat.ethz.ch/~maechler/
Seminar fuer Statistik, ETH-Zentrum  LEO C16	Leonhardstr. 27
ETH (Federal Inst. Technology)	8092 Zurich	SWITZERLAND
phone: x-41-1-632-3408		fax: ...-1228			<><



From spencer.graves at pdf.com  Mon Oct 27 15:24:43 2003
From: spencer.graves at pdf.com (Spencer Graves)
Date: Mon, 27 Oct 2003 06:24:43 -0800
Subject: [R] commenting demos
In-Reply-To: <16285.9700.407428.725616@gargle.gargle.HOWL>
References: <000901c39b7c$96d1d5d0$4e02a8c0@olau>
	<16285.9700.407428.725616@gargle.gargle.HOWL>
Message-ID: <3F9D2AAB.8070502@pdf.com>

Could one use "cat" for comments, as: 

      cat("this is a comment that may survive 'source'\n")

??
hope this helps.  spencer graves

Martin Maechler wrote:

>>>>>>"Olivia" == Olivia Lau <olau at fas.harvard.edu>
>>>>>>    on Sun, 26 Oct 2003 00:49:45 -0400 writes:
>>>>>>            
>>>>>>
>
>    Olivia> Hi, Is there a way to make demo() print comments
>    Olivia> along with the code and output?
>
>not so easily.  The same applies to  example(.).
>Both demo(.) and example(.) end up calling source(.) which
>itself relies on
> 1)  parse(...)
> 2)  eval(...)
>
>parse(.) in R (currently) drops all comments, so they are "lost"
>very early in source() and hence everything else that calls source().
>
>Many have wanted a version of source() {and hence demo() and
>example() and ....} that would be able to echo all comments; but
>this is not really easy: Somehow you need a version of parse()
>{think of multiline statements!} to do this properly.
>
>    Olivia> For example, if I use something like
>    Olivia> readline("..."), demo prints both readline("...")
>    Olivia> and ...; as far as I can tell, this is also true of
>    Olivia> print() and cat().  I just want students to stop and
>    Olivia> think about every command in my demo scripts.
>
>One way, not using demo() would be to ask them to read the R
>script (e.g. in winedt or emacs) and *send* the commands to a
>running R themselves.
>
>But's that's a quite different approach {than demo()}.
>Martin
>
>Martin Maechler <maechler at stat.math.ethz.ch>	http://stat.ethz.ch/~maechler/
>Seminar fuer Statistik, ETH-Zentrum  LEO C16	Leonhardstr. 27
>ETH (Federal Inst. Technology)	8092 Zurich	SWITZERLAND
>phone: x-41-1-632-3408		fax: ...-1228			<><
>
>______________________________________________
>R-help at stat.math.ethz.ch mailing list
>https://www.stat.math.ethz.ch/mailman/listinfo/r-help
>  
>



From tlumley at u.washington.edu  Mon Oct 27 15:43:03 2003
From: tlumley at u.washington.edu (Thomas Lumley)
Date: Mon, 27 Oct 2003 06:43:03 -0800 (PST)
Subject: [R] Bioassays Yielding concentration-Mortality data
In-Reply-To: <004c01c39c90$ce185270$75a7d9a0@kmi07>
References: <004c01c39c90$ce185270$75a7d9a0@kmi07>
Message-ID: <Pine.A41.4.58.0310270632490.161756@homer36.u.washington.edu>

On Mon, 27 Oct 2003, Ing. Michael Rost wrote:

> Dear all, I'm trying reproduce an example of bioassays Yielding
> Concentration-Mortality Data particularly control - adjustment model
> from book Bioassay of Entomopathogenic Microbes and Nematodes chapter 7
> with R.
>
> I used glm with family=binomial and link=probit, but I do not know how
> to implement parameter gamma (control mortality - mortality of the
> untreated control insect in this exaple) into model formula.
>
> Model in book:
> pi(x)=gamma+(1-gamma)F(alpha+beta log(x))

This is a generalised linear model but isn't one of the built-in link
functions.  You would have to modify one of the glm family objects to use
this link function, changing the components
linkfun:  the link function
linkinv: the inverse of the link function
mu.eta: the derivative of the inverse of the link function

As a starting point you probably want the family object returned by
binomial(link=probit).

	-thomas

> F....cumulative probability distribution function
> data:
> x=concentration
> n=number of insect at each run
> y=number of death among n in given batch run at given concentration
>
>
> Xmat<-data.frame(x=rep(c(0.01,0.1,1,10,100),2),n=rep(100,10),y=c(19,20,21,45,80,25,25,27,56,91))
>
>
>
> Reults from book (obtained from SAS) are
>
> intercept  -1.6597 , beta =0.5586, control mortality --- gamma =0.2172
>
> Thanks for any advice.
> Michael
>
> ---
> Odchozí zpráva neobsahuje viry.
> Zkontrolováno antivirovým systémem AVG (http://www.grisoft.cz).
> Verze: 6.0.530 / Virová báze: 325 - datum vydání: 22.10.2003
>
> 	[[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
>

Thomas Lumley			Assoc. Professor, Biostatistics
tlumley at u.washington.edu	University of Washington, Seattle



From magillb at sbcglobal.net  Mon Oct 27 15:57:08 2003
From: magillb at sbcglobal.net (Brett Magill)
Date: Mon, 27 Oct 2003 06:57:08 -0800 (PST)
Subject: [R] Rpy Import Error
Message-ID: <20031027145708.21697.qmail@web80512.mail.yahoo.com>

I am trying to install Rpy to test it out as an R
interface for a project that I am working on. 
However, I get the following error.  Any clues as to
what might be going on? I have alo tried RSPython, but
I gave up due to errors.  RSPython segfaults when
started from R and gives an error message I can;t
remember from python.  Thanks, Brett

 >>> import rpy
  Traceback (most recent call last):
    File "<stdin>", line 1, in ?
    File "/usr/lib/python2.2/site-packages/rpy.py",
line 24, in ?
  import _rpy
  ImportError:
/usr/lib/python2.2/site-packages/_rpymodule.so: 
      undefined symbol: jump_now
  >>>

I have R-1.8.0 installed as a shared library on a
linux 2.4.22 (slackware 9.1) system, compiled on my
machine with GCC 3.2.3.  I have the most recent
version (0.3.1) of Rpy and python 2.3.1.



From deepayan at stat.wisc.edu  Mon Oct 27 15:39:46 2003
From: deepayan at stat.wisc.edu (Deepayan Sarkar)
Date: Mon, 27 Oct 2003 08:39:46 -0600
Subject: [R] Lattice: no grid name space
In-Reply-To: <Pine.LNX.4.44.0310271016110.1187-100000@gannet.stats>
References: <Pine.LNX.4.44.0310271016110.1187-100000@gannet.stats>
Message-ID: <200310270839.47245.deepayan@stat.wisc.edu>

On Monday 27 October 2003 04:19, Prof Brian Ripley wrote:
> On Mon, 27 Oct 2003, Edzer J. Pebesma wrote:
> > The following now occurs to me when I try to
> >
> > load lattice (R 1.7.1, debian stable):
> > > library(lattice)
> >
> > Error in loadNamespace(i, c(lib.loc, .libPaths()), keep.source) :
> >         package `grid' does not have a name space
> > Error in library(lattice) : package/namespace load failed
> >
> > I did an update.packages() as root recently.
> > Any idea what's wrong?
>
> Yes, the current lattice requires 1.8.0.  Its Depends line is
>
> Depends: R (>= 1.7.0), grid (>= 1.8.0), modreg

Yes, I now realize it was somewhat stupid of me to do this. Sorry about that.

Deepayan

> but only versions of R are checked by INSTALL (and that should be R
> (>=1.8.0) as grid is no longer available separately from R).
>
> Time to upgrade to R 1.8.0?  If not, you need to find lattice_0.7.x in
> CRAN's Archive section.



From Isabelle.ZABALZA-MEZGHANI at ifp.fr  Mon Oct 27 16:05:52 2003
From: Isabelle.ZABALZA-MEZGHANI at ifp.fr (ZABALZA-MEZGHANI Isabelle)
Date: Mon, 27 Oct 2003 16:05:52 +0100
Subject: [R] Starting and Terminating the JVM for package  SJava
Message-ID: <488C02265C6AD611BF200002A542182F03F2A7D1@irnts22.ifp.fr>

Hello,

I would like to know if there is a possibility to open an R session via Java
(using the SJava package), then to terminate it, and re-run another.
It seems not to be possible. If this is the case, I would like to understand
where is the problem or the limitation (is it due to the SJava
implementation, to the Java behavior, or to the R application).
In fact, I am interesting in re-starting new R sessions during a same Java
session to manage memory problems in R with large datasets and numerous
commands through SJava interface (just to "clean" memory).

Waiting for your help,

Regards,

Isabelle.


Isabelle Zabalza-Mezghani
IFP - Reservoir Engineering Department
Rueil-Malmaison / France
Tel : +33 1 47 52 61 99



From deepayan at stat.wisc.edu  Mon Oct 27 15:56:50 2003
From: deepayan at stat.wisc.edu (Deepayan Sarkar)
Date: Mon, 27 Oct 2003 08:56:50 -0600
Subject: [R] How can strheight be calculated in lattice/grid?
In-Reply-To: <20031027095002.GA3109@s1x.local>
References: <20031027095002.GA3109@s1x.local>
Message-ID: <200310270856.50451.deepayan@stat.wisc.edu>


On Monday 27 October 2003 03:50, Wolfram Fischer wrote:
> If I have drawn a string with ``ltext( x, y, labels="first string" )''
> how can a draw a second string just one line (or strheight("X")
> below the first string regardless of the size and scales of the panel?

No reliable (that is, documented and guaranteed not to change) way I can think 
of. But in this situation you should directly use grid.text instead. e.g., 

xyplot(1 ~ 1,
       panel = function(x, y, ...) {
           grid.text("first string", unit(x, "native"), unit(y, "native"))
           grid.text("second string", unit(x, "native"),
                     unit(y, "native") - unit(1, "lines"))
       })

There are several other useful 'unit' systems you can use concurrently in 
grid, see ?unit for details.

(In the default scenario, you can replace the first grid.text call by ltext, 
but they could potentially be different because ltext honours some trellis 
settings which grid.text doesn't.)

Deepayan



From ryszard.czerminski at pharma.novartis.com  Mon Oct 27 16:25:37 2003
From: ryszard.czerminski at pharma.novartis.com (ryszard.czerminski@pharma.novartis.com)
Date: Mon, 27 Oct 2003 10:25:37 -0500
Subject: [R] how to select random rows ?
Message-ID: <OFA38E1B40.7980CD0C-ON85256DCC.0052D78C-85256DCC.0054D25C@EU.novartis.net>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20031027/3758bbb2/attachment.pl

From edd at debian.org  Mon Oct 27 16:28:12 2003
From: edd at debian.org (Dirk Eddelbuettel)
Date: Mon, 27 Oct 2003 09:28:12 -0600
Subject: [R] Rpy Import Error
In-Reply-To: <20031027145708.21697.qmail@web80512.mail.yahoo.com>
References: <20031027145708.21697.qmail@web80512.mail.yahoo.com>
Message-ID: <20031027152812.GA30664@sonny.eddelbuettel.com>

On Mon, Oct 27, 2003 at 06:57:08AM -0800, Brett Magill wrote:
> I am trying to install Rpy to test it out as an R
> interface for a project that I am working on. 
> However, I get the following error.  Any clues as to
> what might be going on? I have alo tried RSPython, but
> I gave up due to errors.  RSPython segfaults when
> started from R and gives an error message I can;t
> remember from python.  Thanks, Brett
> 
>  >>> import rpy
>   Traceback (most recent call last):
>     File "<stdin>", line 1, in ?
>     File "/usr/lib/python2.2/site-packages/rpy.py",
> line 24, in ?
>   import _rpy
>   ImportError:
> /usr/lib/python2.2/site-packages/_rpymodule.so: 
>       undefined symbol: jump_now
>   >>>

RPy has not been updated to keep up with changes in R.

--- rpy-0.3.1.orig/src/RPy.h
+++ rpy-0.3.1/src/RPy.h
@@ -90,7 +90,8 @@
 PyOS_sighandler_t python_sigint;
 
 /* R function for jumping to toplevel context */
-extern void jump_now(void);
+/* extern void jump_now(void); */
+extern void Rf_onintr(void); 
 
 /* Global interpreter */
 PyInterpreterState *my_interp;
--- rpy-0.3.1.orig/src/R_eval.c
+++ rpy-0.3.1/src/R_eval.c
@@ -65,7 +65,8 @@
 void interrupt_R(int signum)
 {
   interrupted = 1;
-  jump_now();
+  /* jump_now(); */
+  Rf_onintr();
 }
 
 

I have not tested this extensively, though.

Dirk

-- 
Those are my principles, and if you don't like them... well, I have others.
                                                -- Groucho Marx



From gavin.simpson at ucl.ac.uk  Mon Oct 27 16:37:50 2003
From: gavin.simpson at ucl.ac.uk (Gavin Simpson)
Date: Mon, 27 Oct 2003 15:37:50 +0000
Subject: [R] problem using do.call and substitute for predict.glm using
	poly()
Message-ID: <3F9D3BCE.8040105@ucl.ac.uk>

Hi

I am having a particular problem with some glm models I am running. I 
have been adapting code from Bill Venables 'Programmers niche' in RNews 
Vol 2/2 to fit ca. 1000 glm models to a combination of species 0/1 data 
(as Y) and related physicochemical data (X), to automate the process of 
fitting this many models. I have successfully managed to fit all the 
models and have stored the results in a list, each list has 47 main 
'branches' (one for each species) and each branch has 23 'leaves' that 
each contain a glm object

But R throws up the following error:

Error in poly(Alk1, degree = 2, coefs = structure(list(alpha = 
c(37.7515662650602,  :
         Object "Alk1" not found

When trying to evaluate the following code:

pAsgn <- paste("predList[[i]][[n]] <- try(predict(resList$Y$X, newdata 
= 	data.frame(X = predData$X), type = 'response', se = TRUE))")
pAsgn <- parse(text = pAsgn)[[1]]
for (i in namY) {
     for (n in namX) {
         TAsgn <- do.call("substitute", list(pAsgn, list(n = n, i 
= i, X 			= as.name(n), Y = as.name(i))))
         eval(TAsgn)
     }
}

Alk1 is used above as an example, all 23 predictors are 'not found' 
depending on which part of the loop I'm in. Investigation of the 
predList object after this has run shows for example:

$Unk.nown$NCR
[1] "Error in poly(NCR, degree = 2, coefs = structure(list(alpha = 
c(218.156626506024,  : \n    Object \"NCR\" not found\n"
attr(,"class")
[1] "try-error"

pAsgn contains a parsed R expression:

predList[[i]][[n]] <- try(predict(resList$Y$X, newdata = 
data.frame(X = 		predData$X), type = "response", se = TRUE))

I think I have narrowed the problem down to the fact that the first X in 
newdata = data.frame(X = predData$X)... is not being substitute with the 
variable in question, where as all the other X and Y's are being 
substituted:
(n and i would be supplied by for loops (see above) so I have 
substituted  two values below as if they had been in the loop)

 > do.call("substitute", list(pAsgn, list(n = namX[1], i = 
namY[1], X = 			as.name(n), Y = as.name(i))))
predList[["Acr.harp"]][["Alk1"]] <- try(predict(resList$Unk.nown$NCR,
     newdata = data.frame(X = predData$NCR), type = "response",
     se = TRUE))        ^^^^^^ problem here

If i supply the values I want for one of the runs, such as:

 > predList[[1]][[1]] <- try(predict(resList$Acr.harp$Alk1, newdata = 
data.frame(Alk1 = predData$Alk1), type = "response", se = TRUE))

Then this works, so the question is, how to I get X to be substituted in 
the above call? Perhaps this is not the cause of the error, so if anyone 
else has other suggestions.

Thank you for help.

Gav

ps: version
platform i386-pc-mingw32
arch     i386
os       mingw32
system   i386, mingw32
status
major    1
minor    8.0
year     2003
month    10
day      08
language R
-- 
%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%
Gavin Simpson                     [T] +44 (0)20 7679 5522
ENSIS Research Fellow             [F] +44 (0)20 7679 7565
ENSIS Ltd. & ECRC                 [E] gavin.simpson at ucl.ac.uk
UCL Department of Geography       [W] http://www.ucl.ac.uk/~ucfagls/cv/
26 Bedford Way                    [W] http://www.ucl.ac.uk/~ucfagls/
London.  WC1H 0AP.
%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%



From andy_liaw at merck.com  Mon Oct 27 16:43:21 2003
From: andy_liaw at merck.com (Liaw, Andy)
Date: Mon, 27 Oct 2003 10:43:21 -0500
Subject: [R] how to select random rows ?
Message-ID: <3A822319EB35174CA3714066D590DCD50205CD4D@usrymx25.merck.com>

Like this:

a.subset <- a[sample(nrow(a), how.many.I.want), ]

HTH,
Andy

> -----Original Message-----
> From: ryszard.czerminski at pharma.novartis.com 
> [mailto:ryszard.czerminski at pharma.novartis.com] 
> Sent: Monday, October 27, 2003 10:26 AM
> To: r-help at stat.math.ethz.ch
> Subject: [R] how to select random rows ?
> 
> 
> How can I select random subsets (rows!) from a data set ?
> 
> If I generate simple data set
> 
> > a <- data.frame(x=1:2, y = NaN, z = 2:1)
> > a
>   x   y z
> 1 1 NaN 2
> 2 2 NaN 1
> 
> I can select random subsets (colums) very easily using sample 
> function:
> 
> > sample(a, 2)
>   z   y
> 1 2 NaN
> 2 1 NaN
> 
> I expected that using transpose of a would do the same for 
> rows, but I am 
> getting
> rather unexpected outcome
> 
> > sample(t(a), 1)
> <NA>
>    1
> 
> R
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list 
> https://www.stat.math.ethz.ch/mailman/listinfo> /r-help
>



From gu4 at llnl.gov  Mon Oct 27 16:55:50 2003
From: gu4 at llnl.gov (Pauline Gu)
Date: Mon, 27 Oct 2003 07:55:50 -0800
Subject: [R] neural networks
In-Reply-To: <16282.29664.253465.971666@gargle.gargle.HOWL>
References: <5.2.1.1.2.20031024093551.03f6e3d8@poptop.llnl.gov>
	<5.2.1.1.1.20031024181146.00c26988@cucafera.icm.csic.es>
	<5.2.1.1.2.20031024093551.03f6e3d8@poptop.llnl.gov>
Message-ID: <5.2.1.1.2.20031027074936.03f9ab10@poptop.llnl.gov>

Thanks so much, Martin and Andy for your help.  I understand it now.

Pauline

At 03:00 PM 10/25/2003 +0200, Martin Maechler wrote:
> >>>>> "Pauline" == Pauline Gu <gu4 at llnl.gov>
> >>>>>     on Fri, 24 Oct 2003 10:03:23 -0700 writes:
>
>     Pauline> Hello, experts, Does the number of input for
>     Pauline> training set need to be the same as the number of
>     Pauline> input for the data set that I am trying to predict
>     Pauline> from the nnet result of the training?
>
>If `` the number of input '' is what we usually call
>    "number of (explanatory) variables",
>then
>    the answer is "yes".
>
>If instead, you mean "number of cases", "number of
>    observations", or "number of (ex|s)amples"
>the answer is "no".
>
>Martin Maechler <maechler at stat.math.ethz.ch>    http://stat.ethz.ch/~maechler/
>Seminar fuer Statistik, ETH-Zentrum  LEO C16    Leonhardstr. 27
>ETH (Federal Inst. Technology)  8092 Zurich     SWITZERLAND
>phone: x-41-1-632-3408          fax: ...-1228                   <><



From hb at maths.lth.se  Mon Oct 27 16:55:16 2003
From: hb at maths.lth.se (Henrik Bengtsson)
Date: Mon, 27 Oct 2003 16:55:16 +0100
Subject: [R] how to select random rows ?
In-Reply-To: <OFA38E1B40.7980CD0C-ON85256DCC.0052D78C-85256DCC.0054D25C@EU.novartis.net>
Message-ID: <000001c39ca2$b706b150$e502eb82@maths.lth.se>

rowIdx <- sample(nrow(a), size=nbrOfSamples)
a[rowIdx,]

and/or

colIdx <- sample(ncol(a), size=nbrOfSamples)
a[,colIdx]

/Henrik Bengtsson

> -----Original Message-----
> From: r-help-bounces at stat.math.ethz.ch 
> [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of 
> ryszard.czerminski at pharma.novartis.com
> Sent: den 27 oktober 2003 16:26
> To: r-help at stat.math.ethz.ch
> Subject: [R] how to select random rows ?
> 
> 
> How can I select random subsets (rows!) from a data set ?
> 
> If I generate simple data set
> 
> > a <- data.frame(x=1:2, y = NaN, z = 2:1)
> > a
>   x   y z
> 1 1 NaN 2
> 2 2 NaN 1
> 
> I can select random subsets (colums) very easily using sample 
> function:
> 
> > sample(a, 2)
>   z   y
> 1 2 NaN
> 2 1 NaN
> 
> I expected that using transpose of a would do the same for 
> rows, but I am 
> getting
> rather unexpected outcome
> 
> > sample(t(a), 1)
> <NA>
>    1
> 
> R
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list 
> https://www.stat.math.ethz.ch/mailma> n/listinfo/r-help
> 
>



From uaca at alumni.uv.es  Mon Oct 27 17:42:36 2003
From: uaca at alumni.uv.es (uaca@alumni.uv.es)
Date: Mon, 27 Oct 2003 17:42:36 +0100
Subject: [R] assign a constant to a different column for each row
Message-ID: <20031027164236.GB10061@pusa.informat.uv.es>


Hi all

I want to assign a constant to a different column for each row

eg:

m[1,2] <- 0;
m[2,3] <- 0;
m[3,1] <- 0;
m[4,2] <- 0;
m[5,1] <- 0;
...

etc...
          
i've tried apply/tapply with no luck

and also the following

coefs <- rtt.abs[,5:8];
coefs.i <- coefs[] == 1;
coefs[coefs.i] <- 0;

wich results in

"matrix subscripts not allowed in replacement" 

error message, for wich I have not found any workaround

a trivial for() loop is not fast enough

any help would be greatly appreciated, (maybe a gun too) I'm disperated :-\

Thanks in advance

	Ulisses


                Debian GNU/Linux: a dream come true
-----------------------------------------------------------------------------
"Computers are useless. They can only give answers."            Pablo Picasso

--->	Visita http://www.valux.org/ para saber acerca de la	<---
--->	Asociaci?n Valenciana de Usuarios de Linux		<---



From mkondrin at hppi.troitsk.ru  Tue Oct 28 05:03:33 2003
From: mkondrin at hppi.troitsk.ru (M.Kondrin)
Date: Mon, 27 Oct 2003 20:03:33 -0800
Subject: [R] assign a constant to a different column for each row
In-Reply-To: <20031027164236.GB10061@pusa.informat.uv.es>
References: <20031027164236.GB10061@pusa.informat.uv.es>
Message-ID: <3F9DEA95.1070409@hppi.troitsk.ru>

uaca at alumni.uv.es wrote:
> Hi all
> 
> I want to assign a constant to a different column for each row
> 
> eg:
> 
> m[1,2] <- 0;
> m[2,3] <- 0;
> m[3,1] <- 0;
> m[4,2] <- 0;
> m[5,1] <- 0;
> ...
> 
> etc...
>           
> i've tried apply/tapply with no luck
> 
> and also the following
> 
> coefs <- rtt.abs[,5:8];
> coefs.i <- coefs[] == 1;
> coefs[coefs.i] <- 0;
> 
> wich results in
> 
> "matrix subscripts not allowed in replacement" 
> 
> error message, for wich I have not found any workaround
> 
> a trivial for() loop is not fast enough
> 
> any help would be greatly appreciated, (maybe a gun too) I'm disperated :-\
> 
> Thanks in advance
> 
> 	Ulisses
> 
> 
>                 Debian GNU/Linux: a dream come true
> -----------------------------------------------------------------------------
> "Computers are useless. They can only give answers."            Pablo Picasso
> 
> --->	Visita http://www.valux.org/ para saber acerca de la	<---
> --->	Asociaci?n Valenciana de Usuarios de Linux		<---
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> 
> 
 > cbind(c(1,2,3),c(4,5,6),c(6,7,8))->l
 > l
      [,1] [,2] [,3]
[1,]    1    4    6
[2,]    2    5    7
[3,]    3    6    8
 > l[1:2,1:3]<-4
 > l
      [,1] [,2] [,3]
[1,]    4    4    4
[2,]    4    4    4
[3,]    3    6    8
 >



From ripley at stats.ox.ac.uk  Mon Oct 27 18:15:26 2003
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Mon, 27 Oct 2003 17:15:26 +0000 (GMT)
Subject: [R] assign a constant to a different column for each row
In-Reply-To: <20031027164236.GB10061@pusa.informat.uv.es>
Message-ID: <Pine.LNX.4.44.0310271711480.28341-100000@gannet.stats>

m[1:5 + nrow(m)*c(2,3,1,2,1)] <- 0 if m is a matrix.  Remember you can 
index a matrix as a vector.

If m is a data frame (you didn't say what it is)  I would loop over 
columns (not rows) explicitly, since the code is going to do that 
implicitly.

On Mon, 27 Oct 2003 uaca at alumni.uv.es wrote:

> 
> Hi all
> 
> I want to assign a constant to a different column for each row
> 
> eg:
> 
> m[1,2] <- 0;
> m[2,3] <- 0;
> m[3,1] <- 0;
> m[4,2] <- 0;
> m[5,1] <- 0;
> ...
> 
> etc...
>           
> i've tried apply/tapply with no luck
> 
> and also the following
> 
> coefs <- rtt.abs[,5:8];
> coefs.i <- coefs[] == 1;
> coefs[coefs.i] <- 0;
> 
> wich results in
> 
> "matrix subscripts not allowed in replacement" 
> 
> error message, for wich I have not found any workaround
> 
> a trivial for() loop is not fast enough
> 
> any help would be greatly appreciated, (maybe a gun too) I'm disperated :-\

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From uaca at alumni.uv.es  Mon Oct 27 18:18:08 2003
From: uaca at alumni.uv.es (uaca@alumni.uv.es)
Date: Mon, 27 Oct 2003 18:18:08 +0100
Subject: [R] Re: assign a constant to a different column for each row
In-Reply-To: <20031027164236.GB10061@pusa.informat.uv.es>
References: <20031027164236.GB10061@pusa.informat.uv.es>
Message-ID: <20031027171807.GC10061@pusa.informat.uv.es>


Hi again

Thanks all for your fast! reply

regards

	Ulisses

                Debian GNU/Linux: a dream come true
-----------------------------------------------------------------------------
"Computers are useless. They can only give answers."            Pablo Picasso

--->	Visita http://www.valux.org/ para saber acerca de la	<---
--->	Asociaci?n Valenciana de Usuarios de Linux		<---



From lockwood at rand.org  Mon Oct 27 18:59:45 2003
From: lockwood at rand.org (J.R. Lockwood)
Date: Mon, 27 Oct 2003 12:59:45 -0500 (EST)
Subject: [R] expanding factor with NA
Message-ID: <Pine.LNX.4.33.0310271212230.3855-100000@penguin.rand.org>

I have a factor (with "n" observations and "k" levels), with only
"nobs" < n of the observations not missing.  I would like to produce a
(n x k) model matrix with treatment contrasts for this factor, with
rows of NAs placeholding the missing observations.  If I use
model.matrix() I get back a (nobs x k) matrix.  Is there an easy way
to get the (n x k) without carrying along a row ID and merging?
Thanks.

J.R. Lockwood
412-683-2300 x4941
lockwood at rand.org
http://www.rand.org/methodology/stat/members/lockwood/



From tlumley at u.washington.edu  Mon Oct 27 20:17:04 2003
From: tlumley at u.washington.edu (Thomas Lumley)
Date: Mon, 27 Oct 2003 11:17:04 -0800 (PST)
Subject: [R] problem using do.call and substitute for predict.glm using
	poly()
In-Reply-To: <3F9D3BCE.8040105@ucl.ac.uk>
References: <3F9D3BCE.8040105@ucl.ac.uk>
Message-ID: <Pine.A41.4.58.0310271108310.60166@homer31.u.washington.edu>

On Mon, 27 Oct 2003, Gavin Simpson wrote:

>
> But R throws up the following error:
>
> Error in poly(Alk1, degree = 2, coefs = structure(list(alpha =
> c(37.7515662650602,  :
>          Object "Alk1" not found
>
> When trying to evaluate the following code:
>
> pAsgn <- paste("predList[[i]][[n]] <- try(predict(resList$Y$X, newdata
> = 	data.frame(X = predData$X), type = 'response', se = TRUE))")
> pAsgn <- parse(text = pAsgn)[[1]]
> for (i in namY) {
>      for (n in namX) {
>          TAsgn <- do.call("substitute", list(pAsgn, list(n = n, i
> = i, X 			= as.name(n), Y = as.name(i))))
>          eval(TAsgn)
>      }
> }
>
> Alk1 is used above as an example, all 23 predictors are 'not found'
> depending on which part of the loop I'm in. Investigation of the
> predList object after this has run shows for example:
>
> $Unk.nown$NCR
> [1] "Error in poly(NCR, degree = 2, coefs = structure(list(alpha =
> c(218.156626506024,  : \n    Object \"NCR\" not found\n"
> attr(,"class")
> [1] "try-error"
>
> pAsgn contains a parsed R expression:
>
> predList[[i]][[n]] <- try(predict(resList$Y$X, newdata =
> data.frame(X = 		predData$X), type = "response", se = TRUE))
>
> I think I have narrowed the problem down to the fact that the first X in
> newdata = data.frame(X = predData$X)... is not being substitute with the
> variable in question, where as all the other X and Y's are being
> substituted:

Yes, that's right.

> (n and i would be supplied by for loops (see above) so I have
> substituted  two values below as if they had been in the loop)
>
>  > do.call("substitute", list(pAsgn, list(n = namX[1], i =
> namY[1], X = 			as.name(n), Y = as.name(i))))
> predList[["Acr.harp"]][["Alk1"]] <- try(predict(resList$Unk.nown$NCR,
>      newdata = data.frame(X = predData$NCR), type = "response",
>      se = TRUE))        ^^^^^^ problem here
>
> If i supply the values I want for one of the runs, such as:
>
>  > predList[[1]][[1]] <- try(predict(resList$Acr.harp$Alk1, newdata =
> data.frame(Alk1 = predData$Alk1), type = "response", se = TRUE))
>
> Then this works, so the question is, how to I get X to be substituted in
> the above call? Perhaps this is not the cause of the error, so if anyone
> else has other suggestions.


You should be able to use
   newdata=predData[n]
which would substitute to
   newdata=predData["Alk1"]
or even just
   newdata=predData

If you actually needed to substitute the tags you would (I think) need to
work with the parsed expression directly, which is possible but icky:

> names(pAsgn[[3]][[2]][[3]])[2]<-"Alk1"
> pAsgn
predList[[i]][[n]] <- try(predict(resList$Y$X, newdata = data.frame(Alk1 = predData$X),
    type = "response", se = TRUE))


	-thomas



From gavin.simpson at ucl.ac.uk  Mon Oct 27 20:27:23 2003
From: gavin.simpson at ucl.ac.uk (Gavin Simpson)
Date: Mon, 27 Oct 2003 19:27:23 +0000
Subject: [R] problem using do.call and substitute for predict.glm using
	poly()
In-Reply-To: <3F9D3BCE.8040105@ucl.ac.uk>
References: <3F9D3BCE.8040105@ucl.ac.uk>
Message-ID: <3F9D719B.3090700@ucl.ac.uk>

Dear List,

I think I have found the source of my problem in a reply from Thomas 
Lumley to a previous question on R-Help: 
http://www.r-project.org/nocvs/mail/r-help/2002/0586.html

My code is not working because substitute() does not substitute formal 
arguments to functions, and I guess the first X in data.frame(X = 
predData$X) is a formal argument to data.frame().

The recommended options are to use formals() or parse(deparse()) to 
achieve the required effect, but here I get a somewhat stuck.

I was wondering if anyone on the list could show my how to modify my 
existing code:

pAsgn <- paste("predList[[i]][[n]] <- try(predict(resList$Y$X, newdata 
= 	data.frame(X = predData$X), type = 'response', se = TRUE))")
pAsgn <- parse(text = pAsgn)[[1]]
for (i in namY) {
     for (n in namX) {
         TAsgn <- do.call("substitute", list(pAsgn, list(n = n, i = i, X
		= as.name(n), Y = as.name(i))))
         eval(TAsgn)
     }
}

so that data.frame(X = predData$X) is replaced with the value of X such 
that the output from do.call in the loop is something like:

predList[["Acr.harp"]][["Alk1"]] <- try(predict(resList$Acr.harp$Alk1,
     newdata = data.frame(Alk1 = predData$Alk1), type = "response",
     se = TRUE))

If anyone could point me in the right direction it would be most 
appreciated.

All the best

Gav

Gavin Simpson wrote:

> Hi
> 
> I am having a particular problem with some glm models I am running. I 
> have been adapting code from Bill Venables 'Programmers niche' in RNews 
> Vol 2/2 to fit ca. 1000 glm models to a combination of species 0/1 data 
> (as Y) and related physicochemical data (X), to automate the process of 
> fitting this many models. I have successfully managed to fit all the 
> models and have stored the results in a list, each list has 47 main 
> 'branches' (one for each species) and each branch has 23 'leaves' that 
> each contain a glm object
> 
> But R throws up the following error:
> 
> Error in poly(Alk1, degree = 2, coefs = structure(list(alpha = 
> c(37.7515662650602,  :
>         Object "Alk1" not found
> 
> When trying to evaluate the following code:
> 
> pAsgn <- paste("predList[[i]][[n]] <- try(predict(resList$Y$X, newdata 
> =     data.frame(X = predData$X), type = 'response', se = TRUE))")
> pAsgn <- parse(text = pAsgn)[[1]]
> for (i in namY) {
>     for (n in namX) {
>         TAsgn <- do.call("substitute", list(pAsgn, list(n = n, i = i, 
> X             = as.name(n), Y = as.name(i))))
>         eval(TAsgn)
>     }
> }
> 
> Alk1 is used above as an example, all 23 predictors are 'not found' 
> depending on which part of the loop I'm in. Investigation of the 
> predList object after this has run shows for example:
> 
> $Unk.nown$NCR
> [1] "Error in poly(NCR, degree = 2, coefs = structure(list(alpha = 
> c(218.156626506024,  : \n    Object \"NCR\" not found\n"
> attr(,"class")
> [1] "try-error"
> 
> pAsgn contains a parsed R expression:
> 
> predList[[i]][[n]] <- try(predict(resList$Y$X, newdata = data.frame(X 
> =         predData$X), type = "response", se = TRUE))
> 
> I think I have narrowed the problem down to the fact that the first X in 
> newdata = data.frame(X = predData$X)... is not being substitute with the 
> variable in question, where as all the other X and Y's are being 
> substituted:
> (n and i would be supplied by for loops (see above) so I have 
> substituted  two values below as if they had been in the loop)
> 
>  > do.call("substitute", list(pAsgn, list(n = namX[1], i = namY[1], X 
> =             as.name(n), Y = as.name(i))))
> predList[["Acr.harp"]][["Alk1"]] <- try(predict(resList$Unk.nown$NCR,
>     newdata = data.frame(X = predData$NCR), type = "response",
>     se = TRUE))        ^^^^^^ problem here
> 
> If i supply the values I want for one of the runs, such as:
> 
>  > predList[[1]][[1]] <- try(predict(resList$Acr.harp$Alk1, newdata = 
> data.frame(Alk1 = predData$Alk1), type = "response", se = TRUE))
> 
> Then this works, so the question is, how to I get X to be substituted in 
> the above call? Perhaps this is not the cause of the error, so if anyone 
> else has other suggestions.
> 
> Thank you for help.
> 
> Gav
> 
> ps: version
> platform i386-pc-mingw32
> arch     i386
> os       mingw32
> system   i386, mingw32
> status
> major    1
> minor    8.0
> year     2003
> month    10
> day      08
> language R

-- 
%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%
Gavin Simpson                     [T] +44 (0)20 7679 5522
ENSIS Research Fellow             [F] +44 (0)20 7679 7565
ENSIS Ltd. & ECRC                 [E] gavin.simpson at ucl.ac.uk
UCL Department of Geography       [W] http://www.ucl.ac.uk/~ucfagls/cv/
26 Bedford Way                    [W] http://www.ucl.ac.uk/~ucfagls/
London.  WC1H 0AP.
%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%



From gavin.simpson at ucl.ac.uk  Mon Oct 27 20:39:32 2003
From: gavin.simpson at ucl.ac.uk (Gavin Simpson)
Date: Mon, 27 Oct 2003 19:39:32 +0000
Subject: [R] problem using do.call and substitute for predict.glm using
	poly()
In-Reply-To: <Pine.A41.4.58.0310271108310.60166@homer31.u.washington.edu>
References: <3F9D3BCE.8040105@ucl.ac.uk>
	<Pine.A41.4.58.0310271108310.60166@homer31.u.washington.edu>
Message-ID: <3F9D7474.30807@ucl.ac.uk>

Dear Thomas,

Thank you for you reply and solution - it works perfectly! It hadn't 
occurred to me that a 1 column subset of a data frame was still a data 
frame and that therefore I didn't need to use data.frame() at all!

To the list as well, please ignore my second mail on this thread. I was 
trying to clarify my particular problem by focussing on what the problem 
actually was, without all the confusing information in the first post. 
It was not my intention to 'hassle' the list until someone provided me 
with an answer. As always, peoples' time and effort that goes into 
helping posters to this list solve their R-related problems is very much 
appreciated, and I for one am extremely grateful for the help I have 
received from the list over the past few years.

Many thanks,

Gav

Thomas Lumley wrote:

> On Mon, 27 Oct 2003, Gavin Simpson wrote:
> 
> 
>>But R throws up the following error:
>>
>>Error in poly(Alk1, degree = 2, coefs = structure(list(alpha =
>>c(37.7515662650602,  :
>>         Object "Alk1" not found
>>
>>When trying to evaluate the following code:
>>
>>pAsgn <- paste("predList[[i]][[n]] <- try(predict(resList$Y$X, newdata
>>= 	data.frame(X = predData$X), type = 'response', se = TRUE))")
>>pAsgn <- parse(text = pAsgn)[[1]]
>>for (i in namY) {
>>     for (n in namX) {
>>         TAsgn <- do.call("substitute", list(pAsgn, list(n = n, i
>>= i, X 			= as.name(n), Y = as.name(i))))
>>         eval(TAsgn)
>>     }
>>}
>>
>>Alk1 is used above as an example, all 23 predictors are 'not found'
>>depending on which part of the loop I'm in. Investigation of the
>>predList object after this has run shows for example:
>>
>>$Unk.nown$NCR
>>[1] "Error in poly(NCR, degree = 2, coefs = structure(list(alpha =
>>c(218.156626506024,  : \n    Object \"NCR\" not found\n"
>>attr(,"class")
>>[1] "try-error"
>>
>>pAsgn contains a parsed R expression:
>>
>>predList[[i]][[n]] <- try(predict(resList$Y$X, newdata =
>>data.frame(X = 		predData$X), type = "response", se = TRUE))
>>
>>I think I have narrowed the problem down to the fact that the first X in
>>newdata = data.frame(X = predData$X)... is not being substitute with the
>>variable in question, where as all the other X and Y's are being
>>substituted:
> 
> 
> Yes, that's right.
> 
> 
>>(n and i would be supplied by for loops (see above) so I have
>>substituted  two values below as if they had been in the loop)
>>
>> > do.call("substitute", list(pAsgn, list(n = namX[1], i =
>>namY[1], X = 			as.name(n), Y = as.name(i))))
>>predList[["Acr.harp"]][["Alk1"]] <- try(predict(resList$Unk.nown$NCR,
>>     newdata = data.frame(X = predData$NCR), type = "response",
>>     se = TRUE))        ^^^^^^ problem here
>>
>>If i supply the values I want for one of the runs, such as:
>>
>> > predList[[1]][[1]] <- try(predict(resList$Acr.harp$Alk1, newdata =
>>data.frame(Alk1 = predData$Alk1), type = "response", se = TRUE))
>>
>>Then this works, so the question is, how to I get X to be substituted in
>>the above call? Perhaps this is not the cause of the error, so if anyone
>>else has other suggestions.
> 
> 
> 
> You should be able to use
>    newdata=predData[n]
> which would substitute to
>    newdata=predData["Alk1"]
> or even just
>    newdata=predData
> 
> If you actually needed to substitute the tags you would (I think) need to
> work with the parsed expression directly, which is possible but icky:
> 
> 
>>names(pAsgn[[3]][[2]][[3]])[2]<-"Alk1"
>>pAsgn
> 
> predList[[i]][[n]] <- try(predict(resList$Y$X, newdata = data.frame(Alk1 = predData$X),
>     type = "response", se = TRUE))
> 
> 
> 	-thomas
> 
> 

-- 
%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%
Gavin Simpson                     [T] +44 (0)20 7679 5522
ENSIS Research Fellow             [F] +44 (0)20 7679 7565
ENSIS Ltd. & ECRC                 [E] gavin.simpson at ucl.ac.uk
UCL Department of Geography       [W] http://www.ucl.ac.uk/~ucfagls/cv/
26 Bedford Way                    [W] http://www.ucl.ac.uk/~ucfagls/
London.  WC1H 0AP.
%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%



From tblackw at umich.edu  Mon Oct 27 20:59:59 2003
From: tblackw at umich.edu (Thomas W Blackwell)
Date: Mon, 27 Oct 2003 14:59:59 -0500 (EST)
Subject: [R] expanding factor with NA
In-Reply-To: <Pine.LNX.4.33.0310271212230.3855-100000@penguin.rand.org>
References: <Pine.LNX.4.33.0310271212230.3855-100000@penguin.rand.org>
Message-ID: <Pine.SOL.4.58.0310271435060.22868@timepilot.gpcc.itd.umich.edu>


I would re-expand the model matrix by indexing its (nobs) rows
with a longer vector (of length n) containing the correspondence.
If there is only one term (say "Z") in the formula which contains
the problematic NAs, I would do (roughly)

ff  <- Y ~ Z	 	#  following the example in ?model.matrix
mat <- model.matrix(ff, model.frame(ff, data))[cumsum(!is.na(Z)), ]
mat[is.na(Z), ] <- NA

The second line above creates an  n x k  matrix in which each row
where Z has NA simply duplicates the last preceding non-NA row.
The third line above blanks out those duplicate rows by filling
them with NAs instead.  This simple strategy fails if Z[1] is NA.
I haven't time to think up a solution for that case, other than
permuting rows in the entire data set so that it doesn't happen.

HTH  -  tom blackwell  -  u michigan medical school  -  ann arbor  -

On Mon, 27 Oct 2003, J.R. Lockwood wrote:

> I have a factor (with "n" observations and "k" levels), with only
> "nobs" < n of the observations not missing.  I would like to produce a
> (n x k) model matrix with treatment contrasts for this factor, with
> rows of NAs placeholding the missing observations.  If I use
> model.matrix() I get back a (nobs x k) matrix.  Is there an easy way
> to get the (n x k) without carrying along a row ID and merging?
> Thanks.
>
> J.R. Lockwood
> 412-683-2300 x4941
> lockwood at rand.org
> http://www.rand.org/methodology/stat/members/lockwood/



From tblackw at umich.edu  Mon Oct 27 21:08:01 2003
From: tblackw at umich.edu (Thomas W Blackwell)
Date: Mon, 27 Oct 2003 15:08:01 -0500 (EST)
Subject: [R] expanding factor with NA
In-Reply-To: <Pine.LNX.4.33.0310271212230.3855-100000@penguin.rand.org>
References: <Pine.LNX.4.33.0310271212230.3855-100000@penguin.rand.org>
Message-ID: <Pine.SOL.4.58.0310271504380.22868@timepilot.gpcc.itd.umich.edu>

Perhaps a much simpler method (just thought of it) would be to set

options(na.action="na.pass")

before you start.  Or use  na.action=na.pass()  as an argument in
the call to  model.frame(), since that's where the problem begins.
See  help("na.omit"),  help("model.frame").

-  tom blackwell  -  u michigan medical school  -  ann arbor  -

On Mon, 27 Oct 2003, J.R. Lockwood wrote:

> I have a factor (with "n" observations and "k" levels), with only
> "nobs" < n of the observations not missing.  I would like to produce a
> (n x k) model matrix with treatment contrasts for this factor, with
> rows of NAs placeholding the missing observations.  If I use
> model.matrix() I get back a (nobs x k) matrix.  Is there an easy way
> to get the (n x k) without carrying along a row ID and merging?
> Thanks.
>
> J.R. Lockwood
> 412-683-2300 x4941
> lockwood at rand.org
> http://www.rand.org/methodology/stat/members/lockwood/



From andy_liaw at merck.com  Mon Oct 27 21:21:41 2003
From: andy_liaw at merck.com (Liaw, Andy)
Date: Mon, 27 Oct 2003 15:21:41 -0500
Subject: [R] expanding factor with NA
Message-ID: <3A822319EB35174CA3714066D590DCD50205CD56@usrymx25.merck.com>

Strangely (to me), just passing na.action=na.pass to model.matrix doesn't
work:

> f <- factor(rep(letters[1:3], 5))
> is.na(f[sample(15, 3)]) <- TRUE
> model.matrix(~f, data=model.frame(~f, na.action=na.pass))
   (Intercept) fb fc
1            1  0  0
2            1  1  0
3            1  0  1
4            1  0  0
5            1 NA NA
6            1  0  1
7            1  0  0
8            1 NA NA
9            1  0  1
10           1  0  0
11           1  1  0
12           1  0  1
13           1 NA NA
14           1  1  0
15           1  0  1
attr(,"assign")
[1] 0 1 1
attr(,"contrasts")
attr(,"contrasts")$f
[1] "contr.treatment"

> model.matrix(~f, na.action=na.pass)
   (Intercept) fb fc
1            1  0  0
2            1  1  0
3            1  0  1
4            1  0  0
6            1  0  1
7            1  0  0
9            1  0  1
10           1  0  0
11           1  1  0
12           1  0  1
14           1  1  0
15           1  0  1
attr(,"assign")
[1] 0 1 1
attr(,"contrasts")
attr(,"contrasts")$f
[1] "contr.treatment"

[OK, it's not so strange: na.action is not a documented argument for
model.matrix, and the call to model.frame in model.matrix.default does not
have ..., but shouldn't it?]

Andy

> -----Original Message-----
> From: Thomas W Blackwell [mailto:tblackw at umich.edu] 
> Sent: Monday, October 27, 2003 3:08 PM
> To: J.R. Lockwood
> Cc: r-help at stat.math.ethz.ch
> Subject: Re: [R] expanding factor with NA
> 
> 
> Perhaps a much simpler method (just thought of it) would be to set
> 
> options(na.action="na.pass")
> 
> before you start.  Or use  na.action=na.pass()  as an 
> argument in the call to  model.frame(), since that's where 
> the problem begins. See  help("na.omit"),  help("model.frame").
> 
> -  tom blackwell  -  u michigan medical school  -  ann arbor  -
> 
> On Mon, 27 Oct 2003, J.R. Lockwood wrote:
> 
> > I have a factor (with "n" observations and "k" levels), with only 
> > "nobs" < n of the observations not missing.  I would like 
> to produce a 
> > (n x k) model matrix with treatment contrasts for this factor, with 
> > rows of NAs placeholding the missing observations.  If I use
> > model.matrix() I get back a (nobs x k) matrix.  Is there an 
> easy way 
> > to get the (n x k) without carrying along a row ID and merging? 
> > Thanks.
> >
> > J.R. Lockwood
> > 412-683-2300 x4941
> > lockwood at rand.org 
> > http://www.rand.org/methodology/stat/members/lockwood/
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list 
> https://www.stat.math.ethz.ch/mailman/listinfo> /r-help
>



From ryszard.czerminski at pharma.novartis.com  Mon Oct 27 21:34:11 2003
From: ryszard.czerminski at pharma.novartis.com (ryszard.czerminski@pharma.novartis.com)
Date: Mon, 27 Oct 2003 15:34:11 -0500
Subject: [R] how to remove NaN columns ?
Message-ID: <OFDA817F67.3D1BF21B-ON85256DCC.006E235E-85256DCC.00711280@EU.novartis.net>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20031027/5dacf93d/attachment.pl

From ripley at stats.ox.ac.uk  Mon Oct 27 21:39:12 2003
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Mon, 27 Oct 2003 20:39:12 +0000 (GMT)
Subject: [R] expanding factor with NA
In-Reply-To: <3A822319EB35174CA3714066D590DCD50205CD56@usrymx25.merck.com>
Message-ID: <Pine.LNX.4.44.0310272038380.28747-100000@gannet.stats>

On Mon, 27 Oct 2003, Liaw, Andy wrote:

> [OK, it's not so strange: na.action is not a documented argument for
> model.matrix, and the call to model.frame in model.matrix.default does not
> have ..., but shouldn't it?]


o, as it documented as for arguments to other methods.
-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From p.murrell at auckland.ac.nz  Mon Oct 27 21:51:34 2003
From: p.murrell at auckland.ac.nz (Paul Murrell)
Date: Tue, 28 Oct 2003 09:51:34 +1300
Subject: [R] \mathcal symbols in R?
References: <3F98FEA6.2080103@wiso.uni-erlangen.de>
Message-ID: <3F9D8556.8070604@stat.auckland.ac.nz>

Hi


Michael Grottke wrote:
> Hello,
> 
> Some time ago, I discovered the possibility of using mathematical 
> symbols for axis labels etc. In order ensure consistency between text 
> and graphics of some paper, I would like to include the calligraphic H 
> (obtained in LaTeX via \mathcal{H}) in several diagrams. Is there any 
> way to do so? Is it in general possible to use further mathematical 
> fonts like \mathbb and \mathbf in R?


Mostly no.  I don't suppose the Hershey script font is close enough?  e.g.,

plot.new()
text(.5, .5, "F", vfont=c("script", "plain"))

On some devices, it is possible to set a custom font family (e.g., you 
may be able to produce a postscript file with a Type 1 version of the 
mathcal font), but this is only per-file if at all (e.g., it would make 
all text in the file mathcal).  An exception is the windows device -- 
extra fonts can be set up (see the file $RHOME/etc/devga) and accessed 
via the "font" parameter.  e.g.,

plot(1:20, type="n")
text(1:20, 1:20, "F", font=1:20)

In this case, if you can get a True Type version of the mathcal font, 
and set it up properly, then you should be able to do just the text you 
want.

Greater flexibility in the specification of fonts is in the pipeline, 
but is at least an R version away.

HTH

Paul
-- 
Dr Paul Murrell
Department of Statistics
The University of Auckland
Private Bag 92019
Auckland
New Zealand
64 9 3737599 x85392
paul at stat.auckland.ac.nz
http://www.stat.auckland.ac.nz/~paul/



From yuleih at umich.edu  Mon Oct 27 22:05:26 2003
From: yuleih at umich.edu (Yulei He)
Date: Mon, 27 Oct 2003 16:05:26 -0500 (EST)
Subject: [R] how to set missing values in R
Message-ID: <Pine.SOL.4.58.0310271602590.20816@asteroids.gpcc.itd.umich.edu>

Hi, there.

Can I ask how to set up missing values in R? Suppose I want to assign the
missing value to the elements in vector which is greater than zero like
this:

x<-c(1,3,-1,0,4);

after the missing value assignment, x becomes (NA,NA,-1,0,NA).

Thanks!

Yulei


$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$
Yulei He
1586 Murfin Ave. Apt 37
Ann Arbor, MI 48105-3135
yuleih at umich.edu
734-647-0305(H)
734-763-0421(O)
734-763-0427(O)
734-764-8263(fax)
$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$



From jmacdon at med.umich.edu  Mon Oct 27 22:12:54 2003
From: jmacdon at med.umich.edu (James MacDonald)
Date: Mon, 27 Oct 2003 16:12:54 -0500
Subject: [R] how to set missing values in R
Message-ID: <sf9d441d.075@med-gwia-01a.med.umich.edu>

x[x>0] <- NA



James W. MacDonald
Affymetrix and cDNA Microarray Core
University of Michigan Cancer Center
1500 E. Medical Center Drive
7410 CCGC
Ann Arbor MI 48109
734-647-5623

>>> Yulei He <yuleih at umich.edu> 10/27/03 04:05PM >>>
Hi, there.

Can I ask how to set up missing values in R? Suppose I want to assign
the
missing value to the elements in vector which is greater than zero
like
this:

x<-c(1,3,-1,0,4);

after the missing value assignment, x becomes (NA,NA,-1,0,NA).

Thanks!

Yulei


$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$
Yulei He
1586 Murfin Ave. Apt 37
Ann Arbor, MI 48105-3135
yuleih at umich.edu 
734-647-0305(H)
734-763-0421(O)
734-763-0427(O)
734-764-8263(fax)
$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://www.stat.math.ethz.ch/mailman/listinfo/r-help



From RBaskin at ahrq.gov  Mon Oct 27 22:13:24 2003
From: RBaskin at ahrq.gov (RBaskin@ahrq.gov)
Date: Mon, 27 Oct 2003 16:13:24 -0500
Subject: [R] how to set missing values in R
Message-ID: <3598558AD728D41183350008C7CF291C0F16B979@exchange1.ahrq.gov>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20031027/39722e6f/attachment.pl

From bates at stat.wisc.edu  Mon Oct 27 22:19:39 2003
From: bates at stat.wisc.edu (Douglas Bates)
Date: 27 Oct 2003 15:19:39 -0600
Subject: [R] how to set missing values in R
In-Reply-To: <Pine.SOL.4.58.0310271602590.20816@asteroids.gpcc.itd.umich.edu>
References: <Pine.SOL.4.58.0310271602590.20816@asteroids.gpcc.itd.umich.edu>
Message-ID: <6rllr64bv8.fsf@bates4.stat.wisc.edu>

See the recent discussion about is.na() on this list.  You can do what
you want as

> x<-c(1,3,-1,0,4)
> is.na(x) = x > 0
> x
[1] NA NA -1  0 NA

Yulei He <yuleih at umich.edu> writes:

> Hi, there.
> 
> Can I ask how to set up missing values in R? Suppose I want to assign the
> missing value to the elements in vector which is greater than zero like
> this:
> 
> x<-c(1,3,-1,0,4);
> 
> after the missing value assignment, x becomes (NA,NA,-1,0,NA).



From bacolli at uark.edu  Mon Oct 27 22:52:13 2003
From: bacolli at uark.edu (Bret Collier)
Date: Mon, 27 Oct 2003 15:52:13 -0600
Subject: [R] New User Question regarding simulations
In-Reply-To: <3598558AD728D41183350008C7CF291C0F16B979@exchange1.ahrq.go v>
Message-ID: <5.2.1.1.0.20031027154856.03a7df30@mail.uark.edu>


>R-Users,

Everyone, I am a new user of R and I was wondering if anyone could point me 
to a reference (book or article) that discusses writing population 
simulations in R (or S).

Thanks in advance,

Bret A. Collier
Arkansas Cooperative Fish and Wildlife Research Unit
Department of Biological Sciences  SCEN 513
University of Arkansas
Fayetteville, AR  72701
(479) 575-4720
bacolli at uark.edu



From jeff.laake at noaa.gov  Mon Oct 27 23:14:14 2003
From: jeff.laake at noaa.gov (Jeff Laake)
Date: Mon, 27 Oct 2003 14:14:14 -0800
Subject: [R] New User Question regarding simulations
References: <5.2.1.1.0.20031027154856.03a7df30@mail.uark.edu>
Message-ID: <3F9D98B6.58B29BF3@noaa.gov>

The book by Borcher's et al Estimating Animal Abundance provides R/S+
code that you may find useful for your work.

Jeff Laake

Bret Collier wrote:
> 
> >R-Users,
> 
> Everyone, I am a new user of R and I was wondering if anyone could point me
> to a reference (book or article) that discusses writing population
> simulations in R (or S).
> 
> Thanks in advance,
> 
> Bret A. Collier
> Arkansas Cooperative Fish and Wildlife Research Unit
> Department of Biological Sciences  SCEN 513
> University of Arkansas
> Fayetteville, AR  72701
> (479) 575-4720
> bacolli at uark.edu
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help



From sureshkaran at hotmail.com  Mon Oct 27 23:27:23 2003
From: sureshkaran at hotmail.com (Suresh Kumar Karanam)
Date: Mon, 27 Oct 2003 17:27:23 -0500
Subject: [R] Error in validityMethod(object): No slot of name "phenoLabels"
	for this object o
Message-ID: <BAY2-F165234xOjIL2j00012d28@hotmail.com>

Hi,
I was using the affy package in R for sometime. I reinstalled the R 
environment lately due to some other issues and now I get errors when I use 
the function ReadAffy. The error is reproduced here:

Error in validityMethod(object): No slot of name "phenoLabels" for this 
object of class "phenoData"

I don't know what's wrong. Any help appreciated.
thanks,
suresh

_________________________________________________________________
Never get a busy signal because you are always connected  with high-speed 
Internet access. Click here to comparison-shop providers.  
https://broadband.msn.com



From rxg218 at psu.edu  Tue Oct 28 03:33:15 2003
From: rxg218 at psu.edu (Rajarshi Guha)
Date: 27 Oct 2003 21:33:15 -0500
Subject: [R] stacking histograms
Message-ID: <1067308395.28506.37.camel@ra.chem.psu.edu>

Hi,
  I have a set of observations which are divided into two sets A and B.
I have some code that bins the dataset into 10 bins based on the max and
min of the observed values. 

I would like to make a histogram of A & B using my calculated bins but
plot the distribution of B on top of A (like a stacked barplot). This is
possible since both sets A & B are binned using the same bin ranges.

I have my data in the format:

-4.000000 -3.453000 23
-3.453000 -2.906000 1
-2.906000 -2.359000 5
-2.359000 -1.812000 5
-1.812000 -1.265000 5
-1.265000 -0.718000 13
-0.718000 -0.171000 21
-0.171000 0.376000 49
0.376000 0.923000 26
0.923000 2.017000 13

where the first column is the lower value for the bin, the second column
the upper value for the bin and the last column is the frequency for
that bin

When I call the hist() function I get

> depv <- scan('depv.txt') # a vector of observed values
>
> # the bin boundaries described above
> breakvals <- read.table('bindata.txt') 
>
> hist(depvt, breaks=breakvals$V1)
>
Error in hist.default(depvt, breaks = br$V1) :
        some `x' not counted; maybe `breaks' do not span range of `x'

I get the same error when I specify breakvals$V2. 

Some of my observed values lie beyond the lower boundary of the last bin
(last item of column 1) or below the upper boundary of my first bin
(first row of column 2). Is this the reason why this error occurs?

How should I specify the breaks?

In addition is there any way I can plot the two histograms on top of
each other?

I have tried using the barplot() function but when it comes to marking
the x axis I'm not sure a to how to proceed. What I have been doing is
to calculate the mid point of each bin and use that as the label for
each column - is this a valid way to represent the histogram? (This way
it is easy for me to plot the histograms for the two sets together)

Thanks

-------------------------------------------------------------------
Rajarshi Guha <rxg218 at psu.edu> <http://jijo.cjb.net>
GPG Fingerprint: 0CCA 8EE2 2EEB 25E2 AB04 06F7 1BB9 E634 9B87 56EE
-------------------------------------------------------------------
"A fractal is by definition a set for which the Hausdorff Besicovitch
dimension strictly exceeds the topological dimension."
-- Mandelbrot, "The Fractal Geometry of Nature"



From andy_liaw at merck.com  Tue Oct 28 03:49:31 2003
From: andy_liaw at merck.com (Liaw, Andy)
Date: Mon, 27 Oct 2003 21:49:31 -0500
Subject: [R] stacking histograms
Message-ID: <3A822319EB35174CA3714066D590DCD50205CD61@usrymx25.merck.com>

The hist() function expects to be given data, not the counts in the bins.
It sounded like you are giving hist() the counts.

One thing you may try is by constructing an object of class "histogram" by
hand (see the "Value" section of ?hist), and just plot() it.  However,
beware that by default hist() tries to create a true density; i.e., the
total area of the bars should sum to one.  If you just plot the counts and
the bins do not have contant width, then your plot will look a bit strange.

For "stacking", your barplot() approach is probably easiest, but again be
careful how you read the resulting graph.

HTH,
Andy

> -----Original Message-----
> From: Rajarshi Guha [mailto:rxg218 at psu.edu] 
> Sent: Monday, October 27, 2003 9:33 PM
> To: r-help at stat.math.ethz.ch
> Subject: [R] stacking histograms
> 
> 
> Hi,
>   I have a set of observations which are divided into two 
> sets A and B. I have some code that bins the dataset into 10 
> bins based on the max and min of the observed values. 
> 
> I would like to make a histogram of A & B using my calculated 
> bins but plot the distribution of B on top of A (like a 
> stacked barplot). This is possible since both sets A & B are 
> binned using the same bin ranges.
> 
> I have my data in the format:
> 
> -4.000000 -3.453000 23
> -3.453000 -2.906000 1
> -2.906000 -2.359000 5
> -2.359000 -1.812000 5
> -1.812000 -1.265000 5
> -1.265000 -0.718000 13
> -0.718000 -0.171000 21
> -0.171000 0.376000 49
> 0.376000 0.923000 26
> 0.923000 2.017000 13
> 
> where the first column is the lower value for the bin, the 
> second column the upper value for the bin and the last column 
> is the frequency for that bin
> 
> When I call the hist() function I get
> 
> > depv <- scan('depv.txt') # a vector of observed values
> >
> > # the bin boundaries described above
> > breakvals <- read.table('bindata.txt')
> >
> > hist(depvt, breaks=breakvals$V1)
> >
> Error in hist.default(depvt, breaks = br$V1) :
>         some `x' not counted; maybe `breaks' do not span range of `x'
> 
> I get the same error when I specify breakvals$V2. 
> 
> Some of my observed values lie beyond the lower boundary of 
> the last bin (last item of column 1) or below the upper 
> boundary of my first bin (first row of column 2). Is this the 
> reason why this error occurs?
> 
> How should I specify the breaks?
> 
> In addition is there any way I can plot the two histograms on 
> top of each other?
> 
> I have tried using the barplot() function but when it comes 
> to marking the x axis I'm not sure a to how to proceed. What 
> I have been doing is to calculate the mid point of each bin 
> and use that as the label for each column - is this a valid 
> way to represent the histogram? (This way it is easy for me 
> to plot the histograms for the two sets together)
> 
> Thanks
> 
> -------------------------------------------------------------------
> Rajarshi Guha <rxg218 at psu.edu> <http://jijo.cjb.net>
> GPG Fingerprint: 0CCA 8EE2 2EEB 25E2 AB04 06F7 1BB9 E634 9B87 56EE
> -------------------------------------------------------------------
> "A fractal is by definition a set for which the Hausdorff 
> Besicovitch dimension strictly exceeds the topological dimension."
> -- Mandelbrot, "The Fractal Geometry of Nature"
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list 
> https://www.stat.math.ethz.ch/mailman/listinfo> /r-help
>



From nortonsm at verizon.net  Tue Oct 28 04:34:06 2003
From: nortonsm at verizon.net (Scott Norton)
Date: Mon, 27 Oct 2003 22:34:06 -0500
Subject: [R] outer function problems
Message-ID: <001201c39d04$5795f020$6501a8c0@scott>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20031027/bef01563/attachment.pl

From seniorr at aracnet.com  Tue Oct 28 06:48:22 2003
From: seniorr at aracnet.com (Russell Senior)
Date: 27 Oct 2003 21:48:22 -0800
Subject: [R] variance component analysis for nested model
In-Reply-To: <3F9CE602.3060503@unibas.ch>
References: <86fzhfus8u.fsf@coulee.tdb.com> <3F9CE602.3060503@unibas.ch>
Message-ID: <86znflsyjd.fsf@coulee.tdb.com>

>>>>> "Pascal" == Pascal A Niklaus <Pascal.Niklaus at unibas.ch> writes:

Pascal> lme should do the job (r1,r2,r3 are your random factors):
Pascal> library(nlme) y.lme <- lme(y ~ 1,random = ~ 1 | r1/r2/r3)
Pascal> summary(y.lme)

Pascal> This is equivalent to a call to varcomp in S-Plus

Thanks!  This was the clue I needed.

-- 
Russell Senior         ``I have nine fingers; you have ten.''
seniorr at aracnet.com



From seniorr at aracnet.com  Tue Oct 28 07:17:09 2003
From: seniorr at aracnet.com (Russell Senior)
Date: 27 Oct 2003 22:17:09 -0800
Subject: [R] formula parsing, using parts ...
Message-ID: <86vfq9sx7e.fsf@coulee.tdb.com>


I am writing a little abstraction for a series of tests.  For example,
I am running an anova and kruskal.test on a one-factor model.  That
isn't a particular problem, I have an interface like:

 my.function <- function(model,data) {
   print(deparse(substitute(data)))
   a <- anova(lm(formula,data))
   print(a)
   if(a$"Pr(>F)"[1] < 0.05) {
      pairwise.t.test(???)
   }
   b <- kruskal.test(formula,data)
   print(b)
   if ...
 }

I want to run each test, then depending on the resulting p-value, run
pairwise tests.  I am getting into trouble where I put the ??? above.
The pairwise.t.test has a different interface, that seems to want me
to dismember the formula into constituent parts to feed in.  The other
alternative is to give my.function the constituent parts and let it
build the model.  I haven't figured out how to do either one.  Can
someone give me some pointers?

-- 
Russell Senior         ``I have nine fingers; you have ten.''
seniorr at aracnet.com



From ligges at statistik.uni-dortmund.de  Tue Oct 28 09:30:18 2003
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Tue, 28 Oct 2003 09:30:18 +0100
Subject: [R] New version of R-WinEdt (for Windows)
Message-ID: <3F9E291A.4020203@statistik.uni-dortmund.de>

There was a user request to announce this new version (1.6-0) of
R-WinEdt (actually, the request was to announce version 1.5-1). It is 
propagating through CRAN these days and (is|will be) available at
  yourCRANmirror/contrib/extra/winedt/

For those who have not already noticed the changes since R-WinEdt 1.4-x,
I'd like to summarize the changes below. The most exciting one is the
new automatical installation procedure (since version 1.5-0 R-WinEdt
installs like any other R package on Windows, so knowledge about Windows
*.bat files and shortcuts etc. is no longer required). There are also
changes for syntax highlighting and an automatical detection of the
window mode RGUI is running in.

Some changes are due to user requests, other changes have been proposed
by myself in the corresponding paper of the DSC 2001 proceedings (not
all are implemented). I'd like to thank all those users who helped to
improve the Plug-In with their bug reports, feature requests, or 
"simply" some questions on the usage.


Changes in RWinEdt_1.6-0:
-------------------------
- Detects whether RGui has been started in --sdi or --mdi mode
- Added features to move (indent) blocks of code, and insert
   comments (#) blockwise, as known from the regular WinEdt mode
   for LaTeX et al. editing
- Added version control system that knows when the R-WinEdt
   system has to be updated.
- R-WinEdt no longer sets WinEdt as the default pager.
- Fixed problem that startWinEdt() started WinEdt in its "-V"
   mode.

Changes in RWinEdt_1.5-1:
-------------------------
- Index of (exported) object names updated (R-1.8.0 related).
- Syntax highlighting
   - for Namespace operators (::, :::) added (R-1.8.0 related).
   - for `backtick names` added (R-1.8.0 related).
   - for 'single quotes' added.
   - for old assignment operator "_" removed (R-1.8.0 related).
   - for "double quotes' fixed.
   - for assignment operator "=" fixed.
   - for comparisons (e.g. "==") fixed.
- Documentation related to the broken (doesn't seem to be fixed soon)
   automatical installation of the SWinRegistry package added.
- More carefully loading SWinRegistry using require().

Changes in RWinEdt_1.5-0:
-------------------------
- Now providing R-WinEdt as a package called RWinEdt:
   - introducing a new installation procedure,
     installed like an R package, called like an R package (library())
     - requires Omegahat package SWinRegistry
   - providing an R Menu to call R-WinEdt
- minor fixes for Windows XP timings
- minor fixes for Syntax highlighting ("=", "_", ...)


Uwe Ligges



From NMI13 at student.canterbury.ac.nz  Tue Oct 28 09:38:22 2003
From: NMI13 at student.canterbury.ac.nz (nmi13)
Date: Tue, 28 Oct 2003 21:38:22 +1300
Subject: [R] random number generation
Message-ID: <3F9C80C5@webmail>

Hi every one,

I am trying to generate a normally distributed random variable with the 
following descriptive statistics,

min=1, max=99, variance=125, mean=38.32, 1st quartile=38, median=40, 3rd 
quartile=40, skewness=-0.274.

I know the "rnorm" will allow me to simulate random numbers with mean 38.32 
and Sd=11.18(sqrt(125)). But I need to have the above mentioned descriptive 
statistics for the data that I generate.

I would be thankful to anyone who can help me with this problem.

Regards
Murthy



From ligges at statistik.uni-dortmund.de  Tue Oct 28 10:00:11 2003
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Tue, 28 Oct 2003 10:00:11 +0100
Subject: [R] formula parsing, using parts ...
In-Reply-To: <86vfq9sx7e.fsf@coulee.tdb.com>
References: <86vfq9sx7e.fsf@coulee.tdb.com>
Message-ID: <3F9E301B.3030908@statistik.uni-dortmund.de>

Russell Senior wrote:

> I am writing a little abstraction for a series of tests.  For example,
> I am running an anova and kruskal.test on a one-factor model.  That
> isn't a particular problem, I have an interface like:
> 
>  my.function <- function(model,data) {
>    print(deparse(substitute(data)))
>    a <- anova(lm(formula,data))
>    print(a)
>    if(a$"Pr(>F)"[1] < 0.05) {
>       pairwise.t.test(???)
>    }
>    b <- kruskal.test(formula,data)
>    print(b)
>    if ...
>  }
> 
> I want to run each test, then depending on the resulting p-value, run
> pairwise tests.  I am getting into trouble where I put the ??? above.
> The pairwise.t.test has a different interface, that seems to want me
> to dismember the formula into constituent parts to feed in.  The other
> alternative is to give my.function the constituent parts and let it
> build the model.  I haven't figured out how to do either one.  Can
> someone give me some pointers?
> 

See ?formula and its "See Also" Section on how to do formula 
manipulation. There's also an example on how to construct a formula.

Uwe Ligges



From eia018 at comp.lancs.ac.uk  Tue Oct 28 10:34:15 2003
From: eia018 at comp.lancs.ac.uk (Dr Andrew Wilson)
Date: Tue, 28 Oct 2003 09:34:15 +0000 (GMT)
Subject: [R] Octave scale transformation
Message-ID: <Pine.GSO.4.21.0310280925110.5865-100000@austin>

Is it possible to convert a data table in "R" to an octave scale (as
done, for example, in the MVSP multivariate stats program)?

I work with tables of word or category frequencies across a number of
texts or text segments, e.g.:

Token	sect_1	sect_2	sect_3	sect_4	sect_5	sect_6	sect_7	sect_8
sect_9	sect_10	sect_11	sect_12	sect_13	sect_14	sect_15	sect_16	sect_17
sect_18	sect_19	sect_20	sect_21	sect_22	sect_23
advance	0	0	0	0	0	1	0	0	0
0	4	0	0	0	2	0	0	0	0
0	0	0	0
aed	0	1	3	0	0	1	0	0	0
0	4	0	0	0	0	4	2	3	0
0	0	1	1
agree	0	0	0	1	0	0	0	0	0
0	0	1	0	0	0	0	0	0	0
0	0	1	0
antibiotics	0	0	0	0	0	0	0	0
0	0	0	3	1	0	0	0	0	0
0	1	0	0	0

However, the texts/segments are typically of different lengths and the
analysis program doesn't calculate proportional frequencies.  (NB: It also
doesn't select *all* words in the texts, so it is not possible to
calculate true percentages "after the fact".) 

What I want to do is to transform the data before calculating distances
and carrying out clustering or multidimensional scaling, so that the
differences in text/segment size don't (heavily) bias the results.

Many thanks,
Andrew Wilson



From seniorr at aracnet.com  Tue Oct 28 10:55:13 2003
From: seniorr at aracnet.com (Russell Senior)
Date: 28 Oct 2003 01:55:13 -0800
Subject: [R] formula parsing, using parts ...
In-Reply-To: <3F9E301B.3030908@statistik.uni-dortmund.de>
References: <86vfq9sx7e.fsf@coulee.tdb.com>
	<3F9E301B.3030908@statistik.uni-dortmund.de>
Message-ID: <86n0blr8ji.fsf@coulee.tdb.com>

>>>>> "Uwe" == Uwe Ligges <ligges at statistik.uni-dortmund.de> writes:

Russell> I am writing a little abstraction for a series of tests.  For
Russell> example, I am running an anova and kruskal.test on a
Russell> one-factor model.  That isn't a particular problem, I have an
Russell> interface like: my.function <- function(model,data) {
Russell> print(deparse(substitute(data))) a <- anova(lm(formula,data))
Russell> print(a) if(a$"Pr(>F)"[1] < 0.05) { pairwise.t.test(???)  } b
Russell> <- kruskal.test(formula,data) print(b) if ...  } I want to
Russell> run each test, then depending on the resulting p-value, run
Russell> pairwise tests.  I am getting into trouble where I put the
Russell> ??? above.  The pairwise.t.test has a different interface,
Russell> that seems to want me to dismember the formula into
Russell> constituent parts to feed in.  The other alternative is to
Russell> give my.function the constituent parts and let it build the
Russell> model.  I haven't figured out how to do either one.  Can
Russell> someone give me some pointers?

Uwe> See ?formula and its "See Also" Section on how to do formula
Uwe> manipulation. There's also an example on how to construct a
Uwe> formula.

In order to use the 'as.formula(paste(response," ~ ",factor))'
approach, response and factor seem to need to be strings (at least
they seem to if response is "log(x)" or the like).  Whereas, for
pairwise.t.test they need to be names.  What is the proper way to do
that?

-- 
Russell Senior         ``I have nine fingers; you have ten.''
seniorr at aracnet.com



From seniorr at aracnet.com  Tue Oct 28 11:20:54 2003
From: seniorr at aracnet.com (Russell Senior)
Date: 28 Oct 2003 02:20:54 -0800
Subject: [R] formula parsing, using parts ...
In-Reply-To: <3F9E3E7D.7060003@statistik.uni-dortmund.de>
References: <86vfq9sx7e.fsf@coulee.tdb.com>
	<3F9E301B.3030908@statistik.uni-dortmund.de>
	<86ptghr8ok.fsf@coulee.tdb.com>
	<3F9E3E7D.7060003@statistik.uni-dortmund.de>
Message-ID: <86d6chr7cp.fsf@coulee.tdb.com>

>>>>> "Uwe" == Uwe Ligges <ligges at statistik.uni-dortmund.de> writes:

Russell> I am writing a little abstraction for a series of tests. For
Russell> example, I am running an anova and kruskal.test on a
Russell> one-factor model.  That isn't a particular problem, I have an
Russell> interface like: my.function <- function(model,data) {
Russell> print(deparse(substitute(data))) a <- anova(lm(formula,data))
Russell> print(a) if(a$"Pr(>F)"[1] < 0.05) { pairwise.t.test(???)  } b
Russell> <- kruskal.test(formula,data) print(b) if ...  } I want to
Russell> run each test, then depending on the resulting p-value, run
Russell> pairwise tests.  I am getting into trouble where I put the
Russell> ??? above.  The pairwise.t.test has a different interface,
Russell> that seems to want me to dismember the formula into
Russell> constituent parts to feed in.  The other alternative is to
Russell> give my.function the constituent parts and let it build the
Russell> model.  I haven't figured out how to do either one.  Can
Russell> someone give me some pointers?

Uwe> See ?formula and its "See Also" Section on how to do formula
Uwe> manipulation. There's also an example on how to construct a
Uwe> formula.

Russell> In order to use the 'as.formula(paste(response," ~
Russell> ",factor))' approach, response and factor seem to need to be
Russell> strings (at least they seem to if response is "log(x)" or the
Russell> like).  Whereas, for pairwise.t.test they need to be names.
Russell> What is the proper way to do that?


Uwe> In order to run pairwise.t.test() you can simply get() the values
Uwe> from objects:

Uwe> Let's change the example in ?pairwise.t.test:

Uwe>   data(airquality) 
Uwe>   attach(airquality) 
Uwe>   Month <- factor(Month, labels = month.abb[5:9]) 
Uwe>   x <- "Ozone" 
Uwe>   y <- "Month"
Uwe>   pairwise.t.test(get(x), get(y))

Suppose I want x to be "log(Ozone)"?  The get() function doesn't help
me there.

-- 
Russell Senior         ``I have nine fingers; you have ten.''
seniorr at aracnet.com



From ligges at statistik.uni-dortmund.de  Tue Oct 28 11:27:08 2003
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Tue, 28 Oct 2003 11:27:08 +0100
Subject: [R] formula parsing, using parts ...
In-Reply-To: <86d6chr7cp.fsf@coulee.tdb.com>
References: <86vfq9sx7e.fsf@coulee.tdb.com>	<3F9E301B.3030908@statistik.uni-dortmund.de>	<86ptghr8ok.fsf@coulee.tdb.com>	<3F9E3E7D.7060003@statistik.uni-dortmund.de>
	<86d6chr7cp.fsf@coulee.tdb.com>
Message-ID: <3F9E447C.9060002@statistik.uni-dortmund.de>

Russell Senior wrote:

>>>>>>"Uwe" == Uwe Ligges <ligges at statistik.uni-dortmund.de> writes:
> 
> 
> Russell> I am writing a little abstraction for a series of tests. For
> Russell> example, I am running an anova and kruskal.test on a
> Russell> one-factor model.  That isn't a particular problem, I have an
> Russell> interface like: my.function <- function(model,data) {
> Russell> print(deparse(substitute(data))) a <- anova(lm(formula,data))
> Russell> print(a) if(a$"Pr(>F)"[1] < 0.05) { pairwise.t.test(???)  } b
> Russell> <- kruskal.test(formula,data) print(b) if ...  } I want to
> Russell> run each test, then depending on the resulting p-value, run
> Russell> pairwise tests.  I am getting into trouble where I put the
> Russell> ??? above.  The pairwise.t.test has a different interface,
> Russell> that seems to want me to dismember the formula into
> Russell> constituent parts to feed in.  The other alternative is to
> Russell> give my.function the constituent parts and let it build the
> Russell> model.  I haven't figured out how to do either one.  Can
> Russell> someone give me some pointers?
> 
> Uwe> See ?formula and its "See Also" Section on how to do formula
> Uwe> manipulation. There's also an example on how to construct a
> Uwe> formula.
> 
> Russell> In order to use the 'as.formula(paste(response," ~
> Russell> ",factor))' approach, response and factor seem to need to be
> Russell> strings (at least they seem to if response is "log(x)" or the
> Russell> like).  Whereas, for pairwise.t.test they need to be names.
> Russell> What is the proper way to do that?
> 
> 
> Uwe> In order to run pairwise.t.test() you can simply get() the values
> Uwe> from objects:
> 
> Uwe> Let's change the example in ?pairwise.t.test:
> 
> Uwe>   data(airquality) 
> Uwe>   attach(airquality) 
> Uwe>   Month <- factor(Month, labels = month.abb[5:9]) 
> Uwe>   x <- "Ozone" 
> Uwe>   y <- "Month"
> Uwe>   pairwise.t.test(get(x), get(y))
> 
> Suppose I want x to be "log(Ozone)"?  The get() function doesn't help
> me there.
> 


  eval(parse(text=x))

Uwe Ligges



From seniorr at aracnet.com  Tue Oct 28 11:44:27 2003
From: seniorr at aracnet.com (Russell Senior)
Date: 28 Oct 2003 02:44:27 -0800
Subject: [R] formula parsing, using parts ...
In-Reply-To: <3F9E447C.9060002@statistik.uni-dortmund.de>
References: <86vfq9sx7e.fsf@coulee.tdb.com>
	<3F9E301B.3030908@statistik.uni-dortmund.de>
	<86ptghr8ok.fsf@coulee.tdb.com>
	<3F9E3E7D.7060003@statistik.uni-dortmund.de>
	<86d6chr7cp.fsf@coulee.tdb.com>
	<3F9E447C.9060002@statistik.uni-dortmund.de>
Message-ID: <86znflk5f8.fsf@coulee.tdb.com>

>>>>> "Uwe" == Uwe Ligges <ligges at statistik.uni-dortmund.de> writes:

Russell> Suppose I want x to be "log(Ozone)"?  The get() function
Russell> doesn't help me there.

Uwe>   eval(parse(text=x))

Ah, that seems to have done it.  Thanks!

-- 
Russell Senior         ``I have nine fingers; you have ten.''
seniorr at aracnet.com



From christian.schulz at questico.de  Tue Oct 28 12:31:02 2003
From: christian.schulz at questico.de (Christian Schulz)
Date: Tue, 28 Oct 2003 12:31:02 +0100
Subject: [R] ESS windows/linux
Message-ID: <JAEELBHBOPKJDMMCNHKMGEMBCAAA.christian.schulz@questico.de>

In front of my long intention
migrate to linux i'm trying Xemacs 
and ESS for first in Windows.
Install all and get the Rd-Menu
in Xemacs, but if i want to "Eval a region"
of R-Code i get the message
in the below window "No ESS Processes running"!

P.S.
Have anybody written a document like
"Using R-Project - from Windows2Linux" 
which describe the differencies in one
small paper. If not, perhaps i summarize
in next weeks my experience.

Not misunderstand it should not 
replace the faq's, but perhaps  
speed up migration from windows
to linux when the linux skills at
a beginner level.

many thanks and 
regards,christian



From Bernhard.Pfaff at drkw.com  Tue Oct 28 12:43:20 2003
From: Bernhard.Pfaff at drkw.com (Pfaff, Bernhard)
Date: Tue, 28 Oct 2003 12:43:20 +0100
Subject: [R] ESS windows/linux
Message-ID: <18D602BD42B7E24EB810D6454A58DB9004730697@ibfftce505.is.de.dresdnerkb.com>

> In front of my long intention
> migrate to linux i'm trying Xemacs 
> and ESS for first in Windows.
> Install all and get the Rd-Menu
> in Xemacs, but if i want to "Eval a region"
> of R-Code i get the message
> in the below window "No ESS Processes running"!

Have you started an R process in the first place? 

M-x R

Secondly, have your set your path in 'ess-site.el' correctly?

HTH,
Bernhard


> 
> P.S.
> Have anybody written a document like
> "Using R-Project - from Windows2Linux" 
> which describe the differencies in one
> small paper. If not, perhaps i summarize
> in next weeks my experience.
> 
> Not misunderstand it should not 
> replace the faq's, but perhaps  
> speed up migration from windows
> to linux when the linux skills at
> a beginner level.
> 
> many thanks and 
> regards,christian
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> 


--------------------------------------------------------------------------------
The information contained herein is confidential and is intended solely for the
addressee. Access by any other party is unauthorised without the express 
written permission of the sender. If you are not the intended recipient, please 
contact the sender either via the company switchboard on +44 (0)20 7623 8000, or
via e-mail return. If you have received this e-mail in error or wish to read our
e-mail disclaimer statement and monitoring policy, please refer to 
http://www.drkw.com/disc/email/ or contact the sender.



From christian.schulz at questico.de  Tue Oct 28 12:52:46 2003
From: christian.schulz at questico.de (Christian Schulz)
Date: Tue, 28 Oct 2003 12:52:46 +0100
Subject: AW: [R] ESS windows/linux
In-Reply-To: <18D602BD42B7E24EB810D6454A58DB9004730697@ibfftce505.is.de.dresdnerkb.com>
Message-ID: <JAEELBHBOPKJDMMCNHKMGEMCCAAA.christian.schulz@questico.de>

many thanks, now it works !
christian


-----Urspr?ngliche Nachricht-----
Von: Pfaff, Bernhard [mailto:Bernhard.Pfaff at drkw.com]
Gesendet: Dienstag, 28. Oktober 2003 12:43
An: 'Christian Schulz'; r-help at stat.math.ethz.ch
Betreff: RE: [R] ESS windows/linux


> In front of my long intention
> migrate to linux i'm trying Xemacs
> and ESS for first in Windows.
> Install all and get the Rd-Menu
> in Xemacs, but if i want to "Eval a region"
> of R-Code i get the message
> in the below window "No ESS Processes running"!

Have you started an R process in the first place?

M-x R

Secondly, have your set your path in 'ess-site.el' correctly?

HTH,
Bernhard


>
> P.S.
> Have anybody written a document like
> "Using R-Project - from Windows2Linux"
> which describe the differencies in one
> small paper. If not, perhaps i summarize
> in next weeks my experience.
>
> Not misunderstand it should not
> replace the faq's, but perhaps
> speed up migration from windows
> to linux when the linux skills at
> a beginner level.
>
> many thanks and
> regards,christian
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
>


----------------------------------------------------------------------------
----
The information contained herein is confidential and is intended solely for
the
addressee. Access by any other party is unauthorised without the express
written permission of the sender. If you are not the intended recipient,
please
contact the sender either via the company switchboard on +44 (0)20 7623
8000, or
via e-mail return. If you have received this e-mail in error or wish to read
our
e-mail disclaimer statement and monitoring policy, please refer to
http://www.drkw.com/disc/email/ or contact the sender.
----------------------------------------------------------------------------
----



From NMI13 at student.canterbury.ac.nz  Tue Oct 28 13:20:01 2003
From: NMI13 at student.canterbury.ac.nz (nmi13)
Date: Wed, 29 Oct 2003 01:20:01 +1300
Subject: [R] random number generation
Message-ID: <3F9C9C9B@webmail>

Hi every one,

I am trying to generate a random variable with the following descriptive 
statistics,

min=1, max=99, variance=125, mean=38.32, 1st quartile=38, median=40, 3rd
quartile=40, skewness=-0.274.

I tried with rgamma and as I cannot use rnorm, can any one please suggest me 
what distribution would give me the negative skewness. I need to have the 
above mentioned descriptive statistics for the data generated.

I would be thankful to anyone who can help me with this problem.

Regards
Murthy



From presnell at stat.ufl.edu  Tue Oct 28 13:29:58 2003
From: presnell at stat.ufl.edu (Brett Presnell)
Date: Tue, 28 Oct 2003 07:29:58 -0500
Subject: [R] \mathcal symbols in R?
In-Reply-To: <3F9D8556.8070604@stat.auckland.ac.nz>
References: <3F98FEA6.2080103@wiso.uni-erlangen.de>
	<3F9D8556.8070604@stat.auckland.ac.nz>
Message-ID: <16286.24902.903269.844210@flounder.stat.ufl.edu>


Paul Murrell writes:
> 
> Michael Grottke wrote:
> > 
> > Some time ago, I discovered the possibility of using mathematical 
> > symbols for axis labels etc. In order ensure consistency between text 
> > and graphics of some paper, I would like to include the calligraphic H 
> > (obtained in LaTeX via \mathcal{H}) in several diagrams. Is there any 
> > way to do so? Is it in general possible to use further mathematical 
> > fonts like \mathbb and \mathbf in R?

If you really want to match the latex fonts exactly, you might want to
try psfrag.sty.  I used to use it regularly when putting math in S
figures was more trouble than it is now with R, and it worked very
well.  My approach was to use "meaningful" text strings in the S
graphic, so that the graphic was useful on it's own (outside of the
latex document), and then substitute for those strings using psfrag in
the latex document.

-- 
Brett Presnell
Department of Statistics
University of Florida
http://www.stat.ufl.edu/~presnell/

"We don't think that the popularity of an error makes it the truth."
   -- Richard Stallman



From Pascal.Niklaus at unibas.ch  Tue Oct 28 13:36:05 2003
From: Pascal.Niklaus at unibas.ch (Pascal A. Niklaus)
Date: Tue, 28 Oct 2003 13:36:05 +0100
Subject: [R] random number generation
In-Reply-To: <3F9C80C5@webmail>
References: <3F9C80C5@webmail>
Message-ID: <3F9E62B5.9000108@unibas.ch>

You need to know the exact distribution of the random numbers you want 
to generate. For rnorm, in fact, you do not just specify the mean and 
the variance, but implicitely also that the data is normally 
distributed. Likewise, it is not sufficient to give min, max, skewness 
etc, you also need to know the distribution and then maybe you can use 
runif() as base for your code.

Pascal


nmi13 wrote:

>Hi every one,
>
>I am trying to generate a normally distributed random variable with the 
>following descriptive statistics,
>
>min=1, max=99, variance=125, mean=38.32, 1st quartile=38, median=40, 3rd 
>quartile=40, skewness=-0.274.
>
>I know the "rnorm" will allow me to simulate random numbers with mean 38.32 
>and Sd=11.18(sqrt(125)). But I need to have the above mentioned descriptive 
>statistics for the data that I generate.
>
>I would be thankful to anyone who can help me with this problem.
>
>Regards
>Murthy
>
>  
>



From rolf at math.unb.ca  Tue Oct 28 13:34:35 2003
From: rolf at math.unb.ca (Rolf Turner)
Date: Tue, 28 Oct 2003 08:34:35 -0400 (AST)
Subject: [R] random number generation
Message-ID: <200310281234.h9SCYZjm008006@erdos.math.unb.ca>


Is this a homework problem?

			cheers,

				Rolf Turner
				rolf at math.unb.ca



From ripley at stats.ox.ac.uk  Tue Oct 28 13:42:18 2003
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Tue, 28 Oct 2003 12:42:18 +0000 (GMT)
Subject: [R] random number generation
In-Reply-To: <3F9C9C9B@webmail>
Message-ID: <Pine.LNX.4.44.0310281235030.1148-100000@gannet.stats>

Is this a student exercise?  If not, please enlighten us as to the
real-world problem from which this is extracted.

Given that 50% of the probability mass lies between 38 and 40, and the 
median and 3rd quartile are both 40, this cannot be a continuous
distribution.  I would design a discrete distribution on the integers 
1, ..., 99 to meet your requirements: that is `just' a constrained 
non-linear optimization problem.

BTW, a random variable cannot have those characteristics: its distribution 
could, or a sample could and it is unclear which you mean.  The first is 
easier and so that's what I have assumed.

On Wed, 29 Oct 2003, nmi13 wrote:

> I am trying to generate a random variable with the following descriptive 
> statistics,
> 
> min=1, max=99, variance=125, mean=38.32, 1st quartile=38, median=40, 3rd
> quartile=40, skewness=-0.274.
> 
> I tried with rgamma and as I cannot use rnorm, can any one please suggest me 
> what distribution would give me the negative skewness. I need to have the 
> above mentioned descriptive statistics for the data generated.
> 
> I would be thankful to anyone who can help me with this problem.

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From spencer.graves at pdf.com  Tue Oct 28 14:12:45 2003
From: spencer.graves at pdf.com (Spencer Graves)
Date: Tue, 28 Oct 2003 05:12:45 -0800
Subject: [R] outer function problems
In-Reply-To: <001201c39d04$5795f020$6501a8c0@scott>
References: <001201c39d04$5795f020$6501a8c0@scott>
Message-ID: <3F9E6B4D.9030005@pdf.com>

      I don't know that this is your problem, but I see a potential 
scoping issue:  It is not obvious to me where Dk is getting n0 and w.  
I've solved this kind of problem in the past by declaring n0 and w as 
explicit arguments to Dk and then passing them explicitly via "..." in 
"outer".  In general, I prefer to avoid accessing globals from within 
functions.  This may not help you here, but it might help in the future. 

      hope this helps.  spencer graves

Scott Norton wrote:

>I'm pulling my hair (and there's not much left!) on this one. Basically I'm
>not getting the same result t when I "step" through the program and evaluate
>each element separately than when I use the outer() function in the
>FindLikelihood() function below.
>
> 
>
>Here's the functions:
>
> 
>
>Dk<- function(xk,A,B) 
>
>{
>
>n0 *(A*exp(-0.5*(xk/w)^2) + B)
>
>}
>
> 
>
>FindLikelihood <- function(Nk)
>
>{
>
>A <- seq(0.2,3,by=0.2)
>
>B <- seq(0.2,3,by=0.2)
>
>k <-7
>
>L <- outer(A, B, function(A,B) sum( (Nk*log(Dk(seq(-k,k),A,B))) -
>Dk(seq(-k,k),A,B) ))
>
>return(L)
>
>}
>
> 
>
> 
>
>where Nk <- c(70 , 67 , 75 , 77 , 74 ,102,  75, 104 , 94 , 74 , 78 , 79 , 83
>, 73 , 76)
>
> 
>
> 
>
>Here's an excerpt from my debug session..
>
> 
>
>  
>
>>Nk
>>    
>>
>
> [1]  70  67  75  77  74 102  75 104  94  74  78  79  83  73  76
>
>  
>
>>debug(FindLikelihood)
>>    
>>
>
>  
>
>>L<-FindLikelihood(Nk)
>>    
>>
>
>debugging in: FindLikelihood(Nk)
>
>debug: {
>
>    A <- seq(0.2, 3, by = 0.2)
>
>    B <- seq(0.2, 3, by = 0.2)
>
>    k <- 7
>
>    L <- outer(A, B, function(A, B) sum((Nk * log(Dk(seq(-k, 
>
>        k), A, B))) - Dk(seq(-k, k), A, B)))
>
>    return(L)
>
>}
>
>Browse[1]> n
>
>debug: A <- seq(0.2, 3, by = 0.2)
>
>Browse[1]> n
>
>debug: B <- seq(0.2, 3, by = 0.2)
>
>Browse[1]> n
>
>debug: k <- 7
>
>Browse[1]> n
>
>debug: L <- outer(A, B, function(A, B) sum((Nk * log(Dk(seq(-k, k), 
>
>    A, B))) - Dk(seq(-k, k), A, B)))
>
>Browse[1]> sum((Nk * log(Dk(seq(-k, k),0.2,0.2))) - Dk(seq(-k, k), 0.2,
>0.2))      # WHY DOES THIS LINE GIVE ME THE CORRECT RESULT WHEN I SUBSTITUTE
>0.2, 0.2 FOR A AND B
>
>[1] 2495.242
>
>Browse[1]> outer(A, B, function(A, B) sum((Nk * log(Dk(seq(-k, k), 
>
>+     A, B))) - Dk(seq(-k, k), A, B)))
>
>          [,1]     [,2]     [,3]     [,4]     [,5]     [,6]     [,7]
>[,8]
>
> [1,] 58389.48 58389.48 58389.48 58389.48 58389.48 58389.48 58389.48
>58389.48    # BUT ELEMENT (1,1) WHICH SHOULD ALSO BE (A,B) = (0.2, 0.2),
>GIVES THE INCORRECT RESULT????
>
> [2,] 58389.48 58389.48 58389.48 58389.48 58389.48 58389.48 58389.48
>58389.48
>
> [3,] 58389.48 58389.48 58389.48 58389.48 58389.48 58389.48 58389.48
>58389.48
>
> [4,] 58389.48 58389.48 58389.48 58389.48 58389.48 58389.48 58389.48
>58389.48
>
> [5,] 58389.48 58389.48 58389.48 58389.48 58389.48 58389.48 58389.48
>58389.48
>
> [6,] 58389.48 58389.48 58389.48 58389.48 58389.48 58389.48 58389.48
>58389.48
>
> [7,] 58389.48 58389.48 58389.48 58389.48 58389.48 58389.48 58389.48
>58389.48
>
> [8,] 58389.48 58389.48 58389.48 58389.48 58389.48 58389.48 58389.48
>58389.48
>
> [9,] 58389.48 58389.48 58389.48 58389.48 58389.48 58389.48 58389.48
>58389.48
>
>[10,] 58389.48 58389.48 58389.48 58389.48 58389.48 58389.48 58389.48
>58389.48
>
>[11,] 58389.48 58389.48 58389.48 58389.48 58389.48 58389.48 58389.48
>58389.48
>
>[12,] 58389.48 58389.48 58389.48 58389.48 58389.48 58389.48 58389.48
>58389.48
>
>[13,] 58389.48 58389.48 58389.48 58389.48 58389.48 58389.48 58389.48
>58389.48
>
>[14,] 58389.48 58389.48 58389.48 58389.48 58389.48 58389.48 58389.48
>58389.48
>
>[15,] 58389.48 58389.48 58389.48 58389.48 58389.48 58389.48 58389.48
>58389.48
>
>          [,9]    [,10]    [,11]    [,12]    [,13]    [,14]    [,15]
>
> [1,] 58389.48 58389.48 58389.48 58389.48 58389.48 58389.48 58389.48
>
> [2,] 58389.48 58389.48 58389.48 58389.48 58389.48 58389.48 58389.48
>
> [3,] 58389.48 58389.48 58389.48 58389.48 58389.48 58389.48 58389.48
>
> [4,] 58389.48 58389.48 58389.48 58389.48 58389.48 58389.48 58389.48
>
> [5,] 58389.48 58389.48 58389.48 58389.48 58389.48 58389.48 58389.48
>
> [6,] 58389.48 58389.48 58389.48 58389.48 58389.48 58389.48 58389.48
>
> [7,] 58389.48 58389.48 58389.48 58389.48 58389.48 58389.48 58389.48
>
> [8,] 58389.48 58389.48 58389.48 58389.48 58389.48 58389.48 58389.48
>
> [9,] 58389.48 58389.48 58389.48 58389.48 58389.48 58389.48 58389.48
>
>[10,] 58389.48 58389.48 58389.48 58389.48 58389.48 58389.48 58389.48
>
>[11,] 58389.48 58389.48 58389.48 58389.48 58389.48 58389.48 58389.48
>
>[12,] 58389.48 58389.48 58389.48 58389.48 58389.48 58389.48 58389.48
>
>[13,] 58389.48 58389.48 58389.48 58389.48 58389.48 58389.48 58389.48
>
>[14,] 58389.48 58389.48 58389.48 58389.48 58389.48 58389.48 58389.48
>
>[15,] 58389.48 58389.48 58389.48 58389.48 58389.48 58389.48 58389.48
>
>Browse[1]>
>
> 
>
>As "commented" above, when I evaluate a single A,B element (i.e. A=0.2,
>B=0.2) I get a different result than when I use OUTER() which should also be
>evaluating at A=0.2, B=0.2??
>
> 
>
>Any help appreciated.  I know I'm probably doing something overlooking
>something simple, but can anyone point it out???
>
> 
>
>Thanks!
>
>-Scott
>
> 
>
>Scott Norton, Ph.D.
>
>Engineering Manager
>
>Nanoplex Technologies, Inc.
>
>2375 Garcia Ave.
>
>Mountain View, CA 94043
>
>www.nanoplextech.com
>
> 
>
>
>	[[alternative HTML version deleted]]
>
>______________________________________________
>R-help at stat.math.ethz.ch mailing list
>https://www.stat.math.ethz.ch/mailman/listinfo/r-help
>  
>



From flom at ndri.org  Tue Oct 28 14:21:40 2003
From: flom at ndri.org (Peter Flom)
Date: Tue, 28 Oct 2003 08:21:40 -0500
Subject: [R] random number generation
Message-ID: <sf9e2738.099@MAIL.NDRI.ORG>

These conditions are mutually exclusive for a lot of reasons, therefore,
there's no way to generate such data.  Briefly, the normal distribution
is fully specified by the mean and variance, the other conditions are
superfluous, and, in some cases, impossible

Please tell us what you are actually trying to do and why you need to
do it, and perhaps we can help.   

Peter

Peter L. Flom, PhD
Assistant Director, Statistics and Data Analysis Core
Center for Drug Use and HIV Research
National Development and Research Institutes
71 W. 23rd St
www.peterflom.com
New York, NY 10010
(212) 845-4485 (voice)
(917) 438-0894 (fax)



>>> nmi13 <NMI13 at student.canterbury.ac.nz> 10/28/2003 3:38:22 AM >>>
Hi every one,

I am trying to generate a normally distributed random variable with the

following descriptive statistics,

min=1, max=99, variance=125, mean=38.32, 1st quartile=38, median=40,
3rd 
quartile=40, skewness=-0.274.

I know the "rnorm" will allow me to simulate random numbers with mean
38.32 
and Sd=11.18(sqrt(125)). But I need to have the above mentioned
descriptive 
statistics for the data that I generate.

I would be thankful to anyone who can help me with this problem.

Regards
Murthy

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://www.stat.math.ethz.ch/mailman/listinfo/r-help



From emmanuel.charpentier at sap.ap-hop-paris.fr  Tue Oct 28 15:16:03 2003
From: emmanuel.charpentier at sap.ap-hop-paris.fr (Emmanuel Charpentier)
Date: Tue, 28 Oct 2003 15:16:03 +0100
Subject: [R] Summary : Whitehead's group sequential procedures
Message-ID: <3F9E7A23.4030306@sap.ap-hop-paris.fr>

Dear List,

I recently asked about any R implementations of Whitehead's methods for 
sequential clinical trials. Here's the summary of answers so far :

1) There is no R public implementation of those methods

2) There exists an interest for such a package, which does things quite 
different from Lan-deMets paradigm (shortly : Whitehead's methods allows 
for unplanned interim analyses in any number, while Lan-deMets plans a 
fixed number of interim analyses at fixed (information) times).

3) I know how to implement simple things : design (with approximations), 
  monitoring and stopping rule. I'm still hacking my way through 
Whitehead's book to understand his procedures for the distribution of 
the total number of patients included (which is a random variable) and 
the final analysis (after study termination).

4) It has been suggested that I contact Scott Emerson, author of an S 
package called seqTrials. Does someone has an address for him ?

I'll start implementing the simple stuff, trying to create a framework 
flexible enough to allow generalization. I'll probably plea for help at 
some time in the future ...

However, don't hold your breath : I'm doing this in my spare time, on 
top of a busy schedule ...

					Emmanuel Charpentier

-- 
Emmanuel Charpentier			Tel : +33-(0)1 40 27 35 98
Secr?tariat Scientifique du CEDIT	Fax : +33-(0)1 40 27 55 65
Assistance Publique - H?pitaux de Paris
3, Avenue Victoria, F-75100 Paris RP - France



From MSchwartz at medanalytics.com  Tue Oct 28 15:51:14 2003
From: MSchwartz at medanalytics.com (Marc Schwartz)
Date: Tue, 28 Oct 2003 08:51:14 -0600
Subject: [R] Summary : Whitehead's group sequential procedures
In-Reply-To: <3F9E7A23.4030306@sap.ap-hop-paris.fr>
References: <3F9E7A23.4030306@sap.ap-hop-paris.fr>
Message-ID: <1067352673.1556.21.camel@localhost.localdomain>

On Tue, 2003-10-28 at 08:16, Emmanuel Charpentier wrote:
> Dear List,
> 

SNIP

> 4) It has been suggested that I contact Scott Emerson, author of an S 
> package called seqTrials. Does someone has an address for him ?

http://www.biostat.washington.edu/index.php?page=facdir&fullprofile=yes&lastname=Emerson

Be aware that if you do a search on the product, it is called
"S+SeqTrial". The Insightful page for it is at
http://www.insightful.com/products/seqtrial/default.asp

Also, FWIW, there is another book that you might want to look at, simply
as an alternative resource:

http://www.amazon.com/exec/obidos/tg/detail/-/0849303168/
Group Sequential Methods with Applications to Clinical Trials
by Christopher Jennison, Bruce W. Turnbull
CRC Press; (September 15, 1999)
ISBN: 0849303168

Others here may have other references that would also be helpful.

HTH,

Marc Schwartz



From tblackw at umich.edu  Tue Oct 28 15:52:15 2003
From: tblackw at umich.edu (Thomas W Blackwell)
Date: Tue, 28 Oct 2003 09:52:15 -0500 (EST)
Subject: [R] outer function problems
In-Reply-To: <001201c39d04$5795f020$6501a8c0@scott>
References: <001201c39d04$5795f020$6501a8c0@scott>
Message-ID: <Pine.SOL.4.58.0310280854150.20347@timepilot.gpcc.itd.umich.edu>

Scott  -

I agree with Spencer Graves that there's a scoping issue here:
Where does function  Dk()  pick up the values for  n0  and  w,
and does it get them from the SAME place when it's called from
inside  FindLikelihood()  as from outside ?

But more important is this one:  All arithmetic on vectors or
matrices is done element by element;  every matrix or array is
treated as a vector (no 'dim' attribute) during this process,
and "the elements of shorter vectors are recycled as necessary"
(quoting from  help("Arithmetic")).  Therefore,

	Dk(seq(-k,k), 0.2, 0.2)

should return a vector of length (2 * k + 1),  and

	Nk * log(Dk())
 	#  (I omit the arguments to Dk() here.)

should produce a vector of length  max(length(Nk), 2 * k + 1)
in which element 1 of Nk is paired with  xk = -k,  element 2
of Nk is paired with  xk = (-k + 1), et cetera.  This product
then has a vector of length  (2 * k + 1)  subtracted from it
and the resulting vector is summed.

Now, maybe you have promised to only call  FindLikelihood()
with an argument  Nk  of length 15 = (2 * k + 1), in which
case all the lengths match and element i from Nk is always
paired with the value (i - 8) in the first argument of Dk(),
but there's certainly a lack of defensive programming here.

An alternate way to calculate the grid of likelihood values
which seems to be your intention is to explicitly build four
four-dimensional arrays named  A, B, xk and Nk, all with the
same dimensions, and with the values changing along only one
dimension in each array.  Then do whatever arithmetic you
want with these four arrays (such as the expressions inside
Dk() and  FindLikelihood()  and collapse the result by summing
over rows or slices or whatever at the end.  The functions
array(), aperm(), matrix() and '%*%' are useful in this process.
This business of four or five-dimensional arrays is one I use
routinely.  The result is equivalent to as.vector(outer( ...)),
but it forces you to think carefully about the various dimensions.

HTH  -  tom blackwell  -  u michigan medical school  -  ann arbor  -

On Mon, 27 Oct 2003, Scott Norton wrote:

> I'm pulling my hair (and there's not much left!) on this one. Basically I'm
> not getting the same result t when I "step" through the program and evaluate
> each element separately than when I use the outer() function in the
> FindLikelihood() function below.
>
>
>
> Here's the functions:
>
>
>
> Dk<- function(xk,A,B)
>
> {
>
> n0 *(A*exp(-0.5*(xk/w)^2) + B)
>
> }
>
>
>
> FindLikelihood <- function(Nk)
>
> {
>
> A <- seq(0.2,3,by=0.2)
>
> B <- seq(0.2,3,by=0.2)
>
> k <-7
>
> L <- outer(A, B, function(A,B) sum( (Nk*log(Dk(seq(-k,k),A,B))) -
> Dk(seq(-k,k),A,B) ))
>
> return(L)
>
> }
>
>
>
>
>
> where Nk <- c(70 , 67 , 75 , 77 , 74 ,102,  75, 104 , 94 , 74 , 78 , 79 , 83
> , 73 , 76)
>
>
>
>
>
> Here's an excerpt from my debug session..
>
>
>
> > Nk
>
>  [1]  70  67  75  77  74 102  75 104  94  74  78  79  83  73  76
>
> > debug(FindLikelihood)
>
> > L<-FindLikelihood(Nk)
>
> debugging in: FindLikelihood(Nk)
>
> debug: {
>
>     A <- seq(0.2, 3, by = 0.2)
>
>     B <- seq(0.2, 3, by = 0.2)
>
>     k <- 7
>
>     L <- outer(A, B, function(A, B) sum((Nk * log(Dk(seq(-k,
>
>         k), A, B))) - Dk(seq(-k, k), A, B)))
>
>     return(L)
>
> }
>
> Browse[1]> n
>
> debug: A <- seq(0.2, 3, by = 0.2)
>
> Browse[1]> n
>
> debug: B <- seq(0.2, 3, by = 0.2)
>
> Browse[1]> n
>
> debug: k <- 7
>
> Browse[1]> n
>
> debug: L <- outer(A, B, function(A, B) sum((Nk * log(Dk(seq(-k, k),
>
>     A, B))) - Dk(seq(-k, k), A, B)))
>
> Browse[1]> sum((Nk * log(Dk(seq(-k, k),0.2,0.2))) - Dk(seq(-k, k), 0.2,
> 0.2))      # WHY DOES THIS LINE GIVE ME THE CORRECT RESULT WHEN I SUBSTITUTE
> 0.2, 0.2 FOR A AND B
>
> [1] 2495.242
>
> Browse[1]> outer(A, B, function(A, B) sum((Nk * log(Dk(seq(-k, k),
>
> +     A, B))) - Dk(seq(-k, k), A, B)))
>
>           [,1]     [,2]     [,3]     [,4]     [,5]     [,6]     [,7]
> [,8]
>
>  [1,] 58389.48 58389.48 58389.48 58389.48 58389.48 58389.48 58389.48
> 58389.48    # BUT ELEMENT (1,1) WHICH SHOULD ALSO BE (A,B) = (0.2, 0.2),
> GIVES THE INCORRECT RESULT????
>
>  [2,] 58389.48 58389.48 58389.48 58389.48 58389.48 58389.48 58389.48
> 58389.48
>
>  [3,] 58389.48 58389.48 58389.48 58389.48 58389.48 58389.48 58389.48
> 58389.48
>
>  [4,] 58389.48 58389.48 58389.48 58389.48 58389.48 58389.48 58389.48
> 58389.48
>
>  [5,] 58389.48 58389.48 58389.48 58389.48 58389.48 58389.48 58389.48
> 58389.48
>
>  [6,] 58389.48 58389.48 58389.48 58389.48 58389.48 58389.48 58389.48
> 58389.48
>
>  [7,] 58389.48 58389.48 58389.48 58389.48 58389.48 58389.48 58389.48
> 58389.48
>
>  [8,] 58389.48 58389.48 58389.48 58389.48 58389.48 58389.48 58389.48
> 58389.48
>
>  [9,] 58389.48 58389.48 58389.48 58389.48 58389.48 58389.48 58389.48
> 58389.48
>
> [10,] 58389.48 58389.48 58389.48 58389.48 58389.48 58389.48 58389.48
> 58389.48
>
> [11,] 58389.48 58389.48 58389.48 58389.48 58389.48 58389.48 58389.48
> 58389.48
>
> [12,] 58389.48 58389.48 58389.48 58389.48 58389.48 58389.48 58389.48
> 58389.48
>
> [13,] 58389.48 58389.48 58389.48 58389.48 58389.48 58389.48 58389.48
> 58389.48
>
> [14,] 58389.48 58389.48 58389.48 58389.48 58389.48 58389.48 58389.48
> 58389.48
>
> [15,] 58389.48 58389.48 58389.48 58389.48 58389.48 58389.48 58389.48
> 58389.48
>
>           [,9]    [,10]    [,11]    [,12]    [,13]    [,14]    [,15]
>
>  [1,] 58389.48 58389.48 58389.48 58389.48 58389.48 58389.48 58389.48
>
>  [2,] 58389.48 58389.48 58389.48 58389.48 58389.48 58389.48 58389.48
>
>  [3,] 58389.48 58389.48 58389.48 58389.48 58389.48 58389.48 58389.48
>
>  [4,] 58389.48 58389.48 58389.48 58389.48 58389.48 58389.48 58389.48
>
>  [5,] 58389.48 58389.48 58389.48 58389.48 58389.48 58389.48 58389.48
>
>  [6,] 58389.48 58389.48 58389.48 58389.48 58389.48 58389.48 58389.48
>
>  [7,] 58389.48 58389.48 58389.48 58389.48 58389.48 58389.48 58389.48
>
>  [8,] 58389.48 58389.48 58389.48 58389.48 58389.48 58389.48 58389.48
>
>  [9,] 58389.48 58389.48 58389.48 58389.48 58389.48 58389.48 58389.48
>
> [10,] 58389.48 58389.48 58389.48 58389.48 58389.48 58389.48 58389.48
>
> [11,] 58389.48 58389.48 58389.48 58389.48 58389.48 58389.48 58389.48
>
> [12,] 58389.48 58389.48 58389.48 58389.48 58389.48 58389.48 58389.48
>
> [13,] 58389.48 58389.48 58389.48 58389.48 58389.48 58389.48 58389.48
>
> [14,] 58389.48 58389.48 58389.48 58389.48 58389.48 58389.48 58389.48
>
> [15,] 58389.48 58389.48 58389.48 58389.48 58389.48 58389.48 58389.48
>
> Browse[1]>
>
>
>
> As "commented" above, when I evaluate a single A,B element (i.e. A=0.2,
> B=0.2) I get a different result than when I use OUTER() which should also be
> evaluating at A=0.2, B=0.2??
>
>
>
> Any help appreciated.  I know I'm probably doing something overlooking
> something simple, but can anyone point it out???
>
>
>
> Thanks!
>
> -Scott
>
>
>
> Scott Norton, Ph.D.
>
> Engineering Manager
>
> Nanoplex Technologies, Inc.
>
> 2375 Garcia Ave.
>
> Mountain View, CA 94043
>
> www.nanoplextech.com
>
>
>
>
> 	[[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
>



From laura at env.leeds.ac.uk  Tue Oct 28 15:57:23 2003
From: laura at env.leeds.ac.uk (Laura Quinn)
Date: Tue, 28 Oct 2003 14:57:23 +0000 (GMT)
Subject: [R] Visualising Moving Vectors
Message-ID: <Pine.LNX.4.44.0310281439350.24312-100000@env-pc-phd13>

I am wanting to plot a series of wind vectors onto a contoured area map
for a series of weather stations (eg arrows showing wind speed/direction
for a particular time snapshot), can someone please advise me how best to
approach this?

My desired end point is to be able to link a time series of such data
together so that I will in effect have a "movie" displaying the evolution
of these wind vectors over time - can anyone suggest how this can be
achieved? I believe that there is a function within Image Magick whereby I
might be able to acheive this?

Thanks in advance!



From tlumley at u.washington.edu  Tue Oct 28 16:08:59 2003
From: tlumley at u.washington.edu (Thomas Lumley)
Date: Tue, 28 Oct 2003 07:08:59 -0800 (PST)
Subject: [R] formula parsing, using parts ...
In-Reply-To: <86d6chr7cp.fsf@coulee.tdb.com>
References: <86vfq9sx7e.fsf@coulee.tdb.com>
	<3F9E301B.3030908@statistik.uni-dortmund.de>
	<86ptghr8ok.fsf@coulee.tdb.com>
	<3F9E3E7D.7060003@statistik.uni-dortmund.de>
	<86d6chr7cp.fsf@coulee.tdb.com>
Message-ID: <Pine.A41.4.58.0310280705210.115680@homer11.u.washington.edu>

On Tue, 28 Oct 2003, Russell Senior wrote:

> >>>>> "Uwe" == Uwe Ligges <ligges at statistik.uni-dortmund.de> writes:
>
>
> Uwe> See ?formula and its "See Also" Section on how to do formula
> Uwe> manipulation. There's also an example on how to construct a
> Uwe> formula.
>
> Russell> In order to use the 'as.formula(paste(response," ~
> Russell> ",factor))' approach, response and factor seem to need to be
> Russell> strings (at least they seem to if response is "log(x)" or the
> Russell> like).  Whereas, for pairwise.t.test they need to be names.
> Russell> What is the proper way to do that?
>

I'd actually advise a different strategy.  Consider:

data(airquality)
formula<- log(Ozone)~factor(Month)

m<-lm(formula,data=data)
a<-anova(m)

mf<-model.frame(lm)

pairwise.t.test(mf[,1], mf[,2])


	-thomas



From yukangtu at hotmail.com  Tue Oct 28 16:28:18 2003
From: yukangtu at hotmail.com (Tu Yu-Kang)
Date: Tue, 28 Oct 2003 15:28:18 +0000
Subject: [R] error message in simulation
Message-ID: <Law15-F5479vISLQ6Qo0003ce5f@hotmail.com>

Dear R-users,

I am a dentist (so forgive me if my question looks stupid) and came across 
a problem when I did simulations to compare a few single level and two 
level regressions.

The simulations were interrupted and an error message came out like 'Error 
in MEestimate(lmeSt, grps) : Singularity in backsolve at level 0, block 1'.

My collegue suggested that this might be due to my codes was not efficient 
and ran out of memory.  If this is the reason, could you please help me 
improve my codes writing.

However, as I slightly changed the parameters, it ran well.  So I suspect 
memory is problem.

I use R 1.8.0 and Windows XP professional.  My computer has a Pentium 4 2.4 
with 512 MB memory.

Thanks in advance.

best regards,

Yu-Kang Tu

Clinical Research Fellow
Leeds Dental Institute
University of Leeds

## change scores simulation
close.screen(all=TRUE)
split.screen(c(3,3))
nitns<-10000
nsims<-100
r<-0.1
param1<-c(1:nitns)
param2<-c(1:nitns)
param3<-c(1:nitns)
param4<-c(1:nitns)
param5<-c(1:nitns)
param6<-c(1:nitns)
param7<-c(1:nitns)
param8<-c(1:nitns)
param9<-c(1:nitns)
param10<-c(1:nitns)
param11<-c(1:nitns)
param12<-c(1:nitns)
param13<-c(1:nitns)
param14<-c(1:nitns)
for(itn in 1:nitns){
g<-rbinom(nsims,1,0.5)
b<-rnorm(nsims,0,1)*10
rn<-rnorm(nsims,0,1)*10
a<-b*r+rn*(1-r^2)^0.5
a<-round(a)+50
a<-a-g*5
b<-round(b)+50
abs.2<-function(x) ifelse(x<1,1,x)
b<-abs.2(b)
c<-b-a
p<-c/b
lm1<-lm(a~g)
lm2<-lm(c~g)
lm3<-lm(p~g)
lm4<-lm(a~b+g)
gr<-c(g,g)
occasion<-rep(0:1,c(nsims,nsims))
occ<-occasion-0.5
ppd<-c(b,a)
h<-rep(0,nsims)
mb<-mean(b)
bppd<-b-mb
bappd<-c(h,bppd)
occgr<-occ*gr
subject<-c(1:nsims)
sub<-c(subject,subject)
library(nlme)
lm5<-lme(ppd~occ+gr+occgr,random=~1|sub)
lm5f<-fixed.effects(lm5)
lm5c<-as.matrix(lm5f)
lm5a<-anova(lm5)
lm6<-lme(ppd~occ+gr+occgr,random=~1|sub,method="ML")
lm6f<-fixed.effects(lm6)
lm6c<-as.matrix(lm6f)
lm6a<-anova(lm6)
lm7<-lme(ppd~occ+gr+occgr+bappd,random=~1|sub,method="ML")
lm7f<-fixed.effects(lm7)
lm7c<-as.matrix(lm7f)
lm7a<-anova(lm7)
param1[itn]<-coef(summary(lm1))[2,1]
param2[itn]<-coef(summary(lm2))[2,1]
param3[itn]<-coef(summary(lm3))[2,1]
param4[itn]<-coef(summary(lm4))[3,1]
param5[itn]<-lm5c[4,1]
param6[itn]<-lm6c[4,1]
param7[itn]<-lm7c[4,1]
param8[itn]<-coef(summary(lm1))[2,4]
param9[itn]<-coef(summary(lm2))[2,4]
param10[itn]<-coef(summary(lm3))[2,4]
param11[itn]<-coef(summary(lm4))[3,4]
param12[itn]<-lm5a[4,4]
param13[itn]<-lm6a[4,4]
param14[itn]<-lm7a[4,4]
}
#the error message came out here.
#But if I change some of the variables to:
b<-rnorm(nsims,0,1)*2
rn<-rnorm(nsims,0,1)*2
a<-b*r+rn*(1-r^2)^0.5
a<-round(a)+7
a<-a-g*2
b<-round(b)+9
abs.1<-function(x) ifelse(x<5,5,x)
b<-abs.1(b)
abs.2<-function(x) ifelse(x<1,1,x)
a<-abs.2(a)

There is no poblem to complete the simulations.

_________________________________________________________________
²{¦b´N¤W MSN ¥xÆWºô¯¸¡G»P¿ËªB¦n¤Íºò±KÁpÃ´¡A§Y®É´x´¤·s»D¡B°]¸g¡B®T¼Öªº³Ì·s°T
®§ http://msn.com.tw



From rebecka_babes at yahoo.co.uk  Tue Oct 28 16:40:32 2003
From: rebecka_babes at yahoo.co.uk (Rebecka)
Date: Tue, 28 Oct 2003 15:40:32 -0000
Subject: [R] Re : Hello
Message-ID: <ECOWS03MtX60LUDA1Xi00013d06@smtp-out3.blueyonder.co.uk>

Hiya, 
Sorry I haven't been I contact with you for a few days, my little 
sister Alena broken here leg.........she's was playing around 
and fell down the stairs.....HeeHee.....shouldn't laugh 
Anyway..... 
I was SO gutted that i missed you the other night...i was really 
looking forward to 'Cam' for you - you made me SO wet last time!! 
Have you got your webcam yet ?? let me know 
i cant wait to see you on your cam....Mmmm.......I suppose better 
go now and do some work.....get back to me and we'll arrange 
another Web cam session.....Mmmm.....we have just installed a 
new webcam system that will allow you to see us much faster 
and clearer, like we were on the television !! check it out here 

www.CzechCamGirls.eu.tt 

I'm normally around most days - as you know :) but Alena will be 
out of 'action' for a few weeks due to her broken leg - plus the 
other girls ! ( But me and Alena are the best....im sure you agree 
..!! ) 

Anyway......i look forward to SEEING you online soon 

Yours.........Waiting...... 

Rebecka 
~ X ~ 
P.s You can also contact me via ICQ.....Me details are on the 
Website above

~ X ~
















NEVER SEND SPAM. IT IS BAD.



From spencer.graves at pdf.com  Tue Oct 28 16:44:19 2003
From: spencer.graves at pdf.com (Spencer Graves)
Date: Tue, 28 Oct 2003 07:44:19 -0800
Subject: [R] formula parsing, using parts ...
In-Reply-To: <Pine.A41.4.58.0310280705210.115680@homer11.u.washington.edu>
References: <86vfq9sx7e.fsf@coulee.tdb.com>	<3F9E301B.3030908@statistik.uni-dortmund.de>	<86ptghr8ok.fsf@coulee.tdb.com>	<3F9E3E7D.7060003@statistik.uni-dortmund.de>	<86d6chr7cp.fsf@coulee.tdb.com>
	<Pine.A41.4.58.0310280705210.115680@homer11.u.washington.edu>
Message-ID: <3F9E8ED3.1040807@pdf.com>

      I got errors from Prof. Lumley's code, but the following 
modification produced for me something that seemed to fit his description: 

data(airquality)
formula<- log(Ozone)~factor(Month)

m<-lm(formula,data=airquality)
a<-anova(m)

mf<-model.frame(m)

pairwise.t.test(mf[,1], mf[,2])

      hope this helps.  spencer graves

Thomas Lumley wrote:

>On Tue, 28 Oct 2003, Russell Senior wrote:
>
>  
>
>>>>>>>"Uwe" == Uwe Ligges <ligges at statistik.uni-dortmund.de> writes:
>>>>>>>              
>>>>>>>
>>Uwe> See ?formula and its "See Also" Section on how to do formula
>>Uwe> manipulation. There's also an example on how to construct a
>>Uwe> formula.
>>
>>Russell> In order to use the 'as.formula(paste(response," ~
>>Russell> ",factor))' approach, response and factor seem to need to be
>>Russell> strings (at least they seem to if response is "log(x)" or the
>>Russell> like).  Whereas, for pairwise.t.test they need to be names.
>>Russell> What is the proper way to do that?
>>
>>    
>>
>
>I'd actually advise a different strategy.  Consider:
>
>data(airquality)
>formula<- log(Ozone)~factor(Month)
>
>m<-lm(formula,data=data)
>a<-anova(m)
>
>mf<-model.frame(lm)
>
>pairwise.t.test(mf[,1], mf[,2])
>
>
>	-thomas
>
>______________________________________________
>R-help at stat.math.ethz.ch mailing list
>https://www.stat.math.ethz.ch/mailman/listinfo/r-help
>  
>



From johannes.huesing at medizin.uni-essen.de  Tue Oct 28 16:44:00 2003
From: johannes.huesing at medizin.uni-essen.de (=?iso-8859-1?Q?=22H=FCsing=2C_Johannes=22?=)
Date: Tue, 28 Oct 2003 16:44:00 +0100
Subject: [R] Visualising Moving Vectors
Message-ID: <B3A80C9C13928B45B2FCE4C43656363A2DE63D@mail-srv02.master.medizin.uni-essen.de>

> My desired end point is to be able to link a time series of such data
> together so that I will in effect have a "movie" displaying 
> the evolution
> of these wind vectors over time - can anyone suggest how this can be
> achieved? I believe that there is a function within Image 
> Magick whereby I
> might be able to acheive this?

Animated gif is one option to assemble pictures to a "movie". Another
way, if you would like to go the TeX-PDF-way, would be texpower 
(texpower.sourceforge.net). 

Cheers 


Johannes



From duncan at research.bell-labs.com  Tue Oct 28 16:52:11 2003
From: duncan at research.bell-labs.com (Duncan Temple Lang)
Date: Tue, 28 Oct 2003 10:52:11 -0500
Subject: [R] Re: Starting and Terminating the JVM for package  SJava
In-Reply-To: <488C02265C6AD611BF200002A542182F03F2A7D1@irnts22.ifp.fr>;
	from Isabelle.ZABALZA-MEZGHANI@ifp.fr on Mon, Oct 27, 2003 at
	04:05:52PM +0100
References: <488C02265C6AD611BF200002A542182F03F2A7D1@irnts22.ifp.fr>
Message-ID: <20031028105211.H804@jessie.research.bell-labs.com>

ZABALZA-MEZGHANI Isabelle wrote:
> Hello,
> 
> I would like to know if there is a possibility to open an R session via Java
> (using the SJava package), then to terminate it, and re-run another.

At present, there is no code in the R system to terminate a session
and shut down the engine.  I have experimented putting it into R but
have not finished it. So it is a feature of R that needs to be added.
Nothing about SJava will prohibit us from using it.

> It seems not to be possible. If this is the case, I would like to understand
> where is the problem or the limitation (is it due to the SJava
> implementation, to the Java behavior, or to the R application).
> In fact, I am interesting in re-starting new R sessions during a same Java
> session to manage memory problems in R with large datasets and numerous
> commands through SJava interface (just to "clean" memory).

You might just call the R function gc() periodically (probably from
within Java).  And if you have R objects as Java references , there
are ways to clear these out too.  So, most likely, you don't actually
want to shut down R and restart it.  Instead, you just want to clean
up.


> 
> Waiting for your help,
> 
> Regards,
> 
> Isabelle.
> 
> 
> Isabelle Zabalza-Mezghani
> IFP - Reservoir Engineering Department
> Rueil-Malmaison / France
> Tel : +33 1 47 52 61 99



From wowen at richmond.edu  Tue Oct 28 17:01:05 2003
From: wowen at richmond.edu (Owen, Jason)
Date: Tue, 28 Oct 2003 11:01:05 -0500
Subject: [R] presentation of software
Message-ID: <C1F927C74082D311A25B00508B5BFF1704E0C126@urmail-oz.richmond.edu>

Hello,

I am considering giving a talk at my university
on R to (mostly) academics.  There wouldn't be any
statisticians, but professors from mathematics,
psychology, economics, etc. who do use some statistical
software in teaching and/or research, and have an acquaintance
with procedures and graphics used in statistics.  Has anyone
given such a talk to a similar audience?  If so, I would be
interested in seeing what you talked about.  Please
send me your talk, outline, or whatever materials you
have.  I want to design an "R is the way" -type talk.

Jason



From tblackw at umich.edu  Tue Oct 28 17:07:33 2003
From: tblackw at umich.edu (Thomas W Blackwell)
Date: Tue, 28 Oct 2003 11:07:33 -0500 (EST)
Subject: [R] error message in simulation
In-Reply-To: <Law15-F5479vISLQ6Qo0003ce5f@hotmail.com>
References: <Law15-F5479vISLQ6Qo0003ce5f@hotmail.com>
Message-ID: <Pine.SOL.4.58.0310281054390.7139@timepilot.gpcc.itd.umich.edu>

Yu-Kang  -

Simulations by their nature use randomly generated data.
Sometimes the random data doesn't contain enough information
to fully determine the parameter estimates for one iteration
or another.  It seems likely that that is what happened here.
The design matrix is singular for one iteration (maybe there
are NO simulated subjects in one arm of the trial) and
backsolve() very properly returns an error message.  On the
very next iteration, with a different randomly-generated data
set, everything should work, again.

The function  try()  allows you to get past these problematic
iterations.  It's better, of course, to figure out what the
requirements for a fully-specified data set are, and arrange
the random generator so that these are guaranteed to be met.

-  tom blackwell  -  u michigan medical school  -  ann arbor  -

On Tue, 28 Oct 2003, Tu Yu-Kang wrote:

> Dear R-users,
>
> I am a dentist (so forgive me if my question looks stupid) and came across
> a problem when I did simulations to compare a few single level and two
> level regressions.
>
> The simulations were interrupted and an error message came out like 'Error
> in MEestimate(lmeSt, grps) : Singularity in backsolve at level 0, block 1'.
>
> My collegue suggested that this might be due to my codes was not efficient
> and ran out of memory.  If this is the reason, could you please help me
> improve my codes writing.
>
> However, as I slightly changed the parameters, it ran well.  So I suspect
> memory is problem.
>
> I use R 1.8.0 and Windows XP professional.  My computer has a Pentium 4 2.4
> with 512 MB memory.
>
> Thanks in advance.
>
> best regards,
>
> Yu-Kang Tu
>
> Clinical Research Fellow
> Leeds Dental Institute
> University of Leeds
>
> ## change scores simulation
> close.screen(all=TRUE)
> split.screen(c(3,3))
> nitns<-10000
> nsims<-100
> r<-0.1
> param1<-c(1:nitns)
> param2<-c(1:nitns)
> param3<-c(1:nitns)
> param4<-c(1:nitns)
> param5<-c(1:nitns)
> param6<-c(1:nitns)
> param7<-c(1:nitns)
> param8<-c(1:nitns)
> param9<-c(1:nitns)
> param10<-c(1:nitns)
> param11<-c(1:nitns)
> param12<-c(1:nitns)
> param13<-c(1:nitns)
> param14<-c(1:nitns)
> for(itn in 1:nitns){
> g<-rbinom(nsims,1,0.5)
> b<-rnorm(nsims,0,1)*10
> rn<-rnorm(nsims,0,1)*10
> a<-b*r+rn*(1-r^2)^0.5
> a<-round(a)+50
> a<-a-g*5
> b<-round(b)+50
> abs.2<-function(x) ifelse(x<1,1,x)
> b<-abs.2(b)
> c<-b-a
> p<-c/b
> lm1<-lm(a~g)
> lm2<-lm(c~g)
> lm3<-lm(p~g)
> lm4<-lm(a~b+g)
> gr<-c(g,g)
> occasion<-rep(0:1,c(nsims,nsims))
> occ<-occasion-0.5
> ppd<-c(b,a)
> h<-rep(0,nsims)
> mb<-mean(b)
> bppd<-b-mb
> bappd<-c(h,bppd)
> occgr<-occ*gr
> subject<-c(1:nsims)
> sub<-c(subject,subject)
> library(nlme)
> lm5<-lme(ppd~occ+gr+occgr,random=~1|sub)
> lm5f<-fixed.effects(lm5)
> lm5c<-as.matrix(lm5f)
> lm5a<-anova(lm5)
> lm6<-lme(ppd~occ+gr+occgr,random=~1|sub,method="ML")
> lm6f<-fixed.effects(lm6)
> lm6c<-as.matrix(lm6f)
> lm6a<-anova(lm6)
> lm7<-lme(ppd~occ+gr+occgr+bappd,random=~1|sub,method="ML")
> lm7f<-fixed.effects(lm7)
> lm7c<-as.matrix(lm7f)
> lm7a<-anova(lm7)
> param1[itn]<-coef(summary(lm1))[2,1]
> param2[itn]<-coef(summary(lm2))[2,1]
> param3[itn]<-coef(summary(lm3))[2,1]
> param4[itn]<-coef(summary(lm4))[3,1]
> param5[itn]<-lm5c[4,1]
> param6[itn]<-lm6c[4,1]
> param7[itn]<-lm7c[4,1]
> param8[itn]<-coef(summary(lm1))[2,4]
> param9[itn]<-coef(summary(lm2))[2,4]
> param10[itn]<-coef(summary(lm3))[2,4]
> param11[itn]<-coef(summary(lm4))[3,4]
> param12[itn]<-lm5a[4,4]
> param13[itn]<-lm6a[4,4]
> param14[itn]<-lm7a[4,4]
> }
> #the error message came out here.
> #But if I change some of the variables to:
> b<-rnorm(nsims,0,1)*2
> rn<-rnorm(nsims,0,1)*2
> a<-b*r+rn*(1-r^2)^0.5
> a<-round(a)+7
> a<-a-g*2
> b<-round(b)+9
> abs.1<-function(x) ifelse(x<5,5,x)
> b<-abs.1(b)
> abs.2<-function(x) ifelse(x<1,1,x)
> a<-abs.2(a)
>
> There is no poblem to complete the simulations.
>
> _________________________________________________________________
> ²{¦b´N¤W MSN ¥xÆWºô¯¸¡G»P¿ËªB¦n¤Íºò±KÁpÃ´¡A§Y®É´x´¤·s»D¡B°]¸g¡B®T¼Öªº³Ì·s°T
> ®§ http://msn.com.tw
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
>



From nortonsm at verizon.net  Tue Oct 28 17:19:02 2003
From: nortonsm at verizon.net (Scott Norton)
Date: Tue, 28 Oct 2003 11:19:02 -0500
Subject: [R] outer function problems
In-Reply-To: <3F9E6B4D.9030005@pdf.com>
Message-ID: <003001c39d6f$33d65100$6501a8c0@scott>

Thanks Spencer and Tom for your help!

      Besides the other errors, I realized last night that I'm making a
fundmental error in my interpretation of the outer function.  The following
short code snippet highlights my confusion.

f<-function(A,B) { sum(A+B) }
outer(1:3,2:4,f)
     [,1] [,2] [,3]
[1,]   45   45   45
[2,]   45   45   45
[3,]   45   45   45

I had *thought* that outer() would give:
     [,1] [,2] [,3]
[1,]   3     4    5
[2,]   4     5    6
[3,]   5     6    7

ie. take each combination from A = 1,2,3; B=2,3,4 such as A=1,B=2 put it in
the sum function, get [1,1]=3 ... 
Then grab A[2]=2,B[1]=2, put them in the sum() function to get [2,1]=4,
etc... That "seems" to be the way the instructions explain "outer", i.e.
element-by-element computation of FUN() 
"Description:

     The outer product of the arrays 'X' and 'Y' is the array 'A' with
     dimension 'c(dim(X), dim(Y))' where element 'A[c(arrayindex.x,
     arrayindex.y)] = FUN(X[arrayindex.x], Y[arrayindex.y], ...)'."

Since my interpretation is *definitely* wrong, could someone put in words
how "OUTER" handles the argument vectors and the functional call with
reference to the preceding example?  
Also, what I need to happen in my code is to actually take each combination
of elements from vectors, A and B, and "feed" them repeatedly into a
function, generating a matrix of results.  How then do I do that?

Thanks in advance!!! 
-Scott

Scott Norton, Ph.D.
Engineering Manager
Nanoplex Technologies, Inc.
2375 Garcia Ave.
Mountain View, CA 94043
www.nanoplextech.com


-----Original Message-----
From: Spencer Graves [mailto:spencer.graves at pdf.com] 
Sent: Tuesday, October 28, 2003 8:13 AM
To: Scott Norton
Cc: r-help at stat.math.ethz.ch
Subject: Re: [R] outer function problems

      I don't know that this is your problem, but I see a potential 
scoping issue:  It is not obvious to me where Dk is getting n0 and w.  
I've solved this kind of problem in the past by declaring n0 and w as 
explicit arguments to Dk and then passing them explicitly via "..." in 
"outer".  In general, I prefer to avoid accessing globals from within 
functions.  This may not help you here, but it might help in the future. 

      hope this helps.  spencer graves

Scott Norton wrote:

>I'm pulling my hair (and there's not much left!) on this one. Basically I'm
>not getting the same result t when I "step" through the program and
evaluate
>each element separately than when I use the outer() function in the
>FindLikelihood() function below.
>
> 
>
>Here's the functions:
>
> 
>
>Dk<- function(xk,A,B) 
>
>{
>
>n0 *(A*exp(-0.5*(xk/w)^2) + B)
>
>}
>
> 
>
>FindLikelihood <- function(Nk)
>
>{
>
>A <- seq(0.2,3,by=0.2)
>
>B <- seq(0.2,3,by=0.2)
>
>k <-7
>
>L <- outer(A, B, function(A,B) sum( (Nk*log(Dk(seq(-k,k),A,B))) -
>Dk(seq(-k,k),A,B) ))
>
>return(L)
>
>}
>
> 
>
> 
>
>where Nk <- c(70 , 67 , 75 , 77 , 74 ,102,  75, 104 , 94 , 74 , 78 , 79 ,
83
>, 73 , 76)
>
> 
>
> 
>
>Here's an excerpt from my debug session..
>
> 
>
>  
>
>>Nk
>>    
>>
>
> [1]  70  67  75  77  74 102  75 104  94  74  78  79  83  73  76
>
>  
>
>>debug(FindLikelihood)
>>    
>>
>
>  
>
>>L<-FindLikelihood(Nk)
>>    
>>
>
>debugging in: FindLikelihood(Nk)
>
>debug: {
>
>    A <- seq(0.2, 3, by = 0.2)
>
>    B <- seq(0.2, 3, by = 0.2)
>
>    k <- 7
>
>    L <- outer(A, B, function(A, B) sum((Nk * log(Dk(seq(-k, 
>
>        k), A, B))) - Dk(seq(-k, k), A, B)))
>
>    return(L)
>
>}
>
>Browse[1]> n
>
>debug: A <- seq(0.2, 3, by = 0.2)
>
>Browse[1]> n
>
>debug: B <- seq(0.2, 3, by = 0.2)
>
>Browse[1]> n
>
>debug: k <- 7
>
>Browse[1]> n
>
>debug: L <- outer(A, B, function(A, B) sum((Nk * log(Dk(seq(-k, k), 
>
>    A, B))) - Dk(seq(-k, k), A, B)))
>
>Browse[1]> sum((Nk * log(Dk(seq(-k, k),0.2,0.2))) - Dk(seq(-k, k), 0.2,
>0.2))      # WHY DOES THIS LINE GIVE ME THE CORRECT RESULT WHEN I
SUBSTITUTE
>0.2, 0.2 FOR A AND B
>
>[1] 2495.242
>
>Browse[1]> outer(A, B, function(A, B) sum((Nk * log(Dk(seq(-k, k), 
>
>+     A, B))) - Dk(seq(-k, k), A, B)))
>
>          [,1]     [,2]     [,3]     [,4]     [,5]     [,6]     [,7]
>[,8]
>
> [1,] 58389.48 58389.48 58389.48 58389.48 58389.48 58389.48 58389.48
>58389.48    # BUT ELEMENT (1,1) WHICH SHOULD ALSO BE (A,B) = (0.2, 0.2),
>GIVES THE INCORRECT RESULT????
>
> [2,] 58389.48 58389.48 58389.48 58389.48 58389.48 58389.48 58389.48
>58389.48
>
> [3,] 58389.48 58389.48 58389.48 58389.48 58389.48 58389.48 58389.48
>58389.48
>
> [4,] 58389.48 58389.48 58389.48 58389.48 58389.48 58389.48 58389.48
>58389.48
>
> [5,] 58389.48 58389.48 58389.48 58389.48 58389.48 58389.48 58389.48
>58389.48
>
> [6,] 58389.48 58389.48 58389.48 58389.48 58389.48 58389.48 58389.48
>58389.48
>
> [7,] 58389.48 58389.48 58389.48 58389.48 58389.48 58389.48 58389.48
>58389.48
>
> [8,] 58389.48 58389.48 58389.48 58389.48 58389.48 58389.48 58389.48
>58389.48
>
> [9,] 58389.48 58389.48 58389.48 58389.48 58389.48 58389.48 58389.48
>58389.48
>
>[10,] 58389.48 58389.48 58389.48 58389.48 58389.48 58389.48 58389.48
>58389.48
>
>[11,] 58389.48 58389.48 58389.48 58389.48 58389.48 58389.48 58389.48
>58389.48
>
>[12,] 58389.48 58389.48 58389.48 58389.48 58389.48 58389.48 58389.48
>58389.48
>
>[13,] 58389.48 58389.48 58389.48 58389.48 58389.48 58389.48 58389.48
>58389.48
>
>[14,] 58389.48 58389.48 58389.48 58389.48 58389.48 58389.48 58389.48
>58389.48
>
>[15,] 58389.48 58389.48 58389.48 58389.48 58389.48 58389.48 58389.48
>58389.48
>
>          [,9]    [,10]    [,11]    [,12]    [,13]    [,14]    [,15]
>
> [1,] 58389.48 58389.48 58389.48 58389.48 58389.48 58389.48 58389.48
>
> [2,] 58389.48 58389.48 58389.48 58389.48 58389.48 58389.48 58389.48
>
> [3,] 58389.48 58389.48 58389.48 58389.48 58389.48 58389.48 58389.48
>
> [4,] 58389.48 58389.48 58389.48 58389.48 58389.48 58389.48 58389.48
>
> [5,] 58389.48 58389.48 58389.48 58389.48 58389.48 58389.48 58389.48
>
> [6,] 58389.48 58389.48 58389.48 58389.48 58389.48 58389.48 58389.48
>
> [7,] 58389.48 58389.48 58389.48 58389.48 58389.48 58389.48 58389.48
>
> [8,] 58389.48 58389.48 58389.48 58389.48 58389.48 58389.48 58389.48
>
> [9,] 58389.48 58389.48 58389.48 58389.48 58389.48 58389.48 58389.48
>
>[10,] 58389.48 58389.48 58389.48 58389.48 58389.48 58389.48 58389.48
>
>[11,] 58389.48 58389.48 58389.48 58389.48 58389.48 58389.48 58389.48
>
>[12,] 58389.48 58389.48 58389.48 58389.48 58389.48 58389.48 58389.48
>
>[13,] 58389.48 58389.48 58389.48 58389.48 58389.48 58389.48 58389.48
>
>[14,] 58389.48 58389.48 58389.48 58389.48 58389.48 58389.48 58389.48
>
>[15,] 58389.48 58389.48 58389.48 58389.48 58389.48 58389.48 58389.48
>
>Browse[1]>
>
> 
>
>As "commented" above, when I evaluate a single A,B element (i.e. A=0.2,
>B=0.2) I get a different result than when I use OUTER() which should also
be
>evaluating at A=0.2, B=0.2??
>
> 
>
>Any help appreciated.  I know I'm probably doing something overlooking
>something simple, but can anyone point it out???
>
> 
>
>Thanks!
>
>-Scott
>
> 
>
>Scott Norton, Ph.D.
>
>Engineering Manager
>
>Nanoplex Technologies, Inc.
>
>2375 Garcia Ave.
>
>Mountain View, CA 94043
>
>www.nanoplextech.com
>
> 
>
>
>	[[alternative HTML version deleted]]
>
>______________________________________________
>R-help at stat.math.ethz.ch mailing list
>https://www.stat.math.ethz.ch/mailman/listinfo/r-help
>  
>



From bill.shipley at usherbrooke.ca  Tue Oct 28 17:31:42 2003
From: bill.shipley at usherbrooke.ca (Bill Shipley)
Date: Tue, 28 Oct 2003 11:31:42 -0500
Subject: [R] setting up complicated ANOVA in R
Message-ID: <01c701c39d70$f85df130$8d1ad284@BIO041>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20031028/2108afae/attachment.pl

From ripley at stats.ox.ac.uk  Tue Oct 28 18:27:49 2003
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Tue, 28 Oct 2003 17:27:49 +0000 (GMT)
Subject: [R] setting up complicated ANOVA in R
In-Reply-To: <01c701c39d70$f85df130$8d1ad284@BIO041>
Message-ID: <Pine.LNX.4.44.0310281725550.9637-100000@gannet.stats>

Error can be a formula, and would normally be in split-plot designs.
Examples in MASS4, chapter 10, for example.

Unless you have exact balance, use lme.

On Tue, 28 Oct 2003, Bill Shipley wrote:

> Hello.  I am about to do a rather complicated analysis and am not sure
> how to do it.  The experiment has a split-plot design and also repeated
> measures.  Both of these complications require one to define an error
> term and it seems that one cannot specify two such terms.  The
> split-plot command is:
> 
>  
> 
> aov(y~covariates +A*B+Error(C), data=) where A and B are the fixed
> effects and C is the plot-level source of error.
> 
>  
> 
> The repeated measures command is:
> 
> aov(y~covariates+A*B*time + Error(subject), data=) where subject is the
> error source for the repeated measures over time.
> 
>  
> 
> Can these be somehow combined to include a split-plot &
> repeated-measures design?  If not, can I perhaps use a mixed-model
> analysis with random subjects nested within the whole-plot?
> 
>  
> 
> Any suggestions or leads are appreciated.
> 
>  
> 
> Bill Shipley
> 
> Associate Editor, Ecology
> 
> North American Editor, Annals of Botany
> 
> D?partement de biologie, Universit? de Sherbrooke,
> 
> Sherbrooke (Qu?bec) J1K 2R1 CANADA
> 
> Bill.Shipley at USherbrooke.ca
> 
>  <http://callisto.si.usherb.ca:8080/bshipley/>
> http://callisto.si.usherb.ca:8080/bshipley/
> 
>  
> 
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> 
> 

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From andy_liaw at merck.com  Tue Oct 28 18:43:39 2003
From: andy_liaw at merck.com (Liaw, Andy)
Date: Tue, 28 Oct 2003 12:43:39 -0500
Subject: [R] problem with  the installed R script
Message-ID: <3A822319EB35174CA3714066D590DCD50205CD65@usrymx25.merck.com>

Dear R-help,

I had a problem running a Perl script on AIX inside pipe() or system().  The
Perl script was not finding some modules when run from within R.  The
sysadmin tracked it down to a problem with R_LD_LIBRARY_PATH in the
installed /usr/local/bin/R script:

original:
:
${R_LD_LIBRARY_PATH=${R_HOME}/bin:/opt/freeware/lib:/usr/local/lib:/opt/free
ware/lib:/usr/local/lib:/opt/freeware/lib:/usr/local/lib:${R_HOME}/bin}

new:
:
${R_LD_LIBRARY_PATH=${R_HOME}/bin:/usr/local/lib:/opt/freeware/lib:/usr/loca
l/lib:/opt/freeware/lib:/usr/local/lib:/opt/freeware/lib:/usr/local/lib:${R_
HOME}/bin}

I was told that our installation of Perl must look in /usr/local/lib for the
modules to load.  This directory was at the end of "original" path and
therefore Perl was not able to load the modules.  Does anyone know what, if
anything, needs to be done at the configure step to correct this problem, so
we don't need to fix this by hand every time?

BTW, this is R-1.7.1 (they've been having problem getting 1.8.0 to compile
on that system).

Thanks!
Andy


Andy Liaw, PhD
Biometrics Research      PO Box 2000, RY33-300     
Merck Research Labs           Rahway, NJ 07065
mailto:andy_liaw at merck.com        732-594-0820



From pavlicov at stat.ohio-state.edu  Tue Oct 28 18:43:34 2003
From: pavlicov at stat.ohio-state.edu (Martina Pavlicova)
Date: Tue, 28 Oct 2003 12:43:34 -0500 (EST)
Subject: [R] 'levelplot' with an option 'at'
Message-ID: <Pine.GSO.4.58.0310281239350.1775@spatial.stat.ohio-state.edu>


Hi all,
I encountered a difference between versions 1.6.1 and 1.7.0 when using
levelplot with an option 'at'. Here are the specs of the two platforms
used:

> R.version
         _
platform sparc-sun-solaris2.8
arch     sparc
os       solaris2.8
system   sparc, solaris2.8
status
major    1
minor    6.1
year     2002
month    11
day      01
language R

> R.version
         _
platform i686-pc-linux-gnu
arch     i686
os       linux-gnu
system   i686, linux-gnu
status
major    1
minor    7.0
year     2003
month    04
day      16
language R


I created an easy example of two levelplots (one without an option 'at'
and one with an option 'at') which I run through version 1.6.1. The plot
are called:
        version161.without_at.jpg
        version161.with_at.jpg

After update to version 1.7.0, I run the same two plots and I got the
following files:
        version170.without_at.jpg
        version170.with_at.jpg
When I don't include the option 'at' into a levelplot, the plots
version161.without_at.jpg and version170.without_at.jpg are similar
(differ only in labels for contours). BUT if I include the option 'at',
version 1.7.0 produces very different picture which I believe is wrong
(compare files version161.with_at.jpg and version170.with_at.jpg). WHY is
that? Am I missing something?

I have attached all 4 plots and also the commands I used to create the
small example.

Thank you for all your help.

Martina Pavlicova

---------------------------------------------------------------------
library(lattice)

x <- row(matrix(NA,11,11))-6
y <- col(matrix(NA,11,11))-6
z <- x*y

jpeg("version161.without_at.jpg")
foo <- levelplot(z~x*y,contour=T, cuts=5,
                 ##at=c(-10,-5,0,5,10),
                 panel=function(x,y,...){
                   panel.levelplot(x,y,  ...)
                 })
print(foo)
dev.off()

jpeg("version161.with_at.jpg")
foo <- levelplot(z~x*y,contour=T, cuts=5,
                 at=c(-10,-5,0,5,10),
                 panel=function(x,y,...){
                   panel.levelplot(x,y,  ...)
                 })
print(foo)
dev.off()



--------------------------------------------------------------------------
Department of Statistics             Office Phone: (614) 292-1567
1958 Neil Avenue, 304E Cockins Hall  FAX: (614) 292-2096
The Ohio State University            E-mail: pavlicov at stat.ohio-state.edu
Columbus, OH 43210-1247              www.stat.ohio-state.edu/~pavlicov

From tmurph6 at po-box.mcgill.ca  Tue Oct 28 19:57:36 2003
From: tmurph6 at po-box.mcgill.ca (Tanya Murphy)
Date: Tue, 28 Oct 2003 13:57:36 -0500
Subject: [R] Confidence ellipse for correlation
Message-ID: <3FC34763@webmail.mcgill.ca>

Hello,

SAS' point and click interface has the option of produce a scatterplot with a 
superimposed confidence ellipse for the correlation coefficient. Since I 
generally like R so much better, I would like to reproduce this in R. I've 
been playing with the ellipse package. In order to have the points and the 
ellipse on the same graph I've done the following. 
(Load ellipse package...)
> data(Puromycin)
> attach(Puromycin)
> my<-mean(rate)
> mx<-mean(conc)
> sdy<-sd(rate)
> sdx<-sd(conc)
> r<-cor(conc,rate)
> plot(ellipse(r,scale=c(sdx,sdy),centre=c(mx,my)),type='l')
> points(conc,rate)

1) Is my use of 'scale' and 'centre' theoretically correct?
2) Is there a more efficient way to get the 5 parameters? (I guess I could 
write a little function, but has it already been done?)

The non-linear relationship between these variables brings up another point: 
Is there a way to plot a contour (empirical?) containing, say, 95% of the 
values.

Thanks for your time!

Tanya



From Ted.Harding at nessie.mcc.ac.uk  Tue Oct 28 19:58:10 2003
From: Ted.Harding at nessie.mcc.ac.uk ( (Ted Harding))
Date: Tue, 28 Oct 2003 18:58:10 -0000 (GMT)
Subject: [R] Visualising Moving Vectors
In-Reply-To: <Pine.LNX.4.44.0310281439350.24312-100000@env-pc-phd13>
Message-ID: <XFMail.031028185810.Ted.Harding@nessie.mcc.ac.uk>

On 28-Oct-03 Laura Quinn wrote:
> I am wanting to plot a series of wind vectors onto a contoured area map
> for a series of weather stations (eg arrows showing wind
> speed/direction for a particular time snapshot), can someone please
> advise me how best to approach this?
> 
> My desired end point is to be able to link a time series of such data
> together so that I will in effect have a "movie" displaying the
> evolution of these wind vectors over time - can anyone suggest how
> this can be achieved? I believe that there is a function within Image
> Magick whereby I might be able to acheive this?
> 
> Thanks in advance!

ImageMagick (which is a suite of several stand-alone programs) includes
the program 'animate' which does just this. If you have your sequence
of files in alphabetical sort order (e.g. view001.png, view002.png, ...)
then

  animate view*.png

will cycle through them on order (and rather briskly; however, you
can use the "-delay" option to choose your frame speed). There's
also a facility to merge a sequence of image files into a single
file, an "animated GIF", which can be "played" on any standard
Web Browser, only I don't recall the details.

Hoping this helps,
Ted.


--------------------------------------------------------------------
E-Mail: (Ted Harding) <Ted.Harding at nessie.mcc.ac.uk>
Fax-to-email: +44 (0)870 167 1972
Date: 28-Oct-03                                       Time: 18:58:10
------------------------------ XFMail ------------------------------



From Ted.Harding at nessie.mcc.ac.uk  Tue Oct 28 19:23:42 2003
From: Ted.Harding at nessie.mcc.ac.uk ( (Ted Harding))
Date: Tue, 28 Oct 2003 18:23:42 -0000 (GMT)
Subject: [R] Loading a "sub-package"
Message-ID: <XFMail.031028182342.Ted.Harding@nessie.mcc.ac.uk>

Hi Folks,

The inspiration for this query is described below, but it
prompts a general question:

If one wants to use only one or a few functions from a library,
is there a way to load only these, without loading the library,
short of going into the package source and extracting what is
needed (including of course any auxiliary functions and compiled
code they may depend on)?

What prompted this is that I needed to simulate some MVnormal data.
There's a useful function 'mvrnorm' which does just that, in the
library/package MASS. So that's what I used.

But MASS is huge! Hence the query. (I've also had occasion to filch
single functions from other libraries as well).

(Of course, in reality I've noticed that 'mvrnorm' is a few lines
of pure R code which can easily be lifted out and run "stand-alone",
so the issue is not really a problem in this case. But I think the
question is a good one, well exemplified by the case described.)

With thanks,
Ted.


--------------------------------------------------------------------
E-Mail: (Ted Harding) <Ted.Harding at nessie.mcc.ac.uk>
Fax-to-email: +44 (0)870 167 1972
Date: 28-Oct-03                                       Time: 18:23:42
------------------------------ XFMail ------------------------------



From tlumley at u.washington.edu  Tue Oct 28 20:15:17 2003
From: tlumley at u.washington.edu (Thomas Lumley)
Date: Tue, 28 Oct 2003 11:15:17 -0800 (PST)
Subject: [R] outer function problems
In-Reply-To: <003001c39d6f$33d65100$6501a8c0@scott>
References: <003001c39d6f$33d65100$6501a8c0@scott>
Message-ID: <Pine.A41.4.58.0310281107330.87710@homer04.u.washington.edu>

On Tue, 28 Oct 2003, Scott Norton wrote:

> Thanks Spencer and Tom for your help!
>
>       Besides the other errors, I realized last night that I'm making a
> fundmental error in my interpretation of the outer function.  The following
> short code snippet highlights my confusion.
>
> f<-function(A,B) { sum(A+B) }
> outer(1:3,2:4,f)
>      [,1] [,2] [,3]
> [1,]   45   45   45
> [2,]   45   45   45
> [3,]   45   45   45
>
> I had *thought* that outer() would give:
>      [,1] [,2] [,3]
> [1,]   3     4    5
> [2,]   4     5    6
> [3,]   5     6    7

This is actually a FAQ


> ie. take each combination from A = 1,2,3; B=2,3,4 such as A=1,B=2 put it in
> the sum function, get [1,1]=3 ...
> Then grab A[2]=2,B[1]=2, put them in the sum() function to get [2,1]=4,
> etc... That "seems" to be the way the instructions explain "outer", i.e.
> element-by-element computation of FUN()
> "Description:
>
>      The outer product of the arrays 'X' and 'Y' is the array 'A' with
>      dimension 'c(dim(X), dim(Y))' where element 'A[c(arrayindex.x,
>      arrayindex.y)] = FUN(X[arrayindex.x], Y[arrayindex.y], ...)'."
>
> Since my interpretation is *definitely* wrong, could someone put in words
> how "OUTER" handles the argument vectors and the functional call with
> reference to the preceding example?

As the description says, outer() constructs *one* call to FUN, replicating
all the arguments.

As the Details section says
     'FUN' must be a function (or the name of it) which expects at
     least two arguments and which operates elementwise on arrays.

Your function f() doesn't.


> Also, what I need to happen in my code is to actually take each combination
> of elements from vectors, A and B, and "feed" them repeatedly into a
> function, generating a matrix of results.  How then do I do that?

One general-purpose way is to use mapply

outer(a,b, function(ai,bj) mapply(f,ai,bj))

The reason this isn't the default is that it is fairly slow.  If your
function f() can be vectorised then that will give much better
performance. For example, compare
  outer(a,b, function(ai,bj) mapply(sum,ai,bj))
and
  outer(a,b, "+")
on largish a and b.

An automatic solution *has to* use a loop, and length(a)*length(b)
evaluations of the function.

	-thomas



From ripley at stats.ox.ac.uk  Tue Oct 28 20:20:43 2003
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Tue, 28 Oct 2003 19:20:43 +0000 (GMT)
Subject: [R] Confidence ellipse for correlation
In-Reply-To: <3FC34763@webmail.mcgill.ca>
Message-ID: <Pine.LNX.4.44.0310281912000.9774-100000@gannet.stats>

On Tue, 28 Oct 2003, Tanya Murphy wrote:

> Hello,
> 
> SAS' point and click interface has the option of produce a scatterplot with a 
> superimposed confidence ellipse for the correlation coefficient. Since I 
> generally like R so much better, I would like to reproduce this in R. I've 
> been playing with the ellipse package. In order to have the points and the 
> ellipse on the same graph I've done the following. 
> (Load ellipse package...)
> > data(Puromycin)
> > attach(Puromycin)
> > my<-mean(rate)
> > mx<-mean(conc)
> > sdy<-sd(rate)
> > sdx<-sd(conc)
> > r<-cor(conc,rate)
> > plot(ellipse(r,scale=c(sdx,sdy),centre=c(mx,my)),type='l')
> > points(conc,rate)
> 
> 1) Is my use of 'scale' and 'centre' theoretically correct?

Depends on whose theory you have in mind!  This is not `a confidence
ellipse for the correlation coefficient', as confidence ellipses are for
pairs of parameters, not variables.  It seems to be a plot of a contour of
the fitted bivariate normal.

> 2) Is there a more efficient way to get the 5 parameters? (I guess I could 
> write a little function, but has it already been done?)

You could do things like
mxy <- mean(Puromycin[c("rate", "conc")])
sxy <- sapply(Puromycin[c("rate", "conc")], sd)

> The non-linear relationship between these variables brings up another point: 
> Is there a way to plot a contour (empirical?) containing, say, 95% of the 
> values.

Yes.  You need a 2D density estimate (e.g. kde2d in MASS) then compute the
density values at the points and draw the contour of the density which
includes 95% of the points (at a level computed from the sorted values via
quantile()).

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From ripley at stats.ox.ac.uk  Tue Oct 28 20:35:45 2003
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Tue, 28 Oct 2003 19:35:45 +0000 (GMT)
Subject: [R] Loading a "sub-package"
In-Reply-To: <XFMail.031028182342.Ted.Harding@nessie.mcc.ac.uk>
Message-ID: <Pine.LNX.4.44.0310281928380.9837-100000@gannet.stats>

On Tue, 28 Oct 2003 Ted.Harding at nessie.mcc.ac.uk wrote:

> But MASS is huge! Hence the query. (I've also had occasion to filch
> single functions from other libraries as well).

MASS is *not* huge, and indeed is negligible compared to what is already
loaded.  nlme might be large, but few packages are noticeable and none are
huge ....

Luke and I (but principally Luke) have been experimenting with 
load-on-demand for R objects, and indeed MASS already does that for its 
data objects.  It's possible that this will be a non-issue by the next 
non-patch release.

Some data (R 1.8.0 --vanilla)

> gc()
         used (Mb) gc trigger (Mb)
Ncells 416460 11.2     597831   16
Vcells 113224  0.9     786432    6
> library(MASS)
> gc()
         used (Mb) gc trigger (Mb)
Ncells 463337 12.4     667722 17.9
Vcells 121995  1.0     786432  6.0

I'd fail any student who said `MASS was huge'.

I assume you don't have methods loaded if you are concerned about 
performance ....

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From morozov at uclink.berkeley.edu  Tue Oct 28 21:04:51 2003
From: morozov at uclink.berkeley.edu (morozov)
Date: Tue, 28 Oct 2003 12:04:51 -0800
Subject: [R] strptime command in R
Message-ID: <401BB5C1@bearmail.berkeley.edu>

Hello all:

I have a column of times in format

   x
"16:30:00"
"16:30:03"
"16:59:00"
etc

which I need to convert into time variables and do some operations on.

I do the command y<-strptime(x,"%H:%M:%S"). This executes almost istantly (for 
a column x of length 1000 in Windows, but in Unix, where I run my production 
jobs, this takes over 4 minutes. I know that generally my Unix box is much 
more powerfull than my Win machine, and R runs generally faster on Unix, but 
this particular command is very very slow. Why is that? How can I speed that 
up without having to parse the strings by hand?

Thank you very much,
Vlad.



From morozov at uclink.berkeley.edu  Tue Oct 28 21:10:07 2003
From: morozov at uclink.berkeley.edu (morozov)
Date: Tue, 28 Oct 2003 12:10:07 -0800
Subject: [R] R performance on Unix
Message-ID: <401BD1B2@bearmail.berkeley.edu>

Hi
I'm observing a huge difference in the performance speed of R on Windows and 
Unix, even though I know that my Unix machine is much more powerful than my 
Win machine. 
In particular, any character processing task is very time consuming on Unix.

strptime(x,"%H:%M:%S") is about 10 times slower on Unix for vector x of the 
length of ~ 500. read.table() also is very slow. is there any way to speed up 
these ?

Thanks a lot,
Vlad.



From pavlicov at stat.ohio-state.edu  Tue Oct 28 21:13:49 2003
From: pavlicov at stat.ohio-state.edu (Martina Pavlicova)
Date: Tue, 28 Oct 2003 15:13:49 -0500 (EST)
Subject: [R] 'levelplot' with an option 'at'
In-Reply-To: <Pine.GSO.4.58.0310281239350.1775@spatial.stat.ohio-state.edu>
References: <Pine.GSO.4.58.0310281239350.1775@spatial.stat.ohio-state.edu>
Message-ID: <Pine.GSO.4.58.0310281511300.1775@spatial.stat.ohio-state.edu>


Hi again,

since the attached plots did not go through, I created a quick web-page,
where all the plots are posted. Here it is:

http://www.stat.ohio-state.edu/~pavlicov/levelplot/

Thanks again.

Martina Pavlicova
--------------------------------------------------------------------------
Department of Statistics             Office Phone: (614) 292-1567
1958 Neil Avenue, 304E Cockins Hall  FAX: (614) 292-2096
The Ohio State University            E-mail: pavlicov at stat.ohio-state.edu
Columbus, OH 43210-1247              www.stat.ohio-state.edu/~pavlicov


On Tue, 28 Oct 2003, Martina Pavlicova wrote:

>
> Hi all,
> I encountered a difference between versions 1.6.1 and 1.7.0 when using
> levelplot with an option 'at'. Here are the specs of the two platforms
> used:
>
> > R.version
>          _
> platform sparc-sun-solaris2.8
> arch     sparc
> os       solaris2.8
> system   sparc, solaris2.8
> status
> major    1
> minor    6.1
> year     2002
> month    11
> day      01
> language R
>
> > R.version
>          _
> platform i686-pc-linux-gnu
> arch     i686
> os       linux-gnu
> system   i686, linux-gnu
> status
> major    1
> minor    7.0
> year     2003
> month    04
> day      16
> language R
>
>
> I created an easy example of two levelplots (one without an option 'at'
> and one with an option 'at') which I run through version 1.6.1. The plot
> are called:
>         version161.without_at.jpg
>         version161.with_at.jpg
>
> After update to version 1.7.0, I run the same two plots and I got the
> following files:
>         version170.without_at.jpg
>         version170.with_at.jpg
> When I don't include the option 'at' into a levelplot, the plots
> version161.without_at.jpg and version170.without_at.jpg are similar
> (differ only in labels for contours). BUT if I include the option 'at',
> version 1.7.0 produces very different picture which I believe is wrong
> (compare files version161.with_at.jpg and version170.with_at.jpg). WHY is
> that? Am I missing something?
>
> I have attached all 4 plots and also the commands I used to create the
> small example.
>
> Thank you for all your help.
>
> Martina Pavlicova
>
> ---------------------------------------------------------------------
> library(lattice)
>
> x <- row(matrix(NA,11,11))-6
> y <- col(matrix(NA,11,11))-6
> z <- x*y
>
> jpeg("version161.without_at.jpg")
> foo <- levelplot(z~x*y,contour=T, cuts=5,
>                  ##at=c(-10,-5,0,5,10),
>                  panel=function(x,y,...){
>                    panel.levelplot(x,y,  ...)
>                  })
> print(foo)
> dev.off()
>
> jpeg("version161.with_at.jpg")
> foo <- levelplot(z~x*y,contour=T, cuts=5,
>                  at=c(-10,-5,0,5,10),
>                  panel=function(x,y,...){
>                    panel.levelplot(x,y,  ...)
>                  })
> print(foo)
> dev.off()
>
>
>
> --------------------------------------------------------------------------
> Department of Statistics             Office Phone: (614) 292-1567
> 1958 Neil Avenue, 304E Cockins Hall  FAX: (614) 292-2096
> The Ohio State University            E-mail: pavlicov at stat.ohio-state.edu
> Columbus, OH 43210-1247              www.stat.ohio-state.edu/~pavlicov
>



From ripley at stats.ox.ac.uk  Tue Oct 28 21:21:03 2003
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Tue, 28 Oct 2003 20:21:03 +0000 (GMT)
Subject: [R] strptime command in R
In-Reply-To: <401BB5C1@bearmail.berkeley.edu>
Message-ID: <Pine.LNX.4.44.0310282017400.9837-100000@gannet.stats>

It would appear to mean that your `Unix' box's strptime is seriously 
broken, but you could profile R (the process, not R profiling) to find 
out.

You could try using package chron to convert the strings and then 
as.POSIXct, but that's inelegant at best.

On Tue, 28 Oct 2003, morozov wrote:

> Hello all:
> 
> I have a column of times in format
> 
>    x
> "16:30:00"
> "16:30:03"
> "16:59:00"
> etc
> 
> which I need to convert into time variables and do some operations on.
> 
> I do the command y<-strptime(x,"%H:%M:%S"). This executes almost istantly (for 
> a column x of length 1000 in Windows, but in Unix, where I run my production 
> jobs, this takes over 4 minutes. I know that generally my Unix box is much 
> more powerfull than my Win machine, and R runs generally faster on Unix, but 
> this particular command is very very slow. Why is that? How can I speed that 
> up without having to parse the strings by hand?
> 
> Thank you very much,
> Vlad.
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> 
> 

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From gb at tal.stat.umu.se  Tue Oct 28 21:36:21 2003
From: gb at tal.stat.umu.se (Goran Brostrom)
Date: Tue, 28 Oct 2003 21:36:21 +0100
Subject: [R] ifelse with a factor variable
Message-ID: <20031028203621.GA4186@stat.umu.se>

'ifelse' changes factors to character vectors (R-1.7.1, Linux):

> table(bal$soc.40)

          tax         noble semi-landless      landless       unknown 
         4035          5449         13342          9348             0 


> blah <- ifelse(is.na(bal$soc.40), "unknown", bal$soc.40)
> table(blah)
blah
      1       2       3       4 unknown 
   4035    5449   13342    9348    7970 

How do I get what I want (I mean: simply)? Upgrade to 1.8.0?

G?ran



From andy_liaw at merck.com  Tue Oct 28 21:55:16 2003
From: andy_liaw at merck.com (Liaw, Andy)
Date: Tue, 28 Oct 2003 15:55:16 -0500
Subject: [R] ifelse with a factor variable
Message-ID: <3A822319EB35174CA3714066D590DCD50205CD69@usrymx25.merck.com>

Does the following help you?

> x <- factor(c("A", "B", NA))
> levels(x) <- c(levels(x), "unknown")  # add an "unknown" level
> x[is.na(x)] <- "unknown"  # change NAs to "unknown"
> x
[1] A       B       unknown
Levels: A B unknown

Andy

> -----Original Message-----
> From: Goran Brostrom [mailto:gb at tal.stat.umu.se] 
> Sent: Tuesday, October 28, 2003 3:36 PM
> To: r-help at stat.math.ethz.ch
> Subject: [R] ifelse with a factor variable
> 
> 
> 'ifelse' changes factors to character vectors (R-1.7.1, Linux):
> 
> > table(bal$soc.40)
> 
>           tax         noble semi-landless      landless       unknown 
>          4035          5449         13342          9348             0 
> 
> 
> > blah <- ifelse(is.na(bal$soc.40), "unknown", bal$soc.40)
> > table(blah)
> blah
>       1       2       3       4 unknown 
>    4035    5449   13342    9348    7970 
> 
> How do I get what I want (I mean: simply)? Upgrade to 1.8.0?
> 
> G?ran
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list 
> https://www.stat.math.ethz.ch/mailman/listinfo> /r-help
>



From gb at tal.stat.umu.se  Tue Oct 28 22:04:17 2003
From: gb at tal.stat.umu.se (Goran Brostrom)
Date: Tue, 28 Oct 2003 22:04:17 +0100
Subject: [R] ifelse with a factor variable
In-Reply-To: <3A822319EB35174CA3714066D590DCD50205CD69@usrymx25.merck.com>
References: <3A822319EB35174CA3714066D590DCD50205CD69@usrymx25.merck.com>
Message-ID: <20031028210417.GA4292@stat.umu.se>

* Liaw, Andy <andy_liaw at merck.com> [2003-10-28 21:55]:
> Does the following help you?
> 
> > x <- factor(c("A", "B", NA))
> > levels(x) <- c(levels(x), "unknown")  # add an "unknown" level
> > x[is.na(x)] <- "unknown"  # change NAs to "unknown"
> > x
> [1] A       B       unknown
> Levels: A B unknown

Yes! Thank you,

G?ran

> 
> Andy
> 
> > -----Original Message-----
> > From: Goran Brostrom [mailto:gb at tal.stat.umu.se] 
> > Sent: Tuesday, October 28, 2003 3:36 PM
> > To: r-help at stat.math.ethz.ch
> > Subject: [R] ifelse with a factor variable
> > 
> > 
> > 'ifelse' changes factors to character vectors (R-1.7.1, Linux):
> > 
> > > table(bal$soc.40)
> > 
> >           tax         noble semi-landless      landless       unknown 
> >          4035          5449         13342          9348             0 
> > 
> > 
> > > blah <- ifelse(is.na(bal$soc.40), "unknown", bal$soc.40)
> > > table(blah)
> > blah
> >       1       2       3       4 unknown 
> >    4035    5449   13342    9348    7970 
> > 
> > How do I get what I want (I mean: simply)? Upgrade to 1.8.0?
> > 
> > G?ran
> > 
> > ______________________________________________
> > R-help at stat.math.ethz.ch mailing list 
> > https://www.stat.math.ethz.ch/mailman/listinfo> /r-help
> > 
> 

-- 
 G?ran Brostr?m                    tel: +46 90 786 5223
 Department of Statistics          fax: +46 90 786 6614
 Ume? University                   http://www.stat.umu.se/egna/gb/
 SE-90187 Ume?, Sweden             e-mail: gb at stat.umu.se



From pavlicov at stat.ohio-state.edu  Tue Oct 28 22:12:50 2003
From: pavlicov at stat.ohio-state.edu (Martina Pavlicova)
Date: Tue, 28 Oct 2003 16:12:50 -0500 (EST)
Subject: [R] 'levelplot' with an option 'at'
In-Reply-To: <Pine.GSO.4.58.0310281511300.1775@spatial.stat.ohio-state.edu>
References: <Pine.GSO.4.58.0310281239350.1775@spatial.stat.ohio-state.edu>
	<Pine.GSO.4.58.0310281511300.1775@spatial.stat.ohio-state.edu>
Message-ID: <Pine.GSO.4.58.0310281554540.1775@spatial.stat.ohio-state.edu>


Thank to Deepayan Sarkar I can report that the levelplot works properly
at 1.8.0 (I dont know more specs). Prof. Riply correctly noted that I
did not mention what version of 'lattice' library I am using. Here is
more info about the lattice library installed for each version of R.

I understand that the easiest solution is to forget about the problem
and update R and also lattice library, but because of the set-up on our
system, that is not possible for me in the near future.

I appreciate all your help and comments. Thank you.

Martina Pavlicova

R 1.6.1
-------
Package: lattice
Version: 0.6-8
Date: 2002-12-22
Priority: recommended
Title: Lattice Graphics
Author: Deepayan Sarkar <deepayan at stat.wisc.edu>
Maintainer: Deepayan Sarkar <deepayan at stat.wisc.edu>
Description: Implementation of Trellis Graphics
Depends: R (>= 1.6.0), grid (>= 0.7), modreg
License: GPL version 2 or later
Built: R 1.6.1; sparc-sun-solaris2.8; Thu Jan 2 16:12:59 EST 2003

R 1.7.0
-------
Package: lattice
Version: 0.7-11
Date: 2003/04/10
Priority: recommended
Title: Lattice Graphics
Author: Deepayan Sarkar <deepayan at stat.wisc.edu>
Maintainer: Deepayan Sarkar <deepayan at stat.wisc.edu>
Description: Implementation of Trellis Graphics
Depends: R (>= 1.7.0), grid (>= 0.7), modreg
License: GPL version 2 or later
Built: R 1.7.0; i686-pc-linux-gnu; 2003-04-17 13:07:31



On Tue, 28 Oct 2003, Martina Pavlicova wrote:

>
> Hi again,
>
> since the attached plots did not go through, I created a quick web-page,
> where all the plots are posted. Here it is:
>
> http://www.stat.ohio-state.edu/~pavlicov/levelplot/
>
> Thanks again.
>
> Martina Pavlicova
> --------------------------------------------------------------------------
> Department of Statistics             Office Phone: (614) 292-1567
> 1958 Neil Avenue, 304E Cockins Hall  FAX: (614) 292-2096
> The Ohio State University            E-mail: pavlicov at stat.ohio-state.edu
> Columbus, OH 43210-1247              www.stat.ohio-state.edu/~pavlicov
>



From jasont at indigoindustrial.co.nz  Tue Oct 28 23:13:34 2003
From: jasont at indigoindustrial.co.nz (Jason Turner)
Date: Wed, 29 Oct 2003 11:13:34 +1300
Subject: [R] R performance on Unix
In-Reply-To: <401BD1B2@bearmail.berkeley.edu>
References: <401BD1B2@bearmail.berkeley.edu>
Message-ID: <3F9EEA0E.1030105@indigoindustrial.co.nz>

morozov wrote:

> Hi
> I'm observing a huge difference in the performance speed of R on Windows and 
> Unix, even though I know that my Unix machine is much more powerful than my 
> Win machine. 
> In particular, any character processing task is very time consuming on Unix.
> 
> strptime(x,"%H:%M:%S") is about 10 times slower on Unix for vector x of the 
> length of ~ 500. read.table() also is very slow. is there any way to speed up 
> these ?
> 

As was pointed out, the strptime issue sounds like a C-library problem 
on your machine.  The text processing might be the same.  Is this a 
development version of a Un*x OS?  What's the output of uname -a?  Is it 
a source or binary build of R?  If binary, try building and installing 
from source (provided you've got the disk space, you can do this, even 
if you're not root).  Is the problem apparent on other boxen with 
similar OSs (ie is it hardware)?

If you absolutely must continue to use that box and OS and R build, 
read.table() can be sped up using the colClasses argument (R doesn't 
have to guess what class each column should be), but it sounds like a 
problematic installation.

Cheers

Jason
-- 
Indigo Industrial Controls Ltd.
http://www.indigoindustrial.co.nz
64-21-343-545
jasont at indigoindustrial.co.nz



From hodgess at gator.uhd.edu  Tue Oct 28 23:48:28 2003
From: hodgess at gator.uhd.edu (Erin Hodgess)
Date: Tue, 28 Oct 2003 16:48:28 -0600
Subject: [R] POSIX
Message-ID: <200310282248.h9SMmS705076@gator.dt.uh.edu>

Dear R People:

I have a question about POSIX.  Is this an acronym for something?

If I want to refer to this in a paper, what is the proper way to do so, 
please?

Thanks so much!

Sincerely,
Erin Hodgess
mailto: hodgess at gator.uhd.edu



From edd at debian.org  Wed Oct 29 00:04:06 2003
From: edd at debian.org (Dirk Eddelbuettel)
Date: Tue, 28 Oct 2003 17:04:06 -0600
Subject: [R] POSIX
In-Reply-To: <200310282248.h9SMmS705076@gator.dt.uh.edu>
References: <200310282248.h9SMmS705076@gator.dt.uh.edu>
Message-ID: <20031028230406.GA14324@sonny.eddelbuettel.com>

On Tue, Oct 28, 2003 at 04:48:28PM -0600, Erin Hodgess wrote:
> I have a question about POSIX.  Is this an acronym for something?

http://www.google.com/search?q=POSIX

-- 
Those are my principles, and if you don't like them... well, I have others.
                                                -- Groucho Marx



From solares at unsl.edu.ar  Wed Oct 29 00:08:21 2003
From: solares at unsl.edu.ar (solares@unsl.edu.ar)
Date: Tue, 28 Oct 2003 20:08:21 -0300 (ART)
Subject: [R] R with C code
Message-ID: <50501.170.210.173.216.1067382501.squirrel@inter14.unsl.edu.ar>


Hello my question is if a exists a command in R that permit to execute a 
code written in C, as for example the command "exec" of tcl that permits to 
execute a code of c inside tcl.  Thanks Ruben



From bolker at zoo.ufl.edu  Wed Oct 29 00:23:24 2003
From: bolker at zoo.ufl.edu (Ben Bolker)
Date: Tue, 28 Oct 2003 18:23:24 -0500 (EST)
Subject: [R] R with C code
In-Reply-To: <50501.170.210.173.216.1067382501.squirrel@inter14.unsl.edu.ar>
Message-ID: <Pine.LNX.4.44.0310281823070.25836-100000@bolker.zoo.ufl.edu>


  See "Writing R Extensions" in the documentation that comes with R ...

On Tue, 28 Oct 2003 solares at unsl.edu.ar wrote:

> 
> Hello my question is if a exists a command in R that permit to execute a 
> code written in C, as for example the command "exec" of tcl that permits to 
> execute a code of c inside tcl.  Thanks Ruben
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> 

-- 
620B Bartram Hall                            bolker at zoo.ufl.edu
Zoology Department, University of Florida    http://www.zoo.ufl.edu/bolker
Box 118525                                   (ph)  352-392-5697
Gainesville, FL 32611-8525                   (fax) 352-392-3704



From Simon.Blomberg at anu.edu.au  Wed Oct 29 00:18:51 2003
From: Simon.Blomberg at anu.edu.au (Simon Blomberg)
Date: Wed, 29 Oct 2003 10:18:51 +1100
Subject: [R] R with C code
Message-ID: <7A3A13F416B40842BD2C1753E044B3590122812B@CASEVS02.cas.anu.edu.au>

?.C

Cheers,

Simon.

Simon Blomberg, PhD
Depression & Anxiety Consumer Research Unit
Centre for Mental Health Research
Australian National University
http://www.anu.edu.au/cmhr/
Simon.Blomberg at anu.edu.au  +61 (2) 6125 3379


> -----Original Message-----
> From: solares at unsl.edu.ar [mailto:solares at unsl.edu.ar]
> Sent: Wednesday, 29 October 2003 10:08 AM
> To: R-help at stat.math.ethz.ch
> Subject: [R] R with C code
> 
> 
> 
> Hello my question is if a exists a command in R that permit 
> to execute a 
> code written in C, as for example the command "exec" of tcl 
> that permits to 
> execute a code of c inside tcl.  Thanks Ruben
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
>



From paulojus at est.ufpr.br  Wed Oct 29 00:23:53 2003
From: paulojus at est.ufpr.br (Paulo Justiniano Ribeiro Jr)
Date: Tue, 28 Oct 2003 21:23:53 -0200 (BRST)
Subject: [R] R with C code
In-Reply-To: <50501.170.210.173.216.1067382501.squirrel@inter14.unsl.edu.ar>
References: <50501.170.210.173.216.1067382501.squirrel@inter14.unsl.edu.ar>
Message-ID: <Pine.LNX.4.56.0310282123070.4719@gauss.est.ufpr.br>

I believe you are looking for the .C() function
For more details
read its documentation and the "Writing R extensions Manual"


On Tue, 28 Oct 2003 solares at unsl.edu.ar wrote:

>
> Hello my question is if a exists a command in R that permit to execute a
> code written in C, as for example the command "exec" of tcl that permits to
> execute a code of c inside tcl.  Thanks Ruben
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
>
>

Paulo Justiniano Ribeiro Jr
Departamento de Estat?stica
Universidade Federal do Paran?
Caixa Postal 19.081
CEP 81.531-990
Curitiba, PR  -  Brasil
Tel: (+55) 41 361 3471
Fax: (+55) 41 361 3141
e-mail: pj at est.ufpr.br
http://www.est.ufpr.br/~paulojus



From hodgess at gator.uhd.edu  Wed Oct 29 00:28:06 2003
From: hodgess at gator.uhd.edu (Erin Hodgess)
Date: Tue, 28 Oct 2003 17:28:06 -0600
Subject: [R] ts vs. POSIX
Message-ID: <200310282328.h9SNS6110235@gator.dt.uh.edu>

OK.

What if I have a time series which is collected every Monday, please?

What is the proper way to use the start option within the ts command
in order to indicate that this is Monday data, please?

Thanks again!

Sincerely,
Erin



From jasont at indigoindustrial.co.nz  Wed Oct 29 00:55:02 2003
From: jasont at indigoindustrial.co.nz (Jason Turner)
Date: Wed, 29 Oct 2003 12:55:02 +1300
Subject: [R] ts vs. POSIX
In-Reply-To: <200310282328.h9SNS6110235@gator.dt.uh.edu>
References: <200310282328.h9SNS6110235@gator.dt.uh.edu>
Message-ID: <3F9F01D6.5030801@indigoindustrial.co.nz>

Erin Hodgess wrote:
> What if I have a time series which is collected every Monday, please?
> 
> What is the proper way to use the start option within the ts command
> in order to indicate that this is Monday data, please?
> 

ts objects don't directly support dates.  There is some provision for 
monthly data, but this isn't the same as uniform, across-the-board date 
support.  What they do have is a start time, a deltat, and frequency 
(observations per period).  The main reason to use ts objects *isn't* 
the date/time handling, but for the nice functions (acf, spectrum, etc) 
you can use for regularly spaces time samples.

For weekly data, I'd use one of the following approaches (assuming the 
series starts in the first Monday of 2003):

1)
# dates in a year,week format
 > foo <- ts(1:100,start=c(2003,1),frequency=52)

or

2)
# dates as numeric representation of POSIXct objects
foo <- ts(1:100,start=as.numeric(as.POSIXct("2003-1-6")),deltat=60*60*24*7)
 > start(foo)
[1] 1041764400
 > end(foo)
[1] 1101639600
 > last <- end(foo)
 > class(last) <- "POSIXct"
 > last
[1] "2004-11-29 New Zealand Daylight Time"

(2) depends on as.numeric(POSIXct.object) giving a sensible, 
single-digit answer.  This is not guaranteed.  It works today, but 
nobody promised this approach would work tomorrow.

Hope that helps.

Cheers

Jason
-- 
Indigo Industrial Controls Ltd.
http://www.indigoindustrial.co.nz
64-21-343-545
jasont at indigoindustrial.co.nz



From shadi at titan.nc.velio.com  Wed Oct 29 01:08:20 2003
From: shadi at titan.nc.velio.com (Shadi Barakat)
Date: Tue, 28 Oct 2003 19:08:20 -0500 (EST)
Subject: [R] lm.fit glitch
Message-ID: <Pine.GSO.4.05.10310281902420.9809-100000@mimas.nc.velio.com>

Hello all,

I've seen this error posted before, but no hints to its origin. Does
anyone have any ideas?

------------------------------------------------
Error in lm.fit(x, y, offset = offset, ...) :
        0 (non-NA) cases 
------------------------------------------------

Thx



From zeileis at ci.tuwien.ac.at  Wed Oct 29 01:11:55 2003
From: zeileis at ci.tuwien.ac.at (Achim Zeileis)
Date: Wed, 29 Oct 2003 01:11:55 +0100
Subject: [R] ts vs. POSIX
In-Reply-To: <200310282328.h9SNS6110235@gator.dt.uh.edu>
References: <200310282328.h9SNS6110235@gator.dt.uh.edu>
Message-ID: <200310290011.h9T0Bt1b001109@thorin.ci.tuwien.ac.at>

On Wednesday 29 October 2003 00:28, Erin Hodgess wrote:

> OK.
>
> What if I have a time series which is collected every Monday,
> please?
>
> What is the proper way to use the start option within the ts command
> in order to indicate that this is Monday data, please?

In ts you can just generate regularly spaced time series. As the help 
page states "start" is either a single number or a vector of two 
integers (tyipically years and months or something similar).

So to indiciate that you have monday data you have to store this 
meta-information in the name of the ts object or the man page, for 
example.

An alternative is to use irregularly spaced time series as provided by 
irts() in package tseries or its() in package its. Then you always 
store a full vector of POSIXct times at which the observations were 
made. Both approaches have certain advantages and 
disadvantages...depends on what you want to do with the data.

hth,
Z

> Thanks again!
>
> Sincerely,
> Erin
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help



From spencer.graves at pdf.com  Wed Oct 29 01:22:53 2003
From: spencer.graves at pdf.com (Spencer Graves)
Date: Tue, 28 Oct 2003 16:22:53 -0800
Subject: [R] lm.fit glitch
In-Reply-To: <Pine.GSO.4.05.10310281902420.9809-100000@mimas.nc.velio.com>
References: <Pine.GSO.4.05.10310281902420.9809-100000@mimas.nc.velio.com>
Message-ID: <3F9F085D.8000002@pdf.com>

I can replicate it as follows: 

 > (DF <- data.frame(x=c(1:2, NA, NA), y=c(NA, NA, 3:4)))
   x  y
1  1 NA
2  2 NA
3 NA  3
4 NA  4
 > lm(y~x, DF, na.action=na.omit)
Error in lm.fit(x, y, offset = offset, singular.ok = singular.ok, ...) :
        0 (non-NA) cases

does this help?  spencer graves

Shadi Barakat wrote:

>Hello all,
>
>I've seen this error posted before, but no hints to its origin. Does
>anyone have any ideas?
>
>------------------------------------------------
>Error in lm.fit(x, y, offset = offset, ...) :
>        0 (non-NA) cases 
>------------------------------------------------
>
>Thx
>
>______________________________________________
>R-help at stat.math.ethz.ch mailing list
>https://www.stat.math.ethz.ch/mailman/listinfo/r-help
>  
>



From jfox at mcmaster.ca  Wed Oct 29 02:43:51 2003
From: jfox at mcmaster.ca (John Fox)
Date: Tue, 28 Oct 2003 20:43:51 -0500
Subject: [R] Confidence ellipse for correlation
In-Reply-To: <3FC34763@webmail.mcgill.ca>
Message-ID: <5.1.0.14.2.20031028204042.0204bef0@127.0.0.1>

Dear Tanya,

I believe that the data.ellipse function in the car package will do what 
you want (but note that the probability contours are for a bivariate-normal 
distribution, and hence won't necessarily enclose approximately the 
specified proportion of the data).

I hope that this helps,
  John

At 01:57 PM 10/28/2003 -0500, you wrote:
>Hello,
>
>SAS' point and click interface has the option of produce a scatterplot with a
>superimposed confidence ellipse for the correlation coefficient. Since I
>generally like R so much better, I would like to reproduce this in R. I've
>been playing with the ellipse package. In order to have the points and the
>ellipse on the same graph I've done the following.
>(Load ellipse package...)
> > data(Puromycin)
> > attach(Puromycin)
> > my<-mean(rate)
> > mx<-mean(conc)
> > sdy<-sd(rate)
> > sdx<-sd(conc)
> > r<-cor(conc,rate)
> > plot(ellipse(r,scale=c(sdx,sdy),centre=c(mx,my)),type='l')
> > points(conc,rate)
>
>1) Is my use of 'scale' and 'centre' theoretically correct?
>2) Is there a more efficient way to get the 5 parameters? (I guess I could
>write a little function, but has it already been done?)
>
>The non-linear relationship between these variables brings up another point:
>Is there a way to plot a contour (empirical?) containing, say, 95% of the
>values.
>
>Thanks for your time!
>
>Tanya

-----------------------------------------------------
John Fox
Department of Sociology
McMaster University
Hamilton, Ontario, Canada L8S 4M4
email: jfox at mcmaster.ca
phone: 905-525-9140x23604
web: www.socsci.mcmaster.ca/jfox



From sprad at skku.edu  Wed Oct 29 11:48:22 2003
From: sprad at skku.edu (=?EUC-KR?B?IrHovcUi?=)
Date: Wed, 29 Oct 2003 11:48:22 
Subject: =?EUC-KR?B?W1JdIHJhbmsgZnVuY3Rpb24=?=
Message-ID: <200310290248.h9T2mMT27702@mail.skku.edu>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20031029/4e5ebe0a/attachment.pl

From remigijus.lapinskas at maf.vu.lt  Wed Oct 29 09:17:46 2003
From: remigijus.lapinskas at maf.vu.lt (Remigijus Lapinskas)
Date: Wed, 29 Oct 2003 09:17:46 +0100
Subject: [R] automate save.image
Message-ID: <0387.031029@maf.vu.lt>

Dear all,

Sometimes, during an R session, my computer hangs and I
loose all the objects created during this session.
Is there a way to automatically type save.image() and
savehistory() every 5 minutes?

Best regards,
Remigijus



From Giles.Heywood at CommerzbankIB.com  Wed Oct 29 09:12:05 2003
From: Giles.Heywood at CommerzbankIB.com (Heywood, Giles)
Date: Wed, 29 Oct 2003 08:12:05 -0000
Subject: [R] ts vs. POSIX
Message-ID: <8CBAA121CEB4D5118CB200508BB2BBEF05BF7342@xmx8lonib.lonib.commerzbank.com>

Well, I'm not sure I understand the question exactly, 
but you might want to have a look at the package 'its', 
as Achim said.

A practical example might look like:

You have a .csv file as follows (I have chosen the date
format at random).

,x,y
Monday 08-Sep-2003,1,11
Monday 15-Sep-2003,2,22
Monday 22-Sep-2003,3,33
Monday 29-Sep-2003,4,44

Then the following reads the data into an 'its' object

require(its)
its.format("%A %d-%b-%Y")
xy <- its(readcsvIts("c:/temp/weekly.csv"))

>From there on it depends what you want to do - 'its' 
class inherits from matrix, which may be a useful 
property.

- Giles

> -----Original Message-----
> From: Jason Turner [mailto:jasont at indigoindustrial.co.nz]
> Sent: 28 October 2003 23:55
> To: Erin Hodgess
> Cc: r-help at stat.math.ethz.ch
> Subject: Re: [R] ts vs. POSIX
> 
> 
> Erin Hodgess wrote:
> > What if I have a time series which is collected every 
> Monday, please?
> > 
> > What is the proper way to use the start option within the ts command
> > in order to indicate that this is Monday data, please?
> > 
> 
> ts objects don't directly support dates.  There is some provision for 
> monthly data, but this isn't the same as uniform, 
> across-the-board date 
> support.  What they do have is a start time, a deltat, and frequency 
> (observations per period).  The main reason to use ts objects *isn't* 
> the date/time handling, but for the nice functions (acf, 
> spectrum, etc) 
> you can use for regularly spaces time samples.
> 
> For weekly data, I'd use one of the following approaches 
> (assuming the 
> series starts in the first Monday of 2003):
> 
> 1)
> # dates in a year,week format
>  > foo <- ts(1:100,start=c(2003,1),frequency=52)
> 
> or
> 
> 2)
> # dates as numeric representation of POSIXct objects
> foo <- 
> ts(1:100,start=as.numeric(as.POSIXct("2003-1-6")),deltat=60*60*24*7)
>  > start(foo)
> [1] 1041764400
>  > end(foo)
> [1] 1101639600
>  > last <- end(foo)
>  > class(last) <- "POSIXct"
>  > last
> [1] "2004-11-29 New Zealand Daylight Time"
> 
> (2) depends on as.numeric(POSIXct.object) giving a sensible, 
> single-digit answer.  This is not guaranteed.  It works today, but 
> nobody promised this approach would work tomorrow.
> 
> Hope that helps.
> 
> Cheers
> 
> Jason
> -- 
> Indigo Industrial Controls Ltd.
> http://www.indigoindustrial.co.nz
> 64-21-343-545
> jasont at indigoindustrial.co.nz
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> 


********************************************************************** 
This is a commercial communication from Commerzbank AG.\ \ T...{{dropped}}



From billthebrute at yahoo.fr  Wed Oct 29 09:12:48 2003
From: billthebrute at yahoo.fr (=?iso-8859-1?q?william=20ritchie?=)
Date: Wed, 29 Oct 2003 09:12:48 +0100 (CET)
Subject: [R] long algo
Message-ID: <20031029081248.55040.qmail@web25207.mail.ukl.yahoo.com>

Hi everyone,

 I ve been using R for months and find it really
practical and straight forward.
However (the inevitable however), I am finding it very
slow for one of my operations:
it s basically an itertation over i and j in a pretty
big table (4* 4608). It takes 30 minutes!!!!

Thanks


Ps:if it can help here is the source:

median1<-matrix(nrow=4608,ncol=1)
median2<-matrix(nrow=4608,ncol=1)
median3<-matrix(nrow=4608,ncol=1)
median4<-matrix(nrow=4608,ncol=1)
v<-c(18,19,20,21,23)
for (i in 0:11)
    {
     for (j in 1:384)
        {    
         
median1[j+(i*384),]<-puce[j+(i*384),5]+median(puce[v+384*i,2]-puce[v+384*i,5])
        
median2[j+(i*384),]<-puce[j+(i*384),19]+median(puce[v+384*i,16]-puce[v+384*i,19])
        
median3[j+(i*384),]<-puce[j+(i*384),12]+median(puce[v+384*i,9]-puce[v+384*i,12])
        
median4[j+(i*384),]<-puce[j+(i*384),26]+median(puce[v+384*i,23]-puce[v+384*i,26])
         
       
          puce[,5]<-median1
         puce[,19]<-median2
         puce[,12]<-median3
         puce[,26]<-median4


         }
     }



From steffihui at cuhk.edu.hk  Wed Oct 29 09:15:57 2003
From: steffihui at cuhk.edu.hk (Steffi Hui)
Date: Wed, 29 Oct 2003 16:15:57 +0800
Subject: [R] Kendall rank correlation
Message-ID: <001c01c39df5$1c549740$eb64123d@huinswnv5n1rci>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20031029/1f2dd5a9/attachment.pl

From alessandro.semeria at cramont.it  Wed Oct 29 09:40:03 2003
From: alessandro.semeria at cramont.it (alessandro.semeria@cramont.it)
Date: Wed, 29 Oct 2003 09:40:03 +0100
Subject: [R] long algo
Message-ID: <OF6BB1FE71.29FDAB8C-ONC1256DCE.002F4244@tomware.it>


Is well know that R is inefficent  on loops.
When you have to perform "heavy" loop
is better to use a call to fortran or c code (.Fortran() , .C() functions)
A.S.

----------------------------

Alessandro Semeria
Models and Simulations Laboratory
Montecatini Environmental Research Center (Edison Group),
Via Ciro Menotti 48,
48023 Marina di Ravenna (RA), Italy
Tel. +39 544 536811
Fax. +39 544 538663
E-mail: alessandro.semeria at cramont.it



From Ted.Harding at nessie.mcc.ac.uk  Wed Oct 29 10:17:22 2003
From: Ted.Harding at nessie.mcc.ac.uk ( (Ted Harding))
Date: Wed, 29 Oct 2003 09:17:22 -0000 (GMT)
Subject: [R] Loading a "sub-package"
In-Reply-To: <Pine.LNX.4.44.0310281928380.9837-100000@gannet.stats>
Message-ID: <XFMail.031029091722.Ted.Harding@nessie.mcc.ac.uk>

On 28-Oct-03 Prof Brian Ripley wrote:
> On Tue, 28 Oct 2003 Ted.Harding at nessie.mcc.ac.uk wrote:
> 
>> But MASS is huge! Hence the query. (I've also had occasion to filch
>> single functions from other libraries as well).
> 
> MASS is *not* huge, and indeed is negligible compared to what is
> already loaded.  nlme might be large, but few packages are noticeable
> and none are huge ....

Sorry! I didn't want to give an impression of making derogatory remarks
about MASS (or any other library), and admittedly it is a small fraction
of the size of nlme.

However, loading MASS seems to increase R's footprint in RAM by about
800KB (as reported by 'ps'), a substantial fraction of what's still
free when I'm working on my 128MB laptop, and (from another point of
view) this is indeed huge compared with the 600 bytes or so required
to define 'mvrnorm' on its own; and is a not-negligible bite out of
available RAM! (As I said before, I in fact simply pinched the code
stand-alone for this particular function, but it did prompt the
general query for cases where this might not be so straightforward).

> Luke and I (but principally Luke) have been experimenting with 
> load-on-demand for R objects, and indeed MASS already does that for
> its data objects.  It's possible that this will be a non-issue by
> the next non-patch release.

This will be welcome, and many thanks for the effort! Does this mean
that it would be possible to do something like

  load.function(mvrnorm,package=MASS)

and get just that function (and, in the case of some functions, any
other functions they may depend on)?

> Some data (R 1.8.0 --vanilla)
> 
>> gc()
>          used (Mb) gc trigger (Mb)
> Ncells 416460 11.2     597831   16
> Vcells 113224  0.9     786432    6
>> library(MASS)
>> gc()
>          used (Mb) gc trigger (Mb)
> Ncells 463337 12.4     667722 17.9
> Vcells 121995  1.0     786432  6.0
> 
> I'd fail any student who said `MASS was huge'.

Thankfully I do not anticipate taking any more examinations ... :)

> I assume you don't have methods loaded if you are concerned about 
> performance ....

An interesting question ... to which I have to answer "yes, I think
I do": for the reason it seems to be loaded by default[1] and my reading
of ?Methods has given me the impression (I don't understand R's workings
to the point where I feel I _know_ anything about this for sure) that a
lot of things one takes for granted in normal use of R would not work as
expected without it. For instance:

     The R package 'methods' implements, with a few exceptions, the
     programming interface for classes and methods in the book
     _Programming with Data_ (John M. Chambers, Springer, 1998), in
     particular sections 1.6, 2.7, 2.8, and chapters 7 and 8.

[1] At any rate, things listed by library(help=methods) are known
    to R on startup.

So another interesting question in turn: What would be the effect on
general use of setting up R start-up so that "methods" was not loaded
by default? And, indeed, how might this be done? (I can't find where
the default loading of "methods" is initiated).

With thanks again,
Ted.


--------------------------------------------------------------------
E-Mail: (Ted Harding) <Ted.Harding at nessie.mcc.ac.uk>
Fax-to-email: +44 (0)870 167 1972
Date: 29-Oct-03                                       Time: 09:17:22
------------------------------ XFMail ------------------------------



From ligges at statistik.uni-dortmund.de  Wed Oct 29 10:51:49 2003
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Wed, 29 Oct 2003 10:51:49 +0100
Subject: [R] long algo
In-Reply-To: <20031029081248.55040.qmail@web25207.mail.ukl.yahoo.com>
References: <20031029081248.55040.qmail@web25207.mail.ukl.yahoo.com>
Message-ID: <3F9F8DB5.6040006@statistik.uni-dortmund.de>

william ritchie wrote:

> Hi everyone,
> 
>  I ve been using R for months and find it really
> practical and straight forward.
> However (the inevitable however), I am finding it very
> slow for one of my operations:
> it s basically an itertation over i and j in a pretty
> big table (4* 4608). It takes 30 minutes!!!!
> 
> Thanks

There was a suggestion to use C or Fortran, but in your particular case 
it looks like you can choose a simpler way to get more performance (even 
if not that much as in C) by vectorizing a bit more, see below.



> Ps:if it can help here is the source:
> 
> median1<-matrix(nrow=4608,ncol=1)
> median2<-matrix(nrow=4608,ncol=1)
> median3<-matrix(nrow=4608,ncol=1)
> median4<-matrix(nrow=4608,ncol=1)
> v<-c(18,19,20,21,23)
> for (i in 0:11)
>     {
>      for (j in 1:384)
>         {    
>          
> median1[j+(i*384),]<-puce[j+(i*384),5]+median(puce[v+384*i,2]-puce[v+384*i,5])
>         
> median2[j+(i*384),]<-puce[j+(i*384),19]+median(puce[v+384*i,16]-puce[v+384*i,19])
>         
> median3[j+(i*384),]<-puce[j+(i*384),12]+median(puce[v+384*i,9]-puce[v+384*i,12])
>         
> median4[j+(i*384),]<-puce[j+(i*384),26]+median(puce[v+384*i,23]-puce[v+384*i,26])
>          
>        
>           puce[,5]<-median1
>          puce[,19]<-median2
>          puce[,12]<-median3
>          puce[,26]<-median4
> 
> 
>          }
>      }


The obvious (well, I haven't tried) *first* step (I don't want to 
rewrite your code here!) is, e.g.,


median1 <- median2 <- median3 <- median4 <- numeric(4608)
v <- c(18,19,20,21,23)
for (i in 0:11)
   {
  j <- 1:384
median1[j+(i*384)]<-puce[j+(i*384),5]+median(puce[v+384*i,2]-puce[v+384*i,5])
median2[j+(i*384)]<-puce[j+(i*384),19]+median(puce[v+384*i,16]-puce[v+384*i,19])
median3[j+(i*384)]<-puce[j+(i*384),12]+median(puce[v+384*i,9]-puce[v+384*i,12])
median4[j+(i*384)]<-puce[j+(i*384),26]+median(puce[v+384*i,23]-puce[v+384*i,26])
}
puce[,5]<-median1
puce[,19]<-median2
puce[,12]<-median3
puce[,26]<-median4


Uwe Ligges



From ripley at stats.ox.ac.uk  Wed Oct 29 11:37:48 2003
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Wed, 29 Oct 2003 10:37:48 +0000 (GMT)
Subject: [R] Loading a "sub-package"
In-Reply-To: <XFMail.031029091722.Ted.Harding@nessie.mcc.ac.uk>
Message-ID: <Pine.LNX.4.44.0310291032030.22024-100000@gannet.stats>

Short answers: 

Setting R_DEFAULT_PACKAGES changes the startup packages, as ?Startup says.

You should not need methods unless you are using S4 classes yourself, as 
any package you might use that needs it is supposed to load it.  Methods 
adds a considerable overhead to R's startup time and footprint.
I am not advocating doing this unless you are worried about space.

The code in MASS you never use should be paged out, but the planned change 
would be rather to never load it unless it had been used.

On Wed, 29 Oct 2003 Ted.Harding at nessie.mcc.ac.uk wrote:

> On 28-Oct-03 Prof Brian Ripley wrote:
> > On Tue, 28 Oct 2003 Ted.Harding at nessie.mcc.ac.uk wrote:
> > 
> >> But MASS is huge! Hence the query. (I've also had occasion to filch
> >> single functions from other libraries as well).
> > 
> > MASS is *not* huge, and indeed is negligible compared to what is
> > already loaded.  nlme might be large, but few packages are noticeable
> > and none are huge ....
> 
> Sorry! I didn't want to give an impression of making derogatory remarks
> about MASS (or any other library), and admittedly it is a small fraction
> of the size of nlme.
> 
> However, loading MASS seems to increase R's footprint in RAM by about
> 800KB (as reported by 'ps'), a substantial fraction of what's still
> free when I'm working on my 128MB laptop, and (from another point of
> view) this is indeed huge compared with the 600 bytes or so required
> to define 'mvrnorm' on its own; and is a not-negligible bite out of
> available RAM! (As I said before, I in fact simply pinched the code
> stand-alone for this particular function, but it did prompt the
> general query for cases where this might not be so straightforward).
> 
> > Luke and I (but principally Luke) have been experimenting with 
> > load-on-demand for R objects, and indeed MASS already does that for
> > its data objects.  It's possible that this will be a non-issue by
> > the next non-patch release.
> 
> This will be welcome, and many thanks for the effort! Does this mean
> that it would be possible to do something like
> 
>   load.function(mvrnorm,package=MASS)
> 
> and get just that function (and, in the case of some functions, any
> other functions they may depend on)?
> 
> > Some data (R 1.8.0 --vanilla)
> > 
> >> gc()
> >          used (Mb) gc trigger (Mb)
> > Ncells 416460 11.2     597831   16
> > Vcells 113224  0.9     786432    6
> >> library(MASS)
> >> gc()
> >          used (Mb) gc trigger (Mb)
> > Ncells 463337 12.4     667722 17.9
> > Vcells 121995  1.0     786432  6.0
> > 
> > I'd fail any student who said `MASS was huge'.
> 
> Thankfully I do not anticipate taking any more examinations ... :)
> 
> > I assume you don't have methods loaded if you are concerned about 
> > performance ....
> 
> An interesting question ... to which I have to answer "yes, I think
> I do": for the reason it seems to be loaded by default[1] and my reading
> of ?Methods has given me the impression (I don't understand R's workings
> to the point where I feel I _know_ anything about this for sure) that a
> lot of things one takes for granted in normal use of R would not work as
> expected without it. For instance:
> 
>      The R package 'methods' implements, with a few exceptions, the
>      programming interface for classes and methods in the book
>      _Programming with Data_ (John M. Chambers, Springer, 1998), in
>      particular sections 1.6, 2.7, 2.8, and chapters 7 and 8.
> 
> [1] At any rate, things listed by library(help=methods) are known
>     to R on startup.
> 
> So another interesting question in turn: What would be the effect on
> general use of setting up R start-up so that "methods" was not loaded
> by default? And, indeed, how might this be done? (I can't find where
> the default loading of "methods" is initiated).


-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From R.eschen at cabi-bioscience.ch  Wed Oct 29 12:16:44 2003
From: R.eschen at cabi-bioscience.ch (Rene Eschen)
Date: Wed, 29 Oct 2003 12:16:44 +0100
Subject: [R] P-values in ncf package
Message-ID: <D173FA00F56FD21184C300104BB73257363B3C@DELEMONT_SRVR>

Group,

I'm currently trying to find out how the function correlog in the ncf
package may be useful to me for calculating cross-correlograms. The
function's output includes P-values for all distance classes, but it seems
that only positive values can become significant. Is this true and correct?
If so, why?

Thanks in advance.

Ren? Eschen.

_______________________________________

Ren? Eschen
CABI Bioscience Switzerland Centre
1 Rue des Grillons
CH-2800 Del?mont
Switzerland
+41 32 421 48 87 (Direct)
+41 32 421 48 70 (Secretary)
+41 32 421 48 71 (Fax)



From Giovanni_Millo at generali.com  Wed Oct 29 12:27:40 2003
From: Giovanni_Millo at generali.com (Millo Giovanni)
Date: Wed, 29 Oct 2003 12:27:40 +0100
Subject: [R] Re: presentation of software
Message-ID: <74F2D4ED68558643B63A6CC21746040D01A073BB@BEMAILEXTS1.ad.generali.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20031029/a6747f7c/attachment.pl

From aellen at nln.org  Wed Oct 29 14:08:20 2003
From: aellen at nln.org (aellen@nln.org)
Date: Wed, 29 Oct 2003 08:08:20 -0500 (EST)
Subject: [R] stacked histogram
Message-ID: <200310291308.h9TD8Kf18221@panix1.panix.com>

Is there a way to do a stacked histogram, using 
color. I have two groups with the complication that
one group is about 6% of the total.



From f.mattes at ucl.ac.uk  Wed Oct 29 14:14:59 2003
From: f.mattes at ucl.ac.uk (Frank Mattes)
Date: Wed, 29 Oct 2003 13:14:59 +0000
Subject: [R] I have a problem with the log2 function
Message-ID: <p05210601bbc56c88f878@[128.40.218.142]>

Dear R users,

according the help(log), the function
log2(x) should give the natural logarithm of x.

I expect in case of x=2 to to get 0.6931, however, R gives me 1 as a result.

Similar, logb(2,2) gives 1 again.

I'm wondering if I have missed something ?

Yours
Frank


-- 
Frank Mattes, MD			e-mail:	f.mattes at ucl.ac.uk
Department of Virology			fax	0044(0)207 8302854
Royal Free Hospital and 			tel	0044(0)207 8302997
University College Medical School
London



From rossini at blindglobe.net  Wed Oct 29 14:24:26 2003
From: rossini at blindglobe.net (A.J. Rossini)
Date: Wed, 29 Oct 2003 05:24:26 -0800
Subject: [R] I have a problem with the log2 function
In-Reply-To: <p05210601bbc56c88f878@[128.40.218.142]> (Frank Mattes's
	message of "Wed, 29 Oct 2003 13:14:59 +0000")
References: <p05210601bbc56c88f878@[128.40.218.142]>
Message-ID: <85y8v4b2id.fsf@blindglobe.net>

Frank Mattes <f.mattes at ucl.ac.uk> writes:

> Dear R users,
>
> according the help(log), the function
> log2(x) should give the natural logarithm of x.
>
> I expect in case of x=2 to to get 0.6931, however, R gives me 1 as a result.
>
> Similar, logb(2,2) gives 1 again.
>
> I'm wondering if I have missed something ?
>

I think you are looking at the help page wrong:


Description:

     'log' computes natural logarithms, 'log10' computes common (i.e.,
     base 10) logarithms, and 'log2' computes binary (i.e., base 2)
     logarithms. The general form 'logb(x, base)' computes logarithms
     with base 'base' ('log10' and 'log2' are only special cases).

     'log1p(x)' computes log(1+x) accurately also for |x| << 1 (and
     less accurately when x is approximately -1).

     'exp' computes the exponential function.

     'expm1(x)' computes exp(x) - 1 accurately also for |x| << 1.



log() is what you want.


-- 
rossini at u.washington.edu            http://www.analytics.washington.edu/ 
Biomedical and Health Informatics   University of Washington
Biostatistics, SCHARP/HVTN          Fred Hutchinson Cancer Research Center
UW (Tu/Th/F): 206-616-7630 FAX=206-543-3461 | Voicemail is unreliable
FHCRC  (M/W): 206-667-7025 FAX=206-667-4812 | use Email

CONFIDENTIALITY NOTICE: This e-mail message and any attachme...{{dropped}}



From andy_liaw at merck.com  Wed Oct 29 14:24:35 2003
From: andy_liaw at merck.com (Liaw, Andy)
Date: Wed, 29 Oct 2003 08:24:35 -0500
Subject: [R] automate save.image
Message-ID: <3A822319EB35174CA3714066D590DCD50205CD6B@usrymx25.merck.com>

You can use the callback manager that Duncan Temple Lang implemented in R.
Search the R-help archive for "auto-save".

Andy

> From: Remigijus Lapinskas [mailto:remigijus.lapinskas at maf.vu.lt] 
> 
> Dear all,
> 
> Sometimes, during an R session, my computer hangs and I
> loose all the objects created during this session.
> Is there a way to automatically type save.image() and
> savehistory() every 5 minutes?
> 
> Best regards,
> Remigijus
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list 
> https://www.stat.math.ethz.ch/mailman/listinfo> /r-help
>



From Isabelle.ZABALZA-MEZGHANI at ifp.fr  Wed Oct 29 14:57:29 2003
From: Isabelle.ZABALZA-MEZGHANI at ifp.fr (ZABALZA-MEZGHANI Isabelle)
Date: Wed, 29 Oct 2003 14:57:29 +0100
Subject: [R] add and drop methods for mlm objects
Message-ID: <488C02265C6AD611BF200002A542182F03F2A7DE@irnts22.ifp.fr>

Hello,

I would like to fit several responses depending on the same parameters. Thus
I generate an mlm object. On the other hand, for further analysis, I would
like to be able to simplify as much as possible the models.
Of course, there is no reason that the same formula applies for every
response. That is why I wonder if it exist a method such as drop1 for mlm
objects.

Waiting for your help,

Thanks in advance,

Isabelle.

Isabelle Zabalza-Mezghani
IFP
Rueil-Malmaison / France



From ma at ne.su.se  Wed Oct 29 15:00:02 2003
From: ma at ne.su.se (Mahmood Arai)
Date: Wed, 29 Oct 2003 15:00:02 +0100
Subject: [R] conflicts(detail=TRUE)
References: <Pine.GSO.4.10.10304211046080.27528-100000@quetelet.stat.ucla.edu>
Message-ID: <3F9FC7E2.6090801@ne.su.se>

Why this?

R> conflicts(detail=TRUE)
$"package:methods"
[1] "body<-"

$"package:base"
[1] "body<-"
R>

mahmood arai



From pgilbert at bank-banque-canada.ca  Wed Oct 29 15:28:58 2003
From: pgilbert at bank-banque-canada.ca (Paul Gilbert)
Date: Wed, 29 Oct 2003 09:28:58 -0500
Subject: [R] R performance on Unix
In-Reply-To: <401BD1B2@bearmail.berkeley.edu>
References: <401BD1B2@bearmail.berkeley.edu>
Message-ID: <3F9FCEAA.5040404@bankofcanada.ca>

morozov wrote:

>Hi
>I'm observing a huge difference in the performance speed of R on Windows and 
>Unix, even though I know that my Unix machine is much more powerful than my 
>Win machine. 
>
I've had this experience on a Unix machine that I thought was much more 
powerful than my Windows machine. On closer examination it seems the 
hardware was better at multitasking, but definitely not faster on single 
processes, so you might want to be careful about the part you think you 
know. I haven't had this experience running Windows and Linux on the 
same machine, but even then there are an enormous number of variable in 
the way you build R.

Paul Gilbert

>In particular, any character processing task is very time consuming on Unix.
>
>strptime(x,"%H:%M:%S") is about 10 times slower on Unix for vector x of the 
>length of ~ 500. read.table() also is very slow. is there any way to speed up 
>these ?
>
>Thanks a lot,
>Vlad.
>
>______________________________________________
>R-help at stat.math.ethz.ch mailing list
>https://www.stat.math.ethz.ch/mailman/listinfo/r-help
>
>  
>



From flom at ndri.org  Wed Oct 29 16:12:09 2003
From: flom at ndri.org (Peter Flom)
Date: Wed, 29 Oct 2003 10:12:09 -0500
Subject: [R] One inflated Poisson or Negative Binomal regression
Message-ID: <sf9f9292.024@MAIL.NDRI.ORG>

Hello

I am interested in Poisson or (ideally) Negative Binomial regression
with an inflated number of 1  responses

I have seen JK Lindsey's fmr function in the gnlm library, which fits
zero inflated Poisson (ZIP) or zero inflated negative binomial
regression, but the help file states that for ' Poisson or related
distributions  the mixture involves the zero category'.

I had thought of perhaps subtracting 1 from all the counts and then
fitting the ZIP or ZINB models, and then adding 1, but am not sure if
this is legitimate, or if there is some better method.

Contextual details:
The dependent variable is number of primary sexual partners in the last
year.  The independent variables include a) Being married or in a
committed relationship  b) using hard drugs  c) sex  d) age

N is c. 500

Not surprisingly, there are a large number of 1 responses, especially
for those who are married or in a relationship.  More surprisingly, the
mean number of partners is the same (1.05 vs. 1.02) for people in and
not in relationships, but the variances are very different, mostly
because those in a relationhsip are much more likely to say exactly 1.

Thanks in advance

Peter

Peter L. Flom, PhD
Assistant Director, Statistics and Data Analysis Core
Center for Drug Use and HIV Research
National Development and Research Institutes
71 W. 23rd St
www.peterflom.com
New York, NY 10010
(212) 845-4485 (voice)
(917) 438-0894 (fax)



From f.harrell at vanderbilt.edu  Wed Oct 29 16:22:52 2003
From: f.harrell at vanderbilt.edu (Frank E Harrell Jr)
Date: Wed, 29 Oct 2003 10:22:52 -0500
Subject: [R] /usr/lib/R/library vs /usr/local/lib/R/site-library
Message-ID: <20031029102252.5bb72714.f.harrell@vanderbilt.edu>

I would appreciate getting a clarification of /usr/lib/R/library vs /usr/local/lib/R/site-library.  I am running R 1.8 on Debian Linux.  On one occasion doing update.packages() resulted in versions of one or more libraries being placed in one of these directories without removing the old version, and the old version took precedence over the new.  I'm sorry I did not save the steps I used.  I would appreciate reading how best to manage libraries.  Currently I have the following in /usr/local/lib/R/site-library: Design, Hmisc, Rcmdr, acepack, car.  /usr/lib/R/library contains the following:

KernSmooth  boot     eda      lqs      modreg  nnet     stepfun   ts
MASS        class    foreign  methods  mva     rpart    survival
R.css       cluster  grid     mgcv     nlme    spatial  tcltk
base        ctest    lattice  mle      nls     splines  tools

Thanks -Frank
---
Frank E Harrell Jr    Professor and Chair            School of Medicine
                      Department of Biostatistics    Vanderbilt University
---
Frank E Harrell Jr    Professor and Chair            School of Medicine
                      Department of Biostatistics    Vanderbilt University



From paolo.radaelli at unimib.it  Wed Oct 29 16:26:43 2003
From: paolo.radaelli at unimib.it (Radaelli Paolo - Dottorati di Ricerca)
Date: Wed, 29 Oct 2003 16:26:43 +0100
Subject: pre-compiled win binaries (was: Re: [R] Quantreg Package)
References: <Pine.SOL.4.30.0310231502260.19787-100000@ysidro.econ.uiuc.edu>
Message-ID: <019201c39e31$0ea471b0$6e788495@dimequant.unimib.it>

I've just installed the quantreg package for windows machine.
Thank you
Paolo Radaelli

----- Original Message ----- 
From: "Roger Koenker" <roger at ysidro.econ.uiuc.edu>
To: "Achim Zeileis" <zeileis at ci.tuwien.ac.at>
Cc: "Uwe Ligges" <ligges at statistik.uni-dortmund.de>; "Radaelli Paolo -
Dottorati di Ricerca" <paolo.radaelli at unimib.it>; <r-help at stat.math.ethz.ch>
Sent: Thursday, October 23, 2003 9:06 PM
Subject: Re: pre-compiled win binaries (was: Re: [R] Quantreg Package)


>
> You are right, of course, it would be nice to have notification,
> but I'm also sympathetic to Uwe's situation, and not everything
> that could be automated, could be _easily_ automated within the
> constraints imposed by the rest of world.  The lesson I've drawn
> from this is that complaints will appear, and the check directory
> does help explain problems.  My real difficulty is that I have
> no good way to  explore windows specific problems.  But this is
> just the flip side of saying what a great thing it is that
> the windows binaries are usually appearing automagically without
> any problems!
>
> Roger
>
>
> url: www.econ.uiuc.edu/~roger/my.html Roger Koenker
> email rkoenker at uiuc.edu Department of Economics
> vox: 217-333-4558 University of Illinois
> fax:   217-244-6678 Champaign, IL 61820
>
> On Thu, 23 Oct 2003, Achim Zeileis wrote:
>
> > It happened for the second time in a week that a Windows binary for a
> > CRAN package was not availabe (Hmisc and quantreg) although the
> > package maintainer would have been willing to try to fix the problems
> > that prevented automatic pre-compilation if he would have realized
> > that there is a problem on Windows.
> > Of course, the information was provided on CRAN, but I thought that it
> > might help to automatically notify the package maintainers if some
> > conflicts occur on Windows. I would appreciate such a notification if
> > one of my packages would have failed to compile...and it is not very
> > unlikely that I wouldn't have discovered it myself.
> >
> > I already asked Uwe privately and he would be willing to provide a
> > notification but maybe there are concerns or objections from the
> > maintainers?
> >
> > best,
> > Z
> >
> >
> > On Thursday 23 October 2003 17:28, Uwe Ligges wrote:
> >
> > > Radaelli Paolo - Dottorati di Ricerca wrote:
> > > > I saw the read-me but I didn't undersstand wich is the problem. I
> > > > only know that in a previous version of R I installed on my pc it
> > > > was all ok.
> > >
> > > Yes, but on the recent version it is *not* OK.
> > > http://cran.r-project.org/bin/windows/contrib/1.8/check/quantreg-che
> > >ck.log tells you:
> > >
> > >   [...]
> > >   * checking examples ... ERROR
> > >   Running examples in quantreg-Ex.R failed.
> > >
> > > BTW: Efforts have been made to upload these check logs to CRAN in
> > > order to provide you with this information, so please read those
> > > files!
> > >
> > >
> > > Looking closer (as a hint for Roger), R *crashes* when running the
> > > examples in ?boot.rq.
> > >
> > > > So know I have to download the extensions files and then compile
> > > > them on my own ?
> > >
> > > You can download the source package and compile from source (I don't
> > > know of any "extensions files").
> > > Attention: The problem not passing Rcmd check remains (almost
> > > certain).
> > >
> > > Uwe Ligges
> > >
> > > > Thank you
> > > > Paolo Radaelli
> > > >
> > > > ----- Original Message -----
> > > > From: "Uwe Ligges" <ligges at statistik.uni-dortmund.de>
> > > > To: "Radaelli Paolo - Dottorati di Ricerca"
> > > > <paolo.radaelli at unimib.it> Cc: <r-help at stat.math.ethz.ch>
> > > > Sent: Thursday, October 23, 2003 4:44 PM
> > > > Subject: Re: [R] Quantreg Package
> > > >
> > > >>Radaelli Paolo - Dottorati di Ricerca wrote:
> > > >>>I've just installed R 1.0.8 (for Windows) and I tried to install
> > > >>> the
> > > >
> > > > package Quantreg directly from Cran but it's not in the list of
> > > > downlodable packages.
> > > >
> > > >>>I tried also downloading the zip file and then install it but
> > > >>> there is
> > > >
> > > > an error.
> > > >
> > > >>>How can I do it?
> > > >>>Thank you
> > > >>>[[alternative HTML version deleted]]
> > > >>>
> > > >>>______________________________________________
> > > >>>R-help at stat.math.ethz.ch mailing list
> > > >>>https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> > > >>
> > > >>CRAN/bin/windows/contrib/1.8/ReadMe tells you that Windows
> > > >> binaries of those packages not passing Rcmd check are not
> > > >> published on CRAN. And the corresponding Status file and
> > > >> check-log tells you quantreg is among those packages.
> > > >>
> > > >>So you have to compile from source yourself.
> > > >>
> > > >>Uwe Ligges
> > >
> > > ______________________________________________
> > > R-help at stat.math.ethz.ch mailing list
> > > https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> >
>



From wolfram at fischer-zim.ch  Wed Oct 29 16:36:03 2003
From: wolfram at fischer-zim.ch (Wolfram Fischer)
Date: Wed, 29 Oct 2003 16:36:03 +0100
Subject: [R] grid: dividing units by numbers
Message-ID: <20031029153603.GA5442@s1x.local>

How can I divide a unit by an number
or average a vector of units, e.g.:

	u1 <- unit( 3, 'npc' )
	u2 <- unit( 6, 'npc' )

	u1 / 2
	( u1 + u2 ) / 2
	mean( unit.c(u1,u2) )

I would use that e.g. to to calculate the coordinates
of the midpoint of a line.

Wolfram



From flom at ndri.org  Wed Oct 29 16:49:10 2003
From: flom at ndri.org (Peter Flom)
Date: Wed, 29 Oct 2003 10:49:10 -0500
Subject: [R] Where is rmutil package?
Message-ID: <sf9f9b33.092@MAIL.NDRI.ORG>

Pursing my earlier question, when I tried loading Lindsey's gnlm, I got
a
message

Loading required package: rmutil 
Warning message: 
There is no package called 'rmutil' in: library(package, character.only
= TRUE, logical = TRUE, warn.conflicts = warn.conflicts,  


According to the R documentation
http://finzi.psych.upenn.edu/R/doc/html/packages.html

rmutil is in the standard library.....

If I ignore the message and try fitting a model with one of the
functions in gnlm (e.g. fmr)  I get an error that it coudn't find the
function 'finterp'

Any help appreciated

Thanks

Peter

Peter L. Flom, PhD
Assistant Director, Statistics and Data Analysis Core
Center for Drug Use and HIV Research
National Development and Research Institutes
71 W. 23rd St
www.peterflom.com
New York, NY 10010
(212) 845-4485 (voice)
(917) 438-0894 (fax)



From ma at ne.su.se  Wed Oct 29 16:47:42 2003
From: ma at ne.su.se (Mahmood Arai)
Date: Wed, 29 Oct 2003 16:47:42 +0100
Subject: [R] /usr/lib/R/library vs /usr/local/lib/R/site-library
References: <20031029102252.5bb72714.f.harrell@vanderbilt.edu>
Message-ID: <3F9FE11E.1020607@ne.su.se>

Frank E Harrell Jr wrote:

>I would appreciate getting a clarification of /usr/lib/R/library vs /usr/local/lib/R/site-library.  I am running R 1.8 on Debian Linux.  On one occasion doing update.packages() resulted in versions of one or more libraries being placed in one of these directories without removing the old version, and the old version took precedence over the new.  I'm sorry I did not save the steps I used.  I would appreciate reading how best to manage libraries.  Currently I have the following in /usr/local/lib/R/site-library: Design, Hmisc, Rcmdr, acepack, car.  /usr/lib/R/library contains the following:
>
>KernSmooth  boot     eda      lqs      modreg  nnet     stepfun   ts
>MASS        class    foreign  methods  mva     rpart    survival
>R.css       cluster  grid     mgcv     nlme    spatial  tcltk
>base        ctest    lattice  mle      nls     splines  tools
>
>Thanks -Frank
>---
>Frank E Harrell Jr    Professor and Chair            School of Medicine
>                      Department of Biostatistics    Vanderbilt University
>---
>Frank E Harrell Jr    Professor and Chair            School of Medicine
>                      Department of Biostatistics    Vanderbilt University
>
>______________________________________________
>R-help at stat.math.ethz.ch mailing list
>https://www.stat.math.ethz.ch/mailman/listinfo/r-help
>  
>
I had the same problem on my workstation -  stable version of debian. 
probably depending on a site-installation based on
the R- source files.

I removed usr/local/lib/R,  changed the line  R_LIBS to 
R_LIBS=${R_LIBS-'/usr/lib/R/library'} in /etc/R/Renviron
and installed the missing packages again, they were installed in 
/usr/lib/R/library and
seem to work well - at least so long.

mahmood arai



From rossini at blindglobe.net  Wed Oct 29 16:51:16 2003
From: rossini at blindglobe.net (A.J. Rossini)
Date: Wed, 29 Oct 2003 07:51:16 -0800
Subject: [R] /usr/lib/R/library vs /usr/local/lib/R/site-library
In-Reply-To: <20031029102252.5bb72714.f.harrell@vanderbilt.edu> (Frank E.
	Harrell, Jr.'s message of "Wed, 29 Oct 2003 10:22:52 -0500")
References: <20031029102252.5bb72714.f.harrell@vanderbilt.edu>
Message-ID: <85smlcjb4b.fsf@blindglobe.net>

Frank E Harrell Jr <f.harrell at vanderbilt.edu> writes:

1> I would appreciate getting a clarification of /usr/lib/R/library vs
1> /usr/local/lib/R/site-library.  I am running R 1.8 on Debian Linux.
1> On one occasion doing update.packages() resulted in versions of one
1> or more libraries being placed in one of these directories without
1> removing the old version, and the old version took precedence over
1> the new.  I'm sorry I did not save the steps I used.  I would
1> appreciate reading how best to manage libraries.  Currently I have
1> the following in /usr/local/lib/R/site-library: Design, Hmisc,
1> Rcmdr, acepack, car.  /usr/lib/R/library contains the following:
>
> KernSmooth  boot     eda      lqs      modreg  nnet     stepfun   ts
> MASS        class    foreign  methods  mva     rpart    survival
> R.css       cluster  grid     mgcv     nlme    spatial  tcltk
> base        ctest    lattice  mle      nls     splines  tools
>

On Debian (and I'm sure Dirk will pipe in), /usr/lib/R/library is for
apt-installed by the core R packages, /usr/lib/R/site-library is for
apt-installed R packages (from CRAN, or Jim Lindsey's works), and
/usr/local/lib/R/site-library where you want to put locally installed
stuff (i.e. "install.packages()" should put there).  

best,
-tony

-- 
rossini at u.washington.edu            http://www.analytics.washington.edu/ 
Biomedical and Health Informatics   University of Washington
Biostatistics, SCHARP/HVTN          Fred Hutchinson Cancer Research Center
UW (Tu/Th/F): 206-616-7630 FAX=206-543-3461 | Voicemail is unreliable
FHCRC  (M/W): 206-667-7025 FAX=206-667-4812 | use Email

CONFIDENTIALITY NOTICE: This e-mail message and any attachme...{{dropped}}



From Timur.Elzhov at jinr.ru  Wed Oct 29 16:54:05 2003
From: Timur.Elzhov at jinr.ru (Timur Elzhov)
Date: Wed, 29 Oct 2003 18:54:05 +0300
Subject: [R] evaluating expression within nls()
Message-ID: <20031029155405.GA6653@nf034.jinr.ru>

Dear R experts,

I'd like to fit data with weighted function fcn(data, p1, p2). At
first I used the standard form:

    nls(~ fcn(data, p1, p2),
         start = list(p1 = p01, p2 = p02),
         data  = data)

Then, I decided to change fcn() that it would be able to accept expression
as an additional parameter,

    nls(~ fcn(data, expr, p1, p2), ...)
    # ('expr' is defined just before 'nls' calling)

    Error in eval(expr, envir, enclos) : Object "expr" not found

Yes, in help(nls) is pointed, that
    Arguments:
      data: an optional data frame in which to evaluate the variables in
            `formula'

But, I'd like 'nls' to see my expression 'expr'... :-/  How can I do that?
Thank you!

--
WBR,
Timur.



From pgreen at umich.edu  Wed Oct 29 16:54:33 2003
From: pgreen at umich.edu (Paul Green)
Date: Wed, 29 Oct 2003 10:54:33 -0500
Subject: [R] importing SAS data 
Message-ID: <5.1.0.14.2.20031029104356.00b2c178@mailkardia.sph.umich.edu>

I am trying to import a permanent SAS data
set using read.ssd in the foreign library. I get
the following error:

 > list.files("C:/temp")
[1] "newdat1.sas7bdat" "snpm1.sas7bdat"   "test1.sas"
 > library(foreign)
 > newdat1 <- read.ssd("C:/temp","newdat1")
SAS failed.  SAS program at 
C:\DOCUME~1\paul\LOCALS~1\Temp\Rtmp21143\file30218.sas
a log and other error products should be in the vicinity
Warning messages:
1: sas not found
2: ls not found
3: SAS return code was -1 in: read.ssd("C:/temp", "newdat1")

I can export the SAS data as a tab delimited file and then read
it into R with read.delim, but I have many SAS files I need to
look at. Does anyone have experience with this?

Thanks

Paul Green



From deepayan at stat.wisc.edu  Wed Oct 29 17:04:03 2003
From: deepayan at stat.wisc.edu (Deepayan Sarkar)
Date: Wed, 29 Oct 2003 10:04:03 -0600
Subject: [R] grid: dividing units by numbers
In-Reply-To: <20031029153603.GA5442@s1x.local>
References: <20031029153603.GA5442@s1x.local>
Message-ID: <200310291004.03113.deepayan@stat.wisc.edu>

On Wednesday 29 October 2003 09:36, Wolfram Fischer wrote:
> How can I divide a unit by an number
> or average a vector of units, e.g.:
>
> 	u1 <- unit( 3, 'npc' )
> 	u2 <- unit( 6, 'npc' )
>
> 	u1 / 2

0.5 * u1

> 	( u1 + u2 ) / 2

0.5 * (u1 + u2)

> 	mean( unit.c(u1,u2) )

Not sure if that's generally doable (other than adding all the units one at a 
time in a loop and then multiplying by 1/length).

But if all your units are going to be "npc", you might as well keep the 'x'-s 
separate and do arithmetic on them.

HTH,

Deepayan

> I would use that e.g. to to calculate the coordinates
> of the midpoint of a line.
>
> Wolfram
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help



From flom at ndri.org  Wed Oct 29 17:19:33 2003
From: flom at ndri.org (Peter Flom)
Date: Wed, 29 Oct 2003 11:19:33 -0500
Subject: [R] rmutil located
Message-ID: <sf9fa24c.088@MAIL.NDRI.ORG>

Thanks to all who pointed out that rmutil is also on Jim Lindsey's
site.

Peter



From edd at debian.org  Wed Oct 29 17:24:31 2003
From: edd at debian.org (Dirk Eddelbuettel)
Date: Wed, 29 Oct 2003 10:24:31 -0600
Subject: [R] /usr/lib/R/library vs /usr/local/lib/R/site-library
In-Reply-To: <85smlcjb4b.fsf@blindglobe.net>
References: <20031029102252.5bb72714.f.harrell@vanderbilt.edu>
	<85smlcjb4b.fsf@blindglobe.net>
Message-ID: <20031029162431.GA21973@sonny.eddelbuettel.com>

On Wed, Oct 29, 2003 at 07:51:16AM -0800, A.J. Rossini wrote:
> Frank E Harrell Jr <f.harrell at vanderbilt.edu> writes:
> 
> 1> I would appreciate getting a clarification of /usr/lib/R/library vs
> 1> /usr/local/lib/R/site-library.  I am running R 1.8 on Debian Linux.
> 1> On one occasion doing update.packages() resulted in versions of one
> 1> or more libraries being placed in one of these directories without
> 1> removing the old version, and the old version took precedence over
> 1> the new.  I'm sorry I did not save the steps I used.  I would
> 1> appreciate reading how best to manage libraries.  Currently I have
> 1> the following in /usr/local/lib/R/site-library: Design, Hmisc,
> 1> Rcmdr, acepack, car.  /usr/lib/R/library contains the following:
> >
> > KernSmooth  boot     eda      lqs      modreg  nnet     stepfun   ts
> > MASS        class    foreign  methods  mva     rpart    survival
> > R.css       cluster  grid     mgcv     nlme    spatial  tcltk
> > base        ctest    lattice  mle      nls     splines  tools
> >
> 
> On Debian (and I'm sure Dirk will pipe in), /usr/lib/R/library is for
> apt-installed by the core R packages, 

(i.e. r-base-core and r-recommended)

> /usr/lib/R/site-library is for
> apt-installed R packages (from CRAN, or Jim Lindsey's works), and

(or from Omegahat. [1] )

> /usr/local/lib/R/site-library where you want to put locally installed
> stuff (i.e. "install.packages()" should put there).

All correct. 

This stems from a consensus reached between Drs Bates, Eddelbuettel, Leisch
and Hornik during DSC 2003.  It is not yet fully implemented as not all 
apt-get'able Debian packages of CRAN, Omegahat, ... packages have been
rebuilt -- and thos who haven't are still installing into /usr/lib/R/library
but will install into .../site-library once rebuilt.

The broader aspects of this are outlined in a 'Debian R Policy' draft Doug
and I have been working on, on and off. A new draft is nearing completion,
and I'll make sure I'll let Frank comment too.

Dirk

[1] And one day from BioC (Tony and I are only about a good year late ;-)

-- 
Those are my principles, and if you don't like them... well, I have others.
                                                -- Groucho Marx



From tblackw at umich.edu  Wed Oct 29 17:33:52 2003
From: tblackw at umich.edu (Thomas W Blackwell)
Date: Wed, 29 Oct 2003 11:33:52 -0500 (EST)
Subject: [R] importing SAS data 
In-Reply-To: <5.1.0.14.2.20031029104356.00b2c178@mailkardia.sph.umich.edu>
References: <5.1.0.14.2.20031029104356.00b2c178@mailkardia.sph.umich.edu>
Message-ID: <Pine.SOL.4.58.0310291131100.2611@robotron.gpcc.itd.umich.edu>

Paul  -

I think you have to have an executable copy of SAS on the
same machine as R in order for this to work.  I've never
used it, but I remember a very similar question on the
list from just a few months ago.  Maybe there's some
description of this in the foreign package manual, or in
the help pages.

-  tom blackwell  -  u michigna medical school  -  ann arbor  -

On Wed, 29 Oct 2003, Paul Green wrote:

> I am trying to import a permanent SAS data
> set using read.ssd in the foreign library. I get
> the following error:
>
>  > list.files("C:/temp")
> [1] "newdat1.sas7bdat" "snpm1.sas7bdat"   "test1.sas"
>  > library(foreign)
>  > newdat1 <- read.ssd("C:/temp","newdat1")
> SAS failed.  SAS program at
> C:\DOCUME~1\paul\LOCALS~1\Temp\Rtmp21143\file30218.sas
> a log and other error products should be in the vicinity
> Warning messages:
> 1: sas not found
> 2: ls not found
> 3: SAS return code was -1 in: read.ssd("C:/temp", "newdat1")
>
> I can export the SAS data as a tab delimited file and then read
> it into R with read.delim, but I have many SAS files I need to
> look at. Does anyone have experience with this?
>
> Thanks
>
> Paul Green
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
>



From corvin3 at verizon.net  Wed Oct 29 18:39:57 2003
From: corvin3 at verizon.net (DAWN CORVIN)
Date: Wed, 29 Oct 2003 11:39:57 -0600
Subject: [R] Cigarettes $20.95 per carton S/H 
Message-ID: <000601c39e43$b7ee21a0$f14a3e04@p424ghz>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20031029/6f3ffcba/attachment.pl

From ggrothendieck at myway.com  Wed Oct 29 17:43:08 2003
From: ggrothendieck at myway.com (Gabor Grothendieck)
Date: Wed, 29 Oct 2003 11:43:08 -0500 (EST)
Subject: [R] importing SAS data 
Message-ID: <20031029164308.45D463963@xmxpita.myway.com>



There is a utility called dataload at:

   http://www.vsn-intl.com/genstat/downloads/datald.htm

that says it can convert SAS files (and numerous other
formats) to csv and other formats.  I have successfully 
used it to convert Excel xls files to csv although I don't 
have experience with its SAS capability.


---

From: Paul Green <pgreen at umich.edu>
[ Add to Address Book | Block Address | Report as Spam ] 
To: <r-help at stat.math.ethz.ch> 
Subject: [R] importing SAS data  

 
 
I am trying to import a permanent SAS data
set using read.ssd in the foreign library. I get
the following error:

> list.files("C:/temp")
[1] "newdat1.sas7bdat" "snpm1.sas7bdat" "test1.sas"
> library(foreign)
> newdat1 <- read.ssd("C:/temp","newdat1")
SAS failed. SAS program at 
C:\DOCUME~1\paul\LOCALS~1\Temp\Rtmp21143\file30218.sas
a log and other error products should be in the vicinity
Warning messages:
1: sas not found
2: ls not found
3: SAS return code was -1 in: read.ssd("C:/temp", "newdat1")

I can export the SAS data as a tab delimited file and then read
it into R with read.delim, but I have many SAS files I need to
look at. Does anyone have experience with this?

Thanks

Paul Green


 
 

  
 

        

_______________________________________________
No banners. No pop-ups. No kidding.
Introducing My Way - http://www.myway.com



From Timur.Elzhov at jinr.ru  Wed Oct 29 17:47:22 2003
From: Timur.Elzhov at jinr.ru (Timur Elzhov)
Date: Wed, 29 Oct 2003 19:47:22 +0300
Subject: [R] evaluating expression within nls()
In-Reply-To: <20031029155405.GA6653@nf034.jinr.ru>
References: <20031029155405.GA6653@nf034.jinr.ru>
Message-ID: <20031029164722.GA7009@nf034.jinr.ru>

On Wed, Oct 29, 2003 at 06:54:05PM +0300, Timur Elzhov wrote:

> Yes, in help(nls) is pointed, that
>     Arguments:
>       data: an optional data frame in which to evaluate the variables in
>             `formula'
> 
> But, I'd like 'nls' to see my expression 'expr'... :-/  How can I do that?
Sorry, I was mistaken :)  The bug was in the another my function.

--
WBR



From MSchwartz at medanalytics.com  Wed Oct 29 17:53:15 2003
From: MSchwartz at medanalytics.com (Marc Schwartz)
Date: Wed, 29 Oct 2003 10:53:15 -0600
Subject: [R] importing SAS data
In-Reply-To: <Pine.SOL.4.58.0310291131100.2611@robotron.gpcc.itd.umich.edu>
References: <5.1.0.14.2.20031029104356.00b2c178@mailkardia.sph.umich.edu>
	<Pine.SOL.4.58.0310291131100.2611@robotron.gpcc.itd.umich.edu>
Message-ID: <1067446394.4538.77.camel@localhost.localdomain>

It is not explicitly stated in the help for read.ssd(), but the
description says:

"generates a SAS program to convert the ssd contents to SAS transport
format and then uses read.xport to obtain a dataframe"

I would imagine that "generates a SAS program" implies that SAS itself
must be available to actually run the program.

Presuming that Paul has SAS on his machine or is otherwise directly
accessible, he likely needs to add the path to sas.exe to his 'PATH'
variable under Windows.

Also, Paul did not indicate which version of Windows he is using. I know
XP had an 'ls' command, but I don't recall if the prior versions did or
did not. 'ls' is more common to Unix/Linux.

An alternative to this would be Frank Harrell's sas.get() in the Hmisc
library if you cannot get read.ssd() to work. sas.get() also requires
SAS to be installed and in the 'PATH'.

HTH,

Marc Schwartz

On Wed, 2003-10-29 at 10:33, Thomas W Blackwell wrote:
> Paul  -
> 
> I think you have to have an executable copy of SAS on the
> same machine as R in order for this to work.  I've never
> used it, but I remember a very similar question on the
> list from just a few months ago.  Maybe there's some
> description of this in the foreign package manual, or in
> the help pages.
> 
> -  tom blackwell  -  u michigna medical school  -  ann arbor  -
> 
> On Wed, 29 Oct 2003, Paul Green wrote:
> 
> > I am trying to import a permanent SAS data
> > set using read.ssd in the foreign library. I get
> > the following error:
> >
> >  > list.files("C:/temp")
> > [1] "newdat1.sas7bdat" "snpm1.sas7bdat"   "test1.sas"
> >  > library(foreign)
> >  > newdat1 <- read.ssd("C:/temp","newdat1")
> > SAS failed.  SAS program at
> > C:\DOCUME~1\paul\LOCALS~1\Temp\Rtmp21143\file30218.sas
> > a log and other error products should be in the vicinity
> > Warning messages:
> > 1: sas not found
> > 2: ls not found
> > 3: SAS return code was -1 in: read.ssd("C:/temp", "newdat1")
> >
> > I can export the SAS data as a tab delimited file and then read
> > it into R with read.delim, but I have many SAS files I need to
> > look at. Does anyone have experience with this?
> >
> > Thanks
> >
> > Paul Green



From rvencio at ime.usp.br  Wed Oct 29 17:41:21 2003
From: rvencio at ime.usp.br (Ricardo Zorzetto Nicoliello Vencio)
Date: Wed, 29 Oct 2003 14:41:21 -0200 (BRST)
Subject: [R] =?iso-8859-1?q?constrOptim_doesn=B4t_send_arguments_to_optim?=
 =?iso-8859-1?q?!=28=3F=29?=
Message-ID: <Pine.LNX.4.44.0310291401320.1676-100000@kevlar.ime.usp.br>


Hi,

I think that there something wrong with the 'constrOptim' max/minimization
function because she doesn?t send extra arguments to 'optim' call.

Fact: When I use optim in a f(x,theta)-like function, everything goes ok.
But using constrOptim with the same function leads to error...

Proof: Make a small change in the 'Rosenbrock Banana function' (taken from
the Examples section of those help pages) adding extra (useless)
parameters:

fr <- function(x,extra1,extra2) {   ## Rosenbrock Banana function
    x1 <- x[1]
    x2 <- x[2]
    extra1*extra2*100 * (x2 - x1 * x1)^2 + (1 - x1)^2
}
grr <- function(x,extra1,extra2) { ## Gradient of 'fr'
    x1 <- x[1]
    x2 <- x[2]
    extra1*extra2*c(-400 * x1 * (x2 - x1 * x1) - 2 * (1 - x1),
       200 *      (x2 - x1 * x1))
}


now, call it:

optim(c(-1.2,1), fr, grr, extra1=c(1,2), extra2=c(3,4))

ok!

but now:

constrOptim(c(-1.2,0.9), fr, grr, ui=rbind(c(-1,0),c(0,-1)), ci=c(-1,-1),
extra1=c(1,2), extra2=c(3,4) )

return the error:

Error in f(theta) : Argument "extra1" is missing, with no default

But in 'constrOptim' help page says:

...: Other arguments passed to `optim'

And in 'optim' help page says:

...: Further arguments to be passed to `fn' and `gr'.


My guess is that when 'constrOptim' calls 'optim' she doen?t send the
extra arguments to correct evaluation of functions non-f(x)-like such as
f(x,theta)-like.

But whow I (we?) can fix it ? Any ideas ?



From rossini at blindglobe.net  Wed Oct 29 18:11:13 2003
From: rossini at blindglobe.net (A.J. Rossini)
Date: Wed, 29 Oct 2003 09:11:13 -0800
Subject: [R] /usr/lib/R/library vs /usr/local/lib/R/site-library
In-Reply-To: <20031029162431.GA21973@sonny.eddelbuettel.com> (Dirk
	Eddelbuettel's message of "Wed, 29 Oct 2003 10:24:31 -0600")
References: <20031029102252.5bb72714.f.harrell@vanderbilt.edu>
	<85smlcjb4b.fsf@blindglobe.net>
	<20031029162431.GA21973@sonny.eddelbuettel.com>
Message-ID: <85ekww55qm.fsf@blindglobe.net>

Dirk Eddelbuettel <edd at debian.org> writes:

> [1] And one day from BioC (Tony and I are only about a good year late ;-)

Well, I'm waiting for the release of 1.3.... :-P.

bset,
-tony

-- 
rossini at u.washington.edu            http://www.analytics.washington.edu/ 
Biomedical and Health Informatics   University of Washington
Biostatistics, SCHARP/HVTN          Fred Hutchinson Cancer Research Center
UW (Tu/Th/F): 206-616-7630 FAX=206-543-3461 | Voicemail is unreliable
FHCRC  (M/W): 206-667-7025 FAX=206-667-4812 | use Email

CONFIDENTIALITY NOTICE: This e-mail message and any attachme...{{dropped}}



From bates at stat.wisc.edu  Wed Oct 29 18:35:49 2003
From: bates at stat.wisc.edu (Douglas Bates)
Date: 29 Oct 2003 11:35:49 -0600
Subject: [R] /usr/lib/R/library vs /usr/local/lib/R/site-library
In-Reply-To: <85ekww55qm.fsf@blindglobe.net>
References: <20031029102252.5bb72714.f.harrell@vanderbilt.edu>
	<85smlcjb4b.fsf@blindglobe.net>
	<20031029162431.GA21973@sonny.eddelbuettel.com>
	<85ekww55qm.fsf@blindglobe.net>
Message-ID: <6rznfkaqve.fsf@bates4.stat.wisc.edu>

rossini at blindglobe.net (A.J. Rossini) writes:

> Dirk Eddelbuettel <edd at debian.org> writes:
> 
> > [1] And one day from BioC (Tony and I are only about a good year late ;-)
> 
> Well, I'm waiting for the release of 1.3.... :-P.

In that case I think you should complain to the Bioconductor release
manager :-)

(For those not familiar with Bioconductor internal positions, I should
explain that Tony *is* the Bioconductor release manager.)



From Joerg.Schaber at uv.es  Wed Oct 29 18:44:41 2003
From: Joerg.Schaber at uv.es (Joerg Schaber)
Date: Wed, 29 Oct 2003 18:44:41 +0100
Subject: [R] test for connectedness
Message-ID: <3F9FFC89.2090906@uv.es>

Hi,

in a 2-way linear model with missing data the normal equations can 
sometimes be separated in 2 or more independent sets of equations 
brought about by the nature of the missing data. As far as I know this 
phenomenon is called  'connectedness' of the data.
Does anyone know of a test or algorithm in R that extracts the connected 
sets from the data (all I need is an algorithm for a 2-way classification)

greetings,

joerg



From rpeng at jhsph.edu  Wed Oct 29 18:53:19 2003
From: rpeng at jhsph.edu (Roger D. Peng)
Date: Wed, 29 Oct 2003 12:53:19 -0500
Subject: [R] constrOptim =?ISO-8859-1?Q?doesn=B4t_send_arguments_?=
	=?ISO-8859-1?Q?to_optim!=28=3F=29?=
In-Reply-To: <Pine.LNX.4.44.0310291401320.1676-100000@kevlar.ime.usp.br>
References: <Pine.LNX.4.44.0310291401320.1676-100000@kevlar.ime.usp.br>
Message-ID: <3F9FFE8F.3020907@jhsph.edu>

I'm not sure this is necessarily a bug in constrOptim.  I think the 
problem is that while `...' is in fact passed to optim(), the objective 
function is evaluated in constrOptim a few times before optim() is 
called and `...' is not passed in those cases.

The easiest way to make this problem go away is use lexical scoping.  
For example:

make.fr <- function(extra1, extra2) {
    function(x) {
       x1 <- x[1]
       x2 <- x[2]
       extra1 * extra2 * 100 * (x2-x1 * x1)^2 + (1-x1)^2
    }
}

And then in R

>  f <- make.fr(c(1,2), c(3,4))

Then call constrOptim using this new f().  You don't have to worry about 
passing extra parameters now.

-roger

Ricardo Zorzetto Nicoliello Vencio wrote:

>Hi,
>
>I think that there something wrong with the 'constrOptim' max/minimization
>function because she doesn?t send extra arguments to 'optim' call.
>
>Fact: When I use optim in a f(x,theta)-like function, everything goes ok.
>But using constrOptim with the same function leads to error...
>
>Proof: Make a small change in the 'Rosenbrock Banana function' (taken from
>the Examples section of those help pages) adding extra (useless)
>parameters:
>
>fr <- function(x,extra1,extra2) {   ## Rosenbrock Banana function
>    x1 <- x[1]
>    x2 <- x[2]
>    extra1*extra2*100 * (x2 - x1 * x1)^2 + (1 - x1)^2
>}
>grr <- function(x,extra1,extra2) { ## Gradient of 'fr'
>    x1 <- x[1]
>    x2 <- x[2]
>    extra1*extra2*c(-400 * x1 * (x2 - x1 * x1) - 2 * (1 - x1),
>       200 *      (x2 - x1 * x1))
>}
>
>
>now, call it:
>
>optim(c(-1.2,1), fr, grr, extra1=c(1,2), extra2=c(3,4))
>
>ok!
>
>but now:
>
>constrOptim(c(-1.2,0.9), fr, grr, ui=rbind(c(-1,0),c(0,-1)), ci=c(-1,-1),
>extra1=c(1,2), extra2=c(3,4) )
>
>return the error:
>
>Error in f(theta) : Argument "extra1" is missing, with no default
>
>But in 'constrOptim' help page says:
>
>...: Other arguments passed to `optim'
>
>And in 'optim' help page says:
>
>...: Further arguments to be passed to `fn' and `gr'.
>
>
>My guess is that when 'constrOptim' calls 'optim' she doen?t send the
>extra arguments to correct evaluation of functions non-f(x)-like such as
>f(x,theta)-like.
>
>But whow I (we?) can fix it ? Any ideas ?
>
>______________________________________________
>R-help at stat.math.ethz.ch mailing list
>https://www.stat.math.ethz.ch/mailman/listinfo/r-help
>
>  
>



From umeno at students.uiuc.edu  Wed Oct 29 20:05:30 2003
From: umeno at students.uiuc.edu (umeno)
Date: Wed, 29 Oct 2003 13:05:30 -0600
Subject: [R] constrained OLS
Message-ID: <3FAFB730@webmail.uiuc.edu>

Hi,

I would like to know if anyone has any idea of how to run an OLS with 
constraints?

thank you
Soyoko

______________________________________
Ms. Soyoko Umeno
Graduate Research Assitant for the Illinois-Missouri Biotechnology Alliance (IMBA) at http://www.imba.missouri.edu/
Ph.D. Student at the Department of Agricultural and Consumer Economics
at the University of Illinois at Urbana-Champaign
Office Phone: 217-333-3417 or 217-333-0364
Fax: 217-244-4817
Mailing Address: 1301 W. Gregory Dr. MC710, Urbana, IL 61801



From umeno at students.uiuc.edu  Wed Oct 29 20:10:31 2003
From: umeno at students.uiuc.edu (umeno)
Date: Wed, 29 Oct 2003 13:10:31 -0600
Subject: [R] constrained OLS on coefficient
Message-ID: <3FAFC6C7@webmail.uiuc.edu>

Hi,

I would like to know if anyone has any idea of how to run an OLS with
constraints? I need to contraint a coefficient estimate in the model equal to 
1, and I am not sure how to include it into the OLS estimation...

I was hoping to find something like "cnsreg" in STATA..

thank you
Soyoko

______________________________________
Ms. Soyoko Umeno
Graduate Research Assitant for the Illinois-Missouri Biotechnology Alliance (IMBA) at http://www.imba.missouri.edu/
Ph.D. Student at the Department of Agricultural and Consumer Economics
at the University of Illinois at Urbana-Champaign
Office Phone: 217-333-3417 or 217-333-0364
Fax: 217-244-4817
Mailing Address: 1301 W. Gregory Dr. MC710, Urbana, IL 61801



From tblackw at umich.edu  Wed Oct 29 20:19:03 2003
From: tblackw at umich.edu (Thomas W Blackwell)
Date: Wed, 29 Oct 2003 14:19:03 -0500 (EST)
Subject: [R] constrained OLS on coefficient
In-Reply-To: <3FAFC6C7@webmail.uiuc.edu>
References: <3FAFC6C7@webmail.uiuc.edu>
Message-ID: <Pine.SOL.4.58.0310291417210.8695@robotron.gpcc.itd.umich.edu>


Use  lm() or glm()  with argument 'offset' set to
the value of the column whose coefficient must be 1.
See  help("lm").

-  tom blackwell  -  u michigan medical school  -  ann arbor  -

On Wed, 29 Oct 2003, umeno wrote:

> Hi,
>
> I would like to know if anyone has any idea of how to run an OLS with
> constraints? I need to contraint a coefficient estimate in the model equal to
> 1, and I am not sure how to include it into the OLS estimation...
>
> I was hoping to find something like "cnsreg" in STATA..
>
> thank you
> Soyoko
>
> ______________________________________
> Ms. Soyoko Umeno
> Graduate Research Assitant for the Illinois-Missouri Biotechnology Alliance (IMBA) at http://www.imba.missouri.edu/
> Ph.D. Student at the Department of Agricultural and Consumer Economics
> at the University of Illinois at Urbana-Champaign
> Office Phone: 217-333-3417 or 217-333-0364
> Fax: 217-244-4817
> Mailing Address: 1301 W. Gregory Dr. MC710, Urbana, IL 61801
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
>



From spencer.graves at pdf.com  Wed Oct 29 20:29:27 2003
From: spencer.graves at pdf.com (Spencer Graves)
Date: Wed, 29 Oct 2003 11:29:27 -0800
Subject: [R] constrained OLS on coefficient
In-Reply-To: <3FAFC6C7@webmail.uiuc.edu>
References: <3FAFC6C7@webmail.uiuc.edu>
Message-ID: <3FA01517.2030104@pdf.com>

      I don't know STATA, but if you want to force a specific regression 
coefficient to be 1, I think that can be done with the formula.  
Consider the following: 

      DF <- data.frame(x1=1:6, x2=rep(1:2, 3), y=rep(1:3, 2))
      lm(y-x1~x2-1, DF)

      The formula "y-x1~x2-1" fits a noconstant model, specified by the 
"-1" of y-x1 regressed on x2. 

      Does this answer the question? 
      spencer graves

umeno wrote:

>Hi,
>
>I would like to know if anyone has any idea of how to run an OLS with
>constraints? I need to contraint a coefficient estimate in the model equal to 
>1, and I am not sure how to include it into the OLS estimation...
>
>I was hoping to find something like "cnsreg" in STATA..
>
>thank you
>Soyoko
>
>______________________________________
>Ms. Soyoko Umeno
>Graduate Research Assitant for the Illinois-Missouri Biotechnology Alliance (IMBA) at http://www.imba.missouri.edu/
>Ph.D. Student at the Department of Agricultural and Consumer Economics
>at the University of Illinois at Urbana-Champaign
>Office Phone: 217-333-3417 or 217-333-0364
>Fax: 217-244-4817
>Mailing Address: 1301 W. Gregory Dr. MC710, Urbana, IL 61801
>
>______________________________________________
>R-help at stat.math.ethz.ch mailing list
>https://www.stat.math.ethz.ch/mailman/listinfo/r-help
>  
>



From p.dalgaard at biostat.ku.dk  Wed Oct 29 20:40:20 2003
From: p.dalgaard at biostat.ku.dk (Peter Dalgaard)
Date: 29 Oct 2003 20:40:20 +0100
Subject: [R] Where is rmutil package?
In-Reply-To: <sf9f9b33.092@MAIL.NDRI.ORG>
References: <sf9f9b33.092@MAIL.NDRI.ORG>
Message-ID: <x2y8v3de8r.fsf@biostat.ku.dk>

"Peter Flom" <flom at ndri.org> writes:

> Pursing my earlier question, when I tried loading Lindsey's gnlm, I got
> a
> message
> 
> Loading required package: rmutil 
> Warning message: 
> There is no package called 'rmutil' in: library(package, character.only
> = TRUE, logical = TRUE, warn.conflicts = warn.conflicts,  
> 
> 
> According to the R documentation
> http://finzi.psych.upenn.edu/R/doc/html/packages.html
> 
> rmutil is in the standard library.....

It's *been installed* in the "standard library" on the machine that
serves those pages, along with just about everything in sight... It is
another one of Jim Lindsey's packages, so you need to get it from his
website.

(The maintainers of the site might want to take note of this possible
misunderstanding. I believe that splitting the R library into several
directories and setting R_LIBS would make the listing more readable.)

> If I ignore the message and try fitting a model with one of the
> functions in gnlm (e.g. fmr)  I get an error that it coudn't find the
> function 'finterp'

Not surprising...

-- 
   O__  ---- Peter Dalgaard             Blegdamsvej 3  
  c/ /'_ --- Dept. of Biostatistics     2200 Cph. N   
 (*) \(*) -- University of Copenhagen   Denmark      Ph: (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)             FAX: (+45) 35327907



From p.murrell at auckland.ac.nz  Wed Oct 29 21:00:42 2003
From: p.murrell at auckland.ac.nz (Paul Murrell)
Date: Thu, 30 Oct 2003 09:00:42 +1300
Subject: [R] grid: dividing units by numbers
References: <20031029153603.GA5442@s1x.local>
	<200310291004.03113.deepayan@stat.wisc.edu>
Message-ID: <3FA01C6A.10208@stat.auckland.ac.nz>

Hi


Deepayan Sarkar wrote:
> On Wednesday 29 October 2003 09:36, Wolfram Fischer wrote:
> 
>>How can I divide a unit by an number
>>or average a vector of units, e.g.:
>>
>>	u1 <- unit( 3, 'npc' )
>>	u2 <- unit( 6, 'npc' )
>>
>>	u1 / 2
> 
> 
> 0.5 * u1
> 
> 
>>	( u1 + u2 ) / 2
> 
> 
> 0.5 * (u1 + u2)
> 
> 
>>	mean( unit.c(u1,u2) )
> 
> 
> Not sure if that's generally doable (other than adding all the units one at a 
> time in a loop and then multiplying by 1/length).


u <- unit.c(u1, u2)
1/unit.length(u) * sum(u)


> But if all your units are going to be "npc", you might as well keep the 'x'-s 
> separate and do arithmetic on them.


There are a limited set of mathematical functions/operators for grid 
units so far.

These include: +, -, and *; sum(), min(), and max().

There are also some unit-specific versions of some other useful 
functions:  unit.c(), unit.rep(), unit.length(), unit.pmin(), unit.pmax()

For more complex calculations involving units, you may need to look at 
grid.convert() (but take note of the warning on its help page).

Paul
-- 
Dr Paul Murrell
Department of Statistics
The University of Auckland
Private Bag 92019
Auckland
New Zealand
64 9 3737599 x85392
paul at stat.auckland.ac.nz
http://www.stat.auckland.ac.nz/~paul/



From p.dalgaard at biostat.ku.dk  Wed Oct 29 21:10:02 2003
From: p.dalgaard at biostat.ku.dk (Peter Dalgaard)
Date: 29 Oct 2003 21:10:02 +0100
Subject: [R] importing SAS data
In-Reply-To: <20031029164308.45D463963@xmxpita.myway.com>
References: <20031029164308.45D463963@xmxpita.myway.com>
Message-ID: <x2u15rdcv9.fsf@biostat.ku.dk>

"Gabor Grothendieck" <ggrothendieck at myway.com> writes:

> There is a utility called dataload at:
> 
>    http://www.vsn-intl.com/genstat/downloads/datald.htm
> 
> that says it can convert SAS files (and numerous other
> formats) to csv and other formats.  I have successfully 
> used it to convert Excel xls files to csv although I don't 
> have experience with its SAS capability.

Seems that it only does PC SAS 6.03-12. Rather curious, BTW, to see
software claiming

Dataload.UNI - Unix Motif version of Dataload.exe

and then

DATALOAD.UNI: ELF 32-bit MSB executable, SPARC, version 1 (SYSV), dynamically linked (uses shared libs), not stripped

as if no other Unix than Sun Solaris existed. Also, the program is in
the public domain but the source is not included. Does anyone know
what the author's long-term plans are with this? (It's a GenStat
complimentary item, so GenStat non-users obviously has little say in
the matter.)

-- 
   O__  ---- Peter Dalgaard             Blegdamsvej 3  
  c/ /'_ --- Dept. of Biostatistics     2200 Cph. N   
 (*) \(*) -- University of Copenhagen   Denmark      Ph: (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)             FAX: (+45) 35327907



From feh3k at spamcop.net  Wed Oct 29 22:04:02 2003
From: feh3k at spamcop.net (Frank E Harrell Jr)
Date: Wed, 29 Oct 2003 16:04:02 -0500
Subject: [R] importing SAS data
In-Reply-To: <x2u15rdcv9.fsf@biostat.ku.dk>
References: <20031029164308.45D463963@xmxpita.myway.com>
	<x2u15rdcv9.fsf@biostat.ku.dk>
Message-ID: <20031029160402.57a76a82.feh3k@spamcop.net>

On 29 Oct 2003 21:10:02 +0100
Peter Dalgaard <p.dalgaard at biostat.ku.dk> wrote:

> "Gabor Grothendieck" <ggrothendieck at myway.com> writes:
> 
> > There is a utility called dataload at:
> > 
> >    http://www.vsn-intl.com/genstat/downloads/datald.htm
> > 
> > that says it can convert SAS files (and numerous other
> > formats) to csv and other formats.  I have successfully 
> > used it to convert Excel xls files to csv although I don't 
> > have experience with its SAS capability.
> 
> Seems that it only does PC SAS 6.03-12. Rather curious, BTW, to see
> software claiming
> 
> Dataload.UNI - Unix Motif version of Dataload.exe
> 
> and then
> 
> DATALOAD.UNI: ELF 32-bit MSB executable, SPARC, version 1 (SYSV), dynamically linked (uses shared libs), not stripped
> 
> as if no other Unix than Sun Solaris existed. Also, the program is in
> the public domain but the source is not included. Does anyone know
> what the author's long-term plans are with this? (It's a GenStat
> complimentary item, so GenStat non-users obviously has little say in
> the matter.)
> 
> -- 
>    O__  ---- Peter Dalgaard             Blegdamsvej 3  

The file dataload is a Linux executable.  But my e-mail to the author asking about the ability to enhance the software to read multiple-dataset SAS transport files bounced.

Frank

---
Frank E Harrell Jr    Professor and Chair            School of Medicine
                      Department of Biostatistics    Vanderbilt University



From Soren.Hojsgaard at agrsci.dk  Wed Oct 29 23:10:12 2003
From: Soren.Hojsgaard at agrsci.dk (=?utf-8?Q?S=C3=B8ren_H=C3=B8jsgaard?=)
Date: Wed, 29 Oct 2003 23:10:12 +0100
Subject: [R] loglm() uses only a reference to data,
	and not data itself - is that on purpose??
Message-ID: <C83C5E3DEEE97E498B74729A33F6EAEC105A7D@DJFPOST01.djf.agrsci.dk>

Dear all,
Consider the following:
 
library(MASS); data(HairEyeColor)
l1 <- loglm(~ 1+2+3, data=HairEyeColor)
myloglm <- function(){
  nd <- HairEyeColor
  v <- loglm(~ 1+2+3, data=nd)
  return(v)
}  
  
l2 <- myloglm()  
  
Now, step(l1) works, whereas step(l2) does not, the problem being that data nd does not exist outside myloglm. I was under the impression that objects from functions like glm() and lm() "carried their data" with them, but that does not seem to be the case for loglm(). Is there a reason for these different behaviours??
 
Thanks in advance
S?ren H?jsgaard



From GPetris at uark.edu  Wed Oct 29 23:11:43 2003
From: GPetris at uark.edu (Giovanni Petris)
Date: Wed, 29 Oct 2003 16:11:43 -0600 (CST)
Subject: [R] restarting split.screen
Message-ID: <200310292211.h9TMBhhk026369@definetti.uark.edu>


Is there a way of `restarting' split.screen?
This is what I am getting:

> close.screen()
[1] 10 11 12 13
> close.screen(all=TRUE)
Error in par(args) : parameter "i" in "mfg" is out of range
> graphics.off()
> x11()
> close.screen()
[1] 10 11 12 13
> close.screen(all=TRUE)
Error in par(args) : parameter "i" in "mfg" is out of range

As you can see, closing the graphic device is not enough. 
Is there a solution - short of restarting R?

In case it matters,

> version
         _                   
platform sparc-sun-solaris2.7
arch     sparc               
os       solaris2.7          
system   sparc, solaris2.7   
status                       
major    1                   
minor    8.0                 
year     2003                
month    10                  
day      08                  
language R                   


Thank you in advance,
Giovanni

-- 

 __________________________________________________
[                                                  ]
[ Giovanni Petris                 GPetris at uark.edu ]
[ Department of Mathematical Sciences              ]
[ University of Arkansas - Fayetteville, AR 72701  ]
[ Ph: (479) 575-6324, 575-8630 (fax)               ]
[ http://definetti.uark.edu/~gpetris/              ]
[__________________________________________________]



From ryszard.czerminski at pharma.novartis.com  Wed Oct 29 23:33:49 2003
From: ryszard.czerminski at pharma.novartis.com (ryszard.czerminski@pharma.novartis.com)
Date: Wed, 29 Oct 2003 17:33:49 -0500
Subject: [R] svm from e1071 package
Message-ID: <OF9BE158A4.C18CE709-ON85256DCE.007B37C8-85256DCE.007C06C3@EU.novartis.net>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20031029/669ca256/attachment.pl

From GPetris at uark.edu  Wed Oct 29 23:39:37 2003
From: GPetris at uark.edu (Giovanni Petris)
Date: Wed, 29 Oct 2003 16:39:37 -0600 (CST)
Subject: [R] restarting split.screen (solved)
In-Reply-To: <200310292211.h9TMBhhk026369@definetti.uark.edu> (message from
	Giovanni Petris on Wed, 29 Oct 2003 16:11:43 -0600 (CST))
References: <200310292211.h9TMBhhk026369@definetti.uark.edu>
Message-ID: <200310292239.h9TMdbVP026424@definetti.uark.edu>


Ok, after realizing that even restarting R did not work, I found the
solution to my problem:

> grep("split",ls(all=T),value=T)
[1] ".split.cur.screen"    ".split.par.list"      ".split.saved.pars"   
[4] ".split.screens"       ".split.valid.screens"
> rm(list=grep("split",ls(all=T),value=T))

Sorry for the waste of bandwidth...
Giovanni

> Date: Wed, 29 Oct 2003 16:11:43 -0600 (CST)
> From: Giovanni Petris <GPetris at uark.edu>
> Sender: r-help-bounces at stat.math.ethz.ch
> Precedence: list
> 
> 
> Is there a way of `restarting' split.screen?
> This is what I am getting:
> 
> > close.screen()
> [1] 10 11 12 13
> > close.screen(all=TRUE)
> Error in par(args) : parameter "i" in "mfg" is out of range
> > graphics.off()
> > x11()
> > close.screen()
> [1] 10 11 12 13
> > close.screen(all=TRUE)
> Error in par(args) : parameter "i" in "mfg" is out of range
> 
> As you can see, closing the graphic device is not enough. 
> Is there a solution - short of restarting R?
> 
> In case it matters,
> 
> > version
>          _                   
> platform sparc-sun-solaris2.7
> arch     sparc               
> os       solaris2.7          
> system   sparc, solaris2.7   
> status                       
> major    1                   
> minor    8.0                 
> year     2003                
> month    10                  
> day      08                  
> language R                   
> 
> 
> Thank you in advance,
> Giovanni
> 
> -- 
> 
>  __________________________________________________
> [                                                  ]
> [ Giovanni Petris                 GPetris at uark.edu ]
> [ Department of Mathematical Sciences              ]
> [ University of Arkansas - Fayetteville, AR 72701  ]
> [ Ph: (479) 575-6324, 575-8630 (fax)               ]
> [ http://definetti.uark.edu/~gpetris/              ]
> [__________________________________________________]
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> 
> 
> 


-- 

 __________________________________________________
[                                                  ]
[ Giovanni Petris                 GPetris at uark.edu ]
[ Department of Mathematical Sciences              ]
[ University of Arkansas - Fayetteville, AR 72701  ]
[ Ph: (479) 575-6324, 575-8630 (fax)               ]
[ http://definetti.uark.edu/~gpetris/              ]
[__________________________________________________]



From meyer at ci.tuwien.ac.at  Thu Oct 30 00:27:31 2003
From: meyer at ci.tuwien.ac.at (David Meyer)
Date: Thu, 30 Oct 2003 00:27:31 +0100 (CET)
Subject: [R] svm from e1071 package
In-Reply-To: <OF9BE158A4.C18CE709-ON85256DCE.007B37C8-85256DCE.007C06C3@EU.novartis.net>
Message-ID: <Pine.LNX.4.21.0310300027140.3203-100000@boromir.ci.tuwien.ac.at>

> This suggests to me that data are scrambled each time - the last time I 
> looked at libsvm python interface
> this is what was done. Is this the same here (I hope) ?

yes.

g.,
David



From d.scott at auckland.ac.nz  Thu Oct 30 00:43:33 2003
From: d.scott at auckland.ac.nz (David Scott)
Date: Thu, 30 Oct 2003 12:43:33 +1300 (NZDT)
Subject: [R] importing SAS data
In-Reply-To: <x2u15rdcv9.fsf@biostat.ku.dk>
Message-ID: <Pine.LNX.4.44.0310301239460.19844-100000@hydra.stat.auckland.ac.nz>

On 29 Oct 2003, Peter Dalgaard wrote:

> 
> as if no other Unix than Sun Solaris existed. Also, the program is in
> the public domain but the source is not included. Does anyone know
> what the author's long-term plans are with this? (It's a GenStat
> complimentary item, so GenStat non-users obviously has little say in
> the matter.)
> 
Well David is a very nice fellow and has his email address on the DataLoad 
page for comments. I am sure he would respond to requests for information.

He hasn't listed rda as a format which he can deal with, but previously 
requested information concerning it:

http://www.r-project.org/nocvs/mail/r-help/2002/8604.html

David Scott


_________________________________________________________________
David Scott	Department of Statistics, Tamaki Campus
		The University of Auckland, PB 92019
		Auckland	NEW ZEALAND
Phone: +64 9 373 7599 ext 86830		Fax: +64 9 373 7000
Email:	d.scott at auckland.ac.nz 


Graduate Officer, Department of Statistics

Webmaster, New Zealand Statistical Association:
        http://www.stat.auckland.ac.nz/nzsa/



From ross at biostat.ucsf.edu  Thu Oct 30 01:46:41 2003
From: ross at biostat.ucsf.edu (Ross Boylan)
Date: Wed, 29 Oct 2003 16:46:41 -0800
Subject: [R] packaging a package addon
Message-ID: <1067474801.20574.195.camel@iron.libaux.ucsf.edu>

I am trying to package some code that is a tweak to the survival
package.  When I asked earlier, the list consensus was that it would be
best to do this as a separate package, dependent on survival.

This is proving a bit tricky.

I have some run-time and compile time concerns.

Run-time, my R code needs R code from survival, and my C code needs C
functions from survival.  Will this all be properly loaded by, for
example, R CMD check, (assuming the proper dependency in DESCRIPTION) so
that it doesn't blow up?

The reason I haven't just tried it is that the compile-time issues.  My
C code depends on survival C code for headers.  I believe that the
typical target system where the package would be installed won't have
these at all.  Is that true?  If so, what's the best way around this? 
Maybe stick the headers in a subdirectory and mess with the build
options to include them?  And hope they don't get out of sync with the
real ones?
-- 
Ross Boylan                                      wk:  (415) 502-4031
530 Parnassus Avenue (Library) rm 115-4          ross at biostat.ucsf.edu
Dept of Epidemiology and Biostatistics           fax: (415) 476-9856
University of California, San Francisco
San Francisco, CA 94143-0840                     hm:  (415) 550-1062



From lenvi10 at yahoo.com  Thu Oct 30 03:31:55 2003
From: lenvi10 at yahoo.com (len vir)
Date: Wed, 29 Oct 2003 18:31:55 -0800 (PST)
Subject: [R] A Question for Statisticians about SCHWARZ, SHIBATA, RICE ...
Message-ID: <20031030023155.2390.qmail@web14805.mail.yahoo.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20031029/ce19dc0d/attachment.pl

From Bill.Venables at csiro.au  Thu Oct 30 02:56:52 2003
From: Bill.Venables at csiro.au (Bill.Venables@csiro.au)
Date: Thu, 30 Oct 2003 11:56:52 +1000
Subject: [R] loglm() uses only a reference to data, and not data itsel
	f - is that on purpose??
Message-ID: <E09E527B56BE2D438A3D6A246DDD27A916617F@roper-cv.qld.cmis.csiro.au>

loglm() is a port of an original written for S-PLUS.  The fact that it
carries only a reference to the data frame is neither intentional nor
unintentional, but an unnoticed side-effect.  I can see advantages both
ways.  (I'm not so sure, either, that what you say is standard behaviour for
model fitting functions really is so universal.)

It would not be too hard to come up with a version that embeds the data
frame in the fitted model object, but if you are dealing with a really huge
data set (the kind that loglm() was designed to handle, after all) do you
really want to be carting around several copies of it around in memory?

That said, do you have a suggested patch that might accommodate this
refinement?

Bill Venables.



-----Original Message-----
From: S?ren H?jsgaard [mailto:Soren.Hojsgaard at agrsci.dk]
Sent: Thursday, October 30, 2003 8:10 AM
To: r-help at stat.math.ethz.ch
Subject: [R] loglm() uses only a reference to data, and not data itself
- is that on purpose??


Dear all,
Consider the following:
 
library(MASS); data(HairEyeColor)
l1 <- loglm(~ 1+2+3, data=HairEyeColor)
myloglm <- function(){
  nd <- HairEyeColor
  v <- loglm(~ 1+2+3, data=nd)
  return(v)
}  
  
l2 <- myloglm()  
  
Now, step(l1) works, whereas step(l2) does not, the problem being that data
nd does not exist outside myloglm. I was under the impression that objects
from functions like glm() and lm() "carried their data" with them, but that
does not seem to be the case for loglm(). Is there a reason for these
different behaviours??
 
Thanks in advance
S?ren H?jsgaard

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://www.stat.math.ethz.ch/mailman/listinfo/r-help



From ross at biostat.ucsf.edu  Thu Oct 30 04:13:11 2003
From: ross at biostat.ucsf.edu (Ross Boylan)
Date: Wed, 29 Oct 2003 19:13:11 -0800
Subject: [R] Re: packaging a package addon
In-Reply-To: <1067474801.20574.195.camel@iron.libaux.ucsf.edu>
References: <1067474801.20574.195.camel@iron.libaux.ucsf.edu>
Message-ID: <1067483590.20574.252.camel@iron.libaux.ucsf.edu>

I have some mixed results to report.  I went ahead and built a package
with the dependency, my changed files, and a few headers.  I left the
headers in the same directory as the C files.

My first attempt passed R CMD check, though there were no examples to
exercise the code.  When I tried to run it, it couldn't find the C
routine.  I had neglected to provide a .First.lib for my new library.

When I corrected that, R cmd check failed with
* checking generic/method consistency ... WARNING
Error in .loadPackageQuietly(package, lib.loc) : 
	Error in library(package, lib.loc = lib.loc, character.only = TRUE,
verbose = FALSE) : 
	.First.lib failed
Execution halted
* checking for assignment functions with final arg not named 'value' ...
WARNING
Error in .loadPackageQuietly(package, lib.loc) : 
	Error in library(package, lib.loc = lib.loc, character.only = TRUE,
verbose = FALSE) : 
	.First.lib failed
Execution halted
* checking Rd files ... OK
* checking for undocumented objects ... ERROR
Error in .loadPackageQuietly(package, lib.loc) : 

Hoping this was only a problem for check because it didn't know how to
load dependencies, I tried running the resulting code in a regular R
session.  My first discovery was that dependent libraries need to be
loaded by hand.  After I did that, when I ran my code I got
> library("survival")
> library("survivalrds", lib.loc="/home/ross/src/survivalrds.Rcheck/")
Error in dyn.load(x, as.logical(local), as.logical(now)) : 
	unable to load shared library
"/home/ross/src/survivalrds.Rcheck/survivalrds/libs/survivalrds.so":
  /home/ross/src/survivalrds.Rcheck/survivalrds/libs/survivalrds.so:
undefined symbol: cholesky2
Error in library("survivalrds", lib.loc =
"/home/ross/src/survivalrds.Rcheck/") : 
	.First.lib failed

cholesky2 is one of the entry points in survival referenced by
survivalrds.  I tried adding an "external" to the declaration of
cholesky2 (I think it's redundant) and redoing everything; it didn't
help.

In short, I'm having some dynamic linkage problems.  Any suggestions? 
(I'm on a linux system with the gcc 3.3 toolchain, but obviously it
would be better to solve this portably).
-- 
Ross Boylan                                      wk:  (415) 502-4031
530 Parnassus Avenue (Library) rm 115-4          ross at biostat.ucsf.edu
Dept of Epidemiology and Biostatistics           fax: (415) 476-9856
University of California, San Francisco
San Francisco, CA 94143-0840                     hm:  (415) 550-1062



From s195404 at student.uq.edu.au  Thu Oct 30 05:27:34 2003
From: s195404 at student.uq.edu.au (Andrew C. Ward)
Date: Thu, 30 Oct 2003 04:27:34 +0000
Subject: [R] Trouble reshaping some data
Message-ID: <1067488054.3fa093368a52c@my.uq.edu.au>

I would appreciate some advice on the following task. I have
some data that currently looks like this:
  t1 <- data.frame(id=c(1,1,2,2), 
aspect=c("A","B","A","B"), score=c(10,9,11,12))

I'd like it to look like this:
  id  A  B
   1 10  9
   2 11 12

reshape() looks like a good candidate for this job but I'm
not really sure about the roles of timevar and idvar for
this dataframe. I tried
  t2 <- reshape(t1, direction="wide", timevar=t1[,3],
                idvar=t1[,2])
which is obviously ignorant and wrong.

Thank you in advance for any comments.


Regards,

Andrew C. Ward

CAPE Centre
Department of Chemical Engineering
The University of Queensland
Brisbane Qld 4072 Australia



From Paul.Sorenson at vision-bio.com  Thu Oct 30 05:28:36 2003
From: Paul.Sorenson at vision-bio.com (Paul Sorenson)
Date: Thu, 30 Oct 2003 15:28:36 +1100
Subject: [R] legend over-prints barplot bar
Message-ID: <5E06BFED29594F4C9C5EBE230DE320C602C2B61D@ewok.vsl.com.au>

When I create a bar plot, the legend is obscuring the rightmost bar.

I haven't found a setting that appears to affect the positioning of the legend - any tips re moving the legend would be most appreciated.

paul sorenson



From rossini at blindglobe.net  Thu Oct 30 05:48:31 2003
From: rossini at blindglobe.net (A.J. Rossini)
Date: Wed, 29 Oct 2003 20:48:31 -0800
Subject: [R] Re: packaging a package addon
In-Reply-To: <1067483590.20574.252.camel@iron.libaux.ucsf.edu> (Ross
	Boylan's message of "Wed, 29 Oct 2003 19:13:11 -0800")
References: <1067474801.20574.195.camel@iron.libaux.ucsf.edu>
	<1067483590.20574.252.camel@iron.libaux.ucsf.edu>
Message-ID: <85ad7je3fk.fsf@blindglobe.net>


You might need to install before checking.  Sometimes that helps.  I
still havn't figured out exactly why...

Ross Boylan <ross at biostat.ucsf.edu> writes:

> I have some mixed results to report.  I went ahead and built a package
> with the dependency, my changed files, and a few headers.  I left the
> headers in the same directory as the C files.
>
> My first attempt passed R CMD check, though there were no examples to
> exercise the code.  When I tried to run it, it couldn't find the C
> routine.  I had neglected to provide a .First.lib for my new library.
>
> When I corrected that, R cmd check failed with
> * checking generic/method consistency ... WARNING
> Error in .loadPackageQuietly(package, lib.loc) : 
> 	Error in library(package, lib.loc = lib.loc, character.only = TRUE,
> verbose = FALSE) : 
> 	.First.lib failed
> Execution halted
> * checking for assignment functions with final arg not named 'value' ...
> WARNING
> Error in .loadPackageQuietly(package, lib.loc) : 
> 	Error in library(package, lib.loc = lib.loc, character.only = TRUE,
> verbose = FALSE) : 
> 	.First.lib failed
> Execution halted
> * checking Rd files ... OK
> * checking for undocumented objects ... ERROR
> Error in .loadPackageQuietly(package, lib.loc) : 
>
> Hoping this was only a problem for check because it didn't know how to
> load dependencies, I tried running the resulting code in a regular R
> session.  My first discovery was that dependent libraries need to be
> loaded by hand.  After I did that, when I ran my code I got
>> library("survival")
>> library("survivalrds", lib.loc="/home/ross/src/survivalrds.Rcheck/")
> Error in dyn.load(x, as.logical(local), as.logical(now)) : 
> 	unable to load shared library
> "/home/ross/src/survivalrds.Rcheck/survivalrds/libs/survivalrds.so":
>   /home/ross/src/survivalrds.Rcheck/survivalrds/libs/survivalrds.so:
> undefined symbol: cholesky2
> Error in library("survivalrds", lib.loc =
> "/home/ross/src/survivalrds.Rcheck/") : 
> 	.First.lib failed
>
> cholesky2 is one of the entry points in survival referenced by
> survivalrds.  I tried adding an "external" to the declaration of
> cholesky2 (I think it's redundant) and redoing everything; it didn't
> help.
>
> In short, I'm having some dynamic linkage problems.  Any suggestions? 
> (I'm on a linux system with the gcc 3.3 toolchain, but obviously it
> would be better to solve this portably).
> -- 
> Ross Boylan                                      wk:  (415) 502-4031
> 530 Parnassus Avenue (Library) rm 115-4          ross at biostat.ucsf.edu
> Dept of Epidemiology and Biostatistics           fax: (415) 476-9856
> University of California, San Francisco
> San Francisco, CA 94143-0840                     hm:  (415) 550-1062
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
>

-- 
rossini at u.washington.edu            http://www.analytics.washington.edu/ 
Biomedical and Health Informatics   University of Washington
Biostatistics, SCHARP/HVTN          Fred Hutchinson Cancer Research Center
UW (Tu/Th/F): 206-616-7630 FAX=206-543-3461 | Voicemail is unreliable
FHCRC  (M/W): 206-667-7025 FAX=206-667-4812 | use Email

CONFIDENTIALITY NOTICE: This e-mail message and any attachme...{{dropped}}



From Simon.Blomberg at anu.edu.au  Thu Oct 30 05:58:58 2003
From: Simon.Blomberg at anu.edu.au (Simon Blomberg)
Date: Thu, 30 Oct 2003 15:58:58 +1100
Subject: [R] Trouble reshaping some data
Message-ID: <7A3A13F416B40842BD2C1753E044B3590122813C@CASEVS02.cas.anu.edu.au>

This is nearly right:

reshape(t1, direction="wide", idvar="id", timevar="aspect")

  id score.A score.B
1  1      10       9
3  2      11      12

Simon.

Simon Blomberg, PhD
Depression & Anxiety Consumer Research Unit
Centre for Mental Health Research
Australian National University
http://www.anu.edu.au/cmhr/
Simon.Blomberg at anu.edu.au  +61 (2) 6125 3379


> -----Original Message-----
> From: Andrew C. Ward [mailto:s195404 at student.uq.edu.au]
> Sent: Thursday, 30 October 2003 3:28 PM
> To: r-help at stat.math.ethz.ch
> Subject: [R] Trouble reshaping some data
> 
> 
> I would appreciate some advice on the following task. I have
> some data that currently looks like this:
>   t1 <- data.frame(id=c(1,1,2,2), 
> aspect=c("A","B","A","B"), score=c(10,9,11,12))
> 
> I'd like it to look like this:
>   id  A  B
>    1 10  9
>    2 11 12
> 
> reshape() looks like a good candidate for this job but I'm
> not really sure about the roles of timevar and idvar for
> this dataframe. I tried
>   t2 <- reshape(t1, direction="wide", timevar=t1[,3],
>                 idvar=t1[,2])
> which is obviously ignorant and wrong.
> 
> Thank you in advance for any comments.
> 
> 
> Regards,
> 
> Andrew C. Ward
> 
> CAPE Centre
> Department of Chemical Engineering
> The University of Queensland
> Brisbane Qld 4072 Australia
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
>



From ripley at stats.ox.ac.uk  Thu Oct 30 06:57:31 2003
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Thu, 30 Oct 2003 05:57:31 +0000 (GMT)
Subject: [R] packaging a package addon
In-Reply-To: <1067474801.20574.195.camel@iron.libaux.ucsf.edu>
Message-ID: <Pine.LNX.4.44.0310300548580.25490-100000@gannet.stats>

On Wed, 29 Oct 2003, Ross Boylan wrote:

> I am trying to package some code that is a tweak to the survival
> package.  When I asked earlier, the list consensus was that it would be
> best to do this as a separate package, dependent on survival.
> 
> This is proving a bit tricky.
> 
> I have some run-time and compile time concerns.
> 
> Run-time, my R code needs R code from survival, and my C code needs C
> functions from survival.  Will this all be properly loaded by, for
> example, R CMD check, (assuming the proper dependency in DESCRIPTION) so
> that it doesn't blow up?

No.  It is the responsibility of your package's .First.lib (etc) to load 
any other packages it requires.  DESCRIPTION is used for installation, if 
at all.

The answer would be a bit different if survival were in a namespace.

> The reason I haven't just tried it is that the compile-time issues.  My
> C code depends on survival C code for headers.  I believe that the
> typical target system where the package would be installed won't have
> these at all.  Is that true?  If so, what's the best way around this? 
> Maybe stick the headers in a subdirectory and mess with the build
> options to include them?  And hope they don't get out of sync with the
> real ones?

I think it is worse than that: there is no guarantee that your DLL will be
able to see entry points in an already loaded DLL, as far as I know. The
args for dyn.load(x, local = TRUE, now = TRUE) suggest that at least on
some systems the entry points in survival.so are not shared (and you would
need to alter survival's R code to change that).  Now, you could try to
link independently of R, but on MacOS X a shared module != a dynamic
library, and on Windows and AIX you would need import libraries ....

I would copy the parts of the survival C code you need into your own 
package.

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From nusbj at hotmail.com  Thu Oct 30 07:13:08 2003
From: nusbj at hotmail.com (Zhen Pang)
Date: Thu, 30 Oct 2003 14:13:08 +0800
Subject: [R] Where is rmutil package?
Message-ID: <Sea2-F65uQuN6tdqVvP0000ca43@hotmail.com>

http://alpha.luc.ac.be/~jlindsey/rcode.html



>From: "Peter Flom" <flom at ndri.org>
>To: <r-help at stat.math.ethz.ch>
>Subject: [R] Where is rmutil package?
>Date: Wed, 29 Oct 2003 10:49:10 -0500
>
>Pursing my earlier question, when I tried loading Lindsey's gnlm, I got
>a
>message
>
>Loading required package: rmutil
>Warning message:
>There is no package called 'rmutil' in: library(package, character.only
>= TRUE, logical = TRUE, warn.conflicts = warn.conflicts,
>
>
>According to the R documentation
>http://finzi.psych.upenn.edu/R/doc/html/packages.html
>
>rmutil is in the standard library.....
>
>If I ignore the message and try fitting a model with one of the
>functions in gnlm (e.g. fmr)  I get an error that it coudn't find the
>function 'finterp'
>
>Any help appreciated
>
>Thanks
>
>Peter
>
>Peter L. Flom, PhD
>Assistant Director, Statistics and Data Analysis Core
>Center for Drug Use and HIV Research
>National Development and Research Institutes
>71 W. 23rd St
>www.peterflom.com
>New York, NY 10010
>(212) 845-4485 (voice)
>(917) 438-0894 (fax)
>
>______________________________________________
>R-help at stat.math.ethz.ch mailing list
>https://www.stat.math.ethz.ch/mailman/listinfo/r-help

_________________________________________________________________
Find gifts, buy online with MSN Shopping. http://shopping.msn.com.sg/



From Friedrich.Leisch at ci.tuwien.ac.at  Thu Oct 30 09:35:32 2003
From: Friedrich.Leisch at ci.tuwien.ac.at (Friedrich.Leisch@ci.tuwien.ac.at)
Date: Thu, 30 Oct 2003 09:35:32 +0100
Subject: [R] R News Volume 3/2
Message-ID: <16288.52564.960646.249967@celeborn.leisch.at>


We have published the 2003/2 issue of R News on

	http://cran.R-project.org/doc/Rnews

where you can download the newsletter as PDF or Postscript file. It
will propagate to the CRAN mirrors within a day or two.

Contents of this issue:
                                                                      
R Help Desk
Integrating grid Graphics Output with Base Graphics Output
A New Package for the General Error Distribution
Web-based Microarray Analysis using Bioconductor
Sweave, Part II: Package Vignettes
R Foundation News
Recent Events
Book Reviews
Changes in R 1.8.0
Changes on CRAN
Crossword Solution
Correction to ``Building Microsoft Windows Versions of R and R
	packages under Intel Linux''


For the editorial board,
Fritz Leisch

-- 
-------------------------------------------------------------------
                        Friedrich  Leisch 
Institut f?r Statistik                     Tel: (+43 1) 58801 10715
Technische Universit?t Wien                Fax: (+43 1) 58801 10798
Wiedner Hauptstra?e 8-10/1071      Friedrich.Leisch at ci.tuwien.ac.at
A-1040 Wien, Austria             http://www.ci.tuwien.ac.at/~leisch

_______________________________________________
R-announce at stat.math.ethz.ch mailing list
https://www.stat.math.ethz.ch/mailman/listinfo/r-announce



From bhx2 at mevik.net  Thu Oct 30 10:35:25 2003
From: bhx2 at mevik.net (=?iso-8859-1?q?Bj=F8rn-Helge_Mevik?=)
Date: Thu, 30 Oct 2003 10:35:25 +0100
Subject: [R] /usr/lib/R/library vs /usr/local/lib/R/site-library
In-Reply-To: <20031029162431.GA21973@sonny.eddelbuettel.com> (Dirk
	Eddelbuettel's message of "Wed, 29 Oct 2003 10:24:31 -0600")
References: <20031029102252.5bb72714.f.harrell@vanderbilt.edu>
	<85smlcjb4b.fsf@blindglobe.net>
	<20031029162431.GA21973@sonny.eddelbuettel.com>
Message-ID: <7o65i7yso2.fsf@foo.nemo-project.org>

Dirk Eddelbuettel <edd at debian.org> writes:

> On Wed, Oct 29, 2003 at 07:51:16AM -0800, A.J. Rossini wrote:
>
>> /usr/lib/R/site-library is for
>> apt-installed R packages (from CRAN, or Jim Lindsey's works), and

> It is not yet fully implemented as not all apt-get'able Debian
> packages of CRAN, Omegahat, ...

Forgive my ignorance; I just switched to Debian.  Are there R packages
(such as `car' or `vegan') that are apt-get'able?  How can I find out
which ones, and how do I set up apt to get them?

(I've added `deb http://cran.r-project.org/bin/linux/debian woody
main' to /etc/apt/sources.list, to get the latest version of R.)

-- 
Bj?rn-Helge Mevik



From ma at ne.su.se  Thu Oct 30 13:27:37 2003
From: ma at ne.su.se (Mahmood ARAI)
Date: Thu, 30 Oct 2003 13:27:37 +0100
Subject: [R] conflicts(detail=TRUE)
Message-ID: <20031030122738.E9EBC37E4F@mbox3.su.se>

Dear all, 

Could somebody please explain what the following conflict is about? 

best
mahmood 

R> conflicts(detail=TRUE) 

$"package:methods"
[1] "body<-" 

$"package:base"
[1] "body<-" 

R> 

PS: I send an earlier mail yesterday on this subject
that was definitly not clear,  therefore I try again. 

 

http://www.ne.su.se/~ma



From ripley at stats.ox.ac.uk  Thu Oct 30 13:51:03 2003
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Thu, 30 Oct 2003 12:51:03 +0000 (GMT)
Subject: [R] conflicts(detail=TRUE)
In-Reply-To: <20031030122738.E9EBC37E4F@mbox3.su.se>
Message-ID: <Pine.LNX.4.44.0310301249480.26977-100000@gannet.stats>

What is there to explain?  body<- is defined in two places, and if methods 
is attached, its version takes precedence (and allows S4 methods to be 
set).  End of story.

On Thu, 30 Oct 2003, Mahmood ARAI wrote:

> Dear all, 
> 
> Could somebody please explain what the following conflict is about? 
> 
> best
> mahmood 
> 
> R> conflicts(detail=TRUE) 
> 
> $"package:methods"
> [1] "body<-" 
> 
> $"package:base"
> [1] "body<-" 
> 
> R> 
> 
> PS: I send an earlier mail yesterday on this subject
> that was definitly not clear,  therefore I try again. 

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From veronica at icmc.usp.br  Thu Oct 30 14:21:58 2003
From: veronica at icmc.usp.br (veronica@icmc.usp.br)
Date: Thu, 30 Oct 2003 11:21:58 -0200
Subject: [R] Linking R with C or WEB
Message-ID: <1067520118.3fa110766b813@xapacura.icmc.usp.br>



Hi. I would like to know if it is possible to link R with C or with WEB. If it 
is possible I would to like to know how and what I have to do. There is a 
documentation about this?

Tanks for help,

Veronica.



From duncan at research.bell-labs.com  Thu Oct 30 14:46:06 2003
From: duncan at research.bell-labs.com (Duncan Temple Lang)
Date: Thu, 30 Oct 2003 08:46:06 -0500
Subject: [R] Re: packaging a package addon
In-Reply-To: <1067483590.20574.252.camel@iron.libaux.ucsf.edu>;
	from ross@biostat.ucsf.edu on Wed, Oct 29, 2003 at 07:13:11PM
	-0800
References: <1067474801.20574.195.camel@iron.libaux.ucsf.edu>
	<1067483590.20574.252.camel@iron.libaux.ucsf.edu>
Message-ID: <20031030084606.A17635@jessie.research.bell-labs.com>

Ross Boylan wrote:
> 
> Hoping this was only a problem for check because it didn't know how to
> load dependencies, I tried running the resulting code in a regular R
> session.  My first discovery was that dependent libraries need to be
> loaded by hand.  After I did that, when I ran my code I got
> > library("survival")
> > library("survivalrds", lib.loc="/home/ross/src/survivalrds.Rcheck/")
> Error in dyn.load(x, as.logical(local), as.logical(now)) : 
> 	unable to load shared library
> "/home/ross/src/survivalrds.Rcheck/survivalrds/libs/survivalrds.so":
>   /home/ross/src/survivalrds.Rcheck/survivalrds/libs/survivalrds.so:
> undefined symbol: cholesky2
> Error in library("survivalrds", lib.loc =
> "/home/ross/src/survivalrds.Rcheck/") : 
> 	.First.lib failed
> 
> cholesky2 is one of the entry points in survival referenced by
> survivalrds.  I tried adding an "external" to the declaration of
> cholesky2 (I think it's redundant) and redoing everything; it didn't
> help.
> 
> In short, I'm having some dynamic linkage problems.  Any suggestions? 
> (I'm on a linux system with the gcc 3.3 toolchain, but obviously it
> would be better to solve this portably).

This is a situation that will arise more frequently.  The current
package model is quite simple and only supports "independent" packages
at the C level.  

About a year ago, I did some work which would allow packages to have
dynamically loadable libraries and also have linkable libraries so
that other packages could link against them.  The idea is that one can
specify the different files in the DESCRIPTION file of the package and
the R tools would take care of building the different pieces.  Include
files that were to be made available to other packages were put in the
installed package and could be found using simple tools.  Similarly,
special compilation flags would be available to other packages.  These
other packages could then declare this package as a dependency
and get the necessary linking.

For non-technical reasons, I didn't get to commit this to the R
source.  However, I do use a hand-crafted version in the RGtk and
releated packages which share common code.  And I hope to get back to
this and put the code in place in R as we extended the current package
mechanism.   

Copying the code from survival is one short-term approach, but it is
very undesirable in the medium to long run.  Using the function
dyn.load()'s local = FALSE argument on operating systems that suppport
it, when loading survival will make its symbols visible to the C code
in your package. (There are other caveats too).

And the one robust method for doing this now is to use function
pointers.  It is possible in R to lookup the cholesky2 symbol in
survival and to pass it to C code as a function pointer that you can
use directly in your code.  This is a powerful approach in general for
parameterizing C routines with other routines or S functions.  If you
want more details/examples, let me know.  I am putting together some
tools/package to make this easier and illustrates its use.

 
 D.



> -- 
> Ross Boylan                                      wk:  (415) 502-4031
> 530 Parnassus Avenue (Library) rm 115-4          ross at biostat.ucsf.edu
> Dept of Epidemiology and Biostatistics           fax: (415) 476-9856
> University of California, San Francisco
> San Francisco, CA 94143-0840                     hm:  (415) 550-1062
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help

-- 
_______________________________________________________________

Duncan Temple Lang                duncan at research.bell-labs.com
Bell Labs, Lucent Technologies    office: (908)582-3217
700 Mountain Avenue, Room 2C-259  fax:    (908)582-3340
Murray Hill, NJ  07974-2070       
         http://cm.bell-labs.com/stat/duncan



From MSchwartz at medanalytics.com  Thu Oct 30 15:07:57 2003
From: MSchwartz at medanalytics.com (Marc Schwartz)
Date: Thu, 30 Oct 2003 08:07:57 -0600
Subject: [R] legend over-prints barplot bar
In-Reply-To: <5E06BFED29594F4C9C5EBE230DE320C602C2B61D@ewok.vsl.com.au>
References: <5E06BFED29594F4C9C5EBE230DE320C602C2B61D@ewok.vsl.com.au>
Message-ID: <1067522877.27267.24.camel@localhost.localdomain>

On Wed, 2003-10-29 at 22:28, Paul Sorenson wrote:
> When I create a bar plot, the legend is obscuring the rightmost bar.
> 
> I haven't found a setting that appears to affect the positioning of
> the legend - any tips re moving the legend would be most appreciated.
> 
> paul sorenson


Conceptually, barplot() sets the default axes and legend position based
upon the data that you are using for 'height'. These defaults may not be
appropriate in all cases, as you are seeing.

There are a couple of approaches that you can take:

1. Explicitly locate the legend by using legend() instead of the default
used by barplot(). Don't set 'legend.text' in barplot() in this case.
See ?legend for more details.

2. You can adjust the range of the y axis (if you have vertical bars) or
the x axis (if you have horizontal bars) by using 'ylim' or 'xlim' in
barplot(), respectively. Thus you could do something like:

barplot(height, ..., ylim = c(0, max(height) * 1.3))

which will increase the maximum value of the y axis by 30%, leaving room
for the legend in the upper portion of the plot area.

HTH,

Marc Schwartz



From jmc at research.bell-labs.com  Thu Oct 30 16:22:26 2003
From: jmc at research.bell-labs.com (John Chambers)
Date: Thu, 30 Oct 2003 10:22:26 -0500
Subject: [R] conflicts(detail=TRUE)
References: <20031030122738.E9EBC37E4F@mbox3.su.se>
Message-ID: <3FA12CB2.2244417B@research.bell-labs.com>

Mahmood ARAI wrote:
> 
> Dear all,
> 
> Could somebody please explain what the following conflict is about?
> 
> best
> mahmood
> 
> R> conflicts(detail=TRUE)
> 
> $"package:methods"
> [1] "body<-"
> 
> $"package:base"
> [1] "body<-"
> 

The version on the methods package is a generic function:  There are
some special methods defined in the package for assigning the body of
special classes of functions.  The version on the base package is not a
generic.

It's possible that in the future conflicts() might not report a generic
vs a non-generic version of the same function, if they were compatible.

John Chambers.

> R>
> 
> PS: I send an earlier mail yesterday on this subject
> that was definitly not clear,  therefore I try again.
> 
> 
> 
> http://www.ne.su.se/~ma
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help



From edd at debian.org  Thu Oct 30 15:41:07 2003
From: edd at debian.org (Dirk Eddelbuettel)
Date: Thu, 30 Oct 2003 08:41:07 -0600
Subject: [R] /usr/lib/R/library vs /usr/local/lib/R/site-library
In-Reply-To: <7o65i7yso2.fsf@foo.nemo-project.org>
References: <20031029102252.5bb72714.f.harrell@vanderbilt.edu>
	<85smlcjb4b.fsf@blindglobe.net>
	<20031029162431.GA21973@sonny.eddelbuettel.com>
	<7o65i7yso2.fsf@foo.nemo-project.org>
Message-ID: <20031030144107.GA8031@sonny.eddelbuettel.com>

On Thu, Oct 30, 2003 at 10:35:25AM +0100, Bj?rn-Helge Mevik wrote:
> Dirk Eddelbuettel <edd at debian.org> writes:
> 
> > On Wed, Oct 29, 2003 at 07:51:16AM -0800, A.J. Rossini wrote:
> >
> >> /usr/lib/R/site-library is for
> >> apt-installed R packages (from CRAN, or Jim Lindsey's works), and
> 
> > It is not yet fully implemented as not all apt-get'able Debian
> > packages of CRAN, Omegahat, ...
> 
> Forgive my ignorance; I just switched to Debian.  Are there R packages
> (such as `car' or `vegan') that are apt-get'able?  How can I find out
> which ones, and how do I set up apt to get them?

Yes and no: car is, vegan isn't; a few more are available [1] The choice of
available package reflects the preferences of the (by now four) different
maintainers.

> (I've added `deb http://cran.r-project.org/bin/linux/debian woody
> main' to /etc/apt/sources.list, to get the latest version of R.)

But you won't be able to use 'woody' aka Debian stable. Most packages are in
testing. You should also be able to mix stable and testing, email me off
list if you're curious.
 
Hth, Dirk

[1] its is only on my computer, the others should be on any Debian mirror.

edd at homebud:~> (apt-cache search r-cran; apt-cache search r-noncran; apt-cache search r-omegahat) | sort | uniq
r-cran-car - GNU R Companion to Applied Regression by John Fox
r-cran-coda - Output analysis and diagnostics for MCMC simulations in R
r-cran-design - GNU R regression modeling strategies tools by Frank Harrell
r-cran-gtkdevice - GNU R Gtk device driver package
r-cran-hmisc - GNU R miscellaneous functions by Frank Harrell
r-cran-its - GNU R package for handling irregular time series
r-cran-mcmcpack - routines for Markov Chain Monte Carlo model estimation in R
r-cran-qtl - [Biology] GNU R package for genetic marker linkage analysis
r-cran-rcmdr - GNU R platform-independent basic-statistics GUI
r-cran-rodbc - GNU R package for ODBC database access
r-cran-rquantlib - GNU R package interfacing the QuantLib finance library
r-cran-statdataml - XML based data exchange format (R library)
r-cran-tkrplot - GNU R embedded Tk plotting device package
r-cran-tseries - GNU R package for time-series analysis and comp. finance
r-cran-xml - An XML package for the R language
r-noncran-lindsey - GNU R libraries contributed by Jim and Patrick Lindsey
r-omegahat-ggobi - GNU R package for the GGobi data visualization system
r-omegahat-rgtk - GNU R binding for Gtk


-- 
Those are my principles, and if you don't like them... well, I have others.
                                                -- Groucho Marx



From Timur.Elzhov at jinr.ru  Thu Oct 30 15:54:51 2003
From: Timur.Elzhov at jinr.ru (Timur Elzhov)
Date: Thu, 30 Oct 2003 17:54:51 +0300
Subject: [R] 'nls' and its arguments
Message-ID: <20031030145451.GA11261@nf034.jinr.ru>


Dear R experts!

I'd to fit data by 'nls' with me-supplied function 'fcn'.

1) I'd like 'fcn' to accept arbitrary arguments, i.e. I defined it
   as f(...) {<body>}. (Ok, that's not actually impotant).
2) Second, I would NOT like to supply every parameter in the formula.
   To illustrate this, let's look at the last example of 'nls' help
   page:

    ## weighted nonlinear regression
    data(Puromycin)
    Treated <- Puromycin[Puromycin$state == "treated", ]
    weighted.MM <- function(resp, conc, Vm, K)
    {
        ## Purpose: exactly as white book p.451 -- RHS for nls()
        ##  Weighted version of Michaelis-Menten model
        ## ---------------------------------------------------------------
        ## Arguments: `y', `x' and the two parameters (see book)
        ## ---------------------------------------------------------------
        ## Author: Martin Maechler, Date: 23 Mar 2001, 18:48
        print(resp)
    
        pred <- (Vm * conc)/(K + conc)
        (resp - pred) / sqrt(pred)
    }
    Pur.wt <- nls( ~ weighted.MM(rate, conc, Vm, K), data = Treated,
                  start = list(Vm = 200, K = 0.1))

So, in this example I wouldn't like to write `weighted.MM(rate, conc, Vm, K'),
and `start = list(Vm = 200, K = 0.1)', instead I'd like to supply _lists_.
With the 'start' parameter it's easy - I create list

    p.start <- list(Vm = 200, K = 0.1)

and assign it to 'start' in nls():

    start = p.start

- that works. But, with the formula it's not so simple. Well, I tried
at first to make formula more "list-like":

    Pur.wt <- nls( ~ do.call("weighted.MM", list(rate, conc, Vm, K)),
                  data = Treated,
                  start = p.start)

- that works too. Now, let's try to separate data and parameters:

    Pur.wt <- nls( ~ do.call("weighted.MM", c(list(rate, conc), list(Vm, K))),
                  data = Treated,
                  start = p.start)

- that's right. So, I have a data 'Treated', and a start params list
'p.start'. Now, here is the _point_: I want nls to read names of
variables from the lists and supply it to function in the formula.
In this example 'weighted.MM' has a certain arg list, but my function is
'fcn(...)', and I want supply _all_ these names to the fcn. I tried to
change 'list(Vm, K)' to list(as.name("Vm"), as.name("K")):

    Pur.wt <- nls( ~ do.call("weighted.MM", c(list(rate, conc), list(as.name("Vm"), as.name("K")))),
                  data  = Treated, start = p.start)

- works again! Now,
    p.arg  <- list(as.name("Vm"), as.name("K"))
    Pur.wt <- nls( ~ do.call("weighted.MM", c(list(rate, conc), p.arg)),
                             data  = Treated, start = p.start)
    Error in model.frame(formula, rownames, variables, varnames, extras, extranames,  : 
            invalid variable type

How can I fix this, and supply _lists_ with parameter names (and data names)
to fcn(...) in nls?
Thanks a lot! :-)


--
WBR,
Timur.



From Soren.Hojsgaard at agrsci.dk  Thu Oct 30 15:58:12 2003
From: Soren.Hojsgaard at agrsci.dk (=?iso-8859-1?Q?S=F8ren_H=F8jsgaard?=)
Date: Thu, 30 Oct 2003 15:58:12 +0100
Subject: [R] Difference between CRAN.packages() and whats on CRAN package
	source
Message-ID: <C83C5E3DEEE97E498B74729A33F6EAEC6BFB16@DJFPOST01.djf.agrsci.dk>

Dear all,
Some packages which are on CRAN package source do not appear in CRAN.packages() and hence not in the drop-down menu (windows). What is the reason for that?
Best regards
S?ren H?jsgaard



From ligges at statistik.uni-dortmund.de  Thu Oct 30 16:17:43 2003
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Thu, 30 Oct 2003 16:17:43 +0100
Subject: [R] Difference between CRAN.packages() and whats on CRAN package
	source
In-Reply-To: <C83C5E3DEEE97E498B74729A33F6EAEC6BFB16@DJFPOST01.djf.agrsci.dk>
References: <C83C5E3DEEE97E498B74729A33F6EAEC6BFB16@DJFPOST01.djf.agrsci.dk>
Message-ID: <3FA12B97.6070603@statistik.uni-dortmund.de>

S?ren H?jsgaard wrote:
> Dear all,
> Some packages which are on CRAN package source do not appear in CRAN.packages() and hence not in the drop-down menu (windows). What is the reason for that?

That's mentioned in
CRAN/bin/windows/contrib/1.8/ReadMe

Uwe Ligges


> Best regards
> S?ren H?jsgaard
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help



From tlumley at u.washington.edu  Thu Oct 30 16:52:01 2003
From: tlumley at u.washington.edu (Thomas Lumley)
Date: Thu, 30 Oct 2003 07:52:01 -0800 (PST)
Subject: [R] Trouble reshaping some data
In-Reply-To: <7A3A13F416B40842BD2C1753E044B3590122813C@CASEVS02.cas.anu.edu.au>
References: <7A3A13F416B40842BD2C1753E044B3590122813C@CASEVS02.cas.anu.edu.au>
Message-ID: <Pine.A41.4.58.0310300751220.147926@homer04.u.washington.edu>

On Thu, 30 Oct 2003, Simon Blomberg wrote:

> This is nearly right:
>
> reshape(t1, direction="wide", idvar="id", timevar="aspect")
>
>   id score.A score.B
> 1  1      10       9
> 3  2      11      12
>

The final refinement is
> reshape(t1,direction="wide",idvar="id",
timevar="aspect",varying=list(c("A","B")))
  id  A  B
1  1 10  9
3  2 11 12

	-thomas



> Simon.
>
> Simon Blomberg, PhD
> Depression & Anxiety Consumer Research Unit
> Centre for Mental Health Research
> Australian National University
> http://www.anu.edu.au/cmhr/
> Simon.Blomberg at anu.edu.au  +61 (2) 6125 3379
>
>
> > -----Original Message-----
> > From: Andrew C. Ward [mailto:s195404 at student.uq.edu.au]
> > Sent: Thursday, 30 October 2003 3:28 PM
> > To: r-help at stat.math.ethz.ch
> > Subject: [R] Trouble reshaping some data
> >
> >
> > I would appreciate some advice on the following task. I have
> > some data that currently looks like this:
> >   t1 <- data.frame(id=c(1,1,2,2),
> > aspect=c("A","B","A","B"), score=c(10,9,11,12))
> >
> > I'd like it to look like this:
> >   id  A  B
> >    1 10  9
> >    2 11 12
> >
> > reshape() looks like a good candidate for this job but I'm
> > not really sure about the roles of timevar and idvar for
> > this dataframe. I tried
> >   t2 <- reshape(t1, direction="wide", timevar=t1[,3],
> >                 idvar=t1[,2])
> > which is obviously ignorant and wrong.
> >
> > Thank you in advance for any comments.
> >
> >
> > Regards,
> >
> > Andrew C. Ward
> >
> > CAPE Centre
> > Department of Chemical Engineering
> > The University of Queensland
> > Brisbane Qld 4072 Australia
> >
> > ______________________________________________
> > R-help at stat.math.ethz.ch mailing list
> > https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> >
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
>

Thomas Lumley			Assoc. Professor, Biostatistics
tlumley at u.washington.edu	University of Washington, Seattle



From spencer.graves at pdf.com  Thu Oct 30 17:26:13 2003
From: spencer.graves at pdf.com (Spencer Graves)
Date: Thu, 30 Oct 2003 08:26:13 -0800
Subject: [R] 'nls' and its arguments
In-Reply-To: <20031030145451.GA11261@nf034.jinr.ru>
References: <20031030145451.GA11261@nf034.jinr.ru>
Message-ID: <3FA13BA5.20405@pdf.com>

      Have you considered using "optim" to minimize a function that 
computes the sum of squares of residuals (SSR) from your model of 
interest?  Contour plots of SSR over variations in any two parameters 
provide simultaneous confidence regions using the appropriate percentage 
points of the F distribution.  If my memory is correct, I believe you 
can find this discussed in Bates and Watts (1988) Nonlinear Regression 
Analysis and Its Applications (Wiley).  Setting "hessian=TRUE" gives you 
double the observed information matrix at the optima, which can be used 
to obtain confidence intervals using the less-accurate normal 
approximation for the distribution of parameter estimates.  If your 
function in poorly parameterized or you have poor starting values, 
"optim" will usually give you something;  "nls" will more often refuse 
to produce an answer.  This is either a benefit or a deficiency, 
depending on your point of view. 

      hope this helps. 
      spencer graves

Timur Elzhov wrote:

>Dear R experts!
>
>I'd to fit data by 'nls' with me-supplied function 'fcn'.
>
>1) I'd like 'fcn' to accept arbitrary arguments, i.e. I defined it
>   as f(...) {<body>}. (Ok, that's not actually impotant).
>2) Second, I would NOT like to supply every parameter in the formula.
>   To illustrate this, let's look at the last example of 'nls' help
>   page:
>
>    ## weighted nonlinear regression
>    data(Puromycin)
>    Treated <- Puromycin[Puromycin$state == "treated", ]
>    weighted.MM <- function(resp, conc, Vm, K)
>    {
>        ## Purpose: exactly as white book p.451 -- RHS for nls()
>        ##  Weighted version of Michaelis-Menten model
>        ## ---------------------------------------------------------------
>        ## Arguments: `y', `x' and the two parameters (see book)
>        ## ---------------------------------------------------------------
>        ## Author: Martin Maechler, Date: 23 Mar 2001, 18:48
>        print(resp)
>    
>        pred <- (Vm * conc)/(K + conc)
>        (resp - pred) / sqrt(pred)
>    }
>    Pur.wt <- nls( ~ weighted.MM(rate, conc, Vm, K), data = Treated,
>                  start = list(Vm = 200, K = 0.1))
>
>So, in this example I wouldn't like to write `weighted.MM(rate, conc, Vm, K'),
>and `start = list(Vm = 200, K = 0.1)', instead I'd like to supply _lists_.
>With the 'start' parameter it's easy - I create list
>
>    p.start <- list(Vm = 200, K = 0.1)
>
>and assign it to 'start' in nls():
>
>    start = p.start
>
>- that works. But, with the formula it's not so simple. Well, I tried
>at first to make formula more "list-like":
>
>    Pur.wt <- nls( ~ do.call("weighted.MM", list(rate, conc, Vm, K)),
>                  data = Treated,
>                  start = p.start)
>
>- that works too. Now, let's try to separate data and parameters:
>
>    Pur.wt <- nls( ~ do.call("weighted.MM", c(list(rate, conc), list(Vm, K))),
>                  data = Treated,
>                  start = p.start)
>
>- that's right. So, I have a data 'Treated', and a start params list
>'p.start'. Now, here is the _point_: I want nls to read names of
>variables from the lists and supply it to function in the formula.
>In this example 'weighted.MM' has a certain arg list, but my function is
>'fcn(...)', and I want supply _all_ these names to the fcn. I tried to
>change 'list(Vm, K)' to list(as.name("Vm"), as.name("K")):
>
>    Pur.wt <- nls( ~ do.call("weighted.MM", c(list(rate, conc), list(as.name("Vm"), as.name("K")))),
>                  data  = Treated, start = p.start)
>
>- works again! Now,
>    p.arg  <- list(as.name("Vm"), as.name("K"))
>    Pur.wt <- nls( ~ do.call("weighted.MM", c(list(rate, conc), p.arg)),
>                             data  = Treated, start = p.start)
>    Error in model.frame(formula, rownames, variables, varnames, extras, extranames,  : 
>            invalid variable type
>
>How can I fix this, and supply _lists_ with parameter names (and data names)
>to fcn(...) in nls?
>Thanks a lot! :-)
>
>
>--
>WBR,
>Timur.
>
>______________________________________________
>R-help at stat.math.ethz.ch mailing list
>https://www.stat.math.ethz.ch/mailman/listinfo/r-help
>  
>



From xavier.fim at eresmas.net  Thu Oct 30 17:54:50 2003
From: xavier.fim at eresmas.net (Xavier =?iso-8859-1?q?Fern=E1ndez=20i=20Mar=EDn?=)
Date: Thu, 30 Oct 2003 17:54:50 +0100
Subject: [R] RMySQL and '_' character in column names
Message-ID: <200310301754.50140.xavier.fim@eresmas.net>

Hi,

I'm using RMySQL in order to obtain data from my MySQL server. In my databases 
sometimes I have columns with names that contain '_' character (ex: 
'gdp_capita', 'population_total', etc...). When these names appear as the 
names of the vectors in the data frame that I get, sometimes I have problems 
as:

> cor(gdp_capita, population_total)
Error: object "_capita" not found
use of _ is soon to be removed.

Is there an automatic way to transform the '_' characters in MySQL to '.' in R 
using RMysql?

Thanks,

Xavier



From rossini at blindglobe.net  Thu Oct 30 01:49:21 2003
From: rossini at blindglobe.net (A.J. Rossini)
Date: Wed, 29 Oct 2003 16:49:21 -0800
Subject: [R] [R-pkgs] Release of Bioconductor 1.3
Message-ID: <85fzhbzh0u.fsf@blindglobe.net>


The Bioconductor core group would like to announce the 1.3 release of
the Bioconductor software.  There are many new packages as well as
several major upgrades and fixes in older packages, and users are
encouraged to check them out.  Release 1.3 is intended to be operated
with R version 1.8.X, which can be obtained at CRAN
(http://cran.r-project.org/) 


-- WHAT FEATURES DOES THIS RELEASE PROVIDE?

All packages from the 1.2 release are included.  All current bug fixes
have been applied, and most have upgraded and provide enhanced
functionality.

-- NEW PACKAGES AND MAJOR UPGRADES FOR RELEASE 1.3 --

The following is an overview of the most important changes, additions,
and upgrades:

--- affy

   There have been many improvements to the affy package.  There were
   big speed and memory improvement of ReadAffy, read.affybatch,
   justRMA.  A mas5calls method was added to get Affymetrix's P/M/A
   calls.  Cel and Cdf classes are no longer supported. Function,
   read.celfile and other Cel related methods and functions
   removed. Most Cdf related functions have moved to the makecdfenv
   package.  Function read.probematrix added. It reads CEL files and
   returns a matrix of PM, MM, or both. This function is more memory
   efficient than read.affybatch.  Also, affy no longer depends on the
   affydata package. For this reason some examples have been moved
   from affy vignettes to the affydata vignette.  The previously
   deprecated express function has been completely removed.  Lastly,
   most normalization routines for AffyBatches can now be called with
   the parameter type which specifies whether the normalization should
   be applied as a PM-only, MM-only, both PM and MM together or PM and
   MM separately.

--- affycomp:

   New assessment was added: assessSpikeIn2. Examples of new feature:
Local 
   slopes are computes and ROC curves divided by overall expression.
   Also, all functions that work for hgu95a spike in now also work for
hgu133a
   spike in experiment

--- annaffy:

   Functions for handling data from Bioconductor Affymetrix annotation
   data packages. Produces compact HTML and text reports including
   experimental data and URL links to many online databases. Allows
   searching biological metadata using various criteria.

--- Biobase:

   The generic function as.data.frame() now works on objects of class
   exprSet.  This lets one access the extensive modeling facilities
   through formula offered by R and packages like 'nlm'.

--- factDesign:

   A package containing functions useful for analyzing data from
   factorial designed microarray experiments.

--- gpls:

   Classification using partial least squares (PLS), a popular
   dimension reduction tool in chemometrics, in the context of
   generalized linear regression based on a previous approach,
   Iteratively ReWeighted Partial Least Squares (IRWPLS) by Marx
   (1996).  Both two-group and multi-group classifications are
   incorporated. Firth's bias reduction procedure is also incorporated
   to ensure more stable and finite regression coefficients.

--- graph, Rgraphviz, and RBGL:

   Graph/Network handling is greatly improved. There is tighter
   integration between graph, RBGL and Rgraphviz. Colors and SubGraph
   layouts are now supported in Rgraphviz. RBGL support includes
   minimal spanning trees, both strong and weakly connected
   components, and Dijkstra's shortest path algorithm.  In graph,
   enhancements in validity checking and representation have been
   made. New classes have been established that will allow for general
   representations of nodes and edges. Capabilities include adding or
   deleting both nodes and edges, combining sets of nodes into a
   single node. Unions, intersections and complements of graphs
   defined on a common set of nodes. Joining of two graphs into a
   single graph. Functions to compute indegree and outdegree. 

--- limma:

   Substantial updates including support for more image analysis
   programs, new background correction methods, single channel
   normalization, support for import of exprSet and marrayNorm data
   objects, improved support for design and contrast matrices, new
   fitted model object class, within-gene multiple testing, Venn
   diagrams and generally a move to a simpler command style at the
   user level.

--- matchprobes:

   A new package providing tools for working with probe sequence
   information: calculate ATCG content, reverse, complement, mismatch
   sequences; fast exact sequence matching; combine AffyBatches of
   different chip types; create probe data packages.  

--- Measurement.cor:

   A package to fit a two-level measurement error model for estimation
   of correlation coefficient between two random variables assuming
   bivariate normality for both the true value and measurement
   error. This model allows dependence between measurement errors
   hence is more flexible.

--- ontoTools:

   Software for working with ontologies (structured vocabularies) and
   their associations with general data resources. 

--- Rdbi, RdbiPgSQL:

   Generic framework for database access in R. Adapted from Rdbi
   authored by Timothy H. Keitt, with methods for accessing data
   stored in PostgreSQL tables (Adapted from Rdbi.PgSQL, authored
   by Timothy H. Keitt)

--- SNPtools:

   Currently an interface to the SNPper data resource maintained at
   Childrens' Boston, which curates SNP-related data from a variety of
   sources.

--- splicegear:

   A new package that works with oligonucleotide microarrays designed
   to monitor or confirm the existence of splice variants.  It can
   extract simple splice-variant information in XML format.  This is
   demonstrated by the connectivity to an existing database of
   putative splice variants.  The package can also integrate easily
   with the results obtained from the package 'matchprobes'. 

--- vsn:

   Two new functions were added 'sagmbSimulateData' and 'sagmbAssess'
   to quantitatively verify finite sample properties and outlier
   robustness of the parameter estimation in vsn(). 

-- TOOLS:

The released packages include tools which facilitate:

* annotation (AnnBuilder, annotate).

* data management and organization through the use of the S4 class
  structure (Biobase, marrayClasses, limma).   

* identification of differentially expressed genes and clustering 
  (edd, genefilter, geneplotter, multtest, ROC, limma)

* analysis of Affymetrix expression array data (affy, affycomp,
  gcrma)

* diagnostic plots and normalization for cDNA array data (marrayInput,
  marrayNorm, marrayPlots)

* storage and retrieval of large datasets (rhdf5, externalVector,
   Rdbi, RdbiPgSQL).

* facilitate user interaction (tkWidgets, widgetTools)

There are currently a total of 48 packages, not including precomputed
annotation data packages for Affymetrix GeneChips(tm), KEGG, GO,
and LocusLink mappings.


-- HELP AND RESOURCES:

The packages and more details may be found on the Bioconductor WWW
site: 

      http://www.bioconductor.org/

Information on subscribing to the mailing list and viewing its
archives
can be found at:

      http://www.stat.math.ethz.ch/mailman/listinfo/bioconductor

Please use that list to discuss Bioconductor specific issues, bugs,
and problems.  Note that every package has a vignette (a literate
program which provides an annotated example of the package's use) as
well as possibly some "HOWTO"s.  These document the tool's usage, and
are provided in the "doc" subdirectory of each package library.


-- WHO:

For the Bioconductor development team:

Douglas Bates, University of Wisconsin, USA.
Ben Bolstad, Division of Biostatistics, UC Berkeley, USA.
Vince Carey, Harvard Medical School, USA.
Marcel Dettling, Federal Inst. Technology, Switzerland.
Sandrine Dudoit, Division of Biostatistics, UC Berkeley, USA.
Byron Ellis, Harvard Department of Statistics, USA.
Laurent Gautier, Technial University of Denmark, Denmark.
Robert Gentleman, Harvard Medical School, USA.
Jeff Gentry, Dana-Farber Cancer Institute, USA.
Kurt Hornik, Technische Universitat Wien, Austria.
Torsten Hothorn, Institut fuer Medizininformatik,
        Biometrie und Epidemiologie, Germany.
Wolfgang Huber, DKFZ Heidelberg, Molecular Genome Analysis, Germany.
Stefano Iacus, Italy
Rafael Irizarry, Department of Biostatistics (JHU), USA.
Friedrich Leisch, Technische Universitat Wien, Austria.
Martin Maechler, Federal Inst. Technology, Switzerland.
Colin Smith, Scripps Research Institute, USA.
Gordon Smyth, Walter and Eliza Hall Institute, Australia.
Anthony Rossini, University of Washington
        and the Fred Hutchinson Cancer Research Center, USA.
Gunther Sawitzki, Institute fur Angewandte Mathematik, Germany.
Luke Tierney, University of Iowa, USA.
Jean Yee Hwa Yang, University of California, San Francisco, USA.
Jianhua (John) Zhang, Dana-Farber Cancer Institute, USA.


-- 
rossini at u.washington.edu            http://www.analytics.washington.edu/ 
Biomedical and Health Informatics   University of Washington
Biostatistics, SCHARP/HVTN          Fred Hutchinson Cancer Research Center
UW (Tu/Th/F): 206-616-7630 FAX=206-543-3461 | Voicemail is unreliable
FHCRC  (M/W): 206-667-7025 FAX=206-667-4812 | use Email

CONFIDENTIALITY NOTICE: This e-mail message and any attachme...{{dropped}}

_______________________________________________
R-packages mailing list
R-packages at stat.math.ethz.ch
https://www.stat.math.ethz.ch/mailman/listinfo/r-packages



From umeno at students.uiuc.edu  Thu Oct 30 18:16:47 2003
From: umeno at students.uiuc.edu (umeno)
Date: Thu, 30 Oct 2003 11:16:47 -0600
Subject: [R] How to Extract Std.Error
Message-ID: <3FB8AEB8@webmail.uiuc.edu>

Hi,

I would like to know if anyone has any idea of how to extract Std.Error from 
the regression output.  I know how to extract coefficients, but I could not 
figure out how to retrieve std. error...

Thank you for your help.
soyoko

______________________________________
Ms. Soyoko Umeno
Graduate Research Assitant for the Illinois-Missouri Biotechnology Alliance (IMBA) at http://www.imba.missouri.edu/
Ph.D. Student at the Department of Agricultural and Consumer Economics
at the University of Illinois at Urbana-Champaign
Office Phone: 217-333-3417 or 217-333-0364
Fax: 217-244-4817
Mailing Address: 1301 W. Gregory Dr. MC710, Urbana, IL 61801



From petzoldt at rcs.urz.tu-dresden.de  Thu Oct 30 18:38:07 2003
From: petzoldt at rcs.urz.tu-dresden.de (Thomas Petzoldt)
Date: Thu, 30 Oct 2003 18:38:07 +0100
Subject: [R] How to Extract Std.Error
In-Reply-To: <3FB8AEB8@webmail.uiuc.edu>
References: <3FB8AEB8@webmail.uiuc.edu>
Message-ID: <3FA14C7F.6020808@rcs.urz.tu-dresden.de>

umeno wrote:

> I would like to know if anyone has any idea of how to extract Std.Error from 
> the regression output.  I know how to extract coefficients, but I could not 
> figure out how to retrieve std. error...

Given a model like this

model <- lm(y~x)

first analyse the data structure of the summary() with

str(summary(model))

looking into this, the following (or something similar for the other
values) should work:

summary(reg)$coefficients[,2]


Thomas P.

-- 
Thomas Petzoldt
Dresden University of Technology
Institute of Hydrobiology          petzoldt at rcs.urz.tu-dresden.de
01062 Dresden                      http://www.tu-dresden.de/fghhihb/



From monica.palaseanu-lovejoy at stud.man.ac.uk  Thu Oct 30 18:51:15 2003
From: monica.palaseanu-lovejoy at stud.man.ac.uk (Monica Palaseanu-Lovejoy)
Date: Thu, 30 Oct 2003 17:51:15 -0000
Subject: [R] spatial outliers
Message-ID: <E1AFGxJ-0009lJ-Pu@probity.mcc.ac.uk>

Hello everybody,

I have some questions about identifying spatial clustering in 
spatial autocorrelated data. It seems that there are some 
problems with the usual methods for cluster identification 
because they don?t take into consideration the spatial 
autocorrelation of data. 

I am wondering if there is any R procedure / packages which 
implement the OPTICS method? I am referring here to Ankerst 
et. al., 1999, OPTICS: Ordering Points to Identify the 
Clustering Structure, Proc. ACM SIGMOD?99 Int. Conf. On 
Management of Data, Philadelphia, PA, 
{ HYPERLINK "http://www.cs.ualberta.ca/~joerg/papers/OPTICS-final.pdf" }http://www.cs.ualberta.ca/~joerg/papers/OPTICS-final.pdf

Any help, as usual, is greatly appreciated,

Monica



From bates at stat.wisc.edu  Thu Oct 30 19:02:15 2003
From: bates at stat.wisc.edu (Douglas Bates)
Date: 30 Oct 2003 12:02:15 -0600
Subject: [R] 'nls' and its arguments
In-Reply-To: <20031030145451.GA11261@nf034.jinr.ru>
References: <20031030145451.GA11261@nf034.jinr.ru>
Message-ID: <6rvfq6wqmw.fsf@bates4.stat.wisc.edu>

Timur Elzhov <Timur.Elzhov at jinr.ru> writes:

> Dear R experts!
> 
> I'd to fit data by 'nls' with me-supplied function 'fcn'.
> 
> 1) I'd like 'fcn' to accept arbitrary arguments, i.e. I defined it
>    as f(...) {<body>}. (Ok, that's not actually impotant).
> 2) Second, I would NOT like to supply every parameter in the formula.
>    To illustrate this, let's look at the last example of 'nls' help
>    page:
> 
>     ## weighted nonlinear regression
>     data(Puromycin)
>     Treated <- Puromycin[Puromycin$state == "treated", ]
>     weighted.MM <- function(resp, conc, Vm, K)
>     {
>         ## Purpose: exactly as white book p.451 -- RHS for nls()
>         ##  Weighted version of Michaelis-Menten model
>         ## ---------------------------------------------------------------
>         ## Arguments: `y', `x' and the two parameters (see book)
>         ## ---------------------------------------------------------------
>         ## Author: Martin Maechler, Date: 23 Mar 2001, 18:48
>         print(resp)
>     
>         pred <- (Vm * conc)/(K + conc)
>         (resp - pred) / sqrt(pred)
>     }
>     Pur.wt <- nls( ~ weighted.MM(rate, conc, Vm, K), data = Treated,
>                   start = list(Vm = 200, K = 0.1))
> 
> So, in this example I wouldn't like to write `weighted.MM(rate, conc, Vm, K'),
> and `start = list(Vm = 200, K = 0.1)', instead I'd like to supply _lists_.
> With the 'start' parameter it's easy - I create list
> 
>     p.start <- list(Vm = 200, K = 0.1)
> 
> and assign it to 'start' in nls():
> 
>     start = p.start
> 
> - that works. But, with the formula it's not so simple. Well, I tried
> at first to make formula more "list-like":
> 
>     Pur.wt <- nls( ~ do.call("weighted.MM", list(rate, conc, Vm, K)),
>                   data = Treated,
>                   start = p.start)
> 
> - that works too. Now, let's try to separate data and parameters:
> 
>     Pur.wt <- nls( ~ do.call("weighted.MM", c(list(rate, conc), list(Vm, K))),
>                   data = Treated,
>                   start = p.start)
> 
> - that's right. So, I have a data 'Treated', and a start params list
> 'p.start'. Now, here is the _point_: I want nls to read names of
> variables from the lists and supply it to function in the formula.
> In this example 'weighted.MM' has a certain arg list, but my function is
> 'fcn(...)', and I want supply _all_ these names to the fcn. I tried to
> change 'list(Vm, K)' to list(as.name("Vm"), as.name("K")):
> 
>     Pur.wt <- nls( ~ do.call("weighted.MM", c(list(rate, conc), list(as.name("Vm"), as.name("K")))),
>                   data  = Treated, start = p.start)
> 
> - works again! Now,
>     p.arg  <- list(as.name("Vm"), as.name("K"))
>     Pur.wt <- nls( ~ do.call("weighted.MM", c(list(rate, conc), p.arg)),
>                              data  = Treated, start = p.start)
>     Error in model.frame(formula, rownames, variables, varnames, extras, extranames,  : 
>             invalid variable type

I'm not really sure what you want to do but the problem here is that
you are providing names as starting estimates, not the values of the
names.  You _may_ be able to use lapply(p.arg, eval) instead of p.arg
but you would have to be careful about exactly when the eval is
performed and in what environment.

I would say that this looks like a very complicated way of approaching
a problem and maybe it is time to step back and see if you can come up
with another approach.

> How can I fix this, and supply _lists_ with parameter names (and data names)
> to fcn(...) in nls?



From umeno at students.uiuc.edu  Thu Oct 30 19:19:00 2003
From: umeno at students.uiuc.edu (umeno)
Date: Thu, 30 Oct 2003 12:19:00 -0600
Subject: [R] Variance of a non-linear combination of the coefficient
	estiamtes
Message-ID: <3FB975B2@webmail.uiuc.edu>

Hi,

I would like to know if anyone knows how to compute a variance of the 
non-linear combination of the coefficient estimates.

Say, I get a model of

y~c+ax+bz (1)
where x and z are the independent variables, c is the constant estimate, and a 
and b are the coefficient estimates.

Then, I want to know the variance of

b*c/a (2).

How am I going to get it?

In Stata, I can just use "bs" function by defining the regression model (1) 
and the statistic of the interest(2).

Help!!!

Thank you
Soyoko

______________________________________
Ms. Soyoko Umeno
Graduate Research Assitant for the Illinois-Missouri Biotechnology Alliance (IMBA) at http://www.imba.missouri.edu/
Ph.D. Student at the Department of Agricultural and Consumer Economics
at the University of Illinois at Urbana-Champaign
Office Phone: 217-333-3417 or 217-333-0364
Fax: 217-244-4817
Mailing Address: 1301 W. Gregory Dr. MC710, Urbana, IL 61801



From ripley at stats.ox.ac.uk  Thu Oct 30 19:52:02 2003
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Thu, 30 Oct 2003 18:52:02 +0000 (GMT)
Subject: [R] How to Extract Std.Error
In-Reply-To: <3FA14C7F.6020808@rcs.urz.tu-dresden.de>
Message-ID: <Pine.LNX.4.44.0310301851320.17095-100000@gannet.stats>

Or use the generic function vcov.

On Thu, 30 Oct 2003, Thomas Petzoldt wrote:

> umeno wrote:
> 
> > I would like to know if anyone has any idea of how to extract Std.Error from 
> > the regression output.  I know how to extract coefficients, but I could not 
> > figure out how to retrieve std. error...
> 
> Given a model like this
> 
> model <- lm(y~x)
> 
> first analyse the data structure of the summary() with
> 
> str(summary(model))
> 
> looking into this, the following (or something similar for the other
> values) should work:
> 
> summary(reg)$coefficients[,2]

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From bates at stat.wisc.edu  Thu Oct 30 20:01:13 2003
From: bates at stat.wisc.edu (Douglas Bates)
Date: 30 Oct 2003 13:01:13 -0600
Subject: [R] Change in 'solve' for r-patched
Message-ID: <6rsmlaturq.fsf@bates4.stat.wisc.edu>

The solve function in r-patched has been changed so that it applies a
tolerance when using Lapack routines to calculate the inverse of a
matrix or to solve a system of linear equations.  A tolerance has
always been used with the Linpack routines but not with the Lapack
routines in versions 1.7.x and 1.8.0.  (You can use the optional
argument tol = 0 to override this check for computational singularity
but you probably want to ask yourself why before doing so.  In a
perfect world you would be required to pass a qualifier exam before
being allowed to do that. :-)

We found that making such a change with a not-unreasonable default
value for the tolerance caused several packages to fail R CMD check.
As a result we have changed to a very generous default tolerance and,
by default, declare the matrix to be computationally singular if the
estimated reciprocal condition number is less than
.Machine$double.eps.  In future versions of R we may make that
tolerance tighter.

A quick glance at the R CMD check failures resulting from the earlier
change shows that in most cases the calculation involved was something
like 
  solve(t(X) %*% X)

This is not good numerical linear algebra.  Admittedly we all learn
that, for example, least squares estimates can be written as
(X'X)^{-1} X'y and it is seems reasonable to code this calculation as 
  betahat <- solve(t(X) %*% X) %*% t(X) %*% y
but you shouldn't do that.

In terms of numerical stability and also in terms of efficiency, that
is an remarkably bad way of determining least squares estimates.

To begin with, you don't invert a matrix just to solve a system of
equations.  If A is an n by n matrix and y is an n vector, then

 solve(A)

is the equivalent of performing

 solve(A, y)

n times.  Thus solve(A) %*% y is more than n times as expensive as
solve(A, y).  If you want A^{-1}y then write it as solve(A, y).

(In a perfect world you would be required to fill out a form, in
triplicate, explaining in detail exactly why there is no suitable
alternative to calculating the inverse before being allowed to use
solve(A).)

So, how should you calculate least squares estimates?  We recommend

 betahat <- qr.coef(qr(X), y)

If you want other results from the least squares calculation, such as
fitted values or residuals, you may want to save qr(X) so you can reuse it

 qrX <- qr(X)
 betahat <- qr.coef(qrX, y)
 res <- qr.resid(qrX, y)
 ...

There are alternatives but
 solve(t(X) %*% X) %*% t(X) %*% y
is never a good one.  Seber and Lee discuss discuss such calculations
at length in chapter 11 of their "Linear Regression Analysis (2nd ed)"
(Wiley, 2003).

Some other comments:

 - the condition number of X'X is the square of the condition number
   of X, which is why it is a good idea to avoid working with X'X
   whenever possible 
 - on those rare occasions when you do want (X'X)^{-1}, say to form
   a variance-covariance matrix, you should evaluate it as
    chol2inv(qr.R(qr(X)))
 - if, despite all of the above, you feel you must calculate X'X it is
   best done as
    crossprod(X)
   and not as
    t(X) %*% X
   Similarly, X'y is best calculated as crossprod(X, y).  In general
   you should use crossprod in place of "t(anything) %*% anythingelse"



From bates at stat.wisc.edu  Thu Oct 30 20:06:26 2003
From: bates at stat.wisc.edu (Douglas Bates)
Date: 30 Oct 2003 13:06:26 -0600
Subject: [R] How to Extract Std.Error
In-Reply-To: <3FA14C7F.6020808@rcs.urz.tu-dresden.de>
References: <3FB8AEB8@webmail.uiuc.edu>
	<3FA14C7F.6020808@rcs.urz.tu-dresden.de>
Message-ID: <6roevytuj1.fsf@bates4.stat.wisc.edu>

Thomas Petzoldt <petzoldt at rcs.urz.tu-dresden.de> writes:

> umeno wrote:
> 
> > I would like to know if anyone has any idea of how to extract
> > Std.Error from the regression output.  I know how to extract
> > coefficients, but I could not figure out how to retrieve
> > std. error...
> 
> 
> Given a model like this
> 
> model <- lm(y~x)
> 
> first analyse the data structure of the summary() with
> 
> str(summary(model))
> 
> looking into this, the following (or something similar for the other
> values) should work:
> 
> summary(reg)$coefficients[,2]

It would be better form to use
  coef(summary(reg))[, 2]
or even
  coef(summary(reg))[, "Std. Error"]

It is safer to use extractor functions like coef() than to rely on the
object returned by summary having components with particular names.



From gerifalte28 at hotmail.com  Thu Oct 30 20:10:23 2003
From: gerifalte28 at hotmail.com (Francisco Vergara)
Date: Thu, 30 Oct 2003 19:10:23 +0000
Subject: [R] Transformed predictor in linear model
Message-ID: <Law14-F92ArVc4Iu1B60003a05e@hotmail.com>

Hi

I fitted a mixed model with several categorical and continuous fixed 
variables and 3 nested random intercepts, using a transformation of the 
predictor y = log(y+1) and I used the defaults contrast treatment and 
contrast polynomial.

The resulting coefficients are in log units meaning that the difference 
between the categorical (Factor) variables builted by Contr.treatment is 
expressed on log values but to retransform them to actual (y) values is not 
as straight forward as to exp(beta)-1.  I am sure that this is a problem 
that you guys face every day but I can't find an easy way around this. The 
easiest way to go is to report the betas in transformed values but this is 
not very useful for the practical purposes of the study.

Any comments??

_________________________________________________________________
Want to check if your PC is virus-infected?  Get a FREE computer virus scan 
online from McAfee.



From ripley at stats.ox.ac.uk  Thu Oct 30 20:42:56 2003
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Thu, 30 Oct 2003 19:42:56 +0000 (GMT)
Subject: [R] RMySQL and '_' character in column names
In-Reply-To: <200310301754.50140.xavier.fim@eresmas.net>
Message-ID: <Pine.LNX.4.44.0310301942210.17159-100000@gannet.stats>

?make.names may help.

On Thu, 30 Oct 2003, Xavier Fern?ndez i Mar?n wrote:

> Hi,
> 
> I'm using RMySQL in order to obtain data from my MySQL server. In my databases 
> sometimes I have columns with names that contain '_' character (ex: 
> 'gdp_capita', 'population_total', etc...). When these names appear as the 
> names of the vectors in the data frame that I get, sometimes I have problems 
> as:
> 
> > cor(gdp_capita, population_total)
> Error: object "_capita" not found
> use of _ is soon to be removed.
> 
> Is there an automatic way to transform the '_' characters in MySQL to '.' in R 
> using RMysql?

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From bates at stat.wisc.edu  Thu Oct 30 20:59:00 2003
From: bates at stat.wisc.edu (Douglas Bates)
Date: 30 Oct 2003 13:59:00 -0600
Subject: [R] Change in 'solve' for r-patched
In-Reply-To: <6rsmlaturq.fsf@bates4.stat.wisc.edu>
References: <6rsmlaturq.fsf@bates4.stat.wisc.edu>
Message-ID: <6r1xsuts3f.fsf@bates4.stat.wisc.edu>

Douglas Bates <bates at cs.wisc.edu> writes:

...
> To begin with, you don't invert a matrix just to solve a system of
> equations.  If A is an n by n matrix and y is an n vector, then
> 
>  solve(A)
> 
> is the equivalent of performing
> 
>  solve(A, y)
> 
> n times.  Thus solve(A) %*% y is more than n times as expensive as
> solve(A, y).  If you want A^{-1}y then write it as solve(A, y).

I overstated the comparison between solve(A, y) and solve(A) %*% y.
For both solve(A) and solve(A, y), the first step is to calculate
an LU decomposition of A, and the second step is to solve the linear
system(s) of equations using this decomposition.  It is only the second
step that takes n times as long for solve(A) than it does for solve(A, y)
(when y is a vector).  

The first step, which will account for the majority of the time spent
executing solve(A, y), is the same as in solve(A).



From RBaskin at ahrq.gov  Thu Oct 30 20:49:51 2003
From: RBaskin at ahrq.gov (RBaskin@ahrq.gov)
Date: Thu, 30 Oct 2003 14:49:51 -0500
Subject: [R] Variance of a non-linear combination of the coefficient e
	stiamtes
Message-ID: <3598558AD728D41183350008C7CF291C0F16B97F@exchange1.ahrq.gov>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20031030/30de5981/attachment.pl

From sdavis2 at mail.nih.gov  Thu Oct 30 21:23:07 2003
From: sdavis2 at mail.nih.gov (Sean Davis)
Date: Thu, 30 Oct 2003 15:23:07 -0500
Subject: [R] Finding common sets
Message-ID: <BBC6DD5B.E49%sdavis2@mail.nih.gov>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20031030/74a36d94/attachment.pl

From ggrothendieck at myway.com  Thu Oct 30 21:43:17 2003
From: ggrothendieck at myway.com (Gabor Grothendieck)
Date: Thu, 30 Oct 2003 15:43:17 -0500 (EST)
Subject: [R] Finding common sets
Message-ID: <20031030204317.9978839A2@mprdmxin.myway.com>


If A is the incidence matrix whose columns represent groups 
and whose rows represent elements then the i,j-th element of 
the cross product matrix, crossprod(A), is the number of 
common elements in group i and group j.

In your case A has the form cbind(A1,A2) and you only need 
the block cross diagonal elements, namely crossprod(A1,A2).


---
From: Sean Davis <sdavis2 at mail.nih.gov>
To: r-help <r-help at stat.math.ethz.ch> 
Subject: [R] Finding common sets 

 
 
I am working on a problem in which I have 2 groups of clusters, each of
which was generated from the same original list of members. Within each
group, the clusters cover the original list of members many times over.
What I am interested in finding is the number of common elements in every
pair of clusters when taking one from each group. In short, I need a FAST
way of finding the number of common elements in two groups, as this process
will be over all pairs. (The elements in the two groups can be numeric if
absolutely necessary, but it would be nice to be general here.)

Thanks,
Sean

-- 
Clinical Fellow
National Institutes of Health
NCI/NHGRI


_______________________________________________
No banners. No pop-ups. No kidding.
Introducing My Way - http://www.myway.com



From christoph.bier at web.de  Thu Oct 30 22:34:50 2003
From: christoph.bier at web.de (Christoph Bier)
Date: Thu, 30 Oct 2003 22:34:50 +0100
Subject: [R] Weird problem with median on a factor
Message-ID: <3FA183FA.701@web.de>

Hi all,

I hope this isn't a naive newbie question again. Here you can see column 264 of
a data frame containing data of the same interview in May and September. Column
264 contains the answers of 49 persons to a question in May.

> fbhint.spss1[,264]
  [1] teils/teils  sehr wichtig <NA>         <NA>         sehr wichtig
  [6] sehr wichtig sehr wichtig sehr wichtig <NA>         <NA>
[11] <NA>         <NA>         wichtig      <NA>         <NA>
[16] sehr wichtig <NA>         <NA>         <NA>         <NA>
[21] <NA>         <NA>         <NA>         wichtig      <NA>
[26] <NA>         <NA>         <NA>         <NA>         <NA>
[31] <NA>         <NA>         <NA>         <NA>         teils/teils
[36] sehr wichtig <NA>         <NA>         <NA>         <NA>
[41] wichtig      <NA>         sehr wichtig <NA>         <NA>
[46] sehr wichtig wichtig      <NA>         <NA>
Levels: sehr wichtig wichtig teils/teils unwichtig ganz unwichtig

Column 566 contains the answers from the same persons to the same question in
September.

> fbhint.spss1[,566]
  [1] <NA>         <NA>         <NA>         wichtig      wichtig
  [6] sehr wichtig sehr wichtig wichtig      wichtig      <NA>
[11] <NA>         <NA>         sehr wichtig sehr wichtig sehr wichtig
[16] sehr wichtig <NA>         unwichtig    wichtig      wichtig
[21] <NA>         <NA>         teils/teils  teils/teils  <NA>
[26] unwichtig    <NA>         <NA>         <NA>         <NA>
[31] wichtig      sehr wichtig sehr wichtig <NA>         unwichtig
[36] sehr wichtig <NA>         <NA>         teils/teils  wichtig
[41] wichtig      wichtig      <NA>         <NA>         wichtig
[46] <NA>         sehr wichtig teils/teils  <NA>
Levels: sehr wichtig wichtig teils/teils unwichtig ganz unwichtig

The following works:

> median(fbhint.spss1[,264], na.rm=T)
[1] sehr wichtig
Levels: sehr wichtig wichtig teils/teils unwichtig ganz unwichtig

... but here it doesn't:

> median(fbhint.spss1[,566], na.rm=T)
Error in Summary.factor(..., na.rm = na.rm) :
         "sum" not meaningful for factors

I don't have any ideas why! Can somebody give me a hint?

TIA

Best regards,

Christoph



From p.dalgaard at biostat.ku.dk  Thu Oct 30 22:35:37 2003
From: p.dalgaard at biostat.ku.dk (Peter Dalgaard)
Date: 30 Oct 2003 22:35:37 +0100
Subject: [R] RMySQL and '_' character in column names
In-Reply-To: <Pine.LNX.4.44.0310301942210.17159-100000@gannet.stats>
References: <Pine.LNX.4.44.0310301942210.17159-100000@gannet.stats>
Message-ID: <x27k2mtnme.fsf@biostat.ku.dk>

Prof Brian Ripley <ripley at stats.ox.ac.uk> writes:

> ?make.names may help.

Or backtick quoting in 1.8.0 (which this is clearly not since _ now
gives a syntax error).
 
> On Thu, 30 Oct 2003, Xavier Fern?ndez i Mar?n wrote:
> 
> > Hi,
> > 
> > I'm using RMySQL in order to obtain data from my MySQL server. In my databases 
> > sometimes I have columns with names that contain '_' character (ex: 
> > 'gdp_capita', 'population_total', etc...). When these names appear as the 
> > names of the vectors in the data frame that I get, sometimes I have problems 
> > as:
> > 
> > > cor(gdp_capita, population_total)
> > Error: object "_capita" not found
> > use of _ is soon to be removed.
> > 
> > Is there an automatic way to transform the '_' characters in MySQL to '.' in R 
> > using RMysql?
> 
> -- 
> Brian D. Ripley,                  ripley at stats.ox.ac.uk
> Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
> University of Oxford,             Tel:  +44 1865 272861 (self)
> 1 South Parks Road,                     +44 1865 272866 (PA)
> Oxford OX1 3TG, UK                Fax:  +44 1865 272595
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> 

-- 
   O__  ---- Peter Dalgaard             Blegdamsvej 3  
  c/ /'_ --- Dept. of Biostatistics     2200 Cph. N   
 (*) \(*) -- University of Copenhagen   Denmark      Ph: (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)             FAX: (+45) 35327907



From ross at biostat.ucsf.edu  Thu Oct 30 22:47:40 2003
From: ross at biostat.ucsf.edu (Ross Boylan)
Date: Thu, 30 Oct 2003 13:47:40 -0800
Subject: [R] Re: packaging a package addon
In-Reply-To: <1067483590.20574.252.camel@iron.libaux.ucsf.edu>
References: <1067474801.20574.195.camel@iron.libaux.ucsf.edu>
	<1067483590.20574.252.camel@iron.libaux.ucsf.edu>
Message-ID: <1067550460.20573.266.camel@iron.libaux.ucsf.edu>

Thanks to everyone for their advice.  I took easy/portable route of
adding the necessary C files to my package.  I agree that this is
undesirable as a long term solution.

I also added library("survival") to my .First.lib.  Is library, rather
than require, the right choice here?  I want it to fail if survival
doesn't load.

I'm also a little surprised there isn't a platform out there that will
gag on finding the same symbol in two different libraries...

Finally, a comment on R CMD check: perhaps it could produce some more of
the output when things fail?  I found that to diagnose the loading
problems as I developed this, I had to attempt to load the package
myself in R to see what the actual problem was.  There wasn't enough
info in the R CMD check to tell what exactly the problem was.



From ok at cs.otago.ac.nz  Thu Oct 30 23:55:38 2003
From: ok at cs.otago.ac.nz (Richard A. O'Keefe)
Date: Fri, 31 Oct 2003 11:55:38 +1300 (NZDT)
Subject: [R] long algo
Message-ID: <200310302255.h9UMtc1Y023759@atlas.otago.ac.nz>

"Alessandro Semeria" alessandro.semeria at cramont.it wrote:
	Is well know that R is inefficent  on loops.

This is a dangerous half-truth.  R is an interpreted language.
The interpreter uses techniques similar to those used in Scheme
interpreters.  As interpreters go, it's pretty good.  For comparison,
in processing XML documents, I've had interpreted Scheme running rings
around compiled Java (by doing the task a different way, of course).
Also for comparison, years ago I had a Prolog program for median
polish that made a published Fortran program for median polish look
sick (by using a much better data structure).  With Luke Tierney's
byte-code compiler, I expect R loops will become close to as efficient
as Python ones, and people run entire web sites with Python.

It is more accurate to say that R code qua R code is not as efficient
as the large body of "primitives" that operate on entire arrays.

	When you have to perform "heavy" loop
	is better to use a call to fortran or c code (.Fortran() , .C() functions)

Even if the premiss were literally and exactly true, the conclusion
would not follow.  When you have a speed problem with R code,

(1) Find out where the problem is, exactly.  People's intuition about
    performance bottlenecks is notoriously bad.  Do what the experts do:
    *measure*.
(2) Try to restructure the code *entirely in R* to be as clear and high
    level as possible.  If there have to be subscripts, at least let them
    be vector subscripts.
(3) Measure again.  Chances are that making the code clear and high level
    has fixed the performance problem.
(4) If that fails, try restructuring the code a couple of ways,
    *entirely in R*.  The two basic techniques for optimising a calculation
    are (a) eliminate it entirely and (b) if you can't eliminate the first
    evaluation of an expression, eliminate the second by saving the result.
    As a special case of (b), try moving things out of loops; try splitting
    a calculation into a part that changes a lot and a part that changes
    very little, and update the small-change part only when you have to.
    Perhaps apply the idea of program differentiation.  (NOT the idea of
    taking a function that computes a value and automatically computing
    a function that computes the derivative of the first, but the idea of
    saying if I have z<-f(x,y) and I make a small change to x, do I have
    to recompute z completely or can I came a small change to z?)
    Try to use built in operations as much as possible on data structures
    that are as large as appropriate.
(5) Measure again.  This will probably have fixed the performance problem.
(6) If all else fails, now it's time to try Fortran or C.  It's too bad
    there isn't an existing Fortran or C module you can just call, if there
    had been you'd have used that before writing the original R code.



From yuleih at umich.edu  Fri Oct 31 02:39:57 2003
From: yuleih at umich.edu (Yulei He)
Date: Thu, 30 Oct 2003 20:39:57 -0500 (EST)
Subject: [R] inverse function for positive definite matrix
Message-ID: <Pine.SOL.4.58.0310302038080.28861@zektor.gpcc.itd.umich.edu>

Hi, there.

Can anyone tell me inverse function for positive definite matrix in R? In
GAUSS, this function is called invpd and is known to be faster than the
normal inverse function.

Thanks!


Yulei

$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$
Yulei He
1586 Murfin Ave. Apt 37
Ann Arbor, MI 48105-3135
yuleih at umich.edu
734-647-0305(H)
734-763-0421(O)
734-763-0427(O)
734-764-8263(fax)
$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$



From smyth at wehi.edu.au  Fri Oct 31 07:03:59 2003
From: smyth at wehi.edu.au (Gordon Smyth)
Date: Fri, 31 Oct 2003 17:03:59 +1100
Subject: [R] cross-classified random factors in lme without blocking
Message-ID: <5.2.1.1.1.20031031153622.00acecc8@imaphost.wehi.edu.au>

On page 165 of Mixed-Effects Models in S and S-Plus by Pinheiro and Bates 
there is an example of using lme() in the nlme package to fit a model with 
crossed random factors. The example assumes though that the data is 
grouped. Is it possible to use lme() to fit crossed random factors when the 
data is not grouped?

E.g., y <- rnorm(12); a=gl(4,1,12); b=gl(3,4,12). Can I fit an additive 
model like y~a+b but with a and b random?

Everything I've tried gives an error:

 > lme(y~1,random=~1|(a+b))
Error in switch(mode(object), name = , numeric = , call = object, character 
= as.name(object),  :
         [[ cannot be of mode (
 > lme(y~1,random=~a+b)
Error in getGroups.data.frame(dataMix, groups) :
         Invalid formula for groups

Thanks
Gordon



From umeno at students.uiuc.edu  Fri Oct 31 07:46:33 2003
From: umeno at students.uiuc.edu (umeno)
Date: Fri, 31 Oct 2003 00:46:33 -0600
Subject: [R] which argument is missing from my boot...
Message-ID: <3FBFAF5D@webmail.uiuc.edu>

Hi,

I just cannot figure out what is missing from my boot.. This is what I have in 
my R codes:
-------------------------------------

payment.data<-read.csv("maintenance payment.csv.",header=TRUE)
fee<-lm(log(payment.data[,2])~payment.data[,1])
g<-fee$coefficients[2]
g

phat.data<-read.csv("phat.csv.",header=TRUE)
phat1<-phat.data[,2]+0.01
PreOLS<-lm(log(phat1)~phat.data[,1])
resid<-PreOLS$residuals
esqu<-resid^2
sigma<-(1-phat1)/(phat1*nrow(phat.data))
cnsOLS<-lm((esqu-sigma)~1)
f<-cnsOLS$fitted.values-sigma

window<-phat.data[,1]
phat2<-phat.data[,2]
data<-data.frame(window,phat2)
maxpay<-max(payment.data[2])
minpay<-min(payment.data[2])

delta.fun<-function(data)
{
wOLS<-lm(log(data[,2]+0.01)~data[,1],weights=f^(-0.5))
g -1*wOLS$coefficients[2]*log(maxpay/minpay)/wOLS$coefficients[1]
}

boot(data,delta.fun,100)
--------------------------------

Then, I get this message:
Error in statistic(data, original, ...) : unused argument(s) ( ...)

I tried so many combinations by defining variables in the statistic...I could 
not figure out what arguments are unused...I checked the delta.fun, and it 
seems to be working...

Could anyone help me, PLEASE???

Thank you 
soyoko

______________________________________
Ms. Soyoko Umeno
Graduate Research Assitant for the Illinois-Missouri Biotechnology Alliance (IMBA) at http://www.imba.missouri.edu/
Ph.D. Student at the Department of Agricultural and Consumer Economics
at the University of Illinois at Urbana-Champaign
Office Phone: 217-333-3417 or 217-333-0364
Fax: 217-244-4817
Mailing Address: 1301 W. Gregory Dr. MC710, Urbana, IL 61801



From temiz at deprem.gov.tr  Fri Oct 31 09:11:16 2003
From: temiz at deprem.gov.tr (temiz)
Date: Fri, 31 Oct 2003 10:11:16 +0200
Subject: [R] Therotical basis of Kriging
Message-ID: <3FA21924.801@deprem.gov.tr>

hello

I want to know about therotical basis of Kriging in elemantary level.
I will appreciate if anyone sends me address,link,e-documents, etc..

kind regards
-- 


Ahmet Temiz
General Directory of Disaster Affairs
Ankara TURKEY


______________________________________
Inflex - installed on mailserver for domain @deprem.gov.tr
Queries to: postmaster at deprem.gov.tr

______________________________________
The views and opinions expressed in this e-mail message are ...{{dropped}}



From massimiliano.cannata at supsi.ch  Fri Oct 31 08:57:02 2003
From: massimiliano.cannata at supsi.ch (Massimiliano Cannata)
Date: Fri, 31 Oct 2003 08:57:02 +0100
Subject: [R] Re: [STATSGRASS] Therotical basis of Kriging
References: <3FA21924.801@deprem.gov.tr>
Message-ID: <3FA215CE.CA0AE6A1@supsi.ch>

Hello,
here is a link about kriging, I think is clear enough to understand it.....

http://www.geomatics.ucalgary.ca/~nel-shei/DTMLectureNotes/ENGO%20573%20-%20Chapter%203%20Kriging%20and%20variograms.doc

Bye

temiz wrote:

> hello
>
> I want to know about therotical basis of Kriging in elemantary level.
> I will appreciate if anyone sends me address,link,e-documents, etc..
>
> kind regards
> --
>
> Ahmet Temiz
> General Directory of Disaster Affairs
> Ankara TURKEY
>
> ______________________________________
> Inflex - installed on mailserver for domain @deprem.gov.tr
> Queries to: postmaster at deprem.gov.tr
>
> ______________________________________
> The views and opinions expressed in this e-mail message are the sender's own
> and do not necessarily represent the views and the opinions of Earthquake Research Dept.
> of General Directorate of Disaster Affairs.
>
> Bu e-postadaki fikir ve gorusler gonderenin sahsina ait olup, yasal olarak T.C.
> B.I.B. Afet Isleri Gn.Mud. Deprem Arastirma Dairesi'ni baglayici nitelikte degildir.
>
> _______________________________________________
> statsgrass mailing list
> statsgrass at grass.itc.it
> http://grass.itc.it/mailman/listinfo/statsgrass

--
-------------
note: change my e-mail reference to massimiliano.cannata at supsi.ch
      because the old one will be deleted soon.
-------------
Ing.  Massimiliano Cannata
Istituto di Scienze della Terra - SUPSI
C.P. 72 - CH-6952 Canobbio (Ticino, Switzerland)
Tel +41 91 /935 12 25 - Fax +41 91 /935 12 09
eMail: massimiliano.cannata at supsi.ch
Internet: http://www.ist.supsi.ch



From Simon.Fear at synequanon.com  Fri Oct 31 10:18:06 2003
From: Simon.Fear at synequanon.com (Simon Fear)
Date: Fri, 31 Oct 2003 09:18:06 -0000
Subject: [R] Weird problem with median on a factor
Message-ID: <6C8A8033ABC1E3468048ABC4F13CE572D27519@synequanon01>

What do you expect the median of a factor to be? Are
you sure you don't want the *mode* (most common
value)?
If you want the median numeric code of ordered
factors, maybe use `as.numeric` first. Consider what you
think should happen if this median is not an integer.

HTH

> -----Original Message-----
> From: Christoph Bier [mailto:christoph.bier at web.de]
> Sent: 30 October 2003 21:35
> To: r-help at stat.math.ethz.ch
> Subject: [R] Weird problem with median on a factor
> 
> 
> Security Warning: 
> If you are not sure an attachment is safe to open please contact  
> Andy on x234. There are 0 attachments with this message. 
> ________________________________________________________________ 
>  
> Hi all,
> 
> I hope this isn't a naive newbie question again. Here you can 
> see column 264 of
> a data frame containing data of the same interview in May and 
> September. Column
> 264 contains the answers of 49 persons to a question in May.
> 
> > fbhint.spss1[,264]
>   [1] teils/teils  sehr wichtig <NA>         <NA>         sehr wichtig
>   [6] sehr wichtig sehr wichtig sehr wichtig <NA>         <NA>
> [11] <NA>         <NA>         wichtig      <NA>         <NA>
> [16] sehr wichtig <NA>         <NA>         <NA>         <NA>
> [21] <NA>         <NA>         <NA>         wichtig      <NA>
> [26] <NA>         <NA>         <NA>         <NA>         <NA>
> [31] <NA>         <NA>         <NA>         <NA>         teils/teils
> [36] sehr wichtig <NA>         <NA>         <NA>         <NA>
> [41] wichtig      <NA>         sehr wichtig <NA>         <NA>
> [46] sehr wichtig wichtig      <NA>         <NA>
> Levels: sehr wichtig wichtig teils/teils unwichtig ganz unwichtig
> 
> Column 566 contains the answers from the same persons to the 
> same question in
> September.
> 
> > fbhint.spss1[,566]
>   [1] <NA>         <NA>         <NA>         wichtig      wichtig
>   [6] sehr wichtig sehr wichtig wichtig      wichtig      <NA>
> [11] <NA>         <NA>         sehr wichtig sehr wichtig sehr wichtig
> [16] sehr wichtig <NA>         unwichtig    wichtig      wichtig
> [21] <NA>         <NA>         teils/teils  teils/teils  <NA>
> [26] unwichtig    <NA>         <NA>         <NA>         <NA>
> [31] wichtig      sehr wichtig sehr wichtig <NA>         unwichtig
> [36] sehr wichtig <NA>         <NA>         teils/teils  wichtig
> [41] wichtig      wichtig      <NA>         <NA>         wichtig
> [46] <NA>         sehr wichtig teils/teils  <NA>
> Levels: sehr wichtig wichtig teils/teils unwichtig ganz unwichtig
> 
> The following works:
> 
> > median(fbhint.spss1[,264], na.rm=T)
> [1] sehr wichtig
> Levels: sehr wichtig wichtig teils/teils unwichtig ganz unwichtig
> 
> ... but here it doesn't:
> 
> > median(fbhint.spss1[,566], na.rm=T)
> Error in Summary.factor(..., na.rm = na.rm) :
>          "sum" not meaningful for factors
> 
> I don't have any ideas why! Can somebody give me a hint?
> 
> TIA
> 
> Best regards,
> 
> Christoph
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
>  
 
Simon Fear 
Senior Statistician 
Syne qua non Ltd 
Tel: +44 (0) 1379 644449 
Fax: +44 (0) 1379 644445 
email: Simon.Fear at synequanon.com 
web: http://www.synequanon.com 
  
Number of attachments included with this message: 0 
  
This message (and any associated files) is confidential and\...{{dropped}}



From p.pagel at gsf.de  Fri Oct 31 10:17:21 2003
From: p.pagel at gsf.de (Philipp Pagel)
Date: Fri, 31 Oct 2003 10:17:21 +0100
Subject: [R] weighted rank correlation test?
Message-ID: <20031031091720.GA2346@porcupine.gsf.de>


	Hi R-gurus!

Is there a package that implements corr.test with weights, or will I
have to deal with this myself?

I found corr() from the boot package which will calculate r for me and I
could certainly do the ranking myself but it does not give me a
p-value...

Thanks for any hints.

cu
	Philipp

-- 
Dr. Philipp Pagel                                Tel.  +49-89-3187-3675
Institute for Bioinformatics / MIPS              Fax.  +49-89-3187-3585
GSF - National Research Center for Environment and Health
Ingolstaedter Landstrasse 1
85764 Neuherberg, Germany



From p.dalgaard at biostat.ku.dk  Fri Oct 31 10:28:23 2003
From: p.dalgaard at biostat.ku.dk (Peter Dalgaard)
Date: 31 Oct 2003 10:28:23 +0100
Subject: [R] Weird problem with median on a factor
In-Reply-To: <3FA183FA.701@web.de>
References: <3FA183FA.701@web.de>
Message-ID: <x2vfq5rc20.fsf@biostat.ku.dk>

Christoph Bier <christoph.bier at web.de> writes:

> The following works:
> 
> > median(fbhint.spss1[,264], na.rm=T)
> [1] sehr wichtig
> Levels: sehr wichtig wichtig teils/teils unwichtig ganz unwichtig
> 
> ... but here it doesn't:
> 
> > median(fbhint.spss1[,566], na.rm=T)
> Error in Summary.factor(..., na.rm = na.rm) :
>          "sum" not meaningful for factors
> 
> I don't have any ideas why! Can somebody give me a hint?

Offhand, I'd guess that the "median" is inbetween two factor levels in
one case and not in the other. However, both cases should give an
error, especially for unordered factors, but it is not well-defined
for ordered factors either. If you want to interpret your factor as a
numeric scale, use as.numeric first.

-- 
   O__  ---- Peter Dalgaard             Blegdamsvej 3  
  c/ /'_ --- Dept. of Biostatistics     2200 Cph. N   
 (*) \(*) -- University of Copenhagen   Denmark      Ph: (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)             FAX: (+45) 35327907



From p.dalgaard at biostat.ku.dk  Fri Oct 31 10:37:28 2003
From: p.dalgaard at biostat.ku.dk (Peter Dalgaard)
Date: 31 Oct 2003 10:37:28 +0100
Subject: [R] cross-classified random factors in lme without blocking
In-Reply-To: <5.2.1.1.1.20031031153622.00acecc8@imaphost.wehi.edu.au>
References: <5.2.1.1.1.20031031153622.00acecc8@imaphost.wehi.edu.au>
Message-ID: <x2r80trbmv.fsf@biostat.ku.dk>

Gordon Smyth <smyth at wehi.edu.au> writes:

> On page 165 of Mixed-Effects Models in S and S-Plus by Pinheiro and
> Bates there is an example of using lme() in the nlme package to fit a
> model with crossed random factors. The example assumes though that the
> data is grouped. Is it possible to use lme() to fit crossed random
> factors when the data is not grouped?
> 
> E.g., y <- rnorm(12); a=gl(4,1,12); b=gl(3,4,12). Can I fit an
> additive model like y~a+b but with a and b random?
> 
> Everything I've tried gives an error:
> 
>  > lme(y~1,random=~1|(a+b))
> Error in switch(mode(object), name = , numeric = , call = object,
> character = as.name(object),  :
>          [[ cannot be of mode (
>  > lme(y~1,random=~a+b)
> Error in getGroups.data.frame(dataMix, groups) :
>          Invalid formula for groups
> 

A standard trick is to define a grouping with one level:

one <- rep(1,length(y)
lme(...., random=~pdBlocked(.....)|one)

(Sorry, I'm a little rusty on the syntax, but just follow the example
in P&B)

AFAIR, it also works with random=list(a=~1,one=~b) and vice versa.

(The model is the same but you get different DF calculations, none of
which are correct in the completely balanced case...)

-- 
   O__  ---- Peter Dalgaard             Blegdamsvej 3  
  c/ /'_ --- Dept. of Biostatistics     2200 Cph. N   
 (*) \(*) -- University of Copenhagen   Denmark      Ph: (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)             FAX: (+45) 35327907



From christoph.bier at web.de  Fri Oct 31 10:34:06 2003
From: christoph.bier at web.de (Christoph Bier)
Date: Fri, 31 Oct 2003 10:34:06 +0100
Subject: [R] Weird problem with median on a factor
In-Reply-To: <6C8A8033ABC1E3468048ABC4F13CE572D27519@synequanon01>
References: <6C8A8033ABC1E3468048ABC4F13CE572D27519@synequanon01>
Message-ID: <3FA22C8E.4030705@web.de>

Simon Fear schrieb:
> What do you expect the median of a factor to be? Are
> you sure you don't want the *mode* (most common
> value)?

That's not the point, I was just playing around interested in 
the output of the median and surprised, that it worked for 
column 264. But I wonder why the median does not work with 
column 566, although I can't figure out any difference between 
the two columns.

[...]

Thanks for your answer!

Regards,

Christoph
-- 
Christoph Bier, Dipl.Oecotroph., Email: bier at wiz.uni-kassel.de
Universitaet Kassel, FG Oekologische Lebensmittelqualitaet und
Ernaehrungskultur \\ Postfach 12 52 \\ 37202 Witzenhausen
Tel.: +49 (0) 55 42 / 98 -17 21, Fax: -17 13



From simon at stats.gla.ac.uk  Fri Oct 31 10:51:26 2003
From: simon at stats.gla.ac.uk (Simon Wood)
Date: Fri, 31 Oct 2003 09:51:26 +0000 (GMT)
Subject: [R] inverse function for positive definite matrix
In-Reply-To: <Pine.SOL.4.58.0310302038080.28861@zektor.gpcc.itd.umich.edu>
References: <Pine.SOL.4.58.0310302038080.28861@zektor.gpcc.itd.umich.edu>
Message-ID: <Pine.SOL.4.58.0310310950340.21346@moon.stats.gla.ac.uk>

> Can anyone tell me inverse function for positive definite matrix in R? In
> GAUSS, this function is called invpd and is known to be faster than the
> normal inverse function.

see ?chol and ?chol2inv

simon
_____________________________________________________________________
> Simon Wood simon at stats.gla.ac.uk        www.stats.gla.ac.uk/~simon/
>>  Department of Statistics, University of Glasgow, Glasgow, G12 8QQ
>>>   Direct telephone: (0)141 330 4530          Fax: (0)141 330 4814



From christoph.bier at web.de  Fri Oct 31 10:52:21 2003
From: christoph.bier at web.de (Christoph Bier)
Date: Fri, 31 Oct 2003 10:52:21 +0100
Subject: [R] Weird problem with median on a factor
In-Reply-To: <x2vfq5rc20.fsf@biostat.ku.dk>
References: <3FA183FA.701@web.de> <x2vfq5rc20.fsf@biostat.ku.dk>
Message-ID: <3FA230D5.8010604@web.de>

Peter Dalgaard wrote:

> Offhand, I'd guess that the "median" is inbetween two factor levels in
> one case and not in the other. 

Hm, maybe. A problem, that would not occur, if I used the 
median on the numeric data of this factor.

> However, both cases should give an
> error, especially for unordered factors, but it is not well-defined

That's what I expected.

> for ordered factors either. If you want to interpret your factor as a
> numeric scale, use as.numeric first.

Yes, I already understood this :-) (At last we received your 
book or rather three of your books and the MASS-book).

Thanks for your answer.

Regards,

Christoph
-- 
Christoph Bier, Dipl.Oecotroph., Email: bier at wiz.uni-kassel.de
Universitaet Kassel, FG Oekologische Lebensmittelqualitaet und
Ernaehrungskultur \\ Postfach 12 52 \\ 37202 Witzenhausen
Tel.: +49 (0) 55 42 / 98 -17 21, Fax: -17 13



From tencpa at yahoo.it  Fri Oct 31 11:10:35 2003
From: tencpa at yahoo.it (Paolo Tenconi)
Date: Fri, 31 Oct 2003 11:10:35 +0100
Subject: [R] Optimization of objective function with generic number of
	arguments (R-Extension with C code) 
Message-ID: <oprxwjvxbt0rxtxe@smtp.mail.yahoo.it>

Hi All! I'm a new subscriber to this mailing list. I'm writing an R 
extension with C linked code having a minimization function letting me pass 
it an objective function with a GENERIC number of arguments and letting me 
to optimize over a specific one among them if there are many.


#################################################
###############  IPOTETICAL MAIN  ###############
#################################################


#Objective func with only one parameter
ObjectiveFunction1<-function(a){
...
}

#Objective func with two arguments
ObjectiveFunction2<-function(a,b){
...
}


#Minimize the first function
MyOptim(pars,ObjectiveFunction1)


#Minimize the second function over the first argument
MyOptim(pars,ObjectiveFunction2,b=xxx)


#Minimize the second function over the second argument
MyOptim(pars,ObjectiveFunction2,a=xxx)






######################################################
##############   EXCERPT OF MY CODE   ################
######################################################

#My R minimization function
MyOptim <-function(par, fn, ...)
{
    ...
    fn1 <-function(par) fn(par,...)
    res <-.External(MyOptimC(par, fn1,new.env()))
    ...
}

#My C function
__declspec (dllexport) SEXP MyOptimC(SEXP args)
{
   args = CDR(args); SEXP par = CAR(args);
    args = CDR(args); SEXP fn = CAR(args);
    rho  = CDR(args); SEXP rho = CAR(args);


    SEXP fminfn = lang2(fn, R_NilValue));
    ...	

}


I suceedeed only for objective functions with a specific number of 
arguments, while for the problem at hand I tried to replicate the code in 
optim.c and related files but without any result. Is there someone on the 
list who can me give me some insight, piece of source code or material 
explaining how to do this?

Many thanks.
Paolo



From billthebrute at yahoo.fr  Fri Oct 31 11:26:56 2003
From: billthebrute at yahoo.fr (=?iso-8859-1?q?william=20ritchie?=)
Date: Fri, 31 Oct 2003 11:26:56 +0100 (CET)
Subject: [R] too long
Message-ID: <20031031102656.44715.qmail@web25208.mail.ukl.yahoo.com>

Hi everyone,

 I ve been using R for months and find it really
practical and straight forward.
However (the inevitable however), I am finding it very
slow for one of my operations:
it s basically an itertation over i and j in a pretty
big table (4* 4608). It takes 30 minutes!!!!

Thanks


Ps:if it can help here is the source:

median1<-matrix(nrow=4608,ncol=1)
median2<-matrix(nrow=4608,ncol=1)
median3<-matrix(nrow=4608,ncol=1)
median4<-matrix(nrow=4608,ncol=1)
v<-c(18,19,20,21,23)
for (i in 0:11)
    {
     for (j in 1:384)
        {    
         
median1[j+(i*384),]<-puce[j+(i*384),5]+median(puce[v+384*i,2]-puce[v+384*i,5])
        
median2[j+(i*384),]<-puce[j+(i*384),19]+median(puce[v+384*i,16]-puce[v+384*i,19])
        
median3[j+(i*384),]<-puce[j+(i*384),12]+median(puce[v+384*i,9]-puce[v+384*i,12])
        
median4[j+(i*384),]<-puce[j+(i*384),26]+median(puce[v+384*i,23]-puce[v+384*i,26])
         
       
          puce[,5]<-median1
         puce[,19]<-median2
         puce[,12]<-median3
         puce[,26]<-median4


         }
     }



From christoph.bier at web.de  Fri Oct 31 11:36:34 2003
From: christoph.bier at web.de (Christoph Bier)
Date: Fri, 31 Oct 2003 11:36:34 +0100
Subject: [R] Weird problem with median on a factor
In-Reply-To: <6C8A8033ABC1E3468048ABC4F13CE572D2751B@synequanon01>
References: <6C8A8033ABC1E3468048ABC4F13CE572D2751B@synequanon01>
Message-ID: <3FA23B32.9000707@web.de>

Simon Fear schrieb:
> The last part of my message is what I thought might be the cause -
> maybe your median is not an integer, so what category
> should it be mapped to? What do you get if you slip
> in an `as.numeric` before calculating the median?
> [if still an error, then there is definitely something else
> going wrong to report]

Then everything is ok.

 > as.numeric(fbhint.spss1$V15.SP1) -> tmp.data2
 > median(tmp.data2, na.rm=T)
[1] 2

Christoph
-- 
Christoph Bier, Dipl.Oecotroph., Email: bier at wiz.uni-kassel.de
Universitaet Kassel, FG Oekologische Lebensmittelqualitaet und
Ernaehrungskultur \\ Postfach 12 52 \\ 37202 Witzenhausen
Tel.: +49 (0) 55 42 / 98 -17 21, Fax: -17 13



From ligges at statistik.uni-dortmund.de  Fri Oct 31 12:03:10 2003
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Fri, 31 Oct 2003 12:03:10 +0100
Subject: [R] too long
In-Reply-To: <20031031102656.44715.qmail@web25208.mail.ukl.yahoo.com>
References: <20031031102656.44715.qmail@web25208.mail.ukl.yahoo.com>
Message-ID: <3FA2416E.4070806@statistik.uni-dortmund.de>

william ritchie wrote:

> Hi everyone,
> 
>  I ve been using R for months and find it really
> practical and straight forward.
> However (the inevitable however), I am finding it very
> slow for one of my operations:
> it s basically an itertation over i and j in a pretty
> big table (4* 4608). It takes 30 minutes!!!!
> 
> Thanks


You got at least 3 replies on your first message. Why do you post it 
again???

Uwe Ligges

> 
> Ps:if it can help here is the source:
> 
> median1<-matrix(nrow=4608,ncol=1)
> median2<-matrix(nrow=4608,ncol=1)
> median3<-matrix(nrow=4608,ncol=1)
> median4<-matrix(nrow=4608,ncol=1)
> v<-c(18,19,20,21,23)
> for (i in 0:11)
>     {
>      for (j in 1:384)
>         {    
>          
> median1[j+(i*384),]<-puce[j+(i*384),5]+median(puce[v+384*i,2]-puce[v+384*i,5])
>         
> median2[j+(i*384),]<-puce[j+(i*384),19]+median(puce[v+384*i,16]-puce[v+384*i,19])
>         
> median3[j+(i*384),]<-puce[j+(i*384),12]+median(puce[v+384*i,9]-puce[v+384*i,12])
>         
> median4[j+(i*384),]<-puce[j+(i*384),26]+median(puce[v+384*i,23]-puce[v+384*i,26])
>          
>        
>           puce[,5]<-median1
>          puce[,19]<-median2
>          puce[,12]<-median3
>          puce[,26]<-median4
> 
> 
>          }
>      }
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help



From ripley at stats.ox.ac.uk  Fri Oct 31 12:14:20 2003
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Fri, 31 Oct 2003 11:14:20 +0000 (GMT)
Subject: [R] Re: packaging a package addon
In-Reply-To: <1067550460.20573.266.camel@iron.libaux.ucsf.edu>
Message-ID: <Pine.LNX.4.44.0310311109460.18842-100000@gannet.stats>

On Thu, 30 Oct 2003, Ross Boylan wrote:

> Thanks to everyone for their advice.  I took easy/portable route of
> adding the necessary C files to my package.  I agree that this is
> undesirable as a long term solution.
> 
> I also added library("survival") to my .First.lib.  Is library, rather
> than require, the right choice here?  I want it to fail if survival
> doesn't load.
> 
> I'm also a little surprised there isn't a platform out there that will
> gag on finding the same symbol in two different libraries...

That's why we look up the symbols directly in a shared object (and it is
important to use the PACKAGE= argument to ensure this is the right shared
object) and why dyn.load has local=TRUE as the default.  Various of us,
and particularly Duncan Temple Lang, have worked hard to get to this
point.

> Finally, a comment on R CMD check: perhaps it could produce some more of
> the output when things fail?  I found that to diagnose the loading
> problems as I developed this, I had to attempt to load the package
> myself in R to see what the actual problem was.  There wasn't enough
> info in the R CMD check to tell what exactly the problem was.

I think there usually is, but you have to look in the log file or one of 
the example files.

But I would not be doing R CMD check until I had both installed and 
loaded the package and run a few examples.

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From Simon.Fear at synequanon.com  Fri Oct 31 12:18:20 2003
From: Simon.Fear at synequanon.com (Simon Fear)
Date: Fri, 31 Oct 2003 11:18:20 -0000
Subject: [R] Weird problem with median on a factor
Message-ID: <6C8A8033ABC1E3468048ABC4F13CE572AC0E6F@synequanon01>

Final guess as to observed behaviour: in the first case after
removal of NAs there were an odd number of observations
(so that sum was not called within the code for median).
In your second call I suspect that even though you got
an integer answer, it was found as sum(2,2)/2.

It seems to me the best way to deal with this "bug" would
be to make calling median with a factor argument be an 
immediate error. Or just trust users never to attempt such
a thing ...  
 
Simon Fear 
Senior Statistician 
Syne qua non Ltd 
Tel: +44 (0) 1379 644449 
Fax: +44 (0) 1379 644445 
email: Simon.Fear at synequanon.com 
web: http://www.synequanon.com 
  
Number of attachments included with this message: 0 
  
This message (and any associated files) is confidential and\...{{dropped}}



From ripley at stats.ox.ac.uk  Fri Oct 31 12:25:07 2003
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Fri, 31 Oct 2003 11:25:07 +0000 (GMT)
Subject: [R] Optimization of objective function with generic number of
	arguments (R-Extension with C code) 
In-Reply-To: <oprxwjvxbt0rxtxe@smtp.mail.yahoo.it>
Message-ID: <Pine.LNX.4.44.0310311120250.18842-100000@gannet.stats>

I suggest you do this at R level. You can define a function of one arg to
send to your optimizer using lexical scoping to find the rest.  That is
how optim does it:

    fn1 <- function(par) fn(par, ...)

and you just need to write code to figure out which variable you want (it
is not clear to me what your rules are) and do the same.


On Fri, 31 Oct 2003, Paolo Tenconi wrote:

> Hi All! I'm a new subscriber to this mailing list. I'm writing an R 
> extension with C linked code having a minimization function letting me pass 
> it an objective function with a GENERIC number of arguments and letting me 
> to optimize over a specific one among them if there are many.
> 
> 
> #################################################
> ###############  IPOTETICAL MAIN  ###############
> #################################################
> 
> 
> #Objective func with only one parameter
> ObjectiveFunction1<-function(a){
> ...
> }
> 
> #Objective func with two arguments
> ObjectiveFunction2<-function(a,b){
> ...
> }
> 
> 
> #Minimize the first function
> MyOptim(pars,ObjectiveFunction1)
> 
> 
> #Minimize the second function over the first argument
> MyOptim(pars,ObjectiveFunction2,b=xxx)
> 
> 
> #Minimize the second function over the second argument
> MyOptim(pars,ObjectiveFunction2,a=xxx)
> 
> 
> 
> 
> 
> 
> ######################################################
> ##############   EXCERPT OF MY CODE   ################
> ######################################################
> 
> #My R minimization function
> MyOptim <-function(par, fn, ...)
> {
>     ...
>     fn1 <-function(par) fn(par,...)
>     res <-.External(MyOptimC(par, fn1,new.env()))
>     ...
> }
> 
> #My C function
> __declspec (dllexport) SEXP MyOptimC(SEXP args)
> {
>    args = CDR(args); SEXP par = CAR(args);
>     args = CDR(args); SEXP fn = CAR(args);
>     rho  = CDR(args); SEXP rho = CAR(args);
> 
> 
>     SEXP fminfn = lang2(fn, R_NilValue));
>     ...	
> 
> }
> 
> 
> I suceedeed only for objective functions with a specific number of 
> arguments, while for the problem at hand I tried to replicate the code in 
> optim.c and related files but without any result. Is there someone on the 
> list who can me give me some insight, piece of source code or material 
> explaining how to do this?
> 
> Many thanks.
> Paolo
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> 
> 

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From e.pebesma at geog.uu.nl  Fri Oct 31 12:29:48 2003
From: e.pebesma at geog.uu.nl (Edzer J. Pebesma)
Date: Fri, 31 Oct 2003 12:29:48 +0100
Subject: [R] A package for spatial data classes: request for comments
Message-ID: <3FA247AC.5000405@geog.uu.nl>

Much like time, spatial locations are not likely to change for observed
data. Still, R has no infrastructure or knowledge about spatial locations.

Many packages that deal with spatial data in R use their own classes
to deal with spatial data. On the workshop on spatial statistics software
held during DSC2003 (organized by Roger Bivand), we decided that
a base class that defines classes for spatial data should be helpful,
both as an exchange platform and (later) as a required package for
working with spatial data (compare the ts class for time series).

I started writing such a package, and opened a sourceforge.net project
for it, called r-spatial. I first worked with Barry Rowlinson's
spatial.data.frame (found in r-asp, or rasp, also on sourceforge),
which is  an S3 class. Then I restarted using S4 classes, because
they are here to stay, and allow validation.

Currently I defined three classes:

+ SpatialData
    +-- SpatialDataFrame
        +-- SpatialDataFrameGrid

SpatialData is only meant as a base class; it only contains a bounding
box (2D or 3D data), and information about projection (if present),
anticipating (re)projecting facilities in Roger's proj4R package.

SpatialDataFrame extends this class; holds a data frame, and the
information where in the data frame the coordinates (2D or 3D) are
stored.

SpatialDataFrameGrid extends SpatialDataFrame for the case where
the data are on a regular 2D/3D grid; it contains the offset of the grid,
the cellsize in (x,y,z) and the nr of row/cols/layers. One simple
way of creating these classes (idea taken from Barry) is:

 > data(meuse.grid) # data frame with gridded data
 > coordinates(meuse.grid) = c("x", "y") # promote to SpatialDataFrame
 > gridded(meuse.grid) = TRUE # promote to SpatialDataFrameGrid

in the last expression, the grid topology is auto-detected and stored.

Missing values for coordinates are not allowed.

This now works; I need to add more tests for a lot of pathetic cases.

To be really useful, the class should include vector (polygon) data,
and probably line elements. For this, I need help. The simples approach
would be to extend SpatialDataFrame to SpatialDataFramePolygon,
and add for each row add the corresponding polygon.

Questions:
- did I overlook important things in the current proposal?
- is there some class in a package that serves as a good
starting point for vector/polygon data?
- which information should be stored for each polygon (if I look
at class "Map" in maptools, there is a lot!)
- should we anticipate exchangeability with shapefiles, and
store everything needed for them right at the start? If yes,
how to deal with much simpler representations such as in
packages maps?
- should we work to two extensions of SpatialDataFrame,
first simply with the polygons, a second with all the
shapefile information requirements?
- will there ever be a need to export R data as shape files?
if not, which part of information in shapefiles may be ignored?
- Do we need another name, instead of the current SpatialCls?


You can download SpatialCls by cvs; use:

export CVS_RSH=ssh
cvs -d:pserver:anonymous at cvs.sf.net:/cvsroot/r-spatial login
# press return on the password prompt
cvs -d:pserver:anonymous at cvs.sf.net:/cvsroot/r-spatial co SpatialCls

If you are interested in becoming a co-developer, please join!
--
Edzer



From wb at arb-phys.uni-dortmund.de  Fri Oct 31 13:18:32 2003
From: wb at arb-phys.uni-dortmund.de (Wilhelm B. Kloke)
Date: Fri, 31 Oct 2003 13:18:32 +0100 (MET)
Subject: [R] strange logLik results in gls (nlme)
Message-ID: <200310311218.h9VCIWrU047796@yorikke.arb-phys.uni-dortmund.de>

I am trying to analyse a data with gls/lm using the following set of models

prcn.0.lm <- lm( log10(Y)~(cond-1)+(cond-1):t ,prcn)
prcn.1.gls <- gls( log10(Y)~(cond-1)+(cond-1):t ,prcn,cor=corAR1())
prcn.0.gls <- gls( log10(Y)~(cond-1)+(cond-1):t ,prcn)
prcn.1m.gls <- gls( log10(Y)~(cond-1)+(cond-1):t ,prcn,cor=corAR1(),method="ML")

I get the following AICs for these models:
> AIC(prcn.1m.gls)
[1] -78.3
> AIC(prcn.1.gls)
[1] -46.3
> AIC(prcn.0.gls)
[1] -24.7
> AIC(prcn.0.lm)
[1] -59.8
It is the difference between the last two, which puzzles me. They are
the same models. So I can't compare the AICs of prcn.0.lm and prcn.1.gls
directly. When using anova() for the comparison, I get a sensible result:
> anova(prcn.1.gls,prcn.0.lm)
           Model df   AIC    BIC logLik   Test L.Ratio p-value
prcn.1.gls     1  6 -46.3 -28.62   29.1                       
prcn.0.lm      2  5 -24.7  -9.97   17.3 1 vs 2    23.6  <.0001

Multiple arguments in AIC() give:

> AIC(prcn.1.gls,prcn.0.lm)
           df   AIC
prcn.1.gls  6 -46.3
prcn.0.lm   5 -59.8

How can I be sure to make it right?



From joehl at gmx.de  Fri Oct 31 13:57:52 2003
From: joehl at gmx.de (Jens =?ISO-8859-1?Q?Oehlschl=E4gel?=)
Date: Fri, 31 Oct 2003 13:57:52 +0100 (MET)
Subject: [R] How to grow an R object from C (.Call Interface)
Message-ID: <31840.1067605072@www45.gmx.net>


What is the best way to grow an R return object in writing a C function
using the Rdefines.h macros.
In my application, the final size of the return object is not known during
construction. My understanding is that each time I grow an R object I have to
use PROTECT() again, probably before UNPROTECTing the smaller version.
However, due to the stack character of the PROTECT mechanism, UNPROTECT would not
work to remove the smaller one, after the bigger has been protected. Is this
an indication to use UNPROTECT_PTR ? Or is another approach recommended?

May be the solution to this is worth a sentence in "Wrtiting R Extensions".

Thanks for any help


Jens Oehlschl?gel

-- 
NEU F?R ALLE - GMX MediaCenter - f?r Fotos, Musik, Dateien...
Fotoalbum, File Sharing, MMS, Multimedia-Gru?, GMX FotoService

Jetzt kostenlos anmelden unter http://www.gmx.net

+++ GMX - die erste Adresse f?r Mail, Message, More! +++



From ripley at stats.ox.ac.uk  Fri Oct 31 14:17:18 2003
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Fri, 31 Oct 2003 13:17:18 +0000 (GMT)
Subject: [R] How to grow an R object from C (.Call Interface)
In-Reply-To: <31840.1067605072@www45.gmx.net>
Message-ID: <Pine.LNX.4.44.0310311313490.19000-100000@gannet.stats>

I think the solution is PROTECT_WITH_INDEX. From the Rinternals.h file

/* We sometimes need to coerce a protected value and place the new
   coerced value under protection.  For these cases PROTECT_WITH_INDEX
   saves an index of the protection location that can be used to
   replace the protected value using REPROTECT. */
typedef int PROTECT_INDEX;
#define PROTECT_WITH_INDEX(x,i) R_ProtectWithIndex(x,i)
#define REPROTECT(x,i) R_Reprotect(x,i)

You can see examples in dataentry.c, optim.c and elsewhere.

I agree that should be mentioned in R-exts.

On Fri, 31 Oct 2003, Jens Oehlschl?gel wrote:

> 
> What is the best way to grow an R return object in writing a C function
> using the Rdefines.h macros.
> In my application, the final size of the return object is not known during
> construction. My understanding is that each time I grow an R object I have to
> use PROTECT() again, probably before UNPROTECTing the smaller version.
> However, due to the stack character of the PROTECT mechanism, UNPROTECT would not
> work to remove the smaller one, after the bigger has been protected. Is this
> an indication to use UNPROTECT_PTR ? Or is another approach recommended?
> 
> May be the solution to this is worth a sentence in "Wrtiting R Extensions".
> 
> Thanks for any help
> 
> 
> Jens Oehlschl?gel
> 
> 

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From p.dalgaard at biostat.ku.dk  Fri Oct 31 15:02:38 2003
From: p.dalgaard at biostat.ku.dk (Peter Dalgaard)
Date: 31 Oct 2003 15:02:38 +0100
Subject: [R] How to grow an R object from C (.Call Interface)
In-Reply-To: <31840.1067605072@www45.gmx.net>
References: <31840.1067605072@www45.gmx.net>
Message-ID: <x24qxpqzcx.fsf@biostat.ku.dk>

"Jens Oehlschl?gel" <joehl at gmx.de> writes:

> What is the best way to grow an R return object in writing a C function
> using the Rdefines.h macros.
> In my application, the final size of the return object is not known during
> construction. My understanding is that each time I grow an R object I have to
> use PROTECT() again, probably before UNPROTECTing the smaller version.
> However, due to the stack character of the PROTECT mechanism, UNPROTECT would not
> work to remove the smaller one, after the bigger has been protected. Is this
> an indication to use UNPROTECT_PTR ? Or is another approach recommended?
> 
> May be the solution to this is worth a sentence in "Wrtiting R Extensions".
> 
> Thanks for any help

It depends on the kind of object, I'll assume we're talking vector
objects here. If you're *extending* an existing object (the new object
becomes part of the old object) matters are quite different.

I think you are looking for the PROTECT_WITH_INDEX and REPROTECT
mechanism that Luke added (I almost said "recently", but it was
in fact three years ago!). If that is not in the manual, it should
be. 

UNPROTECT_PTR is older and solves a similar problem, but where the
routine that unprotects is not the same as the one that protects (this
happens a lot in the parser code) and you cannot be sure that it is
the top item that needs to be removed. UNPROTECT_PTR works by
searching the stack for the pointer, pulling the record out of the
stack, and dropping every stack element on top of it. In contrast
REPROTECT knows which record to extract and just changes the pointer
value in it. I.e. UNPROTECT_PTR is potentially much less efficient,
although I don't think there are practical cases where more than an
handful of items have been pushed on the stack (the usual cases are
zero or one).

Also remember not to call any routine that could trigger a garbage
collection while you need access to both the old and the new extended
object. So, allocate, copy, reprotect, and *then* start computing new
entries. 

-- 
   O__  ---- Peter Dalgaard             Blegdamsvej 3  
  c/ /'_ --- Dept. of Biostatistics     2200 Cph. N   
 (*) \(*) -- University of Copenhagen   Denmark      Ph: (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)             FAX: (+45) 35327907



From bates at stat.wisc.edu  Fri Oct 31 14:59:23 2003
From: bates at stat.wisc.edu (Douglas Bates)
Date: 31 Oct 2003 07:59:23 -0600
Subject: [R] cross-classified random factors in lme without blocking
In-Reply-To: <x2r80trbmv.fsf@biostat.ku.dk>
References: <5.2.1.1.1.20031031153622.00acecc8@imaphost.wehi.edu.au>
	<x2r80trbmv.fsf@biostat.ku.dk>
Message-ID: <6rbrrx33us.fsf@bates4.stat.wisc.edu>

Peter Dalgaard <p.dalgaard at biostat.ku.dk> writes:

> Gordon Smyth <smyth at wehi.edu.au> writes:
> 
> > On page 165 of Mixed-Effects Models in S and S-Plus by Pinheiro and
> > Bates there is an example of using lme() in the nlme package to fit a
> > model with crossed random factors. The example assumes though that the
> > data is grouped. Is it possible to use lme() to fit crossed random
> > factors when the data is not grouped?
> > 
> > E.g., y <- rnorm(12); a=gl(4,1,12); b=gl(3,4,12). Can I fit an
> > additive model like y~a+b but with a and b random?
> > 
> > Everything I've tried gives an error:
> > 
> >  > lme(y~1,random=~1|(a+b))
> > Error in switch(mode(object), name = , numeric = , call = object,
> > character = as.name(object),  :
> >          [[ cannot be of mode (
> >  > lme(y~1,random=~a+b)
> > Error in getGroups.data.frame(dataMix, groups) :
> >          Invalid formula for groups
> > 
> 
> A standard trick is to define a grouping with one level:
> 
> one <- rep(1,length(y)
> lme(...., random=~pdBlocked(.....)|one)
> 
> (Sorry, I'm a little rusty on the syntax, but just follow the example
> in P&B)
> 
> AFAIR, it also works with random=list(a=~1,one=~b) and vice versa.

Not sure about that.

> (The model is the same but you get different DF calculations, none of
> which are correct in the completely balanced case...)

I realize that it is awkward to use lme to fit models with crossed
random effects.  As Saikat DebRoy and I described in a recent preprint
        http://www.stat.wisc.edu/~bates/reports/MultiComp.pdf
we now have a good handle on the computational methods for
mixed-effects models with nested or crossed or partially crossed
random effects.

Both the nlme and the lme4 packages are based on structures that are
tuned to nested random effects and do not easily accomodate crossed
random effects.  I have a draft of the contents of classes and methods
for fitting linear mixed-effects models with nested or crossed or
... but it is a long way from the draft to working, tested code.
Although it will take some time to get all the pieces in place I do
offer some encouragement that this awkward phrasing of crossed random
effects will some day be behind us.



From p.dalgaard at biostat.ku.dk  Fri Oct 31 15:23:11 2003
From: p.dalgaard at biostat.ku.dk (Peter Dalgaard)
Date: 31 Oct 2003 15:23:11 +0100
Subject: [R] cross-classified random factors in lme without blocking
In-Reply-To: <6rbrrx33us.fsf@bates4.stat.wisc.edu>
References: <5.2.1.1.1.20031031153622.00acecc8@imaphost.wehi.edu.au>
	<x2r80trbmv.fsf@biostat.ku.dk> <6rbrrx33us.fsf@bates4.stat.wisc.edu>
Message-ID: <x2znfhpju8.fsf@biostat.ku.dk>

Douglas Bates <bates at stat.wisc.edu> writes:

> > (Sorry, I'm a little rusty on the syntax, but just follow the example
> > in P&B)
> > 
> > AFAIR, it also works with random=list(a=~1,one=~b) and vice versa.
> 
> Not sure about that.

Sorry. It's certainly not correct as written. It has to be something like

list(a=1,one=pdIdent(form=~b-1))
 
otherwise you get a general symmetric covariance for the effect of b.

> > (The model is the same but you get different DF calculations, none of
> > which are correct in the completely balanced case...)
> 
> I realize that it is awkward to use lme to fit models with crossed
> random effects.  As Saikat DebRoy and I described in a recent preprint
>         http://www.stat.wisc.edu/~bates/reports/MultiComp.pdf

.../MixedComp.pdf, right?

> we now have a good handle on the computational methods for
> mixed-effects models with nested or crossed or partially crossed
> random effects.
> 
> Both the nlme and the lme4 packages are based on structures that are
> tuned to nested random effects and do not easily accomodate crossed
> random effects.  I have a draft of the contents of classes and methods
> for fitting linear mixed-effects models with nested or crossed or
> ... but it is a long way from the draft to working, tested code.
> Although it will take some time to get all the pieces in place I do
> offer some encouragement that this awkward phrasing of crossed random
> effects will some day be behind us.

Looking forward to it... :-)

-- 
   O__  ---- Peter Dalgaard             Blegdamsvej 3  
  c/ /'_ --- Dept. of Biostatistics     2200 Cph. N   
 (*) \(*) -- University of Copenhagen   Denmark      Ph: (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)             FAX: (+45) 35327907



From bates at stat.wisc.edu  Fri Oct 31 15:24:23 2003
From: bates at stat.wisc.edu (Douglas Bates)
Date: 31 Oct 2003 08:24:23 -0600
Subject: [R] cross-classified random factors in lme without blocking
In-Reply-To: <6rbrrx33us.fsf@bates4.stat.wisc.edu>
References: <5.2.1.1.1.20031031153622.00acecc8@imaphost.wehi.edu.au>
	<x2r80trbmv.fsf@biostat.ku.dk> <6rbrrx33us.fsf@bates4.stat.wisc.edu>
Message-ID: <6rbrrx4h9k.fsf@bates4.stat.wisc.edu>

Douglas Bates <bates at cs.wisc.edu> writes:

...
> I realize that it is awkward to use lme to fit models with crossed
> random effects.  As Saikat DebRoy and I described in a recent preprint
>         http://www.stat.wisc.edu/~bates/reports/MultiComp.pdf
> we now have a good handle on the computational methods for
> mixed-effects models with nested or crossed or partially crossed
> random effects.

That URL should have been
        http://www.stat.wisc.edu/~bates/reports/MixedComp.pdf



From andy_liaw at merck.com  Fri Oct 31 15:54:25 2003
From: andy_liaw at merck.com (Liaw, Andy)
Date: Fri, 31 Oct 2003 09:54:25 -0500
Subject: [R] Re: packaging a package addon
Message-ID: <3A822319EB35174CA3714066D590DCD50205CD83@usrymx25.merck.com>

> From: Prof Brian Ripley [mailto:ripley at stats.ox.ac.uk] 
> 
> On Thu, 30 Oct 2003, Ross Boylan wrote:
[...]
> > Finally, a comment on R CMD check: perhaps it could produce 
> some more 
> > of the output when things fail?  I found that to diagnose 
> the loading 
> > problems as I developed this, I had to attempt to load the package 
> > myself in R to see what the actual problem was.  There 
> wasn't enough 
> > info in the R CMD check to tell what exactly the problem was.
> 
> I think there usually is, but you have to look in the log 
> file or one of 
> the example files.
> 
> But I would not be doing R CMD check until I had both installed and 
> loaded the package and run a few examples.

I often do what Ross does, just because it's easy to do...

Is it possible to get R to at least print the trackback on error when
checking the package, so there's a bit more info in the log file?

Best,
Andy


> -- 
> Brian D. Ripley,                  ripley at stats.ox.ac.uk
> Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
> University of Oxford,             Tel:  +44 1865 272861 (self)
> 1 South Parks Road,                     +44 1865 272866 (PA)
> Oxford OX1 3TG, UK                Fax:  +44 1865 272595
>



From CMiller at PICR.man.ac.uk  Fri Oct 31 15:50:39 2003
From: CMiller at PICR.man.ac.uk (Crispin Miller)
Date: Fri, 31 Oct 2003 14:50:39 -0000
Subject: [R] Creating packages in 1.8
Message-ID: <BAA35444B19AD940997ED02A6996AAE0B5C283@sanmail.picr.man.ac.uk>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20031031/4d181feb/attachment.pl

From ryszard.czerminski at pharma.novartis.com  Fri Oct 31 16:47:36 2003
From: ryszard.czerminski at pharma.novartis.com (ryszard.czerminski@pharma.novartis.com)
Date: Fri, 31 Oct 2003 10:47:36 -0500
Subject: [R] print(), cat() and simple I/O in R
Message-ID: <OF9147DAC7.BD016ACD-ON85256DD0.0054B412-85256DD0.0056D61D@EU.novartis.net>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20031031/e09e03d6/attachment.pl

From spencer.graves at pdf.com  Fri Oct 31 17:06:43 2003
From: spencer.graves at pdf.com (Spencer Graves)
Date: Fri, 31 Oct 2003 08:06:43 -0800
Subject: [R] print(), cat() and simple I/O in R
In-Reply-To: <OF9147DAC7.BD016ACD-ON85256DD0.0054B412-85256DD0.0056D61D@EU.novartis.net>
References: <OF9147DAC7.BD016ACD-ON85256DD0.0054B412-85256DD0.0056D61D@EU.novartis.net>
Message-ID: <3FA28893.3000201@pdf.com>

Have you considered "round" and "paste"? 

hope this helps.  spencer graves

ryszard.czerminski at pharma.novartis.com wrote:

>I am trying to produce rather mundane output of the form e.g.
>
>pi, e = 3.14   2.718
>
>The closest result I achieved so far with print() is:
>
>  
>
>>print (c(pi, exp(1)), digits = 3)
>>    
>>
>[1] 3.14 2.72
>
>  
>
>>print(c("pi, e =", pi, exp(1)), digits = 3)
>>    
>>
>[1] "pi, e ="          "3.14159265358979" "2.71828182845905"
>I understand that c() promotes floats to strings and this is why I get 
>what I get.
>
>and with cat() (it apparently does not have equivalent of digits" 
>parameter)
>  
>
>>cat ("pi, e =", pi, exp(1), "\n")
>>    
>>
>pi, e = 3.141593 2.718282
>
>Any pointers with respect how can I print what I want to print would be 
>greatly appreciated.
>
>
>Ryszard
>	[[alternative HTML version deleted]]
>
>______________________________________________
>R-help at stat.math.ethz.ch mailing list
>https://www.stat.math.ethz.ch/mailman/listinfo/r-help
>  
>



From hb at maths.lth.se  Fri Oct 31 17:13:29 2003
From: hb at maths.lth.se (Henrik Bengtsson)
Date: Fri, 31 Oct 2003 17:13:29 +0100
Subject: [R] print(), cat() and simple I/O in R
In-Reply-To: <OF9147DAC7.BD016ACD-ON85256DD0.0054B412-85256DD0.0056D61D@EU.novartis.net>
Message-ID: <001001c39fc9$ed434ec0$e502eb82@maths.lth.se>

See format(), formatC() and sprintf().

/Henrik

> -----Original Message-----
> From: r-help-bounces at stat.math.ethz.ch 
> [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of 
> ryszard.czerminski at pharma.novartis.com
> Sent: den 31 oktober 2003 16:48
> To: r-help at stat.math.ethz.ch
> Subject: [R] print(), cat() and simple I/O in R
> 
> 
> I am trying to produce rather mundane output of the form e.g.
> 
> pi, e = 3.14   2.718
> 
> The closest result I achieved so far with print() is:
> 
> > print (c(pi, exp(1)), digits = 3)
> [1] 3.14 2.72
> 
> > print(c("pi, e =", pi, exp(1)), digits = 3)
> [1] "pi, e ="          "3.14159265358979" "2.71828182845905"
> I understand that c() promotes floats to strings and this is 
> why I get 
> what I get.
> 
> and with cat() (it apparently does not have equivalent of digits" 
> parameter)
> > cat ("pi, e =", pi, exp(1), "\n")
> pi, e = 3.141593 2.718282
> 
> Any pointers with respect how can I print what I want to 
> print would be 
> greatly appreciated.
> 
> 
> Ryszard
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list 
> https://www.stat.math.ethz.ch/mailma> n/listinfo/r-help
> 
>



From GPetris at uark.edu  Fri Oct 31 17:13:53 2003
From: GPetris at uark.edu (Giovanni Petris)
Date: Fri, 31 Oct 2003 10:13:53 -0600 (CST)
Subject: [R] print(), cat() and simple I/O in R
In-Reply-To: <OF9147DAC7.BD016ACD-ON85256DD0.0054B412-85256DD0.0056D61D@EU.novartis.net>
	(ryszard.czerminski@pharma.novartis.com)
References: <OF9147DAC7.BD016ACD-ON85256DD0.0054B412-85256DD0.0056D61D@EU.novartis.net>
Message-ID: <200310311613.h9VGDrTV028407@definetti.uark.edu>


?format
?formatC

> Date: Fri, 31 Oct 2003 10:47:36 -0500
> From: ryszard.czerminski at pharma.novartis.com
> Sender: r-help-bounces at stat.math.ethz.ch
> Precedence: list
> 
> I am trying to produce rather mundane output of the form e.g.
> 
> pi, e = 3.14   2.718
> 
> The closest result I achieved so far with print() is:
> 
> > print (c(pi, exp(1)), digits = 3)
> [1] 3.14 2.72
> 
> > print(c("pi, e =", pi, exp(1)), digits = 3)
> [1] "pi, e ="          "3.14159265358979" "2.71828182845905"
> I understand that c() promotes floats to strings and this is why I get 
> what I get.
> 
> and with cat() (it apparently does not have equivalent of digits" 
> parameter)
> > cat ("pi, e =", pi, exp(1), "\n")
> pi, e = 3.141593 2.718282
> 
> Any pointers with respect how can I print what I want to print would be 
> greatly appreciated.
> 
> 
> Ryszard
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> 
> 
> 

-- 

 __________________________________________________
[                                                  ]
[ Giovanni Petris                 GPetris at uark.edu ]
[ Department of Mathematical Sciences              ]
[ University of Arkansas - Fayetteville, AR 72701  ]
[ Ph: (479) 575-6324, 575-8630 (fax)               ]
[ http://definetti.uark.edu/~gpetris/              ]
[__________________________________________________]



From bill.shipley at usherbrooke.ca  Fri Oct 31 17:27:17 2003
From: bill.shipley at usherbrooke.ca (Bill Shipley)
Date: Fri, 31 Oct 2003 11:27:17 -0500
Subject: [R] constrained nonlinear optimisation in R?
Message-ID: <004001c39fcb$d9a766b0$8d1ad284@BIO041>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20031031/2eb842d0/attachment.pl

From olefc at daimi.au.dk  Fri Oct 31 17:36:08 2003
From: olefc at daimi.au.dk (Ole F. Christensen)
Date: Fri, 31 Oct 2003 17:36:08 +0100
Subject: [R] Change in 'solve' for r-patched
Message-ID: <3FA28F78.3050906@daimi.au.dk>

Dear R help

Thanks to Professor Bates for his information about how to calculate 
least square estimates [not using solve] in ``the right way''.
This is very useful indead, I am clearly one of of the package 
maintainers who is not using using solve in a proper way at the moment.
However, the calculations in my code look more like GLS than LS.

## GLS could in principlpe be implemented like this :
betahat <- solve(t(X) %*% solve(Omega)%*% X) %*% t(X)%*%solve(Omega)%*% y
## where Omega is a strictly p.d. symmetric matrix

Does someone have a recommendation on how to do this in ``the right way'' ?

My first attempt (trying to imitate the LS solution recommended by Prof. Bates) is :

temp <- backsolve(chol(Omega),cbind(X,y))
betahat <- qr.coef(qr(temp[,1:ncol(X)]), temp[,ncol(X)+1])



Thank you in advance for any help


Cheers Ole

-- 
Ole F. Christensen
Center for Bioinformatik
Datalogisk Institut
Aarhus Universitet
Ny Munkegade, Bygning 540
8000 Aarhus C
Denmark



From simon at stats.gla.ac.uk  Fri Oct 31 17:49:13 2003
From: simon at stats.gla.ac.uk (Simon Wood)
Date: Fri, 31 Oct 2003 16:49:13 +0000 (GMT)
Subject: [R] constrained nonlinear optimisation in R?
In-Reply-To: <004001c39fcb$d9a766b0$8d1ad284@BIO041>
References: <004001c39fcb$d9a766b0$8d1ad284@BIO041>
Message-ID: <Pine.SOL.4.58.0310311646150.21346@moon.stats.gla.ac.uk>

> Hello.  I have searched the archives but have not found anything.  I
> need to solve a constrained optimisation problem for a nonlinear
> function (maximum entropy formalism).  Specifically,
>
> Optimise: -1*SUM(p_ilog(p_i)) for a vector p_i of probabilities,
> conditional on a series of constraints of the form:
>
> SUM(T_i*p_i)=k_i  for given values of T_i and k_i  (these are
> constraints on expectations).
>
A better answer may exist to this question, but here goes anyway....
Could you use sequential quaratic programming here (i.e. just constrain
the QP problem generated at each iterate of Newton's method)? There's an R
library for quadratic programming....

Simon

_____________________________________________________________________
> Simon Wood simon at stats.gla.ac.uk        www.stats.gla.ac.uk/~simon/
>>  Department of Statistics, University of Glasgow, Glasgow, G12 8QQ
>>>   Direct telephone: (0)141 330 4530          Fax: (0)141 330 4814



From spencer.graves at pdf.com  Fri Oct 31 18:10:49 2003
From: spencer.graves at pdf.com (Spencer Graves)
Date: Fri, 31 Oct 2003 09:10:49 -0800
Subject: [R] constrained nonlinear optimisation in R?
In-Reply-To: <Pine.SOL.4.58.0310311646150.21346@moon.stats.gla.ac.uk>
References: <004001c39fcb$d9a766b0$8d1ad284@BIO041>
	<Pine.SOL.4.58.0310311646150.21346@moon.stats.gla.ac.uk>
Message-ID: <3FA29799.4010709@pdf.com>

      Other alternatives to the R library for quadratic programming: 

      1.  What are the nature of your constraints?  "optim" will 
optimize a function with optional box constraints.  "constrOptim" will 
optimize a function subject to linear inequality constraints. 

      2.  If you want to estimate the p[i]'s, i = 1, ..., k, I would 
recommend a multivariate logistic transformation to (k-1) unconstrained 
variables.  I have had serious difficulties with constrained optimizers 
testing values outside the constraints and then stopping because the 
objective function misbehaved.  I don't know if "optim" does this, but I 
don't even try constrained optimization if I can find a sensible, 
unconstrained parameterization.  Often, confidence regions, etc., are 
better behaved in the unconstrained space as well. 

hope this helps.  spencer graves


Simon Wood wrote:

>>Hello.  I have searched the archives but have not found anything.  I
>>need to solve a constrained optimisation problem for a nonlinear
>>function (?maximum entropy formalism?).  Specifically,
>>
>>Optimise: -1*SUM(p_ilog(p_i)) for a vector p_i of probabilities,
>>conditional on a series of constraints of the form:
>>
>>SUM(T_i*p_i)=k_i  for given values of T_i and k_i  (these are
>>constraints on expectations).
>>
>>    
>>
>A better answer may exist to this question, but here goes anyway....
>Could you use sequential quaratic programming here (i.e. just constrain
>the QP problem generated at each iterate of Newton's method)? There's an R
>library for quadratic programming....
>
>Simon
>
>_____________________________________________________________________
>  
>
>>Simon Wood simon at stats.gla.ac.uk        www.stats.gla.ac.uk/~simon/
>>    
>>
>>> Department of Statistics, University of Glasgow, Glasgow, G12 8QQ
>>>      
>>>
>>>>  Direct telephone: (0)141 330 4530          Fax: (0)141 330 4814
>>>>        
>>>>
>
>______________________________________________
>R-help at stat.math.ethz.ch mailing list
>https://www.stat.math.ethz.ch/mailman/listinfo/r-help
>  
>



From CMiller at PICR.man.ac.uk  Fri Oct 31 17:57:59 2003
From: CMiller at PICR.man.ac.uk (Crispin Miller)
Date: Fri, 31 Oct 2003 16:57:59 -0000
Subject: [R] Creating packages in 1.8
Message-ID: <BAA35444B19AD940997ED02A6996AAE00B14D4@sanmail.picr.man.ac.uk>

Hi,
Firstly, sorry to reply to my own posting... Also for not having done
more trawling before I sent the original message...

The problems we are having were down to R being installed in a different
directory to the default one (for various sysadmin reasons)...

We've successfully got R runnning on a single machine in the default
directories where it likes to be - it is happy, and 'R CMD check' works
fine... :-)

Alas, this is not a good long-term solution for us can anyone help us
work out the easiest way to get R installed in another directory
structure instead of  the default ones? 

I appreciate that very similar questions have been asked before, but I
suspect that 1.8 is slightly different (since R CMD check ... worked
fine for us in 1.7.1).


Crispin

> -----Original Message-----
> From: Crispin Miller 
> Sent: 31 October 2003 14:51
> To: R-help at stat.math.ethz.ch
> Subject: [R] Creating packages in 1.8
> 
> 
> Hi,
> I decided to upgrade to 1.8 today... :-)
> Anyway, we are writing our own package that is dependent on a 
> bioconductor library - 'affy'. I've checked and when I fire 
> up R, library(affy) behaves as expected... so it all seems to 
> be installed and OK...
> 
> In the DESCRIPTION file in my package source I have the line:
> 
> Depends: affy
> 
> When I run R CMD check simpleaffy
> 
> I get to:
> 
> ...
> * checking package dependencies ... ERROR
> Packages required but not available:
>   WARNING: ignoring environment value of R_HOME
> Prompt >
> 
> Any ideas what is going on - as far as I can see the only 
> dependency is to affy which is there and OK... I get no list 
> of packages that are missing :-(
> 
> I'm assuming the warning comes because I have R_ENVIRON 
> pointing to a .Renviron file in my home directory...
> 
> Any help would be much appreciated!
> Cheers,
> Crispin
>  
> --------------------------------------------------------
> 
>  
> This email is confidential and intended solely for the use 
> o...{{dropped}}
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list 
> https://www.stat.math.ethz.ch/mailman/listinfo> /r-help
>
 
--------------------------------------------------------

 
This email is confidential and intended solely for the use o...{{dropped}}



From ripley at stats.ox.ac.uk  Fri Oct 31 18:23:38 2003
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Fri, 31 Oct 2003 17:23:38 +0000 (GMT)
Subject: [R] Change in 'solve' for r-patched
In-Reply-To: <3FA28F78.3050906@daimi.au.dk>
Message-ID: <Pine.LNX.4.44.0310311719130.19760-100000@gannet.stats>

GLS is usually solved by taking a matrix square root and converting to 
least squares (and BTW you might want to use lm.fit not solve for least 
squares to allow for aliased columns).

So your idea is right (and is the one given in my 1981 Spatial Statistics 
book).  These days I would usually use an eigendecomposition instead of 
Cholesky as it will enable you to cope better with nearly 
non-positive-definite Omega.  See lm.gls in package MASS for an outline 
implementation.

On Fri, 31 Oct 2003, Ole F. Christensen wrote:

> Dear R help
> 
> Thanks to Professor Bates for his information about how to calculate 
> least square estimates [not using solve] in ``the right way''.
> This is very useful indead, I am clearly one of of the package 
> maintainers who is not using using solve in a proper way at the moment.
> However, the calculations in my code look more like GLS than LS.
> 
> ## GLS could in principlpe be implemented like this :
> betahat <- solve(t(X) %*% solve(Omega)%*% X) %*% t(X)%*%solve(Omega)%*% y
> ## where Omega is a strictly p.d. symmetric matrix
> 
> Does someone have a recommendation on how to do this in ``the right way'' ?
> 
> My first attempt (trying to imitate the LS solution recommended by Prof. Bates) is :
> 
> temp <- backsolve(chol(Omega),cbind(X,y))
> betahat <- qr.coef(qr(temp[,1:ncol(X)]), temp[,ncol(X)+1])
> 
> 
> 
> Thank you in advance for any help
> 
> 
> Cheers Ole
> 
> 

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From rossini at blindglobe.net  Fri Oct 31 18:22:49 2003
From: rossini at blindglobe.net (A.J. Rossini)
Date: Fri, 31 Oct 2003 09:22:49 -0800
Subject: [R] constrained nonlinear optimisation in R?
In-Reply-To: <Pine.SOL.4.58.0310311646150.21346@moon.stats.gla.ac.uk> (Simon
	Wood's message of "Fri, 31 Oct 2003 16:49:13 +0000 (GMT)")
References: <004001c39fcb$d9a766b0$8d1ad284@BIO041>
	<Pine.SOL.4.58.0310311646150.21346@moon.stats.gla.ac.uk>
Message-ID: <85znfhe2za.fsf@blindglobe.net>

Simon Wood <simon at stats.gla.ac.uk> writes:

>> Hello.  I have searched the archives but have not found anything.  I
>> need to solve a constrained optimisation problem for a nonlinear
>> function (?maximum entropy formalism?).  Specifically,
>>
>> Optimise: -1*SUM(p_ilog(p_i)) for a vector p_i of probabilities,
>> conditional on a series of constraints of the form:
>>
>> SUM(T_i*p_i)=k_i  for given values of T_i and k_i  (these are
>> constraints on expectations).
>>
> A better answer may exist to this question, but here goes anyway....
> Could you use sequential quaratic programming here (i.e. just constrain
> the QP problem generated at each iterate of Newton's method)? There's an R
> library for quadratic programming....
>
> Simon
>
> _____________________________________________________________________
>> Simon Wood simon at stats.gla.ac.uk        www.stats.gla.ac.uk/~simon/
>>>  Department of Statistics, University of Glasgow, Glasgow, G12 8QQ
>>>>   Direct telephone: (0)141 330 4530          Fax: (0)141 330 4814
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
>

help.search("constrained") suggests:

constrOptim(base)       Linearly constrained optimisation

which might do the trick.


-- 
rossini at u.washington.edu            http://www.analytics.washington.edu/ 
Biomedical and Health Informatics   University of Washington
Biostatistics, SCHARP/HVTN          Fred Hutchinson Cancer Research Center
UW (Tu/Th/F): 206-616-7630 FAX=206-543-3461 | Voicemail is unreliable
FHCRC  (M/W): 206-667-7025 FAX=206-667-4812 | use Email

CONFIDENTIALITY NOTICE: This e-mail message and any attachme...{{dropped}}



From sfalcon at fhcrc.org  Fri Oct 31 18:27:09 2003
From: sfalcon at fhcrc.org (Seth Falcon)
Date: Fri, 31 Oct 2003 09:27:09 -0800
Subject: [R] Creating packages in 1.8
In-Reply-To: <BAA35444B19AD940997ED02A6996AAE0B5C283@sanmail.picr.man.ac.uk>
References: <BAA35444B19AD940997ED02A6996AAE0B5C283@sanmail.picr.man.ac.uk>
Message-ID: <20031031172703.GF20304@queenbee.fhcrc.org>

Does R CMD check succeed if you unset R_HOME (or unset R_ENVIRON?).

The R shell wrapper script writes the warning message to STDOUT if
R_HOME is set to something other the setting hard-coded into the
wrapper.

This can be a problem if other scripts are depending upon the output of
R CMD <BLAH> stuff.  For example, I could not build RPy until I unset
R_HOME b/c RPy's build script relies upon the output of R CMD HOME and
it didn't like "WARNING: ..." as an include path ;-)

+ seth

On Fri, Oct 31, 2003 at 02:50:39PM -0000, Crispin Miller wrote:
> Hi,
> I decided to upgrade to 1.8 today... :-)
> Anyway, we are writing our own package that is dependent on a
> bioconductor library - 'affy'.
> I've checked and when I fire up R, library(affy) behaves as expected...
> so it all seems to be installed and OK...
> 
> In the DESCRIPTION file in my package source I have the line:
> 
> Depends: affy
> 
> When I run R CMD check simpleaffy
> 
> I get to:
> 
> ...
> * checking package dependencies ... ERROR
> Packages required but not available:
>   WARNING: ignoring environment value of R_HOME
> Prompt >
> 
> Any ideas what is going on - as far as I can see the only dependency is
> to affy which is there and OK... I get no list of packages that are
> missing :-(
> 
> I'm assuming the warning comes because I have R_ENVIRON pointing to a
> .Renviron file in my home directory...
> 
> Any help would be much appreciated!
> Cheers,
> Crispin
>  
> --------------------------------------------------------
> 
>  
> This email is confidential and intended solely for the use o...{{dropped}}
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help



From ripley at stats.ox.ac.uk  Fri Oct 31 18:33:18 2003
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Fri, 31 Oct 2003 17:33:18 +0000 (GMT)
Subject: [R] Creating packages in 1.8
In-Reply-To: <BAA35444B19AD940997ED02A6996AAE00B14D4@sanmail.picr.man.ac.uk>
Message-ID: <Pine.LNX.4.44.0310311731090.19760-100000@gannet.stats>

On what OS?

It's easy on Windows: choose the directory in the installer.

It's easy for a source build on Unix: set --prefix at configure time.

It's easy for rpms if you know how.

I suspect it is easy for .debs.

And there is no R 1.8: it is R 1.8.0.

On Fri, 31 Oct 2003, Crispin Miller wrote:

> Hi,
> Firstly, sorry to reply to my own posting... Also for not having done
> more trawling before I sent the original message...
> 
> The problems we are having were down to R being installed in a different
> directory to the default one (for various sysadmin reasons)...
> 
> We've successfully got R runnning on a single machine in the default
> directories where it likes to be - it is happy, and 'R CMD check' works
> fine... :-)
> 
> Alas, this is not a good long-term solution for us can anyone help us
> work out the easiest way to get R installed in another directory
> structure instead of  the default ones? 
> 
> I appreciate that very similar questions have been asked before, but I
> suspect that 1.8 is slightly different (since R CMD check ... worked
> fine for us in 1.7.1).
> 
> 
> Crispin
> 
> > -----Original Message-----
> > From: Crispin Miller 
> > Sent: 31 October 2003 14:51
> > To: R-help at stat.math.ethz.ch
> > Subject: [R] Creating packages in 1.8
> > 
> > 
> > Hi,
> > I decided to upgrade to 1.8 today... :-)
> > Anyway, we are writing our own package that is dependent on a 
> > bioconductor library - 'affy'. I've checked and when I fire 
> > up R, library(affy) behaves as expected... so it all seems to 
> > be installed and OK...
> > 
> > In the DESCRIPTION file in my package source I have the line:
> > 
> > Depends: affy
> > 
> > When I run R CMD check simpleaffy
> > 
> > I get to:
> > 
> > ...
> > * checking package dependencies ... ERROR
> > Packages required but not available:
> >   WARNING: ignoring environment value of R_HOME
> > Prompt >
> > 
> > Any ideas what is going on - as far as I can see the only 
> > dependency is to affy which is there and OK... I get no list 
> > of packages that are missing :-(
> > 
> > I'm assuming the warning comes because I have R_ENVIRON 
> > pointing to a .Renviron file in my home directory...
> > 
> > Any help would be much appreciated!
> > Cheers,
> > Crispin
> >  
> > --------------------------------------------------------
> > 
> >  
> > This email is confidential and intended solely for the use 
> > o...{{dropped}}
> > 
> > ______________________________________________
> > R-help at stat.math.ethz.ch mailing list 
> > https://www.stat.math.ethz.ch/mailman/listinfo> /r-help
> >
>  
> --------------------------------------------------------
> 
>  
> This email is confidential and intended solely for the use o...{{dropped}}
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> 
> 

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From markus.helbig at gmx.net  Fri Oct 31 19:48:47 2003
From: markus.helbig at gmx.net (Markus Helbig)
Date: Fri, 31 Oct 2003 19:48:47 +0100
Subject: [R] Fatal error in SJava.
Message-ID: <000601c39fdf$a0c19370$0408a8c0@alf>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20031031/05e3550a/attachment.pl

From canty at math.mcmaster.ca  Fri Oct 31 20:16:31 2003
From: canty at math.mcmaster.ca (Angelo Canty)
Date: Fri, 31 Oct 2003 14:16:31 -0500
Subject: [R] Summing elements in a list
Message-ID: <3FA2B50F.C9090E07@icarus.math.mcmaster.ca>

Hi,

Suppose that I have a list where each component is a list of two
matrices.  I also have a vector of weights.  How can I collapse my
list of lists into a single list of two matrices where each matrix
in the result is the weighted sum of the corresponding matrices.

I could use a loop but this is a nested calculation so I was hoping
there is a more efficient way to do this.  To help clarify, here is
the code I would use with a for loop

result <- list(mat1=matrix(0,nrow1,ncol1),
               mat2=matrix(0,nrow2,ncol2))
for (i in seq(along=matlist)) {
   result$mat1 <- result$mat1+w[i]*matlist[[i]]$mat1
   result$mat2 <- result$mat2+w[i]*matlist[[i]]$mat2
}

I apologise if this is a trivial question.  Unfortunately I don't have
my copy of V&R S Programming to hand.

Thanks for your help,
Angelo
-- 
------------------------------------------------------------------
|   Angelo J. Canty                Email: cantya at mcmaster.ca     |
|   Mathematics and Statistics     Phone: (905) 525-9140 x 27079 |
|   McMaster University            Fax  : (905) 522-0935         |
|   1280 Main St. W.                                             |
|   Hamilton ON L8S 4K1                                          |



From marc.m.belisle at usherbrooke.ca  Fri Oct 31 20:40:15 2003
From: marc.m.belisle at usherbrooke.ca (Marc Belisle)
Date: Fri, 31 Oct 2003 14:40:15 -0500
Subject: [R] dnorm() lead to a probability >1
Message-ID: <APEJLJBDGGLPABONJPDMMEOLCAAA.marc.m.belisle@usherbrooke.ca>

Howdee,

One of my student spotted something I can't explain: a probability >1 vs a
normal probability density function.

> dnorm(x=1, mean=1, sd=0.4)
[1] 0.9973557

> dnorm(x=1, mean=1, sd=0.39)
[1] 1.022929

> dnorm(x=1, mean=1, sd=0.3)
[1] 1.329808

> dnorm(x=1, mean=1, sd=0.1)
[1] 3.989423

> dnorm(x=1, mean=1, sd=0.01)
[1] 39.89423

> dnorm(x=1, mean=1, sd=0.001)
[1] 398.9423

Is there a bug with the algorithm?

Thanks,

Marc

========================
Marc B?lisle
Professeur adjoint
D?partement de biologie
Universit? de Sherbrooke
2500 boul. de l'Universit?
Sherbrooke, Qu?bec
J1K 2R1 CANADA

T?l: +1-819-821-8000 poste 1313
Fax: +1-819-821-8049
Courri?l: Marc.M.Belisle at USherbrooke.ca
Site Web:
www.usherbrooke.ca/biologie/recherche/ecologie/Belisle/belisle.html



From jun at galton.uchicago.edu  Fri Oct 31 20:34:07 2003
From: jun at galton.uchicago.edu (Mikyoung Jun)
Date: Fri, 31 Oct 2003 13:34:07 -0600 (CST)
Subject: [R] question about optim
Message-ID: <Pine.LNX.4.44.0310311329470.816-100000@pelham.uchicago.edu>

Hello,

When we use optim and run into errors, such as generates NA/NaN/Inf, and 
the routine stops because of this error, is there a way to print the 
values of the argument of the functions where the error occurs? 
For example, I am doing minimizing negative log likelihood and when the 
optim stops because of those errors, I'd like to know what was the 
parameter values at that time. I heard nlm has something called 
"print.level" and I am wondering there is a way for optim as well. Thank 
you.

Mikyoung Jun



From edd at debian.org  Fri Oct 31 20:36:50 2003
From: edd at debian.org (Dirk Eddelbuettel)
Date: Fri, 31 Oct 2003 13:36:50 -0600
Subject: [R] dnorm() lead to a probability >1
In-Reply-To: <APEJLJBDGGLPABONJPDMMEOLCAAA.marc.m.belisle@usherbrooke.ca>
References: <APEJLJBDGGLPABONJPDMMEOLCAAA.marc.m.belisle@usherbrooke.ca>
Message-ID: <20031031193650.GA13363@sonny.eddelbuettel.com>

On Fri, Oct 31, 2003 at 02:40:15PM -0500, Marc Belisle wrote:
> Howdee,
> 
> One of my student spotted something I can't explain: a probability >1 vs a
> normal probability density function.

The integral has to be 1 --- but dnorm doesn't compute that. 

You were probably looking for pnorm(), and it will give you 0.5 for all
those cases (where x==mean) as you'd expect.

Hth,  Dirk


> > dnorm(x=1, mean=1, sd=0.4)
> [1] 0.9973557
> 
> > dnorm(x=1, mean=1, sd=0.39)
> [1] 1.022929
> 
> > dnorm(x=1, mean=1, sd=0.3)
> [1] 1.329808
> 
> > dnorm(x=1, mean=1, sd=0.1)
> [1] 3.989423
> 
> > dnorm(x=1, mean=1, sd=0.01)
> [1] 39.89423
> 
> > dnorm(x=1, mean=1, sd=0.001)
> [1] 398.9423
> 
> Is there a bug with the algorithm?
> 
> Thanks,
> 
> Marc
> 
> ========================
> Marc B?lisle
> Professeur adjoint
> D?partement de biologie
> Universit? de Sherbrooke
> 2500 boul. de l'Universit?
> Sherbrooke, Qu?bec
> J1K 2R1 CANADA
> 
> T?l: +1-819-821-8000 poste 1313
> Fax: +1-819-821-8049
> Courri?l: Marc.M.Belisle at USherbrooke.ca
> Site Web:
> www.usherbrooke.ca/biologie/recherche/ecologie/Belisle/belisle.html
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> 

-- 
Those are my principles, and if you don't like them... well, I have others.
                                                -- Groucho Marx



From ryszard.czerminski at pharma.novartis.com  Fri Oct 31 20:39:17 2003
From: ryszard.czerminski at pharma.novartis.com (ryszard.czerminski@pharma.novartis.com)
Date: Fri, 31 Oct 2003 14:39:17 -0500
Subject: [R] strange sprintf() behaviour ?
Message-ID: <OF0852459C.E2C634EF-ON85256DD0.006A067D-85256DD0.006C0C72@EU.novartis.net>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20031031/91d0bec7/attachment.pl

From bates at stat.wisc.edu  Fri Oct 31 20:41:26 2003
From: bates at stat.wisc.edu (Douglas Bates)
Date: 31 Oct 2003 13:41:26 -0600
Subject: [R] dnorm() lead to a probability >1
In-Reply-To: <APEJLJBDGGLPABONJPDMMEOLCAAA.marc.m.belisle@usherbrooke.ca>
References: <APEJLJBDGGLPABONJPDMMEOLCAAA.marc.m.belisle@usherbrooke.ca>
Message-ID: <6r8yn12o0p.fsf@bates4.stat.wisc.edu>

"Marc Belisle" <marc.m.belisle at usherbrooke.ca> writes:

> One of my student spotted something I can't explain: a probability >1 vs a
> normal probability density function.
> 
> > dnorm(x=1, mean=1, sd=0.4)
> [1] 0.9973557
> 
> > dnorm(x=1, mean=1, sd=0.39)
> [1] 1.022929
> 
> > dnorm(x=1, mean=1, sd=0.3)
> [1] 1.329808
> 
> > dnorm(x=1, mean=1, sd=0.1)
> [1] 3.989423
> 
> > dnorm(x=1, mean=1, sd=0.01)
> [1] 39.89423
> 
> > dnorm(x=1, mean=1, sd=0.001)
> [1] 398.9423
> 
> Is there a bug with the algorithm?

No.  dnorm does not return probabilities - it returns a probability
density which can be greater than 1.



From rpeng at jhsph.edu  Fri Oct 31 20:43:13 2003
From: rpeng at jhsph.edu (Roger D. Peng)
Date: Fri, 31 Oct 2003 14:43:13 -0500
Subject: [R] question about optim
In-Reply-To: <Pine.LNX.4.44.0310311329470.816-100000@pelham.uchicago.edu>
References: <Pine.LNX.4.44.0310311329470.816-100000@pelham.uchicago.edu>
Message-ID: <3FA2BB51.50504@jhsph.edu>

You might want to try using options(error = recover) or perhaps 
options(error = browser).

-roger

Mikyoung Jun wrote:
> Hello,
> 
> When we use optim and run into errors, such as generates NA/NaN/Inf, and 
> the routine stops because of this error, is there a way to print the 
> values of the argument of the functions where the error occurs? 
> For example, I am doing minimizing negative log likelihood and when the 
> optim stops because of those errors, I'd like to know what was the 
> parameter values at that time. I heard nlm has something called 
> "print.level" and I am wondering there is a way for optim as well. Thank 
> you.
> 
> Mikyoung Jun
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
>



From RBaskin at ahrq.gov  Fri Oct 31 20:43:32 2003
From: RBaskin at ahrq.gov (RBaskin@ahrq.gov)
Date: Fri, 31 Oct 2003 14:43:32 -0500
Subject: [R] dnorm() lead to a probability >1
Message-ID: <3598558AD728D41183350008C7CF291C0F16B985@exchange1.ahrq.gov>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20031031/3898d03f/attachment.pl

From Williams.Elliot at bls.gov  Fri Oct 31 20:50:10 2003
From: Williams.Elliot at bls.gov (Williams, Elliot - BLS)
Date: Fri, 31 Oct 2003 14:50:10 -0500
Subject: [R] Change in 'solve' for r-patched
Message-ID: <70E1C0DB4F9B5E4F9CEDB8433F4A68B90306056A@PSBMAIL2>

Hi all,

Thanks Douglas for bringing up numerical stability and OLS regression
coefficients.  I'm often worried about the speed of regression runs, as I
sometimes run large simulations or resamplings.  Inspired, and prone to run
many regressions, I did a little simulation as follows.  XP results are on a
2.4Ghz Pentium 4 in Windows from the binaries (without BLAS?) and Linux
results are on a 1.5Ghz Pentium M laptop with a Pentium Atlas BLAS.

Setting up the data: x is rnormals with a vector of ones prepended... 
> x <- cbind(rep(1,100), matrix(rnorm(1000), nc=10))
> beta0 <- runif(11)
> y <- x %*% beta0 + rnorm(100)

Estimating Slope Coefficients:

> system.time(for (i in 1:1000) lm( y~ x -1))
XP:  [1] 5.91 0.00 5.90   NA   NA
Linux: [1] 5.27 0.01 5.28 0.00 0.00

> system.time(for (i in 1:1000) solve(t(x) %*% x) %*% t(x) %*% y)
XP: [1] 0.64 0.01 0.65   NA   NA
Linux: [1] 0.51 0.01 0.57 0.00 0.00

> system.time(for (i in 1:1000) qr.coef(qr(x), y) )
XP: [1] 0.75 0.00 0.75   NA   NA
Linux: [1] 0.76 0.00 0.77 0.00 0.00

> system.time(for (i in 1:1000) solve(crossprod(x), crossprod(x,y)))
XP: [1] 0.45 0.00 0.53   NA   NA
Linux: [1] 0.35 0.00 0.36 0.00 0.00

On both platforms, BLAS or not, the solve(crossprod()) method works the
fastest, with the na?ve solve(t(x)%*%x) second-fastest.  

Calculating (t(x)%*%x)^-1:

> system.time(for (i in 1:1000) chol2inv(qr.R(qr(x))))
XP: [1] 0.44 0.00 0.44   NA   NA
Linux: [1] 0.40 0.00 0.40 0.00 0.00

> system.time(for (i in 1:1000) solve(crossprod(x)) )
XP: [1] 0.58 0.01 0.59   NA   NA
Linux: [1] 0.34 0.00 0.34 0.00 0.00

Where the chol2inv() method was faster in Windows but slower in Linux, which
I'm guessing is due to differential uses of BLAS functions.

None of these address the problem of numerical accuracy, which was the
thrust of Doug's comments.  Has anyone done a quick simulation to
investigate the stability of the solutions?  Is it small coefficients or
near-collinearity (or both) that one has to worry about?  

Are the solve(crossprod()) methods obviously unstable?  They surely do work
quickly!

Thanks again R-universe.  I've needed no other statistical software for the
last 2 years.  I hope to get around to contributing a package or two soon.

						Elliot.



From zeileis at ci.tuwien.ac.at  Fri Oct 31 20:52:55 2003
From: zeileis at ci.tuwien.ac.at (Achim Zeileis)
Date: Fri, 31 Oct 2003 20:52:55 +0100
Subject: [R] dnorm() lead to a probability >1
In-Reply-To: <APEJLJBDGGLPABONJPDMMEOLCAAA.marc.m.belisle@usherbrooke.ca>
References: <APEJLJBDGGLPABONJPDMMEOLCAAA.marc.m.belisle@usherbrooke.ca>
Message-ID: <200310311952.h9VJqtnX028650@thorin.ci.tuwien.ac.at>

On Friday 31 October 2003 20:40, Marc Belisle wrote:

> Howdee,
>
> One of my student spotted something I can't explain: a probability
> >1 vs a normal probability density function.
>
> > dnorm(x=1, mean=1, sd=0.4)
>
> [1] 0.9973557
>
> > dnorm(x=1, mean=1, sd=0.39)
>
> [1] 1.022929
>
> > dnorm(x=1, mean=1, sd=0.3)
>
> [1] 1.329808
>
> > dnorm(x=1, mean=1, sd=0.1)
>
> [1] 3.989423
>
> > dnorm(x=1, mean=1, sd=0.01)
>
> [1] 39.89423
>
> > dnorm(x=1, mean=1, sd=0.001)
>
> [1] 398.9423
>
> Is there a bug with the algorithm?

The *area* under the density curve corresponds to the probability in 
the corresponding interval...as you might have learned in a statistics 
course.
So it's perfeclty alright for a density function to exceed 1 if the 
area under the whole curve still equals one. Immediately obvious for
  curve(dunif(x, min = 0, max = 0.5))

hth,
Z

> Thanks,
>
> Marc
>
> ========================
> Marc B?lisle
> Professeur adjoint
> D?partement de biologie
> Universit? de Sherbrooke
> 2500 boul. de l'Universit?
> Sherbrooke, Qu?bec
> J1K 2R1 CANADA
>
> T?l: +1-819-821-8000 poste 1313
> Fax: +1-819-821-8049
> Courri?l: Marc.M.Belisle at USherbrooke.ca
> Site Web:
> www.usherbrooke.ca/biologie/recherche/ecologie/Belisle/belisle.html
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help



From GPetris at uark.edu  Fri Oct 31 21:05:39 2003
From: GPetris at uark.edu (Giovanni Petris)
Date: Fri, 31 Oct 2003 14:05:39 -0600 (CST)
Subject: [R] Summing elements in a list
In-Reply-To: <3FA2B50F.C9090E07@icarus.math.mcmaster.ca> (message from Angelo
	Canty on Fri, 31 Oct 2003 14:16:31 -0500)
References: <3FA2B50F.C9090E07@icarus.math.mcmaster.ca>
Message-ID: <200310312005.h9VK5dJe028725@definetti.uark.edu>


The following seems to give what you want.


tmp <- rep(w, each=nrow1*ncol1+nrow2*ncol2) * unlist(matlist)
tmp <- rowSums(matrix(tmp,nr=nrow1*ncol1+nrow2*ncol2))
mat1 <- tmp[1:(nrow1*ncol1)]; dim(mat1) <- c(nrow1,ncol1)
mat2 <- tmp[nrow1*ncol1+1:(nrow2*ncol2)]; dim(mat2) <- c(nrow2,ncol2)


Giovanni

-- 

 __________________________________________________
[                                                  ]
[ Giovanni Petris                 GPetris at uark.edu ]
[ Department of Mathematical Sciences              ]
[ University of Arkansas - Fayetteville, AR 72701  ]
[ Ph: (479) 575-6324, 575-8630 (fax)               ]
[ http://definetti.uark.edu/~gpetris/              ]
[__________________________________________________]

> Date: Fri, 31 Oct 2003 14:16:31 -0500
> From: Angelo Canty <canty at math.mcmaster.ca>
> Sender: r-help-bounces at stat.math.ethz.ch
> Organization: McMaster University
> Precedence: list
> 
> Hi,
> 
> Suppose that I have a list where each component is a list of two
> matrices.  I also have a vector of weights.  How can I collapse my
> list of lists into a single list of two matrices where each matrix
> in the result is the weighted sum of the corresponding matrices.
> 
> I could use a loop but this is a nested calculation so I was hoping
> there is a more efficient way to do this.  To help clarify, here is
> the code I would use with a for loop
> 
> result <- list(mat1=matrix(0,nrow1,ncol1),
>                mat2=matrix(0,nrow2,ncol2))
> for (i in seq(along=matlist)) {
>    result$mat1 <- result$mat1+w[i]*matlist[[i]]$mat1
>    result$mat2 <- result$mat2+w[i]*matlist[[i]]$mat2
> }
> 
> I apologise if this is a trivial question.  Unfortunately I don't have
> my copy of V&R S Programming to hand.
> 
> Thanks for your help,
> Angelo
> -- 
> ------------------------------------------------------------------
> |   Angelo J. Canty                Email: cantya at mcmaster.ca     |
> |   Mathematics and Statistics     Phone: (905) 525-9140 x 27079 |
> |   McMaster University            Fax  : (905) 522-0935         |
> |   1280 Main St. W.                                             |
> |   Hamilton ON L8S 4K1                                          |
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> 
> 
>



From ypeng at math.mun.ca  Fri Oct 31 21:29:56 2003
From: ypeng at math.mun.ca (Paul Y. Peng)
Date: Fri, 31 Oct 2003 16:59:56 -0330
Subject: [R] Summing elements in a list
In-Reply-To: <3FA2B50F.C9090E07@icarus.math.mcmaster.ca>
References: <3FA2B50F.C9090E07@icarus.math.mcmaster.ca>
Message-ID: <3FA2C644.1080405@math.mun.ca>

Angelo Canty wrote:
> Hi,
> 
> Suppose that I have a list where each component is a list of two
> matrices.  I also have a vector of weights.  How can I collapse my
> list of lists into a single list of two matrices where each matrix
> in the result is the weighted sum of the corresponding matrices.
> 
> I could use a loop but this is a nested calculation so I was hoping
> there is a more efficient way to do this.  To help clarify, here is
> the code I would use with a for loop
> 
> result <- list(mat1=matrix(0,nrow1,ncol1),
>                mat2=matrix(0,nrow2,ncol2))
> for (i in seq(along=matlist)) {
>    result$mat1 <- result$mat1+w[i]*matlist[[i]]$mat1
>    result$mat2 <- result$mat2+w[i]*matlist[[i]]$mat2
> }
> 
> I apologise if this is a trivial question.  Unfortunately I don't have
> my copy of V&R S Programming to hand.

Here is one possibility:

result <- list(
   mat1 = matrix(rowSums(sapply(matlist, function(x)x$mat1) %*% diag(w)), nrow1, ncol1)
   mat2 = matrix(rowSums(sapply(matlist, function(x)x$mat2) %*% diag(w)), nrow2, ncol2)
)

Warning: It doesn't have the readability that the original code has though.

Paul.
-- 
--------------------------------------------------------------------------
   Dr. Paul Y. Peng, Associate Professor            Phone: (709) 737 8080
   Department of Mathematics and Statistics           Fax: (709) 737 3010
   Memorial University of Newfoundland          E-mail: ypeng at math.mun.ca
   St. John's, NL A1C 5S7, Canada             Web: www.math.mun.ca/~ypeng



From ryszard.czerminski at pharma.novartis.com  Fri Oct 31 21:32:50 2003
From: ryszard.czerminski at pharma.novartis.com (ryszard.czerminski@pharma.novartis.com)
Date: Fri, 31 Oct 2003 15:32:50 -0500
Subject: [R] problem with tune.svm
Message-ID: <OFEA899A05.0FE7ED23-ON85256DD0.0070B494-85256DD0.0070F38F@EU.novartis.net>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20031031/103b900b/attachment.pl

From RBaskin at ahrq.gov  Fri Oct 31 21:37:47 2003
From: RBaskin at ahrq.gov (RBaskin@ahrq.gov)
Date: Fri, 31 Oct 2003 15:37:47 -0500
Subject: [R] Weird problem with median on a factor
Message-ID: <3598558AD728D41183350008C7CF291C0F16B986@exchange1.ahrq.gov>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20031031/a98c3e2f/attachment.pl

From Benjamin.STABLER at odot.state.or.us  Fri Oct 31 21:50:52 2003
From: Benjamin.STABLER at odot.state.or.us (Benjamin.STABLER@odot.state.or.us)
Date: Fri, 31 Oct 2003 12:50:52 -0800
Subject: [R] Array Dimension Names
Message-ID: <76A000A82289D411952F001083F9DD06047FE319@exsalem4-bu.odot.state.or.us>

I would like to reference array dimensions by name in an apply and a summary
function.  For example:

apply(x, "workers", sum)

Is there a better way to do this than creating a new attribute for the array
and then creating new methods for apply and summary?  I don't want to name
the individual elements of each dimension (such as with dimnames) but rather
name the dimensions.  Thanks for your help.

Benjamin Stabler
Transportation Planning Analysis Unit
Oregon Department of Transportation
555 13th Street NE, Suite 2
Salem, OR 97301  Ph: 503-986-4104



From tplate at blackmesacapital.com  Fri Oct 31 21:56:16 2003
From: tplate at blackmesacapital.com (Tony Plate)
Date: Fri, 31 Oct 2003 13:56:16 -0700
Subject: [R] Weird problem with median on a factor
In-Reply-To: <3598558AD728D41183350008C7CF291C0F16B986@exchange1.ahrq.go
 v>
Message-ID: <5.2.1.1.2.20031031135318.041756d8@mailhost.blackmesacapital.com>

median() has this test in it:
     if (mode(x) != "numeric")
         stop("need numeric data")

Note the following:

 > is.numeric(factor(letters))
[1] FALSE
 > mode(factor(letters))
[1] "numeric"
 >

It seems as though median() is using the wrong test.

-- Tony Plate

At Friday 03:37 PM 10/31/2003 -0500, RBaskin at ahrq.gov wrote:
>Beating a dead horse...
>
>I am an R beginner trying to understand this factor business.  While the
>entire business of finding the median of factor may be silly from a
>practical point of view, this email chain has helped me understand
>something.
>
>I have looked at the median function and it tests to see if what is passed
>to it is numeric.  If I were building a function, if I tested for mode
>numeric, and if something told me it was numeric then like the median
>function I would naively assume that I could do arithmetic on it:
> > saywhut<-as.factor(c(NA,"1","1","1","1","2","10"))
> > mode(saywhut)
>[1] "numeric"
>
>It appears to me that the when the median function tests for numeric it
>doesn't have the desired result with an object of class factor (and maybe
>other classes?) as was shown by the example.
>
>I have a suspicion that something of class factor has at least two pieces,
>one of which is the levels which can possibly be character or something else
>and the other piece is the ordering of the levels which is of storage.mode
>integer.  Is it this ordering that determines the mode of the factor??
>
>But if the mode of factor is truly numeric, why doesn't the median function
>use the numeric piece for finding the median (like it did with odd n - not
>that anyone would ever really want the median of a factor:)??  I think that
>Simon Fear hit on the right idea because of the definition of median that is
>used for an even number of observations takes the sum of the ordered middle
>two observations.  It is the sum (called by the median function) that chokes
>on a factor.
>
> > sum(saywhut,na.rm=T)
>Error in Summary.factor(..., na.rm = na.rm) :
>         "sum" not meaningful for factors
>
>It appears that whoever built the sum function built in a test for factor
>(Simon Fear's first suggestion for median)
>
>
>On the other hand:
> > sd(saywhut,na.rm=T)
>[1] 3.614784
>(Simon Fear's second suggestion for median)
>
>Bytheway, mean treats factor in different way:
>mean(saywhut)
>[1] NA
>Warning message:
>argument is not numeric or logical: returning NA in: mean.default(saywhut).
>
>
>There is an R-FAQ that tells one how to convert a factor to 'numeric' but if
>I had tested for something being numeric to begin with I never would have
>guessed that I needed to convert it to numeric.  I think what this
>conversion is really doing is getting rid of the machinery associated with
>the class factor:
> > #from the R-FAQ
> > test<-as.numeric(as.character(saywhut))
> > mode(test)
>[1] "numeric"
> > median(test,na.rm=T)
>[1] 1
>
>and bytheway:
> > not.a.factor<-c(NA,"1","1","2","10")
> > mode(not.a.factor)
>[1] "character"
> > median(not.a.factor,na.rm=T)
>Error in median(not.a.factor, na.rm = T) :
>         need numeric data
>
>
><Simon Fear: It seems to me the best way to deal with this "bug" would
>be to make calling median with a factor argument be an immediate error.>
>Do you think that all base functions (sum, sd, mean, median,...) should deal
>with this in a consistent way (This might be much more work.)?  Another
>thing that would make things consistent would be to take the stop-work
>behavior out of sum:)
>
>I don't think there is any real problem in the current behavior of factor as
>long as the interaction between functions and classes produces this
>stop-work behavior - preferably with a warning - and not unexpected side
>effects. I am curious if there are other classes of mode numeric which
>median-mean-sum-sd-etc might choke on.
>
><tongue-in-cheek on>
>Of course, R would produce a median for factors by using the "correct"
>defintion of a median of samples i.e., one that agrees with the definition
>of median on a CDF, even though this concept gives most people apoplexy.
><off>
>Thanks
>Bob
>Usual disclaimers....
>
>
>-----Original Message-----
>From: Simon Fear [mailto:Simon.Fear at synequanon.com]
>Sent: Friday, October 31, 2003 6:18 AM
>To: Christoph Bier
>Cc: r-help at stat.math.ethz.ch
>Subject: RE: [R] Weird problem with median on a factor
>
>Final guess as to observed behaviour: in the first case after
>removal of NAs there were an odd number of observations
>(so that sum was not called within the code for median).
>In your second call I suspect that even though you got
>an integer answer, it was found as sum(2,2)/2.
>
>It seems to me the best way to deal with this "bug" would
>be to make calling median with a factor argument be an
>immediate error. Or just trust users never to attempt such
>a thing ...
>
>Simon Fear
>Senior Statistician
>Syne qua non Ltd
>Tel: +44 (0) 1379 644449
>Fax: +44 (0) 1379 644445
>email: Simon.Fear at synequanon.com
>web: http://www.synequanon.com
>
>Number of attachments included with this message: 0
>
>This message (and any associated files) is confidential and\...{{dropped}}
>
>______________________________________________
>R-help at stat.math.ethz.ch mailing list
>https://www.stat.math.ethz.ch/mailman/listinfo/r-help
>
>         [[alternative HTML version deleted]]
>
>______________________________________________
>R-help at stat.math.ethz.ch mailing list
>https://www.stat.math.ethz.ch/mailman/listinfo/r-help



From bolker at zoo.ufl.edu  Fri Oct 31 22:13:28 2003
From: bolker at zoo.ufl.edu (Ben Bolker)
Date: Fri, 31 Oct 2003 16:13:28 -0500 (EST)
Subject: [R] Array Dimension Names
In-Reply-To: <76A000A82289D411952F001083F9DD06047FE319@exsalem4-bu.odot.state.or.us>
Message-ID: <Pine.LNX.4.44.0310311607050.31814-100000@bolker.zoo.ufl.edu>


  Not that I know of.  BUT dimnames can themselves have names attributes, 
so a very small hack to apply() will do what you want.

I did 

dump("apply",file="apply.R")

and added the following lines after "dn <- dimnames(X)" (line 14) [this is 
in R 1.7.1].

    if (is.character(MARGIN)) {
      if (is.null(dn) stop("dimnames(X) must have names")
      MARGIN <- match(MARGIN,names(dn))
    }

and then did 

source("apply.R")
x = array(1,dim=c(2,2,2))
dimnames(x) = list(a=1:2,b=1:2,c=1:2)
apply(x,"a",sum)
apply(x,c("a","b"),sum)

On Fri, 31 Oct 2003 Benjamin.STABLER at odot.state.or.us wrote:

> I would like to reference array dimensions by name in an apply and a summary
> function.  For example:
> 
> apply(x, "workers", sum)
> 
> Is there a better way to do this than creating a new attribute for the array
> and then creating new methods for apply and summary?  I don't want to name
> the individual elements of each dimension (such as with dimnames) but rather
> name the dimensions.  Thanks for your help.
> 
> Benjamin Stabler
> Transportation Planning Analysis Unit
> Oregon Department of Transportation
> 555 13th Street NE, Suite 2
> Salem, OR 97301  Ph: 503-986-4104
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> 

-- 
620B Bartram Hall                            bolker at zoo.ufl.edu
Zoology Department, University of Florida    http://www.zoo.ufl.edu/bolker
Box 118525                                   (ph)  352-392-5697
Gainesville, FL 32611-8525                   (fax) 352-392-3704



From tplate at blackmesacapital.com  Fri Oct 31 22:06:45 2003
From: tplate at blackmesacapital.com (Tony Plate)
Date: Fri, 31 Oct 2003 14:06:45 -0700
Subject: [R] Array Dimension Names
In-Reply-To: <76A000A82289D411952F001083F9DD06047FE319@exsalem4-bu.odot.
	state.or.us>
Message-ID: <5.2.1.1.2.20031031140346.0414bf60@mailhost.blackmesacapital.com>

You can already name dimensions using standard arrays (but you can't use 
these names for the MARGIN argument of apply) e.g.:

 > x <- array(1:6, 3:2, dimnames=list(rows=letters[1:3],cols=LETTERS[24:25]))
 > x
     cols
rows X Y
    a 1 4
    b 2 5
    c 3 6
 > apply(x, 2, sum)
  X  Y
  6 15
 > apply(x, "cols", sum)
Error in -MARGIN : Invalid argument to unary operator
 >

You could pretty easily create your own version of apply() that checked if 
MARGIN was character, and if it were, matched it against names(dimnames(X))

hope this helps,

Tony Plate

At Friday 12:50 PM 10/31/2003 -0800, Benjamin.STABLER at odot.state.or.us wrote:
>I would like to reference array dimensions by name in an apply and a summary
>function.  For example:
>
>apply(x, "workers", sum)
>
>Is there a better way to do this than creating a new attribute for the array
>and then creating new methods for apply and summary?  I don't want to name
>the individual elements of each dimension (such as with dimnames) but rather
>name the dimensions.  Thanks for your help.
>
>Benjamin Stabler
>Transportation Planning Analysis Unit
>Oregon Department of Transportation
>555 13th Street NE, Suite 2
>Salem, OR 97301  Ph: 503-986-4104
>
>______________________________________________
>R-help at stat.math.ethz.ch mailing list
>https://www.stat.math.ethz.ch/mailman/listinfo/r-help



From hpsbranco at superig.com.br  Fri Oct 31 22:05:31 2003
From: hpsbranco at superig.com.br (=?iso-8859-1?Q?Henrique_Patr=EDcio_Sant'Anna_Branco?=)
Date: Fri, 31 Oct 2003 19:05:31 -0200
Subject: [R] Problems with help.start()
Message-ID: <000901c39ff2$b81bfd90$019da8c0@henrique>

Hello there,
I've just installed R for Windows 1.8.0 and I'm experiencing problems with
help.start().
It opens the help page as it supposed to do, then I go to "Search Engine &
Keywords", but when I click anything there, it returns me a (aparently)
JavaScript error, something like "The object does not support this method or
property".
The Search Engine doesn't work as well.
I tried to open with Internet Explorer 6 and Mozilla (the last one).
Have anybody already seen this problem?
Thanks,
Henrique.



From bill.shipley at usherbrooke.ca  Fri Oct 31 22:09:36 2003
From: bill.shipley at usherbrooke.ca (Bill Shipley)
Date: Fri, 31 Oct 2003 16:09:36 -0500
Subject: [R] help with constrOptim function
Message-ID: <00fa01c39ff3$4a133d30$8d1ad284@BIO041>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20031031/e5d24a9c/attachment.pl

From ross at biostat.ucsf.edu  Fri Oct 31 22:12:10 2003
From: ross at biostat.ucsf.edu (Ross Boylan)
Date: Fri, 31 Oct 2003 13:12:10 -0800
Subject: [R] Re: packaging a package addon
In-Reply-To: <Pine.LNX.4.44.0310311109460.18842-100000@gannet.stats>
References: <Pine.LNX.4.44.0310311109460.18842-100000@gannet.stats>
Message-ID: <1067634730.2454.7.camel@iron.libaux.ucsf.edu>

On Fri, 2003-10-31 at 03:14, Prof Brian Ripley wrote:

> But I would not be doing R CMD check until I had both installed and 
> loaded the package and run a few examples.

That's interesting; I thought R CMD check was supposed to be done before
hand.  So are you saying the proper development sequence is
1.
Do as much as you can with the basic R and C (Fortran, whatever) code to
check it's OK.

2.
R CMD build
test, revise
R CMD build
etc

3. then, when everything looks OK
R CMD check
?



From ross at biostat.ucsf.edu  Fri Oct 31 22:14:04 2003
From: ross at biostat.ucsf.edu (Ross Boylan)
Date: Fri, 31 Oct 2003 13:14:04 -0800
Subject: [R] Re: packaging a package addon
In-Reply-To: <85ad7hmpu1.fsf@blindglobe.net>
References: <1067474801.20574.195.camel@iron.libaux.ucsf.edu>
	<1067483590.20574.252.camel@iron.libaux.ucsf.edu>
	<1067550460.20573.266.camel@iron.libaux.ucsf.edu>
	<85ad7hmpu1.fsf@blindglobe.net>
Message-ID: <1067634844.2454.10.camel@iron.libaux.ucsf.edu>

On Fri, 2003-10-31 at 06:41, A.J. Rossini wrote:
> Ross Boylan <ross at biostat.ucsf.edu> writes:
> >
> > I also added library("survival") to my .First.lib.  Is library, rather
> > than require, the right choice here?  I want it to fail if survival
> > doesn't load.
> 
> test the results from require, something like: 
> 
>      if (!require("survival")) stop("can't load survival")
Doesn't using library do about the same thing?  What's the advantage of
this, clearer diagnostics?



From ripley at stats.ox.ac.uk  Fri Oct 31 22:29:36 2003
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Fri, 31 Oct 2003 21:29:36 +0000 (GMT)
Subject: [R] Re: packaging a package addon
In-Reply-To: <1067634730.2454.7.camel@iron.libaux.ucsf.edu>
Message-ID: <Pine.LNX.4.44.0310312127400.28738-100000@gannet.stats>

I would do 1 3 2  as R CMD check works on a source dir, not a .tar.gz.

Your mileage may vary and all that.

I'd say R CMD check was a final check before distribution via R CMD build.

On Fri, 31 Oct 2003, Ross Boylan wrote:

> On Fri, 2003-10-31 at 03:14, Prof Brian Ripley wrote:
> 
> > But I would not be doing R CMD check until I had both installed and 
> > loaded the package and run a few examples.
> 
> That's interesting; I thought R CMD check was supposed to be done before
> hand.  So are you saying the proper development sequence is
> 1.
> Do as much as you can with the basic R and C (Fortran, whatever) code to
> check it's OK.
> 
> 2.
> R CMD build
> test, revise
> R CMD build
> etc
> 
> 3. then, when everything looks OK
> R CMD check
> ?
> 
> 

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From ripley at stats.ox.ac.uk  Fri Oct 31 22:33:19 2003
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Fri, 31 Oct 2003 21:33:19 +0000 (GMT)
Subject: [R] Problems with help.start()
In-Reply-To: <000901c39ff2$b81bfd90$019da8c0@henrique>
Message-ID: <Pine.LNX.4.44.0310312129530.28738-100000@gannet.stats>

No, so looks like a problem with your browser/OS.  Are you sure you have 
working Java and JavaScript?

Both a fully patched IE6 and Netscape 7.1 work for me (as do Opera 6 
and Mozilla 1.2).

On Fri, 31 Oct 2003, Henrique Patr?cio Sant'Anna Branco wrote:

> Hello there,
> I've just installed R for Windows 1.8.0 and I'm experiencing problems with
> help.start().
> It opens the help page as it supposed to do, then I go to "Search Engine &
> Keywords", but when I click anything there, it returns me a (aparently)
> JavaScript error, something like "The object does not support this method or
> property".
> The Search Engine doesn't work as well.
> I tried to open with Internet Explorer 6 and Mozilla (the last one).
> Have anybody already seen this problem?
> Thanks,
> Henrique.
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> 
> 

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From Benjamin.STABLER at odot.state.or.us  Fri Oct 31 22:33:35 2003
From: Benjamin.STABLER at odot.state.or.us (Benjamin.STABLER@odot.state.or.us)
Date: Fri, 31 Oct 2003 13:33:35 -0800
Subject: [R] Array Dimension Names
Message-ID: <76A000A82289D411952F001083F9DD06047FE31A@exsalem4-bu.odot.state.or.us>

Oh yeah, thanks.  I thought I might write a function such as getNames() that
returns the dimension number of the names of the dimnames of an object.
That way I don't have to rewrite apply, sweep, and aperm.  But even better
would be for R to allow character names in addition to index numbers for the
MARGIN argument to apply and sweep, and the perm argument to aperm.

Thanks again,
Ben Stabler

>-----Original Message-----
>From: Tony Plate [mailto:tplate at blackmesacapital.com]
>Sent: Friday, October 31, 2003 1:07 PM
>To: STABLER Benjamin; r-help at stat.math.ethz.ch
>Subject: Re: [R] Array Dimension Names
>
>
>You can already name dimensions using standard arrays (but you 
>can't use 
>these names for the MARGIN argument of apply) e.g.:
>
> > x <- array(1:6, 3:2, 
>dimnames=list(rows=letters[1:3],cols=LETTERS[24:25]))
> > x
>     cols
>rows X Y
>    a 1 4
>    b 2 5
>    c 3 6
> > apply(x, 2, sum)
>  X  Y
>  6 15
> > apply(x, "cols", sum)
>Error in -MARGIN : Invalid argument to unary operator
> >
>
>You could pretty easily create your own version of apply() 
>that checked if 
>MARGIN was character, and if it were, matched it against 
>names(dimnames(X))
>
>hope this helps,
>
>Tony Plate
>
>At Friday 12:50 PM 10/31/2003 -0800, 
>Benjamin.STABLER at odot.state.or.us wrote:
>>I would like to reference array dimensions by name in an 
>apply and a summary
>>function.  For example:
>>
>>apply(x, "workers", sum)
>>
>>Is there a better way to do this than creating a new 
>attribute for the array
>>and then creating new methods for apply and summary?  I don't 
>want to name
>>the individual elements of each dimension (such as with 
>dimnames) but rather
>>name the dimensions.  Thanks for your help.
>>
>>Benjamin Stabler
>>Transportation Planning Analysis Unit
>>Oregon Department of Transportation
>>555 13th Street NE, Suite 2
>>Salem, OR 97301  Ph: 503-986-4104
>>
>>______________________________________________
>>R-help at stat.math.ethz.ch mailing list
>>https://www.stat.math.ethz.ch/mailman/listinfo/r-help
>



From ross at biostat.ucsf.edu  Fri Oct 31 22:37:59 2003
From: ross at biostat.ucsf.edu (Ross Boylan)
Date: Fri, 31 Oct 2003 13:37:59 -0800
Subject: [R] Re: packaging a package addon
In-Reply-To: <Pine.LNX.4.44.0310312127400.28738-100000@gannet.stats>
References: <Pine.LNX.4.44.0310312127400.28738-100000@gannet.stats>
Message-ID: <1067636279.2454.16.camel@iron.libaux.ucsf.edu>

On Fri, 2003-10-31 at 13:29, Prof Brian Ripley wrote:
> I would do 1 3 2  as R CMD check works on a source dir, not a .tar.gz.
> 
> Your mileage may vary and all that.
I'm not following something.  1 3 2 was what I was doing.  I thought you
said (below on Fri) that you'd do 2, then 3.  I may be misunderstanding
what the phrase "installed and loaded the package" means.  I thought
installing and loading it referred to doing an R CMD build to make the
package, and then R CMD INSTALL on the result.

> 
> I'd say R CMD check was a final check before distribution via R CMD build.
> 
> On Fri, 31 Oct 2003, Ross Boylan wrote:
> 
> > On Fri, 2003-10-31 at 03:14, Prof Brian Ripley wrote:
> > 
> > > But I would not be doing R CMD check until I had both installed and 
> > > loaded the package and run a few examples.
> > 
> > That's interesting; I thought R CMD check was supposed to be done before
> > hand.  So are you saying the proper development sequence is
> > 1.
> > Do as much as you can with the basic R and C (Fortran, whatever) code to
> > check it's OK.
> > 
> > 2.
> > R CMD build
> > test, revise
> > R CMD build
> > etc
> > 
> > 3. then, when everything looks OK
> > R CMD check
> > ?
> > 
> > 
-- 
Ross Boylan                                      wk:  (415) 502-4031
530 Parnassus Avenue (Library) rm 115-4          ross at biostat.ucsf.edu
Dept of Epidemiology and Biostatistics           fax: (415) 476-9856
University of California, San Francisco
San Francisco, CA 94143-0840                     hm:  (415) 550-1062



From tlumley at u.washington.edu  Fri Oct 31 22:42:37 2003
From: tlumley at u.washington.edu (Thomas Lumley)
Date: Fri, 31 Oct 2003 13:42:37 -0800 (PST)
Subject: [R] Re: packaging a package addon
In-Reply-To: <1067634844.2454.10.camel@iron.libaux.ucsf.edu>
References: <1067474801.20574.195.camel@iron.libaux.ucsf.edu>
	<1067483590.20574.252.camel@iron.libaux.ucsf.edu>
	<1067550460.20573.266.camel@iron.libaux.ucsf.edu>
	<85ad7hmpu1.fsf@blindglobe.net>
	<1067634844.2454.10.camel@iron.libaux.ucsf.edu>
Message-ID: <Pine.A41.4.58.0310311338400.184628@homer06.u.washington.edu>

On Fri, 31 Oct 2003, Ross Boylan wrote:

> On Fri, 2003-10-31 at 06:41, A.J. Rossini wrote:
> > Ross Boylan <ross at biostat.ucsf.edu> writes:
> > >
> > > I also added library("survival") to my .First.lib.  Is library, rather
> > > than require, the right choice here?  I want it to fail if survival
> > > doesn't load.
> >
> > test the results from require, something like:
> >
> >      if (!require("survival")) stop("can't load survival")
> Doesn't using library do about the same thing?  What's the advantage of
> this, clearer diagnostics?
>


If you are going to fail when "survival" isn't found you should probably
just use library, though Tony's suggestion is effectively equivalent, and
I have also seen the Perly
   require(tcltk) || stop("error message")

The main point of require() is when failure isn't completely fatal: eg
hypothetically

if (!require(boot)) {
	warning("No `boot' package -- you're not getting confidence intervals")
	conf.int<-FALSE
}


	-thomas



From tlumley at u.washington.edu  Fri Oct 31 22:47:41 2003
From: tlumley at u.washington.edu (Thomas Lumley)
Date: Fri, 31 Oct 2003 13:47:41 -0800 (PST)
Subject: [R] help with constrOptim function
In-Reply-To: <00fa01c39ff3$4a133d30$8d1ad284@BIO041>
References: <00fa01c39ff3$4a133d30$8d1ad284@BIO041>
Message-ID: <Pine.A41.4.58.0310311346100.184628@homer06.u.washington.edu>

On Fri, 31 Oct 2003, Bill Shipley wrote:

> Hello.  I had previously posted a question concerning the optimization
> of a nonlinear function conditional on equality constraints.  I was
> pointed towards the contrOptim function.

Perhaps mistakenly, then.  constrOptim() does linear *inequality*
constraints.  Equality constraints are probably best handled by Lagrange
multipliers or reparametrisation.

	-thomas



From MSchwartz at medanalytics.com  Fri Oct 31 23:00:54 2003
From: MSchwartz at medanalytics.com (Marc Schwartz)
Date: Fri, 31 Oct 2003 16:00:54 -0600
Subject: [R] Problems with help.start()
In-Reply-To: <000901c39ff2$b81bfd90$019da8c0@henrique>
References: <000901c39ff2$b81bfd90$019da8c0@henrique>
Message-ID: <1067637653.4937.358.camel@localhost.localdomain>

On Fri, 2003-10-31 at 15:05, Henrique Patr?cio Sant'Anna Branco wrote:
> Hello there,
> I've just installed R for Windows 1.8.0 and I'm experiencing problems with
> help.start().
> It opens the help page as it supposed to do, then I go to "Search Engine &
> Keywords", but when I click anything there, it returns me a (aparently)
> JavaScript error, something like "The object does not support this method or
> property".
> The Search Engine doesn't work as well.
> I tried to open with Internet Explorer 6 and Mozilla (the last one).
> Have anybody already seen this problem?
> Thanks,
> Henrique.


This has been discussed previously. It is tied to having both Java and
JavaScript functioning in your browser. If Java is installed on your
computer, be sure that both are enabled in your browser. Both must be
functional in order to use the help.start() search engine which is a
java applet.

The following link provides information for Mozilla 1.5, which is the
latest version, regarding installing Java:

http://www.mozilla.org/releases/mozilla1.5/installation-extras.html

There is also a FAQ at Sun here:

http://www.java.com/en/download/help/enable_browser.jsp

If you do not have Java installed on your computer yet, you can go here:

http://www.java.com/en/download/help/auto_install.jsp

for instructions on how to download and install it.

HTH,

Marc Schwartz



From ripley at stats.ox.ac.uk  Fri Oct 31 23:11:57 2003
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Fri, 31 Oct 2003 22:11:57 +0000 (GMT)
Subject: [R] Re: packaging a package addon
In-Reply-To: <1067636279.2454.16.camel@iron.libaux.ucsf.edu>
Message-ID: <Pine.LNX.4.44.0310312211110.28801-100000@gannet.stats>

On Fri, 31 Oct 2003, Ross Boylan wrote:

> On Fri, 2003-10-31 at 13:29, Prof Brian Ripley wrote:
> > I would do 1 3 2  as R CMD check works on a source dir, not a .tar.gz.
> > 
> > Your mileage may vary and all that.
> I'm not following something.  1 3 2 was what I was doing.  I thought you
> said (below on Fri) that you'd do 2, then 3.  I may be misunderstanding
> what the phrase "installed and loaded the package" means.  I thought
> installing and loading it referred to doing an R CMD build to make the
> package, and then R CMD INSTALL on the result.

NO, do R CMD INSTALL on the sources, and the R  run some tests.

> 
> > 
> > I'd say R CMD check was a final check before distribution via R CMD build.
> > 
> > On Fri, 31 Oct 2003, Ross Boylan wrote:
> > 
> > > On Fri, 2003-10-31 at 03:14, Prof Brian Ripley wrote:
> > > 
> > > > But I would not be doing R CMD check until I had both installed and 
> > > > loaded the package and run a few examples.
> > > 
> > > That's interesting; I thought R CMD check was supposed to be done before
> > > hand.  So are you saying the proper development sequence is
> > > 1.
> > > Do as much as you can with the basic R and C (Fortran, whatever) code to
> > > check it's OK.
> > > 
> > > 2.
> > > R CMD build
> > > test, revise
> > > R CMD build
> > > etc
> > > 
> > > 3. then, when everything looks OK
> > > R CMD check
> > > ?
> > > 
> > > 
> 

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



